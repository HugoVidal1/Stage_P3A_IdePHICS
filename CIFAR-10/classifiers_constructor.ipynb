{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337dd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6cb0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# improve the ploting style\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "rcParams['font.size'] = 22\n",
    "rcParams['mathtext.fontset'] = 'stix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7698be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 10]) torch.Size([10000, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 32, 32, 3]) torch.Size([30000, 1]) tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) torch.Size([6000, 32, 32, 3]) torch.Size([6000, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 1024]) torch.Size([6000, 1024])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "transform_data = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) # Apply the (x - mean)/var operation on the components of the data # if x is in [0,1] then Normalise(x) is in [-1,1] # is applied on the three channels RGB\n",
    "])\n",
    "\n",
    "# Data import\n",
    "dtype = torch.float32\n",
    "trainset = torchvision.datasets.CIFAR10(root = './datas', train= True, download = True, transform = transform_data)\n",
    "validset = torchvision.datasets.CIFAR10(root = './datas', train = False, download = True, transform = transform_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 1)\n",
    "\n",
    "x_train, y_train_raw, x_valid, y_valid_raw = torch.tensor(trainset.data), torch.tensor(trainset.targets), torch.tensor(validset.data), torch.tensor(validset.targets)\n",
    "\n",
    "# Modification du format des donnes shape (n_data,1) -> (n_data, n_classes)\n",
    "y_train = torch.zeros((y_train_raw.shape[0], torch.max(y_train_raw)+1))\n",
    "for i, y in enumerate(y_train_raw):\n",
    "    j = int(y.item())\n",
    "    y_train[i,j] = 1\n",
    "\n",
    "y_valid = torch.zeros((y_valid_raw.shape[0], torch.max(y_valid_raw)+1))\n",
    "for i,y in enumerate(y_valid_raw):\n",
    "    j = int(y.item())\n",
    "    y_valid[i,j] = 1 \n",
    "print(y_train.shape, y_valid.shape)\n",
    "\n",
    "# Binary reduction of the classes # To avoid using softmax, we regroup classes in two classes\n",
    "class_binary_reduction = True\n",
    "determination_des_classes = True\n",
    "black_and_white_images = True\n",
    "\n",
    "if class_binary_reduction :\n",
    "    if determination_des_classes :\n",
    "        # Determination des classes\n",
    "        class_list = []\n",
    "        class_index = 0\n",
    "        for i in range (x_train.shape[0]):\n",
    "            if y_train[i, class_index] == 1:\n",
    "                class_list.append(x_train[i])\n",
    "                class_index += 1\n",
    "            if len(class_list) == len(y_train[0]):\n",
    "                break\n",
    "        for i, image in enumerate(class_list):\n",
    "            plt.subplot(2, int(len(class_list)/2+1),i+1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "        plt.show()\n",
    "                \n",
    "    # classe1 = [0, 1, 8, 9]  # vehicles\n",
    "    # classe2 = [2, 3, 4, 5]  # animals\n",
    "    \n",
    "    classe1 = [1, 3, 4]  # elk\n",
    "    classe2 = [5, 7, 9]  # horse\n",
    "\n",
    "    # Cration des masques pour les chantillons appartenant  ces classes\n",
    "    mask_classe1_train = y_train[:, classe1].sum(dim=1) > 0  # True si appartient  classe1\n",
    "    mask_classe2_train = y_train[:, classe2].sum(dim=1) > 0  # True si appartient  classe2\n",
    "    \n",
    "    mask_classe1_valid = y_valid[:, classe1].sum(dim=1) > 0\n",
    "    mask_classe2_valid = y_valid[:, classe2].sum(dim=1) > 0\n",
    "    \n",
    "    # Filtrage des exemples concerns\n",
    "    mask_train = torch.logical_or(mask_classe1_train, mask_classe2_train)\n",
    "    mask_valid = torch.logical_or(mask_classe1_valid, mask_classe2_valid)\n",
    "    x_train, y_train = x_train[mask_train], y_train[mask_train]\n",
    "    x_valid, y_valid = x_valid[mask_valid], y_valid[mask_valid]\n",
    "\n",
    "    # Cration du vecteur de labels binaires (1 pour classe1, 0 pour classe2)\n",
    "    y_train = (y_train[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    y_valid = (y_valid[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    \n",
    "    # Avec tanh\n",
    "    \n",
    "    # x_train = 2*(x_train-0.5)\n",
    "    # y_train = 2*(y_train-0.5)\n",
    "    # x_valid = 2*(x_valid-0.5)\n",
    "    # y_valid = 2*(y_valid-0.5)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, y_train[0:10], x_valid.shape, y_valid.shape)\n",
    "\n",
    "if black_and_white_images :\n",
    "    x_train = 0.299*x_train[:,:,:,0] + 0.587*x_train[:,:,:,1] + 0.114*x_train[:,:,:,2]\n",
    "    x_valid = 0.299*x_valid[:,:,:,0] + 0.587*x_valid[:,:,:,1] + 0.114*x_valid[:,:,:,2]\n",
    "    for i, image in enumerate(x_train[0:10]):\n",
    "        plt.subplot(2, int(len(x_train[0:10])/2+1),i+1)\n",
    "        plt.imshow(image, cmap = 'grey')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "    x_train, x_valid = (x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])).to(dtype), x_valid.reshape(x_valid.shape[0], x_valid.shape[1]*x_valid.shape[2]).to(dtype)\n",
    "    print(x_train.shape, x_valid.shape)\n",
    "\n",
    "else :    \n",
    "    x_train, x_valid = (x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2]*x_train.shape[3])).to(dtype), x_valid.reshape(x_valid.shape[0], x_valid.shape[1]*x_valid.shape[2]*x_valid.shape[3]).to(dtype)\n",
    "    print(x_train.shape, x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45060724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing on :  cpu\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "# else :\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Computing on : \", device)\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0).to(device),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).type(dtype).to(device)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), dj softmax\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque chantillon\n",
    "    \"\"\"\n",
    "    s = s.to(device)\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype).to(device) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = drive de softmax au logit j du ime batch par rapport au logit k du mme batch\n",
    "    for i in range(n):  # Pour chaque chantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-me donne du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des drives croises) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return torch.exp(-x)/((1 + torch.exp(-x))**2)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - (torch.tanh(x)**2)).to(device)\n",
    "\n",
    "class two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajust en fonction du seuil kappa_eff d'apprentissage des donnes.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prdiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output); grad_z2  = grad_z2.to(dtype) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la dure de l'entranement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "   \n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, number_of_classes,lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(three_layer_NN,self).__init__()\n",
    "        # Initialisation des proprits du rseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la proprit dure d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du rseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(number_of_classes, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t()\n",
    "        output = self.softmax(z3) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage alatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            \n",
    "            # Calcul de la prdiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = (torch.einsum('no,noz->nz',grad_output,softmax_derivative(output))).to(dtype) # shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la dure d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "\n",
    "class binary_classification_two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size, lr=1e-3, lr_decay_rate = 1e9, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.lr = lr\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(1, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x, dropout_rate=0):\n",
    "        if dropout_rate != 0 :    \n",
    "            dropout_mask_1 = ((torch.rand((x.shape[0], self.hidden_1_size)) > dropout_rate).to(dtype)).to(device)\n",
    "        else : \n",
    "            dropout_mask_1 = 1\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)*dropout_mask_1  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = torch.sigmoid(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    \n",
    "    # coef_iter est a ajust en fonction du seuil kappa_eff d'apprentissage des donnes.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, lr_decay_rate = 1e9, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True, dropout_rate = 0):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - lr_decay_rate = \" + str(lr_decay_rate) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) + ' - observation rate = ' + str(observation_rate) + ' - Train layer 1 = ' + str(train_layer_1) + ' - Train layer 2 = ' + str(train_layer_2) + ' - Dropout rate = ' + str(dropout_rate)\n",
    "        self.lr = torch.tensor(lr)\n",
    "        self.lr_decay_rate = torch.tensor(lr_decay_rate)\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            \n",
    "            # Calcul de la prdiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch, dropout_rate)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                print(\"Output\", output[20:22])\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = grad_output*sigmoid_derivative(z2); grad_z2  = grad_z2.to(dtype) # shape(n_data, 1)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # if i == 250:\n",
    "            #     break\n",
    "\n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "            \n",
    "            # Learning rate decay\n",
    "            self.lr = lr*torch.exp(torch.tensor(-i/self.lr_decay_rate))\n",
    "        \n",
    "        # Calcul de la dure de l'entranement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "class binary_classification_three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, lr=0.01, lr_decay_rate = 1e9, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, dropout_rate = 0):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_three_layer_NN,self).__init__()\n",
    "        # Initialisation des proprits du rseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.lr = lr\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la proprit dure d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du rseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(1, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x, dropout_rate=0):\n",
    "        if dropout_rate != 0 :    \n",
    "            dropout_mask_1 = ((torch.rand((x.shape[0], self.hidden_1_size)) > dropout_rate).to(dtype)).to(device)\n",
    "            dropout_mask_2 = ((torch.rand((x.shape[0], self.hidden_2_size)) > dropout_rate).to(dtype)).to(device)\n",
    "        else : \n",
    "            dropout_mask_1, dropout_mask_2 = 1, 1\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)*dropout_mask_1  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, hidden_2_size ) # logits layer 2\n",
    "        h2 = ReLU(z2)*dropout_mask_2 # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t() # shape (n_data, 1)\n",
    "        output = sigmoid(z3) # output layer # shape (n_data, 1)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, lr_decay_rate = 1e9, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True, dropout_rate = 0):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - lr_decay_rate \" + str(lr_decay_rate) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = torch.tensor(lr)\n",
    "        self.decay_rate = lr_decay_rate\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage alatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            \n",
    "            # Dropout\n",
    "            # dropout_mask = ((torch.rand((minibatch_size, x_train.shape[1])) > dropout_rate).to(dtype)).to(device)\n",
    "            # x_minibatch = x_minibatch*dropout_mask\n",
    "            \n",
    "            # Calcul de la prdiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch, dropout_rate)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = grad_output*sigmoid_derivative(z3).to(dtype) # shape (n_data, 1) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "            \n",
    "            # Learning rate decay\n",
    "            self.lr = lr*torch.exp(torch.tensor(-i/self.lr_decay_rate))\n",
    "            \n",
    "            # if i == 12000:\n",
    "            #     break\n",
    "        \n",
    "        # Calcul de la dure d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36584c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_2_layer = binary_classification_two_layer_NN(1024, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57236d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2.55, the number of datas used for the training is 237265664.0606286 and the number of iterations is 39544.\n",
      "Output tensor([[0.5108],\n",
      "        [0.4463]])\n",
      "Iteration 0 Training loss 0.13344132900238037 Validation loss 0.1308833211660385 Accuracy 0.4841666519641876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9t/xvrp6p712_b4g4ttdrxvysvr0000gv/T/ipykernel_3319/4241968429.py:427: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.lr = lr*torch.exp(torch.tensor(-i/self.lr_decay_rate))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[0.5492],\n",
      "        [0.4688]])\n",
      "Iteration 10 Training loss 0.13118968904018402 Validation loss 0.1276506930589676 Accuracy 0.500166654586792\n",
      "Output tensor([[0.5088],\n",
      "        [0.4942]])\n",
      "Iteration 20 Training loss 0.13083431124687195 Validation loss 0.1255965530872345 Accuracy 0.5245000123977661\n",
      "Output tensor([[0.3133],\n",
      "        [0.4536]])\n",
      "Iteration 30 Training loss 0.12857112288475037 Validation loss 0.12427426874637604 Accuracy 0.5421666502952576\n",
      "Output tensor([[0.5755],\n",
      "        [0.4636]])\n",
      "Iteration 40 Training loss 0.12806014716625214 Validation loss 0.12365976721048355 Accuracy 0.5576666593551636\n",
      "Output tensor([[0.4762],\n",
      "        [0.4619]])\n",
      "Iteration 50 Training loss 0.12535671889781952 Validation loss 0.12287604063749313 Accuracy 0.5509999990463257\n",
      "Output tensor([[0.4863],\n",
      "        [0.4971]])\n",
      "Iteration 60 Training loss 0.12545986473560333 Validation loss 0.12236533313989639 Accuracy 0.5576666593551636\n",
      "Output tensor([[0.3886],\n",
      "        [0.3843]])\n",
      "Iteration 70 Training loss 0.12508746981620789 Validation loss 0.1220255196094513 Accuracy 0.5641666650772095\n",
      "Output tensor([[0.4958],\n",
      "        [0.5578]])\n",
      "Iteration 80 Training loss 0.12431377917528152 Validation loss 0.12193084508180618 Accuracy 0.5600000023841858\n",
      "Output tensor([[0.2789],\n",
      "        [0.5165]])\n",
      "Iteration 90 Training loss 0.1250174343585968 Validation loss 0.12163209915161133 Accuracy 0.5649999976158142\n",
      "Output tensor([[0.4453],\n",
      "        [0.2520]])\n",
      "Iteration 100 Training loss 0.12412256747484207 Validation loss 0.1213720291852951 Accuracy 0.5676666498184204\n",
      "Output tensor([[0.4986],\n",
      "        [0.6140]])\n",
      "Iteration 110 Training loss 0.12387926131486893 Validation loss 0.12127392739057541 Accuracy 0.5666666626930237\n",
      "Output tensor([[0.4995],\n",
      "        [0.5324]])\n",
      "Iteration 120 Training loss 0.12357020378112793 Validation loss 0.12112016975879669 Accuracy 0.5734999775886536\n",
      "Output tensor([[0.4956],\n",
      "        [0.5437]])\n",
      "Iteration 130 Training loss 0.12375009804964066 Validation loss 0.12105942517518997 Accuracy 0.574999988079071\n",
      "Output tensor([[0.5741],\n",
      "        [0.6058]])\n",
      "Iteration 140 Training loss 0.12232837826013565 Validation loss 0.12091570347547531 Accuracy 0.5741666555404663\n",
      "Output tensor([[0.5446],\n",
      "        [0.5257]])\n",
      "Iteration 150 Training loss 0.12439382076263428 Validation loss 0.12082839757204056 Accuracy 0.5759999752044678\n",
      "Output tensor([[0.3974],\n",
      "        [0.4595]])\n",
      "Iteration 160 Training loss 0.12318389117717743 Validation loss 0.12074679881334305 Accuracy 0.578166663646698\n",
      "Output tensor([[0.4048],\n",
      "        [0.4562]])\n",
      "Iteration 170 Training loss 0.12267110496759415 Validation loss 0.12062954902648926 Accuracy 0.5803333520889282\n",
      "Output tensor([[0.5557],\n",
      "        [0.4721]])\n",
      "Iteration 180 Training loss 0.12334944307804108 Validation loss 0.12057899683713913 Accuracy 0.5808333158493042\n",
      "Output tensor([[0.4648],\n",
      "        [0.5054]])\n",
      "Iteration 190 Training loss 0.1231314018368721 Validation loss 0.12061087042093277 Accuracy 0.577833354473114\n",
      "Output tensor([[0.5850],\n",
      "        [0.4065]])\n",
      "Iteration 200 Training loss 0.12330148369073868 Validation loss 0.1204867735505104 Accuracy 0.5821666717529297\n",
      "Output tensor([[0.5140],\n",
      "        [0.5157]])\n",
      "Iteration 210 Training loss 0.12207118421792984 Validation loss 0.12046345323324203 Accuracy 0.5824999809265137\n",
      "Output tensor([[0.6082],\n",
      "        [0.6037]])\n",
      "Iteration 220 Training loss 0.12273603677749634 Validation loss 0.12039538472890854 Accuracy 0.5836666822433472\n",
      "Output tensor([[0.3081],\n",
      "        [0.5899]])\n",
      "Iteration 230 Training loss 0.12282468378543854 Validation loss 0.12035778909921646 Accuracy 0.5851666927337646\n",
      "Output tensor([[0.3459],\n",
      "        [0.4519]])\n",
      "Iteration 240 Training loss 0.12254007905721664 Validation loss 0.12026489526033401 Accuracy 0.5861666798591614\n",
      "Output tensor([[0.2082],\n",
      "        [0.4361]])\n",
      "Iteration 250 Training loss 0.12282930314540863 Validation loss 0.12036439031362534 Accuracy 0.5863333344459534\n",
      "Output tensor([[0.5583],\n",
      "        [0.5737]])\n",
      "Iteration 260 Training loss 0.12215671688318253 Validation loss 0.12028150260448456 Accuracy 0.5881666541099548\n",
      "Output tensor([[0.5472],\n",
      "        [0.5487]])\n",
      "Iteration 270 Training loss 0.12094979733228683 Validation loss 0.12026432156562805 Accuracy 0.5860000252723694\n",
      "Output tensor([[0.4235],\n",
      "        [0.5038]])\n",
      "Iteration 280 Training loss 0.12222752720117569 Validation loss 0.12013289332389832 Accuracy 0.5873333215713501\n",
      "Output tensor([[0.4970],\n",
      "        [0.4894]])\n",
      "Iteration 290 Training loss 0.12239785492420197 Validation loss 0.12011513113975525 Accuracy 0.5874999761581421\n",
      "Output tensor([[0.5347],\n",
      "        [0.4601]])\n",
      "Iteration 300 Training loss 0.12232597917318344 Validation loss 0.12006459385156631 Accuracy 0.5888333320617676\n",
      "Output tensor([[0.4897],\n",
      "        [0.4878]])\n",
      "Iteration 310 Training loss 0.1228320524096489 Validation loss 0.12002692371606827 Accuracy 0.5863333344459534\n",
      "Output tensor([[0.4106],\n",
      "        [0.5662]])\n",
      "Iteration 320 Training loss 0.12235235422849655 Validation loss 0.12002461403608322 Accuracy 0.5885000228881836\n",
      "Output tensor([[0.5357],\n",
      "        [0.4910]])\n",
      "Iteration 330 Training loss 0.12174835056066513 Validation loss 0.12000581622123718 Accuracy 0.5866666436195374\n",
      "Output tensor([[0.4551],\n",
      "        [0.4940]])\n",
      "Iteration 340 Training loss 0.12213622033596039 Validation loss 0.1199672743678093 Accuracy 0.5860000252723694\n",
      "Output tensor([[0.5465],\n",
      "        [0.4727]])\n",
      "Iteration 350 Training loss 0.1214255765080452 Validation loss 0.12003187835216522 Accuracy 0.5864999890327454\n",
      "Output tensor([[0.6530],\n",
      "        [0.4064]])\n",
      "Iteration 360 Training loss 0.12216092646121979 Validation loss 0.11992651224136353 Accuracy 0.5879999995231628\n",
      "Output tensor([[0.4593],\n",
      "        [0.5561]])\n",
      "Iteration 370 Training loss 0.12167084962129593 Validation loss 0.11990702152252197 Accuracy 0.5881666541099548\n",
      "Output tensor([[0.4855],\n",
      "        [0.5407]])\n",
      "Iteration 380 Training loss 0.12173163145780563 Validation loss 0.11986975371837616 Accuracy 0.5878333449363708\n",
      "Output tensor([[0.5682],\n",
      "        [0.5679]])\n",
      "Iteration 390 Training loss 0.12063995003700256 Validation loss 0.11983880400657654 Accuracy 0.5861666798591614\n",
      "Output tensor([[0.6533],\n",
      "        [0.4005]])\n",
      "Iteration 400 Training loss 0.12109564244747162 Validation loss 0.11982512474060059 Accuracy 0.5868333578109741\n",
      "Output tensor([[0.3871],\n",
      "        [0.6506]])\n",
      "Iteration 410 Training loss 0.12204508483409882 Validation loss 0.11979691684246063 Accuracy 0.5879999995231628\n",
      "Output tensor([[0.4478],\n",
      "        [0.5905]])\n",
      "Iteration 420 Training loss 0.12109597772359848 Validation loss 0.11979003995656967 Accuracy 0.5863333344459534\n",
      "Output tensor([[0.2918],\n",
      "        [0.5567]])\n",
      "Iteration 430 Training loss 0.12089782953262329 Validation loss 0.11976384371519089 Accuracy 0.5885000228881836\n",
      "Output tensor([[0.4490],\n",
      "        [0.6443]])\n",
      "Iteration 440 Training loss 0.12145879864692688 Validation loss 0.12039414793252945 Accuracy 0.581333339214325\n",
      "Output tensor([[0.4985],\n",
      "        [0.5868]])\n",
      "Iteration 450 Training loss 0.12077183276414871 Validation loss 0.11983202397823334 Accuracy 0.5856666564941406\n",
      "Output tensor([[0.5857],\n",
      "        [0.5966]])\n",
      "Iteration 460 Training loss 0.12166484445333481 Validation loss 0.11970897763967514 Accuracy 0.5868333578109741\n",
      "Output tensor([[0.3731],\n",
      "        [0.5632]])\n",
      "Iteration 470 Training loss 0.12097442895174026 Validation loss 0.11967123299837112 Accuracy 0.5878333449363708\n",
      "Output tensor([[0.5122],\n",
      "        [0.6424]])\n",
      "Iteration 480 Training loss 0.12129513919353485 Validation loss 0.11968891322612762 Accuracy 0.5878333449363708\n",
      "Output tensor([[0.6160],\n",
      "        [0.3261]])\n",
      "Iteration 490 Training loss 0.1221330389380455 Validation loss 0.11965900659561157 Accuracy 0.5903333425521851\n",
      "Output tensor([[0.4463],\n",
      "        [0.2521]])\n",
      "Iteration 500 Training loss 0.12168577313423157 Validation loss 0.11961960792541504 Accuracy 0.5876666903495789\n",
      "Output tensor([[0.3504],\n",
      "        [0.4865]])\n",
      "Iteration 510 Training loss 0.12066766619682312 Validation loss 0.11966705322265625 Accuracy 0.5879999995231628\n",
      "Output tensor([[0.3605],\n",
      "        [0.4406]])\n",
      "Iteration 520 Training loss 0.12092426419258118 Validation loss 0.11957427859306335 Accuracy 0.5878333449363708\n",
      "Output tensor([[0.6955],\n",
      "        [0.5726]])\n",
      "Iteration 530 Training loss 0.12096524983644485 Validation loss 0.11957851052284241 Accuracy 0.5893333554267883\n",
      "Output tensor([[0.5206],\n",
      "        [0.5679]])\n",
      "Iteration 540 Training loss 0.12051115185022354 Validation loss 0.119595967233181 Accuracy 0.590833306312561\n",
      "Output tensor([[0.4395],\n",
      "        [0.4910]])\n",
      "Iteration 550 Training loss 0.12055505067110062 Validation loss 0.1195584088563919 Accuracy 0.5893333554267883\n",
      "Output tensor([[0.4665],\n",
      "        [0.4356]])\n",
      "Iteration 560 Training loss 0.12115426361560822 Validation loss 0.1196102723479271 Accuracy 0.5891666412353516\n",
      "Output tensor([[0.5188],\n",
      "        [0.4128]])\n",
      "Iteration 570 Training loss 0.12037772685289383 Validation loss 0.1195884719491005 Accuracy 0.5898333191871643\n",
      "Output tensor([[0.4002],\n",
      "        [0.4625]])\n",
      "Iteration 580 Training loss 0.12054035812616348 Validation loss 0.11950463801622391 Accuracy 0.5891666412353516\n",
      "Output tensor([[0.4620],\n",
      "        [0.5163]])\n",
      "Iteration 590 Training loss 0.12127897143363953 Validation loss 0.11951374262571335 Accuracy 0.5889999866485596\n",
      "Output tensor([[0.6348],\n",
      "        [0.4854]])\n",
      "Iteration 600 Training loss 0.12143176048994064 Validation loss 0.11948350071907043 Accuracy 0.5881666541099548\n",
      "Output tensor([[0.3920],\n",
      "        [0.5479]])\n",
      "Iteration 610 Training loss 0.12038794159889221 Validation loss 0.11961296945810318 Accuracy 0.5911666750907898\n",
      "Output tensor([[0.5663],\n",
      "        [0.3346]])\n",
      "Iteration 620 Training loss 0.12030493468046188 Validation loss 0.11952311545610428 Accuracy 0.5891666412353516\n",
      "Output tensor([[0.5914],\n",
      "        [0.5938]])\n",
      "Iteration 630 Training loss 0.12130860239267349 Validation loss 0.11948779225349426 Accuracy 0.5931666493415833\n",
      "Output tensor([[0.5225],\n",
      "        [0.4352]])\n",
      "Iteration 640 Training loss 0.1203000470995903 Validation loss 0.11945127695798874 Accuracy 0.593500018119812\n",
      "Output tensor([[0.4627],\n",
      "        [0.3964]])\n",
      "Iteration 650 Training loss 0.12066144496202469 Validation loss 0.1195216029882431 Accuracy 0.5910000205039978\n",
      "Output tensor([[0.5812],\n",
      "        [0.5057]])\n",
      "Iteration 660 Training loss 0.12073054164648056 Validation loss 0.11944223940372467 Accuracy 0.5924999713897705\n",
      "Output tensor([[0.5095],\n",
      "        [0.5019]])\n",
      "Iteration 670 Training loss 0.12114199995994568 Validation loss 0.11941380798816681 Accuracy 0.5910000205039978\n",
      "Output tensor([[0.5258],\n",
      "        [0.3719]])\n",
      "Iteration 680 Training loss 0.12077370285987854 Validation loss 0.11938921362161636 Accuracy 0.5893333554267883\n",
      "Output tensor([[0.5018],\n",
      "        [0.4728]])\n",
      "Iteration 690 Training loss 0.12079976499080658 Validation loss 0.11951275914907455 Accuracy 0.5921666622161865\n",
      "Output tensor([[0.5552],\n",
      "        [0.5386]])\n",
      "Iteration 700 Training loss 0.12213785946369171 Validation loss 0.11939331889152527 Accuracy 0.5920000076293945\n",
      "Output tensor([[0.5457],\n",
      "        [0.5549]])\n",
      "Iteration 710 Training loss 0.12003134936094284 Validation loss 0.11942124366760254 Accuracy 0.5910000205039978\n",
      "Output tensor([[0.3935],\n",
      "        [0.4437]])\n",
      "Iteration 720 Training loss 0.12007883936166763 Validation loss 0.11934171617031097 Accuracy 0.5903333425521851\n",
      "Output tensor([[0.5826],\n",
      "        [0.3828]])\n",
      "Iteration 730 Training loss 0.12054314464330673 Validation loss 0.11939734220504761 Accuracy 0.5913333296775818\n",
      "Output tensor([[0.5596],\n",
      "        [0.5677]])\n",
      "Iteration 740 Training loss 0.1207270622253418 Validation loss 0.11931376904249191 Accuracy 0.5893333554267883\n",
      "Output tensor([[0.3259],\n",
      "        [0.5014]])\n",
      "Iteration 750 Training loss 0.12054242193698883 Validation loss 0.11936725676059723 Accuracy 0.5923333168029785\n",
      "Output tensor([[0.5278],\n",
      "        [0.6088]])\n",
      "Iteration 760 Training loss 0.12084192037582397 Validation loss 0.11927063018083572 Accuracy 0.5926666855812073\n",
      "Output tensor([[0.3557],\n",
      "        [0.3092]])\n",
      "Iteration 770 Training loss 0.1203865334391594 Validation loss 0.11925557255744934 Accuracy 0.5931666493415833\n",
      "Output tensor([[0.5313],\n",
      "        [0.4926]])\n",
      "Iteration 780 Training loss 0.12041835486888885 Validation loss 0.11930527538061142 Accuracy 0.5918333530426025\n",
      "Output tensor([[0.3352],\n",
      "        [0.4446]])\n",
      "Iteration 790 Training loss 0.12089349329471588 Validation loss 0.11929000169038773 Accuracy 0.5946666598320007\n",
      "Output tensor([[0.5500],\n",
      "        [0.4803]])\n",
      "Iteration 800 Training loss 0.12092145532369614 Validation loss 0.11926477402448654 Accuracy 0.5913333296775818\n",
      "Output tensor([[0.7380],\n",
      "        [0.5759]])\n",
      "Iteration 810 Training loss 0.12081636488437653 Validation loss 0.11923225969076157 Accuracy 0.5926666855812073\n",
      "Output tensor([[0.4371],\n",
      "        [0.5137]])\n",
      "Iteration 820 Training loss 0.12074613571166992 Validation loss 0.11925742775201797 Accuracy 0.5948333144187927\n",
      "Output tensor([[0.3834],\n",
      "        [0.4765]])\n",
      "Iteration 830 Training loss 0.1210370659828186 Validation loss 0.11920023709535599 Accuracy 0.5928333401679993\n",
      "Output tensor([[0.5118],\n",
      "        [0.4505]])\n",
      "Iteration 840 Training loss 0.1211300939321518 Validation loss 0.11919213831424713 Accuracy 0.5926666855812073\n",
      "Output tensor([[0.2977],\n",
      "        [0.5021]])\n",
      "Iteration 850 Training loss 0.12127409130334854 Validation loss 0.1191985160112381 Accuracy 0.5946666598320007\n",
      "Output tensor([[0.5118],\n",
      "        [0.4263]])\n",
      "Iteration 860 Training loss 0.12104399502277374 Validation loss 0.11918620020151138 Accuracy 0.5931666493415833\n",
      "Output tensor([[0.5391],\n",
      "        [0.5861]])\n",
      "Iteration 870 Training loss 0.12080127000808716 Validation loss 0.11929941922426224 Accuracy 0.5921666622161865\n",
      "Output tensor([[0.5769],\n",
      "        [0.3225]])\n",
      "Iteration 880 Training loss 0.12150910496711731 Validation loss 0.1191345602273941 Accuracy 0.593833327293396\n",
      "Output tensor([[0.3194],\n",
      "        [0.5775]])\n",
      "Iteration 890 Training loss 0.12090235203504562 Validation loss 0.1191498190164566 Accuracy 0.5941666960716248\n",
      "Output tensor([[0.5210],\n",
      "        [0.3342]])\n",
      "Iteration 900 Training loss 0.12124773859977722 Validation loss 0.11917542666196823 Accuracy 0.5926666855812073\n",
      "Output tensor([[0.4561],\n",
      "        [0.4474]])\n",
      "Iteration 910 Training loss 0.12052599340677261 Validation loss 0.11910387128591537 Accuracy 0.5948333144187927\n",
      "Output tensor([[0.4200],\n",
      "        [0.4841]])\n",
      "Iteration 920 Training loss 0.12102049589157104 Validation loss 0.1191331073641777 Accuracy 0.5943333506584167\n",
      "Output tensor([[0.5063],\n",
      "        [0.6287]])\n",
      "Iteration 930 Training loss 0.12061513215303421 Validation loss 0.11916407197713852 Accuracy 0.5933333039283752\n",
      "Output tensor([[0.5034],\n",
      "        [0.6395]])\n",
      "Iteration 940 Training loss 0.12015832215547562 Validation loss 0.11909708380699158 Accuracy 0.5950000286102295\n",
      "Output tensor([[0.4158],\n",
      "        [0.4870]])\n",
      "Iteration 950 Training loss 0.1206965446472168 Validation loss 0.11918553709983826 Accuracy 0.5931666493415833\n",
      "Output tensor([[0.5437],\n",
      "        [0.5391]])\n",
      "Iteration 960 Training loss 0.12035784125328064 Validation loss 0.11914840340614319 Accuracy 0.593666672706604\n",
      "Output tensor([[0.4817],\n",
      "        [0.5400]])\n",
      "Iteration 970 Training loss 0.12085704505443573 Validation loss 0.11910329014062881 Accuracy 0.593999981880188\n",
      "Output tensor([[0.4328],\n",
      "        [0.3751]])\n",
      "Iteration 980 Training loss 0.11978422105312347 Validation loss 0.11907695233821869 Accuracy 0.5956666469573975\n",
      "Output tensor([[0.4305],\n",
      "        [0.4917]])\n",
      "Iteration 990 Training loss 0.12025135010480881 Validation loss 0.11916697025299072 Accuracy 0.5933333039283752\n",
      "Output tensor([[0.4942],\n",
      "        [0.5983]])\n",
      "Iteration 1000 Training loss 0.12036625295877457 Validation loss 0.11918281763792038 Accuracy 0.5945000052452087\n",
      "Output tensor([[0.2835],\n",
      "        [0.4954]])\n",
      "Iteration 1010 Training loss 0.1195739358663559 Validation loss 0.11906736344099045 Accuracy 0.5963333249092102\n",
      "Output tensor([[0.5395],\n",
      "        [0.6126]])\n",
      "Iteration 1020 Training loss 0.12092003226280212 Validation loss 0.11913099884986877 Accuracy 0.593833327293396\n",
      "Output tensor([[0.5777],\n",
      "        [0.6038]])\n",
      "Iteration 1030 Training loss 0.12128857523202896 Validation loss 0.11907168477773666 Accuracy 0.5933333039283752\n",
      "Output tensor([[0.5138],\n",
      "        [0.4496]])\n",
      "Iteration 1040 Training loss 0.12072168290615082 Validation loss 0.11916164308786392 Accuracy 0.5910000205039978\n",
      "Output tensor([[0.5815],\n",
      "        [0.4410]])\n",
      "Iteration 1050 Training loss 0.12055632472038269 Validation loss 0.11897773295640945 Accuracy 0.5975000262260437\n",
      "Output tensor([[0.4576],\n",
      "        [0.4514]])\n",
      "Iteration 1060 Training loss 0.12096446007490158 Validation loss 0.11906791478395462 Accuracy 0.5951666831970215\n",
      "Output tensor([[0.5021],\n",
      "        [0.4595]])\n",
      "Iteration 1070 Training loss 0.12085653841495514 Validation loss 0.11901117116212845 Accuracy 0.5963333249092102\n",
      "Output tensor([[0.5159],\n",
      "        [0.5222]])\n",
      "Iteration 1080 Training loss 0.12098423391580582 Validation loss 0.1189892590045929 Accuracy 0.5951666831970215\n",
      "Output tensor([[0.6117],\n",
      "        [0.6575]])\n",
      "Iteration 1090 Training loss 0.12013455480337143 Validation loss 0.1189969852566719 Accuracy 0.5963333249092102\n",
      "Output tensor([[0.4361],\n",
      "        [0.4116]])\n",
      "Iteration 1100 Training loss 0.11981385946273804 Validation loss 0.11903572827577591 Accuracy 0.5948333144187927\n",
      "Output tensor([[0.4626],\n",
      "        [0.2848]])\n",
      "Iteration 1110 Training loss 0.12006193399429321 Validation loss 0.11902690678834915 Accuracy 0.5963333249092102\n",
      "Output tensor([[0.4720],\n",
      "        [0.4322]])\n",
      "Iteration 1120 Training loss 0.12028615176677704 Validation loss 0.11905895173549652 Accuracy 0.593666672706604\n",
      "Output tensor([[0.5283],\n",
      "        [0.6405]])\n",
      "Iteration 1130 Training loss 0.1198146790266037 Validation loss 0.11896982789039612 Accuracy 0.5948333144187927\n",
      "Output tensor([[0.2550],\n",
      "        [0.4237]])\n",
      "Iteration 1140 Training loss 0.1204892173409462 Validation loss 0.11895257234573364 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.4903],\n",
      "        [0.5835]])\n",
      "Iteration 1150 Training loss 0.12095678597688675 Validation loss 0.11909691244363785 Accuracy 0.5916666388511658\n",
      "Output tensor([[0.5706],\n",
      "        [0.5468]])\n",
      "Iteration 1160 Training loss 0.12049507349729538 Validation loss 0.11894002556800842 Accuracy 0.5960000157356262\n",
      "Output tensor([[0.5410],\n",
      "        [0.3617]])\n",
      "Iteration 1170 Training loss 0.12047410011291504 Validation loss 0.11891648173332214 Accuracy 0.596666693687439\n",
      "Output tensor([[0.4149],\n",
      "        [0.4343]])\n",
      "Iteration 1180 Training loss 0.12079274654388428 Validation loss 0.11892455071210861 Accuracy 0.5953333377838135\n",
      "Output tensor([[0.5832],\n",
      "        [0.2880]])\n",
      "Iteration 1190 Training loss 0.12036789208650589 Validation loss 0.1190502941608429 Accuracy 0.5951666831970215\n",
      "Output tensor([[0.5719],\n",
      "        [0.5650]])\n",
      "Iteration 1200 Training loss 0.120084248483181 Validation loss 0.11894582957029343 Accuracy 0.596833348274231\n",
      "Output tensor([[0.4167],\n",
      "        [0.4371]])\n",
      "Iteration 1210 Training loss 0.12006299197673798 Validation loss 0.11891797930002213 Accuracy 0.5954999923706055\n",
      "Output tensor([[0.3897],\n",
      "        [0.5622]])\n",
      "Iteration 1220 Training loss 0.120354063808918 Validation loss 0.11896897107362747 Accuracy 0.597000002861023\n",
      "Output tensor([[0.2284],\n",
      "        [0.4055]])\n",
      "Iteration 1230 Training loss 0.12077847123146057 Validation loss 0.1189003735780716 Accuracy 0.5956666469573975\n",
      "Output tensor([[0.7022],\n",
      "        [0.6061]])\n",
      "Iteration 1240 Training loss 0.11983463913202286 Validation loss 0.11890171468257904 Accuracy 0.5946666598320007\n",
      "Output tensor([[0.4288],\n",
      "        [0.4417]])\n",
      "Iteration 1250 Training loss 0.11960799247026443 Validation loss 0.1189035028219223 Accuracy 0.5963333249092102\n",
      "Output tensor([[0.3591],\n",
      "        [0.4844]])\n",
      "Iteration 1260 Training loss 0.11965034157037735 Validation loss 0.11890840530395508 Accuracy 0.5954999923706055\n",
      "Output tensor([[0.5753],\n",
      "        [0.6451]])\n",
      "Iteration 1270 Training loss 0.12074336409568787 Validation loss 0.11900463700294495 Accuracy 0.5941666960716248\n",
      "Output tensor([[0.4627],\n",
      "        [0.6669]])\n",
      "Iteration 1280 Training loss 0.12041909992694855 Validation loss 0.11891261488199234 Accuracy 0.596666693687439\n",
      "Output tensor([[0.2980],\n",
      "        [0.1870]])\n",
      "Iteration 1290 Training loss 0.1202106699347496 Validation loss 0.11889626830816269 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.4473],\n",
      "        [0.4452]])\n",
      "Iteration 1300 Training loss 0.11971437931060791 Validation loss 0.11884882301092148 Accuracy 0.5956666469573975\n",
      "Output tensor([[0.4158],\n",
      "        [0.5203]])\n",
      "Iteration 1310 Training loss 0.12010090798139572 Validation loss 0.11895398795604706 Accuracy 0.5953333377838135\n",
      "Output tensor([[0.5227],\n",
      "        [0.6646]])\n",
      "Iteration 1320 Training loss 0.11960692703723907 Validation loss 0.11884739249944687 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.5879],\n",
      "        [0.4941]])\n",
      "Iteration 1330 Training loss 0.12008780986070633 Validation loss 0.1188347339630127 Accuracy 0.5950000286102295\n",
      "Output tensor([[0.5651],\n",
      "        [0.3503]])\n",
      "Iteration 1340 Training loss 0.12081195414066315 Validation loss 0.11885911226272583 Accuracy 0.5963333249092102\n",
      "Output tensor([[0.5163],\n",
      "        [0.4568]])\n",
      "Iteration 1350 Training loss 0.12088795751333237 Validation loss 0.11882366985082626 Accuracy 0.5956666469573975\n",
      "Output tensor([[0.5172],\n",
      "        [0.6295]])\n",
      "Iteration 1360 Training loss 0.1202881708741188 Validation loss 0.11880836635828018 Accuracy 0.5946666598320007\n",
      "Output tensor([[0.5765],\n",
      "        [0.4872]])\n",
      "Iteration 1370 Training loss 0.12010642141103745 Validation loss 0.11880990862846375 Accuracy 0.5961666703224182\n",
      "Output tensor([[0.4577],\n",
      "        [0.3589]])\n",
      "Iteration 1380 Training loss 0.12029492110013962 Validation loss 0.11880041658878326 Accuracy 0.5958333611488342\n",
      "Output tensor([[0.5198],\n",
      "        [0.5552]])\n",
      "Iteration 1390 Training loss 0.11999603360891342 Validation loss 0.11883211135864258 Accuracy 0.596833348274231\n",
      "Output tensor([[0.4313],\n",
      "        [0.5969]])\n",
      "Iteration 1400 Training loss 0.11965784430503845 Validation loss 0.1188783198595047 Accuracy 0.5953333377838135\n",
      "Output tensor([[0.4431],\n",
      "        [0.5771]])\n",
      "Iteration 1410 Training loss 0.12065834552049637 Validation loss 0.11879566311836243 Accuracy 0.5950000286102295\n",
      "Output tensor([[0.4332],\n",
      "        [0.6097]])\n",
      "Iteration 1420 Training loss 0.12024051696062088 Validation loss 0.11879980564117432 Accuracy 0.5950000286102295\n",
      "Output tensor([[0.5454],\n",
      "        [0.3640]])\n",
      "Iteration 1430 Training loss 0.11978591233491898 Validation loss 0.11878158897161484 Accuracy 0.5958333611488342\n",
      "Output tensor([[0.5039],\n",
      "        [0.6351]])\n",
      "Iteration 1440 Training loss 0.12041477113962173 Validation loss 0.11894479393959045 Accuracy 0.5926666855812073\n",
      "Output tensor([[0.4939],\n",
      "        [0.5476]])\n",
      "Iteration 1450 Training loss 0.12119173258543015 Validation loss 0.11899235844612122 Accuracy 0.5921666622161865\n",
      "Output tensor([[0.4314],\n",
      "        [0.4095]])\n",
      "Iteration 1460 Training loss 0.12043143808841705 Validation loss 0.11884251981973648 Accuracy 0.5958333611488342\n",
      "Output tensor([[0.4699],\n",
      "        [0.4494]])\n",
      "Iteration 1470 Training loss 0.1209726557135582 Validation loss 0.11878285557031631 Accuracy 0.5975000262260437\n",
      "Output tensor([[0.5129],\n",
      "        [0.5146]])\n",
      "Iteration 1480 Training loss 0.12007753551006317 Validation loss 0.11881852149963379 Accuracy 0.5953333377838135\n",
      "Output tensor([[0.4332],\n",
      "        [0.2918]])\n",
      "Iteration 1490 Training loss 0.11916932463645935 Validation loss 0.11882681399583817 Accuracy 0.5954999923706055\n",
      "Output tensor([[0.5986],\n",
      "        [0.5937]])\n",
      "Iteration 1500 Training loss 0.11999025195837021 Validation loss 0.11880231648683548 Accuracy 0.5963333249092102\n",
      "Output tensor([[0.4602],\n",
      "        [0.4386]])\n",
      "Iteration 1510 Training loss 0.11942689120769501 Validation loss 0.11888443678617477 Accuracy 0.5948333144187927\n",
      "Output tensor([[0.5046],\n",
      "        [0.6023]])\n",
      "Iteration 1520 Training loss 0.12024489045143127 Validation loss 0.1187594085931778 Accuracy 0.5963333249092102\n",
      "Output tensor([[0.4098],\n",
      "        [0.6522]])\n",
      "Iteration 1530 Training loss 0.11913492530584335 Validation loss 0.11889878660440445 Accuracy 0.5921666622161865\n",
      "Output tensor([[0.5461],\n",
      "        [0.5871]])\n",
      "Iteration 1540 Training loss 0.12024806439876556 Validation loss 0.1187467947602272 Accuracy 0.597000002861023\n",
      "Output tensor([[0.4536],\n",
      "        [0.4107]])\n",
      "Iteration 1550 Training loss 0.1198335587978363 Validation loss 0.11875677853822708 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.5486],\n",
      "        [0.5469]])\n",
      "Iteration 1560 Training loss 0.12048837542533875 Validation loss 0.11874916404485703 Accuracy 0.5958333611488342\n",
      "Output tensor([[0.4451],\n",
      "        [0.5378]])\n",
      "Iteration 1570 Training loss 0.12046989053487778 Validation loss 0.11871561408042908 Accuracy 0.5978333353996277\n",
      "Output tensor([[0.5140],\n",
      "        [0.2825]])\n",
      "Iteration 1580 Training loss 0.12023129314184189 Validation loss 0.11880501359701157 Accuracy 0.5973333120346069\n",
      "Output tensor([[0.5474],\n",
      "        [0.5150]])\n",
      "Iteration 1590 Training loss 0.12009875476360321 Validation loss 0.11868903785943985 Accuracy 0.5973333120346069\n",
      "Output tensor([[0.5936],\n",
      "        [0.3306]])\n",
      "Iteration 1600 Training loss 0.12056732922792435 Validation loss 0.11886279284954071 Accuracy 0.5918333530426025\n",
      "Output tensor([[0.4232],\n",
      "        [0.5483]])\n",
      "Iteration 1610 Training loss 0.11987822502851486 Validation loss 0.11868155747652054 Accuracy 0.5971666574478149\n",
      "Output tensor([[0.2597],\n",
      "        [0.4792]])\n",
      "Iteration 1620 Training loss 0.1195296049118042 Validation loss 0.11868835240602493 Accuracy 0.5963333249092102\n",
      "Output tensor([[0.5130],\n",
      "        [0.5220]])\n",
      "Iteration 1630 Training loss 0.12053424119949341 Validation loss 0.1187799796462059 Accuracy 0.5953333377838135\n",
      "Output tensor([[0.5200],\n",
      "        [0.3930]])\n",
      "Iteration 1640 Training loss 0.12028001248836517 Validation loss 0.11869846284389496 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.4892],\n",
      "        [0.4800]])\n",
      "Iteration 1650 Training loss 0.11961362510919571 Validation loss 0.11870311200618744 Accuracy 0.5958333611488342\n",
      "Output tensor([[0.5676],\n",
      "        [0.4339]])\n",
      "Iteration 1660 Training loss 0.12047179043292999 Validation loss 0.11868654191493988 Accuracy 0.5975000262260437\n",
      "Output tensor([[0.5434],\n",
      "        [0.5357]])\n",
      "Iteration 1670 Training loss 0.11994299292564392 Validation loss 0.11864911764860153 Accuracy 0.5951666831970215\n",
      "Output tensor([[0.4948],\n",
      "        [0.2598]])\n",
      "Iteration 1680 Training loss 0.12067842483520508 Validation loss 0.11865876615047455 Accuracy 0.5960000157356262\n",
      "Output tensor([[0.5036],\n",
      "        [0.4412]])\n",
      "Iteration 1690 Training loss 0.11973916739225388 Validation loss 0.11863557249307632 Accuracy 0.5981666445732117\n",
      "Output tensor([[0.4040],\n",
      "        [0.5125]])\n",
      "Iteration 1700 Training loss 0.12062250822782516 Validation loss 0.11865206062793732 Accuracy 0.5961666703224182\n",
      "Output tensor([[0.4477],\n",
      "        [0.4043]])\n",
      "Iteration 1710 Training loss 0.11894667893648148 Validation loss 0.11867167055606842 Accuracy 0.5954999923706055\n",
      "Output tensor([[0.3228],\n",
      "        [0.2564]])\n",
      "Iteration 1720 Training loss 0.12005434185266495 Validation loss 0.1187049150466919 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.4073],\n",
      "        [0.4739]])\n",
      "Iteration 1730 Training loss 0.11961526423692703 Validation loss 0.11869767308235168 Accuracy 0.5983333587646484\n",
      "Output tensor([[0.3190],\n",
      "        [0.4284]])\n",
      "Iteration 1740 Training loss 0.12038455158472061 Validation loss 0.1186494529247284 Accuracy 0.596666693687439\n",
      "Output tensor([[0.2523],\n",
      "        [0.4506]])\n",
      "Iteration 1750 Training loss 0.12039031833410263 Validation loss 0.11863844096660614 Accuracy 0.5963333249092102\n",
      "Output tensor([[0.4880],\n",
      "        [0.5564]])\n",
      "Iteration 1760 Training loss 0.11979754269123077 Validation loss 0.1186746209859848 Accuracy 0.5953333377838135\n",
      "Output tensor([[0.4938],\n",
      "        [0.5226]])\n",
      "Iteration 1770 Training loss 0.11943302303552628 Validation loss 0.11861903965473175 Accuracy 0.5960000157356262\n",
      "Output tensor([[0.5296],\n",
      "        [0.5526]])\n",
      "Iteration 1780 Training loss 0.11936689913272858 Validation loss 0.11885552853345871 Accuracy 0.5903333425521851\n",
      "Output tensor([[0.4632],\n",
      "        [0.6250]])\n",
      "Iteration 1790 Training loss 0.11994191259145737 Validation loss 0.1187405064702034 Accuracy 0.5998333096504211\n",
      "Output tensor([[0.3284],\n",
      "        [0.6367]])\n",
      "Iteration 1800 Training loss 0.11999712884426117 Validation loss 0.11874273419380188 Accuracy 0.5991666913032532\n",
      "Output tensor([[0.5496],\n",
      "        [0.4830]])\n",
      "Iteration 1810 Training loss 0.11979082971811295 Validation loss 0.11863020062446594 Accuracy 0.5956666469573975\n",
      "Output tensor([[0.3399],\n",
      "        [0.2617]])\n",
      "Iteration 1820 Training loss 0.11950204521417618 Validation loss 0.11866562813520432 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.4008],\n",
      "        [0.5147]])\n",
      "Iteration 1830 Training loss 0.11902493238449097 Validation loss 0.11863210797309875 Accuracy 0.5960000157356262\n",
      "Output tensor([[0.4183],\n",
      "        [0.6162]])\n",
      "Iteration 1840 Training loss 0.12108494341373444 Validation loss 0.11860949546098709 Accuracy 0.5981666445732117\n",
      "Output tensor([[0.5482],\n",
      "        [0.3718]])\n",
      "Iteration 1850 Training loss 0.12000240385532379 Validation loss 0.11858390271663666 Accuracy 0.5960000157356262\n",
      "Output tensor([[0.5396],\n",
      "        [0.5825]])\n",
      "Iteration 1860 Training loss 0.11957893520593643 Validation loss 0.1186397522687912 Accuracy 0.5988333225250244\n",
      "Output tensor([[0.3953],\n",
      "        [0.5020]])\n",
      "Iteration 1870 Training loss 0.11949694901704788 Validation loss 0.11858545243740082 Accuracy 0.5961666703224182\n",
      "Output tensor([[0.3026],\n",
      "        [0.6170]])\n",
      "Iteration 1880 Training loss 0.12025036662817001 Validation loss 0.11859676241874695 Accuracy 0.5961666703224182\n",
      "Output tensor([[0.4874],\n",
      "        [0.5233]])\n",
      "Iteration 1890 Training loss 0.12043491750955582 Validation loss 0.11863616853952408 Accuracy 0.5956666469573975\n",
      "Output tensor([[0.3622],\n",
      "        [0.4766]])\n",
      "Iteration 1900 Training loss 0.12015397846698761 Validation loss 0.11857623606920242 Accuracy 0.5976666808128357\n",
      "Output tensor([[0.4357],\n",
      "        [0.5448]])\n",
      "Iteration 1910 Training loss 0.11968375742435455 Validation loss 0.11857254058122635 Accuracy 0.5958333611488342\n",
      "Output tensor([[0.2992],\n",
      "        [0.5706]])\n",
      "Iteration 1920 Training loss 0.11951407790184021 Validation loss 0.11864573508501053 Accuracy 0.5963333249092102\n",
      "Output tensor([[0.3672],\n",
      "        [0.3494]])\n",
      "Iteration 1930 Training loss 0.11961908638477325 Validation loss 0.11856991797685623 Accuracy 0.5981666445732117\n",
      "Output tensor([[0.4931],\n",
      "        [0.5031]])\n",
      "Iteration 1940 Training loss 0.11936799436807632 Validation loss 0.11856741458177567 Accuracy 0.5978333353996277\n",
      "Output tensor([[0.4831],\n",
      "        [0.3904]])\n",
      "Iteration 1950 Training loss 0.12004512548446655 Validation loss 0.11857445538043976 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.4168],\n",
      "        [0.2485]])\n",
      "Iteration 1960 Training loss 0.12026627361774445 Validation loss 0.11856681108474731 Accuracy 0.596666693687439\n",
      "Output tensor([[0.5468],\n",
      "        [0.4447]])\n",
      "Iteration 1970 Training loss 0.11969411373138428 Validation loss 0.11870268732309341 Accuracy 0.5941666960716248\n",
      "Output tensor([[0.4359],\n",
      "        [0.4978]])\n",
      "Iteration 1980 Training loss 0.11975307017564774 Validation loss 0.11855822801589966 Accuracy 0.5975000262260437\n",
      "Output tensor([[0.6682],\n",
      "        [0.5391]])\n",
      "Iteration 1990 Training loss 0.1198929026722908 Validation loss 0.11855360865592957 Accuracy 0.5961666703224182\n",
      "Output tensor([[0.4795],\n",
      "        [0.4437]])\n",
      "Iteration 2000 Training loss 0.11986048519611359 Validation loss 0.11858896166086197 Accuracy 0.5958333611488342\n",
      "Output tensor([[0.4120],\n",
      "        [0.4275]])\n",
      "Iteration 2010 Training loss 0.1192379742860794 Validation loss 0.11858601868152618 Accuracy 0.5950000286102295\n",
      "Output tensor([[0.4246],\n",
      "        [0.4777]])\n",
      "Iteration 2020 Training loss 0.12047936022281647 Validation loss 0.11875464767217636 Accuracy 0.593833327293396\n",
      "Output tensor([[0.6021],\n",
      "        [0.3461]])\n",
      "Iteration 2030 Training loss 0.12036765366792679 Validation loss 0.11858195066452026 Accuracy 0.596666693687439\n",
      "Output tensor([[0.5913],\n",
      "        [0.4899]])\n",
      "Iteration 2040 Training loss 0.11978393793106079 Validation loss 0.11858902126550674 Accuracy 0.5985000133514404\n",
      "Output tensor([[0.5810],\n",
      "        [0.4432]])\n",
      "Iteration 2050 Training loss 0.12058402597904205 Validation loss 0.11859335005283356 Accuracy 0.5983333587646484\n",
      "Output tensor([[0.4612],\n",
      "        [0.5825]])\n",
      "Iteration 2060 Training loss 0.11936047673225403 Validation loss 0.11858917027711868 Accuracy 0.596833348274231\n",
      "Output tensor([[0.4637],\n",
      "        [0.4437]])\n",
      "Iteration 2070 Training loss 0.11961110681295395 Validation loss 0.11862540990114212 Accuracy 0.5956666469573975\n",
      "Output tensor([[0.4886],\n",
      "        [0.5067]])\n",
      "Iteration 2080 Training loss 0.11986307799816132 Validation loss 0.11857468634843826 Accuracy 0.5979999899864197\n",
      "Output tensor([[0.5484],\n",
      "        [0.4636]])\n",
      "Iteration 2090 Training loss 0.11989585310220718 Validation loss 0.11860445886850357 Accuracy 0.5960000157356262\n",
      "Output tensor([[0.4871],\n",
      "        [0.6064]])\n",
      "Iteration 2100 Training loss 0.1208888366818428 Validation loss 0.11856156587600708 Accuracy 0.5960000157356262\n",
      "Output tensor([[0.5516],\n",
      "        [0.5560]])\n",
      "Iteration 2110 Training loss 0.12026558071374893 Validation loss 0.11849391460418701 Accuracy 0.5961666703224182\n",
      "Output tensor([[0.5693],\n",
      "        [0.4853]])\n",
      "Iteration 2120 Training loss 0.1193796768784523 Validation loss 0.11848703026771545 Accuracy 0.5976666808128357\n",
      "Output tensor([[0.5839],\n",
      "        [0.5366]])\n",
      "Iteration 2130 Training loss 0.12002084404230118 Validation loss 0.11855628341436386 Accuracy 0.5950000286102295\n",
      "Output tensor([[0.5106],\n",
      "        [0.5454]])\n",
      "Iteration 2140 Training loss 0.11981067061424255 Validation loss 0.1185397356748581 Accuracy 0.5963333249092102\n",
      "Output tensor([[0.5400],\n",
      "        [0.2717]])\n",
      "Iteration 2150 Training loss 0.11873719096183777 Validation loss 0.11851688474416733 Accuracy 0.5945000052452087\n",
      "Output tensor([[0.4668],\n",
      "        [0.6140]])\n",
      "Iteration 2160 Training loss 0.11966454982757568 Validation loss 0.11854658275842667 Accuracy 0.5948333144187927\n",
      "Output tensor([[0.6310],\n",
      "        [0.3687]])\n",
      "Iteration 2170 Training loss 0.12017730623483658 Validation loss 0.11851607263088226 Accuracy 0.5996666550636292\n",
      "Output tensor([[0.6328],\n",
      "        [0.4302]])\n",
      "Iteration 2180 Training loss 0.12034286558628082 Validation loss 0.11845480650663376 Accuracy 0.596666693687439\n",
      "Output tensor([[0.5600],\n",
      "        [0.6957]])\n",
      "Iteration 2190 Training loss 0.11984004825353622 Validation loss 0.1184515655040741 Accuracy 0.5976666808128357\n",
      "Output tensor([[0.5679],\n",
      "        [0.5249]])\n",
      "Iteration 2200 Training loss 0.12056829780340195 Validation loss 0.11848273873329163 Accuracy 0.5941666960716248\n",
      "Output tensor([[0.6633],\n",
      "        [0.5910]])\n",
      "Iteration 2210 Training loss 0.12054543197154999 Validation loss 0.1184476688504219 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.5525],\n",
      "        [0.4329]])\n",
      "Iteration 2220 Training loss 0.12000390887260437 Validation loss 0.11848366260528564 Accuracy 0.5993333458900452\n",
      "Output tensor([[0.5131],\n",
      "        [0.6068]])\n",
      "Iteration 2230 Training loss 0.12004335224628448 Validation loss 0.11843780428171158 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.4849],\n",
      "        [0.4173]])\n",
      "Iteration 2240 Training loss 0.1192961037158966 Validation loss 0.11844304203987122 Accuracy 0.596666693687439\n",
      "Output tensor([[0.5765],\n",
      "        [0.4979]])\n",
      "Iteration 2250 Training loss 0.11981384456157684 Validation loss 0.11843610554933548 Accuracy 0.5981666445732117\n",
      "Output tensor([[0.2687],\n",
      "        [0.5253]])\n",
      "Iteration 2260 Training loss 0.11955157667398453 Validation loss 0.11842677742242813 Accuracy 0.5976666808128357\n",
      "Output tensor([[0.4627],\n",
      "        [0.6092]])\n",
      "Iteration 2270 Training loss 0.119733527302742 Validation loss 0.11842750757932663 Accuracy 0.5961666703224182\n",
      "Output tensor([[0.4158],\n",
      "        [0.4037]])\n",
      "Iteration 2280 Training loss 0.11913757026195526 Validation loss 0.11846230179071426 Accuracy 0.5960000157356262\n",
      "Output tensor([[0.5681],\n",
      "        [0.4465]])\n",
      "Iteration 2290 Training loss 0.11961649358272552 Validation loss 0.11853155493736267 Accuracy 0.5953333377838135\n",
      "Output tensor([[0.5649],\n",
      "        [0.3284]])\n",
      "Iteration 2300 Training loss 0.12041373550891876 Validation loss 0.11845266819000244 Accuracy 0.597000002861023\n",
      "Output tensor([[0.4509],\n",
      "        [0.5625]])\n",
      "Iteration 2310 Training loss 0.11998724192380905 Validation loss 0.11844733357429504 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.4968],\n",
      "        [0.5069]])\n",
      "Iteration 2320 Training loss 0.11997849494218826 Validation loss 0.11844948679208755 Accuracy 0.5971666574478149\n",
      "Output tensor([[0.4515],\n",
      "        [0.4002]])\n",
      "Iteration 2330 Training loss 0.11996087431907654 Validation loss 0.11858610063791275 Accuracy 0.593833327293396\n",
      "Output tensor([[0.4549],\n",
      "        [0.4727]])\n",
      "Iteration 2340 Training loss 0.11894793808460236 Validation loss 0.11841817200183868 Accuracy 0.5973333120346069\n",
      "Output tensor([[0.5079],\n",
      "        [0.6547]])\n",
      "Iteration 2350 Training loss 0.11979582905769348 Validation loss 0.11843452602624893 Accuracy 0.5983333587646484\n",
      "Output tensor([[0.5617],\n",
      "        [0.5262]])\n",
      "Iteration 2360 Training loss 0.12002942711114883 Validation loss 0.11841230094432831 Accuracy 0.5981666445732117\n",
      "Output tensor([[0.4393],\n",
      "        [0.4086]])\n",
      "Iteration 2370 Training loss 0.11986670643091202 Validation loss 0.11844072490930557 Accuracy 0.5981666445732117\n",
      "Output tensor([[0.5093],\n",
      "        [0.5291]])\n",
      "Iteration 2380 Training loss 0.11863357573747635 Validation loss 0.11848610639572144 Accuracy 0.5945000052452087\n",
      "Output tensor([[0.5765],\n",
      "        [0.4554]])\n",
      "Iteration 2390 Training loss 0.11957833915948868 Validation loss 0.11849875748157501 Accuracy 0.593500018119812\n",
      "Output tensor([[0.6101],\n",
      "        [0.4394]])\n",
      "Iteration 2400 Training loss 0.11959825456142426 Validation loss 0.11847669631242752 Accuracy 0.5971666574478149\n",
      "Output tensor([[0.6239],\n",
      "        [0.6051]])\n",
      "Iteration 2410 Training loss 0.11886903643608093 Validation loss 0.11856675893068314 Accuracy 0.5941666960716248\n",
      "Output tensor([[0.4461],\n",
      "        [0.5249]])\n",
      "Iteration 2420 Training loss 0.11931860446929932 Validation loss 0.11846079677343369 Accuracy 0.5988333225250244\n",
      "Output tensor([[0.2212],\n",
      "        [0.5057]])\n",
      "Iteration 2430 Training loss 0.11987142264842987 Validation loss 0.11853620409965515 Accuracy 0.5948333144187927\n",
      "Output tensor([[0.3054],\n",
      "        [0.5667]])\n",
      "Iteration 2440 Training loss 0.1199062392115593 Validation loss 0.11844092607498169 Accuracy 0.5985000133514404\n",
      "Output tensor([[0.5414],\n",
      "        [0.6034]])\n",
      "Iteration 2450 Training loss 0.12069317698478699 Validation loss 0.11843601614236832 Accuracy 0.5971666574478149\n",
      "Output tensor([[0.4169],\n",
      "        [0.4510]])\n",
      "Iteration 2460 Training loss 0.11921262741088867 Validation loss 0.11838503181934357 Accuracy 0.5993333458900452\n",
      "Output tensor([[0.5567],\n",
      "        [0.4640]])\n",
      "Iteration 2470 Training loss 0.11925150454044342 Validation loss 0.11837874352931976 Accuracy 0.5981666445732117\n",
      "Output tensor([[0.6374],\n",
      "        [0.4670]])\n",
      "Iteration 2480 Training loss 0.11986193060874939 Validation loss 0.11848074197769165 Accuracy 0.5943333506584167\n",
      "Output tensor([[0.2400],\n",
      "        [0.6174]])\n",
      "Iteration 2490 Training loss 0.11998704820871353 Validation loss 0.11840104311704636 Accuracy 0.5979999899864197\n",
      "Output tensor([[0.5431],\n",
      "        [0.5252]])\n",
      "Iteration 2500 Training loss 0.11939758062362671 Validation loss 0.11841332912445068 Accuracy 0.5978333353996277\n",
      "Output tensor([[0.4664],\n",
      "        [0.6460]])\n",
      "Iteration 2510 Training loss 0.11922287940979004 Validation loss 0.11838775873184204 Accuracy 0.5971666574478149\n",
      "Output tensor([[0.6230],\n",
      "        [0.5375]])\n",
      "Iteration 2520 Training loss 0.11875948309898376 Validation loss 0.11838489025831223 Accuracy 0.5981666445732117\n",
      "Output tensor([[0.4701],\n",
      "        [0.5432]])\n",
      "Iteration 2530 Training loss 0.11944686621427536 Validation loss 0.11838249862194061 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.3088],\n",
      "        [0.4992]])\n",
      "Iteration 2540 Training loss 0.11927823722362518 Validation loss 0.1184309870004654 Accuracy 0.5946666598320007\n",
      "Output tensor([[0.5894],\n",
      "        [0.4694]])\n",
      "Iteration 2550 Training loss 0.11969063431024551 Validation loss 0.1184135153889656 Accuracy 0.5976666808128357\n",
      "Output tensor([[0.6384],\n",
      "        [0.6205]])\n",
      "Iteration 2560 Training loss 0.11899997293949127 Validation loss 0.11842115223407745 Accuracy 0.5983333587646484\n",
      "Output tensor([[0.5226],\n",
      "        [0.4939]])\n",
      "Iteration 2570 Training loss 0.11976899951696396 Validation loss 0.11840711534023285 Accuracy 0.5998333096504211\n",
      "Output tensor([[0.3856],\n",
      "        [0.5715]])\n",
      "Iteration 2580 Training loss 0.11948040872812271 Validation loss 0.11864642053842545 Accuracy 0.5945000052452087\n",
      "Output tensor([[0.4538],\n",
      "        [0.5614]])\n",
      "Iteration 2590 Training loss 0.12031947821378708 Validation loss 0.11838383227586746 Accuracy 0.6004999876022339\n",
      "Output tensor([[0.4848],\n",
      "        [0.3333]])\n",
      "Iteration 2600 Training loss 0.1199093833565712 Validation loss 0.11836352199316025 Accuracy 0.596833348274231\n",
      "Output tensor([[0.5311],\n",
      "        [0.4067]])\n",
      "Iteration 2610 Training loss 0.11994148790836334 Validation loss 0.1183478981256485 Accuracy 0.5986666679382324\n",
      "Output tensor([[0.3558],\n",
      "        [0.3356]])\n",
      "Iteration 2620 Training loss 0.11901025474071503 Validation loss 0.11835646629333496 Accuracy 0.5973333120346069\n",
      "Output tensor([[0.4370],\n",
      "        [0.5089]])\n",
      "Iteration 2630 Training loss 0.1191394031047821 Validation loss 0.11836828291416168 Accuracy 0.5975000262260437\n",
      "Output tensor([[0.5996],\n",
      "        [0.2455]])\n",
      "Iteration 2640 Training loss 0.1191181018948555 Validation loss 0.11833693087100983 Accuracy 0.5978333353996277\n",
      "Output tensor([[0.3536],\n",
      "        [0.4044]])\n",
      "Iteration 2650 Training loss 0.11947987228631973 Validation loss 0.1183609589934349 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.4303],\n",
      "        [0.5905]])\n",
      "Iteration 2660 Training loss 0.12024231255054474 Validation loss 0.11838018149137497 Accuracy 0.596666693687439\n",
      "Output tensor([[0.5406],\n",
      "        [0.5657]])\n",
      "Iteration 2670 Training loss 0.12054643034934998 Validation loss 0.11836396157741547 Accuracy 0.5996666550636292\n",
      "Output tensor([[0.5898],\n",
      "        [0.5002]])\n",
      "Iteration 2680 Training loss 0.1193244457244873 Validation loss 0.11840800195932388 Accuracy 0.597000002861023\n",
      "Output tensor([[0.5405],\n",
      "        [0.6043]])\n",
      "Iteration 2690 Training loss 0.11934924125671387 Validation loss 0.11836766451597214 Accuracy 0.5975000262260437\n",
      "Output tensor([[0.5578],\n",
      "        [0.4626]])\n",
      "Iteration 2700 Training loss 0.12005362659692764 Validation loss 0.11835399270057678 Accuracy 0.5989999771118164\n",
      "Output tensor([[0.6045],\n",
      "        [0.5259]])\n",
      "Iteration 2710 Training loss 0.11933442950248718 Validation loss 0.11835253238677979 Accuracy 0.5979999899864197\n",
      "Output tensor([[0.3958],\n",
      "        [0.6317]])\n",
      "Iteration 2720 Training loss 0.11976568400859833 Validation loss 0.11834766715765 Accuracy 0.5978333353996277\n",
      "Output tensor([[0.6033],\n",
      "        [0.3369]])\n",
      "Iteration 2730 Training loss 0.11883052438497543 Validation loss 0.11832154542207718 Accuracy 0.5989999771118164\n",
      "Output tensor([[0.3265],\n",
      "        [0.4667]])\n",
      "Iteration 2740 Training loss 0.11973245441913605 Validation loss 0.11838221549987793 Accuracy 0.5971666574478149\n",
      "Output tensor([[0.5860],\n",
      "        [0.5656]])\n",
      "Iteration 2750 Training loss 0.11923638731241226 Validation loss 0.11831694841384888 Accuracy 0.5976666808128357\n",
      "Output tensor([[0.6460],\n",
      "        [0.3740]])\n",
      "Iteration 2760 Training loss 0.1206684559583664 Validation loss 0.1183324083685875 Accuracy 0.5981666445732117\n",
      "Output tensor([[0.4608],\n",
      "        [0.4668]])\n",
      "Iteration 2770 Training loss 0.11919775605201721 Validation loss 0.11836609989404678 Accuracy 0.5960000157356262\n",
      "Output tensor([[0.5155],\n",
      "        [0.5384]])\n",
      "Iteration 2780 Training loss 0.12041153758764267 Validation loss 0.11839400231838226 Accuracy 0.5946666598320007\n",
      "Output tensor([[0.4426],\n",
      "        [0.5506]])\n",
      "Iteration 2790 Training loss 0.11932818591594696 Validation loss 0.11829009652137756 Accuracy 0.5979999899864197\n",
      "Output tensor([[0.4538],\n",
      "        [0.3959]])\n",
      "Iteration 2800 Training loss 0.1195538118481636 Validation loss 0.11831790953874588 Accuracy 0.5976666808128357\n",
      "Output tensor([[0.3714],\n",
      "        [0.2594]])\n",
      "Iteration 2810 Training loss 0.12039490044116974 Validation loss 0.11850037425756454 Accuracy 0.5948333144187927\n",
      "Output tensor([[0.4783],\n",
      "        [0.4947]])\n",
      "Iteration 2820 Training loss 0.12002541124820709 Validation loss 0.11828753352165222 Accuracy 0.5985000133514404\n",
      "Output tensor([[0.5849],\n",
      "        [0.5633]])\n",
      "Iteration 2830 Training loss 0.1192527487874031 Validation loss 0.11830177158117294 Accuracy 0.5993333458900452\n",
      "Output tensor([[0.4161],\n",
      "        [0.5930]])\n",
      "Iteration 2840 Training loss 0.11944741755723953 Validation loss 0.11830634623765945 Accuracy 0.5985000133514404\n",
      "Output tensor([[0.3874],\n",
      "        [0.5735]])\n",
      "Iteration 2850 Training loss 0.11912716925144196 Validation loss 0.11832558363676071 Accuracy 0.5995000004768372\n",
      "Output tensor([[0.4820],\n",
      "        [0.3462]])\n",
      "Iteration 2860 Training loss 0.12003198266029358 Validation loss 0.11837676912546158 Accuracy 0.5988333225250244\n",
      "Output tensor([[0.6113],\n",
      "        [0.5687]])\n",
      "Iteration 2870 Training loss 0.11957268416881561 Validation loss 0.11830540001392365 Accuracy 0.5986666679382324\n",
      "Output tensor([[0.5856],\n",
      "        [0.3633]])\n",
      "Iteration 2880 Training loss 0.11894423514604568 Validation loss 0.1182841882109642 Accuracy 0.5993333458900452\n",
      "Output tensor([[0.4183],\n",
      "        [0.5018]])\n",
      "Iteration 2890 Training loss 0.11914901435375214 Validation loss 0.11826706677675247 Accuracy 0.5993333458900452\n",
      "Output tensor([[0.3911],\n",
      "        [0.4486]])\n",
      "Iteration 2900 Training loss 0.12016766518354416 Validation loss 0.11829209327697754 Accuracy 0.5993333458900452\n",
      "Output tensor([[0.4990],\n",
      "        [0.3076]])\n",
      "Iteration 2910 Training loss 0.11999958008527756 Validation loss 0.1182766854763031 Accuracy 0.6003333330154419\n",
      "Output tensor([[0.4001],\n",
      "        [0.5889]])\n",
      "Iteration 2920 Training loss 0.12036045640707016 Validation loss 0.1182761862874031 Accuracy 0.5996666550636292\n",
      "Output tensor([[0.3031],\n",
      "        [0.5187]])\n",
      "Iteration 2930 Training loss 0.1198224276304245 Validation loss 0.11843059957027435 Accuracy 0.596666693687439\n",
      "Output tensor([[0.4611],\n",
      "        [0.6130]])\n",
      "Iteration 2940 Training loss 0.1187385693192482 Validation loss 0.11827497184276581 Accuracy 0.5998333096504211\n",
      "Output tensor([[0.3886],\n",
      "        [0.2917]])\n",
      "Iteration 2950 Training loss 0.11992703378200531 Validation loss 0.11823058128356934 Accuracy 0.5991666913032532\n",
      "Output tensor([[0.4629],\n",
      "        [0.4398]])\n",
      "Iteration 2960 Training loss 0.11951605975627899 Validation loss 0.11823692172765732 Accuracy 0.6000000238418579\n",
      "Output tensor([[0.4018],\n",
      "        [0.5388]])\n",
      "Iteration 2970 Training loss 0.11972270905971527 Validation loss 0.11823197454214096 Accuracy 0.5996666550636292\n",
      "Output tensor([[0.4008],\n",
      "        [0.4276]])\n",
      "Iteration 2980 Training loss 0.1184288039803505 Validation loss 0.11827699095010757 Accuracy 0.5986666679382324\n",
      "Output tensor([[0.5972],\n",
      "        [0.6518]])\n",
      "Iteration 2990 Training loss 0.11924156546592712 Validation loss 0.11825499683618546 Accuracy 0.5991666913032532\n",
      "Output tensor([[0.6129],\n",
      "        [0.5795]])\n",
      "Iteration 3000 Training loss 0.11952079087495804 Validation loss 0.11824650317430496 Accuracy 0.5988333225250244\n",
      "Output tensor([[0.3644],\n",
      "        [0.5465]])\n",
      "Iteration 3010 Training loss 0.12005063891410828 Validation loss 0.11822975426912308 Accuracy 0.5998333096504211\n",
      "Output tensor([[0.4246],\n",
      "        [0.2867]])\n",
      "Iteration 3020 Training loss 0.11935380846261978 Validation loss 0.11823417246341705 Accuracy 0.5998333096504211\n",
      "Output tensor([[0.4618],\n",
      "        [0.3446]])\n",
      "Iteration 3030 Training loss 0.11951322108507156 Validation loss 0.11825951188802719 Accuracy 0.5989999771118164\n",
      "Output tensor([[0.4136],\n",
      "        [0.6594]])\n",
      "Iteration 3040 Training loss 0.11902746558189392 Validation loss 0.1182999238371849 Accuracy 0.5976666808128357\n",
      "Output tensor([[0.6834],\n",
      "        [0.5350]])\n",
      "Iteration 3050 Training loss 0.11936262995004654 Validation loss 0.11824675649404526 Accuracy 0.5995000004768372\n",
      "Output tensor([[0.4679],\n",
      "        [0.2275]])\n",
      "Iteration 3060 Training loss 0.1192808598279953 Validation loss 0.11822053790092468 Accuracy 0.5993333458900452\n",
      "Output tensor([[0.5462],\n",
      "        [0.3989]])\n",
      "Iteration 3070 Training loss 0.11925311386585236 Validation loss 0.11821144819259644 Accuracy 0.5993333458900452\n",
      "Output tensor([[0.5284],\n",
      "        [0.3812]])\n",
      "Iteration 3080 Training loss 0.1195763573050499 Validation loss 0.11820970475673676 Accuracy 0.6014999747276306\n",
      "Output tensor([[0.5217],\n",
      "        [0.5254]])\n",
      "Iteration 3090 Training loss 0.12004471570253372 Validation loss 0.1182534247636795 Accuracy 0.5988333225250244\n",
      "Output tensor([[0.3186],\n",
      "        [0.3955]])\n",
      "Iteration 3100 Training loss 0.1199067160487175 Validation loss 0.11820760369300842 Accuracy 0.5985000133514404\n",
      "Output tensor([[0.4887],\n",
      "        [0.4282]])\n",
      "Iteration 3110 Training loss 0.11915384232997894 Validation loss 0.11820007115602493 Accuracy 0.6001666784286499\n",
      "Output tensor([[0.3166],\n",
      "        [0.5041]])\n",
      "Iteration 3120 Training loss 0.1190904974937439 Validation loss 0.1182328388094902 Accuracy 0.5998333096504211\n",
      "Output tensor([[0.5803],\n",
      "        [0.5068]])\n",
      "Iteration 3130 Training loss 0.12013451009988785 Validation loss 0.11821045726537704 Accuracy 0.6008333563804626\n",
      "Output tensor([[0.2691],\n",
      "        [0.4183]])\n",
      "Iteration 3140 Training loss 0.11986823379993439 Validation loss 0.1181858703494072 Accuracy 0.5998333096504211\n",
      "Output tensor([[0.3714],\n",
      "        [0.4924]])\n",
      "Iteration 3150 Training loss 0.11923173815011978 Validation loss 0.11833178997039795 Accuracy 0.6008333563804626\n",
      "Output tensor([[0.3906],\n",
      "        [0.2635]])\n",
      "Iteration 3160 Training loss 0.1184060275554657 Validation loss 0.1181710809469223 Accuracy 0.6003333330154419\n",
      "Output tensor([[0.5541],\n",
      "        [0.4839]])\n",
      "Iteration 3170 Training loss 0.11931490898132324 Validation loss 0.11817115545272827 Accuracy 0.6023333072662354\n",
      "Output tensor([[0.3572],\n",
      "        [0.5213]])\n",
      "Iteration 3180 Training loss 0.11948662251234055 Validation loss 0.11820729821920395 Accuracy 0.5995000004768372\n",
      "Output tensor([[0.4024],\n",
      "        [0.5199]])\n",
      "Iteration 3190 Training loss 0.11871234327554703 Validation loss 0.11819004267454147 Accuracy 0.5993333458900452\n",
      "Output tensor([[0.5158],\n",
      "        [0.6124]])\n",
      "Iteration 3200 Training loss 0.12016160041093826 Validation loss 0.11832199990749359 Accuracy 0.5964999794960022\n",
      "Output tensor([[0.5351],\n",
      "        [0.3319]])\n",
      "Iteration 3210 Training loss 0.11903010308742523 Validation loss 0.11824976652860641 Accuracy 0.5975000262260437\n",
      "Output tensor([[0.6323],\n",
      "        [0.5770]])\n",
      "Iteration 3220 Training loss 0.11980170011520386 Validation loss 0.11819474399089813 Accuracy 0.6006666421890259\n",
      "Output tensor([[0.6329],\n",
      "        [0.2682]])\n",
      "Iteration 3230 Training loss 0.11954311281442642 Validation loss 0.11832492798566818 Accuracy 0.5963333249092102\n",
      "Output tensor([[0.6294],\n",
      "        [0.6431]])\n",
      "Iteration 3240 Training loss 0.11985225230455399 Validation loss 0.11817589402198792 Accuracy 0.6003333330154419\n",
      "Output tensor([[0.4700],\n",
      "        [0.3511]])\n",
      "Iteration 3250 Training loss 0.11947723478078842 Validation loss 0.11833329498767853 Accuracy 0.5971666574478149\n",
      "Output tensor([[0.5115],\n",
      "        [0.4073]])\n",
      "Iteration 3260 Training loss 0.11935313045978546 Validation loss 0.11817655712366104 Accuracy 0.5998333096504211\n",
      "Output tensor([[0.4919],\n",
      "        [0.4027]])\n",
      "Iteration 3270 Training loss 0.11996981501579285 Validation loss 0.11819753795862198 Accuracy 0.5988333225250244\n",
      "Output tensor([[0.5937],\n",
      "        [0.1200]])\n",
      "Iteration 3280 Training loss 0.12004939466714859 Validation loss 0.1181812733411789 Accuracy 0.5985000133514404\n",
      "Output tensor([[0.4596],\n",
      "        [0.4555]])\n",
      "Iteration 3290 Training loss 0.11924869567155838 Validation loss 0.11814246326684952 Accuracy 0.5991666913032532\n",
      "Output tensor([[0.5483],\n",
      "        [0.4595]])\n",
      "Iteration 3300 Training loss 0.1193007379770279 Validation loss 0.11812290549278259 Accuracy 0.6013333201408386\n",
      "Output tensor([[0.2760],\n",
      "        [0.5745]])\n",
      "Iteration 3310 Training loss 0.11948609352111816 Validation loss 0.11813586205244064 Accuracy 0.5986666679382324\n",
      "Output tensor([[0.3668],\n",
      "        [0.5489]])\n",
      "Iteration 3320 Training loss 0.12013474851846695 Validation loss 0.1181366816163063 Accuracy 0.6011666655540466\n",
      "Output tensor([[0.4817],\n",
      "        [0.5864]])\n",
      "Iteration 3330 Training loss 0.12010705471038818 Validation loss 0.11820628494024277 Accuracy 0.5985000133514404\n",
      "Output tensor([[0.4174],\n",
      "        [0.5739]])\n",
      "Iteration 3340 Training loss 0.11997286230325699 Validation loss 0.1181163415312767 Accuracy 0.6019999980926514\n",
      "Output tensor([[0.5747],\n",
      "        [0.5446]])\n",
      "Iteration 3350 Training loss 0.1199113056063652 Validation loss 0.11816874891519547 Accuracy 0.5996666550636292\n",
      "Output tensor([[0.4540],\n",
      "        [0.5396]])\n",
      "Iteration 3360 Training loss 0.12036912143230438 Validation loss 0.11812572181224823 Accuracy 0.6003333330154419\n",
      "Output tensor([[0.3732],\n",
      "        [0.5400]])\n",
      "Iteration 3370 Training loss 0.11965290457010269 Validation loss 0.11816772818565369 Accuracy 0.5989999771118164\n",
      "Output tensor([[0.4881],\n",
      "        [0.2662]])\n",
      "Iteration 3380 Training loss 0.11926383525133133 Validation loss 0.11817584931850433 Accuracy 0.5981666445732117\n",
      "Output tensor([[0.5204],\n",
      "        [0.5425]])\n",
      "Iteration 3390 Training loss 0.12010626494884491 Validation loss 0.11813023686408997 Accuracy 0.6006666421890259\n",
      "Output tensor([[0.3314],\n",
      "        [0.3393]])\n",
      "Iteration 3400 Training loss 0.1189025342464447 Validation loss 0.11810458451509476 Accuracy 0.6016666889190674\n",
      "Output tensor([[0.4267],\n",
      "        [0.4822]])\n",
      "Iteration 3410 Training loss 0.1188519299030304 Validation loss 0.1181625947356224 Accuracy 0.5986666679382324\n",
      "Output tensor([[0.5346],\n",
      "        [0.5244]])\n",
      "Iteration 3420 Training loss 0.11908093094825745 Validation loss 0.11831686645746231 Accuracy 0.5988333225250244\n",
      "Output tensor([[0.6097],\n",
      "        [0.5794]])\n",
      "Iteration 3430 Training loss 0.11935606598854065 Validation loss 0.11810574680566788 Accuracy 0.6008333563804626\n",
      "Output tensor([[0.5761],\n",
      "        [0.5320]])\n",
      "Iteration 3440 Training loss 0.11870057880878448 Validation loss 0.1181068941950798 Accuracy 0.6013333201408386\n",
      "Output tensor([[0.5789],\n",
      "        [0.6036]])\n",
      "Iteration 3450 Training loss 0.11909088492393494 Validation loss 0.11812780797481537 Accuracy 0.5991666913032532\n",
      "Output tensor([[0.5298],\n",
      "        [0.3803]])\n",
      "Iteration 3460 Training loss 0.1192760020494461 Validation loss 0.11814597249031067 Accuracy 0.6023333072662354\n",
      "Output tensor([[0.4556],\n",
      "        [0.5912]])\n",
      "Iteration 3470 Training loss 0.11945930868387222 Validation loss 0.11815709620714188 Accuracy 0.5991666913032532\n",
      "Output tensor([[0.4046],\n",
      "        [0.4967]])\n",
      "Iteration 3480 Training loss 0.11940094083547592 Validation loss 0.11813635379076004 Accuracy 0.6003333330154419\n",
      "Output tensor([[0.3547],\n",
      "        [0.4356]])\n",
      "Iteration 3490 Training loss 0.1201413944363594 Validation loss 0.11828701943159103 Accuracy 0.5998333096504211\n",
      "Output tensor([[0.5288],\n",
      "        [0.4305]])\n",
      "Iteration 3500 Training loss 0.11918898671865463 Validation loss 0.11811180412769318 Accuracy 0.6016666889190674\n",
      "Output tensor([[0.5386],\n",
      "        [0.6145]])\n",
      "Iteration 3510 Training loss 0.11968788504600525 Validation loss 0.11814170330762863 Accuracy 0.5998333096504211\n",
      "Output tensor([[0.5650],\n",
      "        [0.5691]])\n",
      "Iteration 3520 Training loss 0.11936672776937485 Validation loss 0.11808660626411438 Accuracy 0.6023333072662354\n",
      "Output tensor([[0.5684],\n",
      "        [0.5980]])\n",
      "Iteration 3530 Training loss 0.11900097876787186 Validation loss 0.11811240762472153 Accuracy 0.5996666550636292\n",
      "Output tensor([[0.5301],\n",
      "        [0.5400]])\n",
      "Iteration 3540 Training loss 0.1199948713183403 Validation loss 0.11832968890666962 Accuracy 0.5989999771118164\n",
      "Output tensor([[0.6048],\n",
      "        [0.4797]])\n",
      "Iteration 3550 Training loss 0.11951126158237457 Validation loss 0.11817549914121628 Accuracy 0.5998333096504211\n",
      "Output tensor([[0.4767],\n",
      "        [0.5164]])\n",
      "Iteration 3560 Training loss 0.11939878016710281 Validation loss 0.11820852756500244 Accuracy 0.5985000133514404\n",
      "Output tensor([[0.6933],\n",
      "        [0.4700]])\n",
      "Iteration 3570 Training loss 0.11965087801218033 Validation loss 0.11807064712047577 Accuracy 0.6014999747276306\n",
      "Output tensor([[0.4632],\n",
      "        [0.5947]])\n",
      "Iteration 3580 Training loss 0.11838684231042862 Validation loss 0.1180642694234848 Accuracy 0.6023333072662354\n",
      "Output tensor([[0.3670],\n",
      "        [0.5842]])\n",
      "Iteration 3590 Training loss 0.11942631751298904 Validation loss 0.1181434914469719 Accuracy 0.5996666550636292\n",
      "Output tensor([[0.3531],\n",
      "        [0.4573]])\n",
      "Iteration 3600 Training loss 0.11877961456775665 Validation loss 0.11807645857334137 Accuracy 0.6029999852180481\n",
      "Output tensor([[0.4845],\n",
      "        [0.5723]])\n",
      "Iteration 3610 Training loss 0.11888036131858826 Validation loss 0.11820261925458908 Accuracy 0.6004999876022339\n",
      "Output tensor([[0.5128],\n",
      "        [0.5128]])\n",
      "Iteration 3620 Training loss 0.1198137030005455 Validation loss 0.11807189136743546 Accuracy 0.6019999980926514\n",
      "Output tensor([[0.4921],\n",
      "        [0.5992]])\n",
      "Iteration 3630 Training loss 0.1194494217634201 Validation loss 0.11804068833589554 Accuracy 0.6018333435058594\n",
      "Output tensor([[0.5504],\n",
      "        [0.4461]])\n",
      "Iteration 3640 Training loss 0.1186303198337555 Validation loss 0.11804868280887604 Accuracy 0.6018333435058594\n",
      "Output tensor([[0.6384],\n",
      "        [0.5428]])\n",
      "Iteration 3650 Training loss 0.11942369490861893 Validation loss 0.11802773922681808 Accuracy 0.6018333435058594\n",
      "Output tensor([[0.3087],\n",
      "        [0.4995]])\n",
      "Iteration 3660 Training loss 0.11843901872634888 Validation loss 0.11800709366798401 Accuracy 0.6013333201408386\n",
      "Output tensor([[0.4567],\n",
      "        [0.5497]])\n",
      "Iteration 3670 Training loss 0.1190653070807457 Validation loss 0.11801590025424957 Accuracy 0.6013333201408386\n",
      "Output tensor([[0.4175],\n",
      "        [0.3520]])\n",
      "Iteration 3680 Training loss 0.11906539648771286 Validation loss 0.1180034875869751 Accuracy 0.6023333072662354\n",
      "Output tensor([[0.4763],\n",
      "        [0.3996]])\n",
      "Iteration 3690 Training loss 0.1188068836927414 Validation loss 0.1180819720029831 Accuracy 0.6003333330154419\n",
      "Output tensor([[0.4718],\n",
      "        [0.4360]])\n",
      "Iteration 3700 Training loss 0.11902457475662231 Validation loss 0.1179899275302887 Accuracy 0.6013333201408386\n",
      "Output tensor([[0.3511],\n",
      "        [0.3687]])\n",
      "Iteration 3710 Training loss 0.11895433813333511 Validation loss 0.11800061911344528 Accuracy 0.6026666760444641\n",
      "Output tensor([[0.3438],\n",
      "        [0.2581]])\n",
      "Iteration 3720 Training loss 0.11933436989784241 Validation loss 0.11811544001102448 Accuracy 0.6001666784286499\n",
      "Output tensor([[0.7014],\n",
      "        [0.4626]])\n",
      "Iteration 3730 Training loss 0.11949894577264786 Validation loss 0.11799222975969315 Accuracy 0.6026666760444641\n",
      "Output tensor([[0.5140],\n",
      "        [0.5832]])\n",
      "Iteration 3740 Training loss 0.11978073418140411 Validation loss 0.11798986792564392 Accuracy 0.6025000214576721\n",
      "Output tensor([[0.4831],\n",
      "        [0.4386]])\n",
      "Iteration 3750 Training loss 0.12009468674659729 Validation loss 0.11802492290735245 Accuracy 0.6011666655540466\n",
      "Output tensor([[0.2725],\n",
      "        [0.3499]])\n",
      "Iteration 3760 Training loss 0.11915840953588486 Validation loss 0.11796900629997253 Accuracy 0.6018333435058594\n",
      "Output tensor([[0.5532],\n",
      "        [0.3400]])\n",
      "Iteration 3770 Training loss 0.11910101026296616 Validation loss 0.1180652603507042 Accuracy 0.6006666421890259\n",
      "Output tensor([[0.3796],\n",
      "        [0.5157]])\n",
      "Iteration 3780 Training loss 0.11908937245607376 Validation loss 0.11796989291906357 Accuracy 0.6010000109672546\n",
      "Output tensor([[0.5496],\n",
      "        [0.4940]])\n",
      "Iteration 3790 Training loss 0.11925908178091049 Validation loss 0.11800863593816757 Accuracy 0.6010000109672546\n",
      "Output tensor([[0.4849],\n",
      "        [0.6517]])\n",
      "Iteration 3800 Training loss 0.11878814548254013 Validation loss 0.11804422736167908 Accuracy 0.6008333563804626\n",
      "Output tensor([[0.5616],\n",
      "        [0.6507]])\n",
      "Iteration 3810 Training loss 0.11920057982206345 Validation loss 0.11809063702821732 Accuracy 0.6014999747276306\n",
      "Output tensor([[0.4637],\n",
      "        [0.5221]])\n",
      "Iteration 3820 Training loss 0.1193692535161972 Validation loss 0.11798211932182312 Accuracy 0.6035000085830688\n",
      "Output tensor([[0.5490],\n",
      "        [0.5175]])\n",
      "Iteration 3830 Training loss 0.11917188763618469 Validation loss 0.11799293756484985 Accuracy 0.6023333072662354\n",
      "Output tensor([[0.3155],\n",
      "        [0.4057]])\n",
      "Iteration 3840 Training loss 0.119249127805233 Validation loss 0.11798780411481857 Accuracy 0.6018333435058594\n",
      "Output tensor([[0.4283],\n",
      "        [0.3625]])\n",
      "Iteration 3850 Training loss 0.11963151395320892 Validation loss 0.1179552748799324 Accuracy 0.6026666760444641\n",
      "Output tensor([[0.4921],\n",
      "        [0.5647]])\n",
      "Iteration 3860 Training loss 0.11918788403272629 Validation loss 0.11792414635419846 Accuracy 0.6029999852180481\n",
      "Output tensor([[0.3622],\n",
      "        [0.5371]])\n",
      "Iteration 3870 Training loss 0.12036676704883575 Validation loss 0.11792071163654327 Accuracy 0.6021666526794434\n",
      "Output tensor([[0.4787],\n",
      "        [0.6534]])\n",
      "Iteration 3880 Training loss 0.11975966393947601 Validation loss 0.11793353408575058 Accuracy 0.6026666760444641\n",
      "Output tensor([[0.4356],\n",
      "        [0.4744]])\n",
      "Iteration 3890 Training loss 0.11903536319732666 Validation loss 0.11795122921466827 Accuracy 0.6019999980926514\n",
      "Output tensor([[0.4607],\n",
      "        [0.5700]])\n",
      "Iteration 3900 Training loss 0.12001686543226242 Validation loss 0.11792796850204468 Accuracy 0.6019999980926514\n",
      "Output tensor([[0.3651],\n",
      "        [0.5169]])\n",
      "Iteration 3910 Training loss 0.118903748691082 Validation loss 0.11795394122600555 Accuracy 0.6008333563804626\n",
      "Output tensor([[0.5955],\n",
      "        [0.3687]])\n",
      "Iteration 3920 Training loss 0.11903329193592072 Validation loss 0.11796595901250839 Accuracy 0.6026666760444641\n",
      "Output tensor([[0.5244],\n",
      "        [0.5296]])\n",
      "Iteration 3930 Training loss 0.1201188862323761 Validation loss 0.11807393282651901 Accuracy 0.6008333563804626\n",
      "Output tensor([[0.6115],\n",
      "        [0.5987]])\n",
      "Iteration 3940 Training loss 0.11890138685703278 Validation loss 0.11811917275190353 Accuracy 0.5995000004768372\n",
      "Output tensor([[0.4719],\n",
      "        [0.4864]])\n",
      "Iteration 3950 Training loss 0.12013815343379974 Validation loss 0.1179441288113594 Accuracy 0.6021666526794434\n",
      "Output tensor([[0.3877],\n",
      "        [0.5330]])\n",
      "Iteration 3960 Training loss 0.11991038918495178 Validation loss 0.11794398725032806 Accuracy 0.6026666760444641\n",
      "Output tensor([[0.4237],\n",
      "        [0.5568]])\n",
      "Iteration 3970 Training loss 0.12003205716609955 Validation loss 0.11795036494731903 Accuracy 0.6041666865348816\n",
      "Output tensor([[0.4946],\n",
      "        [0.3837]])\n",
      "Iteration 3980 Training loss 0.11880671977996826 Validation loss 0.11793865263462067 Accuracy 0.6038333177566528\n",
      "Output tensor([[0.6010],\n",
      "        [0.5262]])\n",
      "Iteration 3990 Training loss 0.11964613944292068 Validation loss 0.1179603710770607 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.5237],\n",
      "        [0.5212]])\n",
      "Iteration 4000 Training loss 0.11797290295362473 Validation loss 0.1179511547088623 Accuracy 0.6028333306312561\n",
      "Output tensor([[0.5211],\n",
      "        [0.6071]])\n",
      "Iteration 4010 Training loss 0.11990055441856384 Validation loss 0.11794991791248322 Accuracy 0.6025000214576721\n",
      "Output tensor([[0.3996],\n",
      "        [0.2939]])\n",
      "Iteration 4020 Training loss 0.11980891227722168 Validation loss 0.11803881824016571 Accuracy 0.6010000109672546\n",
      "Output tensor([[0.5567],\n",
      "        [0.5153]])\n",
      "Iteration 4030 Training loss 0.12034136801958084 Validation loss 0.1179928407073021 Accuracy 0.5995000004768372\n",
      "Output tensor([[0.5473],\n",
      "        [0.4155]])\n",
      "Iteration 4040 Training loss 0.11959874629974365 Validation loss 0.11795194447040558 Accuracy 0.6026666760444641\n",
      "Output tensor([[0.5293],\n",
      "        [0.5758]])\n",
      "Iteration 4050 Training loss 0.11940904706716537 Validation loss 0.11806966364383698 Accuracy 0.5996666550636292\n",
      "Output tensor([[0.6316],\n",
      "        [0.5332]])\n",
      "Iteration 4060 Training loss 0.11947640031576157 Validation loss 0.11795682460069656 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.5381],\n",
      "        [0.5303]])\n",
      "Iteration 4070 Training loss 0.11925626546144485 Validation loss 0.11797653138637543 Accuracy 0.6003333330154419\n",
      "Output tensor([[0.3585],\n",
      "        [0.5407]])\n",
      "Iteration 4080 Training loss 0.11921694129705429 Validation loss 0.11796452105045319 Accuracy 0.6028333306312561\n",
      "Output tensor([[0.5158],\n",
      "        [0.4955]])\n",
      "Iteration 4090 Training loss 0.11875481903553009 Validation loss 0.11796282231807709 Accuracy 0.6019999980926514\n",
      "Output tensor([[0.5744],\n",
      "        [0.5551]])\n",
      "Iteration 4100 Training loss 0.11967331916093826 Validation loss 0.11795692890882492 Accuracy 0.6031666398048401\n",
      "Output tensor([[0.5341],\n",
      "        [0.6053]])\n",
      "Iteration 4110 Training loss 0.11950618773698807 Validation loss 0.11795222759246826 Accuracy 0.6014999747276306\n",
      "Output tensor([[0.5502],\n",
      "        [0.3420]])\n",
      "Iteration 4120 Training loss 0.11986466497182846 Validation loss 0.11793681979179382 Accuracy 0.6026666760444641\n",
      "Output tensor([[0.3921],\n",
      "        [0.6074]])\n",
      "Iteration 4130 Training loss 0.11949212849140167 Validation loss 0.11795394122600555 Accuracy 0.6010000109672546\n",
      "Output tensor([[0.4478],\n",
      "        [0.5517]])\n",
      "Iteration 4140 Training loss 0.11846978962421417 Validation loss 0.11795055121183395 Accuracy 0.6033333539962769\n",
      "Output tensor([[0.3601],\n",
      "        [0.5871]])\n",
      "Iteration 4150 Training loss 0.12062335014343262 Validation loss 0.11796288937330246 Accuracy 0.5995000004768372\n",
      "Output tensor([[0.5813],\n",
      "        [0.5216]])\n",
      "Iteration 4160 Training loss 0.11991780251264572 Validation loss 0.11791044473648071 Accuracy 0.6033333539962769\n",
      "Output tensor([[0.6981],\n",
      "        [0.5817]])\n",
      "Iteration 4170 Training loss 0.11869405955076218 Validation loss 0.1180199384689331 Accuracy 0.6006666421890259\n",
      "Output tensor([[0.4246],\n",
      "        [0.3995]])\n",
      "Iteration 4180 Training loss 0.11912263929843903 Validation loss 0.11798911541700363 Accuracy 0.6014999747276306\n",
      "Output tensor([[0.4583],\n",
      "        [0.5015]])\n",
      "Iteration 4190 Training loss 0.11845210939645767 Validation loss 0.11788302659988403 Accuracy 0.6028333306312561\n",
      "Output tensor([[0.4613],\n",
      "        [0.2868]])\n",
      "Iteration 4200 Training loss 0.11969950050115585 Validation loss 0.11801852285861969 Accuracy 0.6001666784286499\n",
      "Output tensor([[0.6101],\n",
      "        [0.5268]])\n",
      "Iteration 4210 Training loss 0.1186184510588646 Validation loss 0.11787398159503937 Accuracy 0.6023333072662354\n",
      "Output tensor([[0.5341],\n",
      "        [0.6200]])\n",
      "Iteration 4220 Training loss 0.11951098591089249 Validation loss 0.11792780458927155 Accuracy 0.6010000109672546\n",
      "Output tensor([[0.4404],\n",
      "        [0.5093]])\n",
      "Iteration 4230 Training loss 0.11916298419237137 Validation loss 0.1178746446967125 Accuracy 0.6023333072662354\n",
      "Output tensor([[0.6016],\n",
      "        [0.5136]])\n",
      "Iteration 4240 Training loss 0.12042099982500076 Validation loss 0.11786889284849167 Accuracy 0.6023333072662354\n",
      "Output tensor([[0.5338],\n",
      "        [0.2826]])\n",
      "Iteration 4250 Training loss 0.11850844323635101 Validation loss 0.1178811639547348 Accuracy 0.6029999852180481\n",
      "Output tensor([[0.4335],\n",
      "        [0.4537]])\n",
      "Iteration 4260 Training loss 0.12032012641429901 Validation loss 0.11792655289173126 Accuracy 0.6028333306312561\n",
      "Output tensor([[0.4700],\n",
      "        [0.5051]])\n",
      "Iteration 4270 Training loss 0.11860988289117813 Validation loss 0.11792969703674316 Accuracy 0.6025000214576721\n",
      "Output tensor([[0.5132],\n",
      "        [0.5787]])\n",
      "Iteration 4280 Training loss 0.11881094425916672 Validation loss 0.11791235208511353 Accuracy 0.6028333306312561\n",
      "Output tensor([[0.5336],\n",
      "        [0.5754]])\n",
      "Iteration 4290 Training loss 0.11917248368263245 Validation loss 0.11794297397136688 Accuracy 0.6016666889190674\n",
      "Output tensor([[0.2858],\n",
      "        [0.5637]])\n",
      "Iteration 4300 Training loss 0.11999012529850006 Validation loss 0.11789149791002274 Accuracy 0.6028333306312561\n",
      "Output tensor([[0.5588],\n",
      "        [0.6113]])\n",
      "Iteration 4310 Training loss 0.11964406073093414 Validation loss 0.11788725107908249 Accuracy 0.6048333048820496\n",
      "Output tensor([[0.4607],\n",
      "        [0.5554]])\n",
      "Iteration 4320 Training loss 0.11876805871725082 Validation loss 0.11788944154977798 Accuracy 0.6016666889190674\n",
      "Output tensor([[0.4524],\n",
      "        [0.4914]])\n",
      "Iteration 4330 Training loss 0.1194678321480751 Validation loss 0.11789996176958084 Accuracy 0.6033333539962769\n",
      "Output tensor([[0.3673],\n",
      "        [0.6009]])\n",
      "Iteration 4340 Training loss 0.11853484809398651 Validation loss 0.11787937581539154 Accuracy 0.6029999852180481\n",
      "Output tensor([[0.4706],\n",
      "        [0.5925]])\n",
      "Iteration 4350 Training loss 0.11989201605319977 Validation loss 0.11791187524795532 Accuracy 0.6023333072662354\n",
      "Output tensor([[0.5296],\n",
      "        [0.4646]])\n",
      "Iteration 4360 Training loss 0.11929156631231308 Validation loss 0.11787383258342743 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.4624],\n",
      "        [0.6343]])\n",
      "Iteration 4370 Training loss 0.11878844350576401 Validation loss 0.11791499704122543 Accuracy 0.6001666784286499\n",
      "Output tensor([[0.4689],\n",
      "        [0.3358]])\n",
      "Iteration 4380 Training loss 0.11866798996925354 Validation loss 0.11783956736326218 Accuracy 0.6028333306312561\n",
      "Output tensor([[0.4817],\n",
      "        [0.4695]])\n",
      "Iteration 4390 Training loss 0.11832018941640854 Validation loss 0.11784498393535614 Accuracy 0.6041666865348816\n",
      "Output tensor([[0.3443],\n",
      "        [0.4768]])\n",
      "Iteration 4400 Training loss 0.11829106509685516 Validation loss 0.11799214780330658 Accuracy 0.6000000238418579\n",
      "Output tensor([[0.3267],\n",
      "        [0.3551]])\n",
      "Iteration 4410 Training loss 0.11806557327508926 Validation loss 0.11792752891778946 Accuracy 0.6011666655540466\n",
      "Output tensor([[0.4897],\n",
      "        [0.4798]])\n",
      "Iteration 4420 Training loss 0.1190212219953537 Validation loss 0.1178976446390152 Accuracy 0.6036666631698608\n",
      "Output tensor([[0.4786],\n",
      "        [0.3440]])\n",
      "Iteration 4430 Training loss 0.12012473493814468 Validation loss 0.11784828454256058 Accuracy 0.6025000214576721\n",
      "Output tensor([[0.5419],\n",
      "        [0.5858]])\n",
      "Iteration 4440 Training loss 0.11959812790155411 Validation loss 0.11785009503364563 Accuracy 0.6036666631698608\n",
      "Output tensor([[0.6215],\n",
      "        [0.3687]])\n",
      "Iteration 4450 Training loss 0.1195405051112175 Validation loss 0.11783789098262787 Accuracy 0.6035000085830688\n",
      "Output tensor([[0.4951],\n",
      "        [0.3335]])\n",
      "Iteration 4460 Training loss 0.11894529312849045 Validation loss 0.11784640699625015 Accuracy 0.6033333539962769\n",
      "Output tensor([[0.5614],\n",
      "        [0.6038]])\n",
      "Iteration 4470 Training loss 0.11965744197368622 Validation loss 0.11787362396717072 Accuracy 0.6016666889190674\n",
      "Output tensor([[0.3948],\n",
      "        [0.7422]])\n",
      "Iteration 4480 Training loss 0.11953596025705338 Validation loss 0.11786636710166931 Accuracy 0.6026666760444641\n",
      "Output tensor([[0.6822],\n",
      "        [0.6171]])\n",
      "Iteration 4490 Training loss 0.11945005506277084 Validation loss 0.1178537979722023 Accuracy 0.6028333306312561\n",
      "Output tensor([[0.5544],\n",
      "        [0.4970]])\n",
      "Iteration 4500 Training loss 0.11993861198425293 Validation loss 0.11784777790307999 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.7194],\n",
      "        [0.4048]])\n",
      "Iteration 4510 Training loss 0.11967294663190842 Validation loss 0.11782942712306976 Accuracy 0.6036666631698608\n",
      "Output tensor([[0.5470],\n",
      "        [0.5980]])\n",
      "Iteration 4520 Training loss 0.11822982132434845 Validation loss 0.11785099655389786 Accuracy 0.6021666526794434\n",
      "Output tensor([[0.5565],\n",
      "        [0.4975]])\n",
      "Iteration 4530 Training loss 0.11993402987718582 Validation loss 0.11782938987016678 Accuracy 0.6018333435058594\n",
      "Output tensor([[0.5504],\n",
      "        [0.3851]])\n",
      "Iteration 4540 Training loss 0.11928384751081467 Validation loss 0.11790557950735092 Accuracy 0.6018333435058594\n",
      "Output tensor([[0.4486],\n",
      "        [0.5202]])\n",
      "Iteration 4550 Training loss 0.1195063441991806 Validation loss 0.11788687855005264 Accuracy 0.6010000109672546\n",
      "Output tensor([[0.3226],\n",
      "        [0.6118]])\n",
      "Iteration 4560 Training loss 0.11871505528688431 Validation loss 0.11782574653625488 Accuracy 0.6026666760444641\n",
      "Output tensor([[0.5022],\n",
      "        [0.3990]])\n",
      "Iteration 4570 Training loss 0.1199345737695694 Validation loss 0.11784540116786957 Accuracy 0.6035000085830688\n",
      "Output tensor([[0.5866],\n",
      "        [0.3019]])\n",
      "Iteration 4580 Training loss 0.11795364320278168 Validation loss 0.11783067137002945 Accuracy 0.6043333411216736\n",
      "Output tensor([[0.4701],\n",
      "        [0.4078]])\n",
      "Iteration 4590 Training loss 0.11821907758712769 Validation loss 0.11782265454530716 Accuracy 0.6028333306312561\n",
      "Output tensor([[0.5427],\n",
      "        [0.5010]])\n",
      "Iteration 4600 Training loss 0.11893710494041443 Validation loss 0.11783839762210846 Accuracy 0.6031666398048401\n",
      "Output tensor([[0.6114],\n",
      "        [0.4824]])\n",
      "Iteration 4610 Training loss 0.11967072635889053 Validation loss 0.1178872287273407 Accuracy 0.6028333306312561\n",
      "Output tensor([[0.5866],\n",
      "        [0.5768]])\n",
      "Iteration 4620 Training loss 0.11828774958848953 Validation loss 0.11783137172460556 Accuracy 0.6029999852180481\n",
      "Output tensor([[0.5082],\n",
      "        [0.5219]])\n",
      "Iteration 4630 Training loss 0.1189974769949913 Validation loss 0.11781920492649078 Accuracy 0.6041666865348816\n",
      "Output tensor([[0.4333],\n",
      "        [0.3545]])\n",
      "Iteration 4640 Training loss 0.11885647475719452 Validation loss 0.11784687638282776 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.5161],\n",
      "        [0.5040]])\n",
      "Iteration 4650 Training loss 0.11926896870136261 Validation loss 0.11785843968391418 Accuracy 0.6033333539962769\n",
      "Output tensor([[0.5416],\n",
      "        [0.4978]])\n",
      "Iteration 4660 Training loss 0.11905372887849808 Validation loss 0.11792462319135666 Accuracy 0.6019999980926514\n",
      "Output tensor([[0.4915],\n",
      "        [0.5557]])\n",
      "Iteration 4670 Training loss 0.11950784176588058 Validation loss 0.11790686100721359 Accuracy 0.6014999747276306\n",
      "Output tensor([[0.4256],\n",
      "        [0.5815]])\n",
      "Iteration 4680 Training loss 0.11940612643957138 Validation loss 0.11788883805274963 Accuracy 0.6018333435058594\n",
      "Output tensor([[0.3509],\n",
      "        [0.5675]])\n",
      "Iteration 4690 Training loss 0.11786855012178421 Validation loss 0.11783207207918167 Accuracy 0.6035000085830688\n",
      "Output tensor([[0.4767],\n",
      "        [0.3858]])\n",
      "Iteration 4700 Training loss 0.11859988421201706 Validation loss 0.11784623563289642 Accuracy 0.6038333177566528\n",
      "Output tensor([[0.6710],\n",
      "        [0.5680]])\n",
      "Iteration 4710 Training loss 0.11920583248138428 Validation loss 0.11783162504434586 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.5425],\n",
      "        [0.7207]])\n",
      "Iteration 4720 Training loss 0.11780621111392975 Validation loss 0.11781258881092072 Accuracy 0.6041666865348816\n",
      "Output tensor([[0.5358],\n",
      "        [0.4785]])\n",
      "Iteration 4730 Training loss 0.11937423050403595 Validation loss 0.11781733483076096 Accuracy 0.6035000085830688\n",
      "Output tensor([[0.6878],\n",
      "        [0.5361]])\n",
      "Iteration 4740 Training loss 0.11934318393468857 Validation loss 0.11780954897403717 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.5760],\n",
      "        [0.5296]])\n",
      "Iteration 4750 Training loss 0.11875071376562119 Validation loss 0.11783755570650101 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.5096],\n",
      "        [0.3956]])\n",
      "Iteration 4760 Training loss 0.11869903653860092 Validation loss 0.11779958009719849 Accuracy 0.6025000214576721\n",
      "Output tensor([[0.5811],\n",
      "        [0.3188]])\n",
      "Iteration 4770 Training loss 0.11960295587778091 Validation loss 0.11781130731105804 Accuracy 0.6025000214576721\n",
      "Output tensor([[0.3022],\n",
      "        [0.5501]])\n",
      "Iteration 4780 Training loss 0.11881902813911438 Validation loss 0.11778830736875534 Accuracy 0.6033333539962769\n",
      "Output tensor([[0.2050],\n",
      "        [0.4731]])\n",
      "Iteration 4790 Training loss 0.11908794194459915 Validation loss 0.11819595098495483 Accuracy 0.5995000004768372\n",
      "Output tensor([[0.4947],\n",
      "        [0.6103]])\n",
      "Iteration 4800 Training loss 0.11984476447105408 Validation loss 0.1177508682012558 Accuracy 0.6031666398048401\n",
      "Output tensor([[0.5382],\n",
      "        [0.5597]])\n",
      "Iteration 4810 Training loss 0.11864504218101501 Validation loss 0.11777718365192413 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.5599],\n",
      "        [0.5106]])\n",
      "Iteration 4820 Training loss 0.11866439878940582 Validation loss 0.11773520708084106 Accuracy 0.6031666398048401\n",
      "Output tensor([[0.5526],\n",
      "        [0.5347]])\n",
      "Iteration 4830 Training loss 0.11870235949754715 Validation loss 0.11776324361562729 Accuracy 0.6029999852180481\n",
      "Output tensor([[0.4423],\n",
      "        [0.4550]])\n",
      "Iteration 4840 Training loss 0.11877664178609848 Validation loss 0.11773885786533356 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.4469],\n",
      "        [0.5345]])\n",
      "Iteration 4850 Training loss 0.11838311702013016 Validation loss 0.11777160316705704 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.6253],\n",
      "        [0.3166]])\n",
      "Iteration 4860 Training loss 0.118785560131073 Validation loss 0.11776666343212128 Accuracy 0.6036666631698608\n",
      "Output tensor([[0.5274],\n",
      "        [0.5008]])\n",
      "Iteration 4870 Training loss 0.11913707107305527 Validation loss 0.11776263266801834 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.5364],\n",
      "        [0.2498]])\n",
      "Iteration 4880 Training loss 0.11769497394561768 Validation loss 0.11779138445854187 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.5584],\n",
      "        [0.3423]])\n",
      "Iteration 4890 Training loss 0.11936936527490616 Validation loss 0.11778304725885391 Accuracy 0.6041666865348816\n",
      "Output tensor([[0.4811],\n",
      "        [0.5077]])\n",
      "Iteration 4900 Training loss 0.11922992020845413 Validation loss 0.1177636906504631 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.4890],\n",
      "        [0.6447]])\n",
      "Iteration 4910 Training loss 0.11908796429634094 Validation loss 0.11778263002634048 Accuracy 0.6036666631698608\n",
      "Output tensor([[0.5283],\n",
      "        [0.3584]])\n",
      "Iteration 4920 Training loss 0.11876390874385834 Validation loss 0.11800091713666916 Accuracy 0.6023333072662354\n",
      "Output tensor([[0.5834],\n",
      "        [0.6217]])\n",
      "Iteration 4930 Training loss 0.1185327023267746 Validation loss 0.11775513738393784 Accuracy 0.6041666865348816\n",
      "Output tensor([[0.7485],\n",
      "        [0.4981]])\n",
      "Iteration 4940 Training loss 0.11964524537324905 Validation loss 0.11776816844940186 Accuracy 0.6028333306312561\n",
      "Output tensor([[0.4422],\n",
      "        [0.5364]])\n",
      "Iteration 4950 Training loss 0.11881054937839508 Validation loss 0.11780829727649689 Accuracy 0.6038333177566528\n",
      "Output tensor([[0.5349],\n",
      "        [0.5914]])\n",
      "Iteration 4960 Training loss 0.11931738257408142 Validation loss 0.11780811846256256 Accuracy 0.6026666760444641\n",
      "Output tensor([[0.3968],\n",
      "        [0.5529]])\n",
      "Iteration 4970 Training loss 0.11883185058832169 Validation loss 0.11778655648231506 Accuracy 0.6029999852180481\n",
      "Output tensor([[0.3621],\n",
      "        [0.3348]])\n",
      "Iteration 4980 Training loss 0.11911586672067642 Validation loss 0.11772209405899048 Accuracy 0.6031666398048401\n",
      "Output tensor([[0.4620],\n",
      "        [0.4291]])\n",
      "Iteration 4990 Training loss 0.1183033213019371 Validation loss 0.1177353635430336 Accuracy 0.6025000214576721\n",
      "Output tensor([[0.5693],\n",
      "        [0.4330]])\n",
      "Iteration 5000 Training loss 0.11819911748170853 Validation loss 0.11776906251907349 Accuracy 0.6035000085830688\n",
      "Output tensor([[0.4289],\n",
      "        [0.3498]])\n",
      "Iteration 5010 Training loss 0.11961083859205246 Validation loss 0.11776107549667358 Accuracy 0.6035000085830688\n",
      "Output tensor([[0.4987],\n",
      "        [0.3299]])\n",
      "Iteration 5020 Training loss 0.11874564737081528 Validation loss 0.11772845685482025 Accuracy 0.6035000085830688\n",
      "Output tensor([[0.5309],\n",
      "        [0.5286]])\n",
      "Iteration 5030 Training loss 0.11851149797439575 Validation loss 0.11773255467414856 Accuracy 0.6029999852180481\n",
      "Output tensor([[0.3136],\n",
      "        [0.5129]])\n",
      "Iteration 5040 Training loss 0.11908566951751709 Validation loss 0.11777273565530777 Accuracy 0.6031666398048401\n",
      "Output tensor([[0.5816],\n",
      "        [0.5054]])\n",
      "Iteration 5050 Training loss 0.11898735910654068 Validation loss 0.11772545427083969 Accuracy 0.6023333072662354\n",
      "Output tensor([[0.3631],\n",
      "        [0.4631]])\n",
      "Iteration 5060 Training loss 0.1180146113038063 Validation loss 0.11772912740707397 Accuracy 0.6038333177566528\n",
      "Output tensor([[0.4219],\n",
      "        [0.5064]])\n",
      "Iteration 5070 Training loss 0.11952529847621918 Validation loss 0.11775103956460953 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.4551],\n",
      "        [0.4381]])\n",
      "Iteration 5080 Training loss 0.1192116066813469 Validation loss 0.11778935045003891 Accuracy 0.6033333539962769\n",
      "Output tensor([[0.5367],\n",
      "        [0.3736]])\n",
      "Iteration 5090 Training loss 0.11914855241775513 Validation loss 0.11776429414749146 Accuracy 0.6051666736602783\n",
      "Output tensor([[0.4140],\n",
      "        [0.5422]])\n",
      "Iteration 5100 Training loss 0.11916481703519821 Validation loss 0.11773255467414856 Accuracy 0.6036666631698608\n",
      "Output tensor([[0.5455],\n",
      "        [0.5591]])\n",
      "Iteration 5110 Training loss 0.11812012642621994 Validation loss 0.11773335933685303 Accuracy 0.6036666631698608\n",
      "Output tensor([[0.5125],\n",
      "        [0.4319]])\n",
      "Iteration 5120 Training loss 0.11905108392238617 Validation loss 0.11776529252529144 Accuracy 0.6046666502952576\n",
      "Output tensor([[0.6489],\n",
      "        [0.5169]])\n",
      "Iteration 5130 Training loss 0.11888255923986435 Validation loss 0.11775864660739899 Accuracy 0.6050000190734863\n",
      "Output tensor([[0.5751],\n",
      "        [0.5903]])\n",
      "Iteration 5140 Training loss 0.11963500082492828 Validation loss 0.11773695051670074 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.1574],\n",
      "        [0.3533]])\n",
      "Iteration 5150 Training loss 0.11952635645866394 Validation loss 0.11772186309099197 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.4234],\n",
      "        [0.4239]])\n",
      "Iteration 5160 Training loss 0.11869966983795166 Validation loss 0.11781627684831619 Accuracy 0.6029999852180481\n",
      "Output tensor([[0.4201],\n",
      "        [0.3793]])\n",
      "Iteration 5170 Training loss 0.12024359405040741 Validation loss 0.11773547530174255 Accuracy 0.6025000214576721\n",
      "Output tensor([[0.4595],\n",
      "        [0.6089]])\n",
      "Iteration 5180 Training loss 0.11826831102371216 Validation loss 0.1176997497677803 Accuracy 0.6038333177566528\n",
      "Output tensor([[0.5316],\n",
      "        [0.5536]])\n",
      "Iteration 5190 Training loss 0.11931375414133072 Validation loss 0.11770127713680267 Accuracy 0.6025000214576721\n",
      "Output tensor([[0.5866],\n",
      "        [0.5649]])\n",
      "Iteration 5200 Training loss 0.11880020797252655 Validation loss 0.11771242320537567 Accuracy 0.6048333048820496\n",
      "Output tensor([[0.5120],\n",
      "        [0.4198]])\n",
      "Iteration 5210 Training loss 0.11901385337114334 Validation loss 0.11771050095558167 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.5773],\n",
      "        [0.5941]])\n",
      "Iteration 5220 Training loss 0.11909618973731995 Validation loss 0.11779046803712845 Accuracy 0.6036666631698608\n",
      "Output tensor([[0.2913],\n",
      "        [0.2844]])\n",
      "Iteration 5230 Training loss 0.11979427188634872 Validation loss 0.11772986501455307 Accuracy 0.6035000085830688\n",
      "Output tensor([[0.6778],\n",
      "        [0.5871]])\n",
      "Iteration 5240 Training loss 0.11895576864480972 Validation loss 0.117767833173275 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.5200],\n",
      "        [0.4994]])\n",
      "Iteration 5250 Training loss 0.11841583251953125 Validation loss 0.11773397773504257 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.4754],\n",
      "        [0.5333]])\n",
      "Iteration 5260 Training loss 0.11812081933021545 Validation loss 0.11775486171245575 Accuracy 0.6050000190734863\n",
      "Output tensor([[0.4997],\n",
      "        [0.4276]])\n",
      "Iteration 5270 Training loss 0.1192159652709961 Validation loss 0.11770027875900269 Accuracy 0.6026666760444641\n",
      "Output tensor([[0.5479],\n",
      "        [0.6287]])\n",
      "Iteration 5280 Training loss 0.11878510564565659 Validation loss 0.11776115000247955 Accuracy 0.6038333177566528\n",
      "Output tensor([[0.3647],\n",
      "        [0.5177]])\n",
      "Iteration 5290 Training loss 0.11888144165277481 Validation loss 0.11773272603750229 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.4649],\n",
      "        [0.4733]])\n",
      "Iteration 5300 Training loss 0.11797171831130981 Validation loss 0.11771801859140396 Accuracy 0.6029999852180481\n",
      "Output tensor([[0.4800],\n",
      "        [0.5095]])\n",
      "Iteration 5310 Training loss 0.11894650012254715 Validation loss 0.1177162304520607 Accuracy 0.6038333177566528\n",
      "Output tensor([[0.3734],\n",
      "        [0.6155]])\n",
      "Iteration 5320 Training loss 0.11893192678689957 Validation loss 0.11781508475542068 Accuracy 0.6026666760444641\n",
      "Output tensor([[0.5375],\n",
      "        [0.4517]])\n",
      "Iteration 5330 Training loss 0.11949903517961502 Validation loss 0.11769700050354004 Accuracy 0.6033333539962769\n",
      "Output tensor([[0.4880],\n",
      "        [0.6255]])\n",
      "Iteration 5340 Training loss 0.11902495473623276 Validation loss 0.11774102598428726 Accuracy 0.6053333282470703\n",
      "Output tensor([[0.6076],\n",
      "        [0.4025]])\n",
      "Iteration 5350 Training loss 0.11877664178609848 Validation loss 0.11771896481513977 Accuracy 0.6053333282470703\n",
      "Output tensor([[0.4465],\n",
      "        [0.3840]])\n",
      "Iteration 5360 Training loss 0.1191813126206398 Validation loss 0.11779659986495972 Accuracy 0.6051666736602783\n",
      "Output tensor([[0.6385],\n",
      "        [0.4837]])\n",
      "Iteration 5370 Training loss 0.11907774955034256 Validation loss 0.11768839508295059 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.4101],\n",
      "        [0.4833]])\n",
      "Iteration 5380 Training loss 0.11869123578071594 Validation loss 0.11773661524057388 Accuracy 0.6051666736602783\n",
      "Output tensor([[0.5909],\n",
      "        [0.5173]])\n",
      "Iteration 5390 Training loss 0.11858437210321426 Validation loss 0.11767147481441498 Accuracy 0.6041666865348816\n",
      "Output tensor([[0.5735],\n",
      "        [0.5734]])\n",
      "Iteration 5400 Training loss 0.1193813756108284 Validation loss 0.11772618442773819 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.3613],\n",
      "        [0.4371]])\n",
      "Iteration 5410 Training loss 0.11926130950450897 Validation loss 0.11772249639034271 Accuracy 0.6051666736602783\n",
      "Output tensor([[0.6101],\n",
      "        [0.4319]])\n",
      "Iteration 5420 Training loss 0.1184530258178711 Validation loss 0.11776711046695709 Accuracy 0.6053333282470703\n",
      "Output tensor([[0.4883],\n",
      "        [0.4380]])\n",
      "Iteration 5430 Training loss 0.1197754517197609 Validation loss 0.11772611737251282 Accuracy 0.6051666736602783\n",
      "Output tensor([[0.6558],\n",
      "        [0.4640]])\n",
      "Iteration 5440 Training loss 0.11766360700130463 Validation loss 0.1176547035574913 Accuracy 0.6051666736602783\n",
      "Output tensor([[0.6195],\n",
      "        [0.4495]])\n",
      "Iteration 5450 Training loss 0.11919461935758591 Validation loss 0.11761672794818878 Accuracy 0.6046666502952576\n",
      "Output tensor([[0.5803],\n",
      "        [0.4935]])\n",
      "Iteration 5460 Training loss 0.11802327632904053 Validation loss 0.11771834641695023 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.5336],\n",
      "        [0.3866]])\n",
      "Iteration 5470 Training loss 0.11909663677215576 Validation loss 0.11763036996126175 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.5792],\n",
      "        [0.5454]])\n",
      "Iteration 5480 Training loss 0.11951014399528503 Validation loss 0.1176077276468277 Accuracy 0.6035000085830688\n",
      "Output tensor([[0.5589],\n",
      "        [0.4880]])\n",
      "Iteration 5490 Training loss 0.11890573799610138 Validation loss 0.11764083057641983 Accuracy 0.606333315372467\n",
      "Output tensor([[0.5254],\n",
      "        [0.3545]])\n",
      "Iteration 5500 Training loss 0.11879058182239532 Validation loss 0.11762178689241409 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.5987],\n",
      "        [0.5973]])\n",
      "Iteration 5510 Training loss 0.11880707740783691 Validation loss 0.11763880401849747 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.5422],\n",
      "        [0.3524]])\n",
      "Iteration 5520 Training loss 0.11934023350477219 Validation loss 0.1176987737417221 Accuracy 0.6029999852180481\n",
      "Output tensor([[0.3152],\n",
      "        [0.3636]])\n",
      "Iteration 5530 Training loss 0.11933687329292297 Validation loss 0.11775805801153183 Accuracy 0.6033333539962769\n",
      "Output tensor([[0.5003],\n",
      "        [0.5282]])\n",
      "Iteration 5540 Training loss 0.11932631582021713 Validation loss 0.11782751232385635 Accuracy 0.6053333282470703\n",
      "Output tensor([[0.4338],\n",
      "        [0.4373]])\n",
      "Iteration 5550 Training loss 0.11918789893388748 Validation loss 0.11760278046131134 Accuracy 0.6060000061988831\n",
      "Output tensor([[0.6321],\n",
      "        [0.5166]])\n",
      "Iteration 5560 Training loss 0.11968757957220078 Validation loss 0.11760204285383224 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.4020],\n",
      "        [0.5081]])\n",
      "Iteration 5570 Training loss 0.11798423528671265 Validation loss 0.11768480390310287 Accuracy 0.6043333411216736\n",
      "Output tensor([[0.3240],\n",
      "        [0.3855]])\n",
      "Iteration 5580 Training loss 0.11897548288106918 Validation loss 0.11781700700521469 Accuracy 0.6041666865348816\n",
      "Output tensor([[0.3845],\n",
      "        [0.2706]])\n",
      "Iteration 5590 Training loss 0.11947587132453918 Validation loss 0.11760939657688141 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.5514],\n",
      "        [0.4962]])\n",
      "Iteration 5600 Training loss 0.11918790638446808 Validation loss 0.11762379854917526 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.4504],\n",
      "        [0.5620]])\n",
      "Iteration 5610 Training loss 0.11916747689247131 Validation loss 0.11761532723903656 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.3940],\n",
      "        [0.5972]])\n",
      "Iteration 5620 Training loss 0.11877842992544174 Validation loss 0.11760163307189941 Accuracy 0.6041666865348816\n",
      "Output tensor([[0.4426],\n",
      "        [0.4583]])\n",
      "Iteration 5630 Training loss 0.11818026006221771 Validation loss 0.11759842932224274 Accuracy 0.606166660785675\n",
      "Output tensor([[0.5641],\n",
      "        [0.2975]])\n",
      "Iteration 5640 Training loss 0.11899343132972717 Validation loss 0.11763014644384384 Accuracy 0.6068333387374878\n",
      "Output tensor([[0.3324],\n",
      "        [0.2223]])\n",
      "Iteration 5650 Training loss 0.12012951821088791 Validation loss 0.1177147701382637 Accuracy 0.6073333621025085\n",
      "Output tensor([[0.5674],\n",
      "        [0.5072]])\n",
      "Iteration 5660 Training loss 0.11877427995204926 Validation loss 0.11758022010326385 Accuracy 0.6050000190734863\n",
      "Output tensor([[0.3943],\n",
      "        [0.4452]])\n",
      "Iteration 5670 Training loss 0.1199001744389534 Validation loss 0.11757470667362213 Accuracy 0.6043333411216736\n",
      "Output tensor([[0.4634],\n",
      "        [0.6135]])\n",
      "Iteration 5680 Training loss 0.11888325959444046 Validation loss 0.11756148189306259 Accuracy 0.6043333411216736\n",
      "Output tensor([[0.5788],\n",
      "        [0.4713]])\n",
      "Iteration 5690 Training loss 0.11917012184858322 Validation loss 0.11764123290777206 Accuracy 0.6046666502952576\n",
      "Output tensor([[0.5258],\n",
      "        [0.7607]])\n",
      "Iteration 5700 Training loss 0.11857941001653671 Validation loss 0.11758219450712204 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.5704],\n",
      "        [0.5678]])\n",
      "Iteration 5710 Training loss 0.11934517323970795 Validation loss 0.11759442090988159 Accuracy 0.6050000190734863\n",
      "Output tensor([[0.5004],\n",
      "        [0.5440]])\n",
      "Iteration 5720 Training loss 0.1178974136710167 Validation loss 0.11759824305772781 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.3108],\n",
      "        [0.6231]])\n",
      "Iteration 5730 Training loss 0.11930133402347565 Validation loss 0.11766904592514038 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.4351],\n",
      "        [0.6243]])\n",
      "Iteration 5740 Training loss 0.11977673321962357 Validation loss 0.11758069694042206 Accuracy 0.6050000190734863\n",
      "Output tensor([[0.6829],\n",
      "        [0.3911]])\n",
      "Iteration 5750 Training loss 0.11854168027639389 Validation loss 0.11756482720375061 Accuracy 0.6060000061988831\n",
      "Output tensor([[0.5265],\n",
      "        [0.5013]])\n",
      "Iteration 5760 Training loss 0.11924874037504196 Validation loss 0.1175786629319191 Accuracy 0.6069999933242798\n",
      "Output tensor([[0.4505],\n",
      "        [0.4660]])\n",
      "Iteration 5770 Training loss 0.11885945498943329 Validation loss 0.11755242943763733 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.5511],\n",
      "        [0.5604]])\n",
      "Iteration 5780 Training loss 0.11891569197177887 Validation loss 0.11760494112968445 Accuracy 0.6066666841506958\n",
      "Output tensor([[0.5653],\n",
      "        [0.4395]])\n",
      "Iteration 5790 Training loss 0.11924298107624054 Validation loss 0.11757542192935944 Accuracy 0.606333315372467\n",
      "Output tensor([[0.4480],\n",
      "        [0.4178]])\n",
      "Iteration 5800 Training loss 0.11996006965637207 Validation loss 0.11760018765926361 Accuracy 0.6068333387374878\n",
      "Output tensor([[0.5791],\n",
      "        [0.4579]])\n",
      "Iteration 5810 Training loss 0.11840642988681793 Validation loss 0.11759448051452637 Accuracy 0.6048333048820496\n",
      "Output tensor([[0.4028],\n",
      "        [0.4282]])\n",
      "Iteration 5820 Training loss 0.11925821751356125 Validation loss 0.11756571382284164 Accuracy 0.6046666502952576\n",
      "Output tensor([[0.5058],\n",
      "        [0.5456]])\n",
      "Iteration 5830 Training loss 0.1193888932466507 Validation loss 0.1175561398267746 Accuracy 0.6046666502952576\n",
      "Output tensor([[0.3179],\n",
      "        [0.6746]])\n",
      "Iteration 5840 Training loss 0.11898716539144516 Validation loss 0.1175636500120163 Accuracy 0.6053333282470703\n",
      "Output tensor([[0.5449],\n",
      "        [0.4303]])\n",
      "Iteration 5850 Training loss 0.11923627555370331 Validation loss 0.11754962056875229 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.4653],\n",
      "        [0.5819]])\n",
      "Iteration 5860 Training loss 0.11890776455402374 Validation loss 0.11753655970096588 Accuracy 0.6069999933242798\n",
      "Output tensor([[0.4359],\n",
      "        [0.2892]])\n",
      "Iteration 5870 Training loss 0.1183764636516571 Validation loss 0.11758801341056824 Accuracy 0.606333315372467\n",
      "Output tensor([[0.2350],\n",
      "        [0.4403]])\n",
      "Iteration 5880 Training loss 0.11906520277261734 Validation loss 0.11773171275854111 Accuracy 0.6038333177566528\n",
      "Output tensor([[0.5568],\n",
      "        [0.4727]])\n",
      "Iteration 5890 Training loss 0.1187121793627739 Validation loss 0.11759218573570251 Accuracy 0.6060000061988831\n",
      "Output tensor([[0.2234],\n",
      "        [0.4981]])\n",
      "Iteration 5900 Training loss 0.11814448982477188 Validation loss 0.117571622133255 Accuracy 0.6046666502952576\n",
      "Output tensor([[0.4037],\n",
      "        [0.6975]])\n",
      "Iteration 5910 Training loss 0.11939064413309097 Validation loss 0.11755669116973877 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.5270],\n",
      "        [0.5646]])\n",
      "Iteration 5920 Training loss 0.11898922920227051 Validation loss 0.11757564544677734 Accuracy 0.6053333282470703\n",
      "Output tensor([[0.5044],\n",
      "        [0.5887]])\n",
      "Iteration 5930 Training loss 0.1183001920580864 Validation loss 0.11753686517477036 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.3359],\n",
      "        [0.8054]])\n",
      "Iteration 5940 Training loss 0.11857495456933975 Validation loss 0.11754953861236572 Accuracy 0.6046666502952576\n",
      "Output tensor([[0.5562],\n",
      "        [0.4095]])\n",
      "Iteration 5950 Training loss 0.11959219723939896 Validation loss 0.11755786091089249 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.4831],\n",
      "        [0.3868]])\n",
      "Iteration 5960 Training loss 0.11966942995786667 Validation loss 0.11755505204200745 Accuracy 0.6053333282470703\n",
      "Output tensor([[0.5853],\n",
      "        [0.6505]])\n",
      "Iteration 5970 Training loss 0.11958868056535721 Validation loss 0.11755578964948654 Accuracy 0.6048333048820496\n",
      "Output tensor([[0.5762],\n",
      "        [0.4375]])\n",
      "Iteration 5980 Training loss 0.11846145987510681 Validation loss 0.117531917989254 Accuracy 0.6036666631698608\n",
      "Output tensor([[0.6280],\n",
      "        [0.3835]])\n",
      "Iteration 5990 Training loss 0.1197449266910553 Validation loss 0.11754845827817917 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.2608],\n",
      "        [0.5527]])\n",
      "Iteration 6000 Training loss 0.11795416474342346 Validation loss 0.11752018332481384 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.5560],\n",
      "        [0.7249]])\n",
      "Iteration 6010 Training loss 0.11819310486316681 Validation loss 0.11753981560468674 Accuracy 0.6048333048820496\n",
      "Output tensor([[0.5746],\n",
      "        [0.6569]])\n",
      "Iteration 6020 Training loss 0.11855209618806839 Validation loss 0.11753631383180618 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.6589],\n",
      "        [0.5285]])\n",
      "Iteration 6030 Training loss 0.1195979118347168 Validation loss 0.11754012852907181 Accuracy 0.606166660785675\n",
      "Output tensor([[0.5018],\n",
      "        [0.4080]])\n",
      "Iteration 6040 Training loss 0.11897887289524078 Validation loss 0.11755096167325974 Accuracy 0.6068333387374878\n",
      "Output tensor([[0.4710],\n",
      "        [0.6612]])\n",
      "Iteration 6050 Training loss 0.11734788864850998 Validation loss 0.11758887767791748 Accuracy 0.6073333621025085\n",
      "Output tensor([[0.4044],\n",
      "        [0.5263]])\n",
      "Iteration 6060 Training loss 0.1179303377866745 Validation loss 0.11760243773460388 Accuracy 0.6053333282470703\n",
      "Output tensor([[0.4968],\n",
      "        [0.5167]])\n",
      "Iteration 6070 Training loss 0.11783554404973984 Validation loss 0.11753498017787933 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.3863],\n",
      "        [0.5168]])\n",
      "Iteration 6080 Training loss 0.11884354799985886 Validation loss 0.11752857267856598 Accuracy 0.6044999957084656\n",
      "Output tensor([[0.4730],\n",
      "        [0.5456]])\n",
      "Iteration 6090 Training loss 0.11894755810499191 Validation loss 0.1175590306520462 Accuracy 0.6060000061988831\n",
      "Output tensor([[0.5496],\n",
      "        [0.4994]])\n",
      "Iteration 6100 Training loss 0.11951061338186264 Validation loss 0.11756160110235214 Accuracy 0.6060000061988831\n",
      "Output tensor([[0.5626],\n",
      "        [0.4305]])\n",
      "Iteration 6110 Training loss 0.1186375617980957 Validation loss 0.11751587688922882 Accuracy 0.6039999723434448\n",
      "Output tensor([[0.4565],\n",
      "        [0.2550]])\n",
      "Iteration 6120 Training loss 0.11836311966180801 Validation loss 0.11751632392406464 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.1379],\n",
      "        [0.4043]])\n",
      "Iteration 6130 Training loss 0.11758286505937576 Validation loss 0.1175144761800766 Accuracy 0.6053333282470703\n",
      "Output tensor([[0.4874],\n",
      "        [0.5920]])\n",
      "Iteration 6140 Training loss 0.11981376260519028 Validation loss 0.11760584264993668 Accuracy 0.6043333411216736\n",
      "Output tensor([[0.2749],\n",
      "        [0.5729]])\n",
      "Iteration 6150 Training loss 0.11804137378931046 Validation loss 0.11768074333667755 Accuracy 0.6053333282470703\n",
      "Output tensor([[0.4173],\n",
      "        [0.4090]])\n",
      "Iteration 6160 Training loss 0.11968772113323212 Validation loss 0.11754780262708664 Accuracy 0.6058333516120911\n",
      "Output tensor([[0.4500],\n",
      "        [0.3634]])\n",
      "Iteration 6170 Training loss 0.11820928007364273 Validation loss 0.11752653121948242 Accuracy 0.6048333048820496\n",
      "Output tensor([[0.5527],\n",
      "        [0.4621]])\n",
      "Iteration 6180 Training loss 0.11797202378511429 Validation loss 0.11750970780849457 Accuracy 0.6058333516120911\n",
      "Output tensor([[0.4435],\n",
      "        [0.4906]])\n",
      "Iteration 6190 Training loss 0.11846417188644409 Validation loss 0.11753914505243301 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.3880],\n",
      "        [0.6384]])\n",
      "Iteration 6200 Training loss 0.11914201080799103 Validation loss 0.11750537157058716 Accuracy 0.6046666502952576\n",
      "Output tensor([[0.4163],\n",
      "        [0.4893]])\n",
      "Iteration 6210 Training loss 0.11854331195354462 Validation loss 0.1174732893705368 Accuracy 0.6050000190734863\n",
      "Output tensor([[0.4901],\n",
      "        [0.5369]])\n",
      "Iteration 6220 Training loss 0.11856162548065186 Validation loss 0.11750368028879166 Accuracy 0.6073333621025085\n",
      "Output tensor([[0.4410],\n",
      "        [0.4040]])\n",
      "Iteration 6230 Training loss 0.1189049482345581 Validation loss 0.11748433113098145 Accuracy 0.6053333282470703\n",
      "Output tensor([[0.4558],\n",
      "        [0.4710]])\n",
      "Iteration 6240 Training loss 0.11827891319990158 Validation loss 0.11747711151838303 Accuracy 0.6054999828338623\n",
      "Output tensor([[0.5248],\n",
      "        [0.5273]])\n",
      "Iteration 6250 Training loss 0.11790329962968826 Validation loss 0.11758384108543396 Accuracy 0.6069999933242798\n",
      "Output tensor([[0.5851],\n",
      "        [0.4705]])\n",
      "Iteration 6260 Training loss 0.1184893324971199 Validation loss 0.11759907007217407 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.5168],\n",
      "        [0.4981]])\n",
      "Iteration 6270 Training loss 0.11804291605949402 Validation loss 0.11748508363962173 Accuracy 0.606166660785675\n",
      "Output tensor([[0.6071],\n",
      "        [0.5120]])\n",
      "Iteration 6280 Training loss 0.11850588768720627 Validation loss 0.11756888031959534 Accuracy 0.6054999828338623\n",
      "Output tensor([[0.3306],\n",
      "        [0.4165]])\n",
      "Iteration 6290 Training loss 0.11850977689027786 Validation loss 0.11749447882175446 Accuracy 0.6035000085830688\n",
      "Output tensor([[0.5574],\n",
      "        [0.3706]])\n",
      "Iteration 6300 Training loss 0.11851902306079865 Validation loss 0.11753927916288376 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.5739],\n",
      "        [0.5836]])\n",
      "Iteration 6310 Training loss 0.11913195997476578 Validation loss 0.11746133118867874 Accuracy 0.6054999828338623\n",
      "Output tensor([[0.4879],\n",
      "        [0.5701]])\n",
      "Iteration 6320 Training loss 0.11834285408258438 Validation loss 0.11746688187122345 Accuracy 0.6051666736602783\n",
      "Output tensor([[0.3767],\n",
      "        [0.5497]])\n",
      "Iteration 6330 Training loss 0.11823116987943649 Validation loss 0.1174720898270607 Accuracy 0.606333315372467\n",
      "Output tensor([[0.4422],\n",
      "        [0.5480]])\n",
      "Iteration 6340 Training loss 0.1189042329788208 Validation loss 0.11745062470436096 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.6546],\n",
      "        [0.4357]])\n",
      "Iteration 6350 Training loss 0.11805614084005356 Validation loss 0.11748924851417542 Accuracy 0.6068333387374878\n",
      "Output tensor([[0.3973],\n",
      "        [0.5688]])\n",
      "Iteration 6360 Training loss 0.11991450935602188 Validation loss 0.11752099543809891 Accuracy 0.6053333282470703\n",
      "Output tensor([[0.4278],\n",
      "        [0.3552]])\n",
      "Iteration 6370 Training loss 0.11848181486129761 Validation loss 0.11748619377613068 Accuracy 0.6051666736602783\n",
      "Output tensor([[0.1789],\n",
      "        [0.4681]])\n",
      "Iteration 6380 Training loss 0.11943098902702332 Validation loss 0.1176084354519844 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.5481],\n",
      "        [0.5117]])\n",
      "Iteration 6390 Training loss 0.11844560503959656 Validation loss 0.11746563017368317 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.4928],\n",
      "        [0.5665]])\n",
      "Iteration 6400 Training loss 0.11816156655550003 Validation loss 0.11745410412549973 Accuracy 0.6066666841506958\n",
      "Output tensor([[0.4434],\n",
      "        [0.4375]])\n",
      "Iteration 6410 Training loss 0.11930079013109207 Validation loss 0.11743646115064621 Accuracy 0.6051666736602783\n",
      "Output tensor([[0.4592],\n",
      "        [0.5672]])\n",
      "Iteration 6420 Training loss 0.11906449496746063 Validation loss 0.117481529712677 Accuracy 0.6054999828338623\n",
      "Output tensor([[0.5022],\n",
      "        [0.6207]])\n",
      "Iteration 6430 Training loss 0.1191745176911354 Validation loss 0.11745981127023697 Accuracy 0.6053333282470703\n",
      "Output tensor([[0.5971],\n",
      "        [0.3255]])\n",
      "Iteration 6440 Training loss 0.11900416016578674 Validation loss 0.11746817827224731 Accuracy 0.6048333048820496\n",
      "Output tensor([[0.3558],\n",
      "        [0.5015]])\n",
      "Iteration 6450 Training loss 0.11971978843212128 Validation loss 0.11747241020202637 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.4916],\n",
      "        [0.3914]])\n",
      "Iteration 6460 Training loss 0.11984450370073318 Validation loss 0.11746449768543243 Accuracy 0.6054999828338623\n",
      "Output tensor([[0.5862],\n",
      "        [0.3324]])\n",
      "Iteration 6470 Training loss 0.12000817805528641 Validation loss 0.11750829964876175 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.5274],\n",
      "        [0.4356]])\n",
      "Iteration 6480 Training loss 0.11918821185827255 Validation loss 0.11746079474687576 Accuracy 0.6060000061988831\n",
      "Output tensor([[0.6057],\n",
      "        [0.4964]])\n",
      "Iteration 6490 Training loss 0.11959485709667206 Validation loss 0.11746198683977127 Accuracy 0.6051666736602783\n",
      "Output tensor([[0.3690],\n",
      "        [0.4332]])\n",
      "Iteration 6500 Training loss 0.1191844716668129 Validation loss 0.11744129657745361 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.5611],\n",
      "        [0.4249]])\n",
      "Iteration 6510 Training loss 0.11820309609174728 Validation loss 0.11759412288665771 Accuracy 0.6073333621025085\n",
      "Output tensor([[0.5288],\n",
      "        [0.6162]])\n",
      "Iteration 6520 Training loss 0.12002883106470108 Validation loss 0.11745848506689072 Accuracy 0.6066666841506958\n",
      "Output tensor([[0.5379],\n",
      "        [0.5950]])\n",
      "Iteration 6530 Training loss 0.11890526115894318 Validation loss 0.11744144558906555 Accuracy 0.6068333387374878\n",
      "Output tensor([[0.4259],\n",
      "        [0.4995]])\n",
      "Iteration 6540 Training loss 0.11765962839126587 Validation loss 0.11760178953409195 Accuracy 0.6048333048820496\n",
      "Output tensor([[0.4613],\n",
      "        [0.6008]])\n",
      "Iteration 6550 Training loss 0.11804694682359695 Validation loss 0.11754710227251053 Accuracy 0.6068333387374878\n",
      "Output tensor([[0.5552],\n",
      "        [0.2847]])\n",
      "Iteration 6560 Training loss 0.11868305504322052 Validation loss 0.11740603297948837 Accuracy 0.6066666841506958\n",
      "Output tensor([[0.6180],\n",
      "        [0.2692]])\n",
      "Iteration 6570 Training loss 0.1183612048625946 Validation loss 0.11750294268131256 Accuracy 0.6085000038146973\n",
      "Output tensor([[0.3326],\n",
      "        [0.3433]])\n",
      "Iteration 6580 Training loss 0.11899998039007187 Validation loss 0.11740612983703613 Accuracy 0.6058333516120911\n",
      "Output tensor([[0.4958],\n",
      "        [0.6385]])\n",
      "Iteration 6590 Training loss 0.11827876418828964 Validation loss 0.11741416156291962 Accuracy 0.6058333516120911\n",
      "Output tensor([[0.4191],\n",
      "        [0.4270]])\n",
      "Iteration 6600 Training loss 0.1189999207854271 Validation loss 0.11739549040794373 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.4365],\n",
      "        [0.6134]])\n",
      "Iteration 6610 Training loss 0.11860120296478271 Validation loss 0.11739552766084671 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.4842],\n",
      "        [0.4860]])\n",
      "Iteration 6620 Training loss 0.11851028352975845 Validation loss 0.11742016673088074 Accuracy 0.6054999828338623\n",
      "Output tensor([[0.4336],\n",
      "        [0.6330]])\n",
      "Iteration 6630 Training loss 0.1183733195066452 Validation loss 0.11744511127471924 Accuracy 0.6066666841506958\n",
      "Output tensor([[0.5750],\n",
      "        [0.4813]])\n",
      "Iteration 6640 Training loss 0.11827878654003143 Validation loss 0.11743336170911789 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.4742],\n",
      "        [0.6228]])\n",
      "Iteration 6650 Training loss 0.11820632219314575 Validation loss 0.11740943044424057 Accuracy 0.6058333516120911\n",
      "Output tensor([[0.4715],\n",
      "        [0.5401]])\n",
      "Iteration 6660 Training loss 0.1192130520939827 Validation loss 0.1173979789018631 Accuracy 0.6073333621025085\n",
      "Output tensor([[0.5681],\n",
      "        [0.4376]])\n",
      "Iteration 6670 Training loss 0.11849318444728851 Validation loss 0.11737275868654251 Accuracy 0.606166660785675\n",
      "Output tensor([[0.3694],\n",
      "        [0.6200]])\n",
      "Iteration 6680 Training loss 0.11813880503177643 Validation loss 0.11739540845155716 Accuracy 0.606333315372467\n",
      "Output tensor([[0.5075],\n",
      "        [0.7219]])\n",
      "Iteration 6690 Training loss 0.11845500767230988 Validation loss 0.1173962727189064 Accuracy 0.6060000061988831\n",
      "Output tensor([[0.7439],\n",
      "        [0.5350]])\n",
      "Iteration 6700 Training loss 0.1191556379199028 Validation loss 0.11739595234394073 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.6618],\n",
      "        [0.3523]])\n",
      "Iteration 6710 Training loss 0.11915120482444763 Validation loss 0.11756398528814316 Accuracy 0.606333315372467\n",
      "Output tensor([[0.4904],\n",
      "        [0.5998]])\n",
      "Iteration 6720 Training loss 0.11991403996944427 Validation loss 0.11738511174917221 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.5970],\n",
      "        [0.5237]])\n",
      "Iteration 6730 Training loss 0.11839509755373001 Validation loss 0.11743182688951492 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.5033],\n",
      "        [0.5809]])\n",
      "Iteration 6740 Training loss 0.11888948827981949 Validation loss 0.1173814907670021 Accuracy 0.6068333387374878\n",
      "Output tensor([[0.5980],\n",
      "        [0.2505]])\n",
      "Iteration 6750 Training loss 0.1176767349243164 Validation loss 0.1174401268362999 Accuracy 0.6086666584014893\n",
      "Output tensor([[0.4112],\n",
      "        [0.4806]])\n",
      "Iteration 6760 Training loss 0.11829254031181335 Validation loss 0.11741887778043747 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.6086],\n",
      "        [0.6357]])\n",
      "Iteration 6770 Training loss 0.11829660832881927 Validation loss 0.11741852015256882 Accuracy 0.6054999828338623\n",
      "Output tensor([[0.5672],\n",
      "        [0.4910]])\n",
      "Iteration 6780 Training loss 0.11782649904489517 Validation loss 0.11739198863506317 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.5596],\n",
      "        [0.3480]])\n",
      "Iteration 6790 Training loss 0.11838211119174957 Validation loss 0.11739731580018997 Accuracy 0.6066666841506958\n",
      "Output tensor([[0.3922],\n",
      "        [0.5599]])\n",
      "Iteration 6800 Training loss 0.11786411702632904 Validation loss 0.1173921599984169 Accuracy 0.6068333387374878\n",
      "Output tensor([[0.3834],\n",
      "        [0.5841]])\n",
      "Iteration 6810 Training loss 0.11987859755754471 Validation loss 0.11736951023340225 Accuracy 0.6060000061988831\n",
      "Output tensor([[0.6854],\n",
      "        [0.4776]])\n",
      "Iteration 6820 Training loss 0.11959683895111084 Validation loss 0.11736547946929932 Accuracy 0.6066666841506958\n",
      "Output tensor([[0.1687],\n",
      "        [0.3036]])\n",
      "Iteration 6830 Training loss 0.11929512768983841 Validation loss 0.11735266447067261 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.4631],\n",
      "        [0.5403]])\n",
      "Iteration 6840 Training loss 0.11859822273254395 Validation loss 0.11735513061285019 Accuracy 0.606333315372467\n",
      "Output tensor([[0.5207],\n",
      "        [0.4749]])\n",
      "Iteration 6850 Training loss 0.11877083033323288 Validation loss 0.11734119802713394 Accuracy 0.6066666841506958\n",
      "Output tensor([[0.6406],\n",
      "        [0.5214]])\n",
      "Iteration 6860 Training loss 0.11854821443557739 Validation loss 0.11735058575868607 Accuracy 0.6068333387374878\n",
      "Output tensor([[0.6110],\n",
      "        [0.4890]])\n",
      "Iteration 6870 Training loss 0.1181967556476593 Validation loss 0.11742852628231049 Accuracy 0.6078333258628845\n",
      "Output tensor([[0.6434],\n",
      "        [0.7000]])\n",
      "Iteration 6880 Training loss 0.1187720000743866 Validation loss 0.11738124489784241 Accuracy 0.6060000061988831\n",
      "Output tensor([[0.5470],\n",
      "        [0.3300]])\n",
      "Iteration 6890 Training loss 0.11940557509660721 Validation loss 0.11739296466112137 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.4088],\n",
      "        [0.5636]])\n",
      "Iteration 6900 Training loss 0.11697585880756378 Validation loss 0.11740446090698242 Accuracy 0.606333315372467\n",
      "Output tensor([[0.4298],\n",
      "        [0.4082]])\n",
      "Iteration 6910 Training loss 0.11753837019205093 Validation loss 0.1173754408955574 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.4245],\n",
      "        [0.5002]])\n",
      "Iteration 6920 Training loss 0.11833631247282028 Validation loss 0.11743074655532837 Accuracy 0.606333315372467\n",
      "Output tensor([[0.5284],\n",
      "        [0.5372]])\n",
      "Iteration 6930 Training loss 0.1178010031580925 Validation loss 0.11737202107906342 Accuracy 0.6068333387374878\n",
      "Output tensor([[0.5397],\n",
      "        [0.3614]])\n",
      "Iteration 6940 Training loss 0.1192091703414917 Validation loss 0.11749719083309174 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.4138],\n",
      "        [0.4726]])\n",
      "Iteration 6950 Training loss 0.1184525117278099 Validation loss 0.11741407960653305 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.4314],\n",
      "        [0.4736]])\n",
      "Iteration 6960 Training loss 0.11930941045284271 Validation loss 0.11736717075109482 Accuracy 0.6069999933242798\n",
      "Output tensor([[0.6412],\n",
      "        [0.5631]])\n",
      "Iteration 6970 Training loss 0.11908325552940369 Validation loss 0.1174074336886406 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.5045],\n",
      "        [0.4499]])\n",
      "Iteration 6980 Training loss 0.11847567558288574 Validation loss 0.11738765239715576 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.3404],\n",
      "        [0.4501]])\n",
      "Iteration 6990 Training loss 0.11887432634830475 Validation loss 0.11737305670976639 Accuracy 0.6076666712760925\n",
      "Output tensor([[0.5628],\n",
      "        [0.3860]])\n",
      "Iteration 7000 Training loss 0.11852990835905075 Validation loss 0.11740558594465256 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.5539],\n",
      "        [0.4401]])\n",
      "Iteration 7010 Training loss 0.11902883648872375 Validation loss 0.11733835935592651 Accuracy 0.6060000061988831\n",
      "Output tensor([[0.3615],\n",
      "        [0.5719]])\n",
      "Iteration 7020 Training loss 0.11844302713871002 Validation loss 0.11749176681041718 Accuracy 0.6066666841506958\n",
      "Output tensor([[0.5473],\n",
      "        [0.2776]])\n",
      "Iteration 7030 Training loss 0.11780096590518951 Validation loss 0.11734713613986969 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.2950],\n",
      "        [0.4723]])\n",
      "Iteration 7040 Training loss 0.11796203255653381 Validation loss 0.11732519418001175 Accuracy 0.6073333621025085\n",
      "Output tensor([[0.5686],\n",
      "        [0.5772]])\n",
      "Iteration 7050 Training loss 0.11870795488357544 Validation loss 0.11739345639944077 Accuracy 0.6073333621025085\n",
      "Output tensor([[0.4336],\n",
      "        [0.5377]])\n",
      "Iteration 7060 Training loss 0.11911832541227341 Validation loss 0.11735624074935913 Accuracy 0.6073333621025085\n",
      "Output tensor([[0.4401],\n",
      "        [0.5691]])\n",
      "Iteration 7070 Training loss 0.11915034800767899 Validation loss 0.11732900142669678 Accuracy 0.6073333621025085\n",
      "Output tensor([[0.5600],\n",
      "        [0.4554]])\n",
      "Iteration 7080 Training loss 0.11854750663042068 Validation loss 0.11731605976819992 Accuracy 0.6081666946411133\n",
      "Output tensor([[0.5850],\n",
      "        [0.4789]])\n",
      "Iteration 7090 Training loss 0.11901769042015076 Validation loss 0.1173262968659401 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.2770],\n",
      "        [0.6184]])\n",
      "Iteration 7100 Training loss 0.11747509986162186 Validation loss 0.11729364842176437 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.5943],\n",
      "        [0.6416]])\n",
      "Iteration 7110 Training loss 0.11861374229192734 Validation loss 0.11732210218906403 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.6078],\n",
      "        [0.4141]])\n",
      "Iteration 7120 Training loss 0.11885656416416168 Validation loss 0.11728905886411667 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.5950],\n",
      "        [0.6002]])\n",
      "Iteration 7130 Training loss 0.11900900304317474 Validation loss 0.11729773133993149 Accuracy 0.6069999933242798\n",
      "Output tensor([[0.5188],\n",
      "        [0.5023]])\n",
      "Iteration 7140 Training loss 0.11759275197982788 Validation loss 0.11732596904039383 Accuracy 0.606166660785675\n",
      "Output tensor([[0.5427],\n",
      "        [0.4679]])\n",
      "Iteration 7150 Training loss 0.11817234009504318 Validation loss 0.11738787591457367 Accuracy 0.6078333258628845\n",
      "Output tensor([[0.4379],\n",
      "        [0.5876]])\n",
      "Iteration 7160 Training loss 0.12005345523357391 Validation loss 0.11744441837072372 Accuracy 0.6066666841506958\n",
      "Output tensor([[0.5542],\n",
      "        [0.5521]])\n",
      "Iteration 7170 Training loss 0.11871051788330078 Validation loss 0.11729469150304794 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.3602],\n",
      "        [0.2620]])\n",
      "Iteration 7180 Training loss 0.11852497607469559 Validation loss 0.11728311330080032 Accuracy 0.6076666712760925\n",
      "Output tensor([[0.3495],\n",
      "        [0.4889]])\n",
      "Iteration 7190 Training loss 0.11895914375782013 Validation loss 0.11730688810348511 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.4847],\n",
      "        [0.3123]])\n",
      "Iteration 7200 Training loss 0.1177818775177002 Validation loss 0.11733005940914154 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.4458],\n",
      "        [0.5365]])\n",
      "Iteration 7210 Training loss 0.11949007213115692 Validation loss 0.11736936122179031 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.6806],\n",
      "        [0.5237]])\n",
      "Iteration 7220 Training loss 0.11801759153604507 Validation loss 0.11734312027692795 Accuracy 0.6081666946411133\n",
      "Output tensor([[0.3503],\n",
      "        [0.8232]])\n",
      "Iteration 7230 Training loss 0.11833667010068893 Validation loss 0.11729437112808228 Accuracy 0.6083333492279053\n",
      "Output tensor([[0.3852],\n",
      "        [0.4266]])\n",
      "Iteration 7240 Training loss 0.1181740015745163 Validation loss 0.1173098012804985 Accuracy 0.6056666374206543\n",
      "Output tensor([[0.5857],\n",
      "        [0.5056]])\n",
      "Iteration 7250 Training loss 0.1192534863948822 Validation loss 0.11729263514280319 Accuracy 0.60916668176651\n",
      "Output tensor([[0.5355],\n",
      "        [0.5981]])\n",
      "Iteration 7260 Training loss 0.11911700665950775 Validation loss 0.117286816239357 Accuracy 0.6081666946411133\n",
      "Output tensor([[0.4677],\n",
      "        [0.5109]])\n",
      "Iteration 7270 Training loss 0.11844830214977264 Validation loss 0.11728882044553757 Accuracy 0.6076666712760925\n",
      "Output tensor([[0.7150],\n",
      "        [0.4783]])\n",
      "Iteration 7280 Training loss 0.11861015111207962 Validation loss 0.11733479797840118 Accuracy 0.6076666712760925\n",
      "Output tensor([[0.6314],\n",
      "        [0.5716]])\n",
      "Iteration 7290 Training loss 0.11782446503639221 Validation loss 0.11729011684656143 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.6179],\n",
      "        [0.6268]])\n",
      "Iteration 7300 Training loss 0.1177731528878212 Validation loss 0.11728043854236603 Accuracy 0.6081666946411133\n",
      "Output tensor([[0.5508],\n",
      "        [0.5557]])\n",
      "Iteration 7310 Training loss 0.11874355375766754 Validation loss 0.11730305850505829 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.4343],\n",
      "        [0.5700]])\n",
      "Iteration 7320 Training loss 0.11824332922697067 Validation loss 0.11726748943328857 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.5559],\n",
      "        [0.6283]])\n",
      "Iteration 7330 Training loss 0.11892879009246826 Validation loss 0.11736973375082016 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.3648],\n",
      "        [0.5465]])\n",
      "Iteration 7340 Training loss 0.11807077378034592 Validation loss 0.11728181689977646 Accuracy 0.6069999933242798\n",
      "Output tensor([[0.5890],\n",
      "        [0.5150]])\n",
      "Iteration 7350 Training loss 0.1180364266037941 Validation loss 0.11727871000766754 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.5413],\n",
      "        [0.6705]])\n",
      "Iteration 7360 Training loss 0.11880922317504883 Validation loss 0.11744926124811172 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.5658],\n",
      "        [0.4399]])\n",
      "Iteration 7370 Training loss 0.11849330365657806 Validation loss 0.11734206229448318 Accuracy 0.6068333387374878\n",
      "Output tensor([[0.5657],\n",
      "        [0.4985]])\n",
      "Iteration 7380 Training loss 0.11768926680088043 Validation loss 0.11732399463653564 Accuracy 0.6083333492279053\n",
      "Output tensor([[0.4673],\n",
      "        [0.5229]])\n",
      "Iteration 7390 Training loss 0.11866515129804611 Validation loss 0.11736186593770981 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.4587],\n",
      "        [0.3724]])\n",
      "Iteration 7400 Training loss 0.11870556324720383 Validation loss 0.1173287034034729 Accuracy 0.6081666946411133\n",
      "Output tensor([[0.4324],\n",
      "        [0.5353]])\n",
      "Iteration 7410 Training loss 0.11921530961990356 Validation loss 0.11727841943502426 Accuracy 0.6076666712760925\n",
      "Output tensor([[0.6765],\n",
      "        [0.3064]])\n",
      "Iteration 7420 Training loss 0.11803451180458069 Validation loss 0.11727938055992126 Accuracy 0.6081666946411133\n",
      "Output tensor([[0.5309],\n",
      "        [0.2766]])\n",
      "Iteration 7430 Training loss 0.11887389421463013 Validation loss 0.11726295948028564 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.5210],\n",
      "        [0.5587]])\n",
      "Iteration 7440 Training loss 0.11793521791696548 Validation loss 0.11726739257574081 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.4926],\n",
      "        [0.4970]])\n",
      "Iteration 7450 Training loss 0.11784899234771729 Validation loss 0.11730074137449265 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.5242],\n",
      "        [0.3164]])\n",
      "Iteration 7460 Training loss 0.11860352754592896 Validation loss 0.1172880008816719 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.3867],\n",
      "        [0.5724]])\n",
      "Iteration 7470 Training loss 0.11758314818143845 Validation loss 0.11736898869276047 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.4045],\n",
      "        [0.3112]])\n",
      "Iteration 7480 Training loss 0.11953343451023102 Validation loss 0.11747156083583832 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.6131],\n",
      "        [0.3892]])\n",
      "Iteration 7490 Training loss 0.11868235468864441 Validation loss 0.1173076182603836 Accuracy 0.6069999933242798\n",
      "Output tensor([[0.3599],\n",
      "        [0.6531]])\n",
      "Iteration 7500 Training loss 0.11962990462779999 Validation loss 0.11727278679609299 Accuracy 0.6083333492279053\n",
      "Output tensor([[0.3183],\n",
      "        [0.7112]])\n",
      "Iteration 7510 Training loss 0.1191595122218132 Validation loss 0.11734581738710403 Accuracy 0.6086666584014893\n",
      "Output tensor([[0.4562],\n",
      "        [0.4600]])\n",
      "Iteration 7520 Training loss 0.11811904609203339 Validation loss 0.11729634553194046 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.4763],\n",
      "        [0.5857]])\n",
      "Iteration 7530 Training loss 0.11819778382778168 Validation loss 0.11733786016702652 Accuracy 0.6083333492279053\n",
      "Output tensor([[0.3893],\n",
      "        [0.4256]])\n",
      "Iteration 7540 Training loss 0.1179664134979248 Validation loss 0.11733777076005936 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.4116],\n",
      "        [0.5040]])\n",
      "Iteration 7550 Training loss 0.11910586804151535 Validation loss 0.11726678162813187 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.4843],\n",
      "        [0.5792]])\n",
      "Iteration 7560 Training loss 0.11899496614933014 Validation loss 0.11732570081949234 Accuracy 0.6076666712760925\n",
      "Output tensor([[0.6047],\n",
      "        [0.5418]])\n",
      "Iteration 7570 Training loss 0.1185993030667305 Validation loss 0.11728540062904358 Accuracy 0.6073333621025085\n",
      "Output tensor([[0.5361],\n",
      "        [0.5377]])\n",
      "Iteration 7580 Training loss 0.11839533597230911 Validation loss 0.1172875165939331 Accuracy 0.6069999933242798\n",
      "Output tensor([[0.5934],\n",
      "        [0.4131]])\n",
      "Iteration 7590 Training loss 0.11928369104862213 Validation loss 0.11726649105548859 Accuracy 0.6073333621025085\n",
      "Output tensor([[0.4198],\n",
      "        [0.3108]])\n",
      "Iteration 7600 Training loss 0.11835505068302155 Validation loss 0.11727612465620041 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.5224],\n",
      "        [0.6072]])\n",
      "Iteration 7610 Training loss 0.1192130297422409 Validation loss 0.11727811396121979 Accuracy 0.6081666946411133\n",
      "Output tensor([[0.3842],\n",
      "        [0.5791]])\n",
      "Iteration 7620 Training loss 0.11888729780912399 Validation loss 0.11731243878602982 Accuracy 0.6076666712760925\n",
      "Output tensor([[0.3935],\n",
      "        [0.3945]])\n",
      "Iteration 7630 Training loss 0.1193552240729332 Validation loss 0.11725327372550964 Accuracy 0.6083333492279053\n",
      "Output tensor([[0.4439],\n",
      "        [0.4616]])\n",
      "Iteration 7640 Training loss 0.11965006589889526 Validation loss 0.11725065112113953 Accuracy 0.6086666584014893\n",
      "Output tensor([[0.4250],\n",
      "        [0.4793]])\n",
      "Iteration 7650 Training loss 0.11829119920730591 Validation loss 0.11723555624485016 Accuracy 0.6086666584014893\n",
      "Output tensor([[0.5267],\n",
      "        [0.3774]])\n",
      "Iteration 7660 Training loss 0.11851540952920914 Validation loss 0.11724800616502762 Accuracy 0.6085000038146973\n",
      "Output tensor([[0.5194],\n",
      "        [0.4573]])\n",
      "Iteration 7670 Training loss 0.11833646893501282 Validation loss 0.11724071949720383 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.4938],\n",
      "        [0.5172]])\n",
      "Iteration 7680 Training loss 0.11841277778148651 Validation loss 0.11726412177085876 Accuracy 0.6085000038146973\n",
      "Output tensor([[0.3514],\n",
      "        [0.6000]])\n",
      "Iteration 7690 Training loss 0.11840417236089706 Validation loss 0.1172826737165451 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.4968],\n",
      "        [0.5111]])\n",
      "Iteration 7700 Training loss 0.11904659122228622 Validation loss 0.11723773181438446 Accuracy 0.6085000038146973\n",
      "Output tensor([[0.5595],\n",
      "        [0.4747]])\n",
      "Iteration 7710 Training loss 0.11872059106826782 Validation loss 0.11724463105201721 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.5499],\n",
      "        [0.3991]])\n",
      "Iteration 7720 Training loss 0.11926444619894028 Validation loss 0.11724609136581421 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.2426],\n",
      "        [0.4268]])\n",
      "Iteration 7730 Training loss 0.11755487322807312 Validation loss 0.11728762090206146 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.5374],\n",
      "        [0.4582]])\n",
      "Iteration 7740 Training loss 0.11866314709186554 Validation loss 0.1172393262386322 Accuracy 0.609000027179718\n",
      "Output tensor([[0.5367],\n",
      "        [0.4334]])\n",
      "Iteration 7750 Training loss 0.11958310753107071 Validation loss 0.11726293712854385 Accuracy 0.6081666946411133\n",
      "Output tensor([[0.5186],\n",
      "        [0.5062]])\n",
      "Iteration 7760 Training loss 0.1181420087814331 Validation loss 0.11724279075860977 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.4731],\n",
      "        [0.5806]])\n",
      "Iteration 7770 Training loss 0.11915026605129242 Validation loss 0.11727248132228851 Accuracy 0.6073333621025085\n",
      "Output tensor([[0.5388],\n",
      "        [0.4627]])\n",
      "Iteration 7780 Training loss 0.11889832466840744 Validation loss 0.11727894842624664 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.5285],\n",
      "        [0.5619]])\n",
      "Iteration 7790 Training loss 0.11806438118219376 Validation loss 0.11722377687692642 Accuracy 0.6083333492279053\n",
      "Output tensor([[0.5378],\n",
      "        [0.5168]])\n",
      "Iteration 7800 Training loss 0.11832616478204727 Validation loss 0.11720451712608337 Accuracy 0.60916668176651\n",
      "Output tensor([[0.5443],\n",
      "        [0.4742]])\n",
      "Iteration 7810 Training loss 0.11979621648788452 Validation loss 0.11722453683614731 Accuracy 0.6071666479110718\n",
      "Output tensor([[0.4963],\n",
      "        [0.4435]])\n",
      "Iteration 7820 Training loss 0.11853782087564468 Validation loss 0.11721736937761307 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.3798],\n",
      "        [0.4800]])\n",
      "Iteration 7830 Training loss 0.11820930987596512 Validation loss 0.11720389872789383 Accuracy 0.60916668176651\n",
      "Output tensor([[0.6604],\n",
      "        [0.5919]])\n",
      "Iteration 7840 Training loss 0.11899629980325699 Validation loss 0.11725616455078125 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.4785],\n",
      "        [0.6128]])\n",
      "Iteration 7850 Training loss 0.11885981261730194 Validation loss 0.11722957342863083 Accuracy 0.6086666584014893\n",
      "Output tensor([[0.4759],\n",
      "        [0.2940]])\n",
      "Iteration 7860 Training loss 0.11861041933298111 Validation loss 0.11720737814903259 Accuracy 0.6083333492279053\n",
      "Output tensor([[0.6218],\n",
      "        [0.6031]])\n",
      "Iteration 7870 Training loss 0.11907892674207687 Validation loss 0.11721818894147873 Accuracy 0.609666645526886\n",
      "Output tensor([[0.3910],\n",
      "        [0.6178]])\n",
      "Iteration 7880 Training loss 0.11947876214981079 Validation loss 0.11721485108137131 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.6582],\n",
      "        [0.2513]])\n",
      "Iteration 7890 Training loss 0.1175784021615982 Validation loss 0.11719966679811478 Accuracy 0.6085000038146973\n",
      "Output tensor([[0.4525],\n",
      "        [0.4947]])\n",
      "Iteration 7900 Training loss 0.11864830553531647 Validation loss 0.11718111485242844 Accuracy 0.609499990940094\n",
      "Output tensor([[0.5659],\n",
      "        [0.6441]])\n",
      "Iteration 7910 Training loss 0.11773068457841873 Validation loss 0.11717808991670609 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.4667],\n",
      "        [0.6165]])\n",
      "Iteration 7920 Training loss 0.11798332631587982 Validation loss 0.11718769371509552 Accuracy 0.6101666688919067\n",
      "Output tensor([[0.4255],\n",
      "        [0.5099]])\n",
      "Iteration 7930 Training loss 0.11908081918954849 Validation loss 0.11719398945569992 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.6156],\n",
      "        [0.5672]])\n",
      "Iteration 7940 Training loss 0.11823815852403641 Validation loss 0.11719638854265213 Accuracy 0.6076666712760925\n",
      "Output tensor([[0.5740],\n",
      "        [0.6435]])\n",
      "Iteration 7950 Training loss 0.11841070652008057 Validation loss 0.11719830334186554 Accuracy 0.6083333492279053\n",
      "Output tensor([[0.5065],\n",
      "        [0.4888]])\n",
      "Iteration 7960 Training loss 0.11863866448402405 Validation loss 0.11717304587364197 Accuracy 0.609333336353302\n",
      "Output tensor([[0.5367],\n",
      "        [0.4419]])\n",
      "Iteration 7970 Training loss 0.11895555257797241 Validation loss 0.11718180030584335 Accuracy 0.609000027179718\n",
      "Output tensor([[0.6082],\n",
      "        [0.3785]])\n",
      "Iteration 7980 Training loss 0.11775700002908707 Validation loss 0.11718989908695221 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.2857],\n",
      "        [0.5421]])\n",
      "Iteration 7990 Training loss 0.11781832575798035 Validation loss 0.11716410517692566 Accuracy 0.609333336353302\n",
      "Output tensor([[0.6253],\n",
      "        [0.5244]])\n",
      "Iteration 8000 Training loss 0.11868108808994293 Validation loss 0.11720684915781021 Accuracy 0.6083333492279053\n",
      "Output tensor([[0.4366],\n",
      "        [0.4593]])\n",
      "Iteration 8010 Training loss 0.11832258850336075 Validation loss 0.11724572628736496 Accuracy 0.6081666946411133\n",
      "Output tensor([[0.4278],\n",
      "        [0.3340]])\n",
      "Iteration 8020 Training loss 0.11884786188602448 Validation loss 0.11738795787096024 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.2940],\n",
      "        [0.4715]])\n",
      "Iteration 8030 Training loss 0.11814336478710175 Validation loss 0.11718316376209259 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.6076],\n",
      "        [0.4021]])\n",
      "Iteration 8040 Training loss 0.11831171810626984 Validation loss 0.11718636751174927 Accuracy 0.609000027179718\n",
      "Output tensor([[0.4448],\n",
      "        [0.5262]])\n",
      "Iteration 8050 Training loss 0.1183655634522438 Validation loss 0.11716893315315247 Accuracy 0.60916668176651\n",
      "Output tensor([[0.4109],\n",
      "        [0.5388]])\n",
      "Iteration 8060 Training loss 0.11857299506664276 Validation loss 0.11720424145460129 Accuracy 0.6085000038146973\n",
      "Output tensor([[0.5886],\n",
      "        [0.5499]])\n",
      "Iteration 8070 Training loss 0.11835826933383942 Validation loss 0.11715898662805557 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.5536],\n",
      "        [0.5863]])\n",
      "Iteration 8080 Training loss 0.11880660057067871 Validation loss 0.11716976016759872 Accuracy 0.609499990940094\n",
      "Output tensor([[0.5383],\n",
      "        [0.4085]])\n",
      "Iteration 8090 Training loss 0.11935766786336899 Validation loss 0.1171688437461853 Accuracy 0.6103333234786987\n",
      "Output tensor([[0.5849],\n",
      "        [0.5374]])\n",
      "Iteration 8100 Training loss 0.11937060207128525 Validation loss 0.11718682944774628 Accuracy 0.6086666584014893\n",
      "Output tensor([[0.4078],\n",
      "        [0.5021]])\n",
      "Iteration 8110 Training loss 0.11904092133045197 Validation loss 0.1171678751707077 Accuracy 0.609000027179718\n",
      "Output tensor([[0.6733],\n",
      "        [0.3972]])\n",
      "Iteration 8120 Training loss 0.11793186515569687 Validation loss 0.11719438433647156 Accuracy 0.609666645526886\n",
      "Output tensor([[0.6761],\n",
      "        [0.6231]])\n",
      "Iteration 8130 Training loss 0.11833006888628006 Validation loss 0.11717667430639267 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.3714],\n",
      "        [0.6258]])\n",
      "Iteration 8140 Training loss 0.1190459355711937 Validation loss 0.11717167496681213 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.6166],\n",
      "        [0.5640]])\n",
      "Iteration 8150 Training loss 0.11871250718832016 Validation loss 0.11717018485069275 Accuracy 0.609499990940094\n",
      "Output tensor([[0.5125],\n",
      "        [0.3066]])\n",
      "Iteration 8160 Training loss 0.11787061393260956 Validation loss 0.11721199750900269 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.4126],\n",
      "        [0.4679]])\n",
      "Iteration 8170 Training loss 0.1181500032544136 Validation loss 0.11729419231414795 Accuracy 0.6085000038146973\n",
      "Output tensor([[0.3141],\n",
      "        [0.4064]])\n",
      "Iteration 8180 Training loss 0.11929453164339066 Validation loss 0.11715460568666458 Accuracy 0.6086666584014893\n",
      "Output tensor([[0.2829],\n",
      "        [0.5059]])\n",
      "Iteration 8190 Training loss 0.11836452037096024 Validation loss 0.11734882742166519 Accuracy 0.6083333492279053\n",
      "Output tensor([[0.5331],\n",
      "        [0.5369]])\n",
      "Iteration 8200 Training loss 0.11883455514907837 Validation loss 0.11718001961708069 Accuracy 0.6100000143051147\n",
      "Output tensor([[0.5428],\n",
      "        [0.5178]])\n",
      "Iteration 8210 Training loss 0.11911211907863617 Validation loss 0.11714820563793182 Accuracy 0.6085000038146973\n",
      "Output tensor([[0.6223],\n",
      "        [0.3872]])\n",
      "Iteration 8220 Training loss 0.11856604367494583 Validation loss 0.11715681850910187 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.6019],\n",
      "        [0.4976]])\n",
      "Iteration 8230 Training loss 0.11953086405992508 Validation loss 0.1171942949295044 Accuracy 0.6079999804496765\n",
      "Output tensor([[0.5575],\n",
      "        [0.2370]])\n",
      "Iteration 8240 Training loss 0.11794964969158173 Validation loss 0.11713537573814392 Accuracy 0.60916668176651\n",
      "Output tensor([[0.4538],\n",
      "        [0.5215]])\n",
      "Iteration 8250 Training loss 0.11799155920743942 Validation loss 0.1171371191740036 Accuracy 0.609666645526886\n",
      "Output tensor([[0.6197],\n",
      "        [0.4596]])\n",
      "Iteration 8260 Training loss 0.11849107593297958 Validation loss 0.11714181303977966 Accuracy 0.609000027179718\n",
      "Output tensor([[0.5513],\n",
      "        [0.3262]])\n",
      "Iteration 8270 Training loss 0.11907245218753815 Validation loss 0.11753599345684052 Accuracy 0.6078333258628845\n",
      "Output tensor([[0.4356],\n",
      "        [0.5539]])\n",
      "Iteration 8280 Training loss 0.11910886317491531 Validation loss 0.11715281009674072 Accuracy 0.609333336353302\n",
      "Output tensor([[0.5276],\n",
      "        [0.6190]])\n",
      "Iteration 8290 Training loss 0.11774499714374542 Validation loss 0.11713403463363647 Accuracy 0.6086666584014893\n",
      "Output tensor([[0.5096],\n",
      "        [0.2695]])\n",
      "Iteration 8300 Training loss 0.11754816770553589 Validation loss 0.117149718105793 Accuracy 0.6085000038146973\n",
      "Output tensor([[0.5683],\n",
      "        [0.6711]])\n",
      "Iteration 8310 Training loss 0.1188063845038414 Validation loss 0.11719346046447754 Accuracy 0.6085000038146973\n",
      "Output tensor([[0.3800],\n",
      "        [0.4417]])\n",
      "Iteration 8320 Training loss 0.11886732280254364 Validation loss 0.11714684218168259 Accuracy 0.6081666946411133\n",
      "Output tensor([[0.6155],\n",
      "        [0.4456]])\n",
      "Iteration 8330 Training loss 0.11813293397426605 Validation loss 0.11714968830347061 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.6661],\n",
      "        [0.4350]])\n",
      "Iteration 8340 Training loss 0.11848438531160355 Validation loss 0.11720219254493713 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.4092],\n",
      "        [0.4345]])\n",
      "Iteration 8350 Training loss 0.1187991127371788 Validation loss 0.11715839803218842 Accuracy 0.609000027179718\n",
      "Output tensor([[0.5522],\n",
      "        [0.5910]])\n",
      "Iteration 8360 Training loss 0.11879150569438934 Validation loss 0.11712092906236649 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.4464],\n",
      "        [0.3094]])\n",
      "Iteration 8370 Training loss 0.12010496109724045 Validation loss 0.11712510883808136 Accuracy 0.6100000143051147\n",
      "Output tensor([[0.1758],\n",
      "        [0.6563]])\n",
      "Iteration 8380 Training loss 0.1175791397690773 Validation loss 0.11710374802350998 Accuracy 0.609499990940094\n",
      "Output tensor([[0.5345],\n",
      "        [0.5082]])\n",
      "Iteration 8390 Training loss 0.11952318996191025 Validation loss 0.1170932799577713 Accuracy 0.609499990940094\n",
      "Output tensor([[0.2824],\n",
      "        [0.2555]])\n",
      "Iteration 8400 Training loss 0.11723700165748596 Validation loss 0.11710958182811737 Accuracy 0.6103333234786987\n",
      "Output tensor([[0.4854],\n",
      "        [0.6030]])\n",
      "Iteration 8410 Training loss 0.11844010651111603 Validation loss 0.11715441942214966 Accuracy 0.6088333129882812\n",
      "Output tensor([[0.3286],\n",
      "        [0.4149]])\n",
      "Iteration 8420 Training loss 0.11867515742778778 Validation loss 0.11710266768932343 Accuracy 0.609000027179718\n",
      "Output tensor([[0.6478],\n",
      "        [0.4281]])\n",
      "Iteration 8430 Training loss 0.11838093400001526 Validation loss 0.11709102243185043 Accuracy 0.6104999780654907\n",
      "Output tensor([[0.7050],\n",
      "        [0.4126]])\n",
      "Iteration 8440 Training loss 0.11803135275840759 Validation loss 0.11707752197980881 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.5681],\n",
      "        [0.4457]])\n",
      "Iteration 8450 Training loss 0.11884738504886627 Validation loss 0.11714887619018555 Accuracy 0.609499990940094\n",
      "Output tensor([[0.5708],\n",
      "        [0.3115]])\n",
      "Iteration 8460 Training loss 0.11804285645484924 Validation loss 0.11709944903850555 Accuracy 0.609499990940094\n",
      "Output tensor([[0.3605],\n",
      "        [0.4981]])\n",
      "Iteration 8470 Training loss 0.11852691322565079 Validation loss 0.11708656698465347 Accuracy 0.6086666584014893\n",
      "Output tensor([[0.4493],\n",
      "        [0.5424]])\n",
      "Iteration 8480 Training loss 0.11858639866113663 Validation loss 0.11714351177215576 Accuracy 0.609666645526886\n",
      "Output tensor([[0.5691],\n",
      "        [0.5531]])\n",
      "Iteration 8490 Training loss 0.11801141500473022 Validation loss 0.11709088087081909 Accuracy 0.6103333234786987\n",
      "Output tensor([[0.5822],\n",
      "        [0.4361]])\n",
      "Iteration 8500 Training loss 0.11946526914834976 Validation loss 0.11713174730539322 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.5528],\n",
      "        [0.4152]])\n",
      "Iteration 8510 Training loss 0.11871067434549332 Validation loss 0.11715519428253174 Accuracy 0.609000027179718\n",
      "Output tensor([[0.5909],\n",
      "        [0.7121]])\n",
      "Iteration 8520 Training loss 0.11839080601930618 Validation loss 0.11707399785518646 Accuracy 0.609499990940094\n",
      "Output tensor([[0.4299],\n",
      "        [0.5479]])\n",
      "Iteration 8530 Training loss 0.11790398508310318 Validation loss 0.11706815659999847 Accuracy 0.6106666922569275\n",
      "Output tensor([[0.6776],\n",
      "        [0.4490]])\n",
      "Iteration 8540 Training loss 0.1182878389954567 Validation loss 0.11708573251962662 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.5596],\n",
      "        [0.5779]])\n",
      "Iteration 8550 Training loss 0.11919870227575302 Validation loss 0.11708541959524155 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.7528],\n",
      "        [0.2985]])\n",
      "Iteration 8560 Training loss 0.11789780855178833 Validation loss 0.11716896295547485 Accuracy 0.6103333234786987\n",
      "Output tensor([[0.5239],\n",
      "        [0.5007]])\n",
      "Iteration 8570 Training loss 0.11867355555295944 Validation loss 0.11707822978496552 Accuracy 0.609333336353302\n",
      "Output tensor([[0.4383],\n",
      "        [0.5769]])\n",
      "Iteration 8580 Training loss 0.11835751682519913 Validation loss 0.1170569658279419 Accuracy 0.609666645526886\n",
      "Output tensor([[0.5182],\n",
      "        [0.4515]])\n",
      "Iteration 8590 Training loss 0.11879418045282364 Validation loss 0.11714889854192734 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.4609],\n",
      "        [0.5817]])\n",
      "Iteration 8600 Training loss 0.11855822056531906 Validation loss 0.11708714812994003 Accuracy 0.609499990940094\n",
      "Output tensor([[0.6141],\n",
      "        [0.5142]])\n",
      "Iteration 8610 Training loss 0.11812617629766464 Validation loss 0.11708924174308777 Accuracy 0.6115000247955322\n",
      "Output tensor([[0.4476],\n",
      "        [0.3736]])\n",
      "Iteration 8620 Training loss 0.11789681762456894 Validation loss 0.11710433661937714 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.4050],\n",
      "        [0.5478]])\n",
      "Iteration 8630 Training loss 0.11882340162992477 Validation loss 0.11705446243286133 Accuracy 0.609666645526886\n",
      "Output tensor([[0.3686],\n",
      "        [0.3408]])\n",
      "Iteration 8640 Training loss 0.1190277710556984 Validation loss 0.11704693734645844 Accuracy 0.6110000014305115\n",
      "Output tensor([[0.6536],\n",
      "        [0.3399]])\n",
      "Iteration 8650 Training loss 0.1190543994307518 Validation loss 0.11704928427934647 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.6278],\n",
      "        [0.5846]])\n",
      "Iteration 8660 Training loss 0.11709819734096527 Validation loss 0.11707635223865509 Accuracy 0.6104999780654907\n",
      "Output tensor([[0.7322],\n",
      "        [0.6017]])\n",
      "Iteration 8670 Training loss 0.11893532425165176 Validation loss 0.11707130819559097 Accuracy 0.6101666688919067\n",
      "Output tensor([[0.5145],\n",
      "        [0.5094]])\n",
      "Iteration 8680 Training loss 0.11847878247499466 Validation loss 0.1170341968536377 Accuracy 0.609499990940094\n",
      "Output tensor([[0.5687],\n",
      "        [0.4684]])\n",
      "Iteration 8690 Training loss 0.11839514970779419 Validation loss 0.11703541874885559 Accuracy 0.6103333234786987\n",
      "Output tensor([[0.5705],\n",
      "        [0.4205]])\n",
      "Iteration 8700 Training loss 0.11847367137670517 Validation loss 0.11701583117246628 Accuracy 0.6115000247955322\n",
      "Output tensor([[0.4730],\n",
      "        [0.4971]])\n",
      "Iteration 8710 Training loss 0.11806685477495193 Validation loss 0.11699896305799484 Accuracy 0.6106666922569275\n",
      "Output tensor([[0.5245],\n",
      "        [0.6762]])\n",
      "Iteration 8720 Training loss 0.1177666112780571 Validation loss 0.11701139062643051 Accuracy 0.6110000014305115\n",
      "Output tensor([[0.5957],\n",
      "        [0.5758]])\n",
      "Iteration 8730 Training loss 0.11857615411281586 Validation loss 0.1170971468091011 Accuracy 0.6104999780654907\n",
      "Output tensor([[0.4796],\n",
      "        [0.3520]])\n",
      "Iteration 8740 Training loss 0.11820279806852341 Validation loss 0.11713100224733353 Accuracy 0.6100000143051147\n",
      "Output tensor([[0.5940],\n",
      "        [0.6383]])\n",
      "Iteration 8750 Training loss 0.11837995052337646 Validation loss 0.11728765815496445 Accuracy 0.606166660785675\n",
      "Output tensor([[0.3804],\n",
      "        [0.4250]])\n",
      "Iteration 8760 Training loss 0.11770003288984299 Validation loss 0.11707298457622528 Accuracy 0.6101666688919067\n",
      "Output tensor([[0.5147],\n",
      "        [0.5728]])\n",
      "Iteration 8770 Training loss 0.11829062551259995 Validation loss 0.11709441989660263 Accuracy 0.609499990940094\n",
      "Output tensor([[0.6024],\n",
      "        [0.4988]])\n",
      "Iteration 8780 Training loss 0.1182541474699974 Validation loss 0.11704792082309723 Accuracy 0.6100000143051147\n",
      "Output tensor([[0.6371],\n",
      "        [0.2322]])\n",
      "Iteration 8790 Training loss 0.11798899620771408 Validation loss 0.11704705655574799 Accuracy 0.609000027179718\n",
      "Output tensor([[0.5262],\n",
      "        [0.6666]])\n",
      "Iteration 8800 Training loss 0.11826448887586594 Validation loss 0.11706097424030304 Accuracy 0.6100000143051147\n",
      "Output tensor([[0.3209],\n",
      "        [0.5743]])\n",
      "Iteration 8810 Training loss 0.11801394820213318 Validation loss 0.11704958230257034 Accuracy 0.609333336353302\n",
      "Output tensor([[0.5800],\n",
      "        [0.6779]])\n",
      "Iteration 8820 Training loss 0.11807572096586227 Validation loss 0.11704106628894806 Accuracy 0.609000027179718\n",
      "Output tensor([[0.5253],\n",
      "        [0.4010]])\n",
      "Iteration 8830 Training loss 0.11892315745353699 Validation loss 0.1171225756406784 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.4888],\n",
      "        [0.5392]])\n",
      "Iteration 8840 Training loss 0.11870896071195602 Validation loss 0.11705473065376282 Accuracy 0.6100000143051147\n",
      "Output tensor([[0.5352],\n",
      "        [0.5691]])\n",
      "Iteration 8850 Training loss 0.11928489059209824 Validation loss 0.11706247925758362 Accuracy 0.609000027179718\n",
      "Output tensor([[0.6144],\n",
      "        [0.4364]])\n",
      "Iteration 8860 Training loss 0.11788664013147354 Validation loss 0.11711469292640686 Accuracy 0.609499990940094\n",
      "Output tensor([[0.4645],\n",
      "        [0.6036]])\n",
      "Iteration 8870 Training loss 0.11802871525287628 Validation loss 0.11710754036903381 Accuracy 0.609333336353302\n",
      "Output tensor([[0.4452],\n",
      "        [0.2069]])\n",
      "Iteration 8880 Training loss 0.12009913474321365 Validation loss 0.11703290045261383 Accuracy 0.609666645526886\n",
      "Output tensor([[0.4580],\n",
      "        [0.2359]])\n",
      "Iteration 8890 Training loss 0.11756101250648499 Validation loss 0.11701633781194687 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.4266],\n",
      "        [0.5544]])\n",
      "Iteration 8900 Training loss 0.1181391179561615 Validation loss 0.11701497435569763 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.5512],\n",
      "        [0.5619]])\n",
      "Iteration 8910 Training loss 0.1180533617734909 Validation loss 0.11700620502233505 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.6294],\n",
      "        [0.4373]])\n",
      "Iteration 8920 Training loss 0.11729007214307785 Validation loss 0.11699517071247101 Accuracy 0.6104999780654907\n",
      "Output tensor([[0.3980],\n",
      "        [0.4719]])\n",
      "Iteration 8930 Training loss 0.11824307590723038 Validation loss 0.11702773720026016 Accuracy 0.6110000014305115\n",
      "Output tensor([[0.3297],\n",
      "        [0.4455]])\n",
      "Iteration 8940 Training loss 0.11878346651792526 Validation loss 0.11709851771593094 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.4796],\n",
      "        [0.3033]])\n",
      "Iteration 8950 Training loss 0.11863533407449722 Validation loss 0.11702018231153488 Accuracy 0.6104999780654907\n",
      "Output tensor([[0.5148],\n",
      "        [0.4826]])\n",
      "Iteration 8960 Training loss 0.11869718134403229 Validation loss 0.11705268174409866 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.5772],\n",
      "        [0.6135]])\n",
      "Iteration 8970 Training loss 0.11828739196062088 Validation loss 0.11709432303905487 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.5130],\n",
      "        [0.3671]])\n",
      "Iteration 8980 Training loss 0.1186755895614624 Validation loss 0.11710911244153976 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.2683],\n",
      "        [0.4132]])\n",
      "Iteration 8990 Training loss 0.11808506399393082 Validation loss 0.1170080229640007 Accuracy 0.609666645526886\n",
      "Output tensor([[0.4718],\n",
      "        [0.5732]])\n",
      "Iteration 9000 Training loss 0.1184171512722969 Validation loss 0.11706667393445969 Accuracy 0.6106666922569275\n",
      "Output tensor([[0.5031],\n",
      "        [0.3933]])\n",
      "Iteration 9010 Training loss 0.11833701282739639 Validation loss 0.11699340492486954 Accuracy 0.6103333234786987\n",
      "Output tensor([[0.6524],\n",
      "        [0.5299]])\n",
      "Iteration 9020 Training loss 0.11812689155340195 Validation loss 0.11698978394269943 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.5207],\n",
      "        [0.5284]])\n",
      "Iteration 9030 Training loss 0.11805842071771622 Validation loss 0.11699899286031723 Accuracy 0.609666645526886\n",
      "Output tensor([[0.4478],\n",
      "        [0.4802]])\n",
      "Iteration 9040 Training loss 0.11732104420661926 Validation loss 0.11699292808771133 Accuracy 0.6104999780654907\n",
      "Output tensor([[0.3135],\n",
      "        [0.6159]])\n",
      "Iteration 9050 Training loss 0.11933764815330505 Validation loss 0.11708743125200272 Accuracy 0.6101666688919067\n",
      "Output tensor([[0.5468],\n",
      "        [0.3971]])\n",
      "Iteration 9060 Training loss 0.11808984726667404 Validation loss 0.1170513704419136 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.5869],\n",
      "        [0.3962]])\n",
      "Iteration 9070 Training loss 0.11779219657182693 Validation loss 0.11699823290109634 Accuracy 0.609666645526886\n",
      "Output tensor([[0.3326],\n",
      "        [0.6932]])\n",
      "Iteration 9080 Training loss 0.1181718111038208 Validation loss 0.11699167639017105 Accuracy 0.6104999780654907\n",
      "Output tensor([[0.4000],\n",
      "        [0.4690]])\n",
      "Iteration 9090 Training loss 0.11848308145999908 Validation loss 0.11699078232049942 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.2855],\n",
      "        [0.6581]])\n",
      "Iteration 9100 Training loss 0.11944995075464249 Validation loss 0.11701628565788269 Accuracy 0.6104999780654907\n",
      "Output tensor([[0.6958],\n",
      "        [0.5313]])\n",
      "Iteration 9110 Training loss 0.11873053014278412 Validation loss 0.11701387166976929 Accuracy 0.6086666584014893\n",
      "Output tensor([[0.5254],\n",
      "        [0.5761]])\n",
      "Iteration 9120 Training loss 0.11910524219274521 Validation loss 0.11703190207481384 Accuracy 0.609499990940094\n",
      "Output tensor([[0.4288],\n",
      "        [0.4785]])\n",
      "Iteration 9130 Training loss 0.11936154961585999 Validation loss 0.11700927466154099 Accuracy 0.6106666922569275\n",
      "Output tensor([[0.5910],\n",
      "        [0.5934]])\n",
      "Iteration 9140 Training loss 0.11832400411367416 Validation loss 0.11702536791563034 Accuracy 0.609000027179718\n",
      "Output tensor([[0.4720],\n",
      "        [0.5856]])\n",
      "Iteration 9150 Training loss 0.11917824298143387 Validation loss 0.11705251038074493 Accuracy 0.609666645526886\n",
      "Output tensor([[0.6315],\n",
      "        [0.4055]])\n",
      "Iteration 9160 Training loss 0.11944896727800369 Validation loss 0.1170240044593811 Accuracy 0.60916668176651\n",
      "Output tensor([[0.3292],\n",
      "        [0.4871]])\n",
      "Iteration 9170 Training loss 0.11792484670877457 Validation loss 0.11704836785793304 Accuracy 0.6110000014305115\n",
      "Output tensor([[0.3988],\n",
      "        [0.6204]])\n",
      "Iteration 9180 Training loss 0.11720206588506699 Validation loss 0.11699780076742172 Accuracy 0.609666645526886\n",
      "Output tensor([[0.5016],\n",
      "        [0.5310]])\n",
      "Iteration 9190 Training loss 0.11794409900903702 Validation loss 0.11716872453689575 Accuracy 0.609666645526886\n",
      "Output tensor([[0.4488],\n",
      "        [0.6524]])\n",
      "Iteration 9200 Training loss 0.11724597960710526 Validation loss 0.1170569434762001 Accuracy 0.6101666688919067\n",
      "Output tensor([[0.5341],\n",
      "        [0.6072]])\n",
      "Iteration 9210 Training loss 0.11805101484060287 Validation loss 0.11705181747674942 Accuracy 0.6083333492279053\n",
      "Output tensor([[0.6347],\n",
      "        [0.6363]])\n",
      "Iteration 9220 Training loss 0.11904982477426529 Validation loss 0.11710482090711594 Accuracy 0.6103333234786987\n",
      "Output tensor([[0.5317],\n",
      "        [0.4663]])\n",
      "Iteration 9230 Training loss 0.11711188405752182 Validation loss 0.11705438047647476 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.4184],\n",
      "        [0.4173]])\n",
      "Iteration 9240 Training loss 0.11841721832752228 Validation loss 0.11713014543056488 Accuracy 0.609666645526886\n",
      "Output tensor([[0.6179],\n",
      "        [0.4529]])\n",
      "Iteration 9250 Training loss 0.11841636896133423 Validation loss 0.11703955382108688 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.2802],\n",
      "        [0.5506]])\n",
      "Iteration 9260 Training loss 0.11824636906385422 Validation loss 0.11712159216403961 Accuracy 0.6101666688919067\n",
      "Output tensor([[0.2617],\n",
      "        [0.6024]])\n",
      "Iteration 9270 Training loss 0.11919400095939636 Validation loss 0.11699961125850677 Accuracy 0.609666645526886\n",
      "Output tensor([[0.4848],\n",
      "        [0.4244]])\n",
      "Iteration 9280 Training loss 0.11799953132867813 Validation loss 0.11701390892267227 Accuracy 0.6111666560173035\n",
      "Output tensor([[0.2185],\n",
      "        [0.6232]])\n",
      "Iteration 9290 Training loss 0.11851683259010315 Validation loss 0.11703867465257645 Accuracy 0.6116666793823242\n",
      "Output tensor([[0.5259],\n",
      "        [0.5637]])\n",
      "Iteration 9300 Training loss 0.11772086471319199 Validation loss 0.11711732298135757 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.4769],\n",
      "        [0.4276]])\n",
      "Iteration 9310 Training loss 0.11886429041624069 Validation loss 0.11698222160339355 Accuracy 0.6104999780654907\n",
      "Output tensor([[0.5862],\n",
      "        [0.2682]])\n",
      "Iteration 9320 Training loss 0.11795365065336227 Validation loss 0.11696565896272659 Accuracy 0.6106666922569275\n",
      "Output tensor([[0.6079],\n",
      "        [0.3034]])\n",
      "Iteration 9330 Training loss 0.11794859915971756 Validation loss 0.1170106828212738 Accuracy 0.6118333339691162\n",
      "Output tensor([[0.4227],\n",
      "        [0.3001]])\n",
      "Iteration 9340 Training loss 0.11755876988172531 Validation loss 0.11696874350309372 Accuracy 0.609666645526886\n",
      "Output tensor([[0.6757],\n",
      "        [0.5892]])\n",
      "Iteration 9350 Training loss 0.11844222247600555 Validation loss 0.11696048825979233 Accuracy 0.6100000143051147\n",
      "Output tensor([[0.3668],\n",
      "        [0.5376]])\n",
      "Iteration 9360 Training loss 0.11810176819562912 Validation loss 0.11703070253133774 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.5859],\n",
      "        [0.4465]])\n",
      "Iteration 9370 Training loss 0.11800654977560043 Validation loss 0.11702162772417068 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.5236],\n",
      "        [0.5192]])\n",
      "Iteration 9380 Training loss 0.11774221062660217 Validation loss 0.11695775389671326 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.3775],\n",
      "        [0.3356]])\n",
      "Iteration 9390 Training loss 0.11848051100969315 Validation loss 0.11707283556461334 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.4895],\n",
      "        [0.6286]])\n",
      "Iteration 9400 Training loss 0.11825615167617798 Validation loss 0.11696828156709671 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.6083],\n",
      "        [0.5173]])\n",
      "Iteration 9410 Training loss 0.118723064661026 Validation loss 0.11695139855146408 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.5484],\n",
      "        [0.4370]])\n",
      "Iteration 9420 Training loss 0.11833886802196503 Validation loss 0.11699569970369339 Accuracy 0.6115000247955322\n",
      "Output tensor([[0.5740],\n",
      "        [0.5388]])\n",
      "Iteration 9430 Training loss 0.1186150535941124 Validation loss 0.11692158877849579 Accuracy 0.609666645526886\n",
      "Output tensor([[0.3553],\n",
      "        [0.4296]])\n",
      "Iteration 9440 Training loss 0.1178397387266159 Validation loss 0.11697257310152054 Accuracy 0.6110000014305115\n",
      "Output tensor([[0.3847],\n",
      "        [0.5800]])\n",
      "Iteration 9450 Training loss 0.11799314618110657 Validation loss 0.1169644370675087 Accuracy 0.6110000014305115\n",
      "Output tensor([[0.6147],\n",
      "        [0.4901]])\n",
      "Iteration 9460 Training loss 0.11862961947917938 Validation loss 0.1169566735625267 Accuracy 0.6101666688919067\n",
      "Output tensor([[0.6216],\n",
      "        [0.6799]])\n",
      "Iteration 9470 Training loss 0.11865799129009247 Validation loss 0.11696912348270416 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.5887],\n",
      "        [0.5838]])\n",
      "Iteration 9480 Training loss 0.11914283037185669 Validation loss 0.11697012186050415 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.4381],\n",
      "        [0.4162]])\n",
      "Iteration 9490 Training loss 0.11949263513088226 Validation loss 0.11694851517677307 Accuracy 0.609499990940094\n",
      "Output tensor([[0.5538],\n",
      "        [0.5156]])\n",
      "Iteration 9500 Training loss 0.11844713985919952 Validation loss 0.11695263534784317 Accuracy 0.609499990940094\n",
      "Output tensor([[0.5021],\n",
      "        [0.4176]])\n",
      "Iteration 9510 Training loss 0.11849775165319443 Validation loss 0.11693886667490005 Accuracy 0.6081666946411133\n",
      "Output tensor([[0.5034],\n",
      "        [0.6497]])\n",
      "Iteration 9520 Training loss 0.11857519298791885 Validation loss 0.1169448271393776 Accuracy 0.6103333234786987\n",
      "Output tensor([[0.2428],\n",
      "        [0.4649]])\n",
      "Iteration 9530 Training loss 0.1179775595664978 Validation loss 0.11694341897964478 Accuracy 0.6111666560173035\n",
      "Output tensor([[0.3581],\n",
      "        [0.5895]])\n",
      "Iteration 9540 Training loss 0.11815983057022095 Validation loss 0.11694315820932388 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.6396],\n",
      "        [0.5821]])\n",
      "Iteration 9550 Training loss 0.11777462810277939 Validation loss 0.11697318404912949 Accuracy 0.6101666688919067\n",
      "Output tensor([[0.3859],\n",
      "        [0.5700]])\n",
      "Iteration 9560 Training loss 0.11827033758163452 Validation loss 0.1169724389910698 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.4088],\n",
      "        [0.4052]])\n",
      "Iteration 9570 Training loss 0.11769308149814606 Validation loss 0.11691971123218536 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.4631],\n",
      "        [0.4287]])\n",
      "Iteration 9580 Training loss 0.11813214421272278 Validation loss 0.11695306748151779 Accuracy 0.6101666688919067\n",
      "Output tensor([[0.4615],\n",
      "        [0.5879]])\n",
      "Iteration 9590 Training loss 0.11777423322200775 Validation loss 0.11701621860265732 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.3284],\n",
      "        [0.5696]])\n",
      "Iteration 9600 Training loss 0.11741125583648682 Validation loss 0.11695830523967743 Accuracy 0.6115000247955322\n",
      "Output tensor([[0.3891],\n",
      "        [0.4552]])\n",
      "Iteration 9610 Training loss 0.11926611512899399 Validation loss 0.11693867295980453 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.4929],\n",
      "        [0.4239]])\n",
      "Iteration 9620 Training loss 0.11886326223611832 Validation loss 0.1169249638915062 Accuracy 0.6098333597183228\n",
      "Output tensor([[0.6420],\n",
      "        [0.5870]])\n",
      "Iteration 9630 Training loss 0.1184612289071083 Validation loss 0.11694157123565674 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.6207],\n",
      "        [0.5266]])\n",
      "Iteration 9640 Training loss 0.11701561510562897 Validation loss 0.11691635847091675 Accuracy 0.6115000247955322\n",
      "Output tensor([[0.7126],\n",
      "        [0.3514]])\n",
      "Iteration 9650 Training loss 0.11774511635303497 Validation loss 0.11694508045911789 Accuracy 0.6116666793823242\n",
      "Output tensor([[0.6368],\n",
      "        [0.5053]])\n",
      "Iteration 9660 Training loss 0.11889622360467911 Validation loss 0.11695613712072372 Accuracy 0.6118333339691162\n",
      "Output tensor([[0.5166],\n",
      "        [0.4190]])\n",
      "Iteration 9670 Training loss 0.11848970502614975 Validation loss 0.11693380028009415 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.5770],\n",
      "        [0.5023]])\n",
      "Iteration 9680 Training loss 0.11829660832881927 Validation loss 0.116963692009449 Accuracy 0.612666666507721\n",
      "Output tensor([[0.5718],\n",
      "        [0.3553]])\n",
      "Iteration 9690 Training loss 0.11765827238559723 Validation loss 0.11696995049715042 Accuracy 0.6103333234786987\n",
      "Output tensor([[0.4748],\n",
      "        [0.5573]])\n",
      "Iteration 9700 Training loss 0.11945540457963943 Validation loss 0.11690033227205276 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.5568],\n",
      "        [0.6117]])\n",
      "Iteration 9710 Training loss 0.11780800670385361 Validation loss 0.11691541224718094 Accuracy 0.6116666793823242\n",
      "Output tensor([[0.5264],\n",
      "        [0.5220]])\n",
      "Iteration 9720 Training loss 0.11872896552085876 Validation loss 0.1169249638915062 Accuracy 0.6115000247955322\n",
      "Output tensor([[0.4728],\n",
      "        [0.4069]])\n",
      "Iteration 9730 Training loss 0.1175449937582016 Validation loss 0.11702711880207062 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.5842],\n",
      "        [0.3632]])\n",
      "Iteration 9740 Training loss 0.11839359253644943 Validation loss 0.11691942065954208 Accuracy 0.612666666507721\n",
      "Output tensor([[0.5761],\n",
      "        [0.4368]])\n",
      "Iteration 9750 Training loss 0.11861244589090347 Validation loss 0.11687619984149933 Accuracy 0.6111666560173035\n",
      "Output tensor([[0.3916],\n",
      "        [0.4026]])\n",
      "Iteration 9760 Training loss 0.11827737092971802 Validation loss 0.11693891137838364 Accuracy 0.612500011920929\n",
      "Output tensor([[0.5943],\n",
      "        [0.5270]])\n",
      "Iteration 9770 Training loss 0.11929573863744736 Validation loss 0.11693422496318817 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.4477],\n",
      "        [0.5900]])\n",
      "Iteration 9780 Training loss 0.11736635118722916 Validation loss 0.11689003556966782 Accuracy 0.6101666688919067\n",
      "Output tensor([[0.4789],\n",
      "        [0.5798]])\n",
      "Iteration 9790 Training loss 0.11832226812839508 Validation loss 0.11691340804100037 Accuracy 0.6104999780654907\n",
      "Output tensor([[0.5436],\n",
      "        [0.4632]])\n",
      "Iteration 9800 Training loss 0.11880481243133545 Validation loss 0.11689649522304535 Accuracy 0.6103333234786987\n",
      "Output tensor([[0.5506],\n",
      "        [0.3394]])\n",
      "Iteration 9810 Training loss 0.11882322281599045 Validation loss 0.11698821932077408 Accuracy 0.6110000014305115\n",
      "Output tensor([[0.5088],\n",
      "        [0.6247]])\n",
      "Iteration 9820 Training loss 0.11789151281118393 Validation loss 0.11687703430652618 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.4264],\n",
      "        [0.4288]])\n",
      "Iteration 9830 Training loss 0.11798741668462753 Validation loss 0.11692902445793152 Accuracy 0.612333357334137\n",
      "Output tensor([[0.2364],\n",
      "        [0.3818]])\n",
      "Iteration 9840 Training loss 0.11891290545463562 Validation loss 0.11695947498083115 Accuracy 0.6111666560173035\n",
      "Output tensor([[0.7036],\n",
      "        [0.4671]])\n",
      "Iteration 9850 Training loss 0.1171233281493187 Validation loss 0.1168694868683815 Accuracy 0.6110000014305115\n",
      "Output tensor([[0.6366],\n",
      "        [0.7005]])\n",
      "Iteration 9860 Training loss 0.11739892512559891 Validation loss 0.1168728768825531 Accuracy 0.6111666560173035\n",
      "Output tensor([[0.5017],\n",
      "        [0.4482]])\n",
      "Iteration 9870 Training loss 0.11824692785739899 Validation loss 0.11685249954462051 Accuracy 0.6110000014305115\n",
      "Output tensor([[0.6379],\n",
      "        [0.5155]])\n",
      "Iteration 9880 Training loss 0.11844988912343979 Validation loss 0.1168452650308609 Accuracy 0.6116666793823242\n",
      "Output tensor([[0.5100],\n",
      "        [0.5096]])\n",
      "Iteration 9890 Training loss 0.11912857741117477 Validation loss 0.11690445989370346 Accuracy 0.612666666507721\n",
      "Output tensor([[0.2923],\n",
      "        [0.3429]])\n",
      "Iteration 9900 Training loss 0.11780621111392975 Validation loss 0.11687159538269043 Accuracy 0.612500011920929\n",
      "Output tensor([[0.4793],\n",
      "        [0.4089]])\n",
      "Iteration 9910 Training loss 0.11827267706394196 Validation loss 0.11690118163824081 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.5205],\n",
      "        [0.5928]])\n",
      "Iteration 9920 Training loss 0.11746635288000107 Validation loss 0.11691281944513321 Accuracy 0.6118333339691162\n",
      "Output tensor([[0.6499],\n",
      "        [0.4364]])\n",
      "Iteration 9930 Training loss 0.11964979022741318 Validation loss 0.11688745766878128 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.3413],\n",
      "        [0.4912]])\n",
      "Iteration 9940 Training loss 0.11869553476572037 Validation loss 0.11684955656528473 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.5134],\n",
      "        [0.5369]])\n",
      "Iteration 9950 Training loss 0.11833876371383667 Validation loss 0.11686040461063385 Accuracy 0.612333357334137\n",
      "Output tensor([[0.3669],\n",
      "        [0.5979]])\n",
      "Iteration 9960 Training loss 0.1186813935637474 Validation loss 0.11685891449451447 Accuracy 0.609666645526886\n",
      "Output tensor([[0.4904],\n",
      "        [0.4219]])\n",
      "Iteration 9970 Training loss 0.11787866055965424 Validation loss 0.11690423637628555 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.3011],\n",
      "        [0.1926]])\n",
      "Iteration 9980 Training loss 0.11735866963863373 Validation loss 0.11692937463521957 Accuracy 0.6118333339691162\n",
      "Output tensor([[0.4681],\n",
      "        [0.4344]])\n",
      "Iteration 9990 Training loss 0.1182619035243988 Validation loss 0.11689114570617676 Accuracy 0.6106666922569275\n",
      "Output tensor([[0.5623],\n",
      "        [0.2823]])\n",
      "Iteration 10000 Training loss 0.11841892451047897 Validation loss 0.11688489466905594 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.5811],\n",
      "        [0.6123]])\n",
      "Iteration 10010 Training loss 0.1181989312171936 Validation loss 0.11687786877155304 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.3879],\n",
      "        [0.4138]])\n",
      "Iteration 10020 Training loss 0.11792988330125809 Validation loss 0.11686486750841141 Accuracy 0.6103333234786987\n",
      "Output tensor([[0.7538],\n",
      "        [0.6299]])\n",
      "Iteration 10030 Training loss 0.1182786226272583 Validation loss 0.1168985590338707 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.6356],\n",
      "        [0.3769]])\n",
      "Iteration 10040 Training loss 0.11850292235612869 Validation loss 0.11694171279668808 Accuracy 0.6110000014305115\n",
      "Output tensor([[0.4011],\n",
      "        [0.5211]])\n",
      "Iteration 10050 Training loss 0.11819199472665787 Validation loss 0.11691145598888397 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.5611],\n",
      "        [0.4506]])\n",
      "Iteration 10060 Training loss 0.11649397760629654 Validation loss 0.11686808615922928 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.6563],\n",
      "        [0.4584]])\n",
      "Iteration 10070 Training loss 0.11919043213129044 Validation loss 0.11694204807281494 Accuracy 0.6118333339691162\n",
      "Output tensor([[0.4260],\n",
      "        [0.7173]])\n",
      "Iteration 10080 Training loss 0.11788506805896759 Validation loss 0.11691518872976303 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.3696],\n",
      "        [0.3781]])\n",
      "Iteration 10090 Training loss 0.11828674376010895 Validation loss 0.11688478291034698 Accuracy 0.6118333339691162\n",
      "Output tensor([[0.5924],\n",
      "        [0.4307]])\n",
      "Iteration 10100 Training loss 0.11930535733699799 Validation loss 0.11710561066865921 Accuracy 0.609333336353302\n",
      "Output tensor([[0.4434],\n",
      "        [0.4380]])\n",
      "Iteration 10110 Training loss 0.11764881759881973 Validation loss 0.11685127019882202 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.4472],\n",
      "        [0.3398]])\n",
      "Iteration 10120 Training loss 0.11822384595870972 Validation loss 0.11684698611497879 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.4584],\n",
      "        [0.2724]])\n",
      "Iteration 10130 Training loss 0.1179443895816803 Validation loss 0.11679016053676605 Accuracy 0.6103333234786987\n",
      "Output tensor([[0.6061],\n",
      "        [0.5241]])\n",
      "Iteration 10140 Training loss 0.11765309423208237 Validation loss 0.11680282652378082 Accuracy 0.6116666793823242\n",
      "Output tensor([[0.3280],\n",
      "        [0.5465]])\n",
      "Iteration 10150 Training loss 0.11860283464193344 Validation loss 0.11683342605829239 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.5314],\n",
      "        [0.5065]])\n",
      "Iteration 10160 Training loss 0.11890330910682678 Validation loss 0.11680572479963303 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.5227],\n",
      "        [0.2335]])\n",
      "Iteration 10170 Training loss 0.11757821589708328 Validation loss 0.116805799305439 Accuracy 0.6101666688919067\n",
      "Output tensor([[0.5489],\n",
      "        [0.6827]])\n",
      "Iteration 10180 Training loss 0.1176907867193222 Validation loss 0.11681277304887772 Accuracy 0.6104999780654907\n",
      "Output tensor([[0.3905],\n",
      "        [0.7266]])\n",
      "Iteration 10190 Training loss 0.11865866929292679 Validation loss 0.11688343435525894 Accuracy 0.612666666507721\n",
      "Output tensor([[0.3862],\n",
      "        [0.4030]])\n",
      "Iteration 10200 Training loss 0.11705311387777328 Validation loss 0.11679563671350479 Accuracy 0.6100000143051147\n",
      "Output tensor([[0.4595],\n",
      "        [0.4870]])\n",
      "Iteration 10210 Training loss 0.11803450435400009 Validation loss 0.11692027747631073 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.5779],\n",
      "        [0.5076]])\n",
      "Iteration 10220 Training loss 0.11809014528989792 Validation loss 0.11681878566741943 Accuracy 0.612666666507721\n",
      "Output tensor([[0.6547],\n",
      "        [0.4067]])\n",
      "Iteration 10230 Training loss 0.11768798530101776 Validation loss 0.11689363420009613 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.5478],\n",
      "        [0.5359]])\n",
      "Iteration 10240 Training loss 0.11816290020942688 Validation loss 0.1169549748301506 Accuracy 0.6115000247955322\n",
      "Output tensor([[0.3270],\n",
      "        [0.5805]])\n",
      "Iteration 10250 Training loss 0.11822517961263657 Validation loss 0.11688610166311264 Accuracy 0.6116666793823242\n",
      "Output tensor([[0.4413],\n",
      "        [0.6391]])\n",
      "Iteration 10260 Training loss 0.11816143989562988 Validation loss 0.1168101578950882 Accuracy 0.6100000143051147\n",
      "Output tensor([[0.4188],\n",
      "        [0.6519]])\n",
      "Iteration 10270 Training loss 0.11647545546293259 Validation loss 0.11679739505052567 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.3824],\n",
      "        [0.4174]])\n",
      "Iteration 10280 Training loss 0.11875584721565247 Validation loss 0.11687158048152924 Accuracy 0.612333357334137\n",
      "Output tensor([[0.3193],\n",
      "        [0.3350]])\n",
      "Iteration 10290 Training loss 0.11877219378948212 Validation loss 0.11693478375673294 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.6568],\n",
      "        [0.5543]])\n",
      "Iteration 10300 Training loss 0.11813664436340332 Validation loss 0.11685965210199356 Accuracy 0.612333357334137\n",
      "Output tensor([[0.2030],\n",
      "        [0.6444]])\n",
      "Iteration 10310 Training loss 0.1181662529706955 Validation loss 0.11683081090450287 Accuracy 0.612333357334137\n",
      "Output tensor([[0.5453],\n",
      "        [0.6012]])\n",
      "Iteration 10320 Training loss 0.11859564483165741 Validation loss 0.11681797355413437 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.5128],\n",
      "        [0.3066]])\n",
      "Iteration 10330 Training loss 0.11856287717819214 Validation loss 0.11680731922388077 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.4015],\n",
      "        [0.5715]])\n",
      "Iteration 10340 Training loss 0.11740709096193314 Validation loss 0.11685892939567566 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.3633],\n",
      "        [0.1740]])\n",
      "Iteration 10350 Training loss 0.1184341236948967 Validation loss 0.11682529002428055 Accuracy 0.6110000014305115\n",
      "Output tensor([[0.5352],\n",
      "        [0.3920]])\n",
      "Iteration 10360 Training loss 0.1182064488530159 Validation loss 0.11685622483491898 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.4367],\n",
      "        [0.4375]])\n",
      "Iteration 10370 Training loss 0.11878666281700134 Validation loss 0.11689919978380203 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.4418],\n",
      "        [0.5196]])\n",
      "Iteration 10380 Training loss 0.11796486377716064 Validation loss 0.11682292819023132 Accuracy 0.6116666793823242\n",
      "Output tensor([[0.5981],\n",
      "        [0.4376]])\n",
      "Iteration 10390 Training loss 0.11803222447633743 Validation loss 0.11681805551052094 Accuracy 0.6118333339691162\n",
      "Output tensor([[0.5379],\n",
      "        [0.5818]])\n",
      "Iteration 10400 Training loss 0.11874347180128098 Validation loss 0.11681777238845825 Accuracy 0.612666666507721\n",
      "Output tensor([[0.5867],\n",
      "        [0.5428]])\n",
      "Iteration 10410 Training loss 0.11780029535293579 Validation loss 0.11685875803232193 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.4543],\n",
      "        [0.6020]])\n",
      "Iteration 10420 Training loss 0.11755988746881485 Validation loss 0.11680129170417786 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.3240],\n",
      "        [0.6886]])\n",
      "Iteration 10430 Training loss 0.11895611882209778 Validation loss 0.11682192981243134 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.4625],\n",
      "        [0.6313]])\n",
      "Iteration 10440 Training loss 0.11791156232357025 Validation loss 0.11678167432546616 Accuracy 0.612500011920929\n",
      "Output tensor([[0.5251],\n",
      "        [0.6520]])\n",
      "Iteration 10450 Training loss 0.11850651353597641 Validation loss 0.11678194254636765 Accuracy 0.612333357334137\n",
      "Output tensor([[0.6579],\n",
      "        [0.4597]])\n",
      "Iteration 10460 Training loss 0.11794213950634003 Validation loss 0.116785429418087 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.6479],\n",
      "        [0.4789]])\n",
      "Iteration 10470 Training loss 0.11775748431682587 Validation loss 0.11687523126602173 Accuracy 0.6111666560173035\n",
      "Output tensor([[0.3257],\n",
      "        [0.5212]])\n",
      "Iteration 10480 Training loss 0.11814404278993607 Validation loss 0.11683487892150879 Accuracy 0.6116666793823242\n",
      "Output tensor([[0.3608],\n",
      "        [0.5435]])\n",
      "Iteration 10490 Training loss 0.11861708760261536 Validation loss 0.11684556305408478 Accuracy 0.6118333339691162\n",
      "Output tensor([[0.4407],\n",
      "        [0.5248]])\n",
      "Iteration 10500 Training loss 0.11782383173704147 Validation loss 0.11683826893568039 Accuracy 0.6116666793823242\n",
      "Output tensor([[0.3824],\n",
      "        [0.3593]])\n",
      "Iteration 10510 Training loss 0.11803556978702545 Validation loss 0.11687881499528885 Accuracy 0.6115000247955322\n",
      "Output tensor([[0.4118],\n",
      "        [0.5026]])\n",
      "Iteration 10520 Training loss 0.11841017752885818 Validation loss 0.11682344228029251 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.2184],\n",
      "        [0.5483]])\n",
      "Iteration 10530 Training loss 0.11719317734241486 Validation loss 0.11678452789783478 Accuracy 0.6115000247955322\n",
      "Output tensor([[0.5370],\n",
      "        [0.5099]])\n",
      "Iteration 10540 Training loss 0.11776835471391678 Validation loss 0.11680001020431519 Accuracy 0.6115000247955322\n",
      "Output tensor([[0.4748],\n",
      "        [0.5920]])\n",
      "Iteration 10550 Training loss 0.11893665790557861 Validation loss 0.11686934530735016 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.5573],\n",
      "        [0.4210]])\n",
      "Iteration 10560 Training loss 0.11785772442817688 Validation loss 0.11680073291063309 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.3803],\n",
      "        [0.5379]])\n",
      "Iteration 10570 Training loss 0.11772707104682922 Validation loss 0.11687050014734268 Accuracy 0.6118333339691162\n",
      "Output tensor([[0.4379],\n",
      "        [0.5763]])\n",
      "Iteration 10580 Training loss 0.11797299236059189 Validation loss 0.11679024249315262 Accuracy 0.612666666507721\n",
      "Output tensor([[0.5260],\n",
      "        [0.4930]])\n",
      "Iteration 10590 Training loss 0.11844977736473083 Validation loss 0.11676838248968124 Accuracy 0.6111666560173035\n",
      "Output tensor([[0.3745],\n",
      "        [0.5879]])\n",
      "Iteration 10600 Training loss 0.11794836074113846 Validation loss 0.11679438501596451 Accuracy 0.612666666507721\n",
      "Output tensor([[0.4386],\n",
      "        [0.2902]])\n",
      "Iteration 10610 Training loss 0.11735383421182632 Validation loss 0.11678323149681091 Accuracy 0.6110000014305115\n",
      "Output tensor([[0.5900],\n",
      "        [0.2523]])\n",
      "Iteration 10620 Training loss 0.11871103942394257 Validation loss 0.11677428334951401 Accuracy 0.6115000247955322\n",
      "Output tensor([[0.4881],\n",
      "        [0.4034]])\n",
      "Iteration 10630 Training loss 0.11678214371204376 Validation loss 0.11678631603717804 Accuracy 0.6118333339691162\n",
      "Output tensor([[0.4201],\n",
      "        [0.5343]])\n",
      "Iteration 10640 Training loss 0.11736277490854263 Validation loss 0.11683890968561172 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.3335],\n",
      "        [0.3934]])\n",
      "Iteration 10650 Training loss 0.11805751174688339 Validation loss 0.11675560474395752 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5043],\n",
      "        [0.5758]])\n",
      "Iteration 10660 Training loss 0.11837039887905121 Validation loss 0.1168181374669075 Accuracy 0.612333357334137\n",
      "Output tensor([[0.5747],\n",
      "        [0.5045]])\n",
      "Iteration 10670 Training loss 0.11766690760850906 Validation loss 0.11675894260406494 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.6318],\n",
      "        [0.5978]])\n",
      "Iteration 10680 Training loss 0.11819148808717728 Validation loss 0.11674772202968597 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.5113],\n",
      "        [0.2390]])\n",
      "Iteration 10690 Training loss 0.11820702999830246 Validation loss 0.11674050986766815 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.6806],\n",
      "        [0.2506]])\n",
      "Iteration 10700 Training loss 0.11774575710296631 Validation loss 0.11671199649572372 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.3540],\n",
      "        [0.5846]])\n",
      "Iteration 10710 Training loss 0.11728476732969284 Validation loss 0.11688674241304398 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.3759],\n",
      "        [0.5112]])\n",
      "Iteration 10720 Training loss 0.11795912683010101 Validation loss 0.11674097925424576 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.6210],\n",
      "        [0.5867]])\n",
      "Iteration 10730 Training loss 0.11798134446144104 Validation loss 0.11673841625452042 Accuracy 0.612500011920929\n",
      "Output tensor([[0.4572],\n",
      "        [0.5422]])\n",
      "Iteration 10740 Training loss 0.1183498427271843 Validation loss 0.11676377058029175 Accuracy 0.612500011920929\n",
      "Output tensor([[0.5701],\n",
      "        [0.4954]])\n",
      "Iteration 10750 Training loss 0.11824722588062286 Validation loss 0.11691348999738693 Accuracy 0.6111666560173035\n",
      "Output tensor([[0.5958],\n",
      "        [0.6378]])\n",
      "Iteration 10760 Training loss 0.11816637217998505 Validation loss 0.1167755126953125 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5086],\n",
      "        [0.4614]])\n",
      "Iteration 10770 Training loss 0.11878527700901031 Validation loss 0.11677414178848267 Accuracy 0.612500011920929\n",
      "Output tensor([[0.7005],\n",
      "        [0.3490]])\n",
      "Iteration 10780 Training loss 0.11782194674015045 Validation loss 0.11677230149507523 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.4876],\n",
      "        [0.6639]])\n",
      "Iteration 10790 Training loss 0.11822546273469925 Validation loss 0.11676395684480667 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.5034],\n",
      "        [0.5468]])\n",
      "Iteration 10800 Training loss 0.11916857957839966 Validation loss 0.11674793809652328 Accuracy 0.612500011920929\n",
      "Output tensor([[0.6194],\n",
      "        [0.5590]])\n",
      "Iteration 10810 Training loss 0.11814618110656738 Validation loss 0.11678715795278549 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5654],\n",
      "        [0.5933]])\n",
      "Iteration 10820 Training loss 0.11846082657575607 Validation loss 0.11675911396741867 Accuracy 0.612500011920929\n",
      "Output tensor([[0.5811],\n",
      "        [0.5370]])\n",
      "Iteration 10830 Training loss 0.11785814166069031 Validation loss 0.11676687747240067 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.2946],\n",
      "        [0.2549]])\n",
      "Iteration 10840 Training loss 0.11771698296070099 Validation loss 0.11677683144807816 Accuracy 0.612333357334137\n",
      "Output tensor([[0.4630],\n",
      "        [0.5937]])\n",
      "Iteration 10850 Training loss 0.1182665005326271 Validation loss 0.11674661934375763 Accuracy 0.6111666560173035\n",
      "Output tensor([[0.5550],\n",
      "        [0.5020]])\n",
      "Iteration 10860 Training loss 0.11767144501209259 Validation loss 0.11673279851675034 Accuracy 0.6110000014305115\n",
      "Output tensor([[0.4396],\n",
      "        [0.5441]])\n",
      "Iteration 10870 Training loss 0.11773327738046646 Validation loss 0.11673695594072342 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.5834],\n",
      "        [0.5962]])\n",
      "Iteration 10880 Training loss 0.11687617748975754 Validation loss 0.11674204468727112 Accuracy 0.612333357334137\n",
      "Output tensor([[0.4831],\n",
      "        [0.5908]])\n",
      "Iteration 10890 Training loss 0.11864838749170303 Validation loss 0.11673697829246521 Accuracy 0.6113333106040955\n",
      "Output tensor([[0.5414],\n",
      "        [0.3978]])\n",
      "Iteration 10900 Training loss 0.11895053088665009 Validation loss 0.11681733280420303 Accuracy 0.612500011920929\n",
      "Output tensor([[0.4814],\n",
      "        [0.4770]])\n",
      "Iteration 10910 Training loss 0.11829259991645813 Validation loss 0.11671734601259232 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.6151],\n",
      "        [0.5319]])\n",
      "Iteration 10920 Training loss 0.11776482313871384 Validation loss 0.11673875153064728 Accuracy 0.6118333339691162\n",
      "Output tensor([[0.5671],\n",
      "        [0.6406]])\n",
      "Iteration 10930 Training loss 0.11754345893859863 Validation loss 0.11674661189317703 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5679],\n",
      "        [0.5133]])\n",
      "Iteration 10940 Training loss 0.11794740706682205 Validation loss 0.11674094945192337 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.5985],\n",
      "        [0.5094]])\n",
      "Iteration 10950 Training loss 0.11731692403554916 Validation loss 0.11675406992435455 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5361],\n",
      "        [0.5597]])\n",
      "Iteration 10960 Training loss 0.11904919147491455 Validation loss 0.11673958599567413 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.2857],\n",
      "        [0.4995]])\n",
      "Iteration 10970 Training loss 0.11837600916624069 Validation loss 0.11674564331769943 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5365],\n",
      "        [0.5303]])\n",
      "Iteration 10980 Training loss 0.11748719215393066 Validation loss 0.11676719039678574 Accuracy 0.612666666507721\n",
      "Output tensor([[0.5678],\n",
      "        [0.5061]])\n",
      "Iteration 10990 Training loss 0.11677435040473938 Validation loss 0.11671990156173706 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.6479],\n",
      "        [0.4337]])\n",
      "Iteration 11000 Training loss 0.11835995316505432 Validation loss 0.11670862883329391 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.4518],\n",
      "        [0.3107]])\n",
      "Iteration 11010 Training loss 0.11839862167835236 Validation loss 0.11669375747442245 Accuracy 0.612666666507721\n",
      "Output tensor([[0.4912],\n",
      "        [0.4183]])\n",
      "Iteration 11020 Training loss 0.11725029349327087 Validation loss 0.11672487109899521 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.6348],\n",
      "        [0.5624]])\n",
      "Iteration 11030 Training loss 0.11838690936565399 Validation loss 0.1167091503739357 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.6691],\n",
      "        [0.4336]])\n",
      "Iteration 11040 Training loss 0.11828934401273727 Validation loss 0.11669560521841049 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.2863],\n",
      "        [0.3096]])\n",
      "Iteration 11050 Training loss 0.11849012970924377 Validation loss 0.11668799817562103 Accuracy 0.612666666507721\n",
      "Output tensor([[0.3458],\n",
      "        [0.4660]])\n",
      "Iteration 11060 Training loss 0.11641260981559753 Validation loss 0.1167062371969223 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4940],\n",
      "        [0.4060]])\n",
      "Iteration 11070 Training loss 0.11883148550987244 Validation loss 0.11676672101020813 Accuracy 0.612666666507721\n",
      "Output tensor([[0.5899],\n",
      "        [0.5096]])\n",
      "Iteration 11080 Training loss 0.1173539087176323 Validation loss 0.11676029115915298 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.2940],\n",
      "        [0.4416]])\n",
      "Iteration 11090 Training loss 0.11747444421052933 Validation loss 0.11671307682991028 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.6697],\n",
      "        [0.5199]])\n",
      "Iteration 11100 Training loss 0.1176890954375267 Validation loss 0.11670433729887009 Accuracy 0.612666666507721\n",
      "Output tensor([[0.6618],\n",
      "        [0.4081]])\n",
      "Iteration 11110 Training loss 0.11723495274782181 Validation loss 0.1167127788066864 Accuracy 0.612333357334137\n",
      "Output tensor([[0.5186],\n",
      "        [0.4750]])\n",
      "Iteration 11120 Training loss 0.1186363697052002 Validation loss 0.11674632877111435 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.4160],\n",
      "        [0.6814]])\n",
      "Iteration 11130 Training loss 0.11859215050935745 Validation loss 0.11671152710914612 Accuracy 0.612666666507721\n",
      "Output tensor([[0.4695],\n",
      "        [0.5574]])\n",
      "Iteration 11140 Training loss 0.11693894118070602 Validation loss 0.11672666668891907 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.4810],\n",
      "        [0.5448]])\n",
      "Iteration 11150 Training loss 0.11841806769371033 Validation loss 0.1167312040925026 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.3783],\n",
      "        [0.3186]])\n",
      "Iteration 11160 Training loss 0.11750704050064087 Validation loss 0.11669912934303284 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.5571],\n",
      "        [0.3987]])\n",
      "Iteration 11170 Training loss 0.11783432960510254 Validation loss 0.11685404181480408 Accuracy 0.6118333339691162\n",
      "Output tensor([[0.4715],\n",
      "        [0.5823]])\n",
      "Iteration 11180 Training loss 0.11803900450468063 Validation loss 0.11669478565454483 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.4581],\n",
      "        [0.5198]])\n",
      "Iteration 11190 Training loss 0.11825622618198395 Validation loss 0.11668974161148071 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5923],\n",
      "        [0.3586]])\n",
      "Iteration 11200 Training loss 0.11823561787605286 Validation loss 0.116689033806324 Accuracy 0.612333357334137\n",
      "Output tensor([[0.4842],\n",
      "        [0.3469]])\n",
      "Iteration 11210 Training loss 0.11847049742937088 Validation loss 0.11670244485139847 Accuracy 0.612333357334137\n",
      "Output tensor([[0.4748],\n",
      "        [0.1596]])\n",
      "Iteration 11220 Training loss 0.11836006492376328 Validation loss 0.11667508631944656 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.5103],\n",
      "        [0.7659]])\n",
      "Iteration 11230 Training loss 0.11825108528137207 Validation loss 0.11668992042541504 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.3984],\n",
      "        [0.5260]])\n",
      "Iteration 11240 Training loss 0.11791309714317322 Validation loss 0.11666519939899445 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4459],\n",
      "        [0.5575]])\n",
      "Iteration 11250 Training loss 0.11817128211259842 Validation loss 0.11678270250558853 Accuracy 0.612666666507721\n",
      "Output tensor([[0.7314],\n",
      "        [0.5930]])\n",
      "Iteration 11260 Training loss 0.11832874268293381 Validation loss 0.11671938747167587 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.5491],\n",
      "        [0.4434]])\n",
      "Iteration 11270 Training loss 0.11720824241638184 Validation loss 0.11668835580348969 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.4928],\n",
      "        [0.6202]])\n",
      "Iteration 11280 Training loss 0.11878371983766556 Validation loss 0.11681900173425674 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.5883],\n",
      "        [0.2398]])\n",
      "Iteration 11290 Training loss 0.11790250986814499 Validation loss 0.1166996881365776 Accuracy 0.6115000247955322\n",
      "Output tensor([[0.4741],\n",
      "        [0.5092]])\n",
      "Iteration 11300 Training loss 0.11729787290096283 Validation loss 0.11666455864906311 Accuracy 0.6116666793823242\n",
      "Output tensor([[0.5816],\n",
      "        [0.3235]])\n",
      "Iteration 11310 Training loss 0.11718983948230743 Validation loss 0.11686045676469803 Accuracy 0.6108333468437195\n",
      "Output tensor([[0.5053],\n",
      "        [0.6882]])\n",
      "Iteration 11320 Training loss 0.11788974702358246 Validation loss 0.11666412651538849 Accuracy 0.612500011920929\n",
      "Output tensor([[0.5175],\n",
      "        [0.2448]])\n",
      "Iteration 11330 Training loss 0.11868974566459656 Validation loss 0.11666382104158401 Accuracy 0.6118333339691162\n",
      "Output tensor([[0.3137],\n",
      "        [0.4680]])\n",
      "Iteration 11340 Training loss 0.11741902679204941 Validation loss 0.11666368693113327 Accuracy 0.6116666793823242\n",
      "Output tensor([[0.5049],\n",
      "        [0.4869]])\n",
      "Iteration 11350 Training loss 0.11760382354259491 Validation loss 0.11665372550487518 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.4846],\n",
      "        [0.5646]])\n",
      "Iteration 11360 Training loss 0.11653964966535568 Validation loss 0.11668320745229721 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.4769],\n",
      "        [0.5546]])\n",
      "Iteration 11370 Training loss 0.11803591251373291 Validation loss 0.1166442483663559 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.3761],\n",
      "        [0.6121]])\n",
      "Iteration 11380 Training loss 0.11764216423034668 Validation loss 0.11666569113731384 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.6114],\n",
      "        [0.2141]])\n",
      "Iteration 11390 Training loss 0.11831721663475037 Validation loss 0.11682110279798508 Accuracy 0.612500011920929\n",
      "Output tensor([[0.5056],\n",
      "        [0.2293]])\n",
      "Iteration 11400 Training loss 0.11737604439258575 Validation loss 0.11668436974287033 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.5877],\n",
      "        [0.3356]])\n",
      "Iteration 11410 Training loss 0.11772619932889938 Validation loss 0.11672447621822357 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.5899],\n",
      "        [0.5189]])\n",
      "Iteration 11420 Training loss 0.11835487931966782 Validation loss 0.1166762113571167 Accuracy 0.612333357334137\n",
      "Output tensor([[0.5234],\n",
      "        [0.3986]])\n",
      "Iteration 11430 Training loss 0.11840083450078964 Validation loss 0.11673636734485626 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.2490],\n",
      "        [0.5778]])\n",
      "Iteration 11440 Training loss 0.11724837124347687 Validation loss 0.11665625125169754 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.5459],\n",
      "        [0.5385]])\n",
      "Iteration 11450 Training loss 0.11854162812232971 Validation loss 0.11670341342687607 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5869],\n",
      "        [0.4361]])\n",
      "Iteration 11460 Training loss 0.11772533506155014 Validation loss 0.11674325913190842 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.3200],\n",
      "        [0.5168]])\n",
      "Iteration 11470 Training loss 0.11877378076314926 Validation loss 0.11665541678667068 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.5540],\n",
      "        [0.4984]])\n",
      "Iteration 11480 Training loss 0.11799495667219162 Validation loss 0.11664336174726486 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4911],\n",
      "        [0.2259]])\n",
      "Iteration 11490 Training loss 0.11732513457536697 Validation loss 0.11667934060096741 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.2560],\n",
      "        [0.3921]])\n",
      "Iteration 11500 Training loss 0.11763934046030045 Validation loss 0.11674875020980835 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.2747],\n",
      "        [0.3954]])\n",
      "Iteration 11510 Training loss 0.11812683194875717 Validation loss 0.11666439473628998 Accuracy 0.612500011920929\n",
      "Output tensor([[0.4877],\n",
      "        [0.3978]])\n",
      "Iteration 11520 Training loss 0.1187315285205841 Validation loss 0.11666335165500641 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.5270],\n",
      "        [0.5107]])\n",
      "Iteration 11530 Training loss 0.11759094148874283 Validation loss 0.11666329205036163 Accuracy 0.612333357334137\n",
      "Output tensor([[0.2018],\n",
      "        [0.6594]])\n",
      "Iteration 11540 Training loss 0.11734122782945633 Validation loss 0.11665628105401993 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5842],\n",
      "        [0.6672]])\n",
      "Iteration 11550 Training loss 0.11816222965717316 Validation loss 0.11667266488075256 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.4253],\n",
      "        [0.4701]])\n",
      "Iteration 11560 Training loss 0.11780843138694763 Validation loss 0.11665996164083481 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.4269],\n",
      "        [0.3856]])\n",
      "Iteration 11570 Training loss 0.11730168014764786 Validation loss 0.11667691916227341 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.3222],\n",
      "        [0.5363]])\n",
      "Iteration 11580 Training loss 0.1171422153711319 Validation loss 0.11669286340475082 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.7074],\n",
      "        [0.4468]])\n",
      "Iteration 11590 Training loss 0.1184040755033493 Validation loss 0.11666858196258545 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.2638],\n",
      "        [0.6559]])\n",
      "Iteration 11600 Training loss 0.11773773282766342 Validation loss 0.11664924025535583 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5609],\n",
      "        [0.4733]])\n",
      "Iteration 11610 Training loss 0.11800868064165115 Validation loss 0.11664135009050369 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.4604],\n",
      "        [0.6477]])\n",
      "Iteration 11620 Training loss 0.1184011846780777 Validation loss 0.11663097143173218 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.4349],\n",
      "        [0.5214]])\n",
      "Iteration 11630 Training loss 0.11691845953464508 Validation loss 0.11666081845760345 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.5417],\n",
      "        [0.5784]])\n",
      "Iteration 11640 Training loss 0.11798607558012009 Validation loss 0.11672583222389221 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4999],\n",
      "        [0.4411]])\n",
      "Iteration 11650 Training loss 0.11717582494020462 Validation loss 0.11664236336946487 Accuracy 0.612666666507721\n",
      "Output tensor([[0.6381],\n",
      "        [0.4461]])\n",
      "Iteration 11660 Training loss 0.11878804862499237 Validation loss 0.11665951460599899 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5911],\n",
      "        [0.5057]])\n",
      "Iteration 11670 Training loss 0.11752252280712128 Validation loss 0.11670167744159698 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5288],\n",
      "        [0.3165]])\n",
      "Iteration 11680 Training loss 0.11959897726774216 Validation loss 0.11670631170272827 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4790],\n",
      "        [0.5059]])\n",
      "Iteration 11690 Training loss 0.1189972460269928 Validation loss 0.1166701540350914 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.5583],\n",
      "        [0.5559]])\n",
      "Iteration 11700 Training loss 0.11917471140623093 Validation loss 0.11662077158689499 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.6550],\n",
      "        [0.5760]])\n",
      "Iteration 11710 Training loss 0.11754725873470306 Validation loss 0.11658911406993866 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.5353],\n",
      "        [0.6616]])\n",
      "Iteration 11720 Training loss 0.1176740750670433 Validation loss 0.11657678335905075 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5296],\n",
      "        [0.4660]])\n",
      "Iteration 11730 Training loss 0.11789489537477493 Validation loss 0.11658335477113724 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5660],\n",
      "        [0.5101]])\n",
      "Iteration 11740 Training loss 0.11744795739650726 Validation loss 0.11660692095756531 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5870],\n",
      "        [0.5433]])\n",
      "Iteration 11750 Training loss 0.11791117489337921 Validation loss 0.11665292084217072 Accuracy 0.612666666507721\n",
      "Output tensor([[0.2952],\n",
      "        [0.6680]])\n",
      "Iteration 11760 Training loss 0.11882058531045914 Validation loss 0.11661429703235626 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.4875],\n",
      "        [0.4211]])\n",
      "Iteration 11770 Training loss 0.11785070598125458 Validation loss 0.11658308655023575 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.5419],\n",
      "        [0.3823]])\n",
      "Iteration 11780 Training loss 0.11848513036966324 Validation loss 0.1166306883096695 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5197],\n",
      "        [0.6278]])\n",
      "Iteration 11790 Training loss 0.11766669899225235 Validation loss 0.11658858507871628 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.6022],\n",
      "        [0.4465]])\n",
      "Iteration 11800 Training loss 0.11857051402330399 Validation loss 0.1165786013007164 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4569],\n",
      "        [0.4582]])\n",
      "Iteration 11810 Training loss 0.11881931871175766 Validation loss 0.11657775938510895 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.2166],\n",
      "        [0.5795]])\n",
      "Iteration 11820 Training loss 0.11825907230377197 Validation loss 0.1165950745344162 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4798],\n",
      "        [0.4389]])\n",
      "Iteration 11830 Training loss 0.11766719818115234 Validation loss 0.11658993363380432 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.6146],\n",
      "        [0.6106]])\n",
      "Iteration 11840 Training loss 0.11747131496667862 Validation loss 0.1165749579668045 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5455],\n",
      "        [0.5372]])\n",
      "Iteration 11850 Training loss 0.11784464865922928 Validation loss 0.11658965796232224 Accuracy 0.612500011920929\n",
      "Output tensor([[0.3159],\n",
      "        [0.3429]])\n",
      "Iteration 11860 Training loss 0.11812964081764221 Validation loss 0.11659043282270432 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.3915],\n",
      "        [0.2818]])\n",
      "Iteration 11870 Training loss 0.11799263209104538 Validation loss 0.11670050024986267 Accuracy 0.612500011920929\n",
      "Output tensor([[0.5076],\n",
      "        [0.4252]])\n",
      "Iteration 11880 Training loss 0.11795707046985626 Validation loss 0.11658478528261185 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5009],\n",
      "        [0.3129]])\n",
      "Iteration 11890 Training loss 0.11754810810089111 Validation loss 0.11656620353460312 Accuracy 0.612666666507721\n",
      "Output tensor([[0.5941],\n",
      "        [0.4172]])\n",
      "Iteration 11900 Training loss 0.11723357439041138 Validation loss 0.11656934767961502 Accuracy 0.612333357334137\n",
      "Output tensor([[0.4564],\n",
      "        [0.5586]])\n",
      "Iteration 11910 Training loss 0.11786489188671112 Validation loss 0.11657016724348068 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.5583],\n",
      "        [0.5849]])\n",
      "Iteration 11920 Training loss 0.11683858186006546 Validation loss 0.11656518280506134 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.6286],\n",
      "        [0.5353]])\n",
      "Iteration 11930 Training loss 0.11704405397176743 Validation loss 0.11665494740009308 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.3408],\n",
      "        [0.3826]])\n",
      "Iteration 11940 Training loss 0.1173902302980423 Validation loss 0.11664166301488876 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.3817],\n",
      "        [0.5354]])\n",
      "Iteration 11950 Training loss 0.11882255971431732 Validation loss 0.11668097972869873 Accuracy 0.612500011920929\n",
      "Output tensor([[0.4374],\n",
      "        [0.5109]])\n",
      "Iteration 11960 Training loss 0.11718916147947311 Validation loss 0.11658614128828049 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.6247],\n",
      "        [0.5607]])\n",
      "Iteration 11970 Training loss 0.11834390461444855 Validation loss 0.11658567190170288 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5902],\n",
      "        [0.4845]])\n",
      "Iteration 11980 Training loss 0.11786110699176788 Validation loss 0.11655501276254654 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.6630],\n",
      "        [0.6168]])\n",
      "Iteration 11990 Training loss 0.11740551888942719 Validation loss 0.11656559258699417 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4403],\n",
      "        [0.5191]])\n",
      "Iteration 12000 Training loss 0.11761633306741714 Validation loss 0.11655665189027786 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.3191],\n",
      "        [0.5667]])\n",
      "Iteration 12010 Training loss 0.11733150482177734 Validation loss 0.11657299846410751 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5736],\n",
      "        [0.5464]])\n",
      "Iteration 12020 Training loss 0.11599121987819672 Validation loss 0.11654593050479889 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5708],\n",
      "        [0.5180]])\n",
      "Iteration 12030 Training loss 0.11675946414470673 Validation loss 0.11654429137706757 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.4379],\n",
      "        [0.2804]])\n",
      "Iteration 12040 Training loss 0.11865047365427017 Validation loss 0.11654435098171234 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.5708],\n",
      "        [0.6269]])\n",
      "Iteration 12050 Training loss 0.11873161792755127 Validation loss 0.11652126908302307 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.4463],\n",
      "        [0.3974]])\n",
      "Iteration 12060 Training loss 0.11776919662952423 Validation loss 0.11651509255170822 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5435],\n",
      "        [0.5521]])\n",
      "Iteration 12070 Training loss 0.11839551478624344 Validation loss 0.11653126776218414 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5233],\n",
      "        [0.4492]])\n",
      "Iteration 12080 Training loss 0.11882764846086502 Validation loss 0.11654157191514969 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.7303],\n",
      "        [0.5058]])\n",
      "Iteration 12090 Training loss 0.11767791956663132 Validation loss 0.11651908606290817 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.7171],\n",
      "        [0.4551]])\n",
      "Iteration 12100 Training loss 0.11787327378988266 Validation loss 0.11653410643339157 Accuracy 0.612333357334137\n",
      "Output tensor([[0.5243],\n",
      "        [0.5392]])\n",
      "Iteration 12110 Training loss 0.11666671931743622 Validation loss 0.11656127870082855 Accuracy 0.612500011920929\n",
      "Output tensor([[0.4761],\n",
      "        [0.4179]])\n",
      "Iteration 12120 Training loss 0.11750058084726334 Validation loss 0.11654534935951233 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.5942],\n",
      "        [0.3881]])\n",
      "Iteration 12130 Training loss 0.11777082085609436 Validation loss 0.11657019704580307 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5758],\n",
      "        [0.3764]])\n",
      "Iteration 12140 Training loss 0.11790543794631958 Validation loss 0.11660098284482956 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.4221],\n",
      "        [0.5524]])\n",
      "Iteration 12150 Training loss 0.11864025890827179 Validation loss 0.11656111478805542 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.2899],\n",
      "        [0.5709]])\n",
      "Iteration 12160 Training loss 0.11789660900831223 Validation loss 0.11661373823881149 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.3737],\n",
      "        [0.6381]])\n",
      "Iteration 12170 Training loss 0.11697357147932053 Validation loss 0.11653847992420197 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.4141],\n",
      "        [0.5031]])\n",
      "Iteration 12180 Training loss 0.11734794080257416 Validation loss 0.11654935777187347 Accuracy 0.612500011920929\n",
      "Output tensor([[0.4503],\n",
      "        [0.5904]])\n",
      "Iteration 12190 Training loss 0.1186368316411972 Validation loss 0.11655490845441818 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4330],\n",
      "        [0.5810]])\n",
      "Iteration 12200 Training loss 0.11812850087881088 Validation loss 0.11652886867523193 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.5633],\n",
      "        [0.4949]])\n",
      "Iteration 12210 Training loss 0.11902229487895966 Validation loss 0.11651982367038727 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.3911],\n",
      "        [0.6177]])\n",
      "Iteration 12220 Training loss 0.11708272248506546 Validation loss 0.1165279671549797 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.6243],\n",
      "        [0.3834]])\n",
      "Iteration 12230 Training loss 0.11906739324331284 Validation loss 0.11651680618524551 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.7594],\n",
      "        [0.2837]])\n",
      "Iteration 12240 Training loss 0.11777491122484207 Validation loss 0.11651816964149475 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.6034],\n",
      "        [0.2111]])\n",
      "Iteration 12250 Training loss 0.1180822029709816 Validation loss 0.1165483221411705 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4720],\n",
      "        [0.4019]])\n",
      "Iteration 12260 Training loss 0.11775241047143936 Validation loss 0.11650887876749039 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.6344],\n",
      "        [0.2721]])\n",
      "Iteration 12270 Training loss 0.11742331087589264 Validation loss 0.11649709194898605 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.4615],\n",
      "        [0.5317]])\n",
      "Iteration 12280 Training loss 0.11727362126111984 Validation loss 0.11650438606739044 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.5069],\n",
      "        [0.5019]])\n",
      "Iteration 12290 Training loss 0.11833169311285019 Validation loss 0.11659803986549377 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.3682],\n",
      "        [0.3206]])\n",
      "Iteration 12300 Training loss 0.11849145591259003 Validation loss 0.11653143167495728 Accuracy 0.612500011920929\n",
      "Output tensor([[0.6202],\n",
      "        [0.4716]])\n",
      "Iteration 12310 Training loss 0.11812716722488403 Validation loss 0.11661425977945328 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.1885],\n",
      "        [0.5321]])\n",
      "Iteration 12320 Training loss 0.11788854748010635 Validation loss 0.1167597621679306 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.4261],\n",
      "        [0.1567]])\n",
      "Iteration 12330 Training loss 0.11799853295087814 Validation loss 0.11656554043292999 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5943],\n",
      "        [0.1887]])\n",
      "Iteration 12340 Training loss 0.11742884665727615 Validation loss 0.11662933230400085 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4933],\n",
      "        [0.5096]])\n",
      "Iteration 12350 Training loss 0.11816440522670746 Validation loss 0.116559237241745 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.4286],\n",
      "        [0.3338]])\n",
      "Iteration 12360 Training loss 0.11732421815395355 Validation loss 0.11656150221824646 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5104],\n",
      "        [0.3758]])\n",
      "Iteration 12370 Training loss 0.11777836084365845 Validation loss 0.1165589988231659 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5448],\n",
      "        [0.4650]])\n",
      "Iteration 12380 Training loss 0.11744317412376404 Validation loss 0.11657355725765228 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.4307],\n",
      "        [0.4405]])\n",
      "Iteration 12390 Training loss 0.11776790022850037 Validation loss 0.11655621230602264 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.6007],\n",
      "        [0.5442]])\n",
      "Iteration 12400 Training loss 0.11816202104091644 Validation loss 0.1165422573685646 Accuracy 0.612666666507721\n",
      "Output tensor([[0.4272],\n",
      "        [0.4032]])\n",
      "Iteration 12410 Training loss 0.11676471680402756 Validation loss 0.11656016111373901 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.4534],\n",
      "        [0.6608]])\n",
      "Iteration 12420 Training loss 0.11761847883462906 Validation loss 0.1166132390499115 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.6018],\n",
      "        [0.6626]])\n",
      "Iteration 12430 Training loss 0.11767996102571487 Validation loss 0.11656138300895691 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.3562],\n",
      "        [0.5115]])\n",
      "Iteration 12440 Training loss 0.1180352196097374 Validation loss 0.11654386669397354 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4776],\n",
      "        [0.3665]])\n",
      "Iteration 12450 Training loss 0.11867009848356247 Validation loss 0.11651486903429031 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.3346],\n",
      "        [0.6011]])\n",
      "Iteration 12460 Training loss 0.11812129616737366 Validation loss 0.11651597917079926 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.6688],\n",
      "        [0.4574]])\n",
      "Iteration 12470 Training loss 0.11774589121341705 Validation loss 0.11651312559843063 Accuracy 0.612333357334137\n",
      "Output tensor([[0.4899],\n",
      "        [0.4393]])\n",
      "Iteration 12480 Training loss 0.1176288053393364 Validation loss 0.11655499041080475 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.5103],\n",
      "        [0.4983]])\n",
      "Iteration 12490 Training loss 0.11701814830303192 Validation loss 0.1165425106883049 Accuracy 0.6119999885559082\n",
      "Output tensor([[0.3504],\n",
      "        [0.4851]])\n",
      "Iteration 12500 Training loss 0.11809077113866806 Validation loss 0.11654523015022278 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5910],\n",
      "        [0.4415]])\n",
      "Iteration 12510 Training loss 0.11694008111953735 Validation loss 0.11655823886394501 Accuracy 0.612333357334137\n",
      "Output tensor([[0.5169],\n",
      "        [0.5774]])\n",
      "Iteration 12520 Training loss 0.11720219254493713 Validation loss 0.11659405380487442 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.6130],\n",
      "        [0.3296]])\n",
      "Iteration 12530 Training loss 0.11834363639354706 Validation loss 0.11668499559164047 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4728],\n",
      "        [0.6044]])\n",
      "Iteration 12540 Training loss 0.11867071688175201 Validation loss 0.11659407615661621 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.7160],\n",
      "        [0.6766]])\n",
      "Iteration 12550 Training loss 0.11830447614192963 Validation loss 0.11656169593334198 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.4045],\n",
      "        [0.5333]])\n",
      "Iteration 12560 Training loss 0.11724672466516495 Validation loss 0.11653883755207062 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.5466],\n",
      "        [0.4022]])\n",
      "Iteration 12570 Training loss 0.1174807921051979 Validation loss 0.116521455347538 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.4651],\n",
      "        [0.3631]])\n",
      "Iteration 12580 Training loss 0.11676865816116333 Validation loss 0.11661318689584732 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.3394],\n",
      "        [0.5557]])\n",
      "Iteration 12590 Training loss 0.1178128570318222 Validation loss 0.116522878408432 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4970],\n",
      "        [0.3355]])\n",
      "Iteration 12600 Training loss 0.11874302476644516 Validation loss 0.11649440228939056 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.5845],\n",
      "        [0.6806]])\n",
      "Iteration 12610 Training loss 0.11806347966194153 Validation loss 0.11648695915937424 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.5521],\n",
      "        [0.5905]])\n",
      "Iteration 12620 Training loss 0.11838500946760178 Validation loss 0.1165236383676529 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.6112],\n",
      "        [0.5409]])\n",
      "Iteration 12630 Training loss 0.11895428597927094 Validation loss 0.1165299341082573 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.2020],\n",
      "        [0.6099]])\n",
      "Iteration 12640 Training loss 0.1174803078174591 Validation loss 0.11653320491313934 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5334],\n",
      "        [0.4008]])\n",
      "Iteration 12650 Training loss 0.11813461035490036 Validation loss 0.11647847294807434 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5539],\n",
      "        [0.4733]])\n",
      "Iteration 12660 Training loss 0.117719866335392 Validation loss 0.11644962430000305 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.2465],\n",
      "        [0.3358]])\n",
      "Iteration 12670 Training loss 0.11765171587467194 Validation loss 0.11655697971582413 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5215],\n",
      "        [0.5765]])\n",
      "Iteration 12680 Training loss 0.11782842874526978 Validation loss 0.11648818850517273 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.5426],\n",
      "        [0.6169]])\n",
      "Iteration 12690 Training loss 0.11775627732276917 Validation loss 0.11649515479803085 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4373],\n",
      "        [0.4800]])\n",
      "Iteration 12700 Training loss 0.1176522895693779 Validation loss 0.11646034568548203 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.3530],\n",
      "        [0.4965]])\n",
      "Iteration 12710 Training loss 0.11717288196086884 Validation loss 0.11663957685232162 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.6181],\n",
      "        [0.4892]])\n",
      "Iteration 12720 Training loss 0.11787420511245728 Validation loss 0.11648057401180267 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.3946],\n",
      "        [0.3936]])\n",
      "Iteration 12730 Training loss 0.11864811927080154 Validation loss 0.11649858951568604 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.1726],\n",
      "        [0.5828]])\n",
      "Iteration 12740 Training loss 0.11743934452533722 Validation loss 0.11650463938713074 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4737],\n",
      "        [0.5716]])\n",
      "Iteration 12750 Training loss 0.11829348653554916 Validation loss 0.11648999899625778 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5721],\n",
      "        [0.4126]])\n",
      "Iteration 12760 Training loss 0.1176898181438446 Validation loss 0.1164335310459137 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.4892],\n",
      "        [0.6026]])\n",
      "Iteration 12770 Training loss 0.1184978038072586 Validation loss 0.11645115911960602 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5079],\n",
      "        [0.4116]])\n",
      "Iteration 12780 Training loss 0.11755174398422241 Validation loss 0.11642885208129883 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.3697],\n",
      "        [0.5423]])\n",
      "Iteration 12790 Training loss 0.11739449203014374 Validation loss 0.11642336845397949 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.6774],\n",
      "        [0.3621]])\n",
      "Iteration 12800 Training loss 0.11822378635406494 Validation loss 0.11660326272249222 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4151],\n",
      "        [0.3564]])\n",
      "Iteration 12810 Training loss 0.11851926892995834 Validation loss 0.11646025627851486 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.4562],\n",
      "        [0.4147]])\n",
      "Iteration 12820 Training loss 0.11733186990022659 Validation loss 0.11643776297569275 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.3730],\n",
      "        [0.4366]])\n",
      "Iteration 12830 Training loss 0.11858703941106796 Validation loss 0.11644673347473145 Accuracy 0.612666666507721\n",
      "Output tensor([[0.2780],\n",
      "        [0.4762]])\n",
      "Iteration 12840 Training loss 0.11733638495206833 Validation loss 0.11645805090665817 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.3787],\n",
      "        [0.4571]])\n",
      "Iteration 12850 Training loss 0.11728618294000626 Validation loss 0.11641509085893631 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.2679],\n",
      "        [0.5451]])\n",
      "Iteration 12860 Training loss 0.11767210066318512 Validation loss 0.11644652485847473 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4796],\n",
      "        [0.6345]])\n",
      "Iteration 12870 Training loss 0.11687739938497543 Validation loss 0.11641336232423782 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.5922],\n",
      "        [0.1992]])\n",
      "Iteration 12880 Training loss 0.11737234145402908 Validation loss 0.11642202734947205 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.5695],\n",
      "        [0.5893]])\n",
      "Iteration 12890 Training loss 0.1174740269780159 Validation loss 0.11644378304481506 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.6377],\n",
      "        [0.6339]])\n",
      "Iteration 12900 Training loss 0.11790569126605988 Validation loss 0.11646042764186859 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.4241],\n",
      "        [0.3498]])\n",
      "Iteration 12910 Training loss 0.11758405715227127 Validation loss 0.11652852594852448 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5838],\n",
      "        [0.5571]])\n",
      "Iteration 12920 Training loss 0.1177491545677185 Validation loss 0.11649100482463837 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.5509],\n",
      "        [0.4525]])\n",
      "Iteration 12930 Training loss 0.1173868477344513 Validation loss 0.11650731414556503 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4635],\n",
      "        [0.4202]])\n",
      "Iteration 12940 Training loss 0.11701197177171707 Validation loss 0.11649411916732788 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.2991],\n",
      "        [0.4432]])\n",
      "Iteration 12950 Training loss 0.11739890277385712 Validation loss 0.11651773750782013 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.4137],\n",
      "        [0.4941]])\n",
      "Iteration 12960 Training loss 0.11784328520298004 Validation loss 0.11645247042179108 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5509],\n",
      "        [0.5611]])\n",
      "Iteration 12970 Training loss 0.11604826897382736 Validation loss 0.11667117476463318 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.4444],\n",
      "        [0.2736]])\n",
      "Iteration 12980 Training loss 0.11800231784582138 Validation loss 0.11643294245004654 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.6162],\n",
      "        [0.3521]])\n",
      "Iteration 12990 Training loss 0.117592953145504 Validation loss 0.11642444878816605 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.6931],\n",
      "        [0.4757]])\n",
      "Iteration 13000 Training loss 0.1183406189084053 Validation loss 0.11641302704811096 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.6435],\n",
      "        [0.3576]])\n",
      "Iteration 13010 Training loss 0.11789495497941971 Validation loss 0.1164107695221901 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.5728],\n",
      "        [0.5030]])\n",
      "Iteration 13020 Training loss 0.11688641458749771 Validation loss 0.11642377823591232 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.4575],\n",
      "        [0.5702]])\n",
      "Iteration 13030 Training loss 0.11763051152229309 Validation loss 0.11639786511659622 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.5522],\n",
      "        [0.6230]])\n",
      "Iteration 13040 Training loss 0.11769270896911621 Validation loss 0.11639605462551117 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4633],\n",
      "        [0.4528]])\n",
      "Iteration 13050 Training loss 0.11773090809583664 Validation loss 0.1163826733827591 Accuracy 0.612333357334137\n",
      "Output tensor([[0.4085],\n",
      "        [0.5611]])\n",
      "Iteration 13060 Training loss 0.11779025942087173 Validation loss 0.11637469381093979 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5625],\n",
      "        [0.5002]])\n",
      "Iteration 13070 Training loss 0.11841564625501633 Validation loss 0.11646903306245804 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5498],\n",
      "        [0.6063]])\n",
      "Iteration 13080 Training loss 0.11777199059724808 Validation loss 0.11640309542417526 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.6208],\n",
      "        [0.6415]])\n",
      "Iteration 13090 Training loss 0.11849945783615112 Validation loss 0.11660853773355484 Accuracy 0.612500011920929\n",
      "Output tensor([[0.4953],\n",
      "        [0.5562]])\n",
      "Iteration 13100 Training loss 0.11827769130468369 Validation loss 0.1165437400341034 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4693],\n",
      "        [0.5281]])\n",
      "Iteration 13110 Training loss 0.11858358979225159 Validation loss 0.1165170669555664 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.6320],\n",
      "        [0.4755]])\n",
      "Iteration 13120 Training loss 0.11768186092376709 Validation loss 0.11640115082263947 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.5273],\n",
      "        [0.6158]])\n",
      "Iteration 13130 Training loss 0.11899677664041519 Validation loss 0.11637936532497406 Accuracy 0.612666666507721\n",
      "Output tensor([[0.4479],\n",
      "        [0.5155]])\n",
      "Iteration 13140 Training loss 0.11813339591026306 Validation loss 0.11656665056943893 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.4796],\n",
      "        [0.2872]])\n",
      "Iteration 13150 Training loss 0.1169089525938034 Validation loss 0.11640913784503937 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.2382],\n",
      "        [0.5112]])\n",
      "Iteration 13160 Training loss 0.11875101923942566 Validation loss 0.11651727557182312 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4237],\n",
      "        [0.5052]])\n",
      "Iteration 13170 Training loss 0.11806900799274445 Validation loss 0.11645728349685669 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.4864],\n",
      "        [0.5045]])\n",
      "Iteration 13180 Training loss 0.11741705983877182 Validation loss 0.11654612421989441 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.6619],\n",
      "        [0.5003]])\n",
      "Iteration 13190 Training loss 0.11892049014568329 Validation loss 0.11639003455638885 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4919],\n",
      "        [0.4906]])\n",
      "Iteration 13200 Training loss 0.11794821918010712 Validation loss 0.11635876446962357 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.5705],\n",
      "        [0.5458]])\n",
      "Iteration 13210 Training loss 0.11734288185834885 Validation loss 0.11634979397058487 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5349],\n",
      "        [0.4665]])\n",
      "Iteration 13220 Training loss 0.11755232512950897 Validation loss 0.11654345691204071 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.4725],\n",
      "        [0.4950]])\n",
      "Iteration 13230 Training loss 0.11796355992555618 Validation loss 0.11635544896125793 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5398],\n",
      "        [0.4203]])\n",
      "Iteration 13240 Training loss 0.11724621802568436 Validation loss 0.11636082082986832 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.2357],\n",
      "        [0.6049]])\n",
      "Iteration 13250 Training loss 0.11799058318138123 Validation loss 0.11635910719633102 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4996],\n",
      "        [0.5143]])\n",
      "Iteration 13260 Training loss 0.11833302676677704 Validation loss 0.11637595295906067 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.6189],\n",
      "        [0.6016]])\n",
      "Iteration 13270 Training loss 0.11776990443468094 Validation loss 0.11640232056379318 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.3384],\n",
      "        [0.5620]])\n",
      "Iteration 13280 Training loss 0.11793334782123566 Validation loss 0.11637379229068756 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.5443],\n",
      "        [0.3976]])\n",
      "Iteration 13290 Training loss 0.11831043660640717 Validation loss 0.11646772921085358 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.4670],\n",
      "        [0.3476]])\n",
      "Iteration 13300 Training loss 0.11783359944820404 Validation loss 0.1163812130689621 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.4109],\n",
      "        [0.5103]])\n",
      "Iteration 13310 Training loss 0.1176569014787674 Validation loss 0.11644624918699265 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.5394],\n",
      "        [0.4633]])\n",
      "Iteration 13320 Training loss 0.11823136359453201 Validation loss 0.11641227453947067 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.3382],\n",
      "        [0.6097]])\n",
      "Iteration 13330 Training loss 0.11738508939743042 Validation loss 0.11638232320547104 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.3571],\n",
      "        [0.4117]])\n",
      "Iteration 13340 Training loss 0.11805713921785355 Validation loss 0.11637017875909805 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.4214],\n",
      "        [0.4276]])\n",
      "Iteration 13350 Training loss 0.1175965666770935 Validation loss 0.11638997495174408 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5160],\n",
      "        [0.4622]])\n",
      "Iteration 13360 Training loss 0.11728499084711075 Validation loss 0.11634612828493118 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.5578],\n",
      "        [0.8251]])\n",
      "Iteration 13370 Training loss 0.11822164058685303 Validation loss 0.11634918302297592 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4713],\n",
      "        [0.6123]])\n",
      "Iteration 13380 Training loss 0.11786751449108124 Validation loss 0.11635011434555054 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.3645],\n",
      "        [0.4572]])\n",
      "Iteration 13390 Training loss 0.11823341995477676 Validation loss 0.11647031456232071 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4771],\n",
      "        [0.5643]])\n",
      "Iteration 13400 Training loss 0.11805498600006104 Validation loss 0.11635026335716248 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.4605],\n",
      "        [0.4300]])\n",
      "Iteration 13410 Training loss 0.11816581338644028 Validation loss 0.11634214967489243 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.4160],\n",
      "        [0.4912]])\n",
      "Iteration 13420 Training loss 0.11781644821166992 Validation loss 0.11634024977684021 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.4706],\n",
      "        [0.4434]])\n",
      "Iteration 13430 Training loss 0.11794213950634003 Validation loss 0.11635535955429077 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.3732],\n",
      "        [0.3597]])\n",
      "Iteration 13440 Training loss 0.11748585850000381 Validation loss 0.11633976548910141 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.6060],\n",
      "        [0.4395]])\n",
      "Iteration 13450 Training loss 0.11820387840270996 Validation loss 0.11634360998868942 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.3694],\n",
      "        [0.6373]])\n",
      "Iteration 13460 Training loss 0.11670392751693726 Validation loss 0.11637650430202484 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.5795],\n",
      "        [0.4434]])\n",
      "Iteration 13470 Training loss 0.11774931102991104 Validation loss 0.11638536304235458 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5567],\n",
      "        [0.2312]])\n",
      "Iteration 13480 Training loss 0.11784853786230087 Validation loss 0.11633572727441788 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.6558],\n",
      "        [0.3353]])\n",
      "Iteration 13490 Training loss 0.1167256087064743 Validation loss 0.11634308099746704 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5116],\n",
      "        [0.4648]])\n",
      "Iteration 13500 Training loss 0.11731929332017899 Validation loss 0.11631719022989273 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.6057],\n",
      "        [0.3735]])\n",
      "Iteration 13510 Training loss 0.1184706836938858 Validation loss 0.11631891131401062 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5892],\n",
      "        [0.4481]])\n",
      "Iteration 13520 Training loss 0.11819212883710861 Validation loss 0.11639908701181412 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.2909],\n",
      "        [0.7229]])\n",
      "Iteration 13530 Training loss 0.11753397434949875 Validation loss 0.11638303846120834 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.2209],\n",
      "        [0.4721]])\n",
      "Iteration 13540 Training loss 0.11876814067363739 Validation loss 0.11633330583572388 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4579],\n",
      "        [0.3873]])\n",
      "Iteration 13550 Training loss 0.11846913397312164 Validation loss 0.11635918915271759 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.4479],\n",
      "        [0.6166]])\n",
      "Iteration 13560 Training loss 0.11756466329097748 Validation loss 0.11637137085199356 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5706],\n",
      "        [0.4543]])\n",
      "Iteration 13570 Training loss 0.11759837716817856 Validation loss 0.11634072661399841 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.3959],\n",
      "        [0.3708]])\n",
      "Iteration 13580 Training loss 0.11704166978597641 Validation loss 0.1163359135389328 Accuracy 0.612500011920929\n",
      "Output tensor([[0.5410],\n",
      "        [0.3207]])\n",
      "Iteration 13590 Training loss 0.11813744157552719 Validation loss 0.11638714373111725 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5244],\n",
      "        [0.5629]])\n",
      "Iteration 13600 Training loss 0.11801419407129288 Validation loss 0.11635372042655945 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.3585],\n",
      "        [0.3900]])\n",
      "Iteration 13610 Training loss 0.11797037720680237 Validation loss 0.11632727086544037 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.3744],\n",
      "        [0.6164]])\n",
      "Iteration 13620 Training loss 0.11928291618824005 Validation loss 0.11632873862981796 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.4677],\n",
      "        [0.4206]])\n",
      "Iteration 13630 Training loss 0.11708594858646393 Validation loss 0.11634470522403717 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5845],\n",
      "        [0.4959]])\n",
      "Iteration 13640 Training loss 0.11652471870183945 Validation loss 0.11632212996482849 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.4838],\n",
      "        [0.3659]])\n",
      "Iteration 13650 Training loss 0.11715853959321976 Validation loss 0.11634596437215805 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5358],\n",
      "        [0.5221]])\n",
      "Iteration 13660 Training loss 0.1188930869102478 Validation loss 0.116326242685318 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.3755],\n",
      "        [0.4933]])\n",
      "Iteration 13670 Training loss 0.11879587918519974 Validation loss 0.1163749098777771 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.5384],\n",
      "        [0.4204]])\n",
      "Iteration 13680 Training loss 0.11663629114627838 Validation loss 0.11633791029453278 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4166],\n",
      "        [0.6217]])\n",
      "Iteration 13690 Training loss 0.11839079111814499 Validation loss 0.11633721739053726 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.6826],\n",
      "        [0.6045]])\n",
      "Iteration 13700 Training loss 0.11942163854837418 Validation loss 0.11632902175188065 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.3719],\n",
      "        [0.2612]])\n",
      "Iteration 13710 Training loss 0.117389015853405 Validation loss 0.11633699387311935 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5787],\n",
      "        [0.5680]])\n",
      "Iteration 13720 Training loss 0.11780444532632828 Validation loss 0.11642240732908249 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.3817],\n",
      "        [0.5801]])\n",
      "Iteration 13730 Training loss 0.11697693914175034 Validation loss 0.11639083176851273 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.4160],\n",
      "        [0.6099]])\n",
      "Iteration 13740 Training loss 0.11714562773704529 Validation loss 0.11634453386068344 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.3135],\n",
      "        [0.4578]])\n",
      "Iteration 13750 Training loss 0.11883801221847534 Validation loss 0.11641369760036469 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.5003],\n",
      "        [0.3803]])\n",
      "Iteration 13760 Training loss 0.11608153581619263 Validation loss 0.11631897836923599 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.3868],\n",
      "        [0.5243]])\n",
      "Iteration 13770 Training loss 0.11843209713697433 Validation loss 0.1163254827260971 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5221],\n",
      "        [0.5473]])\n",
      "Iteration 13780 Training loss 0.11829560250043869 Validation loss 0.11633063107728958 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.3850],\n",
      "        [0.4758]])\n",
      "Iteration 13790 Training loss 0.11809004843235016 Validation loss 0.11634927242994308 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.6857],\n",
      "        [0.4793]])\n",
      "Iteration 13800 Training loss 0.11812391877174377 Validation loss 0.11632972955703735 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5393],\n",
      "        [0.5860]])\n",
      "Iteration 13810 Training loss 0.11808668822050095 Validation loss 0.11630820482969284 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.6406],\n",
      "        [0.2793]])\n",
      "Iteration 13820 Training loss 0.11806557327508926 Validation loss 0.11630739271640778 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5982],\n",
      "        [0.3998]])\n",
      "Iteration 13830 Training loss 0.11709801107645035 Validation loss 0.11628597974777222 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4534],\n",
      "        [0.4449]])\n",
      "Iteration 13840 Training loss 0.11672711372375488 Validation loss 0.11630847305059433 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.5868],\n",
      "        [0.3936]])\n",
      "Iteration 13850 Training loss 0.11669506877660751 Validation loss 0.11630930751562119 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.6152],\n",
      "        [0.5118]])\n",
      "Iteration 13860 Training loss 0.11809228360652924 Validation loss 0.11634320765733719 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.3706],\n",
      "        [0.3572]])\n",
      "Iteration 13870 Training loss 0.11796244978904724 Validation loss 0.11632924526929855 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.4069],\n",
      "        [0.4529]])\n",
      "Iteration 13880 Training loss 0.11695799976587296 Validation loss 0.11635473370552063 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5187],\n",
      "        [0.5772]])\n",
      "Iteration 13890 Training loss 0.11765004694461823 Validation loss 0.11634532362222672 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.4396],\n",
      "        [0.4836]])\n",
      "Iteration 13900 Training loss 0.11782270669937134 Validation loss 0.11633627116680145 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4056],\n",
      "        [0.4942]])\n",
      "Iteration 13910 Training loss 0.11846096068620682 Validation loss 0.11638342589139938 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.2913],\n",
      "        [0.5618]])\n",
      "Iteration 13920 Training loss 0.11859578639268875 Validation loss 0.11632642894983292 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4756],\n",
      "        [0.5246]])\n",
      "Iteration 13930 Training loss 0.11808378994464874 Validation loss 0.11633473634719849 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.5859],\n",
      "        [0.3048]])\n",
      "Iteration 13940 Training loss 0.11723282188177109 Validation loss 0.1163603886961937 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.4421],\n",
      "        [0.5908]])\n",
      "Iteration 13950 Training loss 0.11681825667619705 Validation loss 0.11637552827596664 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5766],\n",
      "        [0.5333]])\n",
      "Iteration 13960 Training loss 0.11764620244503021 Validation loss 0.11651615053415298 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5921],\n",
      "        [0.5099]])\n",
      "Iteration 13970 Training loss 0.11842513829469681 Validation loss 0.1163419559597969 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.4709],\n",
      "        [0.3247]])\n",
      "Iteration 13980 Training loss 0.11808890849351883 Validation loss 0.11633457243442535 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.2895],\n",
      "        [0.4555]])\n",
      "Iteration 13990 Training loss 0.1169167160987854 Validation loss 0.11646045744419098 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5885],\n",
      "        [0.5722]])\n",
      "Iteration 14000 Training loss 0.11767952144145966 Validation loss 0.11630956828594208 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.5839],\n",
      "        [0.3882]])\n",
      "Iteration 14010 Training loss 0.11778293550014496 Validation loss 0.11634107679128647 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.4700],\n",
      "        [0.2613]])\n",
      "Iteration 14020 Training loss 0.116971455514431 Validation loss 0.11634299904108047 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5839],\n",
      "        [0.5033]])\n",
      "Iteration 14030 Training loss 0.11812730878591537 Validation loss 0.11629801243543625 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.5686],\n",
      "        [0.2456]])\n",
      "Iteration 14040 Training loss 0.11731163412332535 Validation loss 0.11647448688745499 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.4473],\n",
      "        [0.2916]])\n",
      "Iteration 14050 Training loss 0.11923529952764511 Validation loss 0.11629758775234222 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.6319],\n",
      "        [0.4311]])\n",
      "Iteration 14060 Training loss 0.11700413376092911 Validation loss 0.11633531749248505 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4500],\n",
      "        [0.6367]])\n",
      "Iteration 14070 Training loss 0.1176382452249527 Validation loss 0.11637942492961884 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.5990],\n",
      "        [0.5493]])\n",
      "Iteration 14080 Training loss 0.11737698316574097 Validation loss 0.11637712270021439 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.3566],\n",
      "        [0.5765]])\n",
      "Iteration 14090 Training loss 0.11905411630868912 Validation loss 0.11648209393024445 Accuracy 0.6116666793823242\n",
      "Output tensor([[0.4834],\n",
      "        [0.4160]])\n",
      "Iteration 14100 Training loss 0.11800191551446915 Validation loss 0.11631398648023605 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.4012],\n",
      "        [0.5558]])\n",
      "Iteration 14110 Training loss 0.11769329011440277 Validation loss 0.11627374589443207 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.3637],\n",
      "        [0.4636]])\n",
      "Iteration 14120 Training loss 0.11771003156900406 Validation loss 0.11636193841695786 Accuracy 0.612333357334137\n",
      "Output tensor([[0.4787],\n",
      "        [0.5212]])\n",
      "Iteration 14130 Training loss 0.11831028014421463 Validation loss 0.11626214534044266 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.5245],\n",
      "        [0.2719]])\n",
      "Iteration 14140 Training loss 0.11659315228462219 Validation loss 0.11625277996063232 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5754],\n",
      "        [0.4777]])\n",
      "Iteration 14150 Training loss 0.11804419010877609 Validation loss 0.1162758618593216 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4432],\n",
      "        [0.4767]])\n",
      "Iteration 14160 Training loss 0.11803865432739258 Validation loss 0.11627113074064255 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4638],\n",
      "        [0.6129]])\n",
      "Iteration 14170 Training loss 0.11806438863277435 Validation loss 0.11626172065734863 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.4963],\n",
      "        [0.5028]])\n",
      "Iteration 14180 Training loss 0.11727280914783478 Validation loss 0.11625844240188599 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5327],\n",
      "        [0.4813]])\n",
      "Iteration 14190 Training loss 0.11777902394533157 Validation loss 0.11625410616397858 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4090],\n",
      "        [0.4063]])\n",
      "Iteration 14200 Training loss 0.11791542917490005 Validation loss 0.11625602096319199 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5742],\n",
      "        [0.3355]])\n",
      "Iteration 14210 Training loss 0.11754702776670456 Validation loss 0.11622539162635803 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5197],\n",
      "        [0.3345]])\n",
      "Iteration 14220 Training loss 0.1178252324461937 Validation loss 0.11624755710363388 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.3854],\n",
      "        [0.2997]])\n",
      "Iteration 14230 Training loss 0.11819691956043243 Validation loss 0.11627373099327087 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.6111],\n",
      "        [0.4051]])\n",
      "Iteration 14240 Training loss 0.11824013292789459 Validation loss 0.11637358367443085 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.2103],\n",
      "        [0.4987]])\n",
      "Iteration 14250 Training loss 0.11929688602685928 Validation loss 0.11622881144285202 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5034],\n",
      "        [0.4169]])\n",
      "Iteration 14260 Training loss 0.11709750443696976 Validation loss 0.11622234433889389 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5069],\n",
      "        [0.4671]])\n",
      "Iteration 14270 Training loss 0.11793017387390137 Validation loss 0.11631187051534653 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.4921],\n",
      "        [0.4478]])\n",
      "Iteration 14280 Training loss 0.11734800040721893 Validation loss 0.11626936495304108 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.5605],\n",
      "        [0.5366]])\n",
      "Iteration 14290 Training loss 0.11832667142152786 Validation loss 0.1162603422999382 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.1515],\n",
      "        [0.4693]])\n",
      "Iteration 14300 Training loss 0.11904416233301163 Validation loss 0.1162470281124115 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.6251],\n",
      "        [0.6078]])\n",
      "Iteration 14310 Training loss 0.11719506978988647 Validation loss 0.11623439192771912 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5498],\n",
      "        [0.4488]])\n",
      "Iteration 14320 Training loss 0.11766055971384048 Validation loss 0.11632353067398071 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5887],\n",
      "        [0.4192]])\n",
      "Iteration 14330 Training loss 0.11837128549814224 Validation loss 0.1163296177983284 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.4824],\n",
      "        [0.7689]])\n",
      "Iteration 14340 Training loss 0.117668516933918 Validation loss 0.11626448482275009 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.1789],\n",
      "        [0.3835]])\n",
      "Iteration 14350 Training loss 0.11698795855045319 Validation loss 0.11633508652448654 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5700],\n",
      "        [0.5227]])\n",
      "Iteration 14360 Training loss 0.11786723881959915 Validation loss 0.11624466627836227 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.6397],\n",
      "        [0.3172]])\n",
      "Iteration 14370 Training loss 0.11863082647323608 Validation loss 0.11626283079385757 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.4897],\n",
      "        [0.6063]])\n",
      "Iteration 14380 Training loss 0.11656734347343445 Validation loss 0.11623138189315796 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.5746],\n",
      "        [0.4679]])\n",
      "Iteration 14390 Training loss 0.11717206984758377 Validation loss 0.11624040454626083 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.6407],\n",
      "        [0.5881]])\n",
      "Iteration 14400 Training loss 0.11789758503437042 Validation loss 0.11633988469839096 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.3049],\n",
      "        [0.5939]])\n",
      "Iteration 14410 Training loss 0.11643499881029129 Validation loss 0.1162782832980156 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4449],\n",
      "        [0.5327]])\n",
      "Iteration 14420 Training loss 0.11796780675649643 Validation loss 0.11629026383161545 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.5537],\n",
      "        [0.4288]])\n",
      "Iteration 14430 Training loss 0.11842963844537735 Validation loss 0.11627781391143799 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.3980],\n",
      "        [0.3775]])\n",
      "Iteration 14440 Training loss 0.1176493912935257 Validation loss 0.11628247797489166 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.3689],\n",
      "        [0.5843]])\n",
      "Iteration 14450 Training loss 0.11751741170883179 Validation loss 0.11627882719039917 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.2612],\n",
      "        [0.4724]])\n",
      "Iteration 14460 Training loss 0.11939884722232819 Validation loss 0.11629927903413773 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.6386],\n",
      "        [0.4753]])\n",
      "Iteration 14470 Training loss 0.11697343736886978 Validation loss 0.11623021215200424 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.4529],\n",
      "        [0.4896]])\n",
      "Iteration 14480 Training loss 0.11813566833734512 Validation loss 0.11622276157140732 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5302],\n",
      "        [0.3452]])\n",
      "Iteration 14490 Training loss 0.11761704832315445 Validation loss 0.11623087525367737 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5302],\n",
      "        [0.6213]])\n",
      "Iteration 14500 Training loss 0.11766623705625534 Validation loss 0.11623343825340271 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.3909],\n",
      "        [0.3856]])\n",
      "Iteration 14510 Training loss 0.11775756627321243 Validation loss 0.11627618223428726 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.4701],\n",
      "        [0.2532]])\n",
      "Iteration 14520 Training loss 0.11753282696008682 Validation loss 0.11619553714990616 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4731],\n",
      "        [0.4251]])\n",
      "Iteration 14530 Training loss 0.11875604093074799 Validation loss 0.11619795858860016 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5043],\n",
      "        [0.6456]])\n",
      "Iteration 14540 Training loss 0.11792702972888947 Validation loss 0.1161889135837555 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.6078],\n",
      "        [0.5446]])\n",
      "Iteration 14550 Training loss 0.11740941554307938 Validation loss 0.1161917895078659 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4817],\n",
      "        [0.5297]])\n",
      "Iteration 14560 Training loss 0.11766114085912704 Validation loss 0.1161804273724556 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.2812],\n",
      "        [0.5055]])\n",
      "Iteration 14570 Training loss 0.11742880940437317 Validation loss 0.11620397865772247 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.5615],\n",
      "        [0.3625]])\n",
      "Iteration 14580 Training loss 0.11819197237491608 Validation loss 0.11617448180913925 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5024],\n",
      "        [0.5314]])\n",
      "Iteration 14590 Training loss 0.11570959538221359 Validation loss 0.1162642389535904 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.3479],\n",
      "        [0.5962]])\n",
      "Iteration 14600 Training loss 0.11783690750598907 Validation loss 0.11621347814798355 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.4062],\n",
      "        [0.5301]])\n",
      "Iteration 14610 Training loss 0.11746878921985626 Validation loss 0.11626642197370529 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.2455],\n",
      "        [0.5458]])\n",
      "Iteration 14620 Training loss 0.1173892691731453 Validation loss 0.11618072539567947 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5521],\n",
      "        [0.5567]])\n",
      "Iteration 14630 Training loss 0.116863913834095 Validation loss 0.11629268527030945 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4669],\n",
      "        [0.5192]])\n",
      "Iteration 14640 Training loss 0.11785724014043808 Validation loss 0.11620264500379562 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5366],\n",
      "        [0.4982]])\n",
      "Iteration 14650 Training loss 0.11664043366909027 Validation loss 0.11637192964553833 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.6355],\n",
      "        [0.4927]])\n",
      "Iteration 14660 Training loss 0.11699929088354111 Validation loss 0.11621617525815964 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5382],\n",
      "        [0.5189]])\n",
      "Iteration 14670 Training loss 0.1178460493683815 Validation loss 0.11620695143938065 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.6861],\n",
      "        [0.4293]])\n",
      "Iteration 14680 Training loss 0.11830300837755203 Validation loss 0.11621683835983276 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4002],\n",
      "        [0.6091]])\n",
      "Iteration 14690 Training loss 0.11806250363588333 Validation loss 0.11620642244815826 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5526],\n",
      "        [0.4391]])\n",
      "Iteration 14700 Training loss 0.11855153739452362 Validation loss 0.11629166454076767 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.2944],\n",
      "        [0.4145]])\n",
      "Iteration 14710 Training loss 0.1172243282198906 Validation loss 0.11620648950338364 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4026],\n",
      "        [0.5570]])\n",
      "Iteration 14720 Training loss 0.11916153877973557 Validation loss 0.11620824038982391 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.3927],\n",
      "        [0.7110]])\n",
      "Iteration 14730 Training loss 0.11904473602771759 Validation loss 0.11620300263166428 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4143],\n",
      "        [0.5627]])\n",
      "Iteration 14740 Training loss 0.1175902783870697 Validation loss 0.11620895564556122 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.6056],\n",
      "        [0.5005]])\n",
      "Iteration 14750 Training loss 0.11717185378074646 Validation loss 0.11621732264757156 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.4542],\n",
      "        [0.3542]])\n",
      "Iteration 14760 Training loss 0.11751607060432434 Validation loss 0.11625103652477264 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.6513],\n",
      "        [0.6525]])\n",
      "Iteration 14770 Training loss 0.11836163699626923 Validation loss 0.1161753311753273 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5087],\n",
      "        [0.4746]])\n",
      "Iteration 14780 Training loss 0.11833304911851883 Validation loss 0.11616001278162003 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5092],\n",
      "        [0.6447]])\n",
      "Iteration 14790 Training loss 0.11766912043094635 Validation loss 0.11617301404476166 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.4173],\n",
      "        [0.3747]])\n",
      "Iteration 14800 Training loss 0.11832686513662338 Validation loss 0.11618924885988235 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.4707],\n",
      "        [0.3328]])\n",
      "Iteration 14810 Training loss 0.11775216460227966 Validation loss 0.11617576330900192 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6288],\n",
      "        [0.5221]])\n",
      "Iteration 14820 Training loss 0.11822114884853363 Validation loss 0.11621858924627304 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4080],\n",
      "        [0.5204]])\n",
      "Iteration 14830 Training loss 0.11787940561771393 Validation loss 0.11616114526987076 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.5277],\n",
      "        [0.4647]])\n",
      "Iteration 14840 Training loss 0.11784262210130692 Validation loss 0.11616586148738861 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.1251],\n",
      "        [0.5029]])\n",
      "Iteration 14850 Training loss 0.11680560559034348 Validation loss 0.11616235226392746 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.4239],\n",
      "        [0.4873]])\n",
      "Iteration 14860 Training loss 0.11666402220726013 Validation loss 0.1161814033985138 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.4905],\n",
      "        [0.3718]])\n",
      "Iteration 14870 Training loss 0.11791931092739105 Validation loss 0.11616607755422592 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.3799],\n",
      "        [0.5361]])\n",
      "Iteration 14880 Training loss 0.11805064231157303 Validation loss 0.11619584262371063 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.6070],\n",
      "        [0.4484]])\n",
      "Iteration 14890 Training loss 0.11731190979480743 Validation loss 0.11626800894737244 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5160],\n",
      "        [0.5769]])\n",
      "Iteration 14900 Training loss 0.1176929920911789 Validation loss 0.11618649959564209 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5578],\n",
      "        [0.5055]])\n",
      "Iteration 14910 Training loss 0.11783655732870102 Validation loss 0.11622296273708344 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5021],\n",
      "        [0.4873]])\n",
      "Iteration 14920 Training loss 0.11742108315229416 Validation loss 0.11618109047412872 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5714],\n",
      "        [0.4608]])\n",
      "Iteration 14930 Training loss 0.1176050528883934 Validation loss 0.11617512255907059 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4854],\n",
      "        [0.3733]])\n",
      "Iteration 14940 Training loss 0.11704793572425842 Validation loss 0.11618438363075256 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4012],\n",
      "        [0.3805]])\n",
      "Iteration 14950 Training loss 0.11861221492290497 Validation loss 0.1161787137389183 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5267],\n",
      "        [0.3383]])\n",
      "Iteration 14960 Training loss 0.11796168982982635 Validation loss 0.11612614244222641 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.5620],\n",
      "        [0.5944]])\n",
      "Iteration 14970 Training loss 0.11709234863519669 Validation loss 0.11618387699127197 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.6646],\n",
      "        [0.5753]])\n",
      "Iteration 14980 Training loss 0.11801493167877197 Validation loss 0.11614208668470383 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.5185],\n",
      "        [0.5746]])\n",
      "Iteration 14990 Training loss 0.11716369539499283 Validation loss 0.11614439636468887 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5508],\n",
      "        [0.5543]])\n",
      "Iteration 15000 Training loss 0.11800618469715118 Validation loss 0.11620049178600311 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.3999],\n",
      "        [0.5524]])\n",
      "Iteration 15010 Training loss 0.11727980524301529 Validation loss 0.11617058515548706 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5957],\n",
      "        [0.2750]])\n",
      "Iteration 15020 Training loss 0.11805802583694458 Validation loss 0.1161547377705574 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.3425],\n",
      "        [0.4026]])\n",
      "Iteration 15030 Training loss 0.1186213493347168 Validation loss 0.11617850512266159 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.3026],\n",
      "        [0.4878]])\n",
      "Iteration 15040 Training loss 0.11803680658340454 Validation loss 0.11625362932682037 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5287],\n",
      "        [0.4476]])\n",
      "Iteration 15050 Training loss 0.11774996668100357 Validation loss 0.11614853888750076 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.6481],\n",
      "        [0.4312]])\n",
      "Iteration 15060 Training loss 0.11759386956691742 Validation loss 0.11614933609962463 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.1671],\n",
      "        [0.5992]])\n",
      "Iteration 15070 Training loss 0.11859141290187836 Validation loss 0.11623160541057587 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.6626],\n",
      "        [0.5649]])\n",
      "Iteration 15080 Training loss 0.11785434931516647 Validation loss 0.11624973267316818 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.5581],\n",
      "        [0.4937]])\n",
      "Iteration 15090 Training loss 0.11847985535860062 Validation loss 0.11634047329425812 Accuracy 0.612666666507721\n",
      "Output tensor([[0.4102],\n",
      "        [0.5713]])\n",
      "Iteration 15100 Training loss 0.11765947192907333 Validation loss 0.11619897186756134 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.2589],\n",
      "        [0.3696]])\n",
      "Iteration 15110 Training loss 0.11750230938196182 Validation loss 0.11618554592132568 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.4541],\n",
      "        [0.6218]])\n",
      "Iteration 15120 Training loss 0.11764876544475555 Validation loss 0.11617392301559448 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5115],\n",
      "        [0.5361]])\n",
      "Iteration 15130 Training loss 0.11803962290287018 Validation loss 0.11614375561475754 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.6385],\n",
      "        [0.5695]])\n",
      "Iteration 15140 Training loss 0.11646640300750732 Validation loss 0.11616314947605133 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.6116],\n",
      "        [0.1819]])\n",
      "Iteration 15150 Training loss 0.11731462925672531 Validation loss 0.11619395762681961 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5624],\n",
      "        [0.5546]])\n",
      "Iteration 15160 Training loss 0.11743857711553574 Validation loss 0.11614229530096054 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.7128],\n",
      "        [0.4264]])\n",
      "Iteration 15170 Training loss 0.11655202507972717 Validation loss 0.11614329367876053 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5257],\n",
      "        [0.1921]])\n",
      "Iteration 15180 Training loss 0.11690618842840195 Validation loss 0.1161797046661377 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5212],\n",
      "        [0.5453]])\n",
      "Iteration 15190 Training loss 0.1162959560751915 Validation loss 0.11612827330827713 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.5023],\n",
      "        [0.3223]])\n",
      "Iteration 15200 Training loss 0.11775173246860504 Validation loss 0.1162407249212265 Accuracy 0.612666666507721\n",
      "Output tensor([[0.4227],\n",
      "        [0.5139]])\n",
      "Iteration 15210 Training loss 0.11817067116498947 Validation loss 0.11619044840335846 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.2270],\n",
      "        [0.7043]])\n",
      "Iteration 15220 Training loss 0.11774839460849762 Validation loss 0.11632790416479111 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.4240],\n",
      "        [0.6731]])\n",
      "Iteration 15230 Training loss 0.11735071986913681 Validation loss 0.11618153750896454 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.6467],\n",
      "        [0.5922]])\n",
      "Iteration 15240 Training loss 0.11772053688764572 Validation loss 0.11615168303251266 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5659],\n",
      "        [0.4101]])\n",
      "Iteration 15250 Training loss 0.11770699918270111 Validation loss 0.11614160984754562 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.6472],\n",
      "        [0.5456]])\n",
      "Iteration 15260 Training loss 0.11710532754659653 Validation loss 0.11621548235416412 Accuracy 0.612500011920929\n",
      "Output tensor([[0.5752],\n",
      "        [0.7480]])\n",
      "Iteration 15270 Training loss 0.11883845925331116 Validation loss 0.11613558232784271 Accuracy 0.612666666507721\n",
      "Output tensor([[0.4475],\n",
      "        [0.4198]])\n",
      "Iteration 15280 Training loss 0.11691058427095413 Validation loss 0.11616738140583038 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4699],\n",
      "        [0.5844]])\n",
      "Iteration 15290 Training loss 0.11725006997585297 Validation loss 0.11613455414772034 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.4502],\n",
      "        [0.4520]])\n",
      "Iteration 15300 Training loss 0.11760507524013519 Validation loss 0.11616319417953491 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.6034],\n",
      "        [0.2575]])\n",
      "Iteration 15310 Training loss 0.11774177849292755 Validation loss 0.11615962535142899 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.3784],\n",
      "        [0.4402]])\n",
      "Iteration 15320 Training loss 0.11782120913267136 Validation loss 0.11615774780511856 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5442],\n",
      "        [0.3952]])\n",
      "Iteration 15330 Training loss 0.11827148497104645 Validation loss 0.11613774299621582 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.4599],\n",
      "        [0.4851]])\n",
      "Iteration 15340 Training loss 0.11770965903997421 Validation loss 0.1161431297659874 Accuracy 0.612333357334137\n",
      "Output tensor([[0.4224],\n",
      "        [0.3897]])\n",
      "Iteration 15350 Training loss 0.1179010421037674 Validation loss 0.11614254862070084 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.3970],\n",
      "        [0.3752]])\n",
      "Iteration 15360 Training loss 0.11807354539632797 Validation loss 0.11616310477256775 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.2744],\n",
      "        [0.5208]])\n",
      "Iteration 15370 Training loss 0.11760424822568893 Validation loss 0.11615367978811264 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.6024],\n",
      "        [0.5638]])\n",
      "Iteration 15380 Training loss 0.11812034249305725 Validation loss 0.11624827980995178 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.4494],\n",
      "        [0.5672]])\n",
      "Iteration 15390 Training loss 0.11696401983499527 Validation loss 0.11612515151500702 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.1413],\n",
      "        [0.3279]])\n",
      "Iteration 15400 Training loss 0.11766372621059418 Validation loss 0.11610356718301773 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4998],\n",
      "        [0.6080]])\n",
      "Iteration 15410 Training loss 0.11747916787862778 Validation loss 0.11621396243572235 Accuracy 0.612666666507721\n",
      "Output tensor([[0.2831],\n",
      "        [0.5547]])\n",
      "Iteration 15420 Training loss 0.11677441745996475 Validation loss 0.11611110717058182 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4953],\n",
      "        [0.5340]])\n",
      "Iteration 15430 Training loss 0.11741619557142258 Validation loss 0.11614435166120529 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4491],\n",
      "        [0.3901]])\n",
      "Iteration 15440 Training loss 0.11599176377058029 Validation loss 0.11611326038837433 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.4759],\n",
      "        [0.3190]])\n",
      "Iteration 15450 Training loss 0.11757656186819077 Validation loss 0.11615230143070221 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.7423],\n",
      "        [0.5365]])\n",
      "Iteration 15460 Training loss 0.1176254078745842 Validation loss 0.11612214893102646 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.6238],\n",
      "        [0.5354]])\n",
      "Iteration 15470 Training loss 0.11774978786706924 Validation loss 0.11621324717998505 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.4772],\n",
      "        [0.4356]])\n",
      "Iteration 15480 Training loss 0.11832718551158905 Validation loss 0.11610158532857895 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.6283],\n",
      "        [0.5435]])\n",
      "Iteration 15490 Training loss 0.11730420589447021 Validation loss 0.1162048950791359 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.4092],\n",
      "        [0.3564]])\n",
      "Iteration 15500 Training loss 0.11704668402671814 Validation loss 0.1164216622710228 Accuracy 0.6121666431427002\n",
      "Output tensor([[0.5238],\n",
      "        [0.5331]])\n",
      "Iteration 15510 Training loss 0.11793897300958633 Validation loss 0.11612420529127121 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4809],\n",
      "        [0.4741]])\n",
      "Iteration 15520 Training loss 0.11719483137130737 Validation loss 0.11613309383392334 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.3657],\n",
      "        [0.4179]])\n",
      "Iteration 15530 Training loss 0.11812452971935272 Validation loss 0.1162726879119873 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.6459],\n",
      "        [0.4908]])\n",
      "Iteration 15540 Training loss 0.11784046143293381 Validation loss 0.11610225588083267 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4634],\n",
      "        [0.6032]])\n",
      "Iteration 15550 Training loss 0.11699040979146957 Validation loss 0.11610332131385803 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.1437],\n",
      "        [0.4743]])\n",
      "Iteration 15560 Training loss 0.1174958124756813 Validation loss 0.11609409749507904 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.1849],\n",
      "        [0.4276]])\n",
      "Iteration 15570 Training loss 0.11829742789268494 Validation loss 0.11607145518064499 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.3193],\n",
      "        [0.4880]])\n",
      "Iteration 15580 Training loss 0.1168408989906311 Validation loss 0.11624888330698013 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.6293],\n",
      "        [0.5110]])\n",
      "Iteration 15590 Training loss 0.11727142333984375 Validation loss 0.11611450463533401 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.6010],\n",
      "        [0.5467]])\n",
      "Iteration 15600 Training loss 0.11838874965906143 Validation loss 0.11608608812093735 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.2916],\n",
      "        [0.4196]])\n",
      "Iteration 15610 Training loss 0.11708779633045197 Validation loss 0.11614664644002914 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.2869],\n",
      "        [0.3312]])\n",
      "Iteration 15620 Training loss 0.11770446598529816 Validation loss 0.11610209196805954 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5945],\n",
      "        [0.4372]])\n",
      "Iteration 15630 Training loss 0.11827468127012253 Validation loss 0.1161513701081276 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.2312],\n",
      "        [0.5019]])\n",
      "Iteration 15640 Training loss 0.117786705493927 Validation loss 0.11609630286693573 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5902],\n",
      "        [0.6211]])\n",
      "Iteration 15650 Training loss 0.11773155629634857 Validation loss 0.11616518348455429 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.5832],\n",
      "        [0.3793]])\n",
      "Iteration 15660 Training loss 0.1188378781080246 Validation loss 0.11608574539422989 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5390],\n",
      "        [0.6473]])\n",
      "Iteration 15670 Training loss 0.11808200925588608 Validation loss 0.1160508543252945 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.1348],\n",
      "        [0.4495]])\n",
      "Iteration 15680 Training loss 0.11775380373001099 Validation loss 0.11604994535446167 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.3188],\n",
      "        [0.4795]])\n",
      "Iteration 15690 Training loss 0.11786174774169922 Validation loss 0.11604312807321548 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.5218],\n",
      "        [0.3539]])\n",
      "Iteration 15700 Training loss 0.11719954758882523 Validation loss 0.11611568182706833 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.6557],\n",
      "        [0.3512]])\n",
      "Iteration 15710 Training loss 0.11778005957603455 Validation loss 0.11603838205337524 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.3598],\n",
      "        [0.3944]])\n",
      "Iteration 15720 Training loss 0.1170937567949295 Validation loss 0.11603229492902756 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.6453],\n",
      "        [0.3398]])\n",
      "Iteration 15730 Training loss 0.11803201586008072 Validation loss 0.11605373024940491 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.4968],\n",
      "        [0.4455]])\n",
      "Iteration 15740 Training loss 0.11831759661436081 Validation loss 0.11610689014196396 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.3901],\n",
      "        [0.5838]])\n",
      "Iteration 15750 Training loss 0.11723925918340683 Validation loss 0.11610164493322372 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4641],\n",
      "        [0.6588]])\n",
      "Iteration 15760 Training loss 0.11709922552108765 Validation loss 0.11609089374542236 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.5874],\n",
      "        [0.5237]])\n",
      "Iteration 15770 Training loss 0.1182972639799118 Validation loss 0.1162569671869278 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.2442],\n",
      "        [0.2619]])\n",
      "Iteration 15780 Training loss 0.11772041767835617 Validation loss 0.11612702161073685 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.2498],\n",
      "        [0.5582]])\n",
      "Iteration 15790 Training loss 0.11779401451349258 Validation loss 0.11604166775941849 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.4839],\n",
      "        [0.4278]])\n",
      "Iteration 15800 Training loss 0.1180502399802208 Validation loss 0.11605633795261383 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.4416],\n",
      "        [0.4747]])\n",
      "Iteration 15810 Training loss 0.11773433536291122 Validation loss 0.11610116809606552 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5740],\n",
      "        [0.5190]])\n",
      "Iteration 15820 Training loss 0.11752201616764069 Validation loss 0.11608166247606277 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.6601],\n",
      "        [0.4590]])\n",
      "Iteration 15830 Training loss 0.11770903319120407 Validation loss 0.11607804149389267 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4228],\n",
      "        [0.4301]])\n",
      "Iteration 15840 Training loss 0.11818592995405197 Validation loss 0.11608140915632248 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5956],\n",
      "        [0.5171]])\n",
      "Iteration 15850 Training loss 0.11757487058639526 Validation loss 0.11606194078922272 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.6248],\n",
      "        [0.5863]])\n",
      "Iteration 15860 Training loss 0.11914579570293427 Validation loss 0.11617518961429596 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.3142],\n",
      "        [0.6008]])\n",
      "Iteration 15870 Training loss 0.11689909547567368 Validation loss 0.11606580764055252 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.3793],\n",
      "        [0.7659]])\n",
      "Iteration 15880 Training loss 0.11778292059898376 Validation loss 0.11606357991695404 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.2627],\n",
      "        [0.5986]])\n",
      "Iteration 15890 Training loss 0.11917860805988312 Validation loss 0.1160663440823555 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.5076],\n",
      "        [0.6651]])\n",
      "Iteration 15900 Training loss 0.11744330823421478 Validation loss 0.11611844599246979 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5178],\n",
      "        [0.4952]])\n",
      "Iteration 15910 Training loss 0.11831548810005188 Validation loss 0.1161181777715683 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4503],\n",
      "        [0.5396]])\n",
      "Iteration 15920 Training loss 0.11818839609622955 Validation loss 0.11605869233608246 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4386],\n",
      "        [0.3135]])\n",
      "Iteration 15930 Training loss 0.11771812289953232 Validation loss 0.11604411900043488 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.6216],\n",
      "        [0.3858]])\n",
      "Iteration 15940 Training loss 0.11703953146934509 Validation loss 0.11611950397491455 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.7344],\n",
      "        [0.4284]])\n",
      "Iteration 15950 Training loss 0.11766339093446732 Validation loss 0.11604981124401093 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4500],\n",
      "        [0.6264]])\n",
      "Iteration 15960 Training loss 0.11696290969848633 Validation loss 0.11604451388120651 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.3818],\n",
      "        [0.6297]])\n",
      "Iteration 15970 Training loss 0.1166246309876442 Validation loss 0.11606425046920776 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4856],\n",
      "        [0.4187]])\n",
      "Iteration 15980 Training loss 0.11710478365421295 Validation loss 0.11605196446180344 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.3527],\n",
      "        [0.6045]])\n",
      "Iteration 15990 Training loss 0.11705878376960754 Validation loss 0.11601302772760391 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.3396],\n",
      "        [0.7056]])\n",
      "Iteration 16000 Training loss 0.11658833175897598 Validation loss 0.11601017415523529 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.6056],\n",
      "        [0.6075]])\n",
      "Iteration 16010 Training loss 0.11698635667562485 Validation loss 0.11601141095161438 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.6277],\n",
      "        [0.5901]])\n",
      "Iteration 16020 Training loss 0.11698631942272186 Validation loss 0.11601302772760391 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5947],\n",
      "        [0.4986]])\n",
      "Iteration 16030 Training loss 0.11802628636360168 Validation loss 0.11609116941690445 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.6223],\n",
      "        [0.4783]])\n",
      "Iteration 16040 Training loss 0.11895415931940079 Validation loss 0.11609100550413132 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.4723],\n",
      "        [0.2962]])\n",
      "Iteration 16050 Training loss 0.11694369465112686 Validation loss 0.11603059619665146 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.6176],\n",
      "        [0.5597]])\n",
      "Iteration 16060 Training loss 0.11754976212978363 Validation loss 0.11609958857297897 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5759],\n",
      "        [0.4367]])\n",
      "Iteration 16070 Training loss 0.1177120953798294 Validation loss 0.11604497581720352 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5074],\n",
      "        [0.5997]])\n",
      "Iteration 16080 Training loss 0.11594444513320923 Validation loss 0.11612139642238617 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5282],\n",
      "        [0.2495]])\n",
      "Iteration 16090 Training loss 0.11774344742298126 Validation loss 0.11612595617771149 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4985],\n",
      "        [0.4425]])\n",
      "Iteration 16100 Training loss 0.1176023930311203 Validation loss 0.11604500561952591 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5420],\n",
      "        [0.2383]])\n",
      "Iteration 16110 Training loss 0.11697713285684586 Validation loss 0.11603428423404694 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.6861],\n",
      "        [0.3784]])\n",
      "Iteration 16120 Training loss 0.11714673042297363 Validation loss 0.1160597950220108 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.3906],\n",
      "        [0.2890]])\n",
      "Iteration 16130 Training loss 0.11763708293437958 Validation loss 0.11605370789766312 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5220],\n",
      "        [0.5842]])\n",
      "Iteration 16140 Training loss 0.1168624684214592 Validation loss 0.1160360723733902 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.6359],\n",
      "        [0.4562]])\n",
      "Iteration 16150 Training loss 0.1183372214436531 Validation loss 0.11600989103317261 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.5314],\n",
      "        [0.5232]])\n",
      "Iteration 16160 Training loss 0.11763690412044525 Validation loss 0.11599918454885483 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.3038],\n",
      "        [0.3522]])\n",
      "Iteration 16170 Training loss 0.11702153831720352 Validation loss 0.11600036919116974 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5316],\n",
      "        [0.3188]])\n",
      "Iteration 16180 Training loss 0.11805430054664612 Validation loss 0.11600887775421143 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.3845],\n",
      "        [0.6201]])\n",
      "Iteration 16190 Training loss 0.1177145466208458 Validation loss 0.1159946471452713 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4430],\n",
      "        [0.5463]])\n",
      "Iteration 16200 Training loss 0.11754045635461807 Validation loss 0.11599361896514893 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.4519],\n",
      "        [0.4551]])\n",
      "Iteration 16210 Training loss 0.117459736764431 Validation loss 0.11604565382003784 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.3262],\n",
      "        [0.3722]])\n",
      "Iteration 16220 Training loss 0.11790936440229416 Validation loss 0.11601806432008743 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6204],\n",
      "        [0.5555]])\n",
      "Iteration 16230 Training loss 0.11810768395662308 Validation loss 0.1160050630569458 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.4457],\n",
      "        [0.5182]])\n",
      "Iteration 16240 Training loss 0.11774251610040665 Validation loss 0.11598583310842514 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.2847],\n",
      "        [0.2787]])\n",
      "Iteration 16250 Training loss 0.11694144457578659 Validation loss 0.11603529006242752 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.3844],\n",
      "        [0.6127]])\n",
      "Iteration 16260 Training loss 0.11850117892026901 Validation loss 0.11599636822938919 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.2911],\n",
      "        [0.4858]])\n",
      "Iteration 16270 Training loss 0.1179504320025444 Validation loss 0.11616188287734985 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.4321],\n",
      "        [0.4586]])\n",
      "Iteration 16280 Training loss 0.1172187328338623 Validation loss 0.11600984632968903 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.5051],\n",
      "        [0.4763]])\n",
      "Iteration 16290 Training loss 0.11748193204402924 Validation loss 0.11604980379343033 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5349],\n",
      "        [0.5521]])\n",
      "Iteration 16300 Training loss 0.11672871559858322 Validation loss 0.11608351767063141 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.6984],\n",
      "        [0.6313]])\n",
      "Iteration 16310 Training loss 0.116762675344944 Validation loss 0.11600808054208755 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5955],\n",
      "        [0.5515]])\n",
      "Iteration 16320 Training loss 0.11729846149682999 Validation loss 0.116018146276474 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.3093],\n",
      "        [0.4285]])\n",
      "Iteration 16330 Training loss 0.11734450608491898 Validation loss 0.1160072460770607 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.4548],\n",
      "        [0.5124]])\n",
      "Iteration 16340 Training loss 0.11759449541568756 Validation loss 0.11601845175027847 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.3829],\n",
      "        [0.3354]])\n",
      "Iteration 16350 Training loss 0.11766814440488815 Validation loss 0.11615192890167236 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.4422],\n",
      "        [0.2817]])\n",
      "Iteration 16360 Training loss 0.11704245954751968 Validation loss 0.11600104719400406 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.5342],\n",
      "        [0.4962]])\n",
      "Iteration 16370 Training loss 0.11759922653436661 Validation loss 0.11598102748394012 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.4161],\n",
      "        [0.4708]])\n",
      "Iteration 16380 Training loss 0.11712770909070969 Validation loss 0.11596023291349411 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.6042],\n",
      "        [0.3688]])\n",
      "Iteration 16390 Training loss 0.11675490438938141 Validation loss 0.11596991866827011 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4442],\n",
      "        [0.4332]])\n",
      "Iteration 16400 Training loss 0.11725402623414993 Validation loss 0.11604742705821991 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.2285],\n",
      "        [0.5267]])\n",
      "Iteration 16410 Training loss 0.11684034019708633 Validation loss 0.11607915908098221 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5748],\n",
      "        [0.4276]])\n",
      "Iteration 16420 Training loss 0.11825081706047058 Validation loss 0.11598760634660721 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.4761],\n",
      "        [0.3744]])\n",
      "Iteration 16430 Training loss 0.11693227291107178 Validation loss 0.11598087847232819 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5803],\n",
      "        [0.3693]])\n",
      "Iteration 16440 Training loss 0.11775156855583191 Validation loss 0.11599289625883102 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5293],\n",
      "        [0.6579]])\n",
      "Iteration 16450 Training loss 0.11744476109743118 Validation loss 0.11596169322729111 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.7545],\n",
      "        [0.2544]])\n",
      "Iteration 16460 Training loss 0.11770147830247879 Validation loss 0.11598280817270279 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.6084],\n",
      "        [0.5422]])\n",
      "Iteration 16470 Training loss 0.11776866763830185 Validation loss 0.11598509550094604 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.3369],\n",
      "        [0.5448]])\n",
      "Iteration 16480 Training loss 0.11707057803869247 Validation loss 0.11596935987472534 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.3800],\n",
      "        [0.3532]])\n",
      "Iteration 16490 Training loss 0.11645951122045517 Validation loss 0.11601690948009491 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5748],\n",
      "        [0.2786]])\n",
      "Iteration 16500 Training loss 0.11715108901262283 Validation loss 0.11595210433006287 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5861],\n",
      "        [0.4658]])\n",
      "Iteration 16510 Training loss 0.11774656176567078 Validation loss 0.11596298217773438 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.4633],\n",
      "        [0.6937]])\n",
      "Iteration 16520 Training loss 0.11774875968694687 Validation loss 0.11609072983264923 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5221],\n",
      "        [0.4851]])\n",
      "Iteration 16530 Training loss 0.11753706634044647 Validation loss 0.1159728392958641 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.3339],\n",
      "        [0.4813]])\n",
      "Iteration 16540 Training loss 0.11714824289083481 Validation loss 0.11600691825151443 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.4762],\n",
      "        [0.5478]])\n",
      "Iteration 16550 Training loss 0.11818931251764297 Validation loss 0.11600535362958908 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5538],\n",
      "        [0.4455]])\n",
      "Iteration 16560 Training loss 0.11770717054605484 Validation loss 0.1159738153219223 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.6362],\n",
      "        [0.6257]])\n",
      "Iteration 16570 Training loss 0.11859835684299469 Validation loss 0.1159827709197998 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.2699],\n",
      "        [0.4282]])\n",
      "Iteration 16580 Training loss 0.11617440730333328 Validation loss 0.11600691825151443 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.4608],\n",
      "        [0.4030]])\n",
      "Iteration 16590 Training loss 0.11672082543373108 Validation loss 0.11597314476966858 Accuracy 0.6131666898727417\n",
      "Output tensor([[0.4441],\n",
      "        [0.3512]])\n",
      "Iteration 16600 Training loss 0.11667758226394653 Validation loss 0.11598097532987595 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.4283],\n",
      "        [0.5071]])\n",
      "Iteration 16610 Training loss 0.11643483489751816 Validation loss 0.11597979813814163 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.6457],\n",
      "        [0.4202]])\n",
      "Iteration 16620 Training loss 0.11726918816566467 Validation loss 0.11605887860059738 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.5260],\n",
      "        [0.6358]])\n",
      "Iteration 16630 Training loss 0.11571241915225983 Validation loss 0.11602356284856796 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.5347],\n",
      "        [0.6474]])\n",
      "Iteration 16640 Training loss 0.11582381278276443 Validation loss 0.1159999817609787 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.3479],\n",
      "        [0.2622]])\n",
      "Iteration 16650 Training loss 0.11662434786558151 Validation loss 0.11597874015569687 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.1662],\n",
      "        [0.6002]])\n",
      "Iteration 16660 Training loss 0.11716859042644501 Validation loss 0.11603593081235886 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4843],\n",
      "        [0.5054]])\n",
      "Iteration 16670 Training loss 0.11809016764163971 Validation loss 0.11598756164312363 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.6470],\n",
      "        [0.3986]])\n",
      "Iteration 16680 Training loss 0.1173330768942833 Validation loss 0.11597397923469543 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.5792],\n",
      "        [0.4933]])\n",
      "Iteration 16690 Training loss 0.11701052635908127 Validation loss 0.11598140746355057 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.4958],\n",
      "        [0.5474]])\n",
      "Iteration 16700 Training loss 0.11724483221769333 Validation loss 0.11594361066818237 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.2669],\n",
      "        [0.5473]])\n",
      "Iteration 16710 Training loss 0.11707370728254318 Validation loss 0.11595793813467026 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4297],\n",
      "        [0.2468]])\n",
      "Iteration 16720 Training loss 0.11760956048965454 Validation loss 0.11595015227794647 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.3039],\n",
      "        [0.3289]])\n",
      "Iteration 16730 Training loss 0.11742283403873444 Validation loss 0.11612614244222641 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4595],\n",
      "        [0.6074]])\n",
      "Iteration 16740 Training loss 0.11811289191246033 Validation loss 0.11595804989337921 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.5342],\n",
      "        [0.6845]])\n",
      "Iteration 16750 Training loss 0.11714494973421097 Validation loss 0.11594875156879425 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.6166],\n",
      "        [0.6239]])\n",
      "Iteration 16760 Training loss 0.11794483661651611 Validation loss 0.11597919464111328 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.6484],\n",
      "        [0.3392]])\n",
      "Iteration 16770 Training loss 0.11628428846597672 Validation loss 0.11600945889949799 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.5128],\n",
      "        [0.6774]])\n",
      "Iteration 16780 Training loss 0.11784671992063522 Validation loss 0.11593925952911377 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.3061],\n",
      "        [0.3215]])\n",
      "Iteration 16790 Training loss 0.11719641089439392 Validation loss 0.11601842939853668 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.4867],\n",
      "        [0.5608]])\n",
      "Iteration 16800 Training loss 0.11745758354663849 Validation loss 0.11598549038171768 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.3452],\n",
      "        [0.4824]])\n",
      "Iteration 16810 Training loss 0.11798635125160217 Validation loss 0.11598528921604156 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4268],\n",
      "        [0.5632]])\n",
      "Iteration 16820 Training loss 0.11655545234680176 Validation loss 0.11598832905292511 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.5078],\n",
      "        [0.3549]])\n",
      "Iteration 16830 Training loss 0.11702488362789154 Validation loss 0.11606177687644958 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.5053],\n",
      "        [0.5905]])\n",
      "Iteration 16840 Training loss 0.11716961860656738 Validation loss 0.11597952991724014 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.2094],\n",
      "        [0.4092]])\n",
      "Iteration 16850 Training loss 0.1166924312710762 Validation loss 0.11606165766716003 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.1309],\n",
      "        [0.5352]])\n",
      "Iteration 16860 Training loss 0.11814586073160172 Validation loss 0.11593642830848694 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5568],\n",
      "        [0.3673]])\n",
      "Iteration 16870 Training loss 0.11639919131994247 Validation loss 0.11601778119802475 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.5339],\n",
      "        [0.5846]])\n",
      "Iteration 16880 Training loss 0.11824218928813934 Validation loss 0.11598197370767593 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.2920],\n",
      "        [0.4343]])\n",
      "Iteration 16890 Training loss 0.11777545511722565 Validation loss 0.11597377806901932 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4239],\n",
      "        [0.3872]])\n",
      "Iteration 16900 Training loss 0.1170617863535881 Validation loss 0.11595192551612854 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5655],\n",
      "        [0.5835]])\n",
      "Iteration 16910 Training loss 0.11708597093820572 Validation loss 0.1159784346818924 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4358],\n",
      "        [0.2690]])\n",
      "Iteration 16920 Training loss 0.11810670047998428 Validation loss 0.11594853550195694 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5345],\n",
      "        [0.5182]])\n",
      "Iteration 16930 Training loss 0.11772185564041138 Validation loss 0.11595263332128525 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.2742],\n",
      "        [0.6043]])\n",
      "Iteration 16940 Training loss 0.11832405626773834 Validation loss 0.11595392972230911 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6439],\n",
      "        [0.4120]])\n",
      "Iteration 16950 Training loss 0.11712927371263504 Validation loss 0.11593298614025116 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.5522],\n",
      "        [0.3664]])\n",
      "Iteration 16960 Training loss 0.11666321009397507 Validation loss 0.11594972759485245 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5031],\n",
      "        [0.2974]])\n",
      "Iteration 16970 Training loss 0.11663319915533066 Validation loss 0.11603479832410812 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5608],\n",
      "        [0.4805]])\n",
      "Iteration 16980 Training loss 0.11702879518270493 Validation loss 0.11592567712068558 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6551],\n",
      "        [0.4242]])\n",
      "Iteration 16990 Training loss 0.11696657538414001 Validation loss 0.11591050028800964 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.6411],\n",
      "        [0.6193]])\n",
      "Iteration 17000 Training loss 0.11646966636180878 Validation loss 0.11589997261762619 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5440],\n",
      "        [0.6102]])\n",
      "Iteration 17010 Training loss 0.11805973201990128 Validation loss 0.11591602861881256 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5137],\n",
      "        [0.4566]])\n",
      "Iteration 17020 Training loss 0.11619730293750763 Validation loss 0.11610016226768494 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.3229],\n",
      "        [0.6450]])\n",
      "Iteration 17030 Training loss 0.11790680885314941 Validation loss 0.11590971052646637 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.4032],\n",
      "        [0.7756]])\n",
      "Iteration 17040 Training loss 0.11677449196577072 Validation loss 0.11590444296598434 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5992],\n",
      "        [0.4108]])\n",
      "Iteration 17050 Training loss 0.1170329600572586 Validation loss 0.11598044633865356 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5456],\n",
      "        [0.5434]])\n",
      "Iteration 17060 Training loss 0.11776842921972275 Validation loss 0.11598371714353561 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5449],\n",
      "        [0.4993]])\n",
      "Iteration 17070 Training loss 0.11728505790233612 Validation loss 0.11602112650871277 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.2994],\n",
      "        [0.4816]])\n",
      "Iteration 17080 Training loss 0.11684539914131165 Validation loss 0.1159452274441719 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.6197],\n",
      "        [0.4672]])\n",
      "Iteration 17090 Training loss 0.11777564138174057 Validation loss 0.11600624769926071 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4406],\n",
      "        [0.5497]])\n",
      "Iteration 17100 Training loss 0.11686623096466064 Validation loss 0.1159229725599289 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4846],\n",
      "        [0.3142]])\n",
      "Iteration 17110 Training loss 0.11687201261520386 Validation loss 0.11594955623149872 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.4514],\n",
      "        [0.4412]])\n",
      "Iteration 17120 Training loss 0.11762535572052002 Validation loss 0.11589760333299637 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4508],\n",
      "        [0.4875]])\n",
      "Iteration 17130 Training loss 0.11749503761529922 Validation loss 0.11590196937322617 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.4830],\n",
      "        [0.4576]])\n",
      "Iteration 17140 Training loss 0.11661289632320404 Validation loss 0.11587580293416977 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5955],\n",
      "        [0.4557]])\n",
      "Iteration 17150 Training loss 0.11700423061847687 Validation loss 0.11591551452875137 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5263],\n",
      "        [0.4680]])\n",
      "Iteration 17160 Training loss 0.11793868988752365 Validation loss 0.11586733162403107 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5295],\n",
      "        [0.5757]])\n",
      "Iteration 17170 Training loss 0.11735772341489792 Validation loss 0.11594575643539429 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5425],\n",
      "        [0.6831]])\n",
      "Iteration 17180 Training loss 0.11748957633972168 Validation loss 0.11589737981557846 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5170],\n",
      "        [0.6106]])\n",
      "Iteration 17190 Training loss 0.11757724732160568 Validation loss 0.11589174717664719 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.2525],\n",
      "        [0.6204]])\n",
      "Iteration 17200 Training loss 0.11701255291700363 Validation loss 0.11593494564294815 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.3397],\n",
      "        [0.2970]])\n",
      "Iteration 17210 Training loss 0.11741953343153 Validation loss 0.11592193692922592 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.3345],\n",
      "        [0.4229]])\n",
      "Iteration 17220 Training loss 0.11724857240915298 Validation loss 0.11595440655946732 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5515],\n",
      "        [0.2632]])\n",
      "Iteration 17230 Training loss 0.11778921633958817 Validation loss 0.11595029383897781 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.5925],\n",
      "        [0.4878]])\n",
      "Iteration 17240 Training loss 0.11736766248941422 Validation loss 0.11587417870759964 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.7369],\n",
      "        [0.4966]])\n",
      "Iteration 17250 Training loss 0.1172603964805603 Validation loss 0.11591115593910217 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.3995],\n",
      "        [0.4072]])\n",
      "Iteration 17260 Training loss 0.1171908974647522 Validation loss 0.11586836725473404 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.3362],\n",
      "        [0.4322]])\n",
      "Iteration 17270 Training loss 0.11752636730670929 Validation loss 0.1159675195813179 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.6569],\n",
      "        [0.3119]])\n",
      "Iteration 17280 Training loss 0.11668501794338226 Validation loss 0.11586538702249527 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5699],\n",
      "        [0.4933]])\n",
      "Iteration 17290 Training loss 0.11615201830863953 Validation loss 0.11588777601718903 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4717],\n",
      "        [0.2816]])\n",
      "Iteration 17300 Training loss 0.11806251108646393 Validation loss 0.11584237962961197 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.4406],\n",
      "        [0.4037]])\n",
      "Iteration 17310 Training loss 0.11753179877996445 Validation loss 0.11584042012691498 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5546],\n",
      "        [0.5182]])\n",
      "Iteration 17320 Training loss 0.1182713434100151 Validation loss 0.1158652976155281 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.2346],\n",
      "        [0.6076]])\n",
      "Iteration 17330 Training loss 0.11717689782381058 Validation loss 0.11588562279939651 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.5023],\n",
      "        [0.3125]])\n",
      "Iteration 17340 Training loss 0.11826995760202408 Validation loss 0.11588582396507263 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.6662],\n",
      "        [0.4496]])\n",
      "Iteration 17350 Training loss 0.11543617397546768 Validation loss 0.11589190363883972 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.6112],\n",
      "        [0.5393]])\n",
      "Iteration 17360 Training loss 0.11762721836566925 Validation loss 0.11590584367513657 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.5611],\n",
      "        [0.4371]])\n",
      "Iteration 17370 Training loss 0.11709675937891006 Validation loss 0.11593428254127502 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4884],\n",
      "        [0.4028]])\n",
      "Iteration 17380 Training loss 0.11736003309488297 Validation loss 0.11587371677160263 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5015],\n",
      "        [0.5974]])\n",
      "Iteration 17390 Training loss 0.1153603345155716 Validation loss 0.11587589234113693 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.3243],\n",
      "        [0.5592]])\n",
      "Iteration 17400 Training loss 0.11638033390045166 Validation loss 0.11587290465831757 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.3783],\n",
      "        [0.3611]])\n",
      "Iteration 17410 Training loss 0.11769850552082062 Validation loss 0.11589788645505905 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.6324],\n",
      "        [0.4428]])\n",
      "Iteration 17420 Training loss 0.11717426776885986 Validation loss 0.11594285070896149 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.2534],\n",
      "        [0.5188]])\n",
      "Iteration 17430 Training loss 0.11698555201292038 Validation loss 0.11592661589384079 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6160],\n",
      "        [0.4944]])\n",
      "Iteration 17440 Training loss 0.11713527143001556 Validation loss 0.11591567099094391 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4622],\n",
      "        [0.2955]])\n",
      "Iteration 17450 Training loss 0.11683506518602371 Validation loss 0.11585904657840729 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6895],\n",
      "        [0.5749]])\n",
      "Iteration 17460 Training loss 0.11727958172559738 Validation loss 0.11586005985736847 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5816],\n",
      "        [0.3793]])\n",
      "Iteration 17470 Training loss 0.11638541519641876 Validation loss 0.11587435752153397 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5577],\n",
      "        [0.5894]])\n",
      "Iteration 17480 Training loss 0.11742384731769562 Validation loss 0.11592107266187668 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.5093],\n",
      "        [0.5331]])\n",
      "Iteration 17490 Training loss 0.11769221723079681 Validation loss 0.11588142812252045 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.5648],\n",
      "        [0.5084]])\n",
      "Iteration 17500 Training loss 0.11682036519050598 Validation loss 0.11590665578842163 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.5671],\n",
      "        [0.7139]])\n",
      "Iteration 17510 Training loss 0.1180429458618164 Validation loss 0.11589660495519638 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5808],\n",
      "        [0.3778]])\n",
      "Iteration 17520 Training loss 0.11777493357658386 Validation loss 0.11586475372314453 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.4064],\n",
      "        [0.5545]])\n",
      "Iteration 17530 Training loss 0.1176358014345169 Validation loss 0.11586761474609375 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.4446],\n",
      "        [0.3856]])\n",
      "Iteration 17540 Training loss 0.11711738258600235 Validation loss 0.115849070250988 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.4882],\n",
      "        [0.5844]])\n",
      "Iteration 17550 Training loss 0.11629433929920197 Validation loss 0.11586061865091324 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5579],\n",
      "        [0.2449]])\n",
      "Iteration 17560 Training loss 0.11759409308433533 Validation loss 0.115898497402668 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.2114],\n",
      "        [0.6603]])\n",
      "Iteration 17570 Training loss 0.11939873546361923 Validation loss 0.11591992527246475 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5884],\n",
      "        [0.5275]])\n",
      "Iteration 17580 Training loss 0.11717692017555237 Validation loss 0.11589141935110092 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5115],\n",
      "        [0.6453]])\n",
      "Iteration 17590 Training loss 0.11675257235765457 Validation loss 0.11588624864816666 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.3279],\n",
      "        [0.3109]])\n",
      "Iteration 17600 Training loss 0.1168520599603653 Validation loss 0.11584063619375229 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.4706],\n",
      "        [0.5201]])\n",
      "Iteration 17610 Training loss 0.11725695431232452 Validation loss 0.11585556715726852 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5680],\n",
      "        [0.6149]])\n",
      "Iteration 17620 Training loss 0.11754464358091354 Validation loss 0.11594435572624207 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.5709],\n",
      "        [0.5757]])\n",
      "Iteration 17630 Training loss 0.11671461910009384 Validation loss 0.115944042801857 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.3260],\n",
      "        [0.2657]])\n",
      "Iteration 17640 Training loss 0.11721327900886536 Validation loss 0.11584478616714478 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.4919],\n",
      "        [0.5560]])\n",
      "Iteration 17650 Training loss 0.11758514493703842 Validation loss 0.11584436148405075 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5443],\n",
      "        [0.6145]])\n",
      "Iteration 17660 Training loss 0.11634470522403717 Validation loss 0.1158825159072876 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.6449],\n",
      "        [0.5791]])\n",
      "Iteration 17670 Training loss 0.11653078347444534 Validation loss 0.11585765331983566 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.3203],\n",
      "        [0.5023]])\n",
      "Iteration 17680 Training loss 0.11711215227842331 Validation loss 0.1158871054649353 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5609],\n",
      "        [0.6719]])\n",
      "Iteration 17690 Training loss 0.11617875099182129 Validation loss 0.11585549265146255 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.5354],\n",
      "        [0.4160]])\n",
      "Iteration 17700 Training loss 0.11708233505487442 Validation loss 0.11584308743476868 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.6548],\n",
      "        [0.6205]])\n",
      "Iteration 17710 Training loss 0.1175612360239029 Validation loss 0.11590216308832169 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.5043],\n",
      "        [0.5308]])\n",
      "Iteration 17720 Training loss 0.11749200522899628 Validation loss 0.11582627147436142 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.5934],\n",
      "        [0.5940]])\n",
      "Iteration 17730 Training loss 0.11735060811042786 Validation loss 0.11589064449071884 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.4890],\n",
      "        [0.5834]])\n",
      "Iteration 17740 Training loss 0.11715041846036911 Validation loss 0.11581232398748398 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.2977],\n",
      "        [0.4306]])\n",
      "Iteration 17750 Training loss 0.11667799204587936 Validation loss 0.1158129870891571 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4627],\n",
      "        [0.2612]])\n",
      "Iteration 17760 Training loss 0.11669476330280304 Validation loss 0.11580538749694824 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.7242],\n",
      "        [0.6348]])\n",
      "Iteration 17770 Training loss 0.1165904775261879 Validation loss 0.11582692712545395 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.7405],\n",
      "        [0.4458]])\n",
      "Iteration 17780 Training loss 0.11711398512125015 Validation loss 0.11585228145122528 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.3860],\n",
      "        [0.4253]])\n",
      "Iteration 17790 Training loss 0.11866983026266098 Validation loss 0.11585111171007156 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5186],\n",
      "        [0.5774]])\n",
      "Iteration 17800 Training loss 0.11745147407054901 Validation loss 0.1158488467335701 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.6663],\n",
      "        [0.5602]])\n",
      "Iteration 17810 Training loss 0.11833269149065018 Validation loss 0.11588303744792938 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.6190],\n",
      "        [0.5577]])\n",
      "Iteration 17820 Training loss 0.1175476685166359 Validation loss 0.11587181687355042 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.3123],\n",
      "        [0.4754]])\n",
      "Iteration 17830 Training loss 0.11787306517362595 Validation loss 0.11583291739225388 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.6743],\n",
      "        [0.7471]])\n",
      "Iteration 17840 Training loss 0.11604445427656174 Validation loss 0.1158263236284256 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.4868],\n",
      "        [0.5091]])\n",
      "Iteration 17850 Training loss 0.11702939867973328 Validation loss 0.11583214998245239 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.3352],\n",
      "        [0.3341]])\n",
      "Iteration 17860 Training loss 0.11761929094791412 Validation loss 0.1158299371600151 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.4458],\n",
      "        [0.5012]])\n",
      "Iteration 17870 Training loss 0.1166251003742218 Validation loss 0.11585572361946106 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5817],\n",
      "        [0.4909]])\n",
      "Iteration 17880 Training loss 0.11781004071235657 Validation loss 0.11601747572422028 Accuracy 0.6128333210945129\n",
      "Output tensor([[0.5210],\n",
      "        [0.5968]])\n",
      "Iteration 17890 Training loss 0.11768750101327896 Validation loss 0.11581985652446747 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.7180],\n",
      "        [0.2971]])\n",
      "Iteration 17900 Training loss 0.11718104779720306 Validation loss 0.11584755778312683 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.6120],\n",
      "        [0.3303]])\n",
      "Iteration 17910 Training loss 0.11765413731336594 Validation loss 0.11582376062870026 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.4485],\n",
      "        [0.4724]])\n",
      "Iteration 17920 Training loss 0.11729614436626434 Validation loss 0.11581886559724808 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4015],\n",
      "        [0.4611]])\n",
      "Iteration 17930 Training loss 0.1164792850613594 Validation loss 0.11594691872596741 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5681],\n",
      "        [0.6305]])\n",
      "Iteration 17940 Training loss 0.11758166551589966 Validation loss 0.11581947654485703 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.6081],\n",
      "        [0.6664]])\n",
      "Iteration 17950 Training loss 0.11736050248146057 Validation loss 0.11580627411603928 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6844],\n",
      "        [0.4191]])\n",
      "Iteration 17960 Training loss 0.11774002760648727 Validation loss 0.11579550057649612 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.4334],\n",
      "        [0.3071]])\n",
      "Iteration 17970 Training loss 0.11664135009050369 Validation loss 0.1158054918050766 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5961],\n",
      "        [0.3613]])\n",
      "Iteration 17980 Training loss 0.11685867607593536 Validation loss 0.11589039117097855 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.2496],\n",
      "        [0.3584]])\n",
      "Iteration 17990 Training loss 0.11842677742242813 Validation loss 0.11576899886131287 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.6764],\n",
      "        [0.6043]])\n",
      "Iteration 18000 Training loss 0.1173570454120636 Validation loss 0.11577259004116058 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5228],\n",
      "        [0.4729]])\n",
      "Iteration 18010 Training loss 0.11781349033117294 Validation loss 0.11583437025547028 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5242],\n",
      "        [0.5604]])\n",
      "Iteration 18020 Training loss 0.11852063983678818 Validation loss 0.1159210130572319 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.2168],\n",
      "        [0.4837]])\n",
      "Iteration 18030 Training loss 0.11750759184360504 Validation loss 0.11578747630119324 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.3836],\n",
      "        [0.6523]])\n",
      "Iteration 18040 Training loss 0.11679232120513916 Validation loss 0.1157790943980217 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.5561],\n",
      "        [0.6152]])\n",
      "Iteration 18050 Training loss 0.11706532537937164 Validation loss 0.11582016944885254 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.3815],\n",
      "        [0.4786]])\n",
      "Iteration 18060 Training loss 0.1171078011393547 Validation loss 0.11579610407352448 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5662],\n",
      "        [0.6959]])\n",
      "Iteration 18070 Training loss 0.1172395870089531 Validation loss 0.11578969657421112 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.4045],\n",
      "        [0.6455]])\n",
      "Iteration 18080 Training loss 0.11723800748586655 Validation loss 0.1158016175031662 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.4302],\n",
      "        [0.5996]])\n",
      "Iteration 18090 Training loss 0.11652041971683502 Validation loss 0.11582668870687485 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.4301],\n",
      "        [0.5937]])\n",
      "Iteration 18100 Training loss 0.11819888651371002 Validation loss 0.11581715941429138 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5045],\n",
      "        [0.5479]])\n",
      "Iteration 18110 Training loss 0.11785520613193512 Validation loss 0.11581769585609436 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.6163],\n",
      "        [0.3698]])\n",
      "Iteration 18120 Training loss 0.11681953817605972 Validation loss 0.11585097014904022 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.2290],\n",
      "        [0.4965]])\n",
      "Iteration 18130 Training loss 0.11663825809955597 Validation loss 0.11590684205293655 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4557],\n",
      "        [0.6247]])\n",
      "Iteration 18140 Training loss 0.11772578954696655 Validation loss 0.11588220298290253 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5930],\n",
      "        [0.4346]])\n",
      "Iteration 18150 Training loss 0.11685381829738617 Validation loss 0.11583579331636429 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5182],\n",
      "        [0.5318]])\n",
      "Iteration 18160 Training loss 0.11691825091838837 Validation loss 0.1158161610364914 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.4639],\n",
      "        [0.3376]])\n",
      "Iteration 18170 Training loss 0.11764882504940033 Validation loss 0.11582623422145844 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.4737],\n",
      "        [0.5104]])\n",
      "Iteration 18180 Training loss 0.11642962694168091 Validation loss 0.11580438166856766 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.6905],\n",
      "        [0.5665]])\n",
      "Iteration 18190 Training loss 0.11739621311426163 Validation loss 0.11582960188388824 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.3770],\n",
      "        [0.4287]])\n",
      "Iteration 18200 Training loss 0.11660020053386688 Validation loss 0.1157807931303978 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.4322],\n",
      "        [0.3932]])\n",
      "Iteration 18210 Training loss 0.1159185990691185 Validation loss 0.11579213291406631 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.3495],\n",
      "        [0.5524]])\n",
      "Iteration 18220 Training loss 0.11687768250703812 Validation loss 0.11581339687108994 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5694],\n",
      "        [0.3777]])\n",
      "Iteration 18230 Training loss 0.11774448305368423 Validation loss 0.11583521217107773 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4876],\n",
      "        [0.3072]])\n",
      "Iteration 18240 Training loss 0.1176084354519844 Validation loss 0.11582515388727188 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.2971],\n",
      "        [0.4091]])\n",
      "Iteration 18250 Training loss 0.11745418608188629 Validation loss 0.115765780210495 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.4302],\n",
      "        [0.5577]])\n",
      "Iteration 18260 Training loss 0.11806583404541016 Validation loss 0.11576840281486511 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.0598],\n",
      "        [0.5172]])\n",
      "Iteration 18270 Training loss 0.11709538847208023 Validation loss 0.11579255014657974 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.5735],\n",
      "        [0.4152]])\n",
      "Iteration 18280 Training loss 0.11638464033603668 Validation loss 0.11574915796518326 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.5742],\n",
      "        [0.5725]])\n",
      "Iteration 18290 Training loss 0.11820997297763824 Validation loss 0.11580275744199753 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.6825],\n",
      "        [0.4949]])\n",
      "Iteration 18300 Training loss 0.1172737330198288 Validation loss 0.11576027423143387 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.3616],\n",
      "        [0.4448]])\n",
      "Iteration 18310 Training loss 0.11790858954191208 Validation loss 0.11579115688800812 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5161],\n",
      "        [0.3868]])\n",
      "Iteration 18320 Training loss 0.11606653034687042 Validation loss 0.11577509343624115 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.3939],\n",
      "        [0.5473]])\n",
      "Iteration 18330 Training loss 0.11642776429653168 Validation loss 0.11578099429607391 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.2025],\n",
      "        [0.5005]])\n",
      "Iteration 18340 Training loss 0.11748338490724564 Validation loss 0.1157798022031784 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.4533],\n",
      "        [0.4243]])\n",
      "Iteration 18350 Training loss 0.11667636036872864 Validation loss 0.11581189930438995 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.6851],\n",
      "        [0.6254]])\n",
      "Iteration 18360 Training loss 0.11721844226121902 Validation loss 0.11583283543586731 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.7934],\n",
      "        [0.4153]])\n",
      "Iteration 18370 Training loss 0.11586505174636841 Validation loss 0.11579661071300507 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.6183],\n",
      "        [0.5070]])\n",
      "Iteration 18380 Training loss 0.11668474227190018 Validation loss 0.11579424887895584 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.3768],\n",
      "        [0.2850]])\n",
      "Iteration 18390 Training loss 0.11684489250183105 Validation loss 0.11580029129981995 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.6212],\n",
      "        [0.3959]])\n",
      "Iteration 18400 Training loss 0.11848625540733337 Validation loss 0.11576501280069351 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.4494],\n",
      "        [0.5847]])\n",
      "Iteration 18410 Training loss 0.11773993819952011 Validation loss 0.11575762927532196 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5510],\n",
      "        [0.3578]])\n",
      "Iteration 18420 Training loss 0.1165885403752327 Validation loss 0.11577929556369781 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.5368],\n",
      "        [0.5535]])\n",
      "Iteration 18430 Training loss 0.11643946170806885 Validation loss 0.1157270073890686 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.4783],\n",
      "        [0.4505]])\n",
      "Iteration 18440 Training loss 0.11733902245759964 Validation loss 0.11591333150863647 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5427],\n",
      "        [0.5380]])\n",
      "Iteration 18450 Training loss 0.1169569194316864 Validation loss 0.1158791109919548 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.5289],\n",
      "        [0.6060]])\n",
      "Iteration 18460 Training loss 0.1175934374332428 Validation loss 0.11573005467653275 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.5490],\n",
      "        [0.4137]])\n",
      "Iteration 18470 Training loss 0.11695554852485657 Validation loss 0.11578425765037537 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.5786],\n",
      "        [0.5212]])\n",
      "Iteration 18480 Training loss 0.11699272692203522 Validation loss 0.11578026413917542 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5300],\n",
      "        [0.5654]])\n",
      "Iteration 18490 Training loss 0.11802536994218826 Validation loss 0.11580739170312881 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.4624],\n",
      "        [0.4618]])\n",
      "Iteration 18500 Training loss 0.1182040274143219 Validation loss 0.11579156666994095 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5469],\n",
      "        [0.3496]])\n",
      "Iteration 18510 Training loss 0.11809701472520828 Validation loss 0.11576148122549057 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.4646],\n",
      "        [0.4993]])\n",
      "Iteration 18520 Training loss 0.11802531778812408 Validation loss 0.11576442420482635 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.4132],\n",
      "        [0.5131]])\n",
      "Iteration 18530 Training loss 0.11729604005813599 Validation loss 0.11572939157485962 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.3351],\n",
      "        [0.4578]])\n",
      "Iteration 18540 Training loss 0.1155654639005661 Validation loss 0.11573073267936707 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.3155],\n",
      "        [0.4604]])\n",
      "Iteration 18550 Training loss 0.11804291605949402 Validation loss 0.11578454822301865 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4954],\n",
      "        [0.5332]])\n",
      "Iteration 18560 Training loss 0.11682752519845963 Validation loss 0.11585459113121033 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4703],\n",
      "        [0.4636]])\n",
      "Iteration 18570 Training loss 0.11786999553442001 Validation loss 0.11580261588096619 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5139],\n",
      "        [0.5317]])\n",
      "Iteration 18580 Training loss 0.11649537831544876 Validation loss 0.11570249497890472 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.5328],\n",
      "        [0.5502]])\n",
      "Iteration 18590 Training loss 0.11742611974477768 Validation loss 0.11576659977436066 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.6486],\n",
      "        [0.4363]])\n",
      "Iteration 18600 Training loss 0.11625061184167862 Validation loss 0.11572200804948807 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5850],\n",
      "        [0.3375]])\n",
      "Iteration 18610 Training loss 0.11681433022022247 Validation loss 0.11569621413946152 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5607],\n",
      "        [0.6678]])\n",
      "Iteration 18620 Training loss 0.11798979341983795 Validation loss 0.11570887267589569 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.6817],\n",
      "        [0.4582]])\n",
      "Iteration 18630 Training loss 0.11799806356430054 Validation loss 0.11570369452238083 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.7361],\n",
      "        [0.5237]])\n",
      "Iteration 18640 Training loss 0.11671984195709229 Validation loss 0.11571738868951797 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.5773],\n",
      "        [0.4145]])\n",
      "Iteration 18650 Training loss 0.11746048927307129 Validation loss 0.11574961245059967 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6608],\n",
      "        [0.5480]])\n",
      "Iteration 18660 Training loss 0.11784471571445465 Validation loss 0.11573351174592972 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.3503],\n",
      "        [0.6348]])\n",
      "Iteration 18670 Training loss 0.1183663159608841 Validation loss 0.11577244848012924 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.6963],\n",
      "        [0.4943]])\n",
      "Iteration 18680 Training loss 0.11780752241611481 Validation loss 0.11587238311767578 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.3296],\n",
      "        [0.7680]])\n",
      "Iteration 18690 Training loss 0.1167050153017044 Validation loss 0.11580008268356323 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.3048],\n",
      "        [0.5591]])\n",
      "Iteration 18700 Training loss 0.1165376678109169 Validation loss 0.11587715893983841 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.4403],\n",
      "        [0.5919]])\n",
      "Iteration 18710 Training loss 0.1160491555929184 Validation loss 0.11578059941530228 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5115],\n",
      "        [0.4508]])\n",
      "Iteration 18720 Training loss 0.11712758243083954 Validation loss 0.11572729796171188 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.4679],\n",
      "        [0.2717]])\n",
      "Iteration 18730 Training loss 0.11831284314393997 Validation loss 0.11569711565971375 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.4035],\n",
      "        [0.5045]])\n",
      "Iteration 18740 Training loss 0.11665399372577667 Validation loss 0.11569619923830032 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6833],\n",
      "        [0.5169]])\n",
      "Iteration 18750 Training loss 0.11769705265760422 Validation loss 0.11569897830486298 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4925],\n",
      "        [0.3261]])\n",
      "Iteration 18760 Training loss 0.11639408022165298 Validation loss 0.11584808677434921 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.6426],\n",
      "        [0.5505]])\n",
      "Iteration 18770 Training loss 0.11634019017219543 Validation loss 0.11571405082941055 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.4621],\n",
      "        [0.3983]])\n",
      "Iteration 18780 Training loss 0.11632450670003891 Validation loss 0.11573289334774017 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.7157],\n",
      "        [0.6307]])\n",
      "Iteration 18790 Training loss 0.11839664727449417 Validation loss 0.11570678651332855 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6053],\n",
      "        [0.5118]])\n",
      "Iteration 18800 Training loss 0.11794775724411011 Validation loss 0.11572785675525665 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.3448],\n",
      "        [0.6878]])\n",
      "Iteration 18810 Training loss 0.11768098175525665 Validation loss 0.11585275083780289 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4620],\n",
      "        [0.5186]])\n",
      "Iteration 18820 Training loss 0.11829804629087448 Validation loss 0.11573448032140732 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.4519],\n",
      "        [0.4307]])\n",
      "Iteration 18830 Training loss 0.11706394702196121 Validation loss 0.11571285128593445 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.4356],\n",
      "        [0.3387]])\n",
      "Iteration 18840 Training loss 0.1155337318778038 Validation loss 0.11568860709667206 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.4682],\n",
      "        [0.4920]])\n",
      "Iteration 18850 Training loss 0.1177917867898941 Validation loss 0.11570733785629272 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.3475],\n",
      "        [0.5289]])\n",
      "Iteration 18860 Training loss 0.11793698370456696 Validation loss 0.11575744301080704 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.7256],\n",
      "        [0.6548]])\n",
      "Iteration 18870 Training loss 0.11843307316303253 Validation loss 0.115761898458004 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.2458],\n",
      "        [0.5738]])\n",
      "Iteration 18880 Training loss 0.11634712666273117 Validation loss 0.11574234813451767 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.6026],\n",
      "        [0.4481]])\n",
      "Iteration 18890 Training loss 0.1175219714641571 Validation loss 0.1157122403383255 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.2959],\n",
      "        [0.5191]])\n",
      "Iteration 18900 Training loss 0.11785117536783218 Validation loss 0.11571396887302399 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.6737],\n",
      "        [0.6952]])\n",
      "Iteration 18910 Training loss 0.11612097173929214 Validation loss 0.11572936177253723 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.4935],\n",
      "        [0.3821]])\n",
      "Iteration 18920 Training loss 0.11638214439153671 Validation loss 0.1157480850815773 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.5543],\n",
      "        [0.4970]])\n",
      "Iteration 18930 Training loss 0.11648212373256683 Validation loss 0.11579474061727524 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.7693],\n",
      "        [0.3175]])\n",
      "Iteration 18940 Training loss 0.11660407483577728 Validation loss 0.11575751006603241 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5875],\n",
      "        [0.4218]])\n",
      "Iteration 18950 Training loss 0.11637388914823532 Validation loss 0.1157994493842125 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.5370],\n",
      "        [0.6027]])\n",
      "Iteration 18960 Training loss 0.117775097489357 Validation loss 0.11572285741567612 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.3173],\n",
      "        [0.6772]])\n",
      "Iteration 18970 Training loss 0.11803597956895828 Validation loss 0.115706667304039 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.2731],\n",
      "        [0.5929]])\n",
      "Iteration 18980 Training loss 0.11628767102956772 Validation loss 0.11576129496097565 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5837],\n",
      "        [0.4965]])\n",
      "Iteration 18990 Training loss 0.11713529378175735 Validation loss 0.11576203256845474 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.5387],\n",
      "        [0.4261]])\n",
      "Iteration 19000 Training loss 0.11770293116569519 Validation loss 0.11569239944219589 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.2550],\n",
      "        [0.4995]])\n",
      "Iteration 19010 Training loss 0.11704056710004807 Validation loss 0.11572831124067307 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.4607],\n",
      "        [0.2721]])\n",
      "Iteration 19020 Training loss 0.11637284606695175 Validation loss 0.115709088742733 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.6667],\n",
      "        [0.3478]])\n",
      "Iteration 19030 Training loss 0.11671549826860428 Validation loss 0.11571602523326874 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.6484],\n",
      "        [0.4944]])\n",
      "Iteration 19040 Training loss 0.11735977977514267 Validation loss 0.11569996178150177 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.6056],\n",
      "        [0.7554]])\n",
      "Iteration 19050 Training loss 0.11728727072477341 Validation loss 0.11567911505699158 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.4617],\n",
      "        [0.3622]])\n",
      "Iteration 19060 Training loss 0.11819609254598618 Validation loss 0.11582556366920471 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4378],\n",
      "        [0.6734]])\n",
      "Iteration 19070 Training loss 0.11769324541091919 Validation loss 0.11578672379255295 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.4039],\n",
      "        [0.6650]])\n",
      "Iteration 19080 Training loss 0.11704538017511368 Validation loss 0.11586707830429077 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.3765],\n",
      "        [0.6041]])\n",
      "Iteration 19090 Training loss 0.11718327552080154 Validation loss 0.11570495367050171 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.7016],\n",
      "        [0.5832]])\n",
      "Iteration 19100 Training loss 0.11722932755947113 Validation loss 0.11569271981716156 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.2640],\n",
      "        [0.5897]])\n",
      "Iteration 19110 Training loss 0.11799217760562897 Validation loss 0.11572792381048203 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.4977],\n",
      "        [0.3599]])\n",
      "Iteration 19120 Training loss 0.11776795238256454 Validation loss 0.11578263342380524 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.5309],\n",
      "        [0.6570]])\n",
      "Iteration 19130 Training loss 0.11838456243276596 Validation loss 0.11573623865842819 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.6116],\n",
      "        [0.5989]])\n",
      "Iteration 19140 Training loss 0.11726485192775726 Validation loss 0.11571814119815826 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.3274],\n",
      "        [0.3901]])\n",
      "Iteration 19150 Training loss 0.11674575507640839 Validation loss 0.11567369848489761 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.3256],\n",
      "        [0.3243]])\n",
      "Iteration 19160 Training loss 0.11727654933929443 Validation loss 0.11567946523427963 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5443],\n",
      "        [0.4186]])\n",
      "Iteration 19170 Training loss 0.11674334853887558 Validation loss 0.1156887337565422 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.4798],\n",
      "        [0.3721]])\n",
      "Iteration 19180 Training loss 0.11680751293897629 Validation loss 0.11571165919303894 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.5236],\n",
      "        [0.4624]])\n",
      "Iteration 19190 Training loss 0.11743143945932388 Validation loss 0.11568831652402878 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5824],\n",
      "        [0.5526]])\n",
      "Iteration 19200 Training loss 0.1172175481915474 Validation loss 0.11572188138961792 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.2853],\n",
      "        [0.6448]])\n",
      "Iteration 19210 Training loss 0.11627823859453201 Validation loss 0.11576499789953232 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.6126],\n",
      "        [0.4585]])\n",
      "Iteration 19220 Training loss 0.11698450893163681 Validation loss 0.11568520218133926 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5429],\n",
      "        [0.3042]])\n",
      "Iteration 19230 Training loss 0.11761193722486496 Validation loss 0.11570750921964645 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.4876],\n",
      "        [0.6272]])\n",
      "Iteration 19240 Training loss 0.11693074554204941 Validation loss 0.11572880297899246 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.3673],\n",
      "        [0.4189]])\n",
      "Iteration 19250 Training loss 0.11743540316820145 Validation loss 0.1156979575753212 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.3990],\n",
      "        [0.6178]])\n",
      "Iteration 19260 Training loss 0.11802350729703903 Validation loss 0.11568033695220947 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.5753],\n",
      "        [0.3851]])\n",
      "Iteration 19270 Training loss 0.11917607486248016 Validation loss 0.115712970495224 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.4424],\n",
      "        [0.6540]])\n",
      "Iteration 19280 Training loss 0.11826902627944946 Validation loss 0.1156933382153511 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5130],\n",
      "        [0.4636]])\n",
      "Iteration 19290 Training loss 0.11726468801498413 Validation loss 0.11565587669610977 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.4150],\n",
      "        [0.1953]])\n",
      "Iteration 19300 Training loss 0.11849360913038254 Validation loss 0.11565188318490982 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.4379],\n",
      "        [0.5179]])\n",
      "Iteration 19310 Training loss 0.11622543632984161 Validation loss 0.11574561893939972 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.5572],\n",
      "        [0.6502]])\n",
      "Iteration 19320 Training loss 0.11726865917444229 Validation loss 0.1156710609793663 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5014],\n",
      "        [0.5760]])\n",
      "Iteration 19330 Training loss 0.1160925105214119 Validation loss 0.11566217243671417 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.4027],\n",
      "        [0.1047]])\n",
      "Iteration 19340 Training loss 0.11737430095672607 Validation loss 0.11569944024085999 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5554],\n",
      "        [0.7168]])\n",
      "Iteration 19350 Training loss 0.11720690876245499 Validation loss 0.11571870744228363 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.7435],\n",
      "        [0.5483]])\n",
      "Iteration 19360 Training loss 0.11726373434066772 Validation loss 0.11568186432123184 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5640],\n",
      "        [0.6458]])\n",
      "Iteration 19370 Training loss 0.11708168685436249 Validation loss 0.11568164080381393 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.1733],\n",
      "        [0.5361]])\n",
      "Iteration 19380 Training loss 0.11827918142080307 Validation loss 0.11576487123966217 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.4183],\n",
      "        [0.7586]])\n",
      "Iteration 19390 Training loss 0.11655301600694656 Validation loss 0.1156979501247406 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5502],\n",
      "        [0.6057]])\n",
      "Iteration 19400 Training loss 0.11772267520427704 Validation loss 0.1157112866640091 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5544],\n",
      "        [0.4904]])\n",
      "Iteration 19410 Training loss 0.11700111627578735 Validation loss 0.11569896340370178 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.3477],\n",
      "        [0.0626]])\n",
      "Iteration 19420 Training loss 0.1180381178855896 Validation loss 0.11582277715206146 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5581],\n",
      "        [0.3985]])\n",
      "Iteration 19430 Training loss 0.11598225682973862 Validation loss 0.11573836952447891 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.7471],\n",
      "        [0.6979]])\n",
      "Iteration 19440 Training loss 0.11665502190589905 Validation loss 0.1156858503818512 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.5570],\n",
      "        [0.5100]])\n",
      "Iteration 19450 Training loss 0.11638179421424866 Validation loss 0.11568769067525864 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5058],\n",
      "        [0.5329]])\n",
      "Iteration 19460 Training loss 0.11752992868423462 Validation loss 0.11566382646560669 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5404],\n",
      "        [0.6567]])\n",
      "Iteration 19470 Training loss 0.11741192638874054 Validation loss 0.11570239067077637 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.6192],\n",
      "        [0.2905]])\n",
      "Iteration 19480 Training loss 0.1169884204864502 Validation loss 0.11566156148910522 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5069],\n",
      "        [0.4885]])\n",
      "Iteration 19490 Training loss 0.11733514070510864 Validation loss 0.1156492531299591 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.3432],\n",
      "        [0.3691]])\n",
      "Iteration 19500 Training loss 0.11784794181585312 Validation loss 0.1156228631734848 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.5847],\n",
      "        [0.6480]])\n",
      "Iteration 19510 Training loss 0.11711496114730835 Validation loss 0.11562485992908478 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.6454],\n",
      "        [0.5372]])\n",
      "Iteration 19520 Training loss 0.1167629212141037 Validation loss 0.11562781035900116 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4620],\n",
      "        [0.5466]])\n",
      "Iteration 19530 Training loss 0.11691891402006149 Validation loss 0.11561985313892365 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.4426],\n",
      "        [0.5288]])\n",
      "Iteration 19540 Training loss 0.11795327067375183 Validation loss 0.11567223817110062 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5769],\n",
      "        [0.3943]])\n",
      "Iteration 19550 Training loss 0.11575423926115036 Validation loss 0.1156296655535698 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.4750],\n",
      "        [0.2381]])\n",
      "Iteration 19560 Training loss 0.11625510454177856 Validation loss 0.1157975047826767 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.4909],\n",
      "        [0.6057]])\n",
      "Iteration 19570 Training loss 0.11763480305671692 Validation loss 0.11564305424690247 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.3999],\n",
      "        [0.3062]])\n",
      "Iteration 19580 Training loss 0.1179722398519516 Validation loss 0.11567055433988571 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5151],\n",
      "        [0.1816]])\n",
      "Iteration 19590 Training loss 0.1173020452260971 Validation loss 0.1156654879450798 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.6723],\n",
      "        [0.4772]])\n",
      "Iteration 19600 Training loss 0.11560817807912827 Validation loss 0.11566857993602753 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.6632],\n",
      "        [0.3826]])\n",
      "Iteration 19610 Training loss 0.11640910804271698 Validation loss 0.11565671861171722 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.3958],\n",
      "        [0.5542]])\n",
      "Iteration 19620 Training loss 0.11769358068704605 Validation loss 0.11572261899709702 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.4892],\n",
      "        [0.4358]])\n",
      "Iteration 19630 Training loss 0.11682946979999542 Validation loss 0.11566222459077835 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.2998],\n",
      "        [0.5489]])\n",
      "Iteration 19640 Training loss 0.11745144426822662 Validation loss 0.11567438393831253 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.7391],\n",
      "        [0.4918]])\n",
      "Iteration 19650 Training loss 0.11704938113689423 Validation loss 0.11565406620502472 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.3906],\n",
      "        [0.4006]])\n",
      "Iteration 19660 Training loss 0.11681884527206421 Validation loss 0.11564172059297562 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.6145],\n",
      "        [0.5638]])\n",
      "Iteration 19670 Training loss 0.11779791116714478 Validation loss 0.11564599722623825 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.6913],\n",
      "        [0.4695]])\n",
      "Iteration 19680 Training loss 0.11614548414945602 Validation loss 0.11569621413946152 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4286],\n",
      "        [0.4269]])\n",
      "Iteration 19690 Training loss 0.1162501648068428 Validation loss 0.11567036807537079 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.4776],\n",
      "        [0.4912]])\n",
      "Iteration 19700 Training loss 0.11729970574378967 Validation loss 0.11567097902297974 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.6199],\n",
      "        [0.6011]])\n",
      "Iteration 19710 Training loss 0.11690059304237366 Validation loss 0.11565757542848587 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.6069],\n",
      "        [0.5799]])\n",
      "Iteration 19720 Training loss 0.11762841045856476 Validation loss 0.11565490067005157 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.4174],\n",
      "        [0.5581]])\n",
      "Iteration 19730 Training loss 0.11727757006883621 Validation loss 0.11567816138267517 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.4750],\n",
      "        [0.3121]])\n",
      "Iteration 19740 Training loss 0.11693662405014038 Validation loss 0.11572126299142838 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.6552],\n",
      "        [0.5374]])\n",
      "Iteration 19750 Training loss 0.11635837703943253 Validation loss 0.11561637371778488 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.4950],\n",
      "        [0.6163]])\n",
      "Iteration 19760 Training loss 0.11757293343544006 Validation loss 0.11569274961948395 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.6056],\n",
      "        [0.5305]])\n",
      "Iteration 19770 Training loss 0.11685793101787567 Validation loss 0.11563148349523544 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.2735],\n",
      "        [0.4917]])\n",
      "Iteration 19780 Training loss 0.11680234968662262 Validation loss 0.11561226099729538 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.5843],\n",
      "        [0.6275]])\n",
      "Iteration 19790 Training loss 0.11770794540643692 Validation loss 0.11567879468202591 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4070],\n",
      "        [0.5487]])\n",
      "Iteration 19800 Training loss 0.11720595508813858 Validation loss 0.11562700569629669 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.4370],\n",
      "        [0.5703]])\n",
      "Iteration 19810 Training loss 0.1180339977145195 Validation loss 0.11561782658100128 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.4415],\n",
      "        [0.5621]])\n",
      "Iteration 19820 Training loss 0.1184149757027626 Validation loss 0.11561378836631775 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.4848],\n",
      "        [0.4468]])\n",
      "Iteration 19830 Training loss 0.11826638132333755 Validation loss 0.11561102420091629 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.4552],\n",
      "        [0.4828]])\n",
      "Iteration 19840 Training loss 0.11799803376197815 Validation loss 0.11566963791847229 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.5081],\n",
      "        [0.2159]])\n",
      "Iteration 19850 Training loss 0.11596275866031647 Validation loss 0.11562686413526535 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5302],\n",
      "        [0.5717]])\n",
      "Iteration 19860 Training loss 0.11773556470870972 Validation loss 0.11561962962150574 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.2943],\n",
      "        [0.5127]])\n",
      "Iteration 19870 Training loss 0.117600217461586 Validation loss 0.11559545248746872 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.7064],\n",
      "        [0.4029]])\n",
      "Iteration 19880 Training loss 0.11693530529737473 Validation loss 0.11557357013225555 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5986],\n",
      "        [0.6803]])\n",
      "Iteration 19890 Training loss 0.11816731840372086 Validation loss 0.1155756413936615 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4558],\n",
      "        [0.4937]])\n",
      "Iteration 19900 Training loss 0.11701937764883041 Validation loss 0.1155896931886673 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.5028],\n",
      "        [0.5959]])\n",
      "Iteration 19910 Training loss 0.11742044240236282 Validation loss 0.1155659407377243 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.7215],\n",
      "        [0.4934]])\n",
      "Iteration 19920 Training loss 0.11668399721384048 Validation loss 0.11556823551654816 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.3844],\n",
      "        [0.5650]])\n",
      "Iteration 19930 Training loss 0.11725548654794693 Validation loss 0.11556447297334671 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4097],\n",
      "        [0.6315]])\n",
      "Iteration 19940 Training loss 0.11758146435022354 Validation loss 0.11556286364793777 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.4147],\n",
      "        [0.5446]])\n",
      "Iteration 19950 Training loss 0.11696077138185501 Validation loss 0.11560825258493423 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.3919],\n",
      "        [0.7796]])\n",
      "Iteration 19960 Training loss 0.11759116500616074 Validation loss 0.11555156111717224 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5756],\n",
      "        [0.5376]])\n",
      "Iteration 19970 Training loss 0.11753177642822266 Validation loss 0.11556249856948853 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5944],\n",
      "        [0.4637]])\n",
      "Iteration 19980 Training loss 0.11633403599262238 Validation loss 0.11560520529747009 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.6211],\n",
      "        [0.5074]])\n",
      "Iteration 19990 Training loss 0.11748109757900238 Validation loss 0.11563171446323395 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5136],\n",
      "        [0.5614]])\n",
      "Iteration 20000 Training loss 0.11750464141368866 Validation loss 0.11561080068349838 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5465],\n",
      "        [0.5898]])\n",
      "Iteration 20010 Training loss 0.11587650328874588 Validation loss 0.11558744311332703 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.5062],\n",
      "        [0.5695]])\n",
      "Iteration 20020 Training loss 0.11759194731712341 Validation loss 0.11558032035827637 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4812],\n",
      "        [0.6552]])\n",
      "Iteration 20030 Training loss 0.11717098206281662 Validation loss 0.1155753880739212 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.6240],\n",
      "        [0.4508]])\n",
      "Iteration 20040 Training loss 0.11855397373437881 Validation loss 0.1155942752957344 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.5276],\n",
      "        [0.4735]])\n",
      "Iteration 20050 Training loss 0.11803293973207474 Validation loss 0.11562091112136841 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.4689],\n",
      "        [0.6528]])\n",
      "Iteration 20060 Training loss 0.11767535656690598 Validation loss 0.11562414467334747 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.6970],\n",
      "        [0.5567]])\n",
      "Iteration 20070 Training loss 0.11730904877185822 Validation loss 0.11563707143068314 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.3893],\n",
      "        [0.6735]])\n",
      "Iteration 20080 Training loss 0.11809150874614716 Validation loss 0.11561383306980133 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.6444],\n",
      "        [0.2816]])\n",
      "Iteration 20090 Training loss 0.11763285100460052 Validation loss 0.11560334265232086 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4439],\n",
      "        [0.5353]])\n",
      "Iteration 20100 Training loss 0.11679909378290176 Validation loss 0.11561107635498047 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.2612],\n",
      "        [0.5039]])\n",
      "Iteration 20110 Training loss 0.11730266362428665 Validation loss 0.11563041061162949 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.3369],\n",
      "        [0.4497]])\n",
      "Iteration 20120 Training loss 0.11682238429784775 Validation loss 0.11560278385877609 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.4466],\n",
      "        [0.3102]])\n",
      "Iteration 20130 Training loss 0.11713142693042755 Validation loss 0.11562569439411163 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5137],\n",
      "        [0.4295]])\n",
      "Iteration 20140 Training loss 0.11606322973966599 Validation loss 0.11558019369840622 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.4685],\n",
      "        [0.6029]])\n",
      "Iteration 20150 Training loss 0.11754179745912552 Validation loss 0.11555110663175583 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5323],\n",
      "        [0.4283]])\n",
      "Iteration 20160 Training loss 0.11497282236814499 Validation loss 0.11556640267372131 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6957],\n",
      "        [0.7220]])\n",
      "Iteration 20170 Training loss 0.11877215653657913 Validation loss 0.11560460180044174 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.6015],\n",
      "        [0.5524]])\n",
      "Iteration 20180 Training loss 0.11642687767744064 Validation loss 0.11560825258493423 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.6270],\n",
      "        [0.5771]])\n",
      "Iteration 20190 Training loss 0.11634890735149384 Validation loss 0.11563991010189056 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.4361],\n",
      "        [0.4606]])\n",
      "Iteration 20200 Training loss 0.1173059418797493 Validation loss 0.11567826569080353 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.2142],\n",
      "        [0.4177]])\n",
      "Iteration 20210 Training loss 0.11754677444696426 Validation loss 0.11558067053556442 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.1703],\n",
      "        [0.3607]])\n",
      "Iteration 20220 Training loss 0.11665313690900803 Validation loss 0.11563338339328766 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5784],\n",
      "        [0.5357]])\n",
      "Iteration 20230 Training loss 0.11597548425197601 Validation loss 0.11556855589151382 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.4306],\n",
      "        [0.4867]])\n",
      "Iteration 20240 Training loss 0.11713745445013046 Validation loss 0.11554412543773651 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.4220],\n",
      "        [0.5663]])\n",
      "Iteration 20250 Training loss 0.11739856004714966 Validation loss 0.11565061658620834 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5503],\n",
      "        [0.4912]])\n",
      "Iteration 20260 Training loss 0.11837337166070938 Validation loss 0.1155613586306572 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4173],\n",
      "        [0.6372]])\n",
      "Iteration 20270 Training loss 0.11662296205759048 Validation loss 0.11558352410793304 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5384],\n",
      "        [0.4125]])\n",
      "Iteration 20280 Training loss 0.11745213717222214 Validation loss 0.11558797955513 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.5124],\n",
      "        [0.6491]])\n",
      "Iteration 20290 Training loss 0.11645621061325073 Validation loss 0.11557060480117798 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.5888],\n",
      "        [0.5246]])\n",
      "Iteration 20300 Training loss 0.11804166436195374 Validation loss 0.11557228863239288 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.3500],\n",
      "        [0.6759]])\n",
      "Iteration 20310 Training loss 0.11693477630615234 Validation loss 0.11565537750720978 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.4073],\n",
      "        [0.3572]])\n",
      "Iteration 20320 Training loss 0.11767534166574478 Validation loss 0.11563000828027725 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5497],\n",
      "        [0.4655]])\n",
      "Iteration 20330 Training loss 0.11772170662879944 Validation loss 0.11564001441001892 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4474],\n",
      "        [0.6459]])\n",
      "Iteration 20340 Training loss 0.1165500357747078 Validation loss 0.1156398132443428 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.6091],\n",
      "        [0.3160]])\n",
      "Iteration 20350 Training loss 0.11649531871080399 Validation loss 0.11563611775636673 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.1872],\n",
      "        [0.4235]])\n",
      "Iteration 20360 Training loss 0.11606862396001816 Validation loss 0.1155620738863945 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4060],\n",
      "        [0.2243]])\n",
      "Iteration 20370 Training loss 0.1167951226234436 Validation loss 0.1155790165066719 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.4045],\n",
      "        [0.6194]])\n",
      "Iteration 20380 Training loss 0.11867193877696991 Validation loss 0.11560198664665222 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.3900],\n",
      "        [0.5028]])\n",
      "Iteration 20390 Training loss 0.11732619255781174 Validation loss 0.11557000875473022 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.5782],\n",
      "        [0.5402]])\n",
      "Iteration 20400 Training loss 0.11575208604335785 Validation loss 0.11558324843645096 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.3374],\n",
      "        [0.5640]])\n",
      "Iteration 20410 Training loss 0.11748338490724564 Validation loss 0.11560617387294769 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.2807],\n",
      "        [0.6047]])\n",
      "Iteration 20420 Training loss 0.11687058210372925 Validation loss 0.11558204889297485 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4683],\n",
      "        [0.5857]])\n",
      "Iteration 20430 Training loss 0.11833953112363815 Validation loss 0.11559329926967621 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5720],\n",
      "        [0.4026]])\n",
      "Iteration 20440 Training loss 0.1178925558924675 Validation loss 0.11560551822185516 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5572],\n",
      "        [0.5114]])\n",
      "Iteration 20450 Training loss 0.11697296053171158 Validation loss 0.1156453862786293 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4235],\n",
      "        [0.2939]])\n",
      "Iteration 20460 Training loss 0.11734574288129807 Validation loss 0.11562345176935196 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4562],\n",
      "        [0.5149]])\n",
      "Iteration 20470 Training loss 0.11806727200746536 Validation loss 0.1156296655535698 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.7066],\n",
      "        [0.7344]])\n",
      "Iteration 20480 Training loss 0.11667327582836151 Validation loss 0.11557263135910034 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.5835],\n",
      "        [0.5709]])\n",
      "Iteration 20490 Training loss 0.11716147512197495 Validation loss 0.11556066572666168 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.7283],\n",
      "        [0.4785]])\n",
      "Iteration 20500 Training loss 0.11727318167686462 Validation loss 0.11558687686920166 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.3495],\n",
      "        [0.5171]])\n",
      "Iteration 20510 Training loss 0.11708316206932068 Validation loss 0.11556432396173477 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5548],\n",
      "        [0.1412]])\n",
      "Iteration 20520 Training loss 0.11663030833005905 Validation loss 0.11556670069694519 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4393],\n",
      "        [0.4685]])\n",
      "Iteration 20530 Training loss 0.11708531528711319 Validation loss 0.11559328436851501 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.5112],\n",
      "        [0.4549]])\n",
      "Iteration 20540 Training loss 0.11747727543115616 Validation loss 0.11554862558841705 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.5000],\n",
      "        [0.6278]])\n",
      "Iteration 20550 Training loss 0.11757556349039078 Validation loss 0.11561533808708191 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.4819],\n",
      "        [0.6128]])\n",
      "Iteration 20560 Training loss 0.11917241662740707 Validation loss 0.11552975326776505 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.6447],\n",
      "        [0.7252]])\n",
      "Iteration 20570 Training loss 0.1165161207318306 Validation loss 0.11558245867490768 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.2253],\n",
      "        [0.4588]])\n",
      "Iteration 20580 Training loss 0.11742780357599258 Validation loss 0.11561885476112366 Accuracy 0.6134999990463257\n",
      "Output tensor([[0.5334],\n",
      "        [0.5600]])\n",
      "Iteration 20590 Training loss 0.11692879348993301 Validation loss 0.11556167900562286 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.5235],\n",
      "        [0.5164]])\n",
      "Iteration 20600 Training loss 0.11549440771341324 Validation loss 0.11553170531988144 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.6246],\n",
      "        [0.7677]])\n",
      "Iteration 20610 Training loss 0.11666359752416611 Validation loss 0.11553484946489334 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.4604],\n",
      "        [0.5508]])\n",
      "Iteration 20620 Training loss 0.11725189536809921 Validation loss 0.11553706973791122 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.4533],\n",
      "        [0.3974]])\n",
      "Iteration 20630 Training loss 0.1185649037361145 Validation loss 0.11552117764949799 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5599],\n",
      "        [0.3834]])\n",
      "Iteration 20640 Training loss 0.117726631462574 Validation loss 0.11551572382450104 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.3899],\n",
      "        [0.5007]])\n",
      "Iteration 20650 Training loss 0.11668914556503296 Validation loss 0.1157272607088089 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.4415],\n",
      "        [0.4800]])\n",
      "Iteration 20660 Training loss 0.11710156500339508 Validation loss 0.11554078012704849 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.4275],\n",
      "        [0.4197]])\n",
      "Iteration 20670 Training loss 0.11779345571994781 Validation loss 0.11553119122982025 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.5806],\n",
      "        [0.4638]])\n",
      "Iteration 20680 Training loss 0.11841373890638351 Validation loss 0.11550983786582947 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6568],\n",
      "        [0.3972]])\n",
      "Iteration 20690 Training loss 0.1171523928642273 Validation loss 0.11563283205032349 Accuracy 0.612666666507721\n",
      "Output tensor([[0.5705],\n",
      "        [0.5730]])\n",
      "Iteration 20700 Training loss 0.11616838723421097 Validation loss 0.11550203710794449 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.3376],\n",
      "        [0.4443]])\n",
      "Iteration 20710 Training loss 0.11779540777206421 Validation loss 0.11551719903945923 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.3246],\n",
      "        [0.3626]])\n",
      "Iteration 20720 Training loss 0.1163744330406189 Validation loss 0.11552876979112625 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.5592],\n",
      "        [0.3726]])\n",
      "Iteration 20730 Training loss 0.11826204508543015 Validation loss 0.11560134589672089 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5996],\n",
      "        [0.6047]])\n",
      "Iteration 20740 Training loss 0.11741460114717484 Validation loss 0.115532286465168 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.3756],\n",
      "        [0.4897]])\n",
      "Iteration 20750 Training loss 0.1180502325296402 Validation loss 0.11553135514259338 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4483],\n",
      "        [0.5278]])\n",
      "Iteration 20760 Training loss 0.11765071749687195 Validation loss 0.11557421833276749 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5413],\n",
      "        [0.6411]])\n",
      "Iteration 20770 Training loss 0.11760708689689636 Validation loss 0.11552905291318893 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.2599],\n",
      "        [0.4371]])\n",
      "Iteration 20780 Training loss 0.11650403589010239 Validation loss 0.11554061621427536 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.2673],\n",
      "        [0.4914]])\n",
      "Iteration 20790 Training loss 0.1183798611164093 Validation loss 0.11552954465150833 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5135],\n",
      "        [0.4206]])\n",
      "Iteration 20800 Training loss 0.11679889261722565 Validation loss 0.11551734060049057 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.5705],\n",
      "        [0.5364]])\n",
      "Iteration 20810 Training loss 0.11749251186847687 Validation loss 0.11554577946662903 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.8331],\n",
      "        [0.4297]])\n",
      "Iteration 20820 Training loss 0.11791649460792542 Validation loss 0.11556324362754822 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.2705],\n",
      "        [0.4441]])\n",
      "Iteration 20830 Training loss 0.11802694946527481 Validation loss 0.11561983078718185 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.4465],\n",
      "        [0.4644]])\n",
      "Iteration 20840 Training loss 0.11760172247886658 Validation loss 0.11551384627819061 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.4511],\n",
      "        [0.3943]])\n",
      "Iteration 20850 Training loss 0.11718173325061798 Validation loss 0.11549215018749237 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.4269],\n",
      "        [0.2519]])\n",
      "Iteration 20860 Training loss 0.1170322373509407 Validation loss 0.11551638692617416 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.4382],\n",
      "        [0.4622]])\n",
      "Iteration 20870 Training loss 0.11684203892946243 Validation loss 0.11549435555934906 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.6123],\n",
      "        [0.5478]])\n",
      "Iteration 20880 Training loss 0.11704551428556442 Validation loss 0.1155262440443039 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.6054],\n",
      "        [0.6348]])\n",
      "Iteration 20890 Training loss 0.11740236729383469 Validation loss 0.11550378054380417 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5435],\n",
      "        [0.5645]])\n",
      "Iteration 20900 Training loss 0.11815805733203888 Validation loss 0.11550476402044296 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.3021],\n",
      "        [0.5203]])\n",
      "Iteration 20910 Training loss 0.11597589403390884 Validation loss 0.11547541618347168 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.6891],\n",
      "        [0.5273]])\n",
      "Iteration 20920 Training loss 0.11658044159412384 Validation loss 0.11550234258174896 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.3865],\n",
      "        [0.6017]])\n",
      "Iteration 20930 Training loss 0.11740157008171082 Validation loss 0.11554926633834839 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.5477],\n",
      "        [0.2844]])\n",
      "Iteration 20940 Training loss 0.11662913113832474 Validation loss 0.11547352373600006 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5162],\n",
      "        [0.5004]])\n",
      "Iteration 20950 Training loss 0.11787040531635284 Validation loss 0.11546820402145386 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.5083],\n",
      "        [0.4316]])\n",
      "Iteration 20960 Training loss 0.11669354140758514 Validation loss 0.11547937989234924 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.6777],\n",
      "        [0.4811]])\n",
      "Iteration 20970 Training loss 0.11680567264556885 Validation loss 0.1154908761382103 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.3951],\n",
      "        [0.6045]])\n",
      "Iteration 20980 Training loss 0.11540602892637253 Validation loss 0.11547007411718369 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4734],\n",
      "        [0.5240]])\n",
      "Iteration 20990 Training loss 0.11848847568035126 Validation loss 0.11549385637044907 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4033],\n",
      "        [0.4090]])\n",
      "Iteration 21000 Training loss 0.11615584045648575 Validation loss 0.11554954200983047 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.3968],\n",
      "        [0.6109]])\n",
      "Iteration 21010 Training loss 0.11642644554376602 Validation loss 0.11550472676753998 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.4551],\n",
      "        [0.4577]])\n",
      "Iteration 21020 Training loss 0.11644019931554794 Validation loss 0.11552628874778748 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.3777],\n",
      "        [0.7469]])\n",
      "Iteration 21030 Training loss 0.11854148656129837 Validation loss 0.11548735201358795 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.6218],\n",
      "        [0.4492]])\n",
      "Iteration 21040 Training loss 0.11660237610340118 Validation loss 0.11548293381929398 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.3784],\n",
      "        [0.4591]])\n",
      "Iteration 21050 Training loss 0.11858941614627838 Validation loss 0.11559648811817169 Accuracy 0.6136666536331177\n",
      "Output tensor([[0.4120],\n",
      "        [0.5326]])\n",
      "Iteration 21060 Training loss 0.11781233549118042 Validation loss 0.11548305302858353 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.4777],\n",
      "        [0.4053]])\n",
      "Iteration 21070 Training loss 0.11707361042499542 Validation loss 0.11549464613199234 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.4871],\n",
      "        [0.2810]])\n",
      "Iteration 21080 Training loss 0.11731123924255371 Validation loss 0.11549758911132812 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.4590],\n",
      "        [0.5030]])\n",
      "Iteration 21090 Training loss 0.1174958273768425 Validation loss 0.11548681557178497 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.3622],\n",
      "        [0.5627]])\n",
      "Iteration 21100 Training loss 0.11724218726158142 Validation loss 0.1154891774058342 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.7490],\n",
      "        [0.6329]])\n",
      "Iteration 21110 Training loss 0.11834697425365448 Validation loss 0.11550742387771606 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.6784],\n",
      "        [0.4204]])\n",
      "Iteration 21120 Training loss 0.1170089989900589 Validation loss 0.11547625809907913 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.7024],\n",
      "        [0.4888]])\n",
      "Iteration 21130 Training loss 0.11664500087499619 Validation loss 0.11549719423055649 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.3285],\n",
      "        [0.5522]])\n",
      "Iteration 21140 Training loss 0.11669840663671494 Validation loss 0.11547806859016418 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.5152],\n",
      "        [0.5587]])\n",
      "Iteration 21150 Training loss 0.11740970611572266 Validation loss 0.11546347290277481 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.6757],\n",
      "        [0.6241]])\n",
      "Iteration 21160 Training loss 0.11722218990325928 Validation loss 0.11554910242557526 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.6483],\n",
      "        [0.2497]])\n",
      "Iteration 21170 Training loss 0.11759008467197418 Validation loss 0.11547961086034775 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.3565],\n",
      "        [0.4495]])\n",
      "Iteration 21180 Training loss 0.11630071699619293 Validation loss 0.11572623997926712 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.5938],\n",
      "        [0.5129]])\n",
      "Iteration 21190 Training loss 0.11748922616243362 Validation loss 0.11554598063230515 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5942],\n",
      "        [0.5564]])\n",
      "Iteration 21200 Training loss 0.11612200736999512 Validation loss 0.11550098657608032 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.4068],\n",
      "        [0.5035]])\n",
      "Iteration 21210 Training loss 0.1176607683300972 Validation loss 0.11565878242254257 Accuracy 0.6133333444595337\n",
      "Output tensor([[0.5612],\n",
      "        [0.5319]])\n",
      "Iteration 21220 Training loss 0.1173061728477478 Validation loss 0.11554817855358124 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.4890],\n",
      "        [0.6578]])\n",
      "Iteration 21230 Training loss 0.11784688383340836 Validation loss 0.1155577152967453 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.7152],\n",
      "        [0.5298]])\n",
      "Iteration 21240 Training loss 0.11686302721500397 Validation loss 0.11550072580575943 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.5210],\n",
      "        [0.3410]])\n",
      "Iteration 21250 Training loss 0.11699914187192917 Validation loss 0.11558076739311218 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.6566],\n",
      "        [0.3321]])\n",
      "Iteration 21260 Training loss 0.11742547899484634 Validation loss 0.11548908799886703 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.5876],\n",
      "        [0.6366]])\n",
      "Iteration 21270 Training loss 0.1170615628361702 Validation loss 0.11551356315612793 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.5287],\n",
      "        [0.4370]])\n",
      "Iteration 21280 Training loss 0.11699094623327255 Validation loss 0.1155397966504097 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5699],\n",
      "        [0.5872]])\n",
      "Iteration 21290 Training loss 0.11727212369441986 Validation loss 0.11550234258174896 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.4081],\n",
      "        [0.5664]])\n",
      "Iteration 21300 Training loss 0.11703743785619736 Validation loss 0.11549142748117447 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.3557],\n",
      "        [0.5550]])\n",
      "Iteration 21310 Training loss 0.11727874726057053 Validation loss 0.11547435820102692 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.5307],\n",
      "        [0.6260]])\n",
      "Iteration 21320 Training loss 0.11659816652536392 Validation loss 0.11548687517642975 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.6514],\n",
      "        [0.5039]])\n",
      "Iteration 21330 Training loss 0.11862831562757492 Validation loss 0.11570192128419876 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.4969],\n",
      "        [0.4903]])\n",
      "Iteration 21340 Training loss 0.11725319176912308 Validation loss 0.11549299210309982 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.5694],\n",
      "        [0.3928]])\n",
      "Iteration 21350 Training loss 0.11700411885976791 Validation loss 0.11548849195241928 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4813],\n",
      "        [0.5667]])\n",
      "Iteration 21360 Training loss 0.11707116663455963 Validation loss 0.11551415920257568 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5955],\n",
      "        [0.2958]])\n",
      "Iteration 21370 Training loss 0.11727763712406158 Validation loss 0.11546272784471512 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.5945],\n",
      "        [0.3454]])\n",
      "Iteration 21380 Training loss 0.11784804612398148 Validation loss 0.11557160317897797 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.5297],\n",
      "        [0.7128]])\n",
      "Iteration 21390 Training loss 0.11651770770549774 Validation loss 0.1154799535870552 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.4867],\n",
      "        [0.5155]])\n",
      "Iteration 21400 Training loss 0.11639811843633652 Validation loss 0.11551326513290405 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.4452],\n",
      "        [0.7015]])\n",
      "Iteration 21410 Training loss 0.1170603334903717 Validation loss 0.11551657319068909 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5943],\n",
      "        [0.5524]])\n",
      "Iteration 21420 Training loss 0.11661987006664276 Validation loss 0.11557736992835999 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.5926],\n",
      "        [0.6808]])\n",
      "Iteration 21430 Training loss 0.11726538836956024 Validation loss 0.1155143603682518 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.3080],\n",
      "        [0.4305]])\n",
      "Iteration 21440 Training loss 0.11615391820669174 Validation loss 0.11547068506479263 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5635],\n",
      "        [0.4305]])\n",
      "Iteration 21450 Training loss 0.11682385206222534 Validation loss 0.11550990492105484 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.8219],\n",
      "        [0.4161]])\n",
      "Iteration 21460 Training loss 0.11592083424329758 Validation loss 0.11548728495836258 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5593],\n",
      "        [0.5872]])\n",
      "Iteration 21470 Training loss 0.11723973602056503 Validation loss 0.11547601968050003 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.4218],\n",
      "        [0.5940]])\n",
      "Iteration 21480 Training loss 0.11682771891355515 Validation loss 0.11548730731010437 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4725],\n",
      "        [0.6136]])\n",
      "Iteration 21490 Training loss 0.11873866617679596 Validation loss 0.11550497263669968 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4398],\n",
      "        [0.4952]])\n",
      "Iteration 21500 Training loss 0.11716890335083008 Validation loss 0.11549302935600281 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.6082],\n",
      "        [0.6155]])\n",
      "Iteration 21510 Training loss 0.11685959994792938 Validation loss 0.11546710133552551 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.5593],\n",
      "        [0.6083]])\n",
      "Iteration 21520 Training loss 0.11738037317991257 Validation loss 0.11546818166971207 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4484],\n",
      "        [0.2536]])\n",
      "Iteration 21530 Training loss 0.11747284233570099 Validation loss 0.11547469347715378 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.4487],\n",
      "        [0.5414]])\n",
      "Iteration 21540 Training loss 0.11747226864099503 Validation loss 0.11546586453914642 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.4986],\n",
      "        [0.4988]])\n",
      "Iteration 21550 Training loss 0.11778144538402557 Validation loss 0.11545763164758682 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.6094],\n",
      "        [0.6404]])\n",
      "Iteration 21560 Training loss 0.1179039478302002 Validation loss 0.11547961086034775 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4563],\n",
      "        [0.8423]])\n",
      "Iteration 21570 Training loss 0.11777032166719437 Validation loss 0.11550349742174149 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.6605],\n",
      "        [0.3867]])\n",
      "Iteration 21580 Training loss 0.11762705445289612 Validation loss 0.11548672616481781 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5541],\n",
      "        [0.4247]])\n",
      "Iteration 21590 Training loss 0.11606143414974213 Validation loss 0.11549043655395508 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.6184],\n",
      "        [0.5618]])\n",
      "Iteration 21600 Training loss 0.11562597751617432 Validation loss 0.11546926200389862 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.4340],\n",
      "        [0.7064]])\n",
      "Iteration 21610 Training loss 0.11855360865592957 Validation loss 0.11553460359573364 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5285],\n",
      "        [0.5395]])\n",
      "Iteration 21620 Training loss 0.11735349893569946 Validation loss 0.11549369245767593 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.3198],\n",
      "        [0.3173]])\n",
      "Iteration 21630 Training loss 0.11633660644292831 Validation loss 0.1155623346567154 Accuracy 0.6138333082199097\n",
      "Output tensor([[0.5606],\n",
      "        [0.6332]])\n",
      "Iteration 21640 Training loss 0.1155824363231659 Validation loss 0.1154637485742569 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.2801],\n",
      "        [0.5835]])\n",
      "Iteration 21650 Training loss 0.11732576787471771 Validation loss 0.11546935141086578 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.8452],\n",
      "        [0.4504]])\n",
      "Iteration 21660 Training loss 0.11665748804807663 Validation loss 0.11546249687671661 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.6043],\n",
      "        [0.5482]])\n",
      "Iteration 21670 Training loss 0.11658617854118347 Validation loss 0.11549133062362671 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.5529],\n",
      "        [0.4527]])\n",
      "Iteration 21680 Training loss 0.11582905799150467 Validation loss 0.1154455691576004 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5111],\n",
      "        [0.3398]])\n",
      "Iteration 21690 Training loss 0.11750893294811249 Validation loss 0.11544018238782883 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.5956],\n",
      "        [0.4514]])\n",
      "Iteration 21700 Training loss 0.1155870333313942 Validation loss 0.11544889211654663 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.4934],\n",
      "        [0.4492]])\n",
      "Iteration 21710 Training loss 0.11762526631355286 Validation loss 0.11542584002017975 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.3632],\n",
      "        [0.5705]])\n",
      "Iteration 21720 Training loss 0.11725181341171265 Validation loss 0.11555026471614838 Accuracy 0.6129999756813049\n",
      "Output tensor([[0.3801],\n",
      "        [0.3906]])\n",
      "Iteration 21730 Training loss 0.11680889129638672 Validation loss 0.11539971828460693 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4351],\n",
      "        [0.8240]])\n",
      "Iteration 21740 Training loss 0.11723437160253525 Validation loss 0.11541564762592316 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.3705],\n",
      "        [0.7196]])\n",
      "Iteration 21750 Training loss 0.11644012480974197 Validation loss 0.11539514362812042 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.4062],\n",
      "        [0.4026]])\n",
      "Iteration 21760 Training loss 0.11630025506019592 Validation loss 0.11540781706571579 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.7371],\n",
      "        [0.5613]])\n",
      "Iteration 21770 Training loss 0.11697062104940414 Validation loss 0.11540498584508896 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4062],\n",
      "        [0.3462]])\n",
      "Iteration 21780 Training loss 0.11653454601764679 Validation loss 0.11539459228515625 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.5094],\n",
      "        [0.5598]])\n",
      "Iteration 21790 Training loss 0.11750300973653793 Validation loss 0.11551910638809204 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.5105],\n",
      "        [0.4684]])\n",
      "Iteration 21800 Training loss 0.11701597273349762 Validation loss 0.11542006582021713 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.3286],\n",
      "        [0.4858]])\n",
      "Iteration 21810 Training loss 0.1168559342622757 Validation loss 0.11540988832712173 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.3436],\n",
      "        [0.4972]])\n",
      "Iteration 21820 Training loss 0.11745046824216843 Validation loss 0.11540671437978745 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.4999],\n",
      "        [0.5650]])\n",
      "Iteration 21830 Training loss 0.11679268628358841 Validation loss 0.1154114156961441 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5872],\n",
      "        [0.4861]])\n",
      "Iteration 21840 Training loss 0.11656250804662704 Validation loss 0.11540154367685318 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.4768],\n",
      "        [0.6532]])\n",
      "Iteration 21850 Training loss 0.11664525419473648 Validation loss 0.11542224138975143 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.6623],\n",
      "        [0.4957]])\n",
      "Iteration 21860 Training loss 0.11648063361644745 Validation loss 0.11541078984737396 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.5631],\n",
      "        [0.4891]])\n",
      "Iteration 21870 Training loss 0.1173618882894516 Validation loss 0.11540462076663971 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.4259],\n",
      "        [0.3940]])\n",
      "Iteration 21880 Training loss 0.11706096678972244 Validation loss 0.11546946316957474 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.6340],\n",
      "        [0.4166]])\n",
      "Iteration 21890 Training loss 0.1173277348279953 Validation loss 0.11538737267255783 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5219],\n",
      "        [0.4848]])\n",
      "Iteration 21900 Training loss 0.11694666743278503 Validation loss 0.11541154235601425 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.2979],\n",
      "        [0.5275]])\n",
      "Iteration 21910 Training loss 0.11630919575691223 Validation loss 0.11537767201662064 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.4055],\n",
      "        [0.4965]])\n",
      "Iteration 21920 Training loss 0.11777498573064804 Validation loss 0.11540742218494415 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.4756],\n",
      "        [0.6409]])\n",
      "Iteration 21930 Training loss 0.11742770671844482 Validation loss 0.11540547758340836 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4515],\n",
      "        [0.5123]])\n",
      "Iteration 21940 Training loss 0.11688753217458725 Validation loss 0.11543518304824829 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.6479],\n",
      "        [0.3340]])\n",
      "Iteration 21950 Training loss 0.11693426221609116 Validation loss 0.11537351459264755 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.2551],\n",
      "        [0.2567]])\n",
      "Iteration 21960 Training loss 0.11737475544214249 Validation loss 0.1155262216925621 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.6154],\n",
      "        [0.5208]])\n",
      "Iteration 21970 Training loss 0.11759037524461746 Validation loss 0.11542464047670364 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4529],\n",
      "        [0.5271]])\n",
      "Iteration 21980 Training loss 0.11598140746355057 Validation loss 0.11541300266981125 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4636],\n",
      "        [0.5479]])\n",
      "Iteration 21990 Training loss 0.11703623086214066 Validation loss 0.11549220979213715 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.2507],\n",
      "        [0.4283]])\n",
      "Iteration 22000 Training loss 0.11815651506185532 Validation loss 0.11543396860361099 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.5456],\n",
      "        [0.7315]])\n",
      "Iteration 22010 Training loss 0.11507222801446915 Validation loss 0.11542589217424393 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.2751],\n",
      "        [0.4781]])\n",
      "Iteration 22020 Training loss 0.11596603691577911 Validation loss 0.11542510986328125 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.3838],\n",
      "        [0.6195]])\n",
      "Iteration 22030 Training loss 0.11810556054115295 Validation loss 0.11540093272924423 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.5878],\n",
      "        [0.4663]])\n",
      "Iteration 22040 Training loss 0.11703391373157501 Validation loss 0.11545200645923615 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5733],\n",
      "        [0.5560]])\n",
      "Iteration 22050 Training loss 0.11762025207281113 Validation loss 0.11543218791484833 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.4868],\n",
      "        [0.3799]])\n",
      "Iteration 22060 Training loss 0.11880689859390259 Validation loss 0.11543431878089905 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4347],\n",
      "        [0.4295]])\n",
      "Iteration 22070 Training loss 0.11782436817884445 Validation loss 0.11544084548950195 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.6099],\n",
      "        [0.6243]])\n",
      "Iteration 22080 Training loss 0.1166820079088211 Validation loss 0.11546090990304947 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.3731],\n",
      "        [0.4196]])\n",
      "Iteration 22090 Training loss 0.11694254726171494 Validation loss 0.11547422409057617 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.4990],\n",
      "        [0.3710]])\n",
      "Iteration 22100 Training loss 0.11617466062307358 Validation loss 0.11551908403635025 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.3947],\n",
      "        [0.4616]])\n",
      "Iteration 22110 Training loss 0.11596713960170746 Validation loss 0.11544771492481232 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5912],\n",
      "        [0.3409]])\n",
      "Iteration 22120 Training loss 0.11606526374816895 Validation loss 0.11548037827014923 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.4909],\n",
      "        [0.3608]])\n",
      "Iteration 22130 Training loss 0.11742208153009415 Validation loss 0.11544089019298553 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.2701],\n",
      "        [0.5702]])\n",
      "Iteration 22140 Training loss 0.1174408346414566 Validation loss 0.11546006053686142 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.3631],\n",
      "        [0.2998]])\n",
      "Iteration 22150 Training loss 0.11668551713228226 Validation loss 0.11542309820652008 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.3521],\n",
      "        [0.4375]])\n",
      "Iteration 22160 Training loss 0.11701402068138123 Validation loss 0.11543887108564377 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5040],\n",
      "        [0.4821]])\n",
      "Iteration 22170 Training loss 0.11699755489826202 Validation loss 0.11543004214763641 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.5988],\n",
      "        [0.3357]])\n",
      "Iteration 22180 Training loss 0.11725820600986481 Validation loss 0.11548233777284622 Accuracy 0.6144999861717224\n",
      "Output tensor([[0.6942],\n",
      "        [0.3360]])\n",
      "Iteration 22190 Training loss 0.11656691133975983 Validation loss 0.11538424342870712 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.7342],\n",
      "        [0.5535]])\n",
      "Iteration 22200 Training loss 0.11698300391435623 Validation loss 0.11568893492221832 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.3658],\n",
      "        [0.5534]])\n",
      "Iteration 22210 Training loss 0.11625378578901291 Validation loss 0.11540309339761734 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.3917],\n",
      "        [0.5765]])\n",
      "Iteration 22220 Training loss 0.11740661412477493 Validation loss 0.11543822288513184 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.1485],\n",
      "        [0.2448]])\n",
      "Iteration 22230 Training loss 0.11648061126470566 Validation loss 0.11540704220533371 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.4831],\n",
      "        [0.3522]])\n",
      "Iteration 22240 Training loss 0.11750136315822601 Validation loss 0.11542753130197525 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5055],\n",
      "        [0.6268]])\n",
      "Iteration 22250 Training loss 0.11736751347780228 Validation loss 0.11541339010000229 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.6135],\n",
      "        [0.6557]])\n",
      "Iteration 22260 Training loss 0.11723770946264267 Validation loss 0.11544008553028107 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.5882],\n",
      "        [0.4966]])\n",
      "Iteration 22270 Training loss 0.11693734675645828 Validation loss 0.11542154848575592 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.4997],\n",
      "        [0.4663]])\n",
      "Iteration 22280 Training loss 0.11632397770881653 Validation loss 0.11544632911682129 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.6021],\n",
      "        [0.4981]])\n",
      "Iteration 22290 Training loss 0.11605250835418701 Validation loss 0.11557574570178986 Accuracy 0.6143333315849304\n",
      "Output tensor([[0.4135],\n",
      "        [0.5324]])\n",
      "Iteration 22300 Training loss 0.11694876849651337 Validation loss 0.11543632298707962 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.6856],\n",
      "        [0.5163]])\n",
      "Iteration 22310 Training loss 0.11764932423830032 Validation loss 0.11547143757343292 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5528],\n",
      "        [0.6551]])\n",
      "Iteration 22320 Training loss 0.11674618721008301 Validation loss 0.11538596451282501 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.4285],\n",
      "        [0.2526]])\n",
      "Iteration 22330 Training loss 0.11685749143362045 Validation loss 0.11539386212825775 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.4449],\n",
      "        [0.3859]])\n",
      "Iteration 22340 Training loss 0.11756983399391174 Validation loss 0.11544020473957062 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.2685],\n",
      "        [0.5957]])\n",
      "Iteration 22350 Training loss 0.11688663065433502 Validation loss 0.11544370651245117 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.0977],\n",
      "        [0.3440]])\n",
      "Iteration 22360 Training loss 0.11700061708688736 Validation loss 0.1154211014509201 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4707],\n",
      "        [0.5266]])\n",
      "Iteration 22370 Training loss 0.11630602180957794 Validation loss 0.11537688970565796 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.4982],\n",
      "        [0.5129]])\n",
      "Iteration 22380 Training loss 0.11761711537837982 Validation loss 0.11539392173290253 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.4908],\n",
      "        [0.5861]])\n",
      "Iteration 22390 Training loss 0.11784730851650238 Validation loss 0.11538375169038773 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.3013],\n",
      "        [0.6431]])\n",
      "Iteration 22400 Training loss 0.1165093183517456 Validation loss 0.11537164449691772 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.7273],\n",
      "        [0.4089]])\n",
      "Iteration 22410 Training loss 0.11703107506036758 Validation loss 0.115387462079525 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.4808],\n",
      "        [0.1873]])\n",
      "Iteration 22420 Training loss 0.11740005761384964 Validation loss 0.11540380120277405 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.5653],\n",
      "        [0.5675]])\n",
      "Iteration 22430 Training loss 0.11684665828943253 Validation loss 0.11539102345705032 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.6364],\n",
      "        [0.1504]])\n",
      "Iteration 22440 Training loss 0.11621427536010742 Validation loss 0.11539394408464432 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.6773],\n",
      "        [0.3794]])\n",
      "Iteration 22450 Training loss 0.1167045459151268 Validation loss 0.11537417769432068 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.3927],\n",
      "        [0.6151]])\n",
      "Iteration 22460 Training loss 0.11624975502490997 Validation loss 0.11545833200216293 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5156],\n",
      "        [0.6551]])\n",
      "Iteration 22470 Training loss 0.11729671061038971 Validation loss 0.11547403782606125 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.5858],\n",
      "        [0.4452]])\n",
      "Iteration 22480 Training loss 0.11632438004016876 Validation loss 0.11539556831121445 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.7130],\n",
      "        [0.4015]])\n",
      "Iteration 22490 Training loss 0.11831022053956985 Validation loss 0.1154036596417427 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.3009],\n",
      "        [0.5936]])\n",
      "Iteration 22500 Training loss 0.11658382415771484 Validation loss 0.11532824486494064 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.3437],\n",
      "        [0.6175]])\n",
      "Iteration 22510 Training loss 0.11649008095264435 Validation loss 0.11532331258058548 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.5469],\n",
      "        [0.5700]])\n",
      "Iteration 22520 Training loss 0.11724360287189484 Validation loss 0.11535167694091797 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.4109],\n",
      "        [0.5990]])\n",
      "Iteration 22530 Training loss 0.1178349182009697 Validation loss 0.11536092311143875 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.3012],\n",
      "        [0.7128]])\n",
      "Iteration 22540 Training loss 0.11755677312612534 Validation loss 0.11533358693122864 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.4910],\n",
      "        [0.4083]])\n",
      "Iteration 22550 Training loss 0.11816604435443878 Validation loss 0.11536240577697754 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.5147],\n",
      "        [0.5708]])\n",
      "Iteration 22560 Training loss 0.11710310727357864 Validation loss 0.11536730825901031 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.3892],\n",
      "        [0.5809]])\n",
      "Iteration 22570 Training loss 0.11781016737222672 Validation loss 0.11541721969842911 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4909],\n",
      "        [0.4404]])\n",
      "Iteration 22580 Training loss 0.1167481392621994 Validation loss 0.11536828428506851 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.3813],\n",
      "        [0.7043]])\n",
      "Iteration 22590 Training loss 0.1165609359741211 Validation loss 0.11534278094768524 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.5360],\n",
      "        [0.3680]])\n",
      "Iteration 22600 Training loss 0.1174343079328537 Validation loss 0.11534877866506577 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.5807],\n",
      "        [0.6327]])\n",
      "Iteration 22610 Training loss 0.11577920615673065 Validation loss 0.11537893861532211 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.3804],\n",
      "        [0.5295]])\n",
      "Iteration 22620 Training loss 0.1165565699338913 Validation loss 0.11536309123039246 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5181],\n",
      "        [0.6329]])\n",
      "Iteration 22630 Training loss 0.11722825467586517 Validation loss 0.11538267880678177 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.3877],\n",
      "        [0.5677]])\n",
      "Iteration 22640 Training loss 0.11763491481542587 Validation loss 0.11538254469633102 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.2346],\n",
      "        [0.4949]])\n",
      "Iteration 22650 Training loss 0.11649678647518158 Validation loss 0.11535241454839706 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.6292],\n",
      "        [0.2751]])\n",
      "Iteration 22660 Training loss 0.11786390095949173 Validation loss 0.11540679633617401 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.4700],\n",
      "        [0.2704]])\n",
      "Iteration 22670 Training loss 0.11635168641805649 Validation loss 0.11546938866376877 Accuracy 0.6146666407585144\n",
      "Output tensor([[0.7525],\n",
      "        [0.4754]])\n",
      "Iteration 22680 Training loss 0.11625156551599503 Validation loss 0.115367591381073 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5428],\n",
      "        [0.5315]])\n",
      "Iteration 22690 Training loss 0.11671294271945953 Validation loss 0.11544587463140488 Accuracy 0.6151666641235352\n",
      "Output tensor([[0.5699],\n",
      "        [0.6750]])\n",
      "Iteration 22700 Training loss 0.11673516035079956 Validation loss 0.11538049578666687 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.4175],\n",
      "        [0.6899]])\n",
      "Iteration 22710 Training loss 0.11766554415225983 Validation loss 0.11535777896642685 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4578],\n",
      "        [0.4828]])\n",
      "Iteration 22720 Training loss 0.11838565766811371 Validation loss 0.11535121500492096 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.5567],\n",
      "        [0.5290]])\n",
      "Iteration 22730 Training loss 0.1161847710609436 Validation loss 0.11534979194402695 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.4550],\n",
      "        [0.5546]])\n",
      "Iteration 22740 Training loss 0.11657393723726273 Validation loss 0.1154068261384964 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5476],\n",
      "        [0.4682]])\n",
      "Iteration 22750 Training loss 0.11584842205047607 Validation loss 0.11539779603481293 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.2421],\n",
      "        [0.5744]])\n",
      "Iteration 22760 Training loss 0.11719538271427155 Validation loss 0.11545097082853317 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.5868],\n",
      "        [0.4919]])\n",
      "Iteration 22770 Training loss 0.11770204454660416 Validation loss 0.11534188687801361 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.6223],\n",
      "        [0.4894]])\n",
      "Iteration 22780 Training loss 0.11787678301334381 Validation loss 0.1154119074344635 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.5358],\n",
      "        [0.3824]])\n",
      "Iteration 22790 Training loss 0.11718861758708954 Validation loss 0.11531582474708557 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.3859],\n",
      "        [0.4277]])\n",
      "Iteration 22800 Training loss 0.11697465926408768 Validation loss 0.11541131883859634 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.4489],\n",
      "        [0.5752]])\n",
      "Iteration 22810 Training loss 0.11669167876243591 Validation loss 0.11534416675567627 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.5443],\n",
      "        [0.4137]])\n",
      "Iteration 22820 Training loss 0.11746128648519516 Validation loss 0.11533384025096893 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.4728],\n",
      "        [0.5170]])\n",
      "Iteration 22830 Training loss 0.11696118116378784 Validation loss 0.11532572656869888 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.4362],\n",
      "        [0.5249]])\n",
      "Iteration 22840 Training loss 0.11785414814949036 Validation loss 0.11533614248037338 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.4723],\n",
      "        [0.5530]])\n",
      "Iteration 22850 Training loss 0.11736955493688583 Validation loss 0.11543293297290802 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4125],\n",
      "        [0.5233]])\n",
      "Iteration 22860 Training loss 0.11573424935340881 Validation loss 0.11532233655452728 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.3138],\n",
      "        [0.4218]])\n",
      "Iteration 22870 Training loss 0.11751461774110794 Validation loss 0.11539996415376663 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4342],\n",
      "        [0.5326]])\n",
      "Iteration 22880 Training loss 0.11687062680721283 Validation loss 0.11533801257610321 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.2667],\n",
      "        [0.3188]])\n",
      "Iteration 22890 Training loss 0.1170537918806076 Validation loss 0.11540915817022324 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.5306],\n",
      "        [0.3781]])\n",
      "Iteration 22900 Training loss 0.11617251485586166 Validation loss 0.11530832201242447 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.4267],\n",
      "        [0.6032]])\n",
      "Iteration 22910 Training loss 0.11658254265785217 Validation loss 0.11532088369131088 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.5078],\n",
      "        [0.2923]])\n",
      "Iteration 22920 Training loss 0.11635294556617737 Validation loss 0.11533189564943314 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.4377],\n",
      "        [0.5112]])\n",
      "Iteration 22930 Training loss 0.11655473709106445 Validation loss 0.11532317847013474 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.3480],\n",
      "        [0.8267]])\n",
      "Iteration 22940 Training loss 0.11619094759225845 Validation loss 0.1153334379196167 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.6486],\n",
      "        [0.5239]])\n",
      "Iteration 22950 Training loss 0.11783190071582794 Validation loss 0.11533296853303909 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.4312],\n",
      "        [0.4190]])\n",
      "Iteration 22960 Training loss 0.11659448593854904 Validation loss 0.1153416782617569 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.4803],\n",
      "        [0.7213]])\n",
      "Iteration 22970 Training loss 0.11640387773513794 Validation loss 0.11538837850093842 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.5452],\n",
      "        [0.6017]])\n",
      "Iteration 22980 Training loss 0.11683373153209686 Validation loss 0.11546832323074341 Accuracy 0.6153333187103271\n",
      "Output tensor([[0.5120],\n",
      "        [0.4186]])\n",
      "Iteration 22990 Training loss 0.11579437553882599 Validation loss 0.11529089510440826 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.6054],\n",
      "        [0.4948]])\n",
      "Iteration 23000 Training loss 0.11614803224802017 Validation loss 0.11531643569469452 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.6997],\n",
      "        [0.3258]])\n",
      "Iteration 23010 Training loss 0.11685812473297119 Validation loss 0.11528942734003067 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.2891],\n",
      "        [0.4912]])\n",
      "Iteration 23020 Training loss 0.11817580461502075 Validation loss 0.11531008034944534 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.4939],\n",
      "        [0.5375]])\n",
      "Iteration 23030 Training loss 0.11822667717933655 Validation loss 0.11528245359659195 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.3530],\n",
      "        [0.6435]])\n",
      "Iteration 23040 Training loss 0.11772629618644714 Validation loss 0.11530030518770218 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5513],\n",
      "        [0.5922]])\n",
      "Iteration 23050 Training loss 0.11685467511415482 Validation loss 0.11528410017490387 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5554],\n",
      "        [0.2776]])\n",
      "Iteration 23060 Training loss 0.11711364984512329 Validation loss 0.1152903214097023 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.7201],\n",
      "        [0.6027]])\n",
      "Iteration 23070 Training loss 0.11783172935247421 Validation loss 0.11528345942497253 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.4484],\n",
      "        [0.5179]])\n",
      "Iteration 23080 Training loss 0.11690328270196915 Validation loss 0.11543788760900497 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6032],\n",
      "        [0.3592]])\n",
      "Iteration 23090 Training loss 0.11822200566530228 Validation loss 0.1152849867939949 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.1304],\n",
      "        [0.6491]])\n",
      "Iteration 23100 Training loss 0.11555708944797516 Validation loss 0.11529360711574554 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.4607],\n",
      "        [0.6029]])\n",
      "Iteration 23110 Training loss 0.11714169383049011 Validation loss 0.1152765080332756 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.6481],\n",
      "        [0.5579]])\n",
      "Iteration 23120 Training loss 0.11662301421165466 Validation loss 0.11529363691806793 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4108],\n",
      "        [0.3770]])\n",
      "Iteration 23130 Training loss 0.11699946969747543 Validation loss 0.11537042260169983 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.5495],\n",
      "        [0.6141]])\n",
      "Iteration 23140 Training loss 0.11711625009775162 Validation loss 0.11530745774507523 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.4086],\n",
      "        [0.4245]])\n",
      "Iteration 23150 Training loss 0.11687228083610535 Validation loss 0.11534982174634933 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.4685],\n",
      "        [0.4408]])\n",
      "Iteration 23160 Training loss 0.11820346117019653 Validation loss 0.11530059576034546 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.3148],\n",
      "        [0.2237]])\n",
      "Iteration 23170 Training loss 0.11682448536157608 Validation loss 0.11531319469213486 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.6704],\n",
      "        [0.3460]])\n",
      "Iteration 23180 Training loss 0.11615043878555298 Validation loss 0.11531580239534378 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.4505],\n",
      "        [0.5152]])\n",
      "Iteration 23190 Training loss 0.11786820739507675 Validation loss 0.11531240493059158 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.2283],\n",
      "        [0.5702]])\n",
      "Iteration 23200 Training loss 0.11803185939788818 Validation loss 0.11540161073207855 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5632],\n",
      "        [0.3074]])\n",
      "Iteration 23210 Training loss 0.11651664227247238 Validation loss 0.11530596017837524 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.5290],\n",
      "        [0.3914]])\n",
      "Iteration 23220 Training loss 0.11610139906406403 Validation loss 0.11527471989393234 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.3054],\n",
      "        [0.7643]])\n",
      "Iteration 23230 Training loss 0.11622363328933716 Validation loss 0.11531251668930054 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.6045],\n",
      "        [0.6116]])\n",
      "Iteration 23240 Training loss 0.11709968745708466 Validation loss 0.11528724431991577 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.3470],\n",
      "        [0.6550]])\n",
      "Iteration 23250 Training loss 0.11725285649299622 Validation loss 0.11533398181200027 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.6407],\n",
      "        [0.4178]])\n",
      "Iteration 23260 Training loss 0.11727576702833176 Validation loss 0.11536984145641327 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.4404],\n",
      "        [0.5465]])\n",
      "Iteration 23270 Training loss 0.11762835830450058 Validation loss 0.11533340811729431 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.5622],\n",
      "        [0.5546]])\n",
      "Iteration 23280 Training loss 0.1167064681649208 Validation loss 0.11536214500665665 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.5690],\n",
      "        [0.5571]])\n",
      "Iteration 23290 Training loss 0.11740615963935852 Validation loss 0.11541619896888733 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.3585],\n",
      "        [0.4941]])\n",
      "Iteration 23300 Training loss 0.1165284812450409 Validation loss 0.11538390815258026 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.5191],\n",
      "        [0.6264]])\n",
      "Iteration 23310 Training loss 0.11632000654935837 Validation loss 0.11531595885753632 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.5443],\n",
      "        [0.2668]])\n",
      "Iteration 23320 Training loss 0.1153656616806984 Validation loss 0.11533423513174057 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.3462],\n",
      "        [0.5722]])\n",
      "Iteration 23330 Training loss 0.11583319306373596 Validation loss 0.11533835530281067 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.3479],\n",
      "        [0.4899]])\n",
      "Iteration 23340 Training loss 0.11714718490839005 Validation loss 0.1153312474489212 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5758],\n",
      "        [0.3613]])\n",
      "Iteration 23350 Training loss 0.11575353890657425 Validation loss 0.11536470800638199 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.5413],\n",
      "        [0.5131]])\n",
      "Iteration 23360 Training loss 0.1178860291838646 Validation loss 0.11543399095535278 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.5566],\n",
      "        [0.3177]])\n",
      "Iteration 23370 Training loss 0.11601535975933075 Validation loss 0.11539890617132187 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.4966],\n",
      "        [0.4725]])\n",
      "Iteration 23380 Training loss 0.11707033216953278 Validation loss 0.11550793796777725 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.5940],\n",
      "        [0.4983]])\n",
      "Iteration 23390 Training loss 0.11661612242460251 Validation loss 0.11536563187837601 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.2443],\n",
      "        [0.4862]])\n",
      "Iteration 23400 Training loss 0.1173170655965805 Validation loss 0.1153440773487091 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.3893],\n",
      "        [0.2888]])\n",
      "Iteration 23410 Training loss 0.1172153651714325 Validation loss 0.11533118039369583 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.6209],\n",
      "        [0.6455]])\n",
      "Iteration 23420 Training loss 0.11754713207483292 Validation loss 0.11528291553258896 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.6374],\n",
      "        [0.3971]])\n",
      "Iteration 23430 Training loss 0.11756153404712677 Validation loss 0.11534257233142853 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5488],\n",
      "        [0.5937]])\n",
      "Iteration 23440 Training loss 0.11621589213609695 Validation loss 0.11525403708219528 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4879],\n",
      "        [0.4055]])\n",
      "Iteration 23450 Training loss 0.1172814667224884 Validation loss 0.1152501106262207 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5779],\n",
      "        [0.5527]])\n",
      "Iteration 23460 Training loss 0.11729162931442261 Validation loss 0.11526262760162354 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.2990],\n",
      "        [0.4403]])\n",
      "Iteration 23470 Training loss 0.11648419499397278 Validation loss 0.11527124047279358 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.5457],\n",
      "        [0.6750]])\n",
      "Iteration 23480 Training loss 0.11759049445390701 Validation loss 0.11528275907039642 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.3361],\n",
      "        [0.5404]])\n",
      "Iteration 23490 Training loss 0.11639123409986496 Validation loss 0.11534006893634796 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.5538],\n",
      "        [0.3851]])\n",
      "Iteration 23500 Training loss 0.11684134602546692 Validation loss 0.1152828261256218 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.5864],\n",
      "        [0.5583]])\n",
      "Iteration 23510 Training loss 0.11663103103637695 Validation loss 0.11546216160058975 Accuracy 0.6141666769981384\n",
      "Output tensor([[0.3186],\n",
      "        [0.5551]])\n",
      "Iteration 23520 Training loss 0.1163811981678009 Validation loss 0.115272656083107 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.6894],\n",
      "        [0.4521]])\n",
      "Iteration 23530 Training loss 0.11642860621213913 Validation loss 0.11544442921876907 Accuracy 0.6159999966621399\n",
      "Output tensor([[0.5061],\n",
      "        [0.5316]])\n",
      "Iteration 23540 Training loss 0.11766377836465836 Validation loss 0.115253746509552 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4453],\n",
      "        [0.3190]])\n",
      "Iteration 23550 Training loss 0.11720307171344757 Validation loss 0.11531340330839157 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.4238],\n",
      "        [0.5358]])\n",
      "Iteration 23560 Training loss 0.11631911247968674 Validation loss 0.11525504291057587 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.3984],\n",
      "        [0.4241]])\n",
      "Iteration 23570 Training loss 0.11620444804430008 Validation loss 0.11526281386613846 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.4930],\n",
      "        [0.2945]])\n",
      "Iteration 23580 Training loss 0.11690808087587357 Validation loss 0.11528688669204712 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.4316],\n",
      "        [0.5096]])\n",
      "Iteration 23590 Training loss 0.11736255139112473 Validation loss 0.11527130007743835 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.4711],\n",
      "        [0.6803]])\n",
      "Iteration 23600 Training loss 0.1165795624256134 Validation loss 0.11528681218624115 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.6630],\n",
      "        [0.3950]])\n",
      "Iteration 23610 Training loss 0.11827979236841202 Validation loss 0.11527889221906662 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.6576],\n",
      "        [0.4677]])\n",
      "Iteration 23620 Training loss 0.11686030775308609 Validation loss 0.11528605222702026 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4850],\n",
      "        [0.6168]])\n",
      "Iteration 23630 Training loss 0.11778513342142105 Validation loss 0.11530546098947525 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.4050],\n",
      "        [0.2249]])\n",
      "Iteration 23640 Training loss 0.1164410412311554 Validation loss 0.11531588435173035 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.2545],\n",
      "        [0.4360]])\n",
      "Iteration 23650 Training loss 0.11771585792303085 Validation loss 0.1153227761387825 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5133],\n",
      "        [0.5907]])\n",
      "Iteration 23660 Training loss 0.11659315973520279 Validation loss 0.11528763175010681 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.4997],\n",
      "        [0.4845]])\n",
      "Iteration 23670 Training loss 0.11838238686323166 Validation loss 0.11542306840419769 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.4030],\n",
      "        [0.5060]])\n",
      "Iteration 23680 Training loss 0.11654574424028397 Validation loss 0.11532171815633774 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.3202],\n",
      "        [0.6341]])\n",
      "Iteration 23690 Training loss 0.11561331152915955 Validation loss 0.11533361673355103 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4235],\n",
      "        [0.5252]])\n",
      "Iteration 23700 Training loss 0.11687064915895462 Validation loss 0.11533714085817337 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.6250],\n",
      "        [0.2694]])\n",
      "Iteration 23710 Training loss 0.11722790449857712 Validation loss 0.1152852475643158 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.4936],\n",
      "        [0.3498]])\n",
      "Iteration 23720 Training loss 0.11772525310516357 Validation loss 0.11532249301671982 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.5143],\n",
      "        [0.5078]])\n",
      "Iteration 23730 Training loss 0.11751207709312439 Validation loss 0.11525260657072067 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.4258],\n",
      "        [0.5447]])\n",
      "Iteration 23740 Training loss 0.11606156080961227 Validation loss 0.11527930945158005 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.3670],\n",
      "        [0.3696]])\n",
      "Iteration 23750 Training loss 0.11659363657236099 Validation loss 0.11523105949163437 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.6034],\n",
      "        [0.5994]])\n",
      "Iteration 23760 Training loss 0.11636088043451309 Validation loss 0.11522633582353592 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4594],\n",
      "        [0.4678]])\n",
      "Iteration 23770 Training loss 0.11712445318698883 Validation loss 0.11521997302770615 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5751],\n",
      "        [0.5595]])\n",
      "Iteration 23780 Training loss 0.11704832315444946 Validation loss 0.11522087454795837 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5598],\n",
      "        [0.6752]])\n",
      "Iteration 23790 Training loss 0.11535054445266724 Validation loss 0.11523815989494324 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.4347],\n",
      "        [0.3881]])\n",
      "Iteration 23800 Training loss 0.11734291911125183 Validation loss 0.11529554426670074 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.3421],\n",
      "        [0.5418]])\n",
      "Iteration 23810 Training loss 0.11631800979375839 Validation loss 0.11542104184627533 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.6277],\n",
      "        [0.6402]])\n",
      "Iteration 23820 Training loss 0.1172809824347496 Validation loss 0.11525225639343262 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.6707],\n",
      "        [0.3501]])\n",
      "Iteration 23830 Training loss 0.11719559878110886 Validation loss 0.1152547299861908 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.5635],\n",
      "        [0.4400]])\n",
      "Iteration 23840 Training loss 0.11706133931875229 Validation loss 0.11537209153175354 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.3670],\n",
      "        [0.3220]])\n",
      "Iteration 23850 Training loss 0.11769004166126251 Validation loss 0.11524344980716705 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.6148],\n",
      "        [0.4880]])\n",
      "Iteration 23860 Training loss 0.11740374565124512 Validation loss 0.11523671448230743 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.4080],\n",
      "        [0.3989]])\n",
      "Iteration 23870 Training loss 0.11740055680274963 Validation loss 0.11523858457803726 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.4171],\n",
      "        [0.5104]])\n",
      "Iteration 23880 Training loss 0.11812766641378403 Validation loss 0.11524548381567001 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.2900],\n",
      "        [0.4845]])\n",
      "Iteration 23890 Training loss 0.11631746590137482 Validation loss 0.11542794108390808 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.6906],\n",
      "        [0.3826]])\n",
      "Iteration 23900 Training loss 0.11572755873203278 Validation loss 0.11520847678184509 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.4412],\n",
      "        [0.3492]])\n",
      "Iteration 23910 Training loss 0.11596269905567169 Validation loss 0.1152077242732048 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.7006],\n",
      "        [0.4674]])\n",
      "Iteration 23920 Training loss 0.1170930489897728 Validation loss 0.11521835625171661 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.4915],\n",
      "        [0.3287]])\n",
      "Iteration 23930 Training loss 0.11714138090610504 Validation loss 0.11522261798381805 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.5461],\n",
      "        [0.4232]])\n",
      "Iteration 23940 Training loss 0.11676624417304993 Validation loss 0.11520814895629883 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5707],\n",
      "        [0.5195]])\n",
      "Iteration 23950 Training loss 0.11568257957696915 Validation loss 0.11533956229686737 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.7158],\n",
      "        [0.4422]])\n",
      "Iteration 23960 Training loss 0.11585573107004166 Validation loss 0.11521892994642258 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.3391],\n",
      "        [0.6172]])\n",
      "Iteration 23970 Training loss 0.11808431148529053 Validation loss 0.11524952203035355 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.6017],\n",
      "        [0.4681]])\n",
      "Iteration 23980 Training loss 0.11753193289041519 Validation loss 0.11519309133291245 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4671],\n",
      "        [0.5689]])\n",
      "Iteration 23990 Training loss 0.11718487739562988 Validation loss 0.11529761552810669 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.1801],\n",
      "        [0.3058]])\n",
      "Iteration 24000 Training loss 0.1170467734336853 Validation loss 0.11519474536180496 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.6683],\n",
      "        [0.4476]])\n",
      "Iteration 24010 Training loss 0.11776277422904968 Validation loss 0.11519511044025421 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.5339],\n",
      "        [0.3337]])\n",
      "Iteration 24020 Training loss 0.11711590737104416 Validation loss 0.11521685868501663 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.3850],\n",
      "        [0.7275]])\n",
      "Iteration 24030 Training loss 0.1166016235947609 Validation loss 0.11524306237697601 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.6990],\n",
      "        [0.6456]])\n",
      "Iteration 24040 Training loss 0.11737129837274551 Validation loss 0.11523008346557617 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.6943],\n",
      "        [0.6633]])\n",
      "Iteration 24050 Training loss 0.11646752059459686 Validation loss 0.11547783762216568 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.2448],\n",
      "        [0.6968]])\n",
      "Iteration 24060 Training loss 0.11641893535852432 Validation loss 0.11518765985965729 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.3398],\n",
      "        [0.4796]])\n",
      "Iteration 24070 Training loss 0.11684846132993698 Validation loss 0.11520525068044662 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.3991],\n",
      "        [0.4160]])\n",
      "Iteration 24080 Training loss 0.11669710278511047 Validation loss 0.11520123481750488 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.5979],\n",
      "        [0.5131]])\n",
      "Iteration 24090 Training loss 0.11758019030094147 Validation loss 0.11523791402578354 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5397],\n",
      "        [0.5169]])\n",
      "Iteration 24100 Training loss 0.11801206320524216 Validation loss 0.11529263108968735 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.5078],\n",
      "        [0.1597]])\n",
      "Iteration 24110 Training loss 0.11711987107992172 Validation loss 0.11518415063619614 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.3712],\n",
      "        [0.4062]])\n",
      "Iteration 24120 Training loss 0.11639668792486191 Validation loss 0.11521267890930176 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.4293],\n",
      "        [0.5924]])\n",
      "Iteration 24130 Training loss 0.11658285558223724 Validation loss 0.11516468971967697 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.2592],\n",
      "        [0.3340]])\n",
      "Iteration 24140 Training loss 0.11720287799835205 Validation loss 0.11521508544683456 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.4805],\n",
      "        [0.4590]])\n",
      "Iteration 24150 Training loss 0.1153857558965683 Validation loss 0.11520466208457947 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.3915],\n",
      "        [0.5117]])\n",
      "Iteration 24160 Training loss 0.11593536287546158 Validation loss 0.11524797230958939 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.6348],\n",
      "        [0.4645]])\n",
      "Iteration 24170 Training loss 0.11692294478416443 Validation loss 0.11521937698125839 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.4154],\n",
      "        [0.4601]])\n",
      "Iteration 24180 Training loss 0.11699999123811722 Validation loss 0.11528092622756958 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.3513],\n",
      "        [0.5135]])\n",
      "Iteration 24190 Training loss 0.11763270944356918 Validation loss 0.1152806207537651 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.6562],\n",
      "        [0.2583]])\n",
      "Iteration 24200 Training loss 0.11662621796131134 Validation loss 0.11523960530757904 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.5042],\n",
      "        [0.7389]])\n",
      "Iteration 24210 Training loss 0.11652658879756927 Validation loss 0.11522237956523895 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.6066],\n",
      "        [0.5782]])\n",
      "Iteration 24220 Training loss 0.11714258044958115 Validation loss 0.11530392616987228 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.4890],\n",
      "        [0.5699]])\n",
      "Iteration 24230 Training loss 0.11635695397853851 Validation loss 0.1152212917804718 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5673],\n",
      "        [0.4008]])\n",
      "Iteration 24240 Training loss 0.11544951051473618 Validation loss 0.11527948081493378 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.2226],\n",
      "        [0.6031]])\n",
      "Iteration 24250 Training loss 0.11595022678375244 Validation loss 0.11526244878768921 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5725],\n",
      "        [0.4682]])\n",
      "Iteration 24260 Training loss 0.11679745465517044 Validation loss 0.11522310227155685 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.4280],\n",
      "        [0.6749]])\n",
      "Iteration 24270 Training loss 0.11633598804473877 Validation loss 0.11525063216686249 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.3863],\n",
      "        [0.5398]])\n",
      "Iteration 24280 Training loss 0.11607523262500763 Validation loss 0.11524324864149094 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4400],\n",
      "        [0.2506]])\n",
      "Iteration 24290 Training loss 0.11729428172111511 Validation loss 0.11528296023607254 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.2656],\n",
      "        [0.4417]])\n",
      "Iteration 24300 Training loss 0.11649767309427261 Validation loss 0.11525598168373108 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.6387],\n",
      "        [0.5304]])\n",
      "Iteration 24310 Training loss 0.116838239133358 Validation loss 0.11524648219347 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.5890],\n",
      "        [0.6350]])\n",
      "Iteration 24320 Training loss 0.11800459027290344 Validation loss 0.11530791968107224 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.4691],\n",
      "        [0.4316]])\n",
      "Iteration 24330 Training loss 0.11669119447469711 Validation loss 0.11523797363042831 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.5340],\n",
      "        [0.6578]])\n",
      "Iteration 24340 Training loss 0.1169564425945282 Validation loss 0.1152452602982521 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4861],\n",
      "        [0.5483]])\n",
      "Iteration 24350 Training loss 0.11811106652021408 Validation loss 0.11522353440523148 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.2210],\n",
      "        [0.5468]])\n",
      "Iteration 24360 Training loss 0.11611045151948929 Validation loss 0.11523027718067169 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.6370],\n",
      "        [0.5884]])\n",
      "Iteration 24370 Training loss 0.11611303687095642 Validation loss 0.11522471159696579 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.6729],\n",
      "        [0.3302]])\n",
      "Iteration 24380 Training loss 0.11697367578744888 Validation loss 0.11523258686065674 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.6177],\n",
      "        [0.3981]])\n",
      "Iteration 24390 Training loss 0.1167936623096466 Validation loss 0.11534295976161957 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.3654],\n",
      "        [0.5835]])\n",
      "Iteration 24400 Training loss 0.11686647683382034 Validation loss 0.11523985862731934 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.5066],\n",
      "        [0.5935]])\n",
      "Iteration 24410 Training loss 0.11681175976991653 Validation loss 0.11523372679948807 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.3865],\n",
      "        [0.3471]])\n",
      "Iteration 24420 Training loss 0.11714094877243042 Validation loss 0.11522538214921951 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.2242],\n",
      "        [0.4468]])\n",
      "Iteration 24430 Training loss 0.11651399731636047 Validation loss 0.11521101742982864 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.3567],\n",
      "        [0.6426]])\n",
      "Iteration 24440 Training loss 0.11714667826890945 Validation loss 0.11525295674800873 Accuracy 0.6154999732971191\n",
      "Output tensor([[0.6388],\n",
      "        [0.5189]])\n",
      "Iteration 24450 Training loss 0.11828437447547913 Validation loss 0.11519414931535721 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.6296],\n",
      "        [0.4871]])\n",
      "Iteration 24460 Training loss 0.11708813160657883 Validation loss 0.11522278189659119 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.6333],\n",
      "        [0.5387]])\n",
      "Iteration 24470 Training loss 0.11722609400749207 Validation loss 0.11528714746236801 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.5465],\n",
      "        [0.5365]])\n",
      "Iteration 24480 Training loss 0.11739976704120636 Validation loss 0.11518692225217819 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.4779],\n",
      "        [0.6393]])\n",
      "Iteration 24490 Training loss 0.11635410785675049 Validation loss 0.11521460115909576 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5652],\n",
      "        [0.5419]])\n",
      "Iteration 24500 Training loss 0.11696197837591171 Validation loss 0.11521617323160172 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.4500],\n",
      "        [0.6338]])\n",
      "Iteration 24510 Training loss 0.11665265262126923 Validation loss 0.1152084469795227 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.3992],\n",
      "        [0.4924]])\n",
      "Iteration 24520 Training loss 0.11808799207210541 Validation loss 0.11529144644737244 Accuracy 0.6148333549499512\n",
      "Output tensor([[0.2024],\n",
      "        [0.4788]])\n",
      "Iteration 24530 Training loss 0.1159958615899086 Validation loss 0.1152016818523407 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5396],\n",
      "        [0.5200]])\n",
      "Iteration 24540 Training loss 0.11671461910009384 Validation loss 0.1151825338602066 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.5623],\n",
      "        [0.3927]])\n",
      "Iteration 24550 Training loss 0.11705143004655838 Validation loss 0.11520098894834518 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.6833],\n",
      "        [0.5868]])\n",
      "Iteration 24560 Training loss 0.1175316870212555 Validation loss 0.1151953935623169 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5338],\n",
      "        [0.5475]])\n",
      "Iteration 24570 Training loss 0.1172742173075676 Validation loss 0.11518748849630356 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.6758],\n",
      "        [0.5122]])\n",
      "Iteration 24580 Training loss 0.11670871824026108 Validation loss 0.11515618115663528 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.4729],\n",
      "        [0.6871]])\n",
      "Iteration 24590 Training loss 0.11641161143779755 Validation loss 0.11515725404024124 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.4853],\n",
      "        [0.4437]])\n",
      "Iteration 24600 Training loss 0.11673956364393234 Validation loss 0.11516094952821732 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.6177],\n",
      "        [0.4858]])\n",
      "Iteration 24610 Training loss 0.11647788435220718 Validation loss 0.11516493558883667 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.7068],\n",
      "        [0.3141]])\n",
      "Iteration 24620 Training loss 0.11602786928415298 Validation loss 0.11516067385673523 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.5052],\n",
      "        [0.2499]])\n",
      "Iteration 24630 Training loss 0.116382896900177 Validation loss 0.11514939367771149 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.5493],\n",
      "        [0.5965]])\n",
      "Iteration 24640 Training loss 0.11709970235824585 Validation loss 0.11514994502067566 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.3539],\n",
      "        [0.7693]])\n",
      "Iteration 24650 Training loss 0.11713665723800659 Validation loss 0.11519530415534973 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.6098],\n",
      "        [0.3121]])\n",
      "Iteration 24660 Training loss 0.11621344089508057 Validation loss 0.1151568815112114 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.4515],\n",
      "        [0.6460]])\n",
      "Iteration 24670 Training loss 0.11757445335388184 Validation loss 0.11513318121433258 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.3954],\n",
      "        [0.7208]])\n",
      "Iteration 24680 Training loss 0.11554346978664398 Validation loss 0.11521437764167786 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.7287],\n",
      "        [0.6361]])\n",
      "Iteration 24690 Training loss 0.11763708293437958 Validation loss 0.1152261272072792 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.3521],\n",
      "        [0.5669]])\n",
      "Iteration 24700 Training loss 0.11726849526166916 Validation loss 0.1151529848575592 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.1887],\n",
      "        [0.6292]])\n",
      "Iteration 24710 Training loss 0.11631040275096893 Validation loss 0.11513539403676987 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.6066],\n",
      "        [0.2672]])\n",
      "Iteration 24720 Training loss 0.1172335222363472 Validation loss 0.11515716463327408 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.4246],\n",
      "        [0.4895]])\n",
      "Iteration 24730 Training loss 0.11752163618803024 Validation loss 0.1151590347290039 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.2748],\n",
      "        [0.2702]])\n",
      "Iteration 24740 Training loss 0.11872666329145432 Validation loss 0.11514028906822205 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.5216],\n",
      "        [0.4037]])\n",
      "Iteration 24750 Training loss 0.11751782149076462 Validation loss 0.11513108015060425 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.2573],\n",
      "        [0.4289]])\n",
      "Iteration 24760 Training loss 0.11613456904888153 Validation loss 0.11518080532550812 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.2082],\n",
      "        [0.4980]])\n",
      "Iteration 24770 Training loss 0.1147761195898056 Validation loss 0.11512868106365204 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4526],\n",
      "        [0.3437]])\n",
      "Iteration 24780 Training loss 0.1167539656162262 Validation loss 0.11513148993253708 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.4835],\n",
      "        [0.4442]])\n",
      "Iteration 24790 Training loss 0.11546532064676285 Validation loss 0.11512753367424011 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.2460],\n",
      "        [0.5959]])\n",
      "Iteration 24800 Training loss 0.1159912645816803 Validation loss 0.1151542067527771 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.4772],\n",
      "        [0.4794]])\n",
      "Iteration 24810 Training loss 0.11771020293235779 Validation loss 0.11513499170541763 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.4863],\n",
      "        [0.6418]])\n",
      "Iteration 24820 Training loss 0.11620853841304779 Validation loss 0.11516115069389343 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.4226],\n",
      "        [0.3349]])\n",
      "Iteration 24830 Training loss 0.11596473306417465 Validation loss 0.11525367200374603 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5455],\n",
      "        [0.5203]])\n",
      "Iteration 24840 Training loss 0.11662308871746063 Validation loss 0.1151561364531517 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.6572],\n",
      "        [0.5775]])\n",
      "Iteration 24850 Training loss 0.11634236574172974 Validation loss 0.11514496803283691 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.6461],\n",
      "        [0.5337]])\n",
      "Iteration 24860 Training loss 0.11757855117321014 Validation loss 0.11515053361654282 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.6322],\n",
      "        [0.3569]])\n",
      "Iteration 24870 Training loss 0.11599449813365936 Validation loss 0.1151304692029953 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.5327],\n",
      "        [0.4914]])\n",
      "Iteration 24880 Training loss 0.11574195325374603 Validation loss 0.11514220386743546 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.4675],\n",
      "        [0.2048]])\n",
      "Iteration 24890 Training loss 0.11557977646589279 Validation loss 0.11515376716852188 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.4422],\n",
      "        [0.6419]])\n",
      "Iteration 24900 Training loss 0.11770131438970566 Validation loss 0.11515246331691742 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.6431],\n",
      "        [0.7448]])\n",
      "Iteration 24910 Training loss 0.11700122058391571 Validation loss 0.1151430755853653 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4423],\n",
      "        [0.5308]])\n",
      "Iteration 24920 Training loss 0.11601212620735168 Validation loss 0.11511917412281036 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.5615],\n",
      "        [0.6081]])\n",
      "Iteration 24930 Training loss 0.11684248596429825 Validation loss 0.11515524983406067 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.2951],\n",
      "        [0.6012]])\n",
      "Iteration 24940 Training loss 0.11572391539812088 Validation loss 0.11514122784137726 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4476],\n",
      "        [0.2384]])\n",
      "Iteration 24950 Training loss 0.11659391969442368 Validation loss 0.11512477695941925 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.5214],\n",
      "        [0.5555]])\n",
      "Iteration 24960 Training loss 0.11722169816493988 Validation loss 0.11515529453754425 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.8355],\n",
      "        [0.5494]])\n",
      "Iteration 24970 Training loss 0.11757420748472214 Validation loss 0.11513856798410416 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.5336],\n",
      "        [0.3929]])\n",
      "Iteration 24980 Training loss 0.11652929335832596 Validation loss 0.11511784046888351 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4889],\n",
      "        [0.2767]])\n",
      "Iteration 24990 Training loss 0.11676885932683945 Validation loss 0.11513537913560867 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.4864],\n",
      "        [0.2981]])\n",
      "Iteration 25000 Training loss 0.11735239624977112 Validation loss 0.1151266098022461 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.7251],\n",
      "        [0.5829]])\n",
      "Iteration 25010 Training loss 0.11631380021572113 Validation loss 0.11512559652328491 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.3251],\n",
      "        [0.4994]])\n",
      "Iteration 25020 Training loss 0.11756262183189392 Validation loss 0.11531398445367813 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.5390],\n",
      "        [0.2323]])\n",
      "Iteration 25030 Training loss 0.11640086024999619 Validation loss 0.11515374481678009 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.6872],\n",
      "        [0.3365]])\n",
      "Iteration 25040 Training loss 0.11690033227205276 Validation loss 0.11516127735376358 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.6028],\n",
      "        [0.5809]])\n",
      "Iteration 25050 Training loss 0.11733707785606384 Validation loss 0.11517887562513351 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.4812],\n",
      "        [0.4183]])\n",
      "Iteration 25060 Training loss 0.1177685335278511 Validation loss 0.11515554040670395 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.6087],\n",
      "        [0.4332]])\n",
      "Iteration 25070 Training loss 0.11733639240264893 Validation loss 0.11517447978258133 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.5652],\n",
      "        [0.3660]])\n",
      "Iteration 25080 Training loss 0.11760471761226654 Validation loss 0.11515548825263977 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.3100],\n",
      "        [0.4503]])\n",
      "Iteration 25090 Training loss 0.11762552708387375 Validation loss 0.11511942744255066 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.6281],\n",
      "        [0.5087]])\n",
      "Iteration 25100 Training loss 0.11687278002500534 Validation loss 0.11513195931911469 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.4391],\n",
      "        [0.3893]])\n",
      "Iteration 25110 Training loss 0.11660683155059814 Validation loss 0.11513427644968033 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.5701],\n",
      "        [0.4476]])\n",
      "Iteration 25120 Training loss 0.11688324064016342 Validation loss 0.11516131460666656 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.5359],\n",
      "        [0.6525]])\n",
      "Iteration 25130 Training loss 0.11743659526109695 Validation loss 0.11518359184265137 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.2760],\n",
      "        [0.4396]])\n",
      "Iteration 25140 Training loss 0.1168639287352562 Validation loss 0.1151699423789978 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.6256],\n",
      "        [0.4518]])\n",
      "Iteration 25150 Training loss 0.11505401879549026 Validation loss 0.11510676145553589 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4112],\n",
      "        [0.3054]])\n",
      "Iteration 25160 Training loss 0.11783892661333084 Validation loss 0.11512164026498795 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.5733],\n",
      "        [0.5779]])\n",
      "Iteration 25170 Training loss 0.11688992381095886 Validation loss 0.11515532433986664 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5750],\n",
      "        [0.4761]])\n",
      "Iteration 25180 Training loss 0.11636843532323837 Validation loss 0.11514009535312653 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5147],\n",
      "        [0.4645]])\n",
      "Iteration 25190 Training loss 0.11626988649368286 Validation loss 0.1151353120803833 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3009],\n",
      "        [0.4971]])\n",
      "Iteration 25200 Training loss 0.11814375966787338 Validation loss 0.1151762306690216 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.3861],\n",
      "        [0.5385]])\n",
      "Iteration 25210 Training loss 0.11813423782587051 Validation loss 0.11513745039701462 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.7089],\n",
      "        [0.4371]])\n",
      "Iteration 25220 Training loss 0.11698058247566223 Validation loss 0.11517197638750076 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.4717],\n",
      "        [0.6283]])\n",
      "Iteration 25230 Training loss 0.11761093884706497 Validation loss 0.11513595283031464 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5095],\n",
      "        [0.4884]])\n",
      "Iteration 25240 Training loss 0.11696412414312363 Validation loss 0.1151110976934433 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.3513],\n",
      "        [0.5273]])\n",
      "Iteration 25250 Training loss 0.11702349781990051 Validation loss 0.1151169016957283 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.3598],\n",
      "        [0.4805]])\n",
      "Iteration 25260 Training loss 0.11753103882074356 Validation loss 0.11536001414060593 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5534],\n",
      "        [0.5111]])\n",
      "Iteration 25270 Training loss 0.11513254791498184 Validation loss 0.11513116210699081 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.6327],\n",
      "        [0.3499]])\n",
      "Iteration 25280 Training loss 0.11658097058534622 Validation loss 0.11511742323637009 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.4379],\n",
      "        [0.5949]])\n",
      "Iteration 25290 Training loss 0.1181994155049324 Validation loss 0.1151006668806076 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.6371],\n",
      "        [0.4138]])\n",
      "Iteration 25300 Training loss 0.11636310070753098 Validation loss 0.11513332277536392 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.3209],\n",
      "        [0.5538]])\n",
      "Iteration 25310 Training loss 0.11535526812076569 Validation loss 0.11514550447463989 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.4744],\n",
      "        [0.4123]])\n",
      "Iteration 25320 Training loss 0.1152072474360466 Validation loss 0.11521632224321365 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.2616],\n",
      "        [0.4921]])\n",
      "Iteration 25330 Training loss 0.1166023388504982 Validation loss 0.11511172354221344 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4193],\n",
      "        [0.3922]])\n",
      "Iteration 25340 Training loss 0.11808319389820099 Validation loss 0.11521805077791214 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.4601],\n",
      "        [0.4466]])\n",
      "Iteration 25350 Training loss 0.11688103526830673 Validation loss 0.11513639241456985 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.4119],\n",
      "        [0.6654]])\n",
      "Iteration 25360 Training loss 0.11655920743942261 Validation loss 0.115203358232975 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.4869],\n",
      "        [0.4127]])\n",
      "Iteration 25370 Training loss 0.11669141799211502 Validation loss 0.11517804861068726 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.3474],\n",
      "        [0.6209]])\n",
      "Iteration 25380 Training loss 0.1173480749130249 Validation loss 0.11514682322740555 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5843],\n",
      "        [0.5251]])\n",
      "Iteration 25390 Training loss 0.11612097918987274 Validation loss 0.11514519900083542 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5348],\n",
      "        [0.7305]])\n",
      "Iteration 25400 Training loss 0.11721912771463394 Validation loss 0.11513572186231613 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5146],\n",
      "        [0.2691]])\n",
      "Iteration 25410 Training loss 0.11617903411388397 Validation loss 0.1151493713259697 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.6270],\n",
      "        [0.6127]])\n",
      "Iteration 25420 Training loss 0.1164972335100174 Validation loss 0.11514490097761154 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.4731],\n",
      "        [0.6269]])\n",
      "Iteration 25430 Training loss 0.11611734330654144 Validation loss 0.1151188462972641 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.4111],\n",
      "        [0.3529]])\n",
      "Iteration 25440 Training loss 0.11688655614852905 Validation loss 0.11514568328857422 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5362],\n",
      "        [0.4429]])\n",
      "Iteration 25450 Training loss 0.11693456768989563 Validation loss 0.11513494700193405 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.5250],\n",
      "        [0.6097]])\n",
      "Iteration 25460 Training loss 0.11747007071971893 Validation loss 0.11509863287210464 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.6769],\n",
      "        [0.5013]])\n",
      "Iteration 25470 Training loss 0.11682132631540298 Validation loss 0.11509161442518234 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.5243],\n",
      "        [0.3243]])\n",
      "Iteration 25480 Training loss 0.11684751510620117 Validation loss 0.11511378735303879 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.5605],\n",
      "        [0.5767]])\n",
      "Iteration 25490 Training loss 0.11611621826887131 Validation loss 0.11514496058225632 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.6492],\n",
      "        [0.7158]])\n",
      "Iteration 25500 Training loss 0.11673109978437424 Validation loss 0.11512530595064163 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4641],\n",
      "        [0.4545]])\n",
      "Iteration 25510 Training loss 0.11695428192615509 Validation loss 0.11512688547372818 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.5376],\n",
      "        [0.6164]])\n",
      "Iteration 25520 Training loss 0.1171574741601944 Validation loss 0.11516123265028 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.4233],\n",
      "        [0.5464]])\n",
      "Iteration 25530 Training loss 0.11752854287624359 Validation loss 0.11517387628555298 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5542],\n",
      "        [0.5594]])\n",
      "Iteration 25540 Training loss 0.1171550527215004 Validation loss 0.11512449383735657 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5512],\n",
      "        [0.4968]])\n",
      "Iteration 25550 Training loss 0.11666062474250793 Validation loss 0.11509743332862854 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5313],\n",
      "        [0.4056]])\n",
      "Iteration 25560 Training loss 0.11627252399921417 Validation loss 0.11510790884494781 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5318],\n",
      "        [0.4910]])\n",
      "Iteration 25570 Training loss 0.11675095558166504 Validation loss 0.11515980958938599 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.5828],\n",
      "        [0.4623]])\n",
      "Iteration 25580 Training loss 0.11688952893018723 Validation loss 0.11511686444282532 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5938],\n",
      "        [0.5038]])\n",
      "Iteration 25590 Training loss 0.1174689456820488 Validation loss 0.1151256337761879 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.2183],\n",
      "        [0.6352]])\n",
      "Iteration 25600 Training loss 0.11746750771999359 Validation loss 0.11516273766756058 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.5773],\n",
      "        [0.5263]])\n",
      "Iteration 25610 Training loss 0.11611423641443253 Validation loss 0.11517880111932755 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.2403],\n",
      "        [0.2807]])\n",
      "Iteration 25620 Training loss 0.11702432483434677 Validation loss 0.11510311812162399 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.2018],\n",
      "        [0.6552]])\n",
      "Iteration 25630 Training loss 0.11739489436149597 Validation loss 0.11508703976869583 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3930],\n",
      "        [0.5503]])\n",
      "Iteration 25640 Training loss 0.11741980165243149 Validation loss 0.1150837317109108 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.3370],\n",
      "        [0.3689]])\n",
      "Iteration 25650 Training loss 0.11628907918930054 Validation loss 0.11507876217365265 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.6186],\n",
      "        [0.7458]])\n",
      "Iteration 25660 Training loss 0.11621622741222382 Validation loss 0.11510386317968369 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.4501],\n",
      "        [0.5569]])\n",
      "Iteration 25670 Training loss 0.1166682317852974 Validation loss 0.11510509997606277 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.6152],\n",
      "        [0.3971]])\n",
      "Iteration 25680 Training loss 0.11769894510507584 Validation loss 0.11514157801866531 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.7178],\n",
      "        [0.3748]])\n",
      "Iteration 25690 Training loss 0.11747212707996368 Validation loss 0.11512845754623413 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.3926],\n",
      "        [0.5375]])\n",
      "Iteration 25700 Training loss 0.11636347323656082 Validation loss 0.11514293402433395 Accuracy 0.6163333058357239\n",
      "Output tensor([[0.6571],\n",
      "        [0.5865]])\n",
      "Iteration 25710 Training loss 0.1171080693602562 Validation loss 0.11512087285518646 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5551],\n",
      "        [0.2765]])\n",
      "Iteration 25720 Training loss 0.11750400811433792 Validation loss 0.1151454970240593 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.6644],\n",
      "        [0.5030]])\n",
      "Iteration 25730 Training loss 0.11590609699487686 Validation loss 0.11512166261672974 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.4400],\n",
      "        [0.5761]])\n",
      "Iteration 25740 Training loss 0.11713423579931259 Validation loss 0.11513006687164307 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.2790],\n",
      "        [0.2487]])\n",
      "Iteration 25750 Training loss 0.11781931668519974 Validation loss 0.11519842594861984 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.4573],\n",
      "        [0.3968]])\n",
      "Iteration 25760 Training loss 0.11719836294651031 Validation loss 0.11514310538768768 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.4891],\n",
      "        [0.4621]])\n",
      "Iteration 25770 Training loss 0.11720212548971176 Validation loss 0.11514550447463989 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.3378],\n",
      "        [0.3978]])\n",
      "Iteration 25780 Training loss 0.11613085120916367 Validation loss 0.11529427766799927 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.4689],\n",
      "        [0.6319]])\n",
      "Iteration 25790 Training loss 0.11591430008411407 Validation loss 0.1152438148856163 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5137],\n",
      "        [0.6404]])\n",
      "Iteration 25800 Training loss 0.11628811806440353 Validation loss 0.11511486023664474 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.5813],\n",
      "        [0.5388]])\n",
      "Iteration 25810 Training loss 0.11659188568592072 Validation loss 0.11512882262468338 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.3094],\n",
      "        [0.4909]])\n",
      "Iteration 25820 Training loss 0.11593301594257355 Validation loss 0.11511588841676712 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.4441],\n",
      "        [0.6309]])\n",
      "Iteration 25830 Training loss 0.11585641652345657 Validation loss 0.11511640250682831 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.6083],\n",
      "        [0.5205]])\n",
      "Iteration 25840 Training loss 0.11716338992118835 Validation loss 0.11506934463977814 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5506],\n",
      "        [0.6565]])\n",
      "Iteration 25850 Training loss 0.117171511054039 Validation loss 0.11512283235788345 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5877],\n",
      "        [0.4386]])\n",
      "Iteration 25860 Training loss 0.11573727428913116 Validation loss 0.11519056558609009 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.6222],\n",
      "        [0.4822]])\n",
      "Iteration 25870 Training loss 0.11607915908098221 Validation loss 0.11508285254240036 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5875],\n",
      "        [0.4342]])\n",
      "Iteration 25880 Training loss 0.1172088086605072 Validation loss 0.11512622237205505 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.6124],\n",
      "        [0.3001]])\n",
      "Iteration 25890 Training loss 0.11648615449666977 Validation loss 0.11507576704025269 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3722],\n",
      "        [0.5787]])\n",
      "Iteration 25900 Training loss 0.11735766381025314 Validation loss 0.11516420543193817 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.4198],\n",
      "        [0.6430]])\n",
      "Iteration 25910 Training loss 0.11637179553508759 Validation loss 0.11514179408550262 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.4437],\n",
      "        [0.2593]])\n",
      "Iteration 25920 Training loss 0.1158575639128685 Validation loss 0.11508927494287491 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.4559],\n",
      "        [0.3087]])\n",
      "Iteration 25930 Training loss 0.11697814613580704 Validation loss 0.1150975152850151 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4474],\n",
      "        [0.5526]])\n",
      "Iteration 25940 Training loss 0.11553601920604706 Validation loss 0.11506430059671402 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5719],\n",
      "        [0.4303]])\n",
      "Iteration 25950 Training loss 0.1156829223036766 Validation loss 0.11507847905158997 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.4261],\n",
      "        [0.5745]])\n",
      "Iteration 25960 Training loss 0.11685756593942642 Validation loss 0.11511511355638504 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.4668],\n",
      "        [0.4155]])\n",
      "Iteration 25970 Training loss 0.1163301169872284 Validation loss 0.11508986353874207 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.2690],\n",
      "        [0.5214]])\n",
      "Iteration 25980 Training loss 0.11626449227333069 Validation loss 0.11509879678487778 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.4530],\n",
      "        [0.3999]])\n",
      "Iteration 25990 Training loss 0.11762459576129913 Validation loss 0.1151059940457344 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.5735],\n",
      "        [0.6328]])\n",
      "Iteration 26000 Training loss 0.11560554802417755 Validation loss 0.11508423089981079 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5727],\n",
      "        [0.6494]])\n",
      "Iteration 26010 Training loss 0.11709736287593842 Validation loss 0.11511088162660599 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5881],\n",
      "        [0.6106]])\n",
      "Iteration 26020 Training loss 0.1156296506524086 Validation loss 0.11508303135633469 Accuracy 0.6168333292007446\n",
      "Output tensor([[0.2563],\n",
      "        [0.4840]])\n",
      "Iteration 26030 Training loss 0.1165253072977066 Validation loss 0.11513344198465347 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5451],\n",
      "        [0.5443]])\n",
      "Iteration 26040 Training loss 0.1168680191040039 Validation loss 0.11509876698255539 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5597],\n",
      "        [0.6588]])\n",
      "Iteration 26050 Training loss 0.116706982254982 Validation loss 0.11509404331445694 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.3747],\n",
      "        [0.3881]])\n",
      "Iteration 26060 Training loss 0.11686348170042038 Validation loss 0.11508609354496002 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5237],\n",
      "        [0.3473]])\n",
      "Iteration 26070 Training loss 0.11603383719921112 Validation loss 0.11509977281093597 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.4290],\n",
      "        [0.5541]])\n",
      "Iteration 26080 Training loss 0.11659432202577591 Validation loss 0.11510728299617767 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.2358],\n",
      "        [0.5600]])\n",
      "Iteration 26090 Training loss 0.1178378090262413 Validation loss 0.11507797986268997 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4495],\n",
      "        [0.3808]])\n",
      "Iteration 26100 Training loss 0.11596431583166122 Validation loss 0.11512131989002228 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.6150],\n",
      "        [0.4009]])\n",
      "Iteration 26110 Training loss 0.11606103181838989 Validation loss 0.11513816565275192 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4452],\n",
      "        [0.5064]])\n",
      "Iteration 26120 Training loss 0.11527226120233536 Validation loss 0.11518432945013046 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.3120],\n",
      "        [0.2512]])\n",
      "Iteration 26130 Training loss 0.11706305295228958 Validation loss 0.11512933671474457 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.6779],\n",
      "        [0.5552]])\n",
      "Iteration 26140 Training loss 0.1170366033911705 Validation loss 0.11509989202022552 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.6328],\n",
      "        [0.4429]])\n",
      "Iteration 26150 Training loss 0.1172865629196167 Validation loss 0.11509110778570175 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.3877],\n",
      "        [0.6955]])\n",
      "Iteration 26160 Training loss 0.11721737682819366 Validation loss 0.11507340520620346 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5733],\n",
      "        [0.4853]])\n",
      "Iteration 26170 Training loss 0.1154734268784523 Validation loss 0.11506379395723343 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5681],\n",
      "        [0.5945]])\n",
      "Iteration 26180 Training loss 0.11610142141580582 Validation loss 0.11507824808359146 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.5394],\n",
      "        [0.4500]])\n",
      "Iteration 26190 Training loss 0.11812087893486023 Validation loss 0.1150924414396286 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.3684],\n",
      "        [0.5906]])\n",
      "Iteration 26200 Training loss 0.11583320051431656 Validation loss 0.1150893121957779 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.3380],\n",
      "        [0.6071]])\n",
      "Iteration 26210 Training loss 0.11753605306148529 Validation loss 0.11505163460969925 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.5068],\n",
      "        [0.3200]])\n",
      "Iteration 26220 Training loss 0.11634291708469391 Validation loss 0.11506036669015884 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.3923],\n",
      "        [0.3726]])\n",
      "Iteration 26230 Training loss 0.11476591974496841 Validation loss 0.11507995426654816 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4649],\n",
      "        [0.5180]])\n",
      "Iteration 26240 Training loss 0.11803553998470306 Validation loss 0.11502550542354584 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.4771],\n",
      "        [0.5898]])\n",
      "Iteration 26250 Training loss 0.11680692434310913 Validation loss 0.1150331199169159 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.3048],\n",
      "        [0.3824]])\n",
      "Iteration 26260 Training loss 0.11703291535377502 Validation loss 0.1150461807847023 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.2015],\n",
      "        [0.1991]])\n",
      "Iteration 26270 Training loss 0.11733724921941757 Validation loss 0.11504077911376953 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4949],\n",
      "        [0.6654]])\n",
      "Iteration 26280 Training loss 0.11699914187192917 Validation loss 0.11500927805900574 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.5973],\n",
      "        [0.4185]])\n",
      "Iteration 26290 Training loss 0.11761458218097687 Validation loss 0.11500973254442215 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.3640],\n",
      "        [0.4644]])\n",
      "Iteration 26300 Training loss 0.11668813228607178 Validation loss 0.1150069609284401 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.4985],\n",
      "        [0.5305]])\n",
      "Iteration 26310 Training loss 0.11600734293460846 Validation loss 0.11523910611867905 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.2846],\n",
      "        [0.4794]])\n",
      "Iteration 26320 Training loss 0.11595025658607483 Validation loss 0.1150255873799324 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5241],\n",
      "        [0.5419]])\n",
      "Iteration 26330 Training loss 0.11657793819904327 Validation loss 0.11500459909439087 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.5754],\n",
      "        [0.5238]])\n",
      "Iteration 26340 Training loss 0.11645974963903427 Validation loss 0.11502795666456223 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.4122],\n",
      "        [0.5673]])\n",
      "Iteration 26350 Training loss 0.11666104942560196 Validation loss 0.11516346037387848 Accuracy 0.6166666746139526\n",
      "Output tensor([[0.5227],\n",
      "        [0.4836]])\n",
      "Iteration 26360 Training loss 0.11720383912324905 Validation loss 0.11509684473276138 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.3622],\n",
      "        [0.2474]])\n",
      "Iteration 26370 Training loss 0.11727404594421387 Validation loss 0.11507049202919006 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5463],\n",
      "        [0.5897]])\n",
      "Iteration 26380 Training loss 0.11654098331928253 Validation loss 0.1150745376944542 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.4086],\n",
      "        [0.3604]])\n",
      "Iteration 26390 Training loss 0.11581514775753021 Validation loss 0.11503732204437256 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.7824],\n",
      "        [0.4349]])\n",
      "Iteration 26400 Training loss 0.11712957918643951 Validation loss 0.11501605063676834 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5816],\n",
      "        [0.6449]])\n",
      "Iteration 26410 Training loss 0.11745216697454453 Validation loss 0.1151292622089386 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4740],\n",
      "        [0.6583]])\n",
      "Iteration 26420 Training loss 0.1169242113828659 Validation loss 0.11501350998878479 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.4652],\n",
      "        [0.4453]])\n",
      "Iteration 26430 Training loss 0.11805249005556107 Validation loss 0.11501509696245193 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.5014],\n",
      "        [0.3453]])\n",
      "Iteration 26440 Training loss 0.11678890138864517 Validation loss 0.1149904802441597 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.6345],\n",
      "        [0.5411]])\n",
      "Iteration 26450 Training loss 0.11718941479921341 Validation loss 0.11504258215427399 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.3867],\n",
      "        [0.4992]])\n",
      "Iteration 26460 Training loss 0.11568062007427216 Validation loss 0.11500030755996704 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.3730],\n",
      "        [0.5111]])\n",
      "Iteration 26470 Training loss 0.1157710999250412 Validation loss 0.11508981138467789 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.5503],\n",
      "        [0.4282]])\n",
      "Iteration 26480 Training loss 0.11603067815303802 Validation loss 0.11510288715362549 Accuracy 0.6171666383743286\n",
      "Output tensor([[0.3006],\n",
      "        [0.5044]])\n",
      "Iteration 26490 Training loss 0.11704955995082855 Validation loss 0.11501697450876236 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5293],\n",
      "        [0.5442]])\n",
      "Iteration 26500 Training loss 0.11576039344072342 Validation loss 0.11503199487924576 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.4030],\n",
      "        [0.1791]])\n",
      "Iteration 26510 Training loss 0.11495158076286316 Validation loss 0.1150393858551979 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.4037],\n",
      "        [0.6031]])\n",
      "Iteration 26520 Training loss 0.11755596101284027 Validation loss 0.11506430804729462 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.2514],\n",
      "        [0.6478]])\n",
      "Iteration 26530 Training loss 0.11798138171434402 Validation loss 0.1151207759976387 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.3957],\n",
      "        [0.2419]])\n",
      "Iteration 26540 Training loss 0.11715403199195862 Validation loss 0.1150989979505539 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.4932],\n",
      "        [0.4557]])\n",
      "Iteration 26550 Training loss 0.1159980446100235 Validation loss 0.11507238447666168 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5752],\n",
      "        [0.4855]])\n",
      "Iteration 26560 Training loss 0.11662160605192184 Validation loss 0.1150357648730278 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.6028],\n",
      "        [0.4835]])\n",
      "Iteration 26570 Training loss 0.1169082522392273 Validation loss 0.11503707617521286 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.6701],\n",
      "        [0.5721]])\n",
      "Iteration 26580 Training loss 0.11774232983589172 Validation loss 0.1150280013680458 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5685],\n",
      "        [0.3916]])\n",
      "Iteration 26590 Training loss 0.1175190657377243 Validation loss 0.11522131413221359 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.4640],\n",
      "        [0.6268]])\n",
      "Iteration 26600 Training loss 0.1164586991071701 Validation loss 0.11505161225795746 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.5552],\n",
      "        [0.5456]])\n",
      "Iteration 26610 Training loss 0.11760322749614716 Validation loss 0.11505956202745438 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.5317],\n",
      "        [0.4855]])\n",
      "Iteration 26620 Training loss 0.11649001389741898 Validation loss 0.11499521881341934 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.4235],\n",
      "        [0.8350]])\n",
      "Iteration 26630 Training loss 0.11733491718769073 Validation loss 0.11499753594398499 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.4543],\n",
      "        [0.3734]])\n",
      "Iteration 26640 Training loss 0.11656595766544342 Validation loss 0.1149834394454956 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.6571],\n",
      "        [0.5236]])\n",
      "Iteration 26650 Training loss 0.11733577400445938 Validation loss 0.11498124897480011 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.5155],\n",
      "        [0.5952]])\n",
      "Iteration 26660 Training loss 0.11688344180583954 Validation loss 0.11499212682247162 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5930],\n",
      "        [0.5518]])\n",
      "Iteration 26670 Training loss 0.11750848591327667 Validation loss 0.11498453468084335 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3556],\n",
      "        [0.5535]])\n",
      "Iteration 26680 Training loss 0.11695113778114319 Validation loss 0.11500373482704163 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.5625],\n",
      "        [0.5496]])\n",
      "Iteration 26690 Training loss 0.11512308567762375 Validation loss 0.11499716341495514 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.6858],\n",
      "        [0.5353]])\n",
      "Iteration 26700 Training loss 0.11729665100574493 Validation loss 0.11502382159233093 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.2999],\n",
      "        [0.1810]])\n",
      "Iteration 26710 Training loss 0.11640910804271698 Validation loss 0.11501507461071014 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5029],\n",
      "        [0.3869]])\n",
      "Iteration 26720 Training loss 0.1173352375626564 Validation loss 0.1150314062833786 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.7763],\n",
      "        [0.2503]])\n",
      "Iteration 26730 Training loss 0.11713036894798279 Validation loss 0.11501507461071014 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.4076],\n",
      "        [0.5483]])\n",
      "Iteration 26740 Training loss 0.11670512706041336 Validation loss 0.11509974300861359 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.4485],\n",
      "        [0.4648]])\n",
      "Iteration 26750 Training loss 0.11718182265758514 Validation loss 0.1150248646736145 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.6000],\n",
      "        [0.5154]])\n",
      "Iteration 26760 Training loss 0.11713077872991562 Validation loss 0.11503933370113373 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.5860],\n",
      "        [0.6240]])\n",
      "Iteration 26770 Training loss 0.11598337441682816 Validation loss 0.11501506716012955 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4344],\n",
      "        [0.2786]])\n",
      "Iteration 26780 Training loss 0.1147679015994072 Validation loss 0.11500546336174011 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.7408],\n",
      "        [0.5122]])\n",
      "Iteration 26790 Training loss 0.11778765916824341 Validation loss 0.11501996964216232 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5429],\n",
      "        [0.5822]])\n",
      "Iteration 26800 Training loss 0.1156703308224678 Validation loss 0.11503055691719055 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.2362],\n",
      "        [0.6797]])\n",
      "Iteration 26810 Training loss 0.11620676517486572 Validation loss 0.11502663046121597 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4378],\n",
      "        [0.5929]])\n",
      "Iteration 26820 Training loss 0.11579351872205734 Validation loss 0.11510700732469559 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5913],\n",
      "        [0.5708]])\n",
      "Iteration 26830 Training loss 0.11620810627937317 Validation loss 0.11504408717155457 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.6349],\n",
      "        [0.5392]])\n",
      "Iteration 26840 Training loss 0.11726696789264679 Validation loss 0.11514115333557129 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5783],\n",
      "        [0.7273]])\n",
      "Iteration 26850 Training loss 0.11534526199102402 Validation loss 0.11501584202051163 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5012],\n",
      "        [0.4437]])\n",
      "Iteration 26860 Training loss 0.1168304830789566 Validation loss 0.11503385752439499 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.6256],\n",
      "        [0.2932]])\n",
      "Iteration 26870 Training loss 0.11479824781417847 Validation loss 0.115031898021698 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4295],\n",
      "        [0.4614]])\n",
      "Iteration 26880 Training loss 0.11738263815641403 Validation loss 0.11505993455648422 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.4264],\n",
      "        [0.6049]])\n",
      "Iteration 26890 Training loss 0.11550533026456833 Validation loss 0.115038201212883 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4741],\n",
      "        [0.5470]])\n",
      "Iteration 26900 Training loss 0.11606689542531967 Validation loss 0.11498315632343292 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.6502],\n",
      "        [0.3412]])\n",
      "Iteration 26910 Training loss 0.11712577193975449 Validation loss 0.11501035839319229 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5744],\n",
      "        [0.4925]])\n",
      "Iteration 26920 Training loss 0.11757248640060425 Validation loss 0.11500515043735504 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5121],\n",
      "        [0.5423]])\n",
      "Iteration 26930 Training loss 0.11665726453065872 Validation loss 0.11500045657157898 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.7600],\n",
      "        [0.5500]])\n",
      "Iteration 26940 Training loss 0.11694668233394623 Validation loss 0.1149791032075882 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.3122],\n",
      "        [0.5896]])\n",
      "Iteration 26950 Training loss 0.11640278995037079 Validation loss 0.11501133441925049 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5811],\n",
      "        [0.7032]])\n",
      "Iteration 26960 Training loss 0.11593855917453766 Validation loss 0.1149614229798317 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.6254],\n",
      "        [0.4505]])\n",
      "Iteration 26970 Training loss 0.11736288666725159 Validation loss 0.1150023564696312 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.2861],\n",
      "        [0.3100]])\n",
      "Iteration 26980 Training loss 0.11709592491388321 Validation loss 0.11508625000715256 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.4532],\n",
      "        [0.5832]])\n",
      "Iteration 26990 Training loss 0.1167600005865097 Validation loss 0.11497338116168976 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.4682],\n",
      "        [0.5069]])\n",
      "Iteration 27000 Training loss 0.1163615733385086 Validation loss 0.11498589813709259 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.3777],\n",
      "        [0.3276]])\n",
      "Iteration 27010 Training loss 0.11519184708595276 Validation loss 0.11499306559562683 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.4012],\n",
      "        [0.6861]])\n",
      "Iteration 27020 Training loss 0.11571107059717178 Validation loss 0.11499816924333572 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.6080],\n",
      "        [0.2618]])\n",
      "Iteration 27030 Training loss 0.11677203327417374 Validation loss 0.11499343067407608 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4948],\n",
      "        [0.5485]])\n",
      "Iteration 27040 Training loss 0.11658158153295517 Validation loss 0.11498867720365524 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.4148],\n",
      "        [0.5991]])\n",
      "Iteration 27050 Training loss 0.1168379932641983 Validation loss 0.11498669534921646 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.2640],\n",
      "        [0.4601]])\n",
      "Iteration 27060 Training loss 0.11699868738651276 Validation loss 0.11496838182210922 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.5766],\n",
      "        [0.4847]])\n",
      "Iteration 27070 Training loss 0.11594037711620331 Validation loss 0.11503171920776367 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.3501],\n",
      "        [0.5925]])\n",
      "Iteration 27080 Training loss 0.11759807169437408 Validation loss 0.11500561237335205 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.5489],\n",
      "        [0.7204]])\n",
      "Iteration 27090 Training loss 0.11563058197498322 Validation loss 0.11499033868312836 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.2853],\n",
      "        [0.5273]])\n",
      "Iteration 27100 Training loss 0.11671756953001022 Validation loss 0.11501095443964005 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.4080],\n",
      "        [0.7766]])\n",
      "Iteration 27110 Training loss 0.1176312044262886 Validation loss 0.11500421911478043 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.6596],\n",
      "        [0.3581]])\n",
      "Iteration 27120 Training loss 0.11626175045967102 Validation loss 0.11511875689029694 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5756],\n",
      "        [0.6389]])\n",
      "Iteration 27130 Training loss 0.11776357144117355 Validation loss 0.11500139534473419 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.5805],\n",
      "        [0.2991]])\n",
      "Iteration 27140 Training loss 0.11687137186527252 Validation loss 0.11499825119972229 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.4726],\n",
      "        [0.5133]])\n",
      "Iteration 27150 Training loss 0.11778625845909119 Validation loss 0.11503013223409653 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5206],\n",
      "        [0.4617]])\n",
      "Iteration 27160 Training loss 0.11741388589143753 Validation loss 0.11500455439090729 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3219],\n",
      "        [0.6219]])\n",
      "Iteration 27170 Training loss 0.11689290404319763 Validation loss 0.11501343548297882 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.4859],\n",
      "        [0.2654]])\n",
      "Iteration 27180 Training loss 0.11581695824861526 Validation loss 0.11500317603349686 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4163],\n",
      "        [0.3886]])\n",
      "Iteration 27190 Training loss 0.11688382178544998 Validation loss 0.11501854658126831 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.6227],\n",
      "        [0.6514]])\n",
      "Iteration 27200 Training loss 0.11554831266403198 Validation loss 0.11504675447940826 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.3845],\n",
      "        [0.7538]])\n",
      "Iteration 27210 Training loss 0.11702171713113785 Validation loss 0.11501011252403259 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.5805],\n",
      "        [0.5836]])\n",
      "Iteration 27220 Training loss 0.11585165560245514 Validation loss 0.11507504433393478 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.4765],\n",
      "        [0.5596]])\n",
      "Iteration 27230 Training loss 0.11585481464862823 Validation loss 0.11497048288583755 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.4832],\n",
      "        [0.5157]])\n",
      "Iteration 27240 Training loss 0.11624741554260254 Validation loss 0.11514347046613693 Accuracy 0.6156666874885559\n",
      "Output tensor([[0.5567],\n",
      "        [0.4353]])\n",
      "Iteration 27250 Training loss 0.11699309945106506 Validation loss 0.11500805616378784 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.6023],\n",
      "        [0.6073]])\n",
      "Iteration 27260 Training loss 0.11701279878616333 Validation loss 0.11498390883207321 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.2938],\n",
      "        [0.3313]])\n",
      "Iteration 27270 Training loss 0.11596576124429703 Validation loss 0.11502046883106232 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.4651],\n",
      "        [0.6999]])\n",
      "Iteration 27280 Training loss 0.11647502332925797 Validation loss 0.11502101272344589 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.7232],\n",
      "        [0.3809]])\n",
      "Iteration 27290 Training loss 0.11798469722270966 Validation loss 0.11499647051095963 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5515],\n",
      "        [0.4968]])\n",
      "Iteration 27300 Training loss 0.11780926585197449 Validation loss 0.11497741937637329 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.6015],\n",
      "        [0.6183]])\n",
      "Iteration 27310 Training loss 0.11600681394338608 Validation loss 0.11497791856527328 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.5262],\n",
      "        [0.4604]])\n",
      "Iteration 27320 Training loss 0.11603251099586487 Validation loss 0.11505210399627686 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.6237],\n",
      "        [0.2912]])\n",
      "Iteration 27330 Training loss 0.11509153246879578 Validation loss 0.11499142646789551 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3716],\n",
      "        [0.5471]])\n",
      "Iteration 27340 Training loss 0.1159825548529625 Validation loss 0.11501340568065643 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.3826],\n",
      "        [0.5502]])\n",
      "Iteration 27350 Training loss 0.1157454401254654 Validation loss 0.11501745879650116 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4082],\n",
      "        [0.4161]])\n",
      "Iteration 27360 Training loss 0.11524751782417297 Validation loss 0.11508570611476898 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.3717],\n",
      "        [0.2530]])\n",
      "Iteration 27370 Training loss 0.11796503514051437 Validation loss 0.11502619087696075 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.3561],\n",
      "        [0.5205]])\n",
      "Iteration 27380 Training loss 0.11568141728639603 Validation loss 0.11503440141677856 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.7656],\n",
      "        [0.4337]])\n",
      "Iteration 27390 Training loss 0.1168433353304863 Validation loss 0.11501844972372055 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.2915],\n",
      "        [0.5624]])\n",
      "Iteration 27400 Training loss 0.11608689278364182 Validation loss 0.11503156274557114 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5761],\n",
      "        [0.6313]])\n",
      "Iteration 27410 Training loss 0.11695336550474167 Validation loss 0.11503922194242477 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.4677],\n",
      "        [0.5422]])\n",
      "Iteration 27420 Training loss 0.11691904813051224 Validation loss 0.11496266722679138 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4626],\n",
      "        [0.4419]])\n",
      "Iteration 27430 Training loss 0.11744297295808792 Validation loss 0.1149711012840271 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.3992],\n",
      "        [0.3124]])\n",
      "Iteration 27440 Training loss 0.11712506413459778 Validation loss 0.11500817537307739 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.6573],\n",
      "        [0.6028]])\n",
      "Iteration 27450 Training loss 0.11672107875347137 Validation loss 0.11498140543699265 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.5281],\n",
      "        [0.6041]])\n",
      "Iteration 27460 Training loss 0.1181669533252716 Validation loss 0.11501088738441467 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5113],\n",
      "        [0.4906]])\n",
      "Iteration 27470 Training loss 0.11677531152963638 Validation loss 0.1149972528219223 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.4539],\n",
      "        [0.5920]])\n",
      "Iteration 27480 Training loss 0.1161232590675354 Validation loss 0.11496458202600479 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.3591],\n",
      "        [0.6963]])\n",
      "Iteration 27490 Training loss 0.11684134602546692 Validation loss 0.11506833136081696 Accuracy 0.6178333163261414\n",
      "Output tensor([[0.3266],\n",
      "        [0.3340]])\n",
      "Iteration 27500 Training loss 0.11771630495786667 Validation loss 0.11497455090284348 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.4355],\n",
      "        [0.3259]])\n",
      "Iteration 27510 Training loss 0.11669144779443741 Validation loss 0.11501436680555344 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.2432],\n",
      "        [0.6333]])\n",
      "Iteration 27520 Training loss 0.11694957315921783 Validation loss 0.11502158641815186 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.6966],\n",
      "        [0.4416]])\n",
      "Iteration 27530 Training loss 0.11753112822771072 Validation loss 0.11497383564710617 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.4043],\n",
      "        [0.4988]])\n",
      "Iteration 27540 Training loss 0.11579124629497528 Validation loss 0.11496730893850327 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.5845],\n",
      "        [0.3754]])\n",
      "Iteration 27550 Training loss 0.11789245903491974 Validation loss 0.11499091237783432 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.4016],\n",
      "        [0.4764]])\n",
      "Iteration 27560 Training loss 0.11662913858890533 Validation loss 0.11508532613515854 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.5574],\n",
      "        [0.5720]])\n",
      "Iteration 27570 Training loss 0.11617889255285263 Validation loss 0.1149611845612526 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5331],\n",
      "        [0.4921]])\n",
      "Iteration 27580 Training loss 0.1171003133058548 Validation loss 0.11496149003505707 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.5477],\n",
      "        [0.3360]])\n",
      "Iteration 27590 Training loss 0.1151791661977768 Validation loss 0.11525598168373108 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.3164],\n",
      "        [0.5157]])\n",
      "Iteration 27600 Training loss 0.11605074256658554 Validation loss 0.11497882753610611 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4669],\n",
      "        [0.3166]])\n",
      "Iteration 27610 Training loss 0.11656729876995087 Validation loss 0.11509490758180618 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5386],\n",
      "        [0.3151]])\n",
      "Iteration 27620 Training loss 0.11644887179136276 Validation loss 0.11498583853244781 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.6253],\n",
      "        [0.4808]])\n",
      "Iteration 27630 Training loss 0.11731728911399841 Validation loss 0.11496461927890778 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.6057],\n",
      "        [0.5887]])\n",
      "Iteration 27640 Training loss 0.11701304465532303 Validation loss 0.1149691492319107 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.4506],\n",
      "        [0.5632]])\n",
      "Iteration 27650 Training loss 0.116450235247612 Validation loss 0.11497900635004044 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.6211],\n",
      "        [0.4210]])\n",
      "Iteration 27660 Training loss 0.1163005605340004 Validation loss 0.11514384299516678 Accuracy 0.6161666512489319\n",
      "Output tensor([[0.5732],\n",
      "        [0.4850]])\n",
      "Iteration 27670 Training loss 0.11677630245685577 Validation loss 0.11496590822935104 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3768],\n",
      "        [0.5579]])\n",
      "Iteration 27680 Training loss 0.11658982187509537 Validation loss 0.1150098666548729 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5003],\n",
      "        [0.4567]])\n",
      "Iteration 27690 Training loss 0.11736057698726654 Validation loss 0.11497174203395844 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.3175],\n",
      "        [0.5086]])\n",
      "Iteration 27700 Training loss 0.11804697662591934 Validation loss 0.11492712050676346 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4725],\n",
      "        [0.5591]])\n",
      "Iteration 27710 Training loss 0.11683935672044754 Validation loss 0.1149769276380539 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.5736],\n",
      "        [0.7819]])\n",
      "Iteration 27720 Training loss 0.11564422398805618 Validation loss 0.11498397588729858 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.7047],\n",
      "        [0.5769]])\n",
      "Iteration 27730 Training loss 0.11794262379407883 Validation loss 0.11495281010866165 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.5419],\n",
      "        [0.2476]])\n",
      "Iteration 27740 Training loss 0.11667134612798691 Validation loss 0.11515683680772781 Accuracy 0.6158333420753479\n",
      "Output tensor([[0.4286],\n",
      "        [0.4886]])\n",
      "Iteration 27750 Training loss 0.11681797355413437 Validation loss 0.11502762138843536 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.2946],\n",
      "        [0.4017]])\n",
      "Iteration 27760 Training loss 0.11657405644655228 Validation loss 0.11501682549715042 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5630],\n",
      "        [0.4562]])\n",
      "Iteration 27770 Training loss 0.11533486098051071 Validation loss 0.11498773097991943 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.5691],\n",
      "        [0.5963]])\n",
      "Iteration 27780 Training loss 0.11728572845458984 Validation loss 0.11492889374494553 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.4616],\n",
      "        [0.4653]])\n",
      "Iteration 27790 Training loss 0.11646614223718643 Validation loss 0.11494706571102142 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5324],\n",
      "        [0.4313]])\n",
      "Iteration 27800 Training loss 0.11645469069480896 Validation loss 0.11495763808488846 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3406],\n",
      "        [0.3303]])\n",
      "Iteration 27810 Training loss 0.11793693900108337 Validation loss 0.1149909570813179 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.6417],\n",
      "        [0.3221]])\n",
      "Iteration 27820 Training loss 0.11667106300592422 Validation loss 0.11496498435735703 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.6395],\n",
      "        [0.2971]])\n",
      "Iteration 27830 Training loss 0.11604901403188705 Validation loss 0.11507395654916763 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4620],\n",
      "        [0.5135]])\n",
      "Iteration 27840 Training loss 0.11703728884458542 Validation loss 0.11496057361364365 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.2368],\n",
      "        [0.5314]])\n",
      "Iteration 27850 Training loss 0.11761777102947235 Validation loss 0.11496596038341522 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5124],\n",
      "        [0.5641]])\n",
      "Iteration 27860 Training loss 0.11619812995195389 Validation loss 0.1149832084774971 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.6266],\n",
      "        [0.7381]])\n",
      "Iteration 27870 Training loss 0.11705003678798676 Validation loss 0.11499987542629242 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.4213],\n",
      "        [0.4735]])\n",
      "Iteration 27880 Training loss 0.11683373898267746 Validation loss 0.11496177315711975 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.3536],\n",
      "        [0.5644]])\n",
      "Iteration 27890 Training loss 0.11557704955339432 Validation loss 0.11498642712831497 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5589],\n",
      "        [0.3373]])\n",
      "Iteration 27900 Training loss 0.11733352392911911 Validation loss 0.11495967954397202 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.3936],\n",
      "        [0.2803]])\n",
      "Iteration 27910 Training loss 0.11480290442705154 Validation loss 0.11491396278142929 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.3848],\n",
      "        [0.6839]])\n",
      "Iteration 27920 Training loss 0.11555784195661545 Validation loss 0.1149333268404007 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.3213],\n",
      "        [0.5207]])\n",
      "Iteration 27930 Training loss 0.11809467524290085 Validation loss 0.11494170874357224 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5605],\n",
      "        [0.4493]])\n",
      "Iteration 27940 Training loss 0.11668052524328232 Validation loss 0.11492513120174408 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.5065],\n",
      "        [0.5136]])\n",
      "Iteration 27950 Training loss 0.11684653908014297 Validation loss 0.1149493083357811 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.7202],\n",
      "        [0.6084]])\n",
      "Iteration 27960 Training loss 0.1159353107213974 Validation loss 0.11491132527589798 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.4960],\n",
      "        [0.4093]])\n",
      "Iteration 27970 Training loss 0.11610054224729538 Validation loss 0.11491148173809052 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.5600],\n",
      "        [0.2749]])\n",
      "Iteration 27980 Training loss 0.11590760946273804 Validation loss 0.11499304324388504 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.5397],\n",
      "        [0.5762]])\n",
      "Iteration 27990 Training loss 0.11619769036769867 Validation loss 0.11495965719223022 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.6062],\n",
      "        [0.3760]])\n",
      "Iteration 28000 Training loss 0.11706768721342087 Validation loss 0.11495892703533173 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.7679],\n",
      "        [0.2553]])\n",
      "Iteration 28010 Training loss 0.11577632278203964 Validation loss 0.11508280783891678 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.5289],\n",
      "        [0.5487]])\n",
      "Iteration 28020 Training loss 0.11650584638118744 Validation loss 0.11495621502399445 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.6036],\n",
      "        [0.2159]])\n",
      "Iteration 28030 Training loss 0.1143297404050827 Validation loss 0.11493845283985138 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.4199],\n",
      "        [0.2626]])\n",
      "Iteration 28040 Training loss 0.11729319393634796 Validation loss 0.114957295358181 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4898],\n",
      "        [0.5286]])\n",
      "Iteration 28050 Training loss 0.116311214864254 Validation loss 0.11504504084587097 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.7346],\n",
      "        [0.4329]])\n",
      "Iteration 28060 Training loss 0.11571571975946426 Validation loss 0.11494565010070801 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.8126],\n",
      "        [0.6064]])\n",
      "Iteration 28070 Training loss 0.1163415014743805 Validation loss 0.11494528502225876 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4947],\n",
      "        [0.4481]])\n",
      "Iteration 28080 Training loss 0.11537665128707886 Validation loss 0.11496423184871674 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4351],\n",
      "        [0.5383]])\n",
      "Iteration 28090 Training loss 0.11752775311470032 Validation loss 0.11502501368522644 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.3799],\n",
      "        [0.4871]])\n",
      "Iteration 28100 Training loss 0.11661582440137863 Validation loss 0.11498767137527466 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5736],\n",
      "        [0.4295]])\n",
      "Iteration 28110 Training loss 0.11652953922748566 Validation loss 0.11499307304620743 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.2580],\n",
      "        [0.3587]])\n",
      "Iteration 28120 Training loss 0.11587022244930267 Validation loss 0.11497202515602112 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5785],\n",
      "        [0.6244]])\n",
      "Iteration 28130 Training loss 0.11648567765951157 Validation loss 0.11496265977621078 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.7379],\n",
      "        [0.6136]])\n",
      "Iteration 28140 Training loss 0.1150541678071022 Validation loss 0.1149352639913559 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.2418],\n",
      "        [0.6165]])\n",
      "Iteration 28150 Training loss 0.11698398739099503 Validation loss 0.11495841294527054 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.6203],\n",
      "        [0.4564]])\n",
      "Iteration 28160 Training loss 0.11548923701047897 Validation loss 0.11491741240024567 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.4475],\n",
      "        [0.6391]])\n",
      "Iteration 28170 Training loss 0.11575300991535187 Validation loss 0.11490176618099213 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5679],\n",
      "        [0.5045]])\n",
      "Iteration 28180 Training loss 0.11580148339271545 Validation loss 0.11497176438570023 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.1776],\n",
      "        [0.5435]])\n",
      "Iteration 28190 Training loss 0.11629996448755264 Validation loss 0.11490689218044281 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.6138],\n",
      "        [0.4667]])\n",
      "Iteration 28200 Training loss 0.11787781864404678 Validation loss 0.11492409557104111 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5463],\n",
      "        [0.7227]])\n",
      "Iteration 28210 Training loss 0.11783652752637863 Validation loss 0.11496797949075699 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5246],\n",
      "        [0.4994]])\n",
      "Iteration 28220 Training loss 0.11672332882881165 Validation loss 0.11497229337692261 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.3431],\n",
      "        [0.4592]])\n",
      "Iteration 28230 Training loss 0.11659757792949677 Validation loss 0.11490931361913681 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.3088],\n",
      "        [0.5049]])\n",
      "Iteration 28240 Training loss 0.11616259813308716 Validation loss 0.11492418497800827 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.6619],\n",
      "        [0.4003]])\n",
      "Iteration 28250 Training loss 0.11724023520946503 Validation loss 0.11490803956985474 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5590],\n",
      "        [0.3439]])\n",
      "Iteration 28260 Training loss 0.11641823500394821 Validation loss 0.11498638987541199 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.6732],\n",
      "        [0.5026]])\n",
      "Iteration 28270 Training loss 0.11551424115896225 Validation loss 0.11491027474403381 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4640],\n",
      "        [0.5999]])\n",
      "Iteration 28280 Training loss 0.11748228222131729 Validation loss 0.11491788923740387 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.5129],\n",
      "        [0.3985]])\n",
      "Iteration 28290 Training loss 0.11739527434110641 Validation loss 0.11497823148965836 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.4265],\n",
      "        [0.2156]])\n",
      "Iteration 28300 Training loss 0.11670977622270584 Validation loss 0.11495994031429291 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.3568],\n",
      "        [0.6528]])\n",
      "Iteration 28310 Training loss 0.11660302430391312 Validation loss 0.11496466398239136 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.5252],\n",
      "        [0.4019]])\n",
      "Iteration 28320 Training loss 0.11698707193136215 Validation loss 0.11497846245765686 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.6924],\n",
      "        [0.4668]])\n",
      "Iteration 28330 Training loss 0.11635583639144897 Validation loss 0.11492433398962021 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.2635],\n",
      "        [0.5932]])\n",
      "Iteration 28340 Training loss 0.11726470291614532 Validation loss 0.11494672298431396 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.5988],\n",
      "        [0.4216]])\n",
      "Iteration 28350 Training loss 0.11625482141971588 Validation loss 0.1149972602725029 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.4941],\n",
      "        [0.5413]])\n",
      "Iteration 28360 Training loss 0.11713166534900665 Validation loss 0.11493003368377686 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.5428],\n",
      "        [0.6258]])\n",
      "Iteration 28370 Training loss 0.11507674306631088 Validation loss 0.11488022655248642 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.6387],\n",
      "        [0.1090]])\n",
      "Iteration 28380 Training loss 0.11635420471429825 Validation loss 0.1148965135216713 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.4213],\n",
      "        [0.4692]])\n",
      "Iteration 28390 Training loss 0.11698003858327866 Validation loss 0.11491402238607407 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.5945],\n",
      "        [0.2465]])\n",
      "Iteration 28400 Training loss 0.11555744707584381 Validation loss 0.11493409425020218 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4904],\n",
      "        [0.6497]])\n",
      "Iteration 28410 Training loss 0.1155954971909523 Validation loss 0.11487764120101929 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.4968],\n",
      "        [0.5398]])\n",
      "Iteration 28420 Training loss 0.11820941418409348 Validation loss 0.11502451449632645 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.5163],\n",
      "        [0.4804]])\n",
      "Iteration 28430 Training loss 0.11756900697946548 Validation loss 0.11492207646369934 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.6226],\n",
      "        [0.3084]])\n",
      "Iteration 28440 Training loss 0.11482824385166168 Validation loss 0.11488974839448929 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4829],\n",
      "        [0.4256]])\n",
      "Iteration 28450 Training loss 0.11675899475812912 Validation loss 0.11494072526693344 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.1997],\n",
      "        [0.2429]])\n",
      "Iteration 28460 Training loss 0.11596214026212692 Validation loss 0.11494120210409164 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.5283],\n",
      "        [0.4563]])\n",
      "Iteration 28470 Training loss 0.11610877513885498 Validation loss 0.1149246022105217 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.2607],\n",
      "        [0.5314]])\n",
      "Iteration 28480 Training loss 0.1174909770488739 Validation loss 0.11504344642162323 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.4923],\n",
      "        [0.5930]])\n",
      "Iteration 28490 Training loss 0.11599534749984741 Validation loss 0.11492712050676346 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.3475],\n",
      "        [0.6481]])\n",
      "Iteration 28500 Training loss 0.11771506071090698 Validation loss 0.11493184417486191 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.5914],\n",
      "        [0.3433]])\n",
      "Iteration 28510 Training loss 0.1176607757806778 Validation loss 0.11494170874357224 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3641],\n",
      "        [0.3013]])\n",
      "Iteration 28520 Training loss 0.11601860821247101 Validation loss 0.11497076600790024 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.4268],\n",
      "        [0.4090]])\n",
      "Iteration 28530 Training loss 0.11660211533308029 Validation loss 0.11495517939329147 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.6509],\n",
      "        [0.3633]])\n",
      "Iteration 28540 Training loss 0.11569221317768097 Validation loss 0.11491900682449341 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.2659],\n",
      "        [0.4202]])\n",
      "Iteration 28550 Training loss 0.11720038950443268 Validation loss 0.11489032953977585 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.6980],\n",
      "        [0.4792]])\n",
      "Iteration 28560 Training loss 0.11631287634372711 Validation loss 0.11491593718528748 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.6213],\n",
      "        [0.4774]])\n",
      "Iteration 28570 Training loss 0.11680639535188675 Validation loss 0.11489208787679672 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4414],\n",
      "        [0.3976]])\n",
      "Iteration 28580 Training loss 0.11677902936935425 Validation loss 0.11501682549715042 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.5608],\n",
      "        [0.6656]])\n",
      "Iteration 28590 Training loss 0.11625809967517853 Validation loss 0.11491377651691437 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.4694],\n",
      "        [0.7032]])\n",
      "Iteration 28600 Training loss 0.11627358943223953 Validation loss 0.11491207033395767 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.3630],\n",
      "        [0.3277]])\n",
      "Iteration 28610 Training loss 0.11524439603090286 Validation loss 0.11493329703807831 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.6990],\n",
      "        [0.5075]])\n",
      "Iteration 28620 Training loss 0.11638599634170532 Validation loss 0.11491119116544724 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.5305],\n",
      "        [0.6785]])\n",
      "Iteration 28630 Training loss 0.11634945869445801 Validation loss 0.1149015724658966 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.6001],\n",
      "        [0.3121]])\n",
      "Iteration 28640 Training loss 0.11617694795131683 Validation loss 0.11496341228485107 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.6872],\n",
      "        [0.4544]])\n",
      "Iteration 28650 Training loss 0.11488689482212067 Validation loss 0.11496669799089432 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.3377],\n",
      "        [0.6050]])\n",
      "Iteration 28660 Training loss 0.11733437329530716 Validation loss 0.11493679136037827 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.5357],\n",
      "        [0.6325]])\n",
      "Iteration 28670 Training loss 0.1172909140586853 Validation loss 0.11494510620832443 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.3557],\n",
      "        [0.5957]])\n",
      "Iteration 28680 Training loss 0.11566837877035141 Validation loss 0.11492598801851273 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.6005],\n",
      "        [0.4138]])\n",
      "Iteration 28690 Training loss 0.11645151674747467 Validation loss 0.11492026597261429 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.5484],\n",
      "        [0.4166]])\n",
      "Iteration 28700 Training loss 0.11577842384576797 Validation loss 0.11490776389837265 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5905],\n",
      "        [0.4943]])\n",
      "Iteration 28710 Training loss 0.11583064496517181 Validation loss 0.11486919969320297 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.4636],\n",
      "        [0.3633]])\n",
      "Iteration 28720 Training loss 0.11606720089912415 Validation loss 0.11489934474229813 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3781],\n",
      "        [0.7407]])\n",
      "Iteration 28730 Training loss 0.11671725660562515 Validation loss 0.11486579477787018 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.5060],\n",
      "        [0.4318]])\n",
      "Iteration 28740 Training loss 0.11678686738014221 Validation loss 0.11484653502702713 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4153],\n",
      "        [0.5692]])\n",
      "Iteration 28750 Training loss 0.11612769216299057 Validation loss 0.11486605554819107 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.5330],\n",
      "        [0.4527]])\n",
      "Iteration 28760 Training loss 0.11688151210546494 Validation loss 0.11523808538913727 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5745],\n",
      "        [0.4371]])\n",
      "Iteration 28770 Training loss 0.11484071612358093 Validation loss 0.11486166715621948 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.3269],\n",
      "        [0.3316]])\n",
      "Iteration 28780 Training loss 0.11509014666080475 Validation loss 0.11486924439668655 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.6752],\n",
      "        [0.5468]])\n",
      "Iteration 28790 Training loss 0.1173274964094162 Validation loss 0.11487333476543427 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.3604],\n",
      "        [0.4722]])\n",
      "Iteration 28800 Training loss 0.11649027466773987 Validation loss 0.11486832797527313 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.6149],\n",
      "        [0.6757]])\n",
      "Iteration 28810 Training loss 0.11651858687400818 Validation loss 0.11489197611808777 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.4892],\n",
      "        [0.5922]])\n",
      "Iteration 28820 Training loss 0.11664809286594391 Validation loss 0.11499511450529099 Accuracy 0.6175000071525574\n",
      "Output tensor([[0.3685],\n",
      "        [0.4467]])\n",
      "Iteration 28830 Training loss 0.11626297235488892 Validation loss 0.1148616299033165 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.6121],\n",
      "        [0.3981]])\n",
      "Iteration 28840 Training loss 0.1165410652756691 Validation loss 0.11502078175544739 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.5359],\n",
      "        [0.5472]])\n",
      "Iteration 28850 Training loss 0.11623895168304443 Validation loss 0.11486493051052094 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.4543],\n",
      "        [0.4044]])\n",
      "Iteration 28860 Training loss 0.1168820932507515 Validation loss 0.11487408727407455 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.2845],\n",
      "        [0.5724]])\n",
      "Iteration 28870 Training loss 0.11638885736465454 Validation loss 0.11486031115055084 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.2417],\n",
      "        [0.4407]])\n",
      "Iteration 28880 Training loss 0.11674033850431442 Validation loss 0.1150284856557846 Accuracy 0.6169999837875366\n",
      "Output tensor([[0.5789],\n",
      "        [0.4524]])\n",
      "Iteration 28890 Training loss 0.11591748148202896 Validation loss 0.11490622907876968 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.3856],\n",
      "        [0.6050]])\n",
      "Iteration 28900 Training loss 0.1158028393983841 Validation loss 0.11486699432134628 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.3587],\n",
      "        [0.4243]])\n",
      "Iteration 28910 Training loss 0.1148204430937767 Validation loss 0.11490694433450699 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.3188],\n",
      "        [0.1689]])\n",
      "Iteration 28920 Training loss 0.11653292179107666 Validation loss 0.11487264186143875 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.4597],\n",
      "        [0.4825]])\n",
      "Iteration 28930 Training loss 0.11554064601659775 Validation loss 0.11489111185073853 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4501],\n",
      "        [0.3156]])\n",
      "Iteration 28940 Training loss 0.11594744026660919 Validation loss 0.1148669421672821 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4620],\n",
      "        [0.5859]])\n",
      "Iteration 28950 Training loss 0.1165611743927002 Validation loss 0.11485321074724197 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.1767],\n",
      "        [0.5709]])\n",
      "Iteration 28960 Training loss 0.1173931136727333 Validation loss 0.11484692245721817 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.4160],\n",
      "        [0.3423]])\n",
      "Iteration 28970 Training loss 0.11590501666069031 Validation loss 0.11493387818336487 Accuracy 0.6176666617393494\n",
      "Output tensor([[0.7090],\n",
      "        [0.6252]])\n",
      "Iteration 28980 Training loss 0.11739566922187805 Validation loss 0.11483630537986755 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.5391],\n",
      "        [0.5359]])\n",
      "Iteration 28990 Training loss 0.11806565523147583 Validation loss 0.11488466709852219 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.4964],\n",
      "        [0.4441]])\n",
      "Iteration 29000 Training loss 0.11771899461746216 Validation loss 0.11483167856931686 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.4729],\n",
      "        [0.5936]])\n",
      "Iteration 29010 Training loss 0.11604579538106918 Validation loss 0.11482422798871994 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3113],\n",
      "        [0.3542]])\n",
      "Iteration 29020 Training loss 0.11554675549268723 Validation loss 0.11484674364328384 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.5881],\n",
      "        [0.5475]])\n",
      "Iteration 29030 Training loss 0.11443807184696198 Validation loss 0.11492343991994858 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.2862],\n",
      "        [0.3616]])\n",
      "Iteration 29040 Training loss 0.11593805253505707 Validation loss 0.11483696848154068 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.3631],\n",
      "        [0.2975]])\n",
      "Iteration 29050 Training loss 0.11553283780813217 Validation loss 0.11485344171524048 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4563],\n",
      "        [0.6936]])\n",
      "Iteration 29060 Training loss 0.11631741374731064 Validation loss 0.11494471877813339 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.4711],\n",
      "        [0.4856]])\n",
      "Iteration 29070 Training loss 0.11620572954416275 Validation loss 0.11486053466796875 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5479],\n",
      "        [0.1476]])\n",
      "Iteration 29080 Training loss 0.117316834628582 Validation loss 0.1149681806564331 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.2991],\n",
      "        [0.4730]])\n",
      "Iteration 29090 Training loss 0.11575715988874435 Validation loss 0.1148635745048523 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.3625],\n",
      "        [0.6321]])\n",
      "Iteration 29100 Training loss 0.11692924052476883 Validation loss 0.11487050354480743 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.2243],\n",
      "        [0.5415]])\n",
      "Iteration 29110 Training loss 0.11639917641878128 Validation loss 0.11486002802848816 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.3830],\n",
      "        [0.5330]])\n",
      "Iteration 29120 Training loss 0.11641039699316025 Validation loss 0.11487354338169098 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.8091],\n",
      "        [0.3704]])\n",
      "Iteration 29130 Training loss 0.1165749803185463 Validation loss 0.11485977470874786 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.5475],\n",
      "        [0.4261]])\n",
      "Iteration 29140 Training loss 0.11620548367500305 Validation loss 0.11487031728029251 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.6209],\n",
      "        [0.5462]])\n",
      "Iteration 29150 Training loss 0.11630237102508545 Validation loss 0.11493568122386932 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5838],\n",
      "        [0.4105]])\n",
      "Iteration 29160 Training loss 0.11503399908542633 Validation loss 0.1148635745048523 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.3053],\n",
      "        [0.6208]])\n",
      "Iteration 29170 Training loss 0.11719994246959686 Validation loss 0.11487068980932236 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.4081],\n",
      "        [0.6040]])\n",
      "Iteration 29180 Training loss 0.11670934408903122 Validation loss 0.11494244635105133 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5581],\n",
      "        [0.6099]])\n",
      "Iteration 29190 Training loss 0.1155306026339531 Validation loss 0.11485789716243744 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.6183],\n",
      "        [0.6184]])\n",
      "Iteration 29200 Training loss 0.11642611771821976 Validation loss 0.11487879604101181 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.2365],\n",
      "        [0.6771]])\n",
      "Iteration 29210 Training loss 0.11574314534664154 Validation loss 0.11489909887313843 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.2218],\n",
      "        [0.2338]])\n",
      "Iteration 29220 Training loss 0.11510088294744492 Validation loss 0.11488141119480133 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4757],\n",
      "        [0.6280]])\n",
      "Iteration 29230 Training loss 0.11611712723970413 Validation loss 0.11488283425569534 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.5389],\n",
      "        [0.6786]])\n",
      "Iteration 29240 Training loss 0.11768210679292679 Validation loss 0.11488203704357147 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4713],\n",
      "        [0.4981]])\n",
      "Iteration 29250 Training loss 0.1169814020395279 Validation loss 0.1149454116821289 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.1887],\n",
      "        [0.6038]])\n",
      "Iteration 29260 Training loss 0.11665021628141403 Validation loss 0.1148868203163147 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5303],\n",
      "        [0.5108]])\n",
      "Iteration 29270 Training loss 0.11725085228681564 Validation loss 0.1148780956864357 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4839],\n",
      "        [0.6345]])\n",
      "Iteration 29280 Training loss 0.11657889932394028 Validation loss 0.11499129235744476 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.2340],\n",
      "        [0.4952]])\n",
      "Iteration 29290 Training loss 0.11565818637609482 Validation loss 0.1148732528090477 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5353],\n",
      "        [0.4942]])\n",
      "Iteration 29300 Training loss 0.11647950112819672 Validation loss 0.11484742909669876 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5635],\n",
      "        [0.5283]])\n",
      "Iteration 29310 Training loss 0.11665689945220947 Validation loss 0.11493556946516037 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5639],\n",
      "        [0.6528]])\n",
      "Iteration 29320 Training loss 0.11625370383262634 Validation loss 0.11484967917203903 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.2546],\n",
      "        [0.6863]])\n",
      "Iteration 29330 Training loss 0.11604737490415573 Validation loss 0.11484956741333008 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.6752],\n",
      "        [0.5400]])\n",
      "Iteration 29340 Training loss 0.11615229398012161 Validation loss 0.1148354709148407 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.3308],\n",
      "        [0.5454]])\n",
      "Iteration 29350 Training loss 0.11548266559839249 Validation loss 0.11483766883611679 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5692],\n",
      "        [0.6517]])\n",
      "Iteration 29360 Training loss 0.11581894755363464 Validation loss 0.11496149003505707 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.4930],\n",
      "        [0.4535]])\n",
      "Iteration 29370 Training loss 0.11609920114278793 Validation loss 0.11482319980859756 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5485],\n",
      "        [0.5310]])\n",
      "Iteration 29380 Training loss 0.11671028286218643 Validation loss 0.1148194968700409 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.7233],\n",
      "        [0.6708]])\n",
      "Iteration 29390 Training loss 0.11571485549211502 Validation loss 0.11488309502601624 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.4840],\n",
      "        [0.5912]])\n",
      "Iteration 29400 Training loss 0.11462287604808807 Validation loss 0.11493591219186783 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.5433],\n",
      "        [0.5735]])\n",
      "Iteration 29410 Training loss 0.11689472198486328 Validation loss 0.11487393081188202 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4749],\n",
      "        [0.5202]])\n",
      "Iteration 29420 Training loss 0.11701394617557526 Validation loss 0.11492510885000229 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5855],\n",
      "        [0.4698]])\n",
      "Iteration 29430 Training loss 0.11649490147829056 Validation loss 0.11494845896959305 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.3693],\n",
      "        [0.3439]])\n",
      "Iteration 29440 Training loss 0.11455544084310532 Validation loss 0.11481217294931412 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.5492],\n",
      "        [0.7099]])\n",
      "Iteration 29450 Training loss 0.11719231307506561 Validation loss 0.11482061445713043 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.5890],\n",
      "        [0.4344]])\n",
      "Iteration 29460 Training loss 0.11697319895029068 Validation loss 0.11480091512203217 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5082],\n",
      "        [0.3613]])\n",
      "Iteration 29470 Training loss 0.11662854254245758 Validation loss 0.11488614976406097 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.2302],\n",
      "        [0.4959]])\n",
      "Iteration 29480 Training loss 0.1175784021615982 Validation loss 0.11485762894153595 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.7120],\n",
      "        [0.3083]])\n",
      "Iteration 29490 Training loss 0.1171390488743782 Validation loss 0.11482277512550354 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.3829],\n",
      "        [0.3523]])\n",
      "Iteration 29500 Training loss 0.11541636288166046 Validation loss 0.11489060521125793 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5747],\n",
      "        [0.5812]])\n",
      "Iteration 29510 Training loss 0.11517233401536942 Validation loss 0.11484290659427643 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3510],\n",
      "        [0.3875]])\n",
      "Iteration 29520 Training loss 0.11597874015569687 Validation loss 0.11483591794967651 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.6405],\n",
      "        [0.4967]])\n",
      "Iteration 29530 Training loss 0.11534523218870163 Validation loss 0.11482501029968262 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4293],\n",
      "        [0.4868]])\n",
      "Iteration 29540 Training loss 0.11589843034744263 Validation loss 0.11482461541891098 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.6690],\n",
      "        [0.5039]])\n",
      "Iteration 29550 Training loss 0.11850886046886444 Validation loss 0.11488966643810272 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.7305],\n",
      "        [0.5970]])\n",
      "Iteration 29560 Training loss 0.11712095141410828 Validation loss 0.11482173949480057 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.4099],\n",
      "        [0.4845]])\n",
      "Iteration 29570 Training loss 0.11590515077114105 Validation loss 0.1148180216550827 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.5018],\n",
      "        [0.6435]])\n",
      "Iteration 29580 Training loss 0.11602696031332016 Validation loss 0.11482866108417511 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.5874],\n",
      "        [0.5380]])\n",
      "Iteration 29590 Training loss 0.11645188182592392 Validation loss 0.11486499756574631 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.5307],\n",
      "        [0.6514]])\n",
      "Iteration 29600 Training loss 0.11607954651117325 Validation loss 0.11484474688768387 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4713],\n",
      "        [0.5711]])\n",
      "Iteration 29610 Training loss 0.11732304096221924 Validation loss 0.11493679136037827 Accuracy 0.6184999942779541\n",
      "Output tensor([[0.4437],\n",
      "        [0.4132]])\n",
      "Iteration 29620 Training loss 0.11689827591180801 Validation loss 0.11487206816673279 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.5333],\n",
      "        [0.3879]])\n",
      "Iteration 29630 Training loss 0.11591503024101257 Validation loss 0.11496182531118393 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5565],\n",
      "        [0.5801]])\n",
      "Iteration 29640 Training loss 0.11696875095367432 Validation loss 0.11496645212173462 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.2666],\n",
      "        [0.1575]])\n",
      "Iteration 29650 Training loss 0.11512583494186401 Validation loss 0.11500532925128937 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5403],\n",
      "        [0.6606]])\n",
      "Iteration 29660 Training loss 0.11565647274255753 Validation loss 0.11483843624591827 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.5273],\n",
      "        [0.5626]])\n",
      "Iteration 29670 Training loss 0.11626438051462173 Validation loss 0.11486178636550903 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.3830],\n",
      "        [0.5627]])\n",
      "Iteration 29680 Training loss 0.1165524274110794 Validation loss 0.1148003414273262 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.4943],\n",
      "        [0.6231]])\n",
      "Iteration 29690 Training loss 0.11618225276470184 Validation loss 0.11479964107275009 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4188],\n",
      "        [0.5255]])\n",
      "Iteration 29700 Training loss 0.11560557037591934 Validation loss 0.11494237184524536 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4363],\n",
      "        [0.4553]])\n",
      "Iteration 29710 Training loss 0.1178644597530365 Validation loss 0.11485224217176437 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5801],\n",
      "        [0.4937]])\n",
      "Iteration 29720 Training loss 0.11577799171209335 Validation loss 0.11484865099191666 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.6528],\n",
      "        [0.3720]])\n",
      "Iteration 29730 Training loss 0.11699972301721573 Validation loss 0.11483833938837051 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.5720],\n",
      "        [0.3376]])\n",
      "Iteration 29740 Training loss 0.11619899421930313 Validation loss 0.11481068283319473 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4688],\n",
      "        [0.4673]])\n",
      "Iteration 29750 Training loss 0.11704942584037781 Validation loss 0.11488056182861328 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.4917],\n",
      "        [0.6063]])\n",
      "Iteration 29760 Training loss 0.11630818992853165 Validation loss 0.11477719992399216 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.3820],\n",
      "        [0.4667]])\n",
      "Iteration 29770 Training loss 0.11631884425878525 Validation loss 0.1149120107293129 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.6710],\n",
      "        [0.5830]])\n",
      "Iteration 29780 Training loss 0.11564964801073074 Validation loss 0.1147916167974472 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.6251],\n",
      "        [0.2642]])\n",
      "Iteration 29790 Training loss 0.1164877861738205 Validation loss 0.11480086296796799 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.2973],\n",
      "        [0.6894]])\n",
      "Iteration 29800 Training loss 0.1182193011045456 Validation loss 0.11481478810310364 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.4893],\n",
      "        [0.5801]])\n",
      "Iteration 29810 Training loss 0.1150527372956276 Validation loss 0.11477557569742203 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.2700],\n",
      "        [0.4592]])\n",
      "Iteration 29820 Training loss 0.1166229248046875 Validation loss 0.11478329449892044 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.3966],\n",
      "        [0.4146]])\n",
      "Iteration 29830 Training loss 0.11628787964582443 Validation loss 0.1147971823811531 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.5512],\n",
      "        [0.6029]])\n",
      "Iteration 29840 Training loss 0.11806854605674744 Validation loss 0.11491762101650238 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.2852],\n",
      "        [0.4738]])\n",
      "Iteration 29850 Training loss 0.11571823805570602 Validation loss 0.11476550251245499 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.4955],\n",
      "        [0.5820]])\n",
      "Iteration 29860 Training loss 0.11722342669963837 Validation loss 0.11485518515110016 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.5609],\n",
      "        [0.4506]])\n",
      "Iteration 29870 Training loss 0.11618226021528244 Validation loss 0.11479073017835617 Accuracy 0.6183333396911621\n",
      "Output tensor([[0.1977],\n",
      "        [0.4593]])\n",
      "Iteration 29880 Training loss 0.11722742021083832 Validation loss 0.11479859054088593 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.4381],\n",
      "        [0.7679]])\n",
      "Iteration 29890 Training loss 0.11719295382499695 Validation loss 0.11482273042201996 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.5014],\n",
      "        [0.4152]])\n",
      "Iteration 29900 Training loss 0.11482575535774231 Validation loss 0.11484894156455994 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.1849],\n",
      "        [0.5429]])\n",
      "Iteration 29910 Training loss 0.11513742804527283 Validation loss 0.11481297016143799 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5096],\n",
      "        [0.4980]])\n",
      "Iteration 29920 Training loss 0.11685409396886826 Validation loss 0.11483167856931686 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.5368],\n",
      "        [0.3430]])\n",
      "Iteration 29930 Training loss 0.11576405167579651 Validation loss 0.11482077836990356 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.6589],\n",
      "        [0.4223]])\n",
      "Iteration 29940 Training loss 0.11737430095672607 Validation loss 0.11485297977924347 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5967],\n",
      "        [0.6633]])\n",
      "Iteration 29950 Training loss 0.11670038849115372 Validation loss 0.11482971161603928 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.5192],\n",
      "        [0.5512]])\n",
      "Iteration 29960 Training loss 0.11549489200115204 Validation loss 0.11480098217725754 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.6477],\n",
      "        [0.3425]])\n",
      "Iteration 29970 Training loss 0.11569416522979736 Validation loss 0.11492633074522018 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.4488],\n",
      "        [0.1462]])\n",
      "Iteration 29980 Training loss 0.11699798703193665 Validation loss 0.11483839899301529 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.4864],\n",
      "        [0.5769]])\n",
      "Iteration 29990 Training loss 0.11593502014875412 Validation loss 0.11480364948511124 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.2850],\n",
      "        [0.3399]])\n",
      "Iteration 30000 Training loss 0.11631496250629425 Validation loss 0.11485619843006134 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4725],\n",
      "        [0.6626]])\n",
      "Iteration 30010 Training loss 0.11614633351564407 Validation loss 0.11479765176773071 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4788],\n",
      "        [0.4285]])\n",
      "Iteration 30020 Training loss 0.11661618947982788 Validation loss 0.1148018166422844 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.4893],\n",
      "        [0.5119]])\n",
      "Iteration 30030 Training loss 0.11721740663051605 Validation loss 0.11479318886995316 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.3855],\n",
      "        [0.4096]])\n",
      "Iteration 30040 Training loss 0.11634105443954468 Validation loss 0.11478187888860703 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.4873],\n",
      "        [0.3069]])\n",
      "Iteration 30050 Training loss 0.11657274514436722 Validation loss 0.1147792786359787 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4872],\n",
      "        [0.2024]])\n",
      "Iteration 30060 Training loss 0.11589721590280533 Validation loss 0.11478713154792786 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.3405],\n",
      "        [0.5644]])\n",
      "Iteration 30070 Training loss 0.11736372113227844 Validation loss 0.11481036245822906 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.3372],\n",
      "        [0.5025]])\n",
      "Iteration 30080 Training loss 0.116222083568573 Validation loss 0.11479758471250534 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.6182],\n",
      "        [0.5488]])\n",
      "Iteration 30090 Training loss 0.11634436249732971 Validation loss 0.1147938221693039 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.6937],\n",
      "        [0.6329]])\n",
      "Iteration 30100 Training loss 0.11625441163778305 Validation loss 0.11478853225708008 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.3631],\n",
      "        [0.5456]])\n",
      "Iteration 30110 Training loss 0.11560777574777603 Validation loss 0.11481165885925293 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.6822],\n",
      "        [0.3477]])\n",
      "Iteration 30120 Training loss 0.11597497761249542 Validation loss 0.11481227725744247 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4644],\n",
      "        [0.5260]])\n",
      "Iteration 30130 Training loss 0.1170320063829422 Validation loss 0.11477728933095932 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4948],\n",
      "        [0.4417]])\n",
      "Iteration 30140 Training loss 0.11486594378948212 Validation loss 0.11475666612386703 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.5415],\n",
      "        [0.3532]])\n",
      "Iteration 30150 Training loss 0.11549844592809677 Validation loss 0.11477265506982803 Accuracy 0.6188333630561829\n",
      "Output tensor([[0.3905],\n",
      "        [0.5489]])\n",
      "Iteration 30160 Training loss 0.11597368121147156 Validation loss 0.11476872116327286 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.5383],\n",
      "        [0.3834]])\n",
      "Iteration 30170 Training loss 0.11702840030193329 Validation loss 0.11479020118713379 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.5718],\n",
      "        [0.4121]])\n",
      "Iteration 30180 Training loss 0.11688914895057678 Validation loss 0.11478565633296967 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.5017],\n",
      "        [0.5239]])\n",
      "Iteration 30190 Training loss 0.11610027402639389 Validation loss 0.11477693915367126 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.3321],\n",
      "        [0.6563]])\n",
      "Iteration 30200 Training loss 0.11621084809303284 Validation loss 0.11474831402301788 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.2168],\n",
      "        [0.6246]])\n",
      "Iteration 30210 Training loss 0.11706170439720154 Validation loss 0.11483380198478699 Accuracy 0.621833324432373\n",
      "Output tensor([[0.3283],\n",
      "        [0.4943]])\n",
      "Iteration 30220 Training loss 0.11564745754003525 Validation loss 0.11473298072814941 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.5467],\n",
      "        [0.4801]])\n",
      "Iteration 30230 Training loss 0.11529146134853363 Validation loss 0.11479190737009048 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4969],\n",
      "        [0.3474]])\n",
      "Iteration 30240 Training loss 0.11634024977684021 Validation loss 0.11473986506462097 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.5093],\n",
      "        [0.3164]])\n",
      "Iteration 30250 Training loss 0.11627239733934402 Validation loss 0.11474902182817459 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.4855],\n",
      "        [0.5409]])\n",
      "Iteration 30260 Training loss 0.11688748002052307 Validation loss 0.11476591974496841 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.4707],\n",
      "        [0.4732]])\n",
      "Iteration 30270 Training loss 0.11602360010147095 Validation loss 0.11476539820432663 Accuracy 0.621833324432373\n",
      "Output tensor([[0.3722],\n",
      "        [0.5498]])\n",
      "Iteration 30280 Training loss 0.11594542115926743 Validation loss 0.11478037387132645 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.6871],\n",
      "        [0.4507]])\n",
      "Iteration 30290 Training loss 0.11632902920246124 Validation loss 0.11480523645877838 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.5951],\n",
      "        [0.5124]])\n",
      "Iteration 30300 Training loss 0.11832728236913681 Validation loss 0.1148124411702156 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5501],\n",
      "        [0.6984]])\n",
      "Iteration 30310 Training loss 0.11638221889734268 Validation loss 0.11481985449790955 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.6565],\n",
      "        [0.2500]])\n",
      "Iteration 30320 Training loss 0.11588435620069504 Validation loss 0.11493818461894989 Accuracy 0.6179999709129333\n",
      "Output tensor([[0.6921],\n",
      "        [0.1808]])\n",
      "Iteration 30330 Training loss 0.1167118102312088 Validation loss 0.11509398370981216 Accuracy 0.6181666851043701\n",
      "Output tensor([[0.5286],\n",
      "        [0.3396]])\n",
      "Iteration 30340 Training loss 0.1165098249912262 Validation loss 0.11482634395360947 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5951],\n",
      "        [0.5085]])\n",
      "Iteration 30350 Training loss 0.11600039899349213 Validation loss 0.11495988070964813 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.4271],\n",
      "        [0.3684]])\n",
      "Iteration 30360 Training loss 0.11559000611305237 Validation loss 0.11482684314250946 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5650],\n",
      "        [0.1549]])\n",
      "Iteration 30370 Training loss 0.11594323813915253 Validation loss 0.11499419808387756 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4013],\n",
      "        [0.5078]])\n",
      "Iteration 30380 Training loss 0.11580415070056915 Validation loss 0.11481653153896332 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.3133],\n",
      "        [0.5868]])\n",
      "Iteration 30390 Training loss 0.11561170220375061 Validation loss 0.1149192824959755 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.4511],\n",
      "        [0.0552]])\n",
      "Iteration 30400 Training loss 0.1159280389547348 Validation loss 0.11477035284042358 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.5209],\n",
      "        [0.6122]])\n",
      "Iteration 30410 Training loss 0.115829698741436 Validation loss 0.11477460712194443 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.3471],\n",
      "        [0.1829]])\n",
      "Iteration 30420 Training loss 0.1174020767211914 Validation loss 0.11478201299905777 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.3968],\n",
      "        [0.4260]])\n",
      "Iteration 30430 Training loss 0.11641459912061691 Validation loss 0.11481256037950516 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.6574],\n",
      "        [0.6060]])\n",
      "Iteration 30440 Training loss 0.11519984900951385 Validation loss 0.11481494456529617 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.2643],\n",
      "        [0.3317]])\n",
      "Iteration 30450 Training loss 0.11557753384113312 Validation loss 0.1147884726524353 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4739],\n",
      "        [0.6412]])\n",
      "Iteration 30460 Training loss 0.11576121300458908 Validation loss 0.11481887102127075 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.5790],\n",
      "        [0.5556]])\n",
      "Iteration 30470 Training loss 0.11735567450523376 Validation loss 0.11503313481807709 Accuracy 0.6173333525657654\n",
      "Output tensor([[0.3283],\n",
      "        [0.3316]])\n",
      "Iteration 30480 Training loss 0.11531632393598557 Validation loss 0.11478579044342041 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.3738],\n",
      "        [0.3875]])\n",
      "Iteration 30490 Training loss 0.11696136742830276 Validation loss 0.11487630009651184 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.6507],\n",
      "        [0.6436]])\n",
      "Iteration 30500 Training loss 0.1158091202378273 Validation loss 0.11479412764310837 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.7076],\n",
      "        [0.7313]])\n",
      "Iteration 30510 Training loss 0.11467818170785904 Validation loss 0.11481614410877228 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.2565],\n",
      "        [0.2736]])\n",
      "Iteration 30520 Training loss 0.1155940517783165 Validation loss 0.11482636630535126 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.6570],\n",
      "        [0.3127]])\n",
      "Iteration 30530 Training loss 0.11719223111867905 Validation loss 0.11479941755533218 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.3865],\n",
      "        [0.3568]])\n",
      "Iteration 30540 Training loss 0.11693812906742096 Validation loss 0.11479391157627106 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.4907],\n",
      "        [0.3666]])\n",
      "Iteration 30550 Training loss 0.1162886843085289 Validation loss 0.1147979125380516 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.3431],\n",
      "        [0.7156]])\n",
      "Iteration 30560 Training loss 0.11561651527881622 Validation loss 0.1147526428103447 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.4609],\n",
      "        [0.7096]])\n",
      "Iteration 30570 Training loss 0.11736010015010834 Validation loss 0.11478032171726227 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.4733],\n",
      "        [0.2643]])\n",
      "Iteration 30580 Training loss 0.11552869528532028 Validation loss 0.11477559059858322 Accuracy 0.621666669845581\n",
      "Output tensor([[0.1487],\n",
      "        [0.5983]])\n",
      "Iteration 30590 Training loss 0.11635487526655197 Validation loss 0.11476430296897888 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.3969],\n",
      "        [0.3479]])\n",
      "Iteration 30600 Training loss 0.11731143295764923 Validation loss 0.11475078016519547 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.7101],\n",
      "        [0.3460]])\n",
      "Iteration 30610 Training loss 0.11617916077375412 Validation loss 0.11474843323230743 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.3774],\n",
      "        [0.4607]])\n",
      "Iteration 30620 Training loss 0.11711867153644562 Validation loss 0.114743672311306 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.6167],\n",
      "        [0.3784]])\n",
      "Iteration 30630 Training loss 0.11632269620895386 Validation loss 0.11474553495645523 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.6990],\n",
      "        [0.5877]])\n",
      "Iteration 30640 Training loss 0.11697504669427872 Validation loss 0.11494927853345871 Accuracy 0.6186666488647461\n",
      "Output tensor([[0.4699],\n",
      "        [0.4697]])\n",
      "Iteration 30650 Training loss 0.11550766974687576 Validation loss 0.11471246182918549 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4476],\n",
      "        [0.4397]])\n",
      "Iteration 30660 Training loss 0.1165952980518341 Validation loss 0.11473127454519272 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.3624],\n",
      "        [0.4899]])\n",
      "Iteration 30670 Training loss 0.11623843014240265 Validation loss 0.11474818736314774 Accuracy 0.621666669845581\n",
      "Output tensor([[0.3260],\n",
      "        [0.5409]])\n",
      "Iteration 30680 Training loss 0.11596209555864334 Validation loss 0.11472855508327484 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.3847],\n",
      "        [0.4751]])\n",
      "Iteration 30690 Training loss 0.11662618070840836 Validation loss 0.11476621776819229 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4285],\n",
      "        [0.6363]])\n",
      "Iteration 30700 Training loss 0.1157446876168251 Validation loss 0.11470023542642593 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.2936],\n",
      "        [0.5405]])\n",
      "Iteration 30710 Training loss 0.1160641759634018 Validation loss 0.11474375426769257 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.6010],\n",
      "        [0.6063]])\n",
      "Iteration 30720 Training loss 0.11793902516365051 Validation loss 0.11471971869468689 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.4923],\n",
      "        [0.4850]])\n",
      "Iteration 30730 Training loss 0.11704017966985703 Validation loss 0.11470390111207962 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.3070],\n",
      "        [0.5574]])\n",
      "Iteration 30740 Training loss 0.117639921605587 Validation loss 0.11466870456933975 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.5753],\n",
      "        [0.6071]])\n",
      "Iteration 30750 Training loss 0.115774966776371 Validation loss 0.11473654210567474 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4324],\n",
      "        [0.5037]])\n",
      "Iteration 30760 Training loss 0.11671236902475357 Validation loss 0.11466231197118759 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.2829],\n",
      "        [0.4296]])\n",
      "Iteration 30770 Training loss 0.1168104037642479 Validation loss 0.11467375606298447 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.2593],\n",
      "        [0.5301]])\n",
      "Iteration 30780 Training loss 0.11670219898223877 Validation loss 0.11469216644763947 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.3541],\n",
      "        [0.4417]])\n",
      "Iteration 30790 Training loss 0.11648720502853394 Validation loss 0.11484808474779129 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5676],\n",
      "        [0.4382]])\n",
      "Iteration 30800 Training loss 0.114934042096138 Validation loss 0.11469386518001556 Accuracy 0.621666669845581\n",
      "Output tensor([[0.4295],\n",
      "        [0.4133]])\n",
      "Iteration 30810 Training loss 0.11699946224689484 Validation loss 0.11472896486520767 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5319],\n",
      "        [0.4951]])\n",
      "Iteration 30820 Training loss 0.11622589081525803 Validation loss 0.11469040811061859 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.2912],\n",
      "        [0.3590]])\n",
      "Iteration 30830 Training loss 0.11665818095207214 Validation loss 0.11470070481300354 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.6802],\n",
      "        [0.5474]])\n",
      "Iteration 30840 Training loss 0.11566600203514099 Validation loss 0.11472215503454208 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.6506],\n",
      "        [0.5138]])\n",
      "Iteration 30850 Training loss 0.11599741876125336 Validation loss 0.11467982828617096 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.2775],\n",
      "        [0.5448]])\n",
      "Iteration 30860 Training loss 0.11683346331119537 Validation loss 0.11469846963882446 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5856],\n",
      "        [0.6618]])\n",
      "Iteration 30870 Training loss 0.11591766029596329 Validation loss 0.11468132585287094 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.6390],\n",
      "        [0.5428]])\n",
      "Iteration 30880 Training loss 0.11617010831832886 Validation loss 0.11468365788459778 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.3737],\n",
      "        [0.6862]])\n",
      "Iteration 30890 Training loss 0.1171603873372078 Validation loss 0.11473539471626282 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4449],\n",
      "        [0.4169]])\n",
      "Iteration 30900 Training loss 0.11653999984264374 Validation loss 0.11473961174488068 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.4091],\n",
      "        [0.4796]])\n",
      "Iteration 30910 Training loss 0.11621344834566116 Validation loss 0.1147528663277626 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.1356],\n",
      "        [0.5805]])\n",
      "Iteration 30920 Training loss 0.11612322926521301 Validation loss 0.1147037148475647 Accuracy 0.621666669845581\n",
      "Output tensor([[0.4971],\n",
      "        [0.6384]])\n",
      "Iteration 30930 Training loss 0.11767876893281937 Validation loss 0.11475569754838943 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.4497],\n",
      "        [0.4372]])\n",
      "Iteration 30940 Training loss 0.11586683243513107 Validation loss 0.11481426656246185 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.4059],\n",
      "        [0.5060]])\n",
      "Iteration 30950 Training loss 0.11661791801452637 Validation loss 0.11470254510641098 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.5551],\n",
      "        [0.5417]])\n",
      "Iteration 30960 Training loss 0.11657929420471191 Validation loss 0.11468464136123657 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.5378],\n",
      "        [0.6270]])\n",
      "Iteration 30970 Training loss 0.11651254445314407 Validation loss 0.11467085778713226 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.5619],\n",
      "        [0.6056]])\n",
      "Iteration 30980 Training loss 0.11584113538265228 Validation loss 0.11471813172101974 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.3649],\n",
      "        [0.2881]])\n",
      "Iteration 30990 Training loss 0.11711081117391586 Validation loss 0.1146983653306961 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5026],\n",
      "        [0.2960]])\n",
      "Iteration 31000 Training loss 0.11741141974925995 Validation loss 0.11481285840272903 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.3381],\n",
      "        [0.3977]])\n",
      "Iteration 31010 Training loss 0.11526491492986679 Validation loss 0.11470095068216324 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.4108],\n",
      "        [0.4219]])\n",
      "Iteration 31020 Training loss 0.1161692887544632 Validation loss 0.11473743617534637 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.4259],\n",
      "        [0.2656]])\n",
      "Iteration 31030 Training loss 0.11581628769636154 Validation loss 0.11469494551420212 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.5231],\n",
      "        [0.6300]])\n",
      "Iteration 31040 Training loss 0.11651947349309921 Validation loss 0.11469611525535583 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.4928],\n",
      "        [0.4893]])\n",
      "Iteration 31050 Training loss 0.11636018753051758 Validation loss 0.11468719691038132 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.6386],\n",
      "        [0.6096]])\n",
      "Iteration 31060 Training loss 0.11674625426530838 Validation loss 0.1146797463297844 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5360],\n",
      "        [0.5159]])\n",
      "Iteration 31070 Training loss 0.11595983803272247 Validation loss 0.11471479386091232 Accuracy 0.621833324432373\n",
      "Output tensor([[0.6587],\n",
      "        [0.5979]])\n",
      "Iteration 31080 Training loss 0.11691676825284958 Validation loss 0.11470895260572433 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.5339],\n",
      "        [0.4127]])\n",
      "Iteration 31090 Training loss 0.11647161841392517 Validation loss 0.11470243334770203 Accuracy 0.621999979019165\n",
      "Output tensor([[0.4339],\n",
      "        [0.6014]])\n",
      "Iteration 31100 Training loss 0.11490096896886826 Validation loss 0.11470086872577667 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.4954],\n",
      "        [0.4143]])\n",
      "Iteration 31110 Training loss 0.11675295233726501 Validation loss 0.11469569802284241 Accuracy 0.621999979019165\n",
      "Output tensor([[0.5865],\n",
      "        [0.4272]])\n",
      "Iteration 31120 Training loss 0.1171061098575592 Validation loss 0.11474504321813583 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5205],\n",
      "        [0.5611]])\n",
      "Iteration 31130 Training loss 0.11602210253477097 Validation loss 0.11469759792089462 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.5534],\n",
      "        [0.6561]])\n",
      "Iteration 31140 Training loss 0.11680397391319275 Validation loss 0.11471376568078995 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.3344],\n",
      "        [0.5781]])\n",
      "Iteration 31150 Training loss 0.11612756550312042 Validation loss 0.11469408124685287 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3795],\n",
      "        [0.4300]])\n",
      "Iteration 31160 Training loss 0.11539915204048157 Validation loss 0.11472813040018082 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.4266],\n",
      "        [0.4076]])\n",
      "Iteration 31170 Training loss 0.11640605330467224 Validation loss 0.11470960825681686 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5049],\n",
      "        [0.5806]])\n",
      "Iteration 31180 Training loss 0.11664776504039764 Validation loss 0.11481495946645737 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.3937],\n",
      "        [0.2383]])\n",
      "Iteration 31190 Training loss 0.11565736681222916 Validation loss 0.11475177854299545 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.6766],\n",
      "        [0.4702]])\n",
      "Iteration 31200 Training loss 0.11657091230154037 Validation loss 0.11473190039396286 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.4950],\n",
      "        [0.3476]])\n",
      "Iteration 31210 Training loss 0.11657027155160904 Validation loss 0.11477884650230408 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5851],\n",
      "        [0.5325]])\n",
      "Iteration 31220 Training loss 0.11717396974563599 Validation loss 0.11469084769487381 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.4412],\n",
      "        [0.5231]])\n",
      "Iteration 31230 Training loss 0.11718165874481201 Validation loss 0.11470624059438705 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.4281],\n",
      "        [0.1665]])\n",
      "Iteration 31240 Training loss 0.11618925631046295 Validation loss 0.1146860122680664 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.4115],\n",
      "        [0.4954]])\n",
      "Iteration 31250 Training loss 0.11569201946258545 Validation loss 0.11478802561759949 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.1846],\n",
      "        [0.2626]])\n",
      "Iteration 31260 Training loss 0.11630106717348099 Validation loss 0.1147511675953865 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5268],\n",
      "        [0.5220]])\n",
      "Iteration 31270 Training loss 0.11666705459356308 Validation loss 0.11468902230262756 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.6296],\n",
      "        [0.6357]])\n",
      "Iteration 31280 Training loss 0.11626571416854858 Validation loss 0.11469480395317078 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.4500],\n",
      "        [0.5711]])\n",
      "Iteration 31290 Training loss 0.11634804308414459 Validation loss 0.1147245466709137 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.5393],\n",
      "        [0.3161]])\n",
      "Iteration 31300 Training loss 0.11661218106746674 Validation loss 0.1147240400314331 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.4392],\n",
      "        [0.4140]])\n",
      "Iteration 31310 Training loss 0.1160251796245575 Validation loss 0.11467693746089935 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.3373],\n",
      "        [0.4486]])\n",
      "Iteration 31320 Training loss 0.11741075664758682 Validation loss 0.11477972567081451 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4689],\n",
      "        [0.4409]])\n",
      "Iteration 31330 Training loss 0.1163448914885521 Validation loss 0.11467809975147247 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.2680],\n",
      "        [0.3703]])\n",
      "Iteration 31340 Training loss 0.11643587052822113 Validation loss 0.11469141393899918 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.4519],\n",
      "        [0.5143]])\n",
      "Iteration 31350 Training loss 0.116560198366642 Validation loss 0.11468304693698883 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.4882],\n",
      "        [0.5430]])\n",
      "Iteration 31360 Training loss 0.11646822094917297 Validation loss 0.11473805457353592 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.5424],\n",
      "        [0.4261]])\n",
      "Iteration 31370 Training loss 0.11740561574697495 Validation loss 0.11471938341856003 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.4791],\n",
      "        [0.5583]])\n",
      "Iteration 31380 Training loss 0.11656809598207474 Validation loss 0.11468113958835602 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.4656],\n",
      "        [0.3701]])\n",
      "Iteration 31390 Training loss 0.11632953584194183 Validation loss 0.11469117552042007 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.7244],\n",
      "        [0.5094]])\n",
      "Iteration 31400 Training loss 0.11614540964365005 Validation loss 0.11467402428388596 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.6856],\n",
      "        [0.4243]])\n",
      "Iteration 31410 Training loss 0.1160445362329483 Validation loss 0.11468952149152756 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4317],\n",
      "        [0.5369]])\n",
      "Iteration 31420 Training loss 0.11691269278526306 Validation loss 0.11467889696359634 Accuracy 0.621999979019165\n",
      "Output tensor([[0.4474],\n",
      "        [0.5599]])\n",
      "Iteration 31430 Training loss 0.11540018767118454 Validation loss 0.11466796696186066 Accuracy 0.6191666722297668\n",
      "Output tensor([[0.3478],\n",
      "        [0.6571]])\n",
      "Iteration 31440 Training loss 0.11552529036998749 Validation loss 0.11465765535831451 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.5078],\n",
      "        [0.1631]])\n",
      "Iteration 31450 Training loss 0.11726643145084381 Validation loss 0.11465980112552643 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.4277],\n",
      "        [0.3550]])\n",
      "Iteration 31460 Training loss 0.11625377088785172 Validation loss 0.11466185748577118 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5017],\n",
      "        [0.3887]])\n",
      "Iteration 31470 Training loss 0.11506687104701996 Validation loss 0.11468743532896042 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.4447],\n",
      "        [0.4853]])\n",
      "Iteration 31480 Training loss 0.11810487508773804 Validation loss 0.11471199244260788 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.5416],\n",
      "        [0.5423]])\n",
      "Iteration 31490 Training loss 0.11707879602909088 Validation loss 0.11468736082315445 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.3739],\n",
      "        [0.5990]])\n",
      "Iteration 31500 Training loss 0.11699381470680237 Validation loss 0.11471632122993469 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.4101],\n",
      "        [0.5463]])\n",
      "Iteration 31510 Training loss 0.1164703443646431 Validation loss 0.11469864845275879 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.4545],\n",
      "        [0.5477]])\n",
      "Iteration 31520 Training loss 0.11612581461668015 Validation loss 0.11468280851840973 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.5486],\n",
      "        [0.4745]])\n",
      "Iteration 31530 Training loss 0.11665403097867966 Validation loss 0.1146601140499115 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5674],\n",
      "        [0.4432]])\n",
      "Iteration 31540 Training loss 0.11528342962265015 Validation loss 0.11467067152261734 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.5005],\n",
      "        [0.4967]])\n",
      "Iteration 31550 Training loss 0.11757645756006241 Validation loss 0.11468280106782913 Accuracy 0.621999979019165\n",
      "Output tensor([[0.4955],\n",
      "        [0.4786]])\n",
      "Iteration 31560 Training loss 0.11610737442970276 Validation loss 0.1146998405456543 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.5990],\n",
      "        [0.5790]])\n",
      "Iteration 31570 Training loss 0.11680600792169571 Validation loss 0.11467590183019638 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.5181],\n",
      "        [0.1048]])\n",
      "Iteration 31580 Training loss 0.11616137623786926 Validation loss 0.1147313341498375 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5777],\n",
      "        [0.4716]])\n",
      "Iteration 31590 Training loss 0.11703667044639587 Validation loss 0.11473581939935684 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.7943],\n",
      "        [0.4787]])\n",
      "Iteration 31600 Training loss 0.11653456836938858 Validation loss 0.1147141307592392 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4899],\n",
      "        [0.5516]])\n",
      "Iteration 31610 Training loss 0.11722217500209808 Validation loss 0.11472465097904205 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.4433],\n",
      "        [0.4102]])\n",
      "Iteration 31620 Training loss 0.11506649106740952 Validation loss 0.11472275853157043 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5130],\n",
      "        [0.4201]])\n",
      "Iteration 31630 Training loss 0.1163550317287445 Validation loss 0.11466652154922485 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.5928],\n",
      "        [0.6349]])\n",
      "Iteration 31640 Training loss 0.1166267991065979 Validation loss 0.11471476405858994 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.5262],\n",
      "        [0.5071]])\n",
      "Iteration 31650 Training loss 0.11720951646566391 Validation loss 0.11478686332702637 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.2955],\n",
      "        [0.4934]])\n",
      "Iteration 31660 Training loss 0.1172080859541893 Validation loss 0.1146773099899292 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.5074],\n",
      "        [0.4079]])\n",
      "Iteration 31670 Training loss 0.11659792810678482 Validation loss 0.1147175207734108 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.5883],\n",
      "        [0.4723]])\n",
      "Iteration 31680 Training loss 0.11735484004020691 Validation loss 0.1147296354174614 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.3744],\n",
      "        [0.7796]])\n",
      "Iteration 31690 Training loss 0.1152498796582222 Validation loss 0.11465991288423538 Accuracy 0.621666669845581\n",
      "Output tensor([[0.6566],\n",
      "        [0.4733]])\n",
      "Iteration 31700 Training loss 0.11629577726125717 Validation loss 0.11467018723487854 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.4048],\n",
      "        [0.3848]])\n",
      "Iteration 31710 Training loss 0.11599621921777725 Validation loss 0.11466428637504578 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5776],\n",
      "        [0.3590]])\n",
      "Iteration 31720 Training loss 0.11722318828105927 Validation loss 0.11467199772596359 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.4248],\n",
      "        [0.3023]])\n",
      "Iteration 31730 Training loss 0.11651983857154846 Validation loss 0.1147322803735733 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4668],\n",
      "        [0.5036]])\n",
      "Iteration 31740 Training loss 0.11568890511989594 Validation loss 0.11471229791641235 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5605],\n",
      "        [0.4133]])\n",
      "Iteration 31750 Training loss 0.1171598881483078 Validation loss 0.11469686776399612 Accuracy 0.621833324432373\n",
      "Output tensor([[0.3758],\n",
      "        [0.5959]])\n",
      "Iteration 31760 Training loss 0.11592864990234375 Validation loss 0.11469325423240662 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.1486],\n",
      "        [0.3643]])\n",
      "Iteration 31770 Training loss 0.11632401496171951 Validation loss 0.11468872427940369 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.5597],\n",
      "        [0.3602]])\n",
      "Iteration 31780 Training loss 0.11645641922950745 Validation loss 0.11467473208904266 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5387],\n",
      "        [0.6678]])\n",
      "Iteration 31790 Training loss 0.11649433523416519 Validation loss 0.11470936983823776 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.4139],\n",
      "        [0.4691]])\n",
      "Iteration 31800 Training loss 0.11594605445861816 Validation loss 0.11467909812927246 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.3419],\n",
      "        [0.5701]])\n",
      "Iteration 31810 Training loss 0.11610245704650879 Validation loss 0.11468125134706497 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.1455],\n",
      "        [0.5077]])\n",
      "Iteration 31820 Training loss 0.1160677820444107 Validation loss 0.11464601755142212 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4941],\n",
      "        [0.4503]])\n",
      "Iteration 31830 Training loss 0.11595414578914642 Validation loss 0.11463123559951782 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.2606],\n",
      "        [0.5304]])\n",
      "Iteration 31840 Training loss 0.11630065739154816 Validation loss 0.11464361846446991 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.3406],\n",
      "        [0.4041]])\n",
      "Iteration 31850 Training loss 0.11578535288572311 Validation loss 0.11463472247123718 Accuracy 0.621833324432373\n",
      "Output tensor([[0.6177],\n",
      "        [0.4804]])\n",
      "Iteration 31860 Training loss 0.11597756296396255 Validation loss 0.11467323452234268 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.3779],\n",
      "        [0.4029]])\n",
      "Iteration 31870 Training loss 0.11602647602558136 Validation loss 0.11465173214673996 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.2026],\n",
      "        [0.2976]])\n",
      "Iteration 31880 Training loss 0.11696260422468185 Validation loss 0.11465764045715332 Accuracy 0.621666669845581\n",
      "Output tensor([[0.4295],\n",
      "        [0.6688]])\n",
      "Iteration 31890 Training loss 0.11642824113368988 Validation loss 0.11465910822153091 Accuracy 0.621999979019165\n",
      "Output tensor([[0.5486],\n",
      "        [0.3442]])\n",
      "Iteration 31900 Training loss 0.1164160743355751 Validation loss 0.11467631161212921 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.4221],\n",
      "        [0.6388]])\n",
      "Iteration 31910 Training loss 0.11511286348104477 Validation loss 0.11465997248888016 Accuracy 0.621999979019165\n",
      "Output tensor([[0.6523],\n",
      "        [0.3617]])\n",
      "Iteration 31920 Training loss 0.11577744781970978 Validation loss 0.11467408388853073 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.4161],\n",
      "        [0.5265]])\n",
      "Iteration 31930 Training loss 0.1167072057723999 Validation loss 0.11469647288322449 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.5719],\n",
      "        [0.3759]])\n",
      "Iteration 31940 Training loss 0.11641665548086166 Validation loss 0.11469870060682297 Accuracy 0.621999979019165\n",
      "Output tensor([[0.3792],\n",
      "        [0.4476]])\n",
      "Iteration 31950 Training loss 0.11693476140499115 Validation loss 0.11466294527053833 Accuracy 0.621833324432373\n",
      "Output tensor([[0.5919],\n",
      "        [0.6646]])\n",
      "Iteration 31960 Training loss 0.11625922471284866 Validation loss 0.11467152833938599 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.6151],\n",
      "        [0.3917]])\n",
      "Iteration 31970 Training loss 0.11598609387874603 Validation loss 0.11467792093753815 Accuracy 0.621999979019165\n",
      "Output tensor([[0.4416],\n",
      "        [0.4349]])\n",
      "Iteration 31980 Training loss 0.11645856499671936 Validation loss 0.1146467924118042 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.3782],\n",
      "        [0.2738]])\n",
      "Iteration 31990 Training loss 0.11670351028442383 Validation loss 0.11465722322463989 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5559],\n",
      "        [0.5869]])\n",
      "Iteration 32000 Training loss 0.11709319800138474 Validation loss 0.1146775484085083 Accuracy 0.621833324432373\n",
      "Output tensor([[0.5190],\n",
      "        [0.4605]])\n",
      "Iteration 32010 Training loss 0.11514485627412796 Validation loss 0.11469469219446182 Accuracy 0.621666669845581\n",
      "Output tensor([[0.2066],\n",
      "        [0.6168]])\n",
      "Iteration 32020 Training loss 0.11544777452945709 Validation loss 0.11466791480779648 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.2569],\n",
      "        [0.3033]])\n",
      "Iteration 32030 Training loss 0.11628741770982742 Validation loss 0.11468599736690521 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.4961],\n",
      "        [0.5650]])\n",
      "Iteration 32040 Training loss 0.11560451239347458 Validation loss 0.1146572157740593 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.3063],\n",
      "        [0.5589]])\n",
      "Iteration 32050 Training loss 0.11598462611436844 Validation loss 0.11467704176902771 Accuracy 0.621833324432373\n",
      "Output tensor([[0.2827],\n",
      "        [0.5722]])\n",
      "Iteration 32060 Training loss 0.11705915629863739 Validation loss 0.1146988645195961 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.6625],\n",
      "        [0.5775]])\n",
      "Iteration 32070 Training loss 0.11654704809188843 Validation loss 0.11464521288871765 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4699],\n",
      "        [0.3867]])\n",
      "Iteration 32080 Training loss 0.11655490845441818 Validation loss 0.11466322094202042 Accuracy 0.621666669845581\n",
      "Output tensor([[0.3259],\n",
      "        [0.4764]])\n",
      "Iteration 32090 Training loss 0.11641328036785126 Validation loss 0.1146606057882309 Accuracy 0.621666669845581\n",
      "Output tensor([[0.6866],\n",
      "        [0.4738]])\n",
      "Iteration 32100 Training loss 0.11684053391218185 Validation loss 0.11463536322116852 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.5932],\n",
      "        [0.6264]])\n",
      "Iteration 32110 Training loss 0.11619386821985245 Validation loss 0.11467001587152481 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.4008],\n",
      "        [0.5598]])\n",
      "Iteration 32120 Training loss 0.11690563708543777 Validation loss 0.11465008556842804 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.2375],\n",
      "        [0.6293]])\n",
      "Iteration 32130 Training loss 0.1170898973941803 Validation loss 0.1146242693066597 Accuracy 0.6193333268165588\n",
      "Output tensor([[0.5146],\n",
      "        [0.6427]])\n",
      "Iteration 32140 Training loss 0.11536310613155365 Validation loss 0.1146257147192955 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.6474],\n",
      "        [0.4527]])\n",
      "Iteration 32150 Training loss 0.11591117084026337 Validation loss 0.11467567086219788 Accuracy 0.621833324432373\n",
      "Output tensor([[0.7062],\n",
      "        [0.4398]])\n",
      "Iteration 32160 Training loss 0.11646484583616257 Validation loss 0.11466030031442642 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5980],\n",
      "        [0.3766]])\n",
      "Iteration 32170 Training loss 0.11647039651870728 Validation loss 0.1146513894200325 Accuracy 0.621833324432373\n",
      "Output tensor([[0.3349],\n",
      "        [0.5808]])\n",
      "Iteration 32180 Training loss 0.11644968390464783 Validation loss 0.11468835175037384 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.5294],\n",
      "        [0.5600]])\n",
      "Iteration 32190 Training loss 0.11659214645624161 Validation loss 0.1146794855594635 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.4791],\n",
      "        [0.2600]])\n",
      "Iteration 32200 Training loss 0.11592087894678116 Validation loss 0.11463496834039688 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.6906],\n",
      "        [0.6336]])\n",
      "Iteration 32210 Training loss 0.11764783412218094 Validation loss 0.11471938341856003 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4914],\n",
      "        [0.7202]])\n",
      "Iteration 32220 Training loss 0.11659759283065796 Validation loss 0.11472668498754501 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.4433],\n",
      "        [0.4690]])\n",
      "Iteration 32230 Training loss 0.115496426820755 Validation loss 0.11465517431497574 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.6224],\n",
      "        [0.3202]])\n",
      "Iteration 32240 Training loss 0.11695143580436707 Validation loss 0.1146269291639328 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5017],\n",
      "        [0.5668]])\n",
      "Iteration 32250 Training loss 0.11656951904296875 Validation loss 0.11464539915323257 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.2571],\n",
      "        [0.5600]])\n",
      "Iteration 32260 Training loss 0.11581627279520035 Validation loss 0.11462345719337463 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5980],\n",
      "        [0.5815]])\n",
      "Iteration 32270 Training loss 0.11590567976236343 Validation loss 0.11474283784627914 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.3295],\n",
      "        [0.6509]])\n",
      "Iteration 32280 Training loss 0.11557897925376892 Validation loss 0.11461890488862991 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4538],\n",
      "        [0.1865]])\n",
      "Iteration 32290 Training loss 0.11641408503055573 Validation loss 0.11462850123643875 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.3819],\n",
      "        [0.4338]])\n",
      "Iteration 32300 Training loss 0.11603757739067078 Validation loss 0.114700548350811 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.4746],\n",
      "        [0.5053]])\n",
      "Iteration 32310 Training loss 0.11595428735017776 Validation loss 0.11468048393726349 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.5263],\n",
      "        [0.6227]])\n",
      "Iteration 32320 Training loss 0.11504703760147095 Validation loss 0.1146933063864708 Accuracy 0.621666669845581\n",
      "Output tensor([[0.7052],\n",
      "        [0.3214]])\n",
      "Iteration 32330 Training loss 0.11516416072845459 Validation loss 0.11465387046337128 Accuracy 0.621999979019165\n",
      "Output tensor([[0.2956],\n",
      "        [0.5136]])\n",
      "Iteration 32340 Training loss 0.11720570921897888 Validation loss 0.11467193812131882 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.4647],\n",
      "        [0.5593]])\n",
      "Iteration 32350 Training loss 0.11613371968269348 Validation loss 0.11467564105987549 Accuracy 0.621999979019165\n",
      "Output tensor([[0.6511],\n",
      "        [0.6059]])\n",
      "Iteration 32360 Training loss 0.11647941172122955 Validation loss 0.11481271684169769 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.5233],\n",
      "        [0.5308]])\n",
      "Iteration 32370 Training loss 0.1159713938832283 Validation loss 0.11465203016996384 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.3431],\n",
      "        [0.7061]])\n",
      "Iteration 32380 Training loss 0.11727449297904968 Validation loss 0.11462164670228958 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.3564],\n",
      "        [0.4298]])\n",
      "Iteration 32390 Training loss 0.11676689982414246 Validation loss 0.11461252719163895 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4707],\n",
      "        [0.5211]])\n",
      "Iteration 32400 Training loss 0.11547156423330307 Validation loss 0.1146320104598999 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.5610],\n",
      "        [0.3585]])\n",
      "Iteration 32410 Training loss 0.11644124239683151 Validation loss 0.11461085826158524 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.3203],\n",
      "        [0.5018]])\n",
      "Iteration 32420 Training loss 0.11629174649715424 Validation loss 0.11459807306528091 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5672],\n",
      "        [0.5603]])\n",
      "Iteration 32430 Training loss 0.11753096431493759 Validation loss 0.11462689191102982 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.4918],\n",
      "        [0.6448]])\n",
      "Iteration 32440 Training loss 0.11467467993497849 Validation loss 0.11460509896278381 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.3707],\n",
      "        [0.5456]])\n",
      "Iteration 32450 Training loss 0.1159786805510521 Validation loss 0.11460968106985092 Accuracy 0.6194999814033508\n",
      "Output tensor([[0.6855],\n",
      "        [0.7267]])\n",
      "Iteration 32460 Training loss 0.11444409191608429 Validation loss 0.11460689455270767 Accuracy 0.621999979019165\n",
      "Output tensor([[0.2987],\n",
      "        [0.6136]])\n",
      "Iteration 32470 Training loss 0.116503044962883 Validation loss 0.1146257147192955 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4332],\n",
      "        [0.5497]])\n",
      "Iteration 32480 Training loss 0.11640284210443497 Validation loss 0.11465183645486832 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.4129],\n",
      "        [0.7868]])\n",
      "Iteration 32490 Training loss 0.11593257635831833 Validation loss 0.11464039236307144 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.1313],\n",
      "        [0.6497]])\n",
      "Iteration 32500 Training loss 0.11629026383161545 Validation loss 0.1147259920835495 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4647],\n",
      "        [0.5164]])\n",
      "Iteration 32510 Training loss 0.11685066670179367 Validation loss 0.11464175581932068 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.4033],\n",
      "        [0.5303]])\n",
      "Iteration 32520 Training loss 0.11578748375177383 Validation loss 0.11466361582279205 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.5096],\n",
      "        [0.5029]])\n",
      "Iteration 32530 Training loss 0.11703091114759445 Validation loss 0.11462438106536865 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.3379],\n",
      "        [0.5370]])\n",
      "Iteration 32540 Training loss 0.11622210592031479 Validation loss 0.11463763564825058 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.5184],\n",
      "        [0.6242]])\n",
      "Iteration 32550 Training loss 0.1159253865480423 Validation loss 0.11461363732814789 Accuracy 0.621666669845581\n",
      "Output tensor([[0.4829],\n",
      "        [0.3950]])\n",
      "Iteration 32560 Training loss 0.11587972193956375 Validation loss 0.1145920604467392 Accuracy 0.621999979019165\n",
      "Output tensor([[0.1738],\n",
      "        [0.5619]])\n",
      "Iteration 32570 Training loss 0.11710981279611588 Validation loss 0.11468622088432312 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.6296],\n",
      "        [0.3176]])\n",
      "Iteration 32580 Training loss 0.11694891005754471 Validation loss 0.11457943916320801 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.5341],\n",
      "        [0.4087]])\n",
      "Iteration 32590 Training loss 0.11530866473913193 Validation loss 0.11461833864450455 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.5408],\n",
      "        [0.4540]])\n",
      "Iteration 32600 Training loss 0.11601638793945312 Validation loss 0.11462851613759995 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.4801],\n",
      "        [0.5307]])\n",
      "Iteration 32610 Training loss 0.11719366163015366 Validation loss 0.1145980954170227 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.5627],\n",
      "        [0.6733]])\n",
      "Iteration 32620 Training loss 0.11709289252758026 Validation loss 0.11457549780607224 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.3174],\n",
      "        [0.2608]])\n",
      "Iteration 32630 Training loss 0.11697889864444733 Validation loss 0.11457685381174088 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.6663],\n",
      "        [0.6842]])\n",
      "Iteration 32640 Training loss 0.11586820334196091 Validation loss 0.11464056372642517 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.4349],\n",
      "        [0.6551]])\n",
      "Iteration 32650 Training loss 0.1158691942691803 Validation loss 0.11458276212215424 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5850],\n",
      "        [0.6490]])\n",
      "Iteration 32660 Training loss 0.1171727105975151 Validation loss 0.11459708958864212 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.5105],\n",
      "        [0.6728]])\n",
      "Iteration 32670 Training loss 0.11523943394422531 Validation loss 0.1145702376961708 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.3924],\n",
      "        [0.2305]])\n",
      "Iteration 32680 Training loss 0.11680993437767029 Validation loss 0.11460709571838379 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4394],\n",
      "        [0.3487]])\n",
      "Iteration 32690 Training loss 0.11652089655399323 Validation loss 0.11456424742937088 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.4668],\n",
      "        [0.6081]])\n",
      "Iteration 32700 Training loss 0.11638399213552475 Validation loss 0.11463689059019089 Accuracy 0.621999979019165\n",
      "Output tensor([[0.3871],\n",
      "        [0.5728]])\n",
      "Iteration 32710 Training loss 0.11558643728494644 Validation loss 0.11454864591360092 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.2945],\n",
      "        [0.7051]])\n",
      "Iteration 32720 Training loss 0.11625806987285614 Validation loss 0.11457183957099915 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.5037],\n",
      "        [0.5081]])\n",
      "Iteration 32730 Training loss 0.11485881358385086 Validation loss 0.11455082893371582 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.3817],\n",
      "        [0.5880]])\n",
      "Iteration 32740 Training loss 0.11544512957334518 Validation loss 0.11470304429531097 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4420],\n",
      "        [0.4449]])\n",
      "Iteration 32750 Training loss 0.11649185419082642 Validation loss 0.11464159935712814 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5414],\n",
      "        [0.4572]])\n",
      "Iteration 32760 Training loss 0.11681369692087173 Validation loss 0.11457674205303192 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.6063],\n",
      "        [0.4286]])\n",
      "Iteration 32770 Training loss 0.11555954813957214 Validation loss 0.11462344229221344 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.4001],\n",
      "        [0.3042]])\n",
      "Iteration 32780 Training loss 0.11623746901750565 Validation loss 0.11458520591259003 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5564],\n",
      "        [0.4912]])\n",
      "Iteration 32790 Training loss 0.11615370959043503 Validation loss 0.11458349972963333 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4696],\n",
      "        [0.3502]])\n",
      "Iteration 32800 Training loss 0.11637833714485168 Validation loss 0.1146940067410469 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.3298],\n",
      "        [0.4575]])\n",
      "Iteration 32810 Training loss 0.1165422797203064 Validation loss 0.1145334392786026 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.6308],\n",
      "        [0.6258]])\n",
      "Iteration 32820 Training loss 0.11545078456401825 Validation loss 0.11455067247152328 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.5555],\n",
      "        [0.1823]])\n",
      "Iteration 32830 Training loss 0.11590831726789474 Validation loss 0.11456067115068436 Accuracy 0.621999979019165\n",
      "Output tensor([[0.4255],\n",
      "        [0.3872]])\n",
      "Iteration 32840 Training loss 0.11518584936857224 Validation loss 0.11461758613586426 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.1650],\n",
      "        [0.4797]])\n",
      "Iteration 32850 Training loss 0.11525506526231766 Validation loss 0.1146659255027771 Accuracy 0.621666669845581\n",
      "Output tensor([[0.3406],\n",
      "        [0.5221]])\n",
      "Iteration 32860 Training loss 0.11554396897554398 Validation loss 0.11463209986686707 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.3699],\n",
      "        [0.3862]])\n",
      "Iteration 32870 Training loss 0.11667482554912567 Validation loss 0.11468169838190079 Accuracy 0.621999979019165\n",
      "Output tensor([[0.5725],\n",
      "        [0.6220]])\n",
      "Iteration 32880 Training loss 0.11678484827280045 Validation loss 0.11454113572835922 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.4047],\n",
      "        [0.5663]])\n",
      "Iteration 32890 Training loss 0.11657029390335083 Validation loss 0.11452480405569077 Accuracy 0.621999979019165\n",
      "Output tensor([[0.4507],\n",
      "        [0.5034]])\n",
      "Iteration 32900 Training loss 0.1149626225233078 Validation loss 0.11454300582408905 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.5045],\n",
      "        [0.5116]])\n",
      "Iteration 32910 Training loss 0.11533481627702713 Validation loss 0.1145944595336914 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.6334],\n",
      "        [0.4977]])\n",
      "Iteration 32920 Training loss 0.11622719466686249 Validation loss 0.11486034840345383 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.1291],\n",
      "        [0.5983]])\n",
      "Iteration 32930 Training loss 0.11700724065303802 Validation loss 0.1146119013428688 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.6004],\n",
      "        [0.6507]])\n",
      "Iteration 32940 Training loss 0.1164233386516571 Validation loss 0.11459203064441681 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.5768],\n",
      "        [0.4522]])\n",
      "Iteration 32950 Training loss 0.11682849377393723 Validation loss 0.11461633443832397 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.3706],\n",
      "        [0.6418]])\n",
      "Iteration 32960 Training loss 0.1170186698436737 Validation loss 0.1146167516708374 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.3237],\n",
      "        [0.3775]])\n",
      "Iteration 32970 Training loss 0.11740733683109283 Validation loss 0.11458524316549301 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.3992],\n",
      "        [0.4694]])\n",
      "Iteration 32980 Training loss 0.11609538644552231 Validation loss 0.11461369693279266 Accuracy 0.621666669845581\n",
      "Output tensor([[0.6271],\n",
      "        [0.7067]])\n",
      "Iteration 32990 Training loss 0.11603521555662155 Validation loss 0.11461994796991348 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.6106],\n",
      "        [0.6598]])\n",
      "Iteration 33000 Training loss 0.11616251617670059 Validation loss 0.11459711939096451 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.6500],\n",
      "        [0.4627]])\n",
      "Iteration 33010 Training loss 0.11603283882141113 Validation loss 0.1145753487944603 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4620],\n",
      "        [0.5834]])\n",
      "Iteration 33020 Training loss 0.11554639786481857 Validation loss 0.11453507840633392 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.3499],\n",
      "        [0.4576]])\n",
      "Iteration 33030 Training loss 0.11664915829896927 Validation loss 0.1145603209733963 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.5910],\n",
      "        [0.4811]])\n",
      "Iteration 33040 Training loss 0.11759041249752045 Validation loss 0.11453868448734283 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5166],\n",
      "        [0.5739]])\n",
      "Iteration 33050 Training loss 0.11627088487148285 Validation loss 0.11460325866937637 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.6065],\n",
      "        [0.6303]])\n",
      "Iteration 33060 Training loss 0.11749935895204544 Validation loss 0.11454419046640396 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.6047],\n",
      "        [0.2428]])\n",
      "Iteration 33070 Training loss 0.11663883179426193 Validation loss 0.11452403664588928 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.2221],\n",
      "        [0.5551]])\n",
      "Iteration 33080 Training loss 0.11585374921560287 Validation loss 0.11456121504306793 Accuracy 0.621666669845581\n",
      "Output tensor([[0.6226],\n",
      "        [0.4655]])\n",
      "Iteration 33090 Training loss 0.11736646294593811 Validation loss 0.11455383151769638 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.3490],\n",
      "        [0.4925]])\n",
      "Iteration 33100 Training loss 0.11573523283004761 Validation loss 0.11456485092639923 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.6131],\n",
      "        [0.3942]])\n",
      "Iteration 33110 Training loss 0.11623059213161469 Validation loss 0.11455006897449493 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.1951],\n",
      "        [0.6782]])\n",
      "Iteration 33120 Training loss 0.11751041561365128 Validation loss 0.11455067992210388 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.5887],\n",
      "        [0.6246]])\n",
      "Iteration 33130 Training loss 0.11515472084283829 Validation loss 0.11454232782125473 Accuracy 0.621666669845581\n",
      "Output tensor([[0.3856],\n",
      "        [0.6463]])\n",
      "Iteration 33140 Training loss 0.11633830517530441 Validation loss 0.11455566436052322 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.4171],\n",
      "        [0.6284]])\n",
      "Iteration 33150 Training loss 0.11553247272968292 Validation loss 0.11455001682043076 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.5422],\n",
      "        [0.7645]])\n",
      "Iteration 33160 Training loss 0.11717354506254196 Validation loss 0.11452986299991608 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4421],\n",
      "        [0.5178]])\n",
      "Iteration 33170 Training loss 0.11687947809696198 Validation loss 0.11456213146448135 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5199],\n",
      "        [0.6717]])\n",
      "Iteration 33180 Training loss 0.11519040912389755 Validation loss 0.11455578356981277 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.4723],\n",
      "        [0.6558]])\n",
      "Iteration 33190 Training loss 0.1156051829457283 Validation loss 0.11453893780708313 Accuracy 0.621833324432373\n",
      "Output tensor([[0.5829],\n",
      "        [0.5200]])\n",
      "Iteration 33200 Training loss 0.11520734429359436 Validation loss 0.11457062512636185 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.4207],\n",
      "        [0.2257]])\n",
      "Iteration 33210 Training loss 0.11593542248010635 Validation loss 0.11461655795574188 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.5902],\n",
      "        [0.5843]])\n",
      "Iteration 33220 Training loss 0.11635690182447433 Validation loss 0.11458158493041992 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.3578],\n",
      "        [0.5286]])\n",
      "Iteration 33230 Training loss 0.11422180384397507 Validation loss 0.11455754935741425 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.6291],\n",
      "        [0.5477]])\n",
      "Iteration 33240 Training loss 0.11591318994760513 Validation loss 0.1145675852894783 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.4539],\n",
      "        [0.4735]])\n",
      "Iteration 33250 Training loss 0.11696624010801315 Validation loss 0.11457885801792145 Accuracy 0.6196666955947876\n",
      "Output tensor([[0.6082],\n",
      "        [0.8616]])\n",
      "Iteration 33260 Training loss 0.1155715212225914 Validation loss 0.11465826630592346 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.1827],\n",
      "        [0.4885]])\n",
      "Iteration 33270 Training loss 0.11534469574689865 Validation loss 0.11456579715013504 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.4218],\n",
      "        [0.5459]])\n",
      "Iteration 33280 Training loss 0.11662308871746063 Validation loss 0.11454856395721436 Accuracy 0.621666669845581\n",
      "Output tensor([[0.6164],\n",
      "        [0.4210]])\n",
      "Iteration 33290 Training loss 0.11678829044103622 Validation loss 0.11458966135978699 Accuracy 0.6203333139419556\n",
      "Output tensor([[0.6252],\n",
      "        [0.6522]])\n",
      "Iteration 33300 Training loss 0.11577252298593521 Validation loss 0.11459957808256149 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.6621],\n",
      "        [0.5919]])\n",
      "Iteration 33310 Training loss 0.11599330604076385 Validation loss 0.11454405635595322 Accuracy 0.621666669845581\n",
      "Output tensor([[0.1828],\n",
      "        [0.6318]])\n",
      "Iteration 33320 Training loss 0.11790674179792404 Validation loss 0.11453854292631149 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.5638],\n",
      "        [0.6175]])\n",
      "Iteration 33330 Training loss 0.11652763187885284 Validation loss 0.11454304307699203 Accuracy 0.621999979019165\n",
      "Output tensor([[0.5351],\n",
      "        [0.5991]])\n",
      "Iteration 33340 Training loss 0.11522406339645386 Validation loss 0.11452808976173401 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5368],\n",
      "        [0.5759]])\n",
      "Iteration 33350 Training loss 0.11689391732215881 Validation loss 0.11452850699424744 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.5290],\n",
      "        [0.7501]])\n",
      "Iteration 33360 Training loss 0.11626684665679932 Validation loss 0.11453480273485184 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.4784],\n",
      "        [0.5740]])\n",
      "Iteration 33370 Training loss 0.11603736132383347 Validation loss 0.11456950008869171 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4512],\n",
      "        [0.2052]])\n",
      "Iteration 33380 Training loss 0.11602406948804855 Validation loss 0.11450551450252533 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.3617],\n",
      "        [0.4696]])\n",
      "Iteration 33390 Training loss 0.11495183408260345 Validation loss 0.1145140677690506 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.5314],\n",
      "        [0.2955]])\n",
      "Iteration 33400 Training loss 0.115633524954319 Validation loss 0.114506796002388 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.5171],\n",
      "        [0.6038]])\n",
      "Iteration 33410 Training loss 0.11584066599607468 Validation loss 0.11470722407102585 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.4837],\n",
      "        [0.5385]])\n",
      "Iteration 33420 Training loss 0.11634442955255508 Validation loss 0.11449920386075974 Accuracy 0.621666669845581\n",
      "Output tensor([[0.4888],\n",
      "        [0.6481]])\n",
      "Iteration 33430 Training loss 0.116659015417099 Validation loss 0.11457643657922745 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.4530],\n",
      "        [0.4116]])\n",
      "Iteration 33440 Training loss 0.11616547405719757 Validation loss 0.11452217400074005 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5886],\n",
      "        [0.5467]])\n",
      "Iteration 33450 Training loss 0.11714938282966614 Validation loss 0.11450944095849991 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5941],\n",
      "        [0.5083]])\n",
      "Iteration 33460 Training loss 0.11703003197908401 Validation loss 0.1145021840929985 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.6537],\n",
      "        [0.3498]])\n",
      "Iteration 33470 Training loss 0.11555318534374237 Validation loss 0.1145215854048729 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.3240],\n",
      "        [0.4340]])\n",
      "Iteration 33480 Training loss 0.1169431135058403 Validation loss 0.11454471945762634 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.6002],\n",
      "        [0.2653]])\n",
      "Iteration 33490 Training loss 0.1163698360323906 Validation loss 0.11451695114374161 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.5021],\n",
      "        [0.6052]])\n",
      "Iteration 33500 Training loss 0.11626735329627991 Validation loss 0.11454358696937561 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.6321],\n",
      "        [0.6684]])\n",
      "Iteration 33510 Training loss 0.1159745305776596 Validation loss 0.11453960835933685 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.2880],\n",
      "        [0.3206]])\n",
      "Iteration 33520 Training loss 0.11703884601593018 Validation loss 0.11454207450151443 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.3439],\n",
      "        [0.5090]])\n",
      "Iteration 33530 Training loss 0.11556758731603622 Validation loss 0.11455351114273071 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.2950],\n",
      "        [0.2162]])\n",
      "Iteration 33540 Training loss 0.1156984344124794 Validation loss 0.11462406069040298 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.5998],\n",
      "        [0.4931]])\n",
      "Iteration 33550 Training loss 0.11718326807022095 Validation loss 0.1145416647195816 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.6453],\n",
      "        [0.3133]])\n",
      "Iteration 33560 Training loss 0.11618345230817795 Validation loss 0.11455235630273819 Accuracy 0.6198333501815796\n",
      "Output tensor([[0.4598],\n",
      "        [0.5650]])\n",
      "Iteration 33570 Training loss 0.1162056103348732 Validation loss 0.11452186852693558 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4458],\n",
      "        [0.2857]])\n",
      "Iteration 33580 Training loss 0.11641503870487213 Validation loss 0.11464916914701462 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.3625],\n",
      "        [0.3327]])\n",
      "Iteration 33590 Training loss 0.11623575538396835 Validation loss 0.11454471945762634 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5527],\n",
      "        [0.2171]])\n",
      "Iteration 33600 Training loss 0.1164812222123146 Validation loss 0.11453647166490555 Accuracy 0.621666669845581\n",
      "Output tensor([[0.2371],\n",
      "        [0.6595]])\n",
      "Iteration 33610 Training loss 0.11651285737752914 Validation loss 0.11460375785827637 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.1810],\n",
      "        [0.6338]])\n",
      "Iteration 33620 Training loss 0.11602156609296799 Validation loss 0.11456393450498581 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.4599],\n",
      "        [0.5435]])\n",
      "Iteration 33630 Training loss 0.11506063491106033 Validation loss 0.11454898118972778 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.2823],\n",
      "        [0.4819]])\n",
      "Iteration 33640 Training loss 0.11526450514793396 Validation loss 0.11455128341913223 Accuracy 0.621999979019165\n",
      "Output tensor([[0.3489],\n",
      "        [0.5195]])\n",
      "Iteration 33650 Training loss 0.1166132390499115 Validation loss 0.11452262103557587 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.4152],\n",
      "        [0.5466]])\n",
      "Iteration 33660 Training loss 0.11674819886684418 Validation loss 0.11458349972963333 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4502],\n",
      "        [0.6043]])\n",
      "Iteration 33670 Training loss 0.11597371101379395 Validation loss 0.11452465504407883 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.4740],\n",
      "        [0.4747]])\n",
      "Iteration 33680 Training loss 0.1164337545633316 Validation loss 0.11452028155326843 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5127],\n",
      "        [0.5629]])\n",
      "Iteration 33690 Training loss 0.1172012910246849 Validation loss 0.11454663425683975 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4663],\n",
      "        [0.5466]])\n",
      "Iteration 33700 Training loss 0.11555175483226776 Validation loss 0.11452040821313858 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.3672],\n",
      "        [0.6326]])\n",
      "Iteration 33710 Training loss 0.11614637076854706 Validation loss 0.11452729254961014 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.5420],\n",
      "        [0.5801]])\n",
      "Iteration 33720 Training loss 0.11611617356538773 Validation loss 0.11452604085206985 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.6056],\n",
      "        [0.3699]])\n",
      "Iteration 33730 Training loss 0.11739140003919601 Validation loss 0.11451862752437592 Accuracy 0.621833324432373\n",
      "Output tensor([[0.5366],\n",
      "        [0.4156]])\n",
      "Iteration 33740 Training loss 0.11554152518510818 Validation loss 0.11451428383588791 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.4950],\n",
      "        [0.4210]])\n",
      "Iteration 33750 Training loss 0.11590754985809326 Validation loss 0.11453799158334732 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4356],\n",
      "        [0.5246]])\n",
      "Iteration 33760 Training loss 0.11773177236318588 Validation loss 0.11456524580717087 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.7261],\n",
      "        [0.7879]])\n",
      "Iteration 33770 Training loss 0.1165241077542305 Validation loss 0.1145031526684761 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.3773],\n",
      "        [0.2848]])\n",
      "Iteration 33780 Training loss 0.11711346358060837 Validation loss 0.11460383981466293 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.6412],\n",
      "        [0.3981]])\n",
      "Iteration 33790 Training loss 0.11656495928764343 Validation loss 0.11454084515571594 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.6455],\n",
      "        [0.4108]])\n",
      "Iteration 33800 Training loss 0.11598773300647736 Validation loss 0.1145375669002533 Accuracy 0.621833324432373\n",
      "Output tensor([[0.6271],\n",
      "        [0.4441]])\n",
      "Iteration 33810 Training loss 0.1158825159072876 Validation loss 0.11452891677618027 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.5153],\n",
      "        [0.6073]])\n",
      "Iteration 33820 Training loss 0.11620976030826569 Validation loss 0.11449747532606125 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.3986],\n",
      "        [0.3664]])\n",
      "Iteration 33830 Training loss 0.11622673273086548 Validation loss 0.11454784870147705 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.6095],\n",
      "        [0.3453]])\n",
      "Iteration 33840 Training loss 0.11748069524765015 Validation loss 0.11453614383935928 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.6033],\n",
      "        [0.2541]])\n",
      "Iteration 33850 Training loss 0.11678210645914078 Validation loss 0.11453496664762497 Accuracy 0.621666669845581\n",
      "Output tensor([[0.4814],\n",
      "        [0.6435]])\n",
      "Iteration 33860 Training loss 0.11445804685354233 Validation loss 0.11450891941785812 Accuracy 0.6201666593551636\n",
      "Output tensor([[0.5952],\n",
      "        [0.5686]])\n",
      "Iteration 33870 Training loss 0.11592936515808105 Validation loss 0.11455875635147095 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4027],\n",
      "        [0.3593]])\n",
      "Iteration 33880 Training loss 0.11635109782218933 Validation loss 0.1145453080534935 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.6508],\n",
      "        [0.3661]])\n",
      "Iteration 33890 Training loss 0.11673915386199951 Validation loss 0.11455151438713074 Accuracy 0.621999979019165\n",
      "Output tensor([[0.5972],\n",
      "        [0.3617]])\n",
      "Iteration 33900 Training loss 0.11541932821273804 Validation loss 0.11476163566112518 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.6489],\n",
      "        [0.5009]])\n",
      "Iteration 33910 Training loss 0.11572206765413284 Validation loss 0.11450904607772827 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.3798],\n",
      "        [0.4582]])\n",
      "Iteration 33920 Training loss 0.11606789380311966 Validation loss 0.11450598388910294 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.4545],\n",
      "        [0.6415]])\n",
      "Iteration 33930 Training loss 0.11659292876720428 Validation loss 0.11460378766059875 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.6265],\n",
      "        [0.5456]])\n",
      "Iteration 33940 Training loss 0.11529824882745743 Validation loss 0.11477327346801758 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.6938],\n",
      "        [0.4785]])\n",
      "Iteration 33950 Training loss 0.11701719462871552 Validation loss 0.1145033985376358 Accuracy 0.621999979019165\n",
      "Output tensor([[0.6134],\n",
      "        [0.5205]])\n",
      "Iteration 33960 Training loss 0.11655566096305847 Validation loss 0.11448901146650314 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4474],\n",
      "        [0.6032]])\n",
      "Iteration 33970 Training loss 0.11540881544351578 Validation loss 0.11449794471263885 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.5118],\n",
      "        [0.4197]])\n",
      "Iteration 33980 Training loss 0.11591365188360214 Validation loss 0.11451047658920288 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4764],\n",
      "        [0.6677]])\n",
      "Iteration 33990 Training loss 0.1163448914885521 Validation loss 0.11446121335029602 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4435],\n",
      "        [0.5313]])\n",
      "Iteration 34000 Training loss 0.1160450130701065 Validation loss 0.11446467787027359 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.1803],\n",
      "        [0.2751]])\n",
      "Iteration 34010 Training loss 0.11491755396127701 Validation loss 0.1144910678267479 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.2538],\n",
      "        [0.5652]])\n",
      "Iteration 34020 Training loss 0.11536305397748947 Validation loss 0.11447753757238388 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.2921],\n",
      "        [0.2353]])\n",
      "Iteration 34030 Training loss 0.11517173051834106 Validation loss 0.11450927704572678 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4336],\n",
      "        [0.3280]])\n",
      "Iteration 34040 Training loss 0.11658786982297897 Validation loss 0.11448556184768677 Accuracy 0.621666669845581\n",
      "Output tensor([[0.6842],\n",
      "        [0.2975]])\n",
      "Iteration 34050 Training loss 0.11590222269296646 Validation loss 0.11448695510625839 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.1685],\n",
      "        [0.6042]])\n",
      "Iteration 34060 Training loss 0.11530979722738266 Validation loss 0.1145162582397461 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.5811],\n",
      "        [0.5217]])\n",
      "Iteration 34070 Training loss 0.11483544856309891 Validation loss 0.11446856707334518 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.5362],\n",
      "        [0.6140]])\n",
      "Iteration 34080 Training loss 0.11657749116420746 Validation loss 0.11451877653598785 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.4501],\n",
      "        [0.5479]])\n",
      "Iteration 34090 Training loss 0.11651277542114258 Validation loss 0.11449044942855835 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.6228],\n",
      "        [0.7345]])\n",
      "Iteration 34100 Training loss 0.11515293270349503 Validation loss 0.11458134651184082 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.3735],\n",
      "        [0.5343]])\n",
      "Iteration 34110 Training loss 0.11686895042657852 Validation loss 0.11451428383588791 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.5130],\n",
      "        [0.4644]])\n",
      "Iteration 34120 Training loss 0.11646854877471924 Validation loss 0.11455563455820084 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.7169],\n",
      "        [0.5105]])\n",
      "Iteration 34130 Training loss 0.11734804511070251 Validation loss 0.11451759934425354 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.3771],\n",
      "        [0.5008]])\n",
      "Iteration 34140 Training loss 0.11606701463460922 Validation loss 0.11446216702461243 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.5534],\n",
      "        [0.7263]])\n",
      "Iteration 34150 Training loss 0.11600235104560852 Validation loss 0.11444919556379318 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.7115],\n",
      "        [0.5322]])\n",
      "Iteration 34160 Training loss 0.11496089398860931 Validation loss 0.11465910077095032 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.5280],\n",
      "        [0.7312]])\n",
      "Iteration 34170 Training loss 0.11430928856134415 Validation loss 0.11471066623926163 Accuracy 0.621833324432373\n",
      "Output tensor([[0.5708],\n",
      "        [0.7050]])\n",
      "Iteration 34180 Training loss 0.11521079391241074 Validation loss 0.11470157653093338 Accuracy 0.621666669845581\n",
      "Output tensor([[0.7239],\n",
      "        [0.6307]])\n",
      "Iteration 34190 Training loss 0.11555375903844833 Validation loss 0.11448082327842712 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5573],\n",
      "        [0.4457]])\n",
      "Iteration 34200 Training loss 0.11687364429235458 Validation loss 0.11450392752885818 Accuracy 0.621666669845581\n",
      "Output tensor([[0.3750],\n",
      "        [0.5611]])\n",
      "Iteration 34210 Training loss 0.11742851883172989 Validation loss 0.1145688071846962 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.5021],\n",
      "        [0.4219]])\n",
      "Iteration 34220 Training loss 0.11647428572177887 Validation loss 0.11448551714420319 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.2691],\n",
      "        [0.4112]])\n",
      "Iteration 34230 Training loss 0.1156194880604744 Validation loss 0.11448545008897781 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5272],\n",
      "        [0.4507]])\n",
      "Iteration 34240 Training loss 0.11573384702205658 Validation loss 0.11447709053754807 Accuracy 0.621666669845581\n",
      "Output tensor([[0.4644],\n",
      "        [0.6223]])\n",
      "Iteration 34250 Training loss 0.11599686741828918 Validation loss 0.11448527127504349 Accuracy 0.621666669845581\n",
      "Output tensor([[0.3227],\n",
      "        [0.6128]])\n",
      "Iteration 34260 Training loss 0.11639698594808578 Validation loss 0.11450502276420593 Accuracy 0.621666669845581\n",
      "Output tensor([[0.4809],\n",
      "        [0.5754]])\n",
      "Iteration 34270 Training loss 0.11592870205640793 Validation loss 0.11447121202945709 Accuracy 0.621999979019165\n",
      "Output tensor([[0.3006],\n",
      "        [0.5961]])\n",
      "Iteration 34280 Training loss 0.11546952277421951 Validation loss 0.1146598532795906 Accuracy 0.621833324432373\n",
      "Output tensor([[0.2322],\n",
      "        [0.7017]])\n",
      "Iteration 34290 Training loss 0.1166607141494751 Validation loss 0.11449345201253891 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.5872],\n",
      "        [0.8364]])\n",
      "Iteration 34300 Training loss 0.1147608608007431 Validation loss 0.11463714390993118 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.5614],\n",
      "        [0.5178]])\n",
      "Iteration 34310 Training loss 0.11572249233722687 Validation loss 0.11453458666801453 Accuracy 0.621833324432373\n",
      "Output tensor([[0.1871],\n",
      "        [0.4454]])\n",
      "Iteration 34320 Training loss 0.11677148193120956 Validation loss 0.11461351811885834 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.6854],\n",
      "        [0.5891]])\n",
      "Iteration 34330 Training loss 0.11706819385290146 Validation loss 0.11451653391122818 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4097],\n",
      "        [0.5832]])\n",
      "Iteration 34340 Training loss 0.11589982360601425 Validation loss 0.1145380362868309 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4388],\n",
      "        [0.4158]])\n",
      "Iteration 34350 Training loss 0.11496002227067947 Validation loss 0.11452807486057281 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.4838],\n",
      "        [0.5438]])\n",
      "Iteration 34360 Training loss 0.11784161627292633 Validation loss 0.11458514630794525 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.5783],\n",
      "        [0.4561]])\n",
      "Iteration 34370 Training loss 0.11753621697425842 Validation loss 0.114564910531044 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.5183],\n",
      "        [0.3550]])\n",
      "Iteration 34380 Training loss 0.11624859273433685 Validation loss 0.11449503898620605 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.4185],\n",
      "        [0.6842]])\n",
      "Iteration 34390 Training loss 0.11701600253582001 Validation loss 0.11455552279949188 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.5555],\n",
      "        [0.5979]])\n",
      "Iteration 34400 Training loss 0.11612378060817719 Validation loss 0.11452339589595795 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.2481],\n",
      "        [0.3863]])\n",
      "Iteration 34410 Training loss 0.1168157160282135 Validation loss 0.11449556797742844 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.5123],\n",
      "        [0.3394]])\n",
      "Iteration 34420 Training loss 0.11716237664222717 Validation loss 0.11449214816093445 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.5580],\n",
      "        [0.4554]])\n",
      "Iteration 34430 Training loss 0.11671654134988785 Validation loss 0.11448299139738083 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.6985],\n",
      "        [0.3625]])\n",
      "Iteration 34440 Training loss 0.11659686267375946 Validation loss 0.11463657766580582 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5363],\n",
      "        [0.4884]])\n",
      "Iteration 34450 Training loss 0.11619759351015091 Validation loss 0.11445242166519165 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.6697],\n",
      "        [0.5150]])\n",
      "Iteration 34460 Training loss 0.11531629413366318 Validation loss 0.11454663425683975 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.6770],\n",
      "        [0.4579]])\n",
      "Iteration 34470 Training loss 0.11624211817979813 Validation loss 0.11447084695100784 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.5384],\n",
      "        [0.6603]])\n",
      "Iteration 34480 Training loss 0.11700475215911865 Validation loss 0.11444103717803955 Accuracy 0.621666669845581\n",
      "Output tensor([[0.4557],\n",
      "        [0.3906]])\n",
      "Iteration 34490 Training loss 0.11539976298809052 Validation loss 0.11445145308971405 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5282],\n",
      "        [0.4208]])\n",
      "Iteration 34500 Training loss 0.11484561115503311 Validation loss 0.11444787681102753 Accuracy 0.621666669845581\n",
      "Output tensor([[0.4705],\n",
      "        [0.5290]])\n",
      "Iteration 34510 Training loss 0.1170305460691452 Validation loss 0.11443988978862762 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.6724],\n",
      "        [0.4146]])\n",
      "Iteration 34520 Training loss 0.11535831540822983 Validation loss 0.1145123615860939 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.6110],\n",
      "        [0.6776]])\n",
      "Iteration 34530 Training loss 0.11586696654558182 Validation loss 0.11451536417007446 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.5154],\n",
      "        [0.4491]])\n",
      "Iteration 34540 Training loss 0.11576712876558304 Validation loss 0.1145268976688385 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.2444],\n",
      "        [0.4819]])\n",
      "Iteration 34550 Training loss 0.11626055091619492 Validation loss 0.11448604613542557 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.6882],\n",
      "        [0.6155]])\n",
      "Iteration 34560 Training loss 0.11461925506591797 Validation loss 0.11448951065540314 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4299],\n",
      "        [0.4163]])\n",
      "Iteration 34570 Training loss 0.1173744723200798 Validation loss 0.11449563503265381 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.7494],\n",
      "        [0.4638]])\n",
      "Iteration 34580 Training loss 0.11592795699834824 Validation loss 0.11446508020162582 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.6011],\n",
      "        [0.5962]])\n",
      "Iteration 34590 Training loss 0.11674942821264267 Validation loss 0.11446540057659149 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.5947],\n",
      "        [0.5643]])\n",
      "Iteration 34600 Training loss 0.11654242128133774 Validation loss 0.11445196717977524 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.5400],\n",
      "        [0.5490]])\n",
      "Iteration 34610 Training loss 0.11569666862487793 Validation loss 0.11443693190813065 Accuracy 0.621666669845581\n",
      "Output tensor([[0.6885],\n",
      "        [0.5896]])\n",
      "Iteration 34620 Training loss 0.11521212011575699 Validation loss 0.11443167924880981 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.3720],\n",
      "        [0.5486]])\n",
      "Iteration 34630 Training loss 0.11645239591598511 Validation loss 0.11444610357284546 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4896],\n",
      "        [0.5662]])\n",
      "Iteration 34640 Training loss 0.11634106189012527 Validation loss 0.11448756605386734 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.2588],\n",
      "        [0.4873]])\n",
      "Iteration 34650 Training loss 0.11705595254898071 Validation loss 0.11444397270679474 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.4158],\n",
      "        [0.3534]])\n",
      "Iteration 34660 Training loss 0.11618403345346451 Validation loss 0.1144247055053711 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.4512],\n",
      "        [0.2188]])\n",
      "Iteration 34670 Training loss 0.11630621552467346 Validation loss 0.11442750692367554 Accuracy 0.621999979019165\n",
      "Output tensor([[0.4946],\n",
      "        [0.4857]])\n",
      "Iteration 34680 Training loss 0.11549149453639984 Validation loss 0.11447200179100037 Accuracy 0.621999979019165\n",
      "Output tensor([[0.6034],\n",
      "        [0.6566]])\n",
      "Iteration 34690 Training loss 0.11541703343391418 Validation loss 0.11442101746797562 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.3279],\n",
      "        [0.6408]])\n",
      "Iteration 34700 Training loss 0.11764933168888092 Validation loss 0.11443943530321121 Accuracy 0.621999979019165\n",
      "Output tensor([[0.8423],\n",
      "        [0.5416]])\n",
      "Iteration 34710 Training loss 0.1156529113650322 Validation loss 0.11442028731107712 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.6832],\n",
      "        [0.3853]])\n",
      "Iteration 34720 Training loss 0.11468327045440674 Validation loss 0.11446386575698853 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5446],\n",
      "        [0.6148]])\n",
      "Iteration 34730 Training loss 0.11647298187017441 Validation loss 0.11444941908121109 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4970],\n",
      "        [0.3332]])\n",
      "Iteration 34740 Training loss 0.11689823120832443 Validation loss 0.11449045687913895 Accuracy 0.621999979019165\n",
      "Output tensor([[0.5401],\n",
      "        [0.6379]])\n",
      "Iteration 34750 Training loss 0.11558549106121063 Validation loss 0.11445683985948563 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.5669],\n",
      "        [0.4811]])\n",
      "Iteration 34760 Training loss 0.11581938713788986 Validation loss 0.11445607244968414 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5290],\n",
      "        [0.5642]])\n",
      "Iteration 34770 Training loss 0.11533813178539276 Validation loss 0.11446013301610947 Accuracy 0.621666669845581\n",
      "Output tensor([[0.6490],\n",
      "        [0.6084]])\n",
      "Iteration 34780 Training loss 0.11610913276672363 Validation loss 0.11446293443441391 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5627],\n",
      "        [0.3898]])\n",
      "Iteration 34790 Training loss 0.11710415780544281 Validation loss 0.11453478038311005 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4904],\n",
      "        [0.6077]])\n",
      "Iteration 34800 Training loss 0.11639528721570969 Validation loss 0.11443279683589935 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.6608],\n",
      "        [0.3683]])\n",
      "Iteration 34810 Training loss 0.11497896164655685 Validation loss 0.11443717777729034 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.5046],\n",
      "        [0.3143]])\n",
      "Iteration 34820 Training loss 0.11616677045822144 Validation loss 0.11441747099161148 Accuracy 0.621666669845581\n",
      "Output tensor([[0.6147],\n",
      "        [0.6073]])\n",
      "Iteration 34830 Training loss 0.11635620892047882 Validation loss 0.11448649317026138 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.5182],\n",
      "        [0.4499]])\n",
      "Iteration 34840 Training loss 0.11593321710824966 Validation loss 0.11452097445726395 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4851],\n",
      "        [0.5900]])\n",
      "Iteration 34850 Training loss 0.11563034355640411 Validation loss 0.1144966334104538 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.4621],\n",
      "        [0.6326]])\n",
      "Iteration 34860 Training loss 0.11602828651666641 Validation loss 0.1145211011171341 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.6313],\n",
      "        [0.3501]])\n",
      "Iteration 34870 Training loss 0.11613822728395462 Validation loss 0.11449871957302094 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.6540],\n",
      "        [0.3957]])\n",
      "Iteration 34880 Training loss 0.11647143214941025 Validation loss 0.11461745947599411 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.6301],\n",
      "        [0.7751]])\n",
      "Iteration 34890 Training loss 0.11604388058185577 Validation loss 0.11457040160894394 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.6208],\n",
      "        [0.5249]])\n",
      "Iteration 34900 Training loss 0.11563314497470856 Validation loss 0.11448103934526443 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5247],\n",
      "        [0.3572]])\n",
      "Iteration 34910 Training loss 0.11573384702205658 Validation loss 0.11450272798538208 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5858],\n",
      "        [0.4000]])\n",
      "Iteration 34920 Training loss 0.11601967364549637 Validation loss 0.11452966183423996 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.4163],\n",
      "        [0.5251]])\n",
      "Iteration 34930 Training loss 0.11609412729740143 Validation loss 0.11449919641017914 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4088],\n",
      "        [0.6979]])\n",
      "Iteration 34940 Training loss 0.11559362709522247 Validation loss 0.11449544131755829 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.3904],\n",
      "        [0.2934]])\n",
      "Iteration 34950 Training loss 0.1168569028377533 Validation loss 0.11448390781879425 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.0953],\n",
      "        [0.3439]])\n",
      "Iteration 34960 Training loss 0.11610738188028336 Validation loss 0.1144535094499588 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.4457],\n",
      "        [0.7219]])\n",
      "Iteration 34970 Training loss 0.11694511771202087 Validation loss 0.11443880200386047 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.7261],\n",
      "        [0.4241]])\n",
      "Iteration 34980 Training loss 0.11671590059995651 Validation loss 0.11444128304719925 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.6178],\n",
      "        [0.5028]])\n",
      "Iteration 34990 Training loss 0.11628402024507523 Validation loss 0.11445233225822449 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.6124],\n",
      "        [0.6387]])\n",
      "Iteration 35000 Training loss 0.11531154066324234 Validation loss 0.1144319474697113 Accuracy 0.621666669845581\n",
      "Output tensor([[0.3423],\n",
      "        [0.5459]])\n",
      "Iteration 35010 Training loss 0.1164613664150238 Validation loss 0.11450780183076859 Accuracy 0.621999979019165\n",
      "Output tensor([[0.3575],\n",
      "        [0.1025]])\n",
      "Iteration 35020 Training loss 0.11446347832679749 Validation loss 0.11447940021753311 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4222],\n",
      "        [0.6447]])\n",
      "Iteration 35030 Training loss 0.11516941338777542 Validation loss 0.11445322632789612 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.5853],\n",
      "        [0.5023]])\n",
      "Iteration 35040 Training loss 0.11628282815217972 Validation loss 0.11448132246732712 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.4703],\n",
      "        [0.3869]])\n",
      "Iteration 35050 Training loss 0.11640654504299164 Validation loss 0.11444789916276932 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.6077],\n",
      "        [0.5994]])\n",
      "Iteration 35060 Training loss 0.11587989330291748 Validation loss 0.11447076499462128 Accuracy 0.621666669845581\n",
      "Output tensor([[0.4757],\n",
      "        [0.5593]])\n",
      "Iteration 35070 Training loss 0.11658364534378052 Validation loss 0.11449135094881058 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.6453],\n",
      "        [0.5261]])\n",
      "Iteration 35080 Training loss 0.11678225547075272 Validation loss 0.11455149203538895 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.4814],\n",
      "        [0.6195]])\n",
      "Iteration 35090 Training loss 0.1158137395977974 Validation loss 0.11467554420232773 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5863],\n",
      "        [0.4377]])\n",
      "Iteration 35100 Training loss 0.11426887661218643 Validation loss 0.11450981348752975 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.5535],\n",
      "        [0.4703]])\n",
      "Iteration 35110 Training loss 0.11682206392288208 Validation loss 0.11449431627988815 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5100],\n",
      "        [0.5782]])\n",
      "Iteration 35120 Training loss 0.11594803631305695 Validation loss 0.11445512622594833 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.4666],\n",
      "        [0.5944]])\n",
      "Iteration 35130 Training loss 0.11663901060819626 Validation loss 0.11463914066553116 Accuracy 0.621833324432373\n",
      "Output tensor([[0.3160],\n",
      "        [0.4872]])\n",
      "Iteration 35140 Training loss 0.11697807908058167 Validation loss 0.11447567492723465 Accuracy 0.6206666827201843\n",
      "Output tensor([[0.0868],\n",
      "        [0.7078]])\n",
      "Iteration 35150 Training loss 0.11644692718982697 Validation loss 0.11445826292037964 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.6518],\n",
      "        [0.2615]])\n",
      "Iteration 35160 Training loss 0.11739189922809601 Validation loss 0.11459407210350037 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4758],\n",
      "        [0.4574]])\n",
      "Iteration 35170 Training loss 0.11668407917022705 Validation loss 0.1145058274269104 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5906],\n",
      "        [0.4102]])\n",
      "Iteration 35180 Training loss 0.1157824918627739 Validation loss 0.11442763358354568 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.5101],\n",
      "        [0.4313]])\n",
      "Iteration 35190 Training loss 0.11668967455625534 Validation loss 0.11445801705121994 Accuracy 0.621999979019165\n",
      "Output tensor([[0.2440],\n",
      "        [0.4928]])\n",
      "Iteration 35200 Training loss 0.11528560519218445 Validation loss 0.11447940766811371 Accuracy 0.6208333373069763\n",
      "Output tensor([[0.5275],\n",
      "        [0.4393]])\n",
      "Iteration 35210 Training loss 0.11473724246025085 Validation loss 0.114481121301651 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.4643],\n",
      "        [0.5771]])\n",
      "Iteration 35220 Training loss 0.116409532725811 Validation loss 0.11452100425958633 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5453],\n",
      "        [0.4509]])\n",
      "Iteration 35230 Training loss 0.11657670140266418 Validation loss 0.1144634261727333 Accuracy 0.6209999918937683\n",
      "Output tensor([[0.6625],\n",
      "        [0.2716]])\n",
      "Iteration 35240 Training loss 0.1157311499118805 Validation loss 0.11452846229076385 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.4846],\n",
      "        [0.4559]])\n",
      "Iteration 35250 Training loss 0.11612161248922348 Validation loss 0.11454187333583832 Accuracy 0.621666669845581\n",
      "Output tensor([[0.4471],\n",
      "        [0.6029]])\n",
      "Iteration 35260 Training loss 0.11455613374710083 Validation loss 0.11449722945690155 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.6344],\n",
      "        [0.3728]])\n",
      "Iteration 35270 Training loss 0.11502689868211746 Validation loss 0.114461749792099 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.6056],\n",
      "        [0.3633]])\n",
      "Iteration 35280 Training loss 0.1156625971198082 Validation loss 0.11443661153316498 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4236],\n",
      "        [0.6385]])\n",
      "Iteration 35290 Training loss 0.1156034916639328 Validation loss 0.11443588137626648 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.3281],\n",
      "        [0.5766]])\n",
      "Iteration 35300 Training loss 0.11577489972114563 Validation loss 0.11450543254613876 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5217],\n",
      "        [0.2846]])\n",
      "Iteration 35310 Training loss 0.1162925586104393 Validation loss 0.11446094512939453 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.5788],\n",
      "        [0.4506]])\n",
      "Iteration 35320 Training loss 0.11556948721408844 Validation loss 0.11444242298603058 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.6215],\n",
      "        [0.4931]])\n",
      "Iteration 35330 Training loss 0.1156143769621849 Validation loss 0.11445695161819458 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4568],\n",
      "        [0.6641]])\n",
      "Iteration 35340 Training loss 0.11518682539463043 Validation loss 0.11443255841732025 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4122],\n",
      "        [0.4954]])\n",
      "Iteration 35350 Training loss 0.11647957563400269 Validation loss 0.11468183249235153 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.3142],\n",
      "        [0.3029]])\n",
      "Iteration 35360 Training loss 0.115176260471344 Validation loss 0.11444535106420517 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.6053],\n",
      "        [0.5083]])\n",
      "Iteration 35370 Training loss 0.11591488867998123 Validation loss 0.11447649449110031 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.3606],\n",
      "        [0.2090]])\n",
      "Iteration 35380 Training loss 0.11600268632173538 Validation loss 0.11443150043487549 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.5576],\n",
      "        [0.6901]])\n",
      "Iteration 35390 Training loss 0.11538273096084595 Validation loss 0.11443649977445602 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.5654],\n",
      "        [0.4535]])\n",
      "Iteration 35400 Training loss 0.11572142690420151 Validation loss 0.11447522044181824 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.6324],\n",
      "        [0.6052]])\n",
      "Iteration 35410 Training loss 0.1166011318564415 Validation loss 0.11445025354623795 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.5726],\n",
      "        [0.6134]])\n",
      "Iteration 35420 Training loss 0.11558279395103455 Validation loss 0.11446788161993027 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5325],\n",
      "        [0.4428]])\n",
      "Iteration 35430 Training loss 0.11534027010202408 Validation loss 0.11446480453014374 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5168],\n",
      "        [0.5006]])\n",
      "Iteration 35440 Training loss 0.11718474328517914 Validation loss 0.11443278193473816 Accuracy 0.621833324432373\n",
      "Output tensor([[0.6258],\n",
      "        [0.4917]])\n",
      "Iteration 35450 Training loss 0.11667263507843018 Validation loss 0.11440464109182358 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.5368],\n",
      "        [0.5887]])\n",
      "Iteration 35460 Training loss 0.1168200671672821 Validation loss 0.11447440832853317 Accuracy 0.621666669845581\n",
      "Output tensor([[0.3407],\n",
      "        [0.5018]])\n",
      "Iteration 35470 Training loss 0.1166629046201706 Validation loss 0.11444459855556488 Accuracy 0.6213333606719971\n",
      "Output tensor([[0.4283],\n",
      "        [0.4132]])\n",
      "Iteration 35480 Training loss 0.11700492352247238 Validation loss 0.11448787152767181 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.5130],\n",
      "        [0.5448]])\n",
      "Iteration 35490 Training loss 0.11596891283988953 Validation loss 0.11441576480865479 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.6324],\n",
      "        [0.3247]])\n",
      "Iteration 35500 Training loss 0.11648128181695938 Validation loss 0.11443020403385162 Accuracy 0.621833324432373\n",
      "Output tensor([[0.5931],\n",
      "        [0.3797]])\n",
      "Iteration 35510 Training loss 0.11586753278970718 Validation loss 0.11440936475992203 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.3439],\n",
      "        [0.6956]])\n",
      "Iteration 35520 Training loss 0.11555086076259613 Validation loss 0.11441103368997574 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.3649],\n",
      "        [0.5208]])\n",
      "Iteration 35530 Training loss 0.11484020948410034 Validation loss 0.11438418924808502 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.4076],\n",
      "        [0.6840]])\n",
      "Iteration 35540 Training loss 0.11599709838628769 Validation loss 0.11439548432826996 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.7017],\n",
      "        [0.4302]])\n",
      "Iteration 35550 Training loss 0.11666402965784073 Validation loss 0.11438653618097305 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.4157],\n",
      "        [0.7587]])\n",
      "Iteration 35560 Training loss 0.11522548645734787 Validation loss 0.11442236602306366 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4666],\n",
      "        [0.4768]])\n",
      "Iteration 35570 Training loss 0.1172320544719696 Validation loss 0.11441337317228317 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.3532],\n",
      "        [0.5647]])\n",
      "Iteration 35580 Training loss 0.11634800583124161 Validation loss 0.11437206715345383 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.6303],\n",
      "        [0.5841]])\n",
      "Iteration 35590 Training loss 0.11615223437547684 Validation loss 0.11439965665340424 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5139],\n",
      "        [0.6140]])\n",
      "Iteration 35600 Training loss 0.11705580353736877 Validation loss 0.11435776948928833 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.5640],\n",
      "        [0.5754]])\n",
      "Iteration 35610 Training loss 0.11549501866102219 Validation loss 0.1143963634967804 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.5605],\n",
      "        [0.4607]])\n",
      "Iteration 35620 Training loss 0.11690879613161087 Validation loss 0.11448681354522705 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4202],\n",
      "        [0.4379]])\n",
      "Iteration 35630 Training loss 0.11541011184453964 Validation loss 0.11438373476266861 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.2009],\n",
      "        [0.4594]])\n",
      "Iteration 35640 Training loss 0.11639454960823059 Validation loss 0.11441299319267273 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.3914],\n",
      "        [0.6523]])\n",
      "Iteration 35650 Training loss 0.11546024680137634 Validation loss 0.11439644545316696 Accuracy 0.621999979019165\n",
      "Output tensor([[0.3236],\n",
      "        [0.6237]])\n",
      "Iteration 35660 Training loss 0.11642839759588242 Validation loss 0.11442423611879349 Accuracy 0.621999979019165\n",
      "Output tensor([[0.5763],\n",
      "        [0.6411]])\n",
      "Iteration 35670 Training loss 0.11551796644926071 Validation loss 0.11439912766218185 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.2500],\n",
      "        [0.2534]])\n",
      "Iteration 35680 Training loss 0.11578663438558578 Validation loss 0.1143965870141983 Accuracy 0.6211666464805603\n",
      "Output tensor([[0.4245],\n",
      "        [0.2695]])\n",
      "Iteration 35690 Training loss 0.11527390778064728 Validation loss 0.11438657343387604 Accuracy 0.621999979019165\n",
      "Output tensor([[0.6013],\n",
      "        [0.2299]])\n",
      "Iteration 35700 Training loss 0.1176820769906044 Validation loss 0.11439700424671173 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.5111],\n",
      "        [0.3405]])\n",
      "Iteration 35710 Training loss 0.11648351699113846 Validation loss 0.11441630125045776 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.7927],\n",
      "        [0.5025]])\n",
      "Iteration 35720 Training loss 0.11693857610225677 Validation loss 0.11440859735012054 Accuracy 0.621666669845581\n",
      "Output tensor([[0.4142],\n",
      "        [0.1558]])\n",
      "Iteration 35730 Training loss 0.11444898694753647 Validation loss 0.11465943604707718 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.5462],\n",
      "        [0.6666]])\n",
      "Iteration 35740 Training loss 0.11657311767339706 Validation loss 0.11451835930347443 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.5641],\n",
      "        [0.3264]])\n",
      "Iteration 35750 Training loss 0.11658164113759995 Validation loss 0.11446241289377213 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4272],\n",
      "        [0.4436]])\n",
      "Iteration 35760 Training loss 0.11474361270666122 Validation loss 0.11445029824972153 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.6552],\n",
      "        [0.4979]])\n",
      "Iteration 35770 Training loss 0.11520341038703918 Validation loss 0.11461133509874344 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.3084],\n",
      "        [0.4385]])\n",
      "Iteration 35780 Training loss 0.11632492393255234 Validation loss 0.11449568718671799 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.7874],\n",
      "        [0.2295]])\n",
      "Iteration 35790 Training loss 0.11548839509487152 Validation loss 0.11455246806144714 Accuracy 0.621999979019165\n",
      "Output tensor([[0.5985],\n",
      "        [0.1974]])\n",
      "Iteration 35800 Training loss 0.1148567721247673 Validation loss 0.1143946498632431 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.1997],\n",
      "        [0.5720]])\n",
      "Iteration 35810 Training loss 0.11397729814052582 Validation loss 0.11441953480243683 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.5068],\n",
      "        [0.5123]])\n",
      "Iteration 35820 Training loss 0.11639023572206497 Validation loss 0.11438990384340286 Accuracy 0.621666669845581\n",
      "Output tensor([[0.2704],\n",
      "        [0.3756]])\n",
      "Iteration 35830 Training loss 0.1148618534207344 Validation loss 0.11448809504508972 Accuracy 0.621833324432373\n",
      "Output tensor([[0.4622],\n",
      "        [0.2931]])\n",
      "Iteration 35840 Training loss 0.11654378473758698 Validation loss 0.11439038813114166 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.5281],\n",
      "        [0.5242]])\n",
      "Iteration 35850 Training loss 0.1155855730175972 Validation loss 0.11442730575799942 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.6172],\n",
      "        [0.2356]])\n",
      "Iteration 35860 Training loss 0.11651444435119629 Validation loss 0.11442329734563828 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4931],\n",
      "        [0.4380]])\n",
      "Iteration 35870 Training loss 0.11496245115995407 Validation loss 0.11439283192157745 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.3981],\n",
      "        [0.5587]])\n",
      "Iteration 35880 Training loss 0.11494109034538269 Validation loss 0.11444667726755142 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.2316],\n",
      "        [0.6845]])\n",
      "Iteration 35890 Training loss 0.11573783308267593 Validation loss 0.1143975630402565 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.6045],\n",
      "        [0.4895]])\n",
      "Iteration 35900 Training loss 0.11561745405197144 Validation loss 0.11436283588409424 Accuracy 0.621833324432373\n",
      "Output tensor([[0.5800],\n",
      "        [0.5153]])\n",
      "Iteration 35910 Training loss 0.11549056321382523 Validation loss 0.11441298574209213 Accuracy 0.624666690826416\n",
      "Output tensor([[0.4153],\n",
      "        [0.5577]])\n",
      "Iteration 35920 Training loss 0.1151115894317627 Validation loss 0.11435115337371826 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.5500],\n",
      "        [0.4188]])\n",
      "Iteration 35930 Training loss 0.1141362339258194 Validation loss 0.11437468230724335 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.5545],\n",
      "        [0.6004]])\n",
      "Iteration 35940 Training loss 0.11608374118804932 Validation loss 0.11436980962753296 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.6006],\n",
      "        [0.3021]])\n",
      "Iteration 35950 Training loss 0.1156405657529831 Validation loss 0.11436095833778381 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.6801],\n",
      "        [0.6487]])\n",
      "Iteration 35960 Training loss 0.11557354778051376 Validation loss 0.11437982320785522 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.3482],\n",
      "        [0.5146]])\n",
      "Iteration 35970 Training loss 0.11536242812871933 Validation loss 0.11443793028593063 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4455],\n",
      "        [0.3450]])\n",
      "Iteration 35980 Training loss 0.11574315279722214 Validation loss 0.11435972154140472 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.3179],\n",
      "        [0.2504]])\n",
      "Iteration 35990 Training loss 0.116169273853302 Validation loss 0.11438468098640442 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.7740],\n",
      "        [0.5324]])\n",
      "Iteration 36000 Training loss 0.11571981012821198 Validation loss 0.11435452103614807 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.6207],\n",
      "        [0.2025]])\n",
      "Iteration 36010 Training loss 0.11638391762971878 Validation loss 0.11440961807966232 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.3610],\n",
      "        [0.5420]])\n",
      "Iteration 36020 Training loss 0.11624184250831604 Validation loss 0.11437425017356873 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4856],\n",
      "        [0.6562]])\n",
      "Iteration 36030 Training loss 0.11534183472394943 Validation loss 0.11436884850263596 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.4795],\n",
      "        [0.6105]])\n",
      "Iteration 36040 Training loss 0.11597228795289993 Validation loss 0.114368736743927 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.2638],\n",
      "        [0.4719]])\n",
      "Iteration 36050 Training loss 0.11717469245195389 Validation loss 0.11437570303678513 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.6978],\n",
      "        [0.3711]])\n",
      "Iteration 36060 Training loss 0.11562810093164444 Validation loss 0.11438104510307312 Accuracy 0.621999979019165\n",
      "Output tensor([[0.6034],\n",
      "        [0.3934]])\n",
      "Iteration 36070 Training loss 0.1178477555513382 Validation loss 0.11439819633960724 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.5742],\n",
      "        [0.3242]])\n",
      "Iteration 36080 Training loss 0.11544524878263474 Validation loss 0.11443058401346207 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.5968],\n",
      "        [0.3471]])\n",
      "Iteration 36090 Training loss 0.1162906289100647 Validation loss 0.11448822170495987 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.6486],\n",
      "        [0.5622]])\n",
      "Iteration 36100 Training loss 0.11662748456001282 Validation loss 0.11438921093940735 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5529],\n",
      "        [0.3334]])\n",
      "Iteration 36110 Training loss 0.11572190374135971 Validation loss 0.11436272412538528 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.4053],\n",
      "        [0.3667]])\n",
      "Iteration 36120 Training loss 0.11603879928588867 Validation loss 0.11436659842729568 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.8114],\n",
      "        [0.5734]])\n",
      "Iteration 36130 Training loss 0.11545170098543167 Validation loss 0.11437062919139862 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5441],\n",
      "        [0.6142]])\n",
      "Iteration 36140 Training loss 0.1170932725071907 Validation loss 0.11440230160951614 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.2429],\n",
      "        [0.3848]])\n",
      "Iteration 36150 Training loss 0.11615201830863953 Validation loss 0.1144072636961937 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.5378],\n",
      "        [0.3624]])\n",
      "Iteration 36160 Training loss 0.11519144475460052 Validation loss 0.11437103152275085 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.5260],\n",
      "        [0.4625]])\n",
      "Iteration 36170 Training loss 0.11644188314676285 Validation loss 0.11437848210334778 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.1216],\n",
      "        [0.3557]])\n",
      "Iteration 36180 Training loss 0.11677216738462448 Validation loss 0.11437546461820602 Accuracy 0.621666669845581\n",
      "Output tensor([[0.5571],\n",
      "        [0.6124]])\n",
      "Iteration 36190 Training loss 0.11659330129623413 Validation loss 0.11439229547977448 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.2828],\n",
      "        [0.4955]])\n",
      "Iteration 36200 Training loss 0.11536002904176712 Validation loss 0.11436164379119873 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5825],\n",
      "        [0.5045]])\n",
      "Iteration 36210 Training loss 0.11572416126728058 Validation loss 0.11437147110700607 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4861],\n",
      "        [0.6385]])\n",
      "Iteration 36220 Training loss 0.11668448895215988 Validation loss 0.11436832696199417 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.2732],\n",
      "        [0.6789]])\n",
      "Iteration 36230 Training loss 0.11584151536226273 Validation loss 0.1143718957901001 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.6767],\n",
      "        [0.6500]])\n",
      "Iteration 36240 Training loss 0.11835142225027084 Validation loss 0.11437159031629562 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.4553],\n",
      "        [0.6782]])\n",
      "Iteration 36250 Training loss 0.11607374250888824 Validation loss 0.11437993496656418 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.6798],\n",
      "        [0.6959]])\n",
      "Iteration 36260 Training loss 0.11600510776042938 Validation loss 0.11440057307481766 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4409],\n",
      "        [0.4437]])\n",
      "Iteration 36270 Training loss 0.11537711322307587 Validation loss 0.11442042887210846 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.1460],\n",
      "        [0.5855]])\n",
      "Iteration 36280 Training loss 0.11578592658042908 Validation loss 0.11441003531217575 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.5564],\n",
      "        [0.4955]])\n",
      "Iteration 36290 Training loss 0.11619764566421509 Validation loss 0.11438386887311935 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.5524],\n",
      "        [0.5648]])\n",
      "Iteration 36300 Training loss 0.1165337935090065 Validation loss 0.11438256502151489 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.4637],\n",
      "        [0.5218]])\n",
      "Iteration 36310 Training loss 0.1158035397529602 Validation loss 0.11440282315015793 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4142],\n",
      "        [0.5944]])\n",
      "Iteration 36320 Training loss 0.11636822670698166 Validation loss 0.11438854783773422 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.3691],\n",
      "        [0.4920]])\n",
      "Iteration 36330 Training loss 0.11526033282279968 Validation loss 0.1145002469420433 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5038],\n",
      "        [0.6091]])\n",
      "Iteration 36340 Training loss 0.11708509176969528 Validation loss 0.11439505964517593 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4744],\n",
      "        [0.5428]])\n",
      "Iteration 36350 Training loss 0.114847332239151 Validation loss 0.1144237071275711 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.6495],\n",
      "        [0.6547]])\n",
      "Iteration 36360 Training loss 0.11677085608243942 Validation loss 0.1144125908613205 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5455],\n",
      "        [0.6117]])\n",
      "Iteration 36370 Training loss 0.11632908880710602 Validation loss 0.11441073566675186 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.3363],\n",
      "        [0.5156]])\n",
      "Iteration 36380 Training loss 0.11579719185829163 Validation loss 0.11436954885721207 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.6461],\n",
      "        [0.2876]])\n",
      "Iteration 36390 Training loss 0.1153070405125618 Validation loss 0.11442817002534866 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.4592],\n",
      "        [0.3107]])\n",
      "Iteration 36400 Training loss 0.11634351313114166 Validation loss 0.11437086015939713 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.8684],\n",
      "        [0.4861]])\n",
      "Iteration 36410 Training loss 0.11508235335350037 Validation loss 0.11438672244548798 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.5204],\n",
      "        [0.3794]])\n",
      "Iteration 36420 Training loss 0.11584165692329407 Validation loss 0.11438818275928497 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.4303],\n",
      "        [0.3396]])\n",
      "Iteration 36430 Training loss 0.11674068123102188 Validation loss 0.11436230689287186 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.5994],\n",
      "        [0.5288]])\n",
      "Iteration 36440 Training loss 0.11488617956638336 Validation loss 0.11435401439666748 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4956],\n",
      "        [0.4448]])\n",
      "Iteration 36450 Training loss 0.11671046912670135 Validation loss 0.11449921876192093 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.6470],\n",
      "        [0.4878]])\n",
      "Iteration 36460 Training loss 0.11529373377561569 Validation loss 0.11436326056718826 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.3405],\n",
      "        [0.3630]])\n",
      "Iteration 36470 Training loss 0.1158432587981224 Validation loss 0.11436932533979416 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.3104],\n",
      "        [0.3471]])\n",
      "Iteration 36480 Training loss 0.11630170047283173 Validation loss 0.11442343890666962 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.5788],\n",
      "        [0.5941]])\n",
      "Iteration 36490 Training loss 0.11595044285058975 Validation loss 0.11437899619340897 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.6237],\n",
      "        [0.5435]])\n",
      "Iteration 36500 Training loss 0.1164560317993164 Validation loss 0.11437055468559265 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.3954],\n",
      "        [0.5064]])\n",
      "Iteration 36510 Training loss 0.11682786047458649 Validation loss 0.11434711515903473 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.4367],\n",
      "        [0.5309]])\n",
      "Iteration 36520 Training loss 0.11665040999650955 Validation loss 0.11437402665615082 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.4924],\n",
      "        [0.5178]])\n",
      "Iteration 36530 Training loss 0.11679130792617798 Validation loss 0.1144140437245369 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.3617],\n",
      "        [0.5375]])\n",
      "Iteration 36540 Training loss 0.11634790897369385 Validation loss 0.11446288973093033 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.5060],\n",
      "        [0.3854]])\n",
      "Iteration 36550 Training loss 0.11590726673603058 Validation loss 0.11440635472536087 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.2958],\n",
      "        [0.3589]])\n",
      "Iteration 36560 Training loss 0.11657854914665222 Validation loss 0.11440620571374893 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.6757],\n",
      "        [0.4227]])\n",
      "Iteration 36570 Training loss 0.11655447632074356 Validation loss 0.11435691267251968 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5298],\n",
      "        [0.3985]])\n",
      "Iteration 36580 Training loss 0.11614745110273361 Validation loss 0.11433561891317368 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.5442],\n",
      "        [0.2405]])\n",
      "Iteration 36590 Training loss 0.1165885329246521 Validation loss 0.11433973908424377 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.1214],\n",
      "        [0.4858]])\n",
      "Iteration 36600 Training loss 0.1164359524846077 Validation loss 0.1143520176410675 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4086],\n",
      "        [0.6204]])\n",
      "Iteration 36610 Training loss 0.11624551564455032 Validation loss 0.11436101049184799 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.7042],\n",
      "        [0.6059]])\n",
      "Iteration 36620 Training loss 0.11558441817760468 Validation loss 0.1143387109041214 Accuracy 0.624666690826416\n",
      "Output tensor([[0.3216],\n",
      "        [0.2811]])\n",
      "Iteration 36630 Training loss 0.11556793004274368 Validation loss 0.11437052488327026 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.2145],\n",
      "        [0.6176]])\n",
      "Iteration 36640 Training loss 0.11621449887752533 Validation loss 0.11432477086782455 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.3207],\n",
      "        [0.7196]])\n",
      "Iteration 36650 Training loss 0.11532063782215118 Validation loss 0.11431294679641724 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.4361],\n",
      "        [0.4529]])\n",
      "Iteration 36660 Training loss 0.11600156873464584 Validation loss 0.11430865526199341 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.2693],\n",
      "        [0.3807]])\n",
      "Iteration 36670 Training loss 0.11752844601869583 Validation loss 0.11432802677154541 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5901],\n",
      "        [0.2843]])\n",
      "Iteration 36680 Training loss 0.11578301340341568 Validation loss 0.11433384567499161 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.6204],\n",
      "        [0.5205]])\n",
      "Iteration 36690 Training loss 0.11600976437330246 Validation loss 0.11438582092523575 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.4229],\n",
      "        [0.5410]])\n",
      "Iteration 36700 Training loss 0.11705830693244934 Validation loss 0.11432939767837524 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.4938],\n",
      "        [0.4541]])\n",
      "Iteration 36710 Training loss 0.11503949016332626 Validation loss 0.11432155221700668 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.3399],\n",
      "        [0.1960]])\n",
      "Iteration 36720 Training loss 0.11621258407831192 Validation loss 0.11430693417787552 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.5122],\n",
      "        [0.4403]])\n",
      "Iteration 36730 Training loss 0.11583676189184189 Validation loss 0.11434999853372574 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4452],\n",
      "        [0.6041]])\n",
      "Iteration 36740 Training loss 0.11495871841907501 Validation loss 0.11432191729545593 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.2517],\n",
      "        [0.5823]])\n",
      "Iteration 36750 Training loss 0.11600636690855026 Validation loss 0.11431648582220078 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.5018],\n",
      "        [0.5200]])\n",
      "Iteration 36760 Training loss 0.11599614471197128 Validation loss 0.11430537700653076 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.5994],\n",
      "        [0.7482]])\n",
      "Iteration 36770 Training loss 0.1160338744521141 Validation loss 0.11431252211332321 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.3795],\n",
      "        [0.5272]])\n",
      "Iteration 36780 Training loss 0.11656735092401505 Validation loss 0.11433395743370056 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.2533],\n",
      "        [0.5816]])\n",
      "Iteration 36790 Training loss 0.11595342308282852 Validation loss 0.1143425703048706 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.6571],\n",
      "        [0.6765]])\n",
      "Iteration 36800 Training loss 0.11734773963689804 Validation loss 0.1143471747636795 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.5963],\n",
      "        [0.4645]])\n",
      "Iteration 36810 Training loss 0.11622920632362366 Validation loss 0.11440794914960861 Accuracy 0.624666690826416\n",
      "Output tensor([[0.4226],\n",
      "        [0.3680]])\n",
      "Iteration 36820 Training loss 0.11647050082683563 Validation loss 0.1143433228135109 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.6370],\n",
      "        [0.3696]])\n",
      "Iteration 36830 Training loss 0.11648691445589066 Validation loss 0.11440711468458176 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.4488],\n",
      "        [0.4437]])\n",
      "Iteration 36840 Training loss 0.11745037883520126 Validation loss 0.11435940861701965 Accuracy 0.625\n",
      "Output tensor([[0.3415],\n",
      "        [0.6299]])\n",
      "Iteration 36850 Training loss 0.11610666662454605 Validation loss 0.1143178790807724 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.7465],\n",
      "        [0.4293]])\n",
      "Iteration 36860 Training loss 0.11611493676900864 Validation loss 0.11431283503770828 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.5941],\n",
      "        [0.6520]])\n",
      "Iteration 36870 Training loss 0.11468296498060226 Validation loss 0.11429180949926376 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.4621],\n",
      "        [0.6815]])\n",
      "Iteration 36880 Training loss 0.11605163663625717 Validation loss 0.1143464669585228 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.4504],\n",
      "        [0.4762]])\n",
      "Iteration 36890 Training loss 0.1161470040678978 Validation loss 0.1143014132976532 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.5589],\n",
      "        [0.5017]])\n",
      "Iteration 36900 Training loss 0.11633636802434921 Validation loss 0.11429044604301453 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.2745],\n",
      "        [0.5907]])\n",
      "Iteration 36910 Training loss 0.11507222801446915 Validation loss 0.11431115865707397 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.1995],\n",
      "        [0.6599]])\n",
      "Iteration 36920 Training loss 0.11630521714687347 Validation loss 0.11430082470178604 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.6136],\n",
      "        [0.3874]])\n",
      "Iteration 36930 Training loss 0.11564942449331284 Validation loss 0.1143287792801857 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.4469],\n",
      "        [0.4034]])\n",
      "Iteration 36940 Training loss 0.11642315983772278 Validation loss 0.11429768800735474 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.2282],\n",
      "        [0.2108]])\n",
      "Iteration 36950 Training loss 0.11555764079093933 Validation loss 0.11427146196365356 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.6337],\n",
      "        [0.4079]])\n",
      "Iteration 36960 Training loss 0.11663375049829483 Validation loss 0.11427907645702362 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.3592],\n",
      "        [0.5232]])\n",
      "Iteration 36970 Training loss 0.11408241838216782 Validation loss 0.11430926620960236 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.6420],\n",
      "        [0.5595]])\n",
      "Iteration 36980 Training loss 0.1164475828409195 Validation loss 0.11425469815731049 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.5635],\n",
      "        [0.6152]])\n",
      "Iteration 36990 Training loss 0.1161985844373703 Validation loss 0.11428576707839966 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.4283],\n",
      "        [0.5129]])\n",
      "Iteration 37000 Training loss 0.11656831949949265 Validation loss 0.11425561457872391 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.4505],\n",
      "        [0.6680]])\n",
      "Iteration 37010 Training loss 0.11714411526918411 Validation loss 0.11427714675664902 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.2430],\n",
      "        [0.4455]])\n",
      "Iteration 37020 Training loss 0.11607043445110321 Validation loss 0.11435699462890625 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4273],\n",
      "        [0.5922]])\n",
      "Iteration 37030 Training loss 0.11465907841920853 Validation loss 0.11442207545042038 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.4890],\n",
      "        [0.6560]])\n",
      "Iteration 37040 Training loss 0.1151137501001358 Validation loss 0.11433190852403641 Accuracy 0.6255000233650208\n",
      "Output tensor([[0.5968],\n",
      "        [0.4833]])\n",
      "Iteration 37050 Training loss 0.1162700206041336 Validation loss 0.11430582404136658 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.5010],\n",
      "        [0.4729]])\n",
      "Iteration 37060 Training loss 0.11694537103176117 Validation loss 0.11433909088373184 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.6019],\n",
      "        [0.4220]])\n",
      "Iteration 37070 Training loss 0.11653783917427063 Validation loss 0.11431091278791428 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.3846],\n",
      "        [0.5786]])\n",
      "Iteration 37080 Training loss 0.11580891907215118 Validation loss 0.11433088034391403 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.6383],\n",
      "        [0.5292]])\n",
      "Iteration 37090 Training loss 0.11542090028524399 Validation loss 0.11451350897550583 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.5309],\n",
      "        [0.4905]])\n",
      "Iteration 37100 Training loss 0.1168360710144043 Validation loss 0.11432864516973495 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.2293],\n",
      "        [0.6027]])\n",
      "Iteration 37110 Training loss 0.11496272683143616 Validation loss 0.11434881389141083 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.6675],\n",
      "        [0.6280]])\n",
      "Iteration 37120 Training loss 0.11627937108278275 Validation loss 0.11433003097772598 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.4480],\n",
      "        [0.5865]])\n",
      "Iteration 37130 Training loss 0.11634183675050735 Validation loss 0.11431600898504257 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.4474],\n",
      "        [0.4164]])\n",
      "Iteration 37140 Training loss 0.11698855459690094 Validation loss 0.11434440314769745 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.4283],\n",
      "        [0.3916]])\n",
      "Iteration 37150 Training loss 0.1156838983297348 Validation loss 0.11436788737773895 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.7491],\n",
      "        [0.5792]])\n",
      "Iteration 37160 Training loss 0.1150650754570961 Validation loss 0.11429407447576523 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.7384],\n",
      "        [0.6140]])\n",
      "Iteration 37170 Training loss 0.11652924865484238 Validation loss 0.11429715901613235 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.3707],\n",
      "        [0.6350]])\n",
      "Iteration 37180 Training loss 0.11587774753570557 Validation loss 0.11436782032251358 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.5697],\n",
      "        [0.4733]])\n",
      "Iteration 37190 Training loss 0.11553163826465607 Validation loss 0.11429152637720108 Accuracy 0.624833345413208\n",
      "Output tensor([[0.5131],\n",
      "        [0.6352]])\n",
      "Iteration 37200 Training loss 0.11609668284654617 Validation loss 0.11430792510509491 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.5074],\n",
      "        [0.4652]])\n",
      "Iteration 37210 Training loss 0.11494539678096771 Validation loss 0.11434821784496307 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.5599],\n",
      "        [0.5734]])\n",
      "Iteration 37220 Training loss 0.11513276398181915 Validation loss 0.11428861320018768 Accuracy 0.621833324432373\n",
      "Output tensor([[0.2400],\n",
      "        [0.4537]])\n",
      "Iteration 37230 Training loss 0.11540266871452332 Validation loss 0.11430902034044266 Accuracy 0.621999979019165\n",
      "Output tensor([[0.5201],\n",
      "        [0.4027]])\n",
      "Iteration 37240 Training loss 0.11760148406028748 Validation loss 0.11428242921829224 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.3841],\n",
      "        [0.5203]])\n",
      "Iteration 37250 Training loss 0.11587300896644592 Validation loss 0.11436568945646286 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.5399],\n",
      "        [0.6329]])\n",
      "Iteration 37260 Training loss 0.11607464402914047 Validation loss 0.11428774148225784 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.4905],\n",
      "        [0.4453]])\n",
      "Iteration 37270 Training loss 0.11606241762638092 Validation loss 0.11429386585950851 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.5482],\n",
      "        [0.6170]])\n",
      "Iteration 37280 Training loss 0.11527920514345169 Validation loss 0.11428701132535934 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.5562],\n",
      "        [0.2505]])\n",
      "Iteration 37290 Training loss 0.11582662910223007 Validation loss 0.11431398242712021 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5717],\n",
      "        [0.5129]])\n",
      "Iteration 37300 Training loss 0.11585626006126404 Validation loss 0.11435618996620178 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.5509],\n",
      "        [0.5461]])\n",
      "Iteration 37310 Training loss 0.11518388986587524 Validation loss 0.11432525515556335 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.4384],\n",
      "        [0.4050]])\n",
      "Iteration 37320 Training loss 0.11564271152019501 Validation loss 0.11428265273571014 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.2922],\n",
      "        [0.5076]])\n",
      "Iteration 37330 Training loss 0.11698154360055923 Validation loss 0.11427491903305054 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.7350],\n",
      "        [0.6491]])\n",
      "Iteration 37340 Training loss 0.11723499745130539 Validation loss 0.11432639509439468 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.3911],\n",
      "        [0.4843]])\n",
      "Iteration 37350 Training loss 0.11438693106174469 Validation loss 0.11431185901165009 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.3937],\n",
      "        [0.5181]])\n",
      "Iteration 37360 Training loss 0.11577467620372772 Validation loss 0.11429128050804138 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.4576],\n",
      "        [0.4254]])\n",
      "Iteration 37370 Training loss 0.1162993535399437 Validation loss 0.11430810391902924 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.6857],\n",
      "        [0.5127]])\n",
      "Iteration 37380 Training loss 0.11577598750591278 Validation loss 0.11458459496498108 Accuracy 0.621999979019165\n",
      "Output tensor([[0.6602],\n",
      "        [0.3205]])\n",
      "Iteration 37390 Training loss 0.1150323823094368 Validation loss 0.11455255001783371 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5147],\n",
      "        [0.5386]])\n",
      "Iteration 37400 Training loss 0.11500482261180878 Validation loss 0.11426275968551636 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.3452],\n",
      "        [0.5377]])\n",
      "Iteration 37410 Training loss 0.11395934224128723 Validation loss 0.11425833404064178 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4639],\n",
      "        [0.4667]])\n",
      "Iteration 37420 Training loss 0.1155322790145874 Validation loss 0.1143280640244484 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.4856],\n",
      "        [0.3125]])\n",
      "Iteration 37430 Training loss 0.11596039682626724 Validation loss 0.11431854218244553 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.5167],\n",
      "        [0.4914]])\n",
      "Iteration 37440 Training loss 0.11483068764209747 Validation loss 0.11427243053913116 Accuracy 0.624666690826416\n",
      "Output tensor([[0.4614],\n",
      "        [0.2850]])\n",
      "Iteration 37450 Training loss 0.11590610444545746 Validation loss 0.11429623514413834 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.4368],\n",
      "        [0.3445]])\n",
      "Iteration 37460 Training loss 0.11512578278779984 Validation loss 0.11433201283216476 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.4936],\n",
      "        [0.5537]])\n",
      "Iteration 37470 Training loss 0.11561521142721176 Validation loss 0.1142871230840683 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4928],\n",
      "        [0.6401]])\n",
      "Iteration 37480 Training loss 0.11452337354421616 Validation loss 0.11433254182338715 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.6810],\n",
      "        [0.7137]])\n",
      "Iteration 37490 Training loss 0.11523094028234482 Validation loss 0.11427655071020126 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.5202],\n",
      "        [0.6092]])\n",
      "Iteration 37500 Training loss 0.11567968875169754 Validation loss 0.11438614875078201 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.5150],\n",
      "        [0.4425]])\n",
      "Iteration 37510 Training loss 0.11657509207725525 Validation loss 0.11443020403385162 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.6180],\n",
      "        [0.5882]])\n",
      "Iteration 37520 Training loss 0.1165664792060852 Validation loss 0.11433316767215729 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.6858],\n",
      "        [0.6555]])\n",
      "Iteration 37530 Training loss 0.11597294360399246 Validation loss 0.11443314701318741 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.5222],\n",
      "        [0.2833]])\n",
      "Iteration 37540 Training loss 0.11638051271438599 Validation loss 0.11427933722734451 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.4009],\n",
      "        [0.4470]])\n",
      "Iteration 37550 Training loss 0.11609231680631638 Validation loss 0.11437231302261353 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.4396],\n",
      "        [0.5441]])\n",
      "Iteration 37560 Training loss 0.11529020220041275 Validation loss 0.11430305987596512 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.4346],\n",
      "        [0.4254]])\n",
      "Iteration 37570 Training loss 0.11489931493997574 Validation loss 0.11434509605169296 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.2968],\n",
      "        [0.6649]])\n",
      "Iteration 37580 Training loss 0.11469552665948868 Validation loss 0.11436565965414047 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.4696],\n",
      "        [0.6472]])\n",
      "Iteration 37590 Training loss 0.11488698422908783 Validation loss 0.11432291567325592 Accuracy 0.621999979019165\n",
      "Output tensor([[0.5370],\n",
      "        [0.4012]])\n",
      "Iteration 37600 Training loss 0.11657870560884476 Validation loss 0.11431717127561569 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.7184],\n",
      "        [0.4758]])\n",
      "Iteration 37610 Training loss 0.11606945842504501 Validation loss 0.11440107971429825 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.4101],\n",
      "        [0.3861]])\n",
      "Iteration 37620 Training loss 0.11512778699398041 Validation loss 0.11431299895048141 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.6277],\n",
      "        [0.2483]])\n",
      "Iteration 37630 Training loss 0.11564572900533676 Validation loss 0.11432492733001709 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.6351],\n",
      "        [0.5699]])\n",
      "Iteration 37640 Training loss 0.11539874225854874 Validation loss 0.11431751400232315 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.5394],\n",
      "        [0.6278]])\n",
      "Iteration 37650 Training loss 0.11641506105661392 Validation loss 0.11428854614496231 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.5714],\n",
      "        [0.5873]])\n",
      "Iteration 37660 Training loss 0.11648840457201004 Validation loss 0.1142742931842804 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.5027],\n",
      "        [0.3304]])\n",
      "Iteration 37670 Training loss 0.11691495776176453 Validation loss 0.11427529156208038 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.5982],\n",
      "        [0.6541]])\n",
      "Iteration 37680 Training loss 0.1165977343916893 Validation loss 0.11433817446231842 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.5239],\n",
      "        [0.4836]])\n",
      "Iteration 37690 Training loss 0.11592798680067062 Validation loss 0.11431650817394257 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.5833],\n",
      "        [0.3738]])\n",
      "Iteration 37700 Training loss 0.1156543716788292 Validation loss 0.11431875824928284 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.3255],\n",
      "        [0.5646]])\n",
      "Iteration 37710 Training loss 0.11567153036594391 Validation loss 0.1144111305475235 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.5014],\n",
      "        [0.5053]])\n",
      "Iteration 37720 Training loss 0.11611615866422653 Validation loss 0.11427449434995651 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.4850],\n",
      "        [0.3012]])\n",
      "Iteration 37730 Training loss 0.11535710841417313 Validation loss 0.1144670844078064 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.3974],\n",
      "        [0.4990]])\n",
      "Iteration 37740 Training loss 0.11584875732660294 Validation loss 0.11425662040710449 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4741],\n",
      "        [0.5856]])\n",
      "Iteration 37750 Training loss 0.11662468314170837 Validation loss 0.11431752890348434 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4702],\n",
      "        [0.2813]])\n",
      "Iteration 37760 Training loss 0.11477034538984299 Validation loss 0.1142459511756897 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.4266],\n",
      "        [0.5433]])\n",
      "Iteration 37770 Training loss 0.11653496325016022 Validation loss 0.114244744181633 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4721],\n",
      "        [0.1666]])\n",
      "Iteration 37780 Training loss 0.11573348939418793 Validation loss 0.11430468410253525 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.5012],\n",
      "        [0.4813]])\n",
      "Iteration 37790 Training loss 0.11637886613607407 Validation loss 0.11423004418611526 Accuracy 0.6221666932106018\n",
      "Output tensor([[0.6276],\n",
      "        [0.4975]])\n",
      "Iteration 37800 Training loss 0.11496731638908386 Validation loss 0.11425794661045074 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.5337],\n",
      "        [0.6090]])\n",
      "Iteration 37810 Training loss 0.11600138247013092 Validation loss 0.11423890292644501 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.4877],\n",
      "        [0.5480]])\n",
      "Iteration 37820 Training loss 0.1163167729973793 Validation loss 0.11425352096557617 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.5938],\n",
      "        [0.5197]])\n",
      "Iteration 37830 Training loss 0.11599371582269669 Validation loss 0.11427362263202667 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.7424],\n",
      "        [0.2639]])\n",
      "Iteration 37840 Training loss 0.11593040078878403 Validation loss 0.11428675055503845 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.2773],\n",
      "        [0.6621]])\n",
      "Iteration 37850 Training loss 0.11676047742366791 Validation loss 0.11424876004457474 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.4551],\n",
      "        [0.7025]])\n",
      "Iteration 37860 Training loss 0.11729136109352112 Validation loss 0.11427431553602219 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.2729],\n",
      "        [0.5068]])\n",
      "Iteration 37870 Training loss 0.1155184656381607 Validation loss 0.11424218863248825 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.6453],\n",
      "        [0.5058]])\n",
      "Iteration 37880 Training loss 0.11326776444911957 Validation loss 0.11424656212329865 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.5042],\n",
      "        [0.6774]])\n",
      "Iteration 37890 Training loss 0.1152324229478836 Validation loss 0.11425981670618057 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.5260],\n",
      "        [0.5382]])\n",
      "Iteration 37900 Training loss 0.11513476818799973 Validation loss 0.11426509916782379 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.2111],\n",
      "        [0.6473]])\n",
      "Iteration 37910 Training loss 0.11510227620601654 Validation loss 0.1142677441239357 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.4274],\n",
      "        [0.6222]])\n",
      "Iteration 37920 Training loss 0.11526807397603989 Validation loss 0.11422419548034668 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.5668],\n",
      "        [0.6300]])\n",
      "Iteration 37930 Training loss 0.11650556325912476 Validation loss 0.11425059288740158 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.5565],\n",
      "        [0.3475]])\n",
      "Iteration 37940 Training loss 0.11660246551036835 Validation loss 0.11432986706495285 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.4466],\n",
      "        [0.5094]])\n",
      "Iteration 37950 Training loss 0.11569146066904068 Validation loss 0.11424088478088379 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.5173],\n",
      "        [0.2549]])\n",
      "Iteration 37960 Training loss 0.11578407138586044 Validation loss 0.11423371732234955 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.6611],\n",
      "        [0.4036]])\n",
      "Iteration 37970 Training loss 0.11512689292430878 Validation loss 0.114324189722538 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.3688],\n",
      "        [0.3045]])\n",
      "Iteration 37980 Training loss 0.11618949472904205 Validation loss 0.1142607182264328 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.5437],\n",
      "        [0.4784]])\n",
      "Iteration 37990 Training loss 0.11662634462118149 Validation loss 0.114255890250206 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.4814],\n",
      "        [0.4767]])\n",
      "Iteration 38000 Training loss 0.1148020327091217 Validation loss 0.11441808193922043 Accuracy 0.6228333115577698\n",
      "Output tensor([[0.5980],\n",
      "        [0.5772]])\n",
      "Iteration 38010 Training loss 0.11633621901273727 Validation loss 0.11432924121618271 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.1125],\n",
      "        [0.3404]])\n",
      "Iteration 38020 Training loss 0.11684120446443558 Validation loss 0.11423969268798828 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.4188],\n",
      "        [0.4238]])\n",
      "Iteration 38030 Training loss 0.11746840178966522 Validation loss 0.11441632360219955 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.6099],\n",
      "        [0.5058]])\n",
      "Iteration 38040 Training loss 0.11641234159469604 Validation loss 0.11430077999830246 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.3489],\n",
      "        [0.6499]])\n",
      "Iteration 38050 Training loss 0.11500673741102219 Validation loss 0.11427749693393707 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.4542],\n",
      "        [0.2137]])\n",
      "Iteration 38060 Training loss 0.11677462607622147 Validation loss 0.11423081159591675 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.4189],\n",
      "        [0.5270]])\n",
      "Iteration 38070 Training loss 0.11658994853496552 Validation loss 0.11425795406103134 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.3766],\n",
      "        [0.6745]])\n",
      "Iteration 38080 Training loss 0.11562622338533401 Validation loss 0.11426152288913727 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.5195],\n",
      "        [0.3295]])\n",
      "Iteration 38090 Training loss 0.11452425271272659 Validation loss 0.11428791284561157 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.5052],\n",
      "        [0.5236]])\n",
      "Iteration 38100 Training loss 0.11527054756879807 Validation loss 0.11423079669475555 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.4434],\n",
      "        [0.1351]])\n",
      "Iteration 38110 Training loss 0.11602134257555008 Validation loss 0.11420949548482895 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.5599],\n",
      "        [0.4040]])\n",
      "Iteration 38120 Training loss 0.11657814681529999 Validation loss 0.11426139622926712 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.4786],\n",
      "        [0.5604]])\n",
      "Iteration 38130 Training loss 0.11565074324607849 Validation loss 0.1142156794667244 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.3595],\n",
      "        [0.4606]])\n",
      "Iteration 38140 Training loss 0.11600732058286667 Validation loss 0.11423973739147186 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.6160],\n",
      "        [0.4042]])\n",
      "Iteration 38150 Training loss 0.11569445580244064 Validation loss 0.11419899761676788 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.6449],\n",
      "        [0.2764]])\n",
      "Iteration 38160 Training loss 0.11643759906291962 Validation loss 0.11421454697847366 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.5550],\n",
      "        [0.4101]])\n",
      "Iteration 38170 Training loss 0.1154732033610344 Validation loss 0.11420650035142899 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.3959],\n",
      "        [0.3402]])\n",
      "Iteration 38180 Training loss 0.11608067154884338 Validation loss 0.1141938790678978 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.4104],\n",
      "        [0.3113]])\n",
      "Iteration 38190 Training loss 0.11671512573957443 Validation loss 0.114199198782444 Accuracy 0.624833345413208\n",
      "Output tensor([[0.6011],\n",
      "        [0.3090]])\n",
      "Iteration 38200 Training loss 0.11489032953977585 Validation loss 0.1142791360616684 Accuracy 0.625\n",
      "Output tensor([[0.4763],\n",
      "        [0.3611]])\n",
      "Iteration 38210 Training loss 0.11521416902542114 Validation loss 0.11424790322780609 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.4003],\n",
      "        [0.4153]])\n",
      "Iteration 38220 Training loss 0.11584069579839706 Validation loss 0.11418982595205307 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5850],\n",
      "        [0.6323]])\n",
      "Iteration 38230 Training loss 0.11705727130174637 Validation loss 0.11419544368982315 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.4762],\n",
      "        [0.5873]])\n",
      "Iteration 38240 Training loss 0.11769828200340271 Validation loss 0.11421162635087967 Accuracy 0.625166654586792\n",
      "Output tensor([[0.2348],\n",
      "        [0.5425]])\n",
      "Iteration 38250 Training loss 0.11575531959533691 Validation loss 0.11421927809715271 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.3623],\n",
      "        [0.2893]])\n",
      "Iteration 38260 Training loss 0.11708292365074158 Validation loss 0.11422690004110336 Accuracy 0.625166654586792\n",
      "Output tensor([[0.3402],\n",
      "        [0.2747]])\n",
      "Iteration 38270 Training loss 0.11537618935108185 Validation loss 0.11422055959701538 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.5312],\n",
      "        [0.4173]])\n",
      "Iteration 38280 Training loss 0.11689391732215881 Validation loss 0.11422528326511383 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.5332],\n",
      "        [0.4238]])\n",
      "Iteration 38290 Training loss 0.11492976546287537 Validation loss 0.11422204226255417 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.2657],\n",
      "        [0.3540]])\n",
      "Iteration 38300 Training loss 0.11627903580665588 Validation loss 0.11427370458841324 Accuracy 0.6258333325386047\n",
      "Output tensor([[0.7599],\n",
      "        [0.5486]])\n",
      "Iteration 38310 Training loss 0.1145787239074707 Validation loss 0.1142396479845047 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.3208],\n",
      "        [0.8266]])\n",
      "Iteration 38320 Training loss 0.11470657587051392 Validation loss 0.11427711695432663 Accuracy 0.625\n",
      "Output tensor([[0.5804],\n",
      "        [0.3479]])\n",
      "Iteration 38330 Training loss 0.1148863136768341 Validation loss 0.11437346041202545 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.3450],\n",
      "        [0.4073]])\n",
      "Iteration 38340 Training loss 0.11612623184919357 Validation loss 0.11428967118263245 Accuracy 0.625166654586792\n",
      "Output tensor([[0.4429],\n",
      "        [0.5216]])\n",
      "Iteration 38350 Training loss 0.11534467339515686 Validation loss 0.11423946171998978 Accuracy 0.625166654586792\n",
      "Output tensor([[0.5734],\n",
      "        [0.3824]])\n",
      "Iteration 38360 Training loss 0.1165059506893158 Validation loss 0.11426947265863419 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.4048],\n",
      "        [0.6954]])\n",
      "Iteration 38370 Training loss 0.11627325415611267 Validation loss 0.11432399600744247 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.4483],\n",
      "        [0.4616]])\n",
      "Iteration 38380 Training loss 0.11610211431980133 Validation loss 0.11419856548309326 Accuracy 0.6258333325386047\n",
      "Output tensor([[0.5151],\n",
      "        [0.4451]])\n",
      "Iteration 38390 Training loss 0.11494389176368713 Validation loss 0.11421690136194229 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.6219],\n",
      "        [0.3824]])\n",
      "Iteration 38400 Training loss 0.11495587974786758 Validation loss 0.11422234773635864 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.3189],\n",
      "        [0.4343]])\n",
      "Iteration 38410 Training loss 0.11600761115550995 Validation loss 0.11422969400882721 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.3733],\n",
      "        [0.5305]])\n",
      "Iteration 38420 Training loss 0.11646250635385513 Validation loss 0.11420737951993942 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.2619],\n",
      "        [0.4203]])\n",
      "Iteration 38430 Training loss 0.11584216356277466 Validation loss 0.11422262340784073 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.5412],\n",
      "        [0.4785]])\n",
      "Iteration 38440 Training loss 0.1161302700638771 Validation loss 0.11418221145868301 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.3494],\n",
      "        [0.4581]])\n",
      "Iteration 38450 Training loss 0.11588938534259796 Validation loss 0.11425496637821198 Accuracy 0.624833345413208\n",
      "Output tensor([[0.5829],\n",
      "        [0.6342]])\n",
      "Iteration 38460 Training loss 0.1159934252500534 Validation loss 0.11419226974248886 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.4435],\n",
      "        [0.5496]])\n",
      "Iteration 38470 Training loss 0.11633511632680893 Validation loss 0.1142178326845169 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.2481],\n",
      "        [0.4913]])\n",
      "Iteration 38480 Training loss 0.11565234512090683 Validation loss 0.11423879116773605 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.4832],\n",
      "        [0.5220]])\n",
      "Iteration 38490 Training loss 0.11570403724908829 Validation loss 0.11422133445739746 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.5018],\n",
      "        [0.6114]])\n",
      "Iteration 38500 Training loss 0.11750072240829468 Validation loss 0.11423466354608536 Accuracy 0.624833345413208\n",
      "Output tensor([[0.2185],\n",
      "        [0.3114]])\n",
      "Iteration 38510 Training loss 0.11697760969400406 Validation loss 0.11426623165607452 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.4351],\n",
      "        [0.4213]])\n",
      "Iteration 38520 Training loss 0.11691457033157349 Validation loss 0.11424672603607178 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.4656],\n",
      "        [0.6700]])\n",
      "Iteration 38530 Training loss 0.11465677618980408 Validation loss 0.11425799876451492 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.2660],\n",
      "        [0.4794]])\n",
      "Iteration 38540 Training loss 0.11576522886753082 Validation loss 0.11427164822816849 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.4610],\n",
      "        [0.4481]])\n",
      "Iteration 38550 Training loss 0.11617930233478546 Validation loss 0.11424806714057922 Accuracy 0.6259999871253967\n",
      "Output tensor([[0.4962],\n",
      "        [0.5804]])\n",
      "Iteration 38560 Training loss 0.11620993912220001 Validation loss 0.11423094570636749 Accuracy 0.624666690826416\n",
      "Output tensor([[0.5505],\n",
      "        [0.7540]])\n",
      "Iteration 38570 Training loss 0.1167575865983963 Validation loss 0.11428657919168472 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.3034],\n",
      "        [0.5361]])\n",
      "Iteration 38580 Training loss 0.11695358157157898 Validation loss 0.11423305422067642 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.2710],\n",
      "        [0.5323]])\n",
      "Iteration 38590 Training loss 0.11470156162977219 Validation loss 0.11424552649259567 Accuracy 0.624666690826416\n",
      "Output tensor([[0.5552],\n",
      "        [0.5775]])\n",
      "Iteration 38600 Training loss 0.11563675105571747 Validation loss 0.11423268914222717 Accuracy 0.624833345413208\n",
      "Output tensor([[0.1554],\n",
      "        [0.4296]])\n",
      "Iteration 38610 Training loss 0.11702283471822739 Validation loss 0.11424210667610168 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.4157],\n",
      "        [0.6640]])\n",
      "Iteration 38620 Training loss 0.11641436070203781 Validation loss 0.11424867808818817 Accuracy 0.625166654586792\n",
      "Output tensor([[0.4877],\n",
      "        [0.5517]])\n",
      "Iteration 38630 Training loss 0.11623703688383102 Validation loss 0.11425057053565979 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.4264],\n",
      "        [0.3219]])\n",
      "Iteration 38640 Training loss 0.11617071181535721 Validation loss 0.11433850228786469 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.4706],\n",
      "        [0.7624]])\n",
      "Iteration 38650 Training loss 0.11639974266290665 Validation loss 0.11427146196365356 Accuracy 0.624833345413208\n",
      "Output tensor([[0.1554],\n",
      "        [0.4813]])\n",
      "Iteration 38660 Training loss 0.1167718693614006 Validation loss 0.1143440455198288 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.5239],\n",
      "        [0.5041]])\n",
      "Iteration 38670 Training loss 0.11571747809648514 Validation loss 0.1142229437828064 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.5956],\n",
      "        [0.5716]])\n",
      "Iteration 38680 Training loss 0.11566782742738724 Validation loss 0.11429674923419952 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.7426],\n",
      "        [0.6715]])\n",
      "Iteration 38690 Training loss 0.11655114591121674 Validation loss 0.11423621326684952 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.4449],\n",
      "        [0.5987]])\n",
      "Iteration 38700 Training loss 0.11510835587978363 Validation loss 0.11438468098640442 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.2999],\n",
      "        [0.5672]])\n",
      "Iteration 38710 Training loss 0.11676543205976486 Validation loss 0.114211805164814 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.4929],\n",
      "        [0.5048]])\n",
      "Iteration 38720 Training loss 0.11517369747161865 Validation loss 0.11421532928943634 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.4466],\n",
      "        [0.6866]])\n",
      "Iteration 38730 Training loss 0.11572162061929703 Validation loss 0.11421748995780945 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.4552],\n",
      "        [0.5714]])\n",
      "Iteration 38740 Training loss 0.11509553343057632 Validation loss 0.11420666426420212 Accuracy 0.624666690826416\n",
      "Output tensor([[0.8446],\n",
      "        [0.2944]])\n",
      "Iteration 38750 Training loss 0.1168380007147789 Validation loss 0.1142132580280304 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.4898],\n",
      "        [0.5751]])\n",
      "Iteration 38760 Training loss 0.11679176986217499 Validation loss 0.1142672523856163 Accuracy 0.6256666779518127\n",
      "Output tensor([[0.6473],\n",
      "        [0.3479]])\n",
      "Iteration 38770 Training loss 0.11619025468826294 Validation loss 0.1142466813325882 Accuracy 0.624666690826416\n",
      "Output tensor([[0.5250],\n",
      "        [0.4920]])\n",
      "Iteration 38780 Training loss 0.11494278907775879 Validation loss 0.1143576055765152 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.4982],\n",
      "        [0.5864]])\n",
      "Iteration 38790 Training loss 0.1168513372540474 Validation loss 0.114181287586689 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.4422],\n",
      "        [0.4905]])\n",
      "Iteration 38800 Training loss 0.11414891481399536 Validation loss 0.11419900506734848 Accuracy 0.624833345413208\n",
      "Output tensor([[0.5778],\n",
      "        [0.5192]])\n",
      "Iteration 38810 Training loss 0.11514268070459366 Validation loss 0.1142173632979393 Accuracy 0.624833345413208\n",
      "Output tensor([[0.5950],\n",
      "        [0.5011]])\n",
      "Iteration 38820 Training loss 0.11599501222372055 Validation loss 0.11422131210565567 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.5179],\n",
      "        [0.5990]])\n",
      "Iteration 38830 Training loss 0.11627288907766342 Validation loss 0.11423148959875107 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.4852],\n",
      "        [0.6967]])\n",
      "Iteration 38840 Training loss 0.11529143154621124 Validation loss 0.11423424631357193 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.3304],\n",
      "        [0.6208]])\n",
      "Iteration 38850 Training loss 0.11607759445905685 Validation loss 0.11421183496713638 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.7071],\n",
      "        [0.2962]])\n",
      "Iteration 38860 Training loss 0.11637965589761734 Validation loss 0.11422113329172134 Accuracy 0.624666690826416\n",
      "Output tensor([[0.7549],\n",
      "        [0.6352]])\n",
      "Iteration 38870 Training loss 0.11714709550142288 Validation loss 0.11433012038469315 Accuracy 0.6223333477973938\n",
      "Output tensor([[0.6036],\n",
      "        [0.5869]])\n",
      "Iteration 38880 Training loss 0.11516650021076202 Validation loss 0.11417052894830704 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.3905],\n",
      "        [0.7070]])\n",
      "Iteration 38890 Training loss 0.11526832729578018 Validation loss 0.11418027430772781 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.3276],\n",
      "        [0.4548]])\n",
      "Iteration 38900 Training loss 0.11614105105400085 Validation loss 0.11423628777265549 Accuracy 0.624833345413208\n",
      "Output tensor([[0.5581],\n",
      "        [0.5428]])\n",
      "Iteration 38910 Training loss 0.11581934243440628 Validation loss 0.11418098956346512 Accuracy 0.625\n",
      "Output tensor([[0.5745],\n",
      "        [0.5911]])\n",
      "Iteration 38920 Training loss 0.1168002113699913 Validation loss 0.11420176178216934 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.0980],\n",
      "        [0.3572]])\n",
      "Iteration 38930 Training loss 0.11472488194704056 Validation loss 0.11427811533212662 Accuracy 0.624666690826416\n",
      "Output tensor([[0.4937],\n",
      "        [0.3022]])\n",
      "Iteration 38940 Training loss 0.11534938216209412 Validation loss 0.11417017877101898 Accuracy 0.624833345413208\n",
      "Output tensor([[0.3757],\n",
      "        [0.4229]])\n",
      "Iteration 38950 Training loss 0.11615193635225296 Validation loss 0.11454610526561737 Accuracy 0.6231666803359985\n",
      "Output tensor([[0.5693],\n",
      "        [0.5180]])\n",
      "Iteration 38960 Training loss 0.11637959629297256 Validation loss 0.11416690796613693 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.3869],\n",
      "        [0.4014]])\n",
      "Iteration 38970 Training loss 0.11624139547348022 Validation loss 0.11417830735445023 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.4888],\n",
      "        [0.8190]])\n",
      "Iteration 38980 Training loss 0.11642399430274963 Validation loss 0.11416450887918472 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.6257],\n",
      "        [0.4739]])\n",
      "Iteration 38990 Training loss 0.11553888767957687 Validation loss 0.11418700963258743 Accuracy 0.625166654586792\n",
      "Output tensor([[0.6793],\n",
      "        [0.5061]])\n",
      "Iteration 39000 Training loss 0.11637436598539352 Validation loss 0.11437603831291199 Accuracy 0.6226666569709778\n",
      "Output tensor([[0.5872],\n",
      "        [0.6719]])\n",
      "Iteration 39010 Training loss 0.11599985510110855 Validation loss 0.11417421698570251 Accuracy 0.624666690826416\n",
      "Output tensor([[0.6752],\n",
      "        [0.5855]])\n",
      "Iteration 39020 Training loss 0.11606741696596146 Validation loss 0.11417392641305923 Accuracy 0.625166654586792\n",
      "Output tensor([[0.4880],\n",
      "        [0.7002]])\n",
      "Iteration 39030 Training loss 0.11581791192293167 Validation loss 0.11419402807950974 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.3083],\n",
      "        [0.5479]])\n",
      "Iteration 39040 Training loss 0.1149435043334961 Validation loss 0.11419905722141266 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.4646],\n",
      "        [0.4355]])\n",
      "Iteration 39050 Training loss 0.11571026593446732 Validation loss 0.1141771748661995 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.2474],\n",
      "        [0.3303]])\n",
      "Iteration 39060 Training loss 0.11558674275875092 Validation loss 0.1141824945807457 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.6000],\n",
      "        [0.4031]])\n",
      "Iteration 39070 Training loss 0.11515378952026367 Validation loss 0.1142301931977272 Accuracy 0.6258333325386047\n",
      "Output tensor([[0.4414],\n",
      "        [0.4751]])\n",
      "Iteration 39080 Training loss 0.11652752012014389 Validation loss 0.11422263830900192 Accuracy 0.624833345413208\n",
      "Output tensor([[0.4335],\n",
      "        [0.3848]])\n",
      "Iteration 39090 Training loss 0.11404427886009216 Validation loss 0.11420237272977829 Accuracy 0.624666690826416\n",
      "Output tensor([[0.2745],\n",
      "        [0.6639]])\n",
      "Iteration 39100 Training loss 0.11609362810850143 Validation loss 0.11427493393421173 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.4317],\n",
      "        [0.5063]])\n",
      "Iteration 39110 Training loss 0.1142912283539772 Validation loss 0.1141643300652504 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.5444],\n",
      "        [0.6001]])\n",
      "Iteration 39120 Training loss 0.11566012352705002 Validation loss 0.11417660862207413 Accuracy 0.6258333325386047\n",
      "Output tensor([[0.5622],\n",
      "        [0.2985]])\n",
      "Iteration 39130 Training loss 0.11545131355524063 Validation loss 0.11419081687927246 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.3339],\n",
      "        [0.3836]])\n",
      "Iteration 39140 Training loss 0.1159561350941658 Validation loss 0.11415231972932816 Accuracy 0.6256666779518127\n",
      "Output tensor([[0.7387],\n",
      "        [0.5925]])\n",
      "Iteration 39150 Training loss 0.11639178544282913 Validation loss 0.11424655467271805 Accuracy 0.6259999871253967\n",
      "Output tensor([[0.5498],\n",
      "        [0.7450]])\n",
      "Iteration 39160 Training loss 0.11506408452987671 Validation loss 0.11422286927700043 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.4900],\n",
      "        [0.4151]])\n",
      "Iteration 39170 Training loss 0.11595802009105682 Validation loss 0.11420554667711258 Accuracy 0.6263333559036255\n",
      "Output tensor([[0.6145],\n",
      "        [0.3421]])\n",
      "Iteration 39180 Training loss 0.11670879274606705 Validation loss 0.11418850719928741 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.3914],\n",
      "        [0.7285]])\n",
      "Iteration 39190 Training loss 0.11617273092269897 Validation loss 0.11433455348014832 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.6510],\n",
      "        [0.5578]])\n",
      "Iteration 39200 Training loss 0.11505453288555145 Validation loss 0.11417210102081299 Accuracy 0.625166654586792\n",
      "Output tensor([[0.5272],\n",
      "        [0.5870]])\n",
      "Iteration 39210 Training loss 0.11699965596199036 Validation loss 0.11417181044816971 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.4504],\n",
      "        [0.3824]])\n",
      "Iteration 39220 Training loss 0.1150258406996727 Validation loss 0.11417532712221146 Accuracy 0.625166654586792\n",
      "Output tensor([[0.3319],\n",
      "        [0.4619]])\n",
      "Iteration 39230 Training loss 0.11664430797100067 Validation loss 0.11417049169540405 Accuracy 0.6255000233650208\n",
      "Output tensor([[0.4086],\n",
      "        [0.5342]])\n",
      "Iteration 39240 Training loss 0.11519032716751099 Validation loss 0.11418287456035614 Accuracy 0.625\n",
      "Output tensor([[0.5371],\n",
      "        [0.2941]])\n",
      "Iteration 39250 Training loss 0.1164361983537674 Validation loss 0.1141546443104744 Accuracy 0.625333309173584\n",
      "Output tensor([[0.2877],\n",
      "        [0.3983]])\n",
      "Iteration 39260 Training loss 0.11650621891021729 Validation loss 0.11418135464191437 Accuracy 0.6256666779518127\n",
      "Output tensor([[0.7007],\n",
      "        [0.5446]])\n",
      "Iteration 39270 Training loss 0.11451160907745361 Validation loss 0.11425711959600449 Accuracy 0.6255000233650208\n",
      "Output tensor([[0.2384],\n",
      "        [0.5956]])\n",
      "Iteration 39280 Training loss 0.11608860641717911 Validation loss 0.11420619487762451 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.5539],\n",
      "        [0.4486]])\n",
      "Iteration 39290 Training loss 0.11610808968544006 Validation loss 0.11418155580759048 Accuracy 0.625333309173584\n",
      "Output tensor([[0.2963],\n",
      "        [0.4161]])\n",
      "Iteration 39300 Training loss 0.11536458134651184 Validation loss 0.11417239159345627 Accuracy 0.624833345413208\n",
      "Output tensor([[0.4010],\n",
      "        [0.4844]])\n",
      "Iteration 39310 Training loss 0.11372729390859604 Validation loss 0.11436283588409424 Accuracy 0.6233333349227905\n",
      "Output tensor([[0.5557],\n",
      "        [0.3180]])\n",
      "Iteration 39320 Training loss 0.11555486917495728 Validation loss 0.11432987451553345 Accuracy 0.6238333582878113\n",
      "Output tensor([[0.6762],\n",
      "        [0.5134]])\n",
      "Iteration 39330 Training loss 0.11586909741163254 Validation loss 0.11417163163423538 Accuracy 0.625333309173584\n",
      "Output tensor([[0.3534],\n",
      "        [0.5094]])\n",
      "Iteration 39340 Training loss 0.11649101972579956 Validation loss 0.1141955703496933 Accuracy 0.6244999766349792\n",
      "Output tensor([[0.6384],\n",
      "        [0.5527]])\n",
      "Iteration 39350 Training loss 0.11646789312362671 Validation loss 0.114173024892807 Accuracy 0.6255000233650208\n",
      "Output tensor([[0.5613],\n",
      "        [0.5176]])\n",
      "Iteration 39360 Training loss 0.11676321178674698 Validation loss 0.1141536682844162 Accuracy 0.625166654586792\n",
      "Output tensor([[0.3403],\n",
      "        [0.6679]])\n",
      "Iteration 39370 Training loss 0.11577615141868591 Validation loss 0.1141791045665741 Accuracy 0.625166654586792\n",
      "Output tensor([[0.3766],\n",
      "        [0.7535]])\n",
      "Iteration 39380 Training loss 0.11654016375541687 Validation loss 0.114176444709301 Accuracy 0.6255000233650208\n",
      "Output tensor([[0.5509],\n",
      "        [0.2341]])\n",
      "Iteration 39390 Training loss 0.11713110655546188 Validation loss 0.11423370242118835 Accuracy 0.625\n",
      "Output tensor([[0.2705],\n",
      "        [0.3030]])\n",
      "Iteration 39400 Training loss 0.11575815826654434 Validation loss 0.11417710036039352 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.5061],\n",
      "        [0.4727]])\n",
      "Iteration 39410 Training loss 0.11588975042104721 Validation loss 0.11417360603809357 Accuracy 0.624666690826416\n",
      "Output tensor([[0.6581],\n",
      "        [0.5110]])\n",
      "Iteration 39420 Training loss 0.11558804661035538 Validation loss 0.11415890604257584 Accuracy 0.6240000128746033\n",
      "Output tensor([[0.5857],\n",
      "        [0.5914]])\n",
      "Iteration 39430 Training loss 0.11695591360330582 Validation loss 0.11423832178115845 Accuracy 0.625333309173584\n",
      "Output tensor([[0.6597],\n",
      "        [0.2889]])\n",
      "Iteration 39440 Training loss 0.11479576677083969 Validation loss 0.11419039219617844 Accuracy 0.625333309173584\n",
      "Output tensor([[0.5166],\n",
      "        [0.2729]])\n",
      "Iteration 39450 Training loss 0.11471257358789444 Validation loss 0.11418288201093674 Accuracy 0.625333309173584\n",
      "Output tensor([[0.5754],\n",
      "        [0.6146]])\n",
      "Iteration 39460 Training loss 0.11631762981414795 Validation loss 0.11412595957517624 Accuracy 0.6236666440963745\n",
      "Output tensor([[0.4093],\n",
      "        [0.4920]])\n",
      "Iteration 39470 Training loss 0.11602083593606949 Validation loss 0.11417427659034729 Accuracy 0.624833345413208\n",
      "Output tensor([[0.3233],\n",
      "        [0.3464]])\n",
      "Iteration 39480 Training loss 0.11549603939056396 Validation loss 0.11426043510437012 Accuracy 0.624833345413208\n",
      "Output tensor([[0.4379],\n",
      "        [0.4203]])\n",
      "Iteration 39490 Training loss 0.1156889945268631 Validation loss 0.11419012397527695 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.1585],\n",
      "        [0.5964]])\n",
      "Iteration 39500 Training loss 0.1156378984451294 Validation loss 0.11415735632181168 Accuracy 0.6241666674613953\n",
      "Output tensor([[0.5948],\n",
      "        [0.4800]])\n",
      "Iteration 39510 Training loss 0.1148306280374527 Validation loss 0.1142096072435379 Accuracy 0.6243333220481873\n",
      "Output tensor([[0.4627],\n",
      "        [0.4433]])\n",
      "Iteration 39520 Training loss 0.11719028651714325 Validation loss 0.11437667161226273 Accuracy 0.6234999895095825\n",
      "Output tensor([[0.2979],\n",
      "        [0.5312]])\n",
      "Iteration 39530 Training loss 0.11688119173049927 Validation loss 0.11422554403543472 Accuracy 0.6256666779518127\n",
      "Output tensor([[0.5838],\n",
      "        [0.4761]])\n",
      "Iteration 39540 Training loss 0.11537687480449677 Validation loss 0.11412834376096725 Accuracy 0.6244999766349792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training done'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2.55, 1e-5, 1e8, 0, 0, 1, 0.2, 10, True, True, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer = binary_classification_three_layer_NN(1024, 512, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c674f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer.train_layers(x_train, y_train, x_valid, y_valid, 2.3, 1e-3, 1e8, 0, 0, 0, 1, 0.2, 10, True, True, True, 0.7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347eb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer_1_untrained = binary_classification_three_layer_NN(1024, 512, 512, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3149bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2.6, the number of datas used for the training is 335544320.0000002 and the number of iterations is 55924.\n",
      "Iteration 0 Training loss 0.12497100979089737 Validation loss 0.1249794289469719 Accuracy 0.5203333497047424\n",
      "Iteration 10 Training loss 0.1243247464299202 Validation loss 0.12364035099744797 Accuracy 0.5454999804496765\n",
      "Iteration 20 Training loss 0.12394989281892776 Validation loss 0.12304861098527908 Accuracy 0.5304999947547913\n",
      "Iteration 30 Training loss 0.1233883947134018 Validation loss 0.12228955328464508 Accuracy 0.5609999895095825\n",
      "Iteration 40 Training loss 0.1228841245174408 Validation loss 0.12207065522670746 Accuracy 0.5554999709129333\n",
      "Iteration 50 Training loss 0.12315582484006882 Validation loss 0.12197253108024597 Accuracy 0.5583333373069763\n",
      "Iteration 60 Training loss 0.12251242995262146 Validation loss 0.12142295390367508 Accuracy 0.5671666860580444\n",
      "Iteration 70 Training loss 0.12271280586719513 Validation loss 0.12131612002849579 Accuracy 0.5661666393280029\n",
      "Iteration 80 Training loss 0.12211815267801285 Validation loss 0.12116710096597672 Accuracy 0.5691666603088379\n",
      "Iteration 90 Training loss 0.12263999134302139 Validation loss 0.12099944800138474 Accuracy 0.5753333568572998\n",
      "Iteration 100 Training loss 0.12171539664268494 Validation loss 0.12109770625829697 Accuracy 0.575166642665863\n",
      "Iteration 110 Training loss 0.12193337827920914 Validation loss 0.12080281227827072 Accuracy 0.5738333463668823\n",
      "Iteration 120 Training loss 0.1215987578034401 Validation loss 0.12066613882780075 Accuracy 0.5788333415985107\n",
      "Iteration 130 Training loss 0.12171851843595505 Validation loss 0.12035877257585526 Accuracy 0.5788333415985107\n",
      "Iteration 140 Training loss 0.12146145850419998 Validation loss 0.12024224549531937 Accuracy 0.5855000019073486\n",
      "Iteration 150 Training loss 0.12121959775686264 Validation loss 0.12007328122854233 Accuracy 0.5835000276565552\n",
      "Iteration 160 Training loss 0.12158512324094772 Validation loss 0.11994288861751556 Accuracy 0.5821666717529297\n",
      "Iteration 170 Training loss 0.1208665668964386 Validation loss 0.11988114565610886 Accuracy 0.5866666436195374\n",
      "Iteration 180 Training loss 0.12005671858787537 Validation loss 0.1198476254940033 Accuracy 0.5896666646003723\n",
      "Iteration 190 Training loss 0.1204572319984436 Validation loss 0.11960693448781967 Accuracy 0.5888333320617676\n",
      "Iteration 200 Training loss 0.12084963172674179 Validation loss 0.11973191052675247 Accuracy 0.5895000100135803\n",
      "Iteration 210 Training loss 0.12132012099027634 Validation loss 0.11950379610061646 Accuracy 0.5928333401679993\n",
      "Iteration 220 Training loss 0.12027600407600403 Validation loss 0.1193886399269104 Accuracy 0.593500018119812\n",
      "Iteration 230 Training loss 0.11991871893405914 Validation loss 0.11974941939115524 Accuracy 0.590833306312561\n",
      "Iteration 240 Training loss 0.12073879688978195 Validation loss 0.11943373829126358 Accuracy 0.5951666831970215\n",
      "Iteration 250 Training loss 0.12013882398605347 Validation loss 0.11999285966157913 Accuracy 0.5889999866485596\n",
      "Iteration 260 Training loss 0.12137294560670853 Validation loss 0.11961513012647629 Accuracy 0.5920000076293945\n",
      "Iteration 270 Training loss 0.1199401468038559 Validation loss 0.11911330372095108 Accuracy 0.5985000133514404\n",
      "Iteration 280 Training loss 0.11959853023290634 Validation loss 0.11891424655914307 Accuracy 0.6008333563804626\n",
      "Iteration 290 Training loss 0.1203039363026619 Validation loss 0.11916657537221909 Accuracy 0.5979999899864197\n",
      "Iteration 300 Training loss 0.11980028450489044 Validation loss 0.11924675852060318 Accuracy 0.5975000262260437\n",
      "Iteration 310 Training loss 0.11969690024852753 Validation loss 0.11919678002595901 Accuracy 0.5979999899864197\n",
      "Iteration 320 Training loss 0.11985879391431808 Validation loss 0.12051238864660263 Accuracy 0.5918333530426025\n",
      "Iteration 330 Training loss 0.11918267607688904 Validation loss 0.11934798210859299 Accuracy 0.5958333611488342\n",
      "Iteration 340 Training loss 0.11952758580446243 Validation loss 0.1187669038772583 Accuracy 0.6019999980926514\n",
      "Iteration 350 Training loss 0.12010148167610168 Validation loss 0.11864504963159561 Accuracy 0.6013333201408386\n",
      "Iteration 360 Training loss 0.11891479790210724 Validation loss 0.11850376427173615 Accuracy 0.6031666398048401\n",
      "Iteration 370 Training loss 0.1190543919801712 Validation loss 0.11854714155197144 Accuracy 0.6043333411216736\n",
      "Iteration 380 Training loss 0.11910131573677063 Validation loss 0.11939687281847 Accuracy 0.596833348274231\n",
      "Iteration 390 Training loss 0.11853572726249695 Validation loss 0.11850228160619736 Accuracy 0.6021666526794434\n",
      "Iteration 400 Training loss 0.1197514459490776 Validation loss 0.1184803694486618 Accuracy 0.6073333621025085\n",
      "Iteration 410 Training loss 0.11872733384370804 Validation loss 0.11841688305139542 Accuracy 0.606333315372467\n",
      "Iteration 420 Training loss 0.11928129196166992 Validation loss 0.12097003310918808 Accuracy 0.5964999794960022\n",
      "Iteration 430 Training loss 0.11840659379959106 Validation loss 0.11862630397081375 Accuracy 0.6021666526794434\n",
      "Iteration 440 Training loss 0.1187005341053009 Validation loss 0.11891056597232819 Accuracy 0.6004999876022339\n",
      "Iteration 450 Training loss 0.11821197718381882 Validation loss 0.11895262449979782 Accuracy 0.5998333096504211\n",
      "Iteration 460 Training loss 0.117900550365448 Validation loss 0.11819197237491608 Accuracy 0.6081666946411133\n",
      "Iteration 470 Training loss 0.11860008537769318 Validation loss 0.11826198548078537 Accuracy 0.6075000166893005\n",
      "Iteration 480 Training loss 0.11854538321495056 Validation loss 0.11981939524412155 Accuracy 0.6010000109672546\n",
      "Iteration 490 Training loss 0.11770271509885788 Validation loss 0.11803513765335083 Accuracy 0.6081666946411133\n",
      "Iteration 500 Training loss 0.1180649995803833 Validation loss 0.11890895664691925 Accuracy 0.6010000109672546\n",
      "Iteration 510 Training loss 0.11711529642343521 Validation loss 0.1179114356637001 Accuracy 0.6103333234786987\n",
      "Iteration 520 Training loss 0.11730816960334778 Validation loss 0.11807507276535034 Accuracy 0.6086666584014893\n",
      "Iteration 530 Training loss 0.11938727647066116 Validation loss 0.11922270059585571 Accuracy 0.6058333516120911\n",
      "Iteration 540 Training loss 0.11887345463037491 Validation loss 0.1190812960267067 Accuracy 0.609000027179718\n",
      "Iteration 550 Training loss 0.11803241074085236 Validation loss 0.1182461827993393 Accuracy 0.6116666793823242\n",
      "Iteration 560 Training loss 0.11832757294178009 Validation loss 0.11778468638658524 Accuracy 0.609666645526886\n",
      "Iteration 570 Training loss 0.1171787902712822 Validation loss 0.11769356578588486 Accuracy 0.6110000014305115\n",
      "Iteration 580 Training loss 0.11950934678316116 Validation loss 0.11850452423095703 Accuracy 0.6101666688919067\n",
      "Iteration 590 Training loss 0.11870471388101578 Validation loss 0.11810313165187836 Accuracy 0.60916668176651\n",
      "Iteration 600 Training loss 0.11755195260047913 Validation loss 0.11748504638671875 Accuracy 0.6115000247955322\n",
      "Iteration 610 Training loss 0.11800510436296463 Validation loss 0.11906198412179947 Accuracy 0.6073333621025085\n",
      "Iteration 620 Training loss 0.11711246520280838 Validation loss 0.11899285763502121 Accuracy 0.6025000214576721\n",
      "Iteration 630 Training loss 0.11789108067750931 Validation loss 0.12101433426141739 Accuracy 0.5985000133514404\n",
      "Iteration 640 Training loss 0.11659238487482071 Validation loss 0.1179981455206871 Accuracy 0.609499990940094\n",
      "Iteration 650 Training loss 0.1174139603972435 Validation loss 0.11738903820514679 Accuracy 0.6153333187103271\n",
      "Iteration 660 Training loss 0.11623508483171463 Validation loss 0.11762459576129913 Accuracy 0.6121666431427002\n",
      "Iteration 670 Training loss 0.11629480123519897 Validation loss 0.11803147196769714 Accuracy 0.6098333597183228\n",
      "Iteration 680 Training loss 0.11689087748527527 Validation loss 0.11742367595434189 Accuracy 0.6156666874885559\n",
      "Iteration 690 Training loss 0.11780542135238647 Validation loss 0.11790207028388977 Accuracy 0.6150000095367432\n",
      "Iteration 700 Training loss 0.11689490079879761 Validation loss 0.11731202155351639 Accuracy 0.6153333187103271\n",
      "Iteration 710 Training loss 0.11765405535697937 Validation loss 0.11767905950546265 Accuracy 0.6143333315849304\n",
      "Iteration 720 Training loss 0.11733190715312958 Validation loss 0.11876890808343887 Accuracy 0.6111666560173035\n",
      "Iteration 730 Training loss 0.1170773133635521 Validation loss 0.11713191866874695 Accuracy 0.6154999732971191\n",
      "Iteration 740 Training loss 0.1163364052772522 Validation loss 0.11709535866975784 Accuracy 0.6165000200271606\n",
      "Iteration 750 Training loss 0.11697392910718918 Validation loss 0.11741966009140015 Accuracy 0.6140000224113464\n",
      "Iteration 760 Training loss 0.11706044524908066 Validation loss 0.11863233149051666 Accuracy 0.6111666560173035\n",
      "Iteration 770 Training loss 0.11708080023527145 Validation loss 0.11778281629085541 Accuracy 0.6144999861717224\n",
      "Iteration 780 Training loss 0.1170864850282669 Validation loss 0.11900218576192856 Accuracy 0.6133333444595337\n",
      "Iteration 790 Training loss 0.1165156438946724 Validation loss 0.11736271530389786 Accuracy 0.6168333292007446\n",
      "Iteration 800 Training loss 0.1164127066731453 Validation loss 0.11715961992740631 Accuracy 0.6188333630561829\n",
      "Iteration 810 Training loss 0.11737547069787979 Validation loss 0.11818592995405197 Accuracy 0.6118333339691162\n",
      "Iteration 820 Training loss 0.11659456044435501 Validation loss 0.1169847622513771 Accuracy 0.6148333549499512\n",
      "Iteration 830 Training loss 0.11709419637918472 Validation loss 0.11925988644361496 Accuracy 0.6108333468437195\n",
      "Iteration 840 Training loss 0.11810550093650818 Validation loss 0.12174536287784576 Accuracy 0.6031666398048401\n",
      "Iteration 850 Training loss 0.11521460115909576 Validation loss 0.11668289452791214 Accuracy 0.6194999814033508\n",
      "Iteration 860 Training loss 0.11617883294820786 Validation loss 0.1168520376086235 Accuracy 0.6138333082199097\n",
      "Iteration 870 Training loss 0.1163228377699852 Validation loss 0.11867479234933853 Accuracy 0.6176666617393494\n",
      "Iteration 880 Training loss 0.11546175181865692 Validation loss 0.11731880903244019 Accuracy 0.6205000281333923\n",
      "Iteration 890 Training loss 0.11650935560464859 Validation loss 0.11937902122735977 Accuracy 0.6133333444595337\n",
      "Iteration 900 Training loss 0.11605456471443176 Validation loss 0.1173672080039978 Accuracy 0.6141666769981384\n",
      "Iteration 910 Training loss 0.11530573666095734 Validation loss 0.1167537048459053 Accuracy 0.6188333630561829\n",
      "Iteration 920 Training loss 0.11624526232481003 Validation loss 0.11928501725196838 Accuracy 0.6133333444595337\n",
      "Iteration 930 Training loss 0.11615288257598877 Validation loss 0.11689836531877518 Accuracy 0.6179999709129333\n",
      "Iteration 940 Training loss 0.11622215807437897 Validation loss 0.11678634583950043 Accuracy 0.6196666955947876\n",
      "Iteration 950 Training loss 0.11496679484844208 Validation loss 0.11706314980983734 Accuracy 0.6194999814033508\n",
      "Iteration 960 Training loss 0.11591546982526779 Validation loss 0.11801109462976456 Accuracy 0.6215000152587891\n",
      "Iteration 970 Training loss 0.11600962281227112 Validation loss 0.1170390397310257 Accuracy 0.6181666851043701\n",
      "Iteration 980 Training loss 0.11602839082479477 Validation loss 0.11712819337844849 Accuracy 0.6194999814033508\n",
      "Iteration 990 Training loss 0.11425388604402542 Validation loss 0.11651719361543655 Accuracy 0.6194999814033508\n",
      "Iteration 1000 Training loss 0.11804724484682083 Validation loss 0.12229903042316437 Accuracy 0.6068333387374878\n",
      "Iteration 1010 Training loss 0.1139778345823288 Validation loss 0.11662760376930237 Accuracy 0.6193333268165588\n",
      "Iteration 1020 Training loss 0.11602384597063065 Validation loss 0.11639164388179779 Accuracy 0.6205000281333923\n",
      "Iteration 1030 Training loss 0.11565051227807999 Validation loss 0.11645191162824631 Accuracy 0.6196666955947876\n",
      "Iteration 1040 Training loss 0.11716290563344955 Validation loss 0.11902971565723419 Accuracy 0.6148333549499512\n",
      "Iteration 1050 Training loss 0.11530847102403641 Validation loss 0.11644808202981949 Accuracy 0.6194999814033508\n",
      "Iteration 1060 Training loss 0.11525464802980423 Validation loss 0.1168745830655098 Accuracy 0.6191666722297668\n",
      "Iteration 1070 Training loss 0.11417596787214279 Validation loss 0.11754713952541351 Accuracy 0.6190000176429749\n",
      "Iteration 1080 Training loss 0.11454233527183533 Validation loss 0.11624199151992798 Accuracy 0.6193333268165588\n",
      "Iteration 1090 Training loss 0.11696802824735641 Validation loss 0.12401051074266434 Accuracy 0.6083333492279053\n",
      "Iteration 1100 Training loss 0.11448925733566284 Validation loss 0.11649583280086517 Accuracy 0.6208333373069763\n",
      "Iteration 1110 Training loss 0.11623858660459518 Validation loss 0.11993776261806488 Accuracy 0.6171666383743286\n",
      "Iteration 1120 Training loss 0.11456983536481857 Validation loss 0.11627806723117828 Accuracy 0.6205000281333923\n",
      "Iteration 1130 Training loss 0.11628375947475433 Validation loss 0.11774279922246933 Accuracy 0.6200000047683716\n",
      "Iteration 1140 Training loss 0.11473637819290161 Validation loss 0.11635012924671173 Accuracy 0.6213333606719971\n",
      "Iteration 1150 Training loss 0.11462768912315369 Validation loss 0.11638163030147552 Accuracy 0.6226666569709778\n",
      "Iteration 1160 Training loss 0.11451888084411621 Validation loss 0.11680666357278824 Accuracy 0.621666669845581\n",
      "Iteration 1170 Training loss 0.11529532819986343 Validation loss 0.11681181192398071 Accuracy 0.6215000152587891\n",
      "Iteration 1180 Training loss 0.11447399854660034 Validation loss 0.11646699905395508 Accuracy 0.6241666674613953\n",
      "Iteration 1190 Training loss 0.11382057517766953 Validation loss 0.11812293529510498 Accuracy 0.6228333115577698\n",
      "Iteration 1200 Training loss 0.11283142864704132 Validation loss 0.11632396280765533 Accuracy 0.6230000257492065\n",
      "Iteration 1210 Training loss 0.11477906256914139 Validation loss 0.11690426617860794 Accuracy 0.6241666674613953\n",
      "Iteration 1220 Training loss 0.11522505432367325 Validation loss 0.11652638018131256 Accuracy 0.6244999766349792\n",
      "Iteration 1230 Training loss 0.115959033370018 Validation loss 0.11975159496068954 Accuracy 0.6168333292007446\n",
      "Iteration 1240 Training loss 0.11510784924030304 Validation loss 0.11654328554868698 Accuracy 0.624666690826416\n",
      "Iteration 1250 Training loss 0.11431358009576797 Validation loss 0.11599639058113098 Accuracy 0.6240000128746033\n",
      "Iteration 1260 Training loss 0.11458837240934372 Validation loss 0.11607863008975983 Accuracy 0.6228333115577698\n",
      "Iteration 1270 Training loss 0.11583767831325531 Validation loss 0.11875467747449875 Accuracy 0.621999979019165\n",
      "Iteration 1280 Training loss 0.11376502364873886 Validation loss 0.11912097036838531 Accuracy 0.621999979019165\n",
      "Iteration 1290 Training loss 0.11336652934551239 Validation loss 0.11949937045574188 Accuracy 0.6198333501815796\n",
      "Iteration 1300 Training loss 0.1140412837266922 Validation loss 0.11636697500944138 Accuracy 0.6263333559036255\n",
      "Iteration 1310 Training loss 0.11436309665441513 Validation loss 0.11698425561189651 Accuracy 0.624666690826416\n",
      "Iteration 1320 Training loss 0.11352431029081345 Validation loss 0.11602050065994263 Accuracy 0.6228333115577698\n",
      "Iteration 1330 Training loss 0.11542665958404541 Validation loss 0.12020578980445862 Accuracy 0.6191666722297668\n",
      "Iteration 1340 Training loss 0.11365150660276413 Validation loss 0.12035331130027771 Accuracy 0.6201666593551636\n",
      "Iteration 1350 Training loss 0.11376555263996124 Validation loss 0.11610718816518784 Accuracy 0.6238333582878113\n",
      "Iteration 1360 Training loss 0.11300679296255112 Validation loss 0.11728902906179428 Accuracy 0.6265000104904175\n",
      "Iteration 1370 Training loss 0.11273842304944992 Validation loss 0.11611077189445496 Accuracy 0.6258333325386047\n",
      "Iteration 1380 Training loss 0.11693374812602997 Validation loss 0.12924517691135406 Accuracy 0.5963333249092102\n",
      "Iteration 1390 Training loss 0.11273572593927383 Validation loss 0.11620879918336868 Accuracy 0.6286666393280029\n",
      "Iteration 1400 Training loss 0.11358717828989029 Validation loss 0.11656869202852249 Accuracy 0.6294999718666077\n",
      "Iteration 1410 Training loss 0.1138957142829895 Validation loss 0.11617348343133926 Accuracy 0.6274999976158142\n",
      "Iteration 1420 Training loss 0.11381173878908157 Validation loss 0.1160261258482933 Accuracy 0.628333330154419\n",
      "Iteration 1430 Training loss 0.11432795971632004 Validation loss 0.11779478937387466 Accuracy 0.6241666674613953\n",
      "Iteration 1440 Training loss 0.11497902125120163 Validation loss 0.117624431848526 Accuracy 0.6236666440963745\n",
      "Iteration 1450 Training loss 0.11536932736635208 Validation loss 0.11628258973360062 Accuracy 0.6276666522026062\n",
      "Iteration 1460 Training loss 0.115030437707901 Validation loss 0.11724308133125305 Accuracy 0.6240000128746033\n",
      "Iteration 1470 Training loss 0.11369761824607849 Validation loss 0.1155889481306076 Accuracy 0.6288333535194397\n",
      "Iteration 1480 Training loss 0.1132165938615799 Validation loss 0.11590537428855896 Accuracy 0.6306666731834412\n",
      "Iteration 1490 Training loss 0.11263483017683029 Validation loss 0.11602092534303665 Accuracy 0.6313333511352539\n",
      "Iteration 1500 Training loss 0.11465421319007874 Validation loss 0.11678196489810944 Accuracy 0.6284999847412109\n",
      "Iteration 1510 Training loss 0.11375583708286285 Validation loss 0.11647562682628632 Accuracy 0.6274999976158142\n",
      "Iteration 1520 Training loss 0.11431049555540085 Validation loss 0.12115826457738876 Accuracy 0.6193333268165588\n",
      "Iteration 1530 Training loss 0.11539570987224579 Validation loss 0.1239313930273056 Accuracy 0.6134999990463257\n",
      "Iteration 1540 Training loss 0.11390312016010284 Validation loss 0.11678140610456467 Accuracy 0.6265000104904175\n",
      "Iteration 1550 Training loss 0.11259321868419647 Validation loss 0.11587581038475037 Accuracy 0.6309999823570251\n",
      "Iteration 1560 Training loss 0.11695699393749237 Validation loss 0.13364601135253906 Accuracy 0.5864999890327454\n",
      "Iteration 1570 Training loss 0.11223104596138 Validation loss 0.11498556286096573 Accuracy 0.6298333406448364\n",
      "Iteration 1580 Training loss 0.11281579732894897 Validation loss 0.11705061793327332 Accuracy 0.6269999742507935\n",
      "Iteration 1590 Training loss 0.11396010220050812 Validation loss 0.12466026097536087 Accuracy 0.6103333234786987\n",
      "Iteration 1600 Training loss 0.11396848410367966 Validation loss 0.11994685977697372 Accuracy 0.6230000257492065\n",
      "Iteration 1610 Training loss 0.11438305675983429 Validation loss 0.12396223843097687 Accuracy 0.6134999990463257\n",
      "Iteration 1620 Training loss 0.11300172656774521 Validation loss 0.11515705287456512 Accuracy 0.6321666836738586\n",
      "Iteration 1630 Training loss 0.11494771391153336 Validation loss 0.11837498843669891 Accuracy 0.6238333582878113\n",
      "Iteration 1640 Training loss 0.11162333190441132 Validation loss 0.11471796780824661 Accuracy 0.6313333511352539\n",
      "Iteration 1650 Training loss 0.1134963110089302 Validation loss 0.12359902262687683 Accuracy 0.6136666536331177\n",
      "Iteration 1660 Training loss 0.11461789160966873 Validation loss 0.12287594377994537 Accuracy 0.6153333187103271\n",
      "Iteration 1670 Training loss 0.11355479061603546 Validation loss 0.12250380218029022 Accuracy 0.6159999966621399\n",
      "Iteration 1680 Training loss 0.11264127492904663 Validation loss 0.11837835609912872 Accuracy 0.6241666674613953\n",
      "Iteration 1690 Training loss 0.11154260486364365 Validation loss 0.11597774177789688 Accuracy 0.6315000057220459\n",
      "Iteration 1700 Training loss 0.11226564645767212 Validation loss 0.12281272560358047 Accuracy 0.6148333549499512\n",
      "Iteration 1710 Training loss 0.11439459025859833 Validation loss 0.12610593438148499 Accuracy 0.6086666584014893\n",
      "Iteration 1720 Training loss 0.11217465996742249 Validation loss 0.12004601955413818 Accuracy 0.6236666440963745\n",
      "Iteration 1730 Training loss 0.11568664759397507 Validation loss 0.12513399124145508 Accuracy 0.6086666584014893\n",
      "Iteration 1740 Training loss 0.11371749639511108 Validation loss 0.12182183563709259 Accuracy 0.6188333630561829\n",
      "Iteration 1750 Training loss 0.11219006031751633 Validation loss 0.11713013052940369 Accuracy 0.6290000081062317\n",
      "Iteration 1760 Training loss 0.11333902180194855 Validation loss 0.11478421837091446 Accuracy 0.6313333511352539\n",
      "Iteration 1770 Training loss 0.11191029101610184 Validation loss 0.11506305634975433 Accuracy 0.6351666450500488\n",
      "Iteration 1780 Training loss 0.11490237712860107 Validation loss 0.1171671524643898 Accuracy 0.6305000185966492\n",
      "Iteration 1790 Training loss 0.11241358518600464 Validation loss 0.11461572349071503 Accuracy 0.6328333616256714\n",
      "Iteration 1800 Training loss 0.11241711676120758 Validation loss 0.11468842625617981 Accuracy 0.6343333125114441\n",
      "Iteration 1810 Training loss 0.11360055208206177 Validation loss 0.1150704175233841 Accuracy 0.6336666941642761\n",
      "Iteration 1820 Training loss 0.11338458955287933 Validation loss 0.11641982197761536 Accuracy 0.6326666474342346\n",
      "Iteration 1830 Training loss 0.11222017556428909 Validation loss 0.11588140577077866 Accuracy 0.6333333253860474\n",
      "Iteration 1840 Training loss 0.11518160998821259 Validation loss 0.12946142256259918 Accuracy 0.6008333563804626\n",
      "Iteration 1850 Training loss 0.11130495369434357 Validation loss 0.11922399699687958 Accuracy 0.6263333559036255\n",
      "Iteration 1860 Training loss 0.11319220066070557 Validation loss 0.12027476727962494 Accuracy 0.6234999895095825\n",
      "Iteration 1870 Training loss 0.11184921115636826 Validation loss 0.12310109287500381 Accuracy 0.6196666955947876\n",
      "Iteration 1880 Training loss 0.11170170456171036 Validation loss 0.12008731812238693 Accuracy 0.6266666650772095\n",
      "Iteration 1890 Training loss 0.11282416433095932 Validation loss 0.12333119660615921 Accuracy 0.6168333292007446\n",
      "Iteration 1900 Training loss 0.11144997179508209 Validation loss 0.12076534330844879 Accuracy 0.6240000128746033\n",
      "Iteration 1910 Training loss 0.11129032075405121 Validation loss 0.1188114807009697 Accuracy 0.6265000104904175\n",
      "Iteration 1920 Training loss 0.11190299689769745 Validation loss 0.11471250653266907 Accuracy 0.6368333101272583\n",
      "Iteration 1930 Training loss 0.1122327670454979 Validation loss 0.1146918311715126 Accuracy 0.6358333230018616\n",
      "Iteration 1940 Training loss 0.10953612625598907 Validation loss 0.11732743680477142 Accuracy 0.6303333044052124\n",
      "Iteration 1950 Training loss 0.11160389333963394 Validation loss 0.11447102576494217 Accuracy 0.6363333463668823\n",
      "Iteration 1960 Training loss 0.11340846866369247 Validation loss 0.1153649091720581 Accuracy 0.6340000033378601\n",
      "Iteration 1970 Training loss 0.11271236836910248 Validation loss 0.11495286971330643 Accuracy 0.637333333492279\n",
      "Iteration 1980 Training loss 0.11080779880285263 Validation loss 0.11392085999250412 Accuracy 0.637666642665863\n",
      "Iteration 1990 Training loss 0.11113489419221878 Validation loss 0.11804138123989105 Accuracy 0.6271666884422302\n",
      "Iteration 2000 Training loss 0.11179162561893463 Validation loss 0.11855344474315643 Accuracy 0.6271666884422302\n",
      "Iteration 2010 Training loss 0.11242049932479858 Validation loss 0.11847344040870667 Accuracy 0.628333330154419\n",
      "Iteration 2020 Training loss 0.1113571971654892 Validation loss 0.1184752807021141 Accuracy 0.6288333535194397\n",
      "Iteration 2030 Training loss 0.11243201792240143 Validation loss 0.12549622356891632 Accuracy 0.6148333549499512\n",
      "Iteration 2040 Training loss 0.10954665392637253 Validation loss 0.11981189250946045 Accuracy 0.6273333430290222\n",
      "Iteration 2050 Training loss 0.11098545044660568 Validation loss 0.11924050003290176 Accuracy 0.628000020980835\n",
      "Iteration 2060 Training loss 0.11166184395551682 Validation loss 0.12633880972862244 Accuracy 0.609499990940094\n",
      "Iteration 2070 Training loss 0.11212801188230515 Validation loss 0.12374777346849442 Accuracy 0.6184999942779541\n",
      "Iteration 2080 Training loss 0.1095568835735321 Validation loss 0.11734375357627869 Accuracy 0.6309999823570251\n",
      "Iteration 2090 Training loss 0.11106321215629578 Validation loss 0.12460032105445862 Accuracy 0.6134999990463257\n",
      "Iteration 2100 Training loss 0.11158506572246552 Validation loss 0.12228317558765411 Accuracy 0.621833324432373\n",
      "Iteration 2110 Training loss 0.1118650883436203 Validation loss 0.11895138770341873 Accuracy 0.6313333511352539\n",
      "Iteration 2120 Training loss 0.11203654110431671 Validation loss 0.1227622777223587 Accuracy 0.6191666722297668\n",
      "Iteration 2130 Training loss 0.10976402461528778 Validation loss 0.12466035038232803 Accuracy 0.6138333082199097\n",
      "Iteration 2140 Training loss 0.11255452781915665 Validation loss 0.12467744946479797 Accuracy 0.6151666641235352\n",
      "Iteration 2150 Training loss 0.111430823802948 Validation loss 0.11494021862745285 Accuracy 0.6378333568572998\n",
      "Iteration 2160 Training loss 0.11138860881328583 Validation loss 0.1141013354063034 Accuracy 0.640999972820282\n",
      "Iteration 2170 Training loss 0.11001584678888321 Validation loss 0.1170109435915947 Accuracy 0.6343333125114441\n",
      "Iteration 2180 Training loss 0.11379294097423553 Validation loss 0.13076040148735046 Accuracy 0.6036666631698608\n",
      "Iteration 2190 Training loss 0.11003691703081131 Validation loss 0.11897554248571396 Accuracy 0.6291666626930237\n",
      "Iteration 2200 Training loss 0.11161297559738159 Validation loss 0.11721991747617722 Accuracy 0.6330000162124634\n",
      "Iteration 2210 Training loss 0.11175083369016647 Validation loss 0.12276122719049454 Accuracy 0.6228333115577698\n",
      "Iteration 2220 Training loss 0.11152826994657516 Validation loss 0.12114576250314713 Accuracy 0.6244999766349792\n",
      "Iteration 2230 Training loss 0.10991538316011429 Validation loss 0.11936318129301071 Accuracy 0.6293333172798157\n",
      "Iteration 2240 Training loss 0.10932943969964981 Validation loss 0.11669891327619553 Accuracy 0.6356666684150696\n",
      "Iteration 2250 Training loss 0.11254281550645828 Validation loss 0.11544997990131378 Accuracy 0.6380000114440918\n",
      "Iteration 2260 Training loss 0.11065000295639038 Validation loss 0.11369488388299942 Accuracy 0.6396666765213013\n",
      "Iteration 2270 Training loss 0.11039787530899048 Validation loss 0.12004359811544418 Accuracy 0.6298333406448364\n",
      "Iteration 2280 Training loss 0.11188827455043793 Validation loss 0.13302260637283325 Accuracy 0.5986666679382324\n",
      "Iteration 2290 Training loss 0.11052728444337845 Validation loss 0.11526336520910263 Accuracy 0.6380000114440918\n",
      "Iteration 2300 Training loss 0.11144900321960449 Validation loss 0.1275240033864975 Accuracy 0.6106666922569275\n",
      "Iteration 2310 Training loss 0.11293967813253403 Validation loss 0.12715007364749908 Accuracy 0.6108333468437195\n",
      "Iteration 2320 Training loss 0.1103801503777504 Validation loss 0.11984643340110779 Accuracy 0.628333330154419\n",
      "Iteration 2330 Training loss 0.10963767766952515 Validation loss 0.12025958299636841 Accuracy 0.6271666884422302\n",
      "Iteration 2340 Training loss 0.11028368771076202 Validation loss 0.11595771461725235 Accuracy 0.6370000243186951\n",
      "Iteration 2350 Training loss 0.11491776257753372 Validation loss 0.12037107348442078 Accuracy 0.6284999847412109\n",
      "Iteration 2360 Training loss 0.11223054677248001 Validation loss 0.114848792552948 Accuracy 0.6401666402816772\n",
      "Iteration 2370 Training loss 0.11188001185655594 Validation loss 0.11331836134195328 Accuracy 0.6445000171661377\n",
      "Iteration 2380 Training loss 0.11100900918245316 Validation loss 0.1140500083565712 Accuracy 0.6428333520889282\n",
      "Iteration 2390 Training loss 0.10921801626682281 Validation loss 0.11770911514759064 Accuracy 0.6336666941642761\n",
      "Iteration 2400 Training loss 0.11002956330776215 Validation loss 0.1142737939953804 Accuracy 0.6430000066757202\n",
      "Iteration 2410 Training loss 0.1106618270277977 Validation loss 0.11379186064004898 Accuracy 0.6448333263397217\n",
      "Iteration 2420 Training loss 0.10915784537792206 Validation loss 0.11322955042123795 Accuracy 0.6430000066757202\n",
      "Iteration 2430 Training loss 0.10869736969470978 Validation loss 0.1158749908208847 Accuracy 0.6383333206176758\n",
      "Iteration 2440 Training loss 0.10935814678668976 Validation loss 0.11492791771888733 Accuracy 0.6388333439826965\n",
      "Iteration 2450 Training loss 0.11084600538015366 Validation loss 0.11492949724197388 Accuracy 0.6420000195503235\n",
      "Iteration 2460 Training loss 0.11256508529186249 Validation loss 0.11394107341766357 Accuracy 0.6423333287239075\n",
      "Iteration 2470 Training loss 0.11269675940275192 Validation loss 0.11325683444738388 Accuracy 0.6443333625793457\n",
      "Iteration 2480 Training loss 0.11100989580154419 Validation loss 0.11307650059461594 Accuracy 0.6441666483879089\n",
      "Iteration 2490 Training loss 0.11010982096195221 Validation loss 0.11282911151647568 Accuracy 0.6443333625793457\n",
      "Iteration 2500 Training loss 0.10916490852832794 Validation loss 0.11410858482122421 Accuracy 0.6393333077430725\n",
      "Iteration 2510 Training loss 0.1112290620803833 Validation loss 0.11498293280601501 Accuracy 0.6411666870117188\n",
      "Iteration 2520 Training loss 0.10972119867801666 Validation loss 0.11295942217111588 Accuracy 0.6445000171661377\n",
      "Iteration 2530 Training loss 0.10931842029094696 Validation loss 0.11701096594333649 Accuracy 0.6384999752044678\n",
      "Iteration 2540 Training loss 0.10923819243907928 Validation loss 0.11581685394048691 Accuracy 0.6386666893959045\n",
      "Iteration 2550 Training loss 0.11327982693910599 Validation loss 0.1145046204328537 Accuracy 0.6416666507720947\n",
      "Iteration 2560 Training loss 0.11095753312110901 Validation loss 0.11307680606842041 Accuracy 0.6466666460037231\n",
      "Iteration 2570 Training loss 0.10911083966493607 Validation loss 0.11356457322835922 Accuracy 0.6439999938011169\n",
      "Iteration 2580 Training loss 0.11474890261888504 Validation loss 0.11429762840270996 Accuracy 0.6416666507720947\n",
      "Iteration 2590 Training loss 0.10899386554956436 Validation loss 0.11299970746040344 Accuracy 0.6448333263397217\n",
      "Iteration 2600 Training loss 0.10908544063568115 Validation loss 0.1132725402712822 Accuracy 0.6470000147819519\n",
      "Iteration 2610 Training loss 0.11034989356994629 Validation loss 0.11356799304485321 Accuracy 0.6439999938011169\n",
      "Iteration 2620 Training loss 0.10923711955547333 Validation loss 0.11319331079721451 Accuracy 0.6458333134651184\n",
      "Iteration 2630 Training loss 0.10942718386650085 Validation loss 0.11331724375486374 Accuracy 0.6456666588783264\n",
      "Iteration 2640 Training loss 0.10981988161802292 Validation loss 0.11332959681749344 Accuracy 0.6471666693687439\n",
      "Iteration 2650 Training loss 0.11305230855941772 Validation loss 0.11534979939460754 Accuracy 0.64083331823349\n",
      "Iteration 2660 Training loss 0.10892798751592636 Validation loss 0.11296723783016205 Accuracy 0.6471666693687439\n",
      "Iteration 2670 Training loss 0.10976451635360718 Validation loss 0.11287909001111984 Accuracy 0.6474999785423279\n",
      "Iteration 2680 Training loss 0.1088850274682045 Validation loss 0.1131080761551857 Accuracy 0.6488333344459534\n",
      "Iteration 2690 Training loss 0.1102939248085022 Validation loss 0.11385223269462585 Accuracy 0.6428333520889282\n",
      "Iteration 2700 Training loss 0.11085351556539536 Validation loss 0.11357185989618301 Accuracy 0.6441666483879089\n",
      "Iteration 2710 Training loss 0.10805942863225937 Validation loss 0.1133548766374588 Accuracy 0.6458333134651184\n",
      "Iteration 2720 Training loss 0.10778163373470306 Validation loss 0.11288909614086151 Accuracy 0.6481666564941406\n",
      "Iteration 2730 Training loss 0.10982328653335571 Validation loss 0.11338187754154205 Accuracy 0.6453333497047424\n",
      "Iteration 2740 Training loss 0.10812018066644669 Validation loss 0.11299793422222137 Accuracy 0.6470000147819519\n",
      "Iteration 2750 Training loss 0.11194614320993423 Validation loss 0.1149754449725151 Accuracy 0.6421666741371155\n",
      "Iteration 2760 Training loss 0.10814107209444046 Validation loss 0.11284720152616501 Accuracy 0.6488333344459534\n",
      "Iteration 2770 Training loss 0.10799210518598557 Validation loss 0.11359688639640808 Accuracy 0.6453333497047424\n",
      "Iteration 2780 Training loss 0.11054582893848419 Validation loss 0.12806296348571777 Accuracy 0.6141666769981384\n",
      "Iteration 2790 Training loss 0.10760074853897095 Validation loss 0.12288600951433182 Accuracy 0.6256666779518127\n",
      "Iteration 2800 Training loss 0.10756050795316696 Validation loss 0.1234910786151886 Accuracy 0.6259999871253967\n",
      "Iteration 2810 Training loss 0.10935996472835541 Validation loss 0.12550516426563263 Accuracy 0.6198333501815796\n",
      "Iteration 2820 Training loss 0.10812801122665405 Validation loss 0.1200321689248085 Accuracy 0.6309999823570251\n",
      "Iteration 2830 Training loss 0.10847175121307373 Validation loss 0.11322495341300964 Accuracy 0.6480000019073486\n",
      "Iteration 2840 Training loss 0.10913871228694916 Validation loss 0.11311367899179459 Accuracy 0.6470000147819519\n",
      "Iteration 2850 Training loss 0.10783801972866058 Validation loss 0.11258769035339355 Accuracy 0.6486666798591614\n",
      "Iteration 2860 Training loss 0.1060170903801918 Validation loss 0.11622344702482224 Accuracy 0.6399999856948853\n",
      "Iteration 2870 Training loss 0.10706476867198944 Validation loss 0.11387860029935837 Accuracy 0.6470000147819519\n",
      "Iteration 2880 Training loss 0.10942734032869339 Validation loss 0.11293965578079224 Accuracy 0.6461666822433472\n",
      "Iteration 2890 Training loss 0.1100551038980484 Validation loss 0.11317140609025955 Accuracy 0.6473333239555359\n",
      "Iteration 2900 Training loss 0.10968994349241257 Validation loss 0.11283465474843979 Accuracy 0.6498333215713501\n",
      "Iteration 2910 Training loss 0.10912571102380753 Validation loss 0.11345020681619644 Accuracy 0.6488333344459534\n",
      "Iteration 2920 Training loss 0.10842223465442657 Validation loss 0.11334818601608276 Accuracy 0.6485000252723694\n",
      "Iteration 2930 Training loss 0.1093873530626297 Validation loss 0.1126747727394104 Accuracy 0.6473333239555359\n",
      "Iteration 2940 Training loss 0.10784421116113663 Validation loss 0.11396655440330505 Accuracy 0.6468333601951599\n",
      "Iteration 2950 Training loss 0.10801326483488083 Validation loss 0.11237376183271408 Accuracy 0.6478333473205566\n",
      "Iteration 2960 Training loss 0.10887238383293152 Validation loss 0.11228632926940918 Accuracy 0.6476666927337646\n",
      "Iteration 2970 Training loss 0.10843978077173233 Validation loss 0.11264792829751968 Accuracy 0.6481666564941406\n",
      "Iteration 2980 Training loss 0.10896563529968262 Validation loss 0.11297919601202011 Accuracy 0.6481666564941406\n",
      "Iteration 2990 Training loss 0.1079353466629982 Validation loss 0.11262983083724976 Accuracy 0.6489999890327454\n",
      "Iteration 3000 Training loss 0.10865433514118195 Validation loss 0.1192861944437027 Accuracy 0.6346666812896729\n",
      "Iteration 3010 Training loss 0.11005455255508423 Validation loss 0.13215188682079315 Accuracy 0.6065000295639038\n",
      "Iteration 3020 Training loss 0.10828382521867752 Validation loss 0.12950626015663147 Accuracy 0.6131666898727417\n",
      "Iteration 3030 Training loss 0.10798273980617523 Validation loss 0.1191166415810585 Accuracy 0.6353333592414856\n",
      "Iteration 3040 Training loss 0.11054018139839172 Validation loss 0.13254740834236145 Accuracy 0.6079999804496765\n",
      "Iteration 3050 Training loss 0.10774645209312439 Validation loss 0.12208198010921478 Accuracy 0.6323333382606506\n",
      "Iteration 3060 Training loss 0.10785023868083954 Validation loss 0.12089505046606064 Accuracy 0.6343333125114441\n",
      "Iteration 3070 Training loss 0.11059486865997314 Validation loss 0.12860749661922455 Accuracy 0.6141666769981384\n",
      "Iteration 3080 Training loss 0.10950654000043869 Validation loss 0.13198092579841614 Accuracy 0.6066666841506958\n",
      "Iteration 3090 Training loss 0.10583484172821045 Validation loss 0.12011454999446869 Accuracy 0.6345000267028809\n",
      "Iteration 3100 Training loss 0.10881456732749939 Validation loss 0.12315474450588226 Accuracy 0.6255000233650208\n",
      "Iteration 3110 Training loss 0.10760803520679474 Validation loss 0.124171182513237 Accuracy 0.6233333349227905\n",
      "Iteration 3120 Training loss 0.10983641445636749 Validation loss 0.1293722540140152 Accuracy 0.612666666507721\n",
      "Iteration 3130 Training loss 0.10759436339139938 Validation loss 0.12631338834762573 Accuracy 0.6203333139419556\n",
      "Iteration 3140 Training loss 0.1083526611328125 Validation loss 0.12782208621501923 Accuracy 0.6165000200271606\n",
      "Iteration 3150 Training loss 0.10808683186769485 Validation loss 0.12283913046121597 Accuracy 0.6265000104904175\n",
      "Iteration 3160 Training loss 0.10707295686006546 Validation loss 0.11582877486944199 Accuracy 0.6439999938011169\n",
      "Iteration 3170 Training loss 0.10758237540721893 Validation loss 0.113021120429039 Accuracy 0.6510000228881836\n",
      "Iteration 3180 Training loss 0.1069982722401619 Validation loss 0.11212585121393204 Accuracy 0.6508333086967468\n",
      "Iteration 3190 Training loss 0.11029313504695892 Validation loss 0.112562395632267 Accuracy 0.6503333449363708\n",
      "Iteration 3200 Training loss 0.1062794178724289 Validation loss 0.11686068028211594 Accuracy 0.6416666507720947\n",
      "Iteration 3210 Training loss 0.10725782066583633 Validation loss 0.12293586879968643 Accuracy 0.6278333067893982\n",
      "Iteration 3220 Training loss 0.10812807083129883 Validation loss 0.11631665378808975 Accuracy 0.6430000066757202\n",
      "Iteration 3230 Training loss 0.10852517932653427 Validation loss 0.11392462998628616 Accuracy 0.6478333473205566\n",
      "Iteration 3240 Training loss 0.1089424416422844 Validation loss 0.11296850442886353 Accuracy 0.6506666541099548\n",
      "Iteration 3250 Training loss 0.10897783190011978 Validation loss 0.11287190765142441 Accuracy 0.6495000123977661\n",
      "Iteration 3260 Training loss 0.10851989686489105 Validation loss 0.11257805675268173 Accuracy 0.6516666412353516\n",
      "Iteration 3270 Training loss 0.10804342478513718 Validation loss 0.11262120306491852 Accuracy 0.6506666541099548\n",
      "Iteration 3280 Training loss 0.10695096105337143 Validation loss 0.11242130398750305 Accuracy 0.6501666903495789\n",
      "Iteration 3290 Training loss 0.10782849043607712 Validation loss 0.11224167048931122 Accuracy 0.6510000228881836\n",
      "Iteration 3300 Training loss 0.10873997211456299 Validation loss 0.11298278719186783 Accuracy 0.6501666903495789\n",
      "Iteration 3310 Training loss 0.10808970034122467 Validation loss 0.11219759285449982 Accuracy 0.6521666646003723\n",
      "Iteration 3320 Training loss 0.10692395269870758 Validation loss 0.11299894005060196 Accuracy 0.6504999995231628\n",
      "Iteration 3330 Training loss 0.10810088366270065 Validation loss 0.11261226236820221 Accuracy 0.6513333320617676\n",
      "Iteration 3340 Training loss 0.11008097976446152 Validation loss 0.11427099257707596 Accuracy 0.6511666774749756\n",
      "Iteration 3350 Training loss 0.10915602743625641 Validation loss 0.11183878779411316 Accuracy 0.6518333554267883\n",
      "Iteration 3360 Training loss 0.10700786113739014 Validation loss 0.11249671131372452 Accuracy 0.6520000100135803\n",
      "Iteration 3370 Training loss 0.10648553818464279 Validation loss 0.11206330358982086 Accuracy 0.6535000205039978\n",
      "Iteration 3380 Training loss 0.10676802694797516 Validation loss 0.11235988140106201 Accuracy 0.6514999866485596\n",
      "Iteration 3390 Training loss 0.10710717737674713 Validation loss 0.11212487518787384 Accuracy 0.6503333449363708\n",
      "Iteration 3400 Training loss 0.10569382458925247 Validation loss 0.11268565058708191 Accuracy 0.6521666646003723\n",
      "Iteration 3410 Training loss 0.1095147579908371 Validation loss 0.11452189832925797 Accuracy 0.6513333320617676\n",
      "Iteration 3420 Training loss 0.10597893595695496 Validation loss 0.11248797923326492 Accuracy 0.652999997138977\n",
      "Iteration 3430 Training loss 0.1088191419839859 Validation loss 0.11316943168640137 Accuracy 0.6501666903495789\n",
      "Iteration 3440 Training loss 0.10594695061445236 Validation loss 0.11329197138547897 Accuracy 0.6513333320617676\n",
      "Iteration 3450 Training loss 0.10820720344781876 Validation loss 0.1127026155591011 Accuracy 0.6499999761581421\n",
      "Iteration 3460 Training loss 0.10770247131586075 Validation loss 0.11231588572263718 Accuracy 0.6506666541099548\n",
      "Iteration 3470 Training loss 0.10407551378011703 Validation loss 0.11441198736429214 Accuracy 0.6499999761581421\n",
      "Iteration 3480 Training loss 0.10935980081558228 Validation loss 0.13243097066879272 Accuracy 0.6110000014305115\n",
      "Iteration 3490 Training loss 0.10858683288097382 Validation loss 0.12927046418190002 Accuracy 0.6154999732971191\n",
      "Iteration 3500 Training loss 0.10850103944540024 Validation loss 0.12699460983276367 Accuracy 0.6183333396911621\n",
      "Iteration 3510 Training loss 0.10935818403959274 Validation loss 0.13227622210979462 Accuracy 0.6113333106040955\n",
      "Iteration 3520 Training loss 0.10655499249696732 Validation loss 0.12322767078876495 Accuracy 0.6288333535194397\n",
      "Iteration 3530 Training loss 0.10482203215360641 Validation loss 0.12065313756465912 Accuracy 0.6358333230018616\n",
      "Iteration 3540 Training loss 0.10841186344623566 Validation loss 0.1333114057779312 Accuracy 0.6079999804496765\n",
      "Iteration 3550 Training loss 0.1084020808339119 Validation loss 0.1253228783607483 Accuracy 0.625333309173584\n",
      "Iteration 3560 Training loss 0.10674285888671875 Validation loss 0.12435687333345413 Accuracy 0.6268333196640015\n",
      "Iteration 3570 Training loss 0.10580909997224808 Validation loss 0.12360640615224838 Accuracy 0.6301666498184204\n",
      "Iteration 3580 Training loss 0.10854225605726242 Validation loss 0.13222140073776245 Accuracy 0.6121666431427002\n",
      "Iteration 3590 Training loss 0.10607632249593735 Validation loss 0.12318017333745956 Accuracy 0.6303333044052124\n",
      "Iteration 3600 Training loss 0.1069214716553688 Validation loss 0.118288055062294 Accuracy 0.6418333053588867\n",
      "Iteration 3610 Training loss 0.10609623044729233 Validation loss 0.11884834617376328 Accuracy 0.640999972820282\n",
      "Iteration 3620 Training loss 0.10617057979106903 Validation loss 0.12864306569099426 Accuracy 0.6178333163261414\n",
      "Iteration 3630 Training loss 0.10461274534463882 Validation loss 0.1194230169057846 Accuracy 0.640500009059906\n",
      "Iteration 3640 Training loss 0.1098036840558052 Validation loss 0.13058386743068695 Accuracy 0.6165000200271606\n",
      "Iteration 3650 Training loss 0.10665132850408554 Validation loss 0.12198513001203537 Accuracy 0.6313333511352539\n",
      "Iteration 3660 Training loss 0.1078331470489502 Validation loss 0.1255577802658081 Accuracy 0.6263333559036255\n",
      "Iteration 3670 Training loss 0.1064193993806839 Validation loss 0.12349974364042282 Accuracy 0.6306666731834412\n",
      "Iteration 3680 Training loss 0.10682681202888489 Validation loss 0.1252470463514328 Accuracy 0.625166654586792\n",
      "Iteration 3690 Training loss 0.10831862688064575 Validation loss 0.128981813788414 Accuracy 0.6166666746139526\n",
      "Iteration 3700 Training loss 0.1058395504951477 Validation loss 0.12004256248474121 Accuracy 0.6380000114440918\n",
      "Iteration 3710 Training loss 0.1093892902135849 Validation loss 0.1309778094291687 Accuracy 0.6128333210945129\n",
      "Iteration 3720 Training loss 0.10488570481538773 Validation loss 0.12197001278400421 Accuracy 0.6334999799728394\n",
      "Iteration 3730 Training loss 0.10974755883216858 Validation loss 0.13300129771232605 Accuracy 0.609499990940094\n",
      "Iteration 3740 Training loss 0.10528077185153961 Validation loss 0.12127510458230972 Accuracy 0.6351666450500488\n",
      "Iteration 3750 Training loss 0.106870137155056 Validation loss 0.13176964223384857 Accuracy 0.6133333444595337\n",
      "Iteration 3760 Training loss 0.10856299847364426 Validation loss 0.1291300505399704 Accuracy 0.6178333163261414\n",
      "Iteration 3770 Training loss 0.10637380182743073 Validation loss 0.12472929060459137 Accuracy 0.6269999742507935\n",
      "Iteration 3780 Training loss 0.10670209676027298 Validation loss 0.12560023367404938 Accuracy 0.6259999871253967\n",
      "Iteration 3790 Training loss 0.10826458036899567 Validation loss 0.129623144865036 Accuracy 0.6173333525657654\n",
      "Iteration 3800 Training loss 0.10817527025938034 Validation loss 0.13090775907039642 Accuracy 0.6143333315849304\n",
      "Iteration 3810 Training loss 0.1058841347694397 Validation loss 0.1213730052113533 Accuracy 0.6355000138282776\n",
      "Iteration 3820 Training loss 0.10779497027397156 Validation loss 0.13033853471279144 Accuracy 0.6165000200271606\n",
      "Iteration 3830 Training loss 0.10517758876085281 Validation loss 0.12961134314537048 Accuracy 0.6173333525657654\n",
      "Iteration 3840 Training loss 0.10853735357522964 Validation loss 0.13473880290985107 Accuracy 0.6076666712760925\n",
      "Iteration 3850 Training loss 0.1036461666226387 Validation loss 0.11980455368757248 Accuracy 0.640333354473114\n",
      "Iteration 3860 Training loss 0.10413212329149246 Validation loss 0.11483904719352722 Accuracy 0.6526666879653931\n",
      "Iteration 3870 Training loss 0.10723234713077545 Validation loss 0.12659235298633575 Accuracy 0.6233333349227905\n",
      "Iteration 3880 Training loss 0.10730806738138199 Validation loss 0.13024309277534485 Accuracy 0.6166666746139526\n",
      "Iteration 3890 Training loss 0.10606376826763153 Validation loss 0.1289905160665512 Accuracy 0.6201666593551636\n",
      "Iteration 3900 Training loss 0.11089702695608139 Validation loss 0.13239233195781708 Accuracy 0.612666666507721\n",
      "Iteration 3910 Training loss 0.104218028485775 Validation loss 0.12210263311862946 Accuracy 0.6345000267028809\n",
      "Iteration 3920 Training loss 0.1060774102807045 Validation loss 0.12281414866447449 Accuracy 0.6324999928474426\n",
      "Iteration 3930 Training loss 0.10778161883354187 Validation loss 0.1344606876373291 Accuracy 0.6101666688919067\n",
      "Iteration 3940 Training loss 0.10742126405239105 Validation loss 0.13463084399700165 Accuracy 0.609499990940094\n",
      "Iteration 3950 Training loss 0.10465766489505768 Validation loss 0.12099821120500565 Accuracy 0.6386666893959045\n",
      "Iteration 3960 Training loss 0.10789520293474197 Validation loss 0.13223662972450256 Accuracy 0.6119999885559082\n",
      "Iteration 3970 Training loss 0.1054893210530281 Validation loss 0.11956959962844849 Accuracy 0.640666663646698\n",
      "Iteration 3980 Training loss 0.10542047768831253 Validation loss 0.12007790803909302 Accuracy 0.6383333206176758\n",
      "Iteration 3990 Training loss 0.10961209237575531 Validation loss 0.13459686934947968 Accuracy 0.6098333597183228\n",
      "Iteration 4000 Training loss 0.10632112622261047 Validation loss 0.12948988378047943 Accuracy 0.6179999709129333\n",
      "Iteration 4010 Training loss 0.10604788362979889 Validation loss 0.1245168074965477 Accuracy 0.6298333406448364\n",
      "Iteration 4020 Training loss 0.10351760685443878 Validation loss 0.11794828623533249 Accuracy 0.6451666951179504\n",
      "Iteration 4030 Training loss 0.10734943300485611 Validation loss 0.12719246745109558 Accuracy 0.6225000023841858\n",
      "Iteration 4040 Training loss 0.10612162202596664 Validation loss 0.12435437738895416 Accuracy 0.6296666860580444\n",
      "Iteration 4050 Training loss 0.103783018887043 Validation loss 0.11798832565546036 Accuracy 0.6461666822433472\n",
      "Iteration 4060 Training loss 0.10610641539096832 Validation loss 0.12221094965934753 Accuracy 0.6386666893959045\n",
      "Iteration 4070 Training loss 0.1070038229227066 Validation loss 0.12691958248615265 Accuracy 0.6243333220481873\n",
      "Iteration 4080 Training loss 0.1045311912894249 Validation loss 0.12223270535469055 Accuracy 0.6368333101272583\n",
      "Iteration 4090 Training loss 0.10519871115684509 Validation loss 0.12480869889259338 Accuracy 0.6294999718666077\n",
      "Iteration 4100 Training loss 0.10637678951025009 Validation loss 0.12342891097068787 Accuracy 0.6330000162124634\n",
      "Iteration 4110 Training loss 0.106497623026371 Validation loss 0.12645570933818817 Accuracy 0.624833345413208\n",
      "Iteration 4120 Training loss 0.104128398001194 Validation loss 0.11969320476055145 Accuracy 0.6428333520889282\n",
      "Iteration 4130 Training loss 0.10597125440835953 Validation loss 0.12964731454849243 Accuracy 0.6196666955947876\n",
      "Iteration 4140 Training loss 0.10332291573286057 Validation loss 0.1233304962515831 Accuracy 0.6341666579246521\n",
      "Iteration 4150 Training loss 0.10490765422582626 Validation loss 0.12727341055870056 Accuracy 0.625166654586792\n",
      "Iteration 4160 Training loss 0.10692838579416275 Validation loss 0.1354811191558838 Accuracy 0.606333315372467\n",
      "Iteration 4170 Training loss 0.10565469413995743 Validation loss 0.1260870397090912 Accuracy 0.6276666522026062\n",
      "Iteration 4180 Training loss 0.10661478340625763 Validation loss 0.1275654137134552 Accuracy 0.625\n",
      "Iteration 4190 Training loss 0.10575791448354721 Validation loss 0.12187295407056808 Accuracy 0.6380000114440918\n",
      "Iteration 4200 Training loss 0.106283038854599 Validation loss 0.12527351081371307 Accuracy 0.6294999718666077\n",
      "Iteration 4210 Training loss 0.10682354867458344 Validation loss 0.12981711328029633 Accuracy 0.6203333139419556\n",
      "Iteration 4220 Training loss 0.10592742264270782 Validation loss 0.12970644235610962 Accuracy 0.6191666722297668\n",
      "Iteration 4230 Training loss 0.10396227985620499 Validation loss 0.12231764942407608 Accuracy 0.637666642665863\n",
      "Iteration 4240 Training loss 0.10717679560184479 Validation loss 0.12981167435646057 Accuracy 0.6188333630561829\n",
      "Iteration 4250 Training loss 0.10581588745117188 Validation loss 0.12584908306598663 Accuracy 0.6303333044052124\n",
      "Iteration 4260 Training loss 0.10320644825696945 Validation loss 0.11626654118299484 Accuracy 0.6501666903495789\n",
      "Iteration 4270 Training loss 0.10588621348142624 Validation loss 0.11263740807771683 Accuracy 0.6578333377838135\n",
      "Iteration 4280 Training loss 0.10457752645015717 Validation loss 0.11332622170448303 Accuracy 0.6539999842643738\n",
      "Iteration 4290 Training loss 0.10502537339925766 Validation loss 0.11857549101114273 Accuracy 0.6468333601951599\n",
      "Iteration 4300 Training loss 0.10767792910337448 Validation loss 0.12599815428256989 Accuracy 0.6288333535194397\n",
      "Iteration 4310 Training loss 0.10875024646520615 Validation loss 0.13240090012550354 Accuracy 0.6169999837875366\n",
      "Iteration 4320 Training loss 0.10505639761686325 Validation loss 0.12463044375181198 Accuracy 0.6320000290870667\n",
      "Iteration 4330 Training loss 0.10410870611667633 Validation loss 0.12091804295778275 Accuracy 0.6380000114440918\n",
      "Iteration 4340 Training loss 0.10330222547054291 Validation loss 0.12533457577228546 Accuracy 0.6320000290870667\n",
      "Iteration 4350 Training loss 0.10302110761404037 Validation loss 0.12282334268093109 Accuracy 0.6368333101272583\n",
      "Iteration 4360 Training loss 0.10731016099452972 Validation loss 0.12893736362457275 Accuracy 0.6223333477973938\n",
      "Iteration 4370 Training loss 0.10577020049095154 Validation loss 0.12611757218837738 Accuracy 0.628333330154419\n",
      "Iteration 4380 Training loss 0.10359770804643631 Validation loss 0.11258894950151443 Accuracy 0.6538333296775818\n",
      "Iteration 4390 Training loss 0.10520829260349274 Validation loss 0.11177761107683182 Accuracy 0.6581666469573975\n",
      "Iteration 4400 Training loss 0.10628651082515717 Validation loss 0.11187764257192612 Accuracy 0.6583333611488342\n",
      "Iteration 4410 Training loss 0.10911121964454651 Validation loss 0.1136074811220169 Accuracy 0.6545000076293945\n",
      "Iteration 4420 Training loss 0.10622280091047287 Validation loss 0.11141865700483322 Accuracy 0.6585000157356262\n",
      "Iteration 4430 Training loss 0.10658479481935501 Validation loss 0.11147622764110565 Accuracy 0.659166693687439\n",
      "Iteration 4440 Training loss 0.10698724538087845 Validation loss 0.11131264269351959 Accuracy 0.6586666703224182\n",
      "Iteration 4450 Training loss 0.10652436316013336 Validation loss 0.11138719320297241 Accuracy 0.6579999923706055\n",
      "Iteration 4460 Training loss 0.10557904839515686 Validation loss 0.11146815121173859 Accuracy 0.6581666469573975\n",
      "Iteration 4470 Training loss 0.10347647964954376 Validation loss 0.11679321527481079 Accuracy 0.6508333086967468\n",
      "Iteration 4480 Training loss 0.10536300390958786 Validation loss 0.11136292666196823 Accuracy 0.6604999899864197\n",
      "Iteration 4490 Training loss 0.10496579855680466 Validation loss 0.11635806411504745 Accuracy 0.6524999737739563\n",
      "Iteration 4500 Training loss 0.10807289928197861 Validation loss 0.13022767007350922 Accuracy 0.6208333373069763\n",
      "Iteration 4510 Training loss 0.10521215945482254 Validation loss 0.12457665801048279 Accuracy 0.6324999928474426\n",
      "Iteration 4520 Training loss 0.10598292946815491 Validation loss 0.12615978717803955 Accuracy 0.6309999823570251\n",
      "Iteration 4530 Training loss 0.10324447602033615 Validation loss 0.1194717139005661 Accuracy 0.6461666822433472\n",
      "Iteration 4540 Training loss 0.10421176254749298 Validation loss 0.12840718030929565 Accuracy 0.6258333325386047\n",
      "Iteration 4550 Training loss 0.10558043420314789 Validation loss 0.12783212959766388 Accuracy 0.6266666650772095\n",
      "Iteration 4560 Training loss 0.10400915890932083 Validation loss 0.12100008130073547 Accuracy 0.6420000195503235\n",
      "Iteration 4570 Training loss 0.1050998643040657 Validation loss 0.12840765714645386 Accuracy 0.6240000128746033\n",
      "Iteration 4580 Training loss 0.10526802390813828 Validation loss 0.12865044176578522 Accuracy 0.6263333559036255\n",
      "Iteration 4590 Training loss 0.1102636307477951 Validation loss 0.1389150321483612 Accuracy 0.6046666502952576\n",
      "Iteration 4600 Training loss 0.10432196408510208 Validation loss 0.12637931108474731 Accuracy 0.6290000081062317\n",
      "Iteration 4610 Training loss 0.10404731333255768 Validation loss 0.12602677941322327 Accuracy 0.6291666626930237\n",
      "Iteration 4620 Training loss 0.10251793265342712 Validation loss 0.11606438457965851 Accuracy 0.6514999866485596\n",
      "Iteration 4630 Training loss 0.10531506687402725 Validation loss 0.11170149594545364 Accuracy 0.6598333120346069\n",
      "Iteration 4640 Training loss 0.1056811735033989 Validation loss 0.11204852163791656 Accuracy 0.6575000286102295\n",
      "Iteration 4650 Training loss 0.10391644388437271 Validation loss 0.12609171867370605 Accuracy 0.6334999799728394\n",
      "Iteration 4660 Training loss 0.10221561789512634 Validation loss 0.11855997890233994 Accuracy 0.6468333601951599\n",
      "Iteration 4670 Training loss 0.10553405433893204 Validation loss 0.12845835089683533 Accuracy 0.6258333325386047\n",
      "Iteration 4680 Training loss 0.10702653229236603 Validation loss 0.1305512934923172 Accuracy 0.6188333630561829\n",
      "Iteration 4690 Training loss 0.10200817883014679 Validation loss 0.12000289559364319 Accuracy 0.6443333625793457\n",
      "Iteration 4700 Training loss 0.10479388386011124 Validation loss 0.1305314004421234 Accuracy 0.6198333501815796\n",
      "Iteration 4710 Training loss 0.1021329015493393 Validation loss 0.11794698238372803 Accuracy 0.6488333344459534\n",
      "Iteration 4720 Training loss 0.10669632256031036 Validation loss 0.11198209971189499 Accuracy 0.6585000157356262\n",
      "Iteration 4730 Training loss 0.1041884645819664 Validation loss 0.11387921124696732 Accuracy 0.656499981880188\n",
      "Iteration 4740 Training loss 0.10533300042152405 Validation loss 0.11190102249383926 Accuracy 0.6600000262260437\n",
      "Iteration 4750 Training loss 0.1058647632598877 Validation loss 0.11268243193626404 Accuracy 0.6603333353996277\n",
      "Iteration 4760 Training loss 0.10104582458734512 Validation loss 0.11371941864490509 Accuracy 0.6570000052452087\n",
      "Iteration 4770 Training loss 0.10460761189460754 Validation loss 0.11226817965507507 Accuracy 0.6586666703224182\n",
      "Iteration 4780 Training loss 0.1028052568435669 Validation loss 0.11183308809995651 Accuracy 0.6583333611488342\n",
      "Iteration 4790 Training loss 0.1070864275097847 Validation loss 0.11273335665464401 Accuracy 0.659166693687439\n",
      "Iteration 4800 Training loss 0.10500126332044601 Validation loss 0.11174550652503967 Accuracy 0.6596666574478149\n",
      "Iteration 4810 Training loss 0.10586751997470856 Validation loss 0.11209626495838165 Accuracy 0.659166693687439\n",
      "Iteration 4820 Training loss 0.10390250384807587 Validation loss 0.11177074909210205 Accuracy 0.6576666831970215\n",
      "Iteration 4830 Training loss 0.10575476288795471 Validation loss 0.11210298538208008 Accuracy 0.659500002861023\n",
      "Iteration 4840 Training loss 0.10220053791999817 Validation loss 0.11239717900753021 Accuracy 0.6589999794960022\n",
      "Iteration 4850 Training loss 0.10299272835254669 Validation loss 0.11291380971670151 Accuracy 0.6588333249092102\n",
      "Iteration 4860 Training loss 0.10509061813354492 Validation loss 0.11166942119598389 Accuracy 0.6600000262260437\n",
      "Iteration 4870 Training loss 0.10334382951259613 Validation loss 0.11214983463287354 Accuracy 0.659333348274231\n",
      "Iteration 4880 Training loss 0.104489266872406 Validation loss 0.11125065386295319 Accuracy 0.6598333120346069\n",
      "Iteration 4890 Training loss 0.10462099313735962 Validation loss 0.11137717962265015 Accuracy 0.6614999771118164\n",
      "Iteration 4900 Training loss 0.10383307188749313 Validation loss 0.11416862159967422 Accuracy 0.6551666855812073\n",
      "Iteration 4910 Training loss 0.10697237402200699 Validation loss 0.11279496550559998 Accuracy 0.6600000262260437\n",
      "Iteration 4920 Training loss 0.10358492285013199 Validation loss 0.11198466271162033 Accuracy 0.659333348274231\n",
      "Iteration 4930 Training loss 0.10494012385606766 Validation loss 0.11144468933343887 Accuracy 0.6600000262260437\n",
      "Iteration 4940 Training loss 0.10378053784370422 Validation loss 0.11188769340515137 Accuracy 0.6606666445732117\n",
      "Iteration 4950 Training loss 0.10604348033666611 Validation loss 0.11303136497735977 Accuracy 0.659500002861023\n",
      "Iteration 4960 Training loss 0.10424401611089706 Validation loss 0.11156308650970459 Accuracy 0.6583333611488342\n",
      "Iteration 4970 Training loss 0.10340812057256699 Validation loss 0.11165355145931244 Accuracy 0.659500002861023\n",
      "Iteration 4980 Training loss 0.10750983655452728 Validation loss 0.11350838094949722 Accuracy 0.6586666703224182\n",
      "Iteration 4990 Training loss 0.10493084043264389 Validation loss 0.11155901104211807 Accuracy 0.6608333587646484\n",
      "Iteration 5000 Training loss 0.10545452684164047 Validation loss 0.11209440976381302 Accuracy 0.6598333120346069\n",
      "Iteration 5010 Training loss 0.10403682291507721 Validation loss 0.11143678426742554 Accuracy 0.6601666808128357\n",
      "Iteration 5020 Training loss 0.1057945042848587 Validation loss 0.11194468289613724 Accuracy 0.6601666808128357\n",
      "Iteration 5030 Training loss 0.10436765849590302 Validation loss 0.11167805641889572 Accuracy 0.6608333587646484\n",
      "Iteration 5040 Training loss 0.10149525851011276 Validation loss 0.11290248483419418 Accuracy 0.659333348274231\n",
      "Iteration 5050 Training loss 0.10784182697534561 Validation loss 0.11247189342975616 Accuracy 0.6613333225250244\n",
      "Iteration 5060 Training loss 0.10197153687477112 Validation loss 0.11338352411985397 Accuracy 0.659166693687439\n",
      "Iteration 5070 Training loss 0.10476621240377426 Validation loss 0.11184138804674149 Accuracy 0.6606666445732117\n",
      "Iteration 5080 Training loss 0.1036369800567627 Validation loss 0.11140431463718414 Accuracy 0.6625000238418579\n",
      "Iteration 5090 Training loss 0.10159945487976074 Validation loss 0.11192527413368225 Accuracy 0.6588333249092102\n",
      "Iteration 5100 Training loss 0.1064157709479332 Validation loss 0.11221873015165329 Accuracy 0.6621666550636292\n",
      "Iteration 5110 Training loss 0.1033882349729538 Validation loss 0.11171939224004745 Accuracy 0.6611666679382324\n",
      "Iteration 5120 Training loss 0.10296104103326797 Validation loss 0.11406086385250092 Accuracy 0.6568333506584167\n",
      "Iteration 5130 Training loss 0.10380773991346359 Validation loss 0.11195585876703262 Accuracy 0.6583333611488342\n",
      "Iteration 5140 Training loss 0.1022619903087616 Validation loss 0.11172137409448624 Accuracy 0.6610000133514404\n",
      "Iteration 5150 Training loss 0.10275012254714966 Validation loss 0.12060239911079407 Accuracy 0.6473333239555359\n",
      "Iteration 5160 Training loss 0.10888416320085526 Validation loss 0.13870638608932495 Accuracy 0.6083333492279053\n",
      "Iteration 5170 Training loss 0.10278048366308212 Validation loss 0.12427596747875214 Accuracy 0.6386666893959045\n",
      "Iteration 5180 Training loss 0.10193784534931183 Validation loss 0.11736545711755753 Accuracy 0.6528333425521851\n",
      "Iteration 5190 Training loss 0.10712680965662003 Validation loss 0.14333277940750122 Accuracy 0.5985000133514404\n",
      "Iteration 5200 Training loss 0.10229996591806412 Validation loss 0.12256427109241486 Accuracy 0.6420000195503235\n",
      "Iteration 5210 Training loss 0.10428822785615921 Validation loss 0.12587518990039825 Accuracy 0.6351666450500488\n",
      "Iteration 5220 Training loss 0.10358916968107224 Validation loss 0.12332574278116226 Accuracy 0.6399999856948853\n",
      "Iteration 5230 Training loss 0.10800109803676605 Validation loss 0.1344556212425232 Accuracy 0.6146666407585144\n",
      "Iteration 5240 Training loss 0.1038103848695755 Validation loss 0.1233600452542305 Accuracy 0.64083331823349\n",
      "Iteration 5250 Training loss 0.10269832611083984 Validation loss 0.12772703170776367 Accuracy 0.6321666836738586\n",
      "Iteration 5260 Training loss 0.10599194467067719 Validation loss 0.13202697038650513 Accuracy 0.6226666569709778\n",
      "Iteration 5270 Training loss 0.10348425060510635 Validation loss 0.12840840220451355 Accuracy 0.6303333044052124\n",
      "Iteration 5280 Training loss 0.10260281711816788 Validation loss 0.12591098248958588 Accuracy 0.6330000162124634\n",
      "Iteration 5290 Training loss 0.10215120762586594 Validation loss 0.11761302500963211 Accuracy 0.6503333449363708\n",
      "Iteration 5300 Training loss 0.10432614386081696 Validation loss 0.11238487809896469 Accuracy 0.6610000133514404\n",
      "Iteration 5310 Training loss 0.1032857596874237 Validation loss 0.11130930483341217 Accuracy 0.6606666445732117\n",
      "Iteration 5320 Training loss 0.1064896211028099 Validation loss 0.11170098930597305 Accuracy 0.6618333458900452\n",
      "Iteration 5330 Training loss 0.10036810487508774 Validation loss 0.11348871141672134 Accuracy 0.6610000133514404\n",
      "Iteration 5340 Training loss 0.1027536615729332 Validation loss 0.11530747264623642 Accuracy 0.6538333296775818\n",
      "Iteration 5350 Training loss 0.10455024987459183 Validation loss 0.12818244099617004 Accuracy 0.6326666474342346\n",
      "Iteration 5360 Training loss 0.10358738154172897 Validation loss 0.12492913752794266 Accuracy 0.6370000243186951\n",
      "Iteration 5370 Training loss 0.10755621641874313 Validation loss 0.13796664774417877 Accuracy 0.609666645526886\n",
      "Iteration 5380 Training loss 0.1033925712108612 Validation loss 0.13069270551204681 Accuracy 0.6244999766349792\n",
      "Iteration 5390 Training loss 0.1029433161020279 Validation loss 0.12664267420768738 Accuracy 0.6324999928474426\n",
      "Iteration 5400 Training loss 0.10403401404619217 Validation loss 0.13086627423763275 Accuracy 0.6241666674613953\n",
      "Iteration 5410 Training loss 0.10215964913368225 Validation loss 0.12239206582307816 Accuracy 0.6460000276565552\n",
      "Iteration 5420 Training loss 0.1045287698507309 Validation loss 0.13283608853816986 Accuracy 0.6208333373069763\n",
      "Iteration 5430 Training loss 0.10274706780910492 Validation loss 0.12133552134037018 Accuracy 0.6458333134651184\n",
      "Iteration 5440 Training loss 0.10316893458366394 Validation loss 0.12784850597381592 Accuracy 0.6316666603088379\n",
      "Iteration 5450 Training loss 0.10046710073947906 Validation loss 0.11925260722637177 Accuracy 0.6506666541099548\n",
      "Iteration 5460 Training loss 0.10396353900432587 Validation loss 0.12230626493692398 Accuracy 0.6476666927337646\n",
      "Iteration 5470 Training loss 0.10347530245780945 Validation loss 0.1330706775188446 Accuracy 0.6213333606719971\n",
      "Iteration 5480 Training loss 0.10254541039466858 Validation loss 0.1254943609237671 Accuracy 0.6356666684150696\n",
      "Iteration 5490 Training loss 0.10410057753324509 Validation loss 0.12264318019151688 Accuracy 0.6426666378974915\n",
      "Iteration 5500 Training loss 0.10392680019140244 Validation loss 0.13215529918670654 Accuracy 0.6233333349227905\n",
      "Iteration 5510 Training loss 0.10349886864423752 Validation loss 0.11755499988794327 Accuracy 0.6516666412353516\n",
      "Iteration 5520 Training loss 0.10287929326295853 Validation loss 0.11438623815774918 Accuracy 0.6579999923706055\n",
      "Iteration 5530 Training loss 0.10458634793758392 Validation loss 0.12592920660972595 Accuracy 0.6359999775886536\n",
      "Iteration 5540 Training loss 0.10429941862821579 Validation loss 0.13221511244773865 Accuracy 0.6213333606719971\n",
      "Iteration 5550 Training loss 0.0999990925192833 Validation loss 0.11612172424793243 Accuracy 0.6545000076293945\n",
      "Iteration 5560 Training loss 0.10516058653593063 Validation loss 0.11217830330133438 Accuracy 0.6604999899864197\n",
      "Iteration 5570 Training loss 0.10417405515909195 Validation loss 0.11149030923843384 Accuracy 0.6610000133514404\n",
      "Iteration 5580 Training loss 0.1031375452876091 Validation loss 0.11165552586317062 Accuracy 0.6611666679382324\n",
      "Iteration 5590 Training loss 0.10524360090494156 Validation loss 0.11135329306125641 Accuracy 0.6611666679382324\n",
      "Iteration 5600 Training loss 0.1016339510679245 Validation loss 0.11189445853233337 Accuracy 0.6610000133514404\n",
      "Iteration 5610 Training loss 0.1047552302479744 Validation loss 0.11170549690723419 Accuracy 0.6606666445732117\n",
      "Iteration 5620 Training loss 0.10437863320112228 Validation loss 0.11122263222932816 Accuracy 0.6611666679382324\n",
      "Iteration 5630 Training loss 0.10131042450666428 Validation loss 0.11547153443098068 Accuracy 0.6570000052452087\n",
      "Iteration 5640 Training loss 0.10133513063192368 Validation loss 0.11288733035326004 Accuracy 0.6641666889190674\n",
      "Iteration 5650 Training loss 0.10723557323217392 Validation loss 0.11480516195297241 Accuracy 0.6616666913032532\n",
      "Iteration 5660 Training loss 0.10247231274843216 Validation loss 0.11183467507362366 Accuracy 0.6623333096504211\n",
      "Iteration 5670 Training loss 0.10400158911943436 Validation loss 0.11191125214099884 Accuracy 0.6603333353996277\n",
      "Iteration 5680 Training loss 0.10640348494052887 Validation loss 0.11236930638551712 Accuracy 0.6610000133514404\n",
      "Iteration 5690 Training loss 0.10270366072654724 Validation loss 0.11157926172018051 Accuracy 0.6618333458900452\n",
      "Iteration 5700 Training loss 0.10210125893354416 Validation loss 0.11216675490140915 Accuracy 0.6631666421890259\n",
      "Iteration 5710 Training loss 0.1035146489739418 Validation loss 0.11132026463747025 Accuracy 0.6623333096504211\n",
      "Iteration 5720 Training loss 0.1044805720448494 Validation loss 0.11137431859970093 Accuracy 0.6620000004768372\n",
      "Iteration 5730 Training loss 0.10287798941135406 Validation loss 0.11278115957975388 Accuracy 0.6623333096504211\n",
      "Iteration 5740 Training loss 0.1055789589881897 Validation loss 0.11144665628671646 Accuracy 0.6613333225250244\n",
      "Iteration 5750 Training loss 0.10263640433549881 Validation loss 0.11164307594299316 Accuracy 0.6614999771118164\n",
      "Iteration 5760 Training loss 0.10398945212364197 Validation loss 0.11193440109491348 Accuracy 0.6639999747276306\n",
      "Iteration 5770 Training loss 0.10112963616847992 Validation loss 0.11454736441373825 Accuracy 0.659333348274231\n",
      "Iteration 5780 Training loss 0.1050226166844368 Validation loss 0.13455457985401154 Accuracy 0.6193333268165588\n",
      "Iteration 5790 Training loss 0.10161871463060379 Validation loss 0.12775126099586487 Accuracy 0.6320000290870667\n",
      "Iteration 5800 Training loss 0.10275596380233765 Validation loss 0.12598879635334015 Accuracy 0.6370000243186951\n",
      "Iteration 5810 Training loss 0.10160989314317703 Validation loss 0.12000077962875366 Accuracy 0.6508333086967468\n",
      "Iteration 5820 Training loss 0.10421092063188553 Validation loss 0.1352820098400116 Accuracy 0.6159999966621399\n",
      "Iteration 5830 Training loss 0.10414473712444305 Validation loss 0.1324712485074997 Accuracy 0.625\n",
      "Iteration 5840 Training loss 0.10087400674819946 Validation loss 0.12350095808506012 Accuracy 0.643666684627533\n",
      "Iteration 5850 Training loss 0.10219822078943253 Validation loss 0.12385804951190948 Accuracy 0.6441666483879089\n",
      "Iteration 5860 Training loss 0.10254817456007004 Validation loss 0.12911413609981537 Accuracy 0.6309999823570251\n",
      "Iteration 5870 Training loss 0.10321483016014099 Validation loss 0.12417346239089966 Accuracy 0.6428333520889282\n",
      "Iteration 5880 Training loss 0.10565342009067535 Validation loss 0.13628722727298737 Accuracy 0.612666666507721\n",
      "Iteration 5890 Training loss 0.10093386471271515 Validation loss 0.12518154084682465 Accuracy 0.6389999985694885\n",
      "Iteration 5900 Training loss 0.10493508726358414 Validation loss 0.12701594829559326 Accuracy 0.6358333230018616\n",
      "Iteration 5910 Training loss 0.10381235927343369 Validation loss 0.12641465663909912 Accuracy 0.6365000009536743\n",
      "Iteration 5920 Training loss 0.10289471596479416 Validation loss 0.1345275491476059 Accuracy 0.6175000071525574\n",
      "Iteration 5930 Training loss 0.10118646174669266 Validation loss 0.12208017706871033 Accuracy 0.6458333134651184\n",
      "Iteration 5940 Training loss 0.10278202593326569 Validation loss 0.13316436111927032 Accuracy 0.621999979019165\n",
      "Iteration 5950 Training loss 0.10342966765165329 Validation loss 0.12011228501796722 Accuracy 0.6511666774749756\n",
      "Iteration 5960 Training loss 0.10351473093032837 Validation loss 0.125493124127388 Accuracy 0.6395000219345093\n",
      "Iteration 5970 Training loss 0.10128267109394073 Validation loss 0.1271214783191681 Accuracy 0.6343333125114441\n",
      "Iteration 5980 Training loss 0.10095438361167908 Validation loss 0.12127949297428131 Accuracy 0.6485000252723694\n",
      "Iteration 5990 Training loss 0.10452350974082947 Validation loss 0.1354409009218216 Accuracy 0.6171666383743286\n",
      "Iteration 6000 Training loss 0.10075685381889343 Validation loss 0.12309008091688156 Accuracy 0.6456666588783264\n",
      "Iteration 6010 Training loss 0.10377731174230576 Validation loss 0.1301347017288208 Accuracy 0.6291666626930237\n",
      "Iteration 6020 Training loss 0.10174719989299774 Validation loss 0.12474127113819122 Accuracy 0.640500009059906\n",
      "Iteration 6030 Training loss 0.10485046356916428 Validation loss 0.13339070975780487 Accuracy 0.6233333349227905\n",
      "Iteration 6040 Training loss 0.10296314209699631 Validation loss 0.12399493157863617 Accuracy 0.6420000195503235\n",
      "Iteration 6050 Training loss 0.10473164170980453 Validation loss 0.13041505217552185 Accuracy 0.6306666731834412\n",
      "Iteration 6060 Training loss 0.10355512797832489 Validation loss 0.12952034175395966 Accuracy 0.6315000057220459\n",
      "Iteration 6070 Training loss 0.10410169512033463 Validation loss 0.13483020663261414 Accuracy 0.6183333396911621\n",
      "Iteration 6080 Training loss 0.1029282659292221 Validation loss 0.12162628024816513 Accuracy 0.6485000252723694\n",
      "Iteration 6090 Training loss 0.10328977555036545 Validation loss 0.1295829713344574 Accuracy 0.6305000185966492\n",
      "Iteration 6100 Training loss 0.1016576811671257 Validation loss 0.12282760441303253 Accuracy 0.6456666588783264\n",
      "Iteration 6110 Training loss 0.10279111564159393 Validation loss 0.12371420115232468 Accuracy 0.6433333158493042\n",
      "Iteration 6120 Training loss 0.10538963973522186 Validation loss 0.13745395839214325 Accuracy 0.612666666507721\n",
      "Iteration 6130 Training loss 0.10218495875597 Validation loss 0.1253150850534439 Accuracy 0.637333333492279\n",
      "Iteration 6140 Training loss 0.10215313732624054 Validation loss 0.12415789067745209 Accuracy 0.6418333053588867\n",
      "Iteration 6150 Training loss 0.1028052344918251 Validation loss 0.12832021713256836 Accuracy 0.6334999799728394\n",
      "Iteration 6160 Training loss 0.10435416549444199 Validation loss 0.1282566487789154 Accuracy 0.6338333487510681\n",
      "Iteration 6170 Training loss 0.10218866914510727 Validation loss 0.12513691186904907 Accuracy 0.6384999752044678\n",
      "Iteration 6180 Training loss 0.10117409378290176 Validation loss 0.12813936173915863 Accuracy 0.6340000033378601\n",
      "Iteration 6190 Training loss 0.10068424791097641 Validation loss 0.1234484389424324 Accuracy 0.6455000042915344\n",
      "Iteration 6200 Training loss 0.1040283814072609 Validation loss 0.13041101396083832 Accuracy 0.6299999952316284\n",
      "Iteration 6210 Training loss 0.1030752882361412 Validation loss 0.12613141536712646 Accuracy 0.6368333101272583\n",
      "Iteration 6220 Training loss 0.10334984213113785 Validation loss 0.13041333854198456 Accuracy 0.6305000185966492\n",
      "Iteration 6230 Training loss 0.10309240967035294 Validation loss 0.13041365146636963 Accuracy 0.6303333044052124\n",
      "Iteration 6240 Training loss 0.10081284493207932 Validation loss 0.1262657195329666 Accuracy 0.6378333568572998\n",
      "Iteration 6250 Training loss 0.10290848463773727 Validation loss 0.12927718460559845 Accuracy 0.6316666603088379\n",
      "Iteration 6260 Training loss 0.10116935521364212 Validation loss 0.12101912498474121 Accuracy 0.6506666541099548\n",
      "Iteration 6270 Training loss 0.10545241087675095 Validation loss 0.13275782763957977 Accuracy 0.625166654586792\n",
      "Iteration 6280 Training loss 0.10333391278982162 Validation loss 0.1264781355857849 Accuracy 0.6366666555404663\n",
      "Iteration 6290 Training loss 0.10464783012866974 Validation loss 0.13083666563034058 Accuracy 0.628000020980835\n",
      "Iteration 6300 Training loss 0.10050181299448013 Validation loss 0.12589330971240997 Accuracy 0.6383333206176758\n",
      "Iteration 6310 Training loss 0.10214480757713318 Validation loss 0.12537039816379547 Accuracy 0.64083331823349\n",
      "Iteration 6320 Training loss 0.09926816821098328 Validation loss 0.12406285852193832 Accuracy 0.643666684627533\n",
      "Iteration 6330 Training loss 0.10269789397716522 Validation loss 0.11579018086194992 Accuracy 0.6608333587646484\n",
      "Iteration 6340 Training loss 0.10065055638551712 Validation loss 0.11567358672618866 Accuracy 0.6604999899864197\n",
      "Iteration 6350 Training loss 0.10746609419584274 Validation loss 0.11344332993030548 Accuracy 0.6644999980926514\n",
      "Iteration 6360 Training loss 0.10158579796552658 Validation loss 0.11104435473680496 Accuracy 0.6633333563804626\n",
      "Iteration 6370 Training loss 0.09927786141633987 Validation loss 0.11147932708263397 Accuracy 0.6625000238418579\n",
      "Iteration 6380 Training loss 0.10373203456401825 Validation loss 0.1117575466632843 Accuracy 0.6628333330154419\n",
      "Iteration 6390 Training loss 0.10105708986520767 Validation loss 0.11159514635801315 Accuracy 0.6636666655540466\n",
      "Iteration 6400 Training loss 0.10013438016176224 Validation loss 0.11187028139829636 Accuracy 0.6639999747276306\n",
      "Iteration 6410 Training loss 0.10146482288837433 Validation loss 0.11130108684301376 Accuracy 0.6631666421890259\n",
      "Iteration 6420 Training loss 0.10355903208255768 Validation loss 0.11123716086149216 Accuracy 0.6633333563804626\n",
      "Iteration 6430 Training loss 0.09945151954889297 Validation loss 0.1125015988945961 Accuracy 0.6644999980926514\n",
      "Iteration 6440 Training loss 0.10282112658023834 Validation loss 0.11172694712877274 Accuracy 0.6621666550636292\n",
      "Iteration 6450 Training loss 0.1034131646156311 Validation loss 0.11136095970869064 Accuracy 0.6648333072662354\n",
      "Iteration 6460 Training loss 0.10325648635625839 Validation loss 0.11154142022132874 Accuracy 0.6628333330154419\n",
      "Iteration 6470 Training loss 0.10181146115064621 Validation loss 0.11157752573490143 Accuracy 0.6633333563804626\n",
      "Iteration 6480 Training loss 0.10278318077325821 Validation loss 0.11114268004894257 Accuracy 0.6639999747276306\n",
      "Iteration 6490 Training loss 0.09968893229961395 Validation loss 0.11205586791038513 Accuracy 0.6639999747276306\n",
      "Iteration 6500 Training loss 0.10040942579507828 Validation loss 0.11502263695001602 Accuracy 0.6621666550636292\n",
      "Iteration 6510 Training loss 0.10900300741195679 Validation loss 0.11505435407161713 Accuracy 0.6626666784286499\n",
      "Iteration 6520 Training loss 0.10086735337972641 Validation loss 0.11171215772628784 Accuracy 0.6631666421890259\n",
      "Iteration 6530 Training loss 0.0994793102145195 Validation loss 0.11423543095588684 Accuracy 0.6623333096504211\n",
      "Iteration 6540 Training loss 0.10349410772323608 Validation loss 0.11169559508562088 Accuracy 0.6658333539962769\n",
      "Iteration 6550 Training loss 0.10368993878364563 Validation loss 0.11150087416172028 Accuracy 0.6618333458900452\n",
      "Iteration 6560 Training loss 0.10422413796186447 Validation loss 0.11174369603395462 Accuracy 0.6631666421890259\n",
      "Iteration 6570 Training loss 0.1037796214222908 Validation loss 0.11129378527402878 Accuracy 0.6643333435058594\n",
      "Iteration 6580 Training loss 0.10103821009397507 Validation loss 0.11195364594459534 Accuracy 0.6629999876022339\n",
      "Iteration 6590 Training loss 0.10291947424411774 Validation loss 0.1114971712231636 Accuracy 0.6631666421890259\n",
      "Iteration 6600 Training loss 0.10328421741724014 Validation loss 0.11133138835430145 Accuracy 0.6631666421890259\n",
      "Iteration 6610 Training loss 0.10178767889738083 Validation loss 0.11200294643640518 Accuracy 0.6641666889190674\n",
      "Iteration 6620 Training loss 0.09953363984823227 Validation loss 0.11158644407987595 Accuracy 0.6626666784286499\n",
      "Iteration 6630 Training loss 0.10312686115503311 Validation loss 0.11142341047525406 Accuracy 0.6643333435058594\n",
      "Iteration 6640 Training loss 0.10057808458805084 Validation loss 0.11204423010349274 Accuracy 0.6625000238418579\n",
      "Iteration 6650 Training loss 0.10179761052131653 Validation loss 0.11140022426843643 Accuracy 0.6639999747276306\n",
      "Iteration 6660 Training loss 0.09819342941045761 Validation loss 0.1123976856470108 Accuracy 0.6641666889190674\n",
      "Iteration 6670 Training loss 0.10270947217941284 Validation loss 0.11143959313631058 Accuracy 0.6633333563804626\n",
      "Iteration 6680 Training loss 0.0998963713645935 Validation loss 0.11488081514835358 Accuracy 0.6629999876022339\n",
      "Iteration 6690 Training loss 0.10358192771673203 Validation loss 0.11103937029838562 Accuracy 0.6664999723434448\n",
      "Iteration 6700 Training loss 0.10195786505937576 Validation loss 0.11100142449140549 Accuracy 0.6643333435058594\n",
      "Iteration 6710 Training loss 0.09976491332054138 Validation loss 0.11219770461320877 Accuracy 0.6636666655540466\n",
      "Iteration 6720 Training loss 0.09993236511945724 Validation loss 0.11509072035551071 Accuracy 0.6598333120346069\n",
      "Iteration 6730 Training loss 0.10467551648616791 Validation loss 0.11205004155635834 Accuracy 0.6620000004768372\n",
      "Iteration 6740 Training loss 0.10153604298830032 Validation loss 0.11194612830877304 Accuracy 0.6650000214576721\n",
      "Iteration 6750 Training loss 0.10295931249856949 Validation loss 0.11148235201835632 Accuracy 0.6633333563804626\n",
      "Iteration 6760 Training loss 0.10468991845846176 Validation loss 0.11162851005792618 Accuracy 0.6639999747276306\n",
      "Iteration 6770 Training loss 0.10190791636705399 Validation loss 0.11152826994657516 Accuracy 0.6636666655540466\n",
      "Iteration 6780 Training loss 0.10069010406732559 Validation loss 0.11927864700555801 Accuracy 0.6545000076293945\n",
      "Iteration 6790 Training loss 0.10138359665870667 Validation loss 0.12865832448005676 Accuracy 0.6351666450500488\n",
      "Iteration 6800 Training loss 0.10063032805919647 Validation loss 0.12466056644916534 Accuracy 0.6451666951179504\n",
      "Iteration 6810 Training loss 0.10138756036758423 Validation loss 0.13250018656253815 Accuracy 0.6263333559036255\n",
      "Iteration 6820 Training loss 0.10372605174779892 Validation loss 0.12938204407691956 Accuracy 0.6313333511352539\n",
      "Iteration 6830 Training loss 0.10162676870822906 Validation loss 0.12418520450592041 Accuracy 0.6476666927337646\n",
      "Iteration 6840 Training loss 0.10050267726182938 Validation loss 0.1250559687614441 Accuracy 0.6449999809265137\n",
      "Iteration 6850 Training loss 0.10283301770687103 Validation loss 0.13070465624332428 Accuracy 0.6324999928474426\n",
      "Iteration 6860 Training loss 0.10318589210510254 Validation loss 0.1290016770362854 Accuracy 0.6361666917800903\n",
      "Iteration 6870 Training loss 0.10084261000156403 Validation loss 0.1275201141834259 Accuracy 0.6355000138282776\n",
      "Iteration 6880 Training loss 0.1020021066069603 Validation loss 0.1302541345357895 Accuracy 0.6338333487510681\n",
      "Iteration 6890 Training loss 0.10177256166934967 Validation loss 0.12733085453510284 Accuracy 0.637333333492279\n",
      "Iteration 6900 Training loss 0.1034223809838295 Validation loss 0.1270301192998886 Accuracy 0.637666642665863\n",
      "Iteration 6910 Training loss 0.1014707013964653 Validation loss 0.12694264948368073 Accuracy 0.6368333101272583\n",
      "Iteration 6920 Training loss 0.10119183361530304 Validation loss 0.12405914068222046 Accuracy 0.6460000276565552\n",
      "Iteration 6930 Training loss 0.1013336256146431 Validation loss 0.12875999510288239 Accuracy 0.6351666450500488\n",
      "Iteration 6940 Training loss 0.10033751279115677 Validation loss 0.12118418514728546 Accuracy 0.6521666646003723\n",
      "Iteration 6950 Training loss 0.10087794810533524 Validation loss 0.12346957623958588 Accuracy 0.6474999785423279\n",
      "Iteration 6960 Training loss 0.10442407429218292 Validation loss 0.1376100480556488 Accuracy 0.6166666746139526\n",
      "Iteration 6970 Training loss 0.10160280019044876 Validation loss 0.1255684345960617 Accuracy 0.643833339214325\n",
      "Iteration 6980 Training loss 0.1005663052201271 Validation loss 0.12484835833311081 Accuracy 0.6441666483879089\n",
      "Iteration 6990 Training loss 0.1020594984292984 Validation loss 0.1311340630054474 Accuracy 0.6316666603088379\n",
      "Iteration 7000 Training loss 0.10067855566740036 Validation loss 0.11748343706130981 Accuracy 0.6579999923706055\n",
      "Iteration 7010 Training loss 0.10386928170919418 Validation loss 0.13338930904865265 Accuracy 0.6255000233650208\n",
      "Iteration 7020 Training loss 0.1012851893901825 Validation loss 0.1285724937915802 Accuracy 0.6365000009536743\n",
      "Iteration 7030 Training loss 0.10012691468000412 Validation loss 0.12356353551149368 Accuracy 0.6481666564941406\n",
      "Iteration 7040 Training loss 0.1010010614991188 Validation loss 0.12899066507816315 Accuracy 0.6345000267028809\n",
      "Iteration 7050 Training loss 0.09976745396852493 Validation loss 0.12499018013477325 Accuracy 0.6430000066757202\n",
      "Iteration 7060 Training loss 0.09991811960935593 Validation loss 0.12172840535640717 Accuracy 0.6511666774749756\n",
      "Iteration 7070 Training loss 0.10256698727607727 Validation loss 0.12888126075267792 Accuracy 0.6348333358764648\n",
      "Iteration 7080 Training loss 0.10010194778442383 Validation loss 0.12580375373363495 Accuracy 0.6416666507720947\n",
      "Iteration 7090 Training loss 0.1031559631228447 Validation loss 0.1282821148633957 Accuracy 0.6351666450500488\n",
      "Iteration 7100 Training loss 0.1025468111038208 Validation loss 0.13186295330524445 Accuracy 0.6299999952316284\n",
      "Iteration 7110 Training loss 0.09988292306661606 Validation loss 0.1264585703611374 Accuracy 0.6383333206176758\n",
      "Iteration 7120 Training loss 0.10009409487247467 Validation loss 0.1258096843957901 Accuracy 0.6428333520889282\n",
      "Iteration 7130 Training loss 0.09953015297651291 Validation loss 0.1266217827796936 Accuracy 0.640333354473114\n",
      "Iteration 7140 Training loss 0.10407509654760361 Validation loss 0.1347740739583969 Accuracy 0.6226666569709778\n",
      "Iteration 7150 Training loss 0.10127739608287811 Validation loss 0.12839660048484802 Accuracy 0.6366666555404663\n",
      "Iteration 7160 Training loss 0.10612017661333084 Validation loss 0.13554665446281433 Accuracy 0.6201666593551636\n",
      "Iteration 7170 Training loss 0.09885447472333908 Validation loss 0.1191093847155571 Accuracy 0.656333327293396\n",
      "Iteration 7180 Training loss 0.10183284431695938 Validation loss 0.12702684104442596 Accuracy 0.6388333439826965\n",
      "Iteration 7190 Training loss 0.10012832283973694 Validation loss 0.12005253881216049 Accuracy 0.6551666855812073\n",
      "Iteration 7200 Training loss 0.09836523234844208 Validation loss 0.11801991611719131 Accuracy 0.6583333611488342\n",
      "Iteration 7210 Training loss 0.09890597313642502 Validation loss 0.12021756172180176 Accuracy 0.653333306312561\n",
      "Iteration 7220 Training loss 0.10641396790742874 Validation loss 0.14449208974838257 Accuracy 0.6013333201408386\n",
      "Iteration 7230 Training loss 0.10181911289691925 Validation loss 0.13028483092784882 Accuracy 0.6343333125114441\n",
      "Iteration 7240 Training loss 0.10377626866102219 Validation loss 0.13211320340633392 Accuracy 0.6290000081062317\n",
      "Iteration 7250 Training loss 0.10186667740345001 Validation loss 0.1257169246673584 Accuracy 0.6421666741371155\n",
      "Iteration 7260 Training loss 0.10263127833604813 Validation loss 0.13019408285617828 Accuracy 0.6331666707992554\n",
      "Iteration 7270 Training loss 0.10082010924816132 Validation loss 0.13058345019817352 Accuracy 0.6328333616256714\n",
      "Iteration 7280 Training loss 0.09924371540546417 Validation loss 0.12100481986999512 Accuracy 0.6541666388511658\n",
      "Iteration 7290 Training loss 0.10175309330224991 Validation loss 0.13358686864376068 Accuracy 0.6255000233650208\n",
      "Iteration 7300 Training loss 0.10010316967964172 Validation loss 0.1308712214231491 Accuracy 0.6326666474342346\n",
      "Iteration 7310 Training loss 0.10145941376686096 Validation loss 0.1276310682296753 Accuracy 0.6383333206176758\n",
      "Iteration 7320 Training loss 0.10128012299537659 Validation loss 0.12627504765987396 Accuracy 0.640333354473114\n",
      "Iteration 7330 Training loss 0.10432922095060349 Validation loss 0.13961386680603027 Accuracy 0.6100000143051147\n",
      "Iteration 7340 Training loss 0.09995143860578537 Validation loss 0.1307321935892105 Accuracy 0.6324999928474426\n",
      "Iteration 7350 Training loss 0.10089820623397827 Validation loss 0.12518109381198883 Accuracy 0.6446666717529297\n",
      "Iteration 7360 Training loss 0.09910524636507034 Validation loss 0.12774404883384705 Accuracy 0.6386666893959045\n",
      "Iteration 7370 Training loss 0.10058990120887756 Validation loss 0.12763826549053192 Accuracy 0.637666642665863\n",
      "Iteration 7380 Training loss 0.09854339808225632 Validation loss 0.12525887787342072 Accuracy 0.6439999938011169\n",
      "Iteration 7390 Training loss 0.09845733642578125 Validation loss 0.125860795378685 Accuracy 0.643833339214325\n",
      "Iteration 7400 Training loss 0.10052069276571274 Validation loss 0.127479687333107 Accuracy 0.6383333206176758\n",
      "Iteration 7410 Training loss 0.10257507115602493 Validation loss 0.12655037641525269 Accuracy 0.640500009059906\n",
      "Iteration 7420 Training loss 0.09850111603736877 Validation loss 0.12373215705156326 Accuracy 0.6481666564941406\n",
      "Iteration 7430 Training loss 0.09813740849494934 Validation loss 0.1183331087231636 Accuracy 0.6570000052452087\n",
      "Iteration 7440 Training loss 0.10297685861587524 Validation loss 0.13589434325695038 Accuracy 0.621999979019165\n",
      "Iteration 7450 Training loss 0.10147678852081299 Validation loss 0.13560818135738373 Accuracy 0.6223333477973938\n",
      "Iteration 7460 Training loss 0.10135600715875626 Validation loss 0.129285991191864 Accuracy 0.6345000267028809\n",
      "Iteration 7470 Training loss 0.10063920170068741 Validation loss 0.12739673256874084 Accuracy 0.6393333077430725\n",
      "Iteration 7480 Training loss 0.10329166799783707 Validation loss 0.1338590830564499 Accuracy 0.6256666779518127\n",
      "Iteration 7490 Training loss 0.10038989037275314 Validation loss 0.12312764674425125 Accuracy 0.6485000252723694\n",
      "Iteration 7500 Training loss 0.10262870043516159 Validation loss 0.13369427621364594 Accuracy 0.6266666650772095\n",
      "Iteration 7510 Training loss 0.10178755968809128 Validation loss 0.125580295920372 Accuracy 0.6424999833106995\n",
      "Iteration 7520 Training loss 0.09915924072265625 Validation loss 0.1274290382862091 Accuracy 0.6393333077430725\n",
      "Iteration 7530 Training loss 0.10299019515514374 Validation loss 0.13073000311851501 Accuracy 0.6341666579246521\n",
      "Iteration 7540 Training loss 0.09988958388566971 Validation loss 0.12543848156929016 Accuracy 0.6445000171661377\n",
      "Iteration 7550 Training loss 0.10147234052419662 Validation loss 0.1301717311143875 Accuracy 0.6358333230018616\n",
      "Iteration 7560 Training loss 0.1007140502333641 Validation loss 0.12725767493247986 Accuracy 0.6399999856948853\n",
      "Iteration 7570 Training loss 0.10465087741613388 Validation loss 0.13225698471069336 Accuracy 0.6311666369438171\n",
      "Iteration 7580 Training loss 0.09827429056167603 Validation loss 0.11827783286571503 Accuracy 0.6575000286102295\n",
      "Iteration 7590 Training loss 0.10066846758127213 Validation loss 0.1266949623823166 Accuracy 0.640333354473114\n",
      "Iteration 7600 Training loss 0.1025778204202652 Validation loss 0.13226890563964844 Accuracy 0.6298333406448364\n",
      "Iteration 7610 Training loss 0.10073743015527725 Validation loss 0.12574689090251923 Accuracy 0.6420000195503235\n",
      "Iteration 7620 Training loss 0.10018380731344223 Validation loss 0.12522684037685394 Accuracy 0.6453333497047424\n",
      "Iteration 7630 Training loss 0.09990588575601578 Validation loss 0.12516096234321594 Accuracy 0.6443333625793457\n",
      "Iteration 7640 Training loss 0.10224241018295288 Validation loss 0.13746283948421478 Accuracy 0.6154999732971191\n",
      "Iteration 7650 Training loss 0.09995988011360168 Validation loss 0.12761026620864868 Accuracy 0.6396666765213013\n",
      "Iteration 7660 Training loss 0.10052888095378876 Validation loss 0.13095180690288544 Accuracy 0.6333333253860474\n",
      "Iteration 7670 Training loss 0.10045034438371658 Validation loss 0.12564589083194733 Accuracy 0.643666684627533\n",
      "Iteration 7680 Training loss 0.10065777599811554 Validation loss 0.12490472197532654 Accuracy 0.6446666717529297\n",
      "Iteration 7690 Training loss 0.10143822431564331 Validation loss 0.13229933381080627 Accuracy 0.6305000185966492\n",
      "Iteration 7700 Training loss 0.10005305707454681 Validation loss 0.1280272752046585 Accuracy 0.6359999775886536\n",
      "Iteration 7710 Training loss 0.09987152367830276 Validation loss 0.12986335158348083 Accuracy 0.6353333592414856\n",
      "Iteration 7720 Training loss 0.10196014493703842 Validation loss 0.12865427136421204 Accuracy 0.6356666684150696\n",
      "Iteration 7730 Training loss 0.10033868253231049 Validation loss 0.12171711027622223 Accuracy 0.6528333425521851\n",
      "Iteration 7740 Training loss 0.10185615718364716 Validation loss 0.1318189948797226 Accuracy 0.6320000290870667\n",
      "Iteration 7750 Training loss 0.1026626005768776 Validation loss 0.13353434205055237 Accuracy 0.6263333559036255\n",
      "Iteration 7760 Training loss 0.10045068711042404 Validation loss 0.12787416577339172 Accuracy 0.6393333077430725\n",
      "Iteration 7770 Training loss 0.0996152013540268 Validation loss 0.11952544003725052 Accuracy 0.656499981880188\n",
      "Iteration 7780 Training loss 0.1026502475142479 Validation loss 0.13711100816726685 Accuracy 0.6178333163261414\n",
      "Iteration 7790 Training loss 0.09984347224235535 Validation loss 0.12638425827026367 Accuracy 0.640666663646698\n",
      "Iteration 7800 Training loss 0.10021466016769409 Validation loss 0.11929468810558319 Accuracy 0.6551666855812073\n",
      "Iteration 7810 Training loss 0.10112494230270386 Validation loss 0.12046919763088226 Accuracy 0.6578333377838135\n",
      "Iteration 7820 Training loss 0.1036929115653038 Validation loss 0.13800100982189178 Accuracy 0.6168333292007446\n",
      "Iteration 7830 Training loss 0.09985169768333435 Validation loss 0.12458662688732147 Accuracy 0.6466666460037231\n",
      "Iteration 7840 Training loss 0.09933493286371231 Validation loss 0.1278315633535385 Accuracy 0.637499988079071\n",
      "Iteration 7850 Training loss 0.10097403824329376 Validation loss 0.12575063109397888 Accuracy 0.6441666483879089\n",
      "Iteration 7860 Training loss 0.10153058171272278 Validation loss 0.13109339773654938 Accuracy 0.6333333253860474\n",
      "Iteration 7870 Training loss 0.09933426976203918 Validation loss 0.12208718061447144 Accuracy 0.6549999713897705\n",
      "Iteration 7880 Training loss 0.0982331931591034 Validation loss 0.11989106982946396 Accuracy 0.656000018119812\n",
      "Iteration 7890 Training loss 0.10324021428823471 Validation loss 0.13596655428409576 Accuracy 0.621833324432373\n",
      "Iteration 7900 Training loss 0.09987262636423111 Validation loss 0.1256384253501892 Accuracy 0.6433333158493042\n",
      "Iteration 7910 Training loss 0.0995345190167427 Validation loss 0.12696793675422668 Accuracy 0.6421666741371155\n",
      "Iteration 7920 Training loss 0.10335806012153625 Validation loss 0.13614650070667267 Accuracy 0.6215000152587891\n",
      "Iteration 7930 Training loss 0.09996446967124939 Validation loss 0.1281680464744568 Accuracy 0.6380000114440918\n",
      "Iteration 7940 Training loss 0.09890938550233841 Validation loss 0.12217482179403305 Accuracy 0.6524999737739563\n",
      "Iteration 7950 Training loss 0.10427382588386536 Validation loss 0.1387263387441635 Accuracy 0.6151666641235352\n",
      "Iteration 7960 Training loss 0.09972166270017624 Validation loss 0.12096814811229706 Accuracy 0.6548333168029785\n",
      "Iteration 7970 Training loss 0.10012087970972061 Validation loss 0.13437584042549133 Accuracy 0.6243333220481873\n",
      "Iteration 7980 Training loss 0.10053746402263641 Validation loss 0.12551617622375488 Accuracy 0.6455000042915344\n",
      "Iteration 7990 Training loss 0.09773571044206619 Validation loss 0.1238570436835289 Accuracy 0.6481666564941406\n",
      "Iteration 8000 Training loss 0.10114550590515137 Validation loss 0.13106413185596466 Accuracy 0.6336666941642761\n",
      "Iteration 8010 Training loss 0.0990467220544815 Validation loss 0.12290121614933014 Accuracy 0.6511666774749756\n",
      "Iteration 8020 Training loss 0.09821628779172897 Validation loss 0.12379975616931915 Accuracy 0.6485000252723694\n",
      "Iteration 8030 Training loss 0.10159905254840851 Validation loss 0.1357799619436264 Accuracy 0.6213333606719971\n",
      "Iteration 8040 Training loss 0.0999695435166359 Validation loss 0.12532584369182587 Accuracy 0.6441666483879089\n",
      "Iteration 8050 Training loss 0.10109301656484604 Validation loss 0.1285572499036789 Accuracy 0.6381666660308838\n",
      "Iteration 8060 Training loss 0.09941768646240234 Validation loss 0.1269909292459488 Accuracy 0.6431666612625122\n",
      "Iteration 8070 Training loss 0.09938638657331467 Validation loss 0.12310858815908432 Accuracy 0.6489999890327454\n",
      "Iteration 8080 Training loss 0.09959936887025833 Validation loss 0.12614339590072632 Accuracy 0.6430000066757202\n",
      "Iteration 8090 Training loss 0.10136130452156067 Validation loss 0.13835816085338593 Accuracy 0.6154999732971191\n",
      "Iteration 8100 Training loss 0.09898800402879715 Validation loss 0.12388170510530472 Accuracy 0.6478333473205566\n",
      "Iteration 8110 Training loss 0.09802006185054779 Validation loss 0.12610775232315063 Accuracy 0.6421666741371155\n",
      "Iteration 8120 Training loss 0.10007431358098984 Validation loss 0.13025669753551483 Accuracy 0.6333333253860474\n",
      "Iteration 8130 Training loss 0.10087651759386063 Validation loss 0.12902581691741943 Accuracy 0.6356666684150696\n",
      "Iteration 8140 Training loss 0.10047660022974014 Validation loss 0.13153253495693207 Accuracy 0.6336666941642761\n",
      "Iteration 8150 Training loss 0.10004427284002304 Validation loss 0.125301793217659 Accuracy 0.6464999914169312\n",
      "Iteration 8160 Training loss 0.09903140366077423 Validation loss 0.1219569519162178 Accuracy 0.6536666750907898\n",
      "Iteration 8170 Training loss 0.10259346663951874 Validation loss 0.1325322687625885 Accuracy 0.6320000290870667\n",
      "Iteration 8180 Training loss 0.10118439048528671 Validation loss 0.13177917897701263 Accuracy 0.6328333616256714\n",
      "Iteration 8190 Training loss 0.10040566325187683 Validation loss 0.12641464173793793 Accuracy 0.6423333287239075\n",
      "Iteration 8200 Training loss 0.1044984981417656 Validation loss 0.1373533457517624 Accuracy 0.6193333268165588\n",
      "Iteration 8210 Training loss 0.09953594952821732 Validation loss 0.12617629766464233 Accuracy 0.6443333625793457\n",
      "Iteration 8220 Training loss 0.0975721925497055 Validation loss 0.1240406408905983 Accuracy 0.6488333344459534\n",
      "Iteration 8230 Training loss 0.10052917152643204 Validation loss 0.13077819347381592 Accuracy 0.6345000267028809\n",
      "Iteration 8240 Training loss 0.10034594684839249 Validation loss 0.12801750004291534 Accuracy 0.6388333439826965\n",
      "Iteration 8250 Training loss 0.09901342540979385 Validation loss 0.12298993021249771 Accuracy 0.6518333554267883\n",
      "Iteration 8260 Training loss 0.09955509752035141 Validation loss 0.12574422359466553 Accuracy 0.6443333625793457\n",
      "Iteration 8270 Training loss 0.10144210606813431 Validation loss 0.12835828959941864 Accuracy 0.637499988079071\n",
      "Iteration 8280 Training loss 0.10095413029193878 Validation loss 0.13048948347568512 Accuracy 0.6359999775886536\n",
      "Iteration 8290 Training loss 0.0980035737156868 Validation loss 0.13070708513259888 Accuracy 0.6338333487510681\n",
      "Iteration 8300 Training loss 0.09923157095909119 Validation loss 0.12392464280128479 Accuracy 0.6496666669845581\n",
      "Iteration 8310 Training loss 0.09941978007555008 Validation loss 0.12869949638843536 Accuracy 0.6370000243186951\n",
      "Iteration 8320 Training loss 0.0992952510714531 Validation loss 0.12995992600917816 Accuracy 0.6336666941642761\n",
      "Iteration 8330 Training loss 0.1014762744307518 Validation loss 0.13062839210033417 Accuracy 0.6340000033378601\n",
      "Iteration 8340 Training loss 0.09763596951961517 Validation loss 0.12692080438137054 Accuracy 0.6399999856948853\n",
      "Iteration 8350 Training loss 0.09968326985836029 Validation loss 0.12959778308868408 Accuracy 0.6358333230018616\n",
      "Iteration 8360 Training loss 0.10304441303014755 Validation loss 0.13956990838050842 Accuracy 0.6131666898727417\n",
      "Iteration 8370 Training loss 0.09919875860214233 Validation loss 0.12028811872005463 Accuracy 0.6571666598320007\n",
      "Iteration 8380 Training loss 0.10303202271461487 Validation loss 0.13855572044849396 Accuracy 0.6163333058357239\n",
      "Iteration 8390 Training loss 0.10066357254981995 Validation loss 0.1305518001317978 Accuracy 0.6345000267028809\n",
      "Iteration 8400 Training loss 0.09789546579122543 Validation loss 0.12202997505664825 Accuracy 0.652999997138977\n",
      "Iteration 8410 Training loss 0.09855247288942337 Validation loss 0.1253442019224167 Accuracy 0.6439999938011169\n",
      "Iteration 8420 Training loss 0.10065962374210358 Validation loss 0.13047093152999878 Accuracy 0.6355000138282776\n",
      "Iteration 8430 Training loss 0.09841955453157425 Validation loss 0.1289716213941574 Accuracy 0.637333333492279\n",
      "Iteration 8440 Training loss 0.09629000723361969 Validation loss 0.11914929002523422 Accuracy 0.6585000157356262\n",
      "Iteration 8450 Training loss 0.10043585300445557 Validation loss 0.132563054561615 Accuracy 0.6318333148956299\n",
      "Iteration 8460 Training loss 0.10003074258565903 Validation loss 0.12957905232906342 Accuracy 0.6371666789054871\n",
      "Iteration 8470 Training loss 0.09931502491235733 Validation loss 0.11945579946041107 Accuracy 0.6585000157356262\n",
      "Iteration 8480 Training loss 0.10114040225744247 Validation loss 0.13014009594917297 Accuracy 0.6359999775886536\n",
      "Iteration 8490 Training loss 0.09921570867300034 Validation loss 0.12988053262233734 Accuracy 0.6356666684150696\n",
      "Iteration 8500 Training loss 0.09897240996360779 Validation loss 0.12254467606544495 Accuracy 0.653333306312561\n",
      "Iteration 8510 Training loss 0.10268808901309967 Validation loss 0.13526837527751923 Accuracy 0.625\n",
      "Iteration 8520 Training loss 0.09663403034210205 Validation loss 0.12298443913459778 Accuracy 0.6516666412353516\n",
      "Iteration 8530 Training loss 0.09844087064266205 Validation loss 0.12344992905855179 Accuracy 0.6520000100135803\n",
      "Iteration 8540 Training loss 0.10091466456651688 Validation loss 0.13311472535133362 Accuracy 0.6311666369438171\n",
      "Iteration 8550 Training loss 0.09875019639730453 Validation loss 0.12626434862613678 Accuracy 0.6433333158493042\n",
      "Iteration 8560 Training loss 0.0978526845574379 Validation loss 0.12466678768396378 Accuracy 0.6461666822433472\n",
      "Iteration 8570 Training loss 0.0994277372956276 Validation loss 0.12742163240909576 Accuracy 0.6398333311080933\n",
      "Iteration 8580 Training loss 0.09800594300031662 Validation loss 0.1241363137960434 Accuracy 0.6496666669845581\n",
      "Iteration 8590 Training loss 0.09978818893432617 Validation loss 0.1305176019668579 Accuracy 0.6343333125114441\n",
      "Iteration 8600 Training loss 0.09896908700466156 Validation loss 0.12623776495456696 Accuracy 0.6431666612625122\n",
      "Iteration 8610 Training loss 0.10090669989585876 Validation loss 0.12630750238895416 Accuracy 0.6424999833106995\n",
      "Iteration 8620 Training loss 0.0990152359008789 Validation loss 0.12613996863365173 Accuracy 0.6433333158493042\n",
      "Iteration 8630 Training loss 0.09801989793777466 Validation loss 0.1268591433763504 Accuracy 0.6423333287239075\n",
      "Iteration 8640 Training loss 0.0996837168931961 Validation loss 0.11477763950824738 Accuracy 0.6661666631698608\n",
      "Iteration 8650 Training loss 0.10176008194684982 Validation loss 0.13777174055576324 Accuracy 0.6193333268165588\n",
      "Iteration 8660 Training loss 0.09570972621440887 Validation loss 0.12316229939460754 Accuracy 0.6506666541099548\n",
      "Iteration 8670 Training loss 0.09900044649839401 Validation loss 0.1336612105369568 Accuracy 0.6306666731834412\n",
      "Iteration 8680 Training loss 0.09789682924747467 Validation loss 0.12254197895526886 Accuracy 0.6541666388511658\n",
      "Iteration 8690 Training loss 0.10040333867073059 Validation loss 0.13164730370044708 Accuracy 0.6343333125114441\n",
      "Iteration 8700 Training loss 0.10083292424678802 Validation loss 0.13067375123500824 Accuracy 0.6355000138282776\n",
      "Iteration 8710 Training loss 0.09926334768533707 Validation loss 0.1237540990114212 Accuracy 0.6496666669845581\n",
      "Iteration 8720 Training loss 0.09855716675519943 Validation loss 0.11547782272100449 Accuracy 0.6653333306312561\n",
      "Iteration 8730 Training loss 0.09725986421108246 Validation loss 0.12620411813259125 Accuracy 0.6426666378974915\n",
      "Iteration 8740 Training loss 0.10045497864484787 Validation loss 0.13302795588970184 Accuracy 0.6326666474342346\n",
      "Iteration 8750 Training loss 0.10068149119615555 Validation loss 0.12694406509399414 Accuracy 0.6418333053588867\n",
      "Iteration 8760 Training loss 0.10143519192934036 Validation loss 0.13913071155548096 Accuracy 0.6175000071525574\n",
      "Iteration 8770 Training loss 0.09997998178005219 Validation loss 0.1215038076043129 Accuracy 0.6558333039283752\n",
      "Iteration 8780 Training loss 0.10115508735179901 Validation loss 0.13470488786697388 Accuracy 0.6263333559036255\n",
      "Iteration 8790 Training loss 0.10259018838405609 Validation loss 0.1319158971309662 Accuracy 0.6331666707992554\n",
      "Iteration 8800 Training loss 0.09887386858463287 Validation loss 0.12347336858510971 Accuracy 0.6511666774749756\n",
      "Iteration 8810 Training loss 0.09903638809919357 Validation loss 0.1327197253704071 Accuracy 0.6330000162124634\n",
      "Iteration 8820 Training loss 0.09916115552186966 Validation loss 0.125498965382576 Accuracy 0.6445000171661377\n",
      "Iteration 8830 Training loss 0.09818188846111298 Validation loss 0.12805017828941345 Accuracy 0.6378333568572998\n",
      "Iteration 8840 Training loss 0.09957429766654968 Validation loss 0.12552925944328308 Accuracy 0.6443333625793457\n",
      "Iteration 8850 Training loss 0.09751616418361664 Validation loss 0.12597985565662384 Accuracy 0.6449999809265137\n",
      "Iteration 8860 Training loss 0.10069403797388077 Validation loss 0.12656836211681366 Accuracy 0.643833339214325\n",
      "Iteration 8870 Training loss 0.09976911544799805 Validation loss 0.12864179909229279 Accuracy 0.6398333311080933\n",
      "Iteration 8880 Training loss 0.09974110126495361 Validation loss 0.12932999432086945 Accuracy 0.6368333101272583\n",
      "Iteration 8890 Training loss 0.10092949122190475 Validation loss 0.132003515958786 Accuracy 0.6336666941642761\n",
      "Iteration 8900 Training loss 0.1000082716345787 Validation loss 0.12946730852127075 Accuracy 0.6380000114440918\n",
      "Iteration 8910 Training loss 0.09755811840295792 Validation loss 0.12312506139278412 Accuracy 0.6528333425521851\n",
      "Iteration 8920 Training loss 0.10307396203279495 Validation loss 0.13999119400978088 Accuracy 0.6134999990463257\n",
      "Iteration 8930 Training loss 0.09653865545988083 Validation loss 0.12033095955848694 Accuracy 0.6575000286102295\n",
      "Iteration 8940 Training loss 0.10339623689651489 Validation loss 0.12954987585544586 Accuracy 0.6371666789054871\n",
      "Iteration 8950 Training loss 0.09830678999423981 Validation loss 0.12833815813064575 Accuracy 0.6378333568572998\n",
      "Iteration 8960 Training loss 0.09916845709085464 Validation loss 0.12305448204278946 Accuracy 0.6526666879653931\n",
      "Iteration 8970 Training loss 0.09993726760149002 Validation loss 0.13117289543151855 Accuracy 0.6349999904632568\n",
      "Iteration 8980 Training loss 0.09930776804685593 Validation loss 0.12308649718761444 Accuracy 0.6518333554267883\n",
      "Iteration 8990 Training loss 0.10296536237001419 Validation loss 0.13369056582450867 Accuracy 0.6311666369438171\n",
      "Iteration 9000 Training loss 0.09791459888219833 Validation loss 0.1261242926120758 Accuracy 0.6448333263397217\n",
      "Iteration 9010 Training loss 0.10093162208795547 Validation loss 0.1274927258491516 Accuracy 0.6420000195503235\n",
      "Iteration 9020 Training loss 0.09935560822486877 Validation loss 0.13155046105384827 Accuracy 0.6359999775886536\n",
      "Iteration 9030 Training loss 0.10204622149467468 Validation loss 0.13340458273887634 Accuracy 0.6316666603088379\n",
      "Iteration 9040 Training loss 0.10052063316106796 Validation loss 0.13297612965106964 Accuracy 0.6330000162124634\n",
      "Iteration 9050 Training loss 0.1012514978647232 Validation loss 0.1312979906797409 Accuracy 0.6361666917800903\n",
      "Iteration 9060 Training loss 0.09640216827392578 Validation loss 0.12034651637077332 Accuracy 0.6573333144187927\n",
      "Iteration 9070 Training loss 0.0989459827542305 Validation loss 0.1240091472864151 Accuracy 0.6523333191871643\n",
      "Iteration 9080 Training loss 0.10216951370239258 Validation loss 0.13479521870613098 Accuracy 0.6274999976158142\n",
      "Iteration 9090 Training loss 0.10005422681570053 Validation loss 0.12562403082847595 Accuracy 0.6456666588783264\n",
      "Iteration 9100 Training loss 0.09634989500045776 Validation loss 0.11254554241895676 Accuracy 0.6700000166893005\n",
      "Iteration 9110 Training loss 0.10256463289260864 Validation loss 0.11159518361091614 Accuracy 0.6703333258628845\n",
      "Iteration 9120 Training loss 0.09800105541944504 Validation loss 0.11189568787813187 Accuracy 0.671833336353302\n",
      "Iteration 9130 Training loss 0.10125618427991867 Validation loss 0.11150640994310379 Accuracy 0.6723333597183228\n",
      "Iteration 9140 Training loss 0.09860146790742874 Validation loss 0.11138830333948135 Accuracy 0.6693333387374878\n",
      "Iteration 9150 Training loss 0.09859254211187363 Validation loss 0.11184968799352646 Accuracy 0.67166668176651\n",
      "Iteration 9160 Training loss 0.09982967376708984 Validation loss 0.1115432009100914 Accuracy 0.6664999723434448\n",
      "Iteration 9170 Training loss 0.09819991141557693 Validation loss 0.11230788379907608 Accuracy 0.671833336353302\n",
      "Iteration 9180 Training loss 0.09831101447343826 Validation loss 0.11327063292264938 Accuracy 0.6696666479110718\n",
      "Iteration 9190 Training loss 0.09860152006149292 Validation loss 0.1116630882024765 Accuracy 0.6685000061988831\n",
      "Iteration 9200 Training loss 0.10066214203834534 Validation loss 0.11188246309757233 Accuracy 0.6698333621025085\n",
      "Iteration 9210 Training loss 0.09914102405309677 Validation loss 0.1109166070818901 Accuracy 0.668666660785675\n",
      "Iteration 9220 Training loss 0.09843310713768005 Validation loss 0.11277423799037933 Accuracy 0.6703333258628845\n",
      "Iteration 9230 Training loss 0.09929387271404266 Validation loss 0.11144537478685379 Accuracy 0.6690000295639038\n",
      "Iteration 9240 Training loss 0.10434003919363022 Validation loss 0.11206284910440445 Accuracy 0.6740000247955322\n",
      "Iteration 9250 Training loss 0.09714054316282272 Validation loss 0.11306912451982498 Accuracy 0.6696666479110718\n",
      "Iteration 9260 Training loss 0.09976926445960999 Validation loss 0.11598283797502518 Accuracy 0.6656666398048401\n",
      "Iteration 9270 Training loss 0.09694173187017441 Validation loss 0.1118716150522232 Accuracy 0.668833315372467\n",
      "Iteration 9280 Training loss 0.10093603283166885 Validation loss 0.11150187999010086 Accuracy 0.6681666374206543\n",
      "Iteration 9290 Training loss 0.10093481093645096 Validation loss 0.11142688244581223 Accuracy 0.6690000295639038\n",
      "Iteration 9300 Training loss 0.09728217869997025 Validation loss 0.11258269101381302 Accuracy 0.671833336353302\n",
      "Iteration 9310 Training loss 0.09890999644994736 Validation loss 0.11137919127941132 Accuracy 0.6706666946411133\n",
      "Iteration 9320 Training loss 0.0963701382279396 Validation loss 0.11293543130159378 Accuracy 0.6696666479110718\n",
      "Iteration 9330 Training loss 0.0999269187450409 Validation loss 0.11144331097602844 Accuracy 0.6669999957084656\n",
      "Iteration 9340 Training loss 0.0953221321105957 Validation loss 0.11152219772338867 Accuracy 0.668833315372467\n",
      "Iteration 9350 Training loss 0.10024401545524597 Validation loss 0.11178743839263916 Accuracy 0.6701666712760925\n",
      "Iteration 9360 Training loss 0.10453682392835617 Validation loss 0.1123921275138855 Accuracy 0.6743333339691162\n",
      "Iteration 9370 Training loss 0.0995560735464096 Validation loss 0.11126137524843216 Accuracy 0.6701666712760925\n",
      "Iteration 9380 Training loss 0.09847375750541687 Validation loss 0.11124643683433533 Accuracy 0.6679999828338623\n",
      "Iteration 9390 Training loss 0.09567578881978989 Validation loss 0.1137506440281868 Accuracy 0.6683333516120911\n",
      "Iteration 9400 Training loss 0.09577386826276779 Validation loss 0.11334166675806046 Accuracy 0.6710000038146973\n",
      "Iteration 9410 Training loss 0.10385097563266754 Validation loss 0.11220361292362213 Accuracy 0.6729999780654907\n",
      "Iteration 9420 Training loss 0.0972815752029419 Validation loss 0.11160223186016083 Accuracy 0.6729999780654907\n",
      "Iteration 9430 Training loss 0.09768815338611603 Validation loss 0.11172786355018616 Accuracy 0.6703333258628845\n",
      "Iteration 9440 Training loss 0.09830007702112198 Validation loss 0.11157356947660446 Accuracy 0.6701666712760925\n",
      "Iteration 9450 Training loss 0.09907564520835876 Validation loss 0.11118798702955246 Accuracy 0.6713333129882812\n",
      "Iteration 9460 Training loss 0.09670917689800262 Validation loss 0.11190533638000488 Accuracy 0.671999990940094\n",
      "Iteration 9470 Training loss 0.10218880325555801 Validation loss 0.11184182018041611 Accuracy 0.672166645526886\n",
      "Iteration 9480 Training loss 0.09710025787353516 Validation loss 0.11286816745996475 Accuracy 0.672166645526886\n",
      "Iteration 9490 Training loss 0.09956885129213333 Validation loss 0.11178808659315109 Accuracy 0.6701666712760925\n",
      "Iteration 9500 Training loss 0.10206049680709839 Validation loss 0.11090045422315598 Accuracy 0.6698333621025085\n",
      "Iteration 9510 Training loss 0.09757090359926224 Validation loss 0.11173643171787262 Accuracy 0.6723333597183228\n",
      "Iteration 9520 Training loss 0.09860105067491531 Validation loss 0.11199358850717545 Accuracy 0.67166668176651\n",
      "Iteration 9530 Training loss 0.09793489426374435 Validation loss 0.11300307512283325 Accuracy 0.671500027179718\n",
      "Iteration 9540 Training loss 0.0983114019036293 Validation loss 0.11122365295886993 Accuracy 0.6723333597183228\n",
      "Iteration 9550 Training loss 0.09832216054201126 Validation loss 0.11202963441610336 Accuracy 0.6729999780654907\n",
      "Iteration 9560 Training loss 0.09894653409719467 Validation loss 0.11123500764369965 Accuracy 0.6703333258628845\n",
      "Iteration 9570 Training loss 0.09554073214530945 Validation loss 0.11348912864923477 Accuracy 0.671500027179718\n",
      "Iteration 9580 Training loss 0.10465198010206223 Validation loss 0.14189104735851288 Accuracy 0.6113333106040955\n",
      "Iteration 9590 Training loss 0.09872608631849289 Validation loss 0.1285434365272522 Accuracy 0.6396666765213013\n",
      "Iteration 9600 Training loss 0.09738253057003021 Validation loss 0.12555532157421112 Accuracy 0.6468333601951599\n",
      "Iteration 9610 Training loss 0.09608952701091766 Validation loss 0.12295682728290558 Accuracy 0.6518333554267883\n",
      "Iteration 9620 Training loss 0.10006336122751236 Validation loss 0.12792526185512543 Accuracy 0.6426666378974915\n",
      "Iteration 9630 Training loss 0.09858686476945877 Validation loss 0.12401842325925827 Accuracy 0.6510000228881836\n",
      "Iteration 9640 Training loss 0.09692899882793427 Validation loss 0.12378175556659698 Accuracy 0.6516666412353516\n",
      "Iteration 9650 Training loss 0.10042474418878555 Validation loss 0.1346346139907837 Accuracy 0.628333330154419\n",
      "Iteration 9660 Training loss 0.09791671484708786 Validation loss 0.1280265748500824 Accuracy 0.6418333053588867\n",
      "Iteration 9670 Training loss 0.10051688551902771 Validation loss 0.12930630147457123 Accuracy 0.640500009059906\n",
      "Iteration 9680 Training loss 0.09917870908975601 Validation loss 0.125676229596138 Accuracy 0.6463333368301392\n",
      "Iteration 9690 Training loss 0.1003958061337471 Validation loss 0.1283913105726242 Accuracy 0.6416666507720947\n",
      "Iteration 9700 Training loss 0.0955573096871376 Validation loss 0.12220600247383118 Accuracy 0.6546666622161865\n",
      "Iteration 9710 Training loss 0.1005597710609436 Validation loss 0.13168323040008545 Accuracy 0.6359999775886536\n",
      "Iteration 9720 Training loss 0.09990346431732178 Validation loss 0.13118718564510345 Accuracy 0.6366666555404663\n",
      "Iteration 9730 Training loss 0.09646651148796082 Validation loss 0.12642046809196472 Accuracy 0.6453333497047424\n",
      "Iteration 9740 Training loss 0.09994673728942871 Validation loss 0.12560799717903137 Accuracy 0.6478333473205566\n",
      "Iteration 9750 Training loss 0.09685193002223969 Validation loss 0.12584832310676575 Accuracy 0.6460000276565552\n",
      "Iteration 9760 Training loss 0.09842102974653244 Validation loss 0.13170970976352692 Accuracy 0.6330000162124634\n",
      "Iteration 9770 Training loss 0.0978672057390213 Validation loss 0.1183042973279953 Accuracy 0.659333348274231\n",
      "Iteration 9780 Training loss 0.09755489975214005 Validation loss 0.1138586699962616 Accuracy 0.67166668176651\n",
      "Iteration 9790 Training loss 0.1092330738902092 Validation loss 0.11472977697849274 Accuracy 0.6700000166893005\n",
      "Iteration 9800 Training loss 0.09737715870141983 Validation loss 0.11127741634845734 Accuracy 0.671833336353302\n",
      "Iteration 9810 Training loss 0.09921880811452866 Validation loss 0.11158766597509384 Accuracy 0.6711666584014893\n",
      "Iteration 9820 Training loss 0.10127703845500946 Validation loss 0.11150538921356201 Accuracy 0.6735000014305115\n",
      "Iteration 9830 Training loss 0.09904328733682632 Validation loss 0.11221791803836823 Accuracy 0.6728333234786987\n",
      "Iteration 9840 Training loss 0.10107000917196274 Validation loss 0.1112198606133461 Accuracy 0.672166645526886\n",
      "Iteration 9850 Training loss 0.09666758030653 Validation loss 0.11405385285615921 Accuracy 0.6693333387374878\n",
      "Iteration 9860 Training loss 0.10174039751291275 Validation loss 0.11142165958881378 Accuracy 0.671500027179718\n",
      "Iteration 9870 Training loss 0.09901456534862518 Validation loss 0.11151421070098877 Accuracy 0.6703333258628845\n",
      "Iteration 9880 Training loss 0.09955143183469772 Validation loss 0.11172735691070557 Accuracy 0.6728333234786987\n",
      "Iteration 9890 Training loss 0.0992332175374031 Validation loss 0.11135638505220413 Accuracy 0.67166668176651\n",
      "Iteration 9900 Training loss 0.09774066507816315 Validation loss 0.11249163746833801 Accuracy 0.672166645526886\n",
      "Iteration 9910 Training loss 0.09772007912397385 Validation loss 0.11146606504917145 Accuracy 0.67166668176651\n",
      "Iteration 9920 Training loss 0.09681809693574905 Validation loss 0.1123688593506813 Accuracy 0.6733333468437195\n",
      "Iteration 9930 Training loss 0.10186509042978287 Validation loss 0.11175698786973953 Accuracy 0.6728333234786987\n",
      "Iteration 9940 Training loss 0.09723351895809174 Validation loss 0.1119747906923294 Accuracy 0.6741666793823242\n",
      "Iteration 9950 Training loss 0.09728240966796875 Validation loss 0.11478537321090698 Accuracy 0.6703333258628845\n",
      "Iteration 9960 Training loss 0.09785178303718567 Validation loss 0.11400929093360901 Accuracy 0.6710000038146973\n",
      "Iteration 9970 Training loss 0.10242323577404022 Validation loss 0.11188432574272156 Accuracy 0.6733333468437195\n",
      "Iteration 9980 Training loss 0.09783176332712173 Validation loss 0.11214354634284973 Accuracy 0.6729999780654907\n",
      "Iteration 9990 Training loss 0.09566985815763474 Validation loss 0.1183534562587738 Accuracy 0.659166693687439\n",
      "Iteration 10000 Training loss 0.09671702980995178 Validation loss 0.12799037992954254 Accuracy 0.6424999833106995\n",
      "Iteration 10010 Training loss 0.101505808532238 Validation loss 0.1368623524904251 Accuracy 0.6240000128746033\n",
      "Iteration 10020 Training loss 0.09935754537582397 Validation loss 0.12932151556015015 Accuracy 0.6399999856948853\n",
      "Iteration 10030 Training loss 0.09806754440069199 Validation loss 0.12634286284446716 Accuracy 0.6461666822433472\n",
      "Iteration 10040 Training loss 0.09911031275987625 Validation loss 0.13510671257972717 Accuracy 0.6290000081062317\n",
      "Iteration 10050 Training loss 0.09575530141592026 Validation loss 0.12633435428142548 Accuracy 0.6445000171661377\n",
      "Iteration 10060 Training loss 0.09984161704778671 Validation loss 0.12481140345335007 Accuracy 0.6496666669845581\n",
      "Iteration 10070 Training loss 0.1015181615948677 Validation loss 0.1375892609357834 Accuracy 0.621666669845581\n",
      "Iteration 10080 Training loss 0.0959421917796135 Validation loss 0.12260176241397858 Accuracy 0.6554999947547913\n",
      "Iteration 10090 Training loss 0.09789818525314331 Validation loss 0.12970025837421417 Accuracy 0.6395000219345093\n",
      "Iteration 10100 Training loss 0.10045615583658218 Validation loss 0.12955187261104584 Accuracy 0.640999972820282\n",
      "Iteration 10110 Training loss 0.09931628406047821 Validation loss 0.12864260375499725 Accuracy 0.6424999833106995\n",
      "Iteration 10120 Training loss 0.09871130436658859 Validation loss 0.12949588894844055 Accuracy 0.640666663646698\n",
      "Iteration 10130 Training loss 0.09757496416568756 Validation loss 0.12664662301540375 Accuracy 0.6441666483879089\n",
      "Iteration 10140 Training loss 0.09978877753019333 Validation loss 0.13397246599197388 Accuracy 0.6298333406448364\n",
      "Iteration 10150 Training loss 0.0980525016784668 Validation loss 0.11869366466999054 Accuracy 0.6613333225250244\n",
      "Iteration 10160 Training loss 0.09871140867471695 Validation loss 0.1257178783416748 Accuracy 0.6460000276565552\n",
      "Iteration 10170 Training loss 0.09897822141647339 Validation loss 0.12674979865550995 Accuracy 0.643666684627533\n",
      "Iteration 10180 Training loss 0.09707716107368469 Validation loss 0.12864260375499725 Accuracy 0.6426666378974915\n",
      "Iteration 10190 Training loss 0.09909271448850632 Validation loss 0.13278108835220337 Accuracy 0.6321666836738586\n",
      "Iteration 10200 Training loss 0.09735891968011856 Validation loss 0.12662780284881592 Accuracy 0.6448333263397217\n",
      "Iteration 10210 Training loss 0.09856130182743073 Validation loss 0.13115835189819336 Accuracy 0.6355000138282776\n",
      "Iteration 10220 Training loss 0.099325992166996 Validation loss 0.13059104979038239 Accuracy 0.6363333463668823\n",
      "Iteration 10230 Training loss 0.09733113646507263 Validation loss 0.1254112273454666 Accuracy 0.6474999785423279\n",
      "Iteration 10240 Training loss 0.09622334688901901 Validation loss 0.11875154823064804 Accuracy 0.659333348274231\n",
      "Iteration 10250 Training loss 0.09643591195344925 Validation loss 0.1285400688648224 Accuracy 0.640333354473114\n",
      "Iteration 10260 Training loss 0.0966583862900734 Validation loss 0.11413935571908951 Accuracy 0.6703333258628845\n",
      "Iteration 10270 Training loss 0.09604822844266891 Validation loss 0.11223655939102173 Accuracy 0.6729999780654907\n",
      "Iteration 10280 Training loss 0.10313708335161209 Validation loss 0.11329687386751175 Accuracy 0.6723333597183228\n",
      "Iteration 10290 Training loss 0.09767704457044601 Validation loss 0.11137070506811142 Accuracy 0.6711666584014893\n",
      "Iteration 10300 Training loss 0.09802451729774475 Validation loss 0.1114266887307167 Accuracy 0.671999990940094\n",
      "Iteration 10310 Training loss 0.09500531107187271 Validation loss 0.1140608936548233 Accuracy 0.6700000166893005\n",
      "Iteration 10320 Training loss 0.10124142467975616 Validation loss 0.11172693222761154 Accuracy 0.675000011920929\n",
      "Iteration 10330 Training loss 0.0973517969250679 Validation loss 0.11457042396068573 Accuracy 0.6706666946411133\n",
      "Iteration 10340 Training loss 0.10013441741466522 Validation loss 0.1121244728565216 Accuracy 0.6729999780654907\n",
      "Iteration 10350 Training loss 0.09686832875013351 Validation loss 0.11209619045257568 Accuracy 0.6736666560173035\n",
      "Iteration 10360 Training loss 0.09968982636928558 Validation loss 0.11206848919391632 Accuracy 0.6726666688919067\n",
      "Iteration 10370 Training loss 0.09759983420372009 Validation loss 0.11170259863138199 Accuracy 0.6713333129882812\n",
      "Iteration 10380 Training loss 0.09788180887699127 Validation loss 0.11177440732717514 Accuracy 0.6711666584014893\n",
      "Iteration 10390 Training loss 0.10037992149591446 Validation loss 0.11129571497440338 Accuracy 0.6725000143051147\n",
      "Iteration 10400 Training loss 0.09948031604290009 Validation loss 0.11114952713251114 Accuracy 0.6728333234786987\n",
      "Iteration 10410 Training loss 0.09733891487121582 Validation loss 0.11150357872247696 Accuracy 0.6729999780654907\n",
      "Iteration 10420 Training loss 0.09634363651275635 Validation loss 0.11278102546930313 Accuracy 0.6743333339691162\n",
      "Iteration 10430 Training loss 0.0988355278968811 Validation loss 0.11166498064994812 Accuracy 0.6723333597183228\n",
      "Iteration 10440 Training loss 0.0989830493927002 Validation loss 0.11143044382333755 Accuracy 0.6733333468437195\n",
      "Iteration 10450 Training loss 0.09835547208786011 Validation loss 0.11202795058488846 Accuracy 0.6744999885559082\n",
      "Iteration 10460 Training loss 0.10189583152532578 Validation loss 0.11126647144556046 Accuracy 0.6733333468437195\n",
      "Iteration 10470 Training loss 0.09873862564563751 Validation loss 0.11216842383146286 Accuracy 0.6728333234786987\n",
      "Iteration 10480 Training loss 0.09986922144889832 Validation loss 0.11127408593893051 Accuracy 0.6733333468437195\n",
      "Iteration 10490 Training loss 0.09805142879486084 Validation loss 0.11176414042711258 Accuracy 0.6744999885559082\n",
      "Iteration 10500 Training loss 0.09711604565382004 Validation loss 0.11425508558750153 Accuracy 0.6703333258628845\n",
      "Iteration 10510 Training loss 0.10277488082647324 Validation loss 0.11370894312858582 Accuracy 0.6728333234786987\n",
      "Iteration 10520 Training loss 0.09794794023036957 Validation loss 0.11148447543382645 Accuracy 0.6743333339691162\n",
      "Iteration 10530 Training loss 0.0972236916422844 Validation loss 0.11149980872869492 Accuracy 0.6729999780654907\n",
      "Iteration 10540 Training loss 0.09885992109775543 Validation loss 0.11126751452684402 Accuracy 0.6736666560173035\n",
      "Iteration 10550 Training loss 0.09613142907619476 Validation loss 0.1116405501961708 Accuracy 0.6735000014305115\n",
      "Iteration 10560 Training loss 0.09526274353265762 Validation loss 0.11279236525297165 Accuracy 0.6726666688919067\n",
      "Iteration 10570 Training loss 0.09571411460638046 Validation loss 0.11695263534784317 Accuracy 0.6651666760444641\n",
      "Iteration 10580 Training loss 0.10190461575984955 Validation loss 0.14247910678386688 Accuracy 0.612333357334137\n",
      "Iteration 10590 Training loss 0.09818204492330551 Validation loss 0.12894092500209808 Accuracy 0.6426666378974915\n",
      "Iteration 10600 Training loss 0.09878632426261902 Validation loss 0.1354730874300003 Accuracy 0.6288333535194397\n",
      "Iteration 10610 Training loss 0.09661584347486496 Validation loss 0.12376489490270615 Accuracy 0.6516666412353516\n",
      "Iteration 10620 Training loss 0.09624388813972473 Validation loss 0.12815499305725098 Accuracy 0.6446666717529297\n",
      "Iteration 10630 Training loss 0.09910430014133453 Validation loss 0.1293857991695404 Accuracy 0.6418333053588867\n",
      "Iteration 10640 Training loss 0.09833526611328125 Validation loss 0.12879544496536255 Accuracy 0.6426666378974915\n",
      "Iteration 10650 Training loss 0.09466560184955597 Validation loss 0.12248911708593369 Accuracy 0.6551666855812073\n",
      "Iteration 10660 Training loss 0.09904795140028 Validation loss 0.1291724592447281 Accuracy 0.64083331823349\n",
      "Iteration 10670 Training loss 0.09688624739646912 Validation loss 0.12569448351860046 Accuracy 0.6493333578109741\n",
      "Iteration 10680 Training loss 0.0963490828871727 Validation loss 0.12626293301582336 Accuracy 0.6458333134651184\n",
      "Iteration 10690 Training loss 0.09771191328763962 Validation loss 0.1279563158750534 Accuracy 0.643666684627533\n",
      "Iteration 10700 Training loss 0.09847158938646317 Validation loss 0.12456481903791428 Accuracy 0.6518333554267883\n",
      "Iteration 10710 Training loss 0.10045064985752106 Validation loss 0.13174478709697723 Accuracy 0.6383333206176758\n",
      "Iteration 10720 Training loss 0.09814884513616562 Validation loss 0.12961061298847198 Accuracy 0.6393333077430725\n",
      "Iteration 10730 Training loss 0.09620867669582367 Validation loss 0.12284647673368454 Accuracy 0.6536666750907898\n",
      "Iteration 10740 Training loss 0.09663619101047516 Validation loss 0.1202695295214653 Accuracy 0.6588333249092102\n",
      "Iteration 10750 Training loss 0.0992819294333458 Validation loss 0.1312752366065979 Accuracy 0.637333333492279\n",
      "Iteration 10760 Training loss 0.09753136336803436 Validation loss 0.12386564165353775 Accuracy 0.6526666879653931\n",
      "Iteration 10770 Training loss 0.0969688892364502 Validation loss 0.12682169675827026 Accuracy 0.6473333239555359\n",
      "Iteration 10780 Training loss 0.09434139728546143 Validation loss 0.1248607262969017 Accuracy 0.6501666903495789\n",
      "Iteration 10790 Training loss 0.09705670177936554 Validation loss 0.12695935368537903 Accuracy 0.6455000042915344\n",
      "Iteration 10800 Training loss 0.09864269196987152 Validation loss 0.1324414312839508 Accuracy 0.6333333253860474\n",
      "Iteration 10810 Training loss 0.09624839574098587 Validation loss 0.12542277574539185 Accuracy 0.6493333578109741\n",
      "Iteration 10820 Training loss 0.09862392395734787 Validation loss 0.13198818266391754 Accuracy 0.6380000114440918\n",
      "Iteration 10830 Training loss 0.09584043174982071 Validation loss 0.12823382019996643 Accuracy 0.6443333625793457\n",
      "Iteration 10840 Training loss 0.10140372067689896 Validation loss 0.13626587390899658 Accuracy 0.6274999976158142\n",
      "Iteration 10850 Training loss 0.09696836024522781 Validation loss 0.12161631137132645 Accuracy 0.656000018119812\n",
      "Iteration 10860 Training loss 0.09479377418756485 Validation loss 0.12259840220212936 Accuracy 0.6545000076293945\n",
      "Iteration 10870 Training loss 0.0979301929473877 Validation loss 0.12693935632705688 Accuracy 0.6466666460037231\n",
      "Iteration 10880 Training loss 0.09875604510307312 Validation loss 0.12942230701446533 Accuracy 0.6430000066757202\n",
      "Iteration 10890 Training loss 0.0981021597981453 Validation loss 0.12630169093608856 Accuracy 0.6480000019073486\n",
      "Iteration 10900 Training loss 0.09720984101295471 Validation loss 0.1298159658908844 Accuracy 0.6399999856948853\n",
      "Iteration 10910 Training loss 0.09628208726644516 Validation loss 0.12524382770061493 Accuracy 0.6491666436195374\n",
      "Iteration 10920 Training loss 0.09903788566589355 Validation loss 0.12632869184017181 Accuracy 0.6480000019073486\n",
      "Iteration 10930 Training loss 0.09741370379924774 Validation loss 0.12369398772716522 Accuracy 0.6526666879653931\n",
      "Iteration 10940 Training loss 0.09803692996501923 Validation loss 0.12526650726795197 Accuracy 0.6480000019073486\n",
      "Iteration 10950 Training loss 0.0983009934425354 Validation loss 0.13126015663146973 Accuracy 0.6383333206176758\n",
      "Iteration 10960 Training loss 0.0961265042424202 Validation loss 0.12003370374441147 Accuracy 0.6598333120346069\n",
      "Iteration 10970 Training loss 0.0947568491101265 Validation loss 0.12072241306304932 Accuracy 0.6586666703224182\n",
      "Iteration 10980 Training loss 0.09941933304071426 Validation loss 0.12863387167453766 Accuracy 0.6431666612625122\n",
      "Iteration 10990 Training loss 0.09453777968883514 Validation loss 0.1237751692533493 Accuracy 0.6536666750907898\n",
      "Iteration 11000 Training loss 0.10003215819597244 Validation loss 0.1331959217786789 Accuracy 0.6324999928474426\n",
      "Iteration 11010 Training loss 0.09541476517915726 Validation loss 0.13091959059238434 Accuracy 0.6401666402816772\n",
      "Iteration 11020 Training loss 0.09803342819213867 Validation loss 0.1300402283668518 Accuracy 0.6418333053588867\n",
      "Iteration 11030 Training loss 0.0977444052696228 Validation loss 0.12456407397985458 Accuracy 0.6513333320617676\n",
      "Iteration 11040 Training loss 0.10030225664377213 Validation loss 0.1353522390127182 Accuracy 0.6265000104904175\n",
      "Iteration 11050 Training loss 0.09743864089250565 Validation loss 0.12196137756109238 Accuracy 0.6558333039283752\n",
      "Iteration 11060 Training loss 0.10122445970773697 Validation loss 0.13862355053424835 Accuracy 0.621999979019165\n",
      "Iteration 11070 Training loss 0.09755226969718933 Validation loss 0.12288729846477509 Accuracy 0.6541666388511658\n",
      "Iteration 11080 Training loss 0.09627082198858261 Validation loss 0.12832264602184296 Accuracy 0.6443333625793457\n",
      "Iteration 11090 Training loss 0.09662117809057236 Validation loss 0.12550368905067444 Accuracy 0.6493333578109741\n",
      "Iteration 11100 Training loss 0.09828102588653564 Validation loss 0.12312697619199753 Accuracy 0.6545000076293945\n",
      "Iteration 11110 Training loss 0.09908963739871979 Validation loss 0.13310059905052185 Accuracy 0.6321666836738586\n",
      "Iteration 11120 Training loss 0.09981308877468109 Validation loss 0.1327177882194519 Accuracy 0.6330000162124634\n",
      "Iteration 11130 Training loss 0.09899136424064636 Validation loss 0.12693586945533752 Accuracy 0.6476666927337646\n",
      "Iteration 11140 Training loss 0.0970141813158989 Validation loss 0.13181357085704803 Accuracy 0.6340000033378601\n",
      "Iteration 11150 Training loss 0.09724491089582443 Validation loss 0.12907454371452332 Accuracy 0.6421666741371155\n",
      "Iteration 11160 Training loss 0.09580773115158081 Validation loss 0.1226155012845993 Accuracy 0.6539999842643738\n",
      "Iteration 11170 Training loss 0.09862165153026581 Validation loss 0.12751023471355438 Accuracy 0.6445000171661377\n",
      "Iteration 11180 Training loss 0.09679391235113144 Validation loss 0.12582916021347046 Accuracy 0.6483333110809326\n",
      "Iteration 11190 Training loss 0.09553661197423935 Validation loss 0.11994042247533798 Accuracy 0.6603333353996277\n",
      "Iteration 11200 Training loss 0.09486757218837738 Validation loss 0.1212066039443016 Accuracy 0.6558333039283752\n",
      "Iteration 11210 Training loss 0.10009899735450745 Validation loss 0.1390519142150879 Accuracy 0.6198333501815796\n",
      "Iteration 11220 Training loss 0.09921099990606308 Validation loss 0.13414756953716278 Accuracy 0.6299999952316284\n",
      "Iteration 11230 Training loss 0.09654709696769714 Validation loss 0.12551429867744446 Accuracy 0.6489999890327454\n",
      "Iteration 11240 Training loss 0.09901481121778488 Validation loss 0.130231112241745 Accuracy 0.6396666765213013\n",
      "Iteration 11250 Training loss 0.09930425882339478 Validation loss 0.13618089258670807 Accuracy 0.6265000104904175\n",
      "Iteration 11260 Training loss 0.0969691127538681 Validation loss 0.12566713988780975 Accuracy 0.6501666903495789\n",
      "Iteration 11270 Training loss 0.09665372967720032 Validation loss 0.13046541810035706 Accuracy 0.6416666507720947\n",
      "Iteration 11280 Training loss 0.09824129194021225 Validation loss 0.1257379800081253 Accuracy 0.6486666798591614\n",
      "Iteration 11290 Training loss 0.09932266920804977 Validation loss 0.1332278996706009 Accuracy 0.6320000290870667\n",
      "Iteration 11300 Training loss 0.09798640757799149 Validation loss 0.12420506030321121 Accuracy 0.6526666879653931\n",
      "Iteration 11310 Training loss 0.09548439830541611 Validation loss 0.12465482950210571 Accuracy 0.6498333215713501\n",
      "Iteration 11320 Training loss 0.0965733453631401 Validation loss 0.12663677334785461 Accuracy 0.6468333601951599\n",
      "Iteration 11330 Training loss 0.1012662947177887 Validation loss 0.13761702179908752 Accuracy 0.6236666440963745\n",
      "Iteration 11340 Training loss 0.09539209306240082 Validation loss 0.12699829041957855 Accuracy 0.6468333601951599\n",
      "Iteration 11350 Training loss 0.0978015661239624 Validation loss 0.1324552744626999 Accuracy 0.6353333592414856\n",
      "Iteration 11360 Training loss 0.09857720136642456 Validation loss 0.12950068712234497 Accuracy 0.6413333415985107\n",
      "Iteration 11370 Training loss 0.10035227239131927 Validation loss 0.13454225659370422 Accuracy 0.628000020980835\n",
      "Iteration 11380 Training loss 0.09512773901224136 Validation loss 0.11825814098119736 Accuracy 0.6613333225250244\n",
      "Iteration 11390 Training loss 0.09535647928714752 Validation loss 0.11966871470212936 Accuracy 0.659333348274231\n",
      "Iteration 11400 Training loss 0.09384074062108994 Validation loss 0.11237919330596924 Accuracy 0.6743333339691162\n",
      "Iteration 11410 Training loss 0.10178707540035248 Validation loss 0.11263197660446167 Accuracy 0.6753333210945129\n",
      "Iteration 11420 Training loss 0.09645485132932663 Validation loss 0.11149223148822784 Accuracy 0.6736666560173035\n",
      "Iteration 11430 Training loss 0.09852909296751022 Validation loss 0.11159047484397888 Accuracy 0.6741666793823242\n",
      "Iteration 11440 Training loss 0.0953124612569809 Validation loss 0.11598841100931168 Accuracy 0.6691666841506958\n",
      "Iteration 11450 Training loss 0.09722953289747238 Validation loss 0.11156349629163742 Accuracy 0.6746666431427002\n",
      "Iteration 11460 Training loss 0.09691110253334045 Validation loss 0.11180856078863144 Accuracy 0.6744999885559082\n",
      "Iteration 11470 Training loss 0.09880959242582321 Validation loss 0.1116943359375 Accuracy 0.6731666922569275\n",
      "Iteration 11480 Training loss 0.09563855826854706 Validation loss 0.1117178350687027 Accuracy 0.675000011920929\n",
      "Iteration 11490 Training loss 0.09492895752191544 Validation loss 0.11477210372686386 Accuracy 0.6713333129882812\n",
      "Iteration 11500 Training loss 0.09639029204845428 Validation loss 0.112872414290905 Accuracy 0.6736666560173035\n",
      "Iteration 11510 Training loss 0.09798608720302582 Validation loss 0.1116858422756195 Accuracy 0.6740000247955322\n",
      "Iteration 11520 Training loss 0.09661360830068588 Validation loss 0.11325393617153168 Accuracy 0.6746666431427002\n",
      "Iteration 11530 Training loss 0.09599994122982025 Validation loss 0.11366339772939682 Accuracy 0.6736666560173035\n",
      "Iteration 11540 Training loss 0.0981115847826004 Validation loss 0.11136776953935623 Accuracy 0.6744999885559082\n",
      "Iteration 11550 Training loss 0.09707430005073547 Validation loss 0.11197242140769958 Accuracy 0.675000011920929\n",
      "Iteration 11560 Training loss 0.10145540535449982 Validation loss 0.11144125461578369 Accuracy 0.6731666922569275\n",
      "Iteration 11570 Training loss 0.09657389670610428 Validation loss 0.11286338418722153 Accuracy 0.6729999780654907\n",
      "Iteration 11580 Training loss 0.09616485238075256 Validation loss 0.1114436462521553 Accuracy 0.6768333315849304\n",
      "Iteration 11590 Training loss 0.09981732070446014 Validation loss 0.11087286472320557 Accuracy 0.6783333420753479\n",
      "Iteration 11600 Training loss 0.09750073403120041 Validation loss 0.11107245087623596 Accuracy 0.6756666898727417\n",
      "Iteration 11610 Training loss 0.09661120921373367 Validation loss 0.11176279932260513 Accuracy 0.675000011920929\n",
      "Iteration 11620 Training loss 0.09742181748151779 Validation loss 0.12475091218948364 Accuracy 0.6523333191871643\n",
      "Iteration 11630 Training loss 0.09892609715461731 Validation loss 0.13527318835258484 Accuracy 0.6276666522026062\n",
      "Iteration 11640 Training loss 0.09740740060806274 Validation loss 0.12483817338943481 Accuracy 0.6495000123977661\n",
      "Iteration 11650 Training loss 0.0971735417842865 Validation loss 0.12918825447559357 Accuracy 0.6411666870117188\n",
      "Iteration 11660 Training loss 0.0986597016453743 Validation loss 0.1330694854259491 Accuracy 0.6328333616256714\n",
      "Iteration 11670 Training loss 0.10071904212236404 Validation loss 0.1400519609451294 Accuracy 0.6176666617393494\n",
      "Iteration 11680 Training loss 0.09771101921796799 Validation loss 0.13119669258594513 Accuracy 0.6380000114440918\n",
      "Iteration 11690 Training loss 0.0948786735534668 Validation loss 0.11843980103731155 Accuracy 0.6625000238418579\n",
      "Iteration 11700 Training loss 0.09475237876176834 Validation loss 0.11981526762247086 Accuracy 0.6585000157356262\n",
      "Iteration 11710 Training loss 0.10245737433433533 Validation loss 0.13962630927562714 Accuracy 0.6215000152587891\n",
      "Iteration 11720 Training loss 0.09952764958143234 Validation loss 0.13453349471092224 Accuracy 0.6301666498184204\n",
      "Iteration 11730 Training loss 0.09652984887361526 Validation loss 0.12390553951263428 Accuracy 0.6521666646003723\n",
      "Iteration 11740 Training loss 0.0987284928560257 Validation loss 0.13064563274383545 Accuracy 0.6391666531562805\n",
      "Iteration 11750 Training loss 0.09723250567913055 Validation loss 0.1335313469171524 Accuracy 0.6321666836738586\n",
      "Iteration 11760 Training loss 0.09405514597892761 Validation loss 0.1227746307849884 Accuracy 0.6553333401679993\n",
      "Iteration 11770 Training loss 0.0992923378944397 Validation loss 0.13294892013072968 Accuracy 0.6346666812896729\n",
      "Iteration 11780 Training loss 0.09640521556138992 Validation loss 0.12769199907779694 Accuracy 0.6471666693687439\n",
      "Iteration 11790 Training loss 0.095139279961586 Validation loss 0.1308116465806961 Accuracy 0.6389999985694885\n",
      "Iteration 11800 Training loss 0.0982910543680191 Validation loss 0.12468050420284271 Accuracy 0.6511666774749756\n",
      "Iteration 11810 Training loss 0.09692733734846115 Validation loss 0.1292404979467392 Accuracy 0.6431666612625122\n",
      "Iteration 11820 Training loss 0.097470723092556 Validation loss 0.126466304063797 Accuracy 0.6466666460037231\n",
      "Iteration 11830 Training loss 0.09948713332414627 Validation loss 0.1291135996580124 Accuracy 0.6455000042915344\n",
      "Iteration 11840 Training loss 0.09817405045032501 Validation loss 0.13572074472904205 Accuracy 0.6274999976158142\n",
      "Iteration 11850 Training loss 0.09619947522878647 Validation loss 0.12633287906646729 Accuracy 0.6476666927337646\n",
      "Iteration 11860 Training loss 0.1000949814915657 Validation loss 0.13505905866622925 Accuracy 0.6288333535194397\n",
      "Iteration 11870 Training loss 0.09544472396373749 Validation loss 0.1195187047123909 Accuracy 0.6600000262260437\n",
      "Iteration 11880 Training loss 0.09472452104091644 Validation loss 0.11317127197980881 Accuracy 0.6729999780654907\n",
      "Iteration 11890 Training loss 0.09655090421438217 Validation loss 0.11231440305709839 Accuracy 0.675166666507721\n",
      "Iteration 11900 Training loss 0.10237029939889908 Validation loss 0.11300879716873169 Accuracy 0.6754999756813049\n",
      "Iteration 11910 Training loss 0.09816557168960571 Validation loss 0.11158870160579681 Accuracy 0.6754999756813049\n",
      "Iteration 11920 Training loss 0.09724540263414383 Validation loss 0.11192140728235245 Accuracy 0.6771666407585144\n",
      "Iteration 11930 Training loss 0.09973646700382233 Validation loss 0.11236366629600525 Accuracy 0.6763333082199097\n",
      "Iteration 11940 Training loss 0.0968344658613205 Validation loss 0.11193892359733582 Accuracy 0.6756666898727417\n",
      "Iteration 11950 Training loss 0.0961000993847847 Validation loss 0.11212300509214401 Accuracy 0.675166666507721\n",
      "Iteration 11960 Training loss 0.09810050576925278 Validation loss 0.11128368228673935 Accuracy 0.6763333082199097\n",
      "Iteration 11970 Training loss 0.09503301233053207 Validation loss 0.11204677075147629 Accuracy 0.6754999756813049\n",
      "Iteration 11980 Training loss 0.09634441882371902 Validation loss 0.1119566336274147 Accuracy 0.6743333339691162\n",
      "Iteration 11990 Training loss 0.09692954272031784 Validation loss 0.11108306050300598 Accuracy 0.6741666793823242\n",
      "Iteration 12000 Training loss 0.10215122252702713 Validation loss 0.11134800314903259 Accuracy 0.675000011920929\n",
      "Iteration 12010 Training loss 0.09657702594995499 Validation loss 0.11125063896179199 Accuracy 0.675166666507721\n",
      "Iteration 12020 Training loss 0.09363692998886108 Validation loss 0.11498421430587769 Accuracy 0.67166668176651\n",
      "Iteration 12030 Training loss 0.09621307253837585 Validation loss 0.11165773868560791 Accuracy 0.6753333210945129\n",
      "Iteration 12040 Training loss 0.09556955844163895 Validation loss 0.11162819713354111 Accuracy 0.6741666793823242\n",
      "Iteration 12050 Training loss 0.09593464434146881 Validation loss 0.11154164373874664 Accuracy 0.6763333082199097\n",
      "Iteration 12060 Training loss 0.09809248149394989 Validation loss 0.1116296797990799 Accuracy 0.6754999756813049\n",
      "Iteration 12070 Training loss 0.0967148020863533 Validation loss 0.11187530308961868 Accuracy 0.6753333210945129\n",
      "Iteration 12080 Training loss 0.09970410913228989 Validation loss 0.11210991442203522 Accuracy 0.6768333315849304\n",
      "Iteration 12090 Training loss 0.09541298449039459 Validation loss 0.11185294389724731 Accuracy 0.6765000224113464\n",
      "Iteration 12100 Training loss 0.09884273260831833 Validation loss 0.11138908565044403 Accuracy 0.675000011920929\n",
      "Iteration 12110 Training loss 0.09766807407140732 Validation loss 0.11142224073410034 Accuracy 0.6731666922569275\n",
      "Iteration 12120 Training loss 0.09816685318946838 Validation loss 0.11150040477514267 Accuracy 0.6754999756813049\n",
      "Iteration 12130 Training loss 0.09580891579389572 Validation loss 0.11180049926042557 Accuracy 0.6766666769981384\n",
      "Iteration 12140 Training loss 0.0952923372387886 Validation loss 0.11674348264932632 Accuracy 0.6679999828338623\n",
      "Iteration 12150 Training loss 0.09722632169723511 Validation loss 0.11184272915124893 Accuracy 0.6768333315849304\n",
      "Iteration 12160 Training loss 0.09604174643754959 Validation loss 0.11151254177093506 Accuracy 0.6738333106040955\n",
      "Iteration 12170 Training loss 0.0964185819029808 Validation loss 0.11166448891162872 Accuracy 0.6761666536331177\n",
      "Iteration 12180 Training loss 0.09626118838787079 Validation loss 0.11170644313097 Accuracy 0.6756666898727417\n",
      "Iteration 12190 Training loss 0.09786318242549896 Validation loss 0.11161472648382187 Accuracy 0.6761666536331177\n",
      "Iteration 12200 Training loss 0.09455843269824982 Validation loss 0.11593814939260483 Accuracy 0.6700000166893005\n",
      "Iteration 12210 Training loss 0.09671535342931747 Validation loss 0.11169492453336716 Accuracy 0.6768333315849304\n",
      "Iteration 12220 Training loss 0.09782907366752625 Validation loss 0.1116059347987175 Accuracy 0.6754999756813049\n",
      "Iteration 12230 Training loss 0.09335511922836304 Validation loss 0.11406432837247849 Accuracy 0.6738333106040955\n",
      "Iteration 12240 Training loss 0.09624721109867096 Validation loss 0.11541026085615158 Accuracy 0.672166645526886\n",
      "Iteration 12250 Training loss 0.09713997691869736 Validation loss 0.11171775311231613 Accuracy 0.674833357334137\n",
      "Iteration 12260 Training loss 0.0957166776061058 Validation loss 0.1118706688284874 Accuracy 0.6766666769981384\n",
      "Iteration 12270 Training loss 0.09582642465829849 Validation loss 0.11475513875484467 Accuracy 0.6736666560173035\n",
      "Iteration 12280 Training loss 0.09671062231063843 Validation loss 0.11175058037042618 Accuracy 0.6766666769981384\n",
      "Iteration 12290 Training loss 0.09734183549880981 Validation loss 0.11095047742128372 Accuracy 0.675000011920929\n",
      "Iteration 12300 Training loss 0.09528204053640366 Validation loss 0.11535422503948212 Accuracy 0.6708333492279053\n",
      "Iteration 12310 Training loss 0.09521972388029099 Validation loss 0.11638637632131577 Accuracy 0.6701666712760925\n",
      "Iteration 12320 Training loss 0.09563014656305313 Validation loss 0.11563580483198166 Accuracy 0.6743333339691162\n",
      "Iteration 12330 Training loss 0.09346848726272583 Validation loss 0.1202804446220398 Accuracy 0.6601666808128357\n",
      "Iteration 12340 Training loss 0.09844021499156952 Validation loss 0.13647812604904175 Accuracy 0.6284999847412109\n",
      "Iteration 12350 Training loss 0.09694354981184006 Validation loss 0.13016845285892487 Accuracy 0.6430000066757202\n",
      "Iteration 12360 Training loss 0.0982464998960495 Validation loss 0.13482367992401123 Accuracy 0.6303333044052124\n",
      "Iteration 12370 Training loss 0.09659326076507568 Validation loss 0.12910214066505432 Accuracy 0.6456666588783264\n",
      "Iteration 12380 Training loss 0.09652888774871826 Validation loss 0.13340207934379578 Accuracy 0.6341666579246521\n",
      "Iteration 12390 Training loss 0.09522762894630432 Validation loss 0.1276683509349823 Accuracy 0.6473333239555359\n",
      "Iteration 12400 Training loss 0.09550663083791733 Validation loss 0.12452791631221771 Accuracy 0.6498333215713501\n",
      "Iteration 12410 Training loss 0.09432186931371689 Validation loss 0.11541882157325745 Accuracy 0.6736666560173035\n",
      "Iteration 12420 Training loss 0.0954679474234581 Validation loss 0.11694113165140152 Accuracy 0.6668333411216736\n",
      "Iteration 12430 Training loss 0.10280957072973251 Validation loss 0.11363066732883453 Accuracy 0.6741666793823242\n",
      "Iteration 12440 Training loss 0.09601637721061707 Validation loss 0.1113395169377327 Accuracy 0.6763333082199097\n",
      "Iteration 12450 Training loss 0.10191317647695541 Validation loss 0.11268340796232224 Accuracy 0.6746666431427002\n",
      "Iteration 12460 Training loss 0.09588226675987244 Validation loss 0.11383000761270523 Accuracy 0.6740000247955322\n",
      "Iteration 12470 Training loss 0.0963839739561081 Validation loss 0.11146431416273117 Accuracy 0.6738333106040955\n",
      "Iteration 12480 Training loss 0.09877024590969086 Validation loss 0.11134236305952072 Accuracy 0.6763333082199097\n",
      "Iteration 12490 Training loss 0.09677755832672119 Validation loss 0.1114778220653534 Accuracy 0.6771666407585144\n",
      "Iteration 12500 Training loss 0.09898404777050018 Validation loss 0.111801378428936 Accuracy 0.6769999861717224\n",
      "Iteration 12510 Training loss 0.09789662808179855 Validation loss 0.11114898324012756 Accuracy 0.6761666536331177\n",
      "Iteration 12520 Training loss 0.09355977177619934 Validation loss 0.11427873373031616 Accuracy 0.6741666793823242\n",
      "Iteration 12530 Training loss 0.09909101575613022 Validation loss 0.11158538609743118 Accuracy 0.6776666641235352\n",
      "Iteration 12540 Training loss 0.09705403447151184 Validation loss 0.11127001792192459 Accuracy 0.6769999861717224\n",
      "Iteration 12550 Training loss 0.09356670081615448 Validation loss 0.11222560703754425 Accuracy 0.6771666407585144\n",
      "Iteration 12560 Training loss 0.09481409937143326 Validation loss 0.11372213810682297 Accuracy 0.6726666688919067\n",
      "Iteration 12570 Training loss 0.09463322162628174 Validation loss 0.12349601089954376 Accuracy 0.652999997138977\n",
      "Iteration 12580 Training loss 0.09478873759508133 Validation loss 0.1275748461484909 Accuracy 0.6449999809265137\n",
      "Iteration 12590 Training loss 0.09434814751148224 Validation loss 0.12569071352481842 Accuracy 0.6498333215713501\n",
      "Iteration 12600 Training loss 0.09611248970031738 Validation loss 0.12847284972667694 Accuracy 0.6448333263397217\n",
      "Iteration 12610 Training loss 0.09602577984333038 Validation loss 0.12458372116088867 Accuracy 0.6521666646003723\n",
      "Iteration 12620 Training loss 0.09790089726448059 Validation loss 0.1316276639699936 Accuracy 0.6386666893959045\n",
      "Iteration 12630 Training loss 0.09870582073926926 Validation loss 0.13166891038417816 Accuracy 0.6393333077430725\n",
      "Iteration 12640 Training loss 0.09867407381534576 Validation loss 0.13090704381465912 Accuracy 0.6401666402816772\n",
      "Iteration 12650 Training loss 0.09771236032247543 Validation loss 0.13118109107017517 Accuracy 0.6389999985694885\n",
      "Iteration 12660 Training loss 0.09746880829334259 Validation loss 0.12267376482486725 Accuracy 0.6528333425521851\n",
      "Iteration 12670 Training loss 0.09399257600307465 Validation loss 0.11967366188764572 Accuracy 0.659166693687439\n",
      "Iteration 12680 Training loss 0.10139674693346024 Validation loss 0.11257802695035934 Accuracy 0.6769999861717224\n",
      "Iteration 12690 Training loss 0.09864534437656403 Validation loss 0.1113671064376831 Accuracy 0.6775000095367432\n",
      "Iteration 12700 Training loss 0.09506136178970337 Validation loss 0.11369746923446655 Accuracy 0.6736666560173035\n",
      "Iteration 12710 Training loss 0.096888966858387 Validation loss 0.11134538054466248 Accuracy 0.6775000095367432\n",
      "Iteration 12720 Training loss 0.09449384361505508 Validation loss 0.11233626306056976 Accuracy 0.6769999861717224\n",
      "Iteration 12730 Training loss 0.09904476255178452 Validation loss 0.11148837953805923 Accuracy 0.6771666407585144\n",
      "Iteration 12740 Training loss 0.09482356905937195 Validation loss 0.11242493987083435 Accuracy 0.6758333444595337\n",
      "Iteration 12750 Training loss 0.09525563567876816 Validation loss 0.111673504114151 Accuracy 0.6759999990463257\n",
      "Iteration 12760 Training loss 0.09695281088352203 Validation loss 0.11204510182142258 Accuracy 0.6741666793823242\n",
      "Iteration 12770 Training loss 0.09400065988302231 Validation loss 0.11150693893432617 Accuracy 0.6766666769981384\n",
      "Iteration 12780 Training loss 0.09686946868896484 Validation loss 0.11157404631376266 Accuracy 0.6758333444595337\n",
      "Iteration 12790 Training loss 0.09652481228113174 Validation loss 0.11151039600372314 Accuracy 0.674833357334137\n",
      "Iteration 12800 Training loss 0.0966317355632782 Validation loss 0.11129789054393768 Accuracy 0.6761666536331177\n",
      "Iteration 12810 Training loss 0.09531138837337494 Validation loss 0.11189815402030945 Accuracy 0.6765000224113464\n",
      "Iteration 12820 Training loss 0.09888327866792679 Validation loss 0.11218475550413132 Accuracy 0.6753333210945129\n",
      "Iteration 12830 Training loss 0.09748083353042603 Validation loss 0.11136545985937119 Accuracy 0.6769999861717224\n",
      "Iteration 12840 Training loss 0.09466973692178726 Validation loss 0.11159829795360565 Accuracy 0.6768333315849304\n",
      "Iteration 12850 Training loss 0.09895814955234528 Validation loss 0.11147803068161011 Accuracy 0.6766666769981384\n",
      "Iteration 12860 Training loss 0.09812646359205246 Validation loss 0.11145589500665665 Accuracy 0.6753333210945129\n",
      "Iteration 12870 Training loss 0.09762824326753616 Validation loss 0.11144474148750305 Accuracy 0.6754999756813049\n",
      "Iteration 12880 Training loss 0.09416491538286209 Validation loss 0.11232148110866547 Accuracy 0.6766666769981384\n",
      "Iteration 12890 Training loss 0.09512265771627426 Validation loss 0.11173737794160843 Accuracy 0.6754999756813049\n",
      "Iteration 12900 Training loss 0.09521856904029846 Validation loss 0.11140371859073639 Accuracy 0.6758333444595337\n",
      "Iteration 12910 Training loss 0.09691282361745834 Validation loss 0.11180171370506287 Accuracy 0.6773333549499512\n",
      "Iteration 12920 Training loss 0.09567932039499283 Validation loss 0.11181504279375076 Accuracy 0.6768333315849304\n",
      "Iteration 12930 Training loss 0.093683622777462 Validation loss 0.11593961715698242 Accuracy 0.6703333258628845\n",
      "Iteration 12940 Training loss 0.09797821193933487 Validation loss 0.13083983957767487 Accuracy 0.6424999833106995\n",
      "Iteration 12950 Training loss 0.0971863642334938 Validation loss 0.13481540977954865 Accuracy 0.6298333406448364\n",
      "Iteration 12960 Training loss 0.09609784185886383 Validation loss 0.12854258716106415 Accuracy 0.6470000147819519\n",
      "Iteration 12970 Training loss 0.09712184965610504 Validation loss 0.13216137886047363 Accuracy 0.6351666450500488\n",
      "Iteration 12980 Training loss 0.09574076533317566 Validation loss 0.12683527171611786 Accuracy 0.6496666669845581\n",
      "Iteration 12990 Training loss 0.09538797289133072 Validation loss 0.12679150700569153 Accuracy 0.6503333449363708\n",
      "Iteration 13000 Training loss 0.09925313293933868 Validation loss 0.13007481396198273 Accuracy 0.6428333520889282\n",
      "Iteration 13010 Training loss 0.0953594371676445 Validation loss 0.12621037662029266 Accuracy 0.6498333215713501\n",
      "Iteration 13020 Training loss 0.09582023322582245 Validation loss 0.12532246112823486 Accuracy 0.6513333320617676\n",
      "Iteration 13030 Training loss 0.09194236248731613 Validation loss 0.1211467981338501 Accuracy 0.6596666574478149\n",
      "Iteration 13040 Training loss 0.09673013538122177 Validation loss 0.12944266200065613 Accuracy 0.6448333263397217\n",
      "Iteration 13050 Training loss 0.09458380937576294 Validation loss 0.12137383222579956 Accuracy 0.659166693687439\n",
      "Iteration 13060 Training loss 0.09482231736183167 Validation loss 0.11635519564151764 Accuracy 0.6711666584014893\n",
      "Iteration 13070 Training loss 0.09642430394887924 Validation loss 0.11178486049175262 Accuracy 0.6771666407585144\n",
      "Iteration 13080 Training loss 0.10082244873046875 Validation loss 0.1131332665681839 Accuracy 0.6746666431427002\n",
      "Iteration 13090 Training loss 0.09899265319108963 Validation loss 0.11158733814954758 Accuracy 0.6776666641235352\n",
      "Iteration 13100 Training loss 0.09574814140796661 Validation loss 0.11136972904205322 Accuracy 0.6768333315849304\n",
      "Iteration 13110 Training loss 0.0963970348238945 Validation loss 0.1111825555562973 Accuracy 0.6756666898727417\n",
      "Iteration 13120 Training loss 0.09699708968400955 Validation loss 0.11195634305477142 Accuracy 0.6773333549499512\n",
      "Iteration 13130 Training loss 0.09493115544319153 Validation loss 0.11212188005447388 Accuracy 0.6769999861717224\n",
      "Iteration 13140 Training loss 0.09993552416563034 Validation loss 0.11265527456998825 Accuracy 0.6776666641235352\n",
      "Iteration 13150 Training loss 0.09558163583278656 Validation loss 0.11211197823286057 Accuracy 0.6765000224113464\n",
      "Iteration 13160 Training loss 0.10155554115772247 Validation loss 0.1127685159444809 Accuracy 0.6761666536331177\n",
      "Iteration 13170 Training loss 0.09736280143260956 Validation loss 0.11237721890211105 Accuracy 0.6768333315849304\n",
      "Iteration 13180 Training loss 0.09662231802940369 Validation loss 0.11122064292430878 Accuracy 0.6771666407585144\n",
      "Iteration 13190 Training loss 0.09663527458906174 Validation loss 0.1114988625049591 Accuracy 0.6765000224113464\n",
      "Iteration 13200 Training loss 0.09753414243459702 Validation loss 0.11133333295583725 Accuracy 0.6775000095367432\n",
      "Iteration 13210 Training loss 0.09367717802524567 Validation loss 0.11268320679664612 Accuracy 0.6756666898727417\n",
      "Iteration 13220 Training loss 0.09404748678207397 Validation loss 0.11365453898906708 Accuracy 0.6743333339691162\n",
      "Iteration 13230 Training loss 0.09546975791454315 Validation loss 0.11222922056913376 Accuracy 0.6736666560173035\n",
      "Iteration 13240 Training loss 0.09557286649942398 Validation loss 0.11153433471918106 Accuracy 0.6769999861717224\n",
      "Iteration 13250 Training loss 0.09616176038980484 Validation loss 0.11172126233577728 Accuracy 0.6765000224113464\n",
      "Iteration 13260 Training loss 0.09469467401504517 Validation loss 0.11208246648311615 Accuracy 0.6765000224113464\n",
      "Iteration 13270 Training loss 0.09624606370925903 Validation loss 0.11140213906764984 Accuracy 0.6768333315849304\n",
      "Iteration 13280 Training loss 0.09399943798780441 Validation loss 0.11483477801084518 Accuracy 0.6746666431427002\n",
      "Iteration 13290 Training loss 0.09526434540748596 Validation loss 0.1116330698132515 Accuracy 0.6765000224113464\n",
      "Iteration 13300 Training loss 0.09555435180664062 Validation loss 0.11148516833782196 Accuracy 0.6775000095367432\n",
      "Iteration 13310 Training loss 0.0980188176035881 Validation loss 0.11191219091415405 Accuracy 0.6781666874885559\n",
      "Iteration 13320 Training loss 0.09935949742794037 Validation loss 0.11159979552030563 Accuracy 0.6766666769981384\n",
      "Iteration 13330 Training loss 0.09414772689342499 Validation loss 0.11347652226686478 Accuracy 0.675000011920929\n",
      "Iteration 13340 Training loss 0.0962686762213707 Validation loss 0.1117289662361145 Accuracy 0.6769999861717224\n",
      "Iteration 13350 Training loss 0.09369882941246033 Validation loss 0.1120152473449707 Accuracy 0.6773333549499512\n",
      "Iteration 13360 Training loss 0.095280721783638 Validation loss 0.11259201169013977 Accuracy 0.6756666898727417\n",
      "Iteration 13370 Training loss 0.09545731544494629 Validation loss 0.11134827882051468 Accuracy 0.6754999756813049\n",
      "Iteration 13380 Training loss 0.09333744645118713 Validation loss 0.11487966030836105 Accuracy 0.6753333210945129\n",
      "Iteration 13390 Training loss 0.09949886053800583 Validation loss 0.1122899204492569 Accuracy 0.6781666874885559\n",
      "Iteration 13400 Training loss 0.09702207148075104 Validation loss 0.11145063489675522 Accuracy 0.6765000224113464\n",
      "Iteration 13410 Training loss 0.09514860063791275 Validation loss 0.11240309476852417 Accuracy 0.6761666536331177\n",
      "Iteration 13420 Training loss 0.09754306823015213 Validation loss 0.11169600486755371 Accuracy 0.6761666536331177\n",
      "Iteration 13430 Training loss 0.09659747034311295 Validation loss 0.1115480586886406 Accuracy 0.6771666407585144\n",
      "Iteration 13440 Training loss 0.09458661079406738 Validation loss 0.11142976582050323 Accuracy 0.6766666769981384\n",
      "Iteration 13450 Training loss 0.09677093476057053 Validation loss 0.11226813495159149 Accuracy 0.6763333082199097\n",
      "Iteration 13460 Training loss 0.09684255719184875 Validation loss 0.111386738717556 Accuracy 0.6761666536331177\n",
      "Iteration 13470 Training loss 0.09580632299184799 Validation loss 0.11139849573373795 Accuracy 0.6768333315849304\n",
      "Iteration 13480 Training loss 0.09872594475746155 Validation loss 0.11174097657203674 Accuracy 0.6783333420753479\n",
      "Iteration 13490 Training loss 0.09434465318918228 Validation loss 0.11209850013256073 Accuracy 0.6778333187103271\n",
      "Iteration 13500 Training loss 0.09667354077100754 Validation loss 0.11194132268428802 Accuracy 0.6768333315849304\n",
      "Iteration 13510 Training loss 0.09632924199104309 Validation loss 0.11165766417980194 Accuracy 0.6768333315849304\n",
      "Iteration 13520 Training loss 0.09432989358901978 Validation loss 0.11361463367938995 Accuracy 0.6740000247955322\n",
      "Iteration 13530 Training loss 0.09336043149232864 Validation loss 0.11395837366580963 Accuracy 0.674833357334137\n",
      "Iteration 13540 Training loss 0.09764790534973145 Validation loss 0.11176890879869461 Accuracy 0.6773333549499512\n",
      "Iteration 13550 Training loss 0.09509959816932678 Validation loss 0.11226439476013184 Accuracy 0.6754999756813049\n",
      "Iteration 13560 Training loss 0.09551839530467987 Validation loss 0.11205412447452545 Accuracy 0.6778333187103271\n",
      "Iteration 13570 Training loss 0.09902653098106384 Validation loss 0.11165938526391983 Accuracy 0.6791666746139526\n",
      "Iteration 13580 Training loss 0.09729354828596115 Validation loss 0.11162786185741425 Accuracy 0.6756666898727417\n",
      "Iteration 13590 Training loss 0.09530816972255707 Validation loss 0.1114705428481102 Accuracy 0.6775000095367432\n",
      "Iteration 13600 Training loss 0.0960526317358017 Validation loss 0.11175933480262756 Accuracy 0.6761666536331177\n",
      "Iteration 13610 Training loss 0.09423915296792984 Validation loss 0.11194827407598495 Accuracy 0.6788333058357239\n",
      "Iteration 13620 Training loss 0.09900826215744019 Validation loss 0.11182782053947449 Accuracy 0.6775000095367432\n",
      "Iteration 13630 Training loss 0.09700899571180344 Validation loss 0.111537404358387 Accuracy 0.6766666769981384\n",
      "Iteration 13640 Training loss 0.09575243294239044 Validation loss 0.11164037883281708 Accuracy 0.6771666407585144\n",
      "Iteration 13650 Training loss 0.09573522955179214 Validation loss 0.11173103749752045 Accuracy 0.6758333444595337\n",
      "Iteration 13660 Training loss 0.0966198742389679 Validation loss 0.1114731952548027 Accuracy 0.6753333210945129\n",
      "Iteration 13670 Training loss 0.09807739406824112 Validation loss 0.11150964349508286 Accuracy 0.6766666769981384\n",
      "Iteration 13680 Training loss 0.09384102374315262 Validation loss 0.1161571741104126 Accuracy 0.6704999804496765\n",
      "Iteration 13690 Training loss 0.09629323333501816 Validation loss 0.1113685891032219 Accuracy 0.6766666769981384\n",
      "Iteration 13700 Training loss 0.09826698899269104 Validation loss 0.1114826425909996 Accuracy 0.6776666641235352\n",
      "Iteration 13710 Training loss 0.09277686476707458 Validation loss 0.11301926523447037 Accuracy 0.6743333339691162\n",
      "Iteration 13720 Training loss 0.09314274042844772 Validation loss 0.11334411799907684 Accuracy 0.6733333468437195\n",
      "Iteration 13730 Training loss 0.09635866433382034 Validation loss 0.11127115786075592 Accuracy 0.6771666407585144\n",
      "Iteration 13740 Training loss 0.09658053517341614 Validation loss 0.11128132790327072 Accuracy 0.6763333082199097\n",
      "Iteration 13750 Training loss 0.09731362760066986 Validation loss 0.11135874688625336 Accuracy 0.6783333420753479\n",
      "Iteration 13760 Training loss 0.09237053245306015 Validation loss 0.1141330748796463 Accuracy 0.675166666507721\n",
      "Iteration 13770 Training loss 0.09329459816217422 Validation loss 0.11665774881839752 Accuracy 0.6685000061988831\n",
      "Iteration 13780 Training loss 0.09428288042545319 Validation loss 0.1283101737499237 Accuracy 0.6476666927337646\n",
      "Iteration 13790 Training loss 0.09963931143283844 Validation loss 0.1377297043800354 Accuracy 0.6243333220481873\n",
      "Iteration 13800 Training loss 0.09543522447347641 Validation loss 0.12629380822181702 Accuracy 0.6503333449363708\n",
      "Iteration 13810 Training loss 0.09769988059997559 Validation loss 0.13564378023147583 Accuracy 0.6293333172798157\n",
      "Iteration 13820 Training loss 0.09410551190376282 Validation loss 0.1228240355849266 Accuracy 0.656000018119812\n",
      "Iteration 13830 Training loss 0.0944814458489418 Validation loss 0.12623435258865356 Accuracy 0.6518333554267883\n",
      "Iteration 13840 Training loss 0.09751057624816895 Validation loss 0.1370522826910019 Accuracy 0.6271666884422302\n",
      "Iteration 13850 Training loss 0.09645090997219086 Validation loss 0.12677398324012756 Accuracy 0.6518333554267883\n",
      "Iteration 13860 Training loss 0.09482616186141968 Validation loss 0.12718792259693146 Accuracy 0.6503333449363708\n",
      "Iteration 13870 Training loss 0.09676839411258698 Validation loss 0.12932299077510834 Accuracy 0.6455000042915344\n",
      "Iteration 13880 Training loss 0.09313850849866867 Validation loss 0.1260388195514679 Accuracy 0.653166651725769\n",
      "Iteration 13890 Training loss 0.09580855071544647 Validation loss 0.1276078075170517 Accuracy 0.6499999761581421\n",
      "Iteration 13900 Training loss 0.09491042792797089 Validation loss 0.12847644090652466 Accuracy 0.6471666693687439\n",
      "Iteration 13910 Training loss 0.09674392640590668 Validation loss 0.1327335238456726 Accuracy 0.637499988079071\n",
      "Iteration 13920 Training loss 0.09461598843336105 Validation loss 0.12450994551181793 Accuracy 0.6545000076293945\n",
      "Iteration 13930 Training loss 0.09589836746454239 Validation loss 0.12694643437862396 Accuracy 0.6501666903495789\n",
      "Iteration 13940 Training loss 0.0986243486404419 Validation loss 0.13493719696998596 Accuracy 0.6313333511352539\n",
      "Iteration 13950 Training loss 0.09704726934432983 Validation loss 0.1367262750864029 Accuracy 0.6259999871253967\n",
      "Iteration 13960 Training loss 0.09294672310352325 Validation loss 0.12231948971748352 Accuracy 0.6551666855812073\n",
      "Iteration 13970 Training loss 0.09493854641914368 Validation loss 0.12678997218608856 Accuracy 0.6508333086967468\n",
      "Iteration 13980 Training loss 0.09745755791664124 Validation loss 0.13351872563362122 Accuracy 0.6348333358764648\n",
      "Iteration 13990 Training loss 0.09638409316539764 Validation loss 0.1288588047027588 Accuracy 0.6470000147819519\n",
      "Iteration 14000 Training loss 0.09444120526313782 Validation loss 0.12287764251232147 Accuracy 0.656000018119812\n",
      "Iteration 14010 Training loss 0.09726149588823318 Validation loss 0.13149340450763702 Accuracy 0.6413333415985107\n",
      "Iteration 14020 Training loss 0.09451833367347717 Validation loss 0.12651506066322327 Accuracy 0.6521666646003723\n",
      "Iteration 14030 Training loss 0.0928836241364479 Validation loss 0.11791925877332687 Accuracy 0.6666666865348816\n",
      "Iteration 14040 Training loss 0.09504842013120651 Validation loss 0.1283280849456787 Accuracy 0.6478333473205566\n",
      "Iteration 14050 Training loss 0.09764442592859268 Validation loss 0.13441714644432068 Accuracy 0.6315000057220459\n",
      "Iteration 14060 Training loss 0.09713000804185867 Validation loss 0.1312750279903412 Accuracy 0.6413333415985107\n",
      "Iteration 14070 Training loss 0.09710007905960083 Validation loss 0.1267452836036682 Accuracy 0.6508333086967468\n",
      "Iteration 14080 Training loss 0.09552647173404694 Validation loss 0.12186676263809204 Accuracy 0.6571666598320007\n",
      "Iteration 14090 Training loss 0.09430418908596039 Validation loss 0.12082009762525558 Accuracy 0.6610000133514404\n",
      "Iteration 14100 Training loss 0.09992393106222153 Validation loss 0.1407712996006012 Accuracy 0.6168333292007446\n",
      "Iteration 14110 Training loss 0.0967923253774643 Validation loss 0.12209916859865189 Accuracy 0.6568333506584167\n",
      "Iteration 14120 Training loss 0.09432122856378555 Validation loss 0.12523454427719116 Accuracy 0.6528333425521851\n",
      "Iteration 14130 Training loss 0.09475740790367126 Validation loss 0.12793056666851044 Accuracy 0.6486666798591614\n",
      "Iteration 14140 Training loss 0.09603757411241531 Validation loss 0.1306643933057785 Accuracy 0.6439999938011169\n",
      "Iteration 14150 Training loss 0.09623091667890549 Validation loss 0.13125723600387573 Accuracy 0.6384999752044678\n",
      "Iteration 14160 Training loss 0.09478331357240677 Validation loss 0.132170170545578 Accuracy 0.6386666893959045\n",
      "Iteration 14170 Training loss 0.09592821449041367 Validation loss 0.13177616894245148 Accuracy 0.6391666531562805\n",
      "Iteration 14180 Training loss 0.09270713478326797 Validation loss 0.11904451251029968 Accuracy 0.6643333435058594\n",
      "Iteration 14190 Training loss 0.09390433877706528 Validation loss 0.11604005098342896 Accuracy 0.6708333492279053\n",
      "Iteration 14200 Training loss 0.09407772123813629 Validation loss 0.11350977420806885 Accuracy 0.675166666507721\n",
      "Iteration 14210 Training loss 0.09981445968151093 Validation loss 0.11206613481044769 Accuracy 0.6794999837875366\n",
      "Iteration 14220 Training loss 0.09813284128904343 Validation loss 0.11161936074495316 Accuracy 0.6783333420753479\n",
      "Iteration 14230 Training loss 0.09458734095096588 Validation loss 0.11163901537656784 Accuracy 0.6768333315849304\n",
      "Iteration 14240 Training loss 0.0956331416964531 Validation loss 0.11340866982936859 Accuracy 0.6769999861717224\n",
      "Iteration 14250 Training loss 0.09934298694133759 Validation loss 0.1126994639635086 Accuracy 0.6793333292007446\n",
      "Iteration 14260 Training loss 0.09704681485891342 Validation loss 0.11131834983825684 Accuracy 0.6759999990463257\n",
      "Iteration 14270 Training loss 0.09756872802972794 Validation loss 0.11179406940937042 Accuracy 0.6773333549499512\n",
      "Iteration 14280 Training loss 0.09584373235702515 Validation loss 0.11143093556165695 Accuracy 0.6768333315849304\n",
      "Iteration 14290 Training loss 0.09526640176773071 Validation loss 0.11191700398921967 Accuracy 0.6769999861717224\n",
      "Iteration 14300 Training loss 0.09580355882644653 Validation loss 0.11184244602918625 Accuracy 0.6786666512489319\n",
      "Iteration 14310 Training loss 0.09687180817127228 Validation loss 0.111642025411129 Accuracy 0.6761666536331177\n",
      "Iteration 14320 Training loss 0.09845729917287827 Validation loss 0.11176277697086334 Accuracy 0.6801666617393494\n",
      "Iteration 14330 Training loss 0.09483136236667633 Validation loss 0.1121651828289032 Accuracy 0.6768333315849304\n",
      "Iteration 14340 Training loss 0.09615404158830643 Validation loss 0.11187443882226944 Accuracy 0.6775000095367432\n",
      "Iteration 14350 Training loss 0.09539568424224854 Validation loss 0.11157585680484772 Accuracy 0.6779999732971191\n",
      "Iteration 14360 Training loss 0.09672707319259644 Validation loss 0.11133231967687607 Accuracy 0.6786666512489319\n",
      "Iteration 14370 Training loss 0.09552241116762161 Validation loss 0.11136721074581146 Accuracy 0.6781666874885559\n",
      "Iteration 14380 Training loss 0.09492948651313782 Validation loss 0.1162935197353363 Accuracy 0.672166645526886\n",
      "Iteration 14390 Training loss 0.09916294366121292 Validation loss 0.13884705305099487 Accuracy 0.6226666569709778\n",
      "Iteration 14400 Training loss 0.09270424395799637 Validation loss 0.12277743220329285 Accuracy 0.656499981880188\n",
      "Iteration 14410 Training loss 0.09705176949501038 Validation loss 0.13329671323299408 Accuracy 0.6356666684150696\n",
      "Iteration 14420 Training loss 0.0938202440738678 Validation loss 0.12204215675592422 Accuracy 0.6576666831970215\n",
      "Iteration 14430 Training loss 0.09601021558046341 Validation loss 0.13263361155986786 Accuracy 0.6363333463668823\n",
      "Iteration 14440 Training loss 0.09468207508325577 Validation loss 0.12435803562402725 Accuracy 0.6543333530426025\n",
      "Iteration 14450 Training loss 0.10048229992389679 Validation loss 0.1359589695930481 Accuracy 0.6290000081062317\n",
      "Iteration 14460 Training loss 0.09588317573070526 Validation loss 0.1289280354976654 Accuracy 0.6471666693687439\n",
      "Iteration 14470 Training loss 0.09634869545698166 Validation loss 0.13314762711524963 Accuracy 0.6368333101272583\n",
      "Iteration 14480 Training loss 0.0956200361251831 Validation loss 0.12696176767349243 Accuracy 0.6496666669845581\n",
      "Iteration 14490 Training loss 0.09711220115423203 Validation loss 0.13044166564941406 Accuracy 0.6430000066757202\n",
      "Iteration 14500 Training loss 0.09514493495225906 Validation loss 0.12785011529922485 Accuracy 0.6476666927337646\n",
      "Iteration 14510 Training loss 0.09488716721534729 Validation loss 0.12286989390850067 Accuracy 0.6579999923706055\n",
      "Iteration 14520 Training loss 0.09651298075914383 Validation loss 0.1320841908454895 Accuracy 0.640333354473114\n",
      "Iteration 14530 Training loss 0.09440125524997711 Validation loss 0.12466521561145782 Accuracy 0.6543333530426025\n",
      "Iteration 14540 Training loss 0.09369166195392609 Validation loss 0.12635856866836548 Accuracy 0.6516666412353516\n",
      "Iteration 14550 Training loss 0.09617672860622406 Validation loss 0.13710618019104004 Accuracy 0.628166675567627\n",
      "Iteration 14560 Training loss 0.09570237249135971 Validation loss 0.12449197471141815 Accuracy 0.6553333401679993\n",
      "Iteration 14570 Training loss 0.09281191974878311 Validation loss 0.12675179541110992 Accuracy 0.6523333191871643\n",
      "Iteration 14580 Training loss 0.09528682380914688 Validation loss 0.13388840854167938 Accuracy 0.6345000267028809\n",
      "Iteration 14590 Training loss 0.09804345667362213 Validation loss 0.13531725108623505 Accuracy 0.6290000081062317\n",
      "Iteration 14600 Training loss 0.0943080335855484 Validation loss 0.1283259093761444 Accuracy 0.6473333239555359\n",
      "Iteration 14610 Training loss 0.09548688679933548 Validation loss 0.1210470199584961 Accuracy 0.6604999899864197\n",
      "Iteration 14620 Training loss 0.09521236270666122 Validation loss 0.13428542017936707 Accuracy 0.6324999928474426\n",
      "Iteration 14630 Training loss 0.09570992738008499 Validation loss 0.13206951320171356 Accuracy 0.6389999985694885\n",
      "Iteration 14640 Training loss 0.09331174939870834 Validation loss 0.1260078400373459 Accuracy 0.6516666412353516\n",
      "Iteration 14650 Training loss 0.09420737624168396 Validation loss 0.12478327751159668 Accuracy 0.6551666855812073\n",
      "Iteration 14660 Training loss 0.09865279495716095 Validation loss 0.13278289139270782 Accuracy 0.6366666555404663\n",
      "Iteration 14670 Training loss 0.0937550887465477 Validation loss 0.12976068258285522 Accuracy 0.643833339214325\n",
      "Iteration 14680 Training loss 0.09463439881801605 Validation loss 0.12751451134681702 Accuracy 0.6485000252723694\n",
      "Iteration 14690 Training loss 0.09448980540037155 Validation loss 0.11886736750602722 Accuracy 0.6658333539962769\n",
      "Iteration 14700 Training loss 0.09843441843986511 Validation loss 0.1390264928340912 Accuracy 0.6215000152587891\n",
      "Iteration 14710 Training loss 0.09301984310150146 Validation loss 0.1228257417678833 Accuracy 0.6573333144187927\n",
      "Iteration 14720 Training loss 0.09152925759553909 Validation loss 0.12338700145483017 Accuracy 0.6553333401679993\n",
      "Iteration 14730 Training loss 0.09430401772260666 Validation loss 0.12790724635124207 Accuracy 0.6498333215713501\n",
      "Iteration 14740 Training loss 0.0979536920785904 Validation loss 0.13356149196624756 Accuracy 0.6356666684150696\n",
      "Iteration 14750 Training loss 0.09361498802900314 Validation loss 0.12127266079187393 Accuracy 0.6586666703224182\n",
      "Iteration 14760 Training loss 0.09363327920436859 Validation loss 0.11936625093221664 Accuracy 0.6648333072662354\n",
      "Iteration 14770 Training loss 0.09725023061037064 Validation loss 0.13732552528381348 Accuracy 0.6266666650772095\n",
      "Iteration 14780 Training loss 0.09422840178012848 Validation loss 0.12561699748039246 Accuracy 0.6539999842643738\n",
      "Iteration 14790 Training loss 0.09619012475013733 Validation loss 0.12648020684719086 Accuracy 0.6506666541099548\n",
      "Iteration 14800 Training loss 0.09713713824748993 Validation loss 0.12327984720468521 Accuracy 0.6570000052452087\n",
      "Iteration 14810 Training loss 0.0964585393667221 Validation loss 0.12816613912582397 Accuracy 0.6488333344459534\n",
      "Iteration 14820 Training loss 0.09454814344644547 Validation loss 0.13058876991271973 Accuracy 0.643833339214325\n",
      "Iteration 14830 Training loss 0.09755320101976395 Validation loss 0.13513167202472687 Accuracy 0.6298333406448364\n",
      "Iteration 14840 Training loss 0.09628994017839432 Validation loss 0.13331808149814606 Accuracy 0.6356666684150696\n",
      "Iteration 14850 Training loss 0.09290244430303574 Validation loss 0.12403224408626556 Accuracy 0.6573333144187927\n",
      "Iteration 14860 Training loss 0.0958641767501831 Validation loss 0.12783996760845184 Accuracy 0.6513333320617676\n",
      "Iteration 14870 Training loss 0.09620613604784012 Validation loss 0.13165371119976044 Accuracy 0.6389999985694885\n",
      "Iteration 14880 Training loss 0.09536962956190109 Validation loss 0.12604470551013947 Accuracy 0.6541666388511658\n",
      "Iteration 14890 Training loss 0.09604419022798538 Validation loss 0.13640281558036804 Accuracy 0.6266666650772095\n",
      "Iteration 14900 Training loss 0.09553786367177963 Validation loss 0.1253676414489746 Accuracy 0.6538333296775818\n",
      "Iteration 14910 Training loss 0.09761495888233185 Validation loss 0.1364351511001587 Accuracy 0.6290000081062317\n",
      "Iteration 14920 Training loss 0.09436832368373871 Validation loss 0.13087980449199677 Accuracy 0.6430000066757202\n",
      "Iteration 14930 Training loss 0.09648285061120987 Validation loss 0.12496909499168396 Accuracy 0.6553333401679993\n",
      "Iteration 14940 Training loss 0.09493853896856308 Validation loss 0.12950041890144348 Accuracy 0.6463333368301392\n",
      "Iteration 14950 Training loss 0.09522981941699982 Validation loss 0.13075482845306396 Accuracy 0.643666684627533\n",
      "Iteration 14960 Training loss 0.0975177139043808 Validation loss 0.14131268858909607 Accuracy 0.6166666746139526\n",
      "Iteration 14970 Training loss 0.09288027882575989 Validation loss 0.1256987303495407 Accuracy 0.6541666388511658\n",
      "Iteration 14980 Training loss 0.09409079700708389 Validation loss 0.12642055749893188 Accuracy 0.6518333554267883\n",
      "Iteration 14990 Training loss 0.09606095403432846 Validation loss 0.1357451230287552 Accuracy 0.6296666860580444\n",
      "Iteration 15000 Training loss 0.09740381687879562 Validation loss 0.13661053776741028 Accuracy 0.6278333067893982\n",
      "Iteration 15010 Training loss 0.09549278020858765 Validation loss 0.1282968670129776 Accuracy 0.6483333110809326\n",
      "Iteration 15020 Training loss 0.09655975550413132 Validation loss 0.13381846249103546 Accuracy 0.6351666450500488\n",
      "Iteration 15030 Training loss 0.09416157752275467 Validation loss 0.13064493238925934 Accuracy 0.6424999833106995\n",
      "Iteration 15040 Training loss 0.09346571564674377 Validation loss 0.12427760660648346 Accuracy 0.6546666622161865\n",
      "Iteration 15050 Training loss 0.09563517570495605 Validation loss 0.12480737268924713 Accuracy 0.6549999713897705\n",
      "Iteration 15060 Training loss 0.09487541019916534 Validation loss 0.13515719771385193 Accuracy 0.6315000057220459\n",
      "Iteration 15070 Training loss 0.09712180495262146 Validation loss 0.13194593787193298 Accuracy 0.6401666402816772\n",
      "Iteration 15080 Training loss 0.09259989112615585 Validation loss 0.11843914538621902 Accuracy 0.6675000190734863\n",
      "Iteration 15090 Training loss 0.09671919047832489 Validation loss 0.13043764233589172 Accuracy 0.6441666483879089\n",
      "Iteration 15100 Training loss 0.09279526770114899 Validation loss 0.12745743989944458 Accuracy 0.6504999995231628\n",
      "Iteration 15110 Training loss 0.09332090616226196 Validation loss 0.12346063554286957 Accuracy 0.6583333611488342\n",
      "Iteration 15120 Training loss 0.09654052555561066 Validation loss 0.1336888074874878 Accuracy 0.6361666917800903\n",
      "Iteration 15130 Training loss 0.09053605794906616 Validation loss 0.12579882144927979 Accuracy 0.6520000100135803\n",
      "Iteration 15140 Training loss 0.0949697196483612 Validation loss 0.11428588628768921 Accuracy 0.674833357334137\n",
      "Iteration 15150 Training loss 0.09362798184156418 Validation loss 0.12289693206548691 Accuracy 0.6570000052452087\n",
      "Iteration 15160 Training loss 0.09800046682357788 Validation loss 0.13621219992637634 Accuracy 0.6305000185966492\n",
      "Iteration 15170 Training loss 0.0941564217209816 Validation loss 0.1272497922182083 Accuracy 0.6501666903495789\n",
      "Iteration 15180 Training loss 0.09376555681228638 Validation loss 0.12141422182321548 Accuracy 0.6611666679382324\n",
      "Iteration 15190 Training loss 0.09593918174505234 Validation loss 0.12558773159980774 Accuracy 0.6545000076293945\n",
      "Iteration 15200 Training loss 0.09358302503824234 Validation loss 0.12388153374195099 Accuracy 0.656333327293396\n",
      "Iteration 15210 Training loss 0.0933249294757843 Validation loss 0.12157785892486572 Accuracy 0.6596666574478149\n",
      "Iteration 15220 Training loss 0.09609536081552505 Validation loss 0.1325197070837021 Accuracy 0.6401666402816772\n",
      "Iteration 15230 Training loss 0.09664485603570938 Validation loss 0.13065974414348602 Accuracy 0.6434999704360962\n",
      "Iteration 15240 Training loss 0.09885779023170471 Validation loss 0.13123956322669983 Accuracy 0.640666663646698\n",
      "Iteration 15250 Training loss 0.09378700703382492 Validation loss 0.1276230812072754 Accuracy 0.6495000123977661\n",
      "Iteration 15260 Training loss 0.09579481929540634 Validation loss 0.12889303267002106 Accuracy 0.6478333473205566\n",
      "Iteration 15270 Training loss 0.09595973044633865 Validation loss 0.13366782665252686 Accuracy 0.6351666450500488\n",
      "Iteration 15280 Training loss 0.09495054185390472 Validation loss 0.12841999530792236 Accuracy 0.6470000147819519\n",
      "Iteration 15290 Training loss 0.0951448604464531 Validation loss 0.13400320708751678 Accuracy 0.6355000138282776\n",
      "Iteration 15300 Training loss 0.09537630528211594 Validation loss 0.1266462504863739 Accuracy 0.6501666903495789\n",
      "Iteration 15310 Training loss 0.09373519569635391 Validation loss 0.12521480023860931 Accuracy 0.652999997138977\n",
      "Iteration 15320 Training loss 0.09271042793989182 Validation loss 0.11994750797748566 Accuracy 0.6623333096504211\n",
      "Iteration 15330 Training loss 0.09587322175502777 Validation loss 0.11163076013326645 Accuracy 0.6788333058357239\n",
      "Iteration 15340 Training loss 0.09632205963134766 Validation loss 0.11156223714351654 Accuracy 0.6778333187103271\n",
      "Iteration 15350 Training loss 0.09536242485046387 Validation loss 0.11172859370708466 Accuracy 0.6779999732971191\n",
      "Iteration 15360 Training loss 0.09151890128850937 Validation loss 0.11458443105220795 Accuracy 0.6746666431427002\n",
      "Iteration 15370 Training loss 0.0982821136713028 Validation loss 0.1117159053683281 Accuracy 0.6803333163261414\n",
      "Iteration 15380 Training loss 0.09356514364480972 Validation loss 0.11302203685045242 Accuracy 0.6778333187103271\n",
      "Iteration 15390 Training loss 0.09517966955900192 Validation loss 0.1117633804678917 Accuracy 0.6775000095367432\n",
      "Iteration 15400 Training loss 0.09505830705165863 Validation loss 0.11157743632793427 Accuracy 0.6798333525657654\n",
      "Iteration 15410 Training loss 0.09647046029567719 Validation loss 0.11156120896339417 Accuracy 0.6800000071525574\n",
      "Iteration 15420 Training loss 0.0906568169593811 Validation loss 0.11284361034631729 Accuracy 0.6776666641235352\n",
      "Iteration 15430 Training loss 0.096567302942276 Validation loss 0.11169037222862244 Accuracy 0.6784999966621399\n",
      "Iteration 15440 Training loss 0.09590474516153336 Validation loss 0.11162354797124863 Accuracy 0.6776666641235352\n",
      "Iteration 15450 Training loss 0.09690648317337036 Validation loss 0.11157941818237305 Accuracy 0.6809999942779541\n",
      "Iteration 15460 Training loss 0.09366807341575623 Validation loss 0.1120632141828537 Accuracy 0.6771666407585144\n",
      "Iteration 15470 Training loss 0.09439776837825775 Validation loss 0.11495637893676758 Accuracy 0.6741666793823242\n",
      "Iteration 15480 Training loss 0.09335494041442871 Validation loss 0.11459030956029892 Accuracy 0.6743333339691162\n",
      "Iteration 15490 Training loss 0.09672824293375015 Validation loss 0.11170872300863266 Accuracy 0.6808333396911621\n",
      "Iteration 15500 Training loss 0.09769077599048615 Validation loss 0.11164051294326782 Accuracy 0.6806666851043701\n",
      "Iteration 15510 Training loss 0.09534589946269989 Validation loss 0.11180325597524643 Accuracy 0.6776666641235352\n",
      "Iteration 15520 Training loss 0.09542097896337509 Validation loss 0.11178123950958252 Accuracy 0.6778333187103271\n",
      "Iteration 15530 Training loss 0.09570454806089401 Validation loss 0.1114722192287445 Accuracy 0.6801666617393494\n",
      "Iteration 15540 Training loss 0.09520713239908218 Validation loss 0.11132516711950302 Accuracy 0.6803333163261414\n",
      "Iteration 15550 Training loss 0.09320665150880814 Validation loss 0.11165545880794525 Accuracy 0.6784999966621399\n",
      "Iteration 15560 Training loss 0.09554413706064224 Validation loss 0.11120820045471191 Accuracy 0.6783333420753479\n",
      "Iteration 15570 Training loss 0.09393880516290665 Validation loss 0.11168491840362549 Accuracy 0.6771666407585144\n",
      "Iteration 15580 Training loss 0.09434641152620316 Validation loss 0.11349735409021378 Accuracy 0.6784999966621399\n",
      "Iteration 15590 Training loss 0.0935705155134201 Validation loss 0.11903802305459976 Accuracy 0.6653333306312561\n",
      "Iteration 15600 Training loss 0.0929199755191803 Validation loss 0.12200247496366501 Accuracy 0.659333348274231\n",
      "Iteration 15610 Training loss 0.09531697630882263 Validation loss 0.13451409339904785 Accuracy 0.6349999904632568\n",
      "Iteration 15620 Training loss 0.09694020450115204 Validation loss 0.13391445577144623 Accuracy 0.6338333487510681\n",
      "Iteration 15630 Training loss 0.09545174986124039 Validation loss 0.1333123743534088 Accuracy 0.6370000243186951\n",
      "Iteration 15640 Training loss 0.09358563274145126 Validation loss 0.12680602073669434 Accuracy 0.6516666412353516\n",
      "Iteration 15650 Training loss 0.09368954598903656 Validation loss 0.1226876825094223 Accuracy 0.659166693687439\n",
      "Iteration 15660 Training loss 0.09660721570253372 Validation loss 0.12694093585014343 Accuracy 0.6506666541099548\n",
      "Iteration 15670 Training loss 0.0931154191493988 Validation loss 0.1294165402650833 Accuracy 0.6460000276565552\n",
      "Iteration 15680 Training loss 0.09439001232385635 Validation loss 0.12520219385623932 Accuracy 0.6528333425521851\n",
      "Iteration 15690 Training loss 0.09473811089992523 Validation loss 0.12186931073665619 Accuracy 0.6586666703224182\n",
      "Iteration 15700 Training loss 0.09705755859613419 Validation loss 0.13379506766796112 Accuracy 0.6366666555404663\n",
      "Iteration 15710 Training loss 0.09510429203510284 Validation loss 0.13168416917324066 Accuracy 0.6411666870117188\n",
      "Iteration 15720 Training loss 0.09597209841012955 Validation loss 0.1299082487821579 Accuracy 0.6443333625793457\n",
      "Iteration 15730 Training loss 0.0944073274731636 Validation loss 0.1263677030801773 Accuracy 0.6511666774749756\n",
      "Iteration 15740 Training loss 0.09169955551624298 Validation loss 0.12559431791305542 Accuracy 0.6553333401679993\n",
      "Iteration 15750 Training loss 0.09726186096668243 Validation loss 0.13577212393283844 Accuracy 0.6309999823570251\n",
      "Iteration 15760 Training loss 0.09594935923814774 Validation loss 0.13065384328365326 Accuracy 0.6431666612625122\n",
      "Iteration 15770 Training loss 0.09696681052446365 Validation loss 0.1381732076406479 Accuracy 0.6238333582878113\n",
      "Iteration 15780 Training loss 0.09507244825363159 Validation loss 0.13374507427215576 Accuracy 0.6346666812896729\n",
      "Iteration 15790 Training loss 0.09362845867872238 Validation loss 0.123328298330307 Accuracy 0.6589999794960022\n",
      "Iteration 15800 Training loss 0.09311899542808533 Validation loss 0.12022672593593597 Accuracy 0.6629999876022339\n",
      "Iteration 15810 Training loss 0.09216424822807312 Validation loss 0.11937800794839859 Accuracy 0.6626666784286499\n",
      "Iteration 15820 Training loss 0.09724035859107971 Validation loss 0.11163356155157089 Accuracy 0.6806666851043701\n",
      "Iteration 15830 Training loss 0.09734310209751129 Validation loss 0.11135479062795639 Accuracy 0.6816666722297668\n",
      "Iteration 15840 Training loss 0.09559790045022964 Validation loss 0.11143023520708084 Accuracy 0.6804999709129333\n",
      "Iteration 15850 Training loss 0.09364797174930573 Validation loss 0.11324691027402878 Accuracy 0.6761666536331177\n",
      "Iteration 15860 Training loss 0.09598741680383682 Validation loss 0.11145411431789398 Accuracy 0.6813333630561829\n",
      "Iteration 15870 Training loss 0.09397166222333908 Validation loss 0.11151782423257828 Accuracy 0.6776666641235352\n",
      "Iteration 15880 Training loss 0.09490116685628891 Validation loss 0.11869784444570541 Accuracy 0.6671666502952576\n",
      "Iteration 15890 Training loss 0.09135274589061737 Validation loss 0.11457525938749313 Accuracy 0.6763333082199097\n",
      "Iteration 15900 Training loss 0.09892293065786362 Validation loss 0.11217177659273148 Accuracy 0.6806666851043701\n",
      "Iteration 15910 Training loss 0.09514005482196808 Validation loss 0.11143249273300171 Accuracy 0.6791666746139526\n",
      "Iteration 15920 Training loss 0.0926285907626152 Validation loss 0.11164849996566772 Accuracy 0.6793333292007446\n",
      "Iteration 15930 Training loss 0.09533274173736572 Validation loss 0.11284776031970978 Accuracy 0.6793333292007446\n",
      "Iteration 15940 Training loss 0.0934436023235321 Validation loss 0.11174212396144867 Accuracy 0.6791666746139526\n",
      "Iteration 15950 Training loss 0.09590602666139603 Validation loss 0.11120699346065521 Accuracy 0.6800000071525574\n",
      "Iteration 15960 Training loss 0.09316451847553253 Validation loss 0.11188334971666336 Accuracy 0.6788333058357239\n",
      "Iteration 15970 Training loss 0.09380102902650833 Validation loss 0.11254475265741348 Accuracy 0.6790000200271606\n",
      "Iteration 15980 Training loss 0.0954526960849762 Validation loss 0.11147094517946243 Accuracy 0.6819999814033508\n",
      "Iteration 15990 Training loss 0.09331774711608887 Validation loss 0.11155932396650314 Accuracy 0.6779999732971191\n",
      "Iteration 16000 Training loss 0.09234100580215454 Validation loss 0.11273909360170364 Accuracy 0.6786666512489319\n",
      "Iteration 16010 Training loss 0.09564072638750076 Validation loss 0.11147261410951614 Accuracy 0.6800000071525574\n",
      "Iteration 16020 Training loss 0.09435361623764038 Validation loss 0.11149664968252182 Accuracy 0.6811666488647461\n",
      "Iteration 16030 Training loss 0.0946168527007103 Validation loss 0.11146096140146255 Accuracy 0.6793333292007446\n",
      "Iteration 16040 Training loss 0.09512246400117874 Validation loss 0.11157505959272385 Accuracy 0.6775000095367432\n",
      "Iteration 16050 Training loss 0.09711456298828125 Validation loss 0.11202453821897507 Accuracy 0.6801666617393494\n",
      "Iteration 16060 Training loss 0.09606742113828659 Validation loss 0.11173927783966064 Accuracy 0.6794999837875366\n",
      "Iteration 16070 Training loss 0.09864050149917603 Validation loss 0.11147493124008179 Accuracy 0.6798333525657654\n",
      "Iteration 16080 Training loss 0.09244658052921295 Validation loss 0.11308497935533524 Accuracy 0.6779999732971191\n",
      "Iteration 16090 Training loss 0.09130706638097763 Validation loss 0.1139327809214592 Accuracy 0.6768333315849304\n",
      "Iteration 16100 Training loss 0.09321165829896927 Validation loss 0.11213649809360504 Accuracy 0.6779999732971191\n",
      "Iteration 16110 Training loss 0.09555331617593765 Validation loss 0.1120917946100235 Accuracy 0.6791666746139526\n",
      "Iteration 16120 Training loss 0.0975763350725174 Validation loss 0.11174540221691132 Accuracy 0.6815000176429749\n",
      "Iteration 16130 Training loss 0.09674980491399765 Validation loss 0.11166668683290482 Accuracy 0.6826666593551636\n",
      "Iteration 16140 Training loss 0.0925239697098732 Validation loss 0.11426591873168945 Accuracy 0.6756666898727417\n",
      "Iteration 16150 Training loss 0.09710568189620972 Validation loss 0.1114257276058197 Accuracy 0.6804999709129333\n",
      "Iteration 16160 Training loss 0.09729436039924622 Validation loss 0.11152390390634537 Accuracy 0.6815000176429749\n",
      "Iteration 16170 Training loss 0.0939868912100792 Validation loss 0.11170048266649246 Accuracy 0.6793333292007446\n",
      "Iteration 16180 Training loss 0.09157688170671463 Validation loss 0.11211171746253967 Accuracy 0.6783333420753479\n",
      "Iteration 16190 Training loss 0.09291300177574158 Validation loss 0.11249924451112747 Accuracy 0.6796666383743286\n",
      "Iteration 16200 Training loss 0.09445733577013016 Validation loss 0.11229926347732544 Accuracy 0.6791666746139526\n",
      "Iteration 16210 Training loss 0.09445375204086304 Validation loss 0.11141138523817062 Accuracy 0.6783333420753479\n",
      "Iteration 16220 Training loss 0.09311281144618988 Validation loss 0.11332549154758453 Accuracy 0.6781666874885559\n",
      "Iteration 16230 Training loss 0.0965610146522522 Validation loss 0.11166168004274368 Accuracy 0.6815000176429749\n",
      "Iteration 16240 Training loss 0.09613655507564545 Validation loss 0.11128310114145279 Accuracy 0.6818333268165588\n",
      "Iteration 16250 Training loss 0.09321174770593643 Validation loss 0.11130247265100479 Accuracy 0.6798333525657654\n",
      "Iteration 16260 Training loss 0.09443572163581848 Validation loss 0.1114056408405304 Accuracy 0.6804999709129333\n",
      "Iteration 16270 Training loss 0.09423097968101501 Validation loss 0.11134285479784012 Accuracy 0.6788333058357239\n",
      "Iteration 16280 Training loss 0.09322792291641235 Validation loss 0.11197217553853989 Accuracy 0.6793333292007446\n",
      "Iteration 16290 Training loss 0.09585583209991455 Validation loss 0.11147920787334442 Accuracy 0.6804999709129333\n",
      "Iteration 16300 Training loss 0.09272383898496628 Validation loss 0.11286790668964386 Accuracy 0.6783333420753479\n",
      "Iteration 16310 Training loss 0.09694810211658478 Validation loss 0.11168001592159271 Accuracy 0.6821666955947876\n",
      "Iteration 16320 Training loss 0.09204833954572678 Validation loss 0.11217055469751358 Accuracy 0.6793333292007446\n",
      "Iteration 16330 Training loss 0.09290827065706253 Validation loss 0.11245699226856232 Accuracy 0.6776666641235352\n",
      "Iteration 16340 Training loss 0.09877088665962219 Validation loss 0.11189828813076019 Accuracy 0.6826666593551636\n",
      "Iteration 16350 Training loss 0.0925627127289772 Validation loss 0.11258402466773987 Accuracy 0.6791666746139526\n",
      "Iteration 16360 Training loss 0.09259649366140366 Validation loss 0.11262648552656174 Accuracy 0.6776666641235352\n",
      "Iteration 16370 Training loss 0.09326373040676117 Validation loss 0.11240261048078537 Accuracy 0.6786666512489319\n",
      "Iteration 16380 Training loss 0.09391790628433228 Validation loss 0.11501622200012207 Accuracy 0.6753333210945129\n",
      "Iteration 16390 Training loss 0.09268831461668015 Validation loss 0.11386358737945557 Accuracy 0.6754999756813049\n",
      "Iteration 16400 Training loss 0.09285642206668854 Validation loss 0.11646704375743866 Accuracy 0.6731666922569275\n",
      "Iteration 16410 Training loss 0.09186111390590668 Validation loss 0.11522794514894485 Accuracy 0.675166666507721\n",
      "Iteration 16420 Training loss 0.09917441755533218 Validation loss 0.11338411271572113 Accuracy 0.6761666536331177\n",
      "Iteration 16430 Training loss 0.09703332185745239 Validation loss 0.1117725819349289 Accuracy 0.6804999709129333\n",
      "Iteration 16440 Training loss 0.09441918879747391 Validation loss 0.11174417287111282 Accuracy 0.6809999942779541\n",
      "Iteration 16450 Training loss 0.09362773597240448 Validation loss 0.11200057715177536 Accuracy 0.6801666617393494\n",
      "Iteration 16460 Training loss 0.09423652291297913 Validation loss 0.11173415929079056 Accuracy 0.6796666383743286\n",
      "Iteration 16470 Training loss 0.09375093877315521 Validation loss 0.11169906705617905 Accuracy 0.6804999709129333\n",
      "Iteration 16480 Training loss 0.09856651723384857 Validation loss 0.11194892227649689 Accuracy 0.6821666955947876\n",
      "Iteration 16490 Training loss 0.09669403731822968 Validation loss 0.11150798201560974 Accuracy 0.6815000176429749\n",
      "Iteration 16500 Training loss 0.09316091239452362 Validation loss 0.1129772812128067 Accuracy 0.6803333163261414\n",
      "Iteration 16510 Training loss 0.0921482965350151 Validation loss 0.12202560156583786 Accuracy 0.6600000262260437\n",
      "Iteration 16520 Training loss 0.0967598482966423 Validation loss 0.13061483204364777 Accuracy 0.6443333625793457\n",
      "Iteration 16530 Training loss 0.1012638732790947 Validation loss 0.13736143708229065 Accuracy 0.6268333196640015\n",
      "Iteration 16540 Training loss 0.09306291490793228 Validation loss 0.11810299009084702 Accuracy 0.6711666584014893\n",
      "Iteration 16550 Training loss 0.0913628563284874 Validation loss 0.12155774980783463 Accuracy 0.6606666445732117\n",
      "Iteration 16560 Training loss 0.09613396972417831 Validation loss 0.12901823222637177 Accuracy 0.6483333110809326\n",
      "Iteration 16570 Training loss 0.09628373384475708 Validation loss 0.1338415890932083 Accuracy 0.6353333592414856\n",
      "Iteration 16580 Training loss 0.0970093384385109 Validation loss 0.13486365973949432 Accuracy 0.6333333253860474\n",
      "Iteration 16590 Training loss 0.09677811712026596 Validation loss 0.1361600160598755 Accuracy 0.6284999847412109\n",
      "Iteration 16600 Training loss 0.0962066650390625 Validation loss 0.12731698155403137 Accuracy 0.6513333320617676\n",
      "Iteration 16610 Training loss 0.09361486881971359 Validation loss 0.12512537837028503 Accuracy 0.6545000076293945\n",
      "Iteration 16620 Training loss 0.09259047359228134 Validation loss 0.12255685776472092 Accuracy 0.659166693687439\n",
      "Iteration 16630 Training loss 0.09333473443984985 Validation loss 0.12182018160820007 Accuracy 0.6621666550636292\n",
      "Iteration 16640 Training loss 0.0956430733203888 Validation loss 0.1339641958475113 Accuracy 0.6346666812896729\n",
      "Iteration 16650 Training loss 0.09653016924858093 Validation loss 0.13195577263832092 Accuracy 0.6396666765213013\n",
      "Iteration 16660 Training loss 0.09345975518226624 Validation loss 0.12110038101673126 Accuracy 0.6610000133514404\n",
      "Iteration 16670 Training loss 0.0943600982427597 Validation loss 0.12304040789604187 Accuracy 0.6583333611488342\n",
      "Iteration 16680 Training loss 0.09298939257860184 Validation loss 0.1251278817653656 Accuracy 0.6558333039283752\n",
      "Iteration 16690 Training loss 0.09751136600971222 Validation loss 0.13422907888889313 Accuracy 0.6348333358764648\n",
      "Iteration 16700 Training loss 0.09321039170026779 Validation loss 0.12779811024665833 Accuracy 0.6503333449363708\n",
      "Iteration 16710 Training loss 0.09261509031057358 Validation loss 0.11792256683111191 Accuracy 0.6698333621025085\n",
      "Iteration 16720 Training loss 0.09112716466188431 Validation loss 0.11901384592056274 Accuracy 0.6675000190734863\n",
      "Iteration 16730 Training loss 0.09641436487436295 Validation loss 0.13329559564590454 Accuracy 0.6380000114440918\n",
      "Iteration 16740 Training loss 0.09577992558479309 Validation loss 0.13032110035419464 Accuracy 0.6453333497047424\n",
      "Iteration 16750 Training loss 0.09404189139604568 Validation loss 0.12788307666778564 Accuracy 0.6489999890327454\n",
      "Iteration 16760 Training loss 0.09202688932418823 Validation loss 0.12404341995716095 Accuracy 0.6568333506584167\n",
      "Iteration 16770 Training loss 0.09416137635707855 Validation loss 0.12897522747516632 Accuracy 0.6481666564941406\n",
      "Iteration 16780 Training loss 0.09273038804531097 Validation loss 0.12781186401844025 Accuracy 0.6510000228881836\n",
      "Iteration 16790 Training loss 0.09200593084096909 Validation loss 0.12551867961883545 Accuracy 0.6538333296775818\n",
      "Iteration 16800 Training loss 0.09701371192932129 Validation loss 0.13127665221691132 Accuracy 0.6430000066757202\n",
      "Iteration 16810 Training loss 0.09534058719873428 Validation loss 0.1379164606332779 Accuracy 0.6259999871253967\n",
      "Iteration 16820 Training loss 0.09277398884296417 Validation loss 0.12320756167173386 Accuracy 0.659166693687439\n",
      "Iteration 16830 Training loss 0.09690272808074951 Validation loss 0.1325065940618515 Accuracy 0.6398333311080933\n",
      "Iteration 16840 Training loss 0.0955483466386795 Validation loss 0.13273237645626068 Accuracy 0.6389999985694885\n",
      "Iteration 16850 Training loss 0.09503942728042603 Validation loss 0.1329021155834198 Accuracy 0.6381666660308838\n",
      "Iteration 16860 Training loss 0.09246580302715302 Validation loss 0.12469476461410522 Accuracy 0.6554999947547913\n",
      "Iteration 16870 Training loss 0.09394341707229614 Validation loss 0.12380683422088623 Accuracy 0.6583333611488342\n",
      "Iteration 16880 Training loss 0.09219282865524292 Validation loss 0.12611939013004303 Accuracy 0.6524999737739563\n",
      "Iteration 16890 Training loss 0.0930347591638565 Validation loss 0.12284845113754272 Accuracy 0.656499981880188\n",
      "Iteration 16900 Training loss 0.09787359833717346 Validation loss 0.1392526775598526 Accuracy 0.625\n",
      "Iteration 16910 Training loss 0.09254007041454315 Validation loss 0.1238408237695694 Accuracy 0.6579999923706055\n",
      "Iteration 16920 Training loss 0.09368161112070084 Validation loss 0.13057231903076172 Accuracy 0.6439999938011169\n",
      "Iteration 16930 Training loss 0.09301232546567917 Validation loss 0.11910916864871979 Accuracy 0.6673333048820496\n",
      "Iteration 16940 Training loss 0.09327741712331772 Validation loss 0.11277034133672714 Accuracy 0.6781666874885559\n",
      "Iteration 16950 Training loss 0.09222731739282608 Validation loss 0.11167356371879578 Accuracy 0.6786666512489319\n",
      "Iteration 16960 Training loss 0.09189262986183167 Validation loss 0.1118624359369278 Accuracy 0.6786666512489319\n",
      "Iteration 16970 Training loss 0.09225322306156158 Validation loss 0.11267686635255814 Accuracy 0.6791666746139526\n",
      "Iteration 16980 Training loss 0.09157418459653854 Validation loss 0.11692017316818237 Accuracy 0.6708333492279053\n",
      "Iteration 16990 Training loss 0.0923711359500885 Validation loss 0.11629307270050049 Accuracy 0.6728333234786987\n",
      "Iteration 17000 Training loss 0.09534212201833725 Validation loss 0.12729904055595398 Accuracy 0.6521666646003723\n",
      "Iteration 17010 Training loss 0.09741701930761337 Validation loss 0.13478782773017883 Accuracy 0.6338333487510681\n",
      "Iteration 17020 Training loss 0.0936141237616539 Validation loss 0.12412632256746292 Accuracy 0.6568333506584167\n",
      "Iteration 17030 Training loss 0.09821058064699173 Validation loss 0.13379991054534912 Accuracy 0.6356666684150696\n",
      "Iteration 17040 Training loss 0.09413512051105499 Validation loss 0.12958510220050812 Accuracy 0.6463333368301392\n",
      "Iteration 17050 Training loss 0.09299098700284958 Validation loss 0.12731720507144928 Accuracy 0.6503333449363708\n",
      "Iteration 17060 Training loss 0.0933777466416359 Validation loss 0.12717744708061218 Accuracy 0.6514999866485596\n",
      "Iteration 17070 Training loss 0.09669425338506699 Validation loss 0.13325364887714386 Accuracy 0.637333333492279\n",
      "Iteration 17080 Training loss 0.09576139599084854 Validation loss 0.1331983506679535 Accuracy 0.6365000009536743\n",
      "Iteration 17090 Training loss 0.09107258170843124 Validation loss 0.12307686358690262 Accuracy 0.659333348274231\n",
      "Iteration 17100 Training loss 0.093994140625 Validation loss 0.13310086727142334 Accuracy 0.6365000009536743\n",
      "Iteration 17110 Training loss 0.09077785164117813 Validation loss 0.12649653851985931 Accuracy 0.6528333425521851\n",
      "Iteration 17120 Training loss 0.09885283559560776 Validation loss 0.13887342810630798 Accuracy 0.6243333220481873\n",
      "Iteration 17130 Training loss 0.09365992993116379 Validation loss 0.1244722381234169 Accuracy 0.6554999947547913\n",
      "Iteration 17140 Training loss 0.09542413055896759 Validation loss 0.13137146830558777 Accuracy 0.6439999938011169\n",
      "Iteration 17150 Training loss 0.0924076959490776 Validation loss 0.1243855282664299 Accuracy 0.6566666960716248\n",
      "Iteration 17160 Training loss 0.09242501109838486 Validation loss 0.12858887016773224 Accuracy 0.6495000123977661\n",
      "Iteration 17170 Training loss 0.09539660811424255 Validation loss 0.12858445942401886 Accuracy 0.6501666903495789\n",
      "Iteration 17180 Training loss 0.09499985724687576 Validation loss 0.12971431016921997 Accuracy 0.6493333578109741\n",
      "Iteration 17190 Training loss 0.09248748421669006 Validation loss 0.12420278042554855 Accuracy 0.6553333401679993\n",
      "Iteration 17200 Training loss 0.09550770372152328 Validation loss 0.13483823835849762 Accuracy 0.6343333125114441\n",
      "Iteration 17210 Training loss 0.0948757454752922 Validation loss 0.1305246204137802 Accuracy 0.6456666588783264\n",
      "Iteration 17220 Training loss 0.09250860661268234 Validation loss 0.12096728384494781 Accuracy 0.6625000238418579\n",
      "Iteration 17230 Training loss 0.09286888688802719 Validation loss 0.12050232291221619 Accuracy 0.6623333096504211\n",
      "Iteration 17240 Training loss 0.09624015539884567 Validation loss 0.11228157579898834 Accuracy 0.6796666383743286\n",
      "Iteration 17250 Training loss 0.09547217935323715 Validation loss 0.11244959384202957 Accuracy 0.6783333420753479\n",
      "Iteration 17260 Training loss 0.09464216977357864 Validation loss 0.11194130778312683 Accuracy 0.6809999942779541\n",
      "Iteration 17270 Training loss 0.09366153925657272 Validation loss 0.11160801351070404 Accuracy 0.6794999837875366\n",
      "Iteration 17280 Training loss 0.09466366469860077 Validation loss 0.1118558943271637 Accuracy 0.6803333163261414\n",
      "Iteration 17290 Training loss 0.09357883781194687 Validation loss 0.1122499629855156 Accuracy 0.6779999732971191\n",
      "Iteration 17300 Training loss 0.0942099317908287 Validation loss 0.11188855022192001 Accuracy 0.6801666617393494\n",
      "Iteration 17310 Training loss 0.09374146163463593 Validation loss 0.11195066571235657 Accuracy 0.6793333292007446\n",
      "Iteration 17320 Training loss 0.09143634140491486 Validation loss 0.11376842111349106 Accuracy 0.6781666874885559\n",
      "Iteration 17330 Training loss 0.09474794566631317 Validation loss 0.11198987066745758 Accuracy 0.6821666955947876\n",
      "Iteration 17340 Training loss 0.09561281651258469 Validation loss 0.11167971044778824 Accuracy 0.6815000176429749\n",
      "Iteration 17350 Training loss 0.0918300449848175 Validation loss 0.11197280138731003 Accuracy 0.6800000071525574\n",
      "Iteration 17360 Training loss 0.0940210372209549 Validation loss 0.11200860887765884 Accuracy 0.6784999966621399\n",
      "Iteration 17370 Training loss 0.09529813379049301 Validation loss 0.11153218895196915 Accuracy 0.6801666617393494\n",
      "Iteration 17380 Training loss 0.09355183690786362 Validation loss 0.11183870583772659 Accuracy 0.6801666617393494\n",
      "Iteration 17390 Training loss 0.09331654012203217 Validation loss 0.111471988260746 Accuracy 0.6819999814033508\n",
      "Iteration 17400 Training loss 0.09583107382059097 Validation loss 0.1116781011223793 Accuracy 0.6809999942779541\n",
      "Iteration 17410 Training loss 0.09403327107429504 Validation loss 0.11243594437837601 Accuracy 0.6788333058357239\n",
      "Iteration 17420 Training loss 0.09628541767597198 Validation loss 0.11148498207330704 Accuracy 0.6818333268165588\n",
      "Iteration 17430 Training loss 0.09574522078037262 Validation loss 0.1117582619190216 Accuracy 0.6788333058357239\n",
      "Iteration 17440 Training loss 0.09230069816112518 Validation loss 0.11293772608041763 Accuracy 0.6796666383743286\n",
      "Iteration 17450 Training loss 0.09331201016902924 Validation loss 0.11163140833377838 Accuracy 0.6813333630561829\n",
      "Iteration 17460 Training loss 0.0945446565747261 Validation loss 0.11152166873216629 Accuracy 0.6808333396911621\n",
      "Iteration 17470 Training loss 0.0907915011048317 Validation loss 0.11339052021503448 Accuracy 0.6813333630561829\n",
      "Iteration 17480 Training loss 0.09131338447332382 Validation loss 0.11344737559556961 Accuracy 0.6816666722297668\n",
      "Iteration 17490 Training loss 0.09372729808092117 Validation loss 0.11235833913087845 Accuracy 0.6790000200271606\n",
      "Iteration 17500 Training loss 0.0952434092760086 Validation loss 0.11179182678461075 Accuracy 0.6816666722297668\n",
      "Iteration 17510 Training loss 0.09357751160860062 Validation loss 0.11159311980009079 Accuracy 0.6831666827201843\n",
      "Iteration 17520 Training loss 0.09484147280454636 Validation loss 0.1118125319480896 Accuracy 0.6800000071525574\n",
      "Iteration 17530 Training loss 0.09167396277189255 Validation loss 0.11181765049695969 Accuracy 0.6776666641235352\n",
      "Iteration 17540 Training loss 0.09166180342435837 Validation loss 0.11870377510786057 Accuracy 0.6679999828338623\n",
      "Iteration 17550 Training loss 0.09144193679094315 Validation loss 0.11288396269083023 Accuracy 0.6778333187103271\n",
      "Iteration 17560 Training loss 0.09449850767850876 Validation loss 0.11215485632419586 Accuracy 0.6808333396911621\n",
      "Iteration 17570 Training loss 0.09616772830486298 Validation loss 0.111565500497818 Accuracy 0.6836666464805603\n",
      "Iteration 17580 Training loss 0.09422082453966141 Validation loss 0.11159761250019073 Accuracy 0.6796666383743286\n",
      "Iteration 17590 Training loss 0.0920058935880661 Validation loss 0.11239424347877502 Accuracy 0.6794999837875366\n",
      "Iteration 17600 Training loss 0.09254754334688187 Validation loss 0.11174128949642181 Accuracy 0.6801666617393494\n",
      "Iteration 17610 Training loss 0.09349221736192703 Validation loss 0.111635223031044 Accuracy 0.6825000047683716\n",
      "Iteration 17620 Training loss 0.09344036877155304 Validation loss 0.11324906349182129 Accuracy 0.6813333630561829\n",
      "Iteration 17630 Training loss 0.09118203073740005 Validation loss 0.11311674863100052 Accuracy 0.6809999942779541\n",
      "Iteration 17640 Training loss 0.0945967435836792 Validation loss 0.12436462193727493 Accuracy 0.6553333401679993\n",
      "Iteration 17650 Training loss 0.09330269694328308 Validation loss 0.13347791135311127 Accuracy 0.6341666579246521\n",
      "Iteration 17660 Training loss 0.0928228348493576 Validation loss 0.12437371909618378 Accuracy 0.6539999842643738\n",
      "Iteration 17670 Training loss 0.09114561229944229 Validation loss 0.1310037076473236 Accuracy 0.6460000276565552\n",
      "Iteration 17680 Training loss 0.0967681035399437 Validation loss 0.1381961703300476 Accuracy 0.6255000233650208\n",
      "Iteration 17690 Training loss 0.09316367655992508 Validation loss 0.1265668421983719 Accuracy 0.6521666646003723\n",
      "Iteration 17700 Training loss 0.09244482219219208 Validation loss 0.1329508274793625 Accuracy 0.6383333206176758\n",
      "Iteration 17710 Training loss 0.09281206130981445 Validation loss 0.12742388248443604 Accuracy 0.6513333320617676\n",
      "Iteration 17720 Training loss 0.09238710254430771 Validation loss 0.13045188784599304 Accuracy 0.6460000276565552\n",
      "Iteration 17730 Training loss 0.09383007884025574 Validation loss 0.12758523225784302 Accuracy 0.6521666646003723\n",
      "Iteration 17740 Training loss 0.09612869471311569 Validation loss 0.1344079226255417 Accuracy 0.6346666812896729\n",
      "Iteration 17750 Training loss 0.09216497838497162 Validation loss 0.11936245858669281 Accuracy 0.6676666736602783\n",
      "Iteration 17760 Training loss 0.09173937886953354 Validation loss 0.1274496614933014 Accuracy 0.6520000100135803\n",
      "Iteration 17770 Training loss 0.09332313388586044 Validation loss 0.1285989135503769 Accuracy 0.6488333344459534\n",
      "Iteration 17780 Training loss 0.09526348114013672 Validation loss 0.12812934815883636 Accuracy 0.6520000100135803\n",
      "Iteration 17790 Training loss 0.0959288626909256 Validation loss 0.13225172460079193 Accuracy 0.640999972820282\n",
      "Iteration 17800 Training loss 0.09886910766363144 Validation loss 0.13537658751010895 Accuracy 0.6321666836738586\n",
      "Iteration 17810 Training loss 0.09289147704839706 Validation loss 0.13087935745716095 Accuracy 0.6441666483879089\n",
      "Iteration 17820 Training loss 0.09500116109848022 Validation loss 0.12880513072013855 Accuracy 0.6506666541099548\n",
      "Iteration 17830 Training loss 0.09229319542646408 Validation loss 0.12047851830720901 Accuracy 0.6636666655540466\n",
      "Iteration 17840 Training loss 0.0969599112868309 Validation loss 0.13439004123210907 Accuracy 0.6351666450500488\n",
      "Iteration 17850 Training loss 0.09546485543251038 Validation loss 0.13468000292778015 Accuracy 0.6333333253860474\n",
      "Iteration 17860 Training loss 0.09205932915210724 Validation loss 0.12841525673866272 Accuracy 0.6493333578109741\n",
      "Iteration 17870 Training loss 0.09459051489830017 Validation loss 0.12893031537532806 Accuracy 0.6499999761581421\n",
      "Iteration 17880 Training loss 0.09529361873865128 Validation loss 0.13325636088848114 Accuracy 0.6380000114440918\n",
      "Iteration 17890 Training loss 0.09213275462388992 Validation loss 0.11872843652963638 Accuracy 0.6669999957084656\n",
      "Iteration 17900 Training loss 0.09311304986476898 Validation loss 0.11339406669139862 Accuracy 0.6794999837875366\n",
      "Iteration 17910 Training loss 0.09470459073781967 Validation loss 0.11187203973531723 Accuracy 0.6816666722297668\n",
      "Iteration 17920 Training loss 0.09263405203819275 Validation loss 0.11214644461870193 Accuracy 0.6800000071525574\n",
      "Iteration 17930 Training loss 0.09354579448699951 Validation loss 0.11216162145137787 Accuracy 0.6801666617393494\n",
      "Iteration 17940 Training loss 0.09642962366342545 Validation loss 0.11176637560129166 Accuracy 0.6808333396911621\n",
      "Iteration 17950 Training loss 0.09272841364145279 Validation loss 0.11248703300952911 Accuracy 0.6794999837875366\n",
      "Iteration 17960 Training loss 0.094143807888031 Validation loss 0.1118413582444191 Accuracy 0.6826666593551636\n",
      "Iteration 17970 Training loss 0.09395935386419296 Validation loss 0.11195679754018784 Accuracy 0.6800000071525574\n",
      "Iteration 17980 Training loss 0.09261572360992432 Validation loss 0.11241570115089417 Accuracy 0.6793333292007446\n",
      "Iteration 17990 Training loss 0.09368835389614105 Validation loss 0.11196208745241165 Accuracy 0.6816666722297668\n",
      "Iteration 18000 Training loss 0.09853158891201019 Validation loss 0.11226648092269897 Accuracy 0.6809999942779541\n",
      "Iteration 18010 Training loss 0.09313614666461945 Validation loss 0.1123647540807724 Accuracy 0.6803333163261414\n",
      "Iteration 18020 Training loss 0.09355462342500687 Validation loss 0.11235889792442322 Accuracy 0.6794999837875366\n",
      "Iteration 18030 Training loss 0.09160447120666504 Validation loss 0.11558370292186737 Accuracy 0.6743333339691162\n",
      "Iteration 18040 Training loss 0.09144124388694763 Validation loss 0.1147698163986206 Accuracy 0.6761666536331177\n",
      "Iteration 18050 Training loss 0.09615004807710648 Validation loss 0.11187270283699036 Accuracy 0.6809999942779541\n",
      "Iteration 18060 Training loss 0.09603564441204071 Validation loss 0.11201991885900497 Accuracy 0.6798333525657654\n",
      "Iteration 18070 Training loss 0.09374573081731796 Validation loss 0.11223193258047104 Accuracy 0.6815000176429749\n",
      "Iteration 18080 Training loss 0.09362133592367172 Validation loss 0.11217974871397018 Accuracy 0.6804999709129333\n",
      "Iteration 18090 Training loss 0.09706616401672363 Validation loss 0.1116328090429306 Accuracy 0.6825000047683716\n",
      "Iteration 18100 Training loss 0.09745794534683228 Validation loss 0.11171085387468338 Accuracy 0.6815000176429749\n",
      "Iteration 18110 Training loss 0.09183608740568161 Validation loss 0.1128835678100586 Accuracy 0.6798333525657654\n",
      "Iteration 18120 Training loss 0.09321112185716629 Validation loss 0.11193639785051346 Accuracy 0.6811666488647461\n",
      "Iteration 18130 Training loss 0.09334525465965271 Validation loss 0.11636865884065628 Accuracy 0.6744999885559082\n",
      "Iteration 18140 Training loss 0.09283938258886337 Validation loss 0.11310601979494095 Accuracy 0.6791666746139526\n",
      "Iteration 18150 Training loss 0.09261716902256012 Validation loss 0.11207958310842514 Accuracy 0.6815000176429749\n",
      "Iteration 18160 Training loss 0.09273272752761841 Validation loss 0.11181679368019104 Accuracy 0.6815000176429749\n",
      "Iteration 18170 Training loss 0.09131045639514923 Validation loss 0.1119566410779953 Accuracy 0.6811666488647461\n",
      "Iteration 18180 Training loss 0.09330473840236664 Validation loss 0.1122722178697586 Accuracy 0.6804999709129333\n",
      "Iteration 18190 Training loss 0.09365154802799225 Validation loss 0.11251149326562881 Accuracy 0.6794999837875366\n",
      "Iteration 18200 Training loss 0.09603271633386612 Validation loss 0.11165551096200943 Accuracy 0.6809999942779541\n",
      "Iteration 18210 Training loss 0.09643950313329697 Validation loss 0.1116214469075203 Accuracy 0.6806666851043701\n",
      "Iteration 18220 Training loss 0.09338602423667908 Validation loss 0.11415033042430878 Accuracy 0.6759999990463257\n",
      "Iteration 18230 Training loss 0.09389849007129669 Validation loss 0.11214880645275116 Accuracy 0.6801666617393494\n",
      "Iteration 18240 Training loss 0.09398628026247025 Validation loss 0.11191444844007492 Accuracy 0.6811666488647461\n",
      "Iteration 18250 Training loss 0.09459535777568817 Validation loss 0.11211752146482468 Accuracy 0.6840000152587891\n",
      "Iteration 18260 Training loss 0.09480895102024078 Validation loss 0.11175739765167236 Accuracy 0.6816666722297668\n",
      "Iteration 18270 Training loss 0.09270544350147247 Validation loss 0.11156752705574036 Accuracy 0.6819999814033508\n",
      "Iteration 18280 Training loss 0.09341230988502502 Validation loss 0.1122429296374321 Accuracy 0.6796666383743286\n",
      "Iteration 18290 Training loss 0.09400101006031036 Validation loss 0.11171317845582962 Accuracy 0.6821666955947876\n",
      "Iteration 18300 Training loss 0.09326918423175812 Validation loss 0.11292007565498352 Accuracy 0.6791666746139526\n",
      "Iteration 18310 Training loss 0.09557420015335083 Validation loss 0.1117452085018158 Accuracy 0.6813333630561829\n",
      "Iteration 18320 Training loss 0.09553667902946472 Validation loss 0.11162231117486954 Accuracy 0.6813333630561829\n",
      "Iteration 18330 Training loss 0.09076754748821259 Validation loss 0.1126864105463028 Accuracy 0.6791666746139526\n",
      "Iteration 18340 Training loss 0.09036561846733093 Validation loss 0.1157073900103569 Accuracy 0.6779999732971191\n",
      "Iteration 18350 Training loss 0.09329831600189209 Validation loss 0.11893501877784729 Accuracy 0.6690000295639038\n",
      "Iteration 18360 Training loss 0.09714791923761368 Validation loss 0.13791424036026 Accuracy 0.6263333559036255\n",
      "Iteration 18370 Training loss 0.09422151744365692 Validation loss 0.12857677042484283 Accuracy 0.6486666798591614\n",
      "Iteration 18380 Training loss 0.0941949337720871 Validation loss 0.13650132715702057 Accuracy 0.628333330154419\n",
      "Iteration 18390 Training loss 0.09276045858860016 Validation loss 0.12492980808019638 Accuracy 0.6556666493415833\n",
      "Iteration 18400 Training loss 0.0944603681564331 Validation loss 0.12653951346874237 Accuracy 0.6536666750907898\n",
      "Iteration 18410 Training loss 0.0937294214963913 Validation loss 0.13563592731952667 Accuracy 0.6320000290870667\n",
      "Iteration 18420 Training loss 0.09660641849040985 Validation loss 0.13784615695476532 Accuracy 0.6273333430290222\n",
      "Iteration 18430 Training loss 0.09205283969640732 Validation loss 0.12698805332183838 Accuracy 0.652999997138977\n",
      "Iteration 18440 Training loss 0.09424255043268204 Validation loss 0.13012266159057617 Accuracy 0.6483333110809326\n",
      "Iteration 18450 Training loss 0.09065351635217667 Validation loss 0.11960136145353317 Accuracy 0.6663333177566528\n",
      "Iteration 18460 Training loss 0.0908798798918724 Validation loss 0.12278471142053604 Accuracy 0.6600000262260437\n",
      "Iteration 18470 Training loss 0.09326619654893875 Validation loss 0.1245071217417717 Accuracy 0.6554999947547913\n",
      "Iteration 18480 Training loss 0.09692405164241791 Validation loss 0.1342519074678421 Accuracy 0.6366666555404663\n",
      "Iteration 18490 Training loss 0.09391617774963379 Validation loss 0.13196668028831482 Accuracy 0.6431666612625122\n",
      "Iteration 18500 Training loss 0.09123598039150238 Validation loss 0.12740936875343323 Accuracy 0.6506666541099548\n",
      "Iteration 18510 Training loss 0.09421102702617645 Validation loss 0.12749014794826508 Accuracy 0.6524999737739563\n",
      "Iteration 18520 Training loss 0.09496423602104187 Validation loss 0.1295022815465927 Accuracy 0.6481666564941406\n",
      "Iteration 18530 Training loss 0.09484312683343887 Validation loss 0.13384930789470673 Accuracy 0.6351666450500488\n",
      "Iteration 18540 Training loss 0.09214945137500763 Validation loss 0.13014855980873108 Accuracy 0.6466666460037231\n",
      "Iteration 18550 Training loss 0.09354902803897858 Validation loss 0.13171696662902832 Accuracy 0.6426666378974915\n",
      "Iteration 18560 Training loss 0.09240923821926117 Validation loss 0.13138486444950104 Accuracy 0.643833339214325\n",
      "Iteration 18570 Training loss 0.09393450617790222 Validation loss 0.1250688135623932 Accuracy 0.6554999947547913\n",
      "Iteration 18580 Training loss 0.09535165131092072 Validation loss 0.1323118954896927 Accuracy 0.6416666507720947\n",
      "Iteration 18590 Training loss 0.0941147655248642 Validation loss 0.12546087801456451 Accuracy 0.6551666855812073\n",
      "Iteration 18600 Training loss 0.09219394624233246 Validation loss 0.1323298215866089 Accuracy 0.643833339214325\n",
      "Iteration 18610 Training loss 0.09142536669969559 Validation loss 0.11722394824028015 Accuracy 0.6704999804496765\n",
      "Iteration 18620 Training loss 0.09220351278781891 Validation loss 0.12594974040985107 Accuracy 0.6541666388511658\n",
      "Iteration 18630 Training loss 0.09257519245147705 Validation loss 0.12406215071678162 Accuracy 0.6570000052452087\n",
      "Iteration 18640 Training loss 0.0982704907655716 Validation loss 0.13551592826843262 Accuracy 0.6338333487510681\n",
      "Iteration 18650 Training loss 0.0927605926990509 Validation loss 0.12677103281021118 Accuracy 0.6538333296775818\n",
      "Iteration 18660 Training loss 0.09411666542291641 Validation loss 0.13399936258792877 Accuracy 0.6368333101272583\n",
      "Iteration 18670 Training loss 0.09568820893764496 Validation loss 0.13121086359024048 Accuracy 0.6460000276565552\n",
      "Iteration 18680 Training loss 0.09555632621049881 Validation loss 0.13077472150325775 Accuracy 0.6458333134651184\n",
      "Iteration 18690 Training loss 0.09140799939632416 Validation loss 0.12273877114057541 Accuracy 0.6608333587646484\n",
      "Iteration 18700 Training loss 0.09422998875379562 Validation loss 0.12741020321846008 Accuracy 0.6518333554267883\n",
      "Iteration 18710 Training loss 0.0921831801533699 Validation loss 0.12752582132816315 Accuracy 0.6518333554267883\n",
      "Iteration 18720 Training loss 0.09575840085744858 Validation loss 0.13251282274723053 Accuracy 0.64083331823349\n",
      "Iteration 18730 Training loss 0.09602337330579758 Validation loss 0.1378321349620819 Accuracy 0.6263333559036255\n",
      "Iteration 18740 Training loss 0.09268868714570999 Validation loss 0.1308639794588089 Accuracy 0.6455000042915344\n",
      "Iteration 18750 Training loss 0.09033578634262085 Validation loss 0.12635894119739532 Accuracy 0.6545000076293945\n",
      "Iteration 18760 Training loss 0.09311892837285995 Validation loss 0.12545379996299744 Accuracy 0.6554999947547913\n",
      "Iteration 18770 Training loss 0.09245355427265167 Validation loss 0.12821927666664124 Accuracy 0.6499999761581421\n",
      "Iteration 18780 Training loss 0.09207247942686081 Validation loss 0.12392520904541016 Accuracy 0.6576666831970215\n",
      "Iteration 18790 Training loss 0.09169543534517288 Validation loss 0.1252039223909378 Accuracy 0.6548333168029785\n",
      "Iteration 18800 Training loss 0.09036915004253387 Validation loss 0.1226113885641098 Accuracy 0.6631666421890259\n",
      "Iteration 18810 Training loss 0.09395261853933334 Validation loss 0.12969113886356354 Accuracy 0.6486666798591614\n",
      "Iteration 18820 Training loss 0.09258372336626053 Validation loss 0.12594525516033173 Accuracy 0.6543333530426025\n",
      "Iteration 18830 Training loss 0.09621236473321915 Validation loss 0.13558317720890045 Accuracy 0.6303333044052124\n",
      "Iteration 18840 Training loss 0.09238902479410172 Validation loss 0.13056835532188416 Accuracy 0.6468333601951599\n",
      "Iteration 18850 Training loss 0.09463383257389069 Validation loss 0.13675543665885925 Accuracy 0.6293333172798157\n",
      "Iteration 18860 Training loss 0.09504911303520203 Validation loss 0.13384930789470673 Accuracy 0.6349999904632568\n",
      "Iteration 18870 Training loss 0.09391336143016815 Validation loss 0.12985701858997345 Accuracy 0.6485000252723694\n",
      "Iteration 18880 Training loss 0.09410051256418228 Validation loss 0.12700305879116058 Accuracy 0.6524999737739563\n",
      "Iteration 18890 Training loss 0.09039677679538727 Validation loss 0.1201152354478836 Accuracy 0.6650000214576721\n",
      "Iteration 18900 Training loss 0.0908423438668251 Validation loss 0.11904317885637283 Accuracy 0.6675000190734863\n",
      "Iteration 18910 Training loss 0.09267484396696091 Validation loss 0.1162964478135109 Accuracy 0.6765000224113464\n",
      "Iteration 18920 Training loss 0.09335143119096756 Validation loss 0.11470086872577667 Accuracy 0.6776666641235352\n",
      "Iteration 18930 Training loss 0.0939440131187439 Validation loss 0.11226648837327957 Accuracy 0.6831666827201843\n",
      "Iteration 18940 Training loss 0.09449868649244308 Validation loss 0.1116439551115036 Accuracy 0.6828333139419556\n",
      "Iteration 18950 Training loss 0.09194283187389374 Validation loss 0.11328863352537155 Accuracy 0.6803333163261414\n",
      "Iteration 18960 Training loss 0.09426239877939224 Validation loss 0.11241684854030609 Accuracy 0.6806666851043701\n",
      "Iteration 18970 Training loss 0.09261775761842728 Validation loss 0.11387339234352112 Accuracy 0.6801666617393494\n",
      "Iteration 18980 Training loss 0.09495657682418823 Validation loss 0.11177057027816772 Accuracy 0.6809999942779541\n",
      "Iteration 18990 Training loss 0.08841747790575027 Validation loss 0.11226876825094223 Accuracy 0.6818333268165588\n",
      "Iteration 19000 Training loss 0.0937376320362091 Validation loss 0.11202723532915115 Accuracy 0.6806666851043701\n",
      "Iteration 19010 Training loss 0.09487978368997574 Validation loss 0.11177118867635727 Accuracy 0.6813333630561829\n",
      "Iteration 19020 Training loss 0.0913684293627739 Validation loss 0.11240454763174057 Accuracy 0.6818333268165588\n",
      "Iteration 19030 Training loss 0.09219209104776382 Validation loss 0.1116529032588005 Accuracy 0.6811666488647461\n",
      "Iteration 19040 Training loss 0.09186053276062012 Validation loss 0.11309875547885895 Accuracy 0.6823333501815796\n",
      "Iteration 19050 Training loss 0.09185782819986343 Validation loss 0.11284061521291733 Accuracy 0.6809999942779541\n",
      "Iteration 19060 Training loss 0.09108413010835648 Validation loss 0.11348327994346619 Accuracy 0.6796666383743286\n",
      "Iteration 19070 Training loss 0.09124699234962463 Validation loss 0.11499582231044769 Accuracy 0.6766666769981384\n",
      "Iteration 19080 Training loss 0.09231943637132645 Validation loss 0.12079266458749771 Accuracy 0.6666666865348816\n",
      "Iteration 19090 Training loss 0.0963670164346695 Validation loss 0.141813725233078 Accuracy 0.6190000176429749\n",
      "Iteration 19100 Training loss 0.09220941364765167 Validation loss 0.12888212502002716 Accuracy 0.6486666798591614\n",
      "Iteration 19110 Training loss 0.09445950388908386 Validation loss 0.13363197445869446 Accuracy 0.6386666893959045\n",
      "Iteration 19120 Training loss 0.09360460191965103 Validation loss 0.12681907415390015 Accuracy 0.6535000205039978\n",
      "Iteration 19130 Training loss 0.09206351637840271 Validation loss 0.13427239656448364 Accuracy 0.6361666917800903\n",
      "Iteration 19140 Training loss 0.09207726269960403 Validation loss 0.12840008735656738 Accuracy 0.6518333554267883\n",
      "Iteration 19150 Training loss 0.0934610515832901 Validation loss 0.13367854058742523 Accuracy 0.6380000114440918\n",
      "Iteration 19160 Training loss 0.0908392146229744 Validation loss 0.1260555386543274 Accuracy 0.653166651725769\n",
      "Iteration 19170 Training loss 0.094696044921875 Validation loss 0.12630105018615723 Accuracy 0.6539999842643738\n",
      "Iteration 19180 Training loss 0.09102145582437515 Validation loss 0.1210462674498558 Accuracy 0.6663333177566528\n",
      "Iteration 19190 Training loss 0.09656212478876114 Validation loss 0.13800585269927979 Accuracy 0.6263333559036255\n",
      "Iteration 19200 Training loss 0.0910247191786766 Validation loss 0.12690721452236176 Accuracy 0.6518333554267883\n",
      "Iteration 19210 Training loss 0.09729316085577011 Validation loss 0.13579759001731873 Accuracy 0.6330000162124634\n",
      "Iteration 19220 Training loss 0.09277165681123734 Validation loss 0.12723539769649506 Accuracy 0.6511666774749756\n",
      "Iteration 19230 Training loss 0.09128877520561218 Validation loss 0.12733091413974762 Accuracy 0.6510000228881836\n",
      "Iteration 19240 Training loss 0.09014232456684113 Validation loss 0.12646925449371338 Accuracy 0.6528333425521851\n",
      "Iteration 19250 Training loss 0.09292228519916534 Validation loss 0.13138896226882935 Accuracy 0.6449999809265137\n",
      "Iteration 19260 Training loss 0.09318289905786514 Validation loss 0.13026274740695953 Accuracy 0.6464999914169312\n",
      "Iteration 19270 Training loss 0.09086555987596512 Validation loss 0.1157311424612999 Accuracy 0.6781666874885559\n",
      "Iteration 19280 Training loss 0.092233806848526 Validation loss 0.1132061630487442 Accuracy 0.6801666617393494\n",
      "Iteration 19290 Training loss 0.09424711018800735 Validation loss 0.11181707680225372 Accuracy 0.6823333501815796\n",
      "Iteration 19300 Training loss 0.09204603731632233 Validation loss 0.11174620687961578 Accuracy 0.6821666955947876\n",
      "Iteration 19310 Training loss 0.09156423062086105 Validation loss 0.11260829865932465 Accuracy 0.6813333630561829\n",
      "Iteration 19320 Training loss 0.09142380952835083 Validation loss 0.11323641240596771 Accuracy 0.6801666617393494\n",
      "Iteration 19330 Training loss 0.09553845226764679 Validation loss 0.11214204877614975 Accuracy 0.6813333630561829\n",
      "Iteration 19340 Training loss 0.08988741040229797 Validation loss 0.11237365007400513 Accuracy 0.6825000047683716\n",
      "Iteration 19350 Training loss 0.0949818342924118 Validation loss 0.11192040890455246 Accuracy 0.6819999814033508\n",
      "Iteration 19360 Training loss 0.09277279675006866 Validation loss 0.11378208547830582 Accuracy 0.6786666512489319\n",
      "Iteration 19370 Training loss 0.09102675318717957 Validation loss 0.1140647679567337 Accuracy 0.6791666746139526\n",
      "Iteration 19380 Training loss 0.09511467814445496 Validation loss 0.11211782693862915 Accuracy 0.6804999709129333\n",
      "Iteration 19390 Training loss 0.09138079732656479 Validation loss 0.11369555443525314 Accuracy 0.6788333058357239\n",
      "Iteration 19400 Training loss 0.0943608209490776 Validation loss 0.11198651045560837 Accuracy 0.6800000071525574\n",
      "Iteration 19410 Training loss 0.09430499374866486 Validation loss 0.11206239461898804 Accuracy 0.6813333630561829\n",
      "Iteration 19420 Training loss 0.094294972717762 Validation loss 0.11221478134393692 Accuracy 0.6819999814033508\n",
      "Iteration 19430 Training loss 0.09212233126163483 Validation loss 0.11235351860523224 Accuracy 0.6816666722297668\n",
      "Iteration 19440 Training loss 0.09169761091470718 Validation loss 0.11220937222242355 Accuracy 0.6819999814033508\n",
      "Iteration 19450 Training loss 0.09339085966348648 Validation loss 0.11227194964885712 Accuracy 0.6830000281333923\n",
      "Iteration 19460 Training loss 0.09412728995084763 Validation loss 0.11196012049913406 Accuracy 0.6838333606719971\n",
      "Iteration 19470 Training loss 0.09644173085689545 Validation loss 0.11186476796865463 Accuracy 0.6803333163261414\n",
      "Iteration 19480 Training loss 0.08929481357336044 Validation loss 0.11428120732307434 Accuracy 0.6775000095367432\n",
      "Iteration 19490 Training loss 0.09194748848676682 Validation loss 0.11329348385334015 Accuracy 0.6809999942779541\n",
      "Iteration 19500 Training loss 0.0933547392487526 Validation loss 0.1118319109082222 Accuracy 0.6819999814033508\n",
      "Iteration 19510 Training loss 0.09212329983711243 Validation loss 0.11175739765167236 Accuracy 0.6825000047683716\n",
      "Iteration 19520 Training loss 0.0912976861000061 Validation loss 0.11291910707950592 Accuracy 0.6821666955947876\n",
      "Iteration 19530 Training loss 0.0914030522108078 Validation loss 0.12042790651321411 Accuracy 0.6666666865348816\n",
      "Iteration 19540 Training loss 0.09289014339447021 Validation loss 0.1291709542274475 Accuracy 0.6499999761581421\n",
      "Iteration 19550 Training loss 0.0908907875418663 Validation loss 0.1287112683057785 Accuracy 0.6510000228881836\n",
      "Iteration 19560 Training loss 0.09269023686647415 Validation loss 0.13128457963466644 Accuracy 0.6466666460037231\n",
      "Iteration 19570 Training loss 0.09311392903327942 Validation loss 0.12975509464740753 Accuracy 0.6471666693687439\n",
      "Iteration 19580 Training loss 0.09402786940336227 Validation loss 0.1321706920862198 Accuracy 0.6416666507720947\n",
      "Iteration 19590 Training loss 0.09183358401060104 Validation loss 0.1242511197924614 Accuracy 0.6578333377838135\n",
      "Iteration 19600 Training loss 0.09287064522504807 Validation loss 0.11618722230195999 Accuracy 0.6763333082199097\n",
      "Iteration 19610 Training loss 0.09193968027830124 Validation loss 0.12568369507789612 Accuracy 0.656499981880188\n",
      "Iteration 19620 Training loss 0.09350927919149399 Validation loss 0.13370271027088165 Accuracy 0.6393333077430725\n",
      "Iteration 19630 Training loss 0.09790311008691788 Validation loss 0.13920825719833374 Accuracy 0.6261666417121887\n",
      "Iteration 19640 Training loss 0.09292905777692795 Validation loss 0.13045339286327362 Accuracy 0.6461666822433472\n",
      "Iteration 19650 Training loss 0.09015260636806488 Validation loss 0.11991971731185913 Accuracy 0.6661666631698608\n",
      "Iteration 19660 Training loss 0.09308107197284698 Validation loss 0.12520310282707214 Accuracy 0.6585000157356262\n",
      "Iteration 19670 Training loss 0.09597796946763992 Validation loss 0.13698548078536987 Accuracy 0.6303333044052124\n",
      "Iteration 19680 Training loss 0.09277022629976273 Validation loss 0.12740068137645721 Accuracy 0.6520000100135803\n",
      "Iteration 19690 Training loss 0.09394817799329758 Validation loss 0.13607437908649445 Accuracy 0.6318333148956299\n",
      "Iteration 19700 Training loss 0.09449651837348938 Validation loss 0.12682285904884338 Accuracy 0.6523333191871643\n",
      "Iteration 19710 Training loss 0.09236094355583191 Validation loss 0.12876935303211212 Accuracy 0.6499999761581421\n",
      "Iteration 19720 Training loss 0.09568949043750763 Validation loss 0.13255254924297333 Accuracy 0.6430000066757202\n",
      "Iteration 19730 Training loss 0.09239430725574493 Validation loss 0.12568031251430511 Accuracy 0.6546666622161865\n",
      "Iteration 19740 Training loss 0.09095174074172974 Validation loss 0.1158558577299118 Accuracy 0.6758333444595337\n",
      "Iteration 19750 Training loss 0.09229293465614319 Validation loss 0.12085351347923279 Accuracy 0.6648333072662354\n",
      "Iteration 19760 Training loss 0.09141584485769272 Validation loss 0.1295807957649231 Accuracy 0.6498333215713501\n",
      "Iteration 19770 Training loss 0.09236320108175278 Validation loss 0.13144131004810333 Accuracy 0.6443333625793457\n",
      "Iteration 19780 Training loss 0.0941765159368515 Validation loss 0.13291992247104645 Accuracy 0.6388333439826965\n",
      "Iteration 19790 Training loss 0.09358914196491241 Validation loss 0.13356932997703552 Accuracy 0.6388333439826965\n",
      "Iteration 19800 Training loss 0.09483256191015244 Validation loss 0.1340482234954834 Accuracy 0.637333333492279\n",
      "Iteration 19810 Training loss 0.09362515062093735 Validation loss 0.1327122151851654 Accuracy 0.6393333077430725\n",
      "Iteration 19820 Training loss 0.08979004621505737 Validation loss 0.12097273766994476 Accuracy 0.6658333539962769\n",
      "Iteration 19830 Training loss 0.08937562257051468 Validation loss 0.11674661189317703 Accuracy 0.6744999885559082\n",
      "Iteration 19840 Training loss 0.08717126399278641 Validation loss 0.1153859868645668 Accuracy 0.6784999966621399\n",
      "Iteration 19850 Training loss 0.09535400569438934 Validation loss 0.11233320087194443 Accuracy 0.6819999814033508\n",
      "Iteration 19860 Training loss 0.09250082075595856 Validation loss 0.11258850246667862 Accuracy 0.6831666827201843\n",
      "Iteration 19870 Training loss 0.09115971624851227 Validation loss 0.11235810071229935 Accuracy 0.6819999814033508\n",
      "Iteration 19880 Training loss 0.09395941346883774 Validation loss 0.11195501685142517 Accuracy 0.6816666722297668\n",
      "Iteration 19890 Training loss 0.09036777913570404 Validation loss 0.11744269728660583 Accuracy 0.6723333597183228\n",
      "Iteration 19900 Training loss 0.0926177054643631 Validation loss 0.11312536895275116 Accuracy 0.6806666851043701\n",
      "Iteration 19910 Training loss 0.0916600152850151 Validation loss 0.12375300377607346 Accuracy 0.6581666469573975\n",
      "Iteration 19920 Training loss 0.0943056270480156 Validation loss 0.1312868893146515 Accuracy 0.6453333497047424\n",
      "Iteration 19930 Training loss 0.09071715176105499 Validation loss 0.12100653350353241 Accuracy 0.6650000214576721\n",
      "Iteration 19940 Training loss 0.09192159026861191 Validation loss 0.11930928379297256 Accuracy 0.6673333048820496\n",
      "Iteration 19950 Training loss 0.09177809953689575 Validation loss 0.1161375343799591 Accuracy 0.675166666507721\n",
      "Iteration 19960 Training loss 0.09290211647748947 Validation loss 0.1178300604224205 Accuracy 0.671999990940094\n",
      "Iteration 19970 Training loss 0.10123895108699799 Validation loss 0.11420696973800659 Accuracy 0.6754999756813049\n",
      "Iteration 19980 Training loss 0.0961814671754837 Validation loss 0.11177823692560196 Accuracy 0.6813333630561829\n",
      "Iteration 19990 Training loss 0.09488774836063385 Validation loss 0.11196529120206833 Accuracy 0.6804999709129333\n",
      "Iteration 20000 Training loss 0.0966411828994751 Validation loss 0.11202307045459747 Accuracy 0.6794999837875366\n",
      "Iteration 20010 Training loss 0.09450815618038177 Validation loss 0.11208786815404892 Accuracy 0.684333324432373\n",
      "Iteration 20020 Training loss 0.09184527397155762 Validation loss 0.1118495985865593 Accuracy 0.6815000176429749\n",
      "Iteration 20030 Training loss 0.09197424352169037 Validation loss 0.11182841658592224 Accuracy 0.6821666955947876\n",
      "Iteration 20040 Training loss 0.0921163335442543 Validation loss 0.11234776675701141 Accuracy 0.6828333139419556\n",
      "Iteration 20050 Training loss 0.0931006669998169 Validation loss 0.11194835603237152 Accuracy 0.6838333606719971\n",
      "Iteration 20060 Training loss 0.09148361533880234 Validation loss 0.11220384389162064 Accuracy 0.6840000152587891\n",
      "Iteration 20070 Training loss 0.09353218227624893 Validation loss 0.11193624138832092 Accuracy 0.6826666593551636\n",
      "Iteration 20080 Training loss 0.09994865208864212 Validation loss 0.11255177855491638 Accuracy 0.6811666488647461\n",
      "Iteration 20090 Training loss 0.09115622192621231 Validation loss 0.11264971643686295 Accuracy 0.6836666464805603\n",
      "Iteration 20100 Training loss 0.09259241074323654 Validation loss 0.1122734546661377 Accuracy 0.6828333139419556\n",
      "Iteration 20110 Training loss 0.0936468318104744 Validation loss 0.11231966316699982 Accuracy 0.6821666955947876\n",
      "Iteration 20120 Training loss 0.09516185522079468 Validation loss 0.11194068193435669 Accuracy 0.6818333268165588\n",
      "Iteration 20130 Training loss 0.0941653847694397 Validation loss 0.11178892850875854 Accuracy 0.6828333139419556\n",
      "Iteration 20140 Training loss 0.09334854036569595 Validation loss 0.11269687861204147 Accuracy 0.6826666593551636\n",
      "Iteration 20150 Training loss 0.09425423294305801 Validation loss 0.11181344836950302 Accuracy 0.6828333139419556\n",
      "Iteration 20160 Training loss 0.0927555114030838 Validation loss 0.11196774244308472 Accuracy 0.684499979019165\n",
      "Iteration 20170 Training loss 0.09342338889837265 Validation loss 0.11210290342569351 Accuracy 0.6840000152587891\n",
      "Iteration 20180 Training loss 0.0919824093580246 Validation loss 0.11334589868783951 Accuracy 0.6798333525657654\n",
      "Iteration 20190 Training loss 0.09518799185752869 Validation loss 0.1122954934835434 Accuracy 0.6811666488647461\n",
      "Iteration 20200 Training loss 0.09142884612083435 Validation loss 0.11227541416883469 Accuracy 0.6809999942779541\n",
      "Iteration 20210 Training loss 0.09317482262849808 Validation loss 0.1120133027434349 Accuracy 0.6834999918937683\n",
      "Iteration 20220 Training loss 0.09245184063911438 Validation loss 0.1121196374297142 Accuracy 0.6834999918937683\n",
      "Iteration 20230 Training loss 0.08997365087270737 Validation loss 0.11226093769073486 Accuracy 0.6825000047683716\n",
      "Iteration 20240 Training loss 0.09356257319450378 Validation loss 0.11188514530658722 Accuracy 0.6833333373069763\n",
      "Iteration 20250 Training loss 0.0902707502245903 Validation loss 0.11339502036571503 Accuracy 0.6811666488647461\n",
      "Iteration 20260 Training loss 0.09191165864467621 Validation loss 0.11294791847467422 Accuracy 0.6819999814033508\n",
      "Iteration 20270 Training loss 0.09479394555091858 Validation loss 0.11194293200969696 Accuracy 0.6819999814033508\n",
      "Iteration 20280 Training loss 0.09327109903097153 Validation loss 0.11221867799758911 Accuracy 0.6848333477973938\n",
      "Iteration 20290 Training loss 0.09415324777364731 Validation loss 0.11182340234518051 Accuracy 0.6806666851043701\n",
      "Iteration 20300 Training loss 0.09030360728502274 Validation loss 0.11401504278182983 Accuracy 0.6793333292007446\n",
      "Iteration 20310 Training loss 0.09394460171461105 Validation loss 0.11166105419397354 Accuracy 0.6833333373069763\n",
      "Iteration 20320 Training loss 0.09038064628839493 Validation loss 0.11348748952150345 Accuracy 0.6806666851043701\n",
      "Iteration 20330 Training loss 0.09242769330739975 Validation loss 0.11249037832021713 Accuracy 0.6848333477973938\n",
      "Iteration 20340 Training loss 0.09037142992019653 Validation loss 0.11709639430046082 Accuracy 0.6756666898727417\n",
      "Iteration 20350 Training loss 0.09178052097558975 Validation loss 0.11997957527637482 Accuracy 0.6678333282470703\n",
      "Iteration 20360 Training loss 0.09088863432407379 Validation loss 0.1299820989370346 Accuracy 0.6496666669845581\n",
      "Iteration 20370 Training loss 0.09184568375349045 Validation loss 0.13096222281455994 Accuracy 0.6448333263397217\n",
      "Iteration 20380 Training loss 0.09279540181159973 Validation loss 0.1280469447374344 Accuracy 0.6514999866485596\n",
      "Iteration 20390 Training loss 0.09446419030427933 Validation loss 0.13436609506607056 Accuracy 0.6371666789054871\n",
      "Iteration 20400 Training loss 0.09384826570749283 Validation loss 0.13260690867900848 Accuracy 0.6413333415985107\n",
      "Iteration 20410 Training loss 0.09135404229164124 Validation loss 0.13149896264076233 Accuracy 0.6449999809265137\n",
      "Iteration 20420 Training loss 0.09254583716392517 Validation loss 0.1287999451160431 Accuracy 0.6503333449363708\n",
      "Iteration 20430 Training loss 0.09233610332012177 Validation loss 0.12483789771795273 Accuracy 0.6575000286102295\n",
      "Iteration 20440 Training loss 0.0954456552863121 Validation loss 0.1318570375442505 Accuracy 0.6443333625793457\n",
      "Iteration 20450 Training loss 0.0938468798995018 Validation loss 0.13768945634365082 Accuracy 0.6288333535194397\n",
      "Iteration 20460 Training loss 0.09199237823486328 Validation loss 0.12308607995510101 Accuracy 0.6608333587646484\n",
      "Iteration 20470 Training loss 0.09025191515684128 Validation loss 0.12393726408481598 Accuracy 0.6603333353996277\n",
      "Iteration 20480 Training loss 0.09064890444278717 Validation loss 0.12219134718179703 Accuracy 0.6631666421890259\n",
      "Iteration 20490 Training loss 0.09023962169885635 Validation loss 0.11551566421985626 Accuracy 0.6775000095367432\n",
      "Iteration 20500 Training loss 0.09180843085050583 Validation loss 0.12876974046230316 Accuracy 0.6496666669845581\n",
      "Iteration 20510 Training loss 0.0945381298661232 Validation loss 0.13374151289463043 Accuracy 0.637499988079071\n",
      "Iteration 20520 Training loss 0.09151230752468109 Validation loss 0.1292952448129654 Accuracy 0.6493333578109741\n",
      "Iteration 20530 Training loss 0.09394890069961548 Validation loss 0.1266433447599411 Accuracy 0.652999997138977\n",
      "Iteration 20540 Training loss 0.09306878596544266 Validation loss 0.1297546923160553 Accuracy 0.6486666798591614\n",
      "Iteration 20550 Training loss 0.0916856899857521 Validation loss 0.12813466787338257 Accuracy 0.6511666774749756\n",
      "Iteration 20560 Training loss 0.09198504686355591 Validation loss 0.1315392255783081 Accuracy 0.6445000171661377\n",
      "Iteration 20570 Training loss 0.09225276857614517 Validation loss 0.12558530271053314 Accuracy 0.6541666388511658\n",
      "Iteration 20580 Training loss 0.09192637354135513 Validation loss 0.12885043025016785 Accuracy 0.6501666903495789\n",
      "Iteration 20590 Training loss 0.09154348820447922 Validation loss 0.12276606261730194 Accuracy 0.6614999771118164\n",
      "Iteration 20600 Training loss 0.0925934761762619 Validation loss 0.12629470229148865 Accuracy 0.653166651725769\n",
      "Iteration 20610 Training loss 0.09487207978963852 Validation loss 0.13387668132781982 Accuracy 0.6371666789054871\n",
      "Iteration 20620 Training loss 0.09415151178836823 Validation loss 0.13709163665771484 Accuracy 0.6298333406448364\n",
      "Iteration 20630 Training loss 0.09244624525308609 Validation loss 0.1291935294866562 Accuracy 0.6498333215713501\n",
      "Iteration 20640 Training loss 0.09016840904951096 Validation loss 0.12367380410432816 Accuracy 0.6596666574478149\n",
      "Iteration 20650 Training loss 0.09285541623830795 Validation loss 0.1332758218050003 Accuracy 0.640333354473114\n",
      "Iteration 20660 Training loss 0.09180853515863419 Validation loss 0.1306251883506775 Accuracy 0.6474999785423279\n",
      "Iteration 20670 Training loss 0.0901414155960083 Validation loss 0.11833866685628891 Accuracy 0.6713333129882812\n",
      "Iteration 20680 Training loss 0.08894094824790955 Validation loss 0.11839207261800766 Accuracy 0.6700000166893005\n",
      "Iteration 20690 Training loss 0.093915656208992 Validation loss 0.11234275251626968 Accuracy 0.6816666722297668\n",
      "Iteration 20700 Training loss 0.09324300289154053 Validation loss 0.11210526525974274 Accuracy 0.6838333606719971\n",
      "Iteration 20710 Training loss 0.09111811220645905 Validation loss 0.11286013573408127 Accuracy 0.6830000281333923\n",
      "Iteration 20720 Training loss 0.09350159019231796 Validation loss 0.11203635483980179 Accuracy 0.6826666593551636\n",
      "Iteration 20730 Training loss 0.09440942108631134 Validation loss 0.11190138012170792 Accuracy 0.684333324432373\n",
      "Iteration 20740 Training loss 0.0936875194311142 Validation loss 0.11178018897771835 Accuracy 0.6834999918937683\n",
      "Iteration 20750 Training loss 0.09284210205078125 Validation loss 0.11347457021474838 Accuracy 0.6803333163261414\n",
      "Iteration 20760 Training loss 0.08940203487873077 Validation loss 0.11628975719213486 Accuracy 0.6761666536331177\n",
      "Iteration 20770 Training loss 0.09633780270814896 Validation loss 0.11208518594503403 Accuracy 0.6816666722297668\n",
      "Iteration 20780 Training loss 0.0939083993434906 Validation loss 0.11187209188938141 Accuracy 0.6815000176429749\n",
      "Iteration 20790 Training loss 0.09338078647851944 Validation loss 0.11186515539884567 Accuracy 0.684333324432373\n",
      "Iteration 20800 Training loss 0.08910513669252396 Validation loss 0.11283693462610245 Accuracy 0.6823333501815796\n",
      "Iteration 20810 Training loss 0.08931935578584671 Validation loss 0.11456164717674255 Accuracy 0.6790000200271606\n",
      "Iteration 20820 Training loss 0.09381473064422607 Validation loss 0.11203407496213913 Accuracy 0.6819999814033508\n",
      "Iteration 20830 Training loss 0.08974548429250717 Validation loss 0.11716637015342712 Accuracy 0.6736666560173035\n",
      "Iteration 20840 Training loss 0.0885966569185257 Validation loss 0.11348261684179306 Accuracy 0.6791666746139526\n",
      "Iteration 20850 Training loss 0.09038259088993073 Validation loss 0.11894677579402924 Accuracy 0.6698333621025085\n",
      "Iteration 20860 Training loss 0.09355698525905609 Validation loss 0.12526434659957886 Accuracy 0.656499981880188\n",
      "Iteration 20870 Training loss 0.09847855567932129 Validation loss 0.14167441427707672 Accuracy 0.6206666827201843\n",
      "Iteration 20880 Training loss 0.09113777428865433 Validation loss 0.1314278095960617 Accuracy 0.6461666822433472\n",
      "Iteration 20890 Training loss 0.09380169957876205 Validation loss 0.1310214400291443 Accuracy 0.6451666951179504\n",
      "Iteration 20900 Training loss 0.09433094412088394 Validation loss 0.12880519032478333 Accuracy 0.6488333344459534\n",
      "Iteration 20910 Training loss 0.09248152375221252 Validation loss 0.13150182366371155 Accuracy 0.6441666483879089\n",
      "Iteration 20920 Training loss 0.09249217063188553 Validation loss 0.13379094004631042 Accuracy 0.6386666893959045\n",
      "Iteration 20930 Training loss 0.09342046827077866 Validation loss 0.13085876405239105 Accuracy 0.6463333368301392\n",
      "Iteration 20940 Training loss 0.09165974706411362 Validation loss 0.12376534193754196 Accuracy 0.6596666574478149\n",
      "Iteration 20950 Training loss 0.09477993100881577 Validation loss 0.13764362037181854 Accuracy 0.6278333067893982\n",
      "Iteration 20960 Training loss 0.0894651785492897 Validation loss 0.1259583979845047 Accuracy 0.6554999947547913\n",
      "Iteration 20970 Training loss 0.09134455025196075 Validation loss 0.12716160714626312 Accuracy 0.6536666750907898\n",
      "Iteration 20980 Training loss 0.0917796716094017 Validation loss 0.11925587058067322 Accuracy 0.6700000166893005\n",
      "Iteration 20990 Training loss 0.0911191925406456 Validation loss 0.1275554746389389 Accuracy 0.6536666750907898\n",
      "Iteration 21000 Training loss 0.09128887951374054 Validation loss 0.13029277324676514 Accuracy 0.6473333239555359\n",
      "Iteration 21010 Training loss 0.09276458621025085 Validation loss 0.13254156708717346 Accuracy 0.6420000195503235\n",
      "Iteration 21020 Training loss 0.08893324434757233 Validation loss 0.12060244381427765 Accuracy 0.6661666631698608\n",
      "Iteration 21030 Training loss 0.09315041452646255 Validation loss 0.1295131891965866 Accuracy 0.6501666903495789\n",
      "Iteration 21040 Training loss 0.09231225401163101 Validation loss 0.13480472564697266 Accuracy 0.6353333592414856\n",
      "Iteration 21050 Training loss 0.0932985320687294 Validation loss 0.13033033907413483 Accuracy 0.6471666693687439\n",
      "Iteration 21060 Training loss 0.09191863238811493 Validation loss 0.12455110996961594 Accuracy 0.659166693687439\n",
      "Iteration 21070 Training loss 0.09261664003133774 Validation loss 0.13319604098796844 Accuracy 0.6399999856948853\n",
      "Iteration 21080 Training loss 0.09326574951410294 Validation loss 0.13121360540390015 Accuracy 0.6456666588783264\n",
      "Iteration 21090 Training loss 0.0901622623205185 Validation loss 0.1289757937192917 Accuracy 0.6480000019073486\n",
      "Iteration 21100 Training loss 0.09233365952968597 Validation loss 0.12502670288085938 Accuracy 0.659166693687439\n",
      "Iteration 21110 Training loss 0.0931137204170227 Validation loss 0.13319385051727295 Accuracy 0.6413333415985107\n",
      "Iteration 21120 Training loss 0.09485860913991928 Validation loss 0.13341549038887024 Accuracy 0.6398333311080933\n",
      "Iteration 21130 Training loss 0.09570451080799103 Validation loss 0.13367152214050293 Accuracy 0.6389999985694885\n",
      "Iteration 21140 Training loss 0.09417903423309326 Validation loss 0.13242216408252716 Accuracy 0.6420000195503235\n",
      "Iteration 21150 Training loss 0.09356563538312912 Validation loss 0.1318226158618927 Accuracy 0.6428333520889282\n",
      "Iteration 21160 Training loss 0.09247426688671112 Validation loss 0.12980704009532928 Accuracy 0.6478333473205566\n",
      "Iteration 21170 Training loss 0.09135343134403229 Validation loss 0.12668092548847198 Accuracy 0.656000018119812\n",
      "Iteration 21180 Training loss 0.09262926876544952 Validation loss 0.12440420687198639 Accuracy 0.659333348274231\n",
      "Iteration 21190 Training loss 0.0925556868314743 Validation loss 0.13257235288619995 Accuracy 0.6416666507720947\n",
      "Iteration 21200 Training loss 0.09225564450025558 Validation loss 0.13074882328510284 Accuracy 0.6468333601951599\n",
      "Iteration 21210 Training loss 0.09059648960828781 Validation loss 0.1258949339389801 Accuracy 0.6554999947547913\n",
      "Iteration 21220 Training loss 0.09726691991090775 Validation loss 0.13572093844413757 Accuracy 0.6324999928474426\n",
      "Iteration 21230 Training loss 0.09162789583206177 Validation loss 0.12092336267232895 Accuracy 0.6673333048820496\n",
      "Iteration 21240 Training loss 0.08957822620868683 Validation loss 0.12234856933355331 Accuracy 0.6620000004768372\n",
      "Iteration 21250 Training loss 0.08930936455726624 Validation loss 0.11867370456457138 Accuracy 0.6701666712760925\n",
      "Iteration 21260 Training loss 0.08936569094657898 Validation loss 0.11557287722826004 Accuracy 0.6786666512489319\n",
      "Iteration 21270 Training loss 0.09117714315652847 Validation loss 0.11291544884443283 Accuracy 0.6838333606719971\n",
      "Iteration 21280 Training loss 0.09761834889650345 Validation loss 0.11280358582735062 Accuracy 0.6803333163261414\n",
      "Iteration 21290 Training loss 0.0914052352309227 Validation loss 0.11458621919155121 Accuracy 0.6786666512489319\n",
      "Iteration 21300 Training loss 0.09113121777772903 Validation loss 0.12207049876451492 Accuracy 0.6638333201408386\n",
      "Iteration 21310 Training loss 0.0899277776479721 Validation loss 0.12203744798898697 Accuracy 0.6641666889190674\n",
      "Iteration 21320 Training loss 0.09461688995361328 Validation loss 0.13137447834014893 Accuracy 0.6478333473205566\n",
      "Iteration 21330 Training loss 0.09276517480611801 Validation loss 0.1320619434118271 Accuracy 0.643833339214325\n",
      "Iteration 21340 Training loss 0.0939444974064827 Validation loss 0.13412721455097198 Accuracy 0.6389999985694885\n",
      "Iteration 21350 Training loss 0.09334655106067657 Validation loss 0.12630204856395721 Accuracy 0.656333327293396\n",
      "Iteration 21360 Training loss 0.09192676097154617 Validation loss 0.13382381200790405 Accuracy 0.6384999752044678\n",
      "Iteration 21370 Training loss 0.09291241317987442 Validation loss 0.13021543622016907 Accuracy 0.6485000252723694\n",
      "Iteration 21380 Training loss 0.08972042053937912 Validation loss 0.11893675476312637 Accuracy 0.6700000166893005\n",
      "Iteration 21390 Training loss 0.09014654904603958 Validation loss 0.1178007572889328 Accuracy 0.6733333468437195\n",
      "Iteration 21400 Training loss 0.09016098082065582 Validation loss 0.11973511427640915 Accuracy 0.668666660785675\n",
      "Iteration 21410 Training loss 0.08955276757478714 Validation loss 0.1205819696187973 Accuracy 0.6664999723434448\n",
      "Iteration 21420 Training loss 0.09448004513978958 Validation loss 0.13727979362010956 Accuracy 0.6320000290870667\n",
      "Iteration 21430 Training loss 0.08939764648675919 Validation loss 0.12667743861675262 Accuracy 0.6543333530426025\n",
      "Iteration 21440 Training loss 0.08707387000322342 Validation loss 0.11902362108230591 Accuracy 0.668666660785675\n",
      "Iteration 21450 Training loss 0.09314263612031937 Validation loss 0.13189546763896942 Accuracy 0.6446666717529297\n",
      "Iteration 21460 Training loss 0.09456741064786911 Validation loss 0.13736283779144287 Accuracy 0.6309999823570251\n",
      "Iteration 21470 Training loss 0.09284103661775589 Validation loss 0.1292906552553177 Accuracy 0.6498333215713501\n",
      "Iteration 21480 Training loss 0.09148063510656357 Validation loss 0.12926210463047028 Accuracy 0.6495000123977661\n",
      "Iteration 21490 Training loss 0.09185225516557693 Validation loss 0.1316767930984497 Accuracy 0.6439999938011169\n",
      "Iteration 21500 Training loss 0.09137200564146042 Validation loss 0.1254139244556427 Accuracy 0.6585000157356262\n",
      "Iteration 21510 Training loss 0.09063224494457245 Validation loss 0.12380585074424744 Accuracy 0.6601666808128357\n",
      "Iteration 21520 Training loss 0.09013944864273071 Validation loss 0.12268129736185074 Accuracy 0.6623333096504211\n",
      "Iteration 21530 Training loss 0.09639004617929459 Validation loss 0.14021475613117218 Accuracy 0.6240000128746033\n",
      "Iteration 21540 Training loss 0.09136667102575302 Validation loss 0.13217271864414215 Accuracy 0.6416666507720947\n",
      "Iteration 21550 Training loss 0.09095359593629837 Validation loss 0.12238863855600357 Accuracy 0.6631666421890259\n",
      "Iteration 21560 Training loss 0.09289130568504333 Validation loss 0.132221981883049 Accuracy 0.6441666483879089\n",
      "Iteration 21570 Training loss 0.09537442773580551 Validation loss 0.1372537612915039 Accuracy 0.6303333044052124\n",
      "Iteration 21580 Training loss 0.09090995043516159 Validation loss 0.12214133888483047 Accuracy 0.6643333435058594\n",
      "Iteration 21590 Training loss 0.0899343490600586 Validation loss 0.12937040627002716 Accuracy 0.6499999761581421\n",
      "Iteration 21600 Training loss 0.09514141827821732 Validation loss 0.13464705646038055 Accuracy 0.637333333492279\n",
      "Iteration 21610 Training loss 0.09235610067844391 Validation loss 0.12687911093235016 Accuracy 0.6539999842643738\n",
      "Iteration 21620 Training loss 0.09354367852210999 Validation loss 0.13085196912288666 Accuracy 0.6468333601951599\n",
      "Iteration 21630 Training loss 0.09116195887327194 Validation loss 0.12977547943592072 Accuracy 0.6474999785423279\n",
      "Iteration 21640 Training loss 0.09171827137470245 Validation loss 0.1301136612892151 Accuracy 0.6481666564941406\n",
      "Iteration 21650 Training loss 0.09025726467370987 Validation loss 0.12635208666324615 Accuracy 0.6556666493415833\n",
      "Iteration 21660 Training loss 0.09617697447538376 Validation loss 0.13709475100040436 Accuracy 0.6305000185966492\n",
      "Iteration 21670 Training loss 0.09348196536302567 Validation loss 0.1241840198636055 Accuracy 0.6603333353996277\n",
      "Iteration 21680 Training loss 0.09294126182794571 Validation loss 0.13218101859092712 Accuracy 0.643666684627533\n",
      "Iteration 21690 Training loss 0.09059930592775345 Validation loss 0.1268726885318756 Accuracy 0.6535000205039978\n",
      "Iteration 21700 Training loss 0.09181497991085052 Validation loss 0.12610623240470886 Accuracy 0.6556666493415833\n",
      "Iteration 21710 Training loss 0.09351440519094467 Validation loss 0.1326114982366562 Accuracy 0.6420000195503235\n",
      "Iteration 21720 Training loss 0.09510310739278793 Validation loss 0.136719211935997 Accuracy 0.6311666369438171\n",
      "Iteration 21730 Training loss 0.09208434075117111 Validation loss 0.1289464384317398 Accuracy 0.6496666669845581\n",
      "Iteration 21740 Training loss 0.09152964502573013 Validation loss 0.12595807015895844 Accuracy 0.656166672706604\n",
      "Iteration 21750 Training loss 0.0915469378232956 Validation loss 0.12152571976184845 Accuracy 0.6658333539962769\n",
      "Iteration 21760 Training loss 0.08776607364416122 Validation loss 0.11545345187187195 Accuracy 0.6786666512489319\n",
      "Iteration 21770 Training loss 0.08911184966564178 Validation loss 0.11394621431827545 Accuracy 0.6806666851043701\n",
      "Iteration 21780 Training loss 0.09145767986774445 Validation loss 0.11324340105056763 Accuracy 0.6834999918937683\n",
      "Iteration 21790 Training loss 0.09485351294279099 Validation loss 0.11204370856285095 Accuracy 0.6836666464805603\n",
      "Iteration 21800 Training loss 0.09009026736021042 Validation loss 0.11436907947063446 Accuracy 0.6786666512489319\n",
      "Iteration 21810 Training loss 0.09051527082920074 Validation loss 0.11302972584962845 Accuracy 0.6825000047683716\n",
      "Iteration 21820 Training loss 0.09388533234596252 Validation loss 0.11212711781263351 Accuracy 0.6816666722297668\n",
      "Iteration 21830 Training loss 0.09546636044979095 Validation loss 0.11168766021728516 Accuracy 0.6836666464805603\n",
      "Iteration 21840 Training loss 0.09276241809129715 Validation loss 0.11201131343841553 Accuracy 0.6826666593551636\n",
      "Iteration 21850 Training loss 0.09379591047763824 Validation loss 0.11188328266143799 Accuracy 0.6846666932106018\n",
      "Iteration 21860 Training loss 0.0929836630821228 Validation loss 0.1118895635008812 Accuracy 0.6836666464805603\n",
      "Iteration 21870 Training loss 0.09184765815734863 Validation loss 0.1124749481678009 Accuracy 0.6836666464805603\n",
      "Iteration 21880 Training loss 0.09030160307884216 Validation loss 0.11424539238214493 Accuracy 0.6786666512489319\n",
      "Iteration 21890 Training loss 0.09326350688934326 Validation loss 0.11312370002269745 Accuracy 0.6830000281333923\n",
      "Iteration 21900 Training loss 0.08996929973363876 Validation loss 0.11303961277008057 Accuracy 0.6831666827201843\n",
      "Iteration 21910 Training loss 0.09157644957304001 Validation loss 0.112261101603508 Accuracy 0.684166669845581\n",
      "Iteration 21920 Training loss 0.0930447056889534 Validation loss 0.11231964081525803 Accuracy 0.6830000281333923\n",
      "Iteration 21930 Training loss 0.09216417372226715 Validation loss 0.1126585602760315 Accuracy 0.6834999918937683\n",
      "Iteration 21940 Training loss 0.0926818922162056 Validation loss 0.11257932335138321 Accuracy 0.6826666593551636\n",
      "Iteration 21950 Training loss 0.09528223425149918 Validation loss 0.11223657429218292 Accuracy 0.6830000281333923\n",
      "Iteration 21960 Training loss 0.094303660094738 Validation loss 0.11201144754886627 Accuracy 0.6833333373069763\n",
      "Iteration 21970 Training loss 0.0894022285938263 Validation loss 0.11473097652196884 Accuracy 0.6794999837875366\n",
      "Iteration 21980 Training loss 0.0911075621843338 Validation loss 0.11226310580968857 Accuracy 0.6836666464805603\n",
      "Iteration 21990 Training loss 0.09237057715654373 Validation loss 0.11204853653907776 Accuracy 0.6808333396911621\n",
      "Iteration 22000 Training loss 0.09274651110172272 Validation loss 0.11243442445993423 Accuracy 0.6828333139419556\n",
      "Iteration 22010 Training loss 0.09347865730524063 Validation loss 0.11209027469158173 Accuracy 0.6823333501815796\n",
      "Iteration 22020 Training loss 0.0913919061422348 Validation loss 0.11201874911785126 Accuracy 0.6848333477973938\n",
      "Iteration 22030 Training loss 0.09069370478391647 Validation loss 0.11260264366865158 Accuracy 0.6834999918937683\n",
      "Iteration 22040 Training loss 0.09218183159828186 Validation loss 0.11210198700428009 Accuracy 0.684166669845581\n",
      "Iteration 22050 Training loss 0.09250454604625702 Validation loss 0.11249250173568726 Accuracy 0.6821666955947876\n",
      "Iteration 22060 Training loss 0.09622412919998169 Validation loss 0.11206383258104324 Accuracy 0.6828333139419556\n",
      "Iteration 22070 Training loss 0.08984561264514923 Validation loss 0.1125604659318924 Accuracy 0.6833333373069763\n",
      "Iteration 22080 Training loss 0.09210329502820969 Validation loss 0.11215704679489136 Accuracy 0.6838333606719971\n",
      "Iteration 22090 Training loss 0.09190578013658524 Validation loss 0.1120634526014328 Accuracy 0.6831666827201843\n",
      "Iteration 22100 Training loss 0.0933891087770462 Validation loss 0.11249364912509918 Accuracy 0.684333324432373\n",
      "Iteration 22110 Training loss 0.0932532250881195 Validation loss 0.11187949776649475 Accuracy 0.6834999918937683\n",
      "Iteration 22120 Training loss 0.08948709070682526 Validation loss 0.11227834224700928 Accuracy 0.6823333501815796\n",
      "Iteration 22130 Training loss 0.09062043577432632 Validation loss 0.12006910890340805 Accuracy 0.6673333048820496\n",
      "Iteration 22140 Training loss 0.09494995325803757 Validation loss 0.1381126046180725 Accuracy 0.6286666393280029\n",
      "Iteration 22150 Training loss 0.09114089608192444 Validation loss 0.12671050429344177 Accuracy 0.656333327293396\n",
      "Iteration 22160 Training loss 0.09192554652690887 Validation loss 0.1280963271856308 Accuracy 0.652999997138977\n",
      "Iteration 22170 Training loss 0.0911107212305069 Validation loss 0.12455668300390244 Accuracy 0.6596666574478149\n",
      "Iteration 22180 Training loss 0.0909801721572876 Validation loss 0.12653888761997223 Accuracy 0.6556666493415833\n",
      "Iteration 22190 Training loss 0.091714046895504 Validation loss 0.13153879344463348 Accuracy 0.6448333263397217\n",
      "Iteration 22200 Training loss 0.09199261665344238 Validation loss 0.1321178823709488 Accuracy 0.6428333520889282\n",
      "Iteration 22210 Training loss 0.09156891703605652 Validation loss 0.1277923434972763 Accuracy 0.6521666646003723\n",
      "Iteration 22220 Training loss 0.08866503834724426 Validation loss 0.11864416301250458 Accuracy 0.6708333492279053\n",
      "Iteration 22230 Training loss 0.09193496406078339 Validation loss 0.1302356719970703 Accuracy 0.6481666564941406\n",
      "Iteration 22240 Training loss 0.09531288594007492 Validation loss 0.14069607853889465 Accuracy 0.6258333325386047\n",
      "Iteration 22250 Training loss 0.0904262512922287 Validation loss 0.12357565015554428 Accuracy 0.6608333587646484\n",
      "Iteration 22260 Training loss 0.09048347175121307 Validation loss 0.1251550316810608 Accuracy 0.6604999899864197\n",
      "Iteration 22270 Training loss 0.09169773012399673 Validation loss 0.13091285526752472 Accuracy 0.6476666927337646\n",
      "Iteration 22280 Training loss 0.09450060874223709 Validation loss 0.1349163055419922 Accuracy 0.6348333358764648\n",
      "Iteration 22290 Training loss 0.0895095244050026 Validation loss 0.127557635307312 Accuracy 0.653166651725769\n",
      "Iteration 22300 Training loss 0.08976294100284576 Validation loss 0.12528665363788605 Accuracy 0.6600000262260437\n",
      "Iteration 22310 Training loss 0.0942593365907669 Validation loss 0.13507571816444397 Accuracy 0.6353333592414856\n",
      "Iteration 22320 Training loss 0.09163033962249756 Validation loss 0.12657038867473602 Accuracy 0.6566666960716248\n",
      "Iteration 22330 Training loss 0.09182814508676529 Validation loss 0.12923844158649445 Accuracy 0.6480000019073486\n",
      "Iteration 22340 Training loss 0.09291645139455795 Validation loss 0.13350041210651398 Accuracy 0.6411666870117188\n",
      "Iteration 22350 Training loss 0.09149520099163055 Validation loss 0.1246885284781456 Accuracy 0.6600000262260437\n",
      "Iteration 22360 Training loss 0.09190452098846436 Validation loss 0.12245774269104004 Accuracy 0.6639999747276306\n",
      "Iteration 22370 Training loss 0.0958283394575119 Validation loss 0.13537994027137756 Accuracy 0.6353333592414856\n",
      "Iteration 22380 Training loss 0.0885101929306984 Validation loss 0.124518483877182 Accuracy 0.6611666679382324\n",
      "Iteration 22390 Training loss 0.08936426043510437 Validation loss 0.12397868186235428 Accuracy 0.6621666550636292\n",
      "Iteration 22400 Training loss 0.09649097919464111 Validation loss 0.13785168528556824 Accuracy 0.6301666498184204\n",
      "Iteration 22410 Training loss 0.093215711414814 Validation loss 0.13001541793346405 Accuracy 0.6478333473205566\n",
      "Iteration 22420 Training loss 0.09363728761672974 Validation loss 0.13353897631168365 Accuracy 0.6399999856948853\n",
      "Iteration 22430 Training loss 0.0898866206407547 Validation loss 0.12552760541439056 Accuracy 0.6589999794960022\n",
      "Iteration 22440 Training loss 0.09138733148574829 Validation loss 0.12483193725347519 Accuracy 0.659166693687439\n",
      "Iteration 22450 Training loss 0.09112812578678131 Validation loss 0.12934210896492004 Accuracy 0.6491666436195374\n",
      "Iteration 22460 Training loss 0.09271042048931122 Validation loss 0.13083617389202118 Accuracy 0.6464999914169312\n",
      "Iteration 22470 Training loss 0.09133927524089813 Validation loss 0.12982003390789032 Accuracy 0.6480000019073486\n",
      "Iteration 22480 Training loss 0.09444808214902878 Validation loss 0.13483692705631256 Accuracy 0.637666642665863\n",
      "Iteration 22490 Training loss 0.09017449617385864 Validation loss 0.1208241805434227 Accuracy 0.6668333411216736\n",
      "Iteration 22500 Training loss 0.08780393749475479 Validation loss 0.12565751373767853 Accuracy 0.6566666960716248\n",
      "Iteration 22510 Training loss 0.0921517014503479 Validation loss 0.12897849082946777 Accuracy 0.6493333578109741\n",
      "Iteration 22520 Training loss 0.09044820070266724 Validation loss 0.12845778465270996 Accuracy 0.6508333086967468\n",
      "Iteration 22530 Training loss 0.0901896059513092 Validation loss 0.12146200239658356 Accuracy 0.6676666736602783\n",
      "Iteration 22540 Training loss 0.09003271162509918 Validation loss 0.12917716801166534 Accuracy 0.6481666564941406\n",
      "Iteration 22550 Training loss 0.09614156931638718 Validation loss 0.13746583461761475 Accuracy 0.6306666731834412\n",
      "Iteration 22560 Training loss 0.09013163298368454 Validation loss 0.1278572380542755 Accuracy 0.6528333425521851\n",
      "Iteration 22570 Training loss 0.09395274519920349 Validation loss 0.13226759433746338 Accuracy 0.6431666612625122\n",
      "Iteration 22580 Training loss 0.0923849418759346 Validation loss 0.136363223195076 Accuracy 0.6330000162124634\n",
      "Iteration 22590 Training loss 0.09297463297843933 Validation loss 0.13339950144290924 Accuracy 0.6384999752044678\n",
      "Iteration 22600 Training loss 0.09205251932144165 Validation loss 0.12940752506256104 Accuracy 0.6486666798591614\n",
      "Iteration 22610 Training loss 0.08992458879947662 Validation loss 0.11988584697246552 Accuracy 0.6691666841506958\n",
      "Iteration 22620 Training loss 0.08944016695022583 Validation loss 0.12287084013223648 Accuracy 0.6621666550636292\n",
      "Iteration 22630 Training loss 0.09631868451833725 Validation loss 0.13859954476356506 Accuracy 0.6276666522026062\n",
      "Iteration 22640 Training loss 0.08862368762493134 Validation loss 0.12829063832759857 Accuracy 0.6521666646003723\n",
      "Iteration 22650 Training loss 0.0929223820567131 Validation loss 0.13365024328231812 Accuracy 0.6380000114440918\n",
      "Iteration 22660 Training loss 0.09197646379470825 Validation loss 0.13297991454601288 Accuracy 0.6411666870117188\n",
      "Iteration 22670 Training loss 0.09119429439306259 Validation loss 0.12155496329069138 Accuracy 0.6651666760444641\n",
      "Iteration 22680 Training loss 0.09041409194469452 Validation loss 0.12209732830524445 Accuracy 0.6641666889190674\n",
      "Iteration 22690 Training loss 0.09201450645923615 Validation loss 0.13526643812656403 Accuracy 0.6363333463668823\n",
      "Iteration 22700 Training loss 0.09197711199522018 Validation loss 0.13414782285690308 Accuracy 0.6384999752044678\n",
      "Iteration 22710 Training loss 0.09194459021091461 Validation loss 0.12888817489147186 Accuracy 0.6510000228881836\n",
      "Iteration 22720 Training loss 0.08999939262866974 Validation loss 0.12861448526382446 Accuracy 0.6511666774749756\n",
      "Iteration 22730 Training loss 0.09007208049297333 Validation loss 0.13038703799247742 Accuracy 0.6496666669845581\n",
      "Iteration 22740 Training loss 0.09183274209499359 Validation loss 0.12975136935710907 Accuracy 0.6495000123977661\n",
      "Iteration 22750 Training loss 0.09161293506622314 Validation loss 0.12770728766918182 Accuracy 0.6535000205039978\n",
      "Iteration 22760 Training loss 0.08852553367614746 Validation loss 0.11799415946006775 Accuracy 0.6743333339691162\n",
      "Iteration 22770 Training loss 0.09222869575023651 Validation loss 0.112215556204319 Accuracy 0.6850000023841858\n",
      "Iteration 22780 Training loss 0.09398937970399857 Validation loss 0.11199550330638885 Accuracy 0.6850000023841858\n",
      "Iteration 22790 Training loss 0.09165506064891815 Validation loss 0.11197706311941147 Accuracy 0.6834999918937683\n",
      "Iteration 22800 Training loss 0.08764105290174484 Validation loss 0.1173965334892273 Accuracy 0.6753333210945129\n",
      "Iteration 22810 Training loss 0.09188573062419891 Validation loss 0.12825022637844086 Accuracy 0.6506666541099548\n",
      "Iteration 22820 Training loss 0.09143751114606857 Validation loss 0.12996691465377808 Accuracy 0.6491666436195374\n",
      "Iteration 22830 Training loss 0.09430494904518127 Validation loss 0.13704152405261993 Accuracy 0.6324999928474426\n",
      "Iteration 22840 Training loss 0.0903097465634346 Validation loss 0.12445801496505737 Accuracy 0.6601666808128357\n",
      "Iteration 22850 Training loss 0.09050901234149933 Validation loss 0.12867781519889832 Accuracy 0.6513333320617676\n",
      "Iteration 22860 Training loss 0.09440254420042038 Validation loss 0.13178464770317078 Accuracy 0.6431666612625122\n",
      "Iteration 22870 Training loss 0.09342321753501892 Validation loss 0.13447876274585724 Accuracy 0.637499988079071\n",
      "Iteration 22880 Training loss 0.09128383547067642 Validation loss 0.1289144903421402 Accuracy 0.6510000228881836\n",
      "Iteration 22890 Training loss 0.09217338263988495 Validation loss 0.13630181550979614 Accuracy 0.6349999904632568\n",
      "Iteration 22900 Training loss 0.08949365466833115 Validation loss 0.1289636194705963 Accuracy 0.6506666541099548\n",
      "Iteration 22910 Training loss 0.09017495065927505 Validation loss 0.12369485199451447 Accuracy 0.6614999771118164\n",
      "Iteration 22920 Training loss 0.09281200915575027 Validation loss 0.1277303844690323 Accuracy 0.6536666750907898\n",
      "Iteration 22930 Training loss 0.09064017981290817 Validation loss 0.11844506859779358 Accuracy 0.6729999780654907\n",
      "Iteration 22940 Training loss 0.09218894690275192 Validation loss 0.13128864765167236 Accuracy 0.6466666460037231\n",
      "Iteration 22950 Training loss 0.09181485325098038 Validation loss 0.13197901844978333 Accuracy 0.643666684627533\n",
      "Iteration 22960 Training loss 0.08882706612348557 Validation loss 0.11987213790416718 Accuracy 0.6706666946411133\n",
      "Iteration 22970 Training loss 0.09083040058612823 Validation loss 0.12137768417596817 Accuracy 0.6664999723434448\n",
      "Iteration 22980 Training loss 0.09298043698072433 Validation loss 0.1313626766204834 Accuracy 0.6464999914169312\n",
      "Iteration 22990 Training loss 0.09518858790397644 Validation loss 0.1388135850429535 Accuracy 0.6291666626930237\n",
      "Iteration 23000 Training loss 0.09092249721288681 Validation loss 0.13081172108650208 Accuracy 0.6460000276565552\n",
      "Iteration 23010 Training loss 0.09219399094581604 Validation loss 0.12827222049236298 Accuracy 0.6518333554267883\n",
      "Iteration 23020 Training loss 0.09099283069372177 Validation loss 0.12659378349781036 Accuracy 0.6545000076293945\n",
      "Iteration 23030 Training loss 0.0909532681107521 Validation loss 0.1311616599559784 Accuracy 0.6451666951179504\n",
      "Iteration 23040 Training loss 0.08888724446296692 Validation loss 0.12678824365139008 Accuracy 0.6554999947547913\n",
      "Iteration 23050 Training loss 0.08948901295661926 Validation loss 0.11364667117595673 Accuracy 0.6823333501815796\n",
      "Iteration 23060 Training loss 0.09185673296451569 Validation loss 0.11230947077274323 Accuracy 0.6850000023841858\n",
      "Iteration 23070 Training loss 0.08934912830591202 Validation loss 0.1131843701004982 Accuracy 0.6833333373069763\n",
      "Iteration 23080 Training loss 0.09040631353855133 Validation loss 0.11224855482578278 Accuracy 0.6838333606719971\n",
      "Iteration 23090 Training loss 0.08946767449378967 Validation loss 0.1132083311676979 Accuracy 0.6821666955947876\n",
      "Iteration 23100 Training loss 0.08966515213251114 Validation loss 0.11364632099866867 Accuracy 0.6816666722297668\n",
      "Iteration 23110 Training loss 0.09398172795772552 Validation loss 0.11238585412502289 Accuracy 0.6806666851043701\n",
      "Iteration 23120 Training loss 0.09302978217601776 Validation loss 0.11216133832931519 Accuracy 0.6833333373069763\n",
      "Iteration 23130 Training loss 0.0907389223575592 Validation loss 0.11230356991291046 Accuracy 0.684499979019165\n",
      "Iteration 23140 Training loss 0.09176909178495407 Validation loss 0.11220163851976395 Accuracy 0.6818333268165588\n",
      "Iteration 23150 Training loss 0.09108185023069382 Validation loss 0.11270751804113388 Accuracy 0.6828333139419556\n",
      "Iteration 23160 Training loss 0.09112105518579483 Validation loss 0.11283756792545319 Accuracy 0.6836666464805603\n",
      "Iteration 23170 Training loss 0.09110738337039948 Validation loss 0.113748699426651 Accuracy 0.6813333630561829\n",
      "Iteration 23180 Training loss 0.09106294810771942 Validation loss 0.11300411075353622 Accuracy 0.6834999918937683\n",
      "Iteration 23190 Training loss 0.09484777599573135 Validation loss 0.11264340579509735 Accuracy 0.6800000071525574\n",
      "Iteration 23200 Training loss 0.09095580875873566 Validation loss 0.11267119646072388 Accuracy 0.6836666464805603\n",
      "Iteration 23210 Training loss 0.09016456454992294 Validation loss 0.11251014471054077 Accuracy 0.6840000152587891\n",
      "Iteration 23220 Training loss 0.09095842391252518 Validation loss 0.11256664246320724 Accuracy 0.6830000281333923\n",
      "Iteration 23230 Training loss 0.0890640988945961 Validation loss 0.11386210471391678 Accuracy 0.6846666932106018\n",
      "Iteration 23240 Training loss 0.09548702836036682 Validation loss 0.11214068531990051 Accuracy 0.6825000047683716\n",
      "Iteration 23250 Training loss 0.08816753327846527 Validation loss 0.11310026049613953 Accuracy 0.6840000152587891\n",
      "Iteration 23260 Training loss 0.08866038173437119 Validation loss 0.11595484614372253 Accuracy 0.6784999966621399\n",
      "Iteration 23270 Training loss 0.09068538248538971 Validation loss 0.1141682118177414 Accuracy 0.6815000176429749\n",
      "Iteration 23280 Training loss 0.09212233126163483 Validation loss 0.11231230944395065 Accuracy 0.6826666593551636\n",
      "Iteration 23290 Training loss 0.09050921350717545 Validation loss 0.11299607157707214 Accuracy 0.6826666593551636\n",
      "Iteration 23300 Training loss 0.09576074033975601 Validation loss 0.11200986057519913 Accuracy 0.6840000152587891\n",
      "Iteration 23310 Training loss 0.09107991307973862 Validation loss 0.11248242110013962 Accuracy 0.6836666464805603\n",
      "Iteration 23320 Training loss 0.08960454165935516 Validation loss 0.11439187824726105 Accuracy 0.6804999709129333\n",
      "Iteration 23330 Training loss 0.09246238321065903 Validation loss 0.11232314258813858 Accuracy 0.6846666932106018\n",
      "Iteration 23340 Training loss 0.09321421384811401 Validation loss 0.1121099665760994 Accuracy 0.6828333139419556\n",
      "Iteration 23350 Training loss 0.09200473874807358 Validation loss 0.11269674450159073 Accuracy 0.6838333606719971\n",
      "Iteration 23360 Training loss 0.09323610365390778 Validation loss 0.11221351474523544 Accuracy 0.6838333606719971\n",
      "Iteration 23370 Training loss 0.09216096997261047 Validation loss 0.11484383046627045 Accuracy 0.6790000200271606\n",
      "Iteration 23380 Training loss 0.09069586545228958 Validation loss 0.11290484666824341 Accuracy 0.6836666464805603\n",
      "Iteration 23390 Training loss 0.09506874531507492 Validation loss 0.11211279034614563 Accuracy 0.6815000176429749\n",
      "Iteration 23400 Training loss 0.09267592430114746 Validation loss 0.11207405477762222 Accuracy 0.684499979019165\n",
      "Iteration 23410 Training loss 0.0911802351474762 Validation loss 0.11336793750524521 Accuracy 0.6840000152587891\n",
      "Iteration 23420 Training loss 0.09517861157655716 Validation loss 0.11236964911222458 Accuracy 0.6826666593551636\n",
      "Iteration 23430 Training loss 0.0918242409825325 Validation loss 0.11228377372026443 Accuracy 0.6853333115577698\n",
      "Iteration 23440 Training loss 0.09413105994462967 Validation loss 0.11227155476808548 Accuracy 0.684499979019165\n",
      "Iteration 23450 Training loss 0.08799486607313156 Validation loss 0.11257908493280411 Accuracy 0.6830000281333923\n",
      "Iteration 23460 Training loss 0.0894520953297615 Validation loss 0.11538217961788177 Accuracy 0.6815000176429749\n",
      "Iteration 23470 Training loss 0.09161082655191422 Validation loss 0.11624380201101303 Accuracy 0.6784999966621399\n",
      "Iteration 23480 Training loss 0.09037505835294724 Validation loss 0.11323346197605133 Accuracy 0.6836666464805603\n",
      "Iteration 23490 Training loss 0.0917462483048439 Validation loss 0.11286402493715286 Accuracy 0.684499979019165\n",
      "Iteration 23500 Training loss 0.09460148215293884 Validation loss 0.1125834584236145 Accuracy 0.6800000071525574\n",
      "Iteration 23510 Training loss 0.08869904279708862 Validation loss 0.1136552095413208 Accuracy 0.6836666464805603\n",
      "Iteration 23520 Training loss 0.09033997356891632 Validation loss 0.11241977661848068 Accuracy 0.6828333139419556\n",
      "Iteration 23530 Training loss 0.09211362898349762 Validation loss 0.11231711506843567 Accuracy 0.684499979019165\n",
      "Iteration 23540 Training loss 0.09035339206457138 Validation loss 0.11286754161119461 Accuracy 0.6838333606719971\n",
      "Iteration 23550 Training loss 0.08816402405500412 Validation loss 0.11779064685106277 Accuracy 0.6740000247955322\n",
      "Iteration 23560 Training loss 0.08913911879062653 Validation loss 0.12057152390480042 Accuracy 0.668666660785675\n",
      "Iteration 23570 Training loss 0.09115763008594513 Validation loss 0.12950482964515686 Accuracy 0.6510000228881836\n",
      "Iteration 23580 Training loss 0.09316539764404297 Validation loss 0.13230782747268677 Accuracy 0.6441666483879089\n",
      "Iteration 23590 Training loss 0.08992553502321243 Validation loss 0.12387476861476898 Accuracy 0.6644999980926514\n",
      "Iteration 23600 Training loss 0.09026559442281723 Validation loss 0.1296110898256302 Accuracy 0.6501666903495789\n",
      "Iteration 23610 Training loss 0.09612683206796646 Validation loss 0.1382220834493637 Accuracy 0.6303333044052124\n",
      "Iteration 23620 Training loss 0.09025944769382477 Validation loss 0.13241717219352722 Accuracy 0.6423333287239075\n",
      "Iteration 23630 Training loss 0.09165547788143158 Validation loss 0.1322915107011795 Accuracy 0.6418333053588867\n",
      "Iteration 23640 Training loss 0.09161641448736191 Validation loss 0.1288970708847046 Accuracy 0.6526666879653931\n",
      "Iteration 23650 Training loss 0.09387827664613724 Validation loss 0.1327061504125595 Accuracy 0.640666663646698\n",
      "Iteration 23660 Training loss 0.09080015867948532 Validation loss 0.12912875413894653 Accuracy 0.6510000228881836\n",
      "Iteration 23670 Training loss 0.0918164998292923 Validation loss 0.12519395351409912 Accuracy 0.6596666574478149\n",
      "Iteration 23680 Training loss 0.09521599113941193 Validation loss 0.13718274235725403 Accuracy 0.6321666836738586\n",
      "Iteration 23690 Training loss 0.09169173240661621 Validation loss 0.13407447934150696 Accuracy 0.6391666531562805\n",
      "Iteration 23700 Training loss 0.08838442713022232 Validation loss 0.12261965870857239 Accuracy 0.6643333435058594\n",
      "Iteration 23710 Training loss 0.09449678659439087 Validation loss 0.13644357025623322 Accuracy 0.6331666707992554\n",
      "Iteration 23720 Training loss 0.0944603756070137 Validation loss 0.13733257353305817 Accuracy 0.6333333253860474\n",
      "Iteration 23730 Training loss 0.08962375670671463 Validation loss 0.12355726957321167 Accuracy 0.6625000238418579\n",
      "Iteration 23740 Training loss 0.09227681159973145 Validation loss 0.13183815777301788 Accuracy 0.6434999704360962\n",
      "Iteration 23750 Training loss 0.09193406999111176 Validation loss 0.12929517030715942 Accuracy 0.6501666903495789\n",
      "Iteration 23760 Training loss 0.09317679703235626 Validation loss 0.13532157242298126 Accuracy 0.6340000033378601\n",
      "Iteration 23770 Training loss 0.09315810352563858 Validation loss 0.13483305275440216 Accuracy 0.6381666660308838\n",
      "Iteration 23780 Training loss 0.08867102116346359 Validation loss 0.12262386083602905 Accuracy 0.6644999980926514\n",
      "Iteration 23790 Training loss 0.08966688811779022 Validation loss 0.1262696087360382 Accuracy 0.656333327293396\n",
      "Iteration 23800 Training loss 0.09155157208442688 Validation loss 0.1280948668718338 Accuracy 0.6526666879653931\n",
      "Iteration 23810 Training loss 0.09148485213518143 Validation loss 0.1325642466545105 Accuracy 0.6434999704360962\n",
      "Iteration 23820 Training loss 0.0875471830368042 Validation loss 0.12023419141769409 Accuracy 0.6685000061988831\n",
      "Iteration 23830 Training loss 0.08983802050352097 Validation loss 0.1219058707356453 Accuracy 0.6660000085830688\n",
      "Iteration 23840 Training loss 0.09293422102928162 Validation loss 0.13325507938861847 Accuracy 0.6420000195503235\n",
      "Iteration 23850 Training loss 0.09123780578374863 Validation loss 0.13750116527080536 Accuracy 0.6330000162124634\n",
      "Iteration 23860 Training loss 0.093341164290905 Validation loss 0.13278725743293762 Accuracy 0.640666663646698\n",
      "Iteration 23870 Training loss 0.09085985273122787 Validation loss 0.12476902455091476 Accuracy 0.6616666913032532\n",
      "Iteration 23880 Training loss 0.09173953533172607 Validation loss 0.12654215097427368 Accuracy 0.6570000052452087\n",
      "Iteration 23890 Training loss 0.09211401641368866 Validation loss 0.12991835176944733 Accuracy 0.6489999890327454\n",
      "Iteration 23900 Training loss 0.09350841492414474 Validation loss 0.13388225436210632 Accuracy 0.6384999752044678\n",
      "Iteration 23910 Training loss 0.09148596227169037 Validation loss 0.12308382987976074 Accuracy 0.6639999747276306\n",
      "Iteration 23920 Training loss 0.09054872393608093 Validation loss 0.1268296241760254 Accuracy 0.6585000157356262\n",
      "Iteration 23930 Training loss 0.09217973053455353 Validation loss 0.13031049072742462 Accuracy 0.6489999890327454\n",
      "Iteration 23940 Training loss 0.09307053685188293 Validation loss 0.13294780254364014 Accuracy 0.6411666870117188\n",
      "Iteration 23950 Training loss 0.09209132194519043 Validation loss 0.1373303234577179 Accuracy 0.6345000267028809\n",
      "Iteration 23960 Training loss 0.09144482761621475 Validation loss 0.1288950890302658 Accuracy 0.6514999866485596\n",
      "Iteration 23970 Training loss 0.09072328358888626 Validation loss 0.13449957966804504 Accuracy 0.6355000138282776\n",
      "Iteration 23980 Training loss 0.08995513617992401 Validation loss 0.12754084169864655 Accuracy 0.6545000076293945\n",
      "Iteration 23990 Training loss 0.08754131942987442 Validation loss 0.12365590780973434 Accuracy 0.6629999876022339\n",
      "Iteration 24000 Training loss 0.08883018046617508 Validation loss 0.11710171401500702 Accuracy 0.6756666898727417\n",
      "Iteration 24010 Training loss 0.0890512466430664 Validation loss 0.1265769898891449 Accuracy 0.6571666598320007\n",
      "Iteration 24020 Training loss 0.0921378880739212 Validation loss 0.13585814833641052 Accuracy 0.6358333230018616\n",
      "Iteration 24030 Training loss 0.09323550760746002 Validation loss 0.13510634005069733 Accuracy 0.637666642665863\n",
      "Iteration 24040 Training loss 0.0910545364022255 Validation loss 0.12955425679683685 Accuracy 0.6506666541099548\n",
      "Iteration 24050 Training loss 0.09299331903457642 Validation loss 0.1319989562034607 Accuracy 0.6449999809265137\n",
      "Iteration 24060 Training loss 0.0932316929101944 Validation loss 0.13580118119716644 Accuracy 0.6353333592414856\n",
      "Iteration 24070 Training loss 0.09140070527791977 Validation loss 0.12949158251285553 Accuracy 0.6498333215713501\n",
      "Iteration 24080 Training loss 0.0901222825050354 Validation loss 0.1295033097267151 Accuracy 0.6496666669845581\n",
      "Iteration 24090 Training loss 0.09299541264772415 Validation loss 0.1271737515926361 Accuracy 0.6548333168029785\n",
      "Iteration 24100 Training loss 0.09067435562610626 Validation loss 0.1224297285079956 Accuracy 0.6675000190734863\n",
      "Iteration 24110 Training loss 0.0913548395037651 Validation loss 0.133426234126091 Accuracy 0.6418333053588867\n",
      "Iteration 24120 Training loss 0.09265122562646866 Validation loss 0.13143056631088257 Accuracy 0.6449999809265137\n",
      "Iteration 24130 Training loss 0.08819451928138733 Validation loss 0.12545336782932281 Accuracy 0.6586666703224182\n",
      "Iteration 24140 Training loss 0.09071789681911469 Validation loss 0.130970299243927 Accuracy 0.6461666822433472\n",
      "Iteration 24150 Training loss 0.09276558458805084 Validation loss 0.13501673936843872 Accuracy 0.6368333101272583\n",
      "Iteration 24160 Training loss 0.09259336441755295 Validation loss 0.1307661384344101 Accuracy 0.6464999914169312\n",
      "Iteration 24170 Training loss 0.08907277137041092 Validation loss 0.13108666241168976 Accuracy 0.6445000171661377\n",
      "Iteration 24180 Training loss 0.09138993173837662 Validation loss 0.12773999571800232 Accuracy 0.6536666750907898\n",
      "Iteration 24190 Training loss 0.09234605729579926 Validation loss 0.1299569457769394 Accuracy 0.6493333578109741\n",
      "Iteration 24200 Training loss 0.09310990571975708 Validation loss 0.13279223442077637 Accuracy 0.6414999961853027\n",
      "Iteration 24210 Training loss 0.0937572717666626 Validation loss 0.13472676277160645 Accuracy 0.6366666555404663\n",
      "Iteration 24220 Training loss 0.09277377277612686 Validation loss 0.129701167345047 Accuracy 0.6488333344459534\n",
      "Iteration 24230 Training loss 0.09187781810760498 Validation loss 0.1336805671453476 Accuracy 0.64083331823349\n",
      "Iteration 24240 Training loss 0.08933383226394653 Validation loss 0.12253759801387787 Accuracy 0.6658333539962769\n",
      "Iteration 24250 Training loss 0.09190952777862549 Validation loss 0.12939102947711945 Accuracy 0.6486666798591614\n",
      "Iteration 24260 Training loss 0.09196756035089493 Validation loss 0.13238339126110077 Accuracy 0.6428333520889282\n",
      "Iteration 24270 Training loss 0.09080544114112854 Validation loss 0.1300264298915863 Accuracy 0.6493333578109741\n",
      "Iteration 24280 Training loss 0.0927545353770256 Validation loss 0.13063281774520874 Accuracy 0.6464999914169312\n",
      "Iteration 24290 Training loss 0.09008469432592392 Validation loss 0.1286018043756485 Accuracy 0.6545000076293945\n",
      "Iteration 24300 Training loss 0.08777201175689697 Validation loss 0.12169773131608963 Accuracy 0.6664999723434448\n",
      "Iteration 24310 Training loss 0.0877961739897728 Validation loss 0.1156652420759201 Accuracy 0.6813333630561829\n",
      "Iteration 24320 Training loss 0.09116008132696152 Validation loss 0.11380619555711746 Accuracy 0.6833333373069763\n",
      "Iteration 24330 Training loss 0.09013934433460236 Validation loss 0.11650901287794113 Accuracy 0.6798333525657654\n",
      "Iteration 24340 Training loss 0.09088645130395889 Validation loss 0.11254671216011047 Accuracy 0.6836666464805603\n",
      "Iteration 24350 Training loss 0.08985595405101776 Validation loss 0.1127314418554306 Accuracy 0.6826666593551636\n",
      "Iteration 24360 Training loss 0.09141948074102402 Validation loss 0.11265240609645844 Accuracy 0.6831666827201843\n",
      "Iteration 24370 Training loss 0.09020259231328964 Validation loss 0.11415091902017593 Accuracy 0.6825000047683716\n",
      "Iteration 24380 Training loss 0.09204605966806412 Validation loss 0.11259116977453232 Accuracy 0.6828333139419556\n",
      "Iteration 24390 Training loss 0.09511920064687729 Validation loss 0.11315342038869858 Accuracy 0.6806666851043701\n",
      "Iteration 24400 Training loss 0.09071720391511917 Validation loss 0.11251384764909744 Accuracy 0.6848333477973938\n",
      "Iteration 24410 Training loss 0.09265348315238953 Validation loss 0.1133672297000885 Accuracy 0.6838333606719971\n",
      "Iteration 24420 Training loss 0.09094256907701492 Validation loss 0.1123528853058815 Accuracy 0.684333324432373\n",
      "Iteration 24430 Training loss 0.09629783034324646 Validation loss 0.11253226548433304 Accuracy 0.6828333139419556\n",
      "Iteration 24440 Training loss 0.0882859155535698 Validation loss 0.11346007138490677 Accuracy 0.6828333139419556\n",
      "Iteration 24450 Training loss 0.09307307749986649 Validation loss 0.11224108934402466 Accuracy 0.6836666464805603\n",
      "Iteration 24460 Training loss 0.0925723984837532 Validation loss 0.11216992139816284 Accuracy 0.6830000281333923\n",
      "Iteration 24470 Training loss 0.08904518932104111 Validation loss 0.11576703190803528 Accuracy 0.6781666874885559\n",
      "Iteration 24480 Training loss 0.09088443964719772 Validation loss 0.13113150000572205 Accuracy 0.6446666717529297\n",
      "Iteration 24490 Training loss 0.08904243260622025 Validation loss 0.12685361504554749 Accuracy 0.656166672706604\n",
      "Iteration 24500 Training loss 0.09101071953773499 Validation loss 0.1342277079820633 Accuracy 0.6386666893959045\n",
      "Iteration 24510 Training loss 0.09028106927871704 Validation loss 0.13195568323135376 Accuracy 0.643666684627533\n",
      "Iteration 24520 Training loss 0.09003782272338867 Validation loss 0.12313088029623032 Accuracy 0.6628333330154419\n",
      "Iteration 24530 Training loss 0.09127955138683319 Validation loss 0.13324043154716492 Accuracy 0.6391666531562805\n",
      "Iteration 24540 Training loss 0.09532870352268219 Validation loss 0.137760728597641 Accuracy 0.6328333616256714\n",
      "Iteration 24550 Training loss 0.09077564626932144 Validation loss 0.12434954941272736 Accuracy 0.6613333225250244\n",
      "Iteration 24560 Training loss 0.08904188871383667 Validation loss 0.12261119484901428 Accuracy 0.6646666526794434\n",
      "Iteration 24570 Training loss 0.09289125353097916 Validation loss 0.1345093548297882 Accuracy 0.6388333439826965\n",
      "Iteration 24580 Training loss 0.09212683886289597 Validation loss 0.12741398811340332 Accuracy 0.6543333530426025\n",
      "Iteration 24590 Training loss 0.0887763500213623 Validation loss 0.12442509829998016 Accuracy 0.659500002861023\n",
      "Iteration 24600 Training loss 0.094805508852005 Validation loss 0.13620783388614655 Accuracy 0.6366666555404663\n",
      "Iteration 24610 Training loss 0.09144148975610733 Validation loss 0.12963221967220306 Accuracy 0.6506666541099548\n",
      "Iteration 24620 Training loss 0.09292972087860107 Validation loss 0.1351788491010666 Accuracy 0.6378333568572998\n",
      "Iteration 24630 Training loss 0.09033100306987762 Validation loss 0.12714125216007233 Accuracy 0.6556666493415833\n",
      "Iteration 24640 Training loss 0.089754618704319 Validation loss 0.12509487569332123 Accuracy 0.6585000157356262\n",
      "Iteration 24650 Training loss 0.09490228444337845 Validation loss 0.13955113291740417 Accuracy 0.6313333511352539\n",
      "Iteration 24660 Training loss 0.089597187936306 Validation loss 0.12497851252555847 Accuracy 0.6598333120346069\n",
      "Iteration 24670 Training loss 0.09043271094560623 Validation loss 0.12435777485370636 Accuracy 0.6610000133514404\n",
      "Iteration 24680 Training loss 0.09021203964948654 Validation loss 0.12546730041503906 Accuracy 0.6585000157356262\n",
      "Iteration 24690 Training loss 0.09107202291488647 Validation loss 0.13098806142807007 Accuracy 0.6463333368301392\n",
      "Iteration 24700 Training loss 0.09161824733018875 Validation loss 0.13337111473083496 Accuracy 0.640666663646698\n",
      "Iteration 24710 Training loss 0.09120957553386688 Validation loss 0.12695196270942688 Accuracy 0.6566666960716248\n",
      "Iteration 24720 Training loss 0.08824731409549713 Validation loss 0.12662933766841888 Accuracy 0.6556666493415833\n",
      "Iteration 24730 Training loss 0.09539013355970383 Validation loss 0.1435588002204895 Accuracy 0.6226666569709778\n",
      "Iteration 24740 Training loss 0.09036070853471756 Validation loss 0.1301649510860443 Accuracy 0.6481666564941406\n",
      "Iteration 24750 Training loss 0.09348420053720474 Validation loss 0.13239027559757233 Accuracy 0.6430000066757202\n",
      "Iteration 24760 Training loss 0.09184708446264267 Validation loss 0.13467969000339508 Accuracy 0.637666642665863\n",
      "Iteration 24770 Training loss 0.09103003144264221 Validation loss 0.12486646324396133 Accuracy 0.6604999899864197\n",
      "Iteration 24780 Training loss 0.08885042369365692 Validation loss 0.11664530634880066 Accuracy 0.6779999732971191\n",
      "Iteration 24790 Training loss 0.08867405354976654 Validation loss 0.12567974627017975 Accuracy 0.6588333249092102\n",
      "Iteration 24800 Training loss 0.08738497644662857 Validation loss 0.12127058953046799 Accuracy 0.6679999828338623\n",
      "Iteration 24810 Training loss 0.09085314720869064 Validation loss 0.14230358600616455 Accuracy 0.6240000128746033\n",
      "Iteration 24820 Training loss 0.09482796490192413 Validation loss 0.13233479857444763 Accuracy 0.643833339214325\n",
      "Iteration 24830 Training loss 0.0892672911286354 Validation loss 0.12433715164661407 Accuracy 0.6618333458900452\n",
      "Iteration 24840 Training loss 0.09218619018793106 Validation loss 0.13360700011253357 Accuracy 0.6401666402816772\n",
      "Iteration 24850 Training loss 0.08949446678161621 Validation loss 0.12930928170681 Accuracy 0.6504999995231628\n",
      "Iteration 24860 Training loss 0.09046223759651184 Validation loss 0.12607896327972412 Accuracy 0.659500002861023\n",
      "Iteration 24870 Training loss 0.0889430120587349 Validation loss 0.1219228133559227 Accuracy 0.6666666865348816\n",
      "Iteration 24880 Training loss 0.0904264748096466 Validation loss 0.12656718492507935 Accuracy 0.6575000286102295\n",
      "Iteration 24890 Training loss 0.09125343710184097 Validation loss 0.13024088740348816 Accuracy 0.6473333239555359\n",
      "Iteration 24900 Training loss 0.08914388716220856 Validation loss 0.12968601286411285 Accuracy 0.6498333215713501\n",
      "Iteration 24910 Training loss 0.09214820712804794 Validation loss 0.13270677626132965 Accuracy 0.6411666870117188\n",
      "Iteration 24920 Training loss 0.08992082625627518 Validation loss 0.1279142200946808 Accuracy 0.6518333554267883\n",
      "Iteration 24930 Training loss 0.09153003245592117 Validation loss 0.1326095163822174 Accuracy 0.6430000066757202\n",
      "Iteration 24940 Training loss 0.08996041119098663 Validation loss 0.1298448145389557 Accuracy 0.6493333578109741\n",
      "Iteration 24950 Training loss 0.09440929442644119 Validation loss 0.1374659240245819 Accuracy 0.6338333487510681\n",
      "Iteration 24960 Training loss 0.08903267234563828 Validation loss 0.12924763560295105 Accuracy 0.6501666903495789\n",
      "Iteration 24970 Training loss 0.08970098942518234 Validation loss 0.12422048300504684 Accuracy 0.6613333225250244\n",
      "Iteration 24980 Training loss 0.08878228813409805 Validation loss 0.12185897678136826 Accuracy 0.6663333177566528\n",
      "Iteration 24990 Training loss 0.09314296394586563 Validation loss 0.13361485302448273 Accuracy 0.640500009059906\n",
      "Iteration 25000 Training loss 0.09379056841135025 Validation loss 0.13701802492141724 Accuracy 0.6355000138282776\n",
      "Iteration 25010 Training loss 0.08993532508611679 Validation loss 0.13101458549499512 Accuracy 0.6466666460037231\n",
      "Iteration 25020 Training loss 0.08838210254907608 Validation loss 0.12123636901378632 Accuracy 0.6675000190734863\n",
      "Iteration 25030 Training loss 0.09083061665296555 Validation loss 0.13070662319660187 Accuracy 0.6474999785423279\n",
      "Iteration 25040 Training loss 0.08757372945547104 Validation loss 0.1305922418832779 Accuracy 0.6468333601951599\n",
      "Iteration 25050 Training loss 0.08860556036233902 Validation loss 0.11991553753614426 Accuracy 0.6691666841506958\n",
      "Iteration 25060 Training loss 0.09433364868164062 Validation loss 0.13152682781219482 Accuracy 0.6458333134651184\n",
      "Iteration 25070 Training loss 0.09242292493581772 Validation loss 0.1339917778968811 Accuracy 0.640666663646698\n",
      "Iteration 25080 Training loss 0.09073273837566376 Validation loss 0.1252363920211792 Accuracy 0.659333348274231\n",
      "Iteration 25090 Training loss 0.09453843533992767 Validation loss 0.13970080018043518 Accuracy 0.6291666626930237\n",
      "Iteration 25100 Training loss 0.09029724448919296 Validation loss 0.12736210227012634 Accuracy 0.6551666855812073\n",
      "Iteration 25110 Training loss 0.09099479019641876 Validation loss 0.13201124966144562 Accuracy 0.6451666951179504\n",
      "Iteration 25120 Training loss 0.09424251317977905 Validation loss 0.13462573289871216 Accuracy 0.6391666531562805\n",
      "Iteration 25130 Training loss 0.09174197167158127 Validation loss 0.12310793995857239 Accuracy 0.6638333201408386\n",
      "Iteration 25140 Training loss 0.0931013897061348 Validation loss 0.1310698240995407 Accuracy 0.6478333473205566\n",
      "Iteration 25150 Training loss 0.09039908647537231 Validation loss 0.1332228034734726 Accuracy 0.6401666402816772\n",
      "Iteration 25160 Training loss 0.08982447534799576 Validation loss 0.1304919272661209 Accuracy 0.6483333110809326\n",
      "Iteration 25170 Training loss 0.09113626182079315 Validation loss 0.13143010437488556 Accuracy 0.6464999914169312\n",
      "Iteration 25180 Training loss 0.08991657197475433 Validation loss 0.12736019492149353 Accuracy 0.6539999842643738\n",
      "Iteration 25190 Training loss 0.088615782558918 Validation loss 0.12420446425676346 Accuracy 0.6621666550636292\n",
      "Iteration 25200 Training loss 0.0882117748260498 Validation loss 0.11766285449266434 Accuracy 0.6754999756813049\n",
      "Iteration 25210 Training loss 0.09235310554504395 Validation loss 0.11311185359954834 Accuracy 0.6815000176429749\n",
      "Iteration 25220 Training loss 0.09213317930698395 Validation loss 0.11284545063972473 Accuracy 0.6826666593551636\n",
      "Iteration 25230 Training loss 0.09222535043954849 Validation loss 0.11243132501840591 Accuracy 0.6815000176429749\n",
      "Iteration 25240 Training loss 0.08820027858018875 Validation loss 0.11413682252168655 Accuracy 0.6825000047683716\n",
      "Iteration 25250 Training loss 0.08737357705831528 Validation loss 0.11979681253433228 Accuracy 0.6713333129882812\n",
      "Iteration 25260 Training loss 0.0931658148765564 Validation loss 0.13141480088233948 Accuracy 0.6476666927337646\n",
      "Iteration 25270 Training loss 0.0899248942732811 Validation loss 0.1333037167787552 Accuracy 0.640500009059906\n",
      "Iteration 25280 Training loss 0.08992915600538254 Validation loss 0.1291433572769165 Accuracy 0.6520000100135803\n",
      "Iteration 25290 Training loss 0.08999484032392502 Validation loss 0.1276158094406128 Accuracy 0.6553333401679993\n",
      "Iteration 25300 Training loss 0.08935666084289551 Validation loss 0.11663392931222916 Accuracy 0.6791666746139526\n",
      "Iteration 25310 Training loss 0.08931230008602142 Validation loss 0.11512646824121475 Accuracy 0.6821666955947876\n",
      "Iteration 25320 Training loss 0.09615743160247803 Validation loss 0.1128496304154396 Accuracy 0.6798333525657654\n",
      "Iteration 25330 Training loss 0.09253942221403122 Validation loss 0.11244873702526093 Accuracy 0.6821666955947876\n",
      "Iteration 25340 Training loss 0.0899866446852684 Validation loss 0.11350714415311813 Accuracy 0.6836666464805603\n",
      "Iteration 25350 Training loss 0.09293066710233688 Validation loss 0.112468421459198 Accuracy 0.684333324432373\n",
      "Iteration 25360 Training loss 0.0920000821352005 Validation loss 0.11458540707826614 Accuracy 0.6830000281333923\n",
      "Iteration 25370 Training loss 0.08729404956102371 Validation loss 0.11665932089090347 Accuracy 0.6775000095367432\n",
      "Iteration 25380 Training loss 0.08980680257081985 Validation loss 0.11313606053590775 Accuracy 0.6826666593551636\n",
      "Iteration 25390 Training loss 0.08868110179901123 Validation loss 0.1138833612203598 Accuracy 0.684333324432373\n",
      "Iteration 25400 Training loss 0.09155195206403732 Validation loss 0.11417059600353241 Accuracy 0.684499979019165\n",
      "Iteration 25410 Training loss 0.09311471879482269 Validation loss 0.11241253465414047 Accuracy 0.684333324432373\n",
      "Iteration 25420 Training loss 0.09833632409572601 Validation loss 0.11441916227340698 Accuracy 0.6763333082199097\n",
      "Iteration 25430 Training loss 0.09120185673236847 Validation loss 0.11236808449029922 Accuracy 0.684166669845581\n",
      "Iteration 25440 Training loss 0.09006495028734207 Validation loss 0.1125553622841835 Accuracy 0.6850000023841858\n",
      "Iteration 25450 Training loss 0.09126507490873337 Validation loss 0.11249309033155441 Accuracy 0.684333324432373\n",
      "Iteration 25460 Training loss 0.08956144005060196 Validation loss 0.11256308853626251 Accuracy 0.684499979019165\n",
      "Iteration 25470 Training loss 0.0943002700805664 Validation loss 0.11228971183300018 Accuracy 0.6833333373069763\n",
      "Iteration 25480 Training loss 0.09207256883382797 Validation loss 0.11220865696668625 Accuracy 0.6833333373069763\n",
      "Iteration 25490 Training loss 0.0913097932934761 Validation loss 0.11307348310947418 Accuracy 0.6833333373069763\n",
      "Iteration 25500 Training loss 0.09122247248888016 Validation loss 0.11213122308254242 Accuracy 0.6834999918937683\n",
      "Iteration 25510 Training loss 0.08961274474859238 Validation loss 0.11541831493377686 Accuracy 0.6811666488647461\n",
      "Iteration 25520 Training loss 0.08984775096178055 Validation loss 0.11375012248754501 Accuracy 0.6848333477973938\n",
      "Iteration 25530 Training loss 0.09255614876747131 Validation loss 0.11224827915430069 Accuracy 0.684166669845581\n",
      "Iteration 25540 Training loss 0.08930301666259766 Validation loss 0.1157446801662445 Accuracy 0.6809999942779541\n",
      "Iteration 25550 Training loss 0.08881965279579163 Validation loss 0.11919328570365906 Accuracy 0.6710000038146973\n",
      "Iteration 25560 Training loss 0.0896492749452591 Validation loss 0.1258930116891861 Accuracy 0.6581666469573975\n",
      "Iteration 25570 Training loss 0.09179467707872391 Validation loss 0.13449135422706604 Accuracy 0.6383333206176758\n",
      "Iteration 25580 Training loss 0.09164107590913773 Validation loss 0.14053025841712952 Accuracy 0.6276666522026062\n",
      "Iteration 25590 Training loss 0.08820322901010513 Validation loss 0.12437552958726883 Accuracy 0.6618333458900452\n",
      "Iteration 25600 Training loss 0.08952346444129944 Validation loss 0.128562793135643 Accuracy 0.6524999737739563\n",
      "Iteration 25610 Training loss 0.08997108787298203 Validation loss 0.126653254032135 Accuracy 0.6554999947547913\n",
      "Iteration 25620 Training loss 0.09445499628782272 Validation loss 0.13626731932163239 Accuracy 0.6359999775886536\n",
      "Iteration 25630 Training loss 0.09168379753828049 Validation loss 0.13090121746063232 Accuracy 0.6470000147819519\n",
      "Iteration 25640 Training loss 0.08641057461500168 Validation loss 0.1181262955069542 Accuracy 0.6746666431427002\n",
      "Iteration 25650 Training loss 0.08951874077320099 Validation loss 0.12827648222446442 Accuracy 0.6523333191871643\n",
      "Iteration 25660 Training loss 0.08917559683322906 Validation loss 0.13026390969753265 Accuracy 0.6481666564941406\n",
      "Iteration 25670 Training loss 0.08900526165962219 Validation loss 0.1300741583108902 Accuracy 0.6485000252723694\n",
      "Iteration 25680 Training loss 0.08983449637889862 Validation loss 0.1282445639371872 Accuracy 0.6524999737739563\n",
      "Iteration 25690 Training loss 0.08776287734508514 Validation loss 0.127251535654068 Accuracy 0.6558333039283752\n",
      "Iteration 25700 Training loss 0.0902961939573288 Validation loss 0.11351491510868073 Accuracy 0.6823333501815796\n",
      "Iteration 25710 Training loss 0.08641466498374939 Validation loss 0.11766094714403152 Accuracy 0.674833357334137\n",
      "Iteration 25720 Training loss 0.08968517184257507 Validation loss 0.1272837072610855 Accuracy 0.6546666622161865\n",
      "Iteration 25730 Training loss 0.09160733968019485 Validation loss 0.13066403567790985 Accuracy 0.6485000252723694\n",
      "Iteration 25740 Training loss 0.09064476191997528 Validation loss 0.12914638221263885 Accuracy 0.6513333320617676\n",
      "Iteration 25750 Training loss 0.09475148469209671 Validation loss 0.13334587216377258 Accuracy 0.640666663646698\n",
      "Iteration 25760 Training loss 0.09078055620193481 Validation loss 0.13300453126430511 Accuracy 0.6418333053588867\n",
      "Iteration 25770 Training loss 0.09148087352514267 Validation loss 0.1314268857240677 Accuracy 0.6464999914169312\n",
      "Iteration 25780 Training loss 0.09195034950971603 Validation loss 0.1337520331144333 Accuracy 0.6386666893959045\n",
      "Iteration 25790 Training loss 0.08887036889791489 Validation loss 0.13044826686382294 Accuracy 0.6466666460037231\n",
      "Iteration 25800 Training loss 0.0917259082198143 Validation loss 0.13116395473480225 Accuracy 0.6458333134651184\n",
      "Iteration 25810 Training loss 0.09242144972085953 Validation loss 0.1261710673570633 Accuracy 0.6568333506584167\n",
      "Iteration 25820 Training loss 0.09158725291490555 Validation loss 0.13192012906074524 Accuracy 0.643833339214325\n",
      "Iteration 25830 Training loss 0.09314841032028198 Validation loss 0.12918660044670105 Accuracy 0.6510000228881836\n",
      "Iteration 25840 Training loss 0.09168890118598938 Validation loss 0.13684815168380737 Accuracy 0.6348333358764648\n",
      "Iteration 25850 Training loss 0.09170136600732803 Validation loss 0.13048803806304932 Accuracy 0.6495000123977661\n",
      "Iteration 25860 Training loss 0.08922125399112701 Validation loss 0.12560944259166718 Accuracy 0.6600000262260437\n",
      "Iteration 25870 Training loss 0.0922318547964096 Validation loss 0.13735200464725494 Accuracy 0.6334999799728394\n",
      "Iteration 25880 Training loss 0.08783047646284103 Validation loss 0.11584638804197311 Accuracy 0.6800000071525574\n",
      "Iteration 25890 Training loss 0.08883946388959885 Validation loss 0.13186964392662048 Accuracy 0.6463333368301392\n",
      "Iteration 25900 Training loss 0.09037899971008301 Validation loss 0.13452576100826263 Accuracy 0.6395000219345093\n",
      "Iteration 25910 Training loss 0.09207543730735779 Validation loss 0.13460375368595123 Accuracy 0.6389999985694885\n",
      "Iteration 25920 Training loss 0.0900549590587616 Validation loss 0.12634702026844025 Accuracy 0.6575000286102295\n",
      "Iteration 25930 Training loss 0.08920291811227798 Validation loss 0.1286735087633133 Accuracy 0.6514999866485596\n",
      "Iteration 25940 Training loss 0.0932251363992691 Validation loss 0.13424688577651978 Accuracy 0.640666663646698\n",
      "Iteration 25950 Training loss 0.09117213636636734 Validation loss 0.1297348290681839 Accuracy 0.6504999995231628\n",
      "Iteration 25960 Training loss 0.08966632187366486 Validation loss 0.12044461816549301 Accuracy 0.6683333516120911\n",
      "Iteration 25970 Training loss 0.09070813655853271 Validation loss 0.12675489485263824 Accuracy 0.656333327293396\n",
      "Iteration 25980 Training loss 0.09526346623897552 Validation loss 0.14067360758781433 Accuracy 0.6296666860580444\n",
      "Iteration 25990 Training loss 0.08791787922382355 Validation loss 0.12591639161109924 Accuracy 0.6585000157356262\n",
      "Iteration 26000 Training loss 0.09066242724657059 Validation loss 0.12433896213769913 Accuracy 0.6635000109672546\n",
      "Iteration 26010 Training loss 0.09120074659585953 Validation loss 0.13331721723079681 Accuracy 0.640500009059906\n",
      "Iteration 26020 Training loss 0.09259411692619324 Validation loss 0.13443651795387268 Accuracy 0.6395000219345093\n",
      "Iteration 26030 Training loss 0.08988548070192337 Validation loss 0.1255521923303604 Accuracy 0.6598333120346069\n",
      "Iteration 26040 Training loss 0.09057528525590897 Validation loss 0.1282169222831726 Accuracy 0.6524999737739563\n",
      "Iteration 26050 Training loss 0.09221790730953217 Validation loss 0.12923792004585266 Accuracy 0.6511666774749756\n",
      "Iteration 26060 Training loss 0.09346127510070801 Validation loss 0.1359713077545166 Accuracy 0.637333333492279\n",
      "Iteration 26070 Training loss 0.09026651829481125 Validation loss 0.13034187257289886 Accuracy 0.6496666669845581\n",
      "Iteration 26080 Training loss 0.0937747210264206 Validation loss 0.13147032260894775 Accuracy 0.6443333625793457\n",
      "Iteration 26090 Training loss 0.09002596139907837 Validation loss 0.12854820489883423 Accuracy 0.6506666541099548\n",
      "Iteration 26100 Training loss 0.09148624539375305 Validation loss 0.13184423744678497 Accuracy 0.6455000042915344\n",
      "Iteration 26110 Training loss 0.0926131159067154 Validation loss 0.13501782715320587 Accuracy 0.6380000114440918\n",
      "Iteration 26120 Training loss 0.08891808986663818 Validation loss 0.12550410628318787 Accuracy 0.659333348274231\n",
      "Iteration 26130 Training loss 0.08724842220544815 Validation loss 0.12232756614685059 Accuracy 0.6658333539962769\n",
      "Iteration 26140 Training loss 0.09162997454404831 Validation loss 0.13246849179267883 Accuracy 0.643833339214325\n",
      "Iteration 26150 Training loss 0.0900067538022995 Validation loss 0.13153740763664246 Accuracy 0.6445000171661377\n",
      "Iteration 26160 Training loss 0.09122993797063828 Validation loss 0.13295607268810272 Accuracy 0.6414999961853027\n",
      "Iteration 26170 Training loss 0.09019909799098969 Validation loss 0.12706004083156586 Accuracy 0.6558333039283752\n",
      "Iteration 26180 Training loss 0.08859645575284958 Validation loss 0.11999398469924927 Accuracy 0.6691666841506958\n",
      "Iteration 26190 Training loss 0.08892468363046646 Validation loss 0.11857204884290695 Accuracy 0.6740000247955322\n",
      "Iteration 26200 Training loss 0.08961503952741623 Validation loss 0.11460459232330322 Accuracy 0.6813333630561829\n",
      "Iteration 26210 Training loss 0.08982518315315247 Validation loss 0.11284436285495758 Accuracy 0.6850000023841858\n",
      "Iteration 26220 Training loss 0.09240352362394333 Validation loss 0.11255379021167755 Accuracy 0.6851666569709778\n",
      "Iteration 26230 Training loss 0.0901748538017273 Validation loss 0.11485224217176437 Accuracy 0.6816666722297668\n",
      "Iteration 26240 Training loss 0.09131616353988647 Validation loss 0.11379553377628326 Accuracy 0.6838333606719971\n",
      "Iteration 26250 Training loss 0.09221246093511581 Validation loss 0.11248773336410522 Accuracy 0.6831666827201843\n",
      "Iteration 26260 Training loss 0.09080265462398529 Validation loss 0.11405334621667862 Accuracy 0.6826666593551636\n",
      "Iteration 26270 Training loss 0.0929071232676506 Validation loss 0.11251315474510193 Accuracy 0.6836666464805603\n",
      "Iteration 26280 Training loss 0.0925729051232338 Validation loss 0.11241227388381958 Accuracy 0.6834999918937683\n",
      "Iteration 26290 Training loss 0.08998840302228928 Validation loss 0.11284851282835007 Accuracy 0.6840000152587891\n",
      "Iteration 26300 Training loss 0.08950170874595642 Validation loss 0.113080233335495 Accuracy 0.684333324432373\n",
      "Iteration 26310 Training loss 0.08878405392169952 Validation loss 0.11700063198804855 Accuracy 0.6781666874885559\n",
      "Iteration 26320 Training loss 0.08909910172224045 Validation loss 0.12223273515701294 Accuracy 0.6668333411216736\n",
      "Iteration 26330 Training loss 0.09132850170135498 Validation loss 0.12953318655490875 Accuracy 0.6504999995231628\n",
      "Iteration 26340 Training loss 0.09409627318382263 Validation loss 0.13546203076839447 Accuracy 0.6388333439826965\n",
      "Iteration 26350 Training loss 0.08864054083824158 Validation loss 0.12175474315881729 Accuracy 0.6676666736602783\n",
      "Iteration 26360 Training loss 0.09068495035171509 Validation loss 0.12744908034801483 Accuracy 0.6549999713897705\n",
      "Iteration 26370 Training loss 0.09342962503433228 Validation loss 0.13385722041130066 Accuracy 0.6388333439826965\n",
      "Iteration 26380 Training loss 0.08770450949668884 Validation loss 0.1272166669368744 Accuracy 0.6545000076293945\n",
      "Iteration 26390 Training loss 0.09005444496870041 Validation loss 0.13061194121837616 Accuracy 0.6489999890327454\n",
      "Iteration 26400 Training loss 0.09071069210767746 Validation loss 0.1323225051164627 Accuracy 0.6445000171661377\n",
      "Iteration 26410 Training loss 0.09102928638458252 Validation loss 0.12473095953464508 Accuracy 0.6635000109672546\n",
      "Iteration 26420 Training loss 0.09001178294420242 Validation loss 0.13039712607860565 Accuracy 0.6483333110809326\n",
      "Iteration 26430 Training loss 0.09309463202953339 Validation loss 0.13045275211334229 Accuracy 0.6504999995231628\n",
      "Iteration 26440 Training loss 0.09096549451351166 Validation loss 0.1296878457069397 Accuracy 0.6503333449363708\n",
      "Iteration 26450 Training loss 0.09218670427799225 Validation loss 0.13393279910087585 Accuracy 0.6395000219345093\n",
      "Iteration 26460 Training loss 0.0895649641752243 Validation loss 0.12731438875198364 Accuracy 0.6548333168029785\n",
      "Iteration 26470 Training loss 0.08820711076259613 Validation loss 0.12112428992986679 Accuracy 0.6685000061988831\n",
      "Iteration 26480 Training loss 0.0892167016863823 Validation loss 0.12264703214168549 Accuracy 0.6658333539962769\n",
      "Iteration 26490 Training loss 0.09105782955884933 Validation loss 0.1262044608592987 Accuracy 0.6566666960716248\n",
      "Iteration 26500 Training loss 0.08914244920015335 Validation loss 0.12756189703941345 Accuracy 0.653333306312561\n",
      "Iteration 26510 Training loss 0.0951685681939125 Validation loss 0.1372966468334198 Accuracy 0.6365000009536743\n",
      "Iteration 26520 Training loss 0.09041495621204376 Validation loss 0.13348883390426636 Accuracy 0.6414999961853027\n",
      "Iteration 26530 Training loss 0.09332320094108582 Validation loss 0.13045530021190643 Accuracy 0.6488333344459534\n",
      "Iteration 26540 Training loss 0.09077087044715881 Validation loss 0.13174143433570862 Accuracy 0.6449999809265137\n",
      "Iteration 26550 Training loss 0.09409105777740479 Validation loss 0.13329827785491943 Accuracy 0.6411666870117188\n",
      "Iteration 26560 Training loss 0.09190349280834198 Validation loss 0.13064147531986237 Accuracy 0.6480000019073486\n",
      "Iteration 26570 Training loss 0.0911882147192955 Validation loss 0.13069826364517212 Accuracy 0.6480000019073486\n",
      "Iteration 26580 Training loss 0.08565200865268707 Validation loss 0.12354914098978043 Accuracy 0.6646666526794434\n",
      "Iteration 26590 Training loss 0.08704731613397598 Validation loss 0.1212693452835083 Accuracy 0.6701666712760925\n",
      "Iteration 26600 Training loss 0.08947651833295822 Validation loss 0.12864089012145996 Accuracy 0.6524999737739563\n",
      "Iteration 26610 Training loss 0.09086037427186966 Validation loss 0.13433925807476044 Accuracy 0.6395000219345093\n",
      "Iteration 26620 Training loss 0.09019647538661957 Validation loss 0.13325166702270508 Accuracy 0.6401666402816772\n",
      "Iteration 26630 Training loss 0.08814682066440582 Validation loss 0.1274382472038269 Accuracy 0.6551666855812073\n",
      "Iteration 26640 Training loss 0.09227218478918076 Validation loss 0.13425786793231964 Accuracy 0.6396666765213013\n",
      "Iteration 26650 Training loss 0.08898663520812988 Validation loss 0.13153015077114105 Accuracy 0.6463333368301392\n",
      "Iteration 26660 Training loss 0.08704632520675659 Validation loss 0.12631715834140778 Accuracy 0.656333327293396\n",
      "Iteration 26670 Training loss 0.0923607349395752 Validation loss 0.1343017816543579 Accuracy 0.640333354473114\n",
      "Iteration 26680 Training loss 0.09466054290533066 Validation loss 0.13275469839572906 Accuracy 0.6421666741371155\n",
      "Iteration 26690 Training loss 0.09163382649421692 Validation loss 0.12997688353061676 Accuracy 0.6495000123977661\n",
      "Iteration 26700 Training loss 0.08987148851156235 Validation loss 0.12775564193725586 Accuracy 0.6551666855812073\n",
      "Iteration 26710 Training loss 0.09226907044649124 Validation loss 0.135679692029953 Accuracy 0.6371666789054871\n",
      "Iteration 26720 Training loss 0.09114114195108414 Validation loss 0.1299109011888504 Accuracy 0.6506666541099548\n",
      "Iteration 26730 Training loss 0.08856568485498428 Validation loss 0.12438833713531494 Accuracy 0.6620000004768372\n",
      "Iteration 26740 Training loss 0.09269870817661285 Validation loss 0.1325758695602417 Accuracy 0.6428333520889282\n",
      "Iteration 26750 Training loss 0.09063441306352615 Validation loss 0.12817712128162384 Accuracy 0.6528333425521851\n",
      "Iteration 26760 Training loss 0.0892859697341919 Validation loss 0.12667685747146606 Accuracy 0.656499981880188\n",
      "Iteration 26770 Training loss 0.08919332176446915 Validation loss 0.12249274551868439 Accuracy 0.6671666502952576\n",
      "Iteration 26780 Training loss 0.09229312837123871 Validation loss 0.13299328088760376 Accuracy 0.6423333287239075\n",
      "Iteration 26790 Training loss 0.0949280709028244 Validation loss 0.13809442520141602 Accuracy 0.6351666450500488\n",
      "Iteration 26800 Training loss 0.08712046593427658 Validation loss 0.12042726576328278 Accuracy 0.6698333621025085\n",
      "Iteration 26810 Training loss 0.08765147626399994 Validation loss 0.12408648431301117 Accuracy 0.6621666550636292\n",
      "Iteration 26820 Training loss 0.0906810611486435 Validation loss 0.12080856412649155 Accuracy 0.6708333492279053\n",
      "Iteration 26830 Training loss 0.08909673243761063 Validation loss 0.12221632152795792 Accuracy 0.6673333048820496\n",
      "Iteration 26840 Training loss 0.08989819139242172 Validation loss 0.11467104405164719 Accuracy 0.6831666827201843\n",
      "Iteration 26850 Training loss 0.09388618916273117 Validation loss 0.1133994311094284 Accuracy 0.6811666488647461\n",
      "Iteration 26860 Training loss 0.0942755714058876 Validation loss 0.11256760358810425 Accuracy 0.684333324432373\n",
      "Iteration 26870 Training loss 0.09050554037094116 Validation loss 0.11318366229534149 Accuracy 0.6859999895095825\n",
      "Iteration 26880 Training loss 0.08858484774827957 Validation loss 0.1143442764878273 Accuracy 0.6826666593551636\n",
      "Iteration 26890 Training loss 0.09046679735183716 Validation loss 0.11337389796972275 Accuracy 0.6846666932106018\n",
      "Iteration 26900 Training loss 0.09375955909490585 Validation loss 0.11283280700445175 Accuracy 0.6833333373069763\n",
      "Iteration 26910 Training loss 0.09202501177787781 Validation loss 0.11233966797590256 Accuracy 0.6828333139419556\n",
      "Iteration 26920 Training loss 0.09099601209163666 Validation loss 0.11322038620710373 Accuracy 0.6826666593551636\n",
      "Iteration 26930 Training loss 0.08826020359992981 Validation loss 0.11429599672555923 Accuracy 0.684166669845581\n",
      "Iteration 26940 Training loss 0.09377942979335785 Validation loss 0.11242556571960449 Accuracy 0.6846666932106018\n",
      "Iteration 26950 Training loss 0.09496773034334183 Validation loss 0.11270079761743546 Accuracy 0.6821666955947876\n",
      "Iteration 26960 Training loss 0.08918990194797516 Validation loss 0.1165926530957222 Accuracy 0.6786666512489319\n",
      "Iteration 26970 Training loss 0.08720681071281433 Validation loss 0.12213221937417984 Accuracy 0.6676666736602783\n",
      "Iteration 26980 Training loss 0.0919274091720581 Validation loss 0.1373351812362671 Accuracy 0.6346666812896729\n",
      "Iteration 26990 Training loss 0.08974161744117737 Validation loss 0.12556985020637512 Accuracy 0.6598333120346069\n",
      "Iteration 27000 Training loss 0.08893565833568573 Validation loss 0.12423063814640045 Accuracy 0.6635000109672546\n",
      "Iteration 27010 Training loss 0.08954411745071411 Validation loss 0.12709759175777435 Accuracy 0.656000018119812\n",
      "Iteration 27020 Training loss 0.09540893882513046 Validation loss 0.13896001875400543 Accuracy 0.6345000267028809\n",
      "Iteration 27030 Training loss 0.08966974169015884 Validation loss 0.13362035155296326 Accuracy 0.6411666870117188\n",
      "Iteration 27040 Training loss 0.0902823656797409 Validation loss 0.13298439979553223 Accuracy 0.6411666870117188\n",
      "Iteration 27050 Training loss 0.09176315367221832 Validation loss 0.12570807337760925 Accuracy 0.6601666808128357\n",
      "Iteration 27060 Training loss 0.08808676898479462 Validation loss 0.12393186241388321 Accuracy 0.6625000238418579\n",
      "Iteration 27070 Training loss 0.09026356041431427 Validation loss 0.13189592957496643 Accuracy 0.6446666717529297\n",
      "Iteration 27080 Training loss 0.0935281440615654 Validation loss 0.13482604920864105 Accuracy 0.6391666531562805\n",
      "Iteration 27090 Training loss 0.09150455892086029 Validation loss 0.11975608021020889 Accuracy 0.6703333258628845\n",
      "Iteration 27100 Training loss 0.08888330310583115 Validation loss 0.12827196717262268 Accuracy 0.6543333530426025\n",
      "Iteration 27110 Training loss 0.08644998073577881 Validation loss 0.12160776555538177 Accuracy 0.6679999828338623\n",
      "Iteration 27120 Training loss 0.09053226560354233 Validation loss 0.11661852151155472 Accuracy 0.6781666874885559\n",
      "Iteration 27130 Training loss 0.09166017919778824 Validation loss 0.11257227510213852 Accuracy 0.6833333373069763\n",
      "Iteration 27140 Training loss 0.09261250495910645 Validation loss 0.11287014931440353 Accuracy 0.6840000152587891\n",
      "Iteration 27150 Training loss 0.0898127630352974 Validation loss 0.11274654418230057 Accuracy 0.6851666569709778\n",
      "Iteration 27160 Training loss 0.08720525354146957 Validation loss 0.11366163939237595 Accuracy 0.6856666803359985\n",
      "Iteration 27170 Training loss 0.09647251665592194 Validation loss 0.11270277947187424 Accuracy 0.6815000176429749\n",
      "Iteration 27180 Training loss 0.09025571495294571 Validation loss 0.11275357753038406 Accuracy 0.6838333606719971\n",
      "Iteration 27190 Training loss 0.09067757427692413 Validation loss 0.11328325420618057 Accuracy 0.6846666932106018\n",
      "Iteration 27200 Training loss 0.0918879508972168 Validation loss 0.11255665868520737 Accuracy 0.6834999918937683\n",
      "Iteration 27210 Training loss 0.0908302441239357 Validation loss 0.11297476291656494 Accuracy 0.684499979019165\n",
      "Iteration 27220 Training loss 0.09113828837871552 Validation loss 0.11345554888248444 Accuracy 0.6826666593551636\n",
      "Iteration 27230 Training loss 0.08823692053556442 Validation loss 0.1137898787856102 Accuracy 0.6853333115577698\n",
      "Iteration 27240 Training loss 0.08844640851020813 Validation loss 0.11537405103445053 Accuracy 0.6828333139419556\n",
      "Iteration 27250 Training loss 0.08961545675992966 Validation loss 0.11259513348340988 Accuracy 0.684166669845581\n",
      "Iteration 27260 Training loss 0.09241195768117905 Validation loss 0.11232011765241623 Accuracy 0.6840000152587891\n",
      "Iteration 27270 Training loss 0.08953000605106354 Validation loss 0.11271744966506958 Accuracy 0.6846666932106018\n",
      "Iteration 27280 Training loss 0.0888248085975647 Validation loss 0.11286631226539612 Accuracy 0.6840000152587891\n",
      "Iteration 27290 Training loss 0.09028341621160507 Validation loss 0.11266351491212845 Accuracy 0.6859999895095825\n",
      "Iteration 27300 Training loss 0.08713746815919876 Validation loss 0.11516518145799637 Accuracy 0.6811666488647461\n",
      "Iteration 27310 Training loss 0.09169240295886993 Validation loss 0.11259031295776367 Accuracy 0.6853333115577698\n",
      "Iteration 27320 Training loss 0.09016457200050354 Validation loss 0.11281207203865051 Accuracy 0.6869999766349792\n",
      "Iteration 27330 Training loss 0.09011074155569077 Validation loss 0.11276643723249435 Accuracy 0.6861666440963745\n",
      "Iteration 27340 Training loss 0.09080479294061661 Validation loss 0.11395302414894104 Accuracy 0.6826666593551636\n",
      "Iteration 27350 Training loss 0.08991945534944534 Validation loss 0.11302535235881805 Accuracy 0.6855000257492065\n",
      "Iteration 27360 Training loss 0.09003209322690964 Validation loss 0.1125846579670906 Accuracy 0.6853333115577698\n",
      "Iteration 27370 Training loss 0.09121515601873398 Validation loss 0.1123819574713707 Accuracy 0.6846666932106018\n",
      "Iteration 27380 Training loss 0.08841323852539062 Validation loss 0.11379475146532059 Accuracy 0.6850000023841858\n",
      "Iteration 27390 Training loss 0.08950351923704147 Validation loss 0.11267955601215363 Accuracy 0.6853333115577698\n",
      "Iteration 27400 Training loss 0.08905889838933945 Validation loss 0.11373910307884216 Accuracy 0.684499979019165\n",
      "Iteration 27410 Training loss 0.08582331240177155 Validation loss 0.11375585198402405 Accuracy 0.6836666464805603\n",
      "Iteration 27420 Training loss 0.08908697217702866 Validation loss 0.11492880433797836 Accuracy 0.6826666593551636\n",
      "Iteration 27430 Training loss 0.08791215717792511 Validation loss 0.12301992624998093 Accuracy 0.6653333306312561\n",
      "Iteration 27440 Training loss 0.09316673129796982 Validation loss 0.1379741132259369 Accuracy 0.6341666579246521\n",
      "Iteration 27450 Training loss 0.08876281976699829 Validation loss 0.1264173537492752 Accuracy 0.6575000286102295\n",
      "Iteration 27460 Training loss 0.08753218501806259 Validation loss 0.12537835538387299 Accuracy 0.6601666808128357\n",
      "Iteration 27470 Training loss 0.08956526964902878 Validation loss 0.13602173328399658 Accuracy 0.6380000114440918\n",
      "Iteration 27480 Training loss 0.08925551176071167 Validation loss 0.12594270706176758 Accuracy 0.6604999899864197\n",
      "Iteration 27490 Training loss 0.09083766490221024 Validation loss 0.1274929642677307 Accuracy 0.6546666622161865\n",
      "Iteration 27500 Training loss 0.0899796262383461 Validation loss 0.1346151977777481 Accuracy 0.6384999752044678\n",
      "Iteration 27510 Training loss 0.09105570614337921 Validation loss 0.13057716190814972 Accuracy 0.6491666436195374\n",
      "Iteration 27520 Training loss 0.09295922517776489 Validation loss 0.13506962358951569 Accuracy 0.6389999985694885\n",
      "Iteration 27530 Training loss 0.0916600376367569 Validation loss 0.13192681968212128 Accuracy 0.6448333263397217\n",
      "Iteration 27540 Training loss 0.09039431065320969 Validation loss 0.12963885068893433 Accuracy 0.6511666774749756\n",
      "Iteration 27550 Training loss 0.09031771123409271 Validation loss 0.1242465004324913 Accuracy 0.6620000004768372\n",
      "Iteration 27560 Training loss 0.08907558023929596 Validation loss 0.12811613082885742 Accuracy 0.6545000076293945\n",
      "Iteration 27570 Training loss 0.09251214563846588 Validation loss 0.13722354173660278 Accuracy 0.6356666684150696\n",
      "Iteration 27580 Training loss 0.09034106135368347 Validation loss 0.12854035198688507 Accuracy 0.6516666412353516\n",
      "Iteration 27590 Training loss 0.08940037339925766 Validation loss 0.13051220774650574 Accuracy 0.6496666669845581\n",
      "Iteration 27600 Training loss 0.09164152294397354 Validation loss 0.1310151368379593 Accuracy 0.6486666798591614\n",
      "Iteration 27610 Training loss 0.0906686782836914 Validation loss 0.13100239634513855 Accuracy 0.6455000042915344\n",
      "Iteration 27620 Training loss 0.08970163017511368 Validation loss 0.13072186708450317 Accuracy 0.6496666669845581\n",
      "Iteration 27630 Training loss 0.08740348368883133 Validation loss 0.12424390017986298 Accuracy 0.6614999771118164\n",
      "Iteration 27640 Training loss 0.08751560747623444 Validation loss 0.12746663391590118 Accuracy 0.6543333530426025\n",
      "Iteration 27650 Training loss 0.09115687757730484 Validation loss 0.1330125480890274 Accuracy 0.6418333053588867\n",
      "Iteration 27660 Training loss 0.0932711586356163 Validation loss 0.13757938146591187 Accuracy 0.6341666579246521\n",
      "Iteration 27670 Training loss 0.09261415898799896 Validation loss 0.1353646218776703 Accuracy 0.6398333311080933\n",
      "Iteration 27680 Training loss 0.0888194814324379 Validation loss 0.12886513769626617 Accuracy 0.653166651725769\n",
      "Iteration 27690 Training loss 0.08772478997707367 Validation loss 0.12196806073188782 Accuracy 0.6676666736602783\n",
      "Iteration 27700 Training loss 0.08710858225822449 Validation loss 0.12620170414447784 Accuracy 0.6583333611488342\n",
      "Iteration 27710 Training loss 0.08835465461015701 Validation loss 0.12688355147838593 Accuracy 0.6573333144187927\n",
      "Iteration 27720 Training loss 0.0887584537267685 Validation loss 0.12526553869247437 Accuracy 0.659500002861023\n",
      "Iteration 27730 Training loss 0.09199865907430649 Validation loss 0.1286143809556961 Accuracy 0.6546666622161865\n",
      "Iteration 27740 Training loss 0.09332002699375153 Validation loss 0.137447789311409 Accuracy 0.6356666684150696\n",
      "Iteration 27750 Training loss 0.08824723958969116 Validation loss 0.12254592031240463 Accuracy 0.6668333411216736\n",
      "Iteration 27760 Training loss 0.0884045735001564 Validation loss 0.12653391063213348 Accuracy 0.6576666831970215\n",
      "Iteration 27770 Training loss 0.08896257728338242 Validation loss 0.12681491672992706 Accuracy 0.6553333401679993\n",
      "Iteration 27780 Training loss 0.09175363928079605 Validation loss 0.12514609098434448 Accuracy 0.6601666808128357\n",
      "Iteration 27790 Training loss 0.08895039558410645 Validation loss 0.12589681148529053 Accuracy 0.6578333377838135\n",
      "Iteration 27800 Training loss 0.09321559965610504 Validation loss 0.1327483206987381 Accuracy 0.643666684627533\n",
      "Iteration 27810 Training loss 0.09269902855157852 Validation loss 0.1372612565755844 Accuracy 0.6356666684150696\n",
      "Iteration 27820 Training loss 0.09183589369058609 Validation loss 0.13108575344085693 Accuracy 0.6464999914169312\n",
      "Iteration 27830 Training loss 0.09305141866207123 Validation loss 0.13673174381256104 Accuracy 0.6368333101272583\n",
      "Iteration 27840 Training loss 0.08706799149513245 Validation loss 0.1275167167186737 Accuracy 0.6543333530426025\n",
      "Iteration 27850 Training loss 0.08782920241355896 Validation loss 0.1297636777162552 Accuracy 0.6506666541099548\n",
      "Iteration 27860 Training loss 0.09007961302995682 Validation loss 0.13065503537654877 Accuracy 0.6471666693687439\n",
      "Iteration 27870 Training loss 0.08851829916238785 Validation loss 0.1305456906557083 Accuracy 0.6488333344459534\n",
      "Iteration 27880 Training loss 0.09007319808006287 Validation loss 0.12391305714845657 Accuracy 0.6628333330154419\n",
      "Iteration 27890 Training loss 0.09107578545808792 Validation loss 0.13199453055858612 Accuracy 0.6471666693687439\n",
      "Iteration 27900 Training loss 0.08975827693939209 Validation loss 0.12881886959075928 Accuracy 0.653166651725769\n",
      "Iteration 27910 Training loss 0.09215014427900314 Validation loss 0.13901101052761078 Accuracy 0.6326666474342346\n",
      "Iteration 27920 Training loss 0.09090610593557358 Validation loss 0.13084541261196136 Accuracy 0.6485000252723694\n",
      "Iteration 27930 Training loss 0.08750054985284805 Validation loss 0.1231902688741684 Accuracy 0.6664999723434448\n",
      "Iteration 27940 Training loss 0.08816937357187271 Validation loss 0.12762802839279175 Accuracy 0.6545000076293945\n",
      "Iteration 27950 Training loss 0.0892188549041748 Validation loss 0.12918047606945038 Accuracy 0.6523333191871643\n",
      "Iteration 27960 Training loss 0.09097635746002197 Validation loss 0.12941874563694 Accuracy 0.6513333320617676\n",
      "Iteration 27970 Training loss 0.09231217205524445 Validation loss 0.13446709513664246 Accuracy 0.640666663646698\n",
      "Iteration 27980 Training loss 0.08796333521604538 Validation loss 0.12880073487758636 Accuracy 0.6518333554267883\n",
      "Iteration 27990 Training loss 0.0885993018746376 Validation loss 0.12400494515895844 Accuracy 0.6626666784286499\n",
      "Iteration 28000 Training loss 0.08744171261787415 Validation loss 0.12290426343679428 Accuracy 0.6669999957084656\n",
      "Iteration 28010 Training loss 0.08955170959234238 Validation loss 0.13142672181129456 Accuracy 0.6476666927337646\n",
      "Iteration 28020 Training loss 0.08901295810937881 Validation loss 0.13360989093780518 Accuracy 0.6426666378974915\n",
      "Iteration 28030 Training loss 0.08694343268871307 Validation loss 0.12225475907325745 Accuracy 0.6666666865348816\n",
      "Iteration 28040 Training loss 0.08820105344057083 Validation loss 0.11721312254667282 Accuracy 0.6775000095367432\n",
      "Iteration 28050 Training loss 0.09410033375024796 Validation loss 0.11279364675283432 Accuracy 0.6833333373069763\n",
      "Iteration 28060 Training loss 0.0912688598036766 Validation loss 0.11269370466470718 Accuracy 0.684333324432373\n",
      "Iteration 28070 Training loss 0.08863286674022675 Validation loss 0.1158330887556076 Accuracy 0.6811666488647461\n",
      "Iteration 28080 Training loss 0.08678855746984482 Validation loss 0.12024936825037003 Accuracy 0.6694999933242798\n",
      "Iteration 28090 Training loss 0.09187136590480804 Validation loss 0.13266746699810028 Accuracy 0.6439999938011169\n",
      "Iteration 28100 Training loss 0.0912066251039505 Validation loss 0.1343984305858612 Accuracy 0.6401666402816772\n",
      "Iteration 28110 Training loss 0.09399808943271637 Validation loss 0.14133618772029877 Accuracy 0.6291666626930237\n",
      "Iteration 28120 Training loss 0.09151898324489594 Validation loss 0.13218125700950623 Accuracy 0.6443333625793457\n",
      "Iteration 28130 Training loss 0.09211711585521698 Validation loss 0.13232555985450745 Accuracy 0.643666684627533\n",
      "Iteration 28140 Training loss 0.09117686748504639 Validation loss 0.13021108508110046 Accuracy 0.6499999761581421\n",
      "Iteration 28150 Training loss 0.09024019539356232 Validation loss 0.12643617391586304 Accuracy 0.6556666493415833\n",
      "Iteration 28160 Training loss 0.0879579484462738 Validation loss 0.12882551550865173 Accuracy 0.6526666879653931\n",
      "Iteration 28170 Training loss 0.09292202442884445 Validation loss 0.13435423374176025 Accuracy 0.6401666402816772\n",
      "Iteration 28180 Training loss 0.08866851776838303 Validation loss 0.1284773200750351 Accuracy 0.6541666388511658\n",
      "Iteration 28190 Training loss 0.08979416638612747 Validation loss 0.1334100365638733 Accuracy 0.6414999961853027\n",
      "Iteration 28200 Training loss 0.091071717441082 Validation loss 0.13646668195724487 Accuracy 0.6365000009536743\n",
      "Iteration 28210 Training loss 0.09068387001752853 Validation loss 0.13322213292121887 Accuracy 0.6418333053588867\n",
      "Iteration 28220 Training loss 0.09046941995620728 Validation loss 0.13109782338142395 Accuracy 0.6474999785423279\n",
      "Iteration 28230 Training loss 0.08755853027105331 Validation loss 0.1321585327386856 Accuracy 0.6473333239555359\n",
      "Iteration 28240 Training loss 0.09006432443857193 Validation loss 0.13336730003356934 Accuracy 0.640333354473114\n",
      "Iteration 28250 Training loss 0.08842337131500244 Validation loss 0.1270764172077179 Accuracy 0.6553333401679993\n",
      "Iteration 28260 Training loss 0.09005136787891388 Validation loss 0.12523998320102692 Accuracy 0.6583333611488342\n",
      "Iteration 28270 Training loss 0.08680162578821182 Validation loss 0.11779317259788513 Accuracy 0.6761666536331177\n",
      "Iteration 28280 Training loss 0.08695530146360397 Validation loss 0.11596681922674179 Accuracy 0.6801666617393494\n",
      "Iteration 28290 Training loss 0.0894009992480278 Validation loss 0.11507009714841843 Accuracy 0.6828333139419556\n",
      "Iteration 28300 Training loss 0.09386259317398071 Validation loss 0.11313463002443314 Accuracy 0.6826666593551636\n",
      "Iteration 28310 Training loss 0.09230075776576996 Validation loss 0.11255520582199097 Accuracy 0.684166669845581\n",
      "Iteration 28320 Training loss 0.09075329452753067 Validation loss 0.11245838552713394 Accuracy 0.6838333606719971\n",
      "Iteration 28330 Training loss 0.0898355096578598 Validation loss 0.11316777765750885 Accuracy 0.6838333606719971\n",
      "Iteration 28340 Training loss 0.08775405585765839 Validation loss 0.11444757878780365 Accuracy 0.684499979019165\n",
      "Iteration 28350 Training loss 0.08737939596176147 Validation loss 0.11647573113441467 Accuracy 0.6806666851043701\n",
      "Iteration 28360 Training loss 0.09359235316514969 Validation loss 0.1374184936285019 Accuracy 0.6356666684150696\n",
      "Iteration 28370 Training loss 0.09198470413684845 Validation loss 0.13165508210659027 Accuracy 0.6453333497047424\n",
      "Iteration 28380 Training loss 0.08938053250312805 Validation loss 0.1262703239917755 Accuracy 0.656000018119812\n",
      "Iteration 28390 Training loss 0.09341108053922653 Validation loss 0.13608060777187347 Accuracy 0.6356666684150696\n",
      "Iteration 28400 Training loss 0.08844504505395889 Validation loss 0.12578703463077545 Accuracy 0.6596666574478149\n",
      "Iteration 28410 Training loss 0.08559244871139526 Validation loss 0.1187274381518364 Accuracy 0.6733333468437195\n",
      "Iteration 28420 Training loss 0.09076857566833496 Validation loss 0.1300111711025238 Accuracy 0.6504999995231628\n",
      "Iteration 28430 Training loss 0.09309997409582138 Validation loss 0.14021840691566467 Accuracy 0.6315000057220459\n",
      "Iteration 28440 Training loss 0.09060990065336227 Validation loss 0.12874947488307953 Accuracy 0.6535000205039978\n",
      "Iteration 28450 Training loss 0.0884622260928154 Validation loss 0.12403988838195801 Accuracy 0.6626666784286499\n",
      "Iteration 28460 Training loss 0.08946286886930466 Validation loss 0.13421978056430817 Accuracy 0.6401666402816772\n",
      "Iteration 28470 Training loss 0.09174645692110062 Validation loss 0.12902016937732697 Accuracy 0.6518333554267883\n",
      "Iteration 28480 Training loss 0.09293153882026672 Validation loss 0.1394028663635254 Accuracy 0.6315000057220459\n",
      "Iteration 28490 Training loss 0.09604211151599884 Validation loss 0.14133359491825104 Accuracy 0.6309999823570251\n",
      "Iteration 28500 Training loss 0.08930947631597519 Validation loss 0.1272607445716858 Accuracy 0.6558333039283752\n",
      "Iteration 28510 Training loss 0.0890643447637558 Validation loss 0.12615929543972015 Accuracy 0.6583333611488342\n",
      "Iteration 28520 Training loss 0.08899062126874924 Validation loss 0.12923060357570648 Accuracy 0.652999997138977\n",
      "Iteration 28530 Training loss 0.08796125650405884 Validation loss 0.12909047305583954 Accuracy 0.652999997138977\n",
      "Iteration 28540 Training loss 0.09572618454694748 Validation loss 0.13812355697155 Accuracy 0.6328333616256714\n",
      "Iteration 28550 Training loss 0.0909779742360115 Validation loss 0.1341753602027893 Accuracy 0.640500009059906\n",
      "Iteration 28560 Training loss 0.09129709750413895 Validation loss 0.13564717769622803 Accuracy 0.6368333101272583\n",
      "Iteration 28570 Training loss 0.08987804502248764 Validation loss 0.12847542762756348 Accuracy 0.6528333425521851\n",
      "Iteration 28580 Training loss 0.08976180106401443 Validation loss 0.12962613999843597 Accuracy 0.6535000205039978\n",
      "Iteration 28590 Training loss 0.087872214615345 Validation loss 0.1236923485994339 Accuracy 0.6636666655540466\n",
      "Iteration 28600 Training loss 0.0897650346159935 Validation loss 0.12971548736095428 Accuracy 0.6528333425521851\n",
      "Iteration 28610 Training loss 0.08942051231861115 Validation loss 0.126766175031662 Accuracy 0.656499981880188\n",
      "Iteration 28620 Training loss 0.08825259655714035 Validation loss 0.11719914525747299 Accuracy 0.6791666746139526\n",
      "Iteration 28630 Training loss 0.08739496767520905 Validation loss 0.1172226220369339 Accuracy 0.6783333420753479\n",
      "Iteration 28640 Training loss 0.08845043927431107 Validation loss 0.11360262334346771 Accuracy 0.6853333115577698\n",
      "Iteration 28650 Training loss 0.09434372931718826 Validation loss 0.11275958269834518 Accuracy 0.6834999918937683\n",
      "Iteration 28660 Training loss 0.08970408141613007 Validation loss 0.11251737177371979 Accuracy 0.6834999918937683\n",
      "Iteration 28670 Training loss 0.08935920894145966 Validation loss 0.11287762224674225 Accuracy 0.6851666569709778\n",
      "Iteration 28680 Training loss 0.09196718037128448 Validation loss 0.11261890828609467 Accuracy 0.6833333373069763\n",
      "Iteration 28690 Training loss 0.08996564894914627 Validation loss 0.11245384067296982 Accuracy 0.6840000152587891\n",
      "Iteration 28700 Training loss 0.0852891355752945 Validation loss 0.11640997231006622 Accuracy 0.6806666851043701\n",
      "Iteration 28710 Training loss 0.08814465999603271 Validation loss 0.1249048039317131 Accuracy 0.6613333225250244\n",
      "Iteration 28720 Training loss 0.08913274109363556 Validation loss 0.11679355800151825 Accuracy 0.6793333292007446\n",
      "Iteration 28730 Training loss 0.09508264809846878 Validation loss 0.11276756972074509 Accuracy 0.6846666932106018\n",
      "Iteration 28740 Training loss 0.08984319865703583 Validation loss 0.11296374350786209 Accuracy 0.6859999895095825\n",
      "Iteration 28750 Training loss 0.08875482529401779 Validation loss 0.11330686509609222 Accuracy 0.6838333606719971\n",
      "Iteration 28760 Training loss 0.08724921941757202 Validation loss 0.11692414432764053 Accuracy 0.6783333420753479\n",
      "Iteration 28770 Training loss 0.08564973622560501 Validation loss 0.1152641773223877 Accuracy 0.6815000176429749\n",
      "Iteration 28780 Training loss 0.08821559697389603 Validation loss 0.12305416166782379 Accuracy 0.6664999723434448\n",
      "Iteration 28790 Training loss 0.09240364283323288 Validation loss 0.13830812275409698 Accuracy 0.6358333230018616\n",
      "Iteration 28800 Training loss 0.09568659216165543 Validation loss 0.14307750761508942 Accuracy 0.6258333325386047\n",
      "Iteration 28810 Training loss 0.09015433490276337 Validation loss 0.1318807601928711 Accuracy 0.6466666460037231\n",
      "Iteration 28820 Training loss 0.08815818279981613 Validation loss 0.12346337735652924 Accuracy 0.6641666889190674\n",
      "Iteration 28830 Training loss 0.08724502474069595 Validation loss 0.11842209100723267 Accuracy 0.675000011920929\n",
      "Iteration 28840 Training loss 0.08753257989883423 Validation loss 0.12253136932849884 Accuracy 0.6671666502952576\n",
      "Iteration 28850 Training loss 0.09704171866178513 Validation loss 0.14840735495090485 Accuracy 0.6134999990463257\n",
      "Iteration 28860 Training loss 0.08920121192932129 Validation loss 0.13174188137054443 Accuracy 0.6443333625793457\n",
      "Iteration 28870 Training loss 0.08696194738149643 Validation loss 0.1278548240661621 Accuracy 0.6546666622161865\n",
      "Iteration 28880 Training loss 0.08974898606538773 Validation loss 0.12900809943675995 Accuracy 0.6524999737739563\n",
      "Iteration 28890 Training loss 0.09228666871786118 Validation loss 0.13579116761684418 Accuracy 0.6381666660308838\n",
      "Iteration 28900 Training loss 0.08974210917949677 Validation loss 0.1306317299604416 Accuracy 0.6498333215713501\n",
      "Iteration 28910 Training loss 0.09223055839538574 Validation loss 0.13690318167209625 Accuracy 0.6365000009536743\n",
      "Iteration 28920 Training loss 0.08694586902856827 Validation loss 0.12635351717472076 Accuracy 0.6568333506584167\n",
      "Iteration 28930 Training loss 0.08972755074501038 Validation loss 0.13178583979606628 Accuracy 0.6470000147819519\n",
      "Iteration 28940 Training loss 0.0908842533826828 Validation loss 0.13469350337982178 Accuracy 0.6414999961853027\n",
      "Iteration 28950 Training loss 0.09027847647666931 Validation loss 0.1331217885017395 Accuracy 0.6418333053588867\n",
      "Iteration 28960 Training loss 0.08720315247774124 Validation loss 0.12168093025684357 Accuracy 0.6683333516120911\n",
      "Iteration 28970 Training loss 0.08806315064430237 Validation loss 0.11943099647760391 Accuracy 0.6710000038146973\n",
      "Iteration 28980 Training loss 0.08630702644586563 Validation loss 0.11662431806325912 Accuracy 0.6800000071525574\n",
      "Iteration 28990 Training loss 0.08600504696369171 Validation loss 0.11750245839357376 Accuracy 0.6783333420753479\n",
      "Iteration 29000 Training loss 0.085672527551651 Validation loss 0.11697591096162796 Accuracy 0.6794999837875366\n",
      "Iteration 29010 Training loss 0.09113126993179321 Validation loss 0.13308274745941162 Accuracy 0.6456666588783264\n",
      "Iteration 29020 Training loss 0.09108240902423859 Validation loss 0.1350085735321045 Accuracy 0.6399999856948853\n",
      "Iteration 29030 Training loss 0.09023107588291168 Validation loss 0.12947183847427368 Accuracy 0.6520000100135803\n",
      "Iteration 29040 Training loss 0.09491732716560364 Validation loss 0.13476477563381195 Accuracy 0.6384999752044678\n",
      "Iteration 29050 Training loss 0.08739069849252701 Validation loss 0.12531042098999023 Accuracy 0.6600000262260437\n",
      "Iteration 29060 Training loss 0.09106077253818512 Validation loss 0.13094796240329742 Accuracy 0.6485000252723694\n",
      "Iteration 29070 Training loss 0.08869750797748566 Validation loss 0.1271323412656784 Accuracy 0.6566666960716248\n",
      "Iteration 29080 Training loss 0.0910453051328659 Validation loss 0.13573019206523895 Accuracy 0.6378333568572998\n",
      "Iteration 29090 Training loss 0.08725471049547195 Validation loss 0.1288256049156189 Accuracy 0.6538333296775818\n",
      "Iteration 29100 Training loss 0.08894888311624527 Validation loss 0.12505264580249786 Accuracy 0.6601666808128357\n",
      "Iteration 29110 Training loss 0.08871535211801529 Validation loss 0.12474661320447922 Accuracy 0.6610000133514404\n",
      "Iteration 29120 Training loss 0.08743029832839966 Validation loss 0.1247919499874115 Accuracy 0.6604999899864197\n",
      "Iteration 29130 Training loss 0.0936930850148201 Validation loss 0.13692829012870789 Accuracy 0.6366666555404663\n",
      "Iteration 29140 Training loss 0.08771364390850067 Validation loss 0.13223329186439514 Accuracy 0.6456666588783264\n",
      "Iteration 29150 Training loss 0.09162221103906631 Validation loss 0.1299896240234375 Accuracy 0.6508333086967468\n",
      "Iteration 29160 Training loss 0.08875437080860138 Validation loss 0.12966343760490417 Accuracy 0.653166651725769\n",
      "Iteration 29170 Training loss 0.08878863602876663 Validation loss 0.13181915879249573 Accuracy 0.6458333134651184\n",
      "Iteration 29180 Training loss 0.08948730677366257 Validation loss 0.1290397197008133 Accuracy 0.6528333425521851\n",
      "Iteration 29190 Training loss 0.09493615478277206 Validation loss 0.13708901405334473 Accuracy 0.6355000138282776\n",
      "Iteration 29200 Training loss 0.08841481804847717 Validation loss 0.12645386159420013 Accuracy 0.6573333144187927\n",
      "Iteration 29210 Training loss 0.09172437340021133 Validation loss 0.1306535303592682 Accuracy 0.6499999761581421\n",
      "Iteration 29220 Training loss 0.09130879491567612 Validation loss 0.13426309823989868 Accuracy 0.6413333415985107\n",
      "Iteration 29230 Training loss 0.09111683070659637 Validation loss 0.13162709772586823 Accuracy 0.6470000147819519\n",
      "Iteration 29240 Training loss 0.0916648581624031 Validation loss 0.13291576504707336 Accuracy 0.6431666612625122\n",
      "Iteration 29250 Training loss 0.09149935096502304 Validation loss 0.13609521090984344 Accuracy 0.637499988079071\n",
      "Iteration 29260 Training loss 0.08725950866937637 Validation loss 0.12445378303527832 Accuracy 0.6635000109672546\n",
      "Iteration 29270 Training loss 0.08805473148822784 Validation loss 0.12654991447925568 Accuracy 0.6554999947547913\n",
      "Iteration 29280 Training loss 0.09248124063014984 Validation loss 0.13811485469341278 Accuracy 0.6334999799728394\n",
      "Iteration 29290 Training loss 0.08748414367437363 Validation loss 0.12718318402767181 Accuracy 0.6554999947547913\n",
      "Iteration 29300 Training loss 0.08733921498060226 Validation loss 0.12314672768115997 Accuracy 0.6661666631698608\n",
      "Iteration 29310 Training loss 0.0891583263874054 Validation loss 0.12609140574932098 Accuracy 0.6568333506584167\n",
      "Iteration 29320 Training loss 0.08991870284080505 Validation loss 0.12911704182624817 Accuracy 0.6523333191871643\n",
      "Iteration 29330 Training loss 0.09201469272375107 Validation loss 0.13440659642219543 Accuracy 0.6423333287239075\n",
      "Iteration 29340 Training loss 0.09343916177749634 Validation loss 0.1342639923095703 Accuracy 0.6416666507720947\n",
      "Iteration 29350 Training loss 0.0890740379691124 Validation loss 0.13032162189483643 Accuracy 0.6503333449363708\n",
      "Iteration 29360 Training loss 0.08824897557497025 Validation loss 0.12790539860725403 Accuracy 0.6543333530426025\n",
      "Iteration 29370 Training loss 0.0876498818397522 Validation loss 0.1250186562538147 Accuracy 0.6620000004768372\n",
      "Iteration 29380 Training loss 0.0891352966427803 Validation loss 0.12566359341144562 Accuracy 0.6583333611488342\n",
      "Iteration 29390 Training loss 0.08708415925502777 Validation loss 0.12697814404964447 Accuracy 0.656000018119812\n",
      "Iteration 29400 Training loss 0.09625232964754105 Validation loss 0.1420421004295349 Accuracy 0.6278333067893982\n",
      "Iteration 29410 Training loss 0.0864141508936882 Validation loss 0.12844334542751312 Accuracy 0.6546666622161865\n",
      "Iteration 29420 Training loss 0.09184468537569046 Validation loss 0.13691604137420654 Accuracy 0.6359999775886536\n",
      "Iteration 29430 Training loss 0.0913156121969223 Validation loss 0.13200196623802185 Accuracy 0.6449999809265137\n",
      "Iteration 29440 Training loss 0.09078331291675568 Validation loss 0.13029664754867554 Accuracy 0.6506666541099548\n",
      "Iteration 29450 Training loss 0.08945874124765396 Validation loss 0.11732488125562668 Accuracy 0.6781666874885559\n",
      "Iteration 29460 Training loss 0.08841537684202194 Validation loss 0.11311706900596619 Accuracy 0.684499979019165\n",
      "Iteration 29470 Training loss 0.09473856538534164 Validation loss 0.11265767365694046 Accuracy 0.6850000023841858\n",
      "Iteration 29480 Training loss 0.09180228412151337 Validation loss 0.11323413997888565 Accuracy 0.6855000257492065\n",
      "Iteration 29490 Training loss 0.08657936751842499 Validation loss 0.11320392787456512 Accuracy 0.6846666932106018\n",
      "Iteration 29500 Training loss 0.0894009917974472 Validation loss 0.11286845058202744 Accuracy 0.6866666674613953\n",
      "Iteration 29510 Training loss 0.09231455624103546 Validation loss 0.1124635860323906 Accuracy 0.6850000023841858\n",
      "Iteration 29520 Training loss 0.0894225686788559 Validation loss 0.11430422216653824 Accuracy 0.6825000047683716\n",
      "Iteration 29530 Training loss 0.08941179513931274 Validation loss 0.11288437992334366 Accuracy 0.6851666569709778\n",
      "Iteration 29540 Training loss 0.09301066398620605 Validation loss 0.11257151514291763 Accuracy 0.684499979019165\n",
      "Iteration 29550 Training loss 0.08986103534698486 Validation loss 0.11260170489549637 Accuracy 0.6850000023841858\n",
      "Iteration 29560 Training loss 0.08942858874797821 Validation loss 0.11299849301576614 Accuracy 0.6858333349227905\n",
      "Iteration 29570 Training loss 0.08633720874786377 Validation loss 0.11351920664310455 Accuracy 0.684166669845581\n",
      "Iteration 29580 Training loss 0.0879356786608696 Validation loss 0.11337842047214508 Accuracy 0.6850000023841858\n",
      "Iteration 29590 Training loss 0.09114303439855576 Validation loss 0.11278511583805084 Accuracy 0.6859999895095825\n",
      "Iteration 29600 Training loss 0.08962293714284897 Validation loss 0.11302807927131653 Accuracy 0.6851666569709778\n",
      "Iteration 29610 Training loss 0.08561292290687561 Validation loss 0.11456722021102905 Accuracy 0.6831666827201843\n",
      "Iteration 29620 Training loss 0.08692740648984909 Validation loss 0.11316677182912827 Accuracy 0.684333324432373\n",
      "Iteration 29630 Training loss 0.09168058633804321 Validation loss 0.11261192709207535 Accuracy 0.684333324432373\n",
      "Iteration 29640 Training loss 0.08855412155389786 Validation loss 0.11317586153745651 Accuracy 0.6840000152587891\n",
      "Iteration 29650 Training loss 0.0856400653719902 Validation loss 0.11484304070472717 Accuracy 0.6819999814033508\n",
      "Iteration 29660 Training loss 0.09011030197143555 Validation loss 0.1149144098162651 Accuracy 0.684499979019165\n",
      "Iteration 29670 Training loss 0.08579421043395996 Validation loss 0.11686403304338455 Accuracy 0.6803333163261414\n",
      "Iteration 29680 Training loss 0.08602061867713928 Validation loss 0.12686292827129364 Accuracy 0.656166672706604\n",
      "Iteration 29690 Training loss 0.09364856034517288 Validation loss 0.14260661602020264 Accuracy 0.628166675567627\n",
      "Iteration 29700 Training loss 0.09162003546953201 Validation loss 0.13493309915065765 Accuracy 0.6401666402816772\n",
      "Iteration 29710 Training loss 0.08595720678567886 Validation loss 0.1318919062614441 Accuracy 0.6464999914169312\n",
      "Iteration 29720 Training loss 0.08546949177980423 Validation loss 0.12281573563814163 Accuracy 0.6679999828338623\n",
      "Iteration 29730 Training loss 0.08749941736459732 Validation loss 0.12160158157348633 Accuracy 0.6713333129882812\n",
      "Iteration 29740 Training loss 0.08993363380432129 Validation loss 0.12993332743644714 Accuracy 0.6511666774749756\n",
      "Iteration 29750 Training loss 0.09051923453807831 Validation loss 0.13365986943244934 Accuracy 0.6426666378974915\n",
      "Iteration 29760 Training loss 0.08812282979488373 Validation loss 0.12889298796653748 Accuracy 0.6524999737739563\n",
      "Iteration 29770 Training loss 0.08916507661342621 Validation loss 0.13051776587963104 Accuracy 0.6486666798591614\n",
      "Iteration 29780 Training loss 0.09159211069345474 Validation loss 0.13866077363491058 Accuracy 0.6340000033378601\n",
      "Iteration 29790 Training loss 0.08912862837314606 Validation loss 0.12423043698072433 Accuracy 0.6641666889190674\n",
      "Iteration 29800 Training loss 0.08813289552927017 Validation loss 0.1222449317574501 Accuracy 0.6681666374206543\n",
      "Iteration 29810 Training loss 0.09022101759910583 Validation loss 0.1331660896539688 Accuracy 0.6443333625793457\n",
      "Iteration 29820 Training loss 0.09180077910423279 Validation loss 0.13509498536586761 Accuracy 0.6398333311080933\n",
      "Iteration 29830 Training loss 0.08940901607275009 Validation loss 0.12895812094211578 Accuracy 0.6543333530426025\n",
      "Iteration 29840 Training loss 0.08797728270292282 Validation loss 0.12761065363883972 Accuracy 0.6551666855812073\n",
      "Iteration 29850 Training loss 0.08886822313070297 Validation loss 0.1267755925655365 Accuracy 0.6553333401679993\n",
      "Iteration 29860 Training loss 0.09166038781404495 Validation loss 0.13117951154708862 Accuracy 0.6473333239555359\n",
      "Iteration 29870 Training loss 0.09041845053434372 Validation loss 0.13386861979961395 Accuracy 0.6433333158493042\n",
      "Iteration 29880 Training loss 0.08930617570877075 Validation loss 0.1253286749124527 Accuracy 0.659166693687439\n",
      "Iteration 29890 Training loss 0.09192295372486115 Validation loss 0.13359850645065308 Accuracy 0.6416666507720947\n",
      "Iteration 29900 Training loss 0.0863630399107933 Validation loss 0.12894786894321442 Accuracy 0.6526666879653931\n",
      "Iteration 29910 Training loss 0.08815960586071014 Validation loss 0.12976273894309998 Accuracy 0.6513333320617676\n",
      "Iteration 29920 Training loss 0.09396236389875412 Validation loss 0.13742612302303314 Accuracy 0.6353333592414856\n",
      "Iteration 29930 Training loss 0.08897344768047333 Validation loss 0.12624883651733398 Accuracy 0.656000018119812\n",
      "Iteration 29940 Training loss 0.08896408975124359 Validation loss 0.12849533557891846 Accuracy 0.6535000205039978\n",
      "Iteration 29950 Training loss 0.09034983068704605 Validation loss 0.13200516998767853 Accuracy 0.6466666460037231\n",
      "Iteration 29960 Training loss 0.08886412531137466 Validation loss 0.13018138706684113 Accuracy 0.6504999995231628\n",
      "Iteration 29970 Training loss 0.08828401565551758 Validation loss 0.13277696073055267 Accuracy 0.6445000171661377\n",
      "Iteration 29980 Training loss 0.09011256694793701 Validation loss 0.1321837604045868 Accuracy 0.6463333368301392\n",
      "Iteration 29990 Training loss 0.08593585342168808 Validation loss 0.12351135909557343 Accuracy 0.6653333306312561\n",
      "Iteration 30000 Training loss 0.08957641571760178 Validation loss 0.13215048611164093 Accuracy 0.6485000252723694\n",
      "Iteration 30010 Training loss 0.09019671380519867 Validation loss 0.13574467599391937 Accuracy 0.6378333568572998\n",
      "Iteration 30020 Training loss 0.08796911686658859 Validation loss 0.12855343520641327 Accuracy 0.6536666750907898\n",
      "Iteration 30030 Training loss 0.08648914843797684 Validation loss 0.1265885978937149 Accuracy 0.6576666831970215\n",
      "Iteration 30040 Training loss 0.08683434873819351 Validation loss 0.12579908967018127 Accuracy 0.6579999923706055\n",
      "Iteration 30050 Training loss 0.08992021530866623 Validation loss 0.12841066718101501 Accuracy 0.6536666750907898\n",
      "Iteration 30060 Training loss 0.0898713693022728 Validation loss 0.13117729127407074 Accuracy 0.6488333344459534\n",
      "Iteration 30070 Training loss 0.09212244302034378 Validation loss 0.1375586986541748 Accuracy 0.6358333230018616\n",
      "Iteration 30080 Training loss 0.08977989852428436 Validation loss 0.13224519789218903 Accuracy 0.6446666717529297\n",
      "Iteration 30090 Training loss 0.08723453432321548 Validation loss 0.12858431041240692 Accuracy 0.653166651725769\n",
      "Iteration 30100 Training loss 0.08987153321504593 Validation loss 0.1199178695678711 Accuracy 0.6735000014305115\n",
      "Iteration 30110 Training loss 0.09392653405666351 Validation loss 0.13442307710647583 Accuracy 0.6424999833106995\n",
      "Iteration 30120 Training loss 0.09123405069112778 Validation loss 0.13158808648586273 Accuracy 0.6478333473205566\n",
      "Iteration 30130 Training loss 0.08908042311668396 Validation loss 0.1298740804195404 Accuracy 0.6521666646003723\n",
      "Iteration 30140 Training loss 0.08845680952072144 Validation loss 0.12949560582637787 Accuracy 0.653166651725769\n",
      "Iteration 30150 Training loss 0.088038370013237 Validation loss 0.12860946357250214 Accuracy 0.6541666388511658\n",
      "Iteration 30160 Training loss 0.09076455980539322 Validation loss 0.13221386075019836 Accuracy 0.6443333625793457\n",
      "Iteration 30170 Training loss 0.08457740396261215 Validation loss 0.12041206657886505 Accuracy 0.6729999780654907\n",
      "Iteration 30180 Training loss 0.09049881994724274 Validation loss 0.12757614254951477 Accuracy 0.6554999947547913\n",
      "Iteration 30190 Training loss 0.09063645452260971 Validation loss 0.1344553828239441 Accuracy 0.640999972820282\n",
      "Iteration 30200 Training loss 0.09216656535863876 Validation loss 0.13446399569511414 Accuracy 0.6413333415985107\n",
      "Iteration 30210 Training loss 0.089874267578125 Validation loss 0.13506735861301422 Accuracy 0.6399999856948853\n",
      "Iteration 30220 Training loss 0.09070724248886108 Validation loss 0.13343113660812378 Accuracy 0.6428333520889282\n",
      "Iteration 30230 Training loss 0.0919150561094284 Validation loss 0.13614435493946075 Accuracy 0.6381666660308838\n",
      "Iteration 30240 Training loss 0.08668632060289383 Validation loss 0.12891118228435516 Accuracy 0.6541666388511658\n",
      "Iteration 30250 Training loss 0.09039191901683807 Validation loss 0.13448767364025116 Accuracy 0.640666663646698\n",
      "Iteration 30260 Training loss 0.09069203585386276 Validation loss 0.13406416773796082 Accuracy 0.6418333053588867\n",
      "Iteration 30270 Training loss 0.09026216715574265 Validation loss 0.13062885403633118 Accuracy 0.6520000100135803\n",
      "Iteration 30280 Training loss 0.0910278707742691 Validation loss 0.13594956696033478 Accuracy 0.6370000243186951\n",
      "Iteration 30290 Training loss 0.08996818959712982 Validation loss 0.1334521621465683 Accuracy 0.6443333625793457\n",
      "Iteration 30300 Training loss 0.09113562107086182 Validation loss 0.1287832409143448 Accuracy 0.6528333425521851\n",
      "Iteration 30310 Training loss 0.08770547807216644 Validation loss 0.12652181088924408 Accuracy 0.6571666598320007\n",
      "Iteration 30320 Training loss 0.08995646983385086 Validation loss 0.13213589787483215 Accuracy 0.6464999914169312\n",
      "Iteration 30330 Training loss 0.08870162069797516 Validation loss 0.12399374693632126 Accuracy 0.6629999876022339\n",
      "Iteration 30340 Training loss 0.08466801047325134 Validation loss 0.11607887595891953 Accuracy 0.6813333630561829\n",
      "Iteration 30350 Training loss 0.09215317666530609 Validation loss 0.11282668262720108 Accuracy 0.684333324432373\n",
      "Iteration 30360 Training loss 0.08889129757881165 Validation loss 0.11261925101280212 Accuracy 0.6859999895095825\n",
      "Iteration 30370 Training loss 0.09112212806940079 Validation loss 0.11413460224866867 Accuracy 0.684166669845581\n",
      "Iteration 30380 Training loss 0.09056469798088074 Validation loss 0.11294159293174744 Accuracy 0.6836666464805603\n",
      "Iteration 30390 Training loss 0.0889856368303299 Validation loss 0.11281449347734451 Accuracy 0.6848333477973938\n",
      "Iteration 30400 Training loss 0.086890809237957 Validation loss 0.11307460814714432 Accuracy 0.6846666932106018\n",
      "Iteration 30410 Training loss 0.09324083477258682 Validation loss 0.11271834373474121 Accuracy 0.684166669845581\n",
      "Iteration 30420 Training loss 0.09006209671497345 Validation loss 0.11284590512514114 Accuracy 0.6851666569709778\n",
      "Iteration 30430 Training loss 0.08982280641794205 Validation loss 0.11287831515073776 Accuracy 0.6858333349227905\n",
      "Iteration 30440 Training loss 0.08753454685211182 Validation loss 0.11347391456365585 Accuracy 0.6853333115577698\n",
      "Iteration 30450 Training loss 0.08841050416231155 Validation loss 0.11686348170042038 Accuracy 0.6776666641235352\n",
      "Iteration 30460 Training loss 0.09291163831949234 Validation loss 0.11281079053878784 Accuracy 0.6848333477973938\n",
      "Iteration 30470 Training loss 0.09244879335165024 Validation loss 0.11290135979652405 Accuracy 0.6855000257492065\n",
      "Iteration 30480 Training loss 0.08820164948701859 Validation loss 0.11363280564546585 Accuracy 0.6856666803359985\n",
      "Iteration 30490 Training loss 0.0922536626458168 Validation loss 0.11318688839673996 Accuracy 0.6855000257492065\n",
      "Iteration 30500 Training loss 0.08894322067499161 Validation loss 0.11358021944761276 Accuracy 0.684166669845581\n",
      "Iteration 30510 Training loss 0.0894336923956871 Validation loss 0.1134655773639679 Accuracy 0.6850000023841858\n",
      "Iteration 30520 Training loss 0.08660612255334854 Validation loss 0.11530686169862747 Accuracy 0.6818333268165588\n",
      "Iteration 30530 Training loss 0.09044890105724335 Validation loss 0.11428067088127136 Accuracy 0.6833333373069763\n",
      "Iteration 30540 Training loss 0.09033386409282684 Validation loss 0.11318445950746536 Accuracy 0.6846666932106018\n",
      "Iteration 30550 Training loss 0.08705637603998184 Validation loss 0.11704469472169876 Accuracy 0.6781666874885559\n",
      "Iteration 30560 Training loss 0.08686944842338562 Validation loss 0.11704830080270767 Accuracy 0.6788333058357239\n",
      "Iteration 30570 Training loss 0.08747708052396774 Validation loss 0.12375335395336151 Accuracy 0.6666666865348816\n",
      "Iteration 30580 Training loss 0.09794976562261581 Validation loss 0.1489398181438446 Accuracy 0.6140000224113464\n",
      "Iteration 30590 Training loss 0.09274408221244812 Validation loss 0.1331000179052353 Accuracy 0.6430000066757202\n",
      "Iteration 30600 Training loss 0.09389066696166992 Validation loss 0.13696636259555817 Accuracy 0.6370000243186951\n",
      "Iteration 30610 Training loss 0.08903881162405014 Validation loss 0.1344575434923172 Accuracy 0.640999972820282\n",
      "Iteration 30620 Training loss 0.09129932522773743 Validation loss 0.14026802778244019 Accuracy 0.6328333616256714\n",
      "Iteration 30630 Training loss 0.08932032436132431 Validation loss 0.13224899768829346 Accuracy 0.6463333368301392\n",
      "Iteration 30640 Training loss 0.08679553121328354 Validation loss 0.12233971804380417 Accuracy 0.668666660785675\n",
      "Iteration 30650 Training loss 0.08963266015052795 Validation loss 0.13262130320072174 Accuracy 0.6455000042915344\n",
      "Iteration 30660 Training loss 0.08938165009021759 Validation loss 0.1354062855243683 Accuracy 0.6389999985694885\n",
      "Iteration 30670 Training loss 0.0888972356915474 Validation loss 0.1277751326560974 Accuracy 0.6546666622161865\n",
      "Iteration 30680 Training loss 0.08722374588251114 Validation loss 0.12097352743148804 Accuracy 0.6710000038146973\n",
      "Iteration 30690 Training loss 0.0872439593076706 Validation loss 0.11910875141620636 Accuracy 0.6731666922569275\n",
      "Iteration 30700 Training loss 0.09044712036848068 Validation loss 0.13116677105426788 Accuracy 0.6480000019073486\n",
      "Iteration 30710 Training loss 0.09315598756074905 Validation loss 0.13807865977287292 Accuracy 0.6353333592414856\n",
      "Iteration 30720 Training loss 0.08830317109823227 Validation loss 0.12962503731250763 Accuracy 0.6518333554267883\n",
      "Iteration 30730 Training loss 0.0914895087480545 Validation loss 0.13491041958332062 Accuracy 0.640333354473114\n",
      "Iteration 30740 Training loss 0.08946651965379715 Validation loss 0.12494775652885437 Accuracy 0.659166693687439\n",
      "Iteration 30750 Training loss 0.09155649691820145 Validation loss 0.1278468370437622 Accuracy 0.6545000076293945\n",
      "Iteration 30760 Training loss 0.08801697939634323 Validation loss 0.12058800458908081 Accuracy 0.671999990940094\n",
      "Iteration 30770 Training loss 0.08716697990894318 Validation loss 0.1156277060508728 Accuracy 0.6821666955947876\n",
      "Iteration 30780 Training loss 0.09305765479803085 Validation loss 0.11272722482681274 Accuracy 0.6834999918937683\n",
      "Iteration 30790 Training loss 0.08867795765399933 Validation loss 0.11390300840139389 Accuracy 0.6850000023841858\n",
      "Iteration 30800 Training loss 0.08671118319034576 Validation loss 0.11355346441268921 Accuracy 0.6851666569709778\n",
      "Iteration 30810 Training loss 0.08593237400054932 Validation loss 0.11302199214696884 Accuracy 0.6859999895095825\n",
      "Iteration 30820 Training loss 0.08951743692159653 Validation loss 0.11373870074748993 Accuracy 0.684333324432373\n",
      "Iteration 30830 Training loss 0.09175071120262146 Validation loss 0.11292663216590881 Accuracy 0.684333324432373\n",
      "Iteration 30840 Training loss 0.08760703355073929 Validation loss 0.11485680937767029 Accuracy 0.6819999814033508\n",
      "Iteration 30850 Training loss 0.08894651383161545 Validation loss 0.11473450064659119 Accuracy 0.6834999918937683\n",
      "Iteration 30860 Training loss 0.08927067369222641 Validation loss 0.11401300132274628 Accuracy 0.684499979019165\n",
      "Iteration 30870 Training loss 0.09207705408334732 Validation loss 0.1129186823964119 Accuracy 0.6836666464805603\n",
      "Iteration 30880 Training loss 0.0909562036395073 Validation loss 0.1130957156419754 Accuracy 0.684499979019165\n",
      "Iteration 30890 Training loss 0.08715252578258514 Validation loss 0.11374328285455704 Accuracy 0.6846666932106018\n",
      "Iteration 30900 Training loss 0.08804342895746231 Validation loss 0.11362557113170624 Accuracy 0.6840000152587891\n",
      "Iteration 30910 Training loss 0.08923592418432236 Validation loss 0.1135491356253624 Accuracy 0.6851666569709778\n",
      "Iteration 30920 Training loss 0.09091880172491074 Validation loss 0.11282642930746078 Accuracy 0.684333324432373\n",
      "Iteration 30930 Training loss 0.09046868979930878 Validation loss 0.11308496445417404 Accuracy 0.6851666569709778\n",
      "Iteration 30940 Training loss 0.08846766501665115 Validation loss 0.115554079413414 Accuracy 0.6821666955947876\n",
      "Iteration 30950 Training loss 0.08692224323749542 Validation loss 0.11475631594657898 Accuracy 0.6834999918937683\n",
      "Iteration 30960 Training loss 0.09280181676149368 Validation loss 0.11302351951599121 Accuracy 0.6840000152587891\n",
      "Iteration 30970 Training loss 0.08838990330696106 Validation loss 0.11371893435716629 Accuracy 0.6856666803359985\n",
      "Iteration 30980 Training loss 0.0914442241191864 Validation loss 0.11307495087385178 Accuracy 0.6848333477973938\n",
      "Iteration 30990 Training loss 0.08747684955596924 Validation loss 0.11388017982244492 Accuracy 0.684333324432373\n",
      "Iteration 31000 Training loss 0.0882832333445549 Validation loss 0.11431080847978592 Accuracy 0.684499979019165\n",
      "Iteration 31010 Training loss 0.08531751483678818 Validation loss 0.11622947454452515 Accuracy 0.6794999837875366\n",
      "Iteration 31020 Training loss 0.09115419536828995 Validation loss 0.11311937123537064 Accuracy 0.6861666440963745\n",
      "Iteration 31030 Training loss 0.08852580934762955 Validation loss 0.11296641826629639 Accuracy 0.6856666803359985\n",
      "Iteration 31040 Training loss 0.0934523493051529 Validation loss 0.1128934919834137 Accuracy 0.6831666827201843\n",
      "Iteration 31050 Training loss 0.09115444868803024 Validation loss 0.11291264742612839 Accuracy 0.6865000128746033\n",
      "Iteration 31060 Training loss 0.08860819786787033 Validation loss 0.11389917135238647 Accuracy 0.6863333582878113\n",
      "Iteration 31070 Training loss 0.08544311672449112 Validation loss 0.11562094837427139 Accuracy 0.6815000176429749\n",
      "Iteration 31080 Training loss 0.09051881730556488 Validation loss 0.1132701188325882 Accuracy 0.6868333220481873\n",
      "Iteration 31090 Training loss 0.08816898614168167 Validation loss 0.11375084519386292 Accuracy 0.6851666569709778\n",
      "Iteration 31100 Training loss 0.08926776051521301 Validation loss 0.11311590671539307 Accuracy 0.6846666932106018\n",
      "Iteration 31110 Training loss 0.09393313527107239 Validation loss 0.11289342492818832 Accuracy 0.6833333373069763\n",
      "Iteration 31120 Training loss 0.0897844061255455 Validation loss 0.11322243511676788 Accuracy 0.6846666932106018\n",
      "Iteration 31130 Training loss 0.08699017018079758 Validation loss 0.11768915504217148 Accuracy 0.6790000200271606\n",
      "Iteration 31140 Training loss 0.0853828638792038 Validation loss 0.12223805487155914 Accuracy 0.6690000295639038\n",
      "Iteration 31150 Training loss 0.08797363936901093 Validation loss 0.13305267691612244 Accuracy 0.6458333134651184\n",
      "Iteration 31160 Training loss 0.08853865414857864 Validation loss 0.13439911603927612 Accuracy 0.6433333158493042\n",
      "Iteration 31170 Training loss 0.09060905128717422 Validation loss 0.13193011283874512 Accuracy 0.6468333601951599\n",
      "Iteration 31180 Training loss 0.08894313871860504 Validation loss 0.13146169483661652 Accuracy 0.6485000252723694\n",
      "Iteration 31190 Training loss 0.09147710353136063 Validation loss 0.13551056385040283 Accuracy 0.6413333415985107\n",
      "Iteration 31200 Training loss 0.08691258728504181 Validation loss 0.1273515671491623 Accuracy 0.6568333506584167\n",
      "Iteration 31210 Training loss 0.08794720470905304 Validation loss 0.13124285638332367 Accuracy 0.6496666669845581\n",
      "Iteration 31220 Training loss 0.08996888250112534 Validation loss 0.13183598220348358 Accuracy 0.6466666460037231\n",
      "Iteration 31230 Training loss 0.08857481926679611 Validation loss 0.12709450721740723 Accuracy 0.656499981880188\n",
      "Iteration 31240 Training loss 0.08872662484645844 Validation loss 0.1280578076839447 Accuracy 0.6536666750907898\n",
      "Iteration 31250 Training loss 0.09358778595924377 Validation loss 0.13601279258728027 Accuracy 0.64083331823349\n",
      "Iteration 31260 Training loss 0.08805963397026062 Validation loss 0.12358302623033524 Accuracy 0.6664999723434448\n",
      "Iteration 31270 Training loss 0.09015500545501709 Validation loss 0.13627193868160248 Accuracy 0.6383333206176758\n",
      "Iteration 31280 Training loss 0.08763357251882553 Validation loss 0.13215474784374237 Accuracy 0.6474999785423279\n",
      "Iteration 31290 Training loss 0.08838392794132233 Validation loss 0.12572677433490753 Accuracy 0.6603333353996277\n",
      "Iteration 31300 Training loss 0.0887398049235344 Validation loss 0.13081064820289612 Accuracy 0.6495000123977661\n",
      "Iteration 31310 Training loss 0.09017683565616608 Validation loss 0.13289575278759003 Accuracy 0.6458333134651184\n",
      "Iteration 31320 Training loss 0.08993418514728546 Validation loss 0.131696417927742 Accuracy 0.6458333134651184\n",
      "Iteration 31330 Training loss 0.09047812968492508 Validation loss 0.13151192665100098 Accuracy 0.6474999785423279\n",
      "Iteration 31340 Training loss 0.0870341882109642 Validation loss 0.1310301274061203 Accuracy 0.6498333215713501\n",
      "Iteration 31350 Training loss 0.0867442861199379 Validation loss 0.1223893016576767 Accuracy 0.6696666479110718\n",
      "Iteration 31360 Training loss 0.08826211839914322 Validation loss 0.11595841497182846 Accuracy 0.6813333630561829\n",
      "Iteration 31370 Training loss 0.08771058171987534 Validation loss 0.11348691582679749 Accuracy 0.6855000257492065\n",
      "Iteration 31380 Training loss 0.08859125524759293 Validation loss 0.11421733349561691 Accuracy 0.6848333477973938\n",
      "Iteration 31390 Training loss 0.0914260670542717 Validation loss 0.11288780719041824 Accuracy 0.684499979019165\n",
      "Iteration 31400 Training loss 0.09016624838113785 Validation loss 0.11283806711435318 Accuracy 0.6846666932106018\n",
      "Iteration 31410 Training loss 0.08991622924804688 Validation loss 0.1128237321972847 Accuracy 0.6853333115577698\n",
      "Iteration 31420 Training loss 0.0895797535777092 Validation loss 0.1128072738647461 Accuracy 0.6850000023841858\n",
      "Iteration 31430 Training loss 0.08694606274366379 Validation loss 0.11405594646930695 Accuracy 0.684333324432373\n",
      "Iteration 31440 Training loss 0.09100603312253952 Validation loss 0.11300753802061081 Accuracy 0.6840000152587891\n",
      "Iteration 31450 Training loss 0.08995816856622696 Validation loss 0.11270575225353241 Accuracy 0.684499979019165\n",
      "Iteration 31460 Training loss 0.08767382800579071 Validation loss 0.11449061334133148 Accuracy 0.684166669845581\n",
      "Iteration 31470 Training loss 0.09079247713088989 Validation loss 0.11287207156419754 Accuracy 0.6848333477973938\n",
      "Iteration 31480 Training loss 0.09070564061403275 Validation loss 0.11286946386098862 Accuracy 0.684499979019165\n",
      "Iteration 31490 Training loss 0.0891682505607605 Validation loss 0.113837331533432 Accuracy 0.6859999895095825\n",
      "Iteration 31500 Training loss 0.08952897042036057 Validation loss 0.11295051872730255 Accuracy 0.6856666803359985\n",
      "Iteration 31510 Training loss 0.08941970020532608 Validation loss 0.11307814717292786 Accuracy 0.6859999895095825\n",
      "Iteration 31520 Training loss 0.09040039032697678 Validation loss 0.11322546750307083 Accuracy 0.6858333349227905\n",
      "Iteration 31530 Training loss 0.09092945605516434 Validation loss 0.11279845237731934 Accuracy 0.6853333115577698\n",
      "Iteration 31540 Training loss 0.09001421928405762 Validation loss 0.11332925409078598 Accuracy 0.6850000023841858\n",
      "Iteration 31550 Training loss 0.08758863061666489 Validation loss 0.11428942531347275 Accuracy 0.6850000023841858\n",
      "Iteration 31560 Training loss 0.08803535252809525 Validation loss 0.1167476624250412 Accuracy 0.6806666851043701\n",
      "Iteration 31570 Training loss 0.08725661039352417 Validation loss 0.11358156055212021 Accuracy 0.6851666569709778\n",
      "Iteration 31580 Training loss 0.09157547354698181 Validation loss 0.11319983750581741 Accuracy 0.6846666932106018\n",
      "Iteration 31590 Training loss 0.09214700013399124 Validation loss 0.11282894015312195 Accuracy 0.6850000023841858\n",
      "Iteration 31600 Training loss 0.09184831380844116 Validation loss 0.1128511130809784 Accuracy 0.6848333477973938\n",
      "Iteration 31610 Training loss 0.09280312806367874 Validation loss 0.11274468153715134 Accuracy 0.6853333115577698\n",
      "Iteration 31620 Training loss 0.08785805106163025 Validation loss 0.11440575867891312 Accuracy 0.6836666464805603\n",
      "Iteration 31630 Training loss 0.08840154111385345 Validation loss 0.11327672004699707 Accuracy 0.6853333115577698\n",
      "Iteration 31640 Training loss 0.08755365014076233 Validation loss 0.11518799513578415 Accuracy 0.6819999814033508\n",
      "Iteration 31650 Training loss 0.0876583680510521 Validation loss 0.12455208599567413 Accuracy 0.6635000109672546\n",
      "Iteration 31660 Training loss 0.08852522820234299 Validation loss 0.13063572347164154 Accuracy 0.6506666541099548\n",
      "Iteration 31670 Training loss 0.09576041996479034 Validation loss 0.14338502287864685 Accuracy 0.6268333196640015\n",
      "Iteration 31680 Training loss 0.09093357622623444 Validation loss 0.1343003809452057 Accuracy 0.6416666507720947\n",
      "Iteration 31690 Training loss 0.08470351248979568 Validation loss 0.1276039183139801 Accuracy 0.6554999947547913\n",
      "Iteration 31700 Training loss 0.09065694361925125 Validation loss 0.1256653070449829 Accuracy 0.6586666703224182\n",
      "Iteration 31710 Training loss 0.08791209012269974 Validation loss 0.12170760333538055 Accuracy 0.6713333129882812\n",
      "Iteration 31720 Training loss 0.09017901867628098 Validation loss 0.1367676854133606 Accuracy 0.6370000243186951\n",
      "Iteration 31730 Training loss 0.09204978495836258 Validation loss 0.13951349258422852 Accuracy 0.6340000033378601\n",
      "Iteration 31740 Training loss 0.08748247474431992 Validation loss 0.12671074271202087 Accuracy 0.6571666598320007\n",
      "Iteration 31750 Training loss 0.08846736699342728 Validation loss 0.12321479618549347 Accuracy 0.6675000190734863\n",
      "Iteration 31760 Training loss 0.09203813970088959 Validation loss 0.13577325642108917 Accuracy 0.6395000219345093\n",
      "Iteration 31770 Training loss 0.08983449637889862 Validation loss 0.13190598785877228 Accuracy 0.6474999785423279\n",
      "Iteration 31780 Training loss 0.08981114625930786 Validation loss 0.12904106080532074 Accuracy 0.6514999866485596\n",
      "Iteration 31790 Training loss 0.08785144984722137 Validation loss 0.1243172213435173 Accuracy 0.6643333435058594\n",
      "Iteration 31800 Training loss 0.09029161930084229 Validation loss 0.13116532564163208 Accuracy 0.6491666436195374\n",
      "Iteration 31810 Training loss 0.08936517685651779 Validation loss 0.12810483574867249 Accuracy 0.653166651725769\n",
      "Iteration 31820 Training loss 0.09112720936536789 Validation loss 0.13014960289001465 Accuracy 0.6499999761581421\n",
      "Iteration 31830 Training loss 0.08985141664743423 Validation loss 0.13240499794483185 Accuracy 0.6453333497047424\n",
      "Iteration 31840 Training loss 0.08956532925367355 Validation loss 0.13140136003494263 Accuracy 0.6486666798591614\n",
      "Iteration 31850 Training loss 0.09342014789581299 Validation loss 0.14107520878314972 Accuracy 0.6306666731834412\n",
      "Iteration 31860 Training loss 0.08764726668596268 Validation loss 0.1257735788822174 Accuracy 0.659166693687439\n",
      "Iteration 31870 Training loss 0.09154633432626724 Validation loss 0.1294177919626236 Accuracy 0.6523333191871643\n",
      "Iteration 31880 Training loss 0.09074080735445023 Validation loss 0.1344832330942154 Accuracy 0.6411666870117188\n",
      "Iteration 31890 Training loss 0.0918864831328392 Validation loss 0.1314753144979477 Accuracy 0.6483333110809326\n",
      "Iteration 31900 Training loss 0.09294869005680084 Validation loss 0.139661505818367 Accuracy 0.6316666603088379\n",
      "Iteration 31910 Training loss 0.08971018344163895 Validation loss 0.13149584829807281 Accuracy 0.6464999914169312\n",
      "Iteration 31920 Training loss 0.08571533113718033 Validation loss 0.12568633258342743 Accuracy 0.6614999771118164\n",
      "Iteration 31930 Training loss 0.08926809579133987 Validation loss 0.1321403533220291 Accuracy 0.6470000147819519\n",
      "Iteration 31940 Training loss 0.09041765332221985 Validation loss 0.1329415738582611 Accuracy 0.6463333368301392\n",
      "Iteration 31950 Training loss 0.08761706203222275 Validation loss 0.11829227954149246 Accuracy 0.6771666407585144\n",
      "Iteration 31960 Training loss 0.09015877544879913 Validation loss 0.11319954693317413 Accuracy 0.6865000128746033\n",
      "Iteration 31970 Training loss 0.08800320327281952 Validation loss 0.11359228193759918 Accuracy 0.6851666569709778\n",
      "Iteration 31980 Training loss 0.08666112273931503 Validation loss 0.11449078470468521 Accuracy 0.6825000047683716\n",
      "Iteration 31990 Training loss 0.0867818221449852 Validation loss 0.11385481804609299 Accuracy 0.6848333477973938\n",
      "Iteration 32000 Training loss 0.08787748962640762 Validation loss 0.11386565119028091 Accuracy 0.6846666932106018\n",
      "Iteration 32010 Training loss 0.0907839760184288 Validation loss 0.11317750811576843 Accuracy 0.6846666932106018\n",
      "Iteration 32020 Training loss 0.08688411116600037 Validation loss 0.11376316100358963 Accuracy 0.6850000023841858\n",
      "Iteration 32030 Training loss 0.09012208133935928 Validation loss 0.11335097998380661 Accuracy 0.684333324432373\n",
      "Iteration 32040 Training loss 0.08713112026453018 Validation loss 0.11359864473342896 Accuracy 0.6856666803359985\n",
      "Iteration 32050 Training loss 0.09228011220693588 Validation loss 0.11296321451663971 Accuracy 0.6853333115577698\n",
      "Iteration 32060 Training loss 0.08973736315965652 Validation loss 0.11391428858041763 Accuracy 0.6848333477973938\n",
      "Iteration 32070 Training loss 0.08623986691236496 Validation loss 0.12054969370365143 Accuracy 0.6731666922569275\n",
      "Iteration 32080 Training loss 0.08616466075181961 Validation loss 0.11899493634700775 Accuracy 0.6744999885559082\n",
      "Iteration 32090 Training loss 0.08800767362117767 Validation loss 0.11941196024417877 Accuracy 0.6769999861717224\n",
      "Iteration 32100 Training loss 0.08601744472980499 Validation loss 0.1279946267604828 Accuracy 0.6541666388511658\n",
      "Iteration 32110 Training loss 0.09445549547672272 Validation loss 0.14206482470035553 Accuracy 0.6296666860580444\n",
      "Iteration 32120 Training loss 0.09027419239282608 Validation loss 0.1334134191274643 Accuracy 0.6448333263397217\n",
      "Iteration 32130 Training loss 0.08625167608261108 Validation loss 0.12279225885868073 Accuracy 0.6676666736602783\n",
      "Iteration 32140 Training loss 0.09007034450769424 Validation loss 0.11343670636415482 Accuracy 0.6851666569709778\n",
      "Iteration 32150 Training loss 0.09212249517440796 Validation loss 0.11308103799819946 Accuracy 0.6850000023841858\n",
      "Iteration 32160 Training loss 0.09143469482660294 Validation loss 0.11289026588201523 Accuracy 0.6833333373069763\n",
      "Iteration 32170 Training loss 0.08950819075107574 Validation loss 0.11317295581102371 Accuracy 0.6865000128746033\n",
      "Iteration 32180 Training loss 0.09160348773002625 Validation loss 0.11308513581752777 Accuracy 0.6846666932106018\n",
      "Iteration 32190 Training loss 0.08699853718280792 Validation loss 0.11375288665294647 Accuracy 0.6861666440963745\n",
      "Iteration 32200 Training loss 0.09031056612730026 Validation loss 0.11331865191459656 Accuracy 0.6853333115577698\n",
      "Iteration 32210 Training loss 0.09030847251415253 Validation loss 0.11380352824926376 Accuracy 0.6855000257492065\n",
      "Iteration 32220 Training loss 0.08998498320579529 Validation loss 0.1152360811829567 Accuracy 0.6813333630561829\n",
      "Iteration 32230 Training loss 0.091106116771698 Validation loss 0.11300960183143616 Accuracy 0.6846666932106018\n",
      "Iteration 32240 Training loss 0.08765127509832382 Validation loss 0.11702580004930496 Accuracy 0.6808333396911621\n",
      "Iteration 32250 Training loss 0.08748915791511536 Validation loss 0.11403890699148178 Accuracy 0.6853333115577698\n",
      "Iteration 32260 Training loss 0.08866963535547256 Validation loss 0.11440950632095337 Accuracy 0.6858333349227905\n",
      "Iteration 32270 Training loss 0.08474953472614288 Validation loss 0.11992897838354111 Accuracy 0.6738333106040955\n",
      "Iteration 32280 Training loss 0.09165044128894806 Validation loss 0.13518302142620087 Accuracy 0.6395000219345093\n",
      "Iteration 32290 Training loss 0.09359243512153625 Validation loss 0.1364089846611023 Accuracy 0.6371666789054871\n",
      "Iteration 32300 Training loss 0.09141860157251358 Validation loss 0.1360500305891037 Accuracy 0.6395000219345093\n",
      "Iteration 32310 Training loss 0.08661399036645889 Validation loss 0.11873156577348709 Accuracy 0.6735000014305115\n",
      "Iteration 32320 Training loss 0.08877179771661758 Validation loss 0.13395072519779205 Accuracy 0.6443333625793457\n",
      "Iteration 32330 Training loss 0.08938838541507721 Validation loss 0.13505148887634277 Accuracy 0.640333354473114\n",
      "Iteration 32340 Training loss 0.09256964921951294 Validation loss 0.13663361966609955 Accuracy 0.637666642665863\n",
      "Iteration 32350 Training loss 0.08878448605537415 Validation loss 0.1309969127178192 Accuracy 0.6504999995231628\n",
      "Iteration 32360 Training loss 0.08692298084497452 Validation loss 0.12133929133415222 Accuracy 0.671999990940094\n",
      "Iteration 32370 Training loss 0.08872097730636597 Validation loss 0.12843552231788635 Accuracy 0.6539999842643738\n",
      "Iteration 32380 Training loss 0.08983556926250458 Validation loss 0.1305742859840393 Accuracy 0.6508333086967468\n",
      "Iteration 32390 Training loss 0.08781580626964569 Validation loss 0.1301177442073822 Accuracy 0.6521666646003723\n",
      "Iteration 32400 Training loss 0.09050600975751877 Validation loss 0.13853979110717773 Accuracy 0.6351666450500488\n",
      "Iteration 32410 Training loss 0.08775785565376282 Validation loss 0.1270248293876648 Accuracy 0.6573333144187927\n",
      "Iteration 32420 Training loss 0.09187636524438858 Validation loss 0.13191023468971252 Accuracy 0.6491666436195374\n",
      "Iteration 32430 Training loss 0.0909164622426033 Validation loss 0.13235555589199066 Accuracy 0.6481666564941406\n",
      "Iteration 32440 Training loss 0.08711361885070801 Validation loss 0.13090333342552185 Accuracy 0.6498333215713501\n",
      "Iteration 32450 Training loss 0.09073684364557266 Validation loss 0.13419798016548157 Accuracy 0.6424999833106995\n",
      "Iteration 32460 Training loss 0.09009727835655212 Validation loss 0.13089825212955475 Accuracy 0.6506666541099548\n",
      "Iteration 32470 Training loss 0.08894932270050049 Validation loss 0.13344533741474152 Accuracy 0.6451666951179504\n",
      "Iteration 32480 Training loss 0.09152764827013016 Validation loss 0.13346892595291138 Accuracy 0.6433333158493042\n",
      "Iteration 32490 Training loss 0.08941883593797684 Validation loss 0.13385404646396637 Accuracy 0.643666684627533\n",
      "Iteration 32500 Training loss 0.09006280452013016 Validation loss 0.13208362460136414 Accuracy 0.6489999890327454\n",
      "Iteration 32510 Training loss 0.08798608183860779 Validation loss 0.12429384142160416 Accuracy 0.6639999747276306\n",
      "Iteration 32520 Training loss 0.08934798091650009 Validation loss 0.13103313744068146 Accuracy 0.6504999995231628\n",
      "Iteration 32530 Training loss 0.09039975702762604 Validation loss 0.13269269466400146 Accuracy 0.6470000147819519\n",
      "Iteration 32540 Training loss 0.08945757895708084 Validation loss 0.12946760654449463 Accuracy 0.6520000100135803\n",
      "Iteration 32550 Training loss 0.08952044695615768 Validation loss 0.11770803481340408 Accuracy 0.6791666746139526\n",
      "Iteration 32560 Training loss 0.08752750605344772 Validation loss 0.12542439997196198 Accuracy 0.6610000133514404\n",
      "Iteration 32570 Training loss 0.09233589470386505 Validation loss 0.14111685752868652 Accuracy 0.6315000057220459\n",
      "Iteration 32580 Training loss 0.08792892098426819 Validation loss 0.1296522170305252 Accuracy 0.6511666774749756\n",
      "Iteration 32590 Training loss 0.08932523429393768 Validation loss 0.13157795369625092 Accuracy 0.6489999890327454\n",
      "Iteration 32600 Training loss 0.08976586908102036 Validation loss 0.1343512237071991 Accuracy 0.6421666741371155\n",
      "Iteration 32610 Training loss 0.08922551572322845 Validation loss 0.13053667545318604 Accuracy 0.6498333215713501\n",
      "Iteration 32620 Training loss 0.0885140672326088 Validation loss 0.12733441591262817 Accuracy 0.6548333168029785\n",
      "Iteration 32630 Training loss 0.0909828171133995 Validation loss 0.13300494849681854 Accuracy 0.6460000276565552\n",
      "Iteration 32640 Training loss 0.08659978955984116 Validation loss 0.12950988113880157 Accuracy 0.6523333191871643\n",
      "Iteration 32650 Training loss 0.09689038246870041 Validation loss 0.1415090262889862 Accuracy 0.6311666369438171\n",
      "Iteration 32660 Training loss 0.08789290487766266 Validation loss 0.12784327566623688 Accuracy 0.6543333530426025\n",
      "Iteration 32670 Training loss 0.0882813036441803 Validation loss 0.12446046620607376 Accuracy 0.6643333435058594\n",
      "Iteration 32680 Training loss 0.08978693932294846 Validation loss 0.13349203765392303 Accuracy 0.6455000042915344\n",
      "Iteration 32690 Training loss 0.08910917490720749 Validation loss 0.13098129630088806 Accuracy 0.6481666564941406\n",
      "Iteration 32700 Training loss 0.08927154541015625 Validation loss 0.1311011165380478 Accuracy 0.6481666564941406\n",
      "Iteration 32710 Training loss 0.09078847616910934 Validation loss 0.13040360808372498 Accuracy 0.6504999995231628\n",
      "Iteration 32720 Training loss 0.08941978216171265 Validation loss 0.1327357441186905 Accuracy 0.6471666693687439\n",
      "Iteration 32730 Training loss 0.08864489942789078 Validation loss 0.12812736630439758 Accuracy 0.6551666855812073\n",
      "Iteration 32740 Training loss 0.08633990585803986 Validation loss 0.12889982759952545 Accuracy 0.6536666750907898\n",
      "Iteration 32750 Training loss 0.08902189880609512 Validation loss 0.12922441959381104 Accuracy 0.6535000205039978\n",
      "Iteration 32760 Training loss 0.0915965586900711 Validation loss 0.13377468287944794 Accuracy 0.6445000171661377\n",
      "Iteration 32770 Training loss 0.09403151273727417 Validation loss 0.13899794220924377 Accuracy 0.6343333125114441\n",
      "Iteration 32780 Training loss 0.08845576643943787 Validation loss 0.1343976855278015 Accuracy 0.6430000066757202\n",
      "Iteration 32790 Training loss 0.08848175406455994 Validation loss 0.12266719341278076 Accuracy 0.668666660785675\n",
      "Iteration 32800 Training loss 0.0855788141489029 Validation loss 0.12152786552906036 Accuracy 0.671500027179718\n",
      "Iteration 32810 Training loss 0.08844443410634995 Validation loss 0.13080847263336182 Accuracy 0.6504999995231628\n",
      "Iteration 32820 Training loss 0.09408292919397354 Validation loss 0.14024236798286438 Accuracy 0.6330000162124634\n",
      "Iteration 32830 Training loss 0.09051970392465591 Validation loss 0.13826054334640503 Accuracy 0.6355000138282776\n",
      "Iteration 32840 Training loss 0.08763539791107178 Validation loss 0.12519648671150208 Accuracy 0.6613333225250244\n",
      "Iteration 32850 Training loss 0.08650104701519012 Validation loss 0.12395031005144119 Accuracy 0.6650000214576721\n",
      "Iteration 32860 Training loss 0.08679655194282532 Validation loss 0.12872560322284698 Accuracy 0.6551666855812073\n",
      "Iteration 32870 Training loss 0.08959515392780304 Validation loss 0.13790683448314667 Accuracy 0.6358333230018616\n",
      "Iteration 32880 Training loss 0.09138566255569458 Validation loss 0.13314925134181976 Accuracy 0.6483333110809326\n",
      "Iteration 32890 Training loss 0.08521726727485657 Validation loss 0.12178817391395569 Accuracy 0.6691666841506958\n",
      "Iteration 32900 Training loss 0.08835679292678833 Validation loss 0.11513775587081909 Accuracy 0.6834999918937683\n",
      "Iteration 32910 Training loss 0.09129327535629272 Validation loss 0.1132255345582962 Accuracy 0.6831666827201843\n",
      "Iteration 32920 Training loss 0.08821027725934982 Validation loss 0.11311596632003784 Accuracy 0.6848333477973938\n",
      "Iteration 32930 Training loss 0.09153775870800018 Validation loss 0.1134832575917244 Accuracy 0.6855000257492065\n",
      "Iteration 32940 Training loss 0.08885813504457474 Validation loss 0.11311276257038116 Accuracy 0.6848333477973938\n",
      "Iteration 32950 Training loss 0.08924847096204758 Validation loss 0.11385515332221985 Accuracy 0.6858333349227905\n",
      "Iteration 32960 Training loss 0.09095796942710876 Validation loss 0.11339755356311798 Accuracy 0.6831666827201843\n",
      "Iteration 32970 Training loss 0.0873836874961853 Validation loss 0.11454400420188904 Accuracy 0.6851666569709778\n",
      "Iteration 32980 Training loss 0.0878477394580841 Validation loss 0.11318491399288177 Accuracy 0.6850000023841858\n",
      "Iteration 32990 Training loss 0.0855388194322586 Validation loss 0.11536997556686401 Accuracy 0.6825000047683716\n",
      "Iteration 33000 Training loss 0.0883144736289978 Validation loss 0.11439605057239532 Accuracy 0.6846666932106018\n",
      "Iteration 33010 Training loss 0.08692403882741928 Validation loss 0.12221919745206833 Accuracy 0.6700000166893005\n",
      "Iteration 33020 Training loss 0.09197383373975754 Validation loss 0.13592861592769623 Accuracy 0.6398333311080933\n",
      "Iteration 33030 Training loss 0.09300299733877182 Validation loss 0.1357390582561493 Accuracy 0.6378333568572998\n",
      "Iteration 33040 Training loss 0.08953972160816193 Validation loss 0.13954757153987885 Accuracy 0.6330000162124634\n",
      "Iteration 33050 Training loss 0.0887286365032196 Validation loss 0.13487142324447632 Accuracy 0.6413333415985107\n",
      "Iteration 33060 Training loss 0.09158370643854141 Validation loss 0.14017118513584137 Accuracy 0.6313333511352539\n",
      "Iteration 33070 Training loss 0.08985650539398193 Validation loss 0.13239414989948273 Accuracy 0.6486666798591614\n",
      "Iteration 33080 Training loss 0.09020701050758362 Validation loss 0.12400452792644501 Accuracy 0.6660000085830688\n",
      "Iteration 33090 Training loss 0.08611354231834412 Validation loss 0.12409697473049164 Accuracy 0.6651666760444641\n",
      "Iteration 33100 Training loss 0.08893196284770966 Validation loss 0.13128745555877686 Accuracy 0.6511666774749756\n",
      "Iteration 33110 Training loss 0.08797676861286163 Validation loss 0.12779051065444946 Accuracy 0.6566666960716248\n",
      "Iteration 33120 Training loss 0.0901893749833107 Validation loss 0.1346529871225357 Accuracy 0.6420000195503235\n",
      "Iteration 33130 Training loss 0.08644705265760422 Validation loss 0.1305416375398636 Accuracy 0.6506666541099548\n",
      "Iteration 33140 Training loss 0.08585398644208908 Validation loss 0.12262795865535736 Accuracy 0.6678333282470703\n",
      "Iteration 33150 Training loss 0.08672048151493073 Validation loss 0.13476872444152832 Accuracy 0.640999972820282\n",
      "Iteration 33160 Training loss 0.08863431960344315 Validation loss 0.13434848189353943 Accuracy 0.6441666483879089\n",
      "Iteration 33170 Training loss 0.08714772760868073 Validation loss 0.12879469990730286 Accuracy 0.6556666493415833\n",
      "Iteration 33180 Training loss 0.08945584297180176 Validation loss 0.12807203829288483 Accuracy 0.6566666960716248\n",
      "Iteration 33190 Training loss 0.09082065522670746 Validation loss 0.13634105026721954 Accuracy 0.6383333206176758\n",
      "Iteration 33200 Training loss 0.0887533575296402 Validation loss 0.13211286067962646 Accuracy 0.6483333110809326\n",
      "Iteration 33210 Training loss 0.08924834430217743 Validation loss 0.13520291447639465 Accuracy 0.6416666507720947\n",
      "Iteration 33220 Training loss 0.0911269560456276 Validation loss 0.1303400844335556 Accuracy 0.6516666412353516\n",
      "Iteration 33230 Training loss 0.08920076489448547 Validation loss 0.1260269731283188 Accuracy 0.6614999771118164\n",
      "Iteration 33240 Training loss 0.08845748752355576 Validation loss 0.12211842834949493 Accuracy 0.6710000038146973\n",
      "Iteration 33250 Training loss 0.08673451840877533 Validation loss 0.1246718242764473 Accuracy 0.6643333435058594\n",
      "Iteration 33260 Training loss 0.09107765555381775 Validation loss 0.1371639221906662 Accuracy 0.6361666917800903\n",
      "Iteration 33270 Training loss 0.09008001536130905 Validation loss 0.13681574165821075 Accuracy 0.6371666789054871\n",
      "Iteration 33280 Training loss 0.09128018468618393 Validation loss 0.13515979051589966 Accuracy 0.6414999961853027\n",
      "Iteration 33290 Training loss 0.08913138508796692 Validation loss 0.11500857770442963 Accuracy 0.6821666955947876\n",
      "Iteration 33300 Training loss 0.08803241699934006 Validation loss 0.11367645114660263 Accuracy 0.6856666803359985\n",
      "Iteration 33310 Training loss 0.09180391579866409 Validation loss 0.11306554824113846 Accuracy 0.6855000257492065\n",
      "Iteration 33320 Training loss 0.08906691521406174 Validation loss 0.11552813649177551 Accuracy 0.6833333373069763\n",
      "Iteration 33330 Training loss 0.08601777255535126 Validation loss 0.11619937419891357 Accuracy 0.6826666593551636\n",
      "Iteration 33340 Training loss 0.0901346206665039 Validation loss 0.11378660798072815 Accuracy 0.6866666674613953\n",
      "Iteration 33350 Training loss 0.09117305278778076 Validation loss 0.11311721056699753 Accuracy 0.6859999895095825\n",
      "Iteration 33360 Training loss 0.0867442861199379 Validation loss 0.11425825953483582 Accuracy 0.6851666569709778\n",
      "Iteration 33370 Training loss 0.08965860307216644 Validation loss 0.11334551870822906 Accuracy 0.6855000257492065\n",
      "Iteration 33380 Training loss 0.08798543363809586 Validation loss 0.11336454004049301 Accuracy 0.6859999895095825\n",
      "Iteration 33390 Training loss 0.08778801560401917 Validation loss 0.11472970247268677 Accuracy 0.6850000023841858\n",
      "Iteration 33400 Training loss 0.08733174949884415 Validation loss 0.11488005518913269 Accuracy 0.6831666827201843\n",
      "Iteration 33410 Training loss 0.08618035912513733 Validation loss 0.11719691008329391 Accuracy 0.6796666383743286\n",
      "Iteration 33420 Training loss 0.08910194784402847 Validation loss 0.11532048881053925 Accuracy 0.6818333268165588\n",
      "Iteration 33430 Training loss 0.08921923488378525 Validation loss 0.11339794844388962 Accuracy 0.684499979019165\n",
      "Iteration 33440 Training loss 0.08833912014961243 Validation loss 0.11482220143079758 Accuracy 0.6823333501815796\n",
      "Iteration 33450 Training loss 0.08825549483299255 Validation loss 0.11354733258485794 Accuracy 0.6859999895095825\n",
      "Iteration 33460 Training loss 0.08972933143377304 Validation loss 0.11311677098274231 Accuracy 0.6848333477973938\n",
      "Iteration 33470 Training loss 0.08859612792730331 Validation loss 0.11287637054920197 Accuracy 0.6856666803359985\n",
      "Iteration 33480 Training loss 0.08984561264514923 Validation loss 0.1132645457983017 Accuracy 0.6851666569709778\n",
      "Iteration 33490 Training loss 0.08805690705776215 Validation loss 0.1134469211101532 Accuracy 0.6863333582878113\n",
      "Iteration 33500 Training loss 0.08785732090473175 Validation loss 0.11338040977716446 Accuracy 0.6856666803359985\n",
      "Iteration 33510 Training loss 0.08845937252044678 Validation loss 0.11367358267307281 Accuracy 0.6846666932106018\n",
      "Iteration 33520 Training loss 0.08640266209840775 Validation loss 0.11395533382892609 Accuracy 0.6833333373069763\n",
      "Iteration 33530 Training loss 0.0876234769821167 Validation loss 0.11347562819719315 Accuracy 0.6850000023841858\n",
      "Iteration 33540 Training loss 0.08793128281831741 Validation loss 0.11440488696098328 Accuracy 0.6851666569709778\n",
      "Iteration 33550 Training loss 0.09132634848356247 Validation loss 0.1133403405547142 Accuracy 0.6853333115577698\n",
      "Iteration 33560 Training loss 0.08854884654283524 Validation loss 0.11324925720691681 Accuracy 0.6838333606719971\n",
      "Iteration 33570 Training loss 0.08766961842775345 Validation loss 0.11496679484844208 Accuracy 0.684166669845581\n",
      "Iteration 33580 Training loss 0.08569218218326569 Validation loss 0.12505795061588287 Accuracy 0.6625000238418579\n",
      "Iteration 33590 Training loss 0.09262984246015549 Validation loss 0.134486123919487 Accuracy 0.6420000195503235\n",
      "Iteration 33600 Training loss 0.09039151668548584 Validation loss 0.12993627786636353 Accuracy 0.6513333320617676\n",
      "Iteration 33610 Training loss 0.08796218037605286 Validation loss 0.12829913198947906 Accuracy 0.6551666855812073\n",
      "Iteration 33620 Training loss 0.0942419245839119 Validation loss 0.14187346398830414 Accuracy 0.6308333277702332\n",
      "Iteration 33630 Training loss 0.08641127496957779 Validation loss 0.1297113448381424 Accuracy 0.652999997138977\n",
      "Iteration 33640 Training loss 0.08746358007192612 Validation loss 0.13467612862586975 Accuracy 0.6428333520889282\n",
      "Iteration 33650 Training loss 0.08907679468393326 Validation loss 0.13156630098819733 Accuracy 0.6499999761581421\n",
      "Iteration 33660 Training loss 0.08659714460372925 Validation loss 0.1245606392621994 Accuracy 0.6648333072662354\n",
      "Iteration 33670 Training loss 0.0888763964176178 Validation loss 0.13600660860538483 Accuracy 0.6395000219345093\n",
      "Iteration 33680 Training loss 0.08899181336164474 Validation loss 0.12829414010047913 Accuracy 0.6546666622161865\n",
      "Iteration 33690 Training loss 0.08868096023797989 Validation loss 0.13278713822364807 Accuracy 0.6481666564941406\n",
      "Iteration 33700 Training loss 0.08721353113651276 Validation loss 0.13112396001815796 Accuracy 0.6491666436195374\n",
      "Iteration 33710 Training loss 0.08987719565629959 Validation loss 0.13398028910160065 Accuracy 0.6456666588783264\n",
      "Iteration 33720 Training loss 0.09067920595407486 Validation loss 0.13030946254730225 Accuracy 0.6514999866485596\n",
      "Iteration 33730 Training loss 0.09036407619714737 Validation loss 0.1331528276205063 Accuracy 0.6460000276565552\n",
      "Iteration 33740 Training loss 0.09075896441936493 Validation loss 0.13596339523792267 Accuracy 0.6391666531562805\n",
      "Iteration 33750 Training loss 0.0886596143245697 Validation loss 0.13268887996673584 Accuracy 0.6480000019073486\n",
      "Iteration 33760 Training loss 0.08821459114551544 Validation loss 0.13067199289798737 Accuracy 0.6503333449363708\n",
      "Iteration 33770 Training loss 0.08810906112194061 Validation loss 0.13604164123535156 Accuracy 0.6391666531562805\n",
      "Iteration 33780 Training loss 0.0883568525314331 Validation loss 0.13142672181129456 Accuracy 0.6488333344459534\n",
      "Iteration 33790 Training loss 0.08954311907291412 Validation loss 0.12944850325584412 Accuracy 0.6539999842643738\n",
      "Iteration 33800 Training loss 0.08746352791786194 Validation loss 0.128310889005661 Accuracy 0.6558333039283752\n",
      "Iteration 33810 Training loss 0.08838806301355362 Validation loss 0.128003790974617 Accuracy 0.656000018119812\n",
      "Iteration 33820 Training loss 0.08975793421268463 Validation loss 0.1346862018108368 Accuracy 0.6414999961853027\n",
      "Iteration 33830 Training loss 0.09017916768789291 Validation loss 0.1277693510055542 Accuracy 0.6538333296775818\n",
      "Iteration 33840 Training loss 0.0888182744383812 Validation loss 0.13125333189964294 Accuracy 0.6508333086967468\n",
      "Iteration 33850 Training loss 0.090468630194664 Validation loss 0.1397467851638794 Accuracy 0.6324999928474426\n",
      "Iteration 33860 Training loss 0.0876036137342453 Validation loss 0.12742707133293152 Accuracy 0.6546666622161865\n",
      "Iteration 33870 Training loss 0.08735402673482895 Validation loss 0.1284593641757965 Accuracy 0.6546666622161865\n",
      "Iteration 33880 Training loss 0.0871063619852066 Validation loss 0.11744172871112823 Accuracy 0.6784999966621399\n",
      "Iteration 33890 Training loss 0.08419576287269592 Validation loss 0.11926177889108658 Accuracy 0.6753333210945129\n",
      "Iteration 33900 Training loss 0.09288860857486725 Validation loss 0.14019514620304108 Accuracy 0.6328333616256714\n",
      "Iteration 33910 Training loss 0.09124723076820374 Validation loss 0.1388138234615326 Accuracy 0.6341666579246521\n",
      "Iteration 33920 Training loss 0.08456078171730042 Validation loss 0.12102311104536057 Accuracy 0.672166645526886\n",
      "Iteration 33930 Training loss 0.08845275640487671 Validation loss 0.12608715891838074 Accuracy 0.6588333249092102\n",
      "Iteration 33940 Training loss 0.09032595902681351 Validation loss 0.1329052448272705 Accuracy 0.6474999785423279\n",
      "Iteration 33950 Training loss 0.09111718833446503 Validation loss 0.14013417065143585 Accuracy 0.6328333616256714\n",
      "Iteration 33960 Training loss 0.08979012072086334 Validation loss 0.13591617345809937 Accuracy 0.6413333415985107\n",
      "Iteration 33970 Training loss 0.08832041174173355 Validation loss 0.12945717573165894 Accuracy 0.6520000100135803\n",
      "Iteration 33980 Training loss 0.08961985260248184 Validation loss 0.13322030007839203 Accuracy 0.6480000019073486\n",
      "Iteration 33990 Training loss 0.08547669649124146 Validation loss 0.11972388625144958 Accuracy 0.6743333339691162\n",
      "Iteration 34000 Training loss 0.08671626448631287 Validation loss 0.11541181802749634 Accuracy 0.6828333139419556\n",
      "Iteration 34010 Training loss 0.08863390982151031 Validation loss 0.11467491835355759 Accuracy 0.6851666569709778\n",
      "Iteration 34020 Training loss 0.08794350177049637 Validation loss 0.11655072867870331 Accuracy 0.6801666617393494\n",
      "Iteration 34030 Training loss 0.08725351840257645 Validation loss 0.12428562343120575 Accuracy 0.6628333330154419\n",
      "Iteration 34040 Training loss 0.09602443128824234 Validation loss 0.14376212656497955 Accuracy 0.6269999742507935\n",
      "Iteration 34050 Training loss 0.08979412913322449 Validation loss 0.13309602439403534 Accuracy 0.6481666564941406\n",
      "Iteration 34060 Training loss 0.08968910574913025 Validation loss 0.12980538606643677 Accuracy 0.6535000205039978\n",
      "Iteration 34070 Training loss 0.08652522414922714 Validation loss 0.13263572752475739 Accuracy 0.6473333239555359\n",
      "Iteration 34080 Training loss 0.08895481377840042 Validation loss 0.1355256885290146 Accuracy 0.6420000195503235\n",
      "Iteration 34090 Training loss 0.08710131794214249 Validation loss 0.12855792045593262 Accuracy 0.656499981880188\n",
      "Iteration 34100 Training loss 0.08921122550964355 Validation loss 0.13093867897987366 Accuracy 0.6510000228881836\n",
      "Iteration 34110 Training loss 0.09286729991436005 Validation loss 0.14043064415454865 Accuracy 0.6316666603088379\n",
      "Iteration 34120 Training loss 0.08695731312036514 Validation loss 0.12504419684410095 Accuracy 0.6625000238418579\n",
      "Iteration 34130 Training loss 0.08679244667291641 Validation loss 0.12356889992952347 Accuracy 0.6658333539962769\n",
      "Iteration 34140 Training loss 0.0889834389090538 Validation loss 0.1279308944940567 Accuracy 0.6556666493415833\n",
      "Iteration 34150 Training loss 0.09118302166461945 Validation loss 0.13912305235862732 Accuracy 0.6346666812896729\n",
      "Iteration 34160 Training loss 0.08944045007228851 Validation loss 0.12688249349594116 Accuracy 0.6589999794960022\n",
      "Iteration 34170 Training loss 0.08589274436235428 Validation loss 0.12750090658664703 Accuracy 0.656333327293396\n",
      "Iteration 34180 Training loss 0.08884318917989731 Validation loss 0.12982571125030518 Accuracy 0.6538333296775818\n",
      "Iteration 34190 Training loss 0.09089214354753494 Validation loss 0.1447405219078064 Accuracy 0.625333309173584\n",
      "Iteration 34200 Training loss 0.08921226114034653 Validation loss 0.13857832551002502 Accuracy 0.6356666684150696\n",
      "Iteration 34210 Training loss 0.09155242890119553 Validation loss 0.13729414343833923 Accuracy 0.637499988079071\n",
      "Iteration 34220 Training loss 0.0894383117556572 Validation loss 0.13031762838363647 Accuracy 0.6521666646003723\n",
      "Iteration 34230 Training loss 0.08665469288825989 Validation loss 0.12804125249385834 Accuracy 0.6566666960716248\n",
      "Iteration 34240 Training loss 0.08738173544406891 Validation loss 0.12738512456417084 Accuracy 0.6575000286102295\n",
      "Iteration 34250 Training loss 0.08776737749576569 Validation loss 0.12804436683654785 Accuracy 0.656000018119812\n",
      "Iteration 34260 Training loss 0.08802103996276855 Validation loss 0.13219796121120453 Accuracy 0.6499999761581421\n",
      "Iteration 34270 Training loss 0.0873434990644455 Validation loss 0.13029593229293823 Accuracy 0.653166651725769\n",
      "Iteration 34280 Training loss 0.0894504114985466 Validation loss 0.13384124636650085 Accuracy 0.6441666483879089\n",
      "Iteration 34290 Training loss 0.08797438442707062 Validation loss 0.129435732960701 Accuracy 0.653333306312561\n",
      "Iteration 34300 Training loss 0.08992262929677963 Validation loss 0.13122355937957764 Accuracy 0.6511666774749756\n",
      "Iteration 34310 Training loss 0.08728659898042679 Validation loss 0.12098805606365204 Accuracy 0.671833336353302\n",
      "Iteration 34320 Training loss 0.08505164086818695 Validation loss 0.1192464828491211 Accuracy 0.6765000224113464\n",
      "Iteration 34330 Training loss 0.08728592842817307 Validation loss 0.12034261971712112 Accuracy 0.6723333597183228\n",
      "Iteration 34340 Training loss 0.08908017724752426 Validation loss 0.12930549681186676 Accuracy 0.653166651725769\n",
      "Iteration 34350 Training loss 0.09771062433719635 Validation loss 0.14854224026203156 Accuracy 0.6158333420753479\n",
      "Iteration 34360 Training loss 0.08569043129682541 Validation loss 0.12172257155179977 Accuracy 0.6698333621025085\n",
      "Iteration 34370 Training loss 0.08776376396417618 Validation loss 0.12642055749893188 Accuracy 0.659166693687439\n",
      "Iteration 34380 Training loss 0.08919616788625717 Validation loss 0.13732604682445526 Accuracy 0.6381666660308838\n",
      "Iteration 34390 Training loss 0.08924952894449234 Validation loss 0.13181227445602417 Accuracy 0.6496666669845581\n",
      "Iteration 34400 Training loss 0.0901767686009407 Validation loss 0.13157527148723602 Accuracy 0.6499999761581421\n",
      "Iteration 34410 Training loss 0.09176192432641983 Validation loss 0.1378646045923233 Accuracy 0.637666642665863\n",
      "Iteration 34420 Training loss 0.09113631397485733 Validation loss 0.1370953768491745 Accuracy 0.6383333206176758\n",
      "Iteration 34430 Training loss 0.08861961215734482 Validation loss 0.13067249953746796 Accuracy 0.6514999866485596\n",
      "Iteration 34440 Training loss 0.08893582969903946 Validation loss 0.12237496674060822 Accuracy 0.6690000295639038\n",
      "Iteration 34450 Training loss 0.08719903230667114 Validation loss 0.11754864454269409 Accuracy 0.6793333292007446\n",
      "Iteration 34460 Training loss 0.08716707676649094 Validation loss 0.11488188803195953 Accuracy 0.684166669845581\n",
      "Iteration 34470 Training loss 0.09116638451814651 Validation loss 0.1131729930639267 Accuracy 0.6868333220481873\n",
      "Iteration 34480 Training loss 0.09107539802789688 Validation loss 0.11308648437261581 Accuracy 0.6858333349227905\n",
      "Iteration 34490 Training loss 0.09217163920402527 Validation loss 0.11312306672334671 Accuracy 0.6851666569709778\n",
      "Iteration 34500 Training loss 0.09012235701084137 Validation loss 0.11342503130435944 Accuracy 0.6840000152587891\n",
      "Iteration 34510 Training loss 0.09182111918926239 Validation loss 0.11303061246871948 Accuracy 0.6865000128746033\n",
      "Iteration 34520 Training loss 0.08755351603031158 Validation loss 0.11403828859329224 Accuracy 0.6866666674613953\n",
      "Iteration 34530 Training loss 0.08632110804319382 Validation loss 0.11858639866113663 Accuracy 0.6783333420753479\n",
      "Iteration 34540 Training loss 0.08317142724990845 Validation loss 0.1178736537694931 Accuracy 0.6790000200271606\n",
      "Iteration 34550 Training loss 0.08615203946828842 Validation loss 0.1238059476017952 Accuracy 0.6668333411216736\n",
      "Iteration 34560 Training loss 0.09227418899536133 Validation loss 0.1413080245256424 Accuracy 0.6311666369438171\n",
      "Iteration 34570 Training loss 0.08717124164104462 Validation loss 0.13241764903068542 Accuracy 0.6474999785423279\n",
      "Iteration 34580 Training loss 0.08843874931335449 Validation loss 0.13024279475212097 Accuracy 0.6516666412353516\n",
      "Iteration 34590 Training loss 0.08943615853786469 Validation loss 0.12965932488441467 Accuracy 0.652999997138977\n",
      "Iteration 34600 Training loss 0.08656390011310577 Validation loss 0.12362302839756012 Accuracy 0.6666666865348816\n",
      "Iteration 34610 Training loss 0.08894646167755127 Validation loss 0.13058367371559143 Accuracy 0.652999997138977\n",
      "Iteration 34620 Training loss 0.09299920499324799 Validation loss 0.14164307713508606 Accuracy 0.6311666369438171\n",
      "Iteration 34630 Training loss 0.0930209532380104 Validation loss 0.13789956271648407 Accuracy 0.6383333206176758\n",
      "Iteration 34640 Training loss 0.08723142743110657 Validation loss 0.1274433583021164 Accuracy 0.659166693687439\n",
      "Iteration 34650 Training loss 0.0858592763543129 Validation loss 0.1270826756954193 Accuracy 0.6576666831970215\n",
      "Iteration 34660 Training loss 0.0874733105301857 Validation loss 0.1283659040927887 Accuracy 0.6543333530426025\n",
      "Iteration 34670 Training loss 0.08554980158805847 Validation loss 0.1206660345196724 Accuracy 0.6723333597183228\n",
      "Iteration 34680 Training loss 0.08578184247016907 Validation loss 0.118342325091362 Accuracy 0.6769999861717224\n",
      "Iteration 34690 Training loss 0.08932802081108093 Validation loss 0.11339191347360611 Accuracy 0.6840000152587891\n",
      "Iteration 34700 Training loss 0.0881945937871933 Validation loss 0.11403538286685944 Accuracy 0.6853333115577698\n",
      "Iteration 34710 Training loss 0.08895184099674225 Validation loss 0.11323989927768707 Accuracy 0.6855000257492065\n",
      "Iteration 34720 Training loss 0.08677828311920166 Validation loss 0.1141948252916336 Accuracy 0.6851666569709778\n",
      "Iteration 34730 Training loss 0.08703134953975677 Validation loss 0.11560823768377304 Accuracy 0.6815000176429749\n",
      "Iteration 34740 Training loss 0.09145015478134155 Validation loss 0.11331091076135635 Accuracy 0.6853333115577698\n",
      "Iteration 34750 Training loss 0.08814696222543716 Validation loss 0.11353538185358047 Accuracy 0.6863333582878113\n",
      "Iteration 34760 Training loss 0.08766244351863861 Validation loss 0.11383764445781708 Accuracy 0.6848333477973938\n",
      "Iteration 34770 Training loss 0.08964961767196655 Validation loss 0.11343670636415482 Accuracy 0.6848333477973938\n",
      "Iteration 34780 Training loss 0.0885283499956131 Validation loss 0.11363669484853745 Accuracy 0.6848333477973938\n",
      "Iteration 34790 Training loss 0.09090153127908707 Validation loss 0.11325705796480179 Accuracy 0.6846666932106018\n",
      "Iteration 34800 Training loss 0.09422136098146439 Validation loss 0.11332786083221436 Accuracy 0.6855000257492065\n",
      "Iteration 34810 Training loss 0.0875634253025055 Validation loss 0.1156623438000679 Accuracy 0.6806666851043701\n",
      "Iteration 34820 Training loss 0.09001155942678452 Validation loss 0.11334270983934402 Accuracy 0.6851666569709778\n",
      "Iteration 34830 Training loss 0.0898171216249466 Validation loss 0.11298687756061554 Accuracy 0.6858333349227905\n",
      "Iteration 34840 Training loss 0.08834172785282135 Validation loss 0.11329247802495956 Accuracy 0.6846666932106018\n",
      "Iteration 34850 Training loss 0.08820633590221405 Validation loss 0.1133740022778511 Accuracy 0.6851666569709778\n",
      "Iteration 34860 Training loss 0.08801085501909256 Validation loss 0.11364702135324478 Accuracy 0.6858333349227905\n",
      "Iteration 34870 Training loss 0.08839553594589233 Validation loss 0.1143101379275322 Accuracy 0.684166669845581\n",
      "Iteration 34880 Training loss 0.08513235300779343 Validation loss 0.12377005070447922 Accuracy 0.6638333201408386\n",
      "Iteration 34890 Training loss 0.08720685541629791 Validation loss 0.12535513937473297 Accuracy 0.6623333096504211\n",
      "Iteration 34900 Training loss 0.09033369272947311 Validation loss 0.1305251270532608 Accuracy 0.6523333191871643\n",
      "Iteration 34910 Training loss 0.0887908861041069 Validation loss 0.13101066648960114 Accuracy 0.6514999866485596\n",
      "Iteration 34920 Training loss 0.09127398580312729 Validation loss 0.1314261257648468 Accuracy 0.6498333215713501\n",
      "Iteration 34930 Training loss 0.08754707872867584 Validation loss 0.13622112572193146 Accuracy 0.6401666402816772\n",
      "Iteration 34940 Training loss 0.08654572814702988 Validation loss 0.12384344637393951 Accuracy 0.6673333048820496\n",
      "Iteration 34950 Training loss 0.08613216131925583 Validation loss 0.1198965534567833 Accuracy 0.6746666431427002\n",
      "Iteration 34960 Training loss 0.08647731691598892 Validation loss 0.12182840704917908 Accuracy 0.6713333129882812\n",
      "Iteration 34970 Training loss 0.08664654195308685 Validation loss 0.12774233520030975 Accuracy 0.656000018119812\n",
      "Iteration 34980 Training loss 0.094899483025074 Validation loss 0.14792539179325104 Accuracy 0.6190000176429749\n",
      "Iteration 34990 Training loss 0.09097744524478912 Validation loss 0.1422131210565567 Accuracy 0.6290000081062317\n",
      "Iteration 35000 Training loss 0.09156285971403122 Validation loss 0.13180001080036163 Accuracy 0.6480000019073486\n",
      "Iteration 35010 Training loss 0.08834719657897949 Validation loss 0.1276807188987732 Accuracy 0.6571666598320007\n",
      "Iteration 35020 Training loss 0.09294510632753372 Validation loss 0.138181671500206 Accuracy 0.6340000033378601\n",
      "Iteration 35030 Training loss 0.08569894731044769 Validation loss 0.12817884981632233 Accuracy 0.6575000286102295\n",
      "Iteration 35040 Training loss 0.08767224103212357 Validation loss 0.11836108565330505 Accuracy 0.6758333444595337\n",
      "Iteration 35050 Training loss 0.08612227439880371 Validation loss 0.12154848128557205 Accuracy 0.6711666584014893\n",
      "Iteration 35060 Training loss 0.08733173459768295 Validation loss 0.12121662497520447 Accuracy 0.6710000038146973\n",
      "Iteration 35070 Training loss 0.09335633367300034 Validation loss 0.14286457002162933 Accuracy 0.6284999847412109\n",
      "Iteration 35080 Training loss 0.08822568506002426 Validation loss 0.13281399011611938 Accuracy 0.6480000019073486\n",
      "Iteration 35090 Training loss 0.08591131865978241 Validation loss 0.1258338838815689 Accuracy 0.6608333587646484\n",
      "Iteration 35100 Training loss 0.0879257470369339 Validation loss 0.12957556545734406 Accuracy 0.6535000205039978\n",
      "Iteration 35110 Training loss 0.09212125837802887 Validation loss 0.13640762865543365 Accuracy 0.6395000219345093\n",
      "Iteration 35120 Training loss 0.08753153681755066 Validation loss 0.1265573799610138 Accuracy 0.6581666469573975\n",
      "Iteration 35130 Training loss 0.08941394835710526 Validation loss 0.13085541129112244 Accuracy 0.6513333320617676\n",
      "Iteration 35140 Training loss 0.09157877415418625 Validation loss 0.1385597288608551 Accuracy 0.6355000138282776\n",
      "Iteration 35150 Training loss 0.08532965183258057 Validation loss 0.12779752910137177 Accuracy 0.6570000052452087\n",
      "Iteration 35160 Training loss 0.09117232263088226 Validation loss 0.13782483339309692 Accuracy 0.6361666917800903\n",
      "Iteration 35170 Training loss 0.08620068430900574 Validation loss 0.12888716161251068 Accuracy 0.6548333168029785\n",
      "Iteration 35180 Training loss 0.08864695578813553 Validation loss 0.13593654334545135 Accuracy 0.6393333077430725\n",
      "Iteration 35190 Training loss 0.08879123628139496 Validation loss 0.13287906348705292 Accuracy 0.6466666460037231\n",
      "Iteration 35200 Training loss 0.09062144160270691 Validation loss 0.13190574944019318 Accuracy 0.6491666436195374\n",
      "Iteration 35210 Training loss 0.08905728906393051 Validation loss 0.12993493676185608 Accuracy 0.653333306312561\n",
      "Iteration 35220 Training loss 0.08926328271627426 Validation loss 0.1318066418170929 Accuracy 0.6503333449363708\n",
      "Iteration 35230 Training loss 0.08900199085474014 Validation loss 0.13320639729499817 Accuracy 0.6478333473205566\n",
      "Iteration 35240 Training loss 0.090738944709301 Validation loss 0.13402815163135529 Accuracy 0.6460000276565552\n",
      "Iteration 35250 Training loss 0.08953465521335602 Validation loss 0.1347142904996872 Accuracy 0.6428333520889282\n",
      "Iteration 35260 Training loss 0.08729591220617294 Validation loss 0.11896901577711105 Accuracy 0.6741666793823242\n",
      "Iteration 35270 Training loss 0.08707723021507263 Validation loss 0.12790097296237946 Accuracy 0.6558333039283752\n",
      "Iteration 35280 Training loss 0.08631757646799088 Validation loss 0.12374182790517807 Accuracy 0.6651666760444641\n",
      "Iteration 35290 Training loss 0.08982960879802704 Validation loss 0.13752879202365875 Accuracy 0.6361666917800903\n",
      "Iteration 35300 Training loss 0.09121895581483841 Validation loss 0.12721869349479675 Accuracy 0.6588333249092102\n",
      "Iteration 35310 Training loss 0.09054058790206909 Validation loss 0.13605311512947083 Accuracy 0.6421666741371155\n",
      "Iteration 35320 Training loss 0.0898313969373703 Validation loss 0.13441747426986694 Accuracy 0.6448333263397217\n",
      "Iteration 35330 Training loss 0.0903405174612999 Validation loss 0.141410231590271 Accuracy 0.6316666603088379\n",
      "Iteration 35340 Training loss 0.08785540610551834 Validation loss 0.13019196689128876 Accuracy 0.6528333425521851\n",
      "Iteration 35350 Training loss 0.09040682017803192 Validation loss 0.13391032814979553 Accuracy 0.6456666588783264\n",
      "Iteration 35360 Training loss 0.09262842684984207 Validation loss 0.14606156945228577 Accuracy 0.6208333373069763\n",
      "Iteration 35370 Training loss 0.08716727793216705 Validation loss 0.13394097983837128 Accuracy 0.6451666951179504\n",
      "Iteration 35380 Training loss 0.08807983249425888 Validation loss 0.1291029453277588 Accuracy 0.6543333530426025\n",
      "Iteration 35390 Training loss 0.08709380775690079 Validation loss 0.12815101444721222 Accuracy 0.6551666855812073\n",
      "Iteration 35400 Training loss 0.08882541209459305 Validation loss 0.12657810747623444 Accuracy 0.6606666445732117\n",
      "Iteration 35410 Training loss 0.08669693022966385 Validation loss 0.12779802083969116 Accuracy 0.656499981880188\n",
      "Iteration 35420 Training loss 0.08923577517271042 Validation loss 0.13724344968795776 Accuracy 0.637666642665863\n",
      "Iteration 35430 Training loss 0.08810777962207794 Validation loss 0.1357046365737915 Accuracy 0.6423333287239075\n",
      "Iteration 35440 Training loss 0.0881848931312561 Validation loss 0.13223540782928467 Accuracy 0.6489999890327454\n",
      "Iteration 35450 Training loss 0.09198488295078278 Validation loss 0.13668884336948395 Accuracy 0.6395000219345093\n",
      "Iteration 35460 Training loss 0.08800734579563141 Validation loss 0.1268499195575714 Accuracy 0.6596666574478149\n",
      "Iteration 35470 Training loss 0.08453131467103958 Validation loss 0.12249553203582764 Accuracy 0.6685000061988831\n",
      "Iteration 35480 Training loss 0.0864444375038147 Validation loss 0.11679677665233612 Accuracy 0.6806666851043701\n",
      "Iteration 35490 Training loss 0.08673112094402313 Validation loss 0.11389883607625961 Accuracy 0.6850000023841858\n",
      "Iteration 35500 Training loss 0.09373379498720169 Validation loss 0.11363961547613144 Accuracy 0.6861666440963745\n",
      "Iteration 35510 Training loss 0.0878402516245842 Validation loss 0.11373467743396759 Accuracy 0.6865000128746033\n",
      "Iteration 35520 Training loss 0.08650346100330353 Validation loss 0.11440171301364899 Accuracy 0.6851666569709778\n",
      "Iteration 35530 Training loss 0.08932375907897949 Validation loss 0.11400679498910904 Accuracy 0.6851666569709778\n",
      "Iteration 35540 Training loss 0.08867960423231125 Validation loss 0.11329241842031479 Accuracy 0.6853333115577698\n",
      "Iteration 35550 Training loss 0.08801751583814621 Validation loss 0.11379430443048477 Accuracy 0.6856666803359985\n",
      "Iteration 35560 Training loss 0.08863672614097595 Validation loss 0.11361274123191833 Accuracy 0.6856666803359985\n",
      "Iteration 35570 Training loss 0.09017502516508102 Validation loss 0.11331110447645187 Accuracy 0.6848333477973938\n",
      "Iteration 35580 Training loss 0.09000984579324722 Validation loss 0.11335189640522003 Accuracy 0.6859999895095825\n",
      "Iteration 35590 Training loss 0.08870536088943481 Validation loss 0.11398342996835709 Accuracy 0.6851666569709778\n",
      "Iteration 35600 Training loss 0.08414726704359055 Validation loss 0.11594168841838837 Accuracy 0.6818333268165588\n",
      "Iteration 35610 Training loss 0.08801107853651047 Validation loss 0.11552443355321884 Accuracy 0.6818333268165588\n",
      "Iteration 35620 Training loss 0.09072425216436386 Validation loss 0.11337042599916458 Accuracy 0.684333324432373\n",
      "Iteration 35630 Training loss 0.08763793110847473 Validation loss 0.11375847458839417 Accuracy 0.6868333220481873\n",
      "Iteration 35640 Training loss 0.08789104968309402 Validation loss 0.11380574852228165 Accuracy 0.6851666569709778\n",
      "Iteration 35650 Training loss 0.0906817838549614 Validation loss 0.1132393404841423 Accuracy 0.6846666932106018\n",
      "Iteration 35660 Training loss 0.08826635777950287 Validation loss 0.11716581135988235 Accuracy 0.6815000176429749\n",
      "Iteration 35670 Training loss 0.08526215702295303 Validation loss 0.1169690266251564 Accuracy 0.6800000071525574\n",
      "Iteration 35680 Training loss 0.08462997525930405 Validation loss 0.1224336251616478 Accuracy 0.6696666479110718\n",
      "Iteration 35690 Training loss 0.08558113873004913 Validation loss 0.12489134073257446 Accuracy 0.6658333539962769\n",
      "Iteration 35700 Training loss 0.08862706273794174 Validation loss 0.1281326562166214 Accuracy 0.6575000286102295\n",
      "Iteration 35710 Training loss 0.0890590026974678 Validation loss 0.13283367455005646 Accuracy 0.6491666436195374\n",
      "Iteration 35720 Training loss 0.08707238733768463 Validation loss 0.13598181307315826 Accuracy 0.6411666870117188\n",
      "Iteration 35730 Training loss 0.09054223448038101 Validation loss 0.14055074751377106 Accuracy 0.6324999928474426\n",
      "Iteration 35740 Training loss 0.0912458673119545 Validation loss 0.13564905524253845 Accuracy 0.6428333520889282\n",
      "Iteration 35750 Training loss 0.08544453233480453 Validation loss 0.12847265601158142 Accuracy 0.6553333401679993\n",
      "Iteration 35760 Training loss 0.08742915838956833 Validation loss 0.12640389800071716 Accuracy 0.6604999899864197\n",
      "Iteration 35770 Training loss 0.09091847389936447 Validation loss 0.13844269514083862 Accuracy 0.6349999904632568\n",
      "Iteration 35780 Training loss 0.09299556165933609 Validation loss 0.13793347775936127 Accuracy 0.6383333206176758\n",
      "Iteration 35790 Training loss 0.09083986282348633 Validation loss 0.1388104110956192 Accuracy 0.6355000138282776\n",
      "Iteration 35800 Training loss 0.09131693840026855 Validation loss 0.13246336579322815 Accuracy 0.6486666798591614\n",
      "Iteration 35810 Training loss 0.08997439593076706 Validation loss 0.13184134662151337 Accuracy 0.6499999761581421\n",
      "Iteration 35820 Training loss 0.09003924578428268 Validation loss 0.13180530071258545 Accuracy 0.6496666669845581\n",
      "Iteration 35830 Training loss 0.08847502619028091 Validation loss 0.13180777430534363 Accuracy 0.6491666436195374\n",
      "Iteration 35840 Training loss 0.08940286934375763 Validation loss 0.13253995776176453 Accuracy 0.6486666798591614\n",
      "Iteration 35850 Training loss 0.08765104413032532 Validation loss 0.128951758146286 Accuracy 0.656166672706604\n",
      "Iteration 35860 Training loss 0.08573903143405914 Validation loss 0.12675443291664124 Accuracy 0.6588333249092102\n",
      "Iteration 35870 Training loss 0.08977453410625458 Validation loss 0.1385020911693573 Accuracy 0.6355000138282776\n",
      "Iteration 35880 Training loss 0.08995264023542404 Validation loss 0.13225510716438293 Accuracy 0.6471666693687439\n",
      "Iteration 35890 Training loss 0.08720258623361588 Validation loss 0.13265584409236908 Accuracy 0.6476666927337646\n",
      "Iteration 35900 Training loss 0.08500460535287857 Validation loss 0.1240905225276947 Accuracy 0.6666666865348816\n",
      "Iteration 35910 Training loss 0.08851049095392227 Validation loss 0.13002492487430573 Accuracy 0.653166651725769\n",
      "Iteration 35920 Training loss 0.08994624763727188 Validation loss 0.12992903590202332 Accuracy 0.6538333296775818\n",
      "Iteration 35930 Training loss 0.08798504620790482 Validation loss 0.129124715924263 Accuracy 0.6553333401679993\n",
      "Iteration 35940 Training loss 0.08998861908912659 Validation loss 0.13314767181873322 Accuracy 0.6470000147819519\n",
      "Iteration 35950 Training loss 0.08945257216691971 Validation loss 0.13595768809318542 Accuracy 0.64083331823349\n",
      "Iteration 35960 Training loss 0.08597825467586517 Validation loss 0.1282528191804886 Accuracy 0.6554999947547913\n",
      "Iteration 35970 Training loss 0.08703771978616714 Validation loss 0.11707845330238342 Accuracy 0.6800000071525574\n",
      "Iteration 35980 Training loss 0.08367660641670227 Validation loss 0.1186465173959732 Accuracy 0.6778333187103271\n",
      "Iteration 35990 Training loss 0.08788466453552246 Validation loss 0.11400561779737473 Accuracy 0.6851666569709778\n",
      "Iteration 36000 Training loss 0.08886747062206268 Validation loss 0.11472098529338837 Accuracy 0.684166669845581\n",
      "Iteration 36010 Training loss 0.09242601692676544 Validation loss 0.11364280432462692 Accuracy 0.6856666803359985\n",
      "Iteration 36020 Training loss 0.08995223790407181 Validation loss 0.11360082030296326 Accuracy 0.6850000023841858\n",
      "Iteration 36030 Training loss 0.09079571813344955 Validation loss 0.11385681480169296 Accuracy 0.684333324432373\n",
      "Iteration 36040 Training loss 0.0894629955291748 Validation loss 0.11337895691394806 Accuracy 0.6856666803359985\n",
      "Iteration 36050 Training loss 0.08777448534965515 Validation loss 0.1140308752655983 Accuracy 0.6851666569709778\n",
      "Iteration 36060 Training loss 0.09012893587350845 Validation loss 0.11334864050149918 Accuracy 0.6859999895095825\n",
      "Iteration 36070 Training loss 0.09023647010326385 Validation loss 0.11337614804506302 Accuracy 0.6855000257492065\n",
      "Iteration 36080 Training loss 0.08704225718975067 Validation loss 0.11454255133867264 Accuracy 0.6856666803359985\n",
      "Iteration 36090 Training loss 0.08730053901672363 Validation loss 0.11462477594614029 Accuracy 0.6848333477973938\n",
      "Iteration 36100 Training loss 0.09102647006511688 Validation loss 0.11342809349298477 Accuracy 0.6850000023841858\n",
      "Iteration 36110 Training loss 0.08638051152229309 Validation loss 0.11348117887973785 Accuracy 0.6851666569709778\n",
      "Iteration 36120 Training loss 0.08718779683113098 Validation loss 0.11349008977413177 Accuracy 0.6853333115577698\n",
      "Iteration 36130 Training loss 0.08867714554071426 Validation loss 0.11358503252267838 Accuracy 0.6856666803359985\n",
      "Iteration 36140 Training loss 0.08355529606342316 Validation loss 0.1175454780459404 Accuracy 0.6783333420753479\n",
      "Iteration 36150 Training loss 0.08808859437704086 Validation loss 0.12711188197135925 Accuracy 0.6589999794960022\n",
      "Iteration 36160 Training loss 0.09002247452735901 Validation loss 0.13538898527622223 Accuracy 0.6421666741371155\n",
      "Iteration 36170 Training loss 0.08725258708000183 Validation loss 0.13367801904678345 Accuracy 0.6466666460037231\n",
      "Iteration 36180 Training loss 0.08396142721176147 Validation loss 0.12466088682413101 Accuracy 0.6646666526794434\n",
      "Iteration 36190 Training loss 0.08999787271022797 Validation loss 0.12944620847702026 Accuracy 0.6545000076293945\n",
      "Iteration 36200 Training loss 0.08582022786140442 Validation loss 0.1299467533826828 Accuracy 0.652999997138977\n",
      "Iteration 36210 Training loss 0.0856427326798439 Validation loss 0.1228020191192627 Accuracy 0.6675000190734863\n",
      "Iteration 36220 Training loss 0.08611596375703812 Validation loss 0.12348863482475281 Accuracy 0.6668333411216736\n",
      "Iteration 36230 Training loss 0.0884971097111702 Validation loss 0.12927035987377167 Accuracy 0.6553333401679993\n",
      "Iteration 36240 Training loss 0.0876130759716034 Validation loss 0.12875734269618988 Accuracy 0.656000018119812\n",
      "Iteration 36250 Training loss 0.0919828861951828 Validation loss 0.13650541007518768 Accuracy 0.6413333415985107\n",
      "Iteration 36260 Training loss 0.09016415476799011 Validation loss 0.1320112645626068 Accuracy 0.6493333578109741\n",
      "Iteration 36270 Training loss 0.0876907929778099 Validation loss 0.1346190869808197 Accuracy 0.6449999809265137\n",
      "Iteration 36280 Training loss 0.0875898003578186 Validation loss 0.12861458957195282 Accuracy 0.656000018119812\n",
      "Iteration 36290 Training loss 0.0869753435254097 Validation loss 0.13014446198940277 Accuracy 0.6528333425521851\n",
      "Iteration 36300 Training loss 0.09098774939775467 Validation loss 0.1366848200559616 Accuracy 0.640999972820282\n",
      "Iteration 36310 Training loss 0.08743444830179214 Validation loss 0.13203534483909607 Accuracy 0.6485000252723694\n",
      "Iteration 36320 Training loss 0.08594439923763275 Validation loss 0.12881578505039215 Accuracy 0.656000018119812\n",
      "Iteration 36330 Training loss 0.0908210277557373 Validation loss 0.1340423971414566 Accuracy 0.6445000171661377\n",
      "Iteration 36340 Training loss 0.08908174932003021 Validation loss 0.13384780287742615 Accuracy 0.6464999914169312\n",
      "Iteration 36350 Training loss 0.08865969628095627 Validation loss 0.13385289907455444 Accuracy 0.6463333368301392\n",
      "Iteration 36360 Training loss 0.08822167664766312 Validation loss 0.12535026669502258 Accuracy 0.6626666784286499\n",
      "Iteration 36370 Training loss 0.08683701604604721 Validation loss 0.12730193138122559 Accuracy 0.6586666703224182\n",
      "Iteration 36380 Training loss 0.08880003541707993 Validation loss 0.13700048625469208 Accuracy 0.6389999985694885\n",
      "Iteration 36390 Training loss 0.08759697526693344 Validation loss 0.13121837377548218 Accuracy 0.6496666669845581\n",
      "Iteration 36400 Training loss 0.08760925382375717 Validation loss 0.12708985805511475 Accuracy 0.6586666703224182\n",
      "Iteration 36410 Training loss 0.08782561123371124 Validation loss 0.12837660312652588 Accuracy 0.6570000052452087\n",
      "Iteration 36420 Training loss 0.09246714413166046 Validation loss 0.13500401377677917 Accuracy 0.6421666741371155\n",
      "Iteration 36430 Training loss 0.09129180759191513 Validation loss 0.13557730615139008 Accuracy 0.6426666378974915\n",
      "Iteration 36440 Training loss 0.08933182805776596 Validation loss 0.13084450364112854 Accuracy 0.6510000228881836\n",
      "Iteration 36450 Training loss 0.08533470332622528 Validation loss 0.13165432214736938 Accuracy 0.6514999866485596\n",
      "Iteration 36460 Training loss 0.08916129916906357 Validation loss 0.13085263967514038 Accuracy 0.6520000100135803\n",
      "Iteration 36470 Training loss 0.08888792246580124 Validation loss 0.13443388044834137 Accuracy 0.6449999809265137\n",
      "Iteration 36480 Training loss 0.0842786580324173 Validation loss 0.1243472769856453 Accuracy 0.6638333201408386\n",
      "Iteration 36490 Training loss 0.08842279762029648 Validation loss 0.12248494476079941 Accuracy 0.6691666841506958\n",
      "Iteration 36500 Training loss 0.08570393174886703 Validation loss 0.12831780314445496 Accuracy 0.6566666960716248\n",
      "Iteration 36510 Training loss 0.09142187237739563 Validation loss 0.13675935566425323 Accuracy 0.6389999985694885\n",
      "Iteration 36520 Training loss 0.08900390565395355 Validation loss 0.1354338526725769 Accuracy 0.6420000195503235\n",
      "Iteration 36530 Training loss 0.08666980266571045 Validation loss 0.1304686963558197 Accuracy 0.652999997138977\n",
      "Iteration 36540 Training loss 0.08493468910455704 Validation loss 0.12597531080245972 Accuracy 0.6623333096504211\n",
      "Iteration 36550 Training loss 0.08930056542158127 Validation loss 0.13198420405387878 Accuracy 0.6493333578109741\n",
      "Iteration 36560 Training loss 0.08733785152435303 Validation loss 0.12737946212291718 Accuracy 0.6581666469573975\n",
      "Iteration 36570 Training loss 0.08770570904016495 Validation loss 0.1330549716949463 Accuracy 0.6474999785423279\n",
      "Iteration 36580 Training loss 0.09152402728796005 Validation loss 0.13864074647426605 Accuracy 0.6368333101272583\n",
      "Iteration 36590 Training loss 0.08906231820583344 Validation loss 0.13271015882492065 Accuracy 0.6481666564941406\n",
      "Iteration 36600 Training loss 0.08623553812503815 Validation loss 0.1279226392507553 Accuracy 0.656000018119812\n",
      "Iteration 36610 Training loss 0.08404974639415741 Validation loss 0.12083172798156738 Accuracy 0.6733333468437195\n",
      "Iteration 36620 Training loss 0.08509068191051483 Validation loss 0.12920105457305908 Accuracy 0.6541666388511658\n",
      "Iteration 36630 Training loss 0.09064739942550659 Validation loss 0.1369084268808365 Accuracy 0.6399999856948853\n",
      "Iteration 36640 Training loss 0.08951925486326218 Validation loss 0.1346743404865265 Accuracy 0.6448333263397217\n",
      "Iteration 36650 Training loss 0.08649595826864243 Validation loss 0.1279883086681366 Accuracy 0.6566666960716248\n",
      "Iteration 36660 Training loss 0.08681150525808334 Validation loss 0.12467989325523376 Accuracy 0.6636666655540466\n",
      "Iteration 36670 Training loss 0.08540553599596024 Validation loss 0.12473846226930618 Accuracy 0.6639999747276306\n",
      "Iteration 36680 Training loss 0.091358482837677 Validation loss 0.13627523183822632 Accuracy 0.6413333415985107\n",
      "Iteration 36690 Training loss 0.0911591425538063 Validation loss 0.14197394251823425 Accuracy 0.6309999823570251\n",
      "Iteration 36700 Training loss 0.08882687240839005 Validation loss 0.1323634386062622 Accuracy 0.6483333110809326\n",
      "Iteration 36710 Training loss 0.08597088605165482 Validation loss 0.12642507255077362 Accuracy 0.659500002861023\n",
      "Iteration 36720 Training loss 0.08686688542366028 Validation loss 0.13006815314292908 Accuracy 0.652999997138977\n",
      "Iteration 36730 Training loss 0.0882868766784668 Validation loss 0.13421854376792908 Accuracy 0.6463333368301392\n",
      "Iteration 36740 Training loss 0.08866476267576218 Validation loss 0.13441602885723114 Accuracy 0.6455000042915344\n",
      "Iteration 36750 Training loss 0.08748510479927063 Validation loss 0.1289307177066803 Accuracy 0.6541666388511658\n",
      "Iteration 36760 Training loss 0.08603191375732422 Validation loss 0.12315551936626434 Accuracy 0.6668333411216736\n",
      "Iteration 36770 Training loss 0.08657024800777435 Validation loss 0.1305222362279892 Accuracy 0.6526666879653931\n",
      "Iteration 36780 Training loss 0.08881974220275879 Validation loss 0.13386717438697815 Accuracy 0.6466666460037231\n",
      "Iteration 36790 Training loss 0.0895969569683075 Validation loss 0.13575662672519684 Accuracy 0.6441666483879089\n",
      "Iteration 36800 Training loss 0.08804752677679062 Validation loss 0.13023340702056885 Accuracy 0.653166651725769\n",
      "Iteration 36810 Training loss 0.0893750712275505 Validation loss 0.12332825362682343 Accuracy 0.6675000190734863\n",
      "Iteration 36820 Training loss 0.08839218318462372 Validation loss 0.1270686835050583 Accuracy 0.6585000157356262\n",
      "Iteration 36830 Training loss 0.08768056333065033 Validation loss 0.130673348903656 Accuracy 0.6523333191871643\n",
      "Iteration 36840 Training loss 0.08895371854305267 Validation loss 0.13736550509929657 Accuracy 0.637666642665863\n",
      "Iteration 36850 Training loss 0.08486218005418777 Validation loss 0.12183784693479538 Accuracy 0.6710000038146973\n",
      "Iteration 36860 Training loss 0.08721799403429031 Validation loss 0.12360493093729019 Accuracy 0.6664999723434448\n",
      "Iteration 36870 Training loss 0.08788572251796722 Validation loss 0.13036014139652252 Accuracy 0.6528333425521851\n",
      "Iteration 36880 Training loss 0.0897412821650505 Validation loss 0.13438037037849426 Accuracy 0.6453333497047424\n",
      "Iteration 36890 Training loss 0.09026885032653809 Validation loss 0.14141517877578735 Accuracy 0.6321666836738586\n",
      "Iteration 36900 Training loss 0.08812632411718369 Validation loss 0.13017265498638153 Accuracy 0.6528333425521851\n",
      "Iteration 36910 Training loss 0.09052327275276184 Validation loss 0.13258108496665955 Accuracy 0.6485000252723694\n",
      "Iteration 36920 Training loss 0.08645781129598618 Validation loss 0.12726248800754547 Accuracy 0.6588333249092102\n",
      "Iteration 36930 Training loss 0.08449191600084305 Validation loss 0.13208162784576416 Accuracy 0.6501666903495789\n",
      "Iteration 36940 Training loss 0.08589678257703781 Validation loss 0.13074824213981628 Accuracy 0.6523333191871643\n",
      "Iteration 36950 Training loss 0.09051741659641266 Validation loss 0.13243316113948822 Accuracy 0.6495000123977661\n",
      "Iteration 36960 Training loss 0.08552543073892593 Validation loss 0.12915335595607758 Accuracy 0.656000018119812\n",
      "Iteration 36970 Training loss 0.08756226301193237 Validation loss 0.12813590466976166 Accuracy 0.6576666831970215\n",
      "Iteration 36980 Training loss 0.08760959655046463 Validation loss 0.12954889237880707 Accuracy 0.6543333530426025\n",
      "Iteration 36990 Training loss 0.09283874183893204 Validation loss 0.13944335281848907 Accuracy 0.6345000267028809\n",
      "Iteration 37000 Training loss 0.08758111298084259 Validation loss 0.13143616914749146 Accuracy 0.6501666903495789\n",
      "Iteration 37010 Training loss 0.08861534297466278 Validation loss 0.13604891300201416 Accuracy 0.6420000195503235\n",
      "Iteration 37020 Training loss 0.08801043778657913 Validation loss 0.12799960374832153 Accuracy 0.6579999923706055\n",
      "Iteration 37030 Training loss 0.08747810125350952 Validation loss 0.1301548182964325 Accuracy 0.6538333296775818\n",
      "Iteration 37040 Training loss 0.08563357591629028 Validation loss 0.1264997273683548 Accuracy 0.6606666445732117\n",
      "Iteration 37050 Training loss 0.08500637114048004 Validation loss 0.12428899854421616 Accuracy 0.6653333306312561\n",
      "Iteration 37060 Training loss 0.08748286962509155 Validation loss 0.12588460743427277 Accuracy 0.6618333458900452\n",
      "Iteration 37070 Training loss 0.08914251625537872 Validation loss 0.13688746094703674 Accuracy 0.640500009059906\n",
      "Iteration 37080 Training loss 0.0884891152381897 Validation loss 0.13610287010669708 Accuracy 0.643666684627533\n",
      "Iteration 37090 Training loss 0.08846195787191391 Validation loss 0.1304064691066742 Accuracy 0.6535000205039978\n",
      "Iteration 37100 Training loss 0.08860258013010025 Validation loss 0.1347482055425644 Accuracy 0.6453333497047424\n",
      "Iteration 37110 Training loss 0.08940467983484268 Validation loss 0.13263258337974548 Accuracy 0.6483333110809326\n",
      "Iteration 37120 Training loss 0.08701090514659882 Validation loss 0.14035125076770782 Accuracy 0.6328333616256714\n",
      "Iteration 37130 Training loss 0.08941229432821274 Validation loss 0.13548408448696136 Accuracy 0.6453333497047424\n",
      "Iteration 37140 Training loss 0.09079118072986603 Validation loss 0.1365807056427002 Accuracy 0.6418333053588867\n",
      "Iteration 37150 Training loss 0.08583143353462219 Validation loss 0.13692842423915863 Accuracy 0.6416666507720947\n",
      "Iteration 37160 Training loss 0.08729223906993866 Validation loss 0.13469839096069336 Accuracy 0.6456666588783264\n",
      "Iteration 37170 Training loss 0.08732259273529053 Validation loss 0.1369294673204422 Accuracy 0.6391666531562805\n",
      "Iteration 37180 Training loss 0.08811157941818237 Validation loss 0.1286013424396515 Accuracy 0.656333327293396\n",
      "Iteration 37190 Training loss 0.08753412216901779 Validation loss 0.12876787781715393 Accuracy 0.6548333168029785\n",
      "Iteration 37200 Training loss 0.087284617125988 Validation loss 0.12819437682628632 Accuracy 0.6576666831970215\n",
      "Iteration 37210 Training loss 0.08726816624403 Validation loss 0.13148054480552673 Accuracy 0.6511666774749756\n",
      "Iteration 37220 Training loss 0.0926603302359581 Validation loss 0.14531663060188293 Accuracy 0.6255000233650208\n",
      "Iteration 37230 Training loss 0.08798913657665253 Validation loss 0.130904883146286 Accuracy 0.6510000228881836\n",
      "Iteration 37240 Training loss 0.08314608037471771 Validation loss 0.12063999474048615 Accuracy 0.6754999756813049\n",
      "Iteration 37250 Training loss 0.08774132281541824 Validation loss 0.130238875746727 Accuracy 0.6520000100135803\n",
      "Iteration 37260 Training loss 0.08896075189113617 Validation loss 0.12917858362197876 Accuracy 0.6553333401679993\n",
      "Iteration 37270 Training loss 0.08795700967311859 Validation loss 0.13334612548351288 Accuracy 0.6480000019073486\n",
      "Iteration 37280 Training loss 0.08687832951545715 Validation loss 0.12874320149421692 Accuracy 0.6566666960716248\n",
      "Iteration 37290 Training loss 0.09114386141300201 Validation loss 0.13168255984783173 Accuracy 0.6504999995231628\n",
      "Iteration 37300 Training loss 0.08684838563203812 Validation loss 0.13126923143863678 Accuracy 0.6514999866485596\n",
      "Iteration 37310 Training loss 0.08723700791597366 Validation loss 0.13357743620872498 Accuracy 0.6466666460037231\n",
      "Iteration 37320 Training loss 0.0893605425953865 Validation loss 0.13364878296852112 Accuracy 0.6464999914169312\n",
      "Iteration 37330 Training loss 0.0876091793179512 Validation loss 0.13186846673488617 Accuracy 0.6496666669845581\n",
      "Iteration 37340 Training loss 0.08643341064453125 Validation loss 0.13374555110931396 Accuracy 0.6473333239555359\n",
      "Iteration 37350 Training loss 0.08565086871385574 Validation loss 0.12846514582633972 Accuracy 0.6583333611488342\n",
      "Iteration 37360 Training loss 0.0855037122964859 Validation loss 0.12131553143262863 Accuracy 0.6706666946411133\n",
      "Iteration 37370 Training loss 0.09006297588348389 Validation loss 0.11427445709705353 Accuracy 0.6856666803359985\n",
      "Iteration 37380 Training loss 0.0873839482665062 Validation loss 0.1139049306511879 Accuracy 0.6846666932106018\n",
      "Iteration 37390 Training loss 0.08646412938833237 Validation loss 0.11711964756250381 Accuracy 0.6815000176429749\n",
      "Iteration 37400 Training loss 0.08982376754283905 Validation loss 0.1135491356253624 Accuracy 0.684166669845581\n",
      "Iteration 37410 Training loss 0.08825591206550598 Validation loss 0.11352188140153885 Accuracy 0.684499979019165\n",
      "Iteration 37420 Training loss 0.0871564969420433 Validation loss 0.11363163590431213 Accuracy 0.6859999895095825\n",
      "Iteration 37430 Training loss 0.0845341831445694 Validation loss 0.12032702565193176 Accuracy 0.675000011920929\n",
      "Iteration 37440 Training loss 0.08788960427045822 Validation loss 0.1332881897687912 Accuracy 0.6473333239555359\n",
      "Iteration 37450 Training loss 0.08734395354986191 Validation loss 0.13230201601982117 Accuracy 0.6499999761581421\n",
      "Iteration 37460 Training loss 0.08801088482141495 Validation loss 0.13844868540763855 Accuracy 0.6386666893959045\n",
      "Iteration 37470 Training loss 0.0832924023270607 Validation loss 0.12270189076662064 Accuracy 0.6683333516120911\n",
      "Iteration 37480 Training loss 0.08553183823823929 Validation loss 0.12265961617231369 Accuracy 0.6696666479110718\n",
      "Iteration 37490 Training loss 0.08692489564418793 Validation loss 0.12823815643787384 Accuracy 0.6558333039283752\n",
      "Iteration 37500 Training loss 0.08328736573457718 Validation loss 0.12330267578363419 Accuracy 0.6673333048820496\n",
      "Iteration 37510 Training loss 0.08644580096006393 Validation loss 0.1151772066950798 Accuracy 0.6828333139419556\n",
      "Iteration 37520 Training loss 0.08946804702281952 Validation loss 0.11350959539413452 Accuracy 0.6861666440963745\n",
      "Iteration 37530 Training loss 0.08999673277139664 Validation loss 0.11347638070583344 Accuracy 0.6858333349227905\n",
      "Iteration 37540 Training loss 0.09182366728782654 Validation loss 0.11351413279771805 Accuracy 0.684166669845581\n",
      "Iteration 37550 Training loss 0.08678442239761353 Validation loss 0.11511388421058655 Accuracy 0.6846666932106018\n",
      "Iteration 37560 Training loss 0.0867781788110733 Validation loss 0.11560270190238953 Accuracy 0.6826666593551636\n",
      "Iteration 37570 Training loss 0.09344197809696198 Validation loss 0.1136884018778801 Accuracy 0.6855000257492065\n",
      "Iteration 37580 Training loss 0.08917242288589478 Validation loss 0.11364530771970749 Accuracy 0.6866666674613953\n",
      "Iteration 37590 Training loss 0.08558693528175354 Validation loss 0.11513952910900116 Accuracy 0.6850000023841858\n",
      "Iteration 37600 Training loss 0.09205686300992966 Validation loss 0.11370041221380234 Accuracy 0.6868333220481873\n",
      "Iteration 37610 Training loss 0.08934240788221359 Validation loss 0.11463622003793716 Accuracy 0.6855000257492065\n",
      "Iteration 37620 Training loss 0.08775262534618378 Validation loss 0.11410068720579147 Accuracy 0.6853333115577698\n",
      "Iteration 37630 Training loss 0.08397191762924194 Validation loss 0.1177045926451683 Accuracy 0.6791666746139526\n",
      "Iteration 37640 Training loss 0.08571752905845642 Validation loss 0.11840672045946121 Accuracy 0.6786666512489319\n",
      "Iteration 37650 Training loss 0.0847248062491417 Validation loss 0.11554905027151108 Accuracy 0.6823333501815796\n",
      "Iteration 37660 Training loss 0.08827093243598938 Validation loss 0.11392337828874588 Accuracy 0.6846666932106018\n",
      "Iteration 37670 Training loss 0.08727994561195374 Validation loss 0.11457950621843338 Accuracy 0.6868333220481873\n",
      "Iteration 37680 Training loss 0.08970120549201965 Validation loss 0.11399760842323303 Accuracy 0.6846666932106018\n",
      "Iteration 37690 Training loss 0.08805830776691437 Validation loss 0.1142493486404419 Accuracy 0.6868333220481873\n",
      "Iteration 37700 Training loss 0.0935530811548233 Validation loss 0.11405474692583084 Accuracy 0.6869999766349792\n",
      "Iteration 37710 Training loss 0.08804520964622498 Validation loss 0.11384286731481552 Accuracy 0.6855000257492065\n",
      "Iteration 37720 Training loss 0.08449337631464005 Validation loss 0.11740068346261978 Accuracy 0.6801666617393494\n",
      "Iteration 37730 Training loss 0.08485349267721176 Validation loss 0.11845297366380692 Accuracy 0.6783333420753479\n",
      "Iteration 37740 Training loss 0.08818349987268448 Validation loss 0.11401117593050003 Accuracy 0.6851666569709778\n",
      "Iteration 37750 Training loss 0.090811587870121 Validation loss 0.11357250064611435 Accuracy 0.6853333115577698\n",
      "Iteration 37760 Training loss 0.08754562586545944 Validation loss 0.11393921822309494 Accuracy 0.6859999895095825\n",
      "Iteration 37770 Training loss 0.08657023310661316 Validation loss 0.11417598277330399 Accuracy 0.6868333220481873\n",
      "Iteration 37780 Training loss 0.0859549269080162 Validation loss 0.11526279896497726 Accuracy 0.6823333501815796\n",
      "Iteration 37790 Training loss 0.08383652567863464 Validation loss 0.11852433532476425 Accuracy 0.6775000095367432\n",
      "Iteration 37800 Training loss 0.08830965310335159 Validation loss 0.12005048990249634 Accuracy 0.6726666688919067\n",
      "Iteration 37810 Training loss 0.09278209507465363 Validation loss 0.1144910678267479 Accuracy 0.6821666955947876\n",
      "Iteration 37820 Training loss 0.09146285057067871 Validation loss 0.1137307658791542 Accuracy 0.6853333115577698\n",
      "Iteration 37830 Training loss 0.0870356634259224 Validation loss 0.11423448473215103 Accuracy 0.6850000023841858\n",
      "Iteration 37840 Training loss 0.0861058160662651 Validation loss 0.11445567011833191 Accuracy 0.6848333477973938\n",
      "Iteration 37850 Training loss 0.09158293902873993 Validation loss 0.11348917335271835 Accuracy 0.6851666569709778\n",
      "Iteration 37860 Training loss 0.09136795997619629 Validation loss 0.11342021822929382 Accuracy 0.687166690826416\n",
      "Iteration 37870 Training loss 0.08705592900514603 Validation loss 0.11347825080156326 Accuracy 0.6858333349227905\n",
      "Iteration 37880 Training loss 0.08998347818851471 Validation loss 0.11351291090250015 Accuracy 0.6851666569709778\n",
      "Iteration 37890 Training loss 0.08663235604763031 Validation loss 0.11525370180606842 Accuracy 0.6846666932106018\n",
      "Iteration 37900 Training loss 0.08654335886240005 Validation loss 0.11515354365110397 Accuracy 0.6836666464805603\n",
      "Iteration 37910 Training loss 0.08927891403436661 Validation loss 0.11490555852651596 Accuracy 0.6848333477973938\n",
      "Iteration 37920 Training loss 0.08806162327528 Validation loss 0.1137603223323822 Accuracy 0.6851666569709778\n",
      "Iteration 37930 Training loss 0.08946698158979416 Validation loss 0.11360670626163483 Accuracy 0.6850000023841858\n",
      "Iteration 37940 Training loss 0.08660058677196503 Validation loss 0.11373572051525116 Accuracy 0.6858333349227905\n",
      "Iteration 37950 Training loss 0.08616645634174347 Validation loss 0.1139938235282898 Accuracy 0.6851666569709778\n",
      "Iteration 37960 Training loss 0.08659036457538605 Validation loss 0.11482764035463333 Accuracy 0.6850000023841858\n",
      "Iteration 37970 Training loss 0.08715777844190598 Validation loss 0.11466111242771149 Accuracy 0.6853333115577698\n",
      "Iteration 37980 Training loss 0.09122084826231003 Validation loss 0.11367490887641907 Accuracy 0.6846666932106018\n",
      "Iteration 37990 Training loss 0.08785033226013184 Validation loss 0.11428331583738327 Accuracy 0.684499979019165\n",
      "Iteration 38000 Training loss 0.08621319383382797 Validation loss 0.11491917073726654 Accuracy 0.6858333349227905\n",
      "Iteration 38010 Training loss 0.08983280509710312 Validation loss 0.11403222382068634 Accuracy 0.6855000257492065\n",
      "Iteration 38020 Training loss 0.08633673936128616 Validation loss 0.11380282789468765 Accuracy 0.6866666674613953\n",
      "Iteration 38030 Training loss 0.09004819393157959 Validation loss 0.1142805889248848 Accuracy 0.6853333115577698\n",
      "Iteration 38040 Training loss 0.08680889755487442 Validation loss 0.11454146355390549 Accuracy 0.6855000257492065\n",
      "Iteration 38050 Training loss 0.0865538939833641 Validation loss 0.11782039701938629 Accuracy 0.6794999837875366\n",
      "Iteration 38060 Training loss 0.08397813886404037 Validation loss 0.12138619273900986 Accuracy 0.67166668176651\n",
      "Iteration 38070 Training loss 0.08339688181877136 Validation loss 0.12149014323949814 Accuracy 0.671833336353302\n",
      "Iteration 38080 Training loss 0.08647137135267258 Validation loss 0.12118788808584213 Accuracy 0.6710000038146973\n",
      "Iteration 38090 Training loss 0.08611490577459335 Validation loss 0.1302843987941742 Accuracy 0.6526666879653931\n",
      "Iteration 38100 Training loss 0.08966585248708725 Validation loss 0.13756829500198364 Accuracy 0.6395000219345093\n",
      "Iteration 38110 Training loss 0.08987906575202942 Validation loss 0.13443350791931152 Accuracy 0.6456666588783264\n",
      "Iteration 38120 Training loss 0.08883876353502274 Validation loss 0.13463132083415985 Accuracy 0.6456666588783264\n",
      "Iteration 38130 Training loss 0.0856553167104721 Validation loss 0.12238632142543793 Accuracy 0.6691666841506958\n",
      "Iteration 38140 Training loss 0.08974935859441757 Validation loss 0.12909798324108124 Accuracy 0.6553333401679993\n",
      "Iteration 38150 Training loss 0.09202834218740463 Validation loss 0.13810177147388458 Accuracy 0.637499988079071\n",
      "Iteration 38160 Training loss 0.08763599395751953 Validation loss 0.1319637894630432 Accuracy 0.6486666798591614\n",
      "Iteration 38170 Training loss 0.08482032269239426 Validation loss 0.11999322474002838 Accuracy 0.675000011920929\n",
      "Iteration 38180 Training loss 0.08616175502538681 Validation loss 0.11489426344633102 Accuracy 0.6840000152587891\n",
      "Iteration 38190 Training loss 0.08754609525203705 Validation loss 0.11371935158967972 Accuracy 0.6846666932106018\n",
      "Iteration 38200 Training loss 0.08651489019393921 Validation loss 0.11446879059076309 Accuracy 0.6861666440963745\n",
      "Iteration 38210 Training loss 0.08812536299228668 Validation loss 0.11373051255941391 Accuracy 0.6853333115577698\n",
      "Iteration 38220 Training loss 0.08533713966608047 Validation loss 0.11436473578214645 Accuracy 0.6856666803359985\n",
      "Iteration 38230 Training loss 0.08279117941856384 Validation loss 0.11830223351716995 Accuracy 0.6786666512489319\n",
      "Iteration 38240 Training loss 0.08808787167072296 Validation loss 0.11511515080928802 Accuracy 0.6833333373069763\n",
      "Iteration 38250 Training loss 0.08872619271278381 Validation loss 0.11350169777870178 Accuracy 0.6858333349227905\n",
      "Iteration 38260 Training loss 0.08898007124662399 Validation loss 0.11361116170883179 Accuracy 0.6866666674613953\n",
      "Iteration 38270 Training loss 0.08750706166028976 Validation loss 0.11551143229007721 Accuracy 0.6836666464805603\n",
      "Iteration 38280 Training loss 0.08880478888750076 Validation loss 0.11364857107400894 Accuracy 0.6859999895095825\n",
      "Iteration 38290 Training loss 0.08889082819223404 Validation loss 0.11396079510450363 Accuracy 0.6851666569709778\n",
      "Iteration 38300 Training loss 0.08741281181573868 Validation loss 0.11554829031229019 Accuracy 0.6831666827201843\n",
      "Iteration 38310 Training loss 0.08280438184738159 Validation loss 0.12228991836309433 Accuracy 0.6696666479110718\n",
      "Iteration 38320 Training loss 0.08490810543298721 Validation loss 0.12140180170536041 Accuracy 0.672166645526886\n",
      "Iteration 38330 Training loss 0.08503912389278412 Validation loss 0.118360735476017 Accuracy 0.6779999732971191\n",
      "Iteration 38340 Training loss 0.08802508562803268 Validation loss 0.11393611878156662 Accuracy 0.6856666803359985\n",
      "Iteration 38350 Training loss 0.09024947881698608 Validation loss 0.1137094646692276 Accuracy 0.6869999766349792\n",
      "Iteration 38360 Training loss 0.09115790575742722 Validation loss 0.11342089623212814 Accuracy 0.6861666440963745\n",
      "Iteration 38370 Training loss 0.08840714395046234 Validation loss 0.11505786329507828 Accuracy 0.6850000023841858\n",
      "Iteration 38380 Training loss 0.08683017641305923 Validation loss 0.1167023554444313 Accuracy 0.6809999942779541\n",
      "Iteration 38390 Training loss 0.09180910885334015 Validation loss 0.11365191638469696 Accuracy 0.6861666440963745\n",
      "Iteration 38400 Training loss 0.08682751655578613 Validation loss 0.11377966403961182 Accuracy 0.6848333477973938\n",
      "Iteration 38410 Training loss 0.09259401261806488 Validation loss 0.11362610012292862 Accuracy 0.6861666440963745\n",
      "Iteration 38420 Training loss 0.08677665144205093 Validation loss 0.1140613779425621 Accuracy 0.6853333115577698\n",
      "Iteration 38430 Training loss 0.08796221762895584 Validation loss 0.11369482427835464 Accuracy 0.6826666593551636\n",
      "Iteration 38440 Training loss 0.08424453437328339 Validation loss 0.11755373328924179 Accuracy 0.6791666746139526\n",
      "Iteration 38450 Training loss 0.08701849728822708 Validation loss 0.11419036984443665 Accuracy 0.6846666932106018\n",
      "Iteration 38460 Training loss 0.09074586629867554 Validation loss 0.11372912675142288 Accuracy 0.6858333349227905\n",
      "Iteration 38470 Training loss 0.08924026787281036 Validation loss 0.1138252392411232 Accuracy 0.684333324432373\n",
      "Iteration 38480 Training loss 0.09055516868829727 Validation loss 0.11357623338699341 Accuracy 0.6853333115577698\n",
      "Iteration 38490 Training loss 0.08483398705720901 Validation loss 0.11434686928987503 Accuracy 0.6861666440963745\n",
      "Iteration 38500 Training loss 0.08655060082674026 Validation loss 0.11399765312671661 Accuracy 0.6838333606719971\n",
      "Iteration 38510 Training loss 0.08789088577032089 Validation loss 0.11494887620210648 Accuracy 0.6863333582878113\n",
      "Iteration 38520 Training loss 0.0823432207107544 Validation loss 0.11624135076999664 Accuracy 0.6801666617393494\n",
      "Iteration 38530 Training loss 0.0863870307803154 Validation loss 0.11570272594690323 Accuracy 0.6816666722297668\n",
      "Iteration 38540 Training loss 0.0918053686618805 Validation loss 0.11364132910966873 Accuracy 0.6851666569709778\n",
      "Iteration 38550 Training loss 0.08819187432527542 Validation loss 0.1136743575334549 Accuracy 0.6863333582878113\n",
      "Iteration 38560 Training loss 0.08935749530792236 Validation loss 0.11394043266773224 Accuracy 0.6855000257492065\n",
      "Iteration 38570 Training loss 0.08563976734876633 Validation loss 0.11589265614748001 Accuracy 0.6831666827201843\n",
      "Iteration 38580 Training loss 0.08597289025783539 Validation loss 0.11450295895338058 Accuracy 0.6850000023841858\n",
      "Iteration 38590 Training loss 0.08473547548055649 Validation loss 0.11943118274211884 Accuracy 0.6768333315849304\n",
      "Iteration 38600 Training loss 0.0880604013800621 Validation loss 0.12915028631687164 Accuracy 0.6566666960716248\n",
      "Iteration 38610 Training loss 0.09089420735836029 Validation loss 0.13484376668930054 Accuracy 0.6449999809265137\n",
      "Iteration 38620 Training loss 0.086823008954525 Validation loss 0.13573746383190155 Accuracy 0.6458333134651184\n",
      "Iteration 38630 Training loss 0.09007547795772552 Validation loss 0.13252432644367218 Accuracy 0.6486666798591614\n",
      "Iteration 38640 Training loss 0.0928550586104393 Validation loss 0.14027941226959229 Accuracy 0.6334999799728394\n",
      "Iteration 38650 Training loss 0.08788536489009857 Validation loss 0.13333523273468018 Accuracy 0.6489999890327454\n",
      "Iteration 38660 Training loss 0.08803495019674301 Validation loss 0.13455182313919067 Accuracy 0.6474999785423279\n",
      "Iteration 38670 Training loss 0.0922740250825882 Validation loss 0.1363155096769333 Accuracy 0.643833339214325\n",
      "Iteration 38680 Training loss 0.08474176377058029 Validation loss 0.1230926588177681 Accuracy 0.6676666736602783\n",
      "Iteration 38690 Training loss 0.08659238368272781 Validation loss 0.1276874989271164 Accuracy 0.6588333249092102\n",
      "Iteration 38700 Training loss 0.09448859840631485 Validation loss 0.14447012543678284 Accuracy 0.6266666650772095\n",
      "Iteration 38710 Training loss 0.08641606569290161 Validation loss 0.13026687502861023 Accuracy 0.6535000205039978\n",
      "Iteration 38720 Training loss 0.08481309562921524 Validation loss 0.11880253255367279 Accuracy 0.6765000224113464\n",
      "Iteration 38730 Training loss 0.08515298366546631 Validation loss 0.11578924208879471 Accuracy 0.6800000071525574\n",
      "Iteration 38740 Training loss 0.08338520675897598 Validation loss 0.11715107411146164 Accuracy 0.6804999709129333\n",
      "Iteration 38750 Training loss 0.08961834758520126 Validation loss 0.11376868933439255 Accuracy 0.6863333582878113\n",
      "Iteration 38760 Training loss 0.09236158430576324 Validation loss 0.11365599930286407 Accuracy 0.6851666569709778\n",
      "Iteration 38770 Training loss 0.08801508694887161 Validation loss 0.11410770565271378 Accuracy 0.684166669845581\n",
      "Iteration 38780 Training loss 0.08786743134260178 Validation loss 0.1135813519358635 Accuracy 0.6848333477973938\n",
      "Iteration 38790 Training loss 0.0854661762714386 Validation loss 0.11957386136054993 Accuracy 0.6756666898727417\n",
      "Iteration 38800 Training loss 0.08623002469539642 Validation loss 0.12665116786956787 Accuracy 0.659166693687439\n",
      "Iteration 38810 Training loss 0.09051794558763504 Validation loss 0.14120234549045563 Accuracy 0.6324999928474426\n",
      "Iteration 38820 Training loss 0.0883420780301094 Validation loss 0.13559038937091827 Accuracy 0.6455000042915344\n",
      "Iteration 38830 Training loss 0.08595845848321915 Validation loss 0.1264626681804657 Accuracy 0.6623333096504211\n",
      "Iteration 38840 Training loss 0.08615473657846451 Validation loss 0.12415695190429688 Accuracy 0.6678333282470703\n",
      "Iteration 38850 Training loss 0.08365237712860107 Validation loss 0.11867088079452515 Accuracy 0.6759999990463257\n",
      "Iteration 38860 Training loss 0.0854252427816391 Validation loss 0.11435090005397797 Accuracy 0.6853333115577698\n",
      "Iteration 38870 Training loss 0.0901055634021759 Validation loss 0.11361991614103317 Accuracy 0.6859999895095825\n",
      "Iteration 38880 Training loss 0.09091533720493317 Validation loss 0.11359533667564392 Accuracy 0.684333324432373\n",
      "Iteration 38890 Training loss 0.0914716050028801 Validation loss 0.11379412561655045 Accuracy 0.6866666674613953\n",
      "Iteration 38900 Training loss 0.09215496480464935 Validation loss 0.11378180235624313 Accuracy 0.6853333115577698\n",
      "Iteration 38910 Training loss 0.08711974322795868 Validation loss 0.11444831639528275 Accuracy 0.6846666932106018\n",
      "Iteration 38920 Training loss 0.08771313726902008 Validation loss 0.11394225805997849 Accuracy 0.6856666803359985\n",
      "Iteration 38930 Training loss 0.0850234106183052 Validation loss 0.11624734848737717 Accuracy 0.6825000047683716\n",
      "Iteration 38940 Training loss 0.08856003731489182 Validation loss 0.11379807442426682 Accuracy 0.6869999766349792\n",
      "Iteration 38950 Training loss 0.08734598755836487 Validation loss 0.11562056839466095 Accuracy 0.6834999918937683\n",
      "Iteration 38960 Training loss 0.08643245697021484 Validation loss 0.11547400057315826 Accuracy 0.6831666827201843\n",
      "Iteration 38970 Training loss 0.08820503950119019 Validation loss 0.11406858265399933 Accuracy 0.6865000128746033\n",
      "Iteration 38980 Training loss 0.0913102999329567 Validation loss 0.11360178887844086 Accuracy 0.6858333349227905\n",
      "Iteration 38990 Training loss 0.0849871039390564 Validation loss 0.11558573693037033 Accuracy 0.6833333373069763\n",
      "Iteration 39000 Training loss 0.08821771293878555 Validation loss 0.11364372819662094 Accuracy 0.6858333349227905\n",
      "Iteration 39010 Training loss 0.08555800467729568 Validation loss 0.11535046249628067 Accuracy 0.6856666803359985\n",
      "Iteration 39020 Training loss 0.08666736632585526 Validation loss 0.11498092859983444 Accuracy 0.6853333115577698\n",
      "Iteration 39030 Training loss 0.08882160484790802 Validation loss 0.11390833556652069 Accuracy 0.6865000128746033\n",
      "Iteration 39040 Training loss 0.08897455036640167 Validation loss 0.11376669257879257 Accuracy 0.6856666803359985\n",
      "Iteration 39050 Training loss 0.08652830123901367 Validation loss 0.11506132781505585 Accuracy 0.6856666803359985\n",
      "Iteration 39060 Training loss 0.0882636085152626 Validation loss 0.11383426189422607 Accuracy 0.6865000128746033\n",
      "Iteration 39070 Training loss 0.08401675522327423 Validation loss 0.114703468978405 Accuracy 0.6851666569709778\n",
      "Iteration 39080 Training loss 0.08569356054067612 Validation loss 0.11387058347463608 Accuracy 0.6855000257492065\n",
      "Iteration 39090 Training loss 0.09003350883722305 Validation loss 0.11409928649663925 Accuracy 0.6859999895095825\n",
      "Iteration 39100 Training loss 0.09005027264356613 Validation loss 0.11371901631355286 Accuracy 0.684333324432373\n",
      "Iteration 39110 Training loss 0.09080454707145691 Validation loss 0.11375455558300018 Accuracy 0.6859999895095825\n",
      "Iteration 39120 Training loss 0.0867319256067276 Validation loss 0.11596396565437317 Accuracy 0.6831666827201843\n",
      "Iteration 39130 Training loss 0.08321519941091537 Validation loss 0.11911097168922424 Accuracy 0.6778333187103271\n",
      "Iteration 39140 Training loss 0.09005225449800491 Validation loss 0.11375448852777481 Accuracy 0.6846666932106018\n",
      "Iteration 39150 Training loss 0.08773262798786163 Validation loss 0.11359193921089172 Accuracy 0.6850000023841858\n",
      "Iteration 39160 Training loss 0.08954907208681107 Validation loss 0.11377673596143723 Accuracy 0.6861666440963745\n",
      "Iteration 39170 Training loss 0.08722958713769913 Validation loss 0.11424189060926437 Accuracy 0.684333324432373\n",
      "Iteration 39180 Training loss 0.08760403096675873 Validation loss 0.11478973180055618 Accuracy 0.6848333477973938\n",
      "Iteration 39190 Training loss 0.08743780851364136 Validation loss 0.11476350575685501 Accuracy 0.6831666827201843\n",
      "Iteration 39200 Training loss 0.08713401108980179 Validation loss 0.11512929946184158 Accuracy 0.6855000257492065\n",
      "Iteration 39210 Training loss 0.08761043101549149 Validation loss 0.11417422443628311 Accuracy 0.6846666932106018\n",
      "Iteration 39220 Training loss 0.08615148812532425 Validation loss 0.1162068098783493 Accuracy 0.6828333139419556\n",
      "Iteration 39230 Training loss 0.08575718849897385 Validation loss 0.11970458924770355 Accuracy 0.6763333082199097\n",
      "Iteration 39240 Training loss 0.0882798284292221 Validation loss 0.133707195520401 Accuracy 0.6478333473205566\n",
      "Iteration 39250 Training loss 0.08566846698522568 Validation loss 0.13091494143009186 Accuracy 0.6536666750907898\n",
      "Iteration 39260 Training loss 0.08898884803056717 Validation loss 0.13206923007965088 Accuracy 0.6499999761581421\n",
      "Iteration 39270 Training loss 0.0899217501282692 Validation loss 0.13685530424118042 Accuracy 0.6418333053588867\n",
      "Iteration 39280 Training loss 0.09082334488630295 Validation loss 0.13384920358657837 Accuracy 0.6470000147819519\n",
      "Iteration 39290 Training loss 0.0876009538769722 Validation loss 0.1355571746826172 Accuracy 0.6451666951179504\n",
      "Iteration 39300 Training loss 0.08989442884922028 Validation loss 0.13398011028766632 Accuracy 0.6461666822433472\n",
      "Iteration 39310 Training loss 0.08525649458169937 Validation loss 0.12629789113998413 Accuracy 0.659333348274231\n",
      "Iteration 39320 Training loss 0.08481597155332565 Validation loss 0.1282450556755066 Accuracy 0.6566666960716248\n",
      "Iteration 39330 Training loss 0.08679036796092987 Validation loss 0.131893128156662 Accuracy 0.6503333449363708\n",
      "Iteration 39340 Training loss 0.08821963518857956 Validation loss 0.13859127461910248 Accuracy 0.6393333077430725\n",
      "Iteration 39350 Training loss 0.08807262033224106 Validation loss 0.13225072622299194 Accuracy 0.6501666903495789\n",
      "Iteration 39360 Training loss 0.09091536700725555 Validation loss 0.1367649883031845 Accuracy 0.6431666612625122\n",
      "Iteration 39370 Training loss 0.0855349600315094 Validation loss 0.12506714463233948 Accuracy 0.6628333330154419\n",
      "Iteration 39380 Training loss 0.08575598150491714 Validation loss 0.1309008151292801 Accuracy 0.6524999737739563\n",
      "Iteration 39390 Training loss 0.08622753620147705 Validation loss 0.1248505562543869 Accuracy 0.6651666760444641\n",
      "Iteration 39400 Training loss 0.08593958616256714 Validation loss 0.13267861306667328 Accuracy 0.6474999785423279\n",
      "Iteration 39410 Training loss 0.08976204693317413 Validation loss 0.13615334033966064 Accuracy 0.643833339214325\n",
      "Iteration 39420 Training loss 0.09007762372493744 Validation loss 0.1396387666463852 Accuracy 0.6349999904632568\n",
      "Iteration 39430 Training loss 0.08657151460647583 Validation loss 0.1253471076488495 Accuracy 0.6639999747276306\n",
      "Iteration 39440 Training loss 0.08592256903648376 Validation loss 0.1275329738855362 Accuracy 0.6581666469573975\n",
      "Iteration 39450 Training loss 0.08683629333972931 Validation loss 0.13080008327960968 Accuracy 0.6523333191871643\n",
      "Iteration 39460 Training loss 0.08484618365764618 Validation loss 0.1246839389204979 Accuracy 0.6646666526794434\n",
      "Iteration 39470 Training loss 0.08528270572423935 Validation loss 0.12347403168678284 Accuracy 0.6656666398048401\n",
      "Iteration 39480 Training loss 0.09001664072275162 Validation loss 0.13583996891975403 Accuracy 0.6423333287239075\n",
      "Iteration 39490 Training loss 0.09066782891750336 Validation loss 0.13712184131145477 Accuracy 0.640999972820282\n",
      "Iteration 39500 Training loss 0.0895005464553833 Validation loss 0.1370592713356018 Accuracy 0.6420000195503235\n",
      "Iteration 39510 Training loss 0.08746034651994705 Validation loss 0.11771443486213684 Accuracy 0.6800000071525574\n",
      "Iteration 39520 Training loss 0.0868028998374939 Validation loss 0.11732282489538193 Accuracy 0.6801666617393494\n",
      "Iteration 39530 Training loss 0.08862746506929398 Validation loss 0.11553177237510681 Accuracy 0.6809999942779541\n",
      "Iteration 39540 Training loss 0.08908429741859436 Validation loss 0.1136406883597374 Accuracy 0.6863333582878113\n",
      "Iteration 39550 Training loss 0.08693061023950577 Validation loss 0.11345221102237701 Accuracy 0.6868333220481873\n",
      "Iteration 39560 Training loss 0.0883171558380127 Validation loss 0.11530575901269913 Accuracy 0.6823333501815796\n",
      "Iteration 39570 Training loss 0.08613318204879761 Validation loss 0.11429386585950851 Accuracy 0.6855000257492065\n",
      "Iteration 39580 Training loss 0.09240347146987915 Validation loss 0.11374019086360931 Accuracy 0.6853333115577698\n",
      "Iteration 39590 Training loss 0.08706431090831757 Validation loss 0.11385221034288406 Accuracy 0.6851666569709778\n",
      "Iteration 39600 Training loss 0.0847061425447464 Validation loss 0.11555442959070206 Accuracy 0.6836666464805603\n",
      "Iteration 39610 Training loss 0.08805989474058151 Validation loss 0.11364779621362686 Accuracy 0.6861666440963745\n",
      "Iteration 39620 Training loss 0.08828212320804596 Validation loss 0.11359438300132751 Accuracy 0.6859999895095825\n",
      "Iteration 39630 Training loss 0.08918467164039612 Validation loss 0.11372178047895432 Accuracy 0.6869999766349792\n",
      "Iteration 39640 Training loss 0.08968841284513474 Validation loss 0.11386517435312271 Accuracy 0.6856666803359985\n",
      "Iteration 39650 Training loss 0.08562272042036057 Validation loss 0.1178823858499527 Accuracy 0.6779999732971191\n",
      "Iteration 39660 Training loss 0.08364716917276382 Validation loss 0.11681599169969559 Accuracy 0.6811666488647461\n",
      "Iteration 39670 Training loss 0.08412487059831619 Validation loss 0.11498197168111801 Accuracy 0.6840000152587891\n",
      "Iteration 39680 Training loss 0.09281253814697266 Validation loss 0.11396315693855286 Accuracy 0.6859999895095825\n",
      "Iteration 39690 Training loss 0.08779072016477585 Validation loss 0.11344728618860245 Accuracy 0.6859999895095825\n",
      "Iteration 39700 Training loss 0.08677417039871216 Validation loss 0.11660151928663254 Accuracy 0.6825000047683716\n",
      "Iteration 39710 Training loss 0.08568501472473145 Validation loss 0.11687672883272171 Accuracy 0.6818333268165588\n",
      "Iteration 39720 Training loss 0.09091556817293167 Validation loss 0.11400812119245529 Accuracy 0.6863333582878113\n",
      "Iteration 39730 Training loss 0.08998725563287735 Validation loss 0.11361926048994064 Accuracy 0.6861666440963745\n",
      "Iteration 39740 Training loss 0.08907473087310791 Validation loss 0.11496192216873169 Accuracy 0.6846666932106018\n",
      "Iteration 39750 Training loss 0.0863838940858841 Validation loss 0.11393257975578308 Accuracy 0.6865000128746033\n",
      "Iteration 39760 Training loss 0.08895901590585709 Validation loss 0.11355141550302505 Accuracy 0.6851666569709778\n",
      "Iteration 39770 Training loss 0.08652666211128235 Validation loss 0.11394178122282028 Accuracy 0.6865000128746033\n",
      "Iteration 39780 Training loss 0.08743725717067719 Validation loss 0.11429253965616226 Accuracy 0.6851666569709778\n",
      "Iteration 39790 Training loss 0.08632039278745651 Validation loss 0.11403021216392517 Accuracy 0.6863333582878113\n",
      "Iteration 39800 Training loss 0.08471187204122543 Validation loss 0.11503463983535767 Accuracy 0.684333324432373\n",
      "Iteration 39810 Training loss 0.0907997265458107 Validation loss 0.11389157176017761 Accuracy 0.6859999895095825\n",
      "Iteration 39820 Training loss 0.08698321878910065 Validation loss 0.1144431084394455 Accuracy 0.6846666932106018\n",
      "Iteration 39830 Training loss 0.09463303536176682 Validation loss 0.11424999684095383 Accuracy 0.6848333477973938\n",
      "Iteration 39840 Training loss 0.08770648390054703 Validation loss 0.11426341533660889 Accuracy 0.684333324432373\n",
      "Iteration 39850 Training loss 0.08729377388954163 Validation loss 0.11545587331056595 Accuracy 0.6836666464805603\n",
      "Iteration 39860 Training loss 0.08948010206222534 Validation loss 0.11413637548685074 Accuracy 0.6848333477973938\n",
      "Iteration 39870 Training loss 0.08868276327848434 Validation loss 0.11371982097625732 Accuracy 0.6866666674613953\n",
      "Iteration 39880 Training loss 0.08548049628734589 Validation loss 0.11533598601818085 Accuracy 0.6850000023841858\n",
      "Iteration 39890 Training loss 0.08636819571256638 Validation loss 0.11519264429807663 Accuracy 0.6831666827201843\n",
      "Iteration 39900 Training loss 0.08785776793956757 Validation loss 0.11569114029407501 Accuracy 0.6830000281333923\n",
      "Iteration 39910 Training loss 0.08884713053703308 Validation loss 0.11409458518028259 Accuracy 0.6859999895095825\n",
      "Iteration 39920 Training loss 0.08472859859466553 Validation loss 0.11445310711860657 Accuracy 0.684499979019165\n",
      "Iteration 39930 Training loss 0.08583986759185791 Validation loss 0.11452458053827286 Accuracy 0.684333324432373\n",
      "Iteration 39940 Training loss 0.08531472086906433 Validation loss 0.11549519747495651 Accuracy 0.6833333373069763\n",
      "Iteration 39950 Training loss 0.08560770004987717 Validation loss 0.11608823388814926 Accuracy 0.6815000176429749\n",
      "Iteration 39960 Training loss 0.08490119129419327 Validation loss 0.11949995160102844 Accuracy 0.6766666769981384\n",
      "Iteration 39970 Training loss 0.08652849495410919 Validation loss 0.11389143764972687 Accuracy 0.687333345413208\n",
      "Iteration 39980 Training loss 0.08797463774681091 Validation loss 0.11386609077453613 Accuracy 0.6863333582878113\n",
      "Iteration 39990 Training loss 0.08796636760234833 Validation loss 0.11377179622650146 Accuracy 0.6865000128746033\n",
      "Iteration 40000 Training loss 0.08814667910337448 Validation loss 0.11567318439483643 Accuracy 0.6836666464805603\n",
      "Iteration 40010 Training loss 0.08471328765153885 Validation loss 0.11583064496517181 Accuracy 0.684499979019165\n",
      "Iteration 40020 Training loss 0.08926533907651901 Validation loss 0.1149088516831398 Accuracy 0.6865000128746033\n",
      "Iteration 40030 Training loss 0.08855950832366943 Validation loss 0.11382605135440826 Accuracy 0.6866666674613953\n",
      "Iteration 40040 Training loss 0.08676131069660187 Validation loss 0.11556383967399597 Accuracy 0.6833333373069763\n",
      "Iteration 40050 Training loss 0.08903922885656357 Validation loss 0.11378294229507446 Accuracy 0.6853333115577698\n",
      "Iteration 40060 Training loss 0.08787766844034195 Validation loss 0.1136525347828865 Accuracy 0.6865000128746033\n",
      "Iteration 40070 Training loss 0.08572398126125336 Validation loss 0.11476780474185944 Accuracy 0.6853333115577698\n",
      "Iteration 40080 Training loss 0.0896536260843277 Validation loss 0.11375262588262558 Accuracy 0.6856666803359985\n",
      "Iteration 40090 Training loss 0.08976168930530548 Validation loss 0.1135735809803009 Accuracy 0.6863333582878113\n",
      "Iteration 40100 Training loss 0.08789870142936707 Validation loss 0.11416225135326385 Accuracy 0.6868333220481873\n",
      "Iteration 40110 Training loss 0.08643310517072678 Validation loss 0.11550457775592804 Accuracy 0.6830000281333923\n",
      "Iteration 40120 Training loss 0.08650781959295273 Validation loss 0.11431732028722763 Accuracy 0.6851666569709778\n",
      "Iteration 40130 Training loss 0.08661824464797974 Validation loss 0.1142759919166565 Accuracy 0.684166669845581\n",
      "Iteration 40140 Training loss 0.08483519405126572 Validation loss 0.11634077876806259 Accuracy 0.6806666851043701\n",
      "Iteration 40150 Training loss 0.08926123380661011 Validation loss 0.11480432003736496 Accuracy 0.684166669845581\n",
      "Iteration 40160 Training loss 0.08552670478820801 Validation loss 0.11419085413217545 Accuracy 0.6868333220481873\n",
      "Iteration 40170 Training loss 0.09019586443901062 Validation loss 0.11380136013031006 Accuracy 0.6848333477973938\n",
      "Iteration 40180 Training loss 0.0867137685418129 Validation loss 0.11417528986930847 Accuracy 0.6853333115577698\n",
      "Iteration 40190 Training loss 0.08658909797668457 Validation loss 0.11641191691160202 Accuracy 0.6815000176429749\n",
      "Iteration 40200 Training loss 0.08926761150360107 Validation loss 0.11400630325078964 Accuracy 0.6859999895095825\n",
      "Iteration 40210 Training loss 0.09099876880645752 Validation loss 0.11376641690731049 Accuracy 0.6863333582878113\n",
      "Iteration 40220 Training loss 0.08688990026712418 Validation loss 0.11455503106117249 Accuracy 0.6846666932106018\n",
      "Iteration 40230 Training loss 0.08613832294940948 Validation loss 0.11646106094121933 Accuracy 0.6811666488647461\n",
      "Iteration 40240 Training loss 0.08650171011686325 Validation loss 0.11793176084756851 Accuracy 0.6771666407585144\n",
      "Iteration 40250 Training loss 0.08689939230680466 Validation loss 0.11549173295497894 Accuracy 0.684166669845581\n",
      "Iteration 40260 Training loss 0.08532188087701797 Validation loss 0.12328241020441055 Accuracy 0.668666660785675\n",
      "Iteration 40270 Training loss 0.087441585958004 Validation loss 0.12260857969522476 Accuracy 0.6685000061988831\n",
      "Iteration 40280 Training loss 0.08543267101049423 Validation loss 0.1233966201543808 Accuracy 0.6668333411216736\n",
      "Iteration 40290 Training loss 0.08881202340126038 Validation loss 0.11419138312339783 Accuracy 0.687166690826416\n",
      "Iteration 40300 Training loss 0.09200284630060196 Validation loss 0.11435666680335999 Accuracy 0.6831666827201843\n",
      "Iteration 40310 Training loss 0.09073653072118759 Validation loss 0.11372701078653336 Accuracy 0.6861666440963745\n",
      "Iteration 40320 Training loss 0.08888820558786392 Validation loss 0.11387640237808228 Accuracy 0.6865000128746033\n",
      "Iteration 40330 Training loss 0.08586391061544418 Validation loss 0.11452288925647736 Accuracy 0.684499979019165\n",
      "Iteration 40340 Training loss 0.08939066529273987 Validation loss 0.11468668282032013 Accuracy 0.6868333220481873\n",
      "Iteration 40350 Training loss 0.08302199095487595 Validation loss 0.11703767627477646 Accuracy 0.6813333630561829\n",
      "Iteration 40360 Training loss 0.08455154299736023 Validation loss 0.11570751667022705 Accuracy 0.6821666955947876\n",
      "Iteration 40370 Training loss 0.08933361619710922 Validation loss 0.11379195004701614 Accuracy 0.6869999766349792\n",
      "Iteration 40380 Training loss 0.08973000198602676 Validation loss 0.1137089654803276 Accuracy 0.6851666569709778\n",
      "Iteration 40390 Training loss 0.08822891116142273 Validation loss 0.11406362056732178 Accuracy 0.6855000257492065\n",
      "Iteration 40400 Training loss 0.08834570646286011 Validation loss 0.11387819051742554 Accuracy 0.6855000257492065\n",
      "Iteration 40410 Training loss 0.08597486466169357 Validation loss 0.11532899737358093 Accuracy 0.6838333606719971\n",
      "Iteration 40420 Training loss 0.0860869437456131 Validation loss 0.11743604391813278 Accuracy 0.6803333163261414\n",
      "Iteration 40430 Training loss 0.08692111074924469 Validation loss 0.11533927172422409 Accuracy 0.6840000152587891\n",
      "Iteration 40440 Training loss 0.08822653442621231 Validation loss 0.11448068171739578 Accuracy 0.6865000128746033\n",
      "Iteration 40450 Training loss 0.08954138308763504 Validation loss 0.11373339593410492 Accuracy 0.687333345413208\n",
      "Iteration 40460 Training loss 0.08690247684717178 Validation loss 0.1159665510058403 Accuracy 0.6828333139419556\n",
      "Iteration 40470 Training loss 0.08986120671033859 Validation loss 0.11389254033565521 Accuracy 0.687333345413208\n",
      "Iteration 40480 Training loss 0.08967069536447525 Validation loss 0.11407357454299927 Accuracy 0.6846666932106018\n",
      "Iteration 40490 Training loss 0.0877779945731163 Validation loss 0.11418420076370239 Accuracy 0.6863333582878113\n",
      "Iteration 40500 Training loss 0.0897953063249588 Validation loss 0.11469071358442307 Accuracy 0.6850000023841858\n",
      "Iteration 40510 Training loss 0.09052563458681107 Validation loss 0.11416620016098022 Accuracy 0.6865000128746033\n",
      "Iteration 40520 Training loss 0.08761916309595108 Validation loss 0.11385774612426758 Accuracy 0.6853333115577698\n",
      "Iteration 40530 Training loss 0.08514175564050674 Validation loss 0.11840038746595383 Accuracy 0.6794999837875366\n",
      "Iteration 40540 Training loss 0.09017177671194077 Validation loss 0.13177594542503357 Accuracy 0.6518333554267883\n",
      "Iteration 40550 Training loss 0.08930833637714386 Validation loss 0.13596506416797638 Accuracy 0.6448333263397217\n",
      "Iteration 40560 Training loss 0.09096303582191467 Validation loss 0.1378292739391327 Accuracy 0.6423333287239075\n",
      "Iteration 40570 Training loss 0.08807521313428879 Validation loss 0.13411976397037506 Accuracy 0.6473333239555359\n",
      "Iteration 40580 Training loss 0.08503333479166031 Validation loss 0.12717506289482117 Accuracy 0.6616666913032532\n",
      "Iteration 40590 Training loss 0.08546607196331024 Validation loss 0.11788976937532425 Accuracy 0.6809999942779541\n",
      "Iteration 40600 Training loss 0.08795737475156784 Validation loss 0.11415757238864899 Accuracy 0.6853333115577698\n",
      "Iteration 40610 Training loss 0.08873023837804794 Validation loss 0.11403591185808182 Accuracy 0.6858333349227905\n",
      "Iteration 40620 Training loss 0.08963249623775482 Validation loss 0.11375530064105988 Accuracy 0.6855000257492065\n",
      "Iteration 40630 Training loss 0.08747399598360062 Validation loss 0.11528165638446808 Accuracy 0.6848333477973938\n",
      "Iteration 40640 Training loss 0.08617941290140152 Validation loss 0.11569729447364807 Accuracy 0.684166669845581\n",
      "Iteration 40650 Training loss 0.09009469300508499 Validation loss 0.11367230117321014 Accuracy 0.6869999766349792\n",
      "Iteration 40660 Training loss 0.0894283577799797 Validation loss 0.11410313099622726 Accuracy 0.6851666569709778\n",
      "Iteration 40670 Training loss 0.08547283709049225 Validation loss 0.11472200602293015 Accuracy 0.6823333501815796\n",
      "Iteration 40680 Training loss 0.09065930545330048 Validation loss 0.11380286514759064 Accuracy 0.6884999871253967\n",
      "Iteration 40690 Training loss 0.09075266867876053 Validation loss 0.11386356502771378 Accuracy 0.6863333582878113\n",
      "Iteration 40700 Training loss 0.0920025035738945 Validation loss 0.11385420709848404 Accuracy 0.6863333582878113\n",
      "Iteration 40710 Training loss 0.08705795556306839 Validation loss 0.11418603360652924 Accuracy 0.6846666932106018\n",
      "Iteration 40720 Training loss 0.09031722694635391 Validation loss 0.11415132880210876 Accuracy 0.6836666464805603\n",
      "Iteration 40730 Training loss 0.087801493704319 Validation loss 0.11404091119766235 Accuracy 0.6856666803359985\n",
      "Iteration 40740 Training loss 0.0878816619515419 Validation loss 0.11606711149215698 Accuracy 0.6831666827201843\n",
      "Iteration 40750 Training loss 0.08438757807016373 Validation loss 0.11504369229078293 Accuracy 0.6863333582878113\n",
      "Iteration 40760 Training loss 0.08727556467056274 Validation loss 0.11487828940153122 Accuracy 0.6865000128746033\n",
      "Iteration 40770 Training loss 0.08733996748924255 Validation loss 0.11409714818000793 Accuracy 0.6855000257492065\n",
      "Iteration 40780 Training loss 0.08995576202869415 Validation loss 0.11388235539197922 Accuracy 0.6850000023841858\n",
      "Iteration 40790 Training loss 0.08612837642431259 Validation loss 0.11381860077381134 Accuracy 0.6851666569709778\n",
      "Iteration 40800 Training loss 0.08688666671514511 Validation loss 0.11445166170597076 Accuracy 0.6856666803359985\n",
      "Iteration 40810 Training loss 0.08262449502944946 Validation loss 0.11943554878234863 Accuracy 0.6776666641235352\n",
      "Iteration 40820 Training loss 0.08545242995023727 Validation loss 0.12060262262821198 Accuracy 0.6740000247955322\n",
      "Iteration 40830 Training loss 0.08426570147275925 Validation loss 0.12241733074188232 Accuracy 0.6694999933242798\n",
      "Iteration 40840 Training loss 0.08760596066713333 Validation loss 0.11408606171607971 Accuracy 0.6861666440963745\n",
      "Iteration 40850 Training loss 0.09407316148281097 Validation loss 0.11408628523349762 Accuracy 0.6865000128746033\n",
      "Iteration 40860 Training loss 0.08815281093120575 Validation loss 0.11410599946975708 Accuracy 0.6856666803359985\n",
      "Iteration 40870 Training loss 0.08919636160135269 Validation loss 0.11407094448804855 Accuracy 0.6869999766349792\n",
      "Iteration 40880 Training loss 0.09011224657297134 Validation loss 0.11413328349590302 Accuracy 0.6846666932106018\n",
      "Iteration 40890 Training loss 0.0845753625035286 Validation loss 0.11530504375696182 Accuracy 0.6853333115577698\n",
      "Iteration 40900 Training loss 0.08815678209066391 Validation loss 0.11644437909126282 Accuracy 0.6833333373069763\n",
      "Iteration 40910 Training loss 0.0893082246184349 Validation loss 0.11366793513298035 Accuracy 0.6869999766349792\n",
      "Iteration 40920 Training loss 0.08823874592781067 Validation loss 0.11372153460979462 Accuracy 0.6869999766349792\n",
      "Iteration 40930 Training loss 0.08655431121587753 Validation loss 0.11531423032283783 Accuracy 0.684333324432373\n",
      "Iteration 40940 Training loss 0.08854099363088608 Validation loss 0.11397109180688858 Accuracy 0.6866666674613953\n",
      "Iteration 40950 Training loss 0.08492254465818405 Validation loss 0.11483825743198395 Accuracy 0.6861666440963745\n",
      "Iteration 40960 Training loss 0.08832310140132904 Validation loss 0.11409124732017517 Accuracy 0.684166669845581\n",
      "Iteration 40970 Training loss 0.0856265127658844 Validation loss 0.11510486155748367 Accuracy 0.684333324432373\n",
      "Iteration 40980 Training loss 0.08678761124610901 Validation loss 0.115107461810112 Accuracy 0.6838333606719971\n",
      "Iteration 40990 Training loss 0.0871623158454895 Validation loss 0.11433620005846024 Accuracy 0.6840000152587891\n",
      "Iteration 41000 Training loss 0.08638264983892441 Validation loss 0.11574715375900269 Accuracy 0.6831666827201843\n",
      "Iteration 41010 Training loss 0.09039946645498276 Validation loss 0.11385641247034073 Accuracy 0.6868333220481873\n",
      "Iteration 41020 Training loss 0.0898452177643776 Validation loss 0.11404724419116974 Accuracy 0.6859999895095825\n",
      "Iteration 41030 Training loss 0.08995810896158218 Validation loss 0.11414259672164917 Accuracy 0.6850000023841858\n",
      "Iteration 41040 Training loss 0.08935452997684479 Validation loss 0.11369232088327408 Accuracy 0.6875\n",
      "Iteration 41050 Training loss 0.08484472334384918 Validation loss 0.11801489442586899 Accuracy 0.6798333525657654\n",
      "Iteration 41060 Training loss 0.08631546795368195 Validation loss 0.11568932980298996 Accuracy 0.6818333268165588\n",
      "Iteration 41070 Training loss 0.08804759383201599 Validation loss 0.11414070427417755 Accuracy 0.6866666674613953\n",
      "Iteration 41080 Training loss 0.0883137583732605 Validation loss 0.11519598215818405 Accuracy 0.684499979019165\n",
      "Iteration 41090 Training loss 0.0872843861579895 Validation loss 0.1137995794415474 Accuracy 0.6865000128746033\n",
      "Iteration 41100 Training loss 0.08562831580638885 Validation loss 0.11458916962146759 Accuracy 0.6865000128746033\n",
      "Iteration 41110 Training loss 0.08789660781621933 Validation loss 0.11470015347003937 Accuracy 0.6859999895095825\n",
      "Iteration 41120 Training loss 0.08972959220409393 Validation loss 0.1137843206524849 Accuracy 0.6858333349227905\n",
      "Iteration 41130 Training loss 0.0881897509098053 Validation loss 0.11460087448358536 Accuracy 0.6856666803359985\n",
      "Iteration 41140 Training loss 0.0833938717842102 Validation loss 0.12155940383672714 Accuracy 0.671999990940094\n",
      "Iteration 41150 Training loss 0.08905201405286789 Validation loss 0.1351679414510727 Accuracy 0.6448333263397217\n",
      "Iteration 41160 Training loss 0.09021500498056412 Validation loss 0.13898302614688873 Accuracy 0.6388333439826965\n",
      "Iteration 41170 Training loss 0.08890791982412338 Validation loss 0.13335897028446198 Accuracy 0.6461666822433472\n",
      "Iteration 41180 Training loss 0.08718190342187881 Validation loss 0.12709437310695648 Accuracy 0.6614999771118164\n",
      "Iteration 41190 Training loss 0.08377784490585327 Validation loss 0.1217808648943901 Accuracy 0.6700000166893005\n",
      "Iteration 41200 Training loss 0.08607285469770432 Validation loss 0.12006638944149017 Accuracy 0.6761666536331177\n",
      "Iteration 41210 Training loss 0.08626332879066467 Validation loss 0.1355515569448471 Accuracy 0.6451666951179504\n",
      "Iteration 41220 Training loss 0.08759260922670364 Validation loss 0.1361089050769806 Accuracy 0.6446666717529297\n",
      "Iteration 41230 Training loss 0.09022530913352966 Validation loss 0.13956528902053833 Accuracy 0.6381666660308838\n",
      "Iteration 41240 Training loss 0.08856610953807831 Validation loss 0.1338137835264206 Accuracy 0.6480000019073486\n",
      "Iteration 41250 Training loss 0.08769245445728302 Validation loss 0.13465672731399536 Accuracy 0.6455000042915344\n",
      "Iteration 41260 Training loss 0.08903194218873978 Validation loss 0.1411556452512741 Accuracy 0.6318333148956299\n",
      "Iteration 41270 Training loss 0.08977260440587997 Validation loss 0.14253099262714386 Accuracy 0.6286666393280029\n",
      "Iteration 41280 Training loss 0.08589546382427216 Validation loss 0.13152405619621277 Accuracy 0.6514999866485596\n",
      "Iteration 41290 Training loss 0.08672604709863663 Validation loss 0.13394974172115326 Accuracy 0.6476666927337646\n",
      "Iteration 41300 Training loss 0.0849754810333252 Validation loss 0.1327766478061676 Accuracy 0.6488333344459534\n",
      "Iteration 41310 Training loss 0.09074672311544418 Validation loss 0.1357710212469101 Accuracy 0.6461666822433472\n",
      "Iteration 41320 Training loss 0.09225079417228699 Validation loss 0.1374618113040924 Accuracy 0.6434999704360962\n",
      "Iteration 41330 Training loss 0.08361103385686874 Validation loss 0.123793825507164 Accuracy 0.6671666502952576\n",
      "Iteration 41340 Training loss 0.08492313325405121 Validation loss 0.12220046669244766 Accuracy 0.6706666946411133\n",
      "Iteration 41350 Training loss 0.08924130350351334 Validation loss 0.13403572142124176 Accuracy 0.6488333344459534\n",
      "Iteration 41360 Training loss 0.0886518731713295 Validation loss 0.13318969309329987 Accuracy 0.6495000123977661\n",
      "Iteration 41370 Training loss 0.08526305109262466 Validation loss 0.1257927268743515 Accuracy 0.6641666889190674\n",
      "Iteration 41380 Training loss 0.08669368177652359 Validation loss 0.1360129863023758 Accuracy 0.6443333625793457\n",
      "Iteration 41390 Training loss 0.08627638965845108 Validation loss 0.13047552108764648 Accuracy 0.6524999737739563\n",
      "Iteration 41400 Training loss 0.0858386754989624 Validation loss 0.1323091685771942 Accuracy 0.6501666903495789\n",
      "Iteration 41410 Training loss 0.08705653995275497 Validation loss 0.1280333548784256 Accuracy 0.6598333120346069\n",
      "Iteration 41420 Training loss 0.08593415468931198 Validation loss 0.11876623332500458 Accuracy 0.6796666383743286\n",
      "Iteration 41430 Training loss 0.09058777987957001 Validation loss 0.11363543570041656 Accuracy 0.6865000128746033\n",
      "Iteration 41440 Training loss 0.08936995267868042 Validation loss 0.11387577652931213 Accuracy 0.6866666674613953\n",
      "Iteration 41450 Training loss 0.09006976336240768 Validation loss 0.11380960047245026 Accuracy 0.6859999895095825\n",
      "Iteration 41460 Training loss 0.0899260863661766 Validation loss 0.11384306102991104 Accuracy 0.6863333582878113\n",
      "Iteration 41470 Training loss 0.08627569675445557 Validation loss 0.11548127979040146 Accuracy 0.684166669845581\n",
      "Iteration 41480 Training loss 0.09239474684000015 Validation loss 0.11400076001882553 Accuracy 0.687166690826416\n",
      "Iteration 41490 Training loss 0.08471713960170746 Validation loss 0.11563411355018616 Accuracy 0.6819999814033508\n",
      "Iteration 41500 Training loss 0.08555859327316284 Validation loss 0.11615895479917526 Accuracy 0.6809999942779541\n",
      "Iteration 41510 Training loss 0.0913405790925026 Validation loss 0.11398207396268845 Accuracy 0.6868333220481873\n",
      "Iteration 41520 Training loss 0.0907619446516037 Validation loss 0.11400947719812393 Accuracy 0.6866666674613953\n",
      "Iteration 41530 Training loss 0.08971260488033295 Validation loss 0.11406499892473221 Accuracy 0.6863333582878113\n",
      "Iteration 41540 Training loss 0.08591676503419876 Validation loss 0.11564642190933228 Accuracy 0.6826666593551636\n",
      "Iteration 41550 Training loss 0.08445637673139572 Validation loss 0.1245342418551445 Accuracy 0.6654999852180481\n",
      "Iteration 41560 Training loss 0.08726019412279129 Validation loss 0.12802453339099884 Accuracy 0.6585000157356262\n",
      "Iteration 41570 Training loss 0.0857352614402771 Validation loss 0.12873977422714233 Accuracy 0.6576666831970215\n",
      "Iteration 41580 Training loss 0.08538216352462769 Validation loss 0.13330422341823578 Accuracy 0.6489999890327454\n",
      "Iteration 41590 Training loss 0.09100561589002609 Validation loss 0.1367432028055191 Accuracy 0.6449999809265137\n",
      "Iteration 41600 Training loss 0.09054941684007645 Validation loss 0.13613471388816833 Accuracy 0.6449999809265137\n",
      "Iteration 41610 Training loss 0.08969464898109436 Validation loss 0.13469184935092926 Accuracy 0.6463333368301392\n",
      "Iteration 41620 Training loss 0.08593747019767761 Validation loss 0.13147033751010895 Accuracy 0.6521666646003723\n",
      "Iteration 41630 Training loss 0.08500660955905914 Validation loss 0.12769104540348053 Accuracy 0.659333348274231\n",
      "Iteration 41640 Training loss 0.08533060550689697 Validation loss 0.12956079840660095 Accuracy 0.6551666855812073\n",
      "Iteration 41650 Training loss 0.09209450334310532 Validation loss 0.13791362941265106 Accuracy 0.643666684627533\n",
      "Iteration 41660 Training loss 0.08910971879959106 Validation loss 0.13333871960639954 Accuracy 0.6486666798591614\n",
      "Iteration 41670 Training loss 0.0875687375664711 Validation loss 0.1290329098701477 Accuracy 0.6573333144187927\n",
      "Iteration 41680 Training loss 0.08611408621072769 Validation loss 0.1308087855577469 Accuracy 0.6538333296775818\n",
      "Iteration 41690 Training loss 0.08489130437374115 Validation loss 0.12877938151359558 Accuracy 0.6568333506584167\n",
      "Iteration 41700 Training loss 0.08726248890161514 Validation loss 0.1348503828048706 Accuracy 0.6473333239555359\n",
      "Iteration 41710 Training loss 0.08234525471925735 Validation loss 0.12613432109355927 Accuracy 0.6626666784286499\n",
      "Iteration 41720 Training loss 0.08732466399669647 Validation loss 0.12842892110347748 Accuracy 0.6575000286102295\n",
      "Iteration 41730 Training loss 0.08545990288257599 Validation loss 0.13123689591884613 Accuracy 0.6514999866485596\n",
      "Iteration 41740 Training loss 0.08917345851659775 Validation loss 0.13255076110363007 Accuracy 0.6488333344459534\n",
      "Iteration 41750 Training loss 0.0897960290312767 Validation loss 0.13617348670959473 Accuracy 0.6460000276565552\n",
      "Iteration 41760 Training loss 0.09072961658239365 Validation loss 0.1401459127664566 Accuracy 0.6363333463668823\n",
      "Iteration 41770 Training loss 0.08661562949419022 Validation loss 0.13056941330432892 Accuracy 0.6545000076293945\n",
      "Iteration 41780 Training loss 0.0848546177148819 Validation loss 0.12257177382707596 Accuracy 0.6696666479110718\n",
      "Iteration 41790 Training loss 0.08646290749311447 Validation loss 0.11557373404502869 Accuracy 0.6833333373069763\n",
      "Iteration 41800 Training loss 0.08758971840143204 Validation loss 0.11508741229772568 Accuracy 0.6846666932106018\n",
      "Iteration 41810 Training loss 0.0903041884303093 Validation loss 0.11376147717237473 Accuracy 0.6861666440963745\n",
      "Iteration 41820 Training loss 0.08867118507623672 Validation loss 0.11415024846792221 Accuracy 0.687833309173584\n",
      "Iteration 41830 Training loss 0.0896528959274292 Validation loss 0.11402925848960876 Accuracy 0.687333345413208\n",
      "Iteration 41840 Training loss 0.08438130468130112 Validation loss 0.11506890505552292 Accuracy 0.6858333349227905\n",
      "Iteration 41850 Training loss 0.09178364276885986 Validation loss 0.11402510851621628 Accuracy 0.687333345413208\n",
      "Iteration 41860 Training loss 0.08737989515066147 Validation loss 0.11469719558954239 Accuracy 0.6861666440963745\n",
      "Iteration 41870 Training loss 0.08567585051059723 Validation loss 0.11515220999717712 Accuracy 0.6838333606719971\n",
      "Iteration 41880 Training loss 0.08672644197940826 Validation loss 0.11585436016321182 Accuracy 0.6813333630561829\n",
      "Iteration 41890 Training loss 0.09045641869306564 Validation loss 0.11390238255262375 Accuracy 0.6868333220481873\n",
      "Iteration 41900 Training loss 0.08671238273382187 Validation loss 0.11514508724212646 Accuracy 0.6838333606719971\n",
      "Iteration 41910 Training loss 0.08513829112052917 Validation loss 0.11461716890335083 Accuracy 0.6855000257492065\n",
      "Iteration 41920 Training loss 0.08405040949583054 Validation loss 0.11477015912532806 Accuracy 0.6861666440963745\n",
      "Iteration 41930 Training loss 0.08439590781927109 Validation loss 0.12285406142473221 Accuracy 0.6691666841506958\n",
      "Iteration 41940 Training loss 0.08764892816543579 Validation loss 0.12608548998832703 Accuracy 0.6646666526794434\n",
      "Iteration 41950 Training loss 0.08556576818227768 Validation loss 0.1219974160194397 Accuracy 0.6703333258628845\n",
      "Iteration 41960 Training loss 0.0860753059387207 Validation loss 0.12894199788570404 Accuracy 0.6583333611488342\n",
      "Iteration 41970 Training loss 0.08816460520029068 Validation loss 0.14043843746185303 Accuracy 0.6365000009536743\n",
      "Iteration 41980 Training loss 0.08754986524581909 Validation loss 0.13867247104644775 Accuracy 0.6396666765213013\n",
      "Iteration 41990 Training loss 0.0870046615600586 Validation loss 0.13196128606796265 Accuracy 0.6501666903495789\n",
      "Iteration 42000 Training loss 0.08710264414548874 Validation loss 0.12438621371984482 Accuracy 0.6671666502952576\n",
      "Iteration 42010 Training loss 0.08495408296585083 Validation loss 0.12767405807971954 Accuracy 0.6581666469573975\n",
      "Iteration 42020 Training loss 0.08414861559867859 Validation loss 0.12421419471502304 Accuracy 0.6666666865348816\n",
      "Iteration 42030 Training loss 0.08749428391456604 Validation loss 0.131082221865654 Accuracy 0.652999997138977\n",
      "Iteration 42040 Training loss 0.0941464975476265 Validation loss 0.14650171995162964 Accuracy 0.621999979019165\n",
      "Iteration 42050 Training loss 0.08726348727941513 Validation loss 0.13637377321720123 Accuracy 0.6458333134651184\n",
      "Iteration 42060 Training loss 0.08864676207304001 Validation loss 0.13442443311214447 Accuracy 0.6485000252723694\n",
      "Iteration 42070 Training loss 0.0882820338010788 Validation loss 0.13798388838768005 Accuracy 0.6424999833106995\n",
      "Iteration 42080 Training loss 0.09013407677412033 Validation loss 0.1372409462928772 Accuracy 0.6448333263397217\n",
      "Iteration 42090 Training loss 0.08373729884624481 Validation loss 0.12376175820827484 Accuracy 0.6671666502952576\n",
      "Iteration 42100 Training loss 0.08724961429834366 Validation loss 0.1310071051120758 Accuracy 0.6526666879653931\n",
      "Iteration 42110 Training loss 0.08302994072437286 Validation loss 0.12485034018754959 Accuracy 0.6650000214576721\n",
      "Iteration 42120 Training loss 0.08809678256511688 Validation loss 0.13415022194385529 Accuracy 0.6474999785423279\n",
      "Iteration 42130 Training loss 0.0908878967165947 Validation loss 0.1393762230873108 Accuracy 0.6391666531562805\n",
      "Iteration 42140 Training loss 0.08497437089681625 Validation loss 0.12376303970813751 Accuracy 0.6664999723434448\n",
      "Iteration 42150 Training loss 0.08508766442537308 Validation loss 0.12647072970867157 Accuracy 0.6631666421890259\n",
      "Iteration 42160 Training loss 0.08884018659591675 Validation loss 0.1339946836233139 Accuracy 0.6495000123977661\n",
      "Iteration 42170 Training loss 0.0905819907784462 Validation loss 0.141959547996521 Accuracy 0.6303333044052124\n",
      "Iteration 42180 Training loss 0.0885251834988594 Validation loss 0.14054803550243378 Accuracy 0.6361666917800903\n",
      "Iteration 42190 Training loss 0.09006358683109283 Validation loss 0.13592305779457092 Accuracy 0.6466666460037231\n",
      "Iteration 42200 Training loss 0.0874563604593277 Validation loss 0.13323283195495605 Accuracy 0.6488333344459534\n",
      "Iteration 42210 Training loss 0.0878080353140831 Validation loss 0.13298490643501282 Accuracy 0.6488333344459534\n",
      "Iteration 42220 Training loss 0.08627225458621979 Validation loss 0.12669207155704498 Accuracy 0.6633333563804626\n",
      "Iteration 42230 Training loss 0.08693613857030869 Validation loss 0.13436421751976013 Accuracy 0.6478333473205566\n",
      "Iteration 42240 Training loss 0.08760044723749161 Validation loss 0.1287657767534256 Accuracy 0.6576666831970215\n",
      "Iteration 42250 Training loss 0.08604104816913605 Validation loss 0.1276398003101349 Accuracy 0.6598333120346069\n",
      "Iteration 42260 Training loss 0.08781007677316666 Validation loss 0.13242217898368835 Accuracy 0.6503333449363708\n",
      "Iteration 42270 Training loss 0.0849343091249466 Validation loss 0.13359257578849792 Accuracy 0.6488333344459534\n",
      "Iteration 42280 Training loss 0.08645853400230408 Validation loss 0.12782572209835052 Accuracy 0.6604999899864197\n",
      "Iteration 42290 Training loss 0.08543218672275543 Validation loss 0.12920613586902618 Accuracy 0.6571666598320007\n",
      "Iteration 42300 Training loss 0.08635582774877548 Validation loss 0.13245098292827606 Accuracy 0.6511666774749756\n",
      "Iteration 42310 Training loss 0.08434591442346573 Validation loss 0.12650741636753082 Accuracy 0.6636666655540466\n",
      "Iteration 42320 Training loss 0.0897800400853157 Validation loss 0.13481169939041138 Accuracy 0.6463333368301392\n",
      "Iteration 42330 Training loss 0.08667708188295364 Validation loss 0.13707348704338074 Accuracy 0.6424999833106995\n",
      "Iteration 42340 Training loss 0.08650939166545868 Validation loss 0.13085974752902985 Accuracy 0.6546666622161865\n",
      "Iteration 42350 Training loss 0.08833939582109451 Validation loss 0.13725052773952484 Accuracy 0.6423333287239075\n",
      "Iteration 42360 Training loss 0.08884494006633759 Validation loss 0.13467763364315033 Accuracy 0.6480000019073486\n",
      "Iteration 42370 Training loss 0.08698876202106476 Validation loss 0.1345369666814804 Accuracy 0.6471666693687439\n",
      "Iteration 42380 Training loss 0.08860842138528824 Validation loss 0.1361285299062729 Accuracy 0.6464999914169312\n",
      "Iteration 42390 Training loss 0.08939874917268753 Validation loss 0.13927698135375977 Accuracy 0.6383333206176758\n",
      "Iteration 42400 Training loss 0.08394470810890198 Validation loss 0.1272089034318924 Accuracy 0.6618333458900452\n",
      "Iteration 42410 Training loss 0.08476761728525162 Validation loss 0.12428734451532364 Accuracy 0.6661666631698608\n",
      "Iteration 42420 Training loss 0.08492446690797806 Validation loss 0.12931674718856812 Accuracy 0.656166672706604\n",
      "Iteration 42430 Training loss 0.08800624310970306 Validation loss 0.13686025142669678 Accuracy 0.643833339214325\n",
      "Iteration 42440 Training loss 0.08825168758630753 Validation loss 0.13434413075447083 Accuracy 0.6493333578109741\n",
      "Iteration 42450 Training loss 0.09115524590015411 Validation loss 0.1398305594921112 Accuracy 0.637333333492279\n",
      "Iteration 42460 Training loss 0.08941426873207092 Validation loss 0.13703320920467377 Accuracy 0.6451666951179504\n",
      "Iteration 42470 Training loss 0.08396189659833908 Validation loss 0.12568829953670502 Accuracy 0.6648333072662354\n",
      "Iteration 42480 Training loss 0.08634065091609955 Validation loss 0.12188655883073807 Accuracy 0.6711666584014893\n",
      "Iteration 42490 Training loss 0.08480795472860336 Validation loss 0.12960702180862427 Accuracy 0.6546666622161865\n",
      "Iteration 42500 Training loss 0.08631827682256699 Validation loss 0.12849046289920807 Accuracy 0.659500002861023\n",
      "Iteration 42510 Training loss 0.08790728449821472 Validation loss 0.1275201290845871 Accuracy 0.6603333353996277\n",
      "Iteration 42520 Training loss 0.08720207959413528 Validation loss 0.12981939315795898 Accuracy 0.656166672706604\n",
      "Iteration 42530 Training loss 0.08983246982097626 Validation loss 0.13738012313842773 Accuracy 0.643833339214325\n",
      "Iteration 42540 Training loss 0.08564769476652145 Validation loss 0.12769834697246552 Accuracy 0.6611666679382324\n",
      "Iteration 42550 Training loss 0.0892626941204071 Validation loss 0.1335752010345459 Accuracy 0.6480000019073486\n",
      "Iteration 42560 Training loss 0.08924130350351334 Validation loss 0.1397317349910736 Accuracy 0.6368333101272583\n",
      "Iteration 42570 Training loss 0.09200417250394821 Validation loss 0.14353229105472565 Accuracy 0.6274999976158142\n",
      "Iteration 42580 Training loss 0.08837173134088516 Validation loss 0.13286961615085602 Accuracy 0.6498333215713501\n",
      "Iteration 42590 Training loss 0.08958925306797028 Validation loss 0.1330515593290329 Accuracy 0.6483333110809326\n",
      "Iteration 42600 Training loss 0.08522030711174011 Validation loss 0.12226457893848419 Accuracy 0.6701666712760925\n",
      "Iteration 42610 Training loss 0.0841703787446022 Validation loss 0.11904223263263702 Accuracy 0.6808333396911621\n",
      "Iteration 42620 Training loss 0.0849987119436264 Validation loss 0.11470463871955872 Accuracy 0.6856666803359985\n",
      "Iteration 42630 Training loss 0.09163297712802887 Validation loss 0.11397797614336014 Accuracy 0.6866666674613953\n",
      "Iteration 42640 Training loss 0.0871146023273468 Validation loss 0.1145353764295578 Accuracy 0.6859999895095825\n",
      "Iteration 42650 Training loss 0.08385437726974487 Validation loss 0.11594473570585251 Accuracy 0.6828333139419556\n",
      "Iteration 42660 Training loss 0.08645381778478622 Validation loss 0.11416001617908478 Accuracy 0.6866666674613953\n",
      "Iteration 42670 Training loss 0.08652752637863159 Validation loss 0.1146514043211937 Accuracy 0.6855000257492065\n",
      "Iteration 42680 Training loss 0.08627762645483017 Validation loss 0.11578371375799179 Accuracy 0.6825000047683716\n",
      "Iteration 42690 Training loss 0.08656296879053116 Validation loss 0.11430957913398743 Accuracy 0.6859999895095825\n",
      "Iteration 42700 Training loss 0.08495456725358963 Validation loss 0.11445064097642899 Accuracy 0.6863333582878113\n",
      "Iteration 42710 Training loss 0.08756327629089355 Validation loss 0.11481422930955887 Accuracy 0.6861666440963745\n",
      "Iteration 42720 Training loss 0.08725769072771072 Validation loss 0.1144467145204544 Accuracy 0.6855000257492065\n",
      "Iteration 42730 Training loss 0.08981285244226456 Validation loss 0.11397797614336014 Accuracy 0.687666654586792\n",
      "Iteration 42740 Training loss 0.08879438042640686 Validation loss 0.11407939344644547 Accuracy 0.6861666440963745\n",
      "Iteration 42750 Training loss 0.08796384930610657 Validation loss 0.11481174826622009 Accuracy 0.6850000023841858\n",
      "Iteration 42760 Training loss 0.08575105667114258 Validation loss 0.11526566743850708 Accuracy 0.684166669845581\n",
      "Iteration 42770 Training loss 0.08382551372051239 Validation loss 0.11478275805711746 Accuracy 0.6851666569709778\n",
      "Iteration 42780 Training loss 0.08942724764347076 Validation loss 0.11388640105724335 Accuracy 0.687333345413208\n",
      "Iteration 42790 Training loss 0.08340137451887131 Validation loss 0.11698105931282043 Accuracy 0.6801666617393494\n",
      "Iteration 42800 Training loss 0.08566273003816605 Validation loss 0.11622535437345505 Accuracy 0.6831666827201843\n",
      "Iteration 42810 Training loss 0.08811356872320175 Validation loss 0.11407314240932465 Accuracy 0.6868333220481873\n",
      "Iteration 42820 Training loss 0.08894486725330353 Validation loss 0.1139957457780838 Accuracy 0.6853333115577698\n",
      "Iteration 42830 Training loss 0.08786342293024063 Validation loss 0.11416589468717575 Accuracy 0.6866666674613953\n",
      "Iteration 42840 Training loss 0.08780978620052338 Validation loss 0.11445155739784241 Accuracy 0.6853333115577698\n",
      "Iteration 42850 Training loss 0.08731336146593094 Validation loss 0.11455102264881134 Accuracy 0.6856666803359985\n",
      "Iteration 42860 Training loss 0.0844712182879448 Validation loss 0.1242944747209549 Accuracy 0.6663333177566528\n",
      "Iteration 42870 Training loss 0.08586222678422928 Validation loss 0.13488006591796875 Accuracy 0.6480000019073486\n",
      "Iteration 42880 Training loss 0.08453397452831268 Validation loss 0.1286739557981491 Accuracy 0.6566666960716248\n",
      "Iteration 42890 Training loss 0.08772430568933487 Validation loss 0.1358407437801361 Accuracy 0.6461666822433472\n",
      "Iteration 42900 Training loss 0.09379293769598007 Validation loss 0.14216890931129456 Accuracy 0.6308333277702332\n",
      "Iteration 42910 Training loss 0.08662066608667374 Validation loss 0.13128450512886047 Accuracy 0.6521666646003723\n",
      "Iteration 42920 Training loss 0.08567965775728226 Validation loss 0.12603043019771576 Accuracy 0.6641666889190674\n",
      "Iteration 42930 Training loss 0.0889238566160202 Validation loss 0.1367833912372589 Accuracy 0.6434999704360962\n",
      "Iteration 42940 Training loss 0.08807893097400665 Validation loss 0.1377539336681366 Accuracy 0.6423333287239075\n",
      "Iteration 42950 Training loss 0.08602195978164673 Validation loss 0.12605243921279907 Accuracy 0.6643333435058594\n",
      "Iteration 42960 Training loss 0.08674907684326172 Validation loss 0.13063351809978485 Accuracy 0.6548333168029785\n",
      "Iteration 42970 Training loss 0.09045413881540298 Validation loss 0.14221875369548798 Accuracy 0.6315000057220459\n",
      "Iteration 42980 Training loss 0.0859665647149086 Validation loss 0.1327788084745407 Accuracy 0.6504999995231628\n",
      "Iteration 42990 Training loss 0.08946330100297928 Validation loss 0.13701792061328888 Accuracy 0.6451666951179504\n",
      "Iteration 43000 Training loss 0.08601118624210358 Validation loss 0.13446809351444244 Accuracy 0.6483333110809326\n",
      "Iteration 43010 Training loss 0.08954472094774246 Validation loss 0.13750682771205902 Accuracy 0.6446666717529297\n",
      "Iteration 43020 Training loss 0.0911044329404831 Validation loss 0.137695774435997 Accuracy 0.643666684627533\n",
      "Iteration 43030 Training loss 0.08402108401060104 Validation loss 0.12698788940906525 Accuracy 0.6626666784286499\n",
      "Iteration 43040 Training loss 0.08470568060874939 Validation loss 0.12877288460731506 Accuracy 0.6581666469573975\n",
      "Iteration 43050 Training loss 0.08595515787601471 Validation loss 0.11696875095367432 Accuracy 0.6804999709129333\n",
      "Iteration 43060 Training loss 0.08394723385572433 Validation loss 0.11836879700422287 Accuracy 0.6803333163261414\n",
      "Iteration 43070 Training loss 0.08814626187086105 Validation loss 0.13234680891036987 Accuracy 0.6516666412353516\n",
      "Iteration 43080 Training loss 0.0869569480419159 Validation loss 0.13272270560264587 Accuracy 0.6496666669845581\n",
      "Iteration 43090 Training loss 0.08931543678045273 Validation loss 0.1331559717655182 Accuracy 0.6491666436195374\n",
      "Iteration 43100 Training loss 0.089385487139225 Validation loss 0.13831676542758942 Accuracy 0.6414999961853027\n",
      "Iteration 43110 Training loss 0.08876887708902359 Validation loss 0.13568438589572906 Accuracy 0.6471666693687439\n",
      "Iteration 43120 Training loss 0.08637396991252899 Validation loss 0.13431686162948608 Accuracy 0.6480000019073486\n",
      "Iteration 43130 Training loss 0.08604305982589722 Validation loss 0.13238763809204102 Accuracy 0.6508333086967468\n",
      "Iteration 43140 Training loss 0.0896322950720787 Validation loss 0.1363041251897812 Accuracy 0.6463333368301392\n",
      "Iteration 43150 Training loss 0.08496405184268951 Validation loss 0.13149631023406982 Accuracy 0.6513333320617676\n",
      "Iteration 43160 Training loss 0.08726688474416733 Validation loss 0.13507606089115143 Accuracy 0.6468333601951599\n",
      "Iteration 43170 Training loss 0.08773467689752579 Validation loss 0.1345275342464447 Accuracy 0.6486666798591614\n",
      "Iteration 43180 Training loss 0.08831524848937988 Validation loss 0.13541162014007568 Accuracy 0.6474999785423279\n",
      "Iteration 43190 Training loss 0.08870121091604233 Validation loss 0.13678951561450958 Accuracy 0.6448333263397217\n",
      "Iteration 43200 Training loss 0.08553269505500793 Validation loss 0.1206744983792305 Accuracy 0.6766666769981384\n",
      "Iteration 43210 Training loss 0.08522700518369675 Validation loss 0.12517577409744263 Accuracy 0.6656666398048401\n",
      "Iteration 43220 Training loss 0.09149546176195145 Validation loss 0.14239974319934845 Accuracy 0.6301666498184204\n",
      "Iteration 43230 Training loss 0.08749669045209885 Validation loss 0.1329623907804489 Accuracy 0.6496666669845581\n",
      "Iteration 43240 Training loss 0.08651237189769745 Validation loss 0.13651862740516663 Accuracy 0.6455000042915344\n",
      "Iteration 43250 Training loss 0.08766833692789078 Validation loss 0.1309921145439148 Accuracy 0.6510000228881836\n",
      "Iteration 43260 Training loss 0.08555629104375839 Validation loss 0.126007542014122 Accuracy 0.6646666526794434\n",
      "Iteration 43270 Training loss 0.08468551933765411 Validation loss 0.127190962433815 Accuracy 0.6601666808128357\n",
      "Iteration 43280 Training loss 0.09357572346925735 Validation loss 0.14513139426708221 Accuracy 0.6255000233650208\n",
      "Iteration 43290 Training loss 0.08714611828327179 Validation loss 0.13532647490501404 Accuracy 0.6474999785423279\n",
      "Iteration 43300 Training loss 0.08879908919334412 Validation loss 0.13801807165145874 Accuracy 0.6424999833106995\n",
      "Iteration 43310 Training loss 0.09058877825737 Validation loss 0.13826346397399902 Accuracy 0.6421666741371155\n",
      "Iteration 43320 Training loss 0.08811014890670776 Validation loss 0.13244891166687012 Accuracy 0.6498333215713501\n",
      "Iteration 43330 Training loss 0.0859266147017479 Validation loss 0.12475278973579407 Accuracy 0.6664999723434448\n",
      "Iteration 43340 Training loss 0.08933684974908829 Validation loss 0.13349227607250214 Accuracy 0.6485000252723694\n",
      "Iteration 43350 Training loss 0.0872451514005661 Validation loss 0.13027089834213257 Accuracy 0.6546666622161865\n",
      "Iteration 43360 Training loss 0.08649924397468567 Validation loss 0.12536832690238953 Accuracy 0.6650000214576721\n",
      "Iteration 43370 Training loss 0.08843985199928284 Validation loss 0.13559414446353912 Accuracy 0.6473333239555359\n",
      "Iteration 43380 Training loss 0.08759035915136337 Validation loss 0.13860604166984558 Accuracy 0.640999972820282\n",
      "Iteration 43390 Training loss 0.08645927160978317 Validation loss 0.13591806590557098 Accuracy 0.6464999914169312\n",
      "Iteration 43400 Training loss 0.08646626770496368 Validation loss 0.13091404736042023 Accuracy 0.6539999842643738\n",
      "Iteration 43410 Training loss 0.09133186936378479 Validation loss 0.13883021473884583 Accuracy 0.6399999856948853\n",
      "Iteration 43420 Training loss 0.08344506472349167 Validation loss 0.12826012074947357 Accuracy 0.6601666808128357\n",
      "Iteration 43430 Training loss 0.08557290583848953 Validation loss 0.12774330377578735 Accuracy 0.6610000133514404\n",
      "Iteration 43440 Training loss 0.08410362899303436 Validation loss 0.12458225339651108 Accuracy 0.6668333411216736\n",
      "Iteration 43450 Training loss 0.09011926501989365 Validation loss 0.1430237889289856 Accuracy 0.628333330154419\n",
      "Iteration 43460 Training loss 0.08688715845346451 Validation loss 0.1334666907787323 Accuracy 0.6483333110809326\n",
      "Iteration 43470 Training loss 0.08709452301263809 Validation loss 0.13308224081993103 Accuracy 0.6498333215713501\n",
      "Iteration 43480 Training loss 0.08643569052219391 Validation loss 0.1319718062877655 Accuracy 0.6524999737739563\n",
      "Iteration 43490 Training loss 0.0863008201122284 Validation loss 0.1281847208738327 Accuracy 0.6596666574478149\n",
      "Iteration 43500 Training loss 0.08700263500213623 Validation loss 0.13415081799030304 Accuracy 0.6486666798591614\n",
      "Iteration 43510 Training loss 0.0887017622590065 Validation loss 0.1323784589767456 Accuracy 0.6508333086967468\n",
      "Iteration 43520 Training loss 0.08530912548303604 Validation loss 0.12805575132369995 Accuracy 0.6604999899864197\n",
      "Iteration 43530 Training loss 0.08794739097356796 Validation loss 0.1321517378091812 Accuracy 0.6511666774749756\n",
      "Iteration 43540 Training loss 0.08937156200408936 Validation loss 0.13854612410068512 Accuracy 0.6418333053588867\n",
      "Iteration 43550 Training loss 0.08669804036617279 Validation loss 0.1331157237291336 Accuracy 0.6495000123977661\n",
      "Iteration 43560 Training loss 0.0849294438958168 Validation loss 0.1307803988456726 Accuracy 0.6539999842643738\n",
      "Iteration 43570 Training loss 0.08290717750787735 Validation loss 0.12955084443092346 Accuracy 0.6568333506584167\n",
      "Iteration 43580 Training loss 0.08353710919618607 Validation loss 0.1191142275929451 Accuracy 0.6776666641235352\n",
      "Iteration 43590 Training loss 0.08799441158771515 Validation loss 0.11533300578594208 Accuracy 0.6831666827201843\n",
      "Iteration 43600 Training loss 0.092930868268013 Validation loss 0.11427934467792511 Accuracy 0.6865000128746033\n",
      "Iteration 43610 Training loss 0.08782429993152618 Validation loss 0.11416175216436386 Accuracy 0.6880000233650208\n",
      "Iteration 43620 Training loss 0.08894556760787964 Validation loss 0.11415445059537888 Accuracy 0.6850000023841858\n",
      "Iteration 43630 Training loss 0.08625981956720352 Validation loss 0.11426986008882523 Accuracy 0.6836666464805603\n",
      "Iteration 43640 Training loss 0.0875210240483284 Validation loss 0.11487286537885666 Accuracy 0.6858333349227905\n",
      "Iteration 43650 Training loss 0.08849555253982544 Validation loss 0.1144188717007637 Accuracy 0.6858333349227905\n",
      "Iteration 43660 Training loss 0.08675343543291092 Validation loss 0.11499792337417603 Accuracy 0.6848333477973938\n",
      "Iteration 43670 Training loss 0.0837084949016571 Validation loss 0.12004793435335159 Accuracy 0.6758333444595337\n",
      "Iteration 43680 Training loss 0.08399777114391327 Validation loss 0.12360332161188126 Accuracy 0.6675000190734863\n",
      "Iteration 43690 Training loss 0.08515148609876633 Validation loss 0.11569012701511383 Accuracy 0.6855000257492065\n",
      "Iteration 43700 Training loss 0.08975543081760406 Validation loss 0.11406370252370834 Accuracy 0.6858333349227905\n",
      "Iteration 43710 Training loss 0.08633927255868912 Validation loss 0.11431065201759338 Accuracy 0.6853333115577698\n",
      "Iteration 43720 Training loss 0.08790374547243118 Validation loss 0.11435434967279434 Accuracy 0.6863333582878113\n",
      "Iteration 43730 Training loss 0.08667708188295364 Validation loss 0.11434604972600937 Accuracy 0.6851666569709778\n",
      "Iteration 43740 Training loss 0.08776974678039551 Validation loss 0.11473734676837921 Accuracy 0.6859999895095825\n",
      "Iteration 43750 Training loss 0.08996732532978058 Validation loss 0.11409533768892288 Accuracy 0.6855000257492065\n",
      "Iteration 43760 Training loss 0.09037920832633972 Validation loss 0.11407557129859924 Accuracy 0.6861666440963745\n",
      "Iteration 43770 Training loss 0.08565838634967804 Validation loss 0.11768252402544022 Accuracy 0.6811666488647461\n",
      "Iteration 43780 Training loss 0.08434019982814789 Validation loss 0.11694653332233429 Accuracy 0.6819999814033508\n",
      "Iteration 43790 Training loss 0.08357184380292892 Validation loss 0.11796826869249344 Accuracy 0.6821666955947876\n",
      "Iteration 43800 Training loss 0.08628670126199722 Validation loss 0.11625468730926514 Accuracy 0.6818333268165588\n",
      "Iteration 43810 Training loss 0.08551029115915298 Validation loss 0.11640036851167679 Accuracy 0.6804999709129333\n",
      "Iteration 43820 Training loss 0.08953704684972763 Validation loss 0.11402560770511627 Accuracy 0.6846666932106018\n",
      "Iteration 43830 Training loss 0.08957429975271225 Validation loss 0.11419309675693512 Accuracy 0.6855000257492065\n",
      "Iteration 43840 Training loss 0.08911441266536713 Validation loss 0.11415664851665497 Accuracy 0.6869999766349792\n",
      "Iteration 43850 Training loss 0.0868602767586708 Validation loss 0.11415456235408783 Accuracy 0.6865000128746033\n",
      "Iteration 43860 Training loss 0.08752989768981934 Validation loss 0.11409027874469757 Accuracy 0.6858333349227905\n",
      "Iteration 43870 Training loss 0.08909284323453903 Validation loss 0.11409492790699005 Accuracy 0.6850000023841858\n",
      "Iteration 43880 Training loss 0.08830460906028748 Validation loss 0.1142897754907608 Accuracy 0.6858333349227905\n",
      "Iteration 43890 Training loss 0.08849737793207169 Validation loss 0.11412421613931656 Accuracy 0.6863333582878113\n",
      "Iteration 43900 Training loss 0.08685215562582016 Validation loss 0.11521878838539124 Accuracy 0.6834999918937683\n",
      "Iteration 43910 Training loss 0.08483928442001343 Validation loss 0.11811842024326324 Accuracy 0.6803333163261414\n",
      "Iteration 43920 Training loss 0.08751323074102402 Validation loss 0.12328458577394485 Accuracy 0.6676666736602783\n",
      "Iteration 43930 Training loss 0.08388593792915344 Validation loss 0.1260405331850052 Accuracy 0.6643333435058594\n",
      "Iteration 43940 Training loss 0.08826755732297897 Validation loss 0.131357803940773 Accuracy 0.6538333296775818\n",
      "Iteration 43950 Training loss 0.0868668407201767 Validation loss 0.12938162684440613 Accuracy 0.6551666855812073\n",
      "Iteration 43960 Training loss 0.08890647441148758 Validation loss 0.13943636417388916 Accuracy 0.6371666789054871\n",
      "Iteration 43970 Training loss 0.08635646849870682 Validation loss 0.1338435709476471 Accuracy 0.6478333473205566\n",
      "Iteration 43980 Training loss 0.08946091681718826 Validation loss 0.1362561285495758 Accuracy 0.6471666693687439\n",
      "Iteration 43990 Training loss 0.08242157846689224 Validation loss 0.12912704050540924 Accuracy 0.6573333144187927\n",
      "Iteration 44000 Training loss 0.08698628097772598 Validation loss 0.1298576295375824 Accuracy 0.656333327293396\n",
      "Iteration 44010 Training loss 0.0870986059308052 Validation loss 0.13371381163597107 Accuracy 0.6493333578109741\n",
      "Iteration 44020 Training loss 0.08745609223842621 Validation loss 0.13377241790294647 Accuracy 0.6498333215713501\n",
      "Iteration 44030 Training loss 0.08747103810310364 Validation loss 0.12728963792324066 Accuracy 0.6626666784286499\n",
      "Iteration 44040 Training loss 0.08747737854719162 Validation loss 0.13208402693271637 Accuracy 0.6523333191871643\n",
      "Iteration 44050 Training loss 0.08673619478940964 Validation loss 0.13249139487743378 Accuracy 0.6506666541099548\n",
      "Iteration 44060 Training loss 0.08634185791015625 Validation loss 0.12818239629268646 Accuracy 0.6603333353996277\n",
      "Iteration 44070 Training loss 0.09361495822668076 Validation loss 0.14542563259601593 Accuracy 0.6255000233650208\n",
      "Iteration 44080 Training loss 0.08612509816884995 Validation loss 0.13481606543064117 Accuracy 0.6483333110809326\n",
      "Iteration 44090 Training loss 0.08670597523450851 Validation loss 0.1344234049320221 Accuracy 0.6489999890327454\n",
      "Iteration 44100 Training loss 0.08686976879835129 Validation loss 0.13248081505298615 Accuracy 0.6511666774749756\n",
      "Iteration 44110 Training loss 0.08408313989639282 Validation loss 0.11797802895307541 Accuracy 0.6811666488647461\n",
      "Iteration 44120 Training loss 0.08634462207555771 Validation loss 0.11508989334106445 Accuracy 0.6848333477973938\n",
      "Iteration 44130 Training loss 0.08868569880723953 Validation loss 0.11392062157392502 Accuracy 0.6866666674613953\n",
      "Iteration 44140 Training loss 0.08722735941410065 Validation loss 0.11434660106897354 Accuracy 0.6865000128746033\n",
      "Iteration 44150 Training loss 0.08787620067596436 Validation loss 0.11463318020105362 Accuracy 0.6863333582878113\n",
      "Iteration 44160 Training loss 0.08584283292293549 Validation loss 0.11540530622005463 Accuracy 0.6834999918937683\n",
      "Iteration 44170 Training loss 0.08452640473842621 Validation loss 0.11586374789476395 Accuracy 0.6836666464805603\n",
      "Iteration 44180 Training loss 0.08707857131958008 Validation loss 0.1154540479183197 Accuracy 0.6834999918937683\n",
      "Iteration 44190 Training loss 0.08829901367425919 Validation loss 0.11464062333106995 Accuracy 0.6865000128746033\n",
      "Iteration 44200 Training loss 0.08614558726549149 Validation loss 0.11441107094287872 Accuracy 0.6866666674613953\n",
      "Iteration 44210 Training loss 0.08686007559299469 Validation loss 0.11416371911764145 Accuracy 0.6865000128746033\n",
      "Iteration 44220 Training loss 0.08396898955106735 Validation loss 0.11498883366584778 Accuracy 0.684499979019165\n",
      "Iteration 44230 Training loss 0.08526194095611572 Validation loss 0.11577148735523224 Accuracy 0.6834999918937683\n",
      "Iteration 44240 Training loss 0.08667115122079849 Validation loss 0.11530989408493042 Accuracy 0.6840000152587891\n",
      "Iteration 44250 Training loss 0.09238116443157196 Validation loss 0.11458367854356766 Accuracy 0.6863333582878113\n",
      "Iteration 44260 Training loss 0.08598999679088593 Validation loss 0.11479329317808151 Accuracy 0.6856666803359985\n",
      "Iteration 44270 Training loss 0.0855390727519989 Validation loss 0.11561132967472076 Accuracy 0.684166669845581\n",
      "Iteration 44280 Training loss 0.08191134035587311 Validation loss 0.11572426557540894 Accuracy 0.6851666569709778\n",
      "Iteration 44290 Training loss 0.08443222939968109 Validation loss 0.11513090878725052 Accuracy 0.684166669845581\n",
      "Iteration 44300 Training loss 0.09078902006149292 Validation loss 0.11400210857391357 Accuracy 0.6880000233650208\n",
      "Iteration 44310 Training loss 0.08670416474342346 Validation loss 0.11404632776975632 Accuracy 0.6881666779518127\n",
      "Iteration 44320 Training loss 0.0878036618232727 Validation loss 0.11399446427822113 Accuracy 0.687833309173584\n",
      "Iteration 44330 Training loss 0.08802349120378494 Validation loss 0.11404412984848022 Accuracy 0.6863333582878113\n",
      "Iteration 44340 Training loss 0.08942861109972 Validation loss 0.11401497572660446 Accuracy 0.6880000233650208\n",
      "Iteration 44350 Training loss 0.08567313849925995 Validation loss 0.11512449383735657 Accuracy 0.6846666932106018\n",
      "Iteration 44360 Training loss 0.08408859372138977 Validation loss 0.11613357812166214 Accuracy 0.6830000281333923\n",
      "Iteration 44370 Training loss 0.08809465169906616 Validation loss 0.11409183591604233 Accuracy 0.6859999895095825\n",
      "Iteration 44380 Training loss 0.0851064920425415 Validation loss 0.1151236966252327 Accuracy 0.6851666569709778\n",
      "Iteration 44390 Training loss 0.08418440818786621 Validation loss 0.11656097322702408 Accuracy 0.6828333139419556\n",
      "Iteration 44400 Training loss 0.08637300878763199 Validation loss 0.11474186182022095 Accuracy 0.6859999895095825\n",
      "Iteration 44410 Training loss 0.08842587471008301 Validation loss 0.11428426206111908 Accuracy 0.6866666674613953\n",
      "Iteration 44420 Training loss 0.08579996973276138 Validation loss 0.11443015187978745 Accuracy 0.6863333582878113\n",
      "Iteration 44430 Training loss 0.08614741265773773 Validation loss 0.11532963067293167 Accuracy 0.6848333477973938\n",
      "Iteration 44440 Training loss 0.0847250372171402 Validation loss 0.11626824736595154 Accuracy 0.684333324432373\n",
      "Iteration 44450 Training loss 0.08533290773630142 Validation loss 0.1155993789434433 Accuracy 0.6825000047683716\n",
      "Iteration 44460 Training loss 0.0852460116147995 Validation loss 0.1143973171710968 Accuracy 0.6858333349227905\n",
      "Iteration 44470 Training loss 0.08983608335256577 Validation loss 0.11404688656330109 Accuracy 0.6875\n",
      "Iteration 44480 Training loss 0.0871470645070076 Validation loss 0.11433616280555725 Accuracy 0.6866666674613953\n",
      "Iteration 44490 Training loss 0.08510513603687286 Validation loss 0.1151619702577591 Accuracy 0.684499979019165\n",
      "Iteration 44500 Training loss 0.08666699379682541 Validation loss 0.11453613638877869 Accuracy 0.6855000257492065\n",
      "Iteration 44510 Training loss 0.08934829384088516 Validation loss 0.11405660212039948 Accuracy 0.6866666674613953\n",
      "Iteration 44520 Training loss 0.08881406486034393 Validation loss 0.11416909843683243 Accuracy 0.6856666803359985\n",
      "Iteration 44530 Training loss 0.0825815424323082 Validation loss 0.11569517850875854 Accuracy 0.6834999918937683\n",
      "Iteration 44540 Training loss 0.08394701778888702 Validation loss 0.11908036470413208 Accuracy 0.6806666851043701\n",
      "Iteration 44550 Training loss 0.08658672869205475 Validation loss 0.11466258764266968 Accuracy 0.6851666569709778\n",
      "Iteration 44560 Training loss 0.0897253006696701 Validation loss 0.11421651393175125 Accuracy 0.6869999766349792\n",
      "Iteration 44570 Training loss 0.08545741438865662 Validation loss 0.11526931077241898 Accuracy 0.6859999895095825\n",
      "Iteration 44580 Training loss 0.08542530983686447 Validation loss 0.11498457193374634 Accuracy 0.6853333115577698\n",
      "Iteration 44590 Training loss 0.08616471290588379 Validation loss 0.11797298491001129 Accuracy 0.6809999942779541\n",
      "Iteration 44600 Training loss 0.08510474860668182 Validation loss 0.11512739211320877 Accuracy 0.6859999895095825\n",
      "Iteration 44610 Training loss 0.09122363477945328 Validation loss 0.11426210403442383 Accuracy 0.6861666440963745\n",
      "Iteration 44620 Training loss 0.08830714225769043 Validation loss 0.11411749571561813 Accuracy 0.6868333220481873\n",
      "Iteration 44630 Training loss 0.08724033832550049 Validation loss 0.11452409625053406 Accuracy 0.6858333349227905\n",
      "Iteration 44640 Training loss 0.08806164562702179 Validation loss 0.1142655611038208 Accuracy 0.6863333582878113\n",
      "Iteration 44650 Training loss 0.08352931588888168 Validation loss 0.11770900338888168 Accuracy 0.6791666746139526\n",
      "Iteration 44660 Training loss 0.08482396602630615 Validation loss 0.12047602981328964 Accuracy 0.6781666874885559\n",
      "Iteration 44670 Training loss 0.08532008528709412 Validation loss 0.11594454199075699 Accuracy 0.6823333501815796\n",
      "Iteration 44680 Training loss 0.08501211553812027 Validation loss 0.11548327654600143 Accuracy 0.6840000152587891\n",
      "Iteration 44690 Training loss 0.08962614834308624 Validation loss 0.11403410136699677 Accuracy 0.6850000023841858\n",
      "Iteration 44700 Training loss 0.08932321518659592 Validation loss 0.11400382220745087 Accuracy 0.6851666569709778\n",
      "Iteration 44710 Training loss 0.08708348870277405 Validation loss 0.11402712017297745 Accuracy 0.684166669845581\n",
      "Iteration 44720 Training loss 0.0863233357667923 Validation loss 0.11437943577766418 Accuracy 0.6855000257492065\n",
      "Iteration 44730 Training loss 0.0845961794257164 Validation loss 0.11437074095010757 Accuracy 0.6868333220481873\n",
      "Iteration 44740 Training loss 0.08741070330142975 Validation loss 0.11433539539575577 Accuracy 0.6868333220481873\n",
      "Iteration 44750 Training loss 0.08639854937791824 Validation loss 0.11477937549352646 Accuracy 0.6858333349227905\n",
      "Iteration 44760 Training loss 0.08626367151737213 Validation loss 0.11445746570825577 Accuracy 0.6863333582878113\n",
      "Iteration 44770 Training loss 0.08615446090698242 Validation loss 0.11515744775533676 Accuracy 0.6848333477973938\n",
      "Iteration 44780 Training loss 0.08650023490190506 Validation loss 0.12350277602672577 Accuracy 0.6675000190734863\n",
      "Iteration 44790 Training loss 0.08821069449186325 Validation loss 0.13368119299411774 Accuracy 0.6480000019073486\n",
      "Iteration 44800 Training loss 0.08886528760194778 Validation loss 0.13903915882110596 Accuracy 0.6395000219345093\n",
      "Iteration 44810 Training loss 0.08656598627567291 Validation loss 0.13382494449615479 Accuracy 0.6491666436195374\n",
      "Iteration 44820 Training loss 0.08699022233486176 Validation loss 0.12683197855949402 Accuracy 0.6628333330154419\n",
      "Iteration 44830 Training loss 0.08816196769475937 Validation loss 0.13411585986614227 Accuracy 0.6481666564941406\n",
      "Iteration 44840 Training loss 0.08687224984169006 Validation loss 0.1310598999261856 Accuracy 0.6545000076293945\n",
      "Iteration 44850 Training loss 0.08663603663444519 Validation loss 0.13720668852329254 Accuracy 0.6441666483879089\n",
      "Iteration 44860 Training loss 0.08760873228311539 Validation loss 0.1338362991809845 Accuracy 0.6485000252723694\n",
      "Iteration 44870 Training loss 0.08439913392066956 Validation loss 0.12628357112407684 Accuracy 0.6638333201408386\n",
      "Iteration 44880 Training loss 0.087416872382164 Validation loss 0.13495492935180664 Accuracy 0.6480000019073486\n",
      "Iteration 44890 Training loss 0.08883421868085861 Validation loss 0.13999494910240173 Accuracy 0.6378333568572998\n",
      "Iteration 44900 Training loss 0.08908167481422424 Validation loss 0.14011681079864502 Accuracy 0.6401666402816772\n",
      "Iteration 44910 Training loss 0.08535390347242355 Validation loss 0.1272900402545929 Accuracy 0.6620000004768372\n",
      "Iteration 44920 Training loss 0.08558937907218933 Validation loss 0.13316917419433594 Accuracy 0.6501666903495789\n",
      "Iteration 44930 Training loss 0.08553768694400787 Validation loss 0.1303529292345047 Accuracy 0.6549999713897705\n",
      "Iteration 44940 Training loss 0.08487273752689362 Validation loss 0.13081130385398865 Accuracy 0.653333306312561\n",
      "Iteration 44950 Training loss 0.08785760402679443 Validation loss 0.1365661770105362 Accuracy 0.6456666588783264\n",
      "Iteration 44960 Training loss 0.08767614513635635 Validation loss 0.13290749490261078 Accuracy 0.6504999995231628\n",
      "Iteration 44970 Training loss 0.08529888093471527 Validation loss 0.13316568732261658 Accuracy 0.6504999995231628\n",
      "Iteration 44980 Training loss 0.08910723775625229 Validation loss 0.1379977911710739 Accuracy 0.6431666612625122\n",
      "Iteration 44990 Training loss 0.08710780739784241 Validation loss 0.1388729214668274 Accuracy 0.6413333415985107\n",
      "Iteration 45000 Training loss 0.08850757032632828 Validation loss 0.13379773497581482 Accuracy 0.6504999995231628\n",
      "Iteration 45010 Training loss 0.08349202573299408 Validation loss 0.12437112629413605 Accuracy 0.6664999723434448\n",
      "Iteration 45020 Training loss 0.0845705047249794 Validation loss 0.1275041699409485 Accuracy 0.6601666808128357\n",
      "Iteration 45030 Training loss 0.0851169303059578 Validation loss 0.1265585571527481 Accuracy 0.6635000109672546\n",
      "Iteration 45040 Training loss 0.09051148593425751 Validation loss 0.13678434491157532 Accuracy 0.6446666717529297\n",
      "Iteration 45050 Training loss 0.08426196873188019 Validation loss 0.13132132589817047 Accuracy 0.6538333296775818\n",
      "Iteration 45060 Training loss 0.08728129416704178 Validation loss 0.13512925803661346 Accuracy 0.6476666927337646\n",
      "Iteration 45070 Training loss 0.09088262915611267 Validation loss 0.13829120993614197 Accuracy 0.6418333053588867\n",
      "Iteration 45080 Training loss 0.08635589480400085 Validation loss 0.13471505045890808 Accuracy 0.6468333601951599\n",
      "Iteration 45090 Training loss 0.08861705660820007 Validation loss 0.13573665916919708 Accuracy 0.6464999914169312\n",
      "Iteration 45100 Training loss 0.08760864287614822 Validation loss 0.13433602452278137 Accuracy 0.6485000252723694\n",
      "Iteration 45110 Training loss 0.08560802042484283 Validation loss 0.12895146012306213 Accuracy 0.6570000052452087\n",
      "Iteration 45120 Training loss 0.08667343854904175 Validation loss 0.13712793588638306 Accuracy 0.6448333263397217\n",
      "Iteration 45130 Training loss 0.09127417206764221 Validation loss 0.13915285468101501 Accuracy 0.640666663646698\n",
      "Iteration 45140 Training loss 0.0861983671784401 Validation loss 0.13231894373893738 Accuracy 0.6524999737739563\n",
      "Iteration 45150 Training loss 0.0868043601512909 Validation loss 0.1351446956396103 Accuracy 0.6470000147819519\n",
      "Iteration 45160 Training loss 0.09100854396820068 Validation loss 0.14220890402793884 Accuracy 0.6331666707992554\n",
      "Iteration 45170 Training loss 0.08749369531869888 Validation loss 0.12950123846530914 Accuracy 0.6575000286102295\n",
      "Iteration 45180 Training loss 0.08649604022502899 Validation loss 0.1303488314151764 Accuracy 0.656333327293396\n",
      "Iteration 45190 Training loss 0.08935017883777618 Validation loss 0.13605038821697235 Accuracy 0.6468333601951599\n",
      "Iteration 45200 Training loss 0.09134401381015778 Validation loss 0.14325158298015594 Accuracy 0.6296666860580444\n",
      "Iteration 45210 Training loss 0.08794061094522476 Validation loss 0.13325634598731995 Accuracy 0.6499999761581421\n",
      "Iteration 45220 Training loss 0.0859721377491951 Validation loss 0.12625427544116974 Accuracy 0.6639999747276306\n",
      "Iteration 45230 Training loss 0.08442893624305725 Validation loss 0.12841399013996124 Accuracy 0.6585000157356262\n",
      "Iteration 45240 Training loss 0.08560798317193985 Validation loss 0.12759505212306976 Accuracy 0.659500002861023\n",
      "Iteration 45250 Training loss 0.08835367113351822 Validation loss 0.1342155933380127 Accuracy 0.6483333110809326\n",
      "Iteration 45260 Training loss 0.08915086090564728 Validation loss 0.13364721834659576 Accuracy 0.6489999890327454\n",
      "Iteration 45270 Training loss 0.08877838402986526 Validation loss 0.13638371229171753 Accuracy 0.6446666717529297\n",
      "Iteration 45280 Training loss 0.08739115297794342 Validation loss 0.13914233446121216 Accuracy 0.6399999856948853\n",
      "Iteration 45290 Training loss 0.08651109039783478 Validation loss 0.12611353397369385 Accuracy 0.6644999980926514\n",
      "Iteration 45300 Training loss 0.08452744781970978 Validation loss 0.12609121203422546 Accuracy 0.6625000238418579\n",
      "Iteration 45310 Training loss 0.08819654583930969 Validation loss 0.13384713232517242 Accuracy 0.6491666436195374\n",
      "Iteration 45320 Training loss 0.08923722058534622 Validation loss 0.1371033489704132 Accuracy 0.6441666483879089\n",
      "Iteration 45330 Training loss 0.0848766639828682 Validation loss 0.12450818717479706 Accuracy 0.6669999957084656\n",
      "Iteration 45340 Training loss 0.08580337464809418 Validation loss 0.13213090598583221 Accuracy 0.6510000228881836\n",
      "Iteration 45350 Training loss 0.09046417474746704 Validation loss 0.14112000167369843 Accuracy 0.6343333125114441\n",
      "Iteration 45360 Training loss 0.08302578330039978 Validation loss 0.12828180193901062 Accuracy 0.659500002861023\n",
      "Iteration 45370 Training loss 0.08638788759708405 Validation loss 0.1313493847846985 Accuracy 0.653333306312561\n",
      "Iteration 45380 Training loss 0.08735398203134537 Validation loss 0.11985214054584503 Accuracy 0.6779999732971191\n",
      "Iteration 45390 Training loss 0.08866788446903229 Validation loss 0.13967587053775787 Accuracy 0.6366666555404663\n",
      "Iteration 45400 Training loss 0.08831750601530075 Validation loss 0.13694240152835846 Accuracy 0.6441666483879089\n",
      "Iteration 45410 Training loss 0.08766880631446838 Validation loss 0.13939142227172852 Accuracy 0.6388333439826965\n",
      "Iteration 45420 Training loss 0.08790614455938339 Validation loss 0.13667552173137665 Accuracy 0.6456666588783264\n",
      "Iteration 45430 Training loss 0.08681956678628922 Validation loss 0.13548970222473145 Accuracy 0.6480000019073486\n",
      "Iteration 45440 Training loss 0.08671809732913971 Validation loss 0.12832094728946686 Accuracy 0.6600000262260437\n",
      "Iteration 45450 Training loss 0.0853031575679779 Validation loss 0.1345851570367813 Accuracy 0.6485000252723694\n",
      "Iteration 45460 Training loss 0.08955048769712448 Validation loss 0.13737711310386658 Accuracy 0.6434999704360962\n",
      "Iteration 45470 Training loss 0.09003718197345734 Validation loss 0.1404915153980255 Accuracy 0.6363333463668823\n",
      "Iteration 45480 Training loss 0.0847674086689949 Validation loss 0.1317111849784851 Accuracy 0.6520000100135803\n",
      "Iteration 45490 Training loss 0.08474548906087875 Validation loss 0.12510675191879272 Accuracy 0.6654999852180481\n",
      "Iteration 45500 Training loss 0.08840688318014145 Validation loss 0.12849591672420502 Accuracy 0.659333348274231\n",
      "Iteration 45510 Training loss 0.08833863586187363 Validation loss 0.13688936829566956 Accuracy 0.6441666483879089\n",
      "Iteration 45520 Training loss 0.0862460732460022 Validation loss 0.13245894014835358 Accuracy 0.6508333086967468\n",
      "Iteration 45530 Training loss 0.08800175040960312 Validation loss 0.1405412256717682 Accuracy 0.637333333492279\n",
      "Iteration 45540 Training loss 0.08781268447637558 Validation loss 0.1350935697555542 Accuracy 0.6485000252723694\n",
      "Iteration 45550 Training loss 0.08628977090120316 Validation loss 0.13357634842395782 Accuracy 0.6481666564941406\n",
      "Iteration 45560 Training loss 0.08616426587104797 Validation loss 0.13514557480812073 Accuracy 0.6478333473205566\n",
      "Iteration 45570 Training loss 0.0891256332397461 Validation loss 0.13301628828048706 Accuracy 0.6496666669845581\n",
      "Iteration 45580 Training loss 0.08644931763410568 Validation loss 0.13612431287765503 Accuracy 0.6456666588783264\n",
      "Iteration 45590 Training loss 0.08785223215818405 Validation loss 0.13223902881145477 Accuracy 0.6511666774749756\n",
      "Iteration 45600 Training loss 0.0877385139465332 Validation loss 0.13534194231033325 Accuracy 0.6481666564941406\n",
      "Iteration 45610 Training loss 0.08554407954216003 Validation loss 0.13341741263866425 Accuracy 0.6501666903495789\n",
      "Iteration 45620 Training loss 0.08863256126642227 Validation loss 0.13555175065994263 Accuracy 0.6481666564941406\n",
      "Iteration 45630 Training loss 0.08275394886732101 Validation loss 0.12331945449113846 Accuracy 0.6690000295639038\n",
      "Iteration 45640 Training loss 0.08446305990219116 Validation loss 0.11728982627391815 Accuracy 0.6813333630561829\n",
      "Iteration 45650 Training loss 0.0854424461722374 Validation loss 0.1153358593583107 Accuracy 0.6828333139419556\n",
      "Iteration 45660 Training loss 0.08706497400999069 Validation loss 0.1144082099199295 Accuracy 0.6859999895095825\n",
      "Iteration 45670 Training loss 0.09099090844392776 Validation loss 0.11424247175455093 Accuracy 0.6880000233650208\n",
      "Iteration 45680 Training loss 0.0865657776594162 Validation loss 0.11500899493694305 Accuracy 0.684333324432373\n",
      "Iteration 45690 Training loss 0.08600497245788574 Validation loss 0.11625765264034271 Accuracy 0.6836666464805603\n",
      "Iteration 45700 Training loss 0.08609852194786072 Validation loss 0.1142251044511795 Accuracy 0.6875\n",
      "Iteration 45710 Training loss 0.08303947746753693 Validation loss 0.11743418127298355 Accuracy 0.6821666955947876\n",
      "Iteration 45720 Training loss 0.08824773877859116 Validation loss 0.1141265481710434 Accuracy 0.6858333349227905\n",
      "Iteration 45730 Training loss 0.09170318394899368 Validation loss 0.11439303308725357 Accuracy 0.6880000233650208\n",
      "Iteration 45740 Training loss 0.08344204723834991 Validation loss 0.11901220679283142 Accuracy 0.6779999732971191\n",
      "Iteration 45750 Training loss 0.08250857889652252 Validation loss 0.118508480489254 Accuracy 0.6801666617393494\n",
      "Iteration 45760 Training loss 0.08460825681686401 Validation loss 0.11783815175294876 Accuracy 0.6798333525657654\n",
      "Iteration 45770 Training loss 0.08968812972307205 Validation loss 0.11439841985702515 Accuracy 0.6866666674613953\n",
      "Iteration 45780 Training loss 0.08731471747159958 Validation loss 0.11439083516597748 Accuracy 0.6853333115577698\n",
      "Iteration 45790 Training loss 0.09217938035726547 Validation loss 0.11414262652397156 Accuracy 0.6893333196640015\n",
      "Iteration 45800 Training loss 0.08859559893608093 Validation loss 0.11422111093997955 Accuracy 0.6875\n",
      "Iteration 45810 Training loss 0.08711414784193039 Validation loss 0.11420459300279617 Accuracy 0.6875\n",
      "Iteration 45820 Training loss 0.08565179258584976 Validation loss 0.11725527048110962 Accuracy 0.6803333163261414\n",
      "Iteration 45830 Training loss 0.08508335053920746 Validation loss 0.11626166105270386 Accuracy 0.6825000047683716\n",
      "Iteration 45840 Training loss 0.08289586007595062 Validation loss 0.11720980703830719 Accuracy 0.6804999709129333\n",
      "Iteration 45850 Training loss 0.09190976619720459 Validation loss 0.11464634537696838 Accuracy 0.6853333115577698\n",
      "Iteration 45860 Training loss 0.08670784533023834 Validation loss 0.11466743797063828 Accuracy 0.6868333220481873\n",
      "Iteration 45870 Training loss 0.08749350905418396 Validation loss 0.11594704538583755 Accuracy 0.6828333139419556\n",
      "Iteration 45880 Training loss 0.08625035732984543 Validation loss 0.1149774044752121 Accuracy 0.6833333373069763\n",
      "Iteration 45890 Training loss 0.08600563555955887 Validation loss 0.11604609340429306 Accuracy 0.6821666955947876\n",
      "Iteration 45900 Training loss 0.08861976116895676 Validation loss 0.11407063156366348 Accuracy 0.6880000233650208\n",
      "Iteration 45910 Training loss 0.08923668414354324 Validation loss 0.11420783400535583 Accuracy 0.687333345413208\n",
      "Iteration 45920 Training loss 0.09070540219545364 Validation loss 0.11421969532966614 Accuracy 0.6863333582878113\n",
      "Iteration 45930 Training loss 0.0820426419377327 Validation loss 0.11630808562040329 Accuracy 0.6836666464805603\n",
      "Iteration 45940 Training loss 0.08764626085758209 Validation loss 0.11624852567911148 Accuracy 0.6848333477973938\n",
      "Iteration 45950 Training loss 0.08918739855289459 Validation loss 0.11430428177118301 Accuracy 0.6868333220481873\n",
      "Iteration 45960 Training loss 0.08914025127887726 Validation loss 0.11409085988998413 Accuracy 0.6851666569709778\n",
      "Iteration 45970 Training loss 0.08534733206033707 Validation loss 0.11860105395317078 Accuracy 0.6788333058357239\n",
      "Iteration 45980 Training loss 0.08580503612756729 Validation loss 0.11533775180578232 Accuracy 0.6833333373069763\n",
      "Iteration 45990 Training loss 0.0848131850361824 Validation loss 0.11483263224363327 Accuracy 0.6855000257492065\n",
      "Iteration 46000 Training loss 0.09299617260694504 Validation loss 0.11451264470815659 Accuracy 0.687166690826416\n",
      "Iteration 46010 Training loss 0.08497735857963562 Validation loss 0.11726934462785721 Accuracy 0.6808333396911621\n",
      "Iteration 46020 Training loss 0.08373527973890305 Validation loss 0.11593768000602722 Accuracy 0.6836666464805603\n",
      "Iteration 46030 Training loss 0.08425263315439224 Validation loss 0.11754173785448074 Accuracy 0.6793333292007446\n",
      "Iteration 46040 Training loss 0.08841869235038757 Validation loss 0.11451947689056396 Accuracy 0.6858333349227905\n",
      "Iteration 46050 Training loss 0.08685043454170227 Validation loss 0.11512880772352219 Accuracy 0.684166669845581\n",
      "Iteration 46060 Training loss 0.08504355698823929 Validation loss 0.11439940333366394 Accuracy 0.6853333115577698\n",
      "Iteration 46070 Training loss 0.08858863264322281 Validation loss 0.11444006115198135 Accuracy 0.687666654586792\n",
      "Iteration 46080 Training loss 0.08745399117469788 Validation loss 0.11445311456918716 Accuracy 0.6865000128746033\n",
      "Iteration 46090 Training loss 0.08732637763023376 Validation loss 0.11501037329435349 Accuracy 0.6868333220481873\n",
      "Iteration 46100 Training loss 0.08763586729764938 Validation loss 0.11420758068561554 Accuracy 0.6868333220481873\n",
      "Iteration 46110 Training loss 0.08385192602872849 Validation loss 0.11766787618398666 Accuracy 0.6800000071525574\n",
      "Iteration 46120 Training loss 0.08339666575193405 Validation loss 0.11785590648651123 Accuracy 0.6813333630561829\n",
      "Iteration 46130 Training loss 0.08627419173717499 Validation loss 0.1148991510272026 Accuracy 0.6856666803359985\n",
      "Iteration 46140 Training loss 0.08998970687389374 Validation loss 0.11425371468067169 Accuracy 0.6858333349227905\n",
      "Iteration 46150 Training loss 0.08258789777755737 Validation loss 0.11688850820064545 Accuracy 0.6815000176429749\n",
      "Iteration 46160 Training loss 0.08854321390390396 Validation loss 0.11434989422559738 Accuracy 0.6865000128746033\n",
      "Iteration 46170 Training loss 0.08970804512500763 Validation loss 0.11414454877376556 Accuracy 0.687666654586792\n",
      "Iteration 46180 Training loss 0.0883883461356163 Validation loss 0.11420037597417831 Accuracy 0.6855000257492065\n",
      "Iteration 46190 Training loss 0.08201941102743149 Validation loss 0.11896970868110657 Accuracy 0.6791666746139526\n",
      "Iteration 46200 Training loss 0.08340117335319519 Validation loss 0.11549792438745499 Accuracy 0.6850000023841858\n",
      "Iteration 46210 Training loss 0.08818566054105759 Validation loss 0.11434225738048553 Accuracy 0.6888333559036255\n",
      "Iteration 46220 Training loss 0.08399487286806107 Validation loss 0.11523158848285675 Accuracy 0.6853333115577698\n",
      "Iteration 46230 Training loss 0.08673318475484848 Validation loss 0.11453147232532501 Accuracy 0.6869999766349792\n",
      "Iteration 46240 Training loss 0.08586812019348145 Validation loss 0.11559543758630753 Accuracy 0.6850000023841858\n",
      "Iteration 46250 Training loss 0.08680728077888489 Validation loss 0.11463159322738647 Accuracy 0.6866666674613953\n",
      "Iteration 46260 Training loss 0.0843644067645073 Validation loss 0.11560381948947906 Accuracy 0.6833333373069763\n",
      "Iteration 46270 Training loss 0.08660173416137695 Validation loss 0.11479955166578293 Accuracy 0.6866666674613953\n",
      "Iteration 46280 Training loss 0.08684392273426056 Validation loss 0.11463861167430878 Accuracy 0.6856666803359985\n",
      "Iteration 46290 Training loss 0.08902809768915176 Validation loss 0.11446105688810349 Accuracy 0.6850000023841858\n",
      "Iteration 46300 Training loss 0.08604620397090912 Validation loss 0.11483634263277054 Accuracy 0.6838333606719971\n",
      "Iteration 46310 Training loss 0.08543454110622406 Validation loss 0.11440358310937881 Accuracy 0.687333345413208\n",
      "Iteration 46320 Training loss 0.08647661656141281 Validation loss 0.11531709879636765 Accuracy 0.6838333606719971\n",
      "Iteration 46330 Training loss 0.08773475885391235 Validation loss 0.11501667648553848 Accuracy 0.6853333115577698\n",
      "Iteration 46340 Training loss 0.08989700675010681 Validation loss 0.11419454962015152 Accuracy 0.6868333220481873\n",
      "Iteration 46350 Training loss 0.08204275369644165 Validation loss 0.11646027863025665 Accuracy 0.6831666827201843\n",
      "Iteration 46360 Training loss 0.08627530932426453 Validation loss 0.11434639245271683 Accuracy 0.6856666803359985\n",
      "Iteration 46370 Training loss 0.0871933177113533 Validation loss 0.11427416652441025 Accuracy 0.6886666417121887\n",
      "Iteration 46380 Training loss 0.08643298596143723 Validation loss 0.11545629799365997 Accuracy 0.6834999918937683\n",
      "Iteration 46390 Training loss 0.08786287158727646 Validation loss 0.11432240158319473 Accuracy 0.687166690826416\n",
      "Iteration 46400 Training loss 0.08599031716585159 Validation loss 0.11480497568845749 Accuracy 0.6855000257492065\n",
      "Iteration 46410 Training loss 0.08501263707876205 Validation loss 0.11655323952436447 Accuracy 0.6813333630561829\n",
      "Iteration 46420 Training loss 0.08849553763866425 Validation loss 0.11440110206604004 Accuracy 0.687166690826416\n",
      "Iteration 46430 Training loss 0.08755393326282501 Validation loss 0.11466267704963684 Accuracy 0.6869999766349792\n",
      "Iteration 46440 Training loss 0.08413087576627731 Validation loss 0.11774282902479172 Accuracy 0.6803333163261414\n",
      "Iteration 46450 Training loss 0.08354384452104568 Validation loss 0.12638795375823975 Accuracy 0.6636666655540466\n",
      "Iteration 46460 Training loss 0.08679656684398651 Validation loss 0.13241922855377197 Accuracy 0.6510000228881836\n",
      "Iteration 46470 Training loss 0.08408939838409424 Validation loss 0.13231080770492554 Accuracy 0.6510000228881836\n",
      "Iteration 46480 Training loss 0.0871826782822609 Validation loss 0.13571348786354065 Accuracy 0.6463333368301392\n",
      "Iteration 46490 Training loss 0.08584882318973541 Validation loss 0.13662002980709076 Accuracy 0.6441666483879089\n",
      "Iteration 46500 Training loss 0.08465912193059921 Validation loss 0.13494452834129333 Accuracy 0.6480000019073486\n",
      "Iteration 46510 Training loss 0.08540104329586029 Validation loss 0.12983042001724243 Accuracy 0.6548333168029785\n",
      "Iteration 46520 Training loss 0.08823885768651962 Validation loss 0.1379883885383606 Accuracy 0.6418333053588867\n",
      "Iteration 46530 Training loss 0.08951181918382645 Validation loss 0.1405877023935318 Accuracy 0.6366666555404663\n",
      "Iteration 46540 Training loss 0.08777832984924316 Validation loss 0.13053886592388153 Accuracy 0.6539999842643738\n",
      "Iteration 46550 Training loss 0.08544494956731796 Validation loss 0.12505823373794556 Accuracy 0.6656666398048401\n",
      "Iteration 46560 Training loss 0.08625543862581253 Validation loss 0.1329345852136612 Accuracy 0.6504999995231628\n",
      "Iteration 46570 Training loss 0.08796431124210358 Validation loss 0.13694162666797638 Accuracy 0.6428333520889282\n",
      "Iteration 46580 Training loss 0.08476296812295914 Validation loss 0.12552571296691895 Accuracy 0.6650000214576721\n",
      "Iteration 46590 Training loss 0.08219882100820541 Validation loss 0.12288086116313934 Accuracy 0.6693333387374878\n",
      "Iteration 46600 Training loss 0.08645013719797134 Validation loss 0.12818573415279388 Accuracy 0.6601666808128357\n",
      "Iteration 46610 Training loss 0.08753552287817001 Validation loss 0.13516737520694733 Accuracy 0.6474999785423279\n",
      "Iteration 46620 Training loss 0.08753129839897156 Validation loss 0.13851338624954224 Accuracy 0.6416666507720947\n",
      "Iteration 46630 Training loss 0.08720327913761139 Validation loss 0.13446898758411407 Accuracy 0.6488333344459534\n",
      "Iteration 46640 Training loss 0.0858340859413147 Validation loss 0.1321714222431183 Accuracy 0.6504999995231628\n",
      "Iteration 46650 Training loss 0.08726613223552704 Validation loss 0.13356097042560577 Accuracy 0.6499999761581421\n",
      "Iteration 46660 Training loss 0.09049969166517258 Validation loss 0.13949017226696014 Accuracy 0.6393333077430725\n",
      "Iteration 46670 Training loss 0.08940917998552322 Validation loss 0.1398518979549408 Accuracy 0.6393333077430725\n",
      "Iteration 46680 Training loss 0.08780311793088913 Validation loss 0.13415783643722534 Accuracy 0.6496666669845581\n",
      "Iteration 46690 Training loss 0.0867161899805069 Validation loss 0.1376693695783615 Accuracy 0.643666684627533\n",
      "Iteration 46700 Training loss 0.08770963549613953 Validation loss 0.13471265137195587 Accuracy 0.6476666927337646\n",
      "Iteration 46710 Training loss 0.08734726905822754 Validation loss 0.13743169605731964 Accuracy 0.6439999938011169\n",
      "Iteration 46720 Training loss 0.08747604489326477 Validation loss 0.13389471173286438 Accuracy 0.6496666669845581\n",
      "Iteration 46730 Training loss 0.08601974695920944 Validation loss 0.1300906538963318 Accuracy 0.6549999713897705\n",
      "Iteration 46740 Training loss 0.08298979699611664 Validation loss 0.12599916756153107 Accuracy 0.6648333072662354\n",
      "Iteration 46750 Training loss 0.08509128540754318 Validation loss 0.13126973807811737 Accuracy 0.6523333191871643\n",
      "Iteration 46760 Training loss 0.08955324441194534 Validation loss 0.13643808662891388 Accuracy 0.6466666460037231\n",
      "Iteration 46770 Training loss 0.08745957165956497 Validation loss 0.12655465304851532 Accuracy 0.6621666550636292\n",
      "Iteration 46780 Training loss 0.08505169302225113 Validation loss 0.12785546481609344 Accuracy 0.6606666445732117\n",
      "Iteration 46790 Training loss 0.09157281368970871 Validation loss 0.14328034222126007 Accuracy 0.6296666860580444\n",
      "Iteration 46800 Training loss 0.0855196937918663 Validation loss 0.12720240652561188 Accuracy 0.6621666550636292\n",
      "Iteration 46810 Training loss 0.08787284046411514 Validation loss 0.13662558794021606 Accuracy 0.6455000042915344\n",
      "Iteration 46820 Training loss 0.08711560070514679 Validation loss 0.13159364461898804 Accuracy 0.6524999737739563\n",
      "Iteration 46830 Training loss 0.0835348442196846 Validation loss 0.12901239097118378 Accuracy 0.6586666703224182\n",
      "Iteration 46840 Training loss 0.08781258016824722 Validation loss 0.135360449552536 Accuracy 0.6456666588783264\n",
      "Iteration 46850 Training loss 0.0882222056388855 Validation loss 0.14087462425231934 Accuracy 0.6351666450500488\n",
      "Iteration 46860 Training loss 0.08762544393539429 Validation loss 0.13216695189476013 Accuracy 0.6520000100135803\n",
      "Iteration 46870 Training loss 0.09102997183799744 Validation loss 0.13688594102859497 Accuracy 0.6448333263397217\n",
      "Iteration 46880 Training loss 0.08260677009820938 Validation loss 0.1302342265844345 Accuracy 0.656000018119812\n",
      "Iteration 46890 Training loss 0.08732203394174576 Validation loss 0.12958528101444244 Accuracy 0.6579999923706055\n",
      "Iteration 46900 Training loss 0.08543481677770615 Validation loss 0.12773917615413666 Accuracy 0.6606666445732117\n",
      "Iteration 46910 Training loss 0.08535978198051453 Validation loss 0.13294099271297455 Accuracy 0.6495000123977661\n",
      "Iteration 46920 Training loss 0.08487576991319656 Validation loss 0.12775841355323792 Accuracy 0.6608333587646484\n",
      "Iteration 46930 Training loss 0.08382852375507355 Validation loss 0.13160766661167145 Accuracy 0.6516666412353516\n",
      "Iteration 46940 Training loss 0.08720214664936066 Validation loss 0.1329478621482849 Accuracy 0.6511666774749756\n",
      "Iteration 46950 Training loss 0.08577733486890793 Validation loss 0.13475006818771362 Accuracy 0.6486666798591614\n",
      "Iteration 46960 Training loss 0.08932332694530487 Validation loss 0.13879215717315674 Accuracy 0.640666663646698\n",
      "Iteration 46970 Training loss 0.08652694523334503 Validation loss 0.13078375160694122 Accuracy 0.6535000205039978\n",
      "Iteration 46980 Training loss 0.08541309833526611 Validation loss 0.12407157570123672 Accuracy 0.6678333282470703\n",
      "Iteration 46990 Training loss 0.08539237827062607 Validation loss 0.13344445824623108 Accuracy 0.6496666669845581\n",
      "Iteration 47000 Training loss 0.08695441484451294 Validation loss 0.13645397126674652 Accuracy 0.6463333368301392\n",
      "Iteration 47010 Training loss 0.09050469100475311 Validation loss 0.14317579567432404 Accuracy 0.6296666860580444\n",
      "Iteration 47020 Training loss 0.08872314542531967 Validation loss 0.1358029544353485 Accuracy 0.6480000019073486\n",
      "Iteration 47030 Training loss 0.0866197720170021 Validation loss 0.1321866363286972 Accuracy 0.6513333320617676\n",
      "Iteration 47040 Training loss 0.08450553566217422 Validation loss 0.13004259765148163 Accuracy 0.656333327293396\n",
      "Iteration 47050 Training loss 0.08412902057170868 Validation loss 0.12665049731731415 Accuracy 0.6641666889190674\n",
      "Iteration 47060 Training loss 0.08485858142375946 Validation loss 0.12994542717933655 Accuracy 0.656499981880188\n",
      "Iteration 47070 Training loss 0.08947263658046722 Validation loss 0.1363145112991333 Accuracy 0.6470000147819519\n",
      "Iteration 47080 Training loss 0.08361164480447769 Validation loss 0.12468123435974121 Accuracy 0.6651666760444641\n",
      "Iteration 47090 Training loss 0.0844072550535202 Validation loss 0.12411683797836304 Accuracy 0.6666666865348816\n",
      "Iteration 47100 Training loss 0.08375903218984604 Validation loss 0.1315314769744873 Accuracy 0.6536666750907898\n",
      "Iteration 47110 Training loss 0.09118637442588806 Validation loss 0.14177659153938293 Accuracy 0.6333333253860474\n",
      "Iteration 47120 Training loss 0.08861438184976578 Validation loss 0.13558489084243774 Accuracy 0.6488333344459534\n",
      "Iteration 47130 Training loss 0.0887908935546875 Validation loss 0.13107721507549286 Accuracy 0.6543333530426025\n",
      "Iteration 47140 Training loss 0.0885295420885086 Validation loss 0.13343468308448792 Accuracy 0.6503333449363708\n",
      "Iteration 47150 Training loss 0.0867806226015091 Validation loss 0.13759519159793854 Accuracy 0.6441666483879089\n",
      "Iteration 47160 Training loss 0.0855269581079483 Validation loss 0.1310260146856308 Accuracy 0.6541666388511658\n",
      "Iteration 47170 Training loss 0.08419724553823471 Validation loss 0.1277334839105606 Accuracy 0.6611666679382324\n",
      "Iteration 47180 Training loss 0.08934919536113739 Validation loss 0.13773754239082336 Accuracy 0.643833339214325\n",
      "Iteration 47190 Training loss 0.08411236852407455 Validation loss 0.12218327075242996 Accuracy 0.6704999804496765\n",
      "Iteration 47200 Training loss 0.08464715629816055 Validation loss 0.11766865849494934 Accuracy 0.6804999709129333\n",
      "Iteration 47210 Training loss 0.08351762592792511 Validation loss 0.11655367910861969 Accuracy 0.6833333373069763\n",
      "Iteration 47220 Training loss 0.08613073080778122 Validation loss 0.1142636239528656 Accuracy 0.687166690826416\n",
      "Iteration 47230 Training loss 0.09241468459367752 Validation loss 0.1144212856888771 Accuracy 0.6858333349227905\n",
      "Iteration 47240 Training loss 0.08631864190101624 Validation loss 0.11433739215135574 Accuracy 0.6868333220481873\n",
      "Iteration 47250 Training loss 0.08360613137483597 Validation loss 0.11752142012119293 Accuracy 0.6801666617393494\n",
      "Iteration 47260 Training loss 0.08539890497922897 Validation loss 0.11497253179550171 Accuracy 0.6866666674613953\n",
      "Iteration 47270 Training loss 0.08855582773685455 Validation loss 0.11467910557985306 Accuracy 0.6863333582878113\n",
      "Iteration 47280 Training loss 0.08507385104894638 Validation loss 0.11531848460435867 Accuracy 0.6859999895095825\n",
      "Iteration 47290 Training loss 0.08647167682647705 Validation loss 0.11432959884405136 Accuracy 0.687333345413208\n",
      "Iteration 47300 Training loss 0.08404652029275894 Validation loss 0.11774653196334839 Accuracy 0.6793333292007446\n",
      "Iteration 47310 Training loss 0.08774435520172119 Validation loss 0.11451192200183868 Accuracy 0.6886666417121887\n",
      "Iteration 47320 Training loss 0.0885644406080246 Validation loss 0.11434081941843033 Accuracy 0.6866666674613953\n",
      "Iteration 47330 Training loss 0.08870819211006165 Validation loss 0.11465279012918472 Accuracy 0.6880000233650208\n",
      "Iteration 47340 Training loss 0.08456317335367203 Validation loss 0.11595896631479263 Accuracy 0.684333324432373\n",
      "Iteration 47350 Training loss 0.08431734889745712 Validation loss 0.11607574671506882 Accuracy 0.6823333501815796\n",
      "Iteration 47360 Training loss 0.09212292730808258 Validation loss 0.11430415511131287 Accuracy 0.6888333559036255\n",
      "Iteration 47370 Training loss 0.08700739592313766 Validation loss 0.11466728895902634 Accuracy 0.687166690826416\n",
      "Iteration 47380 Training loss 0.08788426965475082 Validation loss 0.11445161700248718 Accuracy 0.6858333349227905\n",
      "Iteration 47390 Training loss 0.08462560176849365 Validation loss 0.11662695556879044 Accuracy 0.6818333268165588\n",
      "Iteration 47400 Training loss 0.08294819295406342 Validation loss 0.11894242465496063 Accuracy 0.6800000071525574\n",
      "Iteration 47410 Training loss 0.08708295971155167 Validation loss 0.11693564802408218 Accuracy 0.6806666851043701\n",
      "Iteration 47420 Training loss 0.08439817279577255 Validation loss 0.1151871457695961 Accuracy 0.6848333477973938\n",
      "Iteration 47430 Training loss 0.08676210790872574 Validation loss 0.11461829394102097 Accuracy 0.6861666440963745\n",
      "Iteration 47440 Training loss 0.08386354148387909 Validation loss 0.11490695923566818 Accuracy 0.6866666674613953\n",
      "Iteration 47450 Training loss 0.08477785438299179 Validation loss 0.11570463329553604 Accuracy 0.6851666569709778\n",
      "Iteration 47460 Training loss 0.08886533975601196 Validation loss 0.11450591683387756 Accuracy 0.6866666674613953\n",
      "Iteration 47470 Training loss 0.08579695969820023 Validation loss 0.11543641984462738 Accuracy 0.6861666440963745\n",
      "Iteration 47480 Training loss 0.08692779392004013 Validation loss 0.11448246240615845 Accuracy 0.687333345413208\n",
      "Iteration 47490 Training loss 0.08283332735300064 Validation loss 0.11730463802814484 Accuracy 0.6811666488647461\n",
      "Iteration 47500 Training loss 0.08653172105550766 Validation loss 0.118593730032444 Accuracy 0.6800000071525574\n",
      "Iteration 47510 Training loss 0.09112439304590225 Validation loss 0.11420237272977829 Accuracy 0.6893333196640015\n",
      "Iteration 47520 Training loss 0.0882682278752327 Validation loss 0.1143365278840065 Accuracy 0.687833309173584\n",
      "Iteration 47530 Training loss 0.08590114116668701 Validation loss 0.1149028092622757 Accuracy 0.6865000128746033\n",
      "Iteration 47540 Training loss 0.08576852083206177 Validation loss 0.11615730077028275 Accuracy 0.6851666569709778\n",
      "Iteration 47550 Training loss 0.08692646026611328 Validation loss 0.1149875670671463 Accuracy 0.6850000023841858\n",
      "Iteration 47560 Training loss 0.08873865008354187 Validation loss 0.11438043415546417 Accuracy 0.6865000128746033\n",
      "Iteration 47570 Training loss 0.08589719980955124 Validation loss 0.11443175375461578 Accuracy 0.6863333582878113\n",
      "Iteration 47580 Training loss 0.08425641059875488 Validation loss 0.1165156364440918 Accuracy 0.684166669845581\n",
      "Iteration 47590 Training loss 0.08846286684274673 Validation loss 0.1143835261464119 Accuracy 0.6881666779518127\n",
      "Iteration 47600 Training loss 0.08952929824590683 Validation loss 0.11433713138103485 Accuracy 0.6855000257492065\n",
      "Iteration 47610 Training loss 0.08347546309232712 Validation loss 0.11611580103635788 Accuracy 0.6838333606719971\n",
      "Iteration 47620 Training loss 0.0825948417186737 Validation loss 0.11911820620298386 Accuracy 0.6791666746139526\n",
      "Iteration 47630 Training loss 0.08289632946252823 Validation loss 0.11772169917821884 Accuracy 0.6790000200271606\n",
      "Iteration 47640 Training loss 0.08701814711093903 Validation loss 0.11908036470413208 Accuracy 0.6811666488647461\n",
      "Iteration 47650 Training loss 0.08634886890649796 Validation loss 0.11470188200473785 Accuracy 0.6865000128746033\n",
      "Iteration 47660 Training loss 0.08897596597671509 Validation loss 0.11452453583478928 Accuracy 0.687166690826416\n",
      "Iteration 47670 Training loss 0.08871544152498245 Validation loss 0.11425724625587463 Accuracy 0.6863333582878113\n",
      "Iteration 47680 Training loss 0.08738496154546738 Validation loss 0.11471805721521378 Accuracy 0.6858333349227905\n",
      "Iteration 47690 Training loss 0.08749298751354218 Validation loss 0.11448004841804504 Accuracy 0.6865000128746033\n",
      "Iteration 47700 Training loss 0.08670001477003098 Validation loss 0.11473587900400162 Accuracy 0.6858333349227905\n",
      "Iteration 47710 Training loss 0.09055539965629578 Validation loss 0.11437522619962692 Accuracy 0.687833309173584\n",
      "Iteration 47720 Training loss 0.08463791757822037 Validation loss 0.11464139074087143 Accuracy 0.687333345413208\n",
      "Iteration 47730 Training loss 0.08908475935459137 Validation loss 0.11450748890638351 Accuracy 0.6868333220481873\n",
      "Iteration 47740 Training loss 0.0878559798002243 Validation loss 0.11429734528064728 Accuracy 0.6865000128746033\n",
      "Iteration 47750 Training loss 0.08889519423246384 Validation loss 0.1154356449842453 Accuracy 0.6853333115577698\n",
      "Iteration 47760 Training loss 0.08324743807315826 Validation loss 0.11608824878931046 Accuracy 0.684499979019165\n",
      "Iteration 47770 Training loss 0.08465585112571716 Validation loss 0.11697493493556976 Accuracy 0.6800000071525574\n",
      "Iteration 47780 Training loss 0.08459419012069702 Validation loss 0.11624067276716232 Accuracy 0.6834999918937683\n",
      "Iteration 47790 Training loss 0.08511651307344437 Validation loss 0.11577665060758591 Accuracy 0.6838333606719971\n",
      "Iteration 47800 Training loss 0.08631348609924316 Validation loss 0.11802921444177628 Accuracy 0.6808333396911621\n",
      "Iteration 47810 Training loss 0.08988414704799652 Validation loss 0.11407925188541412 Accuracy 0.687833309173584\n",
      "Iteration 47820 Training loss 0.08663926273584366 Validation loss 0.11523010581731796 Accuracy 0.6855000257492065\n",
      "Iteration 47830 Training loss 0.08742952346801758 Validation loss 0.11467628926038742 Accuracy 0.6856666803359985\n",
      "Iteration 47840 Training loss 0.0853646844625473 Validation loss 0.1143643781542778 Accuracy 0.6858333349227905\n",
      "Iteration 47850 Training loss 0.08776893466711044 Validation loss 0.11420802772045135 Accuracy 0.6880000233650208\n",
      "Iteration 47860 Training loss 0.08936727792024612 Validation loss 0.11442061513662338 Accuracy 0.687333345413208\n",
      "Iteration 47870 Training loss 0.08625852316617966 Validation loss 0.11445426195859909 Accuracy 0.6863333582878113\n",
      "Iteration 47880 Training loss 0.08548396825790405 Validation loss 0.11608162522315979 Accuracy 0.6838333606719971\n",
      "Iteration 47890 Training loss 0.08569235354661942 Validation loss 0.11431703716516495 Accuracy 0.687666654586792\n",
      "Iteration 47900 Training loss 0.08682218194007874 Validation loss 0.1144755482673645 Accuracy 0.687833309173584\n",
      "Iteration 47910 Training loss 0.08694295585155487 Validation loss 0.11470301449298859 Accuracy 0.6853333115577698\n",
      "Iteration 47920 Training loss 0.09136021882295609 Validation loss 0.11425244808197021 Accuracy 0.687166690826416\n",
      "Iteration 47930 Training loss 0.08695437759160995 Validation loss 0.11437781900167465 Accuracy 0.6853333115577698\n",
      "Iteration 47940 Training loss 0.08440151810646057 Validation loss 0.11944348365068436 Accuracy 0.6801666617393494\n",
      "Iteration 47950 Training loss 0.083680160343647 Validation loss 0.1279904842376709 Accuracy 0.6600000262260437\n",
      "Iteration 47960 Training loss 0.09198470413684845 Validation loss 0.14304031431674957 Accuracy 0.6306666731834412\n",
      "Iteration 47970 Training loss 0.08550218492746353 Validation loss 0.13462601602077484 Accuracy 0.6483333110809326\n",
      "Iteration 47980 Training loss 0.08656179159879684 Validation loss 0.13528501987457275 Accuracy 0.6468333601951599\n",
      "Iteration 47990 Training loss 0.08949575573205948 Validation loss 0.13913987576961517 Accuracy 0.640500009059906\n",
      "Iteration 48000 Training loss 0.08749060332775116 Validation loss 0.13378679752349854 Accuracy 0.6488333344459534\n",
      "Iteration 48010 Training loss 0.0861724242568016 Validation loss 0.13380379974842072 Accuracy 0.6496666669845581\n",
      "Iteration 48020 Training loss 0.08911556005477905 Validation loss 0.1368354856967926 Accuracy 0.6460000276565552\n",
      "Iteration 48030 Training loss 0.08190032839775085 Validation loss 0.12211887538433075 Accuracy 0.67166668176651\n",
      "Iteration 48040 Training loss 0.08664558827877045 Validation loss 0.13507382571697235 Accuracy 0.6473333239555359\n",
      "Iteration 48050 Training loss 0.08931367844343185 Validation loss 0.14156872034072876 Accuracy 0.6359999775886536\n",
      "Iteration 48060 Training loss 0.08744647353887558 Validation loss 0.1333530843257904 Accuracy 0.6495000123977661\n",
      "Iteration 48070 Training loss 0.08798470348119736 Validation loss 0.13448840379714966 Accuracy 0.6483333110809326\n",
      "Iteration 48080 Training loss 0.08598301559686661 Validation loss 0.136564701795578 Accuracy 0.6451666951179504\n",
      "Iteration 48090 Training loss 0.08424929529428482 Validation loss 0.13169755041599274 Accuracy 0.653333306312561\n",
      "Iteration 48100 Training loss 0.08387921005487442 Validation loss 0.13154029846191406 Accuracy 0.6543333530426025\n",
      "Iteration 48110 Training loss 0.08553458750247955 Validation loss 0.13419122993946075 Accuracy 0.6481666564941406\n",
      "Iteration 48120 Training loss 0.08707188814878464 Validation loss 0.13436554372310638 Accuracy 0.6488333344459534\n",
      "Iteration 48130 Training loss 0.08736000210046768 Validation loss 0.14103440940380096 Accuracy 0.6359999775886536\n",
      "Iteration 48140 Training loss 0.08443991839885712 Validation loss 0.12247733771800995 Accuracy 0.6729999780654907\n",
      "Iteration 48150 Training loss 0.0882871076464653 Validation loss 0.13167478144168854 Accuracy 0.6520000100135803\n",
      "Iteration 48160 Training loss 0.08636201173067093 Validation loss 0.13876816630363464 Accuracy 0.6401666402816772\n",
      "Iteration 48170 Training loss 0.08355362713336945 Validation loss 0.1321796476840973 Accuracy 0.6516666412353516\n",
      "Iteration 48180 Training loss 0.08533915132284164 Validation loss 0.13021178543567657 Accuracy 0.6556666493415833\n",
      "Iteration 48190 Training loss 0.0884101539850235 Validation loss 0.13562747836112976 Accuracy 0.6471666693687439\n",
      "Iteration 48200 Training loss 0.08817468583583832 Validation loss 0.1404242366552353 Accuracy 0.6378333568572998\n",
      "Iteration 48210 Training loss 0.08926135301589966 Validation loss 0.13738252222537994 Accuracy 0.643833339214325\n",
      "Iteration 48220 Training loss 0.08492816984653473 Validation loss 0.13442915678024292 Accuracy 0.6481666564941406\n",
      "Iteration 48230 Training loss 0.08673299103975296 Validation loss 0.1317935436964035 Accuracy 0.6539999842643738\n",
      "Iteration 48240 Training loss 0.08837684988975525 Validation loss 0.1323089450597763 Accuracy 0.6526666879653931\n",
      "Iteration 48250 Training loss 0.0840858593583107 Validation loss 0.1274036020040512 Accuracy 0.6625000238418579\n",
      "Iteration 48260 Training loss 0.08715285360813141 Validation loss 0.13931375741958618 Accuracy 0.6388333439826965\n",
      "Iteration 48270 Training loss 0.08843467384576797 Validation loss 0.13489258289337158 Accuracy 0.6474999785423279\n",
      "Iteration 48280 Training loss 0.08586307615041733 Validation loss 0.13344301283359528 Accuracy 0.6506666541099548\n",
      "Iteration 48290 Training loss 0.0848730280995369 Validation loss 0.12833870947360992 Accuracy 0.6598333120346069\n",
      "Iteration 48300 Training loss 0.08531747758388519 Validation loss 0.13044589757919312 Accuracy 0.6548333168029785\n",
      "Iteration 48310 Training loss 0.08480232954025269 Validation loss 0.1314292848110199 Accuracy 0.6541666388511658\n",
      "Iteration 48320 Training loss 0.08985137939453125 Validation loss 0.14328926801681519 Accuracy 0.6305000185966492\n",
      "Iteration 48330 Training loss 0.08588799089193344 Validation loss 0.13216890394687653 Accuracy 0.653166651725769\n",
      "Iteration 48340 Training loss 0.08209086954593658 Validation loss 0.12371490150690079 Accuracy 0.6685000061988831\n",
      "Iteration 48350 Training loss 0.08502869307994843 Validation loss 0.13302460312843323 Accuracy 0.6510000228881836\n",
      "Iteration 48360 Training loss 0.08709603548049927 Validation loss 0.13586309552192688 Accuracy 0.6473333239555359\n",
      "Iteration 48370 Training loss 0.08309273421764374 Validation loss 0.12854932248592377 Accuracy 0.6601666808128357\n",
      "Iteration 48380 Training loss 0.08839400112628937 Validation loss 0.13712313771247864 Accuracy 0.643833339214325\n",
      "Iteration 48390 Training loss 0.08438928425312042 Validation loss 0.12862345576286316 Accuracy 0.659333348274231\n",
      "Iteration 48400 Training loss 0.08645861595869064 Validation loss 0.1289282888174057 Accuracy 0.659333348274231\n",
      "Iteration 48410 Training loss 0.08671925961971283 Validation loss 0.13647998869419098 Accuracy 0.6456666588783264\n",
      "Iteration 48420 Training loss 0.08835253864526749 Validation loss 0.13571573793888092 Accuracy 0.6466666460037231\n",
      "Iteration 48430 Training loss 0.09046656638383865 Validation loss 0.13929243385791779 Accuracy 0.6401666402816772\n",
      "Iteration 48440 Training loss 0.08519096672534943 Validation loss 0.12854963541030884 Accuracy 0.659500002861023\n",
      "Iteration 48450 Training loss 0.08307576179504395 Validation loss 0.12900319695472717 Accuracy 0.6583333611488342\n",
      "Iteration 48460 Training loss 0.08822200447320938 Validation loss 0.1357748955488205 Accuracy 0.6470000147819519\n",
      "Iteration 48470 Training loss 0.08373403549194336 Validation loss 0.13035406172275543 Accuracy 0.656000018119812\n",
      "Iteration 48480 Training loss 0.08477774262428284 Validation loss 0.12231790274381638 Accuracy 0.6735000014305115\n",
      "Iteration 48490 Training loss 0.08797424286603928 Validation loss 0.13070973753929138 Accuracy 0.6543333530426025\n",
      "Iteration 48500 Training loss 0.08604258298873901 Validation loss 0.1258564293384552 Accuracy 0.6641666889190674\n",
      "Iteration 48510 Training loss 0.09073929488658905 Validation loss 0.14545418322086334 Accuracy 0.6274999976158142\n",
      "Iteration 48520 Training loss 0.08636140078306198 Validation loss 0.13847751915454865 Accuracy 0.6426666378974915\n",
      "Iteration 48530 Training loss 0.08499712496995926 Validation loss 0.13525575399398804 Accuracy 0.6481666564941406\n",
      "Iteration 48540 Training loss 0.08740843832492828 Validation loss 0.1380498707294464 Accuracy 0.6433333158493042\n",
      "Iteration 48550 Training loss 0.08529875427484512 Validation loss 0.1339113861322403 Accuracy 0.6486666798591614\n",
      "Iteration 48560 Training loss 0.08740315586328506 Validation loss 0.13665087521076202 Accuracy 0.6463333368301392\n",
      "Iteration 48570 Training loss 0.08651149272918701 Validation loss 0.13092131912708282 Accuracy 0.6539999842643738\n",
      "Iteration 48580 Training loss 0.09056834131479263 Validation loss 0.1421782374382019 Accuracy 0.6334999799728394\n",
      "Iteration 48590 Training loss 0.08433512598276138 Validation loss 0.13173986971378326 Accuracy 0.653333306312561\n",
      "Iteration 48600 Training loss 0.08538609743118286 Validation loss 0.13418132066726685 Accuracy 0.6488333344459534\n",
      "Iteration 48610 Training loss 0.08512174338102341 Validation loss 0.13227225840091705 Accuracy 0.6514999866485596\n",
      "Iteration 48620 Training loss 0.0890473946928978 Validation loss 0.1404338926076889 Accuracy 0.6384999752044678\n",
      "Iteration 48630 Training loss 0.08872289955615997 Validation loss 0.13816401362419128 Accuracy 0.643666684627533\n",
      "Iteration 48640 Training loss 0.08479990810155869 Validation loss 0.13088995218276978 Accuracy 0.6548333168029785\n",
      "Iteration 48650 Training loss 0.0821041464805603 Validation loss 0.12823569774627686 Accuracy 0.6586666703224182\n",
      "Iteration 48660 Training loss 0.08795981854200363 Validation loss 0.13589392602443695 Accuracy 0.6461666822433472\n",
      "Iteration 48670 Training loss 0.08624977618455887 Validation loss 0.13151413202285767 Accuracy 0.653333306312561\n",
      "Iteration 48680 Training loss 0.08705826848745346 Validation loss 0.13390769064426422 Accuracy 0.6488333344459534\n",
      "Iteration 48690 Training loss 0.08766691386699677 Validation loss 0.1315363198518753 Accuracy 0.6538333296775818\n",
      "Iteration 48700 Training loss 0.09024902433156967 Validation loss 0.14207033812999725 Accuracy 0.6331666707992554\n",
      "Iteration 48710 Training loss 0.08496833592653275 Validation loss 0.1302090287208557 Accuracy 0.6571666598320007\n",
      "Iteration 48720 Training loss 0.08479014784097672 Validation loss 0.12731406092643738 Accuracy 0.6625000238418579\n",
      "Iteration 48730 Training loss 0.0858103558421135 Validation loss 0.12978392839431763 Accuracy 0.6566666960716248\n",
      "Iteration 48740 Training loss 0.08479592204093933 Validation loss 0.1361646056175232 Accuracy 0.6449999809265137\n",
      "Iteration 48750 Training loss 0.08876192569732666 Validation loss 0.13969461619853973 Accuracy 0.6380000114440918\n",
      "Iteration 48760 Training loss 0.08727029711008072 Validation loss 0.1303512156009674 Accuracy 0.6536666750907898\n",
      "Iteration 48770 Training loss 0.08761163055896759 Validation loss 0.13628718256950378 Accuracy 0.6458333134651184\n",
      "Iteration 48780 Training loss 0.08465863019227982 Validation loss 0.1338929384946823 Accuracy 0.6488333344459534\n",
      "Iteration 48790 Training loss 0.08852648735046387 Validation loss 0.14133243262767792 Accuracy 0.6333333253860474\n",
      "Iteration 48800 Training loss 0.08913950622081757 Validation loss 0.1393299102783203 Accuracy 0.6398333311080933\n",
      "Iteration 48810 Training loss 0.08610693365335464 Validation loss 0.1346411257982254 Accuracy 0.6474999785423279\n",
      "Iteration 48820 Training loss 0.08638723939657211 Validation loss 0.13711780309677124 Accuracy 0.6423333287239075\n",
      "Iteration 48830 Training loss 0.08429227769374847 Validation loss 0.1324700564146042 Accuracy 0.6520000100135803\n",
      "Iteration 48840 Training loss 0.08547241240739822 Validation loss 0.13503412902355194 Accuracy 0.6474999785423279\n",
      "Iteration 48850 Training loss 0.08567067235708237 Validation loss 0.13348060846328735 Accuracy 0.6498333215713501\n",
      "Iteration 48860 Training loss 0.08947373181581497 Validation loss 0.14264003932476044 Accuracy 0.6334999799728394\n",
      "Iteration 48870 Training loss 0.0864926353096962 Validation loss 0.13551843166351318 Accuracy 0.6478333473205566\n",
      "Iteration 48880 Training loss 0.08584414422512054 Validation loss 0.13330717384815216 Accuracy 0.6504999995231628\n",
      "Iteration 48890 Training loss 0.08473166823387146 Validation loss 0.12357723712921143 Accuracy 0.668833315372467\n",
      "Iteration 48900 Training loss 0.08435966074466705 Validation loss 0.12378989905118942 Accuracy 0.668666660785675\n",
      "Iteration 48910 Training loss 0.0842779353260994 Validation loss 0.12469959259033203 Accuracy 0.6663333177566528\n",
      "Iteration 48920 Training loss 0.08766496926546097 Validation loss 0.1313415914773941 Accuracy 0.6526666879653931\n",
      "Iteration 48930 Training loss 0.09136272221803665 Validation loss 0.1434260904788971 Accuracy 0.6308333277702332\n",
      "Iteration 48940 Training loss 0.08464621752500534 Validation loss 0.13339513540267944 Accuracy 0.6504999995231628\n",
      "Iteration 48950 Training loss 0.0889824703335762 Validation loss 0.1394631713628769 Accuracy 0.640999972820282\n",
      "Iteration 48960 Training loss 0.08622556924819946 Validation loss 0.1357341706752777 Accuracy 0.6456666588783264\n",
      "Iteration 48970 Training loss 0.08764918893575668 Validation loss 0.13168965280056 Accuracy 0.6521666646003723\n",
      "Iteration 48980 Training loss 0.09054967015981674 Validation loss 0.14324983954429626 Accuracy 0.6320000290870667\n",
      "Iteration 48990 Training loss 0.08940809965133667 Validation loss 0.1414956897497177 Accuracy 0.6358333230018616\n",
      "Iteration 49000 Training loss 0.0866134762763977 Validation loss 0.13025575876235962 Accuracy 0.6548333168029785\n",
      "Iteration 49010 Training loss 0.08462465554475784 Validation loss 0.1312747746706009 Accuracy 0.6543333530426025\n",
      "Iteration 49020 Training loss 0.08244793862104416 Validation loss 0.11918631196022034 Accuracy 0.6798333525657654\n",
      "Iteration 49030 Training loss 0.08420000970363617 Validation loss 0.11981579661369324 Accuracy 0.6778333187103271\n",
      "Iteration 49040 Training loss 0.08426815271377563 Validation loss 0.11668899655342102 Accuracy 0.6806666851043701\n",
      "Iteration 49050 Training loss 0.08609515428543091 Validation loss 0.11447737365961075 Accuracy 0.6865000128746033\n",
      "Iteration 49060 Training loss 0.08854750543832779 Validation loss 0.1144014298915863 Accuracy 0.6853333115577698\n",
      "Iteration 49070 Training loss 0.08557616174221039 Validation loss 0.11576561629772186 Accuracy 0.6846666932106018\n",
      "Iteration 49080 Training loss 0.08830654621124268 Validation loss 0.11454644799232483 Accuracy 0.6881666779518127\n",
      "Iteration 49090 Training loss 0.08216546475887299 Validation loss 0.1154380813241005 Accuracy 0.6866666674613953\n",
      "Iteration 49100 Training loss 0.08569037169218063 Validation loss 0.11704003065824509 Accuracy 0.6816666722297668\n",
      "Iteration 49110 Training loss 0.08553122729063034 Validation loss 0.11496344208717346 Accuracy 0.6866666674613953\n",
      "Iteration 49120 Training loss 0.08578848093748093 Validation loss 0.11472215503454208 Accuracy 0.6875\n",
      "Iteration 49130 Training loss 0.08717226982116699 Validation loss 0.11501829326152802 Accuracy 0.6884999871253967\n",
      "Iteration 49140 Training loss 0.08945564925670624 Validation loss 0.11452110856771469 Accuracy 0.687166690826416\n",
      "Iteration 49150 Training loss 0.08830609172582626 Validation loss 0.11437692493200302 Accuracy 0.687333345413208\n",
      "Iteration 49160 Training loss 0.08457490056753159 Validation loss 0.11570857465267181 Accuracy 0.6846666932106018\n",
      "Iteration 49170 Training loss 0.08298014104366302 Validation loss 0.11624670773744583 Accuracy 0.684499979019165\n",
      "Iteration 49180 Training loss 0.08761037141084671 Validation loss 0.1149713471531868 Accuracy 0.6856666803359985\n",
      "Iteration 49190 Training loss 0.08058255165815353 Validation loss 0.12207012623548508 Accuracy 0.6725000143051147\n",
      "Iteration 49200 Training loss 0.08146823942661285 Validation loss 0.12318404018878937 Accuracy 0.6700000166893005\n",
      "Iteration 49210 Training loss 0.08807089179754257 Validation loss 0.13726337254047394 Accuracy 0.643666684627533\n",
      "Iteration 49220 Training loss 0.088428795337677 Validation loss 0.1390318125486374 Accuracy 0.640500009059906\n",
      "Iteration 49230 Training loss 0.08491534739732742 Validation loss 0.13008366525173187 Accuracy 0.6566666960716248\n",
      "Iteration 49240 Training loss 0.08551487326622009 Validation loss 0.13374897837638855 Accuracy 0.6506666541099548\n",
      "Iteration 49250 Training loss 0.08217675983905792 Validation loss 0.1258317530155182 Accuracy 0.6643333435058594\n",
      "Iteration 49260 Training loss 0.08640900999307632 Validation loss 0.13464726507663727 Accuracy 0.6480000019073486\n",
      "Iteration 49270 Training loss 0.08641327172517776 Validation loss 0.13805051147937775 Accuracy 0.6433333158493042\n",
      "Iteration 49280 Training loss 0.08636318892240524 Validation loss 0.13599233329296112 Accuracy 0.6466666460037231\n",
      "Iteration 49290 Training loss 0.08870549499988556 Validation loss 0.14064928889274597 Accuracy 0.6381666660308838\n",
      "Iteration 49300 Training loss 0.08419810235500336 Validation loss 0.13232484459877014 Accuracy 0.6504999995231628\n",
      "Iteration 49310 Training loss 0.08229845017194748 Validation loss 0.1285208761692047 Accuracy 0.6579999923706055\n",
      "Iteration 49320 Training loss 0.08434604853391647 Validation loss 0.13061213493347168 Accuracy 0.6556666493415833\n",
      "Iteration 49330 Training loss 0.08388569951057434 Validation loss 0.13350282609462738 Accuracy 0.6483333110809326\n",
      "Iteration 49340 Training loss 0.08850555121898651 Validation loss 0.13891327381134033 Accuracy 0.6421666741371155\n",
      "Iteration 49350 Training loss 0.08555857092142105 Validation loss 0.13546225428581238 Accuracy 0.6474999785423279\n",
      "Iteration 49360 Training loss 0.08536649495363235 Validation loss 0.12928149104118347 Accuracy 0.6589999794960022\n",
      "Iteration 49370 Training loss 0.08797652274370193 Validation loss 0.13778339326381683 Accuracy 0.6446666717529297\n",
      "Iteration 49380 Training loss 0.08380531519651413 Validation loss 0.13033904135227203 Accuracy 0.656000018119812\n",
      "Iteration 49390 Training loss 0.08420093357563019 Validation loss 0.13019835948944092 Accuracy 0.6566666960716248\n",
      "Iteration 49400 Training loss 0.08534533530473709 Validation loss 0.12755326926708221 Accuracy 0.6629999876022339\n",
      "Iteration 49410 Training loss 0.0870819091796875 Validation loss 0.13592076301574707 Accuracy 0.6474999785423279\n",
      "Iteration 49420 Training loss 0.08535418659448624 Validation loss 0.1338905245065689 Accuracy 0.6499999761581421\n",
      "Iteration 49430 Training loss 0.08425018191337585 Validation loss 0.13370999693870544 Accuracy 0.6506666541099548\n",
      "Iteration 49440 Training loss 0.08730550855398178 Validation loss 0.1359967142343521 Accuracy 0.6478333473205566\n",
      "Iteration 49450 Training loss 0.08307324349880219 Validation loss 0.13135744631290436 Accuracy 0.6543333530426025\n",
      "Iteration 49460 Training loss 0.08710231631994247 Validation loss 0.12701474130153656 Accuracy 0.6650000214576721\n",
      "Iteration 49470 Training loss 0.09174779057502747 Validation loss 0.1426716446876526 Accuracy 0.6333333253860474\n",
      "Iteration 49480 Training loss 0.08786734193563461 Validation loss 0.13739152252674103 Accuracy 0.643833339214325\n",
      "Iteration 49490 Training loss 0.08496623486280441 Validation loss 0.13478533923625946 Accuracy 0.6483333110809326\n",
      "Iteration 49500 Training loss 0.08543124049901962 Validation loss 0.12777656316757202 Accuracy 0.6625000238418579\n",
      "Iteration 49510 Training loss 0.08359570056200027 Validation loss 0.1314428746700287 Accuracy 0.652999997138977\n",
      "Iteration 49520 Training loss 0.08791573345661163 Validation loss 0.13964422047138214 Accuracy 0.6388333439826965\n",
      "Iteration 49530 Training loss 0.0872795581817627 Validation loss 0.13569055497646332 Accuracy 0.6473333239555359\n",
      "Iteration 49540 Training loss 0.08585768938064575 Validation loss 0.13173027336597443 Accuracy 0.6535000205039978\n",
      "Iteration 49550 Training loss 0.0883992537856102 Validation loss 0.13927942514419556 Accuracy 0.6418333053588867\n",
      "Iteration 49560 Training loss 0.0851963683962822 Validation loss 0.13532496988773346 Accuracy 0.6488333344459534\n",
      "Iteration 49570 Training loss 0.0841502696275711 Validation loss 0.13321799039840698 Accuracy 0.6518333554267883\n",
      "Iteration 49580 Training loss 0.08663888275623322 Validation loss 0.13849148154258728 Accuracy 0.6426666378974915\n",
      "Iteration 49590 Training loss 0.08588186651468277 Validation loss 0.12969006597995758 Accuracy 0.6588333249092102\n",
      "Iteration 49600 Training loss 0.08537428826093674 Validation loss 0.1326359510421753 Accuracy 0.6526666879653931\n",
      "Iteration 49610 Training loss 0.08878698945045471 Validation loss 0.13704392313957214 Accuracy 0.6448333263397217\n",
      "Iteration 49620 Training loss 0.08594075590372086 Validation loss 0.13884373009204865 Accuracy 0.6420000195503235\n",
      "Iteration 49630 Training loss 0.0880604013800621 Validation loss 0.1333433985710144 Accuracy 0.6501666903495789\n",
      "Iteration 49640 Training loss 0.09084810316562653 Validation loss 0.14038081467151642 Accuracy 0.6401666402816772\n",
      "Iteration 49650 Training loss 0.08154834806919098 Validation loss 0.12114841490983963 Accuracy 0.6763333082199097\n",
      "Iteration 49660 Training loss 0.08364024758338928 Validation loss 0.13232795894145966 Accuracy 0.653333306312561\n",
      "Iteration 49670 Training loss 0.08915404975414276 Validation loss 0.13584817945957184 Accuracy 0.6470000147819519\n",
      "Iteration 49680 Training loss 0.08372833579778671 Validation loss 0.1284942775964737 Accuracy 0.6589999794960022\n",
      "Iteration 49690 Training loss 0.08675848692655563 Validation loss 0.12762665748596191 Accuracy 0.6623333096504211\n",
      "Iteration 49700 Training loss 0.08509179949760437 Validation loss 0.13405431807041168 Accuracy 0.6498333215713501\n",
      "Iteration 49710 Training loss 0.08926133066415787 Validation loss 0.14061777293682098 Accuracy 0.6378333568572998\n",
      "Iteration 49720 Training loss 0.09081605821847916 Validation loss 0.1401984691619873 Accuracy 0.6386666893959045\n",
      "Iteration 49730 Training loss 0.09087526798248291 Validation loss 0.14193199574947357 Accuracy 0.6345000267028809\n",
      "Iteration 49740 Training loss 0.08501806110143661 Validation loss 0.1303327977657318 Accuracy 0.6571666598320007\n",
      "Iteration 49750 Training loss 0.08621939271688461 Validation loss 0.1332208663225174 Accuracy 0.6499999761581421\n",
      "Iteration 49760 Training loss 0.08741440623998642 Validation loss 0.12894362211227417 Accuracy 0.6598333120346069\n",
      "Iteration 49770 Training loss 0.08609356731176376 Validation loss 0.13680699467658997 Accuracy 0.6455000042915344\n",
      "Iteration 49780 Training loss 0.0876513198018074 Validation loss 0.13277851045131683 Accuracy 0.6516666412353516\n",
      "Iteration 49790 Training loss 0.09190711379051208 Validation loss 0.14714401960372925 Accuracy 0.6240000128746033\n",
      "Iteration 49800 Training loss 0.08560189604759216 Validation loss 0.12951911985874176 Accuracy 0.6589999794960022\n",
      "Iteration 49810 Training loss 0.08472277969121933 Validation loss 0.12849874794483185 Accuracy 0.6608333587646484\n",
      "Iteration 49820 Training loss 0.08910636603832245 Validation loss 0.14149661362171173 Accuracy 0.6346666812896729\n",
      "Iteration 49830 Training loss 0.08649009466171265 Validation loss 0.13229972124099731 Accuracy 0.6535000205039978\n",
      "Iteration 49840 Training loss 0.0843345895409584 Validation loss 0.12543876469135284 Accuracy 0.6671666502952576\n",
      "Iteration 49850 Training loss 0.0886855274438858 Validation loss 0.13663531839847565 Accuracy 0.6456666588783264\n",
      "Iteration 49860 Training loss 0.08808179944753647 Validation loss 0.14154432713985443 Accuracy 0.6349999904632568\n",
      "Iteration 49870 Training loss 0.0844385027885437 Validation loss 0.12875239551067352 Accuracy 0.659166693687439\n",
      "Iteration 49880 Training loss 0.08189655840396881 Validation loss 0.12271067500114441 Accuracy 0.671500027179718\n",
      "Iteration 49890 Training loss 0.08514384180307388 Validation loss 0.1320723444223404 Accuracy 0.6524999737739563\n",
      "Iteration 49900 Training loss 0.09199707210063934 Validation loss 0.1386723518371582 Accuracy 0.64083331823349\n",
      "Iteration 49910 Training loss 0.08813729882240295 Validation loss 0.14346623420715332 Accuracy 0.6318333148956299\n",
      "Iteration 49920 Training loss 0.08629223704338074 Validation loss 0.13449226319789886 Accuracy 0.6491666436195374\n",
      "Iteration 49930 Training loss 0.09012506157159805 Validation loss 0.13511475920677185 Accuracy 0.6488333344459534\n",
      "Iteration 49940 Training loss 0.08699291944503784 Validation loss 0.1366303563117981 Accuracy 0.6453333497047424\n",
      "Iteration 49950 Training loss 0.08482669293880463 Validation loss 0.12973585724830627 Accuracy 0.656333327293396\n",
      "Iteration 49960 Training loss 0.0860014259815216 Validation loss 0.13349024951457977 Accuracy 0.6514999866485596\n",
      "Iteration 49970 Training loss 0.08772782236337662 Validation loss 0.13831116259098053 Accuracy 0.6423333287239075\n",
      "Iteration 49980 Training loss 0.08841483294963837 Validation loss 0.1399119645357132 Accuracy 0.6398333311080933\n",
      "Iteration 49990 Training loss 0.08769520372152328 Validation loss 0.13118931651115417 Accuracy 0.6551666855812073\n",
      "Iteration 50000 Training loss 0.08432507514953613 Validation loss 0.12335096299648285 Accuracy 0.6700000166893005\n",
      "Iteration 50010 Training loss 0.08487821370363235 Validation loss 0.1161256730556488 Accuracy 0.6838333606719971\n",
      "Iteration 50020 Training loss 0.09186417609453201 Validation loss 0.11458606272935867 Accuracy 0.687666654586792\n",
      "Iteration 50030 Training loss 0.08624377101659775 Validation loss 0.11601782590150833 Accuracy 0.6831666827201843\n",
      "Iteration 50040 Training loss 0.08389239758253098 Validation loss 0.1164359524846077 Accuracy 0.6838333606719971\n",
      "Iteration 50050 Training loss 0.08493372052907944 Validation loss 0.11557714641094208 Accuracy 0.6856666803359985\n",
      "Iteration 50060 Training loss 0.08822721242904663 Validation loss 0.11453515291213989 Accuracy 0.6866666674613953\n",
      "Iteration 50070 Training loss 0.09048274904489517 Validation loss 0.11460703611373901 Accuracy 0.6875\n",
      "Iteration 50080 Training loss 0.08471377193927765 Validation loss 0.11584161221981049 Accuracy 0.6850000023841858\n",
      "Iteration 50090 Training loss 0.08446892350912094 Validation loss 0.11827471852302551 Accuracy 0.6800000071525574\n",
      "Iteration 50100 Training loss 0.08604787290096283 Validation loss 0.11479189991950989 Accuracy 0.6863333582878113\n",
      "Iteration 50110 Training loss 0.08589133620262146 Validation loss 0.11482348293066025 Accuracy 0.6856666803359985\n",
      "Iteration 50120 Training loss 0.08668094128370285 Validation loss 0.11478394269943237 Accuracy 0.687666654586792\n",
      "Iteration 50130 Training loss 0.08430477231740952 Validation loss 0.1160564199090004 Accuracy 0.684166669845581\n",
      "Iteration 50140 Training loss 0.08593888580799103 Validation loss 0.11596383899450302 Accuracy 0.6838333606719971\n",
      "Iteration 50150 Training loss 0.08525988459587097 Validation loss 0.1151551604270935 Accuracy 0.6861666440963745\n",
      "Iteration 50160 Training loss 0.08503356575965881 Validation loss 0.11548386514186859 Accuracy 0.6856666803359985\n",
      "Iteration 50170 Training loss 0.08505935966968536 Validation loss 0.11562424898147583 Accuracy 0.6853333115577698\n",
      "Iteration 50180 Training loss 0.0864807739853859 Validation loss 0.11493786424398422 Accuracy 0.6861666440963745\n",
      "Iteration 50190 Training loss 0.08576260507106781 Validation loss 0.1149858683347702 Accuracy 0.6856666803359985\n",
      "Iteration 50200 Training loss 0.08507131040096283 Validation loss 0.1158880889415741 Accuracy 0.684499979019165\n",
      "Iteration 50210 Training loss 0.08268963545560837 Validation loss 0.11668125540018082 Accuracy 0.6821666955947876\n",
      "Iteration 50220 Training loss 0.08359110355377197 Validation loss 0.11634277552366257 Accuracy 0.684166669845581\n",
      "Iteration 50230 Training loss 0.08418235182762146 Validation loss 0.11625862866640091 Accuracy 0.6834999918937683\n",
      "Iteration 50240 Training loss 0.08644070476293564 Validation loss 0.11485877633094788 Accuracy 0.6861666440963745\n",
      "Iteration 50250 Training loss 0.08586502820253372 Validation loss 0.11489719897508621 Accuracy 0.684333324432373\n",
      "Iteration 50260 Training loss 0.08662867546081543 Validation loss 0.11650961637496948 Accuracy 0.6831666827201843\n",
      "Iteration 50270 Training loss 0.08730291575193405 Validation loss 0.11441576480865479 Accuracy 0.6865000128746033\n",
      "Iteration 50280 Training loss 0.08553419262170792 Validation loss 0.11514882743358612 Accuracy 0.687166690826416\n",
      "Iteration 50290 Training loss 0.08703148365020752 Validation loss 0.11514884233474731 Accuracy 0.6866666674613953\n",
      "Iteration 50300 Training loss 0.08754490315914154 Validation loss 0.1146448478102684 Accuracy 0.687666654586792\n",
      "Iteration 50310 Training loss 0.08737034350633621 Validation loss 0.11741416156291962 Accuracy 0.6809999942779541\n",
      "Iteration 50320 Training loss 0.08183098584413528 Validation loss 0.12065795063972473 Accuracy 0.6779999732971191\n",
      "Iteration 50330 Training loss 0.08298570662736893 Validation loss 0.1185707300901413 Accuracy 0.6790000200271606\n",
      "Iteration 50340 Training loss 0.08873286098241806 Validation loss 0.11460146307945251 Accuracy 0.6859999895095825\n",
      "Iteration 50350 Training loss 0.0899728387594223 Validation loss 0.11457095295190811 Accuracy 0.6884999871253967\n",
      "Iteration 50360 Training loss 0.0876510813832283 Validation loss 0.11607369035482407 Accuracy 0.6828333139419556\n",
      "Iteration 50370 Training loss 0.08727748692035675 Validation loss 0.11448284238576889 Accuracy 0.6866666674613953\n",
      "Iteration 50380 Training loss 0.08397955447435379 Validation loss 0.11854146420955658 Accuracy 0.6798333525657654\n",
      "Iteration 50390 Training loss 0.08193988353013992 Validation loss 0.11657798290252686 Accuracy 0.6833333373069763\n",
      "Iteration 50400 Training loss 0.08544223755598068 Validation loss 0.11492989212274551 Accuracy 0.6859999895095825\n",
      "Iteration 50410 Training loss 0.08708268404006958 Validation loss 0.11463224142789841 Accuracy 0.6863333582878113\n",
      "Iteration 50420 Training loss 0.08746299147605896 Validation loss 0.11571354418992996 Accuracy 0.6856666803359985\n",
      "Iteration 50430 Training loss 0.0858624130487442 Validation loss 0.11467264592647552 Accuracy 0.6868333220481873\n",
      "Iteration 50440 Training loss 0.08845815062522888 Validation loss 0.11443953216075897 Accuracy 0.6880000233650208\n",
      "Iteration 50450 Training loss 0.08782307803630829 Validation loss 0.11473169177770615 Accuracy 0.6869999766349792\n",
      "Iteration 50460 Training loss 0.08292364329099655 Validation loss 0.11751217395067215 Accuracy 0.6796666383743286\n",
      "Iteration 50470 Training loss 0.08631345629692078 Validation loss 0.11461768299341202 Accuracy 0.6875\n",
      "Iteration 50480 Training loss 0.08527389168739319 Validation loss 0.11501908302307129 Accuracy 0.6853333115577698\n",
      "Iteration 50490 Training loss 0.08226802200078964 Validation loss 0.11904021352529526 Accuracy 0.6815000176429749\n",
      "Iteration 50500 Training loss 0.0828452929854393 Validation loss 0.12108344584703445 Accuracy 0.6761666536331177\n",
      "Iteration 50510 Training loss 0.08280705660581589 Validation loss 0.12253932654857635 Accuracy 0.6735000014305115\n",
      "Iteration 50520 Training loss 0.08411924540996552 Validation loss 0.11827114969491959 Accuracy 0.6794999837875366\n",
      "Iteration 50530 Training loss 0.08965624123811722 Validation loss 0.11474426090717316 Accuracy 0.6869999766349792\n",
      "Iteration 50540 Training loss 0.08575274795293808 Validation loss 0.11484332382678986 Accuracy 0.687166690826416\n",
      "Iteration 50550 Training loss 0.08576485514640808 Validation loss 0.11538899689912796 Accuracy 0.687166690826416\n",
      "Iteration 50560 Training loss 0.08561987429857254 Validation loss 0.11598929762840271 Accuracy 0.6851666569709778\n",
      "Iteration 50570 Training loss 0.08416923880577087 Validation loss 0.11911313980817795 Accuracy 0.6788333058357239\n",
      "Iteration 50580 Training loss 0.08627677708864212 Validation loss 0.11453637480735779 Accuracy 0.6875\n",
      "Iteration 50590 Training loss 0.08831334114074707 Validation loss 0.11443911492824554 Accuracy 0.6875\n",
      "Iteration 50600 Training loss 0.08785267174243927 Validation loss 0.11525954306125641 Accuracy 0.6866666674613953\n",
      "Iteration 50610 Training loss 0.08771475404500961 Validation loss 0.11487662047147751 Accuracy 0.6865000128746033\n",
      "Iteration 50620 Training loss 0.08524058014154434 Validation loss 0.11510878801345825 Accuracy 0.6859999895095825\n",
      "Iteration 50630 Training loss 0.08301620185375214 Validation loss 0.11782914400100708 Accuracy 0.6783333420753479\n",
      "Iteration 50640 Training loss 0.08524967730045319 Validation loss 0.11442836374044418 Accuracy 0.6865000128746033\n",
      "Iteration 50650 Training loss 0.08574611693620682 Validation loss 0.11504402756690979 Accuracy 0.6863333582878113\n",
      "Iteration 50660 Training loss 0.08404945582151413 Validation loss 0.11502337455749512 Accuracy 0.687333345413208\n",
      "Iteration 50670 Training loss 0.08661529421806335 Validation loss 0.11475857347249985 Accuracy 0.687333345413208\n",
      "Iteration 50680 Training loss 0.0916566550731659 Validation loss 0.11449112743139267 Accuracy 0.6865000128746033\n",
      "Iteration 50690 Training loss 0.08696649223566055 Validation loss 0.11569999903440475 Accuracy 0.6856666803359985\n",
      "Iteration 50700 Training loss 0.08521834015846252 Validation loss 0.1161930114030838 Accuracy 0.6834999918937683\n",
      "Iteration 50710 Training loss 0.08393019437789917 Validation loss 0.11599516123533249 Accuracy 0.6851666569709778\n",
      "Iteration 50720 Training loss 0.08614826202392578 Validation loss 0.11494671553373337 Accuracy 0.6848333477973938\n",
      "Iteration 50730 Training loss 0.08853892236948013 Validation loss 0.11464793980121613 Accuracy 0.6865000128746033\n",
      "Iteration 50740 Training loss 0.08745148777961731 Validation loss 0.11462976038455963 Accuracy 0.6869999766349792\n",
      "Iteration 50750 Training loss 0.08602676540613174 Validation loss 0.11516095697879791 Accuracy 0.6859999895095825\n",
      "Iteration 50760 Training loss 0.08927597850561142 Validation loss 0.11471663415431976 Accuracy 0.6875\n",
      "Iteration 50770 Training loss 0.0838509351015091 Validation loss 0.11579758673906326 Accuracy 0.6856666803359985\n",
      "Iteration 50780 Training loss 0.09022415429353714 Validation loss 0.11458976566791534 Accuracy 0.6884999871253967\n",
      "Iteration 50790 Training loss 0.08504436910152435 Validation loss 0.11530017852783203 Accuracy 0.6866666674613953\n",
      "Iteration 50800 Training loss 0.08267687261104584 Validation loss 0.11734233796596527 Accuracy 0.6826666593551636\n",
      "Iteration 50810 Training loss 0.08534041047096252 Validation loss 0.12021833658218384 Accuracy 0.6790000200271606\n",
      "Iteration 50820 Training loss 0.08355331420898438 Validation loss 0.1165183037519455 Accuracy 0.684333324432373\n",
      "Iteration 50830 Training loss 0.08944295346736908 Validation loss 0.11460480093955994 Accuracy 0.6859999895095825\n",
      "Iteration 50840 Training loss 0.08618177473545074 Validation loss 0.11459486186504364 Accuracy 0.6881666779518127\n",
      "Iteration 50850 Training loss 0.08961773663759232 Validation loss 0.11455223709344864 Accuracy 0.6890000104904175\n",
      "Iteration 50860 Training loss 0.08220193535089493 Validation loss 0.11863242834806442 Accuracy 0.6791666746139526\n",
      "Iteration 50870 Training loss 0.086310975253582 Validation loss 0.11559751629829407 Accuracy 0.6853333115577698\n",
      "Iteration 50880 Training loss 0.08575860410928726 Validation loss 0.11666058003902435 Accuracy 0.6818333268165588\n",
      "Iteration 50890 Training loss 0.08625497668981552 Validation loss 0.11465584486722946 Accuracy 0.6868333220481873\n",
      "Iteration 50900 Training loss 0.08557239919900894 Validation loss 0.11542254686355591 Accuracy 0.6853333115577698\n",
      "Iteration 50910 Training loss 0.08491624891757965 Validation loss 0.1150040477514267 Accuracy 0.6856666803359985\n",
      "Iteration 50920 Training loss 0.08587928116321564 Validation loss 0.1152101680636406 Accuracy 0.687166690826416\n",
      "Iteration 50930 Training loss 0.08596252650022507 Validation loss 0.11459670960903168 Accuracy 0.6884999871253967\n",
      "Iteration 50940 Training loss 0.0877901017665863 Validation loss 0.11469443887472153 Accuracy 0.6865000128746033\n",
      "Iteration 50950 Training loss 0.08899401873350143 Validation loss 0.11499081552028656 Accuracy 0.6861666440963745\n",
      "Iteration 50960 Training loss 0.0869588702917099 Validation loss 0.11468011140823364 Accuracy 0.687333345413208\n",
      "Iteration 50970 Training loss 0.08339520543813705 Validation loss 0.11642447859048843 Accuracy 0.6848333477973938\n",
      "Iteration 50980 Training loss 0.08657031506299973 Validation loss 0.1176087036728859 Accuracy 0.6794999837875366\n",
      "Iteration 50990 Training loss 0.08291501551866531 Validation loss 0.11654211580753326 Accuracy 0.684333324432373\n",
      "Iteration 51000 Training loss 0.09046955406665802 Validation loss 0.11476957052946091 Accuracy 0.6868333220481873\n",
      "Iteration 51010 Training loss 0.08817731589078903 Validation loss 0.11470955610275269 Accuracy 0.6865000128746033\n",
      "Iteration 51020 Training loss 0.08487392961978912 Validation loss 0.11643806099891663 Accuracy 0.6834999918937683\n",
      "Iteration 51030 Training loss 0.08394718915224075 Validation loss 0.11599455773830414 Accuracy 0.6851666569709778\n",
      "Iteration 51040 Training loss 0.08491912484169006 Validation loss 0.1174698993563652 Accuracy 0.6804999709129333\n",
      "Iteration 51050 Training loss 0.08757445961236954 Validation loss 0.1146547868847847 Accuracy 0.6868333220481873\n",
      "Iteration 51060 Training loss 0.08549018204212189 Validation loss 0.11513306945562363 Accuracy 0.6868333220481873\n",
      "Iteration 51070 Training loss 0.08441557735204697 Validation loss 0.1162564679980278 Accuracy 0.6848333477973938\n",
      "Iteration 51080 Training loss 0.08213766664266586 Validation loss 0.11707986146211624 Accuracy 0.6815000176429749\n",
      "Iteration 51090 Training loss 0.08422721177339554 Validation loss 0.11693675071001053 Accuracy 0.6830000281333923\n",
      "Iteration 51100 Training loss 0.08420436829328537 Validation loss 0.12133035063743591 Accuracy 0.6746666431427002\n",
      "Iteration 51110 Training loss 0.08308082073926926 Validation loss 0.12443003058433533 Accuracy 0.6681666374206543\n",
      "Iteration 51120 Training loss 0.09291820973157883 Validation loss 0.1459827423095703 Accuracy 0.6256666779518127\n",
      "Iteration 51130 Training loss 0.0888548195362091 Validation loss 0.14170560240745544 Accuracy 0.6370000243186951\n",
      "Iteration 51140 Training loss 0.08665656298398972 Validation loss 0.1325777769088745 Accuracy 0.6528333425521851\n",
      "Iteration 51150 Training loss 0.08462587743997574 Validation loss 0.13130971789360046 Accuracy 0.6554999947547913\n",
      "Iteration 51160 Training loss 0.08617735654115677 Validation loss 0.13306353986263275 Accuracy 0.6520000100135803\n",
      "Iteration 51170 Training loss 0.08644463866949081 Validation loss 0.13744856417179108 Accuracy 0.6443333625793457\n",
      "Iteration 51180 Training loss 0.08378396928310394 Validation loss 0.13586896657943726 Accuracy 0.6486666798591614\n",
      "Iteration 51190 Training loss 0.08308733254671097 Validation loss 0.12820683419704437 Accuracy 0.6629999876022339\n",
      "Iteration 51200 Training loss 0.08464674651622772 Validation loss 0.13018371164798737 Accuracy 0.6558333039283752\n",
      "Iteration 51210 Training loss 0.08350119739770889 Validation loss 0.12473569810390472 Accuracy 0.6673333048820496\n",
      "Iteration 51220 Training loss 0.08408021181821823 Validation loss 0.12866617739200592 Accuracy 0.659166693687439\n",
      "Iteration 51230 Training loss 0.08623915910720825 Validation loss 0.13158512115478516 Accuracy 0.653333306312561\n",
      "Iteration 51240 Training loss 0.0935014933347702 Validation loss 0.15187764167785645 Accuracy 0.6146666407585144\n",
      "Iteration 51250 Training loss 0.08714968711137772 Validation loss 0.13867029547691345 Accuracy 0.643666684627533\n",
      "Iteration 51260 Training loss 0.084952212870121 Validation loss 0.13088195025920868 Accuracy 0.6536666750907898\n",
      "Iteration 51270 Training loss 0.09059596806764603 Validation loss 0.1410282552242279 Accuracy 0.637333333492279\n",
      "Iteration 51280 Training loss 0.08671589940786362 Validation loss 0.1353931427001953 Accuracy 0.6466666460037231\n",
      "Iteration 51290 Training loss 0.08512850105762482 Validation loss 0.13314317166805267 Accuracy 0.6501666903495789\n",
      "Iteration 51300 Training loss 0.08175767958164215 Validation loss 0.12543253600597382 Accuracy 0.6658333539962769\n",
      "Iteration 51310 Training loss 0.08234190940856934 Validation loss 0.1345180720090866 Accuracy 0.6481666564941406\n",
      "Iteration 51320 Training loss 0.08433148264884949 Validation loss 0.13377128541469574 Accuracy 0.6496666669845581\n",
      "Iteration 51330 Training loss 0.0856710895895958 Validation loss 0.1312476247549057 Accuracy 0.6546666622161865\n",
      "Iteration 51340 Training loss 0.0854775458574295 Validation loss 0.13811032474040985 Accuracy 0.643666684627533\n",
      "Iteration 51350 Training loss 0.08935880661010742 Validation loss 0.13938747346401215 Accuracy 0.6401666402816772\n",
      "Iteration 51360 Training loss 0.08539597690105438 Validation loss 0.13710160553455353 Accuracy 0.6446666717529297\n",
      "Iteration 51370 Training loss 0.08736099302768707 Validation loss 0.13667847216129303 Accuracy 0.6451666951179504\n",
      "Iteration 51380 Training loss 0.08442181348800659 Validation loss 0.12957604229450226 Accuracy 0.6579999923706055\n",
      "Iteration 51390 Training loss 0.08540742099285126 Validation loss 0.1340925097465515 Accuracy 0.6501666903495789\n",
      "Iteration 51400 Training loss 0.08487879484891891 Validation loss 0.132895827293396 Accuracy 0.6523333191871643\n",
      "Iteration 51410 Training loss 0.08983253687620163 Validation loss 0.14104443788528442 Accuracy 0.6384999752044678\n",
      "Iteration 51420 Training loss 0.08430549502372742 Validation loss 0.13261693716049194 Accuracy 0.6516666412353516\n",
      "Iteration 51430 Training loss 0.08629607409238815 Validation loss 0.13704872131347656 Accuracy 0.6451666951179504\n",
      "Iteration 51440 Training loss 0.08673650771379471 Validation loss 0.13846227526664734 Accuracy 0.6414999961853027\n",
      "Iteration 51450 Training loss 0.08614115417003632 Validation loss 0.13553829491138458 Accuracy 0.6473333239555359\n",
      "Iteration 51460 Training loss 0.08379026502370834 Validation loss 0.12541422247886658 Accuracy 0.6661666631698608\n",
      "Iteration 51470 Training loss 0.08418308198451996 Validation loss 0.12382254749536514 Accuracy 0.6690000295639038\n",
      "Iteration 51480 Training loss 0.08252756297588348 Validation loss 0.11879565566778183 Accuracy 0.6806666851043701\n",
      "Iteration 51490 Training loss 0.08379799127578735 Validation loss 0.12209761887788773 Accuracy 0.6743333339691162\n",
      "Iteration 51500 Training loss 0.08548017591238022 Validation loss 0.1354975551366806 Accuracy 0.6483333110809326\n",
      "Iteration 51510 Training loss 0.09181144088506699 Validation loss 0.1477171629667282 Accuracy 0.6231666803359985\n",
      "Iteration 51520 Training loss 0.08725200593471527 Validation loss 0.1341104805469513 Accuracy 0.6491666436195374\n",
      "Iteration 51530 Training loss 0.08759104460477829 Validation loss 0.1365959495306015 Accuracy 0.6463333368301392\n",
      "Iteration 51540 Training loss 0.08561631292104721 Validation loss 0.1306239515542984 Accuracy 0.656166672706604\n",
      "Iteration 51550 Training loss 0.08469760417938232 Validation loss 0.12828238308429718 Accuracy 0.6616666913032532\n",
      "Iteration 51560 Training loss 0.0821925699710846 Validation loss 0.1256265640258789 Accuracy 0.6648333072662354\n",
      "Iteration 51570 Training loss 0.08433783799409866 Validation loss 0.13297660648822784 Accuracy 0.6506666541099548\n",
      "Iteration 51580 Training loss 0.08581913262605667 Validation loss 0.13493025302886963 Accuracy 0.6486666798591614\n",
      "Iteration 51590 Training loss 0.08822908252477646 Validation loss 0.14134663343429565 Accuracy 0.637333333492279\n",
      "Iteration 51600 Training loss 0.08455125242471695 Validation loss 0.1352912038564682 Accuracy 0.6473333239555359\n",
      "Iteration 51610 Training loss 0.08550341427326202 Validation loss 0.13488447666168213 Accuracy 0.6485000252723694\n",
      "Iteration 51620 Training loss 0.0849769115447998 Validation loss 0.13181810081005096 Accuracy 0.6520000100135803\n",
      "Iteration 51630 Training loss 0.08363410830497742 Validation loss 0.13341644406318665 Accuracy 0.6504999995231628\n",
      "Iteration 51640 Training loss 0.08640246838331223 Validation loss 0.136323019862175 Accuracy 0.6474999785423279\n",
      "Iteration 51650 Training loss 0.08549053966999054 Validation loss 0.13509514927864075 Accuracy 0.6481666564941406\n",
      "Iteration 51660 Training loss 0.08241721242666245 Validation loss 0.12706875801086426 Accuracy 0.6636666655540466\n",
      "Iteration 51670 Training loss 0.08300825208425522 Validation loss 0.13321885466575623 Accuracy 0.6520000100135803\n",
      "Iteration 51680 Training loss 0.08585848659276962 Validation loss 0.12982040643692017 Accuracy 0.6581666469573975\n",
      "Iteration 51690 Training loss 0.09126989543437958 Validation loss 0.1460440456867218 Accuracy 0.6261666417121887\n",
      "Iteration 51700 Training loss 0.0869646817445755 Validation loss 0.1361694186925888 Accuracy 0.6456666588783264\n",
      "Iteration 51710 Training loss 0.08631882071495056 Validation loss 0.133576899766922 Accuracy 0.6504999995231628\n",
      "Iteration 51720 Training loss 0.09022628515958786 Validation loss 0.14241158962249756 Accuracy 0.6346666812896729\n",
      "Iteration 51730 Training loss 0.08731045573949814 Validation loss 0.13857871294021606 Accuracy 0.6426666378974915\n",
      "Iteration 51740 Training loss 0.0848812386393547 Validation loss 0.1312650889158249 Accuracy 0.6545000076293945\n",
      "Iteration 51750 Training loss 0.08342818915843964 Validation loss 0.1309792697429657 Accuracy 0.6546666622161865\n",
      "Iteration 51760 Training loss 0.08135608583688736 Validation loss 0.12389504164457321 Accuracy 0.6704999804496765\n",
      "Iteration 51770 Training loss 0.08277972042560577 Validation loss 0.12773406505584717 Accuracy 0.6625000238418579\n",
      "Iteration 51780 Training loss 0.08572407811880112 Validation loss 0.13386136293411255 Accuracy 0.6498333215713501\n",
      "Iteration 51790 Training loss 0.08626970648765564 Validation loss 0.13377462327480316 Accuracy 0.6491666436195374\n",
      "Iteration 51800 Training loss 0.08655581623315811 Validation loss 0.1368265002965927 Accuracy 0.6449999809265137\n",
      "Iteration 51810 Training loss 0.08398265391588211 Validation loss 0.1315728724002838 Accuracy 0.6518333554267883\n",
      "Iteration 51820 Training loss 0.09184011071920395 Validation loss 0.14120793342590332 Accuracy 0.6356666684150696\n",
      "Iteration 51830 Training loss 0.08627375215291977 Validation loss 0.13535167276859283 Accuracy 0.6474999785423279\n",
      "Iteration 51840 Training loss 0.08183606714010239 Validation loss 0.13376137614250183 Accuracy 0.6486666798591614\n",
      "Iteration 51850 Training loss 0.08889579027891159 Validation loss 0.13927777111530304 Accuracy 0.6401666402816772\n",
      "Iteration 51860 Training loss 0.08855684101581573 Validation loss 0.14041221141815186 Accuracy 0.6386666893959045\n",
      "Iteration 51870 Training loss 0.08756369352340698 Validation loss 0.1378735601902008 Accuracy 0.643666684627533\n",
      "Iteration 51880 Training loss 0.08879943192005157 Validation loss 0.13692645728588104 Accuracy 0.6448333263397217\n",
      "Iteration 51890 Training loss 0.08646851778030396 Validation loss 0.13264499604701996 Accuracy 0.6508333086967468\n",
      "Iteration 51900 Training loss 0.08342784643173218 Validation loss 0.12337759137153625 Accuracy 0.6696666479110718\n",
      "Iteration 51910 Training loss 0.08256526291370392 Validation loss 0.13544780015945435 Accuracy 0.6480000019073486\n",
      "Iteration 51920 Training loss 0.08435465395450592 Validation loss 0.13092535734176636 Accuracy 0.6551666855812073\n",
      "Iteration 51930 Training loss 0.08381440490484238 Validation loss 0.1350986361503601 Accuracy 0.6476666927337646\n",
      "Iteration 51940 Training loss 0.08487966656684875 Validation loss 0.1290755271911621 Accuracy 0.6600000262260437\n",
      "Iteration 51950 Training loss 0.08654565364122391 Validation loss 0.13635194301605225 Accuracy 0.6455000042915344\n",
      "Iteration 51960 Training loss 0.08253736048936844 Validation loss 0.12863022089004517 Accuracy 0.6610000133514404\n",
      "Iteration 51970 Training loss 0.0845727026462555 Validation loss 0.1322890818119049 Accuracy 0.6535000205039978\n",
      "Iteration 51980 Training loss 0.08652312308549881 Validation loss 0.13681712746620178 Accuracy 0.6448333263397217\n",
      "Iteration 51990 Training loss 0.0868648812174797 Validation loss 0.1387885957956314 Accuracy 0.6398333311080933\n",
      "Iteration 52000 Training loss 0.08601413667201996 Validation loss 0.13386885821819305 Accuracy 0.6510000228881836\n",
      "Iteration 52010 Training loss 0.08338871598243713 Validation loss 0.12144943326711655 Accuracy 0.6759999990463257\n",
      "Iteration 52020 Training loss 0.08609873801469803 Validation loss 0.11508797109127045 Accuracy 0.687333345413208\n",
      "Iteration 52030 Training loss 0.08803438395261765 Validation loss 0.11471137404441833 Accuracy 0.687833309173584\n",
      "Iteration 52040 Training loss 0.08110728114843369 Validation loss 0.11734599620103836 Accuracy 0.6825000047683716\n",
      "Iteration 52050 Training loss 0.08208172768354416 Validation loss 0.12121550738811493 Accuracy 0.6784999966621399\n",
      "Iteration 52060 Training loss 0.08355632424354553 Validation loss 0.126816526055336 Accuracy 0.6636666655540466\n",
      "Iteration 52070 Training loss 0.09014274924993515 Validation loss 0.13845476508140564 Accuracy 0.6423333287239075\n",
      "Iteration 52080 Training loss 0.08906754851341248 Validation loss 0.14095325767993927 Accuracy 0.6378333568572998\n",
      "Iteration 52090 Training loss 0.0871293768286705 Validation loss 0.13732405006885529 Accuracy 0.6443333625793457\n",
      "Iteration 52100 Training loss 0.08755083382129669 Validation loss 0.13569685816764832 Accuracy 0.6468333601951599\n",
      "Iteration 52110 Training loss 0.08197484165430069 Validation loss 0.12986944615840912 Accuracy 0.659500002861023\n",
      "Iteration 52120 Training loss 0.08772742003202438 Validation loss 0.13713306188583374 Accuracy 0.6451666951179504\n",
      "Iteration 52130 Training loss 0.08574330806732178 Validation loss 0.1350269764661789 Accuracy 0.6474999785423279\n",
      "Iteration 52140 Training loss 0.08658656477928162 Validation loss 0.13491998612880707 Accuracy 0.6486666798591614\n",
      "Iteration 52150 Training loss 0.08259855955839157 Validation loss 0.1271788477897644 Accuracy 0.6631666421890259\n",
      "Iteration 52160 Training loss 0.08222029358148575 Validation loss 0.1320900022983551 Accuracy 0.6543333530426025\n",
      "Iteration 52170 Training loss 0.08682762831449509 Validation loss 0.13406910002231598 Accuracy 0.6493333578109741\n",
      "Iteration 52180 Training loss 0.0878986343741417 Validation loss 0.1354173868894577 Accuracy 0.6481666564941406\n",
      "Iteration 52190 Training loss 0.09061693400144577 Validation loss 0.1398632675409317 Accuracy 0.6411666870117188\n",
      "Iteration 52200 Training loss 0.0864391103386879 Validation loss 0.13102158904075623 Accuracy 0.6571666598320007\n",
      "Iteration 52210 Training loss 0.08368872851133347 Validation loss 0.12599211931228638 Accuracy 0.6643333435058594\n",
      "Iteration 52220 Training loss 0.08323833346366882 Validation loss 0.12812118232250214 Accuracy 0.6614999771118164\n",
      "Iteration 52230 Training loss 0.08563106507062912 Validation loss 0.12824353575706482 Accuracy 0.6600000262260437\n",
      "Iteration 52240 Training loss 0.08760204911231995 Validation loss 0.13927492499351501 Accuracy 0.6401666402816772\n",
      "Iteration 52250 Training loss 0.0917012020945549 Validation loss 0.14484037458896637 Accuracy 0.628166675567627\n",
      "Iteration 52260 Training loss 0.08572916686534882 Validation loss 0.13719983398914337 Accuracy 0.6453333497047424\n",
      "Iteration 52270 Training loss 0.08658009022474289 Validation loss 0.13635249435901642 Accuracy 0.6471666693687439\n",
      "Iteration 52280 Training loss 0.08405499905347824 Validation loss 0.13417673110961914 Accuracy 0.6501666903495789\n",
      "Iteration 52290 Training loss 0.08371265977621078 Validation loss 0.129628524184227 Accuracy 0.659166693687439\n",
      "Iteration 52300 Training loss 0.08906853199005127 Validation loss 0.13727876543998718 Accuracy 0.643833339214325\n",
      "Iteration 52310 Training loss 0.08597263693809509 Validation loss 0.1358105093240738 Accuracy 0.6483333110809326\n",
      "Iteration 52320 Training loss 0.0813484713435173 Validation loss 0.12571628391742706 Accuracy 0.6666666865348816\n",
      "Iteration 52330 Training loss 0.0883559137582779 Validation loss 0.13619191944599152 Accuracy 0.6453333497047424\n",
      "Iteration 52340 Training loss 0.08538803458213806 Validation loss 0.14055940508842468 Accuracy 0.6383333206176758\n",
      "Iteration 52350 Training loss 0.08450239896774292 Validation loss 0.13173820078372955 Accuracy 0.6536666750907898\n",
      "Iteration 52360 Training loss 0.08393639326095581 Validation loss 0.1310979276895523 Accuracy 0.6546666622161865\n",
      "Iteration 52370 Training loss 0.08415459841489792 Validation loss 0.13147024810314178 Accuracy 0.6545000076293945\n",
      "Iteration 52380 Training loss 0.08611002564430237 Validation loss 0.13766469061374664 Accuracy 0.6448333263397217\n",
      "Iteration 52390 Training loss 0.09002598375082016 Validation loss 0.14247101545333862 Accuracy 0.6334999799728394\n",
      "Iteration 52400 Training loss 0.08671166002750397 Validation loss 0.13681237399578094 Accuracy 0.6466666460037231\n",
      "Iteration 52410 Training loss 0.08711517602205276 Validation loss 0.13899320363998413 Accuracy 0.6426666378974915\n",
      "Iteration 52420 Training loss 0.08520718663930893 Validation loss 0.12926490604877472 Accuracy 0.659500002861023\n",
      "Iteration 52430 Training loss 0.08783646672964096 Validation loss 0.13937270641326904 Accuracy 0.64083331823349\n",
      "Iteration 52440 Training loss 0.08831419050693512 Validation loss 0.13938696682453156 Accuracy 0.640333354473114\n",
      "Iteration 52450 Training loss 0.0878303050994873 Validation loss 0.13851167261600494 Accuracy 0.6424999833106995\n",
      "Iteration 52460 Training loss 0.0866062119603157 Validation loss 0.13349637389183044 Accuracy 0.6518333554267883\n",
      "Iteration 52470 Training loss 0.08413886278867722 Validation loss 0.13118082284927368 Accuracy 0.6558333039283752\n",
      "Iteration 52480 Training loss 0.08585970103740692 Validation loss 0.13186675310134888 Accuracy 0.6553333401679993\n",
      "Iteration 52490 Training loss 0.08531627058982849 Validation loss 0.1323613077402115 Accuracy 0.6539999842643738\n",
      "Iteration 52500 Training loss 0.08499737828969955 Validation loss 0.12537294626235962 Accuracy 0.6675000190734863\n",
      "Iteration 52510 Training loss 0.08599001914262772 Validation loss 0.12559719383716583 Accuracy 0.6669999957084656\n",
      "Iteration 52520 Training loss 0.08181745558977127 Validation loss 0.119197316467762 Accuracy 0.6804999709129333\n",
      "Iteration 52530 Training loss 0.08984347432851791 Validation loss 0.11479098349809647 Accuracy 0.6868333220481873\n",
      "Iteration 52540 Training loss 0.085284523665905 Validation loss 0.11960402131080627 Accuracy 0.6786666512489319\n",
      "Iteration 52550 Training loss 0.08276253193616867 Validation loss 0.11620648205280304 Accuracy 0.6855000257492065\n",
      "Iteration 52560 Training loss 0.09203364700078964 Validation loss 0.11507151275873184 Accuracy 0.684499979019165\n",
      "Iteration 52570 Training loss 0.08536743372678757 Validation loss 0.11472738534212112 Accuracy 0.6859999895095825\n",
      "Iteration 52580 Training loss 0.08591105043888092 Validation loss 0.11474952846765518 Accuracy 0.6851666569709778\n",
      "Iteration 52590 Training loss 0.08672180026769638 Validation loss 0.11498141288757324 Accuracy 0.6859999895095825\n",
      "Iteration 52600 Training loss 0.08711452037096024 Validation loss 0.11494679003953934 Accuracy 0.6846666932106018\n",
      "Iteration 52610 Training loss 0.08649253100156784 Validation loss 0.11624827235937119 Accuracy 0.6851666569709778\n",
      "Iteration 52620 Training loss 0.09075574576854706 Validation loss 0.11481846123933792 Accuracy 0.687166690826416\n",
      "Iteration 52630 Training loss 0.08344773203134537 Validation loss 0.11812937259674072 Accuracy 0.6801666617393494\n",
      "Iteration 52640 Training loss 0.08237384259700775 Validation loss 0.12216890603303909 Accuracy 0.6728333234786987\n",
      "Iteration 52650 Training loss 0.08342117071151733 Validation loss 0.12597015500068665 Accuracy 0.6656666398048401\n",
      "Iteration 52660 Training loss 0.08842214196920395 Validation loss 0.13231302797794342 Accuracy 0.6528333425521851\n",
      "Iteration 52670 Training loss 0.08770095556974411 Validation loss 0.136875718832016 Accuracy 0.6464999914169312\n",
      "Iteration 52680 Training loss 0.08772392570972443 Validation loss 0.14079350233078003 Accuracy 0.6384999752044678\n",
      "Iteration 52690 Training loss 0.08227968961000443 Validation loss 0.13012242317199707 Accuracy 0.6570000052452087\n",
      "Iteration 52700 Training loss 0.08451427519321442 Validation loss 0.13396617770195007 Accuracy 0.6510000228881836\n",
      "Iteration 52710 Training loss 0.08551329374313354 Validation loss 0.1384112536907196 Accuracy 0.6411666870117188\n",
      "Iteration 52720 Training loss 0.08576324582099915 Validation loss 0.13452495634555817 Accuracy 0.6491666436195374\n",
      "Iteration 52730 Training loss 0.08527166396379471 Validation loss 0.13650643825531006 Accuracy 0.6470000147819519\n",
      "Iteration 52740 Training loss 0.08677531033754349 Validation loss 0.1296679675579071 Accuracy 0.6586666703224182\n",
      "Iteration 52750 Training loss 0.084720179438591 Validation loss 0.13489854335784912 Accuracy 0.6489999890327454\n",
      "Iteration 52760 Training loss 0.09006781876087189 Validation loss 0.14064158499240875 Accuracy 0.6384999752044678\n",
      "Iteration 52770 Training loss 0.08495508879423141 Validation loss 0.13108320534229279 Accuracy 0.6549999713897705\n",
      "Iteration 52780 Training loss 0.088149294257164 Validation loss 0.1400945782661438 Accuracy 0.6383333206176758\n",
      "Iteration 52790 Training loss 0.08927886188030243 Validation loss 0.13830819725990295 Accuracy 0.6416666507720947\n",
      "Iteration 52800 Training loss 0.08511285483837128 Validation loss 0.13344179093837738 Accuracy 0.6521666646003723\n",
      "Iteration 52810 Training loss 0.08574968576431274 Validation loss 0.13170816004276276 Accuracy 0.6549999713897705\n",
      "Iteration 52820 Training loss 0.08553113043308258 Validation loss 0.1361069232225418 Accuracy 0.6463333368301392\n",
      "Iteration 52830 Training loss 0.08563654869794846 Validation loss 0.1290479153394699 Accuracy 0.659333348274231\n",
      "Iteration 52840 Training loss 0.08278654515743256 Validation loss 0.1217137947678566 Accuracy 0.6753333210945129\n",
      "Iteration 52850 Training loss 0.0841764286160469 Validation loss 0.11968345940113068 Accuracy 0.6783333420753479\n",
      "Iteration 52860 Training loss 0.08864535391330719 Validation loss 0.11468817293643951 Accuracy 0.687333345413208\n",
      "Iteration 52870 Training loss 0.08588857203722 Validation loss 0.1160922646522522 Accuracy 0.6861666440963745\n",
      "Iteration 52880 Training loss 0.08294548839330673 Validation loss 0.11642789840698242 Accuracy 0.6848333477973938\n",
      "Iteration 52890 Training loss 0.08847230672836304 Validation loss 0.11469504982233047 Accuracy 0.6875\n",
      "Iteration 52900 Training loss 0.08601034432649612 Validation loss 0.11554106324911118 Accuracy 0.6866666674613953\n",
      "Iteration 52910 Training loss 0.08639200031757355 Validation loss 0.11543217301368713 Accuracy 0.687333345413208\n",
      "Iteration 52920 Training loss 0.08683793246746063 Validation loss 0.11483756452798843 Accuracy 0.6866666674613953\n",
      "Iteration 52930 Training loss 0.08891028165817261 Validation loss 0.11486193537712097 Accuracy 0.687666654586792\n",
      "Iteration 52940 Training loss 0.08632291853427887 Validation loss 0.11586350202560425 Accuracy 0.6859999895095825\n",
      "Iteration 52950 Training loss 0.08420427143573761 Validation loss 0.11524726450443268 Accuracy 0.6846666932106018\n",
      "Iteration 52960 Training loss 0.08760524541139603 Validation loss 0.1149337962269783 Accuracy 0.6866666674613953\n",
      "Iteration 52970 Training loss 0.08788806200027466 Validation loss 0.11492525041103363 Accuracy 0.687333345413208\n",
      "Iteration 52980 Training loss 0.0838700383901596 Validation loss 0.11502465605735779 Accuracy 0.6856666803359985\n",
      "Iteration 52990 Training loss 0.08268141746520996 Validation loss 0.11918492615222931 Accuracy 0.6806666851043701\n",
      "Iteration 53000 Training loss 0.08107790350914001 Validation loss 0.12020859867334366 Accuracy 0.6784999966621399\n",
      "Iteration 53010 Training loss 0.08334986120462418 Validation loss 0.12563656270503998 Accuracy 0.6658333539962769\n",
      "Iteration 53020 Training loss 0.08575739711523056 Validation loss 0.13535340130329132 Accuracy 0.6470000147819519\n",
      "Iteration 53030 Training loss 0.08886411786079407 Validation loss 0.14084003865718842 Accuracy 0.6395000219345093\n",
      "Iteration 53040 Training loss 0.08772581070661545 Validation loss 0.13443300127983093 Accuracy 0.6504999995231628\n",
      "Iteration 53050 Training loss 0.08448200672864914 Validation loss 0.1321980357170105 Accuracy 0.6543333530426025\n",
      "Iteration 53060 Training loss 0.08761373162269592 Validation loss 0.14072762429714203 Accuracy 0.6384999752044678\n",
      "Iteration 53070 Training loss 0.0859149917960167 Validation loss 0.13576950132846832 Accuracy 0.6464999914169312\n",
      "Iteration 53080 Training loss 0.08632341772317886 Validation loss 0.1322391778230667 Accuracy 0.6541666388511658\n",
      "Iteration 53090 Training loss 0.08490663766860962 Validation loss 0.1274358332157135 Accuracy 0.6628333330154419\n",
      "Iteration 53100 Training loss 0.08362892270088196 Validation loss 0.13729527592658997 Accuracy 0.6439999938011169\n",
      "Iteration 53110 Training loss 0.08994316309690475 Validation loss 0.13972702622413635 Accuracy 0.640500009059906\n",
      "Iteration 53120 Training loss 0.08256600797176361 Validation loss 0.13204075396060944 Accuracy 0.6543333530426025\n",
      "Iteration 53130 Training loss 0.08413573354482651 Validation loss 0.12466461211442947 Accuracy 0.668666660785675\n",
      "Iteration 53140 Training loss 0.083809994161129 Validation loss 0.1159001886844635 Accuracy 0.6855000257492065\n",
      "Iteration 53150 Training loss 0.08968301117420197 Validation loss 0.1147395595908165 Accuracy 0.6880000233650208\n",
      "Iteration 53160 Training loss 0.08352609723806381 Validation loss 0.11499641835689545 Accuracy 0.6859999895095825\n",
      "Iteration 53170 Training loss 0.0868103876709938 Validation loss 0.1148388683795929 Accuracy 0.687166690826416\n",
      "Iteration 53180 Training loss 0.08618053048849106 Validation loss 0.11533872783184052 Accuracy 0.6855000257492065\n",
      "Iteration 53190 Training loss 0.08549463003873825 Validation loss 0.11601638048887253 Accuracy 0.6855000257492065\n",
      "Iteration 53200 Training loss 0.08488256484270096 Validation loss 0.1152840405702591 Accuracy 0.6855000257492065\n",
      "Iteration 53210 Training loss 0.08470798283815384 Validation loss 0.11626113951206207 Accuracy 0.6853333115577698\n",
      "Iteration 53220 Training loss 0.08468998223543167 Validation loss 0.11721623688936234 Accuracy 0.6819999814033508\n",
      "Iteration 53230 Training loss 0.08462125808000565 Validation loss 0.11651558429002762 Accuracy 0.6834999918937683\n",
      "Iteration 53240 Training loss 0.08870059251785278 Validation loss 0.11487707495689392 Accuracy 0.687166690826416\n",
      "Iteration 53250 Training loss 0.08532524108886719 Validation loss 0.11535980552434921 Accuracy 0.6853333115577698\n",
      "Iteration 53260 Training loss 0.08363115787506104 Validation loss 0.116169273853302 Accuracy 0.6846666932106018\n",
      "Iteration 53270 Training loss 0.09200213849544525 Validation loss 0.11515248566865921 Accuracy 0.6868333220481873\n",
      "Iteration 53280 Training loss 0.08538134396076202 Validation loss 0.11509743332862854 Accuracy 0.687666654586792\n",
      "Iteration 53290 Training loss 0.08656246215105057 Validation loss 0.11523693799972534 Accuracy 0.6866666674613953\n",
      "Iteration 53300 Training loss 0.08381199836730957 Validation loss 0.11623476445674896 Accuracy 0.6840000152587891\n",
      "Iteration 53310 Training loss 0.08557328581809998 Validation loss 0.11592257022857666 Accuracy 0.6856666803359985\n",
      "Iteration 53320 Training loss 0.08427543193101883 Validation loss 0.11553451418876648 Accuracy 0.6859999895095825\n",
      "Iteration 53330 Training loss 0.08297606557607651 Validation loss 0.11629898846149445 Accuracy 0.684166669845581\n",
      "Iteration 53340 Training loss 0.08519048988819122 Validation loss 0.11466401815414429 Accuracy 0.687333345413208\n",
      "Iteration 53350 Training loss 0.0890011414885521 Validation loss 0.11476270854473114 Accuracy 0.6881666779518127\n",
      "Iteration 53360 Training loss 0.08469540625810623 Validation loss 0.11624721437692642 Accuracy 0.6851666569709778\n",
      "Iteration 53370 Training loss 0.08359965682029724 Validation loss 0.121140256524086 Accuracy 0.6763333082199097\n",
      "Iteration 53380 Training loss 0.08463970571756363 Validation loss 0.12826742231845856 Accuracy 0.6614999771118164\n",
      "Iteration 53390 Training loss 0.0868319720029831 Validation loss 0.13707755506038666 Accuracy 0.6449999809265137\n",
      "Iteration 53400 Training loss 0.08361175656318665 Validation loss 0.13137967884540558 Accuracy 0.656499981880188\n",
      "Iteration 53410 Training loss 0.088467538356781 Validation loss 0.14016452431678772 Accuracy 0.6391666531562805\n",
      "Iteration 53420 Training loss 0.08566900342702866 Validation loss 0.13512255251407623 Accuracy 0.6481666564941406\n",
      "Iteration 53430 Training loss 0.08905180543661118 Validation loss 0.13959725201129913 Accuracy 0.6418333053588867\n",
      "Iteration 53440 Training loss 0.08801140636205673 Validation loss 0.1334415078163147 Accuracy 0.6514999866485596\n",
      "Iteration 53450 Training loss 0.08729695528745651 Validation loss 0.13577501475811005 Accuracy 0.6458333134651184\n",
      "Iteration 53460 Training loss 0.0836598128080368 Validation loss 0.12713004648685455 Accuracy 0.6638333201408386\n",
      "Iteration 53470 Training loss 0.08597809821367264 Validation loss 0.13393479585647583 Accuracy 0.6501666903495789\n",
      "Iteration 53480 Training loss 0.08641231060028076 Validation loss 0.13406962156295776 Accuracy 0.6506666541099548\n",
      "Iteration 53490 Training loss 0.0889071673154831 Validation loss 0.14122889935970306 Accuracy 0.6378333568572998\n",
      "Iteration 53500 Training loss 0.08524321019649506 Validation loss 0.13574276864528656 Accuracy 0.6456666588783264\n",
      "Iteration 53510 Training loss 0.08481300622224808 Validation loss 0.12793220579624176 Accuracy 0.6626666784286499\n",
      "Iteration 53520 Training loss 0.0828520730137825 Validation loss 0.12619033455848694 Accuracy 0.6643333435058594\n",
      "Iteration 53530 Training loss 0.08382833003997803 Validation loss 0.12143097072839737 Accuracy 0.6788333058357239\n",
      "Iteration 53540 Training loss 0.08247614651918411 Validation loss 0.1212894394993782 Accuracy 0.6773333549499512\n",
      "Iteration 53550 Training loss 0.08478046208620071 Validation loss 0.11738964915275574 Accuracy 0.6798333525657654\n",
      "Iteration 53560 Training loss 0.09128280729055405 Validation loss 0.11520564556121826 Accuracy 0.6856666803359985\n",
      "Iteration 53570 Training loss 0.08443804830312729 Validation loss 0.11601763963699341 Accuracy 0.6865000128746033\n",
      "Iteration 53580 Training loss 0.08631568402051926 Validation loss 0.11518504470586777 Accuracy 0.6863333582878113\n",
      "Iteration 53590 Training loss 0.09061253070831299 Validation loss 0.1148226335644722 Accuracy 0.6859999895095825\n",
      "Iteration 53600 Training loss 0.08479561656713486 Validation loss 0.11517604440450668 Accuracy 0.6846666932106018\n",
      "Iteration 53610 Training loss 0.08696512877941132 Validation loss 0.11485213041305542 Accuracy 0.6865000128746033\n",
      "Iteration 53620 Training loss 0.08381565660238266 Validation loss 0.11552585661411285 Accuracy 0.6868333220481873\n",
      "Iteration 53630 Training loss 0.08489563316106796 Validation loss 0.1148369312286377 Accuracy 0.6855000257492065\n",
      "Iteration 53640 Training loss 0.08712033927440643 Validation loss 0.11517200618982315 Accuracy 0.6859999895095825\n",
      "Iteration 53650 Training loss 0.08375352621078491 Validation loss 0.11630448698997498 Accuracy 0.6853333115577698\n",
      "Iteration 53660 Training loss 0.08432614058256149 Validation loss 0.11642048507928848 Accuracy 0.6851666569709778\n",
      "Iteration 53670 Training loss 0.08410780131816864 Validation loss 0.11525969207286835 Accuracy 0.6869999766349792\n",
      "Iteration 53680 Training loss 0.0870802029967308 Validation loss 0.11471384018659592 Accuracy 0.6868333220481873\n",
      "Iteration 53690 Training loss 0.08788816630840302 Validation loss 0.11503177881240845 Accuracy 0.6868333220481873\n",
      "Iteration 53700 Training loss 0.08244652301073074 Validation loss 0.1155739575624466 Accuracy 0.6861666440963745\n",
      "Iteration 53710 Training loss 0.08620939403772354 Validation loss 0.11614347249269485 Accuracy 0.6855000257492065\n",
      "Iteration 53720 Training loss 0.08766710758209229 Validation loss 0.11568833142518997 Accuracy 0.6861666440963745\n",
      "Iteration 53730 Training loss 0.08374688029289246 Validation loss 0.1155300885438919 Accuracy 0.6869999766349792\n",
      "Iteration 53740 Training loss 0.08513523638248444 Validation loss 0.11527067422866821 Accuracy 0.6851666569709778\n",
      "Iteration 53750 Training loss 0.08279948681592941 Validation loss 0.11667206138372421 Accuracy 0.6831666827201843\n",
      "Iteration 53760 Training loss 0.08573267608880997 Validation loss 0.1150449588894844 Accuracy 0.6861666440963745\n",
      "Iteration 53770 Training loss 0.08795154094696045 Validation loss 0.11490889638662338 Accuracy 0.687666654586792\n",
      "Iteration 53780 Training loss 0.08606920391321182 Validation loss 0.11489027738571167 Accuracy 0.687166690826416\n",
      "Iteration 53790 Training loss 0.08645225316286087 Validation loss 0.1150115579366684 Accuracy 0.6866666674613953\n",
      "Iteration 53800 Training loss 0.08568503707647324 Validation loss 0.11497332900762558 Accuracy 0.6863333582878113\n",
      "Iteration 53810 Training loss 0.08606653660535812 Validation loss 0.11620751768350601 Accuracy 0.684333324432373\n",
      "Iteration 53820 Training loss 0.08407711237668991 Validation loss 0.11583061516284943 Accuracy 0.6858333349227905\n",
      "Iteration 53830 Training loss 0.08559581637382507 Validation loss 0.11527448892593384 Accuracy 0.6863333582878113\n",
      "Iteration 53840 Training loss 0.0854327380657196 Validation loss 0.11735290288925171 Accuracy 0.6813333630561829\n",
      "Iteration 53850 Training loss 0.08215229958295822 Validation loss 0.12072938680648804 Accuracy 0.6786666512489319\n",
      "Iteration 53860 Training loss 0.08879295736551285 Validation loss 0.11477568745613098 Accuracy 0.6890000104904175\n",
      "Iteration 53870 Training loss 0.08810596913099289 Validation loss 0.11503829807043076 Accuracy 0.6859999895095825\n",
      "Iteration 53880 Training loss 0.08588448166847229 Validation loss 0.11539548635482788 Accuracy 0.6853333115577698\n",
      "Iteration 53890 Training loss 0.08421000838279724 Validation loss 0.11781828105449677 Accuracy 0.6808333396911621\n",
      "Iteration 53900 Training loss 0.08306335657835007 Validation loss 0.11656441539525986 Accuracy 0.6838333606719971\n",
      "Iteration 53910 Training loss 0.08431477099657059 Validation loss 0.11558565497398376 Accuracy 0.6868333220481873\n",
      "Iteration 53920 Training loss 0.08322829008102417 Validation loss 0.11682002991437912 Accuracy 0.6823333501815796\n",
      "Iteration 53930 Training loss 0.0876392126083374 Validation loss 0.11473286896944046 Accuracy 0.6881666779518127\n",
      "Iteration 53940 Training loss 0.08794770389795303 Validation loss 0.11524692922830582 Accuracy 0.6859999895095825\n",
      "Iteration 53950 Training loss 0.08726485073566437 Validation loss 0.11514437198638916 Accuracy 0.6868333220481873\n",
      "Iteration 53960 Training loss 0.08587538450956345 Validation loss 0.11490210890769958 Accuracy 0.6859999895095825\n",
      "Iteration 53970 Training loss 0.08204589039087296 Validation loss 0.11719805002212524 Accuracy 0.6831666827201843\n",
      "Iteration 53980 Training loss 0.08634370565414429 Validation loss 0.11557728797197342 Accuracy 0.6863333582878113\n",
      "Iteration 53990 Training loss 0.08747214078903198 Validation loss 0.11542385816574097 Accuracy 0.6863333582878113\n",
      "Iteration 54000 Training loss 0.0839887410402298 Validation loss 0.1169237494468689 Accuracy 0.6828333139419556\n",
      "Iteration 54010 Training loss 0.08343269675970078 Validation loss 0.11719384789466858 Accuracy 0.6804999709129333\n",
      "Iteration 54020 Training loss 0.08741445094347 Validation loss 0.1149301528930664 Accuracy 0.6855000257492065\n",
      "Iteration 54030 Training loss 0.08644429594278336 Validation loss 0.1146160364151001 Accuracy 0.6868333220481873\n",
      "Iteration 54040 Training loss 0.08420858532190323 Validation loss 0.11644486337900162 Accuracy 0.6830000281333923\n",
      "Iteration 54050 Training loss 0.08754389733076096 Validation loss 0.11535530537366867 Accuracy 0.6863333582878113\n",
      "Iteration 54060 Training loss 0.08512227237224579 Validation loss 0.11536360532045364 Accuracy 0.6861666440963745\n",
      "Iteration 54070 Training loss 0.08470790833234787 Validation loss 0.11719898134469986 Accuracy 0.6811666488647461\n",
      "Iteration 54080 Training loss 0.08960982412099838 Validation loss 0.1148122176527977 Accuracy 0.6888333559036255\n",
      "Iteration 54090 Training loss 0.08607025444507599 Validation loss 0.11495915055274963 Accuracy 0.6883333325386047\n",
      "Iteration 54100 Training loss 0.08444134891033173 Validation loss 0.1154317706823349 Accuracy 0.6865000128746033\n",
      "Iteration 54110 Training loss 0.0846809670329094 Validation loss 0.1155533716082573 Accuracy 0.6863333582878113\n",
      "Iteration 54120 Training loss 0.08421002328395844 Validation loss 0.11506982147693634 Accuracy 0.6861666440963745\n",
      "Iteration 54130 Training loss 0.08305604755878448 Validation loss 0.11915409564971924 Accuracy 0.6800000071525574\n",
      "Iteration 54140 Training loss 0.08163148164749146 Validation loss 0.1234874576330185 Accuracy 0.671833336353302\n",
      "Iteration 54150 Training loss 0.08258533477783203 Validation loss 0.12394418567419052 Accuracy 0.6703333258628845\n",
      "Iteration 54160 Training loss 0.09245730936527252 Validation loss 0.14696069061756134 Accuracy 0.624666690826416\n",
      "Iteration 54170 Training loss 0.08564115315675735 Validation loss 0.13361558318138123 Accuracy 0.6518333554267883\n",
      "Iteration 54180 Training loss 0.09108525514602661 Validation loss 0.14297395944595337 Accuracy 0.6349999904632568\n",
      "Iteration 54190 Training loss 0.08481459319591522 Validation loss 0.13950766623020172 Accuracy 0.640500009059906\n",
      "Iteration 54200 Training loss 0.08297277987003326 Validation loss 0.12700331211090088 Accuracy 0.6653333306312561\n",
      "Iteration 54210 Training loss 0.08520079404115677 Validation loss 0.13570472598075867 Accuracy 0.6491666436195374\n",
      "Iteration 54220 Training loss 0.08143926411867142 Validation loss 0.1268470585346222 Accuracy 0.6648333072662354\n",
      "Iteration 54230 Training loss 0.0819915160536766 Validation loss 0.12130521982908249 Accuracy 0.6761666536331177\n",
      "Iteration 54240 Training loss 0.08328733593225479 Validation loss 0.12273768335580826 Accuracy 0.6736666560173035\n",
      "Iteration 54250 Training loss 0.08865275233983994 Validation loss 0.1147971823811531 Accuracy 0.6865000128746033\n",
      "Iteration 54260 Training loss 0.08811204880475998 Validation loss 0.1150348111987114 Accuracy 0.6850000023841858\n",
      "Iteration 54270 Training loss 0.08769726753234863 Validation loss 0.11481576412916183 Accuracy 0.687333345413208\n",
      "Iteration 54280 Training loss 0.08718322962522507 Validation loss 0.11487768590450287 Accuracy 0.6861666440963745\n",
      "Iteration 54290 Training loss 0.08435308188199997 Validation loss 0.11551251262426376 Accuracy 0.6859999895095825\n",
      "Iteration 54300 Training loss 0.08312030881643295 Validation loss 0.117165707051754 Accuracy 0.684166669845581\n",
      "Iteration 54310 Training loss 0.08393959701061249 Validation loss 0.11907804012298584 Accuracy 0.6804999709129333\n",
      "Iteration 54320 Training loss 0.08589130640029907 Validation loss 0.11536838114261627 Accuracy 0.6855000257492065\n",
      "Iteration 54330 Training loss 0.08267959952354431 Validation loss 0.11920271813869476 Accuracy 0.6784999966621399\n",
      "Iteration 54340 Training loss 0.08193733543157578 Validation loss 0.1193489357829094 Accuracy 0.6793333292007446\n",
      "Iteration 54350 Training loss 0.08859222382307053 Validation loss 0.11475241929292679 Accuracy 0.6884999871253967\n",
      "Iteration 54360 Training loss 0.08720481395721436 Validation loss 0.11603780090808868 Accuracy 0.6851666569709778\n",
      "Iteration 54370 Training loss 0.09102421253919601 Validation loss 0.1149054542183876 Accuracy 0.687666654586792\n",
      "Iteration 54380 Training loss 0.08857111632823944 Validation loss 0.11510403454303741 Accuracy 0.6856666803359985\n",
      "Iteration 54390 Training loss 0.08334766328334808 Validation loss 0.11760265380144119 Accuracy 0.6831666827201843\n",
      "Iteration 54400 Training loss 0.08233851939439774 Validation loss 0.11689329892396927 Accuracy 0.6836666464805603\n",
      "Iteration 54410 Training loss 0.08758239448070526 Validation loss 0.11563864350318909 Accuracy 0.6875\n",
      "Iteration 54420 Training loss 0.08558201789855957 Validation loss 0.11502819508314133 Accuracy 0.684166669845581\n",
      "Iteration 54430 Training loss 0.08670342713594437 Validation loss 0.1148265078663826 Accuracy 0.6875\n",
      "Iteration 54440 Training loss 0.0837460532784462 Validation loss 0.11519040912389755 Accuracy 0.6848333477973938\n",
      "Iteration 54450 Training loss 0.08536257594823837 Validation loss 0.11594273149967194 Accuracy 0.6866666674613953\n",
      "Iteration 54460 Training loss 0.0878940224647522 Validation loss 0.11496371775865555 Accuracy 0.6863333582878113\n",
      "Iteration 54470 Training loss 0.08702591061592102 Validation loss 0.11501749604940414 Accuracy 0.6866666674613953\n",
      "Iteration 54480 Training loss 0.08252036571502686 Validation loss 0.11788762360811234 Accuracy 0.6808333396911621\n",
      "Iteration 54490 Training loss 0.08598434180021286 Validation loss 0.11502131074666977 Accuracy 0.687666654586792\n",
      "Iteration 54500 Training loss 0.09077459573745728 Validation loss 0.11469385027885437 Accuracy 0.6869999766349792\n",
      "Iteration 54510 Training loss 0.0864192321896553 Validation loss 0.1153465285897255 Accuracy 0.6856666803359985\n",
      "Iteration 54520 Training loss 0.08432537317276001 Validation loss 0.11667148023843765 Accuracy 0.6848333477973938\n",
      "Iteration 54530 Training loss 0.08254002034664154 Validation loss 0.11606001108884811 Accuracy 0.6856666803359985\n",
      "Iteration 54540 Training loss 0.08557964861392975 Validation loss 0.11531058698892593 Accuracy 0.6853333115577698\n",
      "Iteration 54550 Training loss 0.08434147387742996 Validation loss 0.11673806607723236 Accuracy 0.6855000257492065\n",
      "Iteration 54560 Training loss 0.08491317927837372 Validation loss 0.11662670224905014 Accuracy 0.6840000152587891\n",
      "Iteration 54570 Training loss 0.08365735411643982 Validation loss 0.11632245033979416 Accuracy 0.6846666932106018\n",
      "Iteration 54580 Training loss 0.08290593326091766 Validation loss 0.11870881915092468 Accuracy 0.6793333292007446\n",
      "Iteration 54590 Training loss 0.08632918447256088 Validation loss 0.1155753806233406 Accuracy 0.6853333115577698\n",
      "Iteration 54600 Training loss 0.08614199608564377 Validation loss 0.1161632314324379 Accuracy 0.6850000023841858\n",
      "Iteration 54610 Training loss 0.08897887170314789 Validation loss 0.11470994353294373 Accuracy 0.6861666440963745\n",
      "Iteration 54620 Training loss 0.08826062828302383 Validation loss 0.11473000794649124 Accuracy 0.6865000128746033\n",
      "Iteration 54630 Training loss 0.08439455926418304 Validation loss 0.11502178013324738 Accuracy 0.6884999871253967\n",
      "Iteration 54640 Training loss 0.08596528321504593 Validation loss 0.11504266411066055 Accuracy 0.6881666779518127\n",
      "Iteration 54650 Training loss 0.08156213909387589 Validation loss 0.1164020299911499 Accuracy 0.6836666464805603\n",
      "Iteration 54660 Training loss 0.08563385158777237 Validation loss 0.11533305048942566 Accuracy 0.6869999766349792\n",
      "Iteration 54670 Training loss 0.08233214169740677 Validation loss 0.1170540377497673 Accuracy 0.6823333501815796\n",
      "Iteration 54680 Training loss 0.08275328576564789 Validation loss 0.11774230003356934 Accuracy 0.6819999814033508\n",
      "Iteration 54690 Training loss 0.08171544969081879 Validation loss 0.1231762170791626 Accuracy 0.671500027179718\n",
      "Iteration 54700 Training loss 0.08263244479894638 Validation loss 0.11884506046772003 Accuracy 0.6775000095367432\n",
      "Iteration 54710 Training loss 0.08889186382293701 Validation loss 0.11497937142848969 Accuracy 0.6886666417121887\n",
      "Iteration 54720 Training loss 0.09198345243930817 Validation loss 0.11473052948713303 Accuracy 0.687833309173584\n",
      "Iteration 54730 Training loss 0.08663155138492584 Validation loss 0.11523604393005371 Accuracy 0.6868333220481873\n",
      "Iteration 54740 Training loss 0.08573630452156067 Validation loss 0.11503322422504425 Accuracy 0.687333345413208\n",
      "Iteration 54750 Training loss 0.08780179172754288 Validation loss 0.11476511508226395 Accuracy 0.687166690826416\n",
      "Iteration 54760 Training loss 0.0853046104311943 Validation loss 0.11516647040843964 Accuracy 0.6855000257492065\n",
      "Iteration 54770 Training loss 0.08406352996826172 Validation loss 0.11773334443569183 Accuracy 0.6801666617393494\n",
      "Iteration 54780 Training loss 0.08591059595346451 Validation loss 0.11541872471570969 Accuracy 0.6861666440963745\n",
      "Iteration 54790 Training loss 0.0880315974354744 Validation loss 0.11494551599025726 Accuracy 0.6883333325386047\n",
      "Iteration 54800 Training loss 0.08698225766420364 Validation loss 0.11513763666152954 Accuracy 0.687166690826416\n",
      "Iteration 54810 Training loss 0.08557529747486115 Validation loss 0.11494731903076172 Accuracy 0.6886666417121887\n",
      "Iteration 54820 Training loss 0.0842968001961708 Validation loss 0.11583667993545532 Accuracy 0.687166690826416\n",
      "Iteration 54830 Training loss 0.08546898514032364 Validation loss 0.1187405213713646 Accuracy 0.6783333420753479\n",
      "Iteration 54840 Training loss 0.08501026779413223 Validation loss 0.1163109615445137 Accuracy 0.6850000023841858\n",
      "Iteration 54850 Training loss 0.08765311539173126 Validation loss 0.11475950479507446 Accuracy 0.6859999895095825\n",
      "Iteration 54860 Training loss 0.08448630571365356 Validation loss 0.11552184820175171 Accuracy 0.6866666674613953\n",
      "Iteration 54870 Training loss 0.0823323056101799 Validation loss 0.11742788553237915 Accuracy 0.6808333396911621\n",
      "Iteration 54880 Training loss 0.0846700444817543 Validation loss 0.11495250463485718 Accuracy 0.687166690826416\n",
      "Iteration 54890 Training loss 0.08785455673933029 Validation loss 0.11500832438468933 Accuracy 0.6855000257492065\n",
      "Iteration 54900 Training loss 0.08388906717300415 Validation loss 0.11739394068717957 Accuracy 0.6825000047683716\n",
      "Iteration 54910 Training loss 0.08959835022687912 Validation loss 0.11501265317201614 Accuracy 0.6888333559036255\n",
      "Iteration 54920 Training loss 0.08233623206615448 Validation loss 0.11967553943395615 Accuracy 0.6773333549499512\n",
      "Iteration 54930 Training loss 0.08161631226539612 Validation loss 0.1205960065126419 Accuracy 0.6779999732971191\n",
      "Iteration 54940 Training loss 0.08770802617073059 Validation loss 0.1149272546172142 Accuracy 0.687666654586792\n",
      "Iteration 54950 Training loss 0.0837232694029808 Validation loss 0.11649245023727417 Accuracy 0.6850000023841858\n",
      "Iteration 54960 Training loss 0.08386456966400146 Validation loss 0.11722783744335175 Accuracy 0.6816666722297668\n",
      "Iteration 54970 Training loss 0.0840810015797615 Validation loss 0.11514223366975784 Accuracy 0.6884999871253967\n",
      "Iteration 54980 Training loss 0.0858849510550499 Validation loss 0.11535029113292694 Accuracy 0.687333345413208\n",
      "Iteration 54990 Training loss 0.0819971114397049 Validation loss 0.11809815466403961 Accuracy 0.6808333396911621\n",
      "Iteration 55000 Training loss 0.08807864040136337 Validation loss 0.11525183916091919 Accuracy 0.6856666803359985\n",
      "Iteration 55010 Training loss 0.08761072903871536 Validation loss 0.11488364636898041 Accuracy 0.687833309173584\n",
      "Iteration 55020 Training loss 0.08831878006458282 Validation loss 0.11491357535123825 Accuracy 0.6865000128746033\n",
      "Iteration 55030 Training loss 0.08695662766695023 Validation loss 0.11494676023721695 Accuracy 0.687166690826416\n",
      "Iteration 55040 Training loss 0.08825597167015076 Validation loss 0.11512414366006851 Accuracy 0.6869999766349792\n",
      "Iteration 55050 Training loss 0.08883538097143173 Validation loss 0.1150410994887352 Accuracy 0.6886666417121887\n",
      "Iteration 55060 Training loss 0.08200716227293015 Validation loss 0.11709335446357727 Accuracy 0.6838333606719971\n",
      "Iteration 55070 Training loss 0.08183328062295914 Validation loss 0.11696814000606537 Accuracy 0.684499979019165\n",
      "Iteration 55080 Training loss 0.0865643098950386 Validation loss 0.1155942901968956 Accuracy 0.6850000023841858\n",
      "Iteration 55090 Training loss 0.08114010095596313 Validation loss 0.11730539053678513 Accuracy 0.6823333501815796\n",
      "Iteration 55100 Training loss 0.08221021294593811 Validation loss 0.11777260154485703 Accuracy 0.6830000281333923\n",
      "Iteration 55110 Training loss 0.08289439231157303 Validation loss 0.128400519490242 Accuracy 0.6611666679382324\n",
      "Iteration 55120 Training loss 0.08428671956062317 Validation loss 0.13286975026130676 Accuracy 0.6539999842643738\n",
      "Iteration 55130 Training loss 0.09035688638687134 Validation loss 0.1454850435256958 Accuracy 0.6291666626930237\n",
      "Iteration 55140 Training loss 0.08857889473438263 Validation loss 0.14314362406730652 Accuracy 0.6340000033378601\n",
      "Iteration 55150 Training loss 0.08633992820978165 Validation loss 0.13677789270877838 Accuracy 0.6453333497047424\n",
      "Iteration 55160 Training loss 0.08761543035507202 Validation loss 0.14234936237335205 Accuracy 0.6358333230018616\n",
      "Iteration 55170 Training loss 0.08405826985836029 Validation loss 0.1337897777557373 Accuracy 0.6506666541099548\n",
      "Iteration 55180 Training loss 0.08650440722703934 Validation loss 0.13411946594715118 Accuracy 0.6493333578109741\n",
      "Iteration 55190 Training loss 0.08317448198795319 Validation loss 0.13283303380012512 Accuracy 0.6510000228881836\n",
      "Iteration 55200 Training loss 0.08598077297210693 Validation loss 0.13184499740600586 Accuracy 0.6536666750907898\n",
      "Iteration 55210 Training loss 0.08839617669582367 Validation loss 0.14199931919574738 Accuracy 0.6361666917800903\n",
      "Iteration 55220 Training loss 0.08953559398651123 Validation loss 0.13664306700229645 Accuracy 0.6466666460037231\n",
      "Iteration 55230 Training loss 0.08338388800621033 Validation loss 0.13450950384140015 Accuracy 0.6503333449363708\n",
      "Iteration 55240 Training loss 0.08573997765779495 Validation loss 0.13001161813735962 Accuracy 0.659166693687439\n",
      "Iteration 55250 Training loss 0.08262166380882263 Validation loss 0.12633000314235687 Accuracy 0.6666666865348816\n",
      "Iteration 55260 Training loss 0.08260230720043182 Validation loss 0.13441020250320435 Accuracy 0.6483333110809326\n",
      "Iteration 55270 Training loss 0.08690141141414642 Validation loss 0.13726657629013062 Accuracy 0.6455000042915344\n",
      "Iteration 55280 Training loss 0.08480764925479889 Validation loss 0.13731542229652405 Accuracy 0.6458333134651184\n",
      "Iteration 55290 Training loss 0.0821860060095787 Validation loss 0.13296203315258026 Accuracy 0.6520000100135803\n",
      "Iteration 55300 Training loss 0.08445406705141068 Validation loss 0.13422243297100067 Accuracy 0.6504999995231628\n",
      "Iteration 55310 Training loss 0.08550471812486649 Validation loss 0.1371815800666809 Accuracy 0.6445000171661377\n",
      "Iteration 55320 Training loss 0.08389092981815338 Validation loss 0.1290922611951828 Accuracy 0.659333348274231\n",
      "Iteration 55330 Training loss 0.08951766788959503 Validation loss 0.14022476971149445 Accuracy 0.6388333439826965\n",
      "Iteration 55340 Training loss 0.09228989481925964 Validation loss 0.14698775112628937 Accuracy 0.6265000104904175\n",
      "Iteration 55350 Training loss 0.08564689010381699 Validation loss 0.1342182755470276 Accuracy 0.6503333449363708\n",
      "Iteration 55360 Training loss 0.08297699689865112 Validation loss 0.12650111317634583 Accuracy 0.6675000190734863\n",
      "Iteration 55370 Training loss 0.08564590662717819 Validation loss 0.13048984110355377 Accuracy 0.656166672706604\n",
      "Iteration 55380 Training loss 0.0856141597032547 Validation loss 0.1415490359067917 Accuracy 0.6355000138282776\n",
      "Iteration 55390 Training loss 0.08426383137702942 Validation loss 0.13402608036994934 Accuracy 0.6498333215713501\n",
      "Iteration 55400 Training loss 0.08369932323694229 Validation loss 0.13138127326965332 Accuracy 0.656166672706604\n",
      "Iteration 55410 Training loss 0.08194682747125626 Validation loss 0.12382397800683975 Accuracy 0.6708333492279053\n",
      "Iteration 55420 Training loss 0.08103778213262558 Validation loss 0.12284088134765625 Accuracy 0.6735000014305115\n",
      "Iteration 55430 Training loss 0.08192858099937439 Validation loss 0.12475944310426712 Accuracy 0.668833315372467\n",
      "Iteration 55440 Training loss 0.08717106282711029 Validation loss 0.1150299459695816 Accuracy 0.687166690826416\n",
      "Iteration 55450 Training loss 0.08788415789604187 Validation loss 0.115456722676754 Accuracy 0.6861666440963745\n",
      "Iteration 55460 Training loss 0.08521785587072372 Validation loss 0.1153850331902504 Accuracy 0.6856666803359985\n",
      "Iteration 55470 Training loss 0.08502021431922913 Validation loss 0.1152971163392067 Accuracy 0.687166690826416\n",
      "Iteration 55480 Training loss 0.0892634466290474 Validation loss 0.11503926664590836 Accuracy 0.687666654586792\n",
      "Iteration 55490 Training loss 0.08926224708557129 Validation loss 0.11498530954122543 Accuracy 0.6896666884422302\n",
      "Iteration 55500 Training loss 0.08520352840423584 Validation loss 0.11789769679307938 Accuracy 0.6809999942779541\n",
      "Iteration 55510 Training loss 0.0856967568397522 Validation loss 0.11537543684244156 Accuracy 0.6861666440963745\n",
      "Iteration 55520 Training loss 0.08294068276882172 Validation loss 0.11691591143608093 Accuracy 0.6848333477973938\n",
      "Iteration 55530 Training loss 0.08794592320919037 Validation loss 0.11495668441057205 Accuracy 0.6891666650772095\n",
      "Iteration 55540 Training loss 0.08478134870529175 Validation loss 0.11573895066976547 Accuracy 0.6855000257492065\n",
      "Iteration 55550 Training loss 0.08500277251005173 Validation loss 0.11535225063562393 Accuracy 0.6863333582878113\n",
      "Iteration 55560 Training loss 0.08411452919244766 Validation loss 0.11790681630373001 Accuracy 0.6809999942779541\n",
      "Iteration 55570 Training loss 0.08617991209030151 Validation loss 0.11524675786495209 Accuracy 0.6866666674613953\n",
      "Iteration 55580 Training loss 0.08710724115371704 Validation loss 0.11534563452005386 Accuracy 0.6865000128746033\n",
      "Iteration 55590 Training loss 0.08264579623937607 Validation loss 0.12022355198860168 Accuracy 0.6788333058357239\n",
      "Iteration 55600 Training loss 0.08324601501226425 Validation loss 0.11681294441223145 Accuracy 0.6831666827201843\n",
      "Iteration 55610 Training loss 0.08948104828596115 Validation loss 0.11506719887256622 Accuracy 0.687666654586792\n",
      "Iteration 55620 Training loss 0.08477387577295303 Validation loss 0.11494779586791992 Accuracy 0.6891666650772095\n",
      "Iteration 55630 Training loss 0.08469798415899277 Validation loss 0.11550308018922806 Accuracy 0.6861666440963745\n",
      "Iteration 55640 Training loss 0.08535493165254593 Validation loss 0.11609868705272675 Accuracy 0.6866666674613953\n",
      "Iteration 55650 Training loss 0.08409276604652405 Validation loss 0.12135355919599533 Accuracy 0.6771666407585144\n",
      "Iteration 55660 Training loss 0.08616161346435547 Validation loss 0.1381121575832367 Accuracy 0.6431666612625122\n",
      "Iteration 55670 Training loss 0.08332466334104538 Validation loss 0.1288561373949051 Accuracy 0.6603333353996277\n",
      "Iteration 55680 Training loss 0.08384918421506882 Validation loss 0.12755483388900757 Accuracy 0.6628333330154419\n",
      "Iteration 55690 Training loss 0.09101156890392303 Validation loss 0.14646980166435242 Accuracy 0.6266666650772095\n",
      "Iteration 55700 Training loss 0.08587878197431564 Validation loss 0.13479220867156982 Accuracy 0.6488333344459534\n",
      "Iteration 55710 Training loss 0.08253879100084305 Validation loss 0.13248416781425476 Accuracy 0.6541666388511658\n",
      "Iteration 55720 Training loss 0.08508801460266113 Validation loss 0.1348978877067566 Accuracy 0.6495000123977661\n",
      "Iteration 55730 Training loss 0.08508825302124023 Validation loss 0.13331195712089539 Accuracy 0.6523333191871643\n",
      "Iteration 55740 Training loss 0.0853477194905281 Validation loss 0.13865825533866882 Accuracy 0.640999972820282\n",
      "Iteration 55750 Training loss 0.0847162976861 Validation loss 0.13257542252540588 Accuracy 0.6546666622161865\n",
      "Iteration 55760 Training loss 0.08441649377346039 Validation loss 0.13472405076026917 Accuracy 0.6501666903495789\n",
      "Iteration 55770 Training loss 0.08811000734567642 Validation loss 0.13903313875198364 Accuracy 0.6416666507720947\n",
      "Iteration 55780 Training loss 0.0846347063779831 Validation loss 0.13088451325893402 Accuracy 0.6558333039283752\n",
      "Iteration 55790 Training loss 0.08699874579906464 Validation loss 0.14092165231704712 Accuracy 0.6384999752044678\n",
      "Iteration 55800 Training loss 0.08410882204771042 Validation loss 0.13305149972438812 Accuracy 0.6524999737739563\n",
      "Iteration 55810 Training loss 0.08489412069320679 Validation loss 0.1323109269142151 Accuracy 0.653166651725769\n",
      "Iteration 55820 Training loss 0.08622819185256958 Validation loss 0.13960270583629608 Accuracy 0.640333354473114\n",
      "Iteration 55830 Training loss 0.08916005492210388 Validation loss 0.1456601321697235 Accuracy 0.6288333535194397\n",
      "Iteration 55840 Training loss 0.08643895387649536 Validation loss 0.13264335691928864 Accuracy 0.653166651725769\n",
      "Iteration 55850 Training loss 0.08355280756950378 Validation loss 0.12985219061374664 Accuracy 0.6596666574478149\n",
      "Iteration 55860 Training loss 0.08601421117782593 Validation loss 0.13762256503105164 Accuracy 0.643833339214325\n",
      "Iteration 55870 Training loss 0.08658263832330704 Validation loss 0.13583782315254211 Accuracy 0.6486666798591614\n",
      "Iteration 55880 Training loss 0.083478644490242 Validation loss 0.130585178732872 Accuracy 0.6579999923706055\n",
      "Iteration 55890 Training loss 0.08803358674049377 Validation loss 0.14469587802886963 Accuracy 0.6299999952316284\n",
      "Iteration 55900 Training loss 0.08655060082674026 Validation loss 0.13528740406036377 Accuracy 0.6491666436195374\n",
      "Iteration 55910 Training loss 0.08248147368431091 Validation loss 0.13034027814865112 Accuracy 0.6586666703224182\n",
      "Iteration 55920 Training loss 0.08397465199232101 Validation loss 0.1265057623386383 Accuracy 0.6651666760444641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training done'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2.6, 1e-2, 1e8, 0, 0, 0, 1, 0.2, 10, True, False, True, 0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "16479094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer = two_layer_NN(3072,2048,10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2, 1e-5, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer = three_layer_NN(784, 512, 512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44572e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained = three_layer_NN(784, 2048, 2048, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10, True, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a65f0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIIAAAIACAYAAAD+PceFAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAA7HhJREFUeJzs3QeYE9UWB/BD7733Ik060kFEinQFEUFAwC7FgggqRRQsIBZQVLAAPoogNkBRBOkoCIIoTRCkCtJ7L3nf/+INN7MzyaRsdpf8f+/LY92dTCaTycy9Z849N5nH4/EIERERERERERFd95In9AYQEREREREREVF0MBBERERERERERBQjGAgiIiIiIiIiIooRDAQREREREREREcUIBoKIiIiIiIiIiGIEA0FERERERERERDGCgSAiIiIiIiIiohjBQBARERERERERUYxgIIiIiIiIiIiIKEYwEETXvRMnTsirr74qVatWlQwZMkj69OnVz6NHj5ZLly5F9LX++OMPefjhh9VrLFq0KKLrJqKk5dy5czJhwgSpUqWK3HrrrfH2On/++af07t1bsmXLJp988okkpEOHDsns2bMTdBuIKGH9+OOP6lxA14ezZ8/Kxx9/LJUqVYrXa9n16OTJkzJmzBipWLGi33135coV6dy5s2TOnFn69u0b1mt6PB6ZO3eutGrVSpInT/iufiTfG0VWSoknH3zwgSxevFg+/fTT+HoJooDWrl0rt99+uzRv3lxGjBghO3fulAEDBsiaNWvUY9KkSepkmTVr1pBfA8GkGTNmqMDSkiVL5Hrxzz//yK+//qr2GS5kCG4VK1ZMatSoIfnz5/dZFvuxXr16UrRoUUkqfvjhBxkyZIj8/PPPktTs2LFDXnnlFXWO3bJlS0jrOHbsmHz//ffq882dO7c0btxYChcuHPFtjUX47rz//vvy4YcfejtD9evXj+hrXLhwQb7++mvVwMRxkBiMGjVKBaLwvt00DKdOnSovvfSSPPfcc3LfffeF9Jo4Ry1btkytr1y5cuo4TpEiRVAN5t9++01WrlwpR44cUdcCBO5wngtmPU62bdumOk+nT5+WhQsXXledqJ9++klefPFFKViwoAp4BiO+93tSOyfj+Xjuvn37JG3atFKqVCl1zsDNq8QC7YB33nlH3nrrLVm9erXf632uXLnktttuk/vvv18ee+yxBO+M7tmzR33/8C/2b+nSpaVBgwaSLl26BN2uxG779u3qWjZu3Dg5evRovFzLruf+B67P6AefOnUq4L6bN2+et8/85ptvqpvKOE6DgevM//73P9UfwQ2ixCIS743iiSeelC1b1pM6dWrPv//+G18vQeTXpk2bPBkzZvQ0btzY5/fr16/3pE+f3oPDH4+BAweG9To//PCDp1+/fp67777bu048Fi5c6Elqzp8/7xk7dqynWrVq6j0kS5bMU6pUKc+tt97qqVu3rqdw4cLqdzVq1PCMHz/ec+7cOc+ff/7pSZcunc/7bdq0qc++cHo8+OCDavmOHTt6brnlFk+qVKlcPc/p8dtvv7l+r82bN1fPWbNmTdD7aciQIZ4mTZp4cubM6bgtyZMn96RJk8aTI0cOtQ+xT3Cs/fXXX55Q7dixw/Pwww9791ORIkWCXseVK1c8b7zxhvpuZM6c2VO/fn31uWJ78XmcOnUq5O2jq5588knP888/78mbN6/3eMB+jqRdu3Z5hg8f7nn55Zc9KVKk8L7OhAkTPNF24sQJT9u2bdV36tixY36XvXz5sufTTz/1lClTJqxt3rJli6devXrq+RUqVPDUqlVLtTlwLM+bN8/VOn7++WdP5cqVbb+/xYoV83z++eeecFy6dMlTu3btRHNNeOihh/yeP3Fu37hxo6v9dtttt3mf161bt6C2I773uxuTJ08OeD0ZM2ZMvJ+T9+7d62nTpo3t6+P8PHToUHUcJaSTJ096Xn31VU/27Nm927Z9+/aAzzt8+LD6Xt5+++1qHQnh+PHjnvvuu8/nHKkfuH7jOCBnjz/+uGq35M+fP96uZderUaNGeT744APPjTfe6GrfoS9hHp+bN28O+jXfffdd9XmhjW6uK6FF4r1R/IiXo2Pu3LneDxsdJqKEoBvg77zzTpy/TZw4UTV68Xd0iiMBHWwzMJDQjf5QvrdoxGLbEbwYMWKEZ//+/XGW+/vvv1XgC50uBBPwr/X9Igj3448/egYNGqSCROYFAM8ZNmyY+vu2bdt81o3GpdngyJUrl+oUWB/Tpk3zfPTRR2o7ihcvHnQgCBch/fnff//9YXWAGzRo4PP+2rVrpxrvCKjhovzEE094ChYs6BMgwmuePXvW9evs3LnT8+ijj8YJlAXb6UAnvFOnTuq5CEwdPXrU+3sEFfD7qlWrqvdF4Xvrrbei0ni++eabEywQhMAhOns33XST5/Tp047L4RibOnWqT6M41G1evny5J1u2bCrQ+vXXX/ucPxB0xXd70qRJftfxzTffeM9d/h6DBw/2hOqVV17xWVeo1wQETtauXesJBwIWgQLtrVu3Drjf7YL8wQSCorHfA8GxaAYi7R4I4vo7R0finIzjNV++fAH3BW464CZNKObPnx9ypwvBG1yr0R6wbpObQBDgGlOxYkUVrLVrT8QnXMeqVKmizgc4PyHghuubvvbrB25qkX9oRzMQFJqPP/7Y1b7T7bNMmTJ5+vbtG9Zr4gZnYgoERfK9UWTFy9HRsmVL78GHTt2FCxfi42WIHK1evdp7DDrdXVywYIHqLOAE5RY6Mv6gsZEUA0EvvfSSd7vRqXSTybdu3TqfAIfT+0VDMphOX/fu3YNqVOP8ooMbbgNBvXr18r5G2rRpPYcOHfKE6pNPPgnYQMY29unTx2e5OnXquA4G4Vj95ZdfVKcbQaRQA0HPPvuseh46HzoIZGrVqpX6e4sWLYJaL9mbMWNGVBrPZjZiNANBOK6RGZI1a1bPnj17/C6Ljiw6XAcPHlRBDTMrM5ht3r17tzfg/tprr8X5+6pVq9TfUqZM6fnpp59s1/HPP/+ogLQ+3yEIipsDr7/+ugpqWTu9gc77dnAuQpDAzEQI9ZqAYyfYrBsrBC0CBRwQ6PEHAfitW7eqzxCd+2ADQdHY725gvYH2Ba5b8XlOxo0jBCXwnBtuuMEzYMAAdS15//33PXfddZe6YWBuDz6/UGB7XnjhhZCei6w7BO4QEDI7s8EEggCBqAwZMniqV6/uN1gcabgpg6xB3Jgy/frrr56iRYt63wu27cCBA55Ytm/fPr/np1mzZjEQFKLZs2dHfd8hMzeagaD4OleTOzjHBTMiwhTxowPDHqzRdjQeiKIJmWj6+Pv+++8jsk50dBA08AfDp5JaIMjcV+XLlw8qGwTfd6Sv+3u/5kUQjz/++MPvOpHWGmyjGo1LBKXcnAhxgdSdEf1AhyRSKa/+Gsj33nuvz7LPPfdc0K+3ZMmSkAJBK1eu9HYukO1lZ+nSpd51M2U+stmx8dkANI+raAaC0HnFa7755ptBP7dhw4YhbTOGmeA5yFI4c+aM7TKNGjVSy5QrV872RlTPnj1VNtEXX3xh+/z33nvPpyOOc0swNwwwZBavjU6oHr6WkIEgDCNEFg6yKCMFWY/BBoLie7+7DcDgs+ncuXPE1hnKOXn69Onea8DFixfj/H3ZsmU+mThoV1sDGvEdCDJheJqZ/RRMIAhwjgg3AzcYaPchkwrfRTsbNmxQwWL9fsaNG+eJ9SFg/o4TDLdlICg0yH6P9r7DdS9agSAdWKWEg3ZRqG2/iFdvQ4EqBJhSpUrl/d27774b6ZchCligU0udOnVE1olZeTALkD9JrcjlN998o4p9Aoo5oshcpkyZXD+/RIkSqvCbP9ZijBkzZvS7fMqUwdewRyHrBx98UJIlSxZw2fHjx6vCfeY5CgX9Ll++LKEI5vh64YUXfP4bs3DgfBmMnDlzSigGDRqkCupChw4dbJe5+eabJV++fOrnfv36RXxWvVgTrfNBQpx3fvnlF3nttddUkd9HH3006OeHchyvWLFCnbPgjjvucCz0evfdd6t/N2zYIB999JHP3y5evCjTpk2T6dOny1133WX7/J49e6pJBTQUmF21apXr7cRzd+/eLRMnTkzwIrkwfPhw9b3u1q1bxNYZ7OcXjf3uxhdffCGbNm1S58NICeVYxgQLTz31lAwbNsz2mle3bl21vzRcJ2bOnCkJBecYzEoYql69enmLikfjffz7778yefJkSZMmje3fy5YtqyYR0Q4fPiyxXtT4emrbJiYJse+i9ZpoI6IYfLDtWIoczNKq20WhSB7pabpxkr/xxhule/fu3t9jRg+caIii5eDBg96fI9EQHzp0qGpABuImEJFYIBiChrc+gbdt21ZuuummoNeDWUFKlizpep/E1z5CQAuz8/iDQAgC02jQmkEZzJwVzok0mMBZ9uzZvf+NGaUOHDgQ1DqcGrb+YBYbzI4HRYoU8Ts7mJ7VCLPXfPvtt0G/Fl2TGIIA8eXxxx9XwVMEXUKZ2SiU4xg3mrRbbrnFcTnMBqRZA0Hr1q2T6tWrq0CSP5jJzAxaY1YnNxYtWiQjR45UsyslhlkU9+7dq2b8wfsxg9/hCvbzi+/97gaudS+//LI6ZsuUKZNg+wLbsXHjRtWu8Acz4JnHciT3RShC+c6az33ggQfUz3369JHz589LfMIshJi5zB/MMqhF8nhIShCwbt26dcCbPkmpbUvRaXvgPIabQLhBQwnjjz/+kM6dO4e1jogeKQgCYXpJBIHMQBAwK4ii6cyZMxFZDzKA0OGxZnJcD8aOHavuuGqPPPJIyHceevToIUkBgj1///23aiRimzGNbLTPUdYGVbCN61Au8J999pn354oVK/pd1gym4a41kd33SGdqIIAcimCP47Nnz/pkEvg7jhGYRpYg4CYUghBm9sZ7770X8PUQ3Kpdu7b3v7NkyeLqZhjOLXfeeWdEs2/CgawtdLq///57eeONN+TXX3+NyHqD/fzic7+7NWPGDNVwRsbw4MGD5YcffohI1mOw++LChQsqWyxQdqwOBsXHvkiIzmWbNm3Uv7gGo7+Q0NBfAdwYadasmcQadODr1Kkju3btSuhNoSTm2LFj0r59e5VhTwmXCYQbYsePH08cgSBEBtGRQuMLDSCkXWKYgfbpp5/KkSNHQl7/l19+qS4iBQoUUEMx0KjAidtNlgag0frwww/LDTfcoNLJM2fOLDVr1lTDWszhPvgZHTXrQ98l19CBtltOD7MxoaHx3XffqagdGjm4Ywj4t1atWqoxcNttt6k78Kb9+/er9OVq1aqpBgDeN94/tgUNqkDDlIJ97/quiN37wgPbYfXjjz/aLouTRKjH0VdffaU+a6Sy4z3jX9wVw91gfwEe3H3Vr7948WLv7/HcQJ+RHXxmFSpUiBMg8Hdc2MHnj3Ug2wafPzIynn32WTl9+rSr5+JubpMmTSRv3rwqaID0ahxLq1evlnC/rxqOiYYNG4a8vnvuuUdy5Mgh0fb0008HdZf07bffVp8bAtXIzGnXrp33b/Pnz1dDBuITGsBmCnqpUqXU0Jr4tmDBAu/PhQoV8rsstklbvnx5vG4XzoE4dnBc688RWSYffPCB1KhRQ11PsL0IUiLV35r1N3DgQJWBivMaGvMYaoHOuNvzIoYT4ryI18fxgO8oOojBNIyRWYcMEHRecV3CtlSuXFkda8EMN0SgY9SoUerCjjvZCFIWK1ZMvfe//vpLEhMEFwBDWurVqxeV11y5cqXPOdPfcYzvOLLv7I5jHCf4zIMd8mOuz8kTTzyhOvkffvihJAb4zuhtmTVrlhruiawcfGdwPY3m0M/43O9u6QwcBMNeeukl1YbMnz+/9O3bN6pDg3C+QQc8IfdFQsBNBrQ14K233krw4SS//fabOlegPR3JbLnNmzerc4G+NqH9Xr58edVeCdTGwD5BuxqdbPO6iLY6sgxxbUE7snjx4jJkyJCQvsO41iBgjT6aeTMQ6zPbt7qv4g+G33bt2lW1T/HZ1q9f36f97c+ff/6p9hP6HegH4fk4PyFg7bZ/Y5ozZ45j/wUPXF+t0P6yLoebpCYE0t9//33Vl8C1Gf0S/KvbC//8849EAq4dn3/+uTRt2tTV0C4cK7jJ17JlS9UvwPGCNgOOM7f9bbSXcD3H+QhtILw3fJbom44YMUKOHj0a5zkoIYFj2+x/I6ve3Ic4vsJ5bxhK/Mknn6jl9T7He8RwTvw+0HGP/jPeF24KmduCG0Noc+I94rNv2rRpRG6OYD8988wz6hyNzwHrxxBo9N8Q9O/UqZPf5+MYQjYszpE4X+A7jptdSEKwBnowigB95FatWvn8DaMzzM/Add8oUoWKMLMAVvfQQw95f4dio2axKqcCpf5g9iIUW0RVfxSRRcGyb7/9Vk1zqteLGVOcptZEET7MQoQChF27dlWV77EOzJ6ji1qjcCCKKepCgijoiKKFZqE+a4EvvB5mjUCRWbPgnFlsDTMuPPbYY2oKbHM/YP2YycqcTcQ6K8SiRYs82bNnV7/v0aOHKkiLotvNmjXzLo8ZJ/zNwBDse9cF9m655Raf7cLzMcW63fS1KLyLz17PwIB/sZ0oLBjK9LaY8h2v9/DDD6siw3jfmGoQBSb1+lFE0Wm6RMx6ggemDNXbj89S/x4PzDrjBqZJ188x94e5LhQcNOE4MT9nzEZlTu1sPvBZBirEjM+ndOnSqsgnjjdMR41pbfXnEsp3ShcONrcFxVXjC/ZDMLON4DvktvAmZixyW7QSs5xhnY0bN/b+DseSuW0oZhqf7w/fQ3PZDz74IOjXw/qDLUxqFsd+9dVX/S6LmZbMbcRMP5F05MgRdRzjuLbuN8xGhNnU7L4vJUuW9Bw+fNhbnNV6XtUPzNznVCAUMFPbAw884D3ucb7CdwvnOMxyid9jf7n5bPA9Kl68uPou4tqH8yfOs5itRs8G5KZIJGYfKlSokKdGjRrquokpn1GMN0uWLOq5KPT76aefOj4fxXqjVSwa52n9WpgKPlTBbjNmltLL43oQCM4NenlcT0KBcwWej6mvA8E09naTE1ivCdEsFv3UU0/Zfkf0A1Oo49gLBT4zvZ5wZzQLZ7+7NXPmTL/7Ilu2bCF/d0I5J7v18ssvq/WinRnK9OuRKhat1+X2Ou7E/D44teWiARNLoB0czkQRVihs3r9/f9W2r1atmjqe8J3HTHD6eoeC2y+++KLtrF2vvPKKp1ixYnGui2iLmm1a84E2RSiFhHUb9sEHH/SuCz+b7dvjx4/btnP0tex///uft21uPnC9Wrx4ccCZajH5Cvo9c+bMUd9PzJan11G2bFmfvonbtgXaygUKFPDZHuxTXJexj63wPvVsqtjuUaNG+bR58DPOQ/g7JjjAeR59UPTtdD8KbZH169c7bpfdvjP9+eefnqeffjpOm8YfnAv0pAjYLhSfR/+uX79+6jMx2x5O68I2o92Bv7dv3171ub766itVSF8/D+0b635D0XrsNyyrl0O/xDx2MLtkqO/t999/V20LHB94Lo4P7HNcx/VkApi1cuPGjT7PQ/8dy+I4Mgvb6+sT2nhmf10/MmbMGHASG39wjGA/5c6dW7Vt0X7DbJjmDOodOnRwfD5mZESMA8vgOP3uu+983ivapeb2YRZHvZ8xw69+jUGDBvl8Bv7awSaJ9IUb03Zr2AgzmIIvYzAzQOzdu1c9J126dD7rBQQa0IjR637iiSfiPB/L6A9izJgxcf6OHa2fX7ly5Tjbhue4acSbU/eaF1x05DELz5QpU3xmUsNFoVKlSirYY26D7qAhUKFnYrLOsIAD3QwGobNgJ5z3js9NT2uqOzOBoOOD94gvcKhfJH1Cwv6y63BlypRJ/R1fmEANiEg0vk1uT2Dm6yLYh1m4MKMPTk44hjFDlzkrCmYVcups4aRSs2ZNNT2tCRdHva+c9lcgOCGa7wkXtfgSX4EgTPeKC57bBik66ljnl19+6fN7fEb69XCMBTNrmtv3h8AxLmjmcl26dPFEo9OBBoP5upii2B80DszlI91YR+D47bffVg0P83XQIcXFHQ1bfF8QeMZy+L7rZdBgwwwcmK78ySefVAEcLPfOO+/4BLucZrHCeREzyWAZNEBxPjUhEIXgq17PyJEjHd8HLszYDixnN3Wq9TvmdA3BdOd4j23atIkzexBeQ5/30LlwalxHMxCE4LN+rTvvvDPk9QS7zbghopd3M0OJDsZZg79u4ZqoG684DgN9x7CsXSA5IQNBmAnptdde8/Tp00e1B/TNJfOBxjY6EYklEBTMfg8GbirhvIBZutDgNqcPNx+48ZSYAkG6cxzqdy2xBYL0dRgPnMMTAr6v6D88//zzEV2vDqrgu2a9OY0ZDm+99VbH945rGc7v6AeYxyM6lYULF1ZtdrQX0cHD99lcBteQUJntLX/HiTWYgRus+A69++67atYodF7NYBVu6DjRgRe0ka0QJNPrQAAGN25C6U+Yfc9nnnnG7/IItNm1G3AuQkAPf0Mgz/qZDhs2zPsaCMaEGgjC+Rc3gNAGcNPXQIBO72tcF61+/vlnFdTyty4EE3Bc4W8NGjSI83e0j/RzceMs1PNesO9Nt6sQsLFr7yBQooM5efLkUYEms32HQCD63WY7DtcnnPcRXBo7dqz6viCIheCN/LdMq1atPKHCc7EOu/4vrr/+AkGIB+DvuNFlhba63j4Eg9DniY+2X0QCQbrjgE6rlbXzgw/RDTTQdWYKvmx2evXq5dOYsWahIDqGvzVt2tT2+TgJ+evwuJ0uUU+h6+9Eat79zpcvnwoSmZksuIuoOyXmiXD8+PFx1oU72Prv2Ed2wn3vyJ7QGUuB7kRhv+M9oYMVKj2VcPPmzR2XwQXH/FIcPXo0UQeCcFcCnV6r3r17BzzB4iKKiLY140ibNGmSdx05c+Z0nEbZCQKM5nvCySqxBoLQOTYfOhpeq1Yt1w1SZGYhoIzPxNrZHj16tM/2IagQzvvDnRA0XpDNiAsS7vKZd2fweeE1rUGI+Op0IFBibp/T1M0azk3m8rgTEx+QzWgGRZENaNegxZ0VvQw6iMiaMS/+GjJ49HJovPmb8hpBVhxHdnAx1+c+bB8aVVb4fEuVKhUnC9aqevXqfq8h+N7i2EA2gtM5FndO9TrQuLE7bqIZCDLvcqFxFapgt9lsSCJ4G4h5V9PpePAH3108F0H3QHfW7rjjDnU82GXoBroWobNhPcdZH7jOI0AaaLlAN9qwDNpgunOjH7jWBHudjK9AUDD7PVzowJo38vQDWTiJIRCE8wMCwTgP2d2xRtsr0DGB7UHAI9xjJ1KBIDPQgcB/tOH41xkjuJGEDIpQgg1O1yms0ymTBTe3dWAfD7Tl7T5TM5sBndcVK1bEWQ6BQbeBjkgHgpCpiumqrTfN0OZBX0wvt3PnTtuALG4a4/xsB+/f7C+FmvWOdpdeB65Z/qBvgXaZ9TjAzXp/mVd4v/rvaDNY25ZuA0FObS8nuImos6acRsPguPa3LjPIMHjw4Dh/xzGn/46AUbjnPTfvDe9FJ3jYBbg0BOvNcwgy3Kxw010vgxsgCGxZrydo64nx+Tm1Cf1Buw3HM44fO2ivIUvdLhCEfjbOFxg14tQf0FlfeNjdaEo0gaBHHnnE8U7z5s2bfbJhkK7tBjorWB5RTacO/4wZM3w6COaFDBFhnbKIdDenkzI6h1gG22i90Lr98ro5kepOq5tGBj5sfxcKs2Nnl5ofifcOyGTRr4POSKCUa0RYQ70w69fxN/wBFwgzgosTXWIOBNllCVgDjDgBWCEbAn/zF1izZnkEmxWkI9j6YZc1lhgCQW4ebhqkuvFpl5KN4Y3p06f3rg+NkGCCNNb35/RARxbntXA7N8F2OqxDvXDnzp9t27aFdWwFw7xr55Sai8aZOYwWn5cdZFLqZXBHya7Rps95uGb5Yw7hQzDHygzW4xrnBGnm/q4huDsVaHuQKWV+Hvg8EzIQpIemhnveCHabzaFeCAYGohvLelhhsHRHC9c3f5DWjZslyFoN5Vpk7odwH24DMmgrIfhvBmIReAmmQxxfgSC3+z2ScM01s6WwX4LJbo6vQBAyorBODPELdGyF+3CTNRSJQNBnn33mXQfantaM5/iADjrai2aWgPnAjTd/ZRYCwTUdN0MDtdsAnW7zhqZdJxY3KvQydkEg+PDDDyOSmRlKIAiBeKegR926db3LIXvXqS9kd4PFLvszlHO3zu7V/SCcn9HncYJREXb9CWSz6O1AmQ07OisYD6ebOW77kvgumMdloHXhGHB7A9BfZq/d8Ei0s/Tf0W4K97zn5r3hJqz+u7/jAwFIMwMc2WlOmTaBynDk/68cAB6hjGgxS4fY3aDUbQS7QNA999wTsN+rM4rwQOa4NfAXibZf8kgUSJo8ebIqMtWhQwfbwqPm9JcogoZCaoGgMBegMJpTMVVMeThv3jxV0ApFzcwZDVCgCQW+8DunaWZRhBhFolCYDEXGUBg4vqDQlVa3bl2/y6KQFYqkoriiXQHfTJkyeX+2K6gWqffev39/7wxHKLKGoqh2xowZowou3n777RIKFMDT/BUeRYGxe++91/vfKIKJgmKJFYqFBfq9XUE3FEIDFPRGQTS7h3X2kKVLlwa1bdbjxjw+ExMcpyjqaz5Q+BUFxVEozQ3sL5xPUNgWRdOtsC/xndNwfsJ5JVR4LWwnZulBoUgNRd0wm1A4U/CGwlqQM9DrW79T8TkNqTlrm3lesy6TO3fugDPnmN8rPRuM6eOPP1ZFMiFQgWOzuKA+7jSsQ09jjsKAZnFtf9MT23HzXbde/4L9rkcSCuqaRbudPrP4Po7dfIfM4zjYYxjXRswwhSKo/qY73759uypQ/vzzz6sip6HA5AXWc5z1gaKkON8FWs7tRAjYHyhsiaKf5jTS+I4kJLf7PdJQMHTJkiXe79qVK1cCTu0e33AOQ7FTFGXFlPd2UFQ/0DGBayiue4GWC3XW0GCZU7rjO71169Z4f020HTExAIpCo0i4tej2zz//rIoWhwpFc/VkL4GuLeZsgnv37lXHu5XZHsuTJ4/tesxi+W4nSIgUTAyC9lSw7Vu0rTBTGd4fiuI6XfOyZcvmfQ4mSkDh32ChyPrdd9+tfsY6nQr4YxKC33//XU2DboX+FyahQTvOrn/rpj8WanvICfq7WosWLUJue+D8in2EvrvduTaS78vte9N9QVzfMVmIE2ybOVup3WyUbr5Dkfge5TAmyUFBaLP4uoa+sbUNgv40JsECvFc33wVMloEC95Fm/00OwkcffaRmcrJOxWzCF0zPWoMTPz40s/NvhYPup59+Uj8jGBJoak1zek1zBiDAQe5vik7MboZHtKeM9gcXEgTYcKK1Hjw4iSLwoqHBEl/vHX9DxxWdbnQA8LqYdcQ6CxKmYEVD2OnC4A9mF1u4cKGrLyzgs9YNNHSs0YBxO/tGYmEGBxCws1q2bJn6Fw0WPNwIdtYCa2fabjsSA5zM7War04FgdKADwfGLkzMq+CNg6XSOMqfBxIxqmKktFJilQG/z8OHD5cknn1Q/4zjHfw8YMECiydpZDxQ8xewOJj0Nd3xwe85wE6g012U3W9fXX3/t+jyDczAaI/p7gcCg7ujjfKdnGPI3hbl1m+yuc3rmP3RS8HAjUjOUhOLQoUM+/x3NQJD5Wm5uAJjHcTDHMBpfDz30kLr+mddaK1x70alDgxuz14UKHQ08Ar13NDidzoWhwo0VXEN1ewzfkccee0wSgtv9Hl/wOeIaoDsYCOTjO+qmAxMfMIMM2jhz5851PH5Lly7t6tyJ616kj51QWd8Lrs0ICsR3+xv7Cg/MaoV9q9uz+hyPzx7XZszGFaxgri1YPx5oO+triw5YaG5mVTKPy2jO/hdO+1a3bXFuxqxIbuGaF2i/2unZs6dKVNB9VZynrddkBFPRr7Cb0RDnXHxOuN5Y2yC4CTBhwgQ1e6m//lgwAn3uCKyh/aH7d5hFOtS2Fb4LuKmDPrl12fXr1/u0hyMxu1+g94bX1EFh7PdAy+Mzw2xcsHHjRhVUNdv3br5DkfgeFS9eXAWWse1r1qxR1y+cV9Du1zMk4kYmZk43/fLLL952TDCzQcZH+y+sQBAa2wjq4CSLC7jTTkQ0DDsCU54B7kK9+uqrjkEKTHmmTyChZipgWsJwnp/QzO3GHWjsM2QaoAMSKPMmku8dF0Z0pAHTzaORaJ7ocRJFsMou08INRDd1pw1fyEBTeFobDFu2bElygSDzBGXtsOK41xFl7HsE4tzQJ5xQs5XcTjWZmOC4u/XWWwMuh2m8AXc8nc5RuOOO7ENMLQmzZ89WF3pMxRmOxx9/XL755huVCQmYCrJRo0auAliRYt5RcHN3xzpVpVNmW1KChifu+rn9vqBhhI4hLuz6PKMhc0BDYyxUmG5VNwSQYYTpWoOdTjrarMdGMI35SB7Hbu5QmtsazDGMzgLu7uOmi78AErJpcXzgnOG20ZkY4WYDOjTIQMFU0AnF7X5HQxjn0EBwQ8xfR8kOrrd6+mvc4ERbFFNbRxuuP8hgQNsr0J39pMZ6zrDL3oxvOL/j2owArM6GQGd31qxZ0rt376DXZ2aMummLoa2hA0HmteV64K99qzv6uIYh0OqWv6xbf2rXri1VqlRR/QycN2bOnKluCJrXiGnTpsmkSZMc14H+re5L4RjBdxM3CpGsgCwhHM+RyJhxA4E0HWwKp+1h91mhbTx9+nTVp0cfElmZ0RTKd8iE75HTjd74lCJFCrXPcB5B/w3ns8GDB8tbb72lsgxxPrG2wcHMhERGYqC+rxZunyTigSBEwXft2qV+RvqqW0i/QmCjV69etn83O6XIjAmFXgcyTpIqBIBwpw4NThzgaCjhJIZAAQIw0XjvVatWlWbNmqnhY0jPRONEZzigc4UGZMuWLaVgwYIhrV+n0wK+RDjJ+UvjxwkCJ16kyNl1TJIaa6TdPN6RtRNfd/GswxiikZ4dH15//fWAQw1wktXp/27hOETgNdD6A0EjAt8RZI7gs8XFtmPHjqrzGGzwLlT4bprfGZ3N4sR6zg2UrZAUIJPFDAKikxcIhlRo5nkGncNQMj397WfcKEksd+z9sWbiRHNorpn9EOgYtu5ft8cwhnggUwBBhEANLmTB4hqIDEC3zGHyCAq7HcoVn3AeQgAEd1dDbW+FK5j9jmPOTXmBUI9NZHkhEAQJsT9wLUamFtp+oQ63T8zMG4mRyjYIFfYvhtd899133mFI4bZjw7m2XG+c2rdoi0TreoesIH2jGm06MxCEbCEMB3UzDPXzzz9X52z0r/r27asCSHgubvK5uR5FQqTaHiYE61BO5JVXXlFBDZQEwfkH31OUBImWcL5DCf09atKkiQoM3n///bJu3Tpv/xujV3DMYTgf/mYyry3IRjPLH0RbWMUfdDoxUp4CjT/Gl8WMeNmN6bMb/693arD0OnBAbdu2TZIa7DN0HpEVgpMO7jwi+uxmKEWk3/ugQYO8P+OA1tlaaLwhLdJubK1b5jGBi4aZZulmWJNTvZCkyjy5//HHH/H2OsikMV8LaYrXI32OQjZboHMU7kSbd1mQGqtryoQbiDHPd8g0wlDaaMHnXL58ee9/241hdko9xfjphLxARYr1bovOTg3lPGPewQ6n8RGt73okWTM13DTYIsUchodGY6A0fPM4dtPpwPcf1zLc4HJT78c6hDIpw11zCGX4RbiC3e/R2hcJsT8QsEYtqKeffjqq14hosmZPONUAjRazgxZqUMq8voRzbbne6Wse2lXRuvmIui36GEOJEjOIjBvqGJLtr1+FzxMd/fbt26sslE2bNqk+WUIct5Fqe2joHyI7HedfDInFECsEzazB2mgwv0M4Dwb6Llq/Nwn9PapatarKPMMQRDMxAu/lgQceUDX5Emv7L+RAEN4wilaiQ4k73Gho+XsglRfLafgy6eES/lLf0WnBwRkIvhQo+mW3DmSzuIGx2IkBiuFiv+JEiYjss88+G1TqeaTfO4pbI10aMA5Tjx3FHTzcaUXGUKisqdtuUtPNDsD1ljaNQIQ+QeD74fauZrB3P4sUKeKTXo/vY1IMmPqDDLbPPvtMRduRxRboHIULolkfA3d+pkyZEpFtwbnPLEiN4Lke3xwNZr2jQKnoCFRp+nt/PXyvzLHg4ZxnzAYHvjehMosM6jvSbiRkgXzrECudZRYNmPhAf4bI7tJDK+ygo2EWGHWaNMH8HFFzDMWS3Qw7Al1zJNDDbFQjsKp/n5BD/Kz0NsbnhBmR2u9oc/w3463fR6iZjHpf4HtuFhKNbygeiuwUZFibN9+uN9ZJR+yGTUSTWVrAmmUQSjs21tuwkb7mhXu9w80Lc/IHPRER+lk68OEEnfibb75Z1XFq166dag8m5PFqtj0QoAqnpAP6lxg6hzqFffr0kZEjRyZYPTTrdwg3mMx2qB3rjaBo1PoNBP10lMnBvkXShFn+BqMLzGM+lO9CfNUCCzkQpOtuBFMkERExMwqGcZZ2UCDavDOvv7iBtse8A2iOH0TUN9DdQ9R9sH4Y5hAlt3cKwk1zxfMRPcQXAY1upDUGKxLv3cr8nDGTBTKUMF4VJ9FwZhVCbRYzAwyzv7mNiuMYwfOvJ4iK62GW6MjoWYX8wWwvoWRlWQt/+xtu6AaOi3CL5UUSApW4a4/CkG4DqagbYGY9OJ2jQoHzmHmxw9DYaN0VM2dYwHA5f8y7E9YilkkVPn+zLpOb84w5g8Rtt93m/dks7IeGpJssRrtrA4bA6ALIuLHiZqY6ZE8MGTJEEgo6SmaNj2imY+N7aQ7v9Hccm8cwMjz8FWPEdxA3MzAE221NNkAdBTcPc/YTBH/17xOqKLMdfRPAaWac+BDqfo/WvsA5023dhnChvYeAGNpuyF69nllrAoVa/yVSzIAsOv3hBpPCubZc78wan+izuamtgzZZuO0kZNfpvifKkuD7hvYuAq/+gr0IyOrhgrjuRmo4Vqis1zE9hDWU9geuP2i7oG2EIW8JzVrrNdD3yPwO4bhKqMz19evXx+lLoU+L32FEk5mNb06CYH4XkFzhph2Ja6VTAk04QurBIy176tSpKo3XbsYuJ4h8N2/e3PvfKKJql4WAL5t5dwh1afylTmEd6GSZ22KeXPFhOE0dCBjqhKJOuBCbzAav24J2biJ2/oJFSFvUxZ5xggrUgbVbVyTeu906ddo2ipxinC0aSW5nunGCCLTZScU4XH/7B8eevguNcaxO+8cMSNjNIJSYmccxUsQD3WFCNN/fVItOdLqrhuFLgaLwThAwwUXdLigYbHA0EsEkZATowEswRe/QKDTvHqHAcKALkvX9OW0/7iShXpBuTODOKAItwQ4/CyXYjA6GLqyNjCCn4WHYdp1ZiTvq/qYnTWqQIq6hblSgIXK68YkhSWZw3azzgnOL2ym3rXc2ce4y14UbAMi49HdtwVTloXZW4mPoTKjni1CPY7x/6+yYdnC3V3OqRQho5OMzwBDsLl26+H1t3IGN9ox/0YDjCtPYopMRTCAonJteiXm/I4sUbRtrKn987Qt0SjEcDEHWsWPH+l0W52fcHEyoWaIiUc/HzNRDZnIkit6GQ18HMPtPoMxBN9cW3JhDwN7NtSVXrlwJfo2NZnADGcY6uIqMTgQj/B1TqKWIotJ2M3oFA8FG3afEzQu0V1EYuXv37n6fN2PGDO/PbjIM47veFUZnmJMABTpfOLU/0ObUo0Dw/XNTr9LuvUXy2MH+RYaShs/HHzM4iLpuCWn27NmO7wkF//Uxb24z2lE6KwjHJK6D/rLfkOiC4Kl1mHskPoOQAkEoKoU77f4aWE4wc495UcO67Jh3y7Bz2rRpYzukAYETBJfQoTKnl8VONQ9uVO5G4MkKHwBS/rAtZqMczArkeG27KbZxEUGtnECp8mYwwt9dVDPVD69p10k0n28eOPqLGon3HigrCEXL8JlEYgy9OfQNQTC7bdX0VPOIuCJIEq2xtGbKpBmJxrrN48KsG+EmAGV3RwTfK/3lxvtA48Run+C52AcY92w2RIKBYImeLhUNUgSGgp3FAzMxfPvtt47ZgdZU8ECBD3P5UGv0IKiFCDsy1oKdPc88RwEKvvlj3V/+9h8Cqua5DQ0dBDSDCVaax1gwdUr0XR+cJxBwtYOglz4HYXm307uHyvx8/XVu3AR2rQE463cL+xnZpnodmNXB30VX3wm0Htf4DM2GKa5hTsPtzMaT3XGBu53mtQQNPV3c3IRzDgKa+GzMYX52+zG+h46ZnSV/w7Pi4zhGR0IPV0Qj3el5eqZLFHJ2CjSg7dCwYUOVMegvoxKfIbJmcY3Ux09SgeM4UNAAM7ji5g46FWZ2bnydhxJqv2OYR6ChjGhfoIAsvvPBzBYW6r7Qw8GQ/Y1MBX/Z1Tgf4aYZzgHxfV6O9Ps0mTVa3M6UGJ90+x3fg1Bn/zNvlAKGhThBe1EHqlFrxq59Yl67Qm1Hhtu2tdY7CrZta7ddyNowbz6iSDHarnbFljHqAEWc0R6LRGfXHF2B7zf6LoHKWpj9MT2jrPWzNNv/+tprDZq4vT5b27vWPif6uGZbH8Ec67TkmnUbzPYHChXrv6OdbDclubXfZPfe/B07+EzN4yTQewMz4P/DDz/4zPTq1BfEMWU3vC/Y71A43yMMcXYqr4I2iB7lgcCvhnO4GYjE+0UsQ0/AZU12wXUCNwystamcPgO8Z9dFzD1BWrFihSdFihQ4Ejxr164N9umenTt3qufqR/LkyT0LFiywXbZz584+y6ZPn97z0EMPeT788EP16NatmydNmjSevHnzeg4fPhzn+R999JHP85MlS+Zp2bKl55133vF88sknnr59+3py5cql3s+SJUtst6FgwYLe5w8aNMhz5coV799mzJjhKVmypOe2227zLlO/fv0468BzihQp4l3mqaeectw/e/fu9dnm7t27ey5fvqz+dvHiRc8HH3zgyZEjh/fvadOm9Vy4cMGzY8cOz0svvRTR9273PsqXL+9d548//uiJlAEDBnjXi3117NixOMtcunTJU61aNbXMG2+84biuI0eOqGNFr+/ZZ58Ne/sKFSrkXd+sWbPU786ePav26ZkzZ9R/43PKly+fd7lJkybZruu3337zLoP9f+LEiTjLPPPMMz6fHx5Vq1ZV72XYsGHquMBxj9+PHz8+rPf2119/+Ryf1atX92zfvt3Vc/HaN998s+fo0aOOy+C4NN/HtGnT/K7z1ltv9Tlu8Z0Ixt9//+3JlCmTej6+o8HCcZYyZUqfbcZ3xgnOC+ay/fv397t+HC9lypTxec4dd9zhOXjwoKvtw/dOPy916tTqOHTrvvvu837Hzp07F+f73aBBA/X3pk2b+pzr4sPx48e91xI85s+f77i/zO/zrl27XF1bNm3aFGeZ77//Xl1z8Hd8xqtWrbJdF87RWKZVq1a2f//mm298Xqtw4cKeNWvW+CyD71ClSpW8y6RKlcqze/dutV/1OQPat28f51xdr149z8CBAz2vvPKK54EHHvBkyZJFbbfTObdKlSre5+PcHp9++eUX72tVqFAh5PXgvKHX8/zzz7t+3saNG73Hw5gxY+L8feHChd7Pd/Hixbbr+PXXX9X1D+2H0qVL2z5KlSrlKVq0qCdDhgzez+/QoUNBv0+0CfT7xLaF4s8//3R9TtZwHOA1c+bMqc7Tuh1hnudwbsZ3EO2KYJnnPRyvbkRzv5twHcD7xPkS19CTJ0/GWQbX9cyZM3vuvffeoM99oZyT8Z5wrcVz0I502h/Fixf3ZM+e3bv+mTNnBrVtf/zxh+eff/7xhAvvCZ9FoPN1IOb57osvvvDElw0bNqg2GNo3TvC3rFmzenr16hX266E/ZF6nnNoeb7/9tvr7TTfd5Dl//nycv6Mtpa9ReCxatMh2PV999ZV3mdy5c8f5frs1btw4n7afXs/nn3/u085+//33vcvdcMMNjutr3bq1d7l+/frF+TuugebxjIf+3uF6h74A2h+4DuL4D6Z94w/Od2Y7Hq8VCM5FevkaNWqovoU2b948T9myZdV26mXQl8L2PvbYY7afOR7oI6AvZ+f333/32S+41tntv2zZsvmcb6ZMmeKzzKlTpzydOnWybXfjb3h9fX7F4/bbb/fuZ5z7pk+frto05vPR7kI/2/xMcfyabWWca/S5Des0z6Nu3huY2419jv6tFd6D/ixxnPprw+GBPoWTihUrepd79913PcFat26d9zth14bH9yl//vxqmbfeesvnb+j74XnmfkGf/q677vIMHTrU88ILL3juvPNOtY9x3fz333/jrB9tJ/3ctm3ben//+uuvuz6/ug4EoVE9YsQInyBEzZo11cHlJiCERgxOjOg4Wzu4OHni4jx79myfkxku1mhgWJc3HziJo3HhpHfv3n6fjxMugkpOsDPN5XHBRqcNHx6+jMuWLVMflrkMOlMtWrRQHxo6DNYvJC6m6CziRGJ3kDdp0iTOa6JDgpM9fjYbHfpzKFGihGf9+vURfe92Pv30U/VcnCAj2VHEurp06eIT9NiyZYtPp/Gee+7xG0jDhf/rr79WJw/r/n7xxRdVp2D58uUhbTcuUnp9CDLgM8U+x8UR24Zj11wGD5yocJFF8FR/h/AdqFWrls9y7dq188yZM8cnmInj4u677/b7+eHxxBNPeCJh//79Pt9NfCdxwrc7WeOCiuO6efPm6jPDSdkK7/WHH35QnQU0+s1txvdm+PDhqoGzbds2b+cSDRtrpxgPBE0Q2MR3DdvpBMHQ9957zyeohYYEgjgrV64M2FDat2+f+hy7du0aZxtwIu7Ro4dqhJ8+fVrtF3xmCNiZjTZ9YR48eLBqKOv3Z4UAhDXYlC5dOs/999+vjpkDBw7EuWAsXbrUM3XqVJ9gLB5oeGG79HHmD7a9Tp066nn4PuG/9fp1kAjnE3+BvXDhvWE/W68FOKeiAYJAqQ7o4X2hQWEuV7duXbWcPt9t3rxZXfBq167tsxz+G8cUGk4mnPP0Z4br2dy5c71/wzGCxq8OxtgFaTU0JK3HCD4LBHBwLkDjFkEccxk0vtABMAMUeI1bbrkl4Hf9zTff9Hl9nMcQWHjuuefinO9Gjhzp2IGIBJz79HvWx5AbOEdjf1u3Gdfx0aNHq/eD72Eg+LzRsc+YMaPPvsS1EQ0m7IOJEyfaPhevoQPFwTycgoLRCASFwvq9KVeunGqrfPzxx6rxiHMjOmTBBMv37Nmj3gPOs/jMzPXjXIhzPoJWiWG/m/DdMdeJ4BjaETgXoF2LjgKOZbQT3LYPwjkn45x04403Br0v8HnZtRnj088//6yCZG3atIlzPKE9iH0QzDlA7yscC5Hq5Nt9Njoog3M92im4TphwrsCNXpyvIwXXNVz/9fUcHXTzeJo8ebL6OwII1htc6ETaXRcRbMf5Tney0d/CMYe+gLkc2qToU6A9Ggy0s81gBq7/+KzRxkcfDNdjfE/Mm5x44NqGGytok6ENiOsNblKaN3dwjsD5ZvXq1T6viXM2ro/+jnX0d+xu5oRD35TE9cGuU2316quv+mwTri34fHDuxPGLQKN5swfXRbRX8TkDth/HgHXfoaOv951uu+I7pttm+oE2CNra1msiPmcz6IgH+hXoRyMIhdd79NFH47RPsG24zsIjjzzi83d8F3DNQAAIN5lxLscNKP13BEzQRrMmbpg3dPCZIYED6/r2229Dem+4QdmoUSPvcs2aNfMJZGP5hg0bqu+1NbCi29YI6FivNeiLoE2O7wcCevjZ2kfOnj27ajcgaBVsIAgPHBc4X2oIuPXp08fbHrUL/KJNpANFTg+0G50SNqzxgMaNG6tAKl7PbXDYdSDILoCjHzhYAsEB6uZCZ71TgzunuGBbO0064LJ169aAr42TmDUCrb+0ONj9wY60NuZ15FwHKnQgCCd4RORw0OMAQKQy0Pu1a/SikYALhbkcDvqePXt6O90IlJgBB+uJNhLv3Q4uQoEycsKBO5N58uRRr4ELCu5044SBLwIuTP4inOggujnGQml8oFNqZodh23BiMe9AOz0QmAj0HcIDQSyT7pTaXTDxu1GjRnkiDcENnHjN7xtO7jiJ4+SLxisu7ggC+evU4ETk5rN48MEH1fLWYJHTwy4DQMNdPX/PDRTcwLrdbANO/OaFys37s4OIv9PzrHc50NgO9FpuzsOARju2C40/dIjwvcFzcScC5+n4apxraDz5ex84t8HTTz/tdzl8n6BDhw5+l0Pj1AoXVTSwzfMhOoNoACFQiSCP0107EzpC+H7YnV/RYNLnBpy/cXMAjSO7izP2OYIjdt8DnA/xOnbPCXRMuHkPoUBjUr9GMAEnN98bBLHcQGMY51Ycx7hO6A4mfjYbYyZcP92ea6wPu88gMQeCEJAxr1nmAw1PBG7sMqn9sWZ32j0QBEkM+936+mamqflA2wKdFzTIgxHOOdnsPAXzQOct2syOvdNDB+8DwfGmgw44v8cna3sLr4s2JI4DtGNwQzeUUQ2BoH1gfr5on+M1ca5C5x3v2+7mWaDroh5tYAYe7B6hnGMQDDPXgSCT7t+Y5y+7x4QJE1Tn298y2GYr3EzTGcjWB37vdBMtHAj+IAiEm6xu4PqJoI11+9AWRp/A2k5B8M+8ARGobYIbnG76xzp4Y0ICROXKlW37BjqzRf8Oy6FvZR53CIjYnYdwM1ZntpjvHe0iuz4jPnvzpgDakWY7PZT3hv3+8ssvewNRuHagz43gBvrZaCM6HefWDBu77wf6Wm6+a8EGgszvD9aB9iTOnxjJZJeFat5gsTvOdHvGqX+vWZMQsK+sN5P9SYb/kyQA4xgxhg51FFD/BgUzUcTTLYz9wxg+1HLAOGwU/0StA7djrTFDyU8//aTGR2L2GbOoFepqYIwgxr6aU8KFA+NxURcG24tiXqgLgVlmzP2BsaH4G+r8mFPURvq9W+vXoMI5PodIvVcrjG1ENXyMJccYXbyOnuI7IWH8JWqroM4ACjpHa7pCjK3FjEKoEYA6KCjqiTHp5tSE8fFeUTQYr4nxxHrqShTdw1h46zhVSnpQ6BfHFWpnoAYHxifH13c6scK4f9QiQAFTnEMxwwPOjcHUlcJYd+xH1DdDjQnMBoE6NvgZtdRQfBbXBvP87QTj8nGuxvPwfIwtRz2VYGq3RAPqiWHsO4pbY3YMfzUx4hPqFmAWDUwmgYKMuC6b1+ZYh2sH9g+updhXqHuIzw21WcKZ7TMpQtsNbTh833F9Q20JFCuuV6+ez0yRFH8w/TZqtWEiFlx/zJoZ8fV549yAdiTOD5gMAtc61GKL788cbSfUe8MEJ3jt0qVLqwkb4rPdFg7ULcJMwLhOoTZPsLUVQ4Xr5tKlS1U7BBNq4LOpUKFCvL3e888/r9o61lmq/MH2Yf9gn+B55uQsqME2adIkdU3E7Idmbdn4hmMchcnRP8W5HrULUfdIT3SE+pboqzi9Vzx/zpw5qhYPjkvUY8OkThr6OqhdhuMXdXid2v1oP6HWFtos2Lc4r0YC2laYFAK1CPU5G9d3cxsTi+PHj6ti0LjW/vvvv+rzQO039NXc1rhDfSDUfMU5A/Wg0NfCseamRhbajfgc0S/FZxDM9T3JBIIo4aGwIU5ymGEMJz4iIortTh1mt0SnLtRCq0QUG9BBQcfzpZdeUlNzExFRwoqtW0IUlokTJ6rK8/5m+iAioutf586d1U0BTJfMGwNE5M/GjRtVVj/ucmPGOCIiSnjMCCLXaW9Ix0NqHlKriYgotmHIBYYZIS1+3bp1KrWfiMguG+iXX35RQ1kwxJyIiBIeM4Iojvfee0+NBcU4zz59+sjUqVPV2NF//vlHXnzxxYTePCIiSgRQow5DPXA/6aGHHlL/EhGZhg0bpuqsfPfddwwCERElIswIojhQFBiFuaw6duyoClQTERFpmDygVatWqtAnbiQQEcG0adPkqaeeUhNtYJIXIiJKPBgIojhQsX/9+vU+v7v99tvV7Df+ZicjousTZqvDIxJQVNjNLAiUuD43zELhbyYKTCbw+OOPq1kqx40bx5mYiGIYZqkbOHCgrFq1ShWW9zebErohmC02GucpIiK6hmdLigOFP6tWrSpp06ZVUyGjUT9z5kwGgYhi1AMPPKCmEI3EA9ORUnRg+thIfW5Ylz+YfnbChAnSq1cv+eSTT6L2HokocbYjMfXxwoULA06praeojsQD1yoiInKHGUFEROTXjh075NChQxFZV7FixSRHjhwRWRf5t3fvXvWIBHTmAnXoiIiCdfjwYdm+fXtE1pUzZ04pWrRoRNZFRHS9YyCIiIiIiIiIiChGcGgYEREREREREVGMYCCIiIiIiIiIiChGMBBERERERERERBQjGAgiIiIiIiIiIooRDAQREREREREREcUIBoKIiIiIiIiIiGIEA0FERERERERERDGCgSAiIiIiIiIiohjBQBARJZh//vlHnnnmGcmSJUtE1nfs2DEZPHiwlC5dWtKnTy/lypWTN954Qy5duhSR9RMRERERESV1yTwejyehN4KIYsv69etVgObTTz+Vixcvqt+FeyravHmzNGvWTAV9xo0bJzVr1pRly5bJvffeqwJC33//vWTKlClC74CIiIiIiChpYiCIiKLq999/lwULFkiePHmkV69eKosHwjkVYR2VK1eWPXv2yOrVq6VSpUrev82YMUPuvPNOFSRCMIiIiIiIiCiWMRBERAmme/fu8sEHH6ifwzkV6fXcdddd8sUXX/j8DetFRtCmTZtUptADDzwQ9nYTERERERElVawRREQJJnv27GGvA1lA48ePVz+3adMmzt+TJUumMoLg1VdfDXsIGhERERERUVLGQBARJZhUqVKFvQ6zzlC1atVsl0G9INi2bZssWrQo7NckIiIiIiJKqhgIIqIEg2ydcM2dO9e7rqJFi9ouU6pUKe/PixcvDvs1iYiIiIiIkioGgogoSVu7dq36N3fu3JI2bVrbZfLmzev9GcWkiYiIiIiIYlXKhN4AIqJQnTp1Sg4fPqx+zpkzp+Ny6dOn9/584MABx+XOnz+vHtqVK1fkyJEjkiNHjohkLxEREVH8Qz3AkydPSv78+SV5ct73JiKyYiCIiJKsEydOeH/OkCGD43IpU1471enp6u0MGzZMhgwZEsEtJCIiooSye/duKViwYEJvBhFRosNAEBElWeYMYGnSpHFcTheTBn+ZPf3795c+ffp4//v48eNSuHBh2b59u2TNmjUi20wUbchsO3TokMqa451xSop4DFMoN4qKFCkimTJlSuhNISJKlBgIIqIky2zgXbhwwXG5c+fOeX/OnDmz43IIJtkFlBAEYiCIknInGt8PHMPsRFNSxGOYgqWPEw7rJiKyx6spESVZCOroAA1qATjRdYQAGT5ERERERESxioEgIkrSKlasqP7ds2eP4zL//vuv9+fKlStHZbuIiIiIiIgSIwaCiChJa9q0qbcewN69e22X2bZtm/fnxo0bR23biIiIiIiIEhsGgogoSevYsaOkSJFC/bx8+XLbZVatWqX+LVmypNSqVSuq20dERERERJSYMBBERIli1i/z52AUK1ZMunTpon7+8ssvbYuMfvPNN+rngQMHhrytRERERERE1wMGgogowZw+fdr785kzZxyXQ0YPpoFFoWed3WN64403JH/+/CoQhKneTVOmTJEdO3bIbbfdJl27do3wOyAiIiIiIkpaGAgioqg7f/68bNy4Ub799lvv70aPHi2HDh2Sy5cvx1l+4sSJsmvXLtm9e7dMmjQpzt9z5Mghs2bNkixZssgdd9yhgkVHjx6VDz/8UB599FGpX7++fP7555xGloiIiIiIYh4DQUQUVZjBK23atFKuXDnZvHmz9/f9+/eXXLlyybPPPhvnOcjkQTYQHt26dbNdb9WqVWX16tVSp04dadu2reTLl08Fgt555x1ZsGCBChIRERERERHFumSeUAtzEBFd5zATGQJIyC7KmjVrQm8OUUhQJ+vAgQOSO3duSZ6c938o6eExTKFev48fPy6ZM2dO6M0hIkp0eDUlIiIiIiIiIooRDAQREREREREREcWIlAm9AURERNcrjL6+dOmSbRH0aA6ruXjxopw7d47DaihJ4jEcG1KkSCEpU6bkxA5ERFHAQBAREVGEXbhwQY4dO6bqUyAQlNDBKHSkT548yQ4WJUk8hmMHAkGo7YO6fKlTp07ozSEium4xEERERBRB58+flx07dqif0aHJmDGjutOdUB1YnZXEO+2UVPEYjo3PGJmTp06dUhM04FG0aFFJkyZNQm8aEdF1iYEgIiKiCEFndffu3ZIqVSopUqSICgAlNHaiKanjMRw7EDjPlSuX7Ny5U51LEQzC505ERJHFgdZEREQRooeCFSxYMFEEgYiIkhqcO3EOxbkU51QiIoo8BoKIiIgiBMMaMmTIwNoWRERhwDkU51KcU4mIKPIYCCIiIooAFLM9e/as6rwQEVF4cC7FORXnViIiiiwGgoiIiCIAwxhQy4TFTYmIwodzqa4PRUREkcVAEBERUQTou9bJk/PSSkQULn0uZUYQEVHksbVKREQUQZzViIgofDyXEhHFHwaCiIiIiIiIiIhiBANBREREREREREQxgoEgIiIiIiIiIqIYwUAQEREREREREVGMYCCIiKLu8uXLMn78eKlevbpkzJhRChUqJI8//rgcOnQorPV+/fXX0rx5c8mdO7ekTZtWypQpI/3795djx45FbNuJiIiIiIiSMgaCiCiqTp8+LU2bNpWePXvKgw8+KLt27ZJZs2bJsmXLpGLFirJhw4ag13np0iXp2LGjdOjQQRo3bqzWsXXrVrn33nvlzTfflHLlysm6devi5f0QERERERElJQwEEVFUde7cWebPny9vvPGGdO/eXbJnzy5VqlSR2bNny/Hjx6VJkyZy5MiRoNaJ9UybNk0++OADefrppyVXrlxSsGBBGTRokLzzzjuyd+9eadasmRw8eDDe3hcRJR7ffvutOi8UK1ZMTUFt90iZMqVkzpxZChcuLI0aNVLZg3/88UdCbzpRwMzXW265RbJkySJ58+aV++67T3bs2BHx19m9e7e8/vrr6tqJTNv7779fFixYYLvsb7/9Jt26dVPZvWnSpJECBQqo/968eXPEt4uIiCKDgSAiihoEa2bOnKkar+ikmfLnzy9du3ZVQZvevXu7XufixYtl3Lhx3oanFV4HGUHBrpeIkq5WrVrJ2LFjZf369ercor399tvqXHD+/Hk5efKkrF27Vl555RU5deqUDB8+XCpVqqSC1WfOnJHE4Pvvv0/oTaBENKQaWa7t2rVTN0z++usvdf37559/1HG7aNGiiLwOvgsYql2qVCkVYMINFhyHEyZMkIYNG8ZZftSoUWqYNwKr2IZ9+/apGz1z5sxRWb5Tp06NyHYREVFkMRBERFEzdOhQ9W/Lli1Vo9Gqbdu26t8pU6a4vsP5/vvvq3+rVasmyZPbn9JwJxM+++wz2b59e8jbT0RJS4YMGaRmzZre/0bHNF++fJI6dWpJly6dFC9eXLp06SLLly+Xp556Si3z6aefSps2beTKlSsJuOWisimQkUEEffr0UddGHKfIdkUtvNKlS6sMoUyZMsntt9+uhkSHY+PGjSpDd/r06SqQ895770mRIkUcl588ebLaHgRPcUPmhhtuUFm+GKqN56dIkUJ9v5YsWRLWdhERUeQxEEREUbFy5UrZtGmTN2hjp0aNGupfdMBw99EN1BYCNISd3Hrrrd47qhiCRkSxw9+5QUMQGVkMVatWVf89b9481RlOSC+88EKCB6MocUCgcvTo0SqAOWDAAJ+/YcIFZPAgkwd190KF7Lmbb75ZZcohs6d+/fp+l8dQbrwuDBw4MM7fEVDCTRhcd5Gte/HixZC3jYiIIo+BICKKirlz53p/Rt0OO6h5kCdPHvUzUt7d0HV/0Ch1Yr7eqlWrXG8zESV9qAfkBoJB7du39/43itgnlEmTJqkMRiKdTevxeKRevXoq48YpmxaZN6Fk3yBTFsPNTpw4oQKgN954Y8DnIHMOM3JmzZpVDSOz88ADD6h/keHLIWJERIkLA0FEFBWoxaH5SzVH/SBYs2aNq/Wi2KtOaXeCqeS1AwcOuFovEcUeDBfT/AWX4xM62OFkdtD1BTV39I0Up2zakiVLSrZs2dTPGKIVDASYOnXqpF6nR48eqhB1MNm4yEhygqwgfY1OyMAqERHFxUAQEUWFWfMnZ86cjsulT59e/Yv09LNnzwZcr67/sW3bNsep581ZyJzqCAEKyOKOqPkADM/ggw83D3Sq3Dyw7OnzF6PyOHPhkpw+f+m/fy8mikcw+yrchynQsitWrPAui/orTsvhXINhL0WLFlWzJOGchgLVmBHRbnkEoB9++GFVuBqBaRSwR82XkSNHquE1ejnUU0G9FT2MBpmRepYzvFYw7xsBpcaNG0uOHDkkVapUKtuyQYMGatitv+dhiNGwYcNU0AGdeATHKleuLG+++aZcuHDB8Xk//fST3HPPPapwP4Yw4fU6dOigbgLoZfB+rbO3mduD2aesf8cQOf3ZwaVLl1RQAfsbNWnwewxrQgADwwCxn83jC58ptgMzSWK7EDBBgGLIkCFy+vRpv99R1MC57bbb1OeL56Km1BNPPKE+T71cr169bGelq1Wrls/6kHVjXQb1fQJ9jj/++KPaFvB3DOisHBwzwRwnCBxhH+EYwcx5bp9nZuM6LYP3qG/8IBs31O9wqOdjIiJyFrdaKxFRPNBBFV3A1YlZRBpp5+YdejtolH/33XfqZ9ROwKxkVmaBaEwt7wSdH3QOrNDgRQeIyB903tH5QEcVD38QlKn0kv1UzLHg9+cbSvrU0WmCmB1C1Ctx+mxQh+Xzzz9XP6NT/Mgjj9gu+9FHH8mIESPUcB3MOHbu3Dn58MMPVWFnnIteeukleeaZZ3wC0XXr1lVZG5h9CYGS33//XZ577jn59ddfVRaGfh2sG+tq2rSpLF26VA0F+uGHH9Tf0KkOdFxpCIb873//U4EZvAaCGBhqhu1C/ZctW7Z4i/eb1q1bp4YZlS1bVm0HAh8I5CCQ0rdvXxWAQZ01BL/M/YsAwscff6yCRZiZDduJ/YAZp2bMmKGGBaGYMd7rXXfdpd67HiqkvzOgZ3jEa7/44ovev+O7hc8O68cEAXv27FF/Q5ABRbUxm5UOTGA78P4R7Jk4caL6HCtUqKC2A4EU7FdsB9aPIAuCd9bhg0ePHlUFjzGFOt5P7dq11excWBdq9Xz11VfquQhE4ZqBz9Ssk4MZ6zDDl/l5IRCFdaD2DrJv8P4RZAr0mSI4Zq7DaXkUj4adO3fK/v37VQAwEKxr8ODB6mcccyg2/eqrr8ovv/yigl0Yro1txGdvvYGjM4Fw0wY3erBtdvSxgvW5PX7N7cPnf/jwYfWdDAa2i4iInDEQRERRYd6ZNzsRVmZBSTe1PdB4RQMcHTJ0UjBdPBqyqKOAxiPuuOJvGu5sO0FnBnfpzeBVoUKFVPAIdRCI/EFAAJ0PBDPtZsUzpYzxm9Vu9lGkmFmAyDK0vu6uXbtk2rRp6jyB8w+CJgggICPICueYfv36qY4yghZmEBkZhZhKGzM6ITsFhXfh3XffVRmLyLrQnXMELhYuXKim/cZ5Tm+T/ldvM/41h7a6gWAUgkCAgIrusGN2p9WrV6tMIQQ3EKjBrE4agh7NmjVTNdUQUNfbgsDFo48+qpZH/RkEOZ5++mnv8xDUQaYPnoNgj4bfIdsHQXQEZhDgwbkf51Sca3UgCO/R/EwwPBjnYR0Iwt91EADBFWRNYZv+/vtvdV3B6yNY9ueff6rPBsEL1LhBpspjjz2mAgnYdl0IHDPCIWiE94ThTQiUIdCj4RjAFO1//PGHemB7AevFeho1aqQCOngtHA/4TPF+EJwaM2aMWhbHhs5uNWHGOqwf24WMJjdwfGrIsnL63phDtBB81PX2/MHMXghK6UAojntkpOFYwQQPeI9vvfWW+qwwPM085pHxhOsrIDiIzCg7CKrZfc5uYHk8D/s42O9BsMsTEcUaBoKIKOoz96Bj4NRIQ2fa7jn+vPzyy+ruLzpj6PjgjjDu0KLRijviuFOph6YhcOQEnRS7IBUaov6GlBEBjhFz2Ic/yIbZONT5WIwkdJZxZx2dKreFk+NbulQpEmRb7rjjDpVliE46Pi8EexHA0RAIQQCjTJkycZ6LjJTevXurorrly5eP83dkTiAQBAiWIJsHEGgABArMrAp03JGhgywcf/si2P2kh8gieGI9h2JmRgSCEBBDoNwMFjz55JMqQIJggDX7AoEXBEEA2SZ6m5AthEwoDNHFvjVhP+P1EGzBfsZr6vOruV123xfr3/W/OJfjXwxbQyAIQRIEKnD+xwNZPBpmqdTXEwQSzNfQQ4p1AKxOnTre/0YwB1lDzz77rBQuXNhnu6pXr66OnTNnzsi///7rs04EEjG9O97r+PHjvYFAE4J/OA70cLJgs2lxzDg9zwyyIAjmZv2YHU/78ssvfWYKQ7YT3i/2K4JFCKAhMKYDXBgaifeM7UOGXNeuXb31gMzvjA5k4YZKsMeyPjZCuQbzmk1E5B8DQUQUFWhQ6xR3ZE04BYLQOdENd39DyKww3AAPNPzRSEcdCDQg0chHgxXQKXEzGwpRfMOxGa2hUSoQlFwSVSAooXzyyScqkIP9gPMQhqugtg2GTaFAPTIkDh06pLJ6Wrdu7fNc1F5BQBnBEl3U3mQOH0VwR9NZQAhSfPHFFz5ZFThn+St0HwqsExkadtN/mwEWMwCG4AQyehAAQh0hK2TBYFgbhtkiY0R777331PHlFGBHcAHvGYGzSGZV6oASAv54v3aQ/Xn33XerIKjOBgq0H3QGF9i9JzwPmTMYXteiRQufv+Gag6HKuDGB2kLPP/98nBkysW4cVzrLKCGzaQFDFDVcH61KlCih3hOybJHVhgwvnfmDwA6Citj/uM5ifyAAhlpFCPotWLBAXnvtNe92+cvGJSKi6GO4nIiiAkMgNF3fwUoXVQ2n0YgAE4aF6YYw7tDrGiG66CgRxSbUlMHwHARyULMHtXuQlYOsHXTScd7Az8h+QOfX9PPPP3unxEYmjPWBgA4yJ/BAJ1jDDGBYL4Yu4TyIzAkUNwZkCGGYViQhkwNBK10DCOdVZKLgdTGMyq52ki4wjM69U7AB2VKorWNmfSAgAk71YVC3pmfPnq5nogo228PfUCMM8cNU6Kjno98TsoQw3MkM8pj7AUO+UCfH33uqWLGiCo4gSGKFIVUIFiH4YQ5JNoNtKA4eTjatEzOb1pqZ4wTZXXp5p3p83bp18/6MAKOpZcuWasgj/kUxaAynRJYZipTjGEc2keYvG5eIiKKPgSAiigqzEYjGuB00lPXdWTQkw4UMJN3Jwl1L6x1cIiJAoAaZDiiKq6GIrnmu0rVU0BlHIMnfwxwChgybSZMmqRozGCqDnxFMwFAqp3NhJGA7UawZ2Tj4F5mRyNCwgwCINavEjVCfF23I9MLngGLPCE588803ft9PqO8JNyF0oAdD7MyJCjDkDBmpt956a1DrNIen+SuArLNpwW3Gkb5h4m9SBmT46EkWzNk/Ndy0+fbbb9UMbMimwz5Elh2G1umJHDCcrH379q62iYiIooOBICKKChTj1HdQkVpvB3cUAQVMO3XqFNbrYXgY7tyj44U7/+gIERH5YxaLx7lDzyIGesYjZP8EC8OpMFMXagyh043sGwQjEBDShZ0jCedSrBv10vAeUAwbgRCnIUN6+BE68qgv45Z+ns6iSWxQxwdFmZHVggLXqP2Da4vT0GRzGFao7wnHEGr5mFlBuMGBzwJFouMjm1a/V50R5ra+nq4RhRk6/dFTwPuru4PsLAyD1FlaOL71PkSQ1c0sZkREFD0MBBFRVKADgrobgGl8zXR8TU/9jllhrEU6g4FGN2pDoMOG6YJxN5iNUCIKBNk8OvvBzAICneWDIIs5k5MdZETYDZNCIWoUOcaMVXo6eAwdM2u1hAuBdgzFwnkQQ8IwFbzbgAACIZgdyh89HMx8XqDnoGYMZqHSolGrCkEtFOzG+R/b52aWLrN4NmYi8wfZXHpolQnXGh3w0VlBGKKGDC3MehYsXdNKv6YdBC0xbXyw2bQ33XST+hfHCmoAOdFD6xBkcgPZQXof4BqMeklERJS4MBBERFGDGhWoM4G7mnrqYA13y9FYzp8/v5qBxISOF+5IIjiks4acoNGNO99IScdMMOgUFS9ePF7eDxFdX9ChRidWQyfWWkwXQWx/9cZQ5+z999/3/vd9993nU5AYwSbMKoapt9HBx2siYydSMPsX6sW0bdvWZ0pxfzAVuIZglRMMTXrnnXfiPA8zlfkLnKD+kpl1YtYh8lf3BuxuGriB6wgyUpBRY2bV+IPPWweDUPAZhcGdDBgwwHFCA2Qf4W86KwjvH0PzgpkAQUNBbF3A2ymbFoFEvR9xnXULtbC0+fPnOy6na/fh+u0GaiUhWIrsNxzbbjOUiIgoehgIIqKoQacHjWtMSYsCougIYRgCOhBoYOJOPO7emnfk9V1VNCoxMwnqa5jQwcIddqSh424rajCgSCWmkl+yZInt7D5EFDvM4T6B4DyCYaWAIS7mjFTIzNDnJsw+ZgZEzKAFCiq3a9fO+zsEQHS2owmzR2E2Ljh16pTP3/TwGvP3yB7Sw9P80TOW6aFCJjPoggCUVqZMGXVe1sGG4cOHx3kuln/44YdVYWDNDDogy8kuQwaZQNhf+r0CZhDTWS64CWCla8uA/jyCDRLp/YAghnU5p/1gvidMi47gjd0+R2AHQTanQBuyx/TsWphpCzcw9H+HQmfU/Pjjjz7TyWv6+EL9IdwAcQuZY7qQN4KTdvbu3asCagiQIVs3kJdeekk++ugjFQTCsMSaNWu63h4iIooiDxFRlJ0+fdrz8ssve0qXLu1JkyaNp3jx4p6BAwd6jh07Zrv8ypUrPYULF1aPX3/91edvjz76qCdlypSePHnyeJo0aeIZNWqU59ChQxHZzuPHj6MH6Tl69GhE1kfXt7Nnz3o2btyo/k1Mrly54rlw4YL6Nxa1bdtWfY/xWLhwoeNyW7Zs8eTLl8+77HPPPRdnmXHjxnn/jkebNm083377ree3337zTJ8+3VOrVi1Po0aNfJ7TunVrT968eT3//PNPnPU1aNBArWfWrFk+v7/77rvV73F+3LVrl/r87r//flfntlKlSqnn4rw4c+ZM9Ts8b8iQIZ6cOXN6t33x4sXqge2HZcuWeVKkSOH9e8eOHT1z585V59z//e9/nvLly3uqVavmuXjxove1cEw1b97c+5z8+fN73n//fc/q1as98+bN8/Tu3duTOnVqz9dffx1nO8uVK6eeky1bNrXs5cuXPTt37vQ8+eSTnnvvvde7ztq1a6trxr59+7zHcPv27dXf0qZN6zl//rztfnjkkUe86+jfv7/n0qVLnnPnznkmTpyorj36b9gve/bs8bz99tvefVWgQAHv32vWrKk+W3zGeB+33367J3PmzGpb/Tlw4IAnffr0ah0tWrTwhAvXOqwL107r6+D6lzFjRs+mTZviPO/vv//2lC1bVn32+rM2bdiwwZMlSxa17o8++ijO3x9++GFPsmTJbD9D08mTJ9WyWA/2308//eRJyHOqvn7jXyIiiouBICIiBwwEUTAYCEp80DktWLCgt1M/aNAgFaRA5xlBgSNHjnhWrFihAgXoSOsAyrPPPuu4zhdffNEnGGQ+Klas6Dl48GCcQJAOkkyYMMGze/duz/79+z1Dhw5Vv3/wwQfjvMbYsWO960QwIXv27J7XX3/d1XseOXKkzzahk48Az0MPPaQCGvr3CKI0bdrUJ7AzefJkT6pUqWzfW4UKFVQwxgrnxzp16tg+B6+rAyxWn3zySZxl8S8CXgjamH9DIAPLIyC0ZMkStT/03/r06eP5999/4xzfq1at8glsYT/iPSOwtH79ehXcwO+TJ0/uKVGihAoGaWvXrlXBO7v3hMAVtsGNvn37qud89913nnCdOXPG07JlS/X5fPjhh2q///zzz54qVaqobVqwYIHt89544w3vtiOIZWfRokXq+Efg8d1331XHMAJdPXr0UK83ZsyYOM9B4A77DPsCn0Hu3LnVvsTnd/jwYU8kMBBERBR/GAgiInLAQBAFg4GgxOP777/3PP74456SJUs6Bm10EAABAmQb1q9fXwWAkBkUyNKlS1WAB1kWyHhBFs7gwYNV4MlKB4LMR7p06Tx169b1TJs2zXb9CIQ89dRTKvOkSJEiqnPuFjro6Pwj0xKBjxo1angzgxBMQNYSgkNPPPGECoZZrVu3ztOhQwdPrly51HtDEObVV19Vz3WC4wsBqEqVKqnXRKDmzjvv9Pzyyy9+txUZKAjC4DmVK1f2jB8/3vs3BOTatWunAhz6GMY+c/os7bK9cBwgUIL9jWNhxIgRat9Cz549PRkyZFDBFWRdWSEYgs+gaNGiaj8goIjnmAGjQN555x31upH67iFoh2MBAUe8J2Te4Di3C9CZGUE33nij+jz9BaT++usvT7du3VRWHII/eL9du3ZVQTE7U6dOVcE0BKFwTA0YMMCzefNmTyQxEEREFH+S4f+iORSNiCipQC2GLFmyyNGjR1VNCyJ/UKAXxcqLFSvmOD11QsBlHnVOUHcmGrM1EUVaUjyGsc2oWde9e3fp3bt3Qm9OzJ1T9fUbdQgzZ84cb9tIRJRUsVg0EREREVEEYeIDzJCJgtNERESJDQNBREREREQRglnJBg0apGaQQ1YKERFRYsNAEBERERFRiF555RXJkCGDlC1bVvr27SsNGjSQXbt2ycCBAxN604iIiGyltP81EREREREF8vnnn8uZM2dk06ZN6oGaNPPmzWNtOSIiSrSYEUREREREFKLhw4dLkSJFJHv27NKhQwdZs2aN1KhRI6E3i4iIyBEzgoiIiIiIQtSsWTPZsWNHQm8GERGRa8wIIiIiIiIiIiKKEQwEERERERERERHFCAaCiIiIiIiIiIhiBANBREREREREREQxgoEgIiKiCPJ4PAm9CURESR7PpURE8YeBICIioghInvzqJfXKlSsJvSlEREne5cuXfc6tREQUOTyzEhERRUCqVKkkRYoUcvr06YTeFCKiJO/MmTPqnIpzKxERRRYDQURERBGQLFkyyZQpk5w4cYJDGoiIwoBzKM6lOKfi3EpERJHFQBARJUi69/jx46V69eqSMWNGKVSokDz++ONy6NChsBqNn376qTRu3Fhy5MghqVOnljx58kirVq3ku+++i+j2EznJkiWLXLx4Ufbu3ctgEBFRCHDuxDkU51KcU4mIKPJSxsM6iYgcYdhM69atZdmyZTJq1Chp37697Ny5Ux544AGpWLGizJs3T8qVKxfUOi9cuKDW880338hjjz0m77zzjhQsWFC2bdsmL730krRs2VJ69Ogh7733Hu8sUrxKnz69Ovb27NkjZ8+elcyZM6vfYXhDQh176FRdunRJUqZMyeOfkiQew7HxGeMmEYaDIRMIQSCcS3H+JCKiyEvm4S1LIoqiNm3ayMyZM2X06NEqaKPh7l/JkiUla9assm7dOsmePbvrdT777LMyYsQIefHFF+WFF17w+RtOcbfeeqssWbJEPvjgA3nkkUdcrxeNUdyNPHr0qNouIrfQmTl+/LicPHnSW/A0oeA7gALWKLjKTjQlRTyGYweC5hgOhmtvOEEgff3GeRgBeSIi8sVAEBFFzbRp06Rjx46SN29e2b17t7q7a0LWztixY6VLly4yceJEV+vEXWIMBUOjb9++fWrdVhiG9uCDD0qtWrVk+fLlrreXgSAKFy6xuLOdkDOJ4bUPHz6sviecfYeSIh7DsQGfLQpDRyLYx0AQEZF/HBpGRFEzdOhQ9S+GalmDQNC2bVsVCJoyZYpatmjRogHXibpCaPCB3Tohf/783iFkRNGEDg3qVSV0Jxqdq7Rp07ITTUkSj2EiIqLI4tWUiKJi5cqVsmnTJvVztWrVbJepUaOGt9E/YcIEV+vNlSuXpEmTRv2MYtF2UCsIWrRoEdK2ExERERERXS8YCCKiqJg7d67352LFitkugzRuzPQFixcvdl1PoEOHDurn559/Xv744484Q3M++eQTKVWqlPTr1y+Md0BERERERJT0MRBERFGxdu1a789FihRxXE7X+FmzZo3rdQ8bNkwN/8IQsQYNGsj8+fO9fxswYIDKGEKxaNYJICIiIiKiWMcaQUQUFTt27PD+nDNnTsfl9CwhmG0J02+nS5cu4LoRBELw57bbblPTdjdr1kzeeusttQ7MPoLsImQOBXL+/Hn10HTtIQxVS8hiv0ThwLGrZ10iSop4DFOweKwQEfnHQBARRYUOqkCGDBkclzMLPh87dsxVIAjKlCkjK1asUNPTY3jYE088oTKBME29myCQziwaMmRInN8fPHiQhaYpSXeIMHMOOtIstEtJEY9hChZuBBERkTMGgogoKtCA13RxZzuYalsLdgpZnQ305Zdfyp133qmGlz3yyCNqWBoCQoE6EP3795c+ffr4BK8KFSqkClJz+nhKyp1ofJdwHLMTTUkRj2EKFmaYIyIiZwwEEVFUYIiWhuwap0bauXPnbJ8TyPTp01VGzy+//KKm6166dKncfffd8t1338n777+vsosmT57sN7iEAJVdkAodD3Y+KCnDcc/jmJIyHsMUDB4nRET+8SxJRFFRuHBhVynbhw8fVv/myJHD7xAy64xknTp1Uhk9CALpWkMzZ85UQ8X01PKjRo0K810QERERERElbQwEEVFUVKpUyWcIl9PwsQMHDqifK1eu7Gq9GErWs2dP9dzbb789Tr2hzz77TGrWrKn++9VXX5VLly6F8S6IiIiIiIiSNgaCiCgqmjZt6v1506ZNtssgQKRn7WrcuLGr9aJA9LZt21TtCLvC0sgQGjNmjPr50KFDsn79+hDfARERERERUdLHQBARRUXt2rWlRIkS6ufly5fbLrNq1Sr1L2b5wlAvN/bu3av+Nad9t6pSpYpky5ZN/cyMICIiIiIiimUMBBFR1Ap9Dho0SP08Y8YMNQuMFWr6QJcuXXxqCrkZcoZi0Bs3brRdBsWpT58+reoGlS1bNox3QURERERElLQxEEREUdO1a1c1vTuGgE2dOtXnb1u2bFEzf+XPn19GjBgRJ1OoSJEiKjiks4a0MmXKyF133aV+HjhwoM809doHH3yggkFPP/20CgYRERERERHFKgaCiCiqWUGYwr169eqqwPPXX38tx48flx9++EEFiFDnZ86cOepf08SJE2XXrl2ye/dumTRpUpz1jhs3Tm6++WaVadS2bVtZu3atygD6888/ZcCAASoA1K1bN3nhhRei+G6JiIiIiIgSn5QJvQFEFFswLfyiRYtk5MiRarr3HTt2SIECBVRNoH79+kmWLFlsM4lmzZqlfkZAxwrPWbBggXzyyScq0NSwYUM1RT1eq0aNGvLVV19Jq1atovL+iIiIiIiIErNkHrtxFEREJCdOnFBBpqNHj0rWrFkTenOIQoJ6XAcOHJDcuXNL8uRMBKakh8cwhXr9RtZx5syZE3pziIgSHV5NiYiIiIiIiIhiBANBREREREREREQxgoEgIiIiIiIiIqIYwUAQEREREREREVGMYCCIiIiIiIiIiChGMBBERERERERERBQjGAgiIiIiIiIiIooRDAQREREREREREcUIBoKIiIiIiIiIiGIEA0FERERERERERDGCgSAiIiIiIiIiohjBQBARERERERERUYxgIIiIiIiIiIiIKEYwEEREREREREREFCMYCCIiIiIiIiIiihEMBBERERERERERxQgGgogo6i5fvizjx4+X6tWrS8aMGaVQoULy+OOPy6FDh0Ja33vvvSfJkiVz9Xjsscci/n6IiIiIiIiSCgaCiCiqTp8+LU2bNpWePXvKgw8+KLt27ZJZs2bJsmXLpGLFirJhw4ag1ufxeGT06NGul7/99ttD2GoiIiIiIqLrQ8qE3gAiii2dO3eW+fPnq+BN9+7d1e+yZ88us2fPlpIlS0qTJk1k3bp16nduzJkzR3bs2CH9+/dXz82ZM6ekTBn31NaoUSO5ePGi+peIiIiIiChWMRBERFEzbdo0mTlzpuTNm9cbBNLy588vXbt2lbFjx0rv3r1l4sSJrtb50UcfydKlS9UwMye//fab7N27V3r06GEbJCIiIiIiIooVHBpGRFEzdOhQ9W/Lli1tAzJt27ZV/06ZMkVl+QRy4cIF6dKli98gEEyfPl39e88994S45URERERERNcHBoKIKCpWrlwpmzZtUj9Xq1bNdpkaNWqof69cuSITJkwIuM7UqVPLnXfeGXA5BIIKFiwo9erVC3q7iYiIiIiIricMBBFRVMydO9f7c7FixWyXyZIli+TJk0f9vHjx4oi87q+//ip///23tG/fXs0aRkREREREFMsYCCKiqFi7dq335yJFijguh/pBsGbNmoi87meffab+7dixY0TWR0RERERElJSxaioRRYVZ8wczezlJnz69+vfkyZNy9uxZSZcuXViv+/nnn0uJEiUch6OZzp8/rx7aiRMnvEPV8CBKinDsejweHsOUZPEYpmDxWCEi8o+BICKKCh1UgQwZMjguZxaRPnbsWFiBoBUrVsjOnTtl4MCBrpYfNmyYDBkyJM7vDx48qApTEyXVDtHx48dVRzp5ciYCU9LDY5iChZtJRETkjIEgIooKNOC1NGnSOC538eJF78/h1vTRs4W5HRbWv39/6dOnj0/wqlChQpIrVy7JmjVrWNtClJCdaHyXcByzE01JEY9hClbatGkTehOIiBI1BoKIKCoyZcrk/RnZNU6NtHPnztk+J5TAE4aFlS9fXsqVK+fqOQhQ2QWp0PFg54OSMnSieRxTUnY9HcPnL12WNClTRP251zNc83cfOSuFsqfzHitEROSMgSAiiorChQvLb7/95k3ZdgoEHT58WP2bI0cOv0PIAvn5559lz5490qNHj5DXQUTXh73HzkruTGkkZYrwO4cXLl2R5Mkk6HWdPHdRUqVILmlT2Xfir1zBsKdk3n/ddHwvX/EE3I6zFy7LmQuXJEdG50xM/frHzl6U7BlSSziOnL4gOw+flkxpU0malMmlYLarHfNLl6/IriNnpGiODN73h/ew7eApKZw9g6ROmdxnH+85ekZyZ04rGVKnkLW7j8nJ42fEk/ac7DtxXn7ffUyQZHrw1HnZdfiMPHVbKdl99IwcOHFO7rqpoNonJ85dVMvdXCKnrP/nhLw+d7O826mKZE6bKs42b/73pLy7cKv0bVJKiuSIe91Z+OcB2XH4tHSoXkhOnb8kuTOlVft16V8H5ZFJq+WxBiWkTokcUjZfZvUZZ0hj37z+a/9JuW3kEvUz3td9dYvKewu3ef/+TLPS0vPWEnGed/jUean68o8+v7upcFZZs+uYlMmbSb5/sp7fDNrjZy5KpaFzpUax7DLt4Vq2x9f36/bJS99ulOda3ChPTL16ra5ZLLuUzZ9ZJvx0rcbfb8/fJtkcjhF8nlNX7pZy+TNLpUJZvcfVXwdOSfrUKeT8pSvS+K3F8sLtZdU+X7zloOTNklZGdagsWdOnlm9+3yuDZqxXz8ucNqU0KJNbcmZMI+OWbVe/y58lrew9fu1mEXz6UE2pUyKn9Jn+u3z92z/qd4NblZV2FXM47g8iIhJJ5jHHaxARxZMXXnhBhg4d6p0RrEqVKnGWwekINYFQsLlRo0by44++Dd9gPPHEEzJ69GjZtm2bFC9ePKR1YGgYprQ/evQoh4ZRVOA7YO3QoSOFDlS61IGzALAsmB29HYdOycFDh+To5TQyd+MB6d+8jAoKnD5/STbsPSGVCmXxZhjg+Xh5cxvQId95+IxsP3RayhfIIpX/6+BdW/9pWbr1kNxdtaB8v36f7D12TgUA0Mnbd/ycjL23qpTIndF2W6f8slOqFrna2XQKnrw1b4ukSJZMejUoEacDOv3X3Wqf6c4/7D5yRuqNWCgtKuSV5uXzyeP/dWph6yvNfQIn6MwfOnVeBSuWbzusOq+b9p2QdmOXS86MqWVh31tVQENDQKXs4B/Uz7kypZFf+jfy2df6tXVH/a32leXWNxb5bPNPzzWUusMXeP/7iYYlpGudolLN0tH/sU99n/2GTjLeS5OyeWT4XRXlppfmef92f92i8sLtVzMfv1u3T1KnSC6Ny+ZR+6ZY/++8HesT5y6pn7/oXluqFc3uff7gmetl4vKd6uepD9eSIjnSS53/tnHEXRWlffVC4s+5i5elzPNzJBgI/CDgY8Jht31YSyn63GwJx4YhTaXcC1c/p3olc8rSvw55/7bl5eY+QadBM9bJ5BW7vP/9ZY86UrVINp/1Wbfnjkr5Zdbvex1f/8setdVxbeXmfa0a2FgdW8E8757qhdQx4cR8/kuty0mX2kV9/o6gWcUX54pbS59pIIWyX53YwbTgz/3ywCe/qp93DG/patsjAa9lfZ0/BtRT12/Ulsqc2f78QkQUy5gRRERR0bRpU28gaNOmTbaBIGTw6Fm7GjduHFY9iS+++EKqV68echCIKJTgzZ//npDT5y/L1gMnZcGfB+TgyavH8721ikjLivlsh3T8seeYDPh6ncpa0IECrFPflcddf5j71C1SKs+14ZJfrN4jfT//XdpUzi/PNb9R7v7gZzU0AtYPaaoCND2nrFFZGCY8D51h3VHWHSlkl7QavUzduUegAO/pl78PS4cPV/g8/4MuVaVpubze/372yz/kl+1HZMW2wzJ73b447w8ZAL+/0ESypPPNxCg+4GqAwqnz+8wXv8v0X/d4//vjZdtleNsKcne1QpIieTJZs+uoPPPFH+pv2w+dkeeal1E/60DMd+v+VQ/Tt3/skzZVCnj/+453l6lsBa3tTQXkqzVXswoOnbog01bulodvuXYO+WjJ1cwEwGc74ofN3teFh/53tROsPzdrEAjMIBC8s2CrZEmf2na/6c406IDW3I37pUKBq0EbDRkbzzYrowJb+MwBn/GFy9cCLToIBAh0mevWQSB47qs/5NB/xy28Pf+vgIGgHzb47mc3rEEgwK3Jfy0ZH6GYt3G/92czCAS/7ToqNYtfyxYxg0Awftn2OIEgK39BIJi7Yb9tIMgNZMpYvwuBYHv8BYJMz8/cECcQhOMmGK/N+VPe7XRTnN9v2X/tuxRNvKdNRBQ8DqAloqioXbu2msYdli9fbrvMqlWr1L8pUqSQTp06hfxaS5culX379rkuEk1JU7iNfzx//qb9KovDXBc6otVf+VGajVoix844zxa37/hZdRcaGRf4d+X2I9Js1FK5a8zP8uyX6+SHDftVMAAPDFt4/NNrmSnIOkEg4eGJv8od7/7kDQLpQMHNry30Pldr8t+QEg1BIJixdq/UGjbfGwSC//28Qx77NG4QSMMwGhOG7Rw4eU5t1+qdR+X42atF261BIJhkBA0AQSCwCwJpGLJjrXNiWrLlYJwMEzMIpD331Tqp//pCmbvhX5XNpI1dfG14jT+/77m2P8EMAoEOAmmvfLdJZd4gKwoypPEN5Flft1Te0Oqa5XAYaoNhT38fjNu5Pnw67nGJzCb9uZlDwuykSpHM8XuEoV03l8zp/e/9ls/OzlGb7QnV+n+Oh72OsxedAxtpHIbmaat2HFEBPWTJhHqeyZI+7vAzt6wjvPBdCKRzzcKu19+gdK44vwv2LSKgioy+Ryf9KkO+2eD9PTL3EoLd94GIiPxjRhARRQWyCwYNGiT33XefzJgxQ95+++04xRxnzpyp/u3SpYuqKRTObGFYd/v27cPebkqcen26RjbtPSHfPnGzLPzzoBqShM4JdKhWSI6euaAyJ0zFc2WQvw+eDjjcAUEVQKCm8tB5PpkTptrDfDM72n9gH+DU9Pb8uHG/PDTxV5V1gKBLuEPH7Ixe8Jecuxg340IbMedPn/9ev/eEGgplBhVQs8OO3TCvQC5e9sQZ6mbCX4d9t0k2/XtSJtxX3W/nd8/Rs6ouC+qKaM2MDCU3Q+fUa7rs/arAyGsL1XFwxfKc++oUdRXQCTQkypotpYc03fL6Qu9QJwxbQ80bMD8r7fCpC9L6vZ+8/33m4iUVnLRTvWh2tQ3YFutxgjo3hbJdG/aTw+a1rPJmSSeRstUm8BWsE0ZAzApBTxOGOqIGkXbg5Hn5cdN+6TF5tXzctXqcwIypSuGs0vamgvL8f3VttMvG8Y5jueu4lerzRCYbMu/8uWT5O4ZwhpJd5QQ1d/Txv3n/SSmSPYNctOyTQOqWyCEb951QwW5dk+dqgeZrOwvvE+83GBjiNm3VbgmWm2AZERH5YkYQEUVN165dpVmzZmoI2NSpU33+tmXLFhXAyZ8/v4wYMSJOplCRIkVUcEhnDTm5fPmyfPnll1KvXj0pUODaEBBKOMicQeYEHrrBjo4HOl+vfrdJZd2gc9bxwxUqswaPn7f6DufQMFQJf5/9xz75+9BpVRwUQSEdBILPft0dJwgE/oJAMOz7Ta7eDzKG+ny2VoKFwqvw5ZqrmS5OQaBSeZwDLWanHXV43CwHOTOkUjViNN2BM/erGZxBZxhQ9NXq0pXgOo2gO5qox1J7+HzvkDkzQ+iDJX+rzKCfth7yG8Sy6/yaQ6Aalcntsxze95ONSqqfzT62GZxyy9pJ14Wf8f5QINlp36Aui3W721craLv9mlm0GUGei8b7tQbS7IYrYXjSk9Psj9Oftx2W8i/+IAs3H5B5m3yPBbwXMzBg7id8f1Es2YQssu6TV0s4EAAw60JpGKb4VONSQa9v2PfXAp0ZLYWbrZ+7U2Dmp62H1bDH836OxcLZ00uXWkVk+7AW8sR/x5j1OPl89R5ZueOIvDlvi6QzspEQRNLZWX1uKyWVCma5un2Wz/ayEXxEbaJ+TUuros/31irsHcKGZfT7wGeHTDInettwjtQZjOb2jr037pAv67mmWM4MPkFRvU/N4GqwwSW4tXRu9f1tXj6vNL4xj8/fPu5aLSKBMCIiuooZQUQUNbhjOHnyZGnevLn07NlT0qdPLw0bNpQVK1ao2b1y5cols2fPVv+aJk6cKLt2Xa3jMGnSJFX7x8nixYtl//798uKLL8b7+6HA3v7xLxn54xbvfw83Omjah0v+lupFs8mqHdcCI50+/kV1rqzZL9ahSqscMh5CgUwIPZvP6f9qZqDorhWGZC1zCFT5s/W/YUjmXXMU5UUh5h633iBjFm3zdlyxiF3/FAGDK+c9alaiYIZzHDp9UcZ0riI9plwbnmbt6Jl31Tt99Iv8+VIzb62S19tVVAEwdGZRB8RfZlLpPJnkh6duUT83eGORKjKtAx26Hot1eJn5XtCBtA4ds2MGT3RHENt10qiFA8h60ZkJZqc6UGaGSQcTzIwi+HXHEW/NHZ2tYwfFsNFxN+vRpE99rQlmDYzBzLXXAjtbD55UGRgaCnIDOswI6mBIWEpjuBfMWe+/bg/22f0T4gbWEWS6eCXufsK+rTRkrvr78v4N1WuWyZtZDXu0O57sglU6o0x/F7RX7qygamohAKk/Sxx7S/o1UJ8dgkOV86SUx778y/H9OH1nrNkiZoACwbt1/w1F099Fa3BtYMsbHV9TD4XCdwHBnIMnz6lZs8xj65xRf0cHT+b0rqf2nd3wSGsARa8LAaN3Ol6trYfC6fr8ioAyjqvPVu2W75+8RQ3VQl0kBE5QMNzpe4NaYYDjaurKq8dl1vSppFn5fLJ6UGPvLGWoQTbqnquv+/6irTJizmYVHEtufP8xFA/bbf0eI1CKYN4ZlzWIUBB/3H1Xr+8j521RmVma+V6QQffm3ZVUPSy8n91Hrw2LBbMYOBER2eOZkoiiCtPCL1q0SJ555hnp37+/5MmTRwWFUBNo3bp1UqFCBdtMImQD4dGtWze/6//ss88kZcqU0q5du3h8F+SWGQTyxwwCaU4dSZNZEyUY9UvlkvH3+d5hTvdfx9ws1GpX68PaibWDoRIIZGGqZbOORcM3F8lq471e+u9Ouhlwwq90P7JpuTw+d8IxfApFnjG8zC6LBHfq7dxSPIua1trJi99s9BkeAxv2HvdmQ6CuSor/Ag0YbvTpyl0qMDDK5vM1A126Fg06gphRSLNuu5lJgKneB8+8VnfEiZmZgm1t+c5SFXhC9oUJRZ/1JuF1rnZYPUFlNunnWzOCfkU9pTNXpyn3J0Xy5HEKhZvTyOsps53omZg0nVWGwIV32yyZLpaRt64hEGNmpejgF+pO6e8khkUimwTF0S/YBO1G/jdsD8EvK7up1RHsKfrfsauzSzKlSakCA+jUD2hRRqoV8j/zk1Ncz/qZIXCm1X/9WjFvM1vHbSFl69CnDP+dQ6yvaQ3qWLOUzODF05//rgpGAwKiOohiBl408+Wx31BzTBfHnrhip21AUA+NM+v5IBgP+hyBWQWRGYThbANaXAuE6WMYx4EZ9Hlv4VZ1XkJNLet7DSZbJ60RwDHrcWGmQx2ogtvK5lGBIX1+WbT5gO+KWDuaiCggZgQRUdQhE2jgwIHq4QYygHbu9M0gcPLBBx+oB7mHIsEYjvP77uNyR+X8clNh/zPm+PP9un0qI+OuqgWDqg+RL0taNdW4CZ0Ns7MMBbKmk3+OXbv769ThMhXKns6nkPKMXnW9U6AveLq+NHxzsfoZQ9QGfr1OdhhZAejsIHCwaPNBlbWEujn/Worn3lo6l/q7Ce8dWQKY7vyt9pW8WRPW4Wk6IGJ2esygwrC2FdUQIQSKsKyun4EhK3bsMpjgnpvyBLxLPvBr32DEE1PXSu7MabwdtJTG5znqx7/iLK+ZGQ36NUfP/0sFTbRPf/Gdqcn8GDGrkRtmdtnRMxfVw4kOTqFTX3Lg994gm1v4PNftOS5TLNsNHT70XxsKsO+s+9/f0D63kM2h6wVZM0lWbQ+u/pQZXMVwJu3k+Usy/dfdKlhoNeO3vWqWPCtkKq17sYlkSpsqzqxcCMLaBc704aULc7uls496NbhB3lsYuGg4hqgu33bIm3miWc81mt3Mb5r1HKeDpXgPQ7/ZKF1qF5FFW64FKXQgLaVNlE4HYRD4uf3dZfLn0GZS45X53mC33fnUDLoCsu80/AWZVNZhewgYIUiD/8XZBmN9yAzCw+6YRabU34dOxQkkmX7dcVTmbPjX7zkawTezsLf5GZiB6yI5rgYJZ/W6Wb75Y6/ar+b7x6x5JmRe7XEolE9ERFcxEEREdB1BpsPBU+cld6a03t+hDgiGgOTNnFYFQfJmufq3bQdPSaP/giDaJz/vkJ+fayj5s6YLqRZQj/+mrsYwj7Fdqrp6HjqylQpmlX3H/5WX2pSXwTPXqwDMeXQQLEV0UWfnq9/+iVMUFnUlUAQV9YGs0loyMcyARvFcGb1Ddj5a+nec2jTYn+/M/0tGL9iq/nvlwEZx1o+hD1ZNjCBDSj+ZOLrzXr9UbhXYsPaZ9F17awaN0yw5aVLZvxY6cE5BIicIuOmgG4IYZkaC3VAmu6E4f+67mtlgBoHsvDDLXfAnVHrbfzMCENY6Sf4gqIfOuZ0//8ve8AfDtsxOtr/PKlg6uGBOAQ92GWNu6PpQpme+sA88Os3Whv2FIJAVhpRZZ4jTdKDjx01XAyeoARZnO5qWkhE/bJGbCmf1zqj32l0V1bCuigWzugoEwe97jku1/4Y+BROYQ40esxaTNTijzy26Ztn4n7bbrscuqGMGPpBF023CSp+MR7sMSbssIW3L/pM+AXAN5+YbBnxn+xx/5yrrdpszHdpBQfxAcO50CgSZ70wPFS2cI713WJzaXoebDQg+Dfk2buCSiIiu4dAwIqIkChkKKPK749Bpmbxip3cqc9xFxs8N31ikpkfXdUCQyfL41KuBGkD2i513F14NegTr0MlrwQncCbZm5XzVs47tjFMYuqOXR8NeN+71nWRMX42O0eItB32CQLpDp+8Mv9auok9RXqc7/dbMDB1ssStQjC3QQSAYtzRuxy5dKt97Ki0r5pN8xixK/hKjzvyXTYFO6Mfd4hZDDXZ4j1OwB0EIc8rwYFkDQW65ydiKBv0ZW2vAuIWZw4Jh7aCq49ry2ViHioXKWhMpMcP3AsPk7Lg5vh6pV1xm9qqragqZ3+8qhbMFPUOVVaDixgg2Pd+qrN/P+R9LrRondttqDUTpIV7+alr5i9sgw7LFO0slGNY6U1Zh7uI4rAHttEZw1Mx2wkyBdvx95sHOyEhEFGuYEURElMQgkwe1YvSdcye4o/7g/3517HyeOGvfgUT9iknLd6gMm78OnJLHGpRQnc2/DpyUUnkzycI/D0i9krl8ZjXq/9U6b8FRayFd+LJ7HcmdOa3PXV47qF+RTC3lkS7jflGz46D4aiC6P2BXwNjsXIC1Vo51eIW/AACCVoEygnL9Nz3ztW1zXr/O2kAHzG65YDu3TsO/EATyVyMoEDzXbU2dEOJF8c4cThgNV/fXtUwHDAWydrJThxGYS8qcsjjcwHe1UqGsPkOgwglw+mxXgO8HgojW75f1uNJZSoHYZwQF/z5CCc76kypA5PnquTn+OA3Pc3qf/s6PnEmMiMg/BoKIiJIY63CuYJQrcK3gKoZ/ocZIu6oFZVDLG6Xy0Hm29R7emmdf8HlR31tVgdeJy3fECQLpmZSsDfZA/Zb5fx7wjgnYdvC0erih129/p915aFiwnSldpNeU3qjvA9bOopt+LwIHtjVAguzoOQ1vSRlmIAj7bONe/0NBotVZDMUfe9x10CPFHO6is02snexAgQc3dO2opMSp8x5M4XfzOxyJ/QhmcMnt98s6fM1t4NYuGBbK9zPSgaC9x88maJDXHMZrFqN22jVmwWsiIgoOh4YRESUi6DA2HblEDe3CsC/ArEQfL/1bDZEKm9G4zp7hag2P4rkyqCLI91QvFNSqUES11eilrmZ40p21QEECZE6EcidXd4jsOgbWO+3WQI11tqxgpbcMDbO+ntM0677PSW47HXykMoLQ8UwVRnFibN8MYzpzf3YlwiKtiWGAmjXzLJzMmEgWnI42p/f9x3/DPN3wmZkuQuOV/NW9chp6af1vt1ti970OZdhipIdqBRpmaNb/ig9m3SxzJkGngJebcysREdlLei0IIqLrBGo+HD19QQV8MMUvHpjRaPP+q8VnO3y4QjWG+37xu7w8e5Pc+/EvfteHmXow5a8/Hyz5W1q/u0w16Kf/uscneBJoVik7gQqGWjt/+r05CXWYh+4o2PULrHftrXfe9fTMobIODbOu381da2Ts7Dh8Ouw73nZD17yzVoUxhAbvCTVSkqqELlWE2ljWTnso3zerSKwj2sKt5WMNJkVifW75G0YKF10On7Tb5t92B1/TJprvHS5cjt8vkhnYNK8VTu8z2kM+iYiuJxwaRhSjbrrpJpk3b57kyJEjoTclJq3eeUTuGhN42mkU/ETBZ0C9nlPnne/YPtGopJqGPRAUWC7z/Jw4jexwhg4F4nrIRIjboKecdnOn3VqrZdM+d8EsJ+lSBxga5uItIbPAbv8H6nhaOb0XBMrCGhqWIpmUypMp5GLLCc3MLkgIyFywzvoUiWyGQMV97QKtmD48qQflzAyRSA0NC0Wh7L7B0Usu96195mJyuXg5uKD06TCD2MG6Es8RVfM7oWdeg8w2M9AREVF4kt6tJCKKiLVr10rPnj3lxInwOsEkQXfUMezLTRBIDxUzG8flX/jBcVkEQUK5Q6yfg+mG44vbIESoGUG6SKub9x+oIGq4w3PiDBdxkxGkMnbC365ONQrb/h67JZywA/ZZAsdSwnIlEWw8Mv5MWdOF37kNth5T+tQJf/9v4173Q8CimREUSnYVCur7Cww5sQvwZkgT/Gfzg2V2xvjmNuMp0pqVz5sgr0tEdD1jIIgohn3++edSoEABFRDauHFjQm/OdQ0ZCTsPn5bmb7ubzjfTf52CCT/tCOoueSjFQ3VHKj47y277auHWPLF7D3FnDYvwTDuWAI51E9x8JvgMgs3usFO3RE7H4ylj2tCDAKlSJu1aHNGOAxW2CQhYp/+uWiSbRFukj/1Q1CiWI6JBm0jUWoLu9W8I+jnW2K3TrFemdA7LVEuA4yFYbjOeIi0+s1WJiGIVz6xEMWz06NHqgSBQhQoVpGHDhvL111/LlQS663c9e/2HzVL/9UWul9exg09+3hGnA+kvmBBKICjTfwECc8aWSHM7DAbFoiPd4T930fd4jvRQEmsnZdWOIz7/7aafiv0TiWBFhYJZ4vzux6fqqSBTONkgVz8XdxvYsca1ouMNy+QO+TUj2dGPdo0gu4LZ/VuUCTsDxCrYr3tiKK5bqVDcYxQevLlYSHW5/A2Hndmrrut1BqqhVb1otoDnKzdDEAffXtb29wNa3Oj3edaA9tXXk6i6FOEZ6gpkTef4t2QJWAuJiCgWMBBEFKO6desmPXr0kPvuu08WLVqkhoqVLFlSunbtKsWKFZPhw4fL4cNXZ62i0OspIHUfQ8HeX7Qt3l8PHeZQ+nm6/kJiKDwbTGcwsRQFtmbyWANDkZ7iOZihKpA7c+C6UW4ySdx2OtMYAUW7zJhgTXygRljPL5M3kyQGdW6wz9YKh1N2iZNgAwdf9qgt0crueLpJKZ//nnBfdb8BrSX9Gsh3T9STHDbHvJ4NsVKhrHJb2TxhbZc25t6qcX5nDVA0LRd4CJPT2SB7htR+n9evqW8gMVAgJRSBspIuRfgE+15n58kNzBsgjAMREUVewrf6iShBTJgwQZIbdzOREfTBBx/Inj17pHfv3jJ+/HgpVKiQ3H///bJ69eoE3dakGgQqPuA7eXSS87579Jbijn87EWAaX6e6E6HEHHTGTGKYijrcwIE5NKzRf9koi/vdGvZ2+X1NS+co7vTxkmAKZU8nGSOQeRLM0Awz8IUaV/6M61bN79+3D2shdRyGu2mNb/SfdfRR12pSOHtkO8yhWjmgkdQrmVM+6BI3qBCKmsWzRyy7qnfjkj7/jeOmapHssmN4S3mnYxUpmC3wPuxQrZAKzlj3v/6bv+wOZKyZ+6V+qVx+X6twjvRSNn9mn9/90PsW78/Z0qcOashVoKwTHWQtnjOD43M61ijszYhre1MB2/XUL23/vgKdf203L1nwgb0KBewzsuDdTjeFHQiqWyKH38ys6Y/WlhtyZZCH6xWTyoWyyqgOldXvH2tQwuc5ZnH1xJDJRkR0vUn4Vj8RJSpZsmSRp556SrZs2SJvvvmmTJo0SWrUqCF16tSRqVOnyqVLwQcoYsnBk+elwgs/qCBQIPfWKhLR18ZMNMEWjzWDJykiXEQ5FMHOkhWH0U8Z1Kqs6sQWyXGt46Y7o5FkLRAbakZQeT8dtFB9/+S1jjHULBZc4MDMenKbC2B+hP5qiiDj48Z8vh15q0AdQHy+H3erLr8OaiwvtS4nK/o3kmzpU8XJmgin2CwCEuiw+oPXQEd27lO3qGCP6eU25X2ysyY9WDNg5giOU10nzMmj9YvL4FblJNjP0Sng8IAlG+/Th2t6f76jUn5Z9mxDKWYEQaxQ7Py1dhXjBGeQkbN+SFMZfleFq9vg5zuO4G3J3BmlRrHsIQVQSxvZXzpogffVv3kZ+aJ77TiZYiPaVQw62DDbCHRZv9sIDA1rW1EFMNtUjhsI+qpnHcmXxT6ghiGrX/esI589Usv277ZD4DzBDZ9CYO+bx292/HugdQSqEfTQzcVkykO11L+mG3JnVN9VPPDZzn/6VhnY8uoQuTZVCqj91bdpaZ/n3Fwy+Ay61pXzB/0cIqJYlfCtfiJKdHbu3CmPPPKICgih5gEef/zxhzz99NMqS+jFF1+UgwcPJvRmJjr/Hj8n1V/5UU76meJde+SW4hHPFEEcJ5Q4iu4YRvum66CW/mtihOKCkYFi12m1dlLDhY4NAjhv3l3J+7ssltmgAgWCnmp8dUhMidwZfX7/8X+ZFOGwZgNl8jMNMzIZ8joMI7s6a5gn6GBejoyp4wz92/xyM7XfGpTJLfmzpguY0WN6+jbf4UNmtkaX2kUlb5a0sub529T65/SuJ789f5vantaVCsinD9WU1YMay+QHa6rAETqkphYVrgVnyhfI7BNcmNGrrjdzwWrT0Gby03MNVUe2VJ5MKthjzgAXSsAXWSN/vNjEcYgbgiX9m98o6VKnUMOf3GpWLq/t8EEdSH6iUUmV+YN9WLFgVtvzlqmBQ3aL3XGoAy3WgMtYY8gVgiHI6kEwJNwsEF3PBoHZR+vfINWKZo+TfZjTOD6LuMxGxD7XnBLlsO3I/NLq3JBDtrzcXG4q7D87qUrhbFKzuH0x7SZl89oOf9M+twS6/MGxjPOOtYaS9dxldSlA/UAE38FMHHrj7koBsxLtPutQatYlhYLbRESJBQNBRDFq7ty5jgGg0qVLy7hx4+TChQuSPn166devn2zfvl127dolb731lsyZM0dKlCghw4YNS5BtT4zOXLgktYbNd708iuhGunYMsoFCWWee/zr/kS4EGshD9Xw7lasGNg57nU4ZCxiGVzpPJmlXtaCrIqxumMNY7jLWm9WSkRJoVJU1WKI7T41d1jYJxh3GHXNr4Kl5+XyyYkAj+fOlZj6/f/ueyiqYcpulI2oGS5Y928D73+Zd+R633qACLOh4IjjzfKuyPjWEABk9ZpaI7jSanXQte8bUKusHf+vV4Aa/ncoyeTNLtv/qrmD7McQM9WSQaYBgCIaojPmvRkmrivnkrfaVVaDq3U5VpEbRa51x/ZVC5oLd8BczMHDtSRK0Z5qVVvuuaI706vyA97FyYCOVaYRA1Jzet6jPZuy9N6nMEu2jLlXliYYlZHn/hmof23m2WRm5v25RFawyh0+axbyRDdLntlIq88epXk37aoVkyB3l5Psn66ltmXB/aPWbzGwna7bW1SGuoZ8bESSDlhXz+V0OmVrpUl3bjkY35lbvH/t76TMNXL2WvyxKvAcEznJlSiNv31MlqBpsCJIjsw3HZY4MqdX22GVL9ry1hBpihSBQlUJZVZaYCcfD7CduVp/9xqFNvb/Hsfxk45I+1wsETQNt40WXs4aVynPt3GJ3znXD4yIH0RpETpg5zYiIkqbwCwcQUZLUvHlz2bdvn+TOnVsFgF555RWZOHGiXLx4Ud35z5gxo/Tq1UtlAeXMee3OZseOHdXjiy++UIWmN23apJ4Xq7Cv9hw9K/VGLAzqebWK51AZRHa61Coik1bsDGl7Quk/6YyCdlULyYy1eyUamT92d4jRYTLdWaWA7D9xTn7eFrdoOYIL36371ycQAbWL55DRHatIOUvmT/8WN6qHnaI5Msif/54M4h2J/NjnFimR274IsbXDhowhFPU9e/GylM2XWTbuOxHwMzPv9JvQsTxx9qJMW7VbBVkWbz7oXd99dYqqf5uUzSNzN+6PU/MFbq+YT/JnSSul8mZSRcJRyFzTGQzmFNhtKueX1v8NcbmneiF5b+FW+efYWVVnBENM3l3wl8qAK5gtvXzVo64cOX1BZeWYGUjvdw5cD6dK4ayyasdR9XP+rGnV8BI9mx2M7FBJlmw5JHdXLaQ6qwgaRqJuSPMK+VQ2T77MadXnhkAVIDA1/qft6mezszy8bQXZe+ysdL/1Bhm/bLvzEC+XPVIMAZv1+15Z2PdW737DOUW/t9yZ0voMmcFn06y8b4ADGUh9mlxbBgGy9xb6FqfvWruId5YyswgvMoAW/HnA+54DQbCo23/HmVmHaveRs0EN5Rl2VwV57NPfVOAl0hAU+XXHUcdaPIDvBvYbAoIIvpXIlVHtc+wPKwT9hra+NrzPKeDhNBMYhqUFe6wiSI6g5NXZBK8dD1YIQuohVoAssXuqF5ZBM9ZJr1uv1twplz+Letgxg4I6aOpP5UJZZOpK+7+ZQaS7qxWSo2cuSq0ga1iZMqbxn52ks8kOnDwvHT9aITsPn5EGpXPL7ZWOyje/75XnmpeWXqNCfnkiouseA0FEMQqNy6ZNm6pA0MKFC+Xy5cvqd5kyZZLHHntMBYCyZ3duxLVr106WL18uo0aNkvr168uDDz7o+rXxWv/73/9kzJgxKpCULVs2adOmjbzwwgs+Qadwbd68WT777DM1KxoCW7ly5ZLHH39cKle2H+IRrGHfbZIPlvwd8vOd+l3Ny+cNKRCEvkKwHY7X/qvbAejIocZJk5FLJFJw59+a+QNuOgjVimaTzjWL+AQrIE/mNCq4ogNBCDQhEAHozN9eKbg6EW+2ryT3TVglrSvll4+XXe3868ANNL4xj/y4ab/Pc7L+V4jWhODThr0npGUF3446MmDWvdhEdaJPnb8kL8zaoDrjmAJ7xd9HfGqJYPjS9sOn4wwhQY2Vo6cvqHpEeC7qniDzYdHma0M0B/8XxEBh3w17j0vlQnGHSeD4sA6RAQRdzGPnrpsKypdr9sgjt1zLusG+nf90fTUT3i0lr3ayH2tY0qcjaAaBgoGOqg4EIRgTNyhYUD3M9xEpdjMvmQEe83t6T43C3p/N9x5KNgOgps7Ld5b3qSsV7nvD7FL31SkmQ7/dqDrEV7fnGjMQZAZ/Qp2i+7NHasvXv/0jnYx9E0irivlV7SV/QxVDhe9moGw6HYjGMY1sHafCyr/vPq6yaayfyZc96sja3cfifNfthPp5Og2lc5MViUCqGxj2iIxIBBzdwM2CMYu2yY7DZ3x+P7DFjT6ZPziWEKgOx7PNS8vOw6dVAW4nGEqI4aW4bp08d0kF9t65p7K8emd58Vw4K73C2gIiousbA0FEMQx1fwABoMyZM6sgSZ8+fVRgxo0dO3ao577zzjuuA0GnT5+W1q1by7Jly1QQqX379ioj6YEHHpCKFSvKvHnzpFy54AqgWh04cEDNfDZz5kz177Rp01TAK5IGz1wvE5eHlrXj5dC+333Ut5HtVijTv3eo7tvIxp3xaNCdGwxb6DJupTzXLO7UyLrwNaYP7zr+2m3oRX0byKQVO7z/jSyRcCAIgQK/2CYdCEKdlPvqFlVZM6gr9Mv2w9K2SgH533+fuV25HAzfQWfEblgNOiyAji+GIGnWu/0YvmQ3SxY6rrrzik4WatFAamOGMp2JhKwRFIV145ZSuWTJloMqY8T0xt0V5aU25dRMTiasW2cIRVK/pqVVMOrW0rlcb3t8Mj+TUIZbBjNNezAzsrmFQBoyUbyBIGODzPOEWeg81IAFOuK9LDM+uREfQaBAECxduPmACjAHguPQ6VisWiSbeiR1+D6vHdzEVTaYPvcg2+f1Hzar/17U91bZdvCUNLox8sNYEZz6ose1IZD+INieJmMK73GMY+vEhbMR3yYiousJA0FEMQ41gBD8QWHorFndFx2F+fPnq0YX6ge51blzZ/W80aNHS/fu3dXvkHk0e/ZsKVmypDRp0kTWrVvnNxvJHwSY7rrrLhXY+uWXX6R8efuU/nDUfPVH2X/ifNjrcaq7eer81UyUYCFoYQeFcdHZ7zP994DrcOoL1iiaXVbuOBLw+Z1rFpYpv+wKuBwK0+ogDArT+tsWBCtQYPiNHzarAsMYDlGhwLVjNYulJk8orJ3gAtnS+XQW0VlCNo8OBNllT6BD71Rbxe3rBuv1uyvJA5+ssh3S4gbqzfy261ic2cSwXdYgUHzCsKWnjeFNiUlSnbra3GwzLjXm3qrSa8oaNWwJxYFRfybQ1OXXC2TSWWdHi3XmUNBgFc2ZQT2IiCjpiY0rPxHZQgYOgi5DhgwJOggEefPmVXeaW7Ro4Wp5ZOYgSwfP00EgLX/+/NK1a1fZu3evyuIJxYIFC6Rx48Yqo2np0qXxEgTafeRMRIJA4NS/dDsF7pSHrk3v7K+4KQJEGIrhbpuSSff6N8SZZt1tX/iVOyuoacEDTQfvZqb6ZJY7vqiFUeeGq9kytW/IoQr9YkhAJE19uJYamje87bVppXXgB51m1OHpVrtI0AGf+ILMIBT3RRHfUCDYU7dETm/GEjkHLYPNkklouTKmUcN+VDaZEdSrXCirqoukiykjKwj1cojcKM7ADxHRdYEZQUQxbPz48VK0qG/hz2ADL6tWrVKFp90YOnSo+rdly5aSMmXc00/btm1l7NixMmXKFLVsMNv266+/qiFnWO+sWbNUsCnSzl28HHRR6ECzdaGOAgp2frD4b9dT+AKm/kXhYc1fXzVViquz8KDwr1kM2pwxyPRc86vDtD77dbfP8Ji1g2+TMxcuq07uir8Pqw4mZrxCrZ6Xvt2oCvoCsnZQp2boNxt8poa2dlIDCTTVOwr9RhoCTHg4efGO8IYtUtKBwtyz/9gn3eoEP/37uG7VVI0eu4Ld0YLgHjLZcG5wCsgSBQuzvKGOFwpHExFR0sVAEFGMOnjwoOTI4dzhdQNZPAi+uLFy5UpVGBqqVatmu0yNGlenIr5y5YpMmDBBZSq5cfbsWbn77rvl1KlT8sYbb0ipUpGfiQYFVss8Pyfi68W0zhhyZAaC3NQMQTHalEZ9GDdDW0a0qyRdaheVigWzyLp/jvsEkgLpUL2QKsKaNf21Wc80zG6DGaX0rERwR6X8aoYq67Ca9zvfJF+t2SN9bnMeBvRjn/qqTlLFgsFnqRFFSqgzPkHJPJlk0oPXMvaS4rAfIjv4PuCcT0RESRtzwYlilA4C/fbbb/L999/H+fvixYvl+eefl23bfKchDtXcuXO9PxcrZt+IzJIli+TJk8f7+m69+uqrqnA1ZgXr2bOnxAfMlORPvixp1TCMUNgV6lzc71bJlj6VqrmDGawgZ8Zrw5GQlWPWqXEqTvvdE/V8isSiwCkCTZiVym0nEa8TaLiaGQTS7DrQLSrkk4+7Vfdb16dE7oxqGmCihJZU6wMRERER+cOMIKIYNmDAAHnttdfUz59++ql06NDB+zdMCY9C0sj4adasmYwYMUKSuyns4mDt2rXen4sUcR5qgSFd+/fvlzVr1rha7+HDh1UWENxzzz2yZMkS+eKLL9RQMfwNs4XhPaAgdoYModc26DnFeXuQ5YIAR+lBcQNqGqbAnbrSvogyAjKYxv3TX3bJu51uUr8rkiODrB50mxrS8XKb8nLszEXJliG1LN5yUH75+7Ca3tuEaXOtbiqcNeDwKjfK58/MDjERERER0XWCgSCiGIWizcOHD/f+94ULF+IsU716dVm0aJGULVtW/v33X5k8eXLIr4eMHS1nzrjTY2sIPsHJkyfVkK906fwXXZ06daqcO3dO/YwAUJo0aeT++++XZ599VtUweuaZZ2Tw4MFqOUxNX6CA89TX58+fVw/txIkT6t81DrNlNSidSxUWblYujxrO5m/G6GeblfIJBGF5091VC6qH9W9Xrlxda5Z0KdXv65XIoR56HqBf+jdUw9bSpEwWZ512rxOMDKlTyOkLl1Ux4XDWQwlLHZseDz9DSrJ4DFOweKwQEfnHQBBRjHrnnXdUhs/NN98st9xyi3Tq1Ml2OQRtEExBYAWZNajFEwodVAF/mTlmEeljx44FDATpIWfIWPn55599CkyXKFFCzRyG94f6RO3bt1fTyztltwwbNsy2LtH8Dbvtl29eWP174MABn6CN9tjNBWTviQvSqmwOOXfiqNS/Iass3nZMyufN4H1OJCBP68CBa/tXuzFXmrBeZ/K9N8qKHSekRdnMEd1ein6H6Pjx46ojHU5WH1FC4TFMwcLNJCIicpbMg6sqEcUcTBc/atQoue+++wIuu3z5cqlbt64KqCBDKBQlS5aUrVu3qp8vX77s2JivXbu2rFixQv28b9++gLN/YZjZrl271BAwDCmz07lzZzX0Db799ls1a5nbjKBChQpJod7TJXma9Ndmjrq9rNyQK4PKlDEN+/5P+Wjpdu9/Y7iXzvJR6zt3Uc2w1aRsnnidfnz7odMy/88D0rlGYUmXmsViYx060SgOjxpa7ERTUsRjmIKF63e2bNlUADFz5vCHSBMRXW+YEUQUoxDwaNeunatldQbN6tWrQ369TJky+QxDS5s2re1yepiX9TlOdPDHX8CoW7du3kDQ7NmzHQNBGFaGhz+9Gtwg99W1L3b9TLMyaur0Th/94t1vZqcla/o00qlm8FNRB+uG3JnUg0jTxyI70ZRU8RimYPA4ISLyj2dJohhVuHBhdafMDWTRQIoUKcJ6PTcp2yjwrGc1c1PcWQep/A0hQzaTXs6sVRSKHreWcPwbZuOqc4Nz/SMiIiIiIqKExkAQUYzCTGDvvfdewOWWLl0qb775pgqkVKtWLeTXq1SpkvfnPXv22C6Dkaq6Fk3lypVdrVdPN3/06FHHZRBQQmAp3LuEI+6qKBltpkl3kpwzbRERERERUSLDQBBRjOrbt68qGI1C0HaZQQjIDBw4UJo0aeKtm9O7d++QX69p06ben1G42Q4CRPq1Gjdu7Gq9N910kzfTB7OMOdFDvm644QYJVfvqhVwt17V2ESmVJ6O0rJAv5NciIiIiIiKKD6wRRBSjUAR53LhxqpAyMoNq1KghBQsWlIsXL6qizn/88Ycq6qzryffq1UtatWoV8uuhCDRm8cK6UXzabpayVatWeYegOc1iZtWmTRv5+uuvVd0hZC8hcGWF96GHnCETKr4NbV0+3l+DiIiIiIgoFMwIIophHTp0kFmzZkmWLFlk8eLFMmXKFPn8889lzZo1cunSJRUEQu2d4cOHq+yhcGBo2aBBg9TPM2bMULPAWM2cOVP926VLF5+aQv7cc889KsAEY8aMsV0GASYUocZU8qEGglpU8D97GRERERERUVLAQBBRjGvRooVs375dJk+eLA899JAKlOCBmbYQWMHU7Bg+Fgldu3ZV68YQsKlTp/r8bcuWLTJ9+nTJnz+/jBgxIk4gB9PEIziks4a01KlTy4cffiipUqVSQa25c+f6/B0BpxdffFENDfv444+9RaODVblQ1pCeR0RERERElJhwaBgRqSAJhmI5DcfCFO26KHM4EIRBwKl58+bSs2dPSZ8+vTRs2FBWrFghPXr0kFy5cqnp3fGvaeLEiSogBZMmTZLq1av7/L1BgwZqGQSakCH07rvvqtc4ePCgDBgwQH766Sf57LPPpGbNmiFv+/4TV2sXERERERERJWXMCCKigKZNmyYvvfRSRNaF2bsWLVqksoz69++vAkwICiEItW7dOqlQoUKc5yDAg2wgPJCpZAcBIASUEFh68sknJV++fCr7KHfu3PL7779L69atw9ru++sWDev5REREREREiUEyj64ES0TkADN5IYAzatQoNXwsVpw4cULVTyrUe7r89XpbSZMyRUJvElHQMDwSswAiKJo8Oe//UNLDY5hCvX5jVtTMmTMn9OYQESU6HBpGFMN+/PFHefPNN9WwKwR77Ao46xm3zpw5o7KCYikQZGIQiIiIiIiIrgcMBBHFKEy1jjo6CP4wMZCIiIiIiCg2MBBEFKNGjx6tsn0KFSokVatWlUyZMsnXX38td911l89yFy5cUNO6o46PU30eIiIiIiIiShoYCCKKUSisfP/99/tMqY7ZwZ577jkpXbq0z7L33XefFC1aVMqVK5dAW0tERERERESRwIp7RDEKhTcHDx7sDQIBMn4++uijOMtidq9+/frJpk2boryVREREREREFEkMBBHFqNSpU0vWrFl9fte2bVuZMWOGHDt2zOf3yBBKly6dDBgwQGJR/VI5E3oTiIiIiIiIIoKBIKIYheDOhAkTfH6XJk0a6dChg/Tu3dvn95s3b5YjR47IwoULJRblyZw2oTeBiIiIiIgoIhgIIopRKAr99NNPS40aNaRZs2aqIDT06dNH/XzPPffI7NmzZdy4cervgKygWLTtwKmE3gQiIiIiIqKIYLFoohiFrJ/PPvtMfv31V/XfJ06ckNatW0uOHDnUjGJdu3aVzz//3Ls8agm1adNGYtHqXb5D5YiIiIiIiJIqBoKIYlTatGll8eLF8vLLL8uGDRvU9PDavffeK4cOHVJFos+fP69+hyDR66+/noBbTEREREREROFiIIgohmXOnFlGjBjhmDGEWcT++usvKVy4sOTNmzfq20dERERERESRxRpBRDFq1apVcuedd6oaQE6yZcumaggxCERERERERHR9YCCIKEZ16tRJFYV+7LHHEnpTiIiIiIiIKEoYCCKKUSgOHcsFoImIiIiIiGIRA0FEMapXr17qX7cFoFE0umHDhhKLHqlXLKE3gYiIiIiIKCIYCCKKUYMHD1YFoYcPHy4ejyfg8qtXr1azjMWiQtnTJfQmEBERERERRQRnDSOKURMnTpRKlSrJvHnzpFq1aipDKGXKuKeEK1euyJ49e+TDDz+M2GtfvnxZ/ve//8mYMWNk06ZNqig1hqi98MILkjNnzrDWjQynZ555xvZv9evXl0WLFgW9zhTJkoW1TURERERERIkFA0FEMWrgwIGyd+9e738//PDDfpdH1hBqCoXr9OnT0rp1a1m2bJmMGjVK2rdvLzt37pQHHnhAKlasqAJT5cqVC2ndGL721ltvOf790UcfDWm9kXjfREREREREiQEDQUQx6sEHH5ShQ4dG/XU7d+4s8+fPl9GjR0v37t3V77Jnzy6zZ8+WkiVLSpMmTWTdunXqd8EaP368KoJdunTpOH/LmDGjtG3bNqRtvjFfppCeR0RERERElNgk87gpDkJE1x0M9ypWrJgaStWsWTNJkyaNJE9uXzbs7NmzMnLkSPn444/VsK5QTZs2TTp27Ch58+aV3bt3xxmK1qNHDxk7dqx06dJFDV0LxqVLl6RUqVLSs2dP6du3r0QCgkpZsmSRo0ePStasWSOyTqJow/DOAwcOSO7cuR2/40SJGY9hCvX6ffz4ccmcOXNCbw4RUaLDQBBRDMMQrQkTJrjKvtm3b58UKFBANchDVbZsWVUTCNlICCpZYVgYMoLQ0N+2bZsULVrU9bonTZokTz/9tGzfvl0yZMggkcBAEF0P2ImmpI7HMAWLgSAiIv94NSWKYSNGjFCZQG6ybZC9g0BNqFauXKmCQIDi1HZq1KjhbfQjQOUW4tmY/QwZQUuWLFGBGyIiIiIiIoqLgSCiGIZaOm6yZ1DguV+/ftKoUaOQX2vu3LnenzEkzQ7u3uXJk0f9HMxU9TNnzpSNGzfKTz/9JC1atFDruOOOO1RBaiIiIiIiIrqGgSAi8uvMmTOydOlSmT59uhw5ciTk9axdu9b7c5EiRRyXQ/0gWLNmjet1Dxs2zOe/L168KN98843Uq1dPunbtqmocEREREREREWcNI4pZKVKkCPo5GK6FOjyh2LFjh/fnnDlzOi6XPn169e/JkydVACddunQBh4WhCDWW37Vrl6xatUqmTp0qf/31l7d20ObNm2XBggUBs58w/TweZo0BPVQtnNpIRAkJxy6+JzyGKaniMUzB4rFCROQfi0UTxahQCm7ecMMN3gBLsFC/Rz8XWUZOAZ5bbrlFZSDB3r17JV++fEG/FmoaffDBBzJo0CA5duyY+l23bt3kk08+8fu8F198UYYMGRLn93/++acatkaUVDtEKJiKY5iFdikp4jFMwcLNIbQ7WCyaiMgeA0FEMQqN6ZIlS6paOhkzZnRcDkEZZOlUrVpV/fcLL7wQ0uvhtbZu3ap+xhT0To352rVry4oVK7wzlemhYqEOR0NdIwxpS5YsmWzYsEFuvPHGoDKCChUqJIcPH+asYZSkO9EHDx6UXLlysRNNSRKPYQoWrt/ZsmVjIIiIyAGHhhHFsC+//FLKly/vd5lTp06pYErLli0dZ/tyI1OmTN6fL1y4IGnTprVd7ty5c7bPCUXlypVVIen69eurjgTqBvkLBGEGNbtZ1NDxYOeDkjIEQnkcU1LGY5iCweOEiMg/niWJYlTr1q1V2nQgyBYaOHCgCgRhqFaoChcu7JOy7QTZN5AjRw5XM5oFcvPNN6v3Ctu3bw97fUREREREREkZA0FEMerrr7+W1KlTu1oWU7JjeBVq7oSqUqVK3p/37NljuwxGqh44cMCbzRMpOhDkbwgcERERERFRLGAgiIhcjbVHXZ/Zs2eHvI6mTZt6f960aZPtMggQ6Ro9jRs3lkjRBacrVqwYsXUSERERERElRQwEEZFfGMalp4xHbZ9QoQh0iRIl1M/Lly+3XQZTv+up7Tt16iSRgqLTmG2mTZs2EVsnERERERFRUsRi0UQxqnjx4gGXQeBn//79qtAyCnWGk6WD52No2X333SczZsyQt99+O04xRxR2hi5duvjUFArXp59+KsOGDQu7+DQREREREVFSx+njiWIUgjAIzrg9BaCw9IIFCyR//vwhvyZeC/WG5syZI5MnT5bOnTt7/7ZlyxZVRyh79uxq2ndME2xmCrVr1049HzOdVa9e3fu3zZs3y/fffy9NmjSRsmXLxnnNd999VzZu3Cjvv/9+SEPikEl09OhRTh9PSRYCuai9lTt3bs6kQ0kSj2EK9frN6eOJiOwxI4gohiFD5rbbbrMtoowgEaZSR2DmpptukjvuuENSpUoV1uthnQgANW/eXHr27Cnp06eXhg0byooVK6RHjx4q+IM6RGYQCCZOnCi7du1SP0+aNMknEDR48GCZPn26pEyZUrp3764eRYsWla1bt8r48eNV5lMoQSAiIiIiIqLrETOCiGIU7qrOmzdPGjVqFPXXPnPmjIwcOVIFdXbs2CEFChSQjh07Sr9+/dQdPCudEQRfffWVVK1a1afA9DPPPCOLFi1SU8/j+SVLllSBq65du3oLRYeCGUF0PWA2BSV1PIYpWMwIIiLyj4EgohiVLVs2VUQ5bdq0Cb0piRYDQXQ9YCeakjoewxQsBoKIiPzj0DCiGIXgBhEREREREcUW3lYhimEXL15UQ7SeffZZOXXqlM/fFi9eLHfddZdMmDBB3Y0lIiIiIiKipI8ZQUQx6tKlS9K0aVMV8AEUVX700Ue9f69fv76qxfPwww/L2LFj5ZtvvlFp+URERERERJR0MSOIKEaNHj1aFVhGmTA8ihUrFmcZzCY2ZcoUOXv2rAoanT9/PkG2lYiIiIiIiCKDgSCiGIUp2ZHhg+nXf/zxR2nSpIntcijM2bdvX/n999/l7bffjvp2EhERERERUeRwaBhRjNq8ebMKANWpUyfgsuXLl1f/Yrp3TNVORERERERESRMzgohiVKpUqaRKlSqulj1y5Ij6d+vWrfG8VURERERERBSfGAgiilGlSpWSv//+29WymDkMsmTJEs9bRURERERERPGJgSCiGNWuXTt57rnnAk4NP2LECJk6daokS5ZMGjZsGLXtIyIiIiIioshjIIgoRj3++OOyfv16ufnmm2XevHly8eJF799OnjwpX375pfpb//79vUPJBg0alIBbTEREREREROFisWiiGJU+fXqZNWuWNGrUSJo1a6YCPbly5VIBoUOHDqkp5QH/pkiRQsaPHy9ly5ZN6M0mIiIiIiKiMDAjiCiGVahQQdasWSN33HGHCgD9888/cuDAATVcDAEgPKpXry6LFy+WTp06JfTmEhERERERUZiYEUQU4woWLChff/21CgIh4IN/EQDKkyeP1KpVS0qXLp3Qm0hEREREREQRwkAQESkFChRg1g8REREREdF1jkPDiGLcpUuX5MSJE3F+v2rVKvnxxx+9tYKIiIiIiIgo6WMgiCiGffvtt5IvXz7JnTu3LFu2LE79oLVr10rFihXlm2++SbBtJCIiIiIioshhIIgoRv3xxx/Srl07OXz4sCoUjankTWnTppW+ffvKuHHj5O6775b33nsvYq99+fJlNQsZClFnzJhRChUqpKazx2xlkYTXqVevniRLlkwWLVoU0XUTERERERElRQwEEcWoV199VS5cuCCpU6eWunXrqqCQnRo1akiPHj3kqaeektWrV4f9uqdPn5amTZtKz5495cEHH5Rdu3apaeyRkYTsow0bNkikvPLKK3EynYiIiIiIiGIZA0FEMQoZMgjGHD9+XJYsWSI5c+Z0XLZVq1aqltDw4cPDft3OnTvL/Pnz5Y033pDu3btL9uzZpUqVKjJ79my1LU2aNJEjR46E/TorVqyQl156Kez1EBERERERXU8YCCKKUUePHpUXX3xR0qRJE3BZBGsg3OFV06ZNk5kzZ0revHlVEMiUP39+6dq1q+zdu1d69+4d1uucPHlSBZxatGgR1nqIiIiIiIiuNwwEEcWoXLlyScqUKV0t+8svv6h/z5w5E9ZrDh06VP3bsmVL29du27at+nfKlCmyY8eOkF/nsccek5IlS4YdUCIiIiIiIrreMBBEFKNQRBnZOYEcOHBAXn75ZVVwuXTp0iG/3sqVK2XTpk3q52rVqjnWI4IrV67IhAkTQnqdzz77TObMmSOffPKJ2mYiIiIiIiK6hoEgohiFWbqefPJJ+e677xyXmTdvntSqVUsN14L77rsv5NebO3eu9+dixYrZLpMlSxbJkyeP+nnx4sVBvwYKT6OwNYJIGH5GREREREREvtyNCyGi606dOnXkoYcekttvv10Fe1CkuWDBgmoq+a1bt6rAjTmDF5bv1atXyK+3du1a789FihRxXA4BnP3798uaNWuCWj+yiLp06SL33nsvawMRERERERE5YCCIKIa9/vrrqlYP/sUsW1Yej0f927x5c/n0008lRYoUIb+WWfPH3wxl6dOn9xZ8Pnv2rKRLl87V+ocNG6YKYI8YMSLkbTx//rx6aCdOnPAGmfAgSopw7OK7zGOYkioewxQsHitERP4xEEQUw1BDB1PCd+zYUd599101HOuff/5RDW4M0UKmEGbyQiAoXDqoAhkyZHBcziwifezYMVeBoFWrVslrr70my5cvl7Rp04a8jQgmDRkyJM7vDx48KBcuXAh5vUQJ3SE6fvy4+l4nT84R4ZT08BimYOFmEhEROWMgiIikUqVK8tFHH/ld5ty5c/LAAw+ozKBQ6Owi8DdlPYamaW6KPZ86dUo6deqkgjjlypWTcPTv31/69OnjE7wqVKiQmmEta9asYa2bKCE70fgu4ThmJ5qSIh7DFKxwbgoREcUCBoKIyHWNH8zIFWogKFOmTN6fkV3j1EhDwMnuOU6eeOIJufHGG8OqX2QGqOyCVOh4sPNBSRk60TyOKSnjMUzB4HFCROQfA0FEFNDhw4flqaeeCmsdhQsXlt9++82bsu0UCMJrQY4cOfwOIYMvvvhCvvrqK1m0aJHs2bPHdkiX+bNeBkWxiYiIiIiIYhEDQUTkd9jV2LFjVf0dBGjcDNXyN/xs5syZ6mcEZJDibzd87MCBA+rnypUrB1zne++9p+pGVKlSJeCy7du393kdIiIiIiKiWMS8SSKKY+fOnfL000+rzJlnn31Wjhw5EvY6mzZt6v1506ZNtssgQKRn7WrcuHHAdTKgQ0REREREFBwGgojI6+eff5a7775bSpQoIaNGjVLFkhFsiUTApXbt2mq9gNm9nGb/AkxTjwLQgWBImN4+u8fChQu9y+LnSL0XIiIiIiKipIqBIKIYd/nyZZk6darUrFlT6tWrp2ru4HcImGTMmFEeeugh9Tv8Gw4MKxs0aJD6ecaMGWoWGCs9dKxLly6qphARERERERFFFgNBRDHq2LFjqvZP0aJF5d5775Vff/3VmzGD373xxhtqqNaHH34obdq0kZdeeinsbJquXbtKs2bN1HoRfDJt2bJFpk+fLvnz55cRI0bEyRQqUqSICg7prCEiIiIiIiIKHotFE8UYBFww7GvSpEly5swZ9Tsd4Klfv76sXr1aDd3KkyePz/Ny584t33zzTdhZQZMnT5bmzZtLz549JX369NKwYUNZsWKF9OjRQxWQnj17dpxC0hMnTpRdu3apn7Hd1atXD2s7iIiIiIiIYhUDQUQxYv78+TJy5EiZM2eOT62cVKlSSYcOHaRPnz5qpq58+fLZzg6G37Vs2TLs7cC08Kjtg23p37+/7NixQwoUKKBqAvXr10+yZMlim0k0a9Ys9XO3bt3C3gYiIiIiIqJYlczDyqlE17Vx48bJO++8I+vXr1f/rb/y2bNnl0cffVQee+wxFfzR8PPvv/+uMoBiHYplIzB19OhRyZo1a0JvDlFIUI/rwIED6judPDlHhFPSw2OYQr1+Hz9+XDJnzpzQm0NElOgwI4joOrdmzRrZtm2bCgAhq+eGG26Qvn37qiybdOnSJfTmERERERERURTxtgrRde69995T9XWGDBmi7qbu27dPZQft378/oTeNiIiIiIiIooyBIKIYgGFgzz//vOzcuVMVil6wYIGULFlS2rdvLytXrkzozSMiIiIiIqIoYSCIKIakTp1aHnroIdmwYYPMnDlTDh8+LLVq1ZJ69eqpGcFYMoyIiIiIiOj6xkAQUYxq0aKFmkkM08UXLlxY7rrrLilbtqycPHlSLl++bPscFJYmIiIiIiKipIuBIKIYV6VKFZkyZYoqKN2qVSs1nfxNN90kr776qpotS9u9e7eMGTMmQbeViIiIiIiIwsNAEBEphQoVktdff10FfPr16ycffvih+h2mmJ81a5b0798/oTeRiIiIiIiIwpTMw6IgRGQDw8OmT58ur732mqxbt87n97HixIkTkiVLFpUZlTVr1oTeHKKQXLlyRQ4cOKBmDUyenPd/KOnhMUyhXr+PHz8umTNnTujNISJKdHg1JSJbKVKkkI4dO8ratWvlo48+kgwZMiT0JhEREREREVGYGAgiooAeeOABFQwiIiIiIiKipI2BICJy5fbbb5e6desm9GYQERERERFRGBgIIiJX0qdPL0uWLEnozSAiIiIiIqIwMBBERERERERERBQjGAgiIiIiIiIiIooRDAQREREREREREcUIBoKIiIiIiIiIiGIEA0FEFHWXL1+W8ePHS/Xq1SVjxoxSqFAhefzxx+XQoUMhr/PUqVPy/PPPS5kyZSRNmjSSNWtWadSokXz77bcR3XYiIiIiIqKkjIEgIoqq06dPS9OmTaVnz57y4IMPyq5du2TWrFmybNkyqVixomzYsCHodR49elTq1KkjL7/8smzevFkuXLggx48flwULFqhp70eOHBkv74WIiIiIiCipYSCIiKKqc+fOMn/+fHnjjTeke/fukj17dqlSpYrMnj1bBW+aNGkiR44cCWqd7dq1k5IlS8qKFSvUOjZu3Cg9evSQZMmSqb/3799f/v7773h6R0REREREREkHA0FEFDXTpk2TmTNnSt68eVUQyJQ/f37p2rWr7N27V3r37u16nV9++aXUrFnT+2/mzJnlxhtvlPfff18ee+wxtcz58+dl7ty5EX8/RERERERESQ0DQUQUNUOHDlX/tmzZUlKmTBnn723btlX/TpkyRXbs2OFqnaVKlZJXXnnF9m86EAQejyfErSYiIiIiIrp+MBBERFGxcuVK2bRpk/q5WrVqtsvUqFFD/XvlyhWZMGGCq/VWqFDBOwTMCkWoAUGn5s2bh7jlRERERERE1w8GgogoKsyhWcWKFbNdJkuWLJInTx718+LFi8N+TR14evbZZ6Vo0aJhr4+IiIiIiCipYyCIiKJi7dq13p+LFCniuBzqB8GaNWvCfs0333xTOnXq5B2SRkREREREFOviFukgIooHZs2fnDlzOi6XPn169e/Jkyfl7Nmzki5duqBf69KlSzJgwAD57rvv5IcffpDkyd3FvFFUGg/txIkT3qFqeBAlRTh2USOLxzAlVTyGKVg8VoiI/GMgiIiiQgdVIEOGDI7LmUWkjx07FlQg6K+//pJZs2bJ2LFjZevWrep3mEnskUcekTFjxgQMCA0bNkyGDBkS5/cHDx6UCxcuuN4OosTWITp+/LjqSLsNihIlJjyGKVi4mURERM4YCCKiqDBn7UqTJo3jchcvXvT+7FQE2l9noXjx4tKhQweZPHmy7Ny5U/3+ww8/lLRp08rbb7/t9/n9+/eXPn36+ASvUHA6V65ckjVr1qC2hSixwPcC3yUcx+xEU1LEY5iChWs+ERE5S+bhnMpEFAU33XST/Pbbb+pnDPlyaqRVqVLFW0/o1KlTfrOH/EFA6bXXXpPBgwerIBQyjbZs2eJYqNoOAkEoYH306FEGgihJd6IPHDgguXPnZieakiQewxQsff1GJlnmzJkTenOIiBIdXk2JKCoKFy7sKmX78OHD6t8cOXKEHASCVKlSyaBBg7wZPqgbtHTp0pDXR0REREREdD1gIIiIoqJSpUren/fs2WO7DDJ3cNcXKleuHJHX7devn6RIkUL9vHfv3oisk4iIiIiIKKliIIiIoqJp06benzdt2mS7DAJEetauxo0bR+R18+TJI2XKlPH+TEREREREFMsYCCKiqKhdu7aUKFFC/bx8+XLbZVatWqX+RQZPp06dIvbaGTNmVIVGGzZsGLF1EhERERERJUUMBBFRVCAQg5o9MGPGDFX802rmzJnq3y5duvjUFAoH6hGtW7dOBZaKFCkSkXUSERERERElVQwEEVHUdO3aVZo1a6aGgE2dOtXnb5jRa/r06ZI/f34ZMWJEnEwhBHEQHNJZQ9q2bdtk9uzZjgWon3nmGfW80aNHx8M7IiIiIiIiSloYCCKiqGYFTZ48WapXry49e/aUr7/+Wk3t+sMPP6gAUa5cuWTOnDnqX9PEiRNl165dsnv3bpk0aZLP3+rWrSutWrVSwR5MFY/6Q6dPn5b169dLx44d1fN++uknyZYtW5TfLRERERERUeKTzINpeoiIoujMmTMycuRIFdTZsWOHFChQQAVtMMNXlixZ4iyPLKB27dqpn7/66iupWrWq92/ILBo+fLhs3bpVzp07J1mzZlVZRbfccota58033xzydp44cUJtz9GjR9V6iZIiDMPEbHy5c+eW5Ml5/4eSHh7DFOr1GzebMmfOnNCbQ0SU6DAQRETkgIEguh6wE01JHY9hChYDQURE/vFqSkREREREREQUIxgIIiIiIiIiIiKKEQwEERERERERERHFCAaCiIiIiIiIiIhiBANBREREREREREQxgoEgIiIiIiIiIqIYwUAQEREREREREVGMYCCIiIiIiIiIiChGMBBERERERERERBQjGAgiIiIiIiIiIooRDAQREREREREREcUIBoKIiIiIiIiIiGIEA0FERERERERERDGCgSAiIiIiIiIiohjBQBARERERERERUYxgIIiIiIiIiIiIKEYwEEREUXf58mUZP368VK9eXTJmzCiFChWSxx9/XA4dOhTyOv/991/p06ePlCxZUtKkSSNZs2aV+vXryyeffCJXrlyJ6PYTERERERElVQwEEVFUnT59Wpo2bSo9e/aUBx98UHbt2iWzZs2SZcuWScWKFWXDhg1Br3PNmjVSqVIlGTlypGzdulUuXLggx48flyVLlsj9998vzZo1kzNnzsTL+yEiIiIiIkpKGAgioqjq3LmzzJ8/X9544w3p3r27ZM+eXapUqSKzZ89WwZsmTZrIkSNHggostW7dWlKnTi1vvfWWCiitXLlSXnnlFcmcObNaZt68efLAAw/E47siIiIiIiJKGhgIIqKomTZtmsycOVPy5s2rgkCm/PnzS9euXeX/7d0HdFRV/sDxm4QaIIQSQCBEMMAiSBFCERUVBFxcaQurYIIKCqIossKKoJRFcJGVVQ6uWMClg1JC0YAisIggTaSFKi0ChpaEFlru//zuf987M5OZSSEMTOb7OWd8L/PefW3uSO4v9/7usWPHVP/+/bN9zEmTJqnKlSurXbt2qddee001b97cDDl788031bp161SpUqXMfnPmzFHbtm3L83sCAAAAAH9CIAiAz4wcOdIs27VrpwoUKJBpe6dOncxyxowZ6tChQ9k65pIlS9S8efNUiRIlMm27++677XOK1atX38DVAwAAAID/IxAEwCdkuFZiYqJZb9Sokdt9GjdubJaS3HnKlCnZOq70HpLeRJ507NjRXr98+XIOrxoAAAAA8hcCQQB8Yvny5fZ61apV3e5TsmRJVb58+Rz13nniiSe8bo+IiLDX77rrrmxeLQAAAADkTwSCAPjE1q1b7fWoqCiP+0n+IGsmsLwgOYdEaGioevTRR/PkmAAAAADgrzIn6QCAm8Ax50/ZsmU97icBG3Hu3Dl16dIlVbRo0Rs67/fff2+WMo188eLFve4rQ8cch4+lpaXZQ9XkBfgjqbtaa+ow/BZ1GDlFXQEA7wgEAfAJK6giihUr5nE/xyTSKSkpNxwI+uyzz8zMYW+99VaW+44ZM0aNGDEi0/snT55UV65cuaHrAG5lgyg1NdU0pIOD6QgM/0MdRk7JH5MAAJ4RCALgE/ILvKVw4cIe97t69aq9HhQUdEPnXLp0qZlCftq0aXbuIW8GDx6sBgwY4BS8ioyMNHmGwsPDb+hagFvZiJbvktRjGtHwR9Rh5FSRIkVu9SUAwG2NQBAAn3Cc3l1613j6JS09Pd1tmdz8NbBv377q+eefV08//XS2ykiAyl2QShoeND7gz6QRTT2GP6MOIyeoJwDgHf+XBOATVapUyVaX7dOnT5tlmTJlvA4hy6r30TPPPKOqV6+uJk6cmKtjAAAAAEB+RCAIgE/Uq1fPXk9KSvIYwElOTjbr9evXz/W5Ro4cqY4ePaoWLFigChYsmOvjAAAAAEB+QyAIgE+0adPGXk9MTHS7jwSIrFm7WrVqlavzfPzxx2revHkqISHhhoaWAQAAAEB+RCAIgE80a9ZMRUdHm3VJ4OzOxo0bzTIkJER169Ytx+eYOnWq+vDDD9W3336rSpcufYNXDAAAAAD5D4EgAD5L9Dl06FCzvnDhQjMLjKv4+HizjI2NdcoplB1TpkxRo0aNMkEgTzOEHT9+XE2YMCFX1w8AAAAA+QGBIAA+ExcXp9q2bWuGgM2aNctp2969e9XcuXNVxYoV1dixYzP1FIqKijLBIavXkKOPPvpIDRo0yCSGlkTUu3fvtl87d+5UP/30k/rXv/6lmjZtqmrXrn3T7xMAAAAAbldMHw/Ap72Cpk+frh577DEztXtoaKh65JFH1Pr169WLL76oIiIi1NKlS83SdcjXkSNHzPq0adNUTEyMvW348OFqxIgRZr1169Zezy+BpIcffvim3BsAAAAA+AMCQQB8SqaFX7VqlRo/frwaPHiwOnTokKpUqZLJCTRw4EBVsmRJtz2JFi1aZNZ79Ohhvy/HsIJA2SHHkWAUAAAAAASqIC3zNQMAMklLSzOBqbNnz6rw8PBbfTlArkg+ruTkZFWuXDkVHMyIcPgf6jBy++93amqqCgsLu9WXAwC3Hf41BQAAAAAACBAEggAAAAAAAAIEgSAAAAAAAIAAQSAIAAAAAAAgQBAIAgAAAAAACBAEggAAAAAAAAIEgSAAAAAAAIAAQSAIAAAAAAAgQBAIAgAAAAAACBAEggAAAAAAAAIEgSAAAAAAAIAAQSAIAAAAAAAgQBAIAgAAAAAACBAEggAAAAAAAAIEgSAAAAAAAIAAQSAIAAAAAAAgQBAIAuBz169fV5MnT1YxMTGqePHiKjIyUvXr10+dOnUqT46/efNm9dRTT6lWrVrlyfEAAAAAIL8gEATApy5cuKDatGmj+vbtq3r27KmOHDmiFi1apH744QdVt25dtXPnzlwfOyEhQbVs2VI1atRIzZ49W127di1Prx0AAAAA/B2BIAA+1b17d7VixQo1btw41adPH1W6dGnVoEEDtXTpUpWamqpat26tzpw5k+Pjzps3T504ccIEmQAAAAAA7hEIAuAz0ksnPj5eVahQwQSBHFWsWFHFxcWpY8eOqf79++f42J07d1bPPPOMGjRokKpZs2YeXjUAAAAA5B8EggD4zMiRI82yXbt2qkCBApm2d+rUySxnzJihDh06lOvzSC8jAAAAAEBmBIIA+MSGDRtUYmKiWZccPu40btzYLDMyMtSUKVNyfa6CBQvmuiwAAAAA5GcEggD4xPLly+31qlWrut2nZMmSqnz58mZ99erVuT5XUFBQrssCAAAAQH5GIAiAT2zdutVej4qK8rif5A8SW7Zs8cl1AQAAAEAgyZykAwBuAsecP2XLlvW4X2hoqFmeO3dOXbp0SRUtWlT5yuXLl83LkpaWZg9Vkxfgj6Tuaq2pw/Bb1GHkFHUFALwjEATAJ6ygiihWrJjH/RyTSKekpPg0EDRmzBg1YsSITO+fPHlSXblyxWfXAeR1gyg1NdU0pIOD6QgM/0MdRk7JH5MAAJ4RCALgE/ILvKVw4cIe97t69eoty/UzePBgNWDAAKfgVWRkpIqIiFDh4eE+vRYgLxvR8l2SekwjGv6IOoycKlKkyK2+BAC4rREIAuATJUqUsNeld42nX9LS09PdlvEFCVC5C1JJw4PGB/yZNKKpx/Bn1GHkBPUEALzj/5IAfKJKlSrZ6rJ9+vRpsyxTpozXIWQAAAAAgJwjEATAJ+rVq2evJyUleRw+lpycbNbr16/vs2sDAAAAgEBBIAiAT7Rp08ZeT0xMdLuPBIisWbtatWrls2sDAAAAgEBBIAiATzRr1kxFR0eb9XXr1rndZ+PGjWYZEhKiunXr5tPrAwAAAIBAQCAIgM8SfQ4dOtSsL1y40MwC4yo+Pt4sY2NjnXIK5XaGMseZygAAAAAABIIA+FBcXJxq27atGQI2a9Ysp2179+5Vc+fOVRUrVlRjx47N1FMoKirKBIesXkPeXLhwwSwvXryYx3cAAAAAAP6NQBAAn/YKmj59uoqJiVF9+/ZVCxYsUKmpqWrZsmUmQBQREaESEhLM0tHUqVPVkSNH1NGjR9W0adPcHlt6/0gAaPny5WrHjh3mve3bt5vjpaWl0TsIAAAAAAgEAfA1mRZ+1apVatCgQWrw4MGqfPnyJigkOYEkcHPPPfe47UkkvYHk1aNHD7fHnTNnjipevLhJSm0lnJblY489pkqWLKmWLl160+8NAAAAAG53QZo/kwOAW9KTSIJIZ8+eVeHh4bf6coBckXxcycnJqly5cio4mL//wP9Qh5Hbf7+l13FYWNitvhwAuO3wrykAAAAAAECAIBAEAAAAAAAQIAgEAQAAAAAABAgCQQAAAAAAAAGCQBAAAAAAAECAIBAEAAAAAAAQIAgEAQAAAAAABAgCQQAAAAAAAAGCQBAAAAAAAECAIBAEAAAAAAAQIAgEAQAAAAAABAgCQQAAAAAAAAGCQBAAAAAAAECAIBAEAAAAAAAQIAgEAQAAAAAABAgCQQAAAAAAAAGCQBAAn7t+/bqaPHmyiomJUcWLF1eRkZGqX79+6tSpUzd03JSUFPX222+rmjVrqtDQUFW7dm01btw4de3atTy7dgAAAADwZwSCAPjUhQsXVJs2bVTfvn1Vz5491ZEjR9SiRYvUDz/8oOrWrat27tyZq+Pu2bNHNWjQQE2ZMkVNmDBBHT9+XI0dO1a988476qGHHlLnzp3L83sBAAAAAH9DIAiAT3Xv3l2tWLHC9NTp06ePKl26tAngLF26VKWmpqrWrVurM2fO5LgnkASXjh49qpYsWWKOUbJkSdWuXTsTGFq7dq3q2rXrTbsnAAAAAPAXBIIA+Mzs2bNVfHy8qlChggkCOapYsaKKi4tTx44dU/3798/Rcd944w11+PBh1aFDB1WvXj2nbe3bt1e1atVSCQkJZjgaAAAAAAQyAkEAfGbkyJFmKT11ChQokGl7p06dzHLGjBnq0KFD2TpmUlKSHeCRQJCroKAg1bFjR7M+evRopbW+oXsAAAAAAH9GIAiAT2zYsEElJiaa9UaNGrndp3HjxmaZkZFhhnRlx8yZM9XVq1e9HrdJkyZmeeDAAbVq1apcXT8AAAAA5AcEggD4xPLly+31qlWrut1H8vqUL1/erK9evTpHx5WeP3feeafbfWrUqGGvZ/e4AAAAAJAfEQgC4BNbt26116OiojzuJ/mDxJYtW3J03HLlyqkiRYp4PabYvHlztq8ZAAAAAPKbzEk6AOAmcMz5U7ZsWY/7hYaGmqVM937p0iVVtGhRj/ueP39enT59OtvHFMnJyR73u3z5snlZZBYza1YywF/JUMu0tDRVqFAhFRzM33/gf6jDyCmpL4K8gADgHoEgAD79pUwUK1bM436OSaQlAOMtEJTbY3oyZswYNWLEiEzvexrKBgAAbl/yRyUZdg4AcEYgCIBPOP5VrnDhwh73sxI/W3l/fHnMwYMHqwEDBjgFjWQY25EjR/hFEn5LAqaRkZHq6NGjKiws7FZfDpBj1GHklPx+IEGgihUr3upLAYDbEoEgAD5RokQJe/3KlSse8/mkp6e7LZOdY3rieExvjQgJJrkLKEkQiMYH/J3UYeox/Bl1GDnBH3AAwDMGWgPwiSpVqtjr8lc6T6ycP2XKlPE63EtIgyA8PDzbx3S9DgAAAAAINASCAPhEvXr17PWkpCSPXbmtZM7169fP1nHr1q3r9ZjixIkT9np2jwsAAAAA+RGBIAA+0aZNG3s9MTHR7T4SzLFm7WrVqlWOjis5JI4dO+Z2nwMHDtjr2T2ukGFiw4YN85p/CLjdUY/h76jDAADkLQJBAHyiWbNmKjo62qyvW7fO7T4bN240y5CQENWtW7dsHfepp54y+2fnuNWrV1dNmzbN9jVLo2P48OE0PuDXqMfwd9RhAADyFoEgAD4hs3UNHTrUrC9cuFBlZGRk2ic+Pt4sY2Njs53LR6Z2l/3FvHnzMm2X8yxevNisDxky5IbuAQAAAAD8HYEgAD4TFxen2rZta4aAzZo1y2nb3r171dy5c81Ur2PHjs3Uo0emcZfgkNW7x9G4ceNMOQkEHTx40GnbjBkz1KFDh9Sjjz5qzg8AAAAAgYxAEACf9gqaPn26iomJUX379lULFixQqampatmyZSZAFBERoRISEszS0dSpU9WRI0fU0aNH1bRp0zIdV2YYW7RokZkq9oknnjDBorNnz6pPPvlE9e7dW7Vo0UJ9+eWX5vwAAAAAEMiCtEzTAwA+dPHiRTV+/HgT1JHeOpUqVTK5fgYOHGiCOa4ksPPnP//ZrM+fP181bNjQ7XElUDRq1Cj19ddfq5MnT6o6deqoPn36qOeee04FBxP3BgAAAABaRgB8LjQ01OTr2b17t0pPTzezekkAx10QSEgPosOHD5uXpyCQiIyMVJMmTTIBITnupk2bVK9evXIcBLp+/bqaPHmyOW/x4sXNcfv166dOnTqV43tF4JIZ8CTg2ahRI1OPihUrpurVq6dGjhxpZrnLysqVK01POenxVrZsWRMM/eWXX7J17v3796tnnnnG1N0SJUqoBx54wPTAy46UlBT19ttvq5o1a5rvau3atc3wy2vXrmVZlu9OYBg8eLDpYSkJnL2hDgMAcJuSHkEAgP93/vx53bJlS124cGH973//W58+fVpv2bJF169fX99xxx16x44dt/oS4QfOnj2rGzVqJD1u3b6qVq2qd+/e7bH8G2+8YfZ76aWX9NGjR/Xhw4d1t27ddKFChfSsWbO8nnv+/Pm6aNGi+sEHH9Tbt2/XZ86c0e+9954OCgrSr7zyiteyck133nmnrly5sl62bJlOSUnRS5Ys0eHh4bp58+Y6LS3NY1m+O4Fh5cqVOjg42NTPYcOGedyPOgwAwO2LQBAAOGjfvr1pvEyYMMHp/d9++02HhobqihUrmsYB4E2HDh10sWLF9MCBA/Xy5cv1zz//rCdPnqyjo6PtYFC1atX0hQsXMpV9//33zfbOnTs7vX/16lXdsGFDXaBAAb1mzRq3512/fr1paEtD+Ny5c07bXn31VXPcMWPGeAxeRUVF6ZCQEL1161anbQsWLDBl27Zt6/Ge+e7kfxKQiYyMtOuwp0AQdRgAgNsbgSAA+B/5K7U0AipUqGAaLK769OljtsfGxt6S64N/2Lx5s46IiHDbeyA1NdWpp9AHH3zgtP3gwYOmN4Jsk54QrmbPnm22Va9eXaenpzttu3btmr777rvdNmRFUlKSaYDL8Xft2pVpe+/evd023kVGRoauVauW2f75559n2s53JzB06dJFP/HEE14DQdRhAABuf+QIAoD/kdwtol27dqpAgQKZtnfq1MlpSnrAna+++srkqpK8JK7CwsLUlClT7J9Xr17ttP0f//iHyS101113mWTnrh5//HFVqFAhtW/fPjVnzpxM5921a5dZ79ChQ6aykpS9cePG5vjvvfee07akpCSTF8VTWckH07FjR7M+evRo+SOS03a+O/mf1Nu1a9eqzz//3Ot+1GEAAG5/BIIAQCm1YcMGlZiYaNYlua870gARGRkZTo15wFGzZs3cNkQt0jiOjo4269KgtVy5ckXNmjXLax2UhNNWgMm1Qf6f//zHLMuXL68qV67stnyTJk3MUhrgFy5csN+fOXOmunr1qtdzW2UlufuqVavs9/nu5H/ymffv319NnTrVJH32hDoMAIB/IBAEAEqp5cuX2+tVq1Z1u4/MaiYNFHc9OQDLn/70J9P7wJuIiAizlF4Tjo3R1NRUr3VQ1KhRwyzXr19vGt5CGsBWwzY7ZS9evKg2btyYqf7Ldd95551ey7rWf747+ZvMtNWtWzfVu3dv1bJlS6/7UocBAPAPBIIAQCm1detWez0qKsrjfhUqVDDLLVu2+OS6kD8dO3bMLB17DuW0DkoDevv27WZ9z5496tKlS9kuKzZv3pzp3OXKlVNFihTJVdnsnpvvjn+RKeJlSvV33nkny32pwwAA+IfMg6ABIAA55nzwNvQhNDTULM+dO2caLUWLFvXJ9SH/+PXXX9Xhw4fVPffcox566KFc10GRnJx8w2XPnz+vTp8+nauyuTk33x3/sWbNGjVhwgTT86ZgwYJZ7k8dBgDAP9AjCACUUmlpaU45LDxxTCKakpJy068L+c+nn35qluPGjXMaQnYjdfBWlc2L8rg9yRCv2NhY9f777zsNqfKGOgwAgH8gEAQAMiewwwwyhQsX9riflYxUZJUHBnB1/PhxNXHiRNWzZ0/VunXrPKuDt6psXpTH7alPnz4mQbLU1eyiDgMA4B8YGgYASqkSJUrY65K3wlOOifT0dLdlgOzo27evSUYrw22yqoOeONZBmY7+VpZ1V57vjv+T2cFkqvht27blqBx1GAAA/0CPIABQSlWpUsVel/wPnlh5KMqUKeN1CAHg6sMPPzS5VpYsWeI2t0hO66BjmRspKw3i8PDwXJXNzbn57tzeDh48qPr166feffddk3snKSkp08txSJX13uXLl6nDAAD4CQJBAKCUqlevnr3u2NBxJMMHrASj9evX99m1wf999913avTo0SohIUFFRkbmug6KEydOmKUEk6zcLbVq1bKT+WanrGsdrlu3bq7L8t3Jf72BJMDTvXt3U1fdvSzjx4+331u3bh11GAAAP0EgCACUUm3atLHXExMT3e5j/dVbtGrVymfXBv8mvYB69OhhegLVqVPH43733XefPdzEUx0UBw4cMMsHH3xQFSpUyM5rYs1Alp2y0oMiJiYmU/2XAIA1tb2nsq71n+9O/uKYLyenqMMAAPgHAkEAoJRq1qyZio6ONuvyl21PDXoREhKiunXr5tPrg3/65ZdfVJcuXdScOXNUo0aNvO4rDWHZ11sdlKEpMnRHxMXFOW2TGZ7E/v371alTp7zW4b/85S9OSXGfeuopU6+9ndsqW716ddW0aVP7fb47+cvw4cNNMMjbyzJs2DD7PQniUIcBAPAPBIIA4H8zwAwdOtSsL1y4UGVkZGTaJz4+3m6sOOaUANzZsmWLevzxx9XkyZPV/fff73afa9euqbffftv++Y033jDDY3bs2KH27t2baf9FixaZRrc0Wrt27eq0TRrC8r5snz9/vtveEHJcOf7AgQOdtkkCa6sRPm/evExl5fuwePFisz5kyBCnbXx34Ig6DACAH9AAACMjI0O3bdtW/tytp0+f7rRtz549ukiRIrpixYo6OTn5ll0j/MOPP/6oy5QpoydOnKgTExOdXjt37tSbN282daxZs2b6rbfecio7ZswYUwd79erl9P7Fixd17dq1dYECBfSqVavcnnfNmjU6JCRE16pVS1+9etVp23PPPWeOO2rUKLdlT506Zep3oUKF9K+//uq0berUqabso48+ar4nrvjuBBb5nOU1bNgwt9upwwAA3N4IBAGAS0MiJiZGh4WF6fnz5+uUlBSdkJCgq1atqiMjI/W2bdtu9SXiNrd06VIdGhpqN5azeu3fv9+p/PXr100D2mrwSp2UeteyZUvTGJ01a5bX80+ePNk0pDt37qwPHjyok5KS9CuvvGKO179/f69lN23apCMiInSdOnX0hg0b9JkzZ/SkSZN00aJFdYsWLcz3wRO+O4Ejq0AQdRgAgNsbgSAAcHHhwgXTeKlZs6YuXLiwrlatmh4yZIjXBgQgpOEpvR2yGwS6//77PR5rxowZukmTJrpYsWKmYdujRw+9b9++bF3HunXrdLt27UyvpOLFi5ueDitWrMhW2SNHjugXXnhBV65c2dT/hg0b6k8//dQ07rPCdycwZBUIslCHAQC4PQXJf2718DQAAAAAAADcfCSLBgAAAAAACBAEggAAAAAAAAIEgSAAAAAAAIAAQSAIAAAAAAAgQBAIAgAAAAAACBAEggAAAAAAAAIEgSAAAAAAAIAAQSAIAAAAAAAgQBAIAgAAAAAACBAEggAAAAAAAAIEgSAAAAAAAIAAQSAIAIAAdOXKFfXll1+q1q1bq2rVqqlAtmXLFhUbG6vuvPNOFRoaqurUqaPGjBmjLly44LXcmTNn1L333qvuuOMOtW7dOhVI5H7lvuX+5TkAAAD/QSAIABDQpk+froKCgjK9RowYkWXZL774wm1ZefXq1UvdrkaOHKmio6NV165d1bfffqsyMjJUoPrwww9Vx44d1SuvvKK2bdtmns3OnTvVm2++qR566CGvz+b7779XP//8szpx4oSaOXOmCiQzZsww9y33v3Llylt9OQAAIAcIBAEAAlq3bt3U6dOn1ezZs1XNmjXt9yUQNGfOHK9le/Tooc6dO6cSEhJUmTJlzHvjxo1TycnJ6pNPPlG3qwEDBqh9+/ap6tWrq0C2bNky9eqrr6qXX35ZxcTEqLCwMPX666+rt99+22zfvHmzOn78uMfyjzzyiGrQoIGqUKGC6t69u9feMykpKcrffPPNNx63Pf3006ZHUP369dXDDz/s0+sCAAA3JkhrrW/wGAAA5Aupqanq0UcfVRs3bjQ/FylSRK1atUo1adIky7K9e/dWq1evVrt371b+okuXLuqrr75SUVFR6tChQyrQNG3aVP30009q/vz5pleQIwkMyjCxJ5544obPc99995keQzL0zF9cu3bNDBk8cuTIrb4UAACQx+gRBADA/5QsWVL17dvX/jk9PV116NAhW43h8uXLq3Llyil/IoGuQCW9wDZs2GDWixcvnmn7k08+mSdBIOkt5o/5gz777DN19OjRW30ZAADgJiAQBACAixIlSpiXkDwoEhA4f/681zLBwcHm5U9CQkJUoJIgh9UpukCBAjflHFJ3evbsqfzNrl271MCBA2/1ZQAAgJvEv35jBQDAB0qXLq3mzp1rB0p++eUXk0sokJMq58dhgBZJ7p3XZKhdq1at1LFjx5Q/kbouwyOzCnwCAAD/RSAIAAA32rZtqyZOnGj/vHjx4hz1kggPD3eaRcw1P4wMOXOdacyVJCqWWawqV65sZigTBw8eVHFxcSY5dalSpVTnzp3V4cOH7TISeJDhbVKmWLFi6oEHHlDr16/P1jWfPHnSJE6uVKmSKdu8eXM1b968LHvWyIxbknhahprJfUsSZXflrl+/bp7j448/ru666y7znszQ9eCDD5oeWC+88ILdSye7vv76a9W+fXsVGRlpzi/5jiSJtyR6dkc+B3nWMiOYRZIdW5+B4/tZ+fXXX9WQIUPM87I+HyE5hySJstybpWrVqvY5JO+UI5mm/t1331WNGjUyz0GevSShfu+999Tly5cznVfuTZ6V7CsBp7Nnz6rY2FgztFGepQx7syQlJal+/fqZROhFixY1x5bPSuqIa14oqe+SN8kxeOVYPx33//HHH9Vzzz1nhtV5yy+1du1aE0SV+5fPR56V5KZasWKFxzKbNm1Szz//vNOxv/vuO1Ov5J6rVKmiRo8e7bGuyExmf/rTn0xdtOqx7C/3LDmxAAAIeJIsGgAA/L8pU6boqKgo++eBAwdKa9N+ffrpp27LDRs2TLdo0cL++cKFC3r58uU6LCzMlHM8prh06ZL+5ZdfdPXq1e1jW/bv36+7du2qCxYsaG+T61q5cqU5XuXKlXWRIkXsbdHR0fr8+fN606ZNuly5crp06dK6TJky9vbQ0FB94MCBTNfco0cP+9r27t1rlo73ar1ef/11t/ccHx+vK1WqpD/66CN97Ngx/fvvv+t//vOf9rW9+OKL9r5jx441120dU84l9xkREeF0ri1btmTrc7p48aLu3r27LlasmJ40aZI+ffq0uYYxY8aY5xYcHKzfeeedTOWuXbumr169qr/77jv7nLIu78lLtmdlz549ul27duYcjp+P5fr16+ZYn3/+ub1d7tU6R0ZGhtOxpA4MGDDAfAYpKSl6/vz59rNq3LixTktLM/suWbJEN2zY0Ol57d69Wzdr1szpvffff9/s//PPP+tSpUrp8PBwvXTpUp2ammrqiBxT9pM6cvz48UzX/dZbb9nHsq5ZXmLx4sX63nvvdTrfwYMH3T5n+e4UKFBAjx492pzn1KlT+uOPP9YlSpSw64fjs/j666/1H//4x0zHfvPNN81nGhkZqUNCQuxtclxX69evN/Vv0KBB+rfffjN1cubMmfqOO+4wZb788sssP18AAPI7AkEAAHgJBElDtXPnznbjUxqk33//fZaBIItV1jUQZPnrX/+aKRAkDfakpCQTyLC2xcXF6SeffNI0/MXly5f1008/bW+XRnfTpk31smXL7Mb17Nmz7e0vvfSSx0CQBGMaNGigR40apXfs2GECCP369dNBQUF2eWlMO9q4caMuXLiwCaK4+uCDD+xy06ZNM+8dPXrU3FO1atXM+1WqVNHt27c355o1a5YJKN1999363LlzXj8f12ufO3dupm0SrLPOP2HCBLflJahm7SPrOXHlyhUTGPniiy/cBoIs8p63YIkEfeR5SODF1datW+2yPXv2NO9JUEOCKV26dLG3xcbG6qlTp5rPIyYmxgR9Nm/ebPaXn2UfqWOOJODkLZgiddm1TjoG4KR+Pfvss17vzTqGBABdSYDUKut4bdZnLwEia7sE+1577TWdnJxstp04ccIEPmVbyZIlMwXuHnjgAV2nTp1M59y2bZsJShEIAgCAQBAAAF4DQVbjt0mTJnbjVHrcSGM6O4Egx1437nhrdEuQxFsgRxrHVrCmUaNGJoDkygoGSC8QT9dWvHhx01B29a9//cs+f40aNZy23X///bp+/fpu72nnzp12ufvuu89pm/R0srZ99dVXOje++eYbr89UWL1kpHeIBKDyMhBk2b59+w0FgoYOHWp6uEhvJnes3lKFChUyPb4s0gPLOu7LL7/sVMaxh03RokXNPu56RknASLb16dMnR3XSIgE2T/cmz0WCLvLs5bvjjgQ1pazUX+ml5GjixIn2sSdPnpyp7Lhx4+ztu3btctom55Recenp6W7PSSAIAACtyREEAEAWJLfKokWLTJ4TcebMGZPnRnKz3EyO05pL/hhXERERJrG1qF27tgoLC8u0j3XN3q5V8g3dc889md5/9dVX1b333mvW9+7dq/bt22fnxvnhhx/U7t27VYUKFTK9WrRoYR9j+/btTscsXLiwWUquGMlvlBsTJkwwS8l/5Env3r3NMj09XX3yySfqZggNDb2h8v/5z39Mnpu7777b7XO0cv1cuXJF7dmzJ9MztD4jR465pgYPHmye0Z///OdM57ZmxXOXg+hG7/3f//63unbtmqmz8t1xp0+fPmYp9//hhx86bXO8P8nf5Co6OtpeT0lJyVSXk5OT1V/+8henXEmia9euWd4XAACB4ObMlwoAQD5Trlw5k5i4WbNmpvEpgRFpYCckJKiCBQvelHNmZ1pzCRa5NngdWQ1xCSbkhjSet2zZYtYTExNNomFJFGwl1JZGvzeuSbCDg4NvaMp2Say8bNkys16+fHmP+zkmfV65cqUaMWKEymvWveSGJHGWRNtSr7Zu3Zrl/hLgcHdeb8/xrbfeMi/HZOBTp05Vs2bNMonIRW5nwvN275IsO6vPR75HhQoVMvVSPh9H1mx9nkhSbItrIKtXr17ms46PjzfHlUTmkixbnnPHjh2zvC8AAAIBPYIAAMimP/zhD6aRawV+vv/+e/XSSy+p/KxWrVr2ujWluBVEuHTpktueLI4vb8GA3JBZ02T2saymfZfZw6xeK7/99pu63VjPMC0tzTyjrJ7jjQQbd+3aZWaakxnFpAfON998Y3pk3QxSR06cOJHl5yNBIGvmOMdZym6UBL5ef/11E6iSZztq1ChTF/r372968gEAAAJBAADkiAxV+fTTT+2fZf39999X+ZXjECBrGJoM+xHZ6cmS16xglDh16lS2eo6UKlVK3W6sZyhD1xyHfeUl6W3zt7/9zUxFX6dOHTNMT4IkMqTwdvh8ZHp3x2VekN5E7733nplCvn379iYYJc/4gw8+MEPwrN5tAAAEMgJBAADkUI8ePZyG3AwcOFB9++23bvf11ivCH6Smptrr0pAWZcuWNcvff/9drV271mv5rLbnVGRkpMf8Q66k94uoUaOGut1Yz1DMmzfP676SiymnvVnk3p988kk1duxYE6gcNGhQrofj5fS+rOGIO3bssD8DT9d4sz6funXrqoULF5rAzyOPPGLX1w4dOpiebAAABDICQQAA5MLIkSNV9+7d7TwrVt4cV1bi2+zk6MltvpabaefOnWYpSaOrVKli1hs3bmxvl4CYp8a+5G8ZM2ZMnl6PDGmSIXpCen1Yw5BcyfAxK3jiLlnyrSYJj60eVtJbxVvvmeHDh+e4bsjwrwULFph1GRbmKxJsspKFyz1t2rTJ476SsyivPx8JfjmqX7++WrFihRowYID5WfIy/fe//82z8wEA4I8IBAEA4EAa3N56MTiaPHmyybvijdXYl94Ijr1rrGE0a9assX++ePGi0/bsXkd25OZYUuarr76yA1+WevXqmaFGQhLyynAjd8eXYUktW7bM86CXJAC2jvHRRx+53Ud6C0nwTWZNkxneXDkG5qxhWjnleM/u7t+xB47jkCnpkSI9xZ566ik7ICIzqDnuY5FgjgxtcuxBlJ3n6NhbyjVYJtdq3bOVbyk71y3XkZ17tz4fxxneXEmCc8n3JMPCYmNjs3VP7rieW4I+7nIOvfvuu3ZPJXfPGQCAQEIgCAAABxKwkZ4M2WmMSsJbaah7G9piTb8ux5MGsjSAr169aqajlyErjtNrS1BFZiST7a5TY587d87t8a3Gucym5Y41q5Kn8tY2dz1SJk6caAIK0puiXbt2TtvGjx9vzxwlQ48kd5IMcZK8QYsXL1Z//OMf1fLly1Xfvn2dylnDciT4kduZzF544QXVvHlzs/7Pf/7TzGbmSgIQEmz57LPPzOfkyjE4IjN45Ybj5yOJiT0FAa0eOkKGK8nMXWLIkCFmNishvVQkwDZp0iQznEkSkctsV88++6wJYjhyHNrk6dqt3lvi5ZdfNtcqQROpY9Jjx+qNI4m0pY44DnV0d93Sa2nVqlXZuvfHHnvM7pkzY8aMTLOCCQngyXdC6o5rziLHuugaHHXlem757khgyTXAZf1cpEiRLIO3AADkexoAAOiLFy/q//73vzoqKkq6GOihQ4fqEydO6GvXrmVZdv/+/bps2bK6RYsWmbZdvnxZ16hRwxzTeoWEhJj9f/rpJz1s2DD7/cKFC+vY2Fhz3tOnT+u4uDh7W6NGjfThw4f11atXzXFTU1P1lClT7O0RERH6xx9/1JcuXbLPu27dOnMea5+PP/5Ynzt3zr62OXPm6FKlSpltkZGR+osvvtDJycnm9fe//10XLFhQDx8+XGdkZLi9788++0wXKFDA6d6sV5UqVfS+ffsyPd/SpUvb+wwYMMDcq6fjeyPPp3HjxuY45cuX13PnztVnz57VR44c0a+99poODQ3VX375ZaZy8nnu3LlT16lTx76Oe+65R+/YscM82+xey5kzZ/Szzz5rH6NevXr60KFD+vr1607XWKxYMbM9KChIV6pUSTds2FCnp6fb+2zYsMF8du6eodSH+Ph4e1+5vt27d+umTZva+zzyyCN67969dr2wpKWl6cqVK9v7yecUFhZmrmHVqlWmPlnbwsPD9bJly+yyu3bt0sHBwXZdveOOO/Tjjz9utsn9yTOW+7XKP/PMM+ZeHck9ShnZXrx4cT1p0iR96tQp/fvvv+vRo0frQoUK6Q8++MCpjBz7wIEDum7duvaxn3/+eVNOtslnc/LkSfMdsba3bdtWHz9+3H7uJUuWNO/HxMTohIQEU1a+nx07djT3InUcAIBARyAIABDw1qxZ47YhLi93wR131q5daxql7hw9elR36tTJNMQlMCMBHmlMCwkEVatWTf/jH/8wjVzx888/e7yeV1991QQ8PG2vWbOmOYZjQ93xJQ1lRykpKaZB/tBDD5kgjTTQJRjWq1cvExzJyrZt23S3bt10hQoVTOBIyso1Wvdiad68ucdrXrlypc4NCX589NFH+r777jPBDAm61K5dW//1r3/VBw8edFumSZMmHq9DXtOmTcvyvNu3b/dYfsKECU77Ll26VFevXt089+7du5sgmyt5T645OjraPH+pI126dDHP1tGQIUM8nlfqkas9e/aYOin1ToJlL774oh2wkfuU9xs0aGACdK4kyCiBpDJlyuiXX35Znz9/3rwv9+fpGuS5uJo5c6Zu1aqVOU7RokVNULR3795u69b48eM9HnvWrFlez20F/axAkONLgp0dOnQwQTcAAKB1kPznVvdKAgAAAAAAwM1HjiAAAAAAAIAAQSAIAAAAAAAgQBAIAgAAAAAACBAEggAAAAAAAAIEgSAAAAAAAIAAQSAIAAAAAAAgQBAIAgAAAAAACBAEggAAAAAAAAIEgSAAAAAAAIAAQSAIAAAAAAAgQBAIAgAAAAAACBAEggAAAAAAAAIEgSAAAAAAAIAAQSAIAAAAAABABYb/AzAhx1h4YUVxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save = True\n",
    "date = \"07_05_25_\"\n",
    "if save :\n",
    "    # Save the model\n",
    "    model = binary_model_3_layer_1_untrained\n",
    "    model_name = \"CIFAR10_model_(1024+512-512+1)_3\"\n",
    "    save_path = \"Classifiers/\" + date + model_name + \"/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model , save_path + model_name + \".pt\")\n",
    "\n",
    "\n",
    "    # Save Architecture\n",
    "    with open(save_path + \"architecture.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model.architecture + str(model.training_time))\n",
    "\n",
    "    # Save performances of the model\n",
    "    os.makedirs(save_path + \"figures/\", exist_ok=True) \n",
    "    # Plot accuracy = f(n)\n",
    "    plt.plot(np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, len(model.accuracy_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory)) \n",
    "    np.savetxt(save_path +\"figures/accuracy_of_\" + model_name + \".txt\", data, delimiter =\",\", header=\"n,accuracy\")\n",
    "    plt.show() \n",
    "    \n",
    "    # Plot training and validation loss = f(n)\n",
    "    plt.plot(np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(np.linspace(0,len(model.validation_loss_trajectory)*model.observation_rate, len(model.validation_loss_trajectory)), model.validation_loss_trajectory, label=\"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, len(model.training_loss_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0, np.max([model.training_loss_trajectory, model.validation_loss_trajectory])+0.01)\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/loss_training_\" + model_name + \".txt\", data, delimiter=\",\", header=\"n, training_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot accuracy = f(kappa)\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.accuracy_trajectory)*model.observation_rate+1, len(model.accuracy_trajectory))]\n",
    "    plt.plot(kappa, model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((kappa, model.accuracy_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_accuracy_\" + model_name + \".txt\", data, delimiter=\",\", header=\"kappa, accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory))]\n",
    "    plt.plot(kappa, model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(kappa, model.validation_loss_trajectory, label = \"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0, np.max([model.training_loss_trajectory, model.validation_loss_trajectory])+0.01)\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data_training = np.column_stack((kappa, model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_training_\" + model_name + \".txt\", data_training, delimiter=\",\", header=\"kappa, training_loss\")\n",
    "    data_validation = np.column_stack((kappa, model.validation_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_validation_\" + model_name + \".txt\", data_validation, delimiter=\",\", header=\"kappa, validation_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c380b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('Post-processing/08_05_25/Dataset_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "225fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 layers - Training first layer : True - Training second layer : True - kappa = 2.45 - lr = 1e-05 - lr_decay_rate = 100000000.0 - reg1 = 0 - reg2 = 0 - eps_init = 1 - fraction_batch = 0.2 - observation rate = 10 - Train layer 1 = True - Train layer 2 = True - Dropout rate = 0.4\n",
      "[0.         0.48416665]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import model and plot performances\n",
    "\n",
    "date = '07_05_25'\n",
    "model_name = \"CIFAR10_model_(1024+512+1)_save_3\"\n",
    "curve_datas = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name + '/figures/accuracy_of_' + model_name +'.txt', delimiter=\",\", skiprows=1)\n",
    "assessed_model = torch.load('Classifiers/' + date + '/' + date + '_' +  model_name + '/' + model_name + \".pt\", weights_only=False)\n",
    "save_path = 'Post-processing/08_05_25/Dataset_3/'\n",
    "# Details of the model\n",
    "print(assessed_model.architecture)\n",
    "\n",
    "# Plot the performances\n",
    "model_name_1 = \"CIFAR10_model_(1024+512+1)_3\"\n",
    "model_name_2 = \"CIFAR10_model_(1024+512+512+1)_3\"\n",
    "model_name_3 = \"CIFAR10_model_(1024+512-512+1)_3\"\n",
    "\n",
    "curve_datas_model_1 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_1 + '/figures/accuracy_of_' + model_name_1 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_datas_model_2 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_2 + '/figures/accuracy_of_' + date + '_' + model_name_2 +'.txt', delimiter=\",\", skiprows=1)\n",
    "curve_datas_model_3 = np.loadtxt('Classifiers/' + date + '/' + date + '_' +  model_name_3 + '/figures/accuracy_of_' + model_name_3 +'.txt', delimiter=\",\", skiprows=1)\n",
    "\n",
    "print(curve_datas_model_1[0])\n",
    "plt.plot(curve_datas_model_3[:, 0], curve_datas_model_3[:, 1], '.', markersize = '1', color = 'blue', label = ' (1024+512-512+1)')\n",
    "plt.plot(curve_datas_model_2[:, 0], curve_datas_model_2[:, 1], '.', markersize = '1', color = 'red', label = '(1024+512+512+1)')\n",
    "plt.plot(curve_datas_model_1[:, 0], curve_datas_model_1[:, 1], '.', markersize = '1', color = 'green', label = '(1024+512+1)')\n",
    "plt.xlim(-200,np.min([np.max(curve_datas_model_1[:, 0]), np.max(curve_datas_model_2[:, 0]), np.max(curve_datas_model_3[:, 0])]))\n",
    "plt.ylim(0,1)\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration N')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies on dataset 3 for the different architectures', pad = 20)\n",
    "legend = plt.legend()\n",
    "for handle in legend.legend_handles:\n",
    "    handle.set_markersize(15)\n",
    "plt.savefig(save_path + \"Comparison_accuracy_dataset_3.png\", bbox_inches='tight')\n",
    "plt.savefig(save_path + \"Comparison_accuracy_dataset_3.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "# Plots of performances\n",
    "# accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(accuracy)\n",
    "# kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_accuracy)\n",
    "# loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(loss)\n",
    "# kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "# plt.imshow(kappa_loss)\n",
    "# plt.show()\n",
    "\n",
    "# Import datas\n",
    "# accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "# kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70c105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
