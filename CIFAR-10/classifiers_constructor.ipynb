{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337dd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6cb0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# improve the ploting style\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "rcParams['font.size'] = 18\n",
    "rcParams['mathtext.fontset'] = 'stix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba5a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_data = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) # Apply the (x - mean)/var operation on the components of the data # if x is in [0,1] then Normalise(x) is in [-1,1] # is applied on the three channels RGB\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feff3039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "dtype = torch.float32\n",
    "trainset = torchvision.datasets.CIFAR10(root = './datas', train= True, download = True, transform = transform_data)\n",
    "validset = torchvision.datasets.CIFAR10(root = './datas', train = False, download = True, transform = transform_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 1)\n",
    "\n",
    "x_train_raw, y_train_raw, x_valid_raw, y_valid_raw = torch.tensor(trainset.data, ), torch.tensor(trainset.targets), torch.tensor(validset.data), torch.tensor(validset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ded496",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = (x_train_raw.reshape(x_train_raw.shape[0], x_train_raw.shape[1]*x_train_raw.shape[2]*x_train_raw.shape[3])).to(dtype), x_valid_raw.reshape(x_valid_raw.shape[0], x_valid_raw.shape[1]*x_valid_raw.shape[2]*x_valid_raw.shape[3]).to(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8baed1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 59.,  62.,  63.,  43.,  46.,  45.,  50.,  48.,  43.,  68.,  54.,  42.,\n",
      "         98.,  73.,  52., 119.,  91.,  63., 139., 107.,  75., 145., 110.,  80.,\n",
      "        149., 117.,  89., 149., 120.,  93., 131., 103.])\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0,0:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329b3d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 32, 32, 3]) torch.Size([50000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(x_train_raw.shape, y_train_raw.shape)\n",
    "plt.imshow(x_train_raw[17])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7698be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 10]) torch.Size([10000, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40000, 3072]) torch.Size([40000, 1]) tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) torch.Size([8000, 3072]) torch.Size([8000, 1])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "# Modification du format des donnes shape (n_data,1) -> (n_data, n_classes)\n",
    "y_train = torch.zeros((y_train_raw.shape[0], torch.max(y_train_raw)+1))\n",
    "for i, y in enumerate(y_train_raw):\n",
    "    j = int(y.item())\n",
    "    y_train[i,j] = 1\n",
    "\n",
    "y_valid = torch.zeros((y_valid_raw.shape[0], torch.max(y_valid_raw)+1))\n",
    "for i,y in enumerate(y_valid_raw):\n",
    "    j = int(y.item())\n",
    "    y_valid[i,j] = 1 \n",
    "print(y_train.shape, y_valid.shape)\n",
    "\n",
    "# Binary reduction of the classes # To avoid using softmax, we regroup classes in two classes\n",
    "class_binary_reduction = True\n",
    "determination_des_classes = True\n",
    "if class_binary_reduction :\n",
    "    if determination_des_classes :\n",
    "        # Determination des classes\n",
    "        class_list = []\n",
    "        class_index = 0\n",
    "        for i in range (x_train_raw.shape[0]):\n",
    "            if y_train[i, class_index] == 1:\n",
    "                class_list.append(x_train_raw[i])\n",
    "                class_index += 1\n",
    "            if len(class_list) == len(y_train[0]):\n",
    "                break\n",
    "        for i, image in enumerate(class_list):\n",
    "            plt.subplot(2, int(len(class_list)/2+1),i+1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "        plt.show()\n",
    "                \n",
    "    classe1 = [0, 1, 8, 9]\n",
    "    classe2 = [2, 3, 4, 5]\n",
    "\n",
    "    # Cration des masques pour les chantillons appartenant  ces classes\n",
    "    mask_classe1_train = y_train[:, classe1].sum(dim=1) > 0  # True si appartient  classe1\n",
    "    mask_classe2_train = y_train[:, classe2].sum(dim=1) > 0  # True si appartient  classe2\n",
    "    \n",
    "    mask_classe1_valid = y_valid[:, classe1].sum(dim=1) > 0\n",
    "    mask_classe2_valid = y_valid[:, classe2].sum(dim=1) > 0\n",
    "    \n",
    "    # Filtrage des exemples concerns\n",
    "    mask_train = torch.logical_or(mask_classe1_train, mask_classe2_train)\n",
    "    mask_valid = torch.logical_or(mask_classe1_valid, mask_classe2_valid)\n",
    "    x_train, y_train = x_train[mask_train], y_train[mask_train]\n",
    "    x_valid, y_valid = x_valid[mask_valid], y_valid[mask_valid]\n",
    "\n",
    "    # Cration du vecteur de labels binaires (1 pour classe1, 0 pour classe2)\n",
    "    y_train = (y_train[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    y_valid = (y_valid[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    \n",
    "    # Avec tanh\n",
    "    \n",
    "    # x_train = 2*(x_train-0.5)\n",
    "    # y_train = 2*(y_train-0.5)\n",
    "    # x_valid = 2*(x_valid-0.5)\n",
    "    # y_valid = 2*(y_valid-0.5)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, y_train[0:10], x_valid.shape, y_valid.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45060724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing on :  mps\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Computing on : \", device)\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0).to(device),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).type(dtype).to(device)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), dj softmax\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque chantillon\n",
    "    \"\"\"\n",
    "    s = s.to(device)\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype).to(device) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = drive de softmax au logit j du ime batch par rapport au logit k du mme batch\n",
    "    for i in range(n):  # Pour chaque chantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-me donne du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des drives croises) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return torch.exp(-x)/((1 + torch.exp(-x))**2)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - (torch.tanh(x)**2)).to(device)\n",
    "\n",
    "class two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajust en fonction du seuil kappa_eff d'apprentissage des donnes.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prdiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output); grad_z2  = grad_z2.to(dtype) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la dure de l'entranement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "   \n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, number_of_classes,lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(three_layer_NN,self).__init__()\n",
    "        # Initialisation des proprits du rseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la proprit dure d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du rseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(number_of_classes, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t()\n",
    "        output = self.softmax(z3) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage alatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            \n",
    "            # Calcul de la prdiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = (torch.einsum('no,noz->nz',grad_output,softmax_derivative(output))).to(dtype) # shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la dure d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "\n",
    "class binary_classification_two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(1, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = torch.sigmoid(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajust en fonction du seuil kappa_eff d'apprentissage des donnes.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prdiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            #print(output[0:5], z2[0:5], h1[0:5], z1[0:5])\n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                print(\"Output\", output[20:22])\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = grad_output*sigmoid_derivative(z2); grad_z2  = grad_z2.to(dtype) # shape(n_data, 1)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            if i == 8000:\n",
    "                break\n",
    "            # print(\"grad_output\" ,grad_output[0:2,0])            \n",
    "            # print(\"grad_z2\",grad_z2[0:2,0]) \n",
    "            # print(\"grad_h1\",grad_h1[0:2,0])\n",
    "            # print(\"grad_z1\", grad_z1[0:2,0] )\n",
    "            # print(\"x_minibatch\", torch.mean(x_minibatch[0]))\n",
    "            # print(\"grad_W1\",grad_W1[0:2,0])\n",
    "            # print(\"mean W1\", torch.mean(grad_W1))            \n",
    "            # print(\"grad_b1\",grad_b1[0:2,0])\n",
    "            # print(\"grad_W2\",grad_W2[0:2,0])\n",
    "            # print(\"grad_b2\",grad_b2[0:2,0])\n",
    "            # print(\"W1\",self.W1[0:2,0])\n",
    "            # print(\"W2\",self.W2[0:2,0])\n",
    "            # print(\"b1\",self.b1[0:2,0])\n",
    "            # print(\"b2\",self.b2[0:2,0])\n",
    "            # break\n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la dure de l'entranement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "class binary_classification_three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_three_layer_NN,self).__init__()\n",
    "        # Initialisation des proprits du rseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la proprit dure d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du rseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(1, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, hidden_2_size ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t() # shape (n_data, 1)\n",
    "        output = sigmoid(z3) # output layer # shape (n_data, 1)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage alatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            \n",
    "            # Calcul de la prdiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = grad_output*sigmoid_derivative(z3).to(dtype) # shape (n_data, 1) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "            \n",
    "            # if i == 250:\n",
    "            #     break\n",
    "        \n",
    "        # Calcul de la dure d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36584c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_2_layer = binary_classification_two_layer_NN(3072, 2048, eps_init = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57236d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[0.5290],\n",
      "        [0.5549]], device='mps:0')\n",
      "Iteration 9220 Training loss 0.06521587073802948 Validation loss 0.06528531014919281 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.0230],\n",
      "        [0.2788]], device='mps:0')\n",
      "Iteration 9230 Training loss 0.0604189857840538 Validation loss 0.06533485651016235 Accuracy 0.8200000524520874\n",
      "Output tensor([[0.0593],\n",
      "        [0.6792]], device='mps:0')\n",
      "Iteration 9240 Training loss 0.06559104472398758 Validation loss 0.06532539427280426 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.7186],\n",
      "        [0.2481]], device='mps:0')\n",
      "Iteration 9250 Training loss 0.06551045924425125 Validation loss 0.06524262577295303 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.2179],\n",
      "        [0.3588]], device='mps:0')\n",
      "Iteration 9260 Training loss 0.06558247655630112 Validation loss 0.06538383662700653 Accuracy 0.8198750615119934\n",
      "Output tensor([[0.5774],\n",
      "        [0.5808]], device='mps:0')\n",
      "Iteration 9270 Training loss 0.06708681583404541 Validation loss 0.06522320210933685 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.1730],\n",
      "        [0.9743]], device='mps:0')\n",
      "Iteration 9280 Training loss 0.061521466821432114 Validation loss 0.06530981510877609 Accuracy 0.8202500343322754\n",
      "Output tensor([[0.6237],\n",
      "        [0.5561]], device='mps:0')\n",
      "Iteration 9290 Training loss 0.0691835954785347 Validation loss 0.06520459055900574 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.8443],\n",
      "        [0.9856]], device='mps:0')\n",
      "Iteration 9300 Training loss 0.06998170912265778 Validation loss 0.0652722641825676 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.9245],\n",
      "        [0.0186]], device='mps:0')\n",
      "Iteration 9310 Training loss 0.06668491661548615 Validation loss 0.06533703953027725 Accuracy 0.8196250200271606\n",
      "Output tensor([[0.3491],\n",
      "        [0.6806]], device='mps:0')\n",
      "Iteration 9320 Training loss 0.0676904022693634 Validation loss 0.06522110104560852 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.0138],\n",
      "        [0.9410]], device='mps:0')\n",
      "Iteration 9330 Training loss 0.061919644474983215 Validation loss 0.06519069522619247 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.0725],\n",
      "        [0.7730]], device='mps:0')\n",
      "Iteration 9340 Training loss 0.07174631208181381 Validation loss 0.06609534472227097 Accuracy 0.8168750405311584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x102c03ad0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[0.2149],\n",
      "        [0.8041]], device='mps:0')\n",
      "Iteration 9350 Training loss 0.06462163478136063 Validation loss 0.06544353067874908 Accuracy 0.8197500109672546\n",
      "Output tensor([[0.2176],\n",
      "        [0.6466]], device='mps:0')\n",
      "Iteration 9360 Training loss 0.06504693627357483 Validation loss 0.06519900262355804 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.0159],\n",
      "        [0.2996]], device='mps:0')\n",
      "Iteration 9370 Training loss 0.061075448989868164 Validation loss 0.06525903940200806 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.5315],\n",
      "        [0.8964]], device='mps:0')\n",
      "Iteration 9380 Training loss 0.06047108396887779 Validation loss 0.06529561430215836 Accuracy 0.8207500576972961\n",
      "Output tensor([[0.6193],\n",
      "        [0.1656]], device='mps:0')\n",
      "Iteration 9390 Training loss 0.06542260199785233 Validation loss 0.0651678666472435 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.2390],\n",
      "        [0.5191]], device='mps:0')\n",
      "Iteration 9400 Training loss 0.06213509291410446 Validation loss 0.06518973410129547 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.8749],\n",
      "        [0.9444]], device='mps:0')\n",
      "Iteration 9410 Training loss 0.05998639762401581 Validation loss 0.06519759446382523 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.5308],\n",
      "        [0.0595]], device='mps:0')\n",
      "Iteration 9420 Training loss 0.06833292543888092 Validation loss 0.06514687836170197 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.7921],\n",
      "        [0.9682]], device='mps:0')\n",
      "Iteration 9430 Training loss 0.0660301074385643 Validation loss 0.0651300698518753 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.1637],\n",
      "        [0.0503]], device='mps:0')\n",
      "Iteration 9440 Training loss 0.07178354263305664 Validation loss 0.06536361575126648 Accuracy 0.8200000524520874\n",
      "Output tensor([[0.1736],\n",
      "        [0.6491]], device='mps:0')\n",
      "Iteration 9450 Training loss 0.06519079208374023 Validation loss 0.06542950868606567 Accuracy 0.8200000524520874\n",
      "Output tensor([[0.3051],\n",
      "        [0.9899]], device='mps:0')\n",
      "Iteration 9460 Training loss 0.05944168567657471 Validation loss 0.065126433968544 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.6913],\n",
      "        [0.8546]], device='mps:0')\n",
      "Iteration 9470 Training loss 0.06249193847179413 Validation loss 0.06511205434799194 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.1332],\n",
      "        [0.9919]], device='mps:0')\n",
      "Iteration 9480 Training loss 0.06145616993308067 Validation loss 0.06512798368930817 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.8235],\n",
      "        [0.0717]], device='mps:0')\n",
      "Iteration 9490 Training loss 0.059813566505908966 Validation loss 0.06518258899450302 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.3765],\n",
      "        [0.9719]], device='mps:0')\n",
      "Iteration 9500 Training loss 0.06634936481714249 Validation loss 0.0652427077293396 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.2835],\n",
      "        [0.2413]], device='mps:0')\n",
      "Iteration 9510 Training loss 0.07139556109905243 Validation loss 0.06512002646923065 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.0181],\n",
      "        [0.1275]], device='mps:0')\n",
      "Iteration 9520 Training loss 0.06929530203342438 Validation loss 0.06513084471225739 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.3588],\n",
      "        [0.0447]], device='mps:0')\n",
      "Iteration 9530 Training loss 0.0733160749077797 Validation loss 0.06508272141218185 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.2902],\n",
      "        [0.6716]], device='mps:0')\n",
      "Iteration 9540 Training loss 0.0737227126955986 Validation loss 0.06512373685836792 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.3147],\n",
      "        [0.0921]], device='mps:0')\n",
      "Iteration 9550 Training loss 0.06220761314034462 Validation loss 0.06517374515533447 Accuracy 0.8207500576972961\n",
      "Output tensor([[0.6360],\n",
      "        [0.9496]], device='mps:0')\n",
      "Iteration 9560 Training loss 0.06897179782390594 Validation loss 0.06513386964797974 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.6802],\n",
      "        [0.9883]], device='mps:0')\n",
      "Iteration 9570 Training loss 0.06576351821422577 Validation loss 0.06507173180580139 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.9126],\n",
      "        [0.6870]], device='mps:0')\n",
      "Iteration 9580 Training loss 0.06455392390489578 Validation loss 0.06512261927127838 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.7862],\n",
      "        [0.2990]], device='mps:0')\n",
      "Iteration 9590 Training loss 0.06612133979797363 Validation loss 0.06509502977132797 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.1537],\n",
      "        [0.2395]], device='mps:0')\n",
      "Iteration 9600 Training loss 0.06207090616226196 Validation loss 0.06558744609355927 Accuracy 0.8188750147819519\n",
      "Output tensor([[0.4624],\n",
      "        [0.9317]], device='mps:0')\n",
      "Iteration 9610 Training loss 0.0690886452794075 Validation loss 0.06535731256008148 Accuracy 0.8203750252723694\n",
      "Output tensor([[0.0058],\n",
      "        [0.6811]], device='mps:0')\n",
      "Iteration 9620 Training loss 0.06505466997623444 Validation loss 0.0650327131152153 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.8809],\n",
      "        [0.2544]], device='mps:0')\n",
      "Iteration 9630 Training loss 0.07826271653175354 Validation loss 0.06508529931306839 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.1494],\n",
      "        [0.7573]], device='mps:0')\n",
      "Iteration 9640 Training loss 0.06139541417360306 Validation loss 0.06501738727092743 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.9920],\n",
      "        [0.0811]], device='mps:0')\n",
      "Iteration 9650 Training loss 0.06642735749483109 Validation loss 0.0650622621178627 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.6907],\n",
      "        [0.3706]], device='mps:0')\n",
      "Iteration 9660 Training loss 0.06173272058367729 Validation loss 0.06545151025056839 Accuracy 0.8198750615119934\n",
      "Output tensor([[0.0182],\n",
      "        [0.3743]], device='mps:0')\n",
      "Iteration 9670 Training loss 0.05830632522702217 Validation loss 0.06505188345909119 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.1825],\n",
      "        [0.7760]], device='mps:0')\n",
      "Iteration 9680 Training loss 0.06423595547676086 Validation loss 0.06514659523963928 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.9797],\n",
      "        [0.4459]], device='mps:0')\n",
      "Iteration 9690 Training loss 0.08023340255022049 Validation loss 0.06521271914243698 Accuracy 0.8202500343322754\n",
      "Output tensor([[0.0421],\n",
      "        [0.1521]], device='mps:0')\n",
      "Iteration 9700 Training loss 0.06562910974025726 Validation loss 0.06503819674253464 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.2736],\n",
      "        [0.1478]], device='mps:0')\n",
      "Iteration 9710 Training loss 0.06929443031549454 Validation loss 0.06510509550571442 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.7391],\n",
      "        [0.3662]], device='mps:0')\n",
      "Iteration 9720 Training loss 0.06313170492649078 Validation loss 0.0649905651807785 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.4467],\n",
      "        [0.7776]], device='mps:0')\n",
      "Iteration 9730 Training loss 0.06004087254405022 Validation loss 0.0649835541844368 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.1356],\n",
      "        [0.4574]], device='mps:0')\n",
      "Iteration 9740 Training loss 0.059796690940856934 Validation loss 0.06500906497240067 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.1650],\n",
      "        [0.8072]], device='mps:0')\n",
      "Iteration 9750 Training loss 0.06366007775068283 Validation loss 0.06497640907764435 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.9622],\n",
      "        [0.8820]], device='mps:0')\n",
      "Iteration 9760 Training loss 0.06281470507383347 Validation loss 0.06506919115781784 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.9157],\n",
      "        [0.4831]], device='mps:0')\n",
      "Iteration 9770 Training loss 0.06756681948900223 Validation loss 0.06496528536081314 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.3922],\n",
      "        [0.9249]], device='mps:0')\n",
      "Iteration 9780 Training loss 0.06684184074401855 Validation loss 0.06498105823993683 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.3660],\n",
      "        [0.0647]], device='mps:0')\n",
      "Iteration 9790 Training loss 0.0617450475692749 Validation loss 0.06495372205972672 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.8554],\n",
      "        [0.5819]], device='mps:0')\n",
      "Iteration 9800 Training loss 0.06348492205142975 Validation loss 0.06513474136590958 Accuracy 0.8208750486373901\n",
      "Output tensor([[0.0758],\n",
      "        [0.8342]], device='mps:0')\n",
      "Iteration 9810 Training loss 0.06538636237382889 Validation loss 0.06523462384939194 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.6662],\n",
      "        [0.9654]], device='mps:0')\n",
      "Iteration 9820 Training loss 0.072286456823349 Validation loss 0.06494169682264328 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.1213],\n",
      "        [0.8041]], device='mps:0')\n",
      "Iteration 9830 Training loss 0.06316525489091873 Validation loss 0.06509444862604141 Accuracy 0.8207500576972961\n",
      "Output tensor([[0.0683],\n",
      "        [0.1742]], device='mps:0')\n",
      "Iteration 9840 Training loss 0.06487087160348892 Validation loss 0.06493134051561356 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.7179],\n",
      "        [0.1897]], device='mps:0')\n",
      "Iteration 9850 Training loss 0.05916396528482437 Validation loss 0.06494849920272827 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.9367],\n",
      "        [0.9211]], device='mps:0')\n",
      "Iteration 9860 Training loss 0.05637061595916748 Validation loss 0.06491521745920181 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.0865],\n",
      "        [0.4332]], device='mps:0')\n",
      "Iteration 9870 Training loss 0.060207076370716095 Validation loss 0.06517428159713745 Accuracy 0.8210000395774841\n",
      "Output tensor([[0.6754],\n",
      "        [0.9431]], device='mps:0')\n",
      "Iteration 9880 Training loss 0.06628400087356567 Validation loss 0.06492764502763748 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.9767],\n",
      "        [0.2206]], device='mps:0')\n",
      "Iteration 9890 Training loss 0.06007153540849686 Validation loss 0.06491830945014954 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.6854],\n",
      "        [0.2014]], device='mps:0')\n",
      "Iteration 9900 Training loss 0.06838098168373108 Validation loss 0.06491420418024063 Accuracy 0.8212500214576721\n",
      "Output tensor([[0.2354],\n",
      "        [0.0323]], device='mps:0')\n",
      "Iteration 9910 Training loss 0.06481505930423737 Validation loss 0.06493569165468216 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.8610],\n",
      "        [0.1294]], device='mps:0')\n",
      "Iteration 9920 Training loss 0.06573864072561264 Validation loss 0.0648825541138649 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.2353],\n",
      "        [0.0548]], device='mps:0')\n",
      "Iteration 9930 Training loss 0.05864623934030533 Validation loss 0.06500822305679321 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.1178],\n",
      "        [0.3756]], device='mps:0')\n",
      "Iteration 9940 Training loss 0.06849399209022522 Validation loss 0.06532846391201019 Accuracy 0.8203750252723694\n",
      "Output tensor([[0.4749],\n",
      "        [0.9568]], device='mps:0')\n",
      "Iteration 9950 Training loss 0.06720204651355743 Validation loss 0.06489088386297226 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.9029],\n",
      "        [0.0551]], device='mps:0')\n",
      "Iteration 9960 Training loss 0.0659664049744606 Validation loss 0.06490478664636612 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.0540],\n",
      "        [0.2398]], device='mps:0')\n",
      "Iteration 9970 Training loss 0.06483017653226852 Validation loss 0.06493918597698212 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.5867],\n",
      "        [0.3217]], device='mps:0')\n",
      "Iteration 9980 Training loss 0.057752106338739395 Validation loss 0.0648634135723114 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.9438],\n",
      "        [0.8121]], device='mps:0')\n",
      "Iteration 9990 Training loss 0.06271280348300934 Validation loss 0.06486420333385468 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.5584],\n",
      "        [0.1430]], device='mps:0')\n",
      "Iteration 10000 Training loss 0.0587727390229702 Validation loss 0.06485328823328018 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.2818],\n",
      "        [0.8339]], device='mps:0')\n",
      "Iteration 10010 Training loss 0.06070539355278015 Validation loss 0.06498263776302338 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.8072],\n",
      "        [0.8453]], device='mps:0')\n",
      "Iteration 10020 Training loss 0.0662011057138443 Validation loss 0.06491582095623016 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.9141],\n",
      "        [0.1652]], device='mps:0')\n",
      "Iteration 10030 Training loss 0.06595510989427567 Validation loss 0.06491334736347198 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.5474],\n",
      "        [0.8904]], device='mps:0')\n",
      "Iteration 10040 Training loss 0.06792539358139038 Validation loss 0.06483984738588333 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.0663],\n",
      "        [0.0530]], device='mps:0')\n",
      "Iteration 10050 Training loss 0.07033243775367737 Validation loss 0.06486232578754425 Accuracy 0.8212500214576721\n",
      "Output tensor([[0.6963],\n",
      "        [0.9872]], device='mps:0')\n",
      "Iteration 10060 Training loss 0.06734723597764969 Validation loss 0.06525033712387085 Accuracy 0.8212500214576721\n",
      "Output tensor([[0.9223],\n",
      "        [0.3999]], device='mps:0')\n",
      "Iteration 10070 Training loss 0.06642314046621323 Validation loss 0.06486117839813232 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.5332],\n",
      "        [0.7422]], device='mps:0')\n",
      "Iteration 10080 Training loss 0.06520107388496399 Validation loss 0.06482702493667603 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.3644],\n",
      "        [0.9891]], device='mps:0')\n",
      "Iteration 10090 Training loss 0.06060786172747612 Validation loss 0.06485501676797867 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.9641],\n",
      "        [0.9202]], device='mps:0')\n",
      "Iteration 10100 Training loss 0.06614992767572403 Validation loss 0.06480628997087479 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.2113],\n",
      "        [0.3262]], device='mps:0')\n",
      "Iteration 10110 Training loss 0.06686658412218094 Validation loss 0.06480290740728378 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.9929],\n",
      "        [0.1084]], device='mps:0')\n",
      "Iteration 10120 Training loss 0.07289816439151764 Validation loss 0.06481180340051651 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.2474],\n",
      "        [0.3966]], device='mps:0')\n",
      "Iteration 10130 Training loss 0.06779874861240387 Validation loss 0.06506851315498352 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.1586],\n",
      "        [0.5546]], device='mps:0')\n",
      "Iteration 10140 Training loss 0.058090321719646454 Validation loss 0.06500578671693802 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.1058],\n",
      "        [0.8381]], device='mps:0')\n",
      "Iteration 10150 Training loss 0.06289595365524292 Validation loss 0.06505309790372849 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.8559],\n",
      "        [0.9621]], device='mps:0')\n",
      "Iteration 10160 Training loss 0.06770619004964828 Validation loss 0.06485478579998016 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.0287],\n",
      "        [0.1420]], device='mps:0')\n",
      "Iteration 10170 Training loss 0.06355392187833786 Validation loss 0.06489893049001694 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.5088],\n",
      "        [0.2621]], device='mps:0')\n",
      "Iteration 10180 Training loss 0.056115612387657166 Validation loss 0.06480308622121811 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.3213],\n",
      "        [0.3650]], device='mps:0')\n",
      "Iteration 10190 Training loss 0.06584959477186203 Validation loss 0.06478798389434814 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.3128],\n",
      "        [0.9936]], device='mps:0')\n",
      "Iteration 10200 Training loss 0.056108132004737854 Validation loss 0.06481138616800308 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.2584],\n",
      "        [0.7071]], device='mps:0')\n",
      "Iteration 10210 Training loss 0.06426408141851425 Validation loss 0.06485237181186676 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.0494],\n",
      "        [0.9643]], device='mps:0')\n",
      "Iteration 10220 Training loss 0.06081196665763855 Validation loss 0.0647648349404335 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.7093],\n",
      "        [0.6008]], device='mps:0')\n",
      "Iteration 10230 Training loss 0.06465835124254227 Validation loss 0.06479164212942123 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.1817],\n",
      "        [0.4124]], device='mps:0')\n",
      "Iteration 10240 Training loss 0.06572967022657394 Validation loss 0.06486985832452774 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.4491],\n",
      "        [0.6506]], device='mps:0')\n",
      "Iteration 10250 Training loss 0.06511180847883224 Validation loss 0.06475172936916351 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5527],\n",
      "        [0.5488]], device='mps:0')\n",
      "Iteration 10260 Training loss 0.06516368687152863 Validation loss 0.06473855674266815 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.3856],\n",
      "        [0.6362]], device='mps:0')\n",
      "Iteration 10270 Training loss 0.05899614095687866 Validation loss 0.06487306207418442 Accuracy 0.8207500576972961\n",
      "Output tensor([[0.2390],\n",
      "        [0.7880]], device='mps:0')\n",
      "Iteration 10280 Training loss 0.06740594655275345 Validation loss 0.0648946538567543 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.1784],\n",
      "        [0.4428]], device='mps:0')\n",
      "Iteration 10290 Training loss 0.05511561408638954 Validation loss 0.06487029045820236 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.0925],\n",
      "        [0.1728]], device='mps:0')\n",
      "Iteration 10300 Training loss 0.07015649974346161 Validation loss 0.06472180783748627 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.9279],\n",
      "        [0.2655]], device='mps:0')\n",
      "Iteration 10310 Training loss 0.06385175883769989 Validation loss 0.06472453474998474 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.9916],\n",
      "        [0.0613]], device='mps:0')\n",
      "Iteration 10320 Training loss 0.06487134099006653 Validation loss 0.06480620056390762 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.8363],\n",
      "        [0.4811]], device='mps:0')\n",
      "Iteration 10330 Training loss 0.060713183134794235 Validation loss 0.06473863869905472 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.9357],\n",
      "        [0.0468]], device='mps:0')\n",
      "Iteration 10340 Training loss 0.07428701967000961 Validation loss 0.06530128419399261 Accuracy 0.8202500343322754\n",
      "Output tensor([[0.2550],\n",
      "        [0.9166]], device='mps:0')\n",
      "Iteration 10350 Training loss 0.06338232010602951 Validation loss 0.0647246465086937 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.9141],\n",
      "        [0.0881]], device='mps:0')\n",
      "Iteration 10360 Training loss 0.06765209138393402 Validation loss 0.06481462717056274 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.8980],\n",
      "        [0.6469]], device='mps:0')\n",
      "Iteration 10370 Training loss 0.07211463153362274 Validation loss 0.06471199542284012 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.5980],\n",
      "        [0.8190]], device='mps:0')\n",
      "Iteration 10380 Training loss 0.06581307202577591 Validation loss 0.0649908185005188 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.3478],\n",
      "        [0.3600]], device='mps:0')\n",
      "Iteration 10390 Training loss 0.06646841019392014 Validation loss 0.06470081210136414 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.9180],\n",
      "        [0.6409]], device='mps:0')\n",
      "Iteration 10400 Training loss 0.0678977370262146 Validation loss 0.06510747969150543 Accuracy 0.8208750486373901\n",
      "Output tensor([[0.2865],\n",
      "        [0.0670]], device='mps:0')\n",
      "Iteration 10410 Training loss 0.06778649240732193 Validation loss 0.06490416079759598 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.2316],\n",
      "        [0.6075]], device='mps:0')\n",
      "Iteration 10420 Training loss 0.06566916406154633 Validation loss 0.06472636759281158 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.2349],\n",
      "        [0.7305]], device='mps:0')\n",
      "Iteration 10430 Training loss 0.06256638467311859 Validation loss 0.06469187140464783 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.7347],\n",
      "        [0.2402]], device='mps:0')\n",
      "Iteration 10440 Training loss 0.06268307566642761 Validation loss 0.06467680633068085 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.1405],\n",
      "        [0.4168]], device='mps:0')\n",
      "Iteration 10450 Training loss 0.06851600855588913 Validation loss 0.06467387825250626 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.9530],\n",
      "        [0.3657]], device='mps:0')\n",
      "Iteration 10460 Training loss 0.06482867896556854 Validation loss 0.06466406583786011 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.3467],\n",
      "        [0.0434]], device='mps:0')\n",
      "Iteration 10470 Training loss 0.06899034976959229 Validation loss 0.06475628167390823 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.9485],\n",
      "        [0.3453]], device='mps:0')\n",
      "Iteration 10480 Training loss 0.06476648151874542 Validation loss 0.06469735503196716 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.2105],\n",
      "        [0.5305]], device='mps:0')\n",
      "Iteration 10490 Training loss 0.06895233690738678 Validation loss 0.06465213000774384 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.9910],\n",
      "        [0.2498]], device='mps:0')\n",
      "Iteration 10500 Training loss 0.06666876375675201 Validation loss 0.06471994519233704 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.0597],\n",
      "        [0.0791]], device='mps:0')\n",
      "Iteration 10510 Training loss 0.06815087795257568 Validation loss 0.06464001536369324 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.9025],\n",
      "        [0.9868]], device='mps:0')\n",
      "Iteration 10520 Training loss 0.056677255779504776 Validation loss 0.0648282989859581 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.8815],\n",
      "        [0.6793]], device='mps:0')\n",
      "Iteration 10530 Training loss 0.0634327232837677 Validation loss 0.0646517425775528 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.3032],\n",
      "        [0.9699]], device='mps:0')\n",
      "Iteration 10540 Training loss 0.06680983304977417 Validation loss 0.06469585746526718 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.7579],\n",
      "        [0.2309]], device='mps:0')\n",
      "Iteration 10550 Training loss 0.06581244617700577 Validation loss 0.06468840688467026 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.5282],\n",
      "        [0.5731]], device='mps:0')\n",
      "Iteration 10560 Training loss 0.05507823824882507 Validation loss 0.06547246128320694 Accuracy 0.8196250200271606\n",
      "Output tensor([[0.0536],\n",
      "        [0.2737]], device='mps:0')\n",
      "Iteration 10570 Training loss 0.06359200924634933 Validation loss 0.06468245387077332 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.9525],\n",
      "        [0.2583]], device='mps:0')\n",
      "Iteration 10580 Training loss 0.05980682745575905 Validation loss 0.06461848318576813 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.7748],\n",
      "        [0.9469]], device='mps:0')\n",
      "Iteration 10590 Training loss 0.07135801762342453 Validation loss 0.06463547796010971 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.0821],\n",
      "        [0.4720]], device='mps:0')\n",
      "Iteration 10600 Training loss 0.070037841796875 Validation loss 0.06463296711444855 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.0330],\n",
      "        [0.7495]], device='mps:0')\n",
      "Iteration 10610 Training loss 0.06539727002382278 Validation loss 0.06485491991043091 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.8449],\n",
      "        [0.2351]], device='mps:0')\n",
      "Iteration 10620 Training loss 0.06434951722621918 Validation loss 0.06459254771471024 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.0053],\n",
      "        [0.4413]], device='mps:0')\n",
      "Iteration 10630 Training loss 0.06895216554403305 Validation loss 0.06458776444196701 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.8608],\n",
      "        [0.0442]], device='mps:0')\n",
      "Iteration 10640 Training loss 0.06496326625347137 Validation loss 0.06460776180028915 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.0466],\n",
      "        [0.7686]], device='mps:0')\n",
      "Iteration 10650 Training loss 0.060221266001462936 Validation loss 0.06470318883657455 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.2896],\n",
      "        [0.3292]], device='mps:0')\n",
      "Iteration 10660 Training loss 0.07060181349515915 Validation loss 0.06463367491960526 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.8486],\n",
      "        [0.5183]], device='mps:0')\n",
      "Iteration 10670 Training loss 0.05939282104372978 Validation loss 0.06464269757270813 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.1740],\n",
      "        [0.9915]], device='mps:0')\n",
      "Iteration 10680 Training loss 0.07003617286682129 Validation loss 0.06456946581602097 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.9274],\n",
      "        [0.1324]], device='mps:0')\n",
      "Iteration 10690 Training loss 0.07569558173418045 Validation loss 0.06460661441087723 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.8042],\n",
      "        [0.8896]], device='mps:0')\n",
      "Iteration 10700 Training loss 0.07024089992046356 Validation loss 0.06461834907531738 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.7823],\n",
      "        [0.8181]], device='mps:0')\n",
      "Iteration 10710 Training loss 0.0662907138466835 Validation loss 0.06460098177194595 Accuracy 0.8220000267028809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x102c03ad0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[0.0596],\n",
      "        [0.5078]], device='mps:0')\n",
      "Iteration 10720 Training loss 0.06151678040623665 Validation loss 0.06454452127218246 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.0524],\n",
      "        [0.2511]], device='mps:0')\n",
      "Iteration 10730 Training loss 0.0732833594083786 Validation loss 0.06491966545581818 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.5258],\n",
      "        [0.9367]], device='mps:0')\n",
      "Iteration 10740 Training loss 0.07228617370128632 Validation loss 0.06454499065876007 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.9551],\n",
      "        [0.9205]], device='mps:0')\n",
      "Iteration 10750 Training loss 0.06240368261933327 Validation loss 0.06483528763055801 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.8967],\n",
      "        [0.0292]], device='mps:0')\n",
      "Iteration 10760 Training loss 0.06163642928004265 Validation loss 0.06461470574140549 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.5683],\n",
      "        [0.9251]], device='mps:0')\n",
      "Iteration 10770 Training loss 0.07397200167179108 Validation loss 0.06453660875558853 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.7317],\n",
      "        [0.5533]], device='mps:0')\n",
      "Iteration 10780 Training loss 0.06807829439640045 Validation loss 0.06452925503253937 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.0372],\n",
      "        [0.8966]], device='mps:0')\n",
      "Iteration 10790 Training loss 0.06258179992437363 Validation loss 0.06490668654441833 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.9173],\n",
      "        [0.3209]], device='mps:0')\n",
      "Iteration 10800 Training loss 0.06244504824280739 Validation loss 0.06466028839349747 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.6903],\n",
      "        [0.5461]], device='mps:0')\n",
      "Iteration 10810 Training loss 0.06260721385478973 Validation loss 0.06457790732383728 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.3333],\n",
      "        [0.5502]], device='mps:0')\n",
      "Iteration 10820 Training loss 0.058915283530950546 Validation loss 0.06451161950826645 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.8593],\n",
      "        [0.9138]], device='mps:0')\n",
      "Iteration 10830 Training loss 0.0652986541390419 Validation loss 0.06458045542240143 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.9311],\n",
      "        [0.0332]], device='mps:0')\n",
      "Iteration 10840 Training loss 0.06488222628831863 Validation loss 0.06451491266489029 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.3402],\n",
      "        [0.7804]], device='mps:0')\n",
      "Iteration 10850 Training loss 0.06804461032152176 Validation loss 0.0647452250123024 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.9876],\n",
      "        [0.6602]], device='mps:0')\n",
      "Iteration 10860 Training loss 0.0648675411939621 Validation loss 0.06466223299503326 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.1647],\n",
      "        [0.8863]], device='mps:0')\n",
      "Iteration 10870 Training loss 0.06426599621772766 Validation loss 0.06450748443603516 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.6665],\n",
      "        [0.8897]], device='mps:0')\n",
      "Iteration 10880 Training loss 0.06595735251903534 Validation loss 0.06448838114738464 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5320],\n",
      "        [0.3955]], device='mps:0')\n",
      "Iteration 10890 Training loss 0.06436388939619064 Validation loss 0.06447943300008774 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.4491],\n",
      "        [0.9078]], device='mps:0')\n",
      "Iteration 10900 Training loss 0.0589459128677845 Validation loss 0.06464541703462601 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.0098],\n",
      "        [0.0729]], device='mps:0')\n",
      "Iteration 10910 Training loss 0.06478767842054367 Validation loss 0.06447311490774155 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.9046],\n",
      "        [0.8546]], device='mps:0')\n",
      "Iteration 10920 Training loss 0.065516397356987 Validation loss 0.06516281515359879 Accuracy 0.8198750615119934\n",
      "Output tensor([[0.0583],\n",
      "        [0.0227]], device='mps:0')\n",
      "Iteration 10930 Training loss 0.07073809206485748 Validation loss 0.06452185660600662 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.7262],\n",
      "        [0.9014]], device='mps:0')\n",
      "Iteration 10940 Training loss 0.06809007376432419 Validation loss 0.06446431577205658 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.1659],\n",
      "        [0.2663]], device='mps:0')\n",
      "Iteration 10950 Training loss 0.05811408534646034 Validation loss 0.06446734815835953 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5208],\n",
      "        [0.9125]], device='mps:0')\n",
      "Iteration 10960 Training loss 0.06183481961488724 Validation loss 0.06445317715406418 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.8558],\n",
      "        [0.6100]], device='mps:0')\n",
      "Iteration 10970 Training loss 0.06978140771389008 Validation loss 0.06452222913503647 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.9351],\n",
      "        [0.2832]], device='mps:0')\n",
      "Iteration 10980 Training loss 0.0682700052857399 Validation loss 0.06447624415159225 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.2868],\n",
      "        [0.5448]], device='mps:0')\n",
      "Iteration 10990 Training loss 0.06084662303328514 Validation loss 0.06447795778512955 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.8908],\n",
      "        [0.8371]], device='mps:0')\n",
      "Iteration 11000 Training loss 0.06470242142677307 Validation loss 0.06447688490152359 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.6297],\n",
      "        [0.1832]], device='mps:0')\n",
      "Iteration 11010 Training loss 0.06616925448179245 Validation loss 0.06443936377763748 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.0936],\n",
      "        [0.1918]], device='mps:0')\n",
      "Iteration 11020 Training loss 0.0702589675784111 Validation loss 0.06448820978403091 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.6945],\n",
      "        [0.8240]], device='mps:0')\n",
      "Iteration 11030 Training loss 0.06593243777751923 Validation loss 0.06443358212709427 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.5943],\n",
      "        [0.3561]], device='mps:0')\n",
      "Iteration 11040 Training loss 0.07893934100866318 Validation loss 0.06453797966241837 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.6232],\n",
      "        [0.6829]], device='mps:0')\n",
      "Iteration 11050 Training loss 0.06269464641809464 Validation loss 0.064664825797081 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.1713],\n",
      "        [0.3402]], device='mps:0')\n",
      "Iteration 11060 Training loss 0.06554251909255981 Validation loss 0.06478793174028397 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.0816],\n",
      "        [0.2866]], device='mps:0')\n",
      "Iteration 11070 Training loss 0.05859857052564621 Validation loss 0.0645996555685997 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.4252],\n",
      "        [0.7856]], device='mps:0')\n",
      "Iteration 11080 Training loss 0.05495521426200867 Validation loss 0.06440143287181854 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.8193],\n",
      "        [0.8321]], device='mps:0')\n",
      "Iteration 11090 Training loss 0.0685923770070076 Validation loss 0.0644255131483078 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.2919],\n",
      "        [0.0172]], device='mps:0')\n",
      "Iteration 11100 Training loss 0.06482715159654617 Validation loss 0.0644443929195404 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.7730],\n",
      "        [0.7097]], device='mps:0')\n",
      "Iteration 11110 Training loss 0.0631546676158905 Validation loss 0.0643921047449112 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.1897],\n",
      "        [0.2963]], device='mps:0')\n",
      "Iteration 11120 Training loss 0.07022978365421295 Validation loss 0.06443461030721664 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.7826],\n",
      "        [0.2816]], device='mps:0')\n",
      "Iteration 11130 Training loss 0.06786631792783737 Validation loss 0.06439290195703506 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.5036],\n",
      "        [0.4009]], device='mps:0')\n",
      "Iteration 11140 Training loss 0.06270911544561386 Validation loss 0.06437500566244125 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.9360],\n",
      "        [0.8920]], device='mps:0')\n",
      "Iteration 11150 Training loss 0.06263117492198944 Validation loss 0.06437256932258606 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.0490],\n",
      "        [0.5236]], device='mps:0')\n",
      "Iteration 11160 Training loss 0.06278089433908463 Validation loss 0.06438611447811127 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.5437],\n",
      "        [0.3051]], device='mps:0')\n",
      "Iteration 11170 Training loss 0.06795155256986618 Validation loss 0.06448093056678772 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.4769],\n",
      "        [0.6324]], device='mps:0')\n",
      "Iteration 11180 Training loss 0.06974305212497711 Validation loss 0.06435646116733551 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.1607],\n",
      "        [0.0353]], device='mps:0')\n",
      "Iteration 11190 Training loss 0.06555431336164474 Validation loss 0.06433944404125214 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.0441],\n",
      "        [0.8805]], device='mps:0')\n",
      "Iteration 11200 Training loss 0.0730726346373558 Validation loss 0.06454167515039444 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.9052],\n",
      "        [0.6449]], device='mps:0')\n",
      "Iteration 11210 Training loss 0.0577569417655468 Validation loss 0.06449121981859207 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.1460],\n",
      "        [0.1685]], device='mps:0')\n",
      "Iteration 11220 Training loss 0.063850998878479 Validation loss 0.06449241936206818 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.4825],\n",
      "        [0.4254]], device='mps:0')\n",
      "Iteration 11230 Training loss 0.058642104268074036 Validation loss 0.06434345990419388 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.9412],\n",
      "        [0.3932]], device='mps:0')\n",
      "Iteration 11240 Training loss 0.06458944082260132 Validation loss 0.06434622406959534 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.4353],\n",
      "        [0.8568]], device='mps:0')\n",
      "Iteration 11250 Training loss 0.059835661202669144 Validation loss 0.06434638053178787 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.4498],\n",
      "        [0.2468]], device='mps:0')\n",
      "Iteration 11260 Training loss 0.06226421147584915 Validation loss 0.06434421986341476 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.9881],\n",
      "        [0.3687]], device='mps:0')\n",
      "Iteration 11270 Training loss 0.0553436279296875 Validation loss 0.06435194611549377 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.3685],\n",
      "        [0.7784]], device='mps:0')\n",
      "Iteration 11280 Training loss 0.05823859944939613 Validation loss 0.0645778551697731 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.5591],\n",
      "        [0.2791]], device='mps:0')\n",
      "Iteration 11290 Training loss 0.0660664513707161 Validation loss 0.06433846056461334 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.0114],\n",
      "        [0.9706]], device='mps:0')\n",
      "Iteration 11300 Training loss 0.06311327964067459 Validation loss 0.064490906894207 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.4635],\n",
      "        [0.7218]], device='mps:0')\n",
      "Iteration 11310 Training loss 0.060457419604063034 Validation loss 0.06434054672718048 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.9466],\n",
      "        [0.9901]], device='mps:0')\n",
      "Iteration 11320 Training loss 0.05994639918208122 Validation loss 0.06443865597248077 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.1705],\n",
      "        [0.9485]], device='mps:0')\n",
      "Iteration 11330 Training loss 0.06096596643328667 Validation loss 0.06433454900979996 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.7607],\n",
      "        [0.2723]], device='mps:0')\n",
      "Iteration 11340 Training loss 0.058675944805145264 Validation loss 0.0644320547580719 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.2567],\n",
      "        [0.6973]], device='mps:0')\n",
      "Iteration 11350 Training loss 0.06265150755643845 Validation loss 0.06431563198566437 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.1587],\n",
      "        [0.1796]], device='mps:0')\n",
      "Iteration 11360 Training loss 0.06606630980968475 Validation loss 0.06433109194040298 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.0703],\n",
      "        [0.8556]], device='mps:0')\n",
      "Iteration 11370 Training loss 0.05909724161028862 Validation loss 0.06453007459640503 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.2929],\n",
      "        [0.0950]], device='mps:0')\n",
      "Iteration 11380 Training loss 0.06708462536334991 Validation loss 0.06428311765193939 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.4362],\n",
      "        [0.1595]], device='mps:0')\n",
      "Iteration 11390 Training loss 0.05612080544233322 Validation loss 0.06428932398557663 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.3880],\n",
      "        [0.7777]], device='mps:0')\n",
      "Iteration 11400 Training loss 0.06871689856052399 Validation loss 0.06452368199825287 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.5055],\n",
      "        [0.1393]], device='mps:0')\n",
      "Iteration 11410 Training loss 0.06619425117969513 Validation loss 0.06454260647296906 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.9898],\n",
      "        [0.2094]], device='mps:0')\n",
      "Iteration 11420 Training loss 0.06324510276317596 Validation loss 0.06443864107131958 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.8832],\n",
      "        [0.9022]], device='mps:0')\n",
      "Iteration 11430 Training loss 0.05829562991857529 Validation loss 0.06432998925447464 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.4274],\n",
      "        [0.2183]], device='mps:0')\n",
      "Iteration 11440 Training loss 0.06372541934251785 Validation loss 0.06432206928730011 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.7490],\n",
      "        [0.8751]], device='mps:0')\n",
      "Iteration 11450 Training loss 0.0705816000699997 Validation loss 0.06427652388811111 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.8252],\n",
      "        [0.1561]], device='mps:0')\n",
      "Iteration 11460 Training loss 0.07728695869445801 Validation loss 0.06439463794231415 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.2910],\n",
      "        [0.3526]], device='mps:0')\n",
      "Iteration 11470 Training loss 0.06183312460780144 Validation loss 0.06425122916698456 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.1311],\n",
      "        [0.0349]], device='mps:0')\n",
      "Iteration 11480 Training loss 0.060822054743766785 Validation loss 0.06425826251506805 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.3103],\n",
      "        [0.9166]], device='mps:0')\n",
      "Iteration 11490 Training loss 0.054824963212013245 Validation loss 0.06436733156442642 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.2218],\n",
      "        [0.8435]], device='mps:0')\n",
      "Iteration 11500 Training loss 0.05715217441320419 Validation loss 0.06425053626298904 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.2969],\n",
      "        [0.9893]], device='mps:0')\n",
      "Iteration 11510 Training loss 0.0676889717578888 Validation loss 0.06433053314685822 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.4173],\n",
      "        [0.7947]], device='mps:0')\n",
      "Iteration 11520 Training loss 0.061607539653778076 Validation loss 0.06422591954469681 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.3910],\n",
      "        [0.0182]], device='mps:0')\n",
      "Iteration 11530 Training loss 0.06826943904161453 Validation loss 0.06424573063850403 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.0841],\n",
      "        [0.4667]], device='mps:0')\n",
      "Iteration 11540 Training loss 0.05870497599244118 Validation loss 0.0642220750451088 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.4059],\n",
      "        [0.2764]], device='mps:0')\n",
      "Iteration 11550 Training loss 0.07493117451667786 Validation loss 0.06422651559114456 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.9946],\n",
      "        [0.0216]], device='mps:0')\n",
      "Iteration 11560 Training loss 0.06322233378887177 Validation loss 0.06423921138048172 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.8765],\n",
      "        [0.2544]], device='mps:0')\n",
      "Iteration 11570 Training loss 0.05872209370136261 Validation loss 0.06420257687568665 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9961],\n",
      "        [0.7220]], device='mps:0')\n",
      "Iteration 11580 Training loss 0.05787679925560951 Validation loss 0.0642184317111969 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.7254],\n",
      "        [0.7308]], device='mps:0')\n",
      "Iteration 11590 Training loss 0.06369421631097794 Validation loss 0.06445629894733429 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.0764],\n",
      "        [0.4256]], device='mps:0')\n",
      "Iteration 11600 Training loss 0.06577190011739731 Validation loss 0.06431914865970612 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.9345],\n",
      "        [0.0113]], device='mps:0')\n",
      "Iteration 11610 Training loss 0.0669342502951622 Validation loss 0.06428399682044983 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.5493],\n",
      "        [0.8302]], device='mps:0')\n",
      "Iteration 11620 Training loss 0.05880079045891762 Validation loss 0.06423769891262054 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.8232],\n",
      "        [0.0337]], device='mps:0')\n",
      "Iteration 11630 Training loss 0.07101345807313919 Validation loss 0.06462424248456955 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.2323],\n",
      "        [0.2479]], device='mps:0')\n",
      "Iteration 11640 Training loss 0.05492524430155754 Validation loss 0.06437698751688004 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.7871],\n",
      "        [0.0104]], device='mps:0')\n",
      "Iteration 11650 Training loss 0.06459023803472519 Validation loss 0.06418802589178085 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.0366],\n",
      "        [0.6625]], device='mps:0')\n",
      "Iteration 11660 Training loss 0.0592944361269474 Validation loss 0.06417615711688995 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.9911],\n",
      "        [0.0854]], device='mps:0')\n",
      "Iteration 11670 Training loss 0.06895051151514053 Validation loss 0.0641636848449707 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.4922],\n",
      "        [0.5950]], device='mps:0')\n",
      "Iteration 11680 Training loss 0.062473952770233154 Validation loss 0.06443312019109726 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.4724],\n",
      "        [0.1850]], device='mps:0')\n",
      "Iteration 11690 Training loss 0.06683128327131271 Validation loss 0.06422418355941772 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.8032],\n",
      "        [0.1192]], device='mps:0')\n",
      "Iteration 11700 Training loss 0.06176016852259636 Validation loss 0.06416281312704086 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.9185],\n",
      "        [0.0624]], device='mps:0')\n",
      "Iteration 11710 Training loss 0.056661128997802734 Validation loss 0.06420589238405228 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.1030],\n",
      "        [0.0332]], device='mps:0')\n",
      "Iteration 11720 Training loss 0.06455378979444504 Validation loss 0.06425444781780243 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.4504],\n",
      "        [0.5475]], device='mps:0')\n",
      "Iteration 11730 Training loss 0.06761609017848969 Validation loss 0.06416463851928711 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.9188],\n",
      "        [0.8840]], device='mps:0')\n",
      "Iteration 11740 Training loss 0.07421884685754776 Validation loss 0.06415293365716934 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.0706],\n",
      "        [0.1253]], device='mps:0')\n",
      "Iteration 11750 Training loss 0.06192486733198166 Validation loss 0.06414367258548737 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.2156],\n",
      "        [0.2285]], device='mps:0')\n",
      "Iteration 11760 Training loss 0.06479226052761078 Validation loss 0.0642159953713417 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.1265],\n",
      "        [0.9789]], device='mps:0')\n",
      "Iteration 11770 Training loss 0.07649659365415573 Validation loss 0.06412573903799057 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.8635],\n",
      "        [0.0505]], device='mps:0')\n",
      "Iteration 11780 Training loss 0.06812150031328201 Validation loss 0.06412149965763092 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.5128],\n",
      "        [0.1710]], device='mps:0')\n",
      "Iteration 11790 Training loss 0.06607852131128311 Validation loss 0.06411927193403244 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.0096],\n",
      "        [0.8662]], device='mps:0')\n",
      "Iteration 11800 Training loss 0.06900815665721893 Validation loss 0.06444990634918213 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.0154],\n",
      "        [0.9441]], device='mps:0')\n",
      "Iteration 11810 Training loss 0.06724850833415985 Validation loss 0.06421814113855362 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.8563],\n",
      "        [0.8377]], device='mps:0')\n",
      "Iteration 11820 Training loss 0.055181797593832016 Validation loss 0.06418684870004654 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.4769],\n",
      "        [0.8362]], device='mps:0')\n",
      "Iteration 11830 Training loss 0.06886710971593857 Validation loss 0.0641186535358429 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.7956],\n",
      "        [0.0975]], device='mps:0')\n",
      "Iteration 11840 Training loss 0.06607910245656967 Validation loss 0.06409889459609985 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.9571],\n",
      "        [0.1305]], device='mps:0')\n",
      "Iteration 11850 Training loss 0.06136364862322807 Validation loss 0.06409209966659546 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.1000],\n",
      "        [0.0604]], device='mps:0')\n",
      "Iteration 11860 Training loss 0.06358306109905243 Validation loss 0.06409598141908646 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.4806],\n",
      "        [0.9275]], device='mps:0')\n",
      "Iteration 11870 Training loss 0.060918111354112625 Validation loss 0.06408277899026871 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.9515],\n",
      "        [0.0960]], device='mps:0')\n",
      "Iteration 11880 Training loss 0.06221962347626686 Validation loss 0.06414756178855896 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.6253],\n",
      "        [0.9719]], device='mps:0')\n",
      "Iteration 11890 Training loss 0.0640270784497261 Validation loss 0.06407704204320908 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.9439],\n",
      "        [0.9613]], device='mps:0')\n",
      "Iteration 11900 Training loss 0.06476534157991409 Validation loss 0.06409505009651184 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.6337],\n",
      "        [0.7989]], device='mps:0')\n",
      "Iteration 11910 Training loss 0.06102075055241585 Validation loss 0.0640607476234436 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.9208],\n",
      "        [0.6872]], device='mps:0')\n",
      "Iteration 11920 Training loss 0.0698174387216568 Validation loss 0.06430120021104813 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.6893],\n",
      "        [0.8837]], device='mps:0')\n",
      "Iteration 11930 Training loss 0.056728556752204895 Validation loss 0.06408053636550903 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.1550],\n",
      "        [0.2610]], device='mps:0')\n",
      "Iteration 11940 Training loss 0.05997467413544655 Validation loss 0.06406649202108383 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9574],\n",
      "        [0.0341]], device='mps:0')\n",
      "Iteration 11950 Training loss 0.06913662701845169 Validation loss 0.06417623162269592 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.2508],\n",
      "        [0.1758]], device='mps:0')\n",
      "Iteration 11960 Training loss 0.06443943083286285 Validation loss 0.06405288726091385 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.8519],\n",
      "        [0.1424]], device='mps:0')\n",
      "Iteration 11970 Training loss 0.062065739184617996 Validation loss 0.06441187113523483 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.1857],\n",
      "        [0.5207]], device='mps:0')\n",
      "Iteration 11980 Training loss 0.0613677091896534 Validation loss 0.06421326100826263 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.0069],\n",
      "        [0.3901]], device='mps:0')\n",
      "Iteration 11990 Training loss 0.06643867492675781 Validation loss 0.06408336013555527 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.7870],\n",
      "        [0.8911]], device='mps:0')\n",
      "Iteration 12000 Training loss 0.06567275524139404 Validation loss 0.06404300779104233 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.7803],\n",
      "        [0.9009]], device='mps:0')\n",
      "Iteration 12010 Training loss 0.0627647191286087 Validation loss 0.06403782218694687 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.2081],\n",
      "        [0.8153]], device='mps:0')\n",
      "Iteration 12020 Training loss 0.07382135093212128 Validation loss 0.06403644382953644 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.6810],\n",
      "        [0.7913]], device='mps:0')\n",
      "Iteration 12030 Training loss 0.06837575882673264 Validation loss 0.06428326666355133 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.3698],\n",
      "        [0.5191]], device='mps:0')\n",
      "Iteration 12040 Training loss 0.059329163283109665 Validation loss 0.06412450224161148 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.9421],\n",
      "        [0.4762]], device='mps:0')\n",
      "Iteration 12050 Training loss 0.05930495262145996 Validation loss 0.06408596783876419 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.7901],\n",
      "        [0.8550]], device='mps:0')\n",
      "Iteration 12060 Training loss 0.05268925055861473 Validation loss 0.06406742334365845 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.1985],\n",
      "        [0.2766]], device='mps:0')\n",
      "Iteration 12070 Training loss 0.062035128474235535 Validation loss 0.06405553221702576 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.0341],\n",
      "        [0.8757]], device='mps:0')\n",
      "Iteration 12080 Training loss 0.06351321935653687 Validation loss 0.06402169913053513 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.9439],\n",
      "        [0.0676]], device='mps:0')\n",
      "Iteration 12090 Training loss 0.06394888460636139 Validation loss 0.06405649334192276 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.2062],\n",
      "        [0.8752]], device='mps:0')\n",
      "Iteration 12100 Training loss 0.06418982148170471 Validation loss 0.06401731818914413 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.9670],\n",
      "        [0.7420]], device='mps:0')\n",
      "Iteration 12110 Training loss 0.0586359016597271 Validation loss 0.0640091821551323 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.7522],\n",
      "        [0.0494]], device='mps:0')\n",
      "Iteration 12120 Training loss 0.06575103104114532 Validation loss 0.06401455402374268 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.9331],\n",
      "        [0.2755]], device='mps:0')\n",
      "Iteration 12130 Training loss 0.061411842703819275 Validation loss 0.06404025852680206 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.8019],\n",
      "        [0.9296]], device='mps:0')\n",
      "Iteration 12140 Training loss 0.06442558020353317 Validation loss 0.06398709863424301 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.7229],\n",
      "        [0.6477]], device='mps:0')\n",
      "Iteration 12150 Training loss 0.06607267260551453 Validation loss 0.0643080323934555 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.4786],\n",
      "        [0.9038]], device='mps:0')\n",
      "Iteration 12160 Training loss 0.0713452622294426 Validation loss 0.06400731950998306 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.0148],\n",
      "        [0.6406]], device='mps:0')\n",
      "Iteration 12170 Training loss 0.06200893223285675 Validation loss 0.06398925930261612 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.4301],\n",
      "        [0.7499]], device='mps:0')\n",
      "Iteration 12180 Training loss 0.06572974473237991 Validation loss 0.06402800232172012 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.4834],\n",
      "        [0.0780]], device='mps:0')\n",
      "Iteration 12190 Training loss 0.055683739483356476 Validation loss 0.06399893015623093 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.7122],\n",
      "        [0.0962]], device='mps:0')\n",
      "Iteration 12200 Training loss 0.059736479073762894 Validation loss 0.06398218125104904 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.3896],\n",
      "        [0.9085]], device='mps:0')\n",
      "Iteration 12210 Training loss 0.057621944695711136 Validation loss 0.06409876048564911 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.4532],\n",
      "        [0.7336]], device='mps:0')\n",
      "Iteration 12220 Training loss 0.06474132090806961 Validation loss 0.06398028135299683 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.8270],\n",
      "        [0.1816]], device='mps:0')\n",
      "Iteration 12230 Training loss 0.06727521121501923 Validation loss 0.06399980932474136 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.8984],\n",
      "        [0.8970]], device='mps:0')\n",
      "Iteration 12240 Training loss 0.06375651061534882 Validation loss 0.06398861855268478 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.8400],\n",
      "        [0.7999]], device='mps:0')\n",
      "Iteration 12250 Training loss 0.06283199042081833 Validation loss 0.06405240297317505 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.2407],\n",
      "        [0.8856]], device='mps:0')\n",
      "Iteration 12260 Training loss 0.05973787233233452 Validation loss 0.06400445848703384 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.8693],\n",
      "        [0.9405]], device='mps:0')\n",
      "Iteration 12270 Training loss 0.0686625987291336 Validation loss 0.06407693773508072 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.8615],\n",
      "        [0.0583]], device='mps:0')\n",
      "Iteration 12280 Training loss 0.06063426658511162 Validation loss 0.06398780643939972 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.2374],\n",
      "        [0.7740]], device='mps:0')\n",
      "Iteration 12290 Training loss 0.0506841279566288 Validation loss 0.06409013271331787 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.2315],\n",
      "        [0.6069]], device='mps:0')\n",
      "Iteration 12300 Training loss 0.06681258976459503 Validation loss 0.06402357667684555 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.1402],\n",
      "        [0.1176]], device='mps:0')\n",
      "Iteration 12310 Training loss 0.06584243476390839 Validation loss 0.06427609920501709 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.5468],\n",
      "        [0.2753]], device='mps:0')\n",
      "Iteration 12320 Training loss 0.0601215735077858 Validation loss 0.06395085901021957 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.9838],\n",
      "        [0.5478]], device='mps:0')\n",
      "Iteration 12330 Training loss 0.06319558620452881 Validation loss 0.06405835598707199 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.1210],\n",
      "        [0.3072]], device='mps:0')\n",
      "Iteration 12340 Training loss 0.0631023719906807 Validation loss 0.0639527216553688 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.4998],\n",
      "        [0.9632]], device='mps:0')\n",
      "Iteration 12350 Training loss 0.06226037070155144 Validation loss 0.06396180391311646 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9864],\n",
      "        [0.4639]], device='mps:0')\n",
      "Iteration 12360 Training loss 0.0631769672036171 Validation loss 0.06413493305444717 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.1819],\n",
      "        [0.9032]], device='mps:0')\n",
      "Iteration 12370 Training loss 0.055739205330610275 Validation loss 0.06394030153751373 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.5237],\n",
      "        [0.2261]], device='mps:0')\n",
      "Iteration 12380 Training loss 0.05881735682487488 Validation loss 0.06401196867227554 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.8624],\n",
      "        [0.7953]], device='mps:0')\n",
      "Iteration 12390 Training loss 0.06165041774511337 Validation loss 0.06413146108388901 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.6835],\n",
      "        [0.8057]], device='mps:0')\n",
      "Iteration 12400 Training loss 0.06539443880319595 Validation loss 0.06406674534082413 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0209],\n",
      "        [0.7800]], device='mps:0')\n",
      "Iteration 12410 Training loss 0.060570601373910904 Validation loss 0.06404273957014084 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.5764],\n",
      "        [0.9835]], device='mps:0')\n",
      "Iteration 12420 Training loss 0.05460922047495842 Validation loss 0.06439192593097687 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.0675],\n",
      "        [0.6430]], device='mps:0')\n",
      "Iteration 12430 Training loss 0.06070452556014061 Validation loss 0.06392399966716766 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.1652],\n",
      "        [0.4478]], device='mps:0')\n",
      "Iteration 12440 Training loss 0.06472202390432358 Validation loss 0.0640118271112442 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.1030],\n",
      "        [0.1415]], device='mps:0')\n",
      "Iteration 12450 Training loss 0.06085871532559395 Validation loss 0.06401766091585159 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.8979],\n",
      "        [0.0891]], device='mps:0')\n",
      "Iteration 12460 Training loss 0.0661599263548851 Validation loss 0.0640135407447815 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0733],\n",
      "        [0.8887]], device='mps:0')\n",
      "Iteration 12470 Training loss 0.06393687427043915 Validation loss 0.06390932947397232 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.1153],\n",
      "        [0.7446]], device='mps:0')\n",
      "Iteration 12480 Training loss 0.06400027871131897 Validation loss 0.06396161019802094 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.0942],\n",
      "        [0.8540]], device='mps:0')\n",
      "Iteration 12490 Training loss 0.06335426867008209 Validation loss 0.06391438841819763 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.9204],\n",
      "        [0.9716]], device='mps:0')\n",
      "Iteration 12500 Training loss 0.06336317956447601 Validation loss 0.06399077922105789 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.6882],\n",
      "        [0.0734]], device='mps:0')\n",
      "Iteration 12510 Training loss 0.06554199010133743 Validation loss 0.06392594426870346 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.9700],\n",
      "        [0.9013]], device='mps:0')\n",
      "Iteration 12520 Training loss 0.06798171997070312 Validation loss 0.0639406219124794 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.1777],\n",
      "        [0.0156]], device='mps:0')\n",
      "Iteration 12530 Training loss 0.061627425253391266 Validation loss 0.06389765441417694 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.0516],\n",
      "        [0.1579]], device='mps:0')\n",
      "Iteration 12540 Training loss 0.058167990297079086 Validation loss 0.06395778805017471 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.1418],\n",
      "        [0.0806]], device='mps:0')\n",
      "Iteration 12550 Training loss 0.06538499891757965 Validation loss 0.0641188770532608 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.4599],\n",
      "        [0.9146]], device='mps:0')\n",
      "Iteration 12560 Training loss 0.06881321966648102 Validation loss 0.06387355178594589 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.0180],\n",
      "        [0.1080]], device='mps:0')\n",
      "Iteration 12570 Training loss 0.061454132199287415 Validation loss 0.06388840824365616 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.8590],\n",
      "        [0.9037]], device='mps:0')\n",
      "Iteration 12580 Training loss 0.07010655850172043 Validation loss 0.06399528682231903 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.6274],\n",
      "        [0.9763]], device='mps:0')\n",
      "Iteration 12590 Training loss 0.06313192844390869 Validation loss 0.0639934316277504 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.2302],\n",
      "        [0.1050]], device='mps:0')\n",
      "Iteration 12600 Training loss 0.06924299895763397 Validation loss 0.06385435163974762 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.8010],\n",
      "        [0.3545]], device='mps:0')\n",
      "Iteration 12610 Training loss 0.05907103419303894 Validation loss 0.06416171789169312 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.6192],\n",
      "        [0.2510]], device='mps:0')\n",
      "Iteration 12620 Training loss 0.06316059827804565 Validation loss 0.06400797516107559 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.7410],\n",
      "        [0.6384]], device='mps:0')\n",
      "Iteration 12630 Training loss 0.07222233712673187 Validation loss 0.06397496908903122 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.0554],\n",
      "        [0.7614]], device='mps:0')\n",
      "Iteration 12640 Training loss 0.06026795133948326 Validation loss 0.06424534320831299 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.3788],\n",
      "        [0.9204]], device='mps:0')\n",
      "Iteration 12650 Training loss 0.06003635376691818 Validation loss 0.0638275146484375 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.0266],\n",
      "        [0.9470]], device='mps:0')\n",
      "Iteration 12660 Training loss 0.06552772223949432 Validation loss 0.0640680193901062 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.8403],\n",
      "        [0.9137]], device='mps:0')\n",
      "Iteration 12670 Training loss 0.06470397114753723 Validation loss 0.06381671130657196 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.8758],\n",
      "        [0.1339]], device='mps:0')\n",
      "Iteration 12680 Training loss 0.06092962250113487 Validation loss 0.06399179250001907 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.9365],\n",
      "        [0.0983]], device='mps:0')\n",
      "Iteration 12690 Training loss 0.06372705101966858 Validation loss 0.06390813738107681 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.1274],\n",
      "        [0.0232]], device='mps:0')\n",
      "Iteration 12700 Training loss 0.0643143281340599 Validation loss 0.06387286633253098 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.3620],\n",
      "        [0.7200]], device='mps:0')\n",
      "Iteration 12710 Training loss 0.05889344587922096 Validation loss 0.06383557617664337 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.5282],\n",
      "        [0.3720]], device='mps:0')\n",
      "Iteration 12720 Training loss 0.052018776535987854 Validation loss 0.06391377002000809 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.8179],\n",
      "        [0.2828]], device='mps:0')\n",
      "Iteration 12730 Training loss 0.06049931421875954 Validation loss 0.06379969418048859 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.0268],\n",
      "        [0.6832]], device='mps:0')\n",
      "Iteration 12740 Training loss 0.05686401203274727 Validation loss 0.06381906569004059 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.0379],\n",
      "        [0.7865]], device='mps:0')\n",
      "Iteration 12750 Training loss 0.06853242218494415 Validation loss 0.06392160058021545 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.1780],\n",
      "        [0.0500]], device='mps:0')\n",
      "Iteration 12760 Training loss 0.06201121211051941 Validation loss 0.06386525928974152 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.2808],\n",
      "        [0.8675]], device='mps:0')\n",
      "Iteration 12770 Training loss 0.06393783539533615 Validation loss 0.06404872983694077 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.9525],\n",
      "        [0.6210]], device='mps:0')\n",
      "Iteration 12780 Training loss 0.06778386235237122 Validation loss 0.06386326253414154 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.6668],\n",
      "        [0.5195]], device='mps:0')\n",
      "Iteration 12790 Training loss 0.05426511541008949 Validation loss 0.06377559900283813 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.8049],\n",
      "        [0.5248]], device='mps:0')\n",
      "Iteration 12800 Training loss 0.060422610491514206 Validation loss 0.06377261132001877 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.7865],\n",
      "        [0.9751]], device='mps:0')\n",
      "Iteration 12810 Training loss 0.070551797747612 Validation loss 0.06376940757036209 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.9179],\n",
      "        [0.9795]], device='mps:0')\n",
      "Iteration 12820 Training loss 0.06159566715359688 Validation loss 0.06384361535310745 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.6396],\n",
      "        [0.5994]], device='mps:0')\n",
      "Iteration 12830 Training loss 0.06007539480924606 Validation loss 0.06378638744354248 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.6901],\n",
      "        [0.1032]], device='mps:0')\n",
      "Iteration 12840 Training loss 0.06406504660844803 Validation loss 0.0639297217130661 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9195],\n",
      "        [0.1704]], device='mps:0')\n",
      "Iteration 12850 Training loss 0.05764182284474373 Validation loss 0.06383133679628372 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0216],\n",
      "        [0.0661]], device='mps:0')\n",
      "Iteration 12860 Training loss 0.05696006119251251 Validation loss 0.06456912308931351 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.8052],\n",
      "        [0.8789]], device='mps:0')\n",
      "Iteration 12870 Training loss 0.06596100330352783 Validation loss 0.06384537369012833 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.3363],\n",
      "        [0.4069]], device='mps:0')\n",
      "Iteration 12880 Training loss 0.0636112317442894 Validation loss 0.0637444257736206 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.0433],\n",
      "        [0.1074]], device='mps:0')\n",
      "Iteration 12890 Training loss 0.05980054289102554 Validation loss 0.063739113509655 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9545],\n",
      "        [0.1866]], device='mps:0')\n",
      "Iteration 12900 Training loss 0.06742542237043381 Validation loss 0.06373780965805054 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.9867],\n",
      "        [0.3453]], device='mps:0')\n",
      "Iteration 12910 Training loss 0.06673793494701385 Validation loss 0.06374148279428482 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.2192],\n",
      "        [0.7942]], device='mps:0')\n",
      "Iteration 12920 Training loss 0.06537976115942001 Validation loss 0.06373363733291626 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.8549],\n",
      "        [0.1596]], device='mps:0')\n",
      "Iteration 12930 Training loss 0.06086314469575882 Validation loss 0.0637148916721344 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.0592],\n",
      "        [0.3381]], device='mps:0')\n",
      "Iteration 12940 Training loss 0.053471460938453674 Validation loss 0.06380823999643326 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.4847],\n",
      "        [0.9513]], device='mps:0')\n",
      "Iteration 12950 Training loss 0.06570446491241455 Validation loss 0.06371589004993439 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.1468],\n",
      "        [0.2116]], device='mps:0')\n",
      "Iteration 12960 Training loss 0.05616285279393196 Validation loss 0.06375762075185776 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.8397],\n",
      "        [0.9698]], device='mps:0')\n",
      "Iteration 12970 Training loss 0.05601470172405243 Validation loss 0.06371545791625977 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.5698],\n",
      "        [0.5899]], device='mps:0')\n",
      "Iteration 12980 Training loss 0.055518582463264465 Validation loss 0.06373506039381027 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.7444],\n",
      "        [0.7412]], device='mps:0')\n",
      "Iteration 12990 Training loss 0.06119898706674576 Validation loss 0.06369584053754807 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.1585],\n",
      "        [0.0723]], device='mps:0')\n",
      "Iteration 13000 Training loss 0.060742590576410294 Validation loss 0.06407172232866287 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.9032],\n",
      "        [0.2186]], device='mps:0')\n",
      "Iteration 13010 Training loss 0.06046290323138237 Validation loss 0.0636921152472496 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.5907],\n",
      "        [0.6821]], device='mps:0')\n",
      "Iteration 13020 Training loss 0.06299348175525665 Validation loss 0.06368447840213776 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.8151],\n",
      "        [0.3048]], device='mps:0')\n",
      "Iteration 13030 Training loss 0.059255752712488174 Validation loss 0.06369464099407196 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.6943],\n",
      "        [0.9538]], device='mps:0')\n",
      "Iteration 13040 Training loss 0.06183873116970062 Validation loss 0.06379891186952591 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.2593],\n",
      "        [0.8430]], device='mps:0')\n",
      "Iteration 13050 Training loss 0.06648661941289902 Validation loss 0.06367398798465729 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.6340],\n",
      "        [0.9440]], device='mps:0')\n",
      "Iteration 13060 Training loss 0.06267967075109482 Validation loss 0.06377553194761276 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.9845],\n",
      "        [0.4586]], device='mps:0')\n",
      "Iteration 13070 Training loss 0.06300964951515198 Validation loss 0.06366333365440369 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.2776],\n",
      "        [0.0852]], device='mps:0')\n",
      "Iteration 13080 Training loss 0.06365609914064407 Validation loss 0.0636613741517067 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.8432],\n",
      "        [0.6613]], device='mps:0')\n",
      "Iteration 13090 Training loss 0.06397901475429535 Validation loss 0.06365958601236343 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.9279],\n",
      "        [0.7423]], device='mps:0')\n",
      "Iteration 13100 Training loss 0.06330625712871552 Validation loss 0.06366501748561859 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.2318],\n",
      "        [0.6053]], device='mps:0')\n",
      "Iteration 13110 Training loss 0.07063223421573639 Validation loss 0.06367476284503937 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.9800],\n",
      "        [0.1522]], device='mps:0')\n",
      "Iteration 13120 Training loss 0.05680660903453827 Validation loss 0.06364217400550842 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.8014],\n",
      "        [0.7426]], device='mps:0')\n",
      "Iteration 13130 Training loss 0.06543207913637161 Validation loss 0.06365559250116348 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.0834],\n",
      "        [0.1866]], device='mps:0')\n",
      "Iteration 13140 Training loss 0.065567746758461 Validation loss 0.06364022195339203 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.3019],\n",
      "        [0.5437]], device='mps:0')\n",
      "Iteration 13150 Training loss 0.058761946856975555 Validation loss 0.06420731544494629 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.2287],\n",
      "        [0.9604]], device='mps:0')\n",
      "Iteration 13160 Training loss 0.06179192289710045 Validation loss 0.06367133557796478 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.8428],\n",
      "        [0.0714]], device='mps:0')\n",
      "Iteration 13170 Training loss 0.06771907955408096 Validation loss 0.06392031908035278 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.2956],\n",
      "        [0.2514]], device='mps:0')\n",
      "Iteration 13180 Training loss 0.05923342704772949 Validation loss 0.06365154683589935 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.1704],\n",
      "        [0.8714]], device='mps:0')\n",
      "Iteration 13190 Training loss 0.06463828682899475 Validation loss 0.06365399062633514 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.8715],\n",
      "        [0.5712]], device='mps:0')\n",
      "Iteration 13200 Training loss 0.0628485158085823 Validation loss 0.06363046914339066 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.7871],\n",
      "        [0.1331]], device='mps:0')\n",
      "Iteration 13210 Training loss 0.05193645507097244 Validation loss 0.06401185691356659 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.4178],\n",
      "        [0.5204]], device='mps:0')\n",
      "Iteration 13220 Training loss 0.05696266144514084 Validation loss 0.06362398713827133 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.8614],\n",
      "        [0.6498]], device='mps:0')\n",
      "Iteration 13230 Training loss 0.061415575444698334 Validation loss 0.06374195963144302 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.3444],\n",
      "        [0.9198]], device='mps:0')\n",
      "Iteration 13240 Training loss 0.06168283149600029 Validation loss 0.06361398100852966 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.8833],\n",
      "        [0.1769]], device='mps:0')\n",
      "Iteration 13250 Training loss 0.053772877901792526 Validation loss 0.06362169981002808 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.1802],\n",
      "        [0.9617]], device='mps:0')\n",
      "Iteration 13260 Training loss 0.06007922813296318 Validation loss 0.06361179053783417 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.1108],\n",
      "        [0.1229]], device='mps:0')\n",
      "Iteration 13270 Training loss 0.05576813220977783 Validation loss 0.06368013471364975 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.6064],\n",
      "        [0.7104]], device='mps:0')\n",
      "Iteration 13280 Training loss 0.05422003194689751 Validation loss 0.06369254738092422 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.1255],\n",
      "        [0.9900]], device='mps:0')\n",
      "Iteration 13290 Training loss 0.06903325021266937 Validation loss 0.06372395902872086 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.9610],\n",
      "        [0.2380]], device='mps:0')\n",
      "Iteration 13300 Training loss 0.05998937040567398 Validation loss 0.06364817172288895 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.8442],\n",
      "        [0.9452]], device='mps:0')\n",
      "Iteration 13310 Training loss 0.06349944323301315 Validation loss 0.06363317370414734 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.5911],\n",
      "        [0.1276]], device='mps:0')\n",
      "Iteration 13320 Training loss 0.05930432677268982 Validation loss 0.06378977745771408 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.6578],\n",
      "        [0.2986]], device='mps:0')\n",
      "Iteration 13330 Training loss 0.06629709899425507 Validation loss 0.06359745562076569 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0535],\n",
      "        [0.4814]], device='mps:0')\n",
      "Iteration 13340 Training loss 0.06360094249248505 Validation loss 0.06363033503293991 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.9860],\n",
      "        [0.0696]], device='mps:0')\n",
      "Iteration 13350 Training loss 0.05748865008354187 Validation loss 0.06364485621452332 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.5571],\n",
      "        [0.5942]], device='mps:0')\n",
      "Iteration 13360 Training loss 0.06916921585798264 Validation loss 0.06372936815023422 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.6970],\n",
      "        [0.0703]], device='mps:0')\n",
      "Iteration 13370 Training loss 0.06409768760204315 Validation loss 0.06368382275104523 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.1488],\n",
      "        [0.4421]], device='mps:0')\n",
      "Iteration 13380 Training loss 0.06339466571807861 Validation loss 0.06362130492925644 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.9041],\n",
      "        [0.8030]], device='mps:0')\n",
      "Iteration 13390 Training loss 0.06003342941403389 Validation loss 0.0636054277420044 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.6161],\n",
      "        [0.7387]], device='mps:0')\n",
      "Iteration 13400 Training loss 0.06388065963983536 Validation loss 0.06363094598054886 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.2849],\n",
      "        [0.6346]], device='mps:0')\n",
      "Iteration 13410 Training loss 0.07169539481401443 Validation loss 0.06377469748258591 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.8570],\n",
      "        [0.8539]], device='mps:0')\n",
      "Iteration 13420 Training loss 0.061082713305950165 Validation loss 0.06361138820648193 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.3933],\n",
      "        [0.9837]], device='mps:0')\n",
      "Iteration 13430 Training loss 0.06453283876180649 Validation loss 0.06364491581916809 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.7688],\n",
      "        [0.0440]], device='mps:0')\n",
      "Iteration 13440 Training loss 0.05474158003926277 Validation loss 0.06366847455501556 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.1784],\n",
      "        [0.9257]], device='mps:0')\n",
      "Iteration 13450 Training loss 0.06471773236989975 Validation loss 0.06358352303504944 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.8487],\n",
      "        [0.7239]], device='mps:0')\n",
      "Iteration 13460 Training loss 0.07090553641319275 Validation loss 0.06360266357660294 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.9556],\n",
      "        [0.8206]], device='mps:0')\n",
      "Iteration 13470 Training loss 0.061586879193782806 Validation loss 0.06358721107244492 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.8776],\n",
      "        [0.9683]], device='mps:0')\n",
      "Iteration 13480 Training loss 0.05866601690649986 Validation loss 0.06356164067983627 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.8984],\n",
      "        [0.8929]], device='mps:0')\n",
      "Iteration 13490 Training loss 0.06324419379234314 Validation loss 0.06360974162817001 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.8807],\n",
      "        [0.8850]], device='mps:0')\n",
      "Iteration 13500 Training loss 0.05963076278567314 Validation loss 0.06355661898851395 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.0675],\n",
      "        [0.9101]], device='mps:0')\n",
      "Iteration 13510 Training loss 0.06191658973693848 Validation loss 0.06363097578287125 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.1037],\n",
      "        [0.9171]], device='mps:0')\n",
      "Iteration 13520 Training loss 0.06483171135187149 Validation loss 0.06396151334047318 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.3962],\n",
      "        [0.9054]], device='mps:0')\n",
      "Iteration 13530 Training loss 0.058910828083753586 Validation loss 0.06366778165102005 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.3864],\n",
      "        [0.2550]], device='mps:0')\n",
      "Iteration 13540 Training loss 0.05805724859237671 Validation loss 0.06353812664747238 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.6222],\n",
      "        [0.8355]], device='mps:0')\n",
      "Iteration 13550 Training loss 0.06381099671125412 Validation loss 0.06355748325586319 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.2680],\n",
      "        [0.0120]], device='mps:0')\n",
      "Iteration 13560 Training loss 0.06464389711618423 Validation loss 0.06374813616275787 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.0966],\n",
      "        [0.2430]], device='mps:0')\n",
      "Iteration 13570 Training loss 0.0667036697268486 Validation loss 0.06379304081201553 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9366],\n",
      "        [0.0072]], device='mps:0')\n",
      "Iteration 13580 Training loss 0.06378205865621567 Validation loss 0.06354603171348572 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.9067],\n",
      "        [0.0324]], device='mps:0')\n",
      "Iteration 13590 Training loss 0.06462936103343964 Validation loss 0.06369595229625702 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.8049],\n",
      "        [0.5043]], device='mps:0')\n",
      "Iteration 13600 Training loss 0.06329526007175446 Validation loss 0.06412595510482788 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.4739],\n",
      "        [0.8679]], device='mps:0')\n",
      "Iteration 13610 Training loss 0.06454223394393921 Validation loss 0.06351830065250397 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.1804],\n",
      "        [0.9732]], device='mps:0')\n",
      "Iteration 13620 Training loss 0.06041628122329712 Validation loss 0.06351128965616226 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.2105],\n",
      "        [0.8526]], device='mps:0')\n",
      "Iteration 13630 Training loss 0.058503374457359314 Validation loss 0.0635136142373085 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.2769],\n",
      "        [0.9821]], device='mps:0')\n",
      "Iteration 13640 Training loss 0.06225679814815521 Validation loss 0.0635060966014862 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.3732],\n",
      "        [0.0988]], device='mps:0')\n",
      "Iteration 13650 Training loss 0.05696678161621094 Validation loss 0.06350526213645935 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.1614],\n",
      "        [0.0527]], device='mps:0')\n",
      "Iteration 13660 Training loss 0.06483954936265945 Validation loss 0.06362598389387131 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.2004],\n",
      "        [0.0771]], device='mps:0')\n",
      "Iteration 13670 Training loss 0.05934496596455574 Validation loss 0.06362475454807281 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.0605],\n",
      "        [0.3508]], device='mps:0')\n",
      "Iteration 13680 Training loss 0.06120365858078003 Validation loss 0.06365890800952911 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.5810],\n",
      "        [0.6149]], device='mps:0')\n",
      "Iteration 13690 Training loss 0.053501956164836884 Validation loss 0.06349227577447891 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.7006],\n",
      "        [0.7713]], device='mps:0')\n",
      "Iteration 13700 Training loss 0.06681438535451889 Validation loss 0.06369394809007645 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.7905],\n",
      "        [0.0588]], device='mps:0')\n",
      "Iteration 13710 Training loss 0.061065156012773514 Validation loss 0.06349536031484604 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.6127],\n",
      "        [0.6687]], device='mps:0')\n",
      "Iteration 13720 Training loss 0.06013648957014084 Validation loss 0.06348638981580734 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.8749],\n",
      "        [0.6559]], device='mps:0')\n",
      "Iteration 13730 Training loss 0.06452626734972 Validation loss 0.0636698454618454 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.1283],\n",
      "        [0.1393]], device='mps:0')\n",
      "Iteration 13740 Training loss 0.06042467802762985 Validation loss 0.06353242695331573 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.0125],\n",
      "        [0.8017]], device='mps:0')\n",
      "Iteration 13750 Training loss 0.06806817650794983 Validation loss 0.06363139301538467 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.1531],\n",
      "        [0.6624]], device='mps:0')\n",
      "Iteration 13760 Training loss 0.07030811905860901 Validation loss 0.0635024681687355 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.3022],\n",
      "        [0.9971]], device='mps:0')\n",
      "Iteration 13770 Training loss 0.06335542351007462 Validation loss 0.0634918138384819 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.0575],\n",
      "        [0.2617]], device='mps:0')\n",
      "Iteration 13780 Training loss 0.05811699852347374 Validation loss 0.06347424536943436 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.3307],\n",
      "        [0.7298]], device='mps:0')\n",
      "Iteration 13790 Training loss 0.07149515300989151 Validation loss 0.06346942484378815 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.8452],\n",
      "        [0.0086]], device='mps:0')\n",
      "Iteration 13800 Training loss 0.059565361589193344 Validation loss 0.06351690739393234 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.2141],\n",
      "        [0.9071]], device='mps:0')\n",
      "Iteration 13810 Training loss 0.06673411279916763 Validation loss 0.06350882351398468 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.4524],\n",
      "        [0.1495]], device='mps:0')\n",
      "Iteration 13820 Training loss 0.06673231720924377 Validation loss 0.0635114386677742 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.1440],\n",
      "        [0.4725]], device='mps:0')\n",
      "Iteration 13830 Training loss 0.06465465575456619 Validation loss 0.06355642527341843 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.7052],\n",
      "        [0.7074]], device='mps:0')\n",
      "Iteration 13840 Training loss 0.07226341962814331 Validation loss 0.0634840577840805 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.9688],\n",
      "        [0.7104]], device='mps:0')\n",
      "Iteration 13850 Training loss 0.0666271299123764 Validation loss 0.06345069408416748 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.6058],\n",
      "        [0.5899]], device='mps:0')\n",
      "Iteration 13860 Training loss 0.06465557217597961 Validation loss 0.06347659230232239 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.4199],\n",
      "        [0.9062]], device='mps:0')\n",
      "Iteration 13870 Training loss 0.06693321466445923 Validation loss 0.06345099955797195 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.9822],\n",
      "        [0.8731]], device='mps:0')\n",
      "Iteration 13880 Training loss 0.0635000616312027 Validation loss 0.06351839005947113 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.0147],\n",
      "        [0.4917]], device='mps:0')\n",
      "Iteration 13890 Training loss 0.06996180862188339 Validation loss 0.06356863677501678 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.9622],\n",
      "        [0.4567]], device='mps:0')\n",
      "Iteration 13900 Training loss 0.060056399554014206 Validation loss 0.06345389038324356 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.2925],\n",
      "        [0.8385]], device='mps:0')\n",
      "Iteration 13910 Training loss 0.06135948374867439 Validation loss 0.06367640942335129 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.8577],\n",
      "        [0.2984]], device='mps:0')\n",
      "Iteration 13920 Training loss 0.06331835687160492 Validation loss 0.06346336007118225 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.8145],\n",
      "        [0.3551]], device='mps:0')\n",
      "Iteration 13930 Training loss 0.06325118243694305 Validation loss 0.06341884285211563 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.8130],\n",
      "        [0.2584]], device='mps:0')\n",
      "Iteration 13940 Training loss 0.06410548090934753 Validation loss 0.06355934590101242 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.9946],\n",
      "        [0.0985]], device='mps:0')\n",
      "Iteration 13950 Training loss 0.06487409770488739 Validation loss 0.06355102360248566 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.1429],\n",
      "        [0.1814]], device='mps:0')\n",
      "Iteration 13960 Training loss 0.06815128773450851 Validation loss 0.06366972625255585 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.4663],\n",
      "        [0.0886]], device='mps:0')\n",
      "Iteration 13970 Training loss 0.07223093509674072 Validation loss 0.06346365809440613 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.1378],\n",
      "        [0.7365]], device='mps:0')\n",
      "Iteration 13980 Training loss 0.07522580772638321 Validation loss 0.06341691315174103 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.0692],\n",
      "        [0.6590]], device='mps:0')\n",
      "Iteration 13990 Training loss 0.06134021654725075 Validation loss 0.06361459195613861 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.9100],\n",
      "        [0.7906]], device='mps:0')\n",
      "Iteration 14000 Training loss 0.06889750063419342 Validation loss 0.06381823122501373 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.9527],\n",
      "        [0.1265]], device='mps:0')\n",
      "Iteration 14010 Training loss 0.07169167697429657 Validation loss 0.06357164680957794 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.1145],\n",
      "        [0.2808]], device='mps:0')\n",
      "Iteration 14020 Training loss 0.05985942855477333 Validation loss 0.06353869289159775 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.3998],\n",
      "        [0.0261]], device='mps:0')\n",
      "Iteration 14030 Training loss 0.058533575385808945 Validation loss 0.06340981274843216 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.0943],\n",
      "        [0.6559]], device='mps:0')\n",
      "Iteration 14040 Training loss 0.06058841571211815 Validation loss 0.06342501193284988 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.0925],\n",
      "        [0.8766]], device='mps:0')\n",
      "Iteration 14050 Training loss 0.061703603714704514 Validation loss 0.06341623514890671 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.4667],\n",
      "        [0.6804]], device='mps:0')\n",
      "Iteration 14060 Training loss 0.058983754366636276 Validation loss 0.06350509822368622 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.1913],\n",
      "        [0.8062]], device='mps:0')\n",
      "Iteration 14070 Training loss 0.06249336898326874 Validation loss 0.06343834847211838 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.0347],\n",
      "        [0.4604]], device='mps:0')\n",
      "Iteration 14080 Training loss 0.06321385502815247 Validation loss 0.06351588666439056 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.4344],\n",
      "        [0.5857]], device='mps:0')\n",
      "Iteration 14090 Training loss 0.06678327918052673 Validation loss 0.06343788653612137 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.7927],\n",
      "        [0.5223]], device='mps:0')\n",
      "Iteration 14100 Training loss 0.059203583747148514 Validation loss 0.0633721798658371 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.1638],\n",
      "        [0.9517]], device='mps:0')\n",
      "Iteration 14110 Training loss 0.05706577003002167 Validation loss 0.06350864470005035 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.9505],\n",
      "        [0.3187]], device='mps:0')\n",
      "Iteration 14120 Training loss 0.06569882482290268 Validation loss 0.06338335573673248 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.4506],\n",
      "        [0.8208]], device='mps:0')\n",
      "Iteration 14130 Training loss 0.06256919354200363 Validation loss 0.0633685290813446 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.3964],\n",
      "        [0.0529]], device='mps:0')\n",
      "Iteration 14140 Training loss 0.06401386857032776 Validation loss 0.06346827745437622 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.7037],\n",
      "        [0.1255]], device='mps:0')\n",
      "Iteration 14150 Training loss 0.0630670115351677 Validation loss 0.06341240555047989 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.9229],\n",
      "        [0.4324]], device='mps:0')\n",
      "Iteration 14160 Training loss 0.06255348026752472 Validation loss 0.0633559376001358 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.9044],\n",
      "        [0.9274]], device='mps:0')\n",
      "Iteration 14170 Training loss 0.07173191010951996 Validation loss 0.06336607038974762 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.9153],\n",
      "        [0.2789]], device='mps:0')\n",
      "Iteration 14180 Training loss 0.06362506747245789 Validation loss 0.06355296075344086 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.9839],\n",
      "        [0.9774]], device='mps:0')\n",
      "Iteration 14190 Training loss 0.06078571081161499 Validation loss 0.06334024667739868 Accuracy 0.827375054359436\n",
      "Output tensor([[0.4026],\n",
      "        [0.8852]], device='mps:0')\n",
      "Iteration 14200 Training loss 0.05980466306209564 Validation loss 0.0633656457066536 Accuracy 0.827375054359436\n",
      "Output tensor([[0.9904],\n",
      "        [0.8241]], device='mps:0')\n",
      "Iteration 14210 Training loss 0.06224954128265381 Validation loss 0.06333602964878082 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.6682],\n",
      "        [0.0327]], device='mps:0')\n",
      "Iteration 14220 Training loss 0.06253589689731598 Validation loss 0.0633576437830925 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.9704],\n",
      "        [0.0266]], device='mps:0')\n",
      "Iteration 14230 Training loss 0.06316251307725906 Validation loss 0.06342542916536331 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.0283],\n",
      "        [0.9052]], device='mps:0')\n",
      "Iteration 14240 Training loss 0.057525165379047394 Validation loss 0.06332866847515106 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.5381],\n",
      "        [0.0037]], device='mps:0')\n",
      "Iteration 14250 Training loss 0.05946023017168045 Validation loss 0.06332974135875702 Accuracy 0.827375054359436\n",
      "Output tensor([[0.3369],\n",
      "        [0.9720]], device='mps:0')\n",
      "Iteration 14260 Training loss 0.07707224786281586 Validation loss 0.06331325322389603 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.0307],\n",
      "        [0.2714]], device='mps:0')\n",
      "Iteration 14270 Training loss 0.05451778322458267 Validation loss 0.0634860023856163 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.0834],\n",
      "        [0.8844]], device='mps:0')\n",
      "Iteration 14280 Training loss 0.06575724482536316 Validation loss 0.06331236660480499 Accuracy 0.827250063419342\n",
      "Output tensor([[0.7893],\n",
      "        [0.9749]], device='mps:0')\n",
      "Iteration 14290 Training loss 0.06703107804059982 Validation loss 0.0636027604341507 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.0346],\n",
      "        [0.0533]], device='mps:0')\n",
      "Iteration 14300 Training loss 0.06281588226556778 Validation loss 0.06331469863653183 Accuracy 0.827750027179718\n",
      "Output tensor([[0.0727],\n",
      "        [0.3581]], device='mps:0')\n",
      "Iteration 14310 Training loss 0.06686340272426605 Validation loss 0.06330409646034241 Accuracy 0.827625036239624\n",
      "Output tensor([[0.5396],\n",
      "        [0.7470]], device='mps:0')\n",
      "Iteration 14320 Training loss 0.06326835602521896 Validation loss 0.06331080943346024 Accuracy 0.827875018119812\n",
      "Output tensor([[0.2640],\n",
      "        [0.1249]], device='mps:0')\n",
      "Iteration 14330 Training loss 0.0693255290389061 Validation loss 0.0632999911904335 Accuracy 0.827375054359436\n",
      "Output tensor([[0.2920],\n",
      "        [0.2852]], device='mps:0')\n",
      "Iteration 14340 Training loss 0.07088916748762131 Validation loss 0.06344392895698547 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.7112],\n",
      "        [0.4157]], device='mps:0')\n",
      "Iteration 14350 Training loss 0.05459776148200035 Validation loss 0.06329461187124252 Accuracy 0.827625036239624\n",
      "Output tensor([[0.4537],\n",
      "        [0.1165]], device='mps:0')\n",
      "Iteration 14360 Training loss 0.07221303135156631 Validation loss 0.06330379843711853 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.3879],\n",
      "        [0.3606]], device='mps:0')\n",
      "Iteration 14370 Training loss 0.06348223239183426 Validation loss 0.0632883757352829 Accuracy 0.827875018119812\n",
      "Output tensor([[0.3981],\n",
      "        [0.0258]], device='mps:0')\n",
      "Iteration 14380 Training loss 0.05885590240359306 Validation loss 0.06340090930461884 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.1668],\n",
      "        [0.1859]], device='mps:0')\n",
      "Iteration 14390 Training loss 0.057331304997205734 Validation loss 0.0634760633111 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.8852],\n",
      "        [0.7442]], device='mps:0')\n",
      "Iteration 14400 Training loss 0.06325473636388779 Validation loss 0.06336458772420883 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.9160],\n",
      "        [0.5685]], device='mps:0')\n",
      "Iteration 14410 Training loss 0.06274353712797165 Validation loss 0.0632912814617157 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.0468],\n",
      "        [0.2982]], device='mps:0')\n",
      "Iteration 14420 Training loss 0.06483127176761627 Validation loss 0.06336881220340729 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.2917],\n",
      "        [0.8877]], device='mps:0')\n",
      "Iteration 14430 Training loss 0.05592120438814163 Validation loss 0.0635753944516182 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.0888],\n",
      "        [0.3361]], device='mps:0')\n",
      "Iteration 14440 Training loss 0.06379125267267227 Validation loss 0.06329283118247986 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.4308],\n",
      "        [0.0573]], device='mps:0')\n",
      "Iteration 14450 Training loss 0.057660941034555435 Validation loss 0.06327814608812332 Accuracy 0.827375054359436\n",
      "Output tensor([[0.9718],\n",
      "        [0.0701]], device='mps:0')\n",
      "Iteration 14460 Training loss 0.06016289442777634 Validation loss 0.06334579735994339 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.1346],\n",
      "        [0.1759]], device='mps:0')\n",
      "Iteration 14470 Training loss 0.056793369352817535 Validation loss 0.06325618922710419 Accuracy 0.827875018119812\n",
      "Output tensor([[0.1299],\n",
      "        [0.7035]], device='mps:0')\n",
      "Iteration 14480 Training loss 0.05861363932490349 Validation loss 0.06325514614582062 Accuracy 0.827875018119812\n",
      "Output tensor([[0.2725],\n",
      "        [0.1340]], device='mps:0')\n",
      "Iteration 14490 Training loss 0.06981024146080017 Validation loss 0.06325691193342209 Accuracy 0.827625036239624\n",
      "Output tensor([[0.2085],\n",
      "        [0.3221]], device='mps:0')\n",
      "Iteration 14500 Training loss 0.05913779139518738 Validation loss 0.06325356662273407 Accuracy 0.827750027179718\n",
      "Output tensor([[0.9257],\n",
      "        [0.0070]], device='mps:0')\n",
      "Iteration 14510 Training loss 0.06372927874326706 Validation loss 0.06372173875570297 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.9960],\n",
      "        [0.3904]], device='mps:0')\n",
      "Iteration 14520 Training loss 0.06248332932591438 Validation loss 0.0633789673447609 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.3172],\n",
      "        [0.2521]], device='mps:0')\n",
      "Iteration 14530 Training loss 0.067275770008564 Validation loss 0.06323639303445816 Accuracy 0.827750027179718\n",
      "Output tensor([[0.4056],\n",
      "        [0.2831]], device='mps:0')\n",
      "Iteration 14540 Training loss 0.06625991314649582 Validation loss 0.06370826065540314 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.9734],\n",
      "        [0.9673]], device='mps:0')\n",
      "Iteration 14550 Training loss 0.06461955606937408 Validation loss 0.06324518471956253 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.0748],\n",
      "        [0.9492]], device='mps:0')\n",
      "Iteration 14560 Training loss 0.05968984588980675 Validation loss 0.06322964280843735 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.0146],\n",
      "        [0.6716]], device='mps:0')\n",
      "Iteration 14570 Training loss 0.06689273566007614 Validation loss 0.063571035861969 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.8242],\n",
      "        [0.9708]], device='mps:0')\n",
      "Iteration 14580 Training loss 0.06046817824244499 Validation loss 0.06344618648290634 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.6256],\n",
      "        [0.0184]], device='mps:0')\n",
      "Iteration 14590 Training loss 0.07263033837080002 Validation loss 0.06324335932731628 Accuracy 0.827250063419342\n",
      "Output tensor([[0.1526],\n",
      "        [0.8049]], device='mps:0')\n",
      "Iteration 14600 Training loss 0.06402996182441711 Validation loss 0.06347614526748657 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.0409],\n",
      "        [0.3664]], device='mps:0')\n",
      "Iteration 14610 Training loss 0.06964609026908875 Validation loss 0.06319199502468109 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.9138],\n",
      "        [0.8728]], device='mps:0')\n",
      "Iteration 14620 Training loss 0.06058593466877937 Validation loss 0.06320661306381226 Accuracy 0.827750027179718\n",
      "Output tensor([[0.1628],\n",
      "        [0.2184]], device='mps:0')\n",
      "Iteration 14630 Training loss 0.06897608935832977 Validation loss 0.06321078538894653 Accuracy 0.827750027179718\n",
      "Output tensor([[0.0141],\n",
      "        [0.8171]], device='mps:0')\n",
      "Iteration 14640 Training loss 0.05967387184500694 Validation loss 0.06317686289548874 Accuracy 0.827750027179718\n",
      "Output tensor([[0.1141],\n",
      "        [0.1173]], device='mps:0')\n",
      "Iteration 14650 Training loss 0.05522767826914787 Validation loss 0.06335955858230591 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.9133],\n",
      "        [0.2654]], device='mps:0')\n",
      "Iteration 14660 Training loss 0.06436605006456375 Validation loss 0.06317046284675598 Accuracy 0.827875018119812\n",
      "Output tensor([[0.0616],\n",
      "        [0.0087]], device='mps:0')\n",
      "Iteration 14670 Training loss 0.06349259614944458 Validation loss 0.06316959112882614 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.9621],\n",
      "        [0.1642]], device='mps:0')\n",
      "Iteration 14680 Training loss 0.06222144886851311 Validation loss 0.06334105134010315 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.1295],\n",
      "        [0.7249]], device='mps:0')\n",
      "Iteration 14690 Training loss 0.055151838809251785 Validation loss 0.06321759521961212 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.5146],\n",
      "        [0.3818]], device='mps:0')\n",
      "Iteration 14700 Training loss 0.06502220779657364 Validation loss 0.06319122016429901 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.9947],\n",
      "        [0.3862]], device='mps:0')\n",
      "Iteration 14710 Training loss 0.06470652669668198 Validation loss 0.06328359991312027 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.0302],\n",
      "        [0.1692]], device='mps:0')\n",
      "Iteration 14720 Training loss 0.06042993441224098 Validation loss 0.06316617131233215 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.4068],\n",
      "        [0.2024]], device='mps:0')\n",
      "Iteration 14730 Training loss 0.06361807882785797 Validation loss 0.06317325681447983 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.2481],\n",
      "        [0.9286]], device='mps:0')\n",
      "Iteration 14740 Training loss 0.0650070533156395 Validation loss 0.0632384642958641 Accuracy 0.82750004529953\n",
      "Output tensor([[0.9881],\n",
      "        [0.4985]], device='mps:0')\n",
      "Iteration 14750 Training loss 0.051866721361875534 Validation loss 0.0631687119603157 Accuracy 0.827875018119812\n",
      "Output tensor([[0.1723],\n",
      "        [0.5896]], device='mps:0')\n",
      "Iteration 14760 Training loss 0.06558769196271896 Validation loss 0.0631994977593422 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.8177],\n",
      "        [0.9492]], device='mps:0')\n",
      "Iteration 14770 Training loss 0.07077540457248688 Validation loss 0.06321362406015396 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.5100],\n",
      "        [0.9099]], device='mps:0')\n",
      "Iteration 14780 Training loss 0.06759917736053467 Validation loss 0.06315124779939651 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.6549],\n",
      "        [0.0465]], device='mps:0')\n",
      "Iteration 14790 Training loss 0.06144101545214653 Validation loss 0.06317491084337234 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.4135],\n",
      "        [0.8935]], device='mps:0')\n",
      "Iteration 14800 Training loss 0.06006879732012749 Validation loss 0.06313580274581909 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.6731],\n",
      "        [0.2838]], device='mps:0')\n",
      "Iteration 14810 Training loss 0.05936938151717186 Validation loss 0.06313371658325195 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.4816],\n",
      "        [0.1551]], device='mps:0')\n",
      "Iteration 14820 Training loss 0.060304395854473114 Validation loss 0.06322941184043884 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.7672],\n",
      "        [0.0774]], device='mps:0')\n",
      "Iteration 14830 Training loss 0.058493513613939285 Validation loss 0.06321246176958084 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0683],\n",
      "        [0.3031]], device='mps:0')\n",
      "Iteration 14840 Training loss 0.058479566127061844 Validation loss 0.06313952803611755 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.9477],\n",
      "        [0.9309]], device='mps:0')\n",
      "Iteration 14850 Training loss 0.07136309146881104 Validation loss 0.06313593685626984 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.9074],\n",
      "        [0.3346]], device='mps:0')\n",
      "Iteration 14860 Training loss 0.06512271612882614 Validation loss 0.06311473995447159 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.9507],\n",
      "        [0.8237]], device='mps:0')\n",
      "Iteration 14870 Training loss 0.059263408184051514 Validation loss 0.06311000138521194 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.1793],\n",
      "        [0.1423]], device='mps:0')\n",
      "Iteration 14880 Training loss 0.0683833584189415 Validation loss 0.06309928745031357 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.0100],\n",
      "        [0.1933]], device='mps:0')\n",
      "Iteration 14890 Training loss 0.06841582804918289 Validation loss 0.06320028007030487 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9746],\n",
      "        [0.1609]], device='mps:0')\n",
      "Iteration 14900 Training loss 0.0696360170841217 Validation loss 0.0631413385272026 Accuracy 0.827625036239624\n",
      "Output tensor([[0.2136],\n",
      "        [0.1862]], device='mps:0')\n",
      "Iteration 14910 Training loss 0.06779032945632935 Validation loss 0.06308859586715698 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.0583],\n",
      "        [0.9958]], device='mps:0')\n",
      "Iteration 14920 Training loss 0.060671765357255936 Validation loss 0.06308815628290176 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.8756],\n",
      "        [0.7966]], device='mps:0')\n",
      "Iteration 14930 Training loss 0.061926450580358505 Validation loss 0.06319163739681244 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.2618],\n",
      "        [0.8403]], device='mps:0')\n",
      "Iteration 14940 Training loss 0.06378313153982162 Validation loss 0.06307927519083023 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.1848],\n",
      "        [0.3778]], device='mps:0')\n",
      "Iteration 14950 Training loss 0.05986352637410164 Validation loss 0.06311588734388351 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.5621],\n",
      "        [0.5536]], device='mps:0')\n",
      "Iteration 14960 Training loss 0.06127877160906792 Validation loss 0.06308742612600327 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.3568],\n",
      "        [0.9720]], device='mps:0')\n",
      "Iteration 14970 Training loss 0.06095905974507332 Validation loss 0.063075490295887 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.5743],\n",
      "        [0.6532]], device='mps:0')\n",
      "Iteration 14980 Training loss 0.06344374269247055 Validation loss 0.06306708604097366 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.9097],\n",
      "        [0.4384]], device='mps:0')\n",
      "Iteration 14990 Training loss 0.059935063123703 Validation loss 0.06312284618616104 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.0877],\n",
      "        [0.2686]], device='mps:0')\n",
      "Iteration 15000 Training loss 0.05800601840019226 Validation loss 0.06306969374418259 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.6911],\n",
      "        [0.2620]], device='mps:0')\n",
      "Iteration 15010 Training loss 0.062348730862140656 Validation loss 0.06306295096874237 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.1488],\n",
      "        [0.8501]], device='mps:0')\n",
      "Iteration 15020 Training loss 0.0577603243291378 Validation loss 0.06306403875350952 Accuracy 0.827625036239624\n",
      "Output tensor([[0.8696],\n",
      "        [0.0367]], device='mps:0')\n",
      "Iteration 15030 Training loss 0.06217532977461815 Validation loss 0.06334152072668076 Accuracy 0.827250063419342\n",
      "Output tensor([[0.8052],\n",
      "        [0.7667]], device='mps:0')\n",
      "Iteration 15040 Training loss 0.061835382133722305 Validation loss 0.0631261020898819 Accuracy 0.82750004529953\n",
      "Output tensor([[0.3270],\n",
      "        [0.6984]], device='mps:0')\n",
      "Iteration 15050 Training loss 0.06559909135103226 Validation loss 0.06308901309967041 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.1532],\n",
      "        [0.0353]], device='mps:0')\n",
      "Iteration 15060 Training loss 0.061241500079631805 Validation loss 0.06306638568639755 Accuracy 0.827375054359436\n",
      "Output tensor([[0.9502],\n",
      "        [0.2845]], device='mps:0')\n",
      "Iteration 15070 Training loss 0.061101846396923065 Validation loss 0.06325577944517136 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.9753],\n",
      "        [0.7030]], device='mps:0')\n",
      "Iteration 15080 Training loss 0.06600280106067657 Validation loss 0.06304962188005447 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.9251],\n",
      "        [0.9260]], device='mps:0')\n",
      "Iteration 15090 Training loss 0.05656467378139496 Validation loss 0.06306596100330353 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.2029],\n",
      "        [0.9300]], device='mps:0')\n",
      "Iteration 15100 Training loss 0.0504549965262413 Validation loss 0.06315279006958008 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.6412],\n",
      "        [0.9802]], device='mps:0')\n",
      "Iteration 15110 Training loss 0.06486126035451889 Validation loss 0.06303902715444565 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.9341],\n",
      "        [0.3333]], device='mps:0')\n",
      "Iteration 15120 Training loss 0.06714317947626114 Validation loss 0.06361424922943115 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.9844],\n",
      "        [0.0539]], device='mps:0')\n",
      "Iteration 15130 Training loss 0.07142146676778793 Validation loss 0.06303787231445312 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.5040],\n",
      "        [0.9587]], device='mps:0')\n",
      "Iteration 15140 Training loss 0.06404189765453339 Validation loss 0.06334211677312851 Accuracy 0.827250063419342\n",
      "Output tensor([[0.8145],\n",
      "        [0.7383]], device='mps:0')\n",
      "Iteration 15150 Training loss 0.05858740210533142 Validation loss 0.06320556998252869 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.8424],\n",
      "        [0.0763]], device='mps:0')\n",
      "Iteration 15160 Training loss 0.05555640906095505 Validation loss 0.06305012106895447 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.5685],\n",
      "        [0.8574]], device='mps:0')\n",
      "Iteration 15170 Training loss 0.06546108424663544 Validation loss 0.06302490085363388 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.8413],\n",
      "        [0.8104]], device='mps:0')\n",
      "Iteration 15180 Training loss 0.06092196702957153 Validation loss 0.06302341818809509 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.5802],\n",
      "        [0.9426]], device='mps:0')\n",
      "Iteration 15190 Training loss 0.06317588686943054 Validation loss 0.06304115056991577 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.9783],\n",
      "        [0.6340]], device='mps:0')\n",
      "Iteration 15200 Training loss 0.06930642575025558 Validation loss 0.06316953897476196 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.0451],\n",
      "        [0.6229]], device='mps:0')\n",
      "Iteration 15210 Training loss 0.06047513708472252 Validation loss 0.06319519132375717 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.5102],\n",
      "        [0.8670]], device='mps:0')\n",
      "Iteration 15220 Training loss 0.06291183084249496 Validation loss 0.06316465139389038 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.8021],\n",
      "        [0.3278]], device='mps:0')\n",
      "Iteration 15230 Training loss 0.06760694086551666 Validation loss 0.06306318938732147 Accuracy 0.827750027179718\n",
      "Output tensor([[0.1522],\n",
      "        [0.3100]], device='mps:0')\n",
      "Iteration 15240 Training loss 0.06473907083272934 Validation loss 0.06309150159358978 Accuracy 0.82750004529953\n",
      "Output tensor([[0.8402],\n",
      "        [0.0135]], device='mps:0')\n",
      "Iteration 15250 Training loss 0.0593532994389534 Validation loss 0.06300590932369232 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.7788],\n",
      "        [0.9533]], device='mps:0')\n",
      "Iteration 15260 Training loss 0.0669325441122055 Validation loss 0.06299479305744171 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.9921],\n",
      "        [0.0136]], device='mps:0')\n",
      "Iteration 15270 Training loss 0.05844072625041008 Validation loss 0.06309903413057327 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.0592],\n",
      "        [0.9646]], device='mps:0')\n",
      "Iteration 15280 Training loss 0.07445846498012543 Validation loss 0.06301739066839218 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.2490],\n",
      "        [0.7615]], device='mps:0')\n",
      "Iteration 15290 Training loss 0.06316951662302017 Validation loss 0.06325764209032059 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.4957],\n",
      "        [0.4711]], device='mps:0')\n",
      "Iteration 15300 Training loss 0.057952020317316055 Validation loss 0.06301748752593994 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.8830],\n",
      "        [0.1244]], device='mps:0')\n",
      "Iteration 15310 Training loss 0.06152549013495445 Validation loss 0.06299211829900742 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.1738],\n",
      "        [0.2267]], device='mps:0')\n",
      "Iteration 15320 Training loss 0.06393377482891083 Validation loss 0.06303039938211441 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.8468],\n",
      "        [0.5542]], device='mps:0')\n",
      "Iteration 15330 Training loss 0.06506447494029999 Validation loss 0.06298328191041946 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.0494],\n",
      "        [0.5121]], device='mps:0')\n",
      "Iteration 15340 Training loss 0.05522071197628975 Validation loss 0.06302961707115173 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.5130],\n",
      "        [0.5839]], device='mps:0')\n",
      "Iteration 15350 Training loss 0.06892644613981247 Validation loss 0.06296420097351074 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.0312],\n",
      "        [0.6794]], device='mps:0')\n",
      "Iteration 15360 Training loss 0.057575538754463196 Validation loss 0.06310266256332397 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.5213],\n",
      "        [0.9965]], device='mps:0')\n",
      "Iteration 15370 Training loss 0.06512335687875748 Validation loss 0.06299649178981781 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.7617],\n",
      "        [0.0183]], device='mps:0')\n",
      "Iteration 15380 Training loss 0.06315753608942032 Validation loss 0.0629667192697525 Accuracy 0.827750027179718\n",
      "Output tensor([[0.9022],\n",
      "        [0.1461]], device='mps:0')\n",
      "Iteration 15390 Training loss 0.05240439623594284 Validation loss 0.06296376883983612 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.9559],\n",
      "        [0.7616]], device='mps:0')\n",
      "Iteration 15400 Training loss 0.06635615229606628 Validation loss 0.06295802444219589 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.9193],\n",
      "        [0.9565]], device='mps:0')\n",
      "Iteration 15410 Training loss 0.06874696910381317 Validation loss 0.06310046464204788 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.7676],\n",
      "        [0.3523]], device='mps:0')\n",
      "Iteration 15420 Training loss 0.0615781769156456 Validation loss 0.06339073926210403 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.3416],\n",
      "        [0.8633]], device='mps:0')\n",
      "Iteration 15430 Training loss 0.06035211309790611 Validation loss 0.06296286731958389 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1449],\n",
      "        [0.4159]], device='mps:0')\n",
      "Iteration 15440 Training loss 0.05307766795158386 Validation loss 0.06305398792028427 Accuracy 0.827250063419342\n",
      "Output tensor([[0.0524],\n",
      "        [0.0311]], device='mps:0')\n",
      "Iteration 15450 Training loss 0.0556972473859787 Validation loss 0.06306325644254684 Accuracy 0.827625036239624\n",
      "Output tensor([[0.6607],\n",
      "        [0.9794]], device='mps:0')\n",
      "Iteration 15460 Training loss 0.06786499917507172 Validation loss 0.06303401291370392 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.7490],\n",
      "        [0.7590]], device='mps:0')\n",
      "Iteration 15470 Training loss 0.06302664428949356 Validation loss 0.06293301284313202 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.8134],\n",
      "        [0.9537]], device='mps:0')\n",
      "Iteration 15480 Training loss 0.0686342865228653 Validation loss 0.06294182687997818 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.1309],\n",
      "        [0.8667]], device='mps:0')\n",
      "Iteration 15490 Training loss 0.05561971664428711 Validation loss 0.06292840093374252 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.0321],\n",
      "        [0.3191]], device='mps:0')\n",
      "Iteration 15500 Training loss 0.0675283819437027 Validation loss 0.06301700323820114 Accuracy 0.82750004529953\n",
      "Output tensor([[0.9197],\n",
      "        [0.4504]], device='mps:0')\n",
      "Iteration 15510 Training loss 0.061251427978277206 Validation loss 0.0631721243262291 Accuracy 0.827750027179718\n",
      "Output tensor([[0.0844],\n",
      "        [0.4273]], device='mps:0')\n",
      "Iteration 15520 Training loss 0.06470923870801926 Validation loss 0.06309231370687485 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.8834],\n",
      "        [0.6273]], device='mps:0')\n",
      "Iteration 15530 Training loss 0.06312330812215805 Validation loss 0.06290941685438156 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.8701],\n",
      "        [0.0497]], device='mps:0')\n",
      "Iteration 15540 Training loss 0.059201430529356 Validation loss 0.06308629363775253 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.9330],\n",
      "        [0.4951]], device='mps:0')\n",
      "Iteration 15550 Training loss 0.06391578167676926 Validation loss 0.06293229013681412 Accuracy 0.827875018119812\n",
      "Output tensor([[0.6172],\n",
      "        [0.0924]], device='mps:0')\n",
      "Iteration 15560 Training loss 0.05681532248854637 Validation loss 0.06294140964746475 Accuracy 0.827750027179718\n",
      "Output tensor([[0.9738],\n",
      "        [0.9054]], device='mps:0')\n",
      "Iteration 15570 Training loss 0.05504121631383896 Validation loss 0.06290887296199799 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.9178],\n",
      "        [0.8945]], device='mps:0')\n",
      "Iteration 15580 Training loss 0.05728466808795929 Validation loss 0.06290851533412933 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.7608],\n",
      "        [0.7571]], device='mps:0')\n",
      "Iteration 15590 Training loss 0.05493025481700897 Validation loss 0.06290709227323532 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.9527],\n",
      "        [0.7821]], device='mps:0')\n",
      "Iteration 15600 Training loss 0.058509521186351776 Validation loss 0.0628972128033638 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.6578],\n",
      "        [0.0876]], device='mps:0')\n",
      "Iteration 15610 Training loss 0.06929079443216324 Validation loss 0.06289352476596832 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.7883],\n",
      "        [0.2970]], device='mps:0')\n",
      "Iteration 15620 Training loss 0.05949631705880165 Validation loss 0.06301648169755936 Accuracy 0.827625036239624\n",
      "Output tensor([[0.0470],\n",
      "        [0.0876]], device='mps:0')\n",
      "Iteration 15630 Training loss 0.06921693682670593 Validation loss 0.06292024254798889 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.0995],\n",
      "        [0.5286]], device='mps:0')\n",
      "Iteration 15640 Training loss 0.06627191603183746 Validation loss 0.06289432942867279 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8913],\n",
      "        [0.6859]], device='mps:0')\n",
      "Iteration 15650 Training loss 0.07010750472545624 Validation loss 0.06288577616214752 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.8662],\n",
      "        [0.1723]], device='mps:0')\n",
      "Iteration 15660 Training loss 0.07411304116249084 Validation loss 0.06298788636922836 Accuracy 0.827375054359436\n",
      "Output tensor([[0.7563],\n",
      "        [0.7571]], device='mps:0')\n",
      "Iteration 15670 Training loss 0.06717374920845032 Validation loss 0.06291655451059341 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.5305],\n",
      "        [0.7619]], device='mps:0')\n",
      "Iteration 15680 Training loss 0.07171057164669037 Validation loss 0.0629415437579155 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.9251],\n",
      "        [0.2020]], device='mps:0')\n",
      "Iteration 15690 Training loss 0.05744895711541176 Validation loss 0.06292372196912766 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.1711],\n",
      "        [0.2623]], device='mps:0')\n",
      "Iteration 15700 Training loss 0.06411409378051758 Validation loss 0.06302765756845474 Accuracy 0.827375054359436\n",
      "Output tensor([[0.9520],\n",
      "        [0.2662]], device='mps:0')\n",
      "Iteration 15710 Training loss 0.06533511728048325 Validation loss 0.06287247687578201 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.6365],\n",
      "        [0.1303]], device='mps:0')\n",
      "Iteration 15720 Training loss 0.0647696778178215 Validation loss 0.06322552263736725 Accuracy 0.827250063419342\n",
      "Output tensor([[0.9969],\n",
      "        [0.8950]], device='mps:0')\n",
      "Iteration 15730 Training loss 0.061993930488824844 Validation loss 0.06294143199920654 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.0208],\n",
      "        [0.9106]], device='mps:0')\n",
      "Iteration 15740 Training loss 0.05442404747009277 Validation loss 0.0630735531449318 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.9174],\n",
      "        [0.7222]], device='mps:0')\n",
      "Iteration 15750 Training loss 0.0603528767824173 Validation loss 0.06294764578342438 Accuracy 0.827375054359436\n",
      "Output tensor([[0.0158],\n",
      "        [0.5407]], device='mps:0')\n",
      "Iteration 15760 Training loss 0.059784505516290665 Validation loss 0.06299986690282822 Accuracy 0.82750004529953\n",
      "Output tensor([[0.0869],\n",
      "        [0.9875]], device='mps:0')\n",
      "Iteration 15770 Training loss 0.05429891496896744 Validation loss 0.06285102665424347 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.9735],\n",
      "        [0.0703]], device='mps:0')\n",
      "Iteration 15780 Training loss 0.07113911956548691 Validation loss 0.06289518624544144 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.5209],\n",
      "        [0.5864]], device='mps:0')\n",
      "Iteration 15790 Training loss 0.064395971596241 Validation loss 0.06290698051452637 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.2553],\n",
      "        [0.9075]], device='mps:0')\n",
      "Iteration 15800 Training loss 0.06127682700753212 Validation loss 0.06343521177768707 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.7042],\n",
      "        [0.0215]], device='mps:0')\n",
      "Iteration 15810 Training loss 0.05454691872000694 Validation loss 0.06308531016111374 Accuracy 0.827875018119812\n",
      "Output tensor([[0.9862],\n",
      "        [0.9489]], device='mps:0')\n",
      "Iteration 15820 Training loss 0.05454730987548828 Validation loss 0.06283355504274368 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.7937],\n",
      "        [0.1578]], device='mps:0')\n",
      "Iteration 15830 Training loss 0.06157597526907921 Validation loss 0.06293180584907532 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.3386],\n",
      "        [0.8789]], device='mps:0')\n",
      "Iteration 15840 Training loss 0.06572755426168442 Validation loss 0.06284790486097336 Accuracy 0.827625036239624\n",
      "Output tensor([[0.9055],\n",
      "        [0.3577]], device='mps:0')\n",
      "Iteration 15850 Training loss 0.05910717695951462 Validation loss 0.06283672899007797 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.5129],\n",
      "        [0.9679]], device='mps:0')\n",
      "Iteration 15860 Training loss 0.06911731511354446 Validation loss 0.06296113133430481 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.9166],\n",
      "        [0.9393]], device='mps:0')\n",
      "Iteration 15870 Training loss 0.06738259643316269 Validation loss 0.06285151839256287 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.6033],\n",
      "        [0.0443]], device='mps:0')\n",
      "Iteration 15880 Training loss 0.058926522731781006 Validation loss 0.0629580095410347 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.8416],\n",
      "        [0.0448]], device='mps:0')\n",
      "Iteration 15890 Training loss 0.058637846261262894 Validation loss 0.06294339895248413 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.7711],\n",
      "        [0.2091]], device='mps:0')\n",
      "Iteration 15900 Training loss 0.06384791433811188 Validation loss 0.06281288713216782 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.3654],\n",
      "        [0.4098]], device='mps:0')\n",
      "Iteration 15910 Training loss 0.053764402866363525 Validation loss 0.06282258778810501 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.6742],\n",
      "        [0.5104]], device='mps:0')\n",
      "Iteration 15920 Training loss 0.054671984165906906 Validation loss 0.06284519284963608 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.2257],\n",
      "        [0.2953]], device='mps:0')\n",
      "Iteration 15930 Training loss 0.05718941614031792 Validation loss 0.06303209811449051 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.8653],\n",
      "        [0.9586]], device='mps:0')\n",
      "Iteration 15940 Training loss 0.06017659232020378 Validation loss 0.06298558413982391 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.0196],\n",
      "        [0.9689]], device='mps:0')\n",
      "Iteration 15950 Training loss 0.05958656221628189 Validation loss 0.06296628713607788 Accuracy 0.827750027179718\n",
      "Output tensor([[0.9823],\n",
      "        [0.9317]], device='mps:0')\n",
      "Iteration 15960 Training loss 0.054636742919683456 Validation loss 0.06282656639814377 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1328],\n",
      "        [0.2669]], device='mps:0')\n",
      "Iteration 15970 Training loss 0.060607995837926865 Validation loss 0.06284402310848236 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7331],\n",
      "        [0.8062]], device='mps:0')\n",
      "Iteration 15980 Training loss 0.06302165240049362 Validation loss 0.06283802539110184 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.8731],\n",
      "        [0.9278]], device='mps:0')\n",
      "Iteration 15990 Training loss 0.06274519115686417 Validation loss 0.06281831115484238 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.9641],\n",
      "        [0.0243]], device='mps:0')\n",
      "Iteration 16000 Training loss 0.06993045657873154 Validation loss 0.06309455633163452 Accuracy 0.827250063419342\n",
      "Output tensor([[0.9876],\n",
      "        [0.0069]], device='mps:0')\n",
      "Iteration 16010 Training loss 0.06514131277799606 Validation loss 0.06281745433807373 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.5871],\n",
      "        [0.9261]], device='mps:0')\n",
      "Iteration 16020 Training loss 0.05905301868915558 Validation loss 0.06280660629272461 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.0828],\n",
      "        [0.8772]], device='mps:0')\n",
      "Iteration 16030 Training loss 0.05173586308956146 Validation loss 0.06280097365379333 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.1319],\n",
      "        [0.2014]], device='mps:0')\n",
      "Iteration 16040 Training loss 0.06190918758511543 Validation loss 0.06281165778636932 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.1677],\n",
      "        [0.8515]], device='mps:0')\n",
      "Iteration 16050 Training loss 0.056730497628450394 Validation loss 0.062886543571949 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.1318],\n",
      "        [0.9171]], device='mps:0')\n",
      "Iteration 16060 Training loss 0.07217413187026978 Validation loss 0.06278789043426514 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.9865],\n",
      "        [0.7603]], device='mps:0')\n",
      "Iteration 16070 Training loss 0.06921576708555222 Validation loss 0.06286825984716415 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.0972],\n",
      "        [0.2169]], device='mps:0')\n",
      "Iteration 16080 Training loss 0.05961303412914276 Validation loss 0.06294134259223938 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.9245],\n",
      "        [0.9589]], device='mps:0')\n",
      "Iteration 16090 Training loss 0.06020112708210945 Validation loss 0.06279686093330383 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8803],\n",
      "        [0.8377]], device='mps:0')\n",
      "Iteration 16100 Training loss 0.07002224028110504 Validation loss 0.06287402659654617 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.1185],\n",
      "        [0.2863]], device='mps:0')\n",
      "Iteration 16110 Training loss 0.05325586348772049 Validation loss 0.06278175115585327 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.3005],\n",
      "        [0.1379]], device='mps:0')\n",
      "Iteration 16120 Training loss 0.06945398449897766 Validation loss 0.0628449097275734 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.1055],\n",
      "        [0.0427]], device='mps:0')\n",
      "Iteration 16130 Training loss 0.05853617563843727 Validation loss 0.06277413666248322 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.2808],\n",
      "        [0.9709]], device='mps:0')\n",
      "Iteration 16140 Training loss 0.0576820969581604 Validation loss 0.06287150084972382 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.6600],\n",
      "        [0.1055]], device='mps:0')\n",
      "Iteration 16150 Training loss 0.057855814695358276 Validation loss 0.06280767172574997 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.6216],\n",
      "        [0.6967]], device='mps:0')\n",
      "Iteration 16160 Training loss 0.062089718878269196 Validation loss 0.06283147633075714 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.1641],\n",
      "        [0.4249]], device='mps:0')\n",
      "Iteration 16170 Training loss 0.06020127981901169 Validation loss 0.06279933452606201 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.3361],\n",
      "        [0.6055]], device='mps:0')\n",
      "Iteration 16180 Training loss 0.06207405775785446 Validation loss 0.06302881985902786 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.8794],\n",
      "        [0.8441]], device='mps:0')\n",
      "Iteration 16190 Training loss 0.06225302442908287 Validation loss 0.0627434030175209 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9373],\n",
      "        [0.6385]], device='mps:0')\n",
      "Iteration 16200 Training loss 0.06557520478963852 Validation loss 0.06275390088558197 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.7672],\n",
      "        [0.7580]], device='mps:0')\n",
      "Iteration 16210 Training loss 0.059520430862903595 Validation loss 0.06279527395963669 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9317],\n",
      "        [0.0161]], device='mps:0')\n",
      "Iteration 16220 Training loss 0.07195929437875748 Validation loss 0.0629144012928009 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.2277],\n",
      "        [0.1300]], device='mps:0')\n",
      "Iteration 16230 Training loss 0.061916884034872055 Validation loss 0.06285791844129562 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.5960],\n",
      "        [0.0741]], device='mps:0')\n",
      "Iteration 16240 Training loss 0.06255235522985458 Validation loss 0.06277414411306381 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.9433],\n",
      "        [0.7761]], device='mps:0')\n",
      "Iteration 16250 Training loss 0.06495116651058197 Validation loss 0.0627618283033371 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.6434],\n",
      "        [0.7741]], device='mps:0')\n",
      "Iteration 16260 Training loss 0.06313633173704147 Validation loss 0.06276056170463562 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1634],\n",
      "        [0.5640]], device='mps:0')\n",
      "Iteration 16270 Training loss 0.06914987415075302 Validation loss 0.06273403763771057 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.9399],\n",
      "        [0.0765]], device='mps:0')\n",
      "Iteration 16280 Training loss 0.068729467689991 Validation loss 0.06280763447284698 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.9598],\n",
      "        [0.1215]], device='mps:0')\n",
      "Iteration 16290 Training loss 0.05932609364390373 Validation loss 0.06273980438709259 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.1108],\n",
      "        [0.8392]], device='mps:0')\n",
      "Iteration 16300 Training loss 0.07236041128635406 Validation loss 0.0627216100692749 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.8722],\n",
      "        [0.9141]], device='mps:0')\n",
      "Iteration 16310 Training loss 0.06593041867017746 Validation loss 0.06275080889463425 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.3201],\n",
      "        [0.9688]], device='mps:0')\n",
      "Iteration 16320 Training loss 0.056619852781295776 Validation loss 0.06282278895378113 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.9777],\n",
      "        [0.3098]], device='mps:0')\n",
      "Iteration 16330 Training loss 0.0662878230214119 Validation loss 0.06271699070930481 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.4529],\n",
      "        [0.4725]], device='mps:0')\n",
      "Iteration 16340 Training loss 0.06136735901236534 Validation loss 0.06274095177650452 Accuracy 0.827875018119812\n",
      "Output tensor([[0.0997],\n",
      "        [0.1517]], device='mps:0')\n",
      "Iteration 16350 Training loss 0.05343250185251236 Validation loss 0.06277269124984741 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.2398],\n",
      "        [0.3658]], device='mps:0')\n",
      "Iteration 16360 Training loss 0.06607433408498764 Validation loss 0.06270048767328262 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.2533],\n",
      "        [0.9185]], device='mps:0')\n",
      "Iteration 16370 Training loss 0.05438637733459473 Validation loss 0.06275039911270142 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.1924],\n",
      "        [0.7925]], device='mps:0')\n",
      "Iteration 16380 Training loss 0.06735175848007202 Validation loss 0.06290213763713837 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.0979],\n",
      "        [0.4951]], device='mps:0')\n",
      "Iteration 16390 Training loss 0.06722700595855713 Validation loss 0.06295735388994217 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.2215],\n",
      "        [0.0231]], device='mps:0')\n",
      "Iteration 16400 Training loss 0.07273222506046295 Validation loss 0.0626911073923111 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.0203],\n",
      "        [0.0990]], device='mps:0')\n",
      "Iteration 16410 Training loss 0.07782644033432007 Validation loss 0.06273061782121658 Accuracy 0.827625036239624\n",
      "Output tensor([[0.8409],\n",
      "        [0.4901]], device='mps:0')\n",
      "Iteration 16420 Training loss 0.06780828535556793 Validation loss 0.0627700611948967 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.5952],\n",
      "        [0.9788]], device='mps:0')\n",
      "Iteration 16430 Training loss 0.06278657168149948 Validation loss 0.06281676143407822 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.0762],\n",
      "        [0.6271]], device='mps:0')\n",
      "Iteration 16440 Training loss 0.06510132551193237 Validation loss 0.06268162280321121 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.2219],\n",
      "        [0.4163]], device='mps:0')\n",
      "Iteration 16450 Training loss 0.0679296925663948 Validation loss 0.06273609399795532 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.0819],\n",
      "        [0.0242]], device='mps:0')\n",
      "Iteration 16460 Training loss 0.06435488164424896 Validation loss 0.06296265125274658 Accuracy 0.827625036239624\n",
      "Output tensor([[0.9424],\n",
      "        [0.8667]], device='mps:0')\n",
      "Iteration 16470 Training loss 0.06761003285646439 Validation loss 0.06279563903808594 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.2919],\n",
      "        [0.2442]], device='mps:0')\n",
      "Iteration 16480 Training loss 0.06486719846725464 Validation loss 0.06267129629850388 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.3285],\n",
      "        [0.2167]], device='mps:0')\n",
      "Iteration 16490 Training loss 0.06557512283325195 Validation loss 0.0626688227057457 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.8496],\n",
      "        [0.9751]], device='mps:0')\n",
      "Iteration 16500 Training loss 0.06625578552484512 Validation loss 0.06271643936634064 Accuracy 0.827375054359436\n",
      "Output tensor([[0.9103],\n",
      "        [0.0576]], device='mps:0')\n",
      "Iteration 16510 Training loss 0.06429872661828995 Validation loss 0.06274913251399994 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.1340],\n",
      "        [0.2125]], device='mps:0')\n",
      "Iteration 16520 Training loss 0.05670255422592163 Validation loss 0.06266815215349197 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.7070],\n",
      "        [0.0057]], device='mps:0')\n",
      "Iteration 16530 Training loss 0.0578795000910759 Validation loss 0.06277860701084137 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.8611],\n",
      "        [0.4504]], device='mps:0')\n",
      "Iteration 16540 Training loss 0.06243086978793144 Validation loss 0.06290696561336517 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.2102],\n",
      "        [0.7888]], device='mps:0')\n",
      "Iteration 16550 Training loss 0.053102798759937286 Validation loss 0.0627545565366745 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.1953],\n",
      "        [0.3413]], device='mps:0')\n",
      "Iteration 16560 Training loss 0.06540825217962265 Validation loss 0.06265877932310104 Accuracy 0.830500066280365\n",
      "Output tensor([[0.1741],\n",
      "        [0.4098]], device='mps:0')\n",
      "Iteration 16570 Training loss 0.05954309180378914 Validation loss 0.06272117793560028 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.6670],\n",
      "        [0.8990]], device='mps:0')\n",
      "Iteration 16580 Training loss 0.062205955386161804 Validation loss 0.06279253214597702 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.1956],\n",
      "        [0.6808]], device='mps:0')\n",
      "Iteration 16590 Training loss 0.057176731526851654 Validation loss 0.06265418976545334 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.6464],\n",
      "        [0.9435]], device='mps:0')\n",
      "Iteration 16600 Training loss 0.059696514159440994 Validation loss 0.06301712989807129 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.8360],\n",
      "        [0.7874]], device='mps:0')\n",
      "Iteration 16610 Training loss 0.06141702085733414 Validation loss 0.06274817138910294 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.2130],\n",
      "        [0.1416]], device='mps:0')\n",
      "Iteration 16620 Training loss 0.051076941192150116 Validation loss 0.06288296729326248 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.2848],\n",
      "        [0.9365]], device='mps:0')\n",
      "Iteration 16630 Training loss 0.06426986306905746 Validation loss 0.06279575824737549 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.8052],\n",
      "        [0.7834]], device='mps:0')\n",
      "Iteration 16640 Training loss 0.0575510635972023 Validation loss 0.06264922022819519 Accuracy 0.827875018119812\n",
      "Output tensor([[0.5515],\n",
      "        [0.8645]], device='mps:0')\n",
      "Iteration 16650 Training loss 0.06228930503129959 Validation loss 0.0626843050122261 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9359],\n",
      "        [0.8410]], device='mps:0')\n",
      "Iteration 16660 Training loss 0.05835970863699913 Validation loss 0.0626339316368103 Accuracy 0.82750004529953\n",
      "Output tensor([[0.7448],\n",
      "        [0.9017]], device='mps:0')\n",
      "Iteration 16670 Training loss 0.06243444234132767 Validation loss 0.06297847628593445 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.2284],\n",
      "        [0.8098]], device='mps:0')\n",
      "Iteration 16680 Training loss 0.06217168644070625 Validation loss 0.0626392513513565 Accuracy 0.827750027179718\n",
      "Output tensor([[0.2869],\n",
      "        [0.8795]], device='mps:0')\n",
      "Iteration 16690 Training loss 0.07171434164047241 Validation loss 0.0626511424779892 Accuracy 0.827750027179718\n",
      "Output tensor([[0.3591],\n",
      "        [0.8997]], device='mps:0')\n",
      "Iteration 16700 Training loss 0.05868400260806084 Validation loss 0.06275235116481781 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.6926],\n",
      "        [0.1223]], device='mps:0')\n",
      "Iteration 16710 Training loss 0.05534525588154793 Validation loss 0.06265720725059509 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.0289],\n",
      "        [0.6894]], device='mps:0')\n",
      "Iteration 16720 Training loss 0.05941614508628845 Validation loss 0.06260549277067184 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.6138],\n",
      "        [0.9401]], device='mps:0')\n",
      "Iteration 16730 Training loss 0.06869129836559296 Validation loss 0.06260996311903 Accuracy 0.830500066280365\n",
      "Output tensor([[0.0604],\n",
      "        [0.0033]], device='mps:0')\n",
      "Iteration 16740 Training loss 0.06678681820631027 Validation loss 0.06263566017150879 Accuracy 0.830750048160553\n",
      "Output tensor([[0.1110],\n",
      "        [0.6284]], device='mps:0')\n",
      "Iteration 16750 Training loss 0.06470462679862976 Validation loss 0.06275218725204468 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.7141],\n",
      "        [0.8713]], device='mps:0')\n",
      "Iteration 16760 Training loss 0.05958891659975052 Validation loss 0.06264524161815643 Accuracy 0.827625036239624\n",
      "Output tensor([[0.7255],\n",
      "        [0.9738]], device='mps:0')\n",
      "Iteration 16770 Training loss 0.056149084120988846 Validation loss 0.06266097724437714 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9654],\n",
      "        [0.0188]], device='mps:0')\n",
      "Iteration 16780 Training loss 0.06348921358585358 Validation loss 0.06261826306581497 Accuracy 0.827625036239624\n",
      "Output tensor([[0.8726],\n",
      "        [0.3384]], device='mps:0')\n",
      "Iteration 16790 Training loss 0.05373883247375488 Validation loss 0.06288017332553864 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.2918],\n",
      "        [0.8044]], device='mps:0')\n",
      "Iteration 16800 Training loss 0.06587324291467667 Validation loss 0.06261906772851944 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.3301],\n",
      "        [0.9453]], device='mps:0')\n",
      "Iteration 16810 Training loss 0.06067906692624092 Validation loss 0.06262587755918503 Accuracy 0.827375054359436\n",
      "Output tensor([[0.9174],\n",
      "        [0.8473]], device='mps:0')\n",
      "Iteration 16820 Training loss 0.05528225004673004 Validation loss 0.0626339390873909 Accuracy 0.8300000429153442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x102c03ad0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[0.0154],\n",
      "        [0.7436]], device='mps:0')\n",
      "Iteration 16830 Training loss 0.061754874885082245 Validation loss 0.06261065602302551 Accuracy 0.831250011920929\n",
      "Output tensor([[0.5729],\n",
      "        [0.0984]], device='mps:0')\n",
      "Iteration 16840 Training loss 0.05591961741447449 Validation loss 0.06258253008127213 Accuracy 0.830500066280365\n",
      "Output tensor([[0.0103],\n",
      "        [0.6134]], device='mps:0')\n",
      "Iteration 16850 Training loss 0.058704834431409836 Validation loss 0.06258712708950043 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.0280],\n",
      "        [0.0145]], device='mps:0')\n",
      "Iteration 16860 Training loss 0.06585746258497238 Validation loss 0.06258448213338852 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.2023],\n",
      "        [0.9289]], device='mps:0')\n",
      "Iteration 16870 Training loss 0.06425414234399796 Validation loss 0.06275511533021927 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.4767],\n",
      "        [0.6086]], device='mps:0')\n",
      "Iteration 16880 Training loss 0.06153952330350876 Validation loss 0.06257954239845276 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.2343],\n",
      "        [0.0061]], device='mps:0')\n",
      "Iteration 16890 Training loss 0.055600252002477646 Validation loss 0.0625750720500946 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.1362],\n",
      "        [0.9417]], device='mps:0')\n",
      "Iteration 16900 Training loss 0.05761488899588585 Validation loss 0.06262882798910141 Accuracy 0.82750004529953\n",
      "Output tensor([[0.8225],\n",
      "        [0.1843]], device='mps:0')\n",
      "Iteration 16910 Training loss 0.050453294068574905 Validation loss 0.0626380518078804 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.1171],\n",
      "        [0.1561]], device='mps:0')\n",
      "Iteration 16920 Training loss 0.06460918486118317 Validation loss 0.06257893890142441 Accuracy 0.831125020980835\n",
      "Output tensor([[0.7472],\n",
      "        [0.5908]], device='mps:0')\n",
      "Iteration 16930 Training loss 0.05385737866163254 Validation loss 0.0625690147280693 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.5421],\n",
      "        [0.0753]], device='mps:0')\n",
      "Iteration 16940 Training loss 0.05893779546022415 Validation loss 0.06257963180541992 Accuracy 0.82750004529953\n",
      "Output tensor([[0.9061],\n",
      "        [0.8907]], device='mps:0')\n",
      "Iteration 16950 Training loss 0.06258944422006607 Validation loss 0.06257643550634384 Accuracy 0.831250011920929\n",
      "Output tensor([[0.9217],\n",
      "        [0.0945]], device='mps:0')\n",
      "Iteration 16960 Training loss 0.061588354408741 Validation loss 0.062561996281147 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.9317],\n",
      "        [0.9303]], device='mps:0')\n",
      "Iteration 16970 Training loss 0.0637584701180458 Validation loss 0.06256011128425598 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.5785],\n",
      "        [0.1167]], device='mps:0')\n",
      "Iteration 16980 Training loss 0.0628446638584137 Validation loss 0.06267289072275162 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.1356],\n",
      "        [0.2602]], device='mps:0')\n",
      "Iteration 16990 Training loss 0.07451153546571732 Validation loss 0.06255122274160385 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.6176],\n",
      "        [0.0383]], device='mps:0')\n",
      "Iteration 17000 Training loss 0.057551462203264236 Validation loss 0.06298399716615677 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.9761],\n",
      "        [0.7794]], device='mps:0')\n",
      "Iteration 17010 Training loss 0.06130202114582062 Validation loss 0.06282306462526321 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.8388],\n",
      "        [0.4201]], device='mps:0')\n",
      "Iteration 17020 Training loss 0.06806383281946182 Validation loss 0.06264439970254898 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.2728],\n",
      "        [0.9246]], device='mps:0')\n",
      "Iteration 17030 Training loss 0.06795433163642883 Validation loss 0.0626729428768158 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.2565],\n",
      "        [0.1495]], device='mps:0')\n",
      "Iteration 17040 Training loss 0.06270775943994522 Validation loss 0.06271253526210785 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.0309],\n",
      "        [0.0160]], device='mps:0')\n",
      "Iteration 17050 Training loss 0.07174886018037796 Validation loss 0.06264959275722504 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.3211],\n",
      "        [0.1518]], device='mps:0')\n",
      "Iteration 17060 Training loss 0.055887505412101746 Validation loss 0.06256326287984848 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.6133],\n",
      "        [0.8713]], device='mps:0')\n",
      "Iteration 17070 Training loss 0.06550547480583191 Validation loss 0.06257336586713791 Accuracy 0.830875039100647\n",
      "Output tensor([[0.2651],\n",
      "        [0.6388]], device='mps:0')\n",
      "Iteration 17080 Training loss 0.0639139860868454 Validation loss 0.0625338926911354 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0538],\n",
      "        [0.9812]], device='mps:0')\n",
      "Iteration 17090 Training loss 0.06337790191173553 Validation loss 0.0626625344157219 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.7850],\n",
      "        [0.8430]], device='mps:0')\n",
      "Iteration 17100 Training loss 0.055889785289764404 Validation loss 0.06276171654462814 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.1298],\n",
      "        [0.9701]], device='mps:0')\n",
      "Iteration 17110 Training loss 0.061399154365062714 Validation loss 0.0626063123345375 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.5620],\n",
      "        [0.3649]], device='mps:0')\n",
      "Iteration 17120 Training loss 0.06125572696328163 Validation loss 0.06258104741573334 Accuracy 0.827375054359436\n",
      "Output tensor([[0.8584],\n",
      "        [0.0185]], device='mps:0')\n",
      "Iteration 17130 Training loss 0.06342680752277374 Validation loss 0.06260308623313904 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.0274],\n",
      "        [0.1874]], device='mps:0')\n",
      "Iteration 17140 Training loss 0.061433155089616776 Validation loss 0.0626184493303299 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.5399],\n",
      "        [0.8033]], device='mps:0')\n",
      "Iteration 17150 Training loss 0.06198522448539734 Validation loss 0.06272245943546295 Accuracy 0.827250063419342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x102c03ad0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[0.1497],\n",
      "        [0.2446]], device='mps:0')\n",
      "Iteration 17160 Training loss 0.06026117131114006 Validation loss 0.06256838887929916 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.3334],\n",
      "        [0.2181]], device='mps:0')\n",
      "Iteration 17170 Training loss 0.06282373517751694 Validation loss 0.06258414685726166 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.9734],\n",
      "        [0.1334]], device='mps:0')\n",
      "Iteration 17180 Training loss 0.060838811099529266 Validation loss 0.06255927681922913 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.2048],\n",
      "        [0.8813]], device='mps:0')\n",
      "Iteration 17190 Training loss 0.07135296612977982 Validation loss 0.06251296401023865 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.6082],\n",
      "        [0.0615]], device='mps:0')\n",
      "Iteration 17200 Training loss 0.0616677850484848 Validation loss 0.06259334087371826 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.1709],\n",
      "        [0.9715]], device='mps:0')\n",
      "Iteration 17210 Training loss 0.05593264102935791 Validation loss 0.06252855062484741 Accuracy 0.831125020980835\n",
      "Output tensor([[0.8115],\n",
      "        [0.9608]], device='mps:0')\n",
      "Iteration 17220 Training loss 0.06347756087779999 Validation loss 0.06252013891935349 Accuracy 0.827625036239624\n",
      "Output tensor([[0.9773],\n",
      "        [0.9915]], device='mps:0')\n",
      "Iteration 17230 Training loss 0.07091070711612701 Validation loss 0.0625367984175682 Accuracy 0.827250063419342\n",
      "Output tensor([[0.2507],\n",
      "        [0.7740]], device='mps:0')\n",
      "Iteration 17240 Training loss 0.05766765773296356 Validation loss 0.06250036507844925 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.1167],\n",
      "        [0.9138]], device='mps:0')\n",
      "Iteration 17250 Training loss 0.0629981979727745 Validation loss 0.06253895908594131 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.0605],\n",
      "        [0.2702]], device='mps:0')\n",
      "Iteration 17260 Training loss 0.06879937648773193 Validation loss 0.06250867247581482 Accuracy 0.831125020980835\n",
      "Output tensor([[0.7209],\n",
      "        [0.9535]], device='mps:0')\n",
      "Iteration 17270 Training loss 0.055303867906332016 Validation loss 0.06275464594364166 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.9920],\n",
      "        [0.6742]], device='mps:0')\n",
      "Iteration 17280 Training loss 0.06570444256067276 Validation loss 0.06254313141107559 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.2732],\n",
      "        [0.2335]], device='mps:0')\n",
      "Iteration 17290 Training loss 0.05843478441238403 Validation loss 0.06255108118057251 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.2000],\n",
      "        [0.2981]], device='mps:0')\n",
      "Iteration 17300 Training loss 0.06037650629878044 Validation loss 0.06326595693826675 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.9414],\n",
      "        [0.3237]], device='mps:0')\n",
      "Iteration 17310 Training loss 0.06586931645870209 Validation loss 0.06247822195291519 Accuracy 0.830500066280365\n",
      "Output tensor([[0.1150],\n",
      "        [0.2572]], device='mps:0')\n",
      "Iteration 17320 Training loss 0.06848959624767303 Validation loss 0.06252418458461761 Accuracy 0.830500066280365\n",
      "Output tensor([[0.9770],\n",
      "        [0.0871]], device='mps:0')\n",
      "Iteration 17330 Training loss 0.060225121676921844 Validation loss 0.06256335973739624 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.5422],\n",
      "        [0.6978]], device='mps:0')\n",
      "Iteration 17340 Training loss 0.06515257805585861 Validation loss 0.0627613365650177 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.0167],\n",
      "        [0.8855]], device='mps:0')\n",
      "Iteration 17350 Training loss 0.06428636610507965 Validation loss 0.06252012401819229 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.2066],\n",
      "        [0.0562]], device='mps:0')\n",
      "Iteration 17360 Training loss 0.055470723658800125 Validation loss 0.06252682954072952 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.2416],\n",
      "        [0.3692]], device='mps:0')\n",
      "Iteration 17370 Training loss 0.05569584667682648 Validation loss 0.06256551295518875 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0181],\n",
      "        [0.0959]], device='mps:0')\n",
      "Iteration 17380 Training loss 0.06261942535638809 Validation loss 0.06246163696050644 Accuracy 0.8303750157356262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x102c03ad0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[0.9211],\n",
      "        [0.9513]], device='mps:0')\n",
      "Iteration 17390 Training loss 0.05489273741841316 Validation loss 0.06247162073850632 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.5029],\n",
      "        [0.0297]], device='mps:0')\n",
      "Iteration 17400 Training loss 0.06363918632268906 Validation loss 0.06262180954217911 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.0858],\n",
      "        [0.8748]], device='mps:0')\n",
      "Iteration 17410 Training loss 0.056301239877939224 Validation loss 0.06248527392745018 Accuracy 0.827875018119812\n",
      "Output tensor([[0.1834],\n",
      "        [0.4824]], device='mps:0')\n",
      "Iteration 17420 Training loss 0.059344977140426636 Validation loss 0.06245122849941254 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.9599],\n",
      "        [0.8460]], device='mps:0')\n",
      "Iteration 17430 Training loss 0.05904126912355423 Validation loss 0.06274933367967606 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.6518],\n",
      "        [0.9410]], device='mps:0')\n",
      "Iteration 17440 Training loss 0.06063571944832802 Validation loss 0.06286782026290894 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.5502],\n",
      "        [0.1107]], device='mps:0')\n",
      "Iteration 17450 Training loss 0.057613492012023926 Validation loss 0.06248844787478447 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.5270],\n",
      "        [0.5540]], device='mps:0')\n",
      "Iteration 17460 Training loss 0.06659650802612305 Validation loss 0.06251979619264603 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.8933],\n",
      "        [0.0378]], device='mps:0')\n",
      "Iteration 17470 Training loss 0.05916185304522514 Validation loss 0.0625152513384819 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.9537],\n",
      "        [0.2356]], device='mps:0')\n",
      "Iteration 17480 Training loss 0.05800087749958038 Validation loss 0.062450844794511795 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.0878],\n",
      "        [0.5959]], device='mps:0')\n",
      "Iteration 17490 Training loss 0.06621773540973663 Validation loss 0.062460146844387054 Accuracy 0.831125020980835\n",
      "Output tensor([[0.0919],\n",
      "        [0.8213]], device='mps:0')\n",
      "Iteration 17500 Training loss 0.05679110810160637 Validation loss 0.06260302662849426 Accuracy 0.827375054359436\n",
      "Output tensor([[0.9018],\n",
      "        [0.2029]], device='mps:0')\n",
      "Iteration 17510 Training loss 0.055966854095458984 Validation loss 0.06271600723266602 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.8642],\n",
      "        [0.1078]], device='mps:0')\n",
      "Iteration 17520 Training loss 0.05199830234050751 Validation loss 0.06245328485965729 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.4147],\n",
      "        [0.0748]], device='mps:0')\n",
      "Iteration 17530 Training loss 0.062112484127283096 Validation loss 0.06268590688705444 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.9072],\n",
      "        [0.7705]], device='mps:0')\n",
      "Iteration 17540 Training loss 0.057672567665576935 Validation loss 0.06242602318525314 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.3716],\n",
      "        [0.8836]], device='mps:0')\n",
      "Iteration 17550 Training loss 0.06172868609428406 Validation loss 0.06245026737451553 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.8375],\n",
      "        [0.4654]], device='mps:0')\n",
      "Iteration 17560 Training loss 0.05617048591375351 Validation loss 0.06255821883678436 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.7751],\n",
      "        [0.9745]], device='mps:0')\n",
      "Iteration 17570 Training loss 0.06708064675331116 Validation loss 0.06242222338914871 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.1805],\n",
      "        [0.5645]], device='mps:0')\n",
      "Iteration 17580 Training loss 0.06380486488342285 Validation loss 0.06261467188596725 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.4914],\n",
      "        [0.9191]], device='mps:0')\n",
      "Iteration 17590 Training loss 0.05273227021098137 Validation loss 0.0624191015958786 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9873],\n",
      "        [0.8473]], device='mps:0')\n",
      "Iteration 17600 Training loss 0.060267508029937744 Validation loss 0.06240810081362724 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.9816],\n",
      "        [0.7362]], device='mps:0')\n",
      "Iteration 17610 Training loss 0.05338284373283386 Validation loss 0.06253217905759811 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.8713],\n",
      "        [0.5063]], device='mps:0')\n",
      "Iteration 17620 Training loss 0.061358146369457245 Validation loss 0.06240304931998253 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.0267],\n",
      "        [0.5375]], device='mps:0')\n",
      "Iteration 17630 Training loss 0.05905810743570328 Validation loss 0.06243811175227165 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.8391],\n",
      "        [0.2827]], device='mps:0')\n",
      "Iteration 17640 Training loss 0.058160196989774704 Validation loss 0.06241739168763161 Accuracy 0.830750048160553\n",
      "Output tensor([[0.1976],\n",
      "        [0.7872]], device='mps:0')\n",
      "Iteration 17650 Training loss 0.06126576289534569 Validation loss 0.062422070652246475 Accuracy 0.8285000324249268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x102c03ad0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[0.8753],\n",
      "        [0.1526]], device='mps:0')\n",
      "Iteration 17660 Training loss 0.061365943402051926 Validation loss 0.06239665672183037 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.9398],\n",
      "        [0.3549]], device='mps:0')\n",
      "Iteration 17670 Training loss 0.06282537430524826 Validation loss 0.06254983693361282 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.1646],\n",
      "        [0.2295]], device='mps:0')\n",
      "Iteration 17680 Training loss 0.0702337846159935 Validation loss 0.06247672811150551 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.0842],\n",
      "        [0.9216]], device='mps:0')\n",
      "Iteration 17690 Training loss 0.061212461441755295 Validation loss 0.06262818723917007 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.4451],\n",
      "        [0.7101]], device='mps:0')\n",
      "Iteration 17700 Training loss 0.05878270044922829 Validation loss 0.062408044934272766 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.8547],\n",
      "        [0.9816]], device='mps:0')\n",
      "Iteration 17710 Training loss 0.060302652418613434 Validation loss 0.06240734085440636 Accuracy 0.830625057220459\n",
      "Output tensor([[0.6550],\n",
      "        [0.6031]], device='mps:0')\n",
      "Iteration 17720 Training loss 0.06478308886289597 Validation loss 0.062417324632406235 Accuracy 0.830500066280365\n",
      "Output tensor([[0.1968],\n",
      "        [0.9697]], device='mps:0')\n",
      "Iteration 17730 Training loss 0.04986528307199478 Validation loss 0.062421925365924835 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.2049],\n",
      "        [0.8332]], device='mps:0')\n",
      "Iteration 17740 Training loss 0.05938520282506943 Validation loss 0.06245459243655205 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.8121],\n",
      "        [0.9247]], device='mps:0')\n",
      "Iteration 17750 Training loss 0.06584427505731583 Validation loss 0.06252872198820114 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.1459],\n",
      "        [0.9019]], device='mps:0')\n",
      "Iteration 17760 Training loss 0.06439948081970215 Validation loss 0.06238006800413132 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.8989],\n",
      "        [0.4265]], device='mps:0')\n",
      "Iteration 17770 Training loss 0.053709015250205994 Validation loss 0.06238091364502907 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.9305],\n",
      "        [0.8837]], device='mps:0')\n",
      "Iteration 17780 Training loss 0.06709843128919601 Validation loss 0.06237431988120079 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.1958],\n",
      "        [0.2485]], device='mps:0')\n",
      "Iteration 17790 Training loss 0.05990402400493622 Validation loss 0.06251420825719833 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.1624],\n",
      "        [0.7780]], device='mps:0')\n",
      "Iteration 17800 Training loss 0.06234404072165489 Validation loss 0.062357135117053986 Accuracy 0.831250011920929\n",
      "Output tensor([[0.8975],\n",
      "        [0.2327]], device='mps:0')\n",
      "Iteration 17810 Training loss 0.05919799581170082 Validation loss 0.06236857548356056 Accuracy 0.831000030040741\n",
      "Output tensor([[0.9938],\n",
      "        [0.8929]], device='mps:0')\n",
      "Iteration 17820 Training loss 0.056512679904699326 Validation loss 0.06234395503997803 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.0928],\n",
      "        [0.3056]], device='mps:0')\n",
      "Iteration 17830 Training loss 0.05791153386235237 Validation loss 0.06261192262172699 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.9119],\n",
      "        [0.0332]], device='mps:0')\n",
      "Iteration 17840 Training loss 0.05861888453364372 Validation loss 0.062333814799785614 Accuracy 0.830500066280365\n",
      "Output tensor([[0.8225],\n",
      "        [0.2625]], device='mps:0')\n",
      "Iteration 17850 Training loss 0.06529108434915543 Validation loss 0.06232639029622078 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.2305],\n",
      "        [0.0595]], device='mps:0')\n",
      "Iteration 17860 Training loss 0.057769373059272766 Validation loss 0.062494855374097824 Accuracy 0.8288750648498535\n"
     ]
    }
   ],
   "source": [
    "binary_model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2, 1e-8, 0, 0, 1e-1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer = binary_classification_three_layer_NN(3072, 2048, 2048, eps_init = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "4c674f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2, the number of datas used for the training is 943718400.0 and the number of iterations is 2359296.\n",
      "Iteration 0 Training loss 0.12499746680259705 Validation loss 0.12499942630529404 Accuracy 0.5\n",
      "Iteration 10 Training loss 0.12492745369672775 Validation loss 0.12494922429323196 Accuracy 0.5\n",
      "Iteration 20 Training loss 0.12489787489175797 Validation loss 0.1248689517378807 Accuracy 0.5\n",
      "Iteration 30 Training loss 0.12475873529911041 Validation loss 0.12474826723337173 Accuracy 0.5\n",
      "Iteration 40 Training loss 0.12466166913509369 Validation loss 0.12459097802639008 Accuracy 0.5\n",
      "Iteration 50 Training loss 0.12460171431303024 Validation loss 0.1242944672703743 Accuracy 0.5\n",
      "Iteration 60 Training loss 0.12379714846611023 Validation loss 0.12390395253896713 Accuracy 0.5052500367164612\n",
      "Iteration 70 Training loss 0.12270442396402359 Validation loss 0.12323042750358582 Accuracy 0.5158750414848328\n",
      "Iteration 80 Training loss 0.12138772010803223 Validation loss 0.12212933599948883 Accuracy 0.5236250162124634\n",
      "Iteration 90 Training loss 0.12069398909807205 Validation loss 0.12078540772199631 Accuracy 0.565750002861023\n",
      "Iteration 100 Training loss 0.11939741671085358 Validation loss 0.11884581297636032 Accuracy 0.5865000486373901\n",
      "Iteration 110 Training loss 0.11474481225013733 Validation loss 0.1163613572716713 Accuracy 0.6192500591278076\n",
      "Iteration 120 Training loss 0.11379437148571014 Validation loss 0.11308518052101135 Accuracy 0.6736250519752502\n",
      "Iteration 130 Training loss 0.11153409630060196 Validation loss 0.10935743153095245 Accuracy 0.6952500343322754\n",
      "Iteration 140 Training loss 0.10533087700605392 Validation loss 0.10490776598453522 Accuracy 0.7345000505447388\n",
      "Iteration 150 Training loss 0.10074187815189362 Validation loss 0.10051627457141876 Accuracy 0.7438750267028809\n",
      "Iteration 160 Training loss 0.09519238024950027 Validation loss 0.0964912623167038 Accuracy 0.7460000514984131\n",
      "Iteration 170 Training loss 0.09218306094408035 Validation loss 0.09335407614707947 Accuracy 0.7471250295639038\n",
      "Iteration 180 Training loss 0.08608640730381012 Validation loss 0.09069167077541351 Accuracy 0.7513750195503235\n",
      "Iteration 190 Training loss 0.09037014842033386 Validation loss 0.08912859857082367 Accuracy 0.7516250610351562\n",
      "Iteration 200 Training loss 0.0857231467962265 Validation loss 0.08745048195123672 Accuracy 0.7536250352859497\n",
      "Iteration 210 Training loss 0.079153873026371 Validation loss 0.08549951761960983 Accuracy 0.7600000500679016\n",
      "Iteration 220 Training loss 0.0849948599934578 Validation loss 0.08431266993284225 Accuracy 0.7633750438690186\n",
      "Iteration 230 Training loss 0.08086235821247101 Validation loss 0.08332593739032745 Accuracy 0.7646250128746033\n",
      "Iteration 240 Training loss 0.07908488810062408 Validation loss 0.08338052779436111 Accuracy 0.7621250152587891\n",
      "Iteration 250 Training loss 0.07891086488962173 Validation loss 0.08173298090696335 Accuracy 0.7675000429153442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training done'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model_3_layer.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-4, 0, 0, 0, 1, 0.01, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "16479094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer = two_layer_NN(3072,2048,10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "d2ba3b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2, the number of datas used for the training is 943718400.0 and the number of iterations is 1887436.\n",
      "Iteration 0 Training loss 0.08147968351840973 Validation loss 0.07874380797147751 Accuracy 0.21130000054836273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 Training loss 0.07778441905975342 Validation loss 0.07876531779766083 Accuracy 0.2110999971628189\n",
      "Iteration 20 Training loss 0.07566280663013458 Validation loss 0.07889723777770996 Accuracy 0.20970000326633453\n",
      "Iteration 30 Training loss 0.07870082557201385 Validation loss 0.07874888926744461 Accuracy 0.21140000224113464\n",
      "Iteration 40 Training loss 0.07923334836959839 Validation loss 0.07881540060043335 Accuracy 0.2102999985218048\n",
      "Iteration 50 Training loss 0.08033191412687302 Validation loss 0.07885144650936127 Accuracy 0.20970000326633453\n",
      "Iteration 60 Training loss 0.07787960022687912 Validation loss 0.07878050953149796 Accuracy 0.21089999377727509\n",
      "Iteration 70 Training loss 0.07815016806125641 Validation loss 0.07884414494037628 Accuracy 0.210099995136261\n",
      "Iteration 80 Training loss 0.07846667617559433 Validation loss 0.07886102050542831 Accuracy 0.210099995136261\n",
      "Iteration 90 Training loss 0.07827076315879822 Validation loss 0.07884978502988815 Accuracy 0.21050000190734863\n",
      "Iteration 100 Training loss 0.08006088435649872 Validation loss 0.07884161919355392 Accuracy 0.21050000190734863\n",
      "Iteration 110 Training loss 0.07618530094623566 Validation loss 0.07886958867311478 Accuracy 0.21040000021457672\n",
      "Iteration 120 Training loss 0.07824952155351639 Validation loss 0.07886408269405365 Accuracy 0.210099995136261\n",
      "Iteration 130 Training loss 0.0810968279838562 Validation loss 0.07889508455991745 Accuracy 0.210099995136261\n",
      "Iteration 140 Training loss 0.07486019283533096 Validation loss 0.07890035212039948 Accuracy 0.20980000495910645\n",
      "Iteration 150 Training loss 0.0750947892665863 Validation loss 0.07892737537622452 Accuracy 0.2094999998807907\n",
      "Iteration 160 Training loss 0.08031171560287476 Validation loss 0.07884259521961212 Accuracy 0.2101999968290329\n",
      "Iteration 170 Training loss 0.07841864973306656 Validation loss 0.07881847769021988 Accuracy 0.2102999985218048\n",
      "Iteration 180 Training loss 0.07678952068090439 Validation loss 0.07881966233253479 Accuracy 0.21050000190734863\n",
      "Iteration 190 Training loss 0.0791359692811966 Validation loss 0.07880336791276932 Accuracy 0.21070000529289246\n",
      "Iteration 200 Training loss 0.07774309068918228 Validation loss 0.07879544049501419 Accuracy 0.21050000190734863\n",
      "Iteration 210 Training loss 0.08309483528137207 Validation loss 0.07877356559038162 Accuracy 0.210999995470047\n",
      "Iteration 220 Training loss 0.07778993993997574 Validation loss 0.0788198858499527 Accuracy 0.21050000190734863\n",
      "Iteration 230 Training loss 0.07984614372253418 Validation loss 0.07882556319236755 Accuracy 0.2101999968290329\n",
      "Iteration 240 Training loss 0.0768418237566948 Validation loss 0.07872980087995529 Accuracy 0.21119999885559082\n",
      "Iteration 250 Training loss 0.08076255023479462 Validation loss 0.07876512408256531 Accuracy 0.21070000529289246\n",
      "Iteration 260 Training loss 0.08041510730981827 Validation loss 0.0788460224866867 Accuracy 0.2102999985218048\n",
      "Iteration 270 Training loss 0.08119401335716248 Validation loss 0.07893231511116028 Accuracy 0.2092999964952469\n",
      "Iteration 280 Training loss 0.07796266674995422 Validation loss 0.0789680927991867 Accuracy 0.20900000631809235\n",
      "Iteration 290 Training loss 0.08198511600494385 Validation loss 0.07924836874008179 Accuracy 0.2062000036239624\n",
      "Iteration 300 Training loss 0.0808778926730156 Validation loss 0.07910149544477463 Accuracy 0.2079000025987625\n",
      "Iteration 310 Training loss 0.08049064874649048 Validation loss 0.07916242629289627 Accuracy 0.20720000565052032\n",
      "Iteration 320 Training loss 0.07737164199352264 Validation loss 0.07881346344947815 Accuracy 0.21050000190734863\n",
      "Iteration 330 Training loss 0.07990474998950958 Validation loss 0.07889369130134583 Accuracy 0.20960000157356262\n",
      "Iteration 340 Training loss 0.07963390648365021 Validation loss 0.0790855661034584 Accuracy 0.20810000598430634\n",
      "Iteration 350 Training loss 0.0782441645860672 Validation loss 0.07905162870883942 Accuracy 0.20829999446868896\n",
      "Iteration 360 Training loss 0.0792950987815857 Validation loss 0.07911565899848938 Accuracy 0.2076999992132187\n",
      "Iteration 370 Training loss 0.07561931759119034 Validation loss 0.07891824841499329 Accuracy 0.20960000157356262\n",
      "Iteration 380 Training loss 0.07704614102840424 Validation loss 0.07876289635896683 Accuracy 0.210999995470047\n",
      "Iteration 390 Training loss 0.07721357047557831 Validation loss 0.07903532683849335 Accuracy 0.2085999995470047\n",
      "Iteration 400 Training loss 0.07786233723163605 Validation loss 0.07912665605545044 Accuracy 0.20749999582767487\n",
      "Iteration 410 Training loss 0.0785202756524086 Validation loss 0.07912562787532806 Accuracy 0.20749999582767487\n",
      "Iteration 420 Training loss 0.07928663492202759 Validation loss 0.07913148403167725 Accuracy 0.2076999992132187\n",
      "Iteration 430 Training loss 0.07910290360450745 Validation loss 0.0790318176150322 Accuracy 0.20819999277591705\n",
      "Iteration 440 Training loss 0.0792236253619194 Validation loss 0.07891158014535904 Accuracy 0.2094999998807907\n",
      "Iteration 450 Training loss 0.07893309742212296 Validation loss 0.0788814127445221 Accuracy 0.20990000665187836\n",
      "Iteration 460 Training loss 0.07734064757823944 Validation loss 0.07881034910678864 Accuracy 0.2102999985218048\n",
      "Iteration 470 Training loss 0.07548750936985016 Validation loss 0.07868681848049164 Accuracy 0.211899995803833\n",
      "Iteration 480 Training loss 0.07932539284229279 Validation loss 0.07873773574829102 Accuracy 0.21130000054836273\n",
      "Iteration 490 Training loss 0.07708068192005157 Validation loss 0.07864166051149368 Accuracy 0.21209999918937683\n",
      "Iteration 500 Training loss 0.0780801996588707 Validation loss 0.07886438071727753 Accuracy 0.210099995136261\n",
      "Iteration 510 Training loss 0.07888373732566833 Validation loss 0.078781358897686 Accuracy 0.21050000190734863\n",
      "Iteration 520 Training loss 0.07852061837911606 Validation loss 0.07860396057367325 Accuracy 0.2126999944448471\n",
      "Iteration 530 Training loss 0.07754034548997879 Validation loss 0.07865800708532333 Accuracy 0.21209999918937683\n",
      "Iteration 540 Training loss 0.07542520761489868 Validation loss 0.07865976542234421 Accuracy 0.21199999749660492\n",
      "Iteration 550 Training loss 0.07954470813274384 Validation loss 0.07860241830348969 Accuracy 0.21279999613761902\n",
      "Iteration 560 Training loss 0.07515988498926163 Validation loss 0.07867343723773956 Accuracy 0.2117999941110611\n",
      "Iteration 570 Training loss 0.07832404226064682 Validation loss 0.07861439138650894 Accuracy 0.21230000257492065\n",
      "Iteration 580 Training loss 0.07645829021930695 Validation loss 0.07865515351295471 Accuracy 0.21230000257492065\n",
      "Iteration 590 Training loss 0.07900073379278183 Validation loss 0.07866592705249786 Accuracy 0.21209999918937683\n",
      "Iteration 600 Training loss 0.07872562110424042 Validation loss 0.07862420380115509 Accuracy 0.2125999927520752\n",
      "Iteration 610 Training loss 0.07970356941223145 Validation loss 0.07867475599050522 Accuracy 0.21170000731945038\n",
      "Iteration 620 Training loss 0.07639019936323166 Validation loss 0.07875283062458038 Accuracy 0.210999995470047\n",
      "Iteration 630 Training loss 0.07822678238153458 Validation loss 0.07869045436382294 Accuracy 0.21160000562667847\n",
      "Iteration 640 Training loss 0.07866236567497253 Validation loss 0.07867684215307236 Accuracy 0.21160000562667847\n",
      "Iteration 650 Training loss 0.07810385525226593 Validation loss 0.07867736369371414 Accuracy 0.211899995803833\n",
      "Iteration 660 Training loss 0.0786622166633606 Validation loss 0.07860227674245834 Accuracy 0.2126999944448471\n",
      "Iteration 670 Training loss 0.07755253463983536 Validation loss 0.0786496251821518 Accuracy 0.21220000088214874\n",
      "Iteration 680 Training loss 0.0773116871714592 Validation loss 0.07868936657905579 Accuracy 0.21170000731945038\n",
      "Iteration 690 Training loss 0.08008778095245361 Validation loss 0.07866765558719635 Accuracy 0.21209999918937683\n",
      "Iteration 700 Training loss 0.07890748232603073 Validation loss 0.07861387729644775 Accuracy 0.2125999927520752\n",
      "Iteration 710 Training loss 0.08093263953924179 Validation loss 0.07862837612628937 Accuracy 0.21240000426769257\n",
      "Iteration 720 Training loss 0.08227566629648209 Validation loss 0.07871971279382706 Accuracy 0.21119999885559082\n",
      "Iteration 730 Training loss 0.07647685706615448 Validation loss 0.0786670669913292 Accuracy 0.21199999749660492\n",
      "Iteration 740 Training loss 0.07906075567007065 Validation loss 0.07869897037744522 Accuracy 0.21140000224113464\n",
      "Iteration 750 Training loss 0.07775041460990906 Validation loss 0.07865484803915024 Accuracy 0.21199999749660492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1054c6810>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 760 Training loss 0.07550739496946335 Validation loss 0.07867453247308731 Accuracy 0.21199999749660492\n",
      "Iteration 770 Training loss 0.08073295652866364 Validation loss 0.0786907821893692 Accuracy 0.21140000224113464\n",
      "Iteration 780 Training loss 0.07847923040390015 Validation loss 0.07888838648796082 Accuracy 0.20990000665187836\n",
      "Iteration 790 Training loss 0.07851509004831314 Validation loss 0.07884514331817627 Accuracy 0.20999999344348907\n",
      "Iteration 800 Training loss 0.0829850435256958 Validation loss 0.07893738895654678 Accuracy 0.20919999480247498\n",
      "Iteration 810 Training loss 0.07937177270650864 Validation loss 0.07861357182264328 Accuracy 0.21230000257492065\n",
      "Iteration 820 Training loss 0.0766185000538826 Validation loss 0.07864893972873688 Accuracy 0.21230000257492065\n",
      "Iteration 830 Training loss 0.07587696611881256 Validation loss 0.07860717922449112 Accuracy 0.21250000596046448\n",
      "Iteration 840 Training loss 0.07858560979366302 Validation loss 0.07861397415399551 Accuracy 0.21279999613761902\n",
      "Iteration 850 Training loss 0.078885018825531 Validation loss 0.07864049077033997 Accuracy 0.2125999927520752\n",
      "Iteration 860 Training loss 0.07356708496809006 Validation loss 0.07863979786634445 Accuracy 0.21250000596046448\n",
      "Iteration 870 Training loss 0.07596060633659363 Validation loss 0.07861461490392685 Accuracy 0.21250000596046448\n",
      "Iteration 880 Training loss 0.07809054106473923 Validation loss 0.07863333821296692 Accuracy 0.21240000426769257\n",
      "Iteration 890 Training loss 0.07893028110265732 Validation loss 0.07854946702718735 Accuracy 0.21320000290870667\n",
      "Iteration 900 Training loss 0.07786527276039124 Validation loss 0.07862392067909241 Accuracy 0.2126999944448471\n",
      "Iteration 910 Training loss 0.08042644709348679 Validation loss 0.07864705473184586 Accuracy 0.21199999749660492\n",
      "Iteration 920 Training loss 0.07865993678569794 Validation loss 0.07859581708908081 Accuracy 0.2126999944448471\n",
      "Iteration 930 Training loss 0.0785205066204071 Validation loss 0.07859119027853012 Accuracy 0.21250000596046448\n",
      "Iteration 940 Training loss 0.0787445679306984 Validation loss 0.07862266153097153 Accuracy 0.2125999927520752\n",
      "Iteration 950 Training loss 0.08190693706274033 Validation loss 0.07858160138130188 Accuracy 0.2126999944448471\n",
      "Iteration 960 Training loss 0.08122111856937408 Validation loss 0.07857762277126312 Accuracy 0.2126999944448471\n",
      "Iteration 970 Training loss 0.08152012526988983 Validation loss 0.07854045927524567 Accuracy 0.21310000121593475\n",
      "Iteration 980 Training loss 0.07659824937582016 Validation loss 0.07859679311513901 Accuracy 0.21250000596046448\n",
      "Iteration 990 Training loss 0.07753635942935944 Validation loss 0.0785912275314331 Accuracy 0.21250000596046448\n",
      "Iteration 1000 Training loss 0.07789251208305359 Validation loss 0.07866505533456802 Accuracy 0.2117999941110611\n",
      "Iteration 1010 Training loss 0.08265030384063721 Validation loss 0.07874121516942978 Accuracy 0.2110999971628189\n",
      "Iteration 1020 Training loss 0.07743265479803085 Validation loss 0.07876098155975342 Accuracy 0.2110999971628189\n",
      "Iteration 1030 Training loss 0.07794669270515442 Validation loss 0.07857611030340195 Accuracy 0.21289999783039093\n",
      "Iteration 1040 Training loss 0.08210507780313492 Validation loss 0.07859198749065399 Accuracy 0.21279999613761902\n",
      "Iteration 1050 Training loss 0.07785063236951828 Validation loss 0.0786144882440567 Accuracy 0.21240000426769257\n",
      "Iteration 1060 Training loss 0.07971841841936111 Validation loss 0.07867742329835892 Accuracy 0.21199999749660492\n",
      "Iteration 1070 Training loss 0.07764972746372223 Validation loss 0.07860749959945679 Accuracy 0.21250000596046448\n",
      "Iteration 1080 Training loss 0.08047057688236237 Validation loss 0.07863522320985794 Accuracy 0.21230000257492065\n",
      "Iteration 1090 Training loss 0.0808623880147934 Validation loss 0.07858522981405258 Accuracy 0.2126999944448471\n",
      "Iteration 1100 Training loss 0.07970810681581497 Validation loss 0.07858758419752121 Accuracy 0.2126999944448471\n",
      "Iteration 1110 Training loss 0.07950617372989655 Validation loss 0.0786098837852478 Accuracy 0.2125999927520752\n",
      "Iteration 1120 Training loss 0.07725287228822708 Validation loss 0.07855217903852463 Accuracy 0.21289999783039093\n",
      "Iteration 1130 Training loss 0.07667757570743561 Validation loss 0.07857915759086609 Accuracy 0.21289999783039093\n",
      "Iteration 1140 Training loss 0.07824353873729706 Validation loss 0.07859717309474945 Accuracy 0.2125999927520752\n",
      "Iteration 1150 Training loss 0.07633192837238312 Validation loss 0.07865779846906662 Accuracy 0.211899995803833\n",
      "Iteration 1160 Training loss 0.07914116233587265 Validation loss 0.07862354069948196 Accuracy 0.21220000088214874\n",
      "Iteration 1170 Training loss 0.07749782502651215 Validation loss 0.0786118134856224 Accuracy 0.21240000426769257\n",
      "Iteration 1180 Training loss 0.08057956397533417 Validation loss 0.07856310158967972 Accuracy 0.21310000121593475\n",
      "Iteration 1190 Training loss 0.07867380231618881 Validation loss 0.07860715687274933 Accuracy 0.2125999927520752\n",
      "Iteration 1200 Training loss 0.07631683349609375 Validation loss 0.07857848703861237 Accuracy 0.21279999613761902\n",
      "Iteration 1210 Training loss 0.0792563185095787 Validation loss 0.07855404913425446 Accuracy 0.21310000121593475\n",
      "Iteration 1220 Training loss 0.07854097336530685 Validation loss 0.07858239114284515 Accuracy 0.21289999783039093\n",
      "Iteration 1230 Training loss 0.08019036799669266 Validation loss 0.07858496904373169 Accuracy 0.2125999927520752\n",
      "Iteration 1240 Training loss 0.07671535760164261 Validation loss 0.07855689525604248 Accuracy 0.21279999613761902\n",
      "Iteration 1250 Training loss 0.08148038387298584 Validation loss 0.07860003411769867 Accuracy 0.21250000596046448\n",
      "Iteration 1260 Training loss 0.08189675211906433 Validation loss 0.07858835905790329 Accuracy 0.21250000596046448\n",
      "Iteration 1270 Training loss 0.08081745356321335 Validation loss 0.07857880741357803 Accuracy 0.2126999944448471\n",
      "Iteration 1280 Training loss 0.07878082990646362 Validation loss 0.07858461886644363 Accuracy 0.21279999613761902\n",
      "Iteration 1290 Training loss 0.08066253364086151 Validation loss 0.07859577238559723 Accuracy 0.2125999927520752\n",
      "Iteration 1300 Training loss 0.07904308289289474 Validation loss 0.07867144793272018 Accuracy 0.21220000088214874\n",
      "Iteration 1310 Training loss 0.07958795130252838 Validation loss 0.07867199927568436 Accuracy 0.21199999749660492\n",
      "Iteration 1320 Training loss 0.07957137376070023 Validation loss 0.07874331623315811 Accuracy 0.2110999971628189\n",
      "Iteration 1330 Training loss 0.08166521042585373 Validation loss 0.07877317816019058 Accuracy 0.21130000054836273\n",
      "Iteration 1340 Training loss 0.08028047531843185 Validation loss 0.07881268858909607 Accuracy 0.21070000529289246\n",
      "Iteration 1350 Training loss 0.08025643974542618 Validation loss 0.07873691618442535 Accuracy 0.21119999885559082\n",
      "Iteration 1360 Training loss 0.08073355257511139 Validation loss 0.07871876657009125 Accuracy 0.21150000393390656\n",
      "Iteration 1370 Training loss 0.08132054656744003 Validation loss 0.07876840978860855 Accuracy 0.21070000529289246\n",
      "Iteration 1380 Training loss 0.07831519842147827 Validation loss 0.07870621234178543 Accuracy 0.21160000562667847\n",
      "Iteration 1390 Training loss 0.07883427292108536 Validation loss 0.0786786898970604 Accuracy 0.21170000731945038\n",
      "Iteration 1400 Training loss 0.07952073961496353 Validation loss 0.07876548916101456 Accuracy 0.21119999885559082\n",
      "Iteration 1410 Training loss 0.07800676673650742 Validation loss 0.0786956325173378 Accuracy 0.21199999749660492\n",
      "Iteration 1420 Training loss 0.07908820360898972 Validation loss 0.07858416438102722 Accuracy 0.2126999944448471\n",
      "Iteration 1430 Training loss 0.07862692326307297 Validation loss 0.07858801633119583 Accuracy 0.2126999944448471\n",
      "Iteration 1440 Training loss 0.08191845566034317 Validation loss 0.078550785779953 Accuracy 0.21310000121593475\n",
      "Iteration 1450 Training loss 0.08247509598731995 Validation loss 0.0785873532295227 Accuracy 0.21279999613761902\n",
      "Iteration 1460 Training loss 0.07929842919111252 Validation loss 0.07867277413606644 Accuracy 0.21199999749660492\n",
      "Iteration 1470 Training loss 0.07586555927991867 Validation loss 0.07863258570432663 Accuracy 0.21199999749660492\n",
      "Iteration 1480 Training loss 0.07723875343799591 Validation loss 0.07855115830898285 Accuracy 0.21330000460147858\n",
      "Iteration 1490 Training loss 0.07592234760522842 Validation loss 0.07861698418855667 Accuracy 0.21289999783039093\n",
      "Iteration 1500 Training loss 0.08234355598688126 Validation loss 0.07861494272947311 Accuracy 0.21230000257492065\n",
      "Iteration 1510 Training loss 0.07746634632349014 Validation loss 0.07865218818187714 Accuracy 0.21230000257492065\n",
      "Iteration 1520 Training loss 0.07352928072214127 Validation loss 0.07863407582044601 Accuracy 0.21220000088214874\n",
      "Iteration 1530 Training loss 0.07932697236537933 Validation loss 0.07864423841238022 Accuracy 0.21209999918937683\n",
      "Iteration 1540 Training loss 0.08025854080915451 Validation loss 0.07863114029169083 Accuracy 0.21199999749660492\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[257]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_2_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[247]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mtwo_layer_NN.train_layers\u001b[39m\u001b[34m(self, x_train, y_train, x_valid, y_valid, kappa, lr, reg1, reg2, eps_init, fraction_batch, observation_rate, train_layer_1, train_layer_2)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\u001b[39;00m\n\u001b[32m    127\u001b[39m \n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Calcul des gradients\u001b[39;00m\n\u001b[32m    129\u001b[39m grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m grad_z2 = torch.einsum(\u001b[33m'\u001b[39m\u001b[33mnoz, no->nz\u001b[39m\u001b[33m'\u001b[39m, \u001b[43msoftmax_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m, grad_output); grad_z2  = grad_z2.to(dtype) \u001b[38;5;66;03m# shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\u001b[39;00m\n\u001b[32m    131\u001b[39m grad_h1 = torch.mm(grad_z2, \u001b[38;5;28mself\u001b[39m.W2); grad_h1  = grad_h1.to(dtype)  \u001b[38;5;66;03m# shape (n_data, hidden_1_size)\u001b[39;00m\n\u001b[32m    132\u001b[39m grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) \u001b[38;5;66;03m# shape (n_data, hidden_1_size)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[247]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36msoftmax_derivative\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):  \u001b[38;5;66;03m# Pour chaque chantillon du batch, on calcule la jacobienne de softmax\u001b[39;00m\n\u001b[32m     24\u001b[39m     si = s[i].unsqueeze(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# shape (C, 1) # vecteur softmax(logits) pour la i-me donne du batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     jacobians[i] = torch.diagflat(si) - \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msi\u001b[49m\u001b[43m,\u001b[49m\u001b[43msi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# calcul de la jacobienne (matrice des drives croises) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jacobians\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2, 1e-5, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer = three_layer_NN(784, 512, 512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c253240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 2, the number of datas used for the training is 61465600.0 and the number of iterations is 102442.\n",
      "Iteration 0 Training loss 0.08748721331357956 Validation loss 0.08817725628614426 Accuracy 0.102294921875\n",
      "Iteration 10 Training loss 0.07844120264053345 Validation loss 0.07942960411310196 Accuracy 0.200927734375\n",
      "Iteration 20 Training loss 0.08213479816913605 Validation loss 0.08172647655010223 Accuracy 0.1795654296875\n",
      "Iteration 30 Training loss 0.07555239647626877 Validation loss 0.07497331500053406 Accuracy 0.2467041015625\n",
      "Iteration 40 Training loss 0.07391677051782608 Validation loss 0.07393834739923477 Accuracy 0.257080078125\n",
      "Iteration 50 Training loss 0.07477433979511261 Validation loss 0.07315880805253983 Accuracy 0.265869140625\n",
      "Iteration 60 Training loss 0.0741143673658371 Validation loss 0.0726979598402977 Accuracy 0.2705078125\n",
      "Iteration 70 Training loss 0.06965716183185577 Validation loss 0.07211826741695404 Accuracy 0.273681640625\n",
      "Iteration 80 Training loss 0.07196424156427383 Validation loss 0.0720154345035553 Accuracy 0.275634765625\n",
      "Iteration 90 Training loss 0.07721260190010071 Validation loss 0.07507993280887604 Accuracy 0.246826171875\n",
      "Iteration 100 Training loss 0.0759267807006836 Validation loss 0.07473825663328171 Accuracy 0.2491455078125\n",
      "Iteration 110 Training loss 0.0717630386352539 Validation loss 0.07257895171642303 Accuracy 0.272216796875\n",
      "Iteration 120 Training loss 0.07214990258216858 Validation loss 0.07182168960571289 Accuracy 0.279296875\n",
      "Iteration 130 Training loss 0.07019497454166412 Validation loss 0.07167469710111618 Accuracy 0.281494140625\n",
      "Iteration 140 Training loss 0.07342763245105743 Validation loss 0.07225783169269562 Accuracy 0.27490234375\n",
      "Iteration 150 Training loss 0.07062767446041107 Validation loss 0.07140801101922989 Accuracy 0.283935546875\n",
      "Iteration 160 Training loss 0.0714464783668518 Validation loss 0.07155710458755493 Accuracy 0.28271484375\n",
      "Iteration 170 Training loss 0.06770876795053482 Validation loss 0.07163205742835999 Accuracy 0.280517578125\n",
      "Iteration 180 Training loss 0.07008950412273407 Validation loss 0.07130321860313416 Accuracy 0.284912109375\n",
      "Iteration 190 Training loss 0.07315066456794739 Validation loss 0.07129567116498947 Accuracy 0.2841796875\n",
      "Iteration 200 Training loss 0.07338987290859222 Validation loss 0.07246636599302292 Accuracy 0.272216796875\n",
      "Iteration 210 Training loss 0.07204689085483551 Validation loss 0.07138799130916595 Accuracy 0.283935546875\n",
      "Iteration 220 Training loss 0.07235988229513168 Validation loss 0.0716899186372757 Accuracy 0.280517578125\n",
      "Iteration 230 Training loss 0.06942839920520782 Validation loss 0.07127534598112106 Accuracy 0.285400390625\n",
      "Iteration 240 Training loss 0.07183083146810532 Validation loss 0.07111244648694992 Accuracy 0.286865234375\n",
      "Iteration 250 Training loss 0.07132838666439056 Validation loss 0.07122338563203812 Accuracy 0.285888671875\n",
      "Iteration 260 Training loss 0.07041209191083908 Validation loss 0.07107365131378174 Accuracy 0.287353515625\n",
      "Iteration 270 Training loss 0.07304098457098007 Validation loss 0.07182393223047256 Accuracy 0.2802734375\n",
      "Iteration 280 Training loss 0.07284821569919586 Validation loss 0.07115738838911057 Accuracy 0.287109375\n",
      "Iteration 290 Training loss 0.06719397753477097 Validation loss 0.07122351229190826 Accuracy 0.2861328125\n",
      "Iteration 300 Training loss 0.06866003572940826 Validation loss 0.07116074860095978 Accuracy 0.28662109375\n",
      "Iteration 310 Training loss 0.07362893968820572 Validation loss 0.07191783934831619 Accuracy 0.278564453125\n",
      "Iteration 320 Training loss 0.07060062885284424 Validation loss 0.07154465466737747 Accuracy 0.28271484375\n",
      "Iteration 330 Training loss 0.07253018766641617 Validation loss 0.0712813064455986 Accuracy 0.285400390625\n",
      "Iteration 340 Training loss 0.0675342008471489 Validation loss 0.07087622582912445 Accuracy 0.2890625\n",
      "Iteration 350 Training loss 0.07122082263231277 Validation loss 0.07136025279760361 Accuracy 0.284423828125\n",
      "Iteration 360 Training loss 0.07279221713542938 Validation loss 0.07099083065986633 Accuracy 0.2880859375\n",
      "Iteration 370 Training loss 0.07295078784227371 Validation loss 0.07121231406927109 Accuracy 0.28662109375\n",
      "Iteration 380 Training loss 0.0708935558795929 Validation loss 0.07141188532114029 Accuracy 0.284423828125\n",
      "Iteration 390 Training loss 0.07062819600105286 Validation loss 0.07096616178750992 Accuracy 0.288330078125\n",
      "Iteration 400 Training loss 0.07079897075891495 Validation loss 0.07133126258850098 Accuracy 0.28515625\n",
      "Iteration 410 Training loss 0.0764952301979065 Validation loss 0.07111677527427673 Accuracy 0.286865234375\n",
      "Iteration 420 Training loss 0.07374773919582367 Validation loss 0.0709817185997963 Accuracy 0.288330078125\n",
      "Iteration 430 Training loss 0.069486603140831 Validation loss 0.07096990197896957 Accuracy 0.28857421875\n",
      "Iteration 440 Training loss 0.07080136984586716 Validation loss 0.07135660201311111 Accuracy 0.284912109375\n",
      "Iteration 450 Training loss 0.07041679322719574 Validation loss 0.07168243080377579 Accuracy 0.28173828125\n",
      "Iteration 460 Training loss 0.06918210536241531 Validation loss 0.07151027768850327 Accuracy 0.2841796875\n",
      "Iteration 470 Training loss 0.07236537337303162 Validation loss 0.07149641215801239 Accuracy 0.2841796875\n",
      "Iteration 480 Training loss 0.07356106489896774 Validation loss 0.07383649051189423 Accuracy 0.259033203125\n",
      "Iteration 490 Training loss 0.07197108864784241 Validation loss 0.07144664227962494 Accuracy 0.283935546875\n",
      "Iteration 500 Training loss 0.07025496661663055 Validation loss 0.07100322097539902 Accuracy 0.28857421875\n",
      "Iteration 510 Training loss 0.06805302947759628 Validation loss 0.07116653025150299 Accuracy 0.286865234375\n",
      "Iteration 520 Training loss 0.07254565507173538 Validation loss 0.07102245837450027 Accuracy 0.288818359375\n",
      "Iteration 530 Training loss 0.0699809119105339 Validation loss 0.0709109976887703 Accuracy 0.28955078125\n",
      "Iteration 540 Training loss 0.07113824039697647 Validation loss 0.07132300734519958 Accuracy 0.285400390625\n",
      "Iteration 550 Training loss 0.0713396817445755 Validation loss 0.07096821069717407 Accuracy 0.288818359375\n",
      "Iteration 560 Training loss 0.06839148700237274 Validation loss 0.07093196362257004 Accuracy 0.289306640625\n",
      "Iteration 570 Training loss 0.07086930423974991 Validation loss 0.07089254260063171 Accuracy 0.289794921875\n",
      "Iteration 580 Training loss 0.07126834243535995 Validation loss 0.07098757475614548 Accuracy 0.288330078125\n",
      "Iteration 590 Training loss 0.06976868957281113 Validation loss 0.07094131410121918 Accuracy 0.288818359375\n",
      "Iteration 600 Training loss 0.06865645945072174 Validation loss 0.0709078311920166 Accuracy 0.2890625\n",
      "Iteration 610 Training loss 0.069943867623806 Validation loss 0.07087381929159164 Accuracy 0.2900390625\n",
      "Iteration 620 Training loss 0.07164662331342697 Validation loss 0.07086215913295746 Accuracy 0.289794921875\n",
      "Iteration 630 Training loss 0.07246143370866776 Validation loss 0.07170584052801132 Accuracy 0.281005859375\n",
      "Iteration 640 Training loss 0.07280461490154266 Validation loss 0.07087419927120209 Accuracy 0.289794921875\n",
      "Iteration 650 Training loss 0.07073314487934113 Validation loss 0.0709058940410614 Accuracy 0.28955078125\n",
      "Iteration 660 Training loss 0.0718618780374527 Validation loss 0.07140098512172699 Accuracy 0.284423828125\n",
      "Iteration 670 Training loss 0.06944730132818222 Validation loss 0.07085523754358292 Accuracy 0.290283203125\n",
      "Iteration 680 Training loss 0.06835298240184784 Validation loss 0.070886991918087 Accuracy 0.2900390625\n",
      "Iteration 690 Training loss 0.06835570186376572 Validation loss 0.0709516704082489 Accuracy 0.288818359375\n",
      "Iteration 700 Training loss 0.07111096382141113 Validation loss 0.0710340365767479 Accuracy 0.287841796875\n",
      "Iteration 710 Training loss 0.0693669468164444 Validation loss 0.07090948522090912 Accuracy 0.289306640625\n",
      "Iteration 720 Training loss 0.07367297261953354 Validation loss 0.0709872841835022 Accuracy 0.288818359375\n",
      "Iteration 730 Training loss 0.07023213803768158 Validation loss 0.07189958542585373 Accuracy 0.279541015625\n",
      "Iteration 740 Training loss 0.0705447718501091 Validation loss 0.07095008343458176 Accuracy 0.289306640625\n",
      "Iteration 750 Training loss 0.07101698964834213 Validation loss 0.07118270546197891 Accuracy 0.28662109375\n",
      "Iteration 760 Training loss 0.07190178334712982 Validation loss 0.07092393189668655 Accuracy 0.288818359375\n",
      "Iteration 770 Training loss 0.07203580439090729 Validation loss 0.07095949351787567 Accuracy 0.2890625\n",
      "Iteration 780 Training loss 0.07244843989610672 Validation loss 0.07166728377342224 Accuracy 0.281494140625\n",
      "Iteration 790 Training loss 0.06828044354915619 Validation loss 0.07156912237405777 Accuracy 0.282958984375\n",
      "Iteration 800 Training loss 0.06984912604093552 Validation loss 0.0709584653377533 Accuracy 0.289306640625\n",
      "Iteration 810 Training loss 0.0709235817193985 Validation loss 0.07130976021289825 Accuracy 0.28564453125\n",
      "Iteration 820 Training loss 0.06978222727775574 Validation loss 0.07090403884649277 Accuracy 0.2900390625\n",
      "Iteration 830 Training loss 0.0666196420788765 Validation loss 0.07110550254583359 Accuracy 0.287109375\n",
      "Iteration 840 Training loss 0.07090701162815094 Validation loss 0.07107207924127579 Accuracy 0.28759765625\n",
      "Iteration 850 Training loss 0.0703013613820076 Validation loss 0.07100392878055573 Accuracy 0.287841796875\n",
      "Iteration 860 Training loss 0.06607306748628616 Validation loss 0.07084790617227554 Accuracy 0.290283203125\n",
      "Iteration 870 Training loss 0.06924082338809967 Validation loss 0.070888452231884 Accuracy 0.28955078125\n",
      "Iteration 880 Training loss 0.07218343764543533 Validation loss 0.07095890492200851 Accuracy 0.2890625\n",
      "Iteration 890 Training loss 0.07241138815879822 Validation loss 0.07146570086479187 Accuracy 0.283935546875\n",
      "Iteration 900 Training loss 0.07275329530239105 Validation loss 0.07120218127965927 Accuracy 0.28662109375\n",
      "Iteration 910 Training loss 0.06899648904800415 Validation loss 0.07096637785434723 Accuracy 0.289306640625\n",
      "Iteration 920 Training loss 0.07266399264335632 Validation loss 0.07096680998802185 Accuracy 0.288818359375\n",
      "Iteration 930 Training loss 0.07198655605316162 Validation loss 0.07093052566051483 Accuracy 0.28955078125\n",
      "Iteration 940 Training loss 0.07313224673271179 Validation loss 0.07088159769773483 Accuracy 0.289794921875\n",
      "Iteration 950 Training loss 0.07198300957679749 Validation loss 0.07088141143321991 Accuracy 0.289794921875\n",
      "Iteration 960 Training loss 0.07069901376962662 Validation loss 0.07090603560209274 Accuracy 0.289306640625\n",
      "Iteration 970 Training loss 0.068463996052742 Validation loss 0.07124479115009308 Accuracy 0.28564453125\n",
      "Iteration 980 Training loss 0.07002169638872147 Validation loss 0.07087346166372299 Accuracy 0.2900390625\n",
      "Iteration 990 Training loss 0.07251188158988953 Validation loss 0.07091904431581497 Accuracy 0.288818359375\n",
      "Iteration 1000 Training loss 0.07140665501356125 Validation loss 0.07091684639453888 Accuracy 0.289794921875\n",
      "Iteration 1010 Training loss 0.07076683640480042 Validation loss 0.07089675217866898 Accuracy 0.2900390625\n",
      "Iteration 1020 Training loss 0.07277538627386093 Validation loss 0.0711304098367691 Accuracy 0.28759765625\n",
      "Iteration 1030 Training loss 0.06998053193092346 Validation loss 0.07096902281045914 Accuracy 0.2890625\n",
      "Iteration 1040 Training loss 0.07045979052782059 Validation loss 0.07197359204292297 Accuracy 0.279052734375\n",
      "Iteration 1050 Training loss 0.0690397322177887 Validation loss 0.0712752640247345 Accuracy 0.286376953125\n",
      "Iteration 1060 Training loss 0.06954051554203033 Validation loss 0.0708736777305603 Accuracy 0.2900390625\n",
      "Iteration 1070 Training loss 0.07047631591558456 Validation loss 0.07087226957082748 Accuracy 0.2900390625\n",
      "Iteration 1080 Training loss 0.07157787680625916 Validation loss 0.07090029865503311 Accuracy 0.2900390625\n",
      "Iteration 1090 Training loss 0.07485942542552948 Validation loss 0.07091999053955078 Accuracy 0.2900390625\n",
      "Iteration 1100 Training loss 0.06777343153953552 Validation loss 0.07096996158361435 Accuracy 0.289306640625\n",
      "Iteration 1110 Training loss 0.07133302092552185 Validation loss 0.07100218534469604 Accuracy 0.28857421875\n",
      "Iteration 1120 Training loss 0.07427632808685303 Validation loss 0.07083120197057724 Accuracy 0.290771484375\n",
      "Iteration 1130 Training loss 0.07330437004566193 Validation loss 0.07113571465015411 Accuracy 0.28759765625\n",
      "Iteration 1140 Training loss 0.07542432844638824 Validation loss 0.07089205086231232 Accuracy 0.289794921875\n",
      "Iteration 1150 Training loss 0.07413903623819351 Validation loss 0.07091964036226273 Accuracy 0.2900390625\n",
      "Iteration 1160 Training loss 0.0711735188961029 Validation loss 0.07084916532039642 Accuracy 0.2900390625\n",
      "Iteration 1170 Training loss 0.0750732347369194 Validation loss 0.07110694795846939 Accuracy 0.287841796875\n",
      "Iteration 1180 Training loss 0.0701955258846283 Validation loss 0.07074986398220062 Accuracy 0.291259765625\n",
      "Iteration 1190 Training loss 0.06984959542751312 Validation loss 0.07078535854816437 Accuracy 0.290771484375\n",
      "Iteration 1200 Training loss 0.06958358734846115 Validation loss 0.07079155743122101 Accuracy 0.290771484375\n",
      "Iteration 1210 Training loss 0.06942287087440491 Validation loss 0.07082700729370117 Accuracy 0.290283203125\n",
      "Iteration 1220 Training loss 0.0716485008597374 Validation loss 0.07092670351266861 Accuracy 0.289794921875\n",
      "Iteration 1230 Training loss 0.07133103907108307 Validation loss 0.07110080868005753 Accuracy 0.2880859375\n",
      "Iteration 1240 Training loss 0.07077404856681824 Validation loss 0.07095834612846375 Accuracy 0.28955078125\n",
      "Iteration 1250 Training loss 0.07206308096647263 Validation loss 0.07091616094112396 Accuracy 0.289306640625\n",
      "Iteration 1260 Training loss 0.07053090631961823 Validation loss 0.07075399905443192 Accuracy 0.291015625\n",
      "Iteration 1270 Training loss 0.07068249583244324 Validation loss 0.07090498507022858 Accuracy 0.28857421875\n",
      "Iteration 1280 Training loss 0.0731329470872879 Validation loss 0.07084915786981583 Accuracy 0.2900390625\n",
      "Iteration 1290 Training loss 0.06880169361829758 Validation loss 0.07100631296634674 Accuracy 0.28759765625\n",
      "Iteration 1300 Training loss 0.0708022266626358 Validation loss 0.07068739086389542 Accuracy 0.290771484375\n",
      "Iteration 1310 Training loss 0.07250919193029404 Validation loss 0.07072808593511581 Accuracy 0.2900390625\n",
      "Iteration 1320 Training loss 0.06867685914039612 Validation loss 0.07213373482227325 Accuracy 0.276123046875\n",
      "Iteration 1330 Training loss 0.07291387021541595 Validation loss 0.07077855616807938 Accuracy 0.2900390625\n",
      "Iteration 1340 Training loss 0.073598712682724 Validation loss 0.07077180594205856 Accuracy 0.2900390625\n",
      "Iteration 1350 Training loss 0.07080293446779251 Validation loss 0.07067251950502396 Accuracy 0.290771484375\n",
      "Iteration 1360 Training loss 0.07300635427236557 Validation loss 0.07115969806909561 Accuracy 0.287353515625\n",
      "Iteration 1370 Training loss 0.06955400854349136 Validation loss 0.07073092460632324 Accuracy 0.291259765625\n",
      "Iteration 1380 Training loss 0.07030471414327621 Validation loss 0.07099571079015732 Accuracy 0.288330078125\n",
      "Iteration 1390 Training loss 0.07154569774866104 Validation loss 0.0710015669465065 Accuracy 0.288330078125\n",
      "Iteration 1400 Training loss 0.07042553275823593 Validation loss 0.07072373479604721 Accuracy 0.291748046875\n",
      "Iteration 1410 Training loss 0.07128766179084778 Validation loss 0.07076279819011688 Accuracy 0.290771484375\n",
      "Iteration 1420 Training loss 0.0720587745308876 Validation loss 0.07083477079868317 Accuracy 0.28857421875\n",
      "Iteration 1430 Training loss 0.07040541619062424 Validation loss 0.07071482390165329 Accuracy 0.29052734375\n",
      "Iteration 1440 Training loss 0.07119150459766388 Validation loss 0.07074024528265 Accuracy 0.289794921875\n",
      "Iteration 1450 Training loss 0.06773757189512253 Validation loss 0.07065847516059875 Accuracy 0.2919921875\n",
      "Iteration 1460 Training loss 0.07127724587917328 Validation loss 0.07126982510089874 Accuracy 0.284912109375\n",
      "Iteration 1470 Training loss 0.06687890738248825 Validation loss 0.07065518200397491 Accuracy 0.290771484375\n",
      "Iteration 1480 Training loss 0.07309740781784058 Validation loss 0.07085776329040527 Accuracy 0.2900390625\n",
      "Iteration 1490 Training loss 0.07057816535234451 Validation loss 0.07064719498157501 Accuracy 0.291259765625\n",
      "Iteration 1500 Training loss 0.07122515887022018 Validation loss 0.07073195278644562 Accuracy 0.290283203125\n",
      "Iteration 1510 Training loss 0.07100459188222885 Validation loss 0.07064064592123032 Accuracy 0.2919921875\n",
      "Iteration 1520 Training loss 0.0718037411570549 Validation loss 0.07067393511533737 Accuracy 0.291748046875\n",
      "Iteration 1530 Training loss 0.06797703355550766 Validation loss 0.07073825597763062 Accuracy 0.291015625\n",
      "Iteration 1540 Training loss 0.07115545868873596 Validation loss 0.07071293145418167 Accuracy 0.291015625\n",
      "Iteration 1550 Training loss 0.06913604587316513 Validation loss 0.07074557989835739 Accuracy 0.291259765625\n",
      "Iteration 1560 Training loss 0.07267960906028748 Validation loss 0.07196936011314392 Accuracy 0.278564453125\n",
      "Iteration 1570 Training loss 0.07157102227210999 Validation loss 0.07077410817146301 Accuracy 0.291015625\n",
      "Iteration 1580 Training loss 0.06872744113206863 Validation loss 0.07076103985309601 Accuracy 0.29150390625\n",
      "Iteration 1590 Training loss 0.07011581212282181 Validation loss 0.07077451795339584 Accuracy 0.29150390625\n",
      "Iteration 1600 Training loss 0.07277750223875046 Validation loss 0.07133348286151886 Accuracy 0.285888671875\n",
      "Iteration 1610 Training loss 0.07265085726976395 Validation loss 0.07070106267929077 Accuracy 0.2919921875\n",
      "Iteration 1620 Training loss 0.06966511905193329 Validation loss 0.07074091583490372 Accuracy 0.29150390625\n",
      "Iteration 1630 Training loss 0.06842321902513504 Validation loss 0.07075219601392746 Accuracy 0.29150390625\n",
      "Iteration 1640 Training loss 0.07073908299207687 Validation loss 0.07097158581018448 Accuracy 0.2890625\n",
      "Iteration 1650 Training loss 0.07242466509342194 Validation loss 0.07067615538835526 Accuracy 0.292236328125\n",
      "Iteration 1660 Training loss 0.06931308656930923 Validation loss 0.07064443081617355 Accuracy 0.29150390625\n",
      "Iteration 1670 Training loss 0.07115614414215088 Validation loss 0.07066036760807037 Accuracy 0.291259765625\n",
      "Iteration 1680 Training loss 0.06997215747833252 Validation loss 0.07067224383354187 Accuracy 0.290771484375\n",
      "Iteration 1690 Training loss 0.0707758367061615 Validation loss 0.07077130675315857 Accuracy 0.29150390625\n",
      "Iteration 1700 Training loss 0.06885051727294922 Validation loss 0.07067126035690308 Accuracy 0.291259765625\n",
      "Iteration 1710 Training loss 0.06856928765773773 Validation loss 0.07082043588161469 Accuracy 0.289306640625\n",
      "Iteration 1720 Training loss 0.07103782892227173 Validation loss 0.07075110077857971 Accuracy 0.29150390625\n",
      "Iteration 1730 Training loss 0.06994456797838211 Validation loss 0.07081338763237 Accuracy 0.291015625\n",
      "Iteration 1740 Training loss 0.07197965681552887 Validation loss 0.07089214026927948 Accuracy 0.2900390625\n",
      "Iteration 1750 Training loss 0.07030081003904343 Validation loss 0.07095678895711899 Accuracy 0.289794921875\n",
      "Iteration 1760 Training loss 0.06868743151426315 Validation loss 0.07078579068183899 Accuracy 0.29052734375\n",
      "Iteration 1770 Training loss 0.06928679347038269 Validation loss 0.07105005532503128 Accuracy 0.28857421875\n",
      "Iteration 1780 Training loss 0.06943286955356598 Validation loss 0.07084399461746216 Accuracy 0.29052734375\n",
      "Iteration 1790 Training loss 0.06840281933546066 Validation loss 0.07082416862249374 Accuracy 0.291015625\n",
      "Iteration 1800 Training loss 0.06697490066289902 Validation loss 0.07107087969779968 Accuracy 0.287841796875\n",
      "Iteration 1810 Training loss 0.07013625651597977 Validation loss 0.07086192816495895 Accuracy 0.288818359375\n",
      "Iteration 1820 Training loss 0.07050022482872009 Validation loss 0.0708223432302475 Accuracy 0.289794921875\n",
      "Iteration 1830 Training loss 0.07275490462779999 Validation loss 0.07095272094011307 Accuracy 0.287353515625\n",
      "Iteration 1840 Training loss 0.06845035403966904 Validation loss 0.0706239566206932 Accuracy 0.29150390625\n",
      "Iteration 1850 Training loss 0.06880553066730499 Validation loss 0.07112187892198563 Accuracy 0.28515625\n",
      "Iteration 1860 Training loss 0.06951829791069031 Validation loss 0.07079421728849411 Accuracy 0.2890625\n",
      "Iteration 1870 Training loss 0.0690208151936531 Validation loss 0.07135221362113953 Accuracy 0.28271484375\n",
      "Iteration 1880 Training loss 0.07079115509986877 Validation loss 0.07131745666265488 Accuracy 0.283447265625\n",
      "Iteration 1890 Training loss 0.07010744512081146 Validation loss 0.07114146649837494 Accuracy 0.28564453125\n",
      "Iteration 1900 Training loss 0.0745733305811882 Validation loss 0.07071832567453384 Accuracy 0.28955078125\n",
      "Iteration 1910 Training loss 0.07295206934213638 Validation loss 0.07064103335142136 Accuracy 0.289794921875\n",
      "Iteration 1920 Training loss 0.07357163727283478 Validation loss 0.07076605409383774 Accuracy 0.289306640625\n",
      "Iteration 1930 Training loss 0.07053698599338531 Validation loss 0.07105719298124313 Accuracy 0.28564453125\n",
      "Iteration 1940 Training loss 0.071494922041893 Validation loss 0.07091862708330154 Accuracy 0.2880859375\n",
      "Iteration 1950 Training loss 0.07048382610082626 Validation loss 0.0708765983581543 Accuracy 0.287841796875\n",
      "Iteration 1960 Training loss 0.07481282204389572 Validation loss 0.07083059102296829 Accuracy 0.28857421875\n",
      "Iteration 1970 Training loss 0.07017706334590912 Validation loss 0.07056481391191483 Accuracy 0.29248046875\n",
      "Iteration 1980 Training loss 0.07019739598035812 Validation loss 0.07067269086837769 Accuracy 0.290771484375\n",
      "Iteration 1990 Training loss 0.06763412058353424 Validation loss 0.07069166004657745 Accuracy 0.291015625\n",
      "Iteration 2000 Training loss 0.0715768039226532 Validation loss 0.07068493217229843 Accuracy 0.29150390625\n",
      "Iteration 2010 Training loss 0.0712374597787857 Validation loss 0.07066895067691803 Accuracy 0.29248046875\n",
      "Iteration 2020 Training loss 0.0695357546210289 Validation loss 0.07060419768095016 Accuracy 0.29248046875\n",
      "Iteration 2030 Training loss 0.07241848856210709 Validation loss 0.07063541561365128 Accuracy 0.29248046875\n",
      "Iteration 2040 Training loss 0.07348804920911789 Validation loss 0.07060158252716064 Accuracy 0.29248046875\n",
      "Iteration 2050 Training loss 0.0712938904762268 Validation loss 0.07121356576681137 Accuracy 0.2861328125\n",
      "Iteration 2060 Training loss 0.07210507988929749 Validation loss 0.07067672908306122 Accuracy 0.29150390625\n",
      "Iteration 2070 Training loss 0.06989893317222595 Validation loss 0.07065939903259277 Accuracy 0.29248046875\n",
      "Iteration 2080 Training loss 0.07113947719335556 Validation loss 0.07099207490682602 Accuracy 0.288818359375\n",
      "Iteration 2090 Training loss 0.0723721832036972 Validation loss 0.0707317441701889 Accuracy 0.29150390625\n",
      "Iteration 2100 Training loss 0.06872650980949402 Validation loss 0.0706641748547554 Accuracy 0.29248046875\n",
      "Iteration 2110 Training loss 0.06982405483722687 Validation loss 0.07068432122468948 Accuracy 0.2919921875\n",
      "Iteration 2120 Training loss 0.07035789638757706 Validation loss 0.0706649050116539 Accuracy 0.290771484375\n",
      "Iteration 2130 Training loss 0.07042589038610458 Validation loss 0.07068021595478058 Accuracy 0.291259765625\n",
      "Iteration 2140 Training loss 0.0722566619515419 Validation loss 0.0706423744559288 Accuracy 0.29052734375\n",
      "Iteration 2150 Training loss 0.07101913541555405 Validation loss 0.07063884288072586 Accuracy 0.2919921875\n",
      "Iteration 2160 Training loss 0.06970888376235962 Validation loss 0.07066420465707779 Accuracy 0.2919921875\n",
      "Iteration 2170 Training loss 0.0715966522693634 Validation loss 0.07069651037454605 Accuracy 0.291748046875\n",
      "Iteration 2180 Training loss 0.06930733472108841 Validation loss 0.0706821084022522 Accuracy 0.2919921875\n",
      "Iteration 2190 Training loss 0.0718834325671196 Validation loss 0.07062046229839325 Accuracy 0.292236328125\n",
      "Iteration 2200 Training loss 0.06940855830907822 Validation loss 0.07065702229738235 Accuracy 0.29150390625\n",
      "Iteration 2210 Training loss 0.06970285624265671 Validation loss 0.07123170793056488 Accuracy 0.284423828125\n",
      "Iteration 2220 Training loss 0.06686464697122574 Validation loss 0.07082045823335648 Accuracy 0.289306640625\n",
      "Iteration 2230 Training loss 0.06926864385604858 Validation loss 0.0708613395690918 Accuracy 0.288818359375\n",
      "Iteration 2240 Training loss 0.06798002868890762 Validation loss 0.07064370810985565 Accuracy 0.290771484375\n",
      "Iteration 2250 Training loss 0.0706840232014656 Validation loss 0.07077696174383163 Accuracy 0.2900390625\n",
      "Iteration 2260 Training loss 0.07095770537853241 Validation loss 0.07077760249376297 Accuracy 0.290283203125\n",
      "Iteration 2270 Training loss 0.07245387881994247 Validation loss 0.07065604627132416 Accuracy 0.29150390625\n",
      "Iteration 2280 Training loss 0.07010240852832794 Validation loss 0.07079929113388062 Accuracy 0.2900390625\n",
      "Iteration 2290 Training loss 0.07222370058298111 Validation loss 0.07076942175626755 Accuracy 0.290283203125\n",
      "Iteration 2300 Training loss 0.07244838029146194 Validation loss 0.0712137371301651 Accuracy 0.28564453125\n",
      "Iteration 2310 Training loss 0.06916630268096924 Validation loss 0.07069013267755508 Accuracy 0.291015625\n",
      "Iteration 2320 Training loss 0.07256174832582474 Validation loss 0.07079630345106125 Accuracy 0.2900390625\n",
      "Iteration 2330 Training loss 0.06882895529270172 Validation loss 0.07068304717540741 Accuracy 0.2919921875\n",
      "Iteration 2340 Training loss 0.06989791989326477 Validation loss 0.07068132609128952 Accuracy 0.291015625\n",
      "Iteration 2350 Training loss 0.07419371604919434 Validation loss 0.07086748629808426 Accuracy 0.2900390625\n",
      "Iteration 2360 Training loss 0.07168794423341751 Validation loss 0.07079755514860153 Accuracy 0.290771484375\n",
      "Iteration 2370 Training loss 0.07276266068220139 Validation loss 0.07078531384468079 Accuracy 0.291015625\n",
      "Iteration 2380 Training loss 0.06886181235313416 Validation loss 0.07063354551792145 Accuracy 0.2919921875\n",
      "Iteration 2390 Training loss 0.06901255995035172 Validation loss 0.07060328125953674 Accuracy 0.29150390625\n",
      "Iteration 2400 Training loss 0.07289545238018036 Validation loss 0.07106277346611023 Accuracy 0.287109375\n",
      "Iteration 2410 Training loss 0.0697309672832489 Validation loss 0.07063058018684387 Accuracy 0.2919921875\n",
      "Iteration 2420 Training loss 0.06965501606464386 Validation loss 0.0710323303937912 Accuracy 0.288818359375\n",
      "Iteration 2430 Training loss 0.07050737738609314 Validation loss 0.07064785808324814 Accuracy 0.292236328125\n",
      "Iteration 2440 Training loss 0.06561043113470078 Validation loss 0.07064936310052872 Accuracy 0.29248046875\n",
      "Iteration 2450 Training loss 0.07214679569005966 Validation loss 0.07063011825084686 Accuracy 0.291015625\n",
      "Iteration 2460 Training loss 0.07411666959524155 Validation loss 0.07073312997817993 Accuracy 0.2890625\n",
      "Iteration 2470 Training loss 0.0713767558336258 Validation loss 0.07230707257986069 Accuracy 0.27490234375\n",
      "Iteration 2480 Training loss 0.07014410197734833 Validation loss 0.07052874565124512 Accuracy 0.29150390625\n",
      "Iteration 2490 Training loss 0.07055525481700897 Validation loss 0.07196377962827682 Accuracy 0.278076171875\n",
      "Iteration 2500 Training loss 0.07023564726114273 Validation loss 0.07086565345525742 Accuracy 0.2880859375\n",
      "Iteration 2510 Training loss 0.07208947837352753 Validation loss 0.0707261860370636 Accuracy 0.289306640625\n",
      "Iteration 2520 Training loss 0.07183244824409485 Validation loss 0.06986680626869202 Accuracy 0.298095703125\n",
      "Iteration 2530 Training loss 0.06352957338094711 Validation loss 0.06436977535486221 Accuracy 0.352783203125\n",
      "Iteration 2540 Training loss 0.06372959166765213 Validation loss 0.06253422051668167 Accuracy 0.368896484375\n",
      "Iteration 2550 Training loss 0.06333152204751968 Validation loss 0.06245812028646469 Accuracy 0.37060546875\n",
      "Iteration 2560 Training loss 0.058828845620155334 Validation loss 0.06276752054691315 Accuracy 0.369384765625\n",
      "Iteration 2570 Training loss 0.061252329498529434 Validation loss 0.06163516268134117 Accuracy 0.37744140625\n",
      "Iteration 2580 Training loss 0.059775106608867645 Validation loss 0.06322302669286728 Accuracy 0.36328125\n",
      "Iteration 2590 Training loss 0.06182854250073433 Validation loss 0.06204444169998169 Accuracy 0.377197265625\n",
      "Iteration 2600 Training loss 0.06367304176092148 Validation loss 0.06188160181045532 Accuracy 0.377197265625\n",
      "Iteration 2610 Training loss 0.05975241959095001 Validation loss 0.061960820108652115 Accuracy 0.376220703125\n",
      "Iteration 2620 Training loss 0.0621224045753479 Validation loss 0.06307647377252579 Accuracy 0.3671875\n",
      "Iteration 2630 Training loss 0.06122095510363579 Validation loss 0.061656318604946136 Accuracy 0.381103515625\n",
      "Iteration 2640 Training loss 0.05953725799918175 Validation loss 0.06144162267446518 Accuracy 0.382080078125\n",
      "Iteration 2650 Training loss 0.06492391228675842 Validation loss 0.06219768524169922 Accuracy 0.374267578125\n",
      "Iteration 2660 Training loss 0.06497340649366379 Validation loss 0.061347831040620804 Accuracy 0.3828125\n",
      "Iteration 2670 Training loss 0.06079782545566559 Validation loss 0.062002670019865036 Accuracy 0.375732421875\n",
      "Iteration 2680 Training loss 0.06257936358451843 Validation loss 0.06122655048966408 Accuracy 0.3828125\n",
      "Iteration 2690 Training loss 0.058907561004161835 Validation loss 0.06129096448421478 Accuracy 0.382568359375\n",
      "Iteration 2700 Training loss 0.06277383118867874 Validation loss 0.0619521290063858 Accuracy 0.37548828125\n",
      "Iteration 2710 Training loss 0.06104732304811478 Validation loss 0.06258106976747513 Accuracy 0.370361328125\n",
      "Iteration 2720 Training loss 0.05796518921852112 Validation loss 0.06137348338961601 Accuracy 0.382568359375\n",
      "Iteration 2730 Training loss 0.059293508529663086 Validation loss 0.061555009335279465 Accuracy 0.38037109375\n",
      "Iteration 2740 Training loss 0.061566539108753204 Validation loss 0.061494167894124985 Accuracy 0.382568359375\n",
      "Iteration 2750 Training loss 0.05821249261498451 Validation loss 0.0615902915596962 Accuracy 0.38134765625\n",
      "Iteration 2760 Training loss 0.0614095963537693 Validation loss 0.062975212931633 Accuracy 0.36767578125\n",
      "Iteration 2770 Training loss 0.06117112189531326 Validation loss 0.06176271662116051 Accuracy 0.378662109375\n",
      "Iteration 2780 Training loss 0.05855633318424225 Validation loss 0.0615900419652462 Accuracy 0.38037109375\n",
      "Iteration 2790 Training loss 0.0639413371682167 Validation loss 0.06248094514012337 Accuracy 0.373291015625\n",
      "Iteration 2800 Training loss 0.06260446459054947 Validation loss 0.06253065913915634 Accuracy 0.371337890625\n",
      "Iteration 2810 Training loss 0.06112275272607803 Validation loss 0.06161097064614296 Accuracy 0.3818359375\n",
      "Iteration 2820 Training loss 0.06475833058357239 Validation loss 0.061128851026296616 Accuracy 0.385009765625\n",
      "Iteration 2830 Training loss 0.060731496661901474 Validation loss 0.06122457981109619 Accuracy 0.3857421875\n",
      "Iteration 2840 Training loss 0.06510581076145172 Validation loss 0.06162354350090027 Accuracy 0.38232421875\n",
      "Iteration 2850 Training loss 0.05939977988600731 Validation loss 0.06167266517877579 Accuracy 0.3818359375\n",
      "Iteration 2860 Training loss 0.06048210710287094 Validation loss 0.06147334724664688 Accuracy 0.3837890625\n",
      "Iteration 2870 Training loss 0.05802134796977043 Validation loss 0.06121779978275299 Accuracy 0.3857421875\n",
      "Iteration 2880 Training loss 0.062283292412757874 Validation loss 0.0611722394824028 Accuracy 0.385498046875\n",
      "Iteration 2890 Training loss 0.06415450572967529 Validation loss 0.061778221279382706 Accuracy 0.381103515625\n",
      "Iteration 2900 Training loss 0.058627452701330185 Validation loss 0.06118112802505493 Accuracy 0.384521484375\n",
      "Iteration 2910 Training loss 0.06334862858057022 Validation loss 0.06154220551252365 Accuracy 0.381591796875\n",
      "Iteration 2920 Training loss 0.06068137660622597 Validation loss 0.06144697591662407 Accuracy 0.38330078125\n",
      "Iteration 2930 Training loss 0.06126343831419945 Validation loss 0.06243954598903656 Accuracy 0.372802734375\n",
      "Iteration 2940 Training loss 0.05811047926545143 Validation loss 0.061227887868881226 Accuracy 0.38427734375\n",
      "Iteration 2950 Training loss 0.06203186884522438 Validation loss 0.061082873493433 Accuracy 0.385986328125\n",
      "Iteration 2960 Training loss 0.05871753767132759 Validation loss 0.06127851828932762 Accuracy 0.385009765625\n",
      "Iteration 2970 Training loss 0.06067468225955963 Validation loss 0.06135697662830353 Accuracy 0.3828125\n",
      "Iteration 2980 Training loss 0.06110954284667969 Validation loss 0.061166733503341675 Accuracy 0.385009765625\n",
      "Iteration 2990 Training loss 0.062137421220541 Validation loss 0.061334967613220215 Accuracy 0.3828125\n",
      "Iteration 3000 Training loss 0.06444761902093887 Validation loss 0.06132418289780617 Accuracy 0.38232421875\n",
      "Iteration 3010 Training loss 0.05919363722205162 Validation loss 0.06109168380498886 Accuracy 0.38525390625\n",
      "Iteration 3020 Training loss 0.06118636578321457 Validation loss 0.061317961663007736 Accuracy 0.38330078125\n",
      "Iteration 3030 Training loss 0.06192416325211525 Validation loss 0.06166740134358406 Accuracy 0.380126953125\n",
      "Iteration 3040 Training loss 0.061675842851400375 Validation loss 0.06171690300107002 Accuracy 0.379638671875\n",
      "Iteration 3050 Training loss 0.06495697796344757 Validation loss 0.06126938387751579 Accuracy 0.38427734375\n",
      "Iteration 3060 Training loss 0.06437855213880539 Validation loss 0.06116185337305069 Accuracy 0.385009765625\n",
      "Iteration 3070 Training loss 0.05759882554411888 Validation loss 0.06118816137313843 Accuracy 0.384521484375\n",
      "Iteration 3080 Training loss 0.05655236169695854 Validation loss 0.06118180602788925 Accuracy 0.385009765625\n",
      "Iteration 3090 Training loss 0.06350190192461014 Validation loss 0.06157363951206207 Accuracy 0.382080078125\n",
      "Iteration 3100 Training loss 0.06134757027029991 Validation loss 0.06130237132310867 Accuracy 0.384521484375\n",
      "Iteration 3110 Training loss 0.058159951120615005 Validation loss 0.061371296644210815 Accuracy 0.383056640625\n",
      "Iteration 3120 Training loss 0.0593370720744133 Validation loss 0.06138097494840622 Accuracy 0.384033203125\n",
      "Iteration 3130 Training loss 0.06415412575006485 Validation loss 0.061461396515369415 Accuracy 0.38330078125\n",
      "Iteration 3140 Training loss 0.06351548433303833 Validation loss 0.06124526262283325 Accuracy 0.38525390625\n",
      "Iteration 3150 Training loss 0.06018265709280968 Validation loss 0.06129898875951767 Accuracy 0.385009765625\n",
      "Iteration 3160 Training loss 0.06176214665174484 Validation loss 0.061200857162475586 Accuracy 0.384765625\n",
      "Iteration 3170 Training loss 0.0614425353705883 Validation loss 0.0612839013338089 Accuracy 0.385009765625\n",
      "Iteration 3180 Training loss 0.062528096139431 Validation loss 0.06145220994949341 Accuracy 0.38330078125\n",
      "Iteration 3190 Training loss 0.06122031435370445 Validation loss 0.06169099360704422 Accuracy 0.381591796875\n",
      "Iteration 3200 Training loss 0.06384416669607162 Validation loss 0.06130821257829666 Accuracy 0.3837890625\n",
      "Iteration 3210 Training loss 0.06099637970328331 Validation loss 0.061116356402635574 Accuracy 0.385498046875\n",
      "Iteration 3220 Training loss 0.0570160448551178 Validation loss 0.06128521263599396 Accuracy 0.385009765625\n",
      "Iteration 3230 Training loss 0.06174303963780403 Validation loss 0.06152782961726189 Accuracy 0.382080078125\n",
      "Iteration 3240 Training loss 0.05811203271150589 Validation loss 0.06134258955717087 Accuracy 0.3837890625\n",
      "Iteration 3250 Training loss 0.05781476944684982 Validation loss 0.061189550906419754 Accuracy 0.3857421875\n",
      "Iteration 3260 Training loss 0.0613967590034008 Validation loss 0.06182558834552765 Accuracy 0.378662109375\n",
      "Iteration 3270 Training loss 0.06017503887414932 Validation loss 0.06104778125882149 Accuracy 0.386962890625\n",
      "Iteration 3280 Training loss 0.06051269918680191 Validation loss 0.06146244332194328 Accuracy 0.381103515625\n",
      "Iteration 3290 Training loss 0.059136763215065 Validation loss 0.061065368354320526 Accuracy 0.38671875\n",
      "Iteration 3300 Training loss 0.06211116537451744 Validation loss 0.06130781024694443 Accuracy 0.385009765625\n",
      "Iteration 3310 Training loss 0.06197291612625122 Validation loss 0.061330828815698624 Accuracy 0.383056640625\n",
      "Iteration 3320 Training loss 0.06278657913208008 Validation loss 0.06268342584371567 Accuracy 0.36962890625\n",
      "Iteration 3330 Training loss 0.06093983352184296 Validation loss 0.061167653650045395 Accuracy 0.384765625\n",
      "Iteration 3340 Training loss 0.0646175816655159 Validation loss 0.06167543679475784 Accuracy 0.382080078125\n",
      "Iteration 3350 Training loss 0.0627027302980423 Validation loss 0.061459802091121674 Accuracy 0.383544921875\n",
      "Iteration 3360 Training loss 0.056548237800598145 Validation loss 0.05519186332821846 Accuracy 0.445068359375\n",
      "Iteration 3370 Training loss 0.057088445872068405 Validation loss 0.05489534139633179 Accuracy 0.44873046875\n",
      "Iteration 3380 Training loss 0.050530821084976196 Validation loss 0.054285019636154175 Accuracy 0.45458984375\n",
      "Iteration 3390 Training loss 0.05284414067864418 Validation loss 0.05354375019669533 Accuracy 0.4619140625\n",
      "Iteration 3400 Training loss 0.055678486824035645 Validation loss 0.05345776677131653 Accuracy 0.463623046875\n",
      "Iteration 3410 Training loss 0.05499023199081421 Validation loss 0.0532764308154583 Accuracy 0.464599609375\n",
      "Iteration 3420 Training loss 0.053867045789957047 Validation loss 0.05345506593585014 Accuracy 0.4638671875\n",
      "Iteration 3430 Training loss 0.056030649691820145 Validation loss 0.05341716110706329 Accuracy 0.463134765625\n",
      "Iteration 3440 Training loss 0.05506303161382675 Validation loss 0.05301117151975632 Accuracy 0.467529296875\n",
      "Iteration 3450 Training loss 0.05523405969142914 Validation loss 0.052840203046798706 Accuracy 0.469482421875\n",
      "Iteration 3460 Training loss 0.052478112280368805 Validation loss 0.05403773486614227 Accuracy 0.4580078125\n",
      "Iteration 3470 Training loss 0.052560094743967056 Validation loss 0.05269533395767212 Accuracy 0.470703125\n",
      "Iteration 3480 Training loss 0.05548984557390213 Validation loss 0.05272790789604187 Accuracy 0.469970703125\n",
      "Iteration 3490 Training loss 0.050412822514772415 Validation loss 0.05276774242520332 Accuracy 0.470703125\n",
      "Iteration 3500 Training loss 0.05237458273768425 Validation loss 0.0526011623442173 Accuracy 0.4716796875\n",
      "Iteration 3510 Training loss 0.05185665935277939 Validation loss 0.05291905999183655 Accuracy 0.46923828125\n",
      "Iteration 3520 Training loss 0.050958652049303055 Validation loss 0.053050920367240906 Accuracy 0.46728515625\n",
      "Iteration 3530 Training loss 0.054816700518131256 Validation loss 0.052696920931339264 Accuracy 0.470947265625\n",
      "Iteration 3540 Training loss 0.0566725879907608 Validation loss 0.05424826219677925 Accuracy 0.456298828125\n",
      "Iteration 3550 Training loss 0.051826801151037216 Validation loss 0.05311167240142822 Accuracy 0.466796875\n",
      "Iteration 3560 Training loss 0.05051837116479874 Validation loss 0.05274757370352745 Accuracy 0.47021484375\n",
      "Iteration 3570 Training loss 0.052720438688993454 Validation loss 0.05273052677512169 Accuracy 0.470703125\n",
      "Iteration 3580 Training loss 0.05037475377321243 Validation loss 0.052974577993154526 Accuracy 0.46875\n",
      "Iteration 3590 Training loss 0.051652852445840836 Validation loss 0.0525863841176033 Accuracy 0.471923828125\n",
      "Iteration 3600 Training loss 0.05157936364412308 Validation loss 0.053051162511110306 Accuracy 0.468505859375\n",
      "Iteration 3610 Training loss 0.05120137333869934 Validation loss 0.0534822940826416 Accuracy 0.4619140625\n",
      "Iteration 3620 Training loss 0.050834767520427704 Validation loss 0.05340787023305893 Accuracy 0.4638671875\n",
      "Iteration 3630 Training loss 0.05305740237236023 Validation loss 0.053399767726659775 Accuracy 0.464111328125\n",
      "Iteration 3640 Training loss 0.05327731370925903 Validation loss 0.052648499608039856 Accuracy 0.471435546875\n",
      "Iteration 3650 Training loss 0.05721479281783104 Validation loss 0.05257878825068474 Accuracy 0.47216796875\n",
      "Iteration 3660 Training loss 0.053512927144765854 Validation loss 0.05262576416134834 Accuracy 0.47119140625\n",
      "Iteration 3670 Training loss 0.05068053677678108 Validation loss 0.05350151285529137 Accuracy 0.46337890625\n",
      "Iteration 3680 Training loss 0.05127726122736931 Validation loss 0.052599236369132996 Accuracy 0.471923828125\n",
      "Iteration 3690 Training loss 0.05427267402410507 Validation loss 0.05280442163348198 Accuracy 0.469970703125\n",
      "Iteration 3700 Training loss 0.05497928708791733 Validation loss 0.05351787060499191 Accuracy 0.462890625\n",
      "Iteration 3710 Training loss 0.05343614146113396 Validation loss 0.052809812128543854 Accuracy 0.469970703125\n",
      "Iteration 3720 Training loss 0.048992425203323364 Validation loss 0.05274173244833946 Accuracy 0.470703125\n",
      "Iteration 3730 Training loss 0.05305762216448784 Validation loss 0.05331265553832054 Accuracy 0.464599609375\n",
      "Iteration 3740 Training loss 0.051560670137405396 Validation loss 0.05467524752020836 Accuracy 0.44921875\n",
      "Iteration 3750 Training loss 0.05420340597629547 Validation loss 0.05263923108577728 Accuracy 0.4716796875\n",
      "Iteration 3760 Training loss 0.05111518129706383 Validation loss 0.052976787090301514 Accuracy 0.468505859375\n",
      "Iteration 3770 Training loss 0.05155232176184654 Validation loss 0.05314444378018379 Accuracy 0.46630859375\n",
      "Iteration 3780 Training loss 0.0533936470746994 Validation loss 0.05254673212766647 Accuracy 0.471923828125\n",
      "Iteration 3790 Training loss 0.05198604241013527 Validation loss 0.05253998935222626 Accuracy 0.472900390625\n",
      "Iteration 3800 Training loss 0.052153680473566055 Validation loss 0.05265237018465996 Accuracy 0.4716796875\n",
      "Iteration 3810 Training loss 0.052593130618333817 Validation loss 0.052528899163007736 Accuracy 0.472900390625\n",
      "Iteration 3820 Training loss 0.052614323794841766 Validation loss 0.05250333249568939 Accuracy 0.472900390625\n",
      "Iteration 3830 Training loss 0.04858184605836868 Validation loss 0.05281458795070648 Accuracy 0.46923828125\n",
      "Iteration 3840 Training loss 0.04830420762300491 Validation loss 0.05272859334945679 Accuracy 0.469970703125\n",
      "Iteration 3850 Training loss 0.05227158963680267 Validation loss 0.05295512452721596 Accuracy 0.46923828125\n",
      "Iteration 3860 Training loss 0.05023953318595886 Validation loss 0.05293605476617813 Accuracy 0.46875\n",
      "Iteration 3870 Training loss 0.05184418708086014 Validation loss 0.052608367055654526 Accuracy 0.4716796875\n",
      "Iteration 3880 Training loss 0.0523306168615818 Validation loss 0.05340433493256569 Accuracy 0.464599609375\n",
      "Iteration 3890 Training loss 0.054719533771276474 Validation loss 0.05307382717728615 Accuracy 0.467041015625\n",
      "Iteration 3900 Training loss 0.052548542618751526 Validation loss 0.05296742171049118 Accuracy 0.468017578125\n",
      "Iteration 3910 Training loss 0.052355121821165085 Validation loss 0.052690066397190094 Accuracy 0.47021484375\n",
      "Iteration 3920 Training loss 0.05431956797838211 Validation loss 0.05240446329116821 Accuracy 0.473388671875\n",
      "Iteration 3930 Training loss 0.04939424619078636 Validation loss 0.052569642663002014 Accuracy 0.470947265625\n",
      "Iteration 3940 Training loss 0.05484160780906677 Validation loss 0.05330298840999603 Accuracy 0.464599609375\n",
      "Iteration 3950 Training loss 0.0531114786863327 Validation loss 0.05286020413041115 Accuracy 0.468994140625\n",
      "Iteration 3960 Training loss 0.049692098051309586 Validation loss 0.053208135068416595 Accuracy 0.464599609375\n",
      "Iteration 3970 Training loss 0.05126228928565979 Validation loss 0.05277349427342415 Accuracy 0.469970703125\n",
      "Iteration 3980 Training loss 0.046081166714429855 Validation loss 0.0525723434984684 Accuracy 0.4716796875\n",
      "Iteration 3990 Training loss 0.05033772066235542 Validation loss 0.05284598469734192 Accuracy 0.46875\n",
      "Iteration 4000 Training loss 0.0527382418513298 Validation loss 0.052434902638196945 Accuracy 0.473388671875\n",
      "Iteration 4010 Training loss 0.0532485656440258 Validation loss 0.05239573121070862 Accuracy 0.473388671875\n",
      "Iteration 4020 Training loss 0.05038578808307648 Validation loss 0.05299913138151169 Accuracy 0.468505859375\n",
      "Iteration 4030 Training loss 0.05262576416134834 Validation loss 0.053071923553943634 Accuracy 0.4677734375\n",
      "Iteration 4040 Training loss 0.05268166959285736 Validation loss 0.05263007804751396 Accuracy 0.470703125\n",
      "Iteration 4050 Training loss 0.05395613610744476 Validation loss 0.052913401275873184 Accuracy 0.4677734375\n",
      "Iteration 4060 Training loss 0.05115882307291031 Validation loss 0.052639275789260864 Accuracy 0.470703125\n",
      "Iteration 4070 Training loss 0.05744246393442154 Validation loss 0.05243806168437004 Accuracy 0.472412109375\n",
      "Iteration 4080 Training loss 0.053704190999269485 Validation loss 0.05251167714595795 Accuracy 0.47119140625\n",
      "Iteration 4090 Training loss 0.0471007265150547 Validation loss 0.05266042798757553 Accuracy 0.470458984375\n",
      "Iteration 4100 Training loss 0.052115269005298615 Validation loss 0.053041722625494 Accuracy 0.46630859375\n",
      "Iteration 4110 Training loss 0.0509251207113266 Validation loss 0.05229213833808899 Accuracy 0.473876953125\n",
      "Iteration 4120 Training loss 0.05605437234044075 Validation loss 0.052179910242557526 Accuracy 0.474609375\n",
      "Iteration 4130 Training loss 0.05459189787507057 Validation loss 0.052677225321531296 Accuracy 0.47119140625\n",
      "Iteration 4140 Training loss 0.05179424583911896 Validation loss 0.053095944225788116 Accuracy 0.467041015625\n",
      "Iteration 4150 Training loss 0.05127905681729317 Validation loss 0.05312662944197655 Accuracy 0.465087890625\n",
      "Iteration 4160 Training loss 0.054529231041669846 Validation loss 0.05248827114701271 Accuracy 0.470703125\n",
      "Iteration 4170 Training loss 0.048797424882650375 Validation loss 0.05259234830737114 Accuracy 0.469970703125\n",
      "Iteration 4180 Training loss 0.05837306007742882 Validation loss 0.05242489278316498 Accuracy 0.472412109375\n",
      "Iteration 4190 Training loss 0.054320327937603 Validation loss 0.05234629288315773 Accuracy 0.47216796875\n",
      "Iteration 4200 Training loss 0.0518704317510128 Validation loss 0.052289873361587524 Accuracy 0.472900390625\n",
      "Iteration 4210 Training loss 0.05297376587986946 Validation loss 0.05226224660873413 Accuracy 0.473388671875\n",
      "Iteration 4220 Training loss 0.05505865439772606 Validation loss 0.052348241209983826 Accuracy 0.47216796875\n",
      "Iteration 4230 Training loss 0.05167749151587486 Validation loss 0.05212812498211861 Accuracy 0.474609375\n",
      "Iteration 4240 Training loss 0.057543493807315826 Validation loss 0.05237344279885292 Accuracy 0.47265625\n",
      "Iteration 4250 Training loss 0.050895243883132935 Validation loss 0.05302318185567856 Accuracy 0.466796875\n",
      "Iteration 4260 Training loss 0.054175201803445816 Validation loss 0.05356704443693161 Accuracy 0.461181640625\n",
      "Iteration 4270 Training loss 0.05243002250790596 Validation loss 0.052914608269929886 Accuracy 0.468505859375\n",
      "Iteration 4280 Training loss 0.054698407649993896 Validation loss 0.052845779806375504 Accuracy 0.46923828125\n",
      "Iteration 4290 Training loss 0.04992816224694252 Validation loss 0.052664823830127716 Accuracy 0.470703125\n",
      "Iteration 4300 Training loss 0.052543818950653076 Validation loss 0.052468299865722656 Accuracy 0.471923828125\n",
      "Iteration 4310 Training loss 0.05477188900113106 Validation loss 0.052420180290937424 Accuracy 0.4716796875\n",
      "Iteration 4320 Training loss 0.05128263309597969 Validation loss 0.053399648517370224 Accuracy 0.461181640625\n",
      "Iteration 4330 Training loss 0.05045231059193611 Validation loss 0.052583884447813034 Accuracy 0.469970703125\n",
      "Iteration 4340 Training loss 0.052052877843379974 Validation loss 0.05279266834259033 Accuracy 0.468017578125\n",
      "Iteration 4350 Training loss 0.05218592658638954 Validation loss 0.05271751806139946 Accuracy 0.46923828125\n",
      "Iteration 4360 Training loss 0.04954244941473007 Validation loss 0.05286579951643944 Accuracy 0.468017578125\n",
      "Iteration 4370 Training loss 0.050787150859832764 Validation loss 0.05232495069503784 Accuracy 0.473388671875\n",
      "Iteration 4380 Training loss 0.054525554180145264 Validation loss 0.05230918899178505 Accuracy 0.4736328125\n",
      "Iteration 4390 Training loss 0.055639490485191345 Validation loss 0.05211132764816284 Accuracy 0.47509765625\n",
      "Iteration 4400 Training loss 0.051810137927532196 Validation loss 0.05231517553329468 Accuracy 0.473388671875\n",
      "Iteration 4410 Training loss 0.04859361797571182 Validation loss 0.052168577909469604 Accuracy 0.474853515625\n",
      "Iteration 4420 Training loss 0.05466519668698311 Validation loss 0.05200117453932762 Accuracy 0.47607421875\n",
      "Iteration 4430 Training loss 0.05326671898365021 Validation loss 0.05212608724832535 Accuracy 0.474609375\n",
      "Iteration 4440 Training loss 0.05520514026284218 Validation loss 0.05242707207798958 Accuracy 0.472900390625\n",
      "Iteration 4450 Training loss 0.053288258612155914 Validation loss 0.05315761640667915 Accuracy 0.465087890625\n",
      "Iteration 4460 Training loss 0.054502081125974655 Validation loss 0.053067516535520554 Accuracy 0.46533203125\n",
      "Iteration 4470 Training loss 0.051012735813856125 Validation loss 0.05223380774259567 Accuracy 0.473388671875\n",
      "Iteration 4480 Training loss 0.055893804877996445 Validation loss 0.053449928760528564 Accuracy 0.46240234375\n",
      "Iteration 4490 Training loss 0.05228769779205322 Validation loss 0.0521930456161499 Accuracy 0.474853515625\n",
      "Iteration 4500 Training loss 0.05445140600204468 Validation loss 0.05332063511013985 Accuracy 0.464599609375\n",
      "Iteration 4510 Training loss 0.05013288930058479 Validation loss 0.05228138715028763 Accuracy 0.472412109375\n",
      "Iteration 4520 Training loss 0.054805830121040344 Validation loss 0.05254334211349487 Accuracy 0.47021484375\n",
      "Iteration 4530 Training loss 0.05085491016507149 Validation loss 0.05210375413298607 Accuracy 0.4755859375\n",
      "Iteration 4540 Training loss 0.05327468737959862 Validation loss 0.05227573961019516 Accuracy 0.47412109375\n",
      "Iteration 4550 Training loss 0.05318427458405495 Validation loss 0.052643466740846634 Accuracy 0.470703125\n",
      "Iteration 4560 Training loss 0.05049952119588852 Validation loss 0.05240784212946892 Accuracy 0.47265625\n",
      "Iteration 4570 Training loss 0.052858274430036545 Validation loss 0.05310681089758873 Accuracy 0.465576171875\n",
      "Iteration 4580 Training loss 0.05491029471158981 Validation loss 0.05265739932656288 Accuracy 0.470458984375\n",
      "Iteration 4590 Training loss 0.05096408352255821 Validation loss 0.052212443202733994 Accuracy 0.474365234375\n",
      "Iteration 4600 Training loss 0.05127161741256714 Validation loss 0.0522400364279747 Accuracy 0.47412109375\n",
      "Iteration 4610 Training loss 0.05261845886707306 Validation loss 0.05273928493261337 Accuracy 0.469970703125\n",
      "Iteration 4620 Training loss 0.052480533719062805 Validation loss 0.052174162119627 Accuracy 0.474853515625\n",
      "Iteration 4630 Training loss 0.052191488444805145 Validation loss 0.05318928137421608 Accuracy 0.46630859375\n",
      "Iteration 4640 Training loss 0.05201161652803421 Validation loss 0.05259501934051514 Accuracy 0.471923828125\n",
      "Iteration 4650 Training loss 0.05215970799326897 Validation loss 0.05262136459350586 Accuracy 0.4716796875\n",
      "Iteration 4660 Training loss 0.05208619311451912 Validation loss 0.052606355398893356 Accuracy 0.4716796875\n",
      "Iteration 4670 Training loss 0.051966842263936996 Validation loss 0.05221100524067879 Accuracy 0.474853515625\n",
      "Iteration 4680 Training loss 0.04947218671441078 Validation loss 0.05226042866706848 Accuracy 0.47509765625\n",
      "Iteration 4690 Training loss 0.053752634674310684 Validation loss 0.05233931168913841 Accuracy 0.472900390625\n",
      "Iteration 4700 Training loss 0.052171315997838974 Validation loss 0.052368368953466415 Accuracy 0.472900390625\n",
      "Iteration 4710 Training loss 0.052296243607997894 Validation loss 0.052771247923374176 Accuracy 0.469482421875\n",
      "Iteration 4720 Training loss 0.0528574101626873 Validation loss 0.05294567719101906 Accuracy 0.46630859375\n",
      "Iteration 4730 Training loss 0.04949810355901718 Validation loss 0.05234783887863159 Accuracy 0.4736328125\n",
      "Iteration 4740 Training loss 0.04436717554926872 Validation loss 0.04539083316922188 Accuracy 0.54150390625\n",
      "Iteration 4750 Training loss 0.04086260497570038 Validation loss 0.04555503651499748 Accuracy 0.5419921875\n",
      "Iteration 4760 Training loss 0.046451445668935776 Validation loss 0.04470350593328476 Accuracy 0.54833984375\n",
      "Iteration 4770 Training loss 0.04604858532547951 Validation loss 0.04548073932528496 Accuracy 0.5390625\n",
      "Iteration 4780 Training loss 0.04953023046255112 Validation loss 0.04571780189871788 Accuracy 0.5390625\n",
      "Iteration 4790 Training loss 0.04583849012851715 Validation loss 0.044544901698827744 Accuracy 0.55029296875\n",
      "Iteration 4800 Training loss 0.045205626636743546 Validation loss 0.04455706104636192 Accuracy 0.55126953125\n",
      "Iteration 4810 Training loss 0.041058141738176346 Validation loss 0.04438895359635353 Accuracy 0.55126953125\n",
      "Iteration 4820 Training loss 0.046654753386974335 Validation loss 0.044307079166173935 Accuracy 0.552734375\n",
      "Iteration 4830 Training loss 0.04175594449043274 Validation loss 0.044625137001276016 Accuracy 0.55078125\n",
      "Iteration 4840 Training loss 0.04435684531927109 Validation loss 0.04410974681377411 Accuracy 0.55419921875\n",
      "Iteration 4850 Training loss 0.04125373810529709 Validation loss 0.0454762727022171 Accuracy 0.5419921875\n",
      "Iteration 4860 Training loss 0.048088982701301575 Validation loss 0.04456474632024765 Accuracy 0.55078125\n",
      "Iteration 4870 Training loss 0.04446323961019516 Validation loss 0.044587742537260056 Accuracy 0.55029296875\n",
      "Iteration 4880 Training loss 0.04419764503836632 Validation loss 0.045454081147909164 Accuracy 0.54296875\n",
      "Iteration 4890 Training loss 0.04271752014756203 Validation loss 0.04454898461699486 Accuracy 0.5498046875\n",
      "Iteration 4900 Training loss 0.04475310072302818 Validation loss 0.04466373100876808 Accuracy 0.54931640625\n",
      "Iteration 4910 Training loss 0.04666561260819435 Validation loss 0.04419964551925659 Accuracy 0.55517578125\n",
      "Iteration 4920 Training loss 0.03989559784531593 Validation loss 0.04449980705976486 Accuracy 0.5517578125\n",
      "Iteration 4930 Training loss 0.04630608484148979 Validation loss 0.044227972626686096 Accuracy 0.55419921875\n",
      "Iteration 4940 Training loss 0.044582489877939224 Validation loss 0.04469374567270279 Accuracy 0.548828125\n",
      "Iteration 4950 Training loss 0.04359535127878189 Validation loss 0.044262856245040894 Accuracy 0.55419921875\n",
      "Iteration 4960 Training loss 0.046470507979393005 Validation loss 0.04418860748410225 Accuracy 0.5556640625\n",
      "Iteration 4970 Training loss 0.04023756831884384 Validation loss 0.04438367113471031 Accuracy 0.55224609375\n",
      "Iteration 4980 Training loss 0.04739046096801758 Validation loss 0.044231776148080826 Accuracy 0.55224609375\n",
      "Iteration 4990 Training loss 0.04557628184556961 Validation loss 0.043782565742731094 Accuracy 0.5576171875\n",
      "Iteration 5000 Training loss 0.04471297189593315 Validation loss 0.04453069344162941 Accuracy 0.55126953125\n",
      "Iteration 5010 Training loss 0.04478570073843002 Validation loss 0.043945878744125366 Accuracy 0.55615234375\n",
      "Iteration 5020 Training loss 0.04588751494884491 Validation loss 0.0441003181040287 Accuracy 0.55419921875\n",
      "Iteration 5030 Training loss 0.04764940217137337 Validation loss 0.04525403305888176 Accuracy 0.544921875\n",
      "Iteration 5040 Training loss 0.04361152648925781 Validation loss 0.04372304677963257 Accuracy 0.55908203125\n",
      "Iteration 5050 Training loss 0.04379141330718994 Validation loss 0.04501995071768761 Accuracy 0.54541015625\n",
      "Iteration 5060 Training loss 0.0445907823741436 Validation loss 0.04418480768799782 Accuracy 0.55419921875\n",
      "Iteration 5070 Training loss 0.04253649711608887 Validation loss 0.0442747101187706 Accuracy 0.55322265625\n",
      "Iteration 5080 Training loss 0.048708364367485046 Validation loss 0.04444570094347 Accuracy 0.55224609375\n",
      "Iteration 5090 Training loss 0.04433014988899231 Validation loss 0.044603895395994186 Accuracy 0.54931640625\n",
      "Iteration 5100 Training loss 0.04466957226395607 Validation loss 0.043983057141304016 Accuracy 0.556640625\n",
      "Iteration 5110 Training loss 0.04661884903907776 Validation loss 0.044799432158470154 Accuracy 0.54931640625\n",
      "Iteration 5120 Training loss 0.041263382881879807 Validation loss 0.044520653784275055 Accuracy 0.55078125\n",
      "Iteration 5130 Training loss 0.04116268455982208 Validation loss 0.04378310590982437 Accuracy 0.55859375\n",
      "Iteration 5140 Training loss 0.041770417243242264 Validation loss 0.043895844370126724 Accuracy 0.55810546875\n",
      "Iteration 5150 Training loss 0.04401552304625511 Validation loss 0.04395119845867157 Accuracy 0.556640625\n",
      "Iteration 5160 Training loss 0.04312827065587044 Validation loss 0.043677255511283875 Accuracy 0.55908203125\n",
      "Iteration 5170 Training loss 0.043992094695568085 Validation loss 0.04356935992836952 Accuracy 0.56005859375\n",
      "Iteration 5180 Training loss 0.040891848504543304 Validation loss 0.04421674832701683 Accuracy 0.5546875\n",
      "Iteration 5190 Training loss 0.046239059418439865 Validation loss 0.04352511465549469 Accuracy 0.56005859375\n",
      "Iteration 5200 Training loss 0.0417119637131691 Validation loss 0.04415270313620567 Accuracy 0.556640625\n",
      "Iteration 5210 Training loss 0.04512948542833328 Validation loss 0.043989941477775574 Accuracy 0.5576171875\n",
      "Iteration 5220 Training loss 0.04038636013865471 Validation loss 0.04412702098488808 Accuracy 0.556640625\n",
      "Iteration 5230 Training loss 0.04728583246469498 Validation loss 0.0435740128159523 Accuracy 0.56103515625\n",
      "Iteration 5240 Training loss 0.038041722029447556 Validation loss 0.04369714856147766 Accuracy 0.560546875\n",
      "Iteration 5250 Training loss 0.04405684396624565 Validation loss 0.0437377393245697 Accuracy 0.5595703125\n",
      "Iteration 5260 Training loss 0.044598549604415894 Validation loss 0.04447448253631592 Accuracy 0.5517578125\n",
      "Iteration 5270 Training loss 0.04182332754135132 Validation loss 0.044219374656677246 Accuracy 0.552734375\n",
      "Iteration 5280 Training loss 0.04247516766190529 Validation loss 0.04350894317030907 Accuracy 0.5615234375\n",
      "Iteration 5290 Training loss 0.04334248974919319 Validation loss 0.043865811079740524 Accuracy 0.55712890625\n",
      "Iteration 5300 Training loss 0.040974561125040054 Validation loss 0.04432303458452225 Accuracy 0.55419921875\n",
      "Iteration 5310 Training loss 0.04236410930752754 Validation loss 0.044356297701597214 Accuracy 0.55322265625\n",
      "Iteration 5320 Training loss 0.04407939687371254 Validation loss 0.044364530593156815 Accuracy 0.55322265625\n",
      "Iteration 5330 Training loss 0.04262394830584526 Validation loss 0.043736692517995834 Accuracy 0.5595703125\n",
      "Iteration 5340 Training loss 0.044470809400081635 Validation loss 0.043863240629434586 Accuracy 0.55810546875\n",
      "Iteration 5350 Training loss 0.0431334413588047 Validation loss 0.04346437007188797 Accuracy 0.5625\n",
      "Iteration 5360 Training loss 0.04219779372215271 Validation loss 0.04466306418180466 Accuracy 0.5498046875\n",
      "Iteration 5370 Training loss 0.04603058472275734 Validation loss 0.04488428682088852 Accuracy 0.548828125\n",
      "Iteration 5380 Training loss 0.043323494493961334 Validation loss 0.04412408173084259 Accuracy 0.55517578125\n",
      "Iteration 5390 Training loss 0.04377451539039612 Validation loss 0.043885715305805206 Accuracy 0.5576171875\n",
      "Iteration 5400 Training loss 0.046182312071323395 Validation loss 0.04438774287700653 Accuracy 0.552734375\n",
      "Iteration 5410 Training loss 0.04381490498781204 Validation loss 0.043799128383398056 Accuracy 0.55859375\n",
      "Iteration 5420 Training loss 0.04196873679757118 Validation loss 0.04411875829100609 Accuracy 0.5546875\n",
      "Iteration 5430 Training loss 0.045073602348566055 Validation loss 0.04380885139107704 Accuracy 0.55859375\n",
      "Iteration 5440 Training loss 0.042943622916936874 Validation loss 0.04447697103023529 Accuracy 0.55126953125\n",
      "Iteration 5450 Training loss 0.0413016639649868 Validation loss 0.044040314853191376 Accuracy 0.556640625\n",
      "Iteration 5460 Training loss 0.04333651065826416 Validation loss 0.043805334717035294 Accuracy 0.55810546875\n",
      "Iteration 5470 Training loss 0.04132108390331268 Validation loss 0.043536774814128876 Accuracy 0.56103515625\n",
      "Iteration 5480 Training loss 0.04255499690771103 Validation loss 0.04368407651782036 Accuracy 0.56005859375\n",
      "Iteration 5490 Training loss 0.04265180230140686 Validation loss 0.044084686785936356 Accuracy 0.55712890625\n",
      "Iteration 5500 Training loss 0.043997183442115784 Validation loss 0.04457565397024155 Accuracy 0.5517578125\n",
      "Iteration 5510 Training loss 0.04347763955593109 Validation loss 0.04367905110120773 Accuracy 0.55810546875\n",
      "Iteration 5520 Training loss 0.038476791232824326 Validation loss 0.04368794709444046 Accuracy 0.55859375\n",
      "Iteration 5530 Training loss 0.0482686348259449 Validation loss 0.04360039532184601 Accuracy 0.56005859375\n",
      "Iteration 5540 Training loss 0.04202372208237648 Validation loss 0.0433630608022213 Accuracy 0.5634765625\n",
      "Iteration 5550 Training loss 0.04350965842604637 Validation loss 0.04341650754213333 Accuracy 0.56298828125\n",
      "Iteration 5560 Training loss 0.043888550251722336 Validation loss 0.04325864091515541 Accuracy 0.56396484375\n",
      "Iteration 5570 Training loss 0.04138987883925438 Validation loss 0.04370856657624245 Accuracy 0.560546875\n",
      "Iteration 5580 Training loss 0.04515417292714119 Validation loss 0.043584052473306656 Accuracy 0.56201171875\n",
      "Iteration 5590 Training loss 0.04412772133946419 Validation loss 0.04382222518324852 Accuracy 0.5595703125\n",
      "Iteration 5600 Training loss 0.044861018657684326 Validation loss 0.04460776224732399 Accuracy 0.55078125\n",
      "Iteration 5610 Training loss 0.04132606461644173 Validation loss 0.044145915657281876 Accuracy 0.5556640625\n",
      "Iteration 5620 Training loss 0.04238157719373703 Validation loss 0.04395214840769768 Accuracy 0.55810546875\n",
      "Iteration 5630 Training loss 0.038971614092588425 Validation loss 0.04374897480010986 Accuracy 0.56005859375\n",
      "Iteration 5640 Training loss 0.04401645436882973 Validation loss 0.04381973668932915 Accuracy 0.55810546875\n",
      "Iteration 5650 Training loss 0.0396357923746109 Validation loss 0.043960947543382645 Accuracy 0.55615234375\n",
      "Iteration 5660 Training loss 0.04375302046537399 Validation loss 0.04348530247807503 Accuracy 0.56103515625\n",
      "Iteration 5670 Training loss 0.042186032980680466 Validation loss 0.043570443987846375 Accuracy 0.56201171875\n",
      "Iteration 5680 Training loss 0.04019151255488396 Validation loss 0.043562039732933044 Accuracy 0.56201171875\n",
      "Iteration 5690 Training loss 0.041603896766901016 Validation loss 0.04368690401315689 Accuracy 0.56005859375\n",
      "Iteration 5700 Training loss 0.04172369837760925 Validation loss 0.043451398611068726 Accuracy 0.5615234375\n",
      "Iteration 5710 Training loss 0.044425129890441895 Validation loss 0.04386981204152107 Accuracy 0.55712890625\n",
      "Iteration 5720 Training loss 0.0452781580388546 Validation loss 0.04344608262181282 Accuracy 0.56201171875\n",
      "Iteration 5730 Training loss 0.042325060814619064 Validation loss 0.043849147856235504 Accuracy 0.55810546875\n",
      "Iteration 5740 Training loss 0.043193668127059937 Validation loss 0.043524011969566345 Accuracy 0.56201171875\n",
      "Iteration 5750 Training loss 0.04155987501144409 Validation loss 0.043192069977521896 Accuracy 0.56396484375\n",
      "Iteration 5760 Training loss 0.042884133756160736 Validation loss 0.0433632954955101 Accuracy 0.56201171875\n",
      "Iteration 5770 Training loss 0.04203333333134651 Validation loss 0.04419220983982086 Accuracy 0.55517578125\n",
      "Iteration 5780 Training loss 0.04088945686817169 Validation loss 0.04332950711250305 Accuracy 0.56201171875\n",
      "Iteration 5790 Training loss 0.04360754415392876 Validation loss 0.043744370341300964 Accuracy 0.5595703125\n",
      "Iteration 5800 Training loss 0.04444760084152222 Validation loss 0.04373414069414139 Accuracy 0.5595703125\n",
      "Iteration 5810 Training loss 0.04451167583465576 Validation loss 0.043534960597753525 Accuracy 0.56103515625\n",
      "Iteration 5820 Training loss 0.04220427945256233 Validation loss 0.04331497102975845 Accuracy 0.56201171875\n",
      "Iteration 5830 Training loss 0.04350663349032402 Validation loss 0.043109871447086334 Accuracy 0.564453125\n",
      "Iteration 5840 Training loss 0.04298347607254982 Validation loss 0.043922461569309235 Accuracy 0.55810546875\n",
      "Iteration 5850 Training loss 0.0457274466753006 Validation loss 0.043401919305324554 Accuracy 0.5625\n",
      "Iteration 5860 Training loss 0.042126163840293884 Validation loss 0.043735381215810776 Accuracy 0.55859375\n",
      "Iteration 5870 Training loss 0.03963400796055794 Validation loss 0.043689701706171036 Accuracy 0.56005859375\n",
      "Iteration 5880 Training loss 0.04662974923849106 Validation loss 0.043467726558446884 Accuracy 0.56103515625\n",
      "Iteration 5890 Training loss 0.04625697806477547 Validation loss 0.043412577360868454 Accuracy 0.5615234375\n",
      "Iteration 5900 Training loss 0.04073823615908623 Validation loss 0.043788231909275055 Accuracy 0.5576171875\n",
      "Iteration 5910 Training loss 0.040241602808237076 Validation loss 0.04365519806742668 Accuracy 0.56005859375\n",
      "Iteration 5920 Training loss 0.04236207902431488 Validation loss 0.04401221498847008 Accuracy 0.55712890625\n",
      "Iteration 5930 Training loss 0.04318821802735329 Validation loss 0.04372284188866615 Accuracy 0.55859375\n",
      "Iteration 5940 Training loss 0.04121096432209015 Validation loss 0.043393418192863464 Accuracy 0.5615234375\n",
      "Iteration 5950 Training loss 0.0399416908621788 Validation loss 0.043370071798563004 Accuracy 0.56298828125\n",
      "Iteration 5960 Training loss 0.03984919562935829 Validation loss 0.043442077934741974 Accuracy 0.56201171875\n",
      "Iteration 5970 Training loss 0.0420224592089653 Validation loss 0.04318287968635559 Accuracy 0.56396484375\n",
      "Iteration 5980 Training loss 0.03865104913711548 Validation loss 0.04348977282643318 Accuracy 0.5615234375\n",
      "Iteration 5990 Training loss 0.040319304913282394 Validation loss 0.043897248804569244 Accuracy 0.55712890625\n",
      "Iteration 6000 Training loss 0.041340455412864685 Validation loss 0.043406255543231964 Accuracy 0.56201171875\n",
      "Iteration 6010 Training loss 0.04188816621899605 Validation loss 0.0431048609316349 Accuracy 0.56494140625\n",
      "Iteration 6020 Training loss 0.04473474621772766 Validation loss 0.04347003623843193 Accuracy 0.56103515625\n",
      "Iteration 6030 Training loss 0.04235639423131943 Validation loss 0.043317411094903946 Accuracy 0.5634765625\n",
      "Iteration 6040 Training loss 0.041518017649650574 Validation loss 0.04322236403822899 Accuracy 0.564453125\n",
      "Iteration 6050 Training loss 0.04028113931417465 Validation loss 0.04329052194952965 Accuracy 0.5634765625\n",
      "Iteration 6060 Training loss 0.04361194372177124 Validation loss 0.04336625710129738 Accuracy 0.56201171875\n",
      "Iteration 6070 Training loss 0.04182208701968193 Validation loss 0.04340977594256401 Accuracy 0.5615234375\n",
      "Iteration 6080 Training loss 0.0445362813770771 Validation loss 0.04424098879098892 Accuracy 0.55322265625\n",
      "Iteration 6090 Training loss 0.04441653564572334 Validation loss 0.04387187212705612 Accuracy 0.55810546875\n",
      "Iteration 6100 Training loss 0.041888296604156494 Validation loss 0.0429471954703331 Accuracy 0.56591796875\n",
      "Iteration 6110 Training loss 0.04101664200425148 Validation loss 0.04316054657101631 Accuracy 0.56396484375\n",
      "Iteration 6120 Training loss 0.044852934777736664 Validation loss 0.04452717676758766 Accuracy 0.5517578125\n",
      "Iteration 6130 Training loss 0.042047761380672455 Validation loss 0.0435156375169754 Accuracy 0.56201171875\n",
      "Iteration 6140 Training loss 0.045292530208826065 Validation loss 0.04328888654708862 Accuracy 0.5634765625\n",
      "Iteration 6150 Training loss 0.03871461749076843 Validation loss 0.04314723610877991 Accuracy 0.5634765625\n",
      "Iteration 6160 Training loss 0.04588177427649498 Validation loss 0.0436297170817852 Accuracy 0.560546875\n",
      "Iteration 6170 Training loss 0.04354516416788101 Validation loss 0.04290192946791649 Accuracy 0.56689453125\n",
      "Iteration 6180 Training loss 0.039368774741888046 Validation loss 0.042939167469739914 Accuracy 0.56591796875\n",
      "Iteration 6190 Training loss 0.042626772075891495 Validation loss 0.043153081089258194 Accuracy 0.564453125\n",
      "Iteration 6200 Training loss 0.03933604434132576 Validation loss 0.04323357343673706 Accuracy 0.56494140625\n",
      "Iteration 6210 Training loss 0.04624100774526596 Validation loss 0.04470814764499664 Accuracy 0.548828125\n",
      "Iteration 6220 Training loss 0.04621167108416557 Validation loss 0.04320153221487999 Accuracy 0.56494140625\n",
      "Iteration 6230 Training loss 0.04617409035563469 Validation loss 0.04517027735710144 Accuracy 0.54248046875\n",
      "Iteration 6240 Training loss 0.04167252406477928 Validation loss 0.04288971424102783 Accuracy 0.56689453125\n",
      "Iteration 6250 Training loss 0.04258580505847931 Validation loss 0.04323044419288635 Accuracy 0.56201171875\n",
      "Iteration 6260 Training loss 0.042443446815013885 Validation loss 0.043160490691661835 Accuracy 0.56396484375\n",
      "Iteration 6270 Training loss 0.04317058250308037 Validation loss 0.042869798839092255 Accuracy 0.56689453125\n",
      "Iteration 6280 Training loss 0.04341118782758713 Validation loss 0.04355233907699585 Accuracy 0.560546875\n",
      "Iteration 6290 Training loss 0.038740646094083786 Validation loss 0.043162256479263306 Accuracy 0.56494140625\n",
      "Iteration 6300 Training loss 0.043905749917030334 Validation loss 0.04329195246100426 Accuracy 0.56201171875\n",
      "Iteration 6310 Training loss 0.03743337094783783 Validation loss 0.04349600523710251 Accuracy 0.56103515625\n",
      "Iteration 6320 Training loss 0.04104763641953468 Validation loss 0.04309528321027756 Accuracy 0.56396484375\n",
      "Iteration 6330 Training loss 0.042157094925642014 Validation loss 0.04308914765715599 Accuracy 0.564453125\n",
      "Iteration 6340 Training loss 0.04117630422115326 Validation loss 0.042824968695640564 Accuracy 0.56689453125\n",
      "Iteration 6350 Training loss 0.043375276029109955 Validation loss 0.043041981756687164 Accuracy 0.56494140625\n",
      "Iteration 6360 Training loss 0.0386161133646965 Validation loss 0.04343939945101738 Accuracy 0.5634765625\n",
      "Iteration 6370 Training loss 0.04488645866513252 Validation loss 0.04308917745947838 Accuracy 0.5654296875\n",
      "Iteration 6380 Training loss 0.04212073236703873 Validation loss 0.0432111956179142 Accuracy 0.56396484375\n",
      "Iteration 6390 Training loss 0.039986077696084976 Validation loss 0.043169055134058 Accuracy 0.564453125\n",
      "Iteration 6400 Training loss 0.04170433431863785 Validation loss 0.04295781999826431 Accuracy 0.56640625\n",
      "Iteration 6410 Training loss 0.04411070793867111 Validation loss 0.04367193207144737 Accuracy 0.5595703125\n",
      "Iteration 6420 Training loss 0.04209805279970169 Validation loss 0.04348618537187576 Accuracy 0.56103515625\n",
      "Iteration 6430 Training loss 0.04156726971268654 Validation loss 0.044741082936525345 Accuracy 0.54931640625\n",
      "Iteration 6440 Training loss 0.03967254236340523 Validation loss 0.04291696846485138 Accuracy 0.5654296875\n",
      "Iteration 6450 Training loss 0.042862262576818466 Validation loss 0.0428335964679718 Accuracy 0.56591796875\n",
      "Iteration 6460 Training loss 0.04069620370864868 Validation loss 0.04381635785102844 Accuracy 0.55615234375\n",
      "Iteration 6470 Training loss 0.04430701583623886 Validation loss 0.042863085865974426 Accuracy 0.5654296875\n",
      "Iteration 6480 Training loss 0.04002414271235466 Validation loss 0.04313961789011955 Accuracy 0.56298828125\n",
      "Iteration 6490 Training loss 0.04466401785612106 Validation loss 0.04343390837311745 Accuracy 0.560546875\n",
      "Iteration 6500 Training loss 0.039653435349464417 Validation loss 0.04312979802489281 Accuracy 0.5634765625\n",
      "Iteration 6510 Training loss 0.043340060859918594 Validation loss 0.043732572346925735 Accuracy 0.55859375\n",
      "Iteration 6520 Training loss 0.04110690578818321 Validation loss 0.043522078543901443 Accuracy 0.56103515625\n",
      "Iteration 6530 Training loss 0.04521657153964043 Validation loss 0.043248601257801056 Accuracy 0.5625\n",
      "Iteration 6540 Training loss 0.04300128296017647 Validation loss 0.04315996170043945 Accuracy 0.56396484375\n",
      "Iteration 6550 Training loss 0.041494570672512054 Validation loss 0.04298385605216026 Accuracy 0.5634765625\n",
      "Iteration 6560 Training loss 0.043101489543914795 Validation loss 0.04313408583402634 Accuracy 0.56298828125\n",
      "Iteration 6570 Training loss 0.043951984494924545 Validation loss 0.044341396540403366 Accuracy 0.55224609375\n",
      "Iteration 6580 Training loss 0.04700702801346779 Validation loss 0.044739462435245514 Accuracy 0.54736328125\n",
      "Iteration 6590 Training loss 0.043274667114019394 Validation loss 0.04326137527823448 Accuracy 0.56298828125\n",
      "Iteration 6600 Training loss 0.046821512281894684 Validation loss 0.043103743344545364 Accuracy 0.5634765625\n",
      "Iteration 6610 Training loss 0.04088238999247551 Validation loss 0.04315781965851784 Accuracy 0.5625\n",
      "Iteration 6620 Training loss 0.043268971145153046 Validation loss 0.04301685467362404 Accuracy 0.5634765625\n",
      "Iteration 6630 Training loss 0.040940120816230774 Validation loss 0.043549053370952606 Accuracy 0.5595703125\n",
      "Iteration 6640 Training loss 0.04749494791030884 Validation loss 0.04411771520972252 Accuracy 0.5537109375\n",
      "Iteration 6650 Training loss 0.04341449588537216 Validation loss 0.043597009032964706 Accuracy 0.56005859375\n",
      "Iteration 6660 Training loss 0.03828553855419159 Validation loss 0.04306074231863022 Accuracy 0.56298828125\n",
      "Iteration 6670 Training loss 0.04031846299767494 Validation loss 0.042817845940589905 Accuracy 0.56494140625\n",
      "Iteration 6680 Training loss 0.04305786266922951 Validation loss 0.0429244264960289 Accuracy 0.56298828125\n",
      "Iteration 6690 Training loss 0.04074978828430176 Validation loss 0.04307801276445389 Accuracy 0.56396484375\n",
      "Iteration 6700 Training loss 0.04309828579425812 Validation loss 0.04319782927632332 Accuracy 0.56201171875\n",
      "Iteration 6710 Training loss 0.037474196404218674 Validation loss 0.042943958193063736 Accuracy 0.56396484375\n",
      "Iteration 6720 Training loss 0.04165874049067497 Validation loss 0.0434265173971653 Accuracy 0.5595703125\n",
      "Iteration 6730 Training loss 0.04340570420026779 Validation loss 0.04304700717329979 Accuracy 0.5634765625\n",
      "Iteration 6740 Training loss 0.04501083493232727 Validation loss 0.042778052389621735 Accuracy 0.56689453125\n",
      "Iteration 6750 Training loss 0.04397913068532944 Validation loss 0.0428374707698822 Accuracy 0.564453125\n",
      "Iteration 6760 Training loss 0.04138360545039177 Validation loss 0.043040964752435684 Accuracy 0.56298828125\n",
      "Iteration 6770 Training loss 0.04495478421449661 Validation loss 0.04290890693664551 Accuracy 0.56494140625\n",
      "Iteration 6780 Training loss 0.0415494330227375 Validation loss 0.04283913969993591 Accuracy 0.56494140625\n",
      "Iteration 6790 Training loss 0.042234864085912704 Validation loss 0.04294973611831665 Accuracy 0.564453125\n",
      "Iteration 6800 Training loss 0.043568648397922516 Validation loss 0.04337430000305176 Accuracy 0.56103515625\n",
      "Iteration 6810 Training loss 0.04046300798654556 Validation loss 0.0436185784637928 Accuracy 0.55810546875\n",
      "Iteration 6820 Training loss 0.04413788393139839 Validation loss 0.043768513947725296 Accuracy 0.55859375\n",
      "Iteration 6830 Training loss 0.04211042821407318 Validation loss 0.043202586472034454 Accuracy 0.5634765625\n",
      "Iteration 6840 Training loss 0.04325202479958534 Validation loss 0.04307729750871658 Accuracy 0.56494140625\n",
      "Iteration 6850 Training loss 0.04259369522333145 Validation loss 0.043607622385025024 Accuracy 0.5576171875\n",
      "Iteration 6860 Training loss 0.04247035086154938 Validation loss 0.042739976197481155 Accuracy 0.56591796875\n",
      "Iteration 6870 Training loss 0.03775252401828766 Validation loss 0.04046507552266121 Accuracy 0.58935546875\n",
      "Iteration 6880 Training loss 0.038283221423625946 Validation loss 0.03858131170272827 Accuracy 0.61083984375\n",
      "Iteration 6890 Training loss 0.039179619401693344 Validation loss 0.039352476596832275 Accuracy 0.603515625\n",
      "Iteration 6900 Training loss 0.037432774901390076 Validation loss 0.03835182636976242 Accuracy 0.6123046875\n",
      "Iteration 6910 Training loss 0.036878764629364014 Validation loss 0.038489021360874176 Accuracy 0.61181640625\n",
      "Iteration 6920 Training loss 0.03406573459506035 Validation loss 0.038141585886478424 Accuracy 0.6142578125\n",
      "Iteration 6930 Training loss 0.04093441367149353 Validation loss 0.03878796845674515 Accuracy 0.60791015625\n",
      "Iteration 6940 Training loss 0.03620689734816551 Validation loss 0.03830622881650925 Accuracy 0.6123046875\n",
      "Iteration 6950 Training loss 0.03643743321299553 Validation loss 0.03800332918763161 Accuracy 0.6162109375\n",
      "Iteration 6960 Training loss 0.03385801240801811 Validation loss 0.03847789391875267 Accuracy 0.61181640625\n",
      "Iteration 6970 Training loss 0.03821370378136635 Validation loss 0.03782813251018524 Accuracy 0.619140625\n",
      "Iteration 6980 Training loss 0.037497542798519135 Validation loss 0.03772379830479622 Accuracy 0.6201171875\n",
      "Iteration 6990 Training loss 0.03737882524728775 Validation loss 0.037914153188467026 Accuracy 0.61767578125\n",
      "Iteration 7000 Training loss 0.03569438308477402 Validation loss 0.039185862988233566 Accuracy 0.60400390625\n",
      "Iteration 7010 Training loss 0.034768395125865936 Validation loss 0.038318246603012085 Accuracy 0.61328125\n",
      "Iteration 7020 Training loss 0.03704338148236275 Validation loss 0.03809596970677376 Accuracy 0.615234375\n",
      "Iteration 7030 Training loss 0.03565242141485214 Validation loss 0.037568144500255585 Accuracy 0.6201171875\n",
      "Iteration 7040 Training loss 0.034876152873039246 Validation loss 0.03765837103128433 Accuracy 0.619140625\n",
      "Iteration 7050 Training loss 0.033220816403627396 Validation loss 0.037637535482645035 Accuracy 0.61962890625\n",
      "Iteration 7060 Training loss 0.03813345730304718 Validation loss 0.03821149840950966 Accuracy 0.61376953125\n",
      "Iteration 7070 Training loss 0.03632979094982147 Validation loss 0.038784418255090714 Accuracy 0.60791015625\n",
      "Iteration 7080 Training loss 0.036943767219781876 Validation loss 0.03797085955739021 Accuracy 0.6171875\n",
      "Iteration 7090 Training loss 0.03819011524319649 Validation loss 0.03788083419203758 Accuracy 0.6171875\n",
      "Iteration 7100 Training loss 0.03511688485741615 Validation loss 0.03757105767726898 Accuracy 0.62060546875\n",
      "Iteration 7110 Training loss 0.036379374563694 Validation loss 0.03752145543694496 Accuracy 0.62158203125\n",
      "Iteration 7120 Training loss 0.03373634070158005 Validation loss 0.037289947271347046 Accuracy 0.62255859375\n",
      "Iteration 7130 Training loss 0.035163167864084244 Validation loss 0.03749127686023712 Accuracy 0.62060546875\n",
      "Iteration 7140 Training loss 0.03754248097538948 Validation loss 0.037879716604948044 Accuracy 0.61767578125\n",
      "Iteration 7150 Training loss 0.0378950759768486 Validation loss 0.04067229852080345 Accuracy 0.587890625\n",
      "Iteration 7160 Training loss 0.03952379524707794 Validation loss 0.037682224065065384 Accuracy 0.6181640625\n",
      "Iteration 7170 Training loss 0.035323843359947205 Validation loss 0.03789205104112625 Accuracy 0.6171875\n",
      "Iteration 7180 Training loss 0.036871835589408875 Validation loss 0.03739852085709572 Accuracy 0.6220703125\n",
      "Iteration 7190 Training loss 0.037185292690992355 Validation loss 0.038240544497966766 Accuracy 0.6142578125\n",
      "Iteration 7200 Training loss 0.03856479004025459 Validation loss 0.037697482854127884 Accuracy 0.619140625\n",
      "Iteration 7210 Training loss 0.03295856714248657 Validation loss 0.0374123752117157 Accuracy 0.62158203125\n",
      "Iteration 7220 Training loss 0.03806131333112717 Validation loss 0.03742905706167221 Accuracy 0.62109375\n",
      "Iteration 7230 Training loss 0.03358935937285423 Validation loss 0.03732769936323166 Accuracy 0.62353515625\n",
      "Iteration 7240 Training loss 0.03878827020525932 Validation loss 0.03713660314679146 Accuracy 0.625\n",
      "Iteration 7250 Training loss 0.0379573330283165 Validation loss 0.0375845804810524 Accuracy 0.6201171875\n",
      "Iteration 7260 Training loss 0.03233569115400314 Validation loss 0.037892282009124756 Accuracy 0.6162109375\n",
      "Iteration 7270 Training loss 0.03518793731927872 Validation loss 0.03797009959816933 Accuracy 0.61767578125\n",
      "Iteration 7280 Training loss 0.034458644688129425 Validation loss 0.0383189357817173 Accuracy 0.61083984375\n",
      "Iteration 7290 Training loss 0.03340741619467735 Validation loss 0.03811540827155113 Accuracy 0.61474609375\n",
      "Iteration 7300 Training loss 0.037296194583177567 Validation loss 0.03734392300248146 Accuracy 0.6220703125\n",
      "Iteration 7310 Training loss 0.032555364072322845 Validation loss 0.03732119873166084 Accuracy 0.62255859375\n",
      "Iteration 7320 Training loss 0.032429106533527374 Validation loss 0.037336062639951706 Accuracy 0.62158203125\n",
      "Iteration 7330 Training loss 0.04019375890493393 Validation loss 0.03745678812265396 Accuracy 0.61962890625\n",
      "Iteration 7340 Training loss 0.032286886125802994 Validation loss 0.03687680885195732 Accuracy 0.626953125\n",
      "Iteration 7350 Training loss 0.03360145539045334 Validation loss 0.03712394833564758 Accuracy 0.625\n",
      "Iteration 7360 Training loss 0.03343786671757698 Validation loss 0.03524588420987129 Accuracy 0.64208984375\n",
      "Iteration 7370 Training loss 0.03220345824956894 Validation loss 0.034041840583086014 Accuracy 0.65234375\n",
      "Iteration 7380 Training loss 0.029278788715600967 Validation loss 0.03376345708966255 Accuracy 0.65771484375\n",
      "Iteration 7390 Training loss 0.033759456127882004 Validation loss 0.03464064747095108 Accuracy 0.64892578125\n",
      "Iteration 7400 Training loss 0.03286134824156761 Validation loss 0.033610664308071136 Accuracy 0.65869140625\n",
      "Iteration 7410 Training loss 0.032502010464668274 Validation loss 0.03358250483870506 Accuracy 0.65771484375\n",
      "Iteration 7420 Training loss 0.037126876413822174 Validation loss 0.03318164870142937 Accuracy 0.6630859375\n",
      "Iteration 7430 Training loss 0.033108294010162354 Validation loss 0.03418900817632675 Accuracy 0.65380859375\n",
      "Iteration 7440 Training loss 0.028470413759350777 Validation loss 0.031698863953351974 Accuracy 0.67724609375\n",
      "Iteration 7450 Training loss 0.033968839794397354 Validation loss 0.03330113738775253 Accuracy 0.66064453125\n",
      "Iteration 7460 Training loss 0.028713079169392586 Validation loss 0.03099757805466652 Accuracy 0.68359375\n",
      "Iteration 7470 Training loss 0.029699889943003654 Validation loss 0.031686145812273026 Accuracy 0.67724609375\n",
      "Iteration 7480 Training loss 0.027318140491843224 Validation loss 0.03162771090865135 Accuracy 0.677734375\n",
      "Iteration 7490 Training loss 0.027173316106200218 Validation loss 0.030854957178235054 Accuracy 0.68505859375\n",
      "Iteration 7500 Training loss 0.032075557857751846 Validation loss 0.03326309472322464 Accuracy 0.66259765625\n",
      "Iteration 7510 Training loss 0.030811116099357605 Validation loss 0.031119821593165398 Accuracy 0.68310546875\n",
      "Iteration 7520 Training loss 0.030891217291355133 Validation loss 0.031120875850319862 Accuracy 0.68408203125\n",
      "Iteration 7530 Training loss 0.033219851553440094 Validation loss 0.031463924795389175 Accuracy 0.6796875\n",
      "Iteration 7540 Training loss 0.03144798427820206 Validation loss 0.032331738620996475 Accuracy 0.671875\n",
      "Iteration 7550 Training loss 0.027094395831227303 Validation loss 0.0316031388938427 Accuracy 0.67919921875\n",
      "Iteration 7560 Training loss 0.02940979227423668 Validation loss 0.03203275427222252 Accuracy 0.6748046875\n",
      "Iteration 7570 Training loss 0.029634205624461174 Validation loss 0.03194455802440643 Accuracy 0.67529296875\n",
      "Iteration 7580 Training loss 0.032321497797966 Validation loss 0.03164574131369591 Accuracy 0.6787109375\n",
      "Iteration 7590 Training loss 0.030712706968188286 Validation loss 0.03126341104507446 Accuracy 0.6826171875\n",
      "Iteration 7600 Training loss 0.030713897198438644 Validation loss 0.030526602640748024 Accuracy 0.68896484375\n",
      "Iteration 7610 Training loss 0.028760986402630806 Validation loss 0.031717170029878616 Accuracy 0.677734375\n",
      "Iteration 7620 Training loss 0.033041827380657196 Validation loss 0.03797266632318497 Accuracy 0.61572265625\n",
      "Iteration 7630 Training loss 0.030204974114894867 Validation loss 0.03124387562274933 Accuracy 0.681640625\n",
      "Iteration 7640 Training loss 0.028805037960410118 Validation loss 0.03240704536437988 Accuracy 0.67138671875\n",
      "Iteration 7650 Training loss 0.027818839997053146 Validation loss 0.03213616833090782 Accuracy 0.6728515625\n",
      "Iteration 7660 Training loss 0.0356982946395874 Validation loss 0.035040441900491714 Accuracy 0.64697265625\n",
      "Iteration 7670 Training loss 0.03485509380698204 Validation loss 0.03442079946398735 Accuracy 0.65087890625\n",
      "Iteration 7680 Training loss 0.03019058145582676 Validation loss 0.03266562521457672 Accuracy 0.66796875\n",
      "Iteration 7690 Training loss 0.026003343984484673 Validation loss 0.03064189851284027 Accuracy 0.6875\n",
      "Iteration 7700 Training loss 0.029086997732520103 Validation loss 0.03095114603638649 Accuracy 0.685546875\n",
      "Iteration 7710 Training loss 0.030623784288764 Validation loss 0.03255688026547432 Accuracy 0.669921875\n",
      "Iteration 7720 Training loss 0.030068090185523033 Validation loss 0.03227539360523224 Accuracy 0.6728515625\n",
      "Iteration 7730 Training loss 0.0307490024715662 Validation loss 0.030534248799085617 Accuracy 0.68896484375\n",
      "Iteration 7740 Training loss 0.035712145268917084 Validation loss 0.03445374220609665 Accuracy 0.65087890625\n",
      "Iteration 7750 Training loss 0.030761225149035454 Validation loss 0.030957113951444626 Accuracy 0.6845703125\n",
      "Iteration 7760 Training loss 0.02935541234910488 Validation loss 0.0324554480612278 Accuracy 0.6708984375\n",
      "Iteration 7770 Training loss 0.032240718603134155 Validation loss 0.030582888051867485 Accuracy 0.68798828125\n",
      "Iteration 7780 Training loss 0.03155471384525299 Validation loss 0.0315869115293026 Accuracy 0.6787109375\n",
      "Iteration 7790 Training loss 0.03228555619716644 Validation loss 0.031028812751173973 Accuracy 0.68310546875\n",
      "Iteration 7800 Training loss 0.030906086787581444 Validation loss 0.030719716101884842 Accuracy 0.6875\n",
      "Iteration 7810 Training loss 0.02900327555835247 Validation loss 0.030723944306373596 Accuracy 0.68701171875\n",
      "Iteration 7820 Training loss 0.029982414096593857 Validation loss 0.030575644224882126 Accuracy 0.6884765625\n",
      "Iteration 7830 Training loss 0.032904159277677536 Validation loss 0.031683601438999176 Accuracy 0.67724609375\n",
      "Iteration 7840 Training loss 0.02872474119067192 Validation loss 0.03027024306356907 Accuracy 0.69091796875\n",
      "Iteration 7850 Training loss 0.03148789331316948 Validation loss 0.034235887229442596 Accuracy 0.6533203125\n",
      "Iteration 7860 Training loss 0.030920017510652542 Validation loss 0.030929232016205788 Accuracy 0.68408203125\n",
      "Iteration 7870 Training loss 0.029127538204193115 Validation loss 0.03065752610564232 Accuracy 0.6875\n",
      "Iteration 7880 Training loss 0.033522699028253555 Validation loss 0.03318967670202255 Accuracy 0.66162109375\n",
      "Iteration 7890 Training loss 0.02755344659090042 Validation loss 0.032235004007816315 Accuracy 0.67333984375\n",
      "Iteration 7900 Training loss 0.032539743930101395 Validation loss 0.03307776525616646 Accuracy 0.66357421875\n",
      "Iteration 7910 Training loss 0.03323785960674286 Validation loss 0.03453819826245308 Accuracy 0.6494140625\n",
      "Iteration 7920 Training loss 0.03316228836774826 Validation loss 0.03185245022177696 Accuracy 0.6767578125\n",
      "Iteration 7930 Training loss 0.02726142294704914 Validation loss 0.030723214149475098 Accuracy 0.68701171875\n",
      "Iteration 7940 Training loss 0.03230014443397522 Validation loss 0.031748734414577484 Accuracy 0.6767578125\n",
      "Iteration 7950 Training loss 0.02994055487215519 Validation loss 0.030470507219433784 Accuracy 0.68994140625\n",
      "Iteration 7960 Training loss 0.02857290394604206 Validation loss 0.031623829156160355 Accuracy 0.6767578125\n",
      "Iteration 7970 Training loss 0.028205877169966698 Validation loss 0.031162962317466736 Accuracy 0.68359375\n",
      "Iteration 7980 Training loss 0.031480222940444946 Validation loss 0.03362598642706871 Accuracy 0.6572265625\n",
      "Iteration 7990 Training loss 0.027466878294944763 Validation loss 0.031036188825964928 Accuracy 0.68310546875\n",
      "Iteration 8000 Training loss 0.02760910801589489 Validation loss 0.030939489603042603 Accuracy 0.68505859375\n",
      "Iteration 8010 Training loss 0.029840240254998207 Validation loss 0.03156331554055214 Accuracy 0.67919921875\n",
      "Iteration 8020 Training loss 0.027991607785224915 Validation loss 0.030782576650381088 Accuracy 0.68603515625\n",
      "Iteration 8030 Training loss 0.031075583770871162 Validation loss 0.030914828181266785 Accuracy 0.6865234375\n",
      "Iteration 8040 Training loss 0.03060149773955345 Validation loss 0.0315147303044796 Accuracy 0.67919921875\n",
      "Iteration 8050 Training loss 0.027119887992739677 Validation loss 0.03038814291357994 Accuracy 0.69091796875\n",
      "Iteration 8060 Training loss 0.029621507972478867 Validation loss 0.030599545687437057 Accuracy 0.6865234375\n",
      "Iteration 8070 Training loss 0.030200021341443062 Validation loss 0.030441349372267723 Accuracy 0.69091796875\n",
      "Iteration 8080 Training loss 0.028628524392843246 Validation loss 0.03008476085960865 Accuracy 0.69287109375\n",
      "Iteration 8090 Training loss 0.027430996298789978 Validation loss 0.02993793413043022 Accuracy 0.6943359375\n",
      "Iteration 8100 Training loss 0.02897854521870613 Validation loss 0.031288836151361465 Accuracy 0.6806640625\n",
      "Iteration 8110 Training loss 0.03095744363963604 Validation loss 0.029854169115424156 Accuracy 0.6953125\n",
      "Iteration 8120 Training loss 0.029749775305390358 Validation loss 0.03047586791217327 Accuracy 0.6904296875\n",
      "Iteration 8130 Training loss 0.03345712274312973 Validation loss 0.030806556344032288 Accuracy 0.68603515625\n",
      "Iteration 8140 Training loss 0.03163331001996994 Validation loss 0.0322258397936821 Accuracy 0.67333984375\n",
      "Iteration 8150 Training loss 0.030553199350833893 Validation loss 0.03068581409752369 Accuracy 0.6875\n",
      "Iteration 8160 Training loss 0.029314231127500534 Validation loss 0.030666569247841835 Accuracy 0.6865234375\n",
      "Iteration 8170 Training loss 0.0298884529620409 Validation loss 0.030046286061406136 Accuracy 0.69384765625\n",
      "Iteration 8180 Training loss 0.029464660212397575 Validation loss 0.03117876872420311 Accuracy 0.681640625\n",
      "Iteration 8190 Training loss 0.027353202924132347 Validation loss 0.02998408116400242 Accuracy 0.69482421875\n",
      "Iteration 8200 Training loss 0.03176011145114899 Validation loss 0.0310454573482275 Accuracy 0.68359375\n",
      "Iteration 8210 Training loss 0.028336601331830025 Validation loss 0.02968381531536579 Accuracy 0.6962890625\n",
      "Iteration 8220 Training loss 0.028204889968037605 Validation loss 0.02981581911444664 Accuracy 0.6953125\n",
      "Iteration 8230 Training loss 0.02869001217186451 Validation loss 0.03054213710129261 Accuracy 0.68701171875\n",
      "Iteration 8240 Training loss 0.03293171897530556 Validation loss 0.031052889302372932 Accuracy 0.6826171875\n",
      "Iteration 8250 Training loss 0.030993977561593056 Validation loss 0.03099105879664421 Accuracy 0.68408203125\n",
      "Iteration 8260 Training loss 0.027679352089762688 Validation loss 0.030614973977208138 Accuracy 0.68701171875\n",
      "Iteration 8270 Training loss 0.030518589541316032 Validation loss 0.03088081069290638 Accuracy 0.6845703125\n",
      "Iteration 8280 Training loss 0.028561405837535858 Validation loss 0.03177553787827492 Accuracy 0.67724609375\n",
      "Iteration 8290 Training loss 0.027356678619980812 Validation loss 0.029447108507156372 Accuracy 0.69921875\n",
      "Iteration 8300 Training loss 0.027408065274357796 Validation loss 0.029687367379665375 Accuracy 0.69677734375\n",
      "Iteration 8310 Training loss 0.028598245233297348 Validation loss 0.03004773147404194 Accuracy 0.6943359375\n",
      "Iteration 8320 Training loss 0.03157990053296089 Validation loss 0.029928764328360558 Accuracy 0.6943359375\n",
      "Iteration 8330 Training loss 0.029087601229548454 Validation loss 0.030691862106323242 Accuracy 0.6865234375\n",
      "Iteration 8340 Training loss 0.026144886389374733 Validation loss 0.030074454843997955 Accuracy 0.69482421875\n",
      "Iteration 8350 Training loss 0.03245946392416954 Validation loss 0.030903836712241173 Accuracy 0.6865234375\n",
      "Iteration 8360 Training loss 0.031065212562680244 Validation loss 0.032765842974185944 Accuracy 0.66650390625\n",
      "Iteration 8370 Training loss 0.027054153382778168 Validation loss 0.02982155606150627 Accuracy 0.69580078125\n",
      "Iteration 8380 Training loss 0.026127511635422707 Validation loss 0.030082840472459793 Accuracy 0.69189453125\n",
      "Iteration 8390 Training loss 0.03137262910604477 Validation loss 0.031043803319334984 Accuracy 0.68408203125\n",
      "Iteration 8400 Training loss 0.030089057981967926 Validation loss 0.030587665736675262 Accuracy 0.68701171875\n",
      "Iteration 8410 Training loss 0.033270660787820816 Validation loss 0.031841229647397995 Accuracy 0.6748046875\n",
      "Iteration 8420 Training loss 0.02832123078405857 Validation loss 0.02962687984108925 Accuracy 0.6982421875\n",
      "Iteration 8430 Training loss 0.024137739092111588 Validation loss 0.03009822592139244 Accuracy 0.69384765625\n",
      "Iteration 8440 Training loss 0.030798105522990227 Validation loss 0.03077869489789009 Accuracy 0.68603515625\n",
      "Iteration 8450 Training loss 0.030032318085432053 Validation loss 0.030000779777765274 Accuracy 0.69482421875\n",
      "Iteration 8460 Training loss 0.02776009403169155 Validation loss 0.029673870652914047 Accuracy 0.697265625\n",
      "Iteration 8470 Training loss 0.029149316251277924 Validation loss 0.030021199956536293 Accuracy 0.69384765625\n",
      "Iteration 8480 Training loss 0.028339294716715813 Validation loss 0.03003263846039772 Accuracy 0.69482421875\n",
      "Iteration 8490 Training loss 0.02799369767308235 Validation loss 0.030177753418684006 Accuracy 0.69287109375\n",
      "Iteration 8500 Training loss 0.02711164765059948 Validation loss 0.029717791825532913 Accuracy 0.6982421875\n",
      "Iteration 8510 Training loss 0.030663060024380684 Validation loss 0.029920805245637894 Accuracy 0.69482421875\n",
      "Iteration 8520 Training loss 0.026353389024734497 Validation loss 0.02976119890809059 Accuracy 0.6953125\n",
      "Iteration 8530 Training loss 0.02791457064449787 Validation loss 0.02978183515369892 Accuracy 0.6962890625\n",
      "Iteration 8540 Training loss 0.028641102835536003 Validation loss 0.030048448592424393 Accuracy 0.69140625\n",
      "Iteration 8550 Training loss 0.02722369320690632 Validation loss 0.030396994203329086 Accuracy 0.689453125\n",
      "Iteration 8560 Training loss 0.027523808181285858 Validation loss 0.03181976079940796 Accuracy 0.6767578125\n",
      "Iteration 8570 Training loss 0.028438441455364227 Validation loss 0.029314013198018074 Accuracy 0.701171875\n",
      "Iteration 8580 Training loss 0.02933187037706375 Validation loss 0.030178045853972435 Accuracy 0.6904296875\n",
      "Iteration 8590 Training loss 0.030586516484618187 Validation loss 0.029643623158335686 Accuracy 0.697265625\n",
      "Iteration 8600 Training loss 0.024357527494430542 Validation loss 0.029189366847276688 Accuracy 0.70166015625\n",
      "Iteration 8610 Training loss 0.026375889778137207 Validation loss 0.029680706560611725 Accuracy 0.6962890625\n",
      "Iteration 8620 Training loss 0.030062003061175346 Validation loss 0.02942056581377983 Accuracy 0.69921875\n",
      "Iteration 8630 Training loss 0.025989441201090813 Validation loss 0.02979840710759163 Accuracy 0.6962890625\n",
      "Iteration 8640 Training loss 0.027811802923679352 Validation loss 0.02942643314599991 Accuracy 0.69921875\n",
      "Iteration 8650 Training loss 0.02825791947543621 Validation loss 0.02950294315814972 Accuracy 0.697265625\n",
      "Iteration 8660 Training loss 0.025022059679031372 Validation loss 0.029241686686873436 Accuracy 0.7001953125\n",
      "Iteration 8670 Training loss 0.025942591950297356 Validation loss 0.030022071674466133 Accuracy 0.69189453125\n",
      "Iteration 8680 Training loss 0.02876291424036026 Validation loss 0.02987469732761383 Accuracy 0.6943359375\n",
      "Iteration 8690 Training loss 0.02937520481646061 Validation loss 0.029197128489613533 Accuracy 0.70068359375\n",
      "Iteration 8700 Training loss 0.027515435591340065 Validation loss 0.030047211796045303 Accuracy 0.69189453125\n",
      "Iteration 8710 Training loss 0.02617878094315529 Validation loss 0.028978437185287476 Accuracy 0.70263671875\n",
      "Iteration 8720 Training loss 0.027401473373174667 Validation loss 0.029347263276576996 Accuracy 0.69873046875\n",
      "Iteration 8730 Training loss 0.025678085163235664 Validation loss 0.02925904281437397 Accuracy 0.70068359375\n",
      "Iteration 8740 Training loss 0.026251979172229767 Validation loss 0.029250599443912506 Accuracy 0.70068359375\n",
      "Iteration 8750 Training loss 0.03151731193065643 Validation loss 0.02976394258439541 Accuracy 0.6953125\n",
      "Iteration 8760 Training loss 0.027701212093234062 Validation loss 0.03023066371679306 Accuracy 0.69091796875\n",
      "Iteration 8770 Training loss 0.025969650596380234 Validation loss 0.029061131179332733 Accuracy 0.7021484375\n",
      "Iteration 8780 Training loss 0.03169579803943634 Validation loss 0.03272654861211777 Accuracy 0.66650390625\n",
      "Iteration 8790 Training loss 0.025839243084192276 Validation loss 0.029890624806284904 Accuracy 0.6943359375\n",
      "Iteration 8800 Training loss 0.02646080031991005 Validation loss 0.02950049191713333 Accuracy 0.697265625\n",
      "Iteration 8810 Training loss 0.026502853259444237 Validation loss 0.03037901781499386 Accuracy 0.68798828125\n",
      "Iteration 8820 Training loss 0.02927199751138687 Validation loss 0.0291096493601799 Accuracy 0.70166015625\n",
      "Iteration 8830 Training loss 0.02813032455742359 Validation loss 0.029219970107078552 Accuracy 0.701171875\n",
      "Iteration 8840 Training loss 0.028032083064317703 Validation loss 0.0288620013743639 Accuracy 0.70458984375\n",
      "Iteration 8850 Training loss 0.02814548648893833 Validation loss 0.029667314141988754 Accuracy 0.69482421875\n",
      "Iteration 8860 Training loss 0.025713711977005005 Validation loss 0.02894379384815693 Accuracy 0.7021484375\n",
      "Iteration 8870 Training loss 0.019214250147342682 Validation loss 0.020636746659874916 Accuracy 0.7880859375\n",
      "Iteration 8880 Training loss 0.015699809417128563 Validation loss 0.02158716507256031 Accuracy 0.779296875\n",
      "Iteration 8890 Training loss 0.02006777748465538 Validation loss 0.020622026175260544 Accuracy 0.7890625\n",
      "Iteration 8900 Training loss 0.014146855100989342 Validation loss 0.020985662937164307 Accuracy 0.78515625\n",
      "Iteration 8910 Training loss 0.0213426873087883 Validation loss 0.02188922092318535 Accuracy 0.7763671875\n",
      "Iteration 8920 Training loss 0.02018379233777523 Validation loss 0.02092329040169716 Accuracy 0.7861328125\n",
      "Iteration 8930 Training loss 0.021284906193614006 Validation loss 0.0218126829713583 Accuracy 0.77587890625\n",
      "Iteration 8940 Training loss 0.020098775625228882 Validation loss 0.0228671133518219 Accuracy 0.765625\n",
      "Iteration 8950 Training loss 0.019963782280683517 Validation loss 0.021019956097006798 Accuracy 0.7841796875\n",
      "Iteration 8960 Training loss 0.01951751485466957 Validation loss 0.02054251916706562 Accuracy 0.78955078125\n",
      "Iteration 8970 Training loss 0.021432770416140556 Validation loss 0.022679701447486877 Accuracy 0.76806640625\n",
      "Iteration 8980 Training loss 0.018202079460024834 Validation loss 0.020428303629159927 Accuracy 0.79150390625\n",
      "Iteration 8990 Training loss 0.019375959411263466 Validation loss 0.020913900807499886 Accuracy 0.7861328125\n",
      "Iteration 9000 Training loss 0.020389942452311516 Validation loss 0.02030777372419834 Accuracy 0.7919921875\n",
      "Iteration 9010 Training loss 0.01635976880788803 Validation loss 0.020106585696339607 Accuracy 0.7939453125\n",
      "Iteration 9020 Training loss 0.016196349635720253 Validation loss 0.02015765942633152 Accuracy 0.79296875\n",
      "Iteration 9030 Training loss 0.01375990454107523 Validation loss 0.020162619650363922 Accuracy 0.79296875\n",
      "Iteration 9040 Training loss 0.016564449295401573 Validation loss 0.019780900329351425 Accuracy 0.796875\n",
      "Iteration 9050 Training loss 0.019248150289058685 Validation loss 0.022151855751872063 Accuracy 0.7734375\n",
      "Iteration 9060 Training loss 0.017804928123950958 Validation loss 0.022300785407423973 Accuracy 0.77197265625\n",
      "Iteration 9070 Training loss 0.02157003805041313 Validation loss 0.020584603771567345 Accuracy 0.7890625\n",
      "Iteration 9080 Training loss 0.0162399522960186 Validation loss 0.01999194547533989 Accuracy 0.79443359375\n",
      "Iteration 9090 Training loss 0.01806027628481388 Validation loss 0.02085449919104576 Accuracy 0.7861328125\n",
      "Iteration 9100 Training loss 0.01570982113480568 Validation loss 0.020790379494428635 Accuracy 0.78662109375\n",
      "Iteration 9110 Training loss 0.017088372260332108 Validation loss 0.020760685205459595 Accuracy 0.7880859375\n",
      "Iteration 9120 Training loss 0.017419613897800446 Validation loss 0.020000005140900612 Accuracy 0.79541015625\n",
      "Iteration 9130 Training loss 0.017558086663484573 Validation loss 0.020191827788949013 Accuracy 0.79443359375\n",
      "Iteration 9140 Training loss 0.018046004697680473 Validation loss 0.020005598664283752 Accuracy 0.7958984375\n",
      "Iteration 9150 Training loss 0.020836804062128067 Validation loss 0.020257335156202316 Accuracy 0.79296875\n",
      "Iteration 9160 Training loss 0.020600933581590652 Validation loss 0.01996024139225483 Accuracy 0.7958984375\n",
      "Iteration 9170 Training loss 0.020328020676970482 Validation loss 0.01979345642030239 Accuracy 0.796875\n",
      "Iteration 9180 Training loss 0.018792353570461273 Validation loss 0.020219571888446808 Accuracy 0.79248046875\n",
      "Iteration 9190 Training loss 0.015445144847035408 Validation loss 0.020085565745830536 Accuracy 0.79443359375\n",
      "Iteration 9200 Training loss 0.018876586109399796 Validation loss 0.020236898213624954 Accuracy 0.79248046875\n",
      "Iteration 9210 Training loss 0.01862042397260666 Validation loss 0.019837195053696632 Accuracy 0.796875\n",
      "Iteration 9220 Training loss 0.01693749614059925 Validation loss 0.020405223593115807 Accuracy 0.791015625\n",
      "Iteration 9230 Training loss 0.018039321526885033 Validation loss 0.020466135814785957 Accuracy 0.7900390625\n",
      "Iteration 9240 Training loss 0.01776171661913395 Validation loss 0.02035635896027088 Accuracy 0.79296875\n",
      "Iteration 9250 Training loss 0.01905910298228264 Validation loss 0.01995561458170414 Accuracy 0.7958984375\n",
      "Iteration 9260 Training loss 0.01891482062637806 Validation loss 0.020512495189905167 Accuracy 0.7890625\n",
      "Iteration 9270 Training loss 0.018384814262390137 Validation loss 0.021349556744098663 Accuracy 0.78173828125\n",
      "Iteration 9280 Training loss 0.019415847957134247 Validation loss 0.021009251475334167 Accuracy 0.78564453125\n",
      "Iteration 9290 Training loss 0.018765782937407494 Validation loss 0.02205444686114788 Accuracy 0.7744140625\n",
      "Iteration 9300 Training loss 0.014420815743505955 Validation loss 0.02023608237504959 Accuracy 0.79296875\n",
      "Iteration 9310 Training loss 0.01912388764321804 Validation loss 0.020623158663511276 Accuracy 0.7890625\n",
      "Iteration 9320 Training loss 0.019495347514748573 Validation loss 0.020414847880601883 Accuracy 0.79150390625\n",
      "Iteration 9330 Training loss 0.020493071526288986 Validation loss 0.020773084834218025 Accuracy 0.78662109375\n",
      "Iteration 9340 Training loss 0.015596074052155018 Validation loss 0.020144445821642876 Accuracy 0.79443359375\n",
      "Iteration 9350 Training loss 0.019433697685599327 Validation loss 0.02083783969283104 Accuracy 0.78662109375\n",
      "Iteration 9360 Training loss 0.016844727098941803 Validation loss 0.01997091807425022 Accuracy 0.794921875\n",
      "Iteration 9370 Training loss 0.01865839958190918 Validation loss 0.020519418641924858 Accuracy 0.78955078125\n",
      "Iteration 9380 Training loss 0.01772906817495823 Validation loss 0.01970367506146431 Accuracy 0.7978515625\n",
      "Iteration 9390 Training loss 0.017125368118286133 Validation loss 0.020826486870646477 Accuracy 0.78759765625\n",
      "Iteration 9400 Training loss 0.018425190821290016 Validation loss 0.019831791520118713 Accuracy 0.79736328125\n",
      "Iteration 9410 Training loss 0.017906898632645607 Validation loss 0.020858339965343475 Accuracy 0.78662109375\n",
      "Iteration 9420 Training loss 0.01639486849308014 Validation loss 0.02074403502047062 Accuracy 0.7880859375\n",
      "Iteration 9430 Training loss 0.015937915071845055 Validation loss 0.020885344594717026 Accuracy 0.78662109375\n",
      "Iteration 9440 Training loss 0.020476611331105232 Validation loss 0.019876297563314438 Accuracy 0.79638671875\n",
      "Iteration 9450 Training loss 0.01788383163511753 Validation loss 0.019601253792643547 Accuracy 0.79931640625\n",
      "Iteration 9460 Training loss 0.0184605922549963 Validation loss 0.021680055186152458 Accuracy 0.77880859375\n",
      "Iteration 9470 Training loss 0.017935074865818024 Validation loss 0.019800756126642227 Accuracy 0.79736328125\n",
      "Iteration 9480 Training loss 0.01778441295027733 Validation loss 0.02109544537961483 Accuracy 0.78369140625\n",
      "Iteration 9490 Training loss 0.01742994226515293 Validation loss 0.019674869254231453 Accuracy 0.79833984375\n",
      "Iteration 9500 Training loss 0.018666358664631844 Validation loss 0.019832324236631393 Accuracy 0.796875\n",
      "Iteration 9510 Training loss 0.02220427617430687 Validation loss 0.02217697538435459 Accuracy 0.77392578125\n",
      "Iteration 9520 Training loss 0.01910226419568062 Validation loss 0.021455636247992516 Accuracy 0.78173828125\n",
      "Iteration 9530 Training loss 0.018697915598750114 Validation loss 0.020076416432857513 Accuracy 0.7939453125\n",
      "Iteration 9540 Training loss 0.01780695654451847 Validation loss 0.021741138771176338 Accuracy 0.77783203125\n",
      "Iteration 9550 Training loss 0.020359808579087257 Validation loss 0.02033735252916813 Accuracy 0.791015625\n",
      "Iteration 9560 Training loss 0.01647556945681572 Validation loss 0.01941700652241707 Accuracy 0.80126953125\n",
      "Iteration 9570 Training loss 0.016892826184630394 Validation loss 0.019478194415569305 Accuracy 0.7998046875\n",
      "Iteration 9580 Training loss 0.017929384484887123 Validation loss 0.019910836592316628 Accuracy 0.7958984375\n",
      "Iteration 9590 Training loss 0.018898630514740944 Validation loss 0.020899245515465736 Accuracy 0.78564453125\n",
      "Iteration 9600 Training loss 0.01879434660077095 Validation loss 0.019453490152955055 Accuracy 0.80078125\n",
      "Iteration 9610 Training loss 0.01960163563489914 Validation loss 0.01933608017861843 Accuracy 0.80224609375\n",
      "Iteration 9620 Training loss 0.019744819030165672 Validation loss 0.020466124638915062 Accuracy 0.7900390625\n",
      "Iteration 9630 Training loss 0.019070053473114967 Validation loss 0.020970890298485756 Accuracy 0.78515625\n",
      "Iteration 9640 Training loss 0.017876043915748596 Validation loss 0.01993369869887829 Accuracy 0.79541015625\n",
      "Iteration 9650 Training loss 0.01712799072265625 Validation loss 0.019426686689257622 Accuracy 0.80126953125\n",
      "Iteration 9660 Training loss 0.01691288687288761 Validation loss 0.019894830882549286 Accuracy 0.796875\n",
      "Iteration 9670 Training loss 0.017892617732286453 Validation loss 0.020475640892982483 Accuracy 0.79052734375\n",
      "Iteration 9680 Training loss 0.018314389511942863 Validation loss 0.019494466483592987 Accuracy 0.80078125\n",
      "Iteration 9690 Training loss 0.0177454873919487 Validation loss 0.01928243786096573 Accuracy 0.8017578125\n",
      "Iteration 9700 Training loss 0.01980256848037243 Validation loss 0.019835643470287323 Accuracy 0.796875\n",
      "Iteration 9710 Training loss 0.020435186102986336 Validation loss 0.021783065050840378 Accuracy 0.77783203125\n",
      "Iteration 9720 Training loss 0.016193941235542297 Validation loss 0.01944672130048275 Accuracy 0.80078125\n",
      "Iteration 9730 Training loss 0.017865855246782303 Validation loss 0.01946433074772358 Accuracy 0.80078125\n",
      "Iteration 9740 Training loss 0.015822598710656166 Validation loss 0.019420647993683815 Accuracy 0.80126953125\n",
      "Iteration 9750 Training loss 0.014625894837081432 Validation loss 0.019726421684026718 Accuracy 0.7978515625\n",
      "Iteration 9760 Training loss 0.017617762088775635 Validation loss 0.019357826560735703 Accuracy 0.8017578125\n",
      "Iteration 9770 Training loss 0.017024453729391098 Validation loss 0.01923290267586708 Accuracy 0.80224609375\n",
      "Iteration 9780 Training loss 0.018639422953128815 Validation loss 0.02057894878089428 Accuracy 0.78955078125\n",
      "Iteration 9790 Training loss 0.01724919117987156 Validation loss 0.01975051686167717 Accuracy 0.79736328125\n",
      "Iteration 9800 Training loss 0.018024155870079994 Validation loss 0.019452720880508423 Accuracy 0.7998046875\n",
      "Iteration 9810 Training loss 0.014598206616938114 Validation loss 0.02079935558140278 Accuracy 0.78662109375\n",
      "Iteration 9820 Training loss 0.015976065769791603 Validation loss 0.020123090595006943 Accuracy 0.79345703125\n",
      "Iteration 9830 Training loss 0.01702173426747322 Validation loss 0.019513139501214027 Accuracy 0.7998046875\n",
      "Iteration 9840 Training loss 0.01790929026901722 Validation loss 0.019764602184295654 Accuracy 0.79736328125\n",
      "Iteration 9850 Training loss 0.017543917521834373 Validation loss 0.019498132169246674 Accuracy 0.7998046875\n",
      "Iteration 9860 Training loss 0.018206896260380745 Validation loss 0.01921425200998783 Accuracy 0.802734375\n",
      "Iteration 9870 Training loss 0.017670800909399986 Validation loss 0.019972914829850197 Accuracy 0.7958984375\n",
      "Iteration 9880 Training loss 0.0208203736692667 Validation loss 0.023435750976204872 Accuracy 0.759765625\n",
      "Iteration 9890 Training loss 0.019403720274567604 Validation loss 0.019460024312138557 Accuracy 0.80029296875\n",
      "Iteration 9900 Training loss 0.020212389528751373 Validation loss 0.019460679963231087 Accuracy 0.7998046875\n",
      "Iteration 9910 Training loss 0.015538250096142292 Validation loss 0.020079340785741806 Accuracy 0.79345703125\n",
      "Iteration 9920 Training loss 0.018667254596948624 Validation loss 0.01974157616496086 Accuracy 0.7978515625\n",
      "Iteration 9930 Training loss 0.018532155081629753 Validation loss 0.019408227875828743 Accuracy 0.80029296875\n",
      "Iteration 9940 Training loss 0.019107159227132797 Validation loss 0.019255900755524635 Accuracy 0.802734375\n",
      "Iteration 9950 Training loss 0.019315578043460846 Validation loss 0.020188653841614723 Accuracy 0.79296875\n",
      "Iteration 9960 Training loss 0.015822719782590866 Validation loss 0.0191703662276268 Accuracy 0.8037109375\n",
      "Iteration 9970 Training loss 0.016268381848931313 Validation loss 0.019786197692155838 Accuracy 0.79736328125\n",
      "Iteration 9980 Training loss 0.02072531171143055 Validation loss 0.020063426345586777 Accuracy 0.7939453125\n",
      "Iteration 9990 Training loss 0.016730522736907005 Validation loss 0.019278258085250854 Accuracy 0.80126953125\n",
      "Iteration 10000 Training loss 0.01993531733751297 Validation loss 0.02012518420815468 Accuracy 0.79248046875\n",
      "Iteration 10010 Training loss 0.020878713577985764 Validation loss 0.020316001027822495 Accuracy 0.7919921875\n",
      "Iteration 10020 Training loss 0.018675176426768303 Validation loss 0.019745493307709694 Accuracy 0.7978515625\n",
      "Iteration 10030 Training loss 0.018480196595191956 Validation loss 0.019538938999176025 Accuracy 0.80029296875\n",
      "Iteration 10040 Training loss 0.018736733123660088 Validation loss 0.019408516585826874 Accuracy 0.80078125\n",
      "Iteration 10050 Training loss 0.01774699054658413 Validation loss 0.01993601769208908 Accuracy 0.7958984375\n",
      "Iteration 10060 Training loss 0.019305473193526268 Validation loss 0.01951269432902336 Accuracy 0.80078125\n",
      "Iteration 10070 Training loss 0.018417958170175552 Validation loss 0.019859762862324715 Accuracy 0.79638671875\n",
      "Iteration 10080 Training loss 0.01708434522151947 Validation loss 0.021060211583971977 Accuracy 0.78466796875\n",
      "Iteration 10090 Training loss 0.01717841997742653 Validation loss 0.018970241770148277 Accuracy 0.80517578125\n",
      "Iteration 10100 Training loss 0.0179500263184309 Validation loss 0.019279126077890396 Accuracy 0.80224609375\n",
      "Iteration 10110 Training loss 0.016610272228717804 Validation loss 0.020082036033272743 Accuracy 0.79443359375\n",
      "Iteration 10120 Training loss 0.017145950347185135 Validation loss 0.01947970502078533 Accuracy 0.80029296875\n",
      "Iteration 10130 Training loss 0.018037224188447 Validation loss 0.019358763471245766 Accuracy 0.8017578125\n",
      "Iteration 10140 Training loss 0.02255971170961857 Validation loss 0.0213957317173481 Accuracy 0.78125\n",
      "Iteration 10150 Training loss 0.017608147114515305 Validation loss 0.019034184515476227 Accuracy 0.8046875\n",
      "Iteration 10160 Training loss 0.014996535144746304 Validation loss 0.019295262172818184 Accuracy 0.8017578125\n",
      "Iteration 10170 Training loss 0.01786603033542633 Validation loss 0.01934295892715454 Accuracy 0.80126953125\n",
      "Iteration 10180 Training loss 0.01755378395318985 Validation loss 0.019686300307512283 Accuracy 0.79833984375\n",
      "Iteration 10190 Training loss 0.017384273931384087 Validation loss 0.019118409603834152 Accuracy 0.8037109375\n",
      "Iteration 10200 Training loss 0.016212813556194305 Validation loss 0.01950869895517826 Accuracy 0.79931640625\n",
      "Iteration 10210 Training loss 0.016926856711506844 Validation loss 0.01918199099600315 Accuracy 0.80224609375\n",
      "Iteration 10220 Training loss 0.01799914985895157 Validation loss 0.019573846831917763 Accuracy 0.79931640625\n",
      "Iteration 10230 Training loss 0.019422970712184906 Validation loss 0.019525058567523956 Accuracy 0.79931640625\n",
      "Iteration 10240 Training loss 0.015944629907608032 Validation loss 0.0195353701710701 Accuracy 0.79931640625\n",
      "Iteration 10250 Training loss 0.01756981573998928 Validation loss 0.019539305940270424 Accuracy 0.7998046875\n",
      "Iteration 10260 Training loss 0.018867967650294304 Validation loss 0.01968155801296234 Accuracy 0.79931640625\n",
      "Iteration 10270 Training loss 0.018498245626688004 Validation loss 0.01932128146290779 Accuracy 0.80126953125\n",
      "Iteration 10280 Training loss 0.015109439380466938 Validation loss 0.019675571471452713 Accuracy 0.79736328125\n",
      "Iteration 10290 Training loss 0.024870721623301506 Validation loss 0.0268386397510767 Accuracy 0.72705078125\n",
      "Iteration 10300 Training loss 0.01893800124526024 Validation loss 0.01915230229496956 Accuracy 0.8037109375\n",
      "Iteration 10310 Training loss 0.01670254021883011 Validation loss 0.019324783235788345 Accuracy 0.8017578125\n",
      "Iteration 10320 Training loss 0.018446436151862144 Validation loss 0.020007580518722534 Accuracy 0.794921875\n",
      "Iteration 10330 Training loss 0.015599957667291164 Validation loss 0.019266044721007347 Accuracy 0.80224609375\n",
      "Iteration 10340 Training loss 0.020415741950273514 Validation loss 0.019397128373384476 Accuracy 0.80126953125\n",
      "Iteration 10350 Training loss 0.01861044578254223 Validation loss 0.019884610548615456 Accuracy 0.7958984375\n",
      "Iteration 10360 Training loss 0.015756715089082718 Validation loss 0.020224522799253464 Accuracy 0.79248046875\n",
      "Iteration 10370 Training loss 0.017127973958849907 Validation loss 0.020935770124197006 Accuracy 0.7861328125\n",
      "Iteration 10380 Training loss 0.01911725290119648 Validation loss 0.02146914042532444 Accuracy 0.77978515625\n",
      "Iteration 10390 Training loss 0.017604148015379906 Validation loss 0.018856346607208252 Accuracy 0.806640625\n",
      "Iteration 10400 Training loss 0.018199317157268524 Validation loss 0.01970786042511463 Accuracy 0.79736328125\n",
      "Iteration 10410 Training loss 0.01569247618317604 Validation loss 0.019451148808002472 Accuracy 0.7998046875\n",
      "Iteration 10420 Training loss 0.016818825155496597 Validation loss 0.01952175237238407 Accuracy 0.7998046875\n",
      "Iteration 10430 Training loss 0.01587350107729435 Validation loss 0.018995795398950577 Accuracy 0.8056640625\n",
      "Iteration 10440 Training loss 0.0184959564357996 Validation loss 0.019261792302131653 Accuracy 0.80322265625\n",
      "Iteration 10450 Training loss 0.01663435623049736 Validation loss 0.01920098066329956 Accuracy 0.80322265625\n",
      "Iteration 10460 Training loss 0.015983330085873604 Validation loss 0.019753633067011833 Accuracy 0.79736328125\n",
      "Iteration 10470 Training loss 0.016911586746573448 Validation loss 0.019374441355466843 Accuracy 0.80029296875\n",
      "Iteration 10480 Training loss 0.017192255705595016 Validation loss 0.019185271114110947 Accuracy 0.80322265625\n",
      "Iteration 10490 Training loss 0.01567317172884941 Validation loss 0.01940545253455639 Accuracy 0.80078125\n",
      "Iteration 10500 Training loss 0.017133677378296852 Validation loss 0.019226817414164543 Accuracy 0.80322265625\n",
      "Iteration 10510 Training loss 0.01912142150104046 Validation loss 0.019615471363067627 Accuracy 0.798828125\n",
      "Iteration 10520 Training loss 0.016600847244262695 Validation loss 0.01978563517332077 Accuracy 0.79638671875\n",
      "Iteration 10530 Training loss 0.012696768157184124 Validation loss 0.01953066512942314 Accuracy 0.7998046875\n",
      "Iteration 10540 Training loss 0.015796523541212082 Validation loss 0.01990526355803013 Accuracy 0.79541015625\n",
      "Iteration 10550 Training loss 0.015625767409801483 Validation loss 0.019680766388773918 Accuracy 0.79736328125\n",
      "Iteration 10560 Training loss 0.017381813377141953 Validation loss 0.019811686128377914 Accuracy 0.796875\n",
      "Iteration 10570 Training loss 0.016921836882829666 Validation loss 0.019146161153912544 Accuracy 0.80322265625\n",
      "Iteration 10580 Training loss 0.020052917301654816 Validation loss 0.020775234326720238 Accuracy 0.7880859375\n",
      "Iteration 10590 Training loss 0.01459770929068327 Validation loss 0.020068859681487083 Accuracy 0.7939453125\n",
      "Iteration 10600 Training loss 0.017218811437487602 Validation loss 0.019052058458328247 Accuracy 0.80419921875\n",
      "Iteration 10610 Training loss 0.01865246705710888 Validation loss 0.019544100388884544 Accuracy 0.7998046875\n",
      "Iteration 10620 Training loss 0.017856549471616745 Validation loss 0.019987083971500397 Accuracy 0.79443359375\n",
      "Iteration 10630 Training loss 0.020060617476701736 Validation loss 0.019361378625035286 Accuracy 0.80126953125\n",
      "Iteration 10640 Training loss 0.01777247153222561 Validation loss 0.0203799270093441 Accuracy 0.791015625\n",
      "Iteration 10650 Training loss 0.017489118501544 Validation loss 0.01944335736334324 Accuracy 0.7998046875\n",
      "Iteration 10660 Training loss 0.016936006024479866 Validation loss 0.02042994648218155 Accuracy 0.791015625\n",
      "Iteration 10670 Training loss 0.019688505679368973 Validation loss 0.019700780510902405 Accuracy 0.79736328125\n",
      "Iteration 10680 Training loss 0.018780160695314407 Validation loss 0.019322536885738373 Accuracy 0.8017578125\n",
      "Iteration 10690 Training loss 0.015973486006259918 Validation loss 0.01902632974088192 Accuracy 0.80419921875\n",
      "Iteration 10700 Training loss 0.017027705907821655 Validation loss 0.019408948719501495 Accuracy 0.80078125\n",
      "Iteration 10710 Training loss 0.015592320822179317 Validation loss 0.019580185413360596 Accuracy 0.79931640625\n",
      "Iteration 10720 Training loss 0.014749188907444477 Validation loss 0.019716499373316765 Accuracy 0.79736328125\n",
      "Iteration 10730 Training loss 0.016695888713002205 Validation loss 0.019252153113484383 Accuracy 0.802734375\n",
      "Iteration 10740 Training loss 0.019246093928813934 Validation loss 0.018985988572239876 Accuracy 0.8046875\n",
      "Iteration 10750 Training loss 0.018996916711330414 Validation loss 0.020628366619348526 Accuracy 0.7890625\n",
      "Iteration 10760 Training loss 0.016740664839744568 Validation loss 0.019442329183220863 Accuracy 0.80126953125\n",
      "Iteration 10770 Training loss 0.01755458489060402 Validation loss 0.018955668434500694 Accuracy 0.8046875\n",
      "Iteration 10780 Training loss 0.01777089387178421 Validation loss 0.01988648995757103 Accuracy 0.7958984375\n",
      "Iteration 10790 Training loss 0.017468875274062157 Validation loss 0.019289741292595863 Accuracy 0.80126953125\n",
      "Iteration 10800 Training loss 0.017258981242775917 Validation loss 0.01907425746321678 Accuracy 0.8037109375\n",
      "Iteration 10810 Training loss 0.016080159693956375 Validation loss 0.019236918538808823 Accuracy 0.802734375\n",
      "Iteration 10820 Training loss 0.018485797569155693 Validation loss 0.01995115727186203 Accuracy 0.7958984375\n",
      "Iteration 10830 Training loss 0.019037986174225807 Validation loss 0.01978325843811035 Accuracy 0.79541015625\n",
      "Iteration 10840 Training loss 0.014031333848834038 Validation loss 0.01922329142689705 Accuracy 0.80224609375\n",
      "Iteration 10850 Training loss 0.018455075100064278 Validation loss 0.01920267939567566 Accuracy 0.8017578125\n",
      "Iteration 10860 Training loss 0.01641135849058628 Validation loss 0.019116198644042015 Accuracy 0.80419921875\n",
      "Iteration 10870 Training loss 0.01641050912439823 Validation loss 0.01939520426094532 Accuracy 0.80078125\n",
      "Iteration 10880 Training loss 0.019042156636714935 Validation loss 0.019257333129644394 Accuracy 0.802734375\n",
      "Iteration 10890 Training loss 0.01703675091266632 Validation loss 0.01967458613216877 Accuracy 0.79833984375\n",
      "Iteration 10900 Training loss 0.01405761856585741 Validation loss 0.01949797011911869 Accuracy 0.7998046875\n",
      "Iteration 10910 Training loss 0.017682000994682312 Validation loss 0.019380131736397743 Accuracy 0.80126953125\n",
      "Iteration 10920 Training loss 0.013316141441464424 Validation loss 0.01947961188852787 Accuracy 0.7998046875\n",
      "Iteration 10930 Training loss 0.017075780779123306 Validation loss 0.018788745626807213 Accuracy 0.8076171875\n",
      "Iteration 10940 Training loss 0.01955261453986168 Validation loss 0.0190445389598608 Accuracy 0.8046875\n",
      "Iteration 10950 Training loss 0.01655837520956993 Validation loss 0.01920902542769909 Accuracy 0.80322265625\n",
      "Iteration 10960 Training loss 0.015675140544772148 Validation loss 0.02089403010904789 Accuracy 0.78662109375\n",
      "Iteration 10970 Training loss 0.01752210035920143 Validation loss 0.01931513287127018 Accuracy 0.8017578125\n",
      "Iteration 10980 Training loss 0.018221639096736908 Validation loss 0.0203991886228323 Accuracy 0.7919921875\n",
      "Iteration 10990 Training loss 0.02087813802063465 Validation loss 0.023198995739221573 Accuracy 0.76416015625\n",
      "Iteration 11000 Training loss 0.016288967803120613 Validation loss 0.019312938675284386 Accuracy 0.8017578125\n",
      "Iteration 11010 Training loss 0.01549972128123045 Validation loss 0.019526103511452675 Accuracy 0.80029296875\n",
      "Iteration 11020 Training loss 0.019322330132126808 Validation loss 0.019126281142234802 Accuracy 0.80322265625\n",
      "Iteration 11030 Training loss 0.016084136441349983 Validation loss 0.018998563289642334 Accuracy 0.80615234375\n",
      "Iteration 11040 Training loss 0.01699438877403736 Validation loss 0.019397050142288208 Accuracy 0.80078125\n",
      "Iteration 11050 Training loss 0.018962102010846138 Validation loss 0.019650476053357124 Accuracy 0.7978515625\n",
      "Iteration 11060 Training loss 0.017347341403365135 Validation loss 0.019421042874455452 Accuracy 0.7998046875\n",
      "Iteration 11070 Training loss 0.016548361629247665 Validation loss 0.0193682461977005 Accuracy 0.80078125\n",
      "Iteration 11080 Training loss 0.015867656096816063 Validation loss 0.019568519666790962 Accuracy 0.79833984375\n",
      "Iteration 11090 Training loss 0.01752907782793045 Validation loss 0.019171418622136116 Accuracy 0.80224609375\n",
      "Iteration 11100 Training loss 0.01616874523460865 Validation loss 0.019006889313459396 Accuracy 0.80517578125\n",
      "Iteration 11110 Training loss 0.020415719598531723 Validation loss 0.018669748678803444 Accuracy 0.80810546875\n",
      "Iteration 11120 Training loss 0.019485928118228912 Validation loss 0.019510148093104362 Accuracy 0.7998046875\n",
      "Iteration 11130 Training loss 0.01509378757327795 Validation loss 0.019007869064807892 Accuracy 0.8046875\n",
      "Iteration 11140 Training loss 0.016513461247086525 Validation loss 0.01918088272213936 Accuracy 0.8037109375\n",
      "Iteration 11150 Training loss 0.017033269628882408 Validation loss 0.019935360178351402 Accuracy 0.796875\n",
      "Iteration 11160 Training loss 0.01639353670179844 Validation loss 0.019634855911135674 Accuracy 0.798828125\n",
      "Iteration 11170 Training loss 0.01655144803225994 Validation loss 0.018889667466282845 Accuracy 0.80615234375\n",
      "Iteration 11180 Training loss 0.016590608283877373 Validation loss 0.019422387704253197 Accuracy 0.80029296875\n",
      "Iteration 11190 Training loss 0.02093949168920517 Validation loss 0.019812002778053284 Accuracy 0.79541015625\n",
      "Iteration 11200 Training loss 0.013415832072496414 Validation loss 0.01902903988957405 Accuracy 0.80517578125\n",
      "Iteration 11210 Training loss 0.016369732096791267 Validation loss 0.019137509167194366 Accuracy 0.80419921875\n",
      "Iteration 11220 Training loss 0.01588122732937336 Validation loss 0.019540756940841675 Accuracy 0.7998046875\n",
      "Iteration 11230 Training loss 0.019484924152493477 Validation loss 0.01899307779967785 Accuracy 0.8046875\n",
      "Iteration 11240 Training loss 0.01733790896832943 Validation loss 0.018864983692765236 Accuracy 0.8056640625\n",
      "Iteration 11250 Training loss 0.016177136451005936 Validation loss 0.01877153292298317 Accuracy 0.80712890625\n",
      "Iteration 11260 Training loss 0.013047341257333755 Validation loss 0.01917962357401848 Accuracy 0.802734375\n",
      "Iteration 11270 Training loss 0.015201589092612267 Validation loss 0.01906314119696617 Accuracy 0.80322265625\n",
      "Iteration 11280 Training loss 0.017147734761238098 Validation loss 0.019248835742473602 Accuracy 0.8017578125\n",
      "Iteration 11290 Training loss 0.015617633238434792 Validation loss 0.019156619906425476 Accuracy 0.80322265625\n",
      "Iteration 11300 Training loss 0.01586667262017727 Validation loss 0.019254952669143677 Accuracy 0.8017578125\n",
      "Iteration 11310 Training loss 0.01598214916884899 Validation loss 0.019225645810365677 Accuracy 0.80224609375\n",
      "Iteration 11320 Training loss 0.017189135774970055 Validation loss 0.01900475099682808 Accuracy 0.80419921875\n",
      "Iteration 11330 Training loss 0.018844593316316605 Validation loss 0.019959252327680588 Accuracy 0.79541015625\n",
      "Iteration 11340 Training loss 0.016949234530329704 Validation loss 0.019108669832348824 Accuracy 0.8037109375\n",
      "Iteration 11350 Training loss 0.018481263890862465 Validation loss 0.018702073022723198 Accuracy 0.8076171875\n",
      "Iteration 11360 Training loss 0.02260185219347477 Validation loss 0.020872605964541435 Accuracy 0.78564453125\n",
      "Iteration 11370 Training loss 0.016078991815447807 Validation loss 0.01881127990782261 Accuracy 0.806640625\n",
      "Iteration 11380 Training loss 0.019877588376402855 Validation loss 0.019923407584428787 Accuracy 0.7958984375\n",
      "Iteration 11390 Training loss 0.01847863756120205 Validation loss 0.018884483724832535 Accuracy 0.8056640625\n",
      "Iteration 11400 Training loss 0.014756014570593834 Validation loss 0.018709855154156685 Accuracy 0.80810546875\n",
      "Iteration 11410 Training loss 0.017988057807087898 Validation loss 0.019014891237020493 Accuracy 0.80517578125\n",
      "Iteration 11420 Training loss 0.01672234758734703 Validation loss 0.018890375271439552 Accuracy 0.8056640625\n",
      "Iteration 11430 Training loss 0.01652885414659977 Validation loss 0.018856890499591827 Accuracy 0.8056640625\n",
      "Iteration 11440 Training loss 0.018199708312749863 Validation loss 0.019278710708022118 Accuracy 0.8017578125\n",
      "Iteration 11450 Training loss 0.01668773591518402 Validation loss 0.019611943513154984 Accuracy 0.79833984375\n",
      "Iteration 11460 Training loss 0.018052106723189354 Validation loss 0.018875043839216232 Accuracy 0.80615234375\n",
      "Iteration 11470 Training loss 0.020095720887184143 Validation loss 0.02053925208747387 Accuracy 0.78955078125\n",
      "Iteration 11480 Training loss 0.01625349372625351 Validation loss 0.019033562391996384 Accuracy 0.80419921875\n",
      "Iteration 11490 Training loss 0.018466049805283546 Validation loss 0.020032357424497604 Accuracy 0.79443359375\n",
      "Iteration 11500 Training loss 0.016674522310495377 Validation loss 0.019166437909007072 Accuracy 0.80322265625\n",
      "Iteration 11510 Training loss 0.014070598408579826 Validation loss 0.018930448219180107 Accuracy 0.80517578125\n",
      "Iteration 11520 Training loss 0.01580420881509781 Validation loss 0.018890641629695892 Accuracy 0.80517578125\n",
      "Iteration 11530 Training loss 0.016671955585479736 Validation loss 0.019382033497095108 Accuracy 0.80029296875\n",
      "Iteration 11540 Training loss 0.019578730687499046 Validation loss 0.01986413449048996 Accuracy 0.79541015625\n",
      "Iteration 11550 Training loss 0.0157410129904747 Validation loss 0.019225606694817543 Accuracy 0.80224609375\n",
      "Iteration 11560 Training loss 0.015443655662238598 Validation loss 0.01923125609755516 Accuracy 0.80224609375\n",
      "Iteration 11570 Training loss 0.018458882346749306 Validation loss 0.019878821447491646 Accuracy 0.79638671875\n",
      "Iteration 11580 Training loss 0.016137229278683662 Validation loss 0.019339824095368385 Accuracy 0.80126953125\n",
      "Iteration 11590 Training loss 0.014291430823504925 Validation loss 0.019076241180300713 Accuracy 0.802734375\n",
      "Iteration 11600 Training loss 0.01896396465599537 Validation loss 0.02025717869400978 Accuracy 0.79296875\n",
      "Iteration 11610 Training loss 0.015293828211724758 Validation loss 0.018927814438939095 Accuracy 0.80615234375\n",
      "Iteration 11620 Training loss 0.015051581896841526 Validation loss 0.018959227949380875 Accuracy 0.80615234375\n",
      "Iteration 11630 Training loss 0.016376730054616928 Validation loss 0.019679659977555275 Accuracy 0.79833984375\n",
      "Iteration 11640 Training loss 0.01808128133416176 Validation loss 0.018991859629750252 Accuracy 0.80419921875\n",
      "Iteration 11650 Training loss 0.018884776160120964 Validation loss 0.01945531740784645 Accuracy 0.7998046875\n",
      "Iteration 11660 Training loss 0.01519758440554142 Validation loss 0.01885291188955307 Accuracy 0.8046875\n",
      "Iteration 11670 Training loss 0.01551747415214777 Validation loss 0.02033834718167782 Accuracy 0.79150390625\n",
      "Iteration 11680 Training loss 0.01596435159444809 Validation loss 0.019192537292838097 Accuracy 0.8017578125\n",
      "Iteration 11690 Training loss 0.01692468672990799 Validation loss 0.01995108835399151 Accuracy 0.79443359375\n",
      "Iteration 11700 Training loss 0.01774645783007145 Validation loss 0.019748693332076073 Accuracy 0.796875\n",
      "Iteration 11710 Training loss 0.017093565315008163 Validation loss 0.019321149215102196 Accuracy 0.80126953125\n",
      "Iteration 11720 Training loss 0.018206441774964333 Validation loss 0.019297299906611443 Accuracy 0.802734375\n",
      "Iteration 11730 Training loss 0.016064560040831566 Validation loss 0.01917239837348461 Accuracy 0.80419921875\n",
      "Iteration 11740 Training loss 0.017194753512740135 Validation loss 0.01861945539712906 Accuracy 0.80908203125\n",
      "Iteration 11750 Training loss 0.01554478146135807 Validation loss 0.019328352063894272 Accuracy 0.80029296875\n",
      "Iteration 11760 Training loss 0.015561078675091267 Validation loss 0.019124969840049744 Accuracy 0.80322265625\n",
      "Iteration 11770 Training loss 0.018524976447224617 Validation loss 0.01898115500807762 Accuracy 0.8046875\n",
      "Iteration 11780 Training loss 0.01581263542175293 Validation loss 0.01869904436171055 Accuracy 0.8076171875\n",
      "Iteration 11790 Training loss 0.016401465982198715 Validation loss 0.019352586939930916 Accuracy 0.80078125\n",
      "Iteration 11800 Training loss 0.014739476144313812 Validation loss 0.01927519403398037 Accuracy 0.8017578125\n",
      "Iteration 11810 Training loss 0.0156302098184824 Validation loss 0.01935766451060772 Accuracy 0.80078125\n",
      "Iteration 11820 Training loss 0.01619757153093815 Validation loss 0.018782351166009903 Accuracy 0.806640625\n",
      "Iteration 11830 Training loss 0.017441468313336372 Validation loss 0.019033662974834442 Accuracy 0.80419921875\n",
      "Iteration 11840 Training loss 0.02043784223496914 Validation loss 0.01897379569709301 Accuracy 0.8046875\n",
      "Iteration 11850 Training loss 0.017381705343723297 Validation loss 0.020395753905177116 Accuracy 0.79150390625\n",
      "Iteration 11860 Training loss 0.0181315615773201 Validation loss 0.019037971273064613 Accuracy 0.80419921875\n",
      "Iteration 11870 Training loss 0.016847483813762665 Validation loss 0.01901852898299694 Accuracy 0.80517578125\n",
      "Iteration 11880 Training loss 0.018133552744984627 Validation loss 0.019550535827875137 Accuracy 0.7998046875\n",
      "Iteration 11890 Training loss 0.014912881888449192 Validation loss 0.018987229093909264 Accuracy 0.8037109375\n",
      "Iteration 11900 Training loss 0.014587732963263988 Validation loss 0.019287750124931335 Accuracy 0.8017578125\n",
      "Iteration 11910 Training loss 0.016495641320943832 Validation loss 0.019081346690654755 Accuracy 0.80419921875\n",
      "Iteration 11920 Training loss 0.015503793954849243 Validation loss 0.0200611911714077 Accuracy 0.79296875\n",
      "Iteration 11930 Training loss 0.016712281852960587 Validation loss 0.01921766810119152 Accuracy 0.80126953125\n",
      "Iteration 11940 Training loss 0.015025581233203411 Validation loss 0.018762987107038498 Accuracy 0.80712890625\n",
      "Iteration 11950 Training loss 0.016804512590169907 Validation loss 0.01891457289457321 Accuracy 0.80517578125\n",
      "Iteration 11960 Training loss 0.016472412273287773 Validation loss 0.018889809027314186 Accuracy 0.8056640625\n",
      "Iteration 11970 Training loss 0.018141621723771095 Validation loss 0.01905881054699421 Accuracy 0.80419921875\n",
      "Iteration 11980 Training loss 0.018943417817354202 Validation loss 0.018974894657731056 Accuracy 0.80517578125\n",
      "Iteration 11990 Training loss 0.01830288954079151 Validation loss 0.019246995449066162 Accuracy 0.80322265625\n",
      "Iteration 12000 Training loss 0.017315737903118134 Validation loss 0.019176801666617393 Accuracy 0.8017578125\n",
      "Iteration 12010 Training loss 0.01632680743932724 Validation loss 0.018992243334650993 Accuracy 0.8037109375\n",
      "Iteration 12020 Training loss 0.019277451559901237 Validation loss 0.019661039113998413 Accuracy 0.7978515625\n",
      "Iteration 12030 Training loss 0.014997283928096294 Validation loss 0.01844244822859764 Accuracy 0.81005859375\n",
      "Iteration 12040 Training loss 0.01837490126490593 Validation loss 0.018785642459988594 Accuracy 0.8056640625\n",
      "Iteration 12050 Training loss 0.014767097309231758 Validation loss 0.018734591081738472 Accuracy 0.80712890625\n",
      "Iteration 12060 Training loss 0.015712643042206764 Validation loss 0.018956545740365982 Accuracy 0.80517578125\n",
      "Iteration 12070 Training loss 0.01634032651782036 Validation loss 0.01890505850315094 Accuracy 0.806640625\n",
      "Iteration 12080 Training loss 0.014822385273873806 Validation loss 0.019052905961871147 Accuracy 0.80419921875\n",
      "Iteration 12090 Training loss 0.017878934741020203 Validation loss 0.018968364223837852 Accuracy 0.80517578125\n",
      "Iteration 12100 Training loss 0.01769719459116459 Validation loss 0.01892508566379547 Accuracy 0.8046875\n",
      "Iteration 12110 Training loss 0.015890026465058327 Validation loss 0.019091736525297165 Accuracy 0.802734375\n",
      "Iteration 12120 Training loss 0.012853463180363178 Validation loss 0.01909632608294487 Accuracy 0.80322265625\n",
      "Iteration 12130 Training loss 0.015301528386771679 Validation loss 0.020004916936159134 Accuracy 0.79443359375\n",
      "Iteration 12140 Training loss 0.017393426969647408 Validation loss 0.019626840949058533 Accuracy 0.79833984375\n",
      "Iteration 12150 Training loss 0.014649108983576298 Validation loss 0.01874413713812828 Accuracy 0.80810546875\n",
      "Iteration 12160 Training loss 0.015498371794819832 Validation loss 0.018937626853585243 Accuracy 0.80517578125\n",
      "Iteration 12170 Training loss 0.01759394258260727 Validation loss 0.019528286531567574 Accuracy 0.79931640625\n",
      "Iteration 12180 Training loss 0.017637303099036217 Validation loss 0.018977336585521698 Accuracy 0.8046875\n",
      "Iteration 12190 Training loss 0.01735331118106842 Validation loss 0.018667226657271385 Accuracy 0.80810546875\n",
      "Iteration 12200 Training loss 0.01845472864806652 Validation loss 0.019674818962812424 Accuracy 0.7978515625\n",
      "Iteration 12210 Training loss 0.015749836340546608 Validation loss 0.018853839486837387 Accuracy 0.806640625\n",
      "Iteration 12220 Training loss 0.015326235443353653 Validation loss 0.019137291237711906 Accuracy 0.80322265625\n",
      "Iteration 12230 Training loss 0.015702642500400543 Validation loss 0.018987232819199562 Accuracy 0.8056640625\n",
      "Iteration 12240 Training loss 0.01495231781154871 Validation loss 0.018756940960884094 Accuracy 0.806640625\n",
      "Iteration 12250 Training loss 0.02117742970585823 Validation loss 0.01897892728447914 Accuracy 0.8037109375\n",
      "Iteration 12260 Training loss 0.017022959887981415 Validation loss 0.018710697069764137 Accuracy 0.80712890625\n",
      "Iteration 12270 Training loss 0.017509382218122482 Validation loss 0.019131362438201904 Accuracy 0.802734375\n",
      "Iteration 12280 Training loss 0.01515077706426382 Validation loss 0.01923222839832306 Accuracy 0.80126953125\n",
      "Iteration 12290 Training loss 0.018237732350826263 Validation loss 0.018964603543281555 Accuracy 0.80517578125\n",
      "Iteration 12300 Training loss 0.016856443136930466 Validation loss 0.018771832808852196 Accuracy 0.8076171875\n",
      "Iteration 12310 Training loss 0.018235519528388977 Validation loss 0.019059013575315475 Accuracy 0.80419921875\n",
      "Iteration 12320 Training loss 0.017244936898350716 Validation loss 0.019150981679558754 Accuracy 0.80322265625\n",
      "Iteration 12330 Training loss 0.017191242426633835 Validation loss 0.019018199294805527 Accuracy 0.8046875\n",
      "Iteration 12340 Training loss 0.017366452142596245 Validation loss 0.01893913559615612 Accuracy 0.80517578125\n",
      "Iteration 12350 Training loss 0.014103055000305176 Validation loss 0.01883726939558983 Accuracy 0.806640625\n",
      "Iteration 12360 Training loss 0.016085902228951454 Validation loss 0.019089404493570328 Accuracy 0.8046875\n",
      "Iteration 12370 Training loss 0.014380858279764652 Validation loss 0.019010724499821663 Accuracy 0.8046875\n",
      "Iteration 12380 Training loss 0.01281630527228117 Validation loss 0.019110407680273056 Accuracy 0.8046875\n",
      "Iteration 12390 Training loss 0.015960106626152992 Validation loss 0.020255232229828835 Accuracy 0.79248046875\n",
      "Iteration 12400 Training loss 0.01565876044332981 Validation loss 0.01937939040362835 Accuracy 0.80078125\n",
      "Iteration 12410 Training loss 0.016110029071569443 Validation loss 0.018904339522123337 Accuracy 0.8056640625\n",
      "Iteration 12420 Training loss 0.017377212643623352 Validation loss 0.01929723657667637 Accuracy 0.80224609375\n",
      "Iteration 12430 Training loss 0.01646154187619686 Validation loss 0.01861540786921978 Accuracy 0.80908203125\n",
      "Iteration 12440 Training loss 0.015557807870209217 Validation loss 0.018453752622008324 Accuracy 0.810546875\n",
      "Iteration 12450 Training loss 0.018476752564311028 Validation loss 0.01896478980779648 Accuracy 0.8046875\n",
      "Iteration 12460 Training loss 0.019222361966967583 Validation loss 0.018939141184091568 Accuracy 0.80517578125\n",
      "Iteration 12470 Training loss 0.01450270600616932 Validation loss 0.018958818167448044 Accuracy 0.80419921875\n",
      "Iteration 12480 Training loss 0.01649964042007923 Validation loss 0.018545180559158325 Accuracy 0.8095703125\n",
      "Iteration 12490 Training loss 0.01586175337433815 Validation loss 0.01870667189359665 Accuracy 0.80615234375\n",
      "Iteration 12500 Training loss 0.013331093825399876 Validation loss 0.018975187093019485 Accuracy 0.8046875\n",
      "Iteration 12510 Training loss 0.015091652981936932 Validation loss 0.019690409302711487 Accuracy 0.79833984375\n",
      "Iteration 12520 Training loss 0.016573650762438774 Validation loss 0.01831621490418911 Accuracy 0.81103515625\n",
      "Iteration 12530 Training loss 0.016296232119202614 Validation loss 0.018777789548039436 Accuracy 0.80615234375\n",
      "Iteration 12540 Training loss 0.018388040363788605 Validation loss 0.01848602294921875 Accuracy 0.8095703125\n",
      "Iteration 12550 Training loss 0.016348106786608696 Validation loss 0.018952567130327225 Accuracy 0.80322265625\n",
      "Iteration 12560 Training loss 0.018468813970685005 Validation loss 0.01961517333984375 Accuracy 0.79736328125\n",
      "Iteration 12570 Training loss 0.014949322678148746 Validation loss 0.018743284046649933 Accuracy 0.8056640625\n",
      "Iteration 12580 Training loss 0.01707957126200199 Validation loss 0.019590994343161583 Accuracy 0.79833984375\n",
      "Iteration 12590 Training loss 0.016642460599541664 Validation loss 0.018751440569758415 Accuracy 0.80712890625\n",
      "Iteration 12600 Training loss 0.01666734740138054 Validation loss 0.01885570026934147 Accuracy 0.80712890625\n",
      "Iteration 12610 Training loss 0.014808962121605873 Validation loss 0.018594535067677498 Accuracy 0.8076171875\n",
      "Iteration 12620 Training loss 0.01654234156012535 Validation loss 0.018925746902823448 Accuracy 0.80419921875\n",
      "Iteration 12630 Training loss 0.018335215747356415 Validation loss 0.01843925565481186 Accuracy 0.8095703125\n",
      "Iteration 12640 Training loss 0.018737606704235077 Validation loss 0.019238295033574104 Accuracy 0.8017578125\n",
      "Iteration 12650 Training loss 0.01787818782031536 Validation loss 0.018874621018767357 Accuracy 0.80517578125\n",
      "Iteration 12660 Training loss 0.018294962123036385 Validation loss 0.019266659393906593 Accuracy 0.80126953125\n",
      "Iteration 12670 Training loss 0.0173883568495512 Validation loss 0.019602132961153984 Accuracy 0.7978515625\n",
      "Iteration 12680 Training loss 0.017988627776503563 Validation loss 0.019577516242861748 Accuracy 0.7978515625\n",
      "Iteration 12690 Training loss 0.017427757382392883 Validation loss 0.018841952085494995 Accuracy 0.8056640625\n",
      "Iteration 12700 Training loss 0.01434249710291624 Validation loss 0.018720535561442375 Accuracy 0.8076171875\n",
      "Iteration 12710 Training loss 0.016362905502319336 Validation loss 0.018727390095591545 Accuracy 0.80712890625\n",
      "Iteration 12720 Training loss 0.014394573867321014 Validation loss 0.018584182485938072 Accuracy 0.80908203125\n",
      "Iteration 12730 Training loss 0.01501868199557066 Validation loss 0.018954738974571228 Accuracy 0.80419921875\n",
      "Iteration 12740 Training loss 0.01571366935968399 Validation loss 0.018745746463537216 Accuracy 0.80712890625\n",
      "Iteration 12750 Training loss 0.015107415616512299 Validation loss 0.018793698400259018 Accuracy 0.8056640625\n",
      "Iteration 12760 Training loss 0.013517135754227638 Validation loss 0.018707670271396637 Accuracy 0.80615234375\n",
      "Iteration 12770 Training loss 0.01708320714533329 Validation loss 0.019556129351258278 Accuracy 0.798828125\n",
      "Iteration 12780 Training loss 0.016909008845686913 Validation loss 0.018905578181147575 Accuracy 0.80615234375\n",
      "Iteration 12790 Training loss 0.013690761290490627 Validation loss 0.018905162811279297 Accuracy 0.80615234375\n",
      "Iteration 12800 Training loss 0.01427381206303835 Validation loss 0.01852688379585743 Accuracy 0.8095703125\n",
      "Iteration 12810 Training loss 0.01772174797952175 Validation loss 0.01899479702115059 Accuracy 0.8046875\n",
      "Iteration 12820 Training loss 0.018249517306685448 Validation loss 0.01899007335305214 Accuracy 0.8046875\n",
      "Iteration 12830 Training loss 0.015784693881869316 Validation loss 0.018694883212447166 Accuracy 0.80810546875\n",
      "Iteration 12840 Training loss 0.015676017850637436 Validation loss 0.018431102856993675 Accuracy 0.81005859375\n",
      "Iteration 12850 Training loss 0.01736716739833355 Validation loss 0.01850145123898983 Accuracy 0.80908203125\n",
      "Iteration 12860 Training loss 0.02066030353307724 Validation loss 0.01972450688481331 Accuracy 0.7958984375\n",
      "Iteration 12870 Training loss 0.01631971448659897 Validation loss 0.01899918168783188 Accuracy 0.8037109375\n",
      "Iteration 12880 Training loss 0.015923112630844116 Validation loss 0.019032157957553864 Accuracy 0.80322265625\n",
      "Iteration 12890 Training loss 0.01925630122423172 Validation loss 0.019111357629299164 Accuracy 0.80322265625\n",
      "Iteration 12900 Training loss 0.0163718294352293 Validation loss 0.0188551414757967 Accuracy 0.80517578125\n",
      "Iteration 12910 Training loss 0.01942463591694832 Validation loss 0.019726771861314774 Accuracy 0.79736328125\n",
      "Iteration 12920 Training loss 0.014025884680449963 Validation loss 0.01904181018471718 Accuracy 0.80419921875\n",
      "Iteration 12930 Training loss 0.016672076657414436 Validation loss 0.018560728058218956 Accuracy 0.80810546875\n",
      "Iteration 12940 Training loss 0.01668175496160984 Validation loss 0.01856847107410431 Accuracy 0.80712890625\n",
      "Iteration 12950 Training loss 0.0135949170216918 Validation loss 0.019065529108047485 Accuracy 0.80322265625\n",
      "Iteration 12960 Training loss 0.01374827604740858 Validation loss 0.01889747381210327 Accuracy 0.80419921875\n",
      "Iteration 12970 Training loss 0.017223022878170013 Validation loss 0.01953713409602642 Accuracy 0.7978515625\n",
      "Iteration 12980 Training loss 0.015409957617521286 Validation loss 0.01942463405430317 Accuracy 0.7998046875\n",
      "Iteration 12990 Training loss 0.01568642631173134 Validation loss 0.01887470670044422 Accuracy 0.8046875\n",
      "Iteration 13000 Training loss 0.017520802095532417 Validation loss 0.018838876858353615 Accuracy 0.80517578125\n",
      "Iteration 13010 Training loss 0.014026926830410957 Validation loss 0.018715379759669304 Accuracy 0.806640625\n",
      "Iteration 13020 Training loss 0.014062142930924892 Validation loss 0.018815500661730766 Accuracy 0.80712890625\n",
      "Iteration 13030 Training loss 0.013959714211523533 Validation loss 0.018566595390439034 Accuracy 0.80859375\n",
      "Iteration 13040 Training loss 0.014665690250694752 Validation loss 0.01914331316947937 Accuracy 0.8037109375\n",
      "Iteration 13050 Training loss 0.016405818983912468 Validation loss 0.018680226057767868 Accuracy 0.80810546875\n",
      "Iteration 13060 Training loss 0.014370208606123924 Validation loss 0.018893815577030182 Accuracy 0.80419921875\n",
      "Iteration 13070 Training loss 0.017990674823522568 Validation loss 0.018923116847872734 Accuracy 0.8037109375\n",
      "Iteration 13080 Training loss 0.016379840672016144 Validation loss 0.018537182360887527 Accuracy 0.80859375\n",
      "Iteration 13090 Training loss 0.015004641376435757 Validation loss 0.01868295669555664 Accuracy 0.80810546875\n",
      "Iteration 13100 Training loss 0.015699047595262527 Validation loss 0.018794022500514984 Accuracy 0.8056640625\n",
      "Iteration 13110 Training loss 0.0161670483648777 Validation loss 0.018627025187015533 Accuracy 0.8076171875\n",
      "Iteration 13120 Training loss 0.01745646260678768 Validation loss 0.01879294402897358 Accuracy 0.8056640625\n",
      "Iteration 13130 Training loss 0.013534445315599442 Validation loss 0.018450958654284477 Accuracy 0.80908203125\n",
      "Iteration 13140 Training loss 0.016773724928498268 Validation loss 0.018854239955544472 Accuracy 0.8037109375\n",
      "Iteration 13150 Training loss 0.016500147059559822 Validation loss 0.01869514398276806 Accuracy 0.80712890625\n",
      "Iteration 13160 Training loss 0.017394106835126877 Validation loss 0.018524926155805588 Accuracy 0.80908203125\n",
      "Iteration 13170 Training loss 0.017586389556527138 Validation loss 0.01955721713602543 Accuracy 0.79736328125\n",
      "Iteration 13180 Training loss 0.017683615908026695 Validation loss 0.018828140571713448 Accuracy 0.8056640625\n",
      "Iteration 13190 Training loss 0.015501691028475761 Validation loss 0.01872045174241066 Accuracy 0.8056640625\n",
      "Iteration 13200 Training loss 0.019230496138334274 Validation loss 0.019318940117955208 Accuracy 0.8017578125\n",
      "Iteration 13210 Training loss 0.02018718607723713 Validation loss 0.019442692399024963 Accuracy 0.80029296875\n",
      "Iteration 13220 Training loss 0.016663312911987305 Validation loss 0.019013117998838425 Accuracy 0.80419921875\n",
      "Iteration 13230 Training loss 0.016411345452070236 Validation loss 0.01884370669722557 Accuracy 0.806640625\n",
      "Iteration 13240 Training loss 0.018692374229431152 Validation loss 0.019108224660158157 Accuracy 0.802734375\n",
      "Iteration 13250 Training loss 0.014481175690889359 Validation loss 0.018675094470381737 Accuracy 0.8076171875\n",
      "Iteration 13260 Training loss 0.01925336942076683 Validation loss 0.018425431102514267 Accuracy 0.8095703125\n",
      "Iteration 13270 Training loss 0.019906604662537575 Validation loss 0.019717292860150337 Accuracy 0.7978515625\n",
      "Iteration 13280 Training loss 0.01661224663257599 Validation loss 0.01897607557475567 Accuracy 0.80419921875\n",
      "Iteration 13290 Training loss 0.017111051827669144 Validation loss 0.018437987193465233 Accuracy 0.81005859375\n",
      "Iteration 13300 Training loss 0.016006002202630043 Validation loss 0.018866868689656258 Accuracy 0.80517578125\n",
      "Iteration 13310 Training loss 0.014460115693509579 Validation loss 0.018283652141690254 Accuracy 0.8115234375\n",
      "Iteration 13320 Training loss 0.0150214284658432 Validation loss 0.019117828458547592 Accuracy 0.802734375\n",
      "Iteration 13330 Training loss 0.01668880134820938 Validation loss 0.018868666142225266 Accuracy 0.80419921875\n",
      "Iteration 13340 Training loss 0.015364769846200943 Validation loss 0.018779562786221504 Accuracy 0.80517578125\n",
      "Iteration 13350 Training loss 0.016415180638432503 Validation loss 0.019189627841114998 Accuracy 0.8017578125\n",
      "Iteration 13360 Training loss 0.013906191103160381 Validation loss 0.01864372007548809 Accuracy 0.80810546875\n",
      "Iteration 13370 Training loss 0.014977316372096539 Validation loss 0.018596995621919632 Accuracy 0.8076171875\n",
      "Iteration 13380 Training loss 0.017422489821910858 Validation loss 0.01853039488196373 Accuracy 0.8076171875\n",
      "Iteration 13390 Training loss 0.01441926322877407 Validation loss 0.01827274262905121 Accuracy 0.81103515625\n",
      "Iteration 13400 Training loss 0.01759110577404499 Validation loss 0.018603673204779625 Accuracy 0.8076171875\n",
      "Iteration 13410 Training loss 0.015415745787322521 Validation loss 0.018489260226488113 Accuracy 0.80859375\n",
      "Iteration 13420 Training loss 0.016791507601737976 Validation loss 0.018819527700543404 Accuracy 0.8056640625\n",
      "Iteration 13430 Training loss 0.015316522680222988 Validation loss 0.018946362659335136 Accuracy 0.8046875\n",
      "Iteration 13440 Training loss 0.014647398144006729 Validation loss 0.018584439530968666 Accuracy 0.8076171875\n",
      "Iteration 13450 Training loss 0.017183659598231316 Validation loss 0.019358297809958458 Accuracy 0.798828125\n",
      "Iteration 13460 Training loss 0.01523649599403143 Validation loss 0.01878480426967144 Accuracy 0.80615234375\n",
      "Iteration 13470 Training loss 0.01436314545571804 Validation loss 0.018413253128528595 Accuracy 0.81005859375\n",
      "Iteration 13480 Training loss 0.014857240952551365 Validation loss 0.018621258437633514 Accuracy 0.80712890625\n",
      "Iteration 13490 Training loss 0.012752347625792027 Validation loss 0.018741395324468613 Accuracy 0.8056640625\n",
      "Iteration 13500 Training loss 0.0164723489433527 Validation loss 0.0184037983417511 Accuracy 0.8095703125\n",
      "Iteration 13510 Training loss 0.02000049129128456 Validation loss 0.01891011744737625 Accuracy 0.80322265625\n",
      "Iteration 13520 Training loss 0.015680935233831406 Validation loss 0.01818283647298813 Accuracy 0.81103515625\n",
      "Iteration 13530 Training loss 0.016353605315089226 Validation loss 0.01839986816048622 Accuracy 0.8095703125\n",
      "Iteration 13540 Training loss 0.01362114679068327 Validation loss 0.018698837608098984 Accuracy 0.80615234375\n",
      "Iteration 13550 Training loss 0.012263988144695759 Validation loss 0.018440265208482742 Accuracy 0.810546875\n",
      "Iteration 13560 Training loss 0.016285637393593788 Validation loss 0.018588555976748466 Accuracy 0.8076171875\n",
      "Iteration 13570 Training loss 0.016626780852675438 Validation loss 0.018443433567881584 Accuracy 0.80859375\n",
      "Iteration 13580 Training loss 0.013534177094697952 Validation loss 0.018489720299839973 Accuracy 0.80810546875\n",
      "Iteration 13590 Training loss 0.014129010029137135 Validation loss 0.018194444477558136 Accuracy 0.81103515625\n",
      "Iteration 13600 Training loss 0.01578105427324772 Validation loss 0.01872534118592739 Accuracy 0.80517578125\n",
      "Iteration 13610 Training loss 0.016209792345762253 Validation loss 0.01867685467004776 Accuracy 0.80615234375\n",
      "Iteration 13620 Training loss 0.016713207587599754 Validation loss 0.01831366866827011 Accuracy 0.810546875\n",
      "Iteration 13630 Training loss 0.01654166541993618 Validation loss 0.018834685906767845 Accuracy 0.8037109375\n",
      "Iteration 13640 Training loss 0.01436953991651535 Validation loss 0.01858573593199253 Accuracy 0.80810546875\n",
      "Iteration 13650 Training loss 0.016490301117300987 Validation loss 0.01850230246782303 Accuracy 0.80859375\n",
      "Iteration 13660 Training loss 0.01773110032081604 Validation loss 0.019018063321709633 Accuracy 0.80322265625\n",
      "Iteration 13670 Training loss 0.016465120017528534 Validation loss 0.019403226673603058 Accuracy 0.798828125\n",
      "Iteration 13680 Training loss 0.016803346574306488 Validation loss 0.018636727705597878 Accuracy 0.8076171875\n",
      "Iteration 13690 Training loss 0.015230707824230194 Validation loss 0.018586572259664536 Accuracy 0.80810546875\n",
      "Iteration 13700 Training loss 0.01637698896229267 Validation loss 0.019733134657144547 Accuracy 0.79736328125\n",
      "Iteration 13710 Training loss 0.015869975090026855 Validation loss 0.01852625235915184 Accuracy 0.80859375\n",
      "Iteration 13720 Training loss 0.0187546256929636 Validation loss 0.01992400363087654 Accuracy 0.79443359375\n",
      "Iteration 13730 Training loss 0.017314068973064423 Validation loss 0.018756888806819916 Accuracy 0.8056640625\n",
      "Iteration 13740 Training loss 0.016823623329401016 Validation loss 0.018672989681363106 Accuracy 0.80712890625\n",
      "Iteration 13750 Training loss 0.01613643206655979 Validation loss 0.01847204752266407 Accuracy 0.8095703125\n",
      "Iteration 13760 Training loss 0.017346220090985298 Validation loss 0.018643591552972794 Accuracy 0.80712890625\n",
      "Iteration 13770 Training loss 0.015959784388542175 Validation loss 0.01880570687353611 Accuracy 0.80615234375\n",
      "Iteration 13780 Training loss 0.01779499463737011 Validation loss 0.018670879304409027 Accuracy 0.80615234375\n",
      "Iteration 13790 Training loss 0.014200927689671516 Validation loss 0.01863757334649563 Accuracy 0.806640625\n",
      "Iteration 13800 Training loss 0.015797242522239685 Validation loss 0.018935687839984894 Accuracy 0.80419921875\n",
      "Iteration 13810 Training loss 0.015002377331256866 Validation loss 0.018546951934695244 Accuracy 0.80859375\n",
      "Iteration 13820 Training loss 0.014326670207083225 Validation loss 0.01869778148829937 Accuracy 0.80615234375\n",
      "Iteration 13830 Training loss 0.016470085829496384 Validation loss 0.01842041127383709 Accuracy 0.81005859375\n",
      "Iteration 13840 Training loss 0.015101665630936623 Validation loss 0.018459906801581383 Accuracy 0.80810546875\n",
      "Iteration 13850 Training loss 0.015412088483572006 Validation loss 0.0185107234865427 Accuracy 0.80859375\n",
      "Iteration 13860 Training loss 0.016969729214906693 Validation loss 0.018521703779697418 Accuracy 0.80859375\n",
      "Iteration 13870 Training loss 0.014716732315719128 Validation loss 0.018694385886192322 Accuracy 0.80712890625\n",
      "Iteration 13880 Training loss 0.017758948728442192 Validation loss 0.018740326166152954 Accuracy 0.8056640625\n",
      "Iteration 13890 Training loss 0.015952302142977715 Validation loss 0.018529493361711502 Accuracy 0.80908203125\n",
      "Iteration 13900 Training loss 0.019297994673252106 Validation loss 0.018661413341760635 Accuracy 0.80712890625\n",
      "Iteration 13910 Training loss 0.016652705147862434 Validation loss 0.018378395587205887 Accuracy 0.80908203125\n",
      "Iteration 13920 Training loss 0.016807029023766518 Validation loss 0.01841118186712265 Accuracy 0.81005859375\n",
      "Iteration 13930 Training loss 0.01760607212781906 Validation loss 0.01839854009449482 Accuracy 0.81005859375\n",
      "Iteration 13940 Training loss 0.015362787991762161 Validation loss 0.018360160291194916 Accuracy 0.8095703125\n",
      "Iteration 13950 Training loss 0.013484136201441288 Validation loss 0.01839120127260685 Accuracy 0.8095703125\n",
      "Iteration 13960 Training loss 0.0156979039311409 Validation loss 0.018495140597224236 Accuracy 0.80810546875\n",
      "Iteration 13970 Training loss 0.016927585005760193 Validation loss 0.019358597695827484 Accuracy 0.7998046875\n",
      "Iteration 13980 Training loss 0.014789922162890434 Validation loss 0.019326385110616684 Accuracy 0.798828125\n",
      "Iteration 13990 Training loss 0.018493741750717163 Validation loss 0.019015345722436905 Accuracy 0.802734375\n",
      "Iteration 14000 Training loss 0.017146853730082512 Validation loss 0.019108377397060394 Accuracy 0.8037109375\n",
      "Iteration 14010 Training loss 0.015042525716125965 Validation loss 0.018634630367159843 Accuracy 0.80712890625\n",
      "Iteration 14020 Training loss 0.014638606458902359 Validation loss 0.018689002841711044 Accuracy 0.806640625\n",
      "Iteration 14030 Training loss 0.01599322259426117 Validation loss 0.01834707520902157 Accuracy 0.8095703125\n",
      "Iteration 14040 Training loss 0.015781601890921593 Validation loss 0.019521109759807587 Accuracy 0.79833984375\n",
      "Iteration 14050 Training loss 0.018572745844721794 Validation loss 0.01847890391945839 Accuracy 0.80859375\n",
      "Iteration 14060 Training loss 0.014468975365161896 Validation loss 0.01862333156168461 Accuracy 0.806640625\n",
      "Iteration 14070 Training loss 0.0157243050634861 Validation loss 0.01889663003385067 Accuracy 0.802734375\n",
      "Iteration 14080 Training loss 0.01439580786973238 Validation loss 0.018926825374364853 Accuracy 0.80322265625\n",
      "Iteration 14090 Training loss 0.014562695287168026 Validation loss 0.018510473892092705 Accuracy 0.8076171875\n",
      "Iteration 14100 Training loss 0.016077451407909393 Validation loss 0.018785575404763222 Accuracy 0.8046875\n",
      "Iteration 14110 Training loss 0.013928454369306564 Validation loss 0.018488457426428795 Accuracy 0.80810546875\n",
      "Iteration 14120 Training loss 0.012959431856870651 Validation loss 0.018564406782388687 Accuracy 0.8076171875\n",
      "Iteration 14130 Training loss 0.01547912321984768 Validation loss 0.01863142102956772 Accuracy 0.806640625\n",
      "Iteration 14140 Training loss 0.017700886353850365 Validation loss 0.018652236089110374 Accuracy 0.80615234375\n",
      "Iteration 14150 Training loss 0.013529034331440926 Validation loss 0.018383346498012543 Accuracy 0.80908203125\n",
      "Iteration 14160 Training loss 0.014973591081798077 Validation loss 0.018632791936397552 Accuracy 0.80712890625\n",
      "Iteration 14170 Training loss 0.012576424516737461 Validation loss 0.018486903980374336 Accuracy 0.80908203125\n",
      "Iteration 14180 Training loss 0.012893694452941418 Validation loss 0.018356474116444588 Accuracy 0.8095703125\n",
      "Iteration 14190 Training loss 0.016360612586140633 Validation loss 0.01879573054611683 Accuracy 0.8046875\n",
      "Iteration 14200 Training loss 0.014117174781858921 Validation loss 0.018005361780524254 Accuracy 0.8134765625\n",
      "Iteration 14210 Training loss 0.015215090475976467 Validation loss 0.0184367373585701 Accuracy 0.80810546875\n",
      "Iteration 14220 Training loss 0.01605016365647316 Validation loss 0.019131137058138847 Accuracy 0.8017578125\n",
      "Iteration 14230 Training loss 0.014560192823410034 Validation loss 0.01904805190861225 Accuracy 0.802734375\n",
      "Iteration 14240 Training loss 0.018755266442894936 Validation loss 0.01933952420949936 Accuracy 0.7998046875\n",
      "Iteration 14250 Training loss 0.013271176256239414 Validation loss 0.018447093665599823 Accuracy 0.80859375\n",
      "Iteration 14260 Training loss 0.014421643689274788 Validation loss 0.018482357263565063 Accuracy 0.80810546875\n",
      "Iteration 14270 Training loss 0.01721552386879921 Validation loss 0.018120737746357918 Accuracy 0.8125\n",
      "Iteration 14280 Training loss 0.01563955470919609 Validation loss 0.018666110932826996 Accuracy 0.806640625\n",
      "Iteration 14290 Training loss 0.017333708703517914 Validation loss 0.018345976248383522 Accuracy 0.8095703125\n",
      "Iteration 14300 Training loss 0.016509991139173508 Validation loss 0.018703032284975052 Accuracy 0.80615234375\n",
      "Iteration 14310 Training loss 0.016309354454278946 Validation loss 0.018739525228738785 Accuracy 0.8056640625\n",
      "Iteration 14320 Training loss 0.017002349719405174 Validation loss 0.018160507082939148 Accuracy 0.8125\n",
      "Iteration 14330 Training loss 0.016066523268818855 Validation loss 0.018622487783432007 Accuracy 0.80712890625\n",
      "Iteration 14340 Training loss 0.014841010794043541 Validation loss 0.018285058438777924 Accuracy 0.81103515625\n",
      "Iteration 14350 Training loss 0.01595268025994301 Validation loss 0.018444128334522247 Accuracy 0.80810546875\n",
      "Iteration 14360 Training loss 0.014564508572220802 Validation loss 0.01863083243370056 Accuracy 0.806640625\n",
      "Iteration 14370 Training loss 0.015435920096933842 Validation loss 0.018297895789146423 Accuracy 0.81005859375\n",
      "Iteration 14380 Training loss 0.016655806452035904 Validation loss 0.01811337284743786 Accuracy 0.8115234375\n",
      "Iteration 14390 Training loss 0.014962023124098778 Validation loss 0.018060065805912018 Accuracy 0.8125\n",
      "Iteration 14400 Training loss 0.015105511993169785 Validation loss 0.01826884225010872 Accuracy 0.810546875\n",
      "Iteration 14410 Training loss 0.012575769796967506 Validation loss 0.019060563296079636 Accuracy 0.8017578125\n",
      "Iteration 14420 Training loss 0.014781690202653408 Validation loss 0.018429776653647423 Accuracy 0.81005859375\n",
      "Iteration 14430 Training loss 0.01567974127829075 Validation loss 0.0185557771474123 Accuracy 0.80810546875\n",
      "Iteration 14440 Training loss 0.016713600605726242 Validation loss 0.01868453249335289 Accuracy 0.80712890625\n",
      "Iteration 14450 Training loss 0.018167296424508095 Validation loss 0.018253348767757416 Accuracy 0.81005859375\n",
      "Iteration 14460 Training loss 0.014897636137902737 Validation loss 0.018277497962117195 Accuracy 0.80908203125\n",
      "Iteration 14470 Training loss 0.014433883130550385 Validation loss 0.01860233023762703 Accuracy 0.80712890625\n",
      "Iteration 14480 Training loss 0.016116760671138763 Validation loss 0.01845964603126049 Accuracy 0.80859375\n",
      "Iteration 14490 Training loss 0.018249046057462692 Validation loss 0.01829397864639759 Accuracy 0.81005859375\n",
      "Iteration 14500 Training loss 0.01699681766331196 Validation loss 0.018303420394659042 Accuracy 0.810546875\n",
      "Iteration 14510 Training loss 0.013047505170106888 Validation loss 0.018434759229421616 Accuracy 0.80908203125\n",
      "Iteration 14520 Training loss 0.015212373808026314 Validation loss 0.01861557550728321 Accuracy 0.80712890625\n",
      "Iteration 14530 Training loss 0.01488039642572403 Validation loss 0.01829581893980503 Accuracy 0.81005859375\n",
      "Iteration 14540 Training loss 0.018544290214776993 Validation loss 0.019039997830986977 Accuracy 0.802734375\n",
      "Iteration 14550 Training loss 0.016803249716758728 Validation loss 0.01832122541964054 Accuracy 0.81005859375\n",
      "Iteration 14560 Training loss 0.015242879278957844 Validation loss 0.018409280106425285 Accuracy 0.80859375\n",
      "Iteration 14570 Training loss 0.014840856194496155 Validation loss 0.01831831783056259 Accuracy 0.8095703125\n",
      "Iteration 14580 Training loss 0.016471482813358307 Validation loss 0.018808653578162193 Accuracy 0.80419921875\n",
      "Iteration 14590 Training loss 0.014268750324845314 Validation loss 0.01819480024278164 Accuracy 0.81103515625\n",
      "Iteration 14600 Training loss 0.016110597178339958 Validation loss 0.01844807155430317 Accuracy 0.8076171875\n",
      "Iteration 14610 Training loss 0.018064089119434357 Validation loss 0.020094096660614014 Accuracy 0.79150390625\n",
      "Iteration 14620 Training loss 0.01646418683230877 Validation loss 0.018464788794517517 Accuracy 0.80859375\n",
      "Iteration 14630 Training loss 0.01569165103137493 Validation loss 0.01802739128470421 Accuracy 0.8125\n",
      "Iteration 14640 Training loss 0.010912188328802586 Validation loss 0.014726641587913036 Accuracy 0.845703125\n",
      "Iteration 14650 Training loss 0.010517633520066738 Validation loss 0.014506022445857525 Accuracy 0.84814453125\n",
      "Iteration 14660 Training loss 0.012877007946372032 Validation loss 0.015610819682478905 Accuracy 0.83740234375\n",
      "Iteration 14670 Training loss 0.009510007686913013 Validation loss 0.014740981161594391 Accuracy 0.84619140625\n",
      "Iteration 14680 Training loss 0.012846323661506176 Validation loss 0.014133657328784466 Accuracy 0.8525390625\n",
      "Iteration 14690 Training loss 0.010261187329888344 Validation loss 0.014883366413414478 Accuracy 0.8447265625\n",
      "Iteration 14700 Training loss 0.011169067583978176 Validation loss 0.014142709784209728 Accuracy 0.8525390625\n",
      "Iteration 14710 Training loss 0.008866585791110992 Validation loss 0.01371508277952671 Accuracy 0.8544921875\n",
      "Iteration 14720 Training loss 0.011911292560398579 Validation loss 0.01375108864158392 Accuracy 0.857421875\n",
      "Iteration 14730 Training loss 0.012028200551867485 Validation loss 0.013522477820515633 Accuracy 0.85888671875\n",
      "Iteration 14740 Training loss 0.010760308243334293 Validation loss 0.013510571792721748 Accuracy 0.85888671875\n",
      "Iteration 14750 Training loss 0.01401557121425867 Validation loss 0.015424384735524654 Accuracy 0.83837890625\n",
      "Iteration 14760 Training loss 0.011701817624270916 Validation loss 0.014534229412674904 Accuracy 0.84716796875\n",
      "Iteration 14770 Training loss 0.01020811963826418 Validation loss 0.014247670769691467 Accuracy 0.8515625\n",
      "Iteration 14780 Training loss 0.010956293903291225 Validation loss 0.013388622552156448 Accuracy 0.86083984375\n",
      "Iteration 14790 Training loss 0.009049015119671822 Validation loss 0.013299201615154743 Accuracy 0.861328125\n",
      "Iteration 14800 Training loss 0.009298079647123814 Validation loss 0.013126823119819164 Accuracy 0.86328125\n",
      "Iteration 14810 Training loss 0.011429807171225548 Validation loss 0.014179423451423645 Accuracy 0.8525390625\n",
      "Iteration 14820 Training loss 0.011617721058428288 Validation loss 0.013869289308786392 Accuracy 0.85498046875\n",
      "Iteration 14830 Training loss 0.011694811284542084 Validation loss 0.014235073700547218 Accuracy 0.85107421875\n",
      "Iteration 14840 Training loss 0.011726335622370243 Validation loss 0.014009950682520866 Accuracy 0.853515625\n",
      "Iteration 14850 Training loss 0.01175269391387701 Validation loss 0.013547985814511776 Accuracy 0.8583984375\n",
      "Iteration 14860 Training loss 0.009136194363236427 Validation loss 0.01296147145330906 Accuracy 0.865234375\n",
      "Iteration 14870 Training loss 0.010298590175807476 Validation loss 0.014062987640500069 Accuracy 0.8525390625\n",
      "Iteration 14880 Training loss 0.010818498209118843 Validation loss 0.013688365928828716 Accuracy 0.85693359375\n",
      "Iteration 14890 Training loss 0.01174042746424675 Validation loss 0.01372595690190792 Accuracy 0.857421875\n",
      "Iteration 14900 Training loss 0.011778337880969048 Validation loss 0.014428644441068172 Accuracy 0.849609375\n",
      "Iteration 14910 Training loss 0.010950678028166294 Validation loss 0.013868283480405807 Accuracy 0.85595703125\n",
      "Iteration 14920 Training loss 0.008451879024505615 Validation loss 0.013625108636915684 Accuracy 0.85791015625\n",
      "Iteration 14930 Training loss 0.011644861660897732 Validation loss 0.01356505136936903 Accuracy 0.859375\n",
      "Iteration 14940 Training loss 0.009832755662500858 Validation loss 0.013300632126629353 Accuracy 0.861328125\n",
      "Iteration 14950 Training loss 0.008285013027489185 Validation loss 0.013698039576411247 Accuracy 0.857421875\n",
      "Iteration 14960 Training loss 0.01066703163087368 Validation loss 0.01420533936470747 Accuracy 0.85205078125\n",
      "Iteration 14970 Training loss 0.008930251933634281 Validation loss 0.013482457958161831 Accuracy 0.85986328125\n",
      "Iteration 14980 Training loss 0.010162466205656528 Validation loss 0.013689755462110043 Accuracy 0.85791015625\n",
      "Iteration 14990 Training loss 0.007794716861099005 Validation loss 0.012977750040590763 Accuracy 0.86572265625\n",
      "Iteration 15000 Training loss 0.010708709247410297 Validation loss 0.01311374083161354 Accuracy 0.86376953125\n",
      "Iteration 15010 Training loss 0.009220589883625507 Validation loss 0.01335434801876545 Accuracy 0.86083984375\n",
      "Iteration 15020 Training loss 0.009859371930360794 Validation loss 0.013312953524291515 Accuracy 0.861328125\n",
      "Iteration 15030 Training loss 0.010782492347061634 Validation loss 0.013285180553793907 Accuracy 0.86181640625\n",
      "Iteration 15040 Training loss 0.011038286611437798 Validation loss 0.012864772230386734 Accuracy 0.8662109375\n",
      "Iteration 15050 Training loss 0.009377744980156422 Validation loss 0.012679245322942734 Accuracy 0.8681640625\n",
      "Iteration 15060 Training loss 0.007932690903544426 Validation loss 0.012795394286513329 Accuracy 0.8662109375\n",
      "Iteration 15070 Training loss 0.00888761319220066 Validation loss 0.01301631424576044 Accuracy 0.8642578125\n",
      "Iteration 15080 Training loss 0.009489121846854687 Validation loss 0.013086005114018917 Accuracy 0.8642578125\n",
      "Iteration 15090 Training loss 0.010592013597488403 Validation loss 0.012981530278921127 Accuracy 0.865234375\n",
      "Iteration 15100 Training loss 0.010883376002311707 Validation loss 0.014378041960299015 Accuracy 0.84912109375\n",
      "Iteration 15110 Training loss 0.011893551796674728 Validation loss 0.013234131969511509 Accuracy 0.86279296875\n",
      "Iteration 15120 Training loss 0.013321688398718834 Validation loss 0.017167173326015472 Accuracy 0.822265625\n",
      "Iteration 15130 Training loss 0.012225860729813576 Validation loss 0.01487198006361723 Accuracy 0.84521484375\n",
      "Iteration 15140 Training loss 0.009443646296858788 Validation loss 0.013164984993636608 Accuracy 0.86328125\n",
      "Iteration 15150 Training loss 0.01130435150116682 Validation loss 0.013126139529049397 Accuracy 0.8623046875\n",
      "Iteration 15160 Training loss 0.011256901547312737 Validation loss 0.013051887974143028 Accuracy 0.86376953125\n",
      "Iteration 15170 Training loss 0.011159513145685196 Validation loss 0.014256244525313377 Accuracy 0.8525390625\n",
      "Iteration 15180 Training loss 0.013602552935481071 Validation loss 0.01575990580022335 Accuracy 0.83544921875\n",
      "Iteration 15190 Training loss 0.009647564962506294 Validation loss 0.012965505011379719 Accuracy 0.8642578125\n",
      "Iteration 15200 Training loss 0.009072056040167809 Validation loss 0.01329717505723238 Accuracy 0.86083984375\n",
      "Iteration 15210 Training loss 0.008868159726262093 Validation loss 0.013511992059648037 Accuracy 0.85791015625\n",
      "Iteration 15220 Training loss 0.013794147409498692 Validation loss 0.015714704990386963 Accuracy 0.8369140625\n",
      "Iteration 15230 Training loss 0.009213426150381565 Validation loss 0.013171089813113213 Accuracy 0.86181640625\n",
      "Iteration 15240 Training loss 0.009686785750091076 Validation loss 0.012925783172249794 Accuracy 0.86474609375\n",
      "Iteration 15250 Training loss 0.009710525162518024 Validation loss 0.012887179851531982 Accuracy 0.865234375\n",
      "Iteration 15260 Training loss 0.012509406544268131 Validation loss 0.014966132119297981 Accuracy 0.84326171875\n",
      "Iteration 15270 Training loss 0.009718041867017746 Validation loss 0.013415313325822353 Accuracy 0.85986328125\n",
      "Iteration 15280 Training loss 0.008435082621872425 Validation loss 0.01290966011583805 Accuracy 0.86572265625\n",
      "Iteration 15290 Training loss 0.009978200308978558 Validation loss 0.012890475802123547 Accuracy 0.86572265625\n",
      "Iteration 15300 Training loss 0.00804896280169487 Validation loss 0.012986703775823116 Accuracy 0.8642578125\n",
      "Iteration 15310 Training loss 0.00911190640181303 Validation loss 0.013949920423328876 Accuracy 0.85400390625\n",
      "Iteration 15320 Training loss 0.01074172742664814 Validation loss 0.013224294409155846 Accuracy 0.861328125\n",
      "Iteration 15330 Training loss 0.009128909558057785 Validation loss 0.013699489645659924 Accuracy 0.857421875\n",
      "Iteration 15340 Training loss 0.007241987157613039 Validation loss 0.013157021254301071 Accuracy 0.8623046875\n",
      "Iteration 15350 Training loss 0.009636270813643932 Validation loss 0.012730513699352741 Accuracy 0.8671875\n",
      "Iteration 15360 Training loss 0.00842283759266138 Validation loss 0.012739906087517738 Accuracy 0.8662109375\n",
      "Iteration 15370 Training loss 0.009574447758495808 Validation loss 0.012592155486345291 Accuracy 0.8681640625\n",
      "Iteration 15380 Training loss 0.010986607521772385 Validation loss 0.01345137134194374 Accuracy 0.85986328125\n",
      "Iteration 15390 Training loss 0.008344556204974651 Validation loss 0.013171061873435974 Accuracy 0.86279296875\n",
      "Iteration 15400 Training loss 0.010377734899520874 Validation loss 0.013465077616274357 Accuracy 0.85986328125\n",
      "Iteration 15410 Training loss 0.009664642624557018 Validation loss 0.013357043266296387 Accuracy 0.85986328125\n",
      "Iteration 15420 Training loss 0.009547661989927292 Validation loss 0.012660844251513481 Accuracy 0.8681640625\n",
      "Iteration 15430 Training loss 0.010995735414326191 Validation loss 0.012876858003437519 Accuracy 0.865234375\n",
      "Iteration 15440 Training loss 0.011383275501430035 Validation loss 0.013673617504537106 Accuracy 0.8564453125\n",
      "Iteration 15450 Training loss 0.010420218110084534 Validation loss 0.01327365543693304 Accuracy 0.8623046875\n",
      "Iteration 15460 Training loss 0.010574573650956154 Validation loss 0.01309913583099842 Accuracy 0.86328125\n",
      "Iteration 15470 Training loss 0.007695936597883701 Validation loss 0.012806104496121407 Accuracy 0.865234375\n",
      "Iteration 15480 Training loss 0.009850955568253994 Validation loss 0.012858648784458637 Accuracy 0.865234375\n",
      "Iteration 15490 Training loss 0.007194054313004017 Validation loss 0.012488010339438915 Accuracy 0.86962890625\n",
      "Iteration 15500 Training loss 0.007604797836393118 Validation loss 0.01256110891699791 Accuracy 0.8681640625\n",
      "Iteration 15510 Training loss 0.008996589109301567 Validation loss 0.012991823256015778 Accuracy 0.8642578125\n",
      "Iteration 15520 Training loss 0.009684378281235695 Validation loss 0.013310873880982399 Accuracy 0.8603515625\n",
      "Iteration 15530 Training loss 0.00918505247682333 Validation loss 0.013212003745138645 Accuracy 0.861328125\n",
      "Iteration 15540 Training loss 0.008557593449950218 Validation loss 0.013133120723068714 Accuracy 0.86328125\n",
      "Iteration 15550 Training loss 0.009751803241670132 Validation loss 0.012926936149597168 Accuracy 0.86572265625\n",
      "Iteration 15560 Training loss 0.00824015587568283 Validation loss 0.012577560730278492 Accuracy 0.869140625\n",
      "Iteration 15570 Training loss 0.00950371753424406 Validation loss 0.014023425988852978 Accuracy 0.8544921875\n",
      "Iteration 15580 Training loss 0.008735926821827888 Validation loss 0.012834088876843452 Accuracy 0.8662109375\n",
      "Iteration 15590 Training loss 0.009622820653021336 Validation loss 0.012656408362090588 Accuracy 0.8681640625\n",
      "Iteration 15600 Training loss 0.012811682187020779 Validation loss 0.015666987746953964 Accuracy 0.83642578125\n",
      "Iteration 15610 Training loss 0.009475312195718288 Validation loss 0.013207084499299526 Accuracy 0.86181640625\n",
      "Iteration 15620 Training loss 0.008423499763011932 Validation loss 0.013004770502448082 Accuracy 0.86474609375\n",
      "Iteration 15630 Training loss 0.00759802246466279 Validation loss 0.01283557340502739 Accuracy 0.86572265625\n",
      "Iteration 15640 Training loss 0.008718674071133137 Validation loss 0.013191452249884605 Accuracy 0.86279296875\n",
      "Iteration 15650 Training loss 0.00991471204906702 Validation loss 0.013715161010622978 Accuracy 0.8564453125\n",
      "Iteration 15660 Training loss 0.008391711860895157 Validation loss 0.013452745974063873 Accuracy 0.85986328125\n",
      "Iteration 15670 Training loss 0.008755099959671497 Validation loss 0.012810208834707737 Accuracy 0.8662109375\n",
      "Iteration 15680 Training loss 0.010849326848983765 Validation loss 0.01273118332028389 Accuracy 0.8671875\n",
      "Iteration 15690 Training loss 0.010114212520420551 Validation loss 0.01285307202488184 Accuracy 0.865234375\n",
      "Iteration 15700 Training loss 0.009661003015935421 Validation loss 0.013028646819293499 Accuracy 0.8623046875\n",
      "Iteration 15710 Training loss 0.008906549774110317 Validation loss 0.012913540005683899 Accuracy 0.8642578125\n",
      "Iteration 15720 Training loss 0.012105347588658333 Validation loss 0.01529866922646761 Accuracy 0.83984375\n",
      "Iteration 15730 Training loss 0.00880027562379837 Validation loss 0.012308643199503422 Accuracy 0.87158203125\n",
      "Iteration 15740 Training loss 0.008352442644536495 Validation loss 0.012976394034922123 Accuracy 0.86376953125\n",
      "Iteration 15750 Training loss 0.011778129264712334 Validation loss 0.013670983724296093 Accuracy 0.85693359375\n",
      "Iteration 15760 Training loss 0.00729397451505065 Validation loss 0.012551247142255306 Accuracy 0.86865234375\n",
      "Iteration 15770 Training loss 0.006911237724125385 Validation loss 0.01280741672962904 Accuracy 0.8662109375\n",
      "Iteration 15780 Training loss 0.008304670453071594 Validation loss 0.012840420007705688 Accuracy 0.86572265625\n",
      "Iteration 15790 Training loss 0.006739966105669737 Validation loss 0.013017436489462852 Accuracy 0.8642578125\n",
      "Iteration 15800 Training loss 0.008376228623092175 Validation loss 0.012818015180528164 Accuracy 0.86669921875\n",
      "Iteration 15810 Training loss 0.007938949391245842 Validation loss 0.012120477855205536 Accuracy 0.87353515625\n",
      "Iteration 15820 Training loss 0.008257615379989147 Validation loss 0.01285752933472395 Accuracy 0.865234375\n",
      "Iteration 15830 Training loss 0.008238697424530983 Validation loss 0.01223689690232277 Accuracy 0.8720703125\n",
      "Iteration 15840 Training loss 0.0075705512426793575 Validation loss 0.01340350229293108 Accuracy 0.859375\n",
      "Iteration 15850 Training loss 0.01029575802385807 Validation loss 0.013268161565065384 Accuracy 0.861328125\n",
      "Iteration 15860 Training loss 0.00966846477240324 Validation loss 0.012162513099610806 Accuracy 0.87353515625\n",
      "Iteration 15870 Training loss 0.009936577640473843 Validation loss 0.012546162120997906 Accuracy 0.8681640625\n",
      "Iteration 15880 Training loss 0.007531074341386557 Validation loss 0.012554384768009186 Accuracy 0.8681640625\n",
      "Iteration 15890 Training loss 0.011631464585661888 Validation loss 0.013005864806473255 Accuracy 0.86474609375\n",
      "Iteration 15900 Training loss 0.010156561620533466 Validation loss 0.012288479134440422 Accuracy 0.87109375\n",
      "Iteration 15910 Training loss 0.008197205141186714 Validation loss 0.012582577764987946 Accuracy 0.8681640625\n",
      "Iteration 15920 Training loss 0.007098095957189798 Validation loss 0.012577297165989876 Accuracy 0.869140625\n",
      "Iteration 15930 Training loss 0.008987155742943287 Validation loss 0.012039633467793465 Accuracy 0.875\n",
      "Iteration 15940 Training loss 0.009244582615792751 Validation loss 0.012287970632314682 Accuracy 0.8720703125\n",
      "Iteration 15950 Training loss 0.008584403432905674 Validation loss 0.012977343052625656 Accuracy 0.86474609375\n",
      "Iteration 15960 Training loss 0.010066743940114975 Validation loss 0.012825684621930122 Accuracy 0.865234375\n",
      "Iteration 15970 Training loss 0.010216385126113892 Validation loss 0.01358543150126934 Accuracy 0.85986328125\n",
      "Iteration 15980 Training loss 0.008565082214772701 Validation loss 0.012793922796845436 Accuracy 0.86669921875\n",
      "Iteration 15990 Training loss 0.010324256494641304 Validation loss 0.012443935498595238 Accuracy 0.86962890625\n",
      "Iteration 16000 Training loss 0.008029273711144924 Validation loss 0.012547746300697327 Accuracy 0.86962890625\n",
      "Iteration 16010 Training loss 0.009192746132612228 Validation loss 0.012244799174368382 Accuracy 0.87109375\n",
      "Iteration 16020 Training loss 0.0073545933701097965 Validation loss 0.01274361927062273 Accuracy 0.86865234375\n",
      "Iteration 16030 Training loss 0.010984476655721664 Validation loss 0.013141484931111336 Accuracy 0.8623046875\n",
      "Iteration 16040 Training loss 0.006456204690039158 Validation loss 0.012257883325219154 Accuracy 0.8720703125\n",
      "Iteration 16050 Training loss 0.0092574842274189 Validation loss 0.012928777374327183 Accuracy 0.865234375\n",
      "Iteration 16060 Training loss 0.009759387001395226 Validation loss 0.013192879036068916 Accuracy 0.861328125\n",
      "Iteration 16070 Training loss 0.008007718250155449 Validation loss 0.012605089694261551 Accuracy 0.86669921875\n",
      "Iteration 16080 Training loss 0.0076903486624360085 Validation loss 0.012093329802155495 Accuracy 0.87255859375\n",
      "Iteration 16090 Training loss 0.008426693268120289 Validation loss 0.012036618776619434 Accuracy 0.8740234375\n",
      "Iteration 16100 Training loss 0.007469514850527048 Validation loss 0.012490151450037956 Accuracy 0.869140625\n",
      "Iteration 16110 Training loss 0.009312246926128864 Validation loss 0.013179682195186615 Accuracy 0.861328125\n",
      "Iteration 16120 Training loss 0.007299148943275213 Validation loss 0.013479425571858883 Accuracy 0.8583984375\n",
      "Iteration 16130 Training loss 0.011877697892487049 Validation loss 0.013123217038810253 Accuracy 0.86328125\n",
      "Iteration 16140 Training loss 0.008694848045706749 Validation loss 0.012479116208851337 Accuracy 0.8701171875\n",
      "Iteration 16150 Training loss 0.007498151157051325 Validation loss 0.012544454075396061 Accuracy 0.86865234375\n",
      "Iteration 16160 Training loss 0.0066994475200772285 Validation loss 0.012790456414222717 Accuracy 0.8662109375\n",
      "Iteration 16170 Training loss 0.005987110082060099 Validation loss 0.01246443297713995 Accuracy 0.86962890625\n",
      "Iteration 16180 Training loss 0.010190349072217941 Validation loss 0.01257797610014677 Accuracy 0.869140625\n",
      "Iteration 16190 Training loss 0.009119792841374874 Validation loss 0.012318495661020279 Accuracy 0.87109375\n",
      "Iteration 16200 Training loss 0.007119846064597368 Validation loss 0.01219567097723484 Accuracy 0.8720703125\n",
      "Iteration 16210 Training loss 0.010338831692934036 Validation loss 0.01243955735117197 Accuracy 0.8701171875\n",
      "Iteration 16220 Training loss 0.010989496484398842 Validation loss 0.01328026968985796 Accuracy 0.86083984375\n",
      "Iteration 16230 Training loss 0.00917789340019226 Validation loss 0.013183292001485825 Accuracy 0.861328125\n",
      "Iteration 16240 Training loss 0.008644004352390766 Validation loss 0.012627423740923405 Accuracy 0.86767578125\n",
      "Iteration 16250 Training loss 0.008577785454690456 Validation loss 0.012972009368240833 Accuracy 0.8642578125\n",
      "Iteration 16260 Training loss 0.009298074059188366 Validation loss 0.012552578002214432 Accuracy 0.8681640625\n",
      "Iteration 16270 Training loss 0.008389213122427464 Validation loss 0.012147168628871441 Accuracy 0.8720703125\n",
      "Iteration 16280 Training loss 0.010803950019180775 Validation loss 0.012634553015232086 Accuracy 0.8681640625\n",
      "Iteration 16290 Training loss 0.011316019110381603 Validation loss 0.012863132171332836 Accuracy 0.8662109375\n",
      "Iteration 16300 Training loss 0.007554051000624895 Validation loss 0.012290474958717823 Accuracy 0.87158203125\n",
      "Iteration 16310 Training loss 0.010149591602385044 Validation loss 0.012742002494633198 Accuracy 0.8662109375\n",
      "Iteration 16320 Training loss 0.010271706618368626 Validation loss 0.013129918836057186 Accuracy 0.86181640625\n",
      "Iteration 16330 Training loss 0.007210333365947008 Validation loss 0.01258821040391922 Accuracy 0.86865234375\n",
      "Iteration 16340 Training loss 0.007438191212713718 Validation loss 0.012342612259089947 Accuracy 0.87109375\n",
      "Iteration 16350 Training loss 0.010466252453625202 Validation loss 0.013237688690423965 Accuracy 0.86083984375\n",
      "Iteration 16360 Training loss 0.007485757581889629 Validation loss 0.012406840920448303 Accuracy 0.86962890625\n",
      "Iteration 16370 Training loss 0.00943309348076582 Validation loss 0.013300793245434761 Accuracy 0.861328125\n",
      "Iteration 16380 Training loss 0.009948907420039177 Validation loss 0.012855501845479012 Accuracy 0.86572265625\n",
      "Iteration 16390 Training loss 0.009419601410627365 Validation loss 0.012661966495215893 Accuracy 0.86767578125\n",
      "Iteration 16400 Training loss 0.008154837414622307 Validation loss 0.012276223860681057 Accuracy 0.87158203125\n",
      "Iteration 16410 Training loss 0.007297352887690067 Validation loss 0.012199175544083118 Accuracy 0.87255859375\n",
      "Iteration 16420 Training loss 0.00941611547023058 Validation loss 0.01228010468184948 Accuracy 0.87060546875\n",
      "Iteration 16430 Training loss 0.008527428843080997 Validation loss 0.012334347702562809 Accuracy 0.8701171875\n",
      "Iteration 16440 Training loss 0.010806003585457802 Validation loss 0.012852320447564125 Accuracy 0.86572265625\n",
      "Iteration 16450 Training loss 0.007931315340101719 Validation loss 0.012019484303891659 Accuracy 0.8740234375\n",
      "Iteration 16460 Training loss 0.009325521998107433 Validation loss 0.012388289906084538 Accuracy 0.87060546875\n",
      "Iteration 16470 Training loss 0.009360849857330322 Validation loss 0.012094123288989067 Accuracy 0.87353515625\n",
      "Iteration 16480 Training loss 0.00856911763548851 Validation loss 0.01256994716823101 Accuracy 0.8671875\n",
      "Iteration 16490 Training loss 0.008562438189983368 Validation loss 0.012606486678123474 Accuracy 0.86767578125\n",
      "Iteration 16500 Training loss 0.009840044192969799 Validation loss 0.0124092698097229 Accuracy 0.87060546875\n",
      "Iteration 16510 Training loss 0.011333869770169258 Validation loss 0.0125571945682168 Accuracy 0.869140625\n",
      "Iteration 16520 Training loss 0.008615516126155853 Validation loss 0.012946883216500282 Accuracy 0.86328125\n",
      "Iteration 16530 Training loss 0.00822633970528841 Validation loss 0.01325264573097229 Accuracy 0.86083984375\n",
      "Iteration 16540 Training loss 0.007586903870105743 Validation loss 0.012363173067569733 Accuracy 0.8701171875\n",
      "Iteration 16550 Training loss 0.008038287051022053 Validation loss 0.01214252132922411 Accuracy 0.87353515625\n",
      "Iteration 16560 Training loss 0.012347614392638206 Validation loss 0.014448351226747036 Accuracy 0.84912109375\n",
      "Iteration 16570 Training loss 0.010543017648160458 Validation loss 0.012897985056042671 Accuracy 0.8642578125\n",
      "Iteration 16580 Training loss 0.00900206994265318 Validation loss 0.012048550881445408 Accuracy 0.87353515625\n",
      "Iteration 16590 Training loss 0.010732121765613556 Validation loss 0.012203981168568134 Accuracy 0.8720703125\n",
      "Iteration 16600 Training loss 0.009327670559287071 Validation loss 0.01231563463807106 Accuracy 0.87158203125\n",
      "Iteration 16610 Training loss 0.008072937838733196 Validation loss 0.012235412374138832 Accuracy 0.87158203125\n",
      "Iteration 16620 Training loss 0.009663554839789867 Validation loss 0.01268539298325777 Accuracy 0.86669921875\n",
      "Iteration 16630 Training loss 0.008121966384351254 Validation loss 0.01249082013964653 Accuracy 0.86962890625\n",
      "Iteration 16640 Training loss 0.008841491304337978 Validation loss 0.012287761084735394 Accuracy 0.8720703125\n",
      "Iteration 16650 Training loss 0.008307745680212975 Validation loss 0.012113512493669987 Accuracy 0.87255859375\n",
      "Iteration 16660 Training loss 0.010361053049564362 Validation loss 0.012875927612185478 Accuracy 0.86572265625\n",
      "Iteration 16670 Training loss 0.00785602442920208 Validation loss 0.011963929980993271 Accuracy 0.87451171875\n",
      "Iteration 16680 Training loss 0.008656112477183342 Validation loss 0.012411614879965782 Accuracy 0.87109375\n",
      "Iteration 16690 Training loss 0.007889355532824993 Validation loss 0.01236882247030735 Accuracy 0.86962890625\n",
      "Iteration 16700 Training loss 0.0076001472771167755 Validation loss 0.01233151089400053 Accuracy 0.87060546875\n",
      "Iteration 16710 Training loss 0.006917861755937338 Validation loss 0.01249720435589552 Accuracy 0.869140625\n",
      "Iteration 16720 Training loss 0.007949942722916603 Validation loss 0.012258455157279968 Accuracy 0.87158203125\n",
      "Iteration 16730 Training loss 0.010192982852458954 Validation loss 0.012461886741220951 Accuracy 0.86865234375\n",
      "Iteration 16740 Training loss 0.007943804375827312 Validation loss 0.012453261762857437 Accuracy 0.87060546875\n",
      "Iteration 16750 Training loss 0.009096014313399792 Validation loss 0.01206530723720789 Accuracy 0.87353515625\n",
      "Iteration 16760 Training loss 0.009190039709210396 Validation loss 0.012366468086838722 Accuracy 0.87109375\n",
      "Iteration 16770 Training loss 0.007953677326440811 Validation loss 0.012076755054295063 Accuracy 0.87353515625\n",
      "Iteration 16780 Training loss 0.008548232726752758 Validation loss 0.013360419310629368 Accuracy 0.85986328125\n",
      "Iteration 16790 Training loss 0.006554177962243557 Validation loss 0.012160909362137318 Accuracy 0.8720703125\n",
      "Iteration 16800 Training loss 0.00893909577280283 Validation loss 0.012338008731603622 Accuracy 0.87060546875\n",
      "Iteration 16810 Training loss 0.00938087236136198 Validation loss 0.012193773873150349 Accuracy 0.873046875\n",
      "Iteration 16820 Training loss 0.009047501720488071 Validation loss 0.012031168676912785 Accuracy 0.87451171875\n",
      "Iteration 16830 Training loss 0.008762169629335403 Validation loss 0.012856453657150269 Accuracy 0.86572265625\n",
      "Iteration 16840 Training loss 0.007564541418105364 Validation loss 0.012040780857205391 Accuracy 0.8740234375\n",
      "Iteration 16850 Training loss 0.007029613945633173 Validation loss 0.01204263512045145 Accuracy 0.87353515625\n",
      "Iteration 16860 Training loss 0.007998130284249783 Validation loss 0.011911685578525066 Accuracy 0.87548828125\n",
      "Iteration 16870 Training loss 0.007255412172526121 Validation loss 0.012068038806319237 Accuracy 0.87353515625\n",
      "Iteration 16880 Training loss 0.007389638107270002 Validation loss 0.012060346081852913 Accuracy 0.87353515625\n",
      "Iteration 16890 Training loss 0.00795253086835146 Validation loss 0.012241852469742298 Accuracy 0.8720703125\n",
      "Iteration 16900 Training loss 0.007608878891915083 Validation loss 0.01309204287827015 Accuracy 0.8623046875\n",
      "Iteration 16910 Training loss 0.010184643790125847 Validation loss 0.012109627015888691 Accuracy 0.873046875\n",
      "Iteration 16920 Training loss 0.006308667361736298 Validation loss 0.012795965187251568 Accuracy 0.86669921875\n",
      "Iteration 16930 Training loss 0.008098463527858257 Validation loss 0.01228122878819704 Accuracy 0.87060546875\n",
      "Iteration 16940 Training loss 0.009699725545942783 Validation loss 0.012235491536557674 Accuracy 0.87255859375\n",
      "Iteration 16950 Training loss 0.010709172114729881 Validation loss 0.013162338174879551 Accuracy 0.8623046875\n",
      "Iteration 16960 Training loss 0.007585454266518354 Validation loss 0.011969550512731075 Accuracy 0.875\n",
      "Iteration 16970 Training loss 0.009061826393008232 Validation loss 0.012944230809807777 Accuracy 0.86328125\n",
      "Iteration 16980 Training loss 0.007026189006865025 Validation loss 0.012291367165744305 Accuracy 0.87109375\n",
      "Iteration 16990 Training loss 0.00997874978929758 Validation loss 0.012385230511426926 Accuracy 0.87060546875\n",
      "Iteration 17000 Training loss 0.010898260399699211 Validation loss 0.01292564906179905 Accuracy 0.8642578125\n",
      "Iteration 17010 Training loss 0.008877642452716827 Validation loss 0.013046948239207268 Accuracy 0.86279296875\n",
      "Iteration 17020 Training loss 0.007747103460133076 Validation loss 0.0122302807867527 Accuracy 0.8720703125\n",
      "Iteration 17030 Training loss 0.006897935178130865 Validation loss 0.012427294626832008 Accuracy 0.86962890625\n",
      "Iteration 17040 Training loss 0.007821155712008476 Validation loss 0.012399539351463318 Accuracy 0.86962890625\n",
      "Iteration 17050 Training loss 0.009056692942976952 Validation loss 0.012548825703561306 Accuracy 0.8681640625\n",
      "Iteration 17060 Training loss 0.008017509244382381 Validation loss 0.0124978581443429 Accuracy 0.869140625\n",
      "Iteration 17070 Training loss 0.007689122576266527 Validation loss 0.012217008508741856 Accuracy 0.87158203125\n",
      "Iteration 17080 Training loss 0.007785280235111713 Validation loss 0.012165150605142117 Accuracy 0.87255859375\n",
      "Iteration 17090 Training loss 0.011128829792141914 Validation loss 0.012158242054283619 Accuracy 0.87353515625\n",
      "Iteration 17100 Training loss 0.007611299864947796 Validation loss 0.012189987115561962 Accuracy 0.87255859375\n",
      "Iteration 17110 Training loss 0.009271802380681038 Validation loss 0.012231830507516861 Accuracy 0.87255859375\n",
      "Iteration 17120 Training loss 0.00744756031781435 Validation loss 0.012091065756976604 Accuracy 0.873046875\n",
      "Iteration 17130 Training loss 0.007683014031499624 Validation loss 0.012656640261411667 Accuracy 0.8671875\n",
      "Iteration 17140 Training loss 0.009637040086090565 Validation loss 0.012447553686797619 Accuracy 0.87109375\n",
      "Iteration 17150 Training loss 0.007494731340557337 Validation loss 0.012340519577264786 Accuracy 0.87158203125\n",
      "Iteration 17160 Training loss 0.007916323840618134 Validation loss 0.012227265164256096 Accuracy 0.87158203125\n",
      "Iteration 17170 Training loss 0.009616601280868053 Validation loss 0.012142272666096687 Accuracy 0.873046875\n",
      "Iteration 17180 Training loss 0.006846939213573933 Validation loss 0.012102242559194565 Accuracy 0.87353515625\n",
      "Iteration 17190 Training loss 0.009517182596027851 Validation loss 0.012894738465547562 Accuracy 0.865234375\n",
      "Iteration 17200 Training loss 0.009259947575628757 Validation loss 0.012958308681845665 Accuracy 0.865234375\n",
      "Iteration 17210 Training loss 0.009029882028698921 Validation loss 0.012522551231086254 Accuracy 0.86962890625\n",
      "Iteration 17220 Training loss 0.00862072128802538 Validation loss 0.012855482287704945 Accuracy 0.86669921875\n",
      "Iteration 17230 Training loss 0.01047752145677805 Validation loss 0.01217186264693737 Accuracy 0.8720703125\n",
      "Iteration 17240 Training loss 0.007202975917607546 Validation loss 0.01199689693748951 Accuracy 0.87451171875\n",
      "Iteration 17250 Training loss 0.008606605231761932 Validation loss 0.012060514651238918 Accuracy 0.8740234375\n",
      "Iteration 17260 Training loss 0.010324847884476185 Validation loss 0.013046498410403728 Accuracy 0.86376953125\n",
      "Iteration 17270 Training loss 0.011141130700707436 Validation loss 0.012029238976538181 Accuracy 0.8740234375\n",
      "Iteration 17280 Training loss 0.00932281743735075 Validation loss 0.012042751535773277 Accuracy 0.8740234375\n",
      "Iteration 17290 Training loss 0.008401528932154179 Validation loss 0.011979672126471996 Accuracy 0.87451171875\n",
      "Iteration 17300 Training loss 0.007632842753082514 Validation loss 0.012566948309540749 Accuracy 0.86865234375\n",
      "Iteration 17310 Training loss 0.00956827774643898 Validation loss 0.013531851582229137 Accuracy 0.85888671875\n",
      "Iteration 17320 Training loss 0.009121722541749477 Validation loss 0.012262721545994282 Accuracy 0.8720703125\n",
      "Iteration 17330 Training loss 0.008622588589787483 Validation loss 0.012070422992110252 Accuracy 0.873046875\n",
      "Iteration 17340 Training loss 0.007617984898388386 Validation loss 0.012114237993955612 Accuracy 0.8740234375\n",
      "Iteration 17350 Training loss 0.007224349305033684 Validation loss 0.011822253465652466 Accuracy 0.87548828125\n",
      "Iteration 17360 Training loss 0.00731930136680603 Validation loss 0.012745697051286697 Accuracy 0.86669921875\n",
      "Iteration 17370 Training loss 0.00950920395553112 Validation loss 0.014003981836140156 Accuracy 0.853515625\n",
      "Iteration 17380 Training loss 0.010750571265816689 Validation loss 0.012805228121578693 Accuracy 0.8662109375\n",
      "Iteration 17390 Training loss 0.008360170759260654 Validation loss 0.012629449367523193 Accuracy 0.86865234375\n",
      "Iteration 17400 Training loss 0.007206539157778025 Validation loss 0.012512258253991604 Accuracy 0.8681640625\n",
      "Iteration 17410 Training loss 0.009232074953615665 Validation loss 0.013112055137753487 Accuracy 0.8623046875\n",
      "Iteration 17420 Training loss 0.01075277291238308 Validation loss 0.01254324335604906 Accuracy 0.8681640625\n",
      "Iteration 17430 Training loss 0.00823536328971386 Validation loss 0.012066977098584175 Accuracy 0.8740234375\n",
      "Iteration 17440 Training loss 0.009529651142656803 Validation loss 0.01282102707773447 Accuracy 0.8662109375\n",
      "Iteration 17450 Training loss 0.008008904755115509 Validation loss 0.012566127814352512 Accuracy 0.86767578125\n",
      "Iteration 17460 Training loss 0.007027417421340942 Validation loss 0.012476591393351555 Accuracy 0.8701171875\n",
      "Iteration 17470 Training loss 0.007967226207256317 Validation loss 0.012097069062292576 Accuracy 0.87353515625\n",
      "Iteration 17480 Training loss 0.006999021861702204 Validation loss 0.012679965235292912 Accuracy 0.8671875\n",
      "Iteration 17490 Training loss 0.0078254584223032 Validation loss 0.012403750792145729 Accuracy 0.86962890625\n",
      "Iteration 17500 Training loss 0.00763087859377265 Validation loss 0.012381386011838913 Accuracy 0.8701171875\n",
      "Iteration 17510 Training loss 0.00835593044757843 Validation loss 0.01253406424075365 Accuracy 0.86865234375\n",
      "Iteration 17520 Training loss 0.007481062319129705 Validation loss 0.012172311544418335 Accuracy 0.87255859375\n",
      "Iteration 17530 Training loss 0.00866605993360281 Validation loss 0.012320253998041153 Accuracy 0.87060546875\n",
      "Iteration 17540 Training loss 0.008853031322360039 Validation loss 0.012385976500809193 Accuracy 0.87060546875\n",
      "Iteration 17550 Training loss 0.008403206244111061 Validation loss 0.01196147408336401 Accuracy 0.875\n",
      "Iteration 17560 Training loss 0.009548647329211235 Validation loss 0.012113328091800213 Accuracy 0.8740234375\n",
      "Iteration 17570 Training loss 0.008404407650232315 Validation loss 0.011891700327396393 Accuracy 0.875\n",
      "Iteration 17580 Training loss 0.009005511179566383 Validation loss 0.012313609011471272 Accuracy 0.87158203125\n",
      "Iteration 17590 Training loss 0.010484926402568817 Validation loss 0.012437905184924603 Accuracy 0.8701171875\n",
      "Iteration 17600 Training loss 0.007984085008502007 Validation loss 0.013076890259981155 Accuracy 0.86328125\n",
      "Iteration 17610 Training loss 0.01003616489470005 Validation loss 0.012426985427737236 Accuracy 0.87109375\n",
      "Iteration 17620 Training loss 0.00881833303719759 Validation loss 0.01194668747484684 Accuracy 0.875\n",
      "Iteration 17630 Training loss 0.008400388062000275 Validation loss 0.01217428594827652 Accuracy 0.87158203125\n",
      "Iteration 17640 Training loss 0.008267819881439209 Validation loss 0.012082831934094429 Accuracy 0.87353515625\n",
      "Iteration 17650 Training loss 0.00790893193334341 Validation loss 0.012154902331531048 Accuracy 0.873046875\n",
      "Iteration 17660 Training loss 0.007388266269117594 Validation loss 0.01220572367310524 Accuracy 0.87255859375\n",
      "Iteration 17670 Training loss 0.00929209589958191 Validation loss 0.012491585686802864 Accuracy 0.8701171875\n",
      "Iteration 17680 Training loss 0.007551889400929213 Validation loss 0.012401185929775238 Accuracy 0.87109375\n",
      "Iteration 17690 Training loss 0.008293851278722286 Validation loss 0.012086117640137672 Accuracy 0.87353515625\n",
      "Iteration 17700 Training loss 0.008255751803517342 Validation loss 0.012050453573465347 Accuracy 0.8740234375\n",
      "Iteration 17710 Training loss 0.0073249503038823605 Validation loss 0.011799799278378487 Accuracy 0.876953125\n",
      "Iteration 17720 Training loss 0.00736484257504344 Validation loss 0.012358500622212887 Accuracy 0.87060546875\n",
      "Iteration 17730 Training loss 0.010131895542144775 Validation loss 0.013097197748720646 Accuracy 0.86279296875\n",
      "Iteration 17740 Training loss 0.0075784414075315 Validation loss 0.012318598106503487 Accuracy 0.87109375\n",
      "Iteration 17750 Training loss 0.005978664383292198 Validation loss 0.011770601384341717 Accuracy 0.87646484375\n",
      "Iteration 17760 Training loss 0.007737824693322182 Validation loss 0.011909511871635914 Accuracy 0.87548828125\n",
      "Iteration 17770 Training loss 0.007146931253373623 Validation loss 0.011965488083660603 Accuracy 0.8740234375\n",
      "Iteration 17780 Training loss 0.006724728271365166 Validation loss 0.012135080061852932 Accuracy 0.87158203125\n",
      "Iteration 17790 Training loss 0.010652828961610794 Validation loss 0.012383642606437206 Accuracy 0.86962890625\n",
      "Iteration 17800 Training loss 0.007352277170866728 Validation loss 0.012026410549879074 Accuracy 0.873046875\n",
      "Iteration 17810 Training loss 0.007759288884699345 Validation loss 0.012415889650583267 Accuracy 0.869140625\n",
      "Iteration 17820 Training loss 0.0056677754037082195 Validation loss 0.01220118347555399 Accuracy 0.87255859375\n",
      "Iteration 17830 Training loss 0.009126885794103146 Validation loss 0.01237899623811245 Accuracy 0.8701171875\n",
      "Iteration 17840 Training loss 0.007870843634009361 Validation loss 0.012250326573848724 Accuracy 0.87158203125\n",
      "Iteration 17850 Training loss 0.008034514263272285 Validation loss 0.012483011931180954 Accuracy 0.869140625\n",
      "Iteration 17860 Training loss 0.007146541494876146 Validation loss 0.012022458016872406 Accuracy 0.87451171875\n",
      "Iteration 17870 Training loss 0.009393214248120785 Validation loss 0.012034470215439796 Accuracy 0.8740234375\n",
      "Iteration 17880 Training loss 0.007535846438258886 Validation loss 0.012049969285726547 Accuracy 0.8740234375\n",
      "Iteration 17890 Training loss 0.007788159418851137 Validation loss 0.012374097481369972 Accuracy 0.87109375\n",
      "Iteration 17900 Training loss 0.006119538564234972 Validation loss 0.012482991442084312 Accuracy 0.86962890625\n",
      "Iteration 17910 Training loss 0.008001819252967834 Validation loss 0.012652176432311535 Accuracy 0.8681640625\n",
      "Iteration 17920 Training loss 0.009259996004402637 Validation loss 0.011853749863803387 Accuracy 0.87646484375\n",
      "Iteration 17930 Training loss 0.010046213865280151 Validation loss 0.011818856000900269 Accuracy 0.8759765625\n",
      "Iteration 17940 Training loss 0.006777065806090832 Validation loss 0.012953581288456917 Accuracy 0.865234375\n",
      "Iteration 17950 Training loss 0.006108947563916445 Validation loss 0.012441486120223999 Accuracy 0.8701171875\n",
      "Iteration 17960 Training loss 0.008185870945453644 Validation loss 0.013031239621341228 Accuracy 0.86328125\n",
      "Iteration 17970 Training loss 0.008081172592937946 Validation loss 0.0121424850076437 Accuracy 0.8720703125\n",
      "Iteration 17980 Training loss 0.008401292376220226 Validation loss 0.012194096110761166 Accuracy 0.8720703125\n",
      "Iteration 17990 Training loss 0.008295070379972458 Validation loss 0.011837858706712723 Accuracy 0.87548828125\n",
      "Iteration 18000 Training loss 0.005906211212277412 Validation loss 0.012022646144032478 Accuracy 0.8740234375\n",
      "Iteration 18010 Training loss 0.007015631999820471 Validation loss 0.011866770684719086 Accuracy 0.8759765625\n",
      "Iteration 18020 Training loss 0.009427153505384922 Validation loss 0.011805597692728043 Accuracy 0.87548828125\n",
      "Iteration 18030 Training loss 0.007418048568069935 Validation loss 0.011900250799953938 Accuracy 0.8759765625\n",
      "Iteration 18040 Training loss 0.006197401788085699 Validation loss 0.012838916853070259 Accuracy 0.86474609375\n",
      "Iteration 18050 Training loss 0.008540241979062557 Validation loss 0.01176405232399702 Accuracy 0.8759765625\n",
      "Iteration 18060 Training loss 0.008283022791147232 Validation loss 0.011763494461774826 Accuracy 0.87646484375\n",
      "Iteration 18070 Training loss 0.007220031227916479 Validation loss 0.011692040599882603 Accuracy 0.8759765625\n",
      "Iteration 18080 Training loss 0.007091033738106489 Validation loss 0.012312542647123337 Accuracy 0.87158203125\n",
      "Iteration 18090 Training loss 0.007665902841836214 Validation loss 0.011848466470837593 Accuracy 0.875\n",
      "Iteration 18100 Training loss 0.0082173440605402 Validation loss 0.012023900635540485 Accuracy 0.87353515625\n",
      "Iteration 18110 Training loss 0.008027750998735428 Validation loss 0.012495091184973717 Accuracy 0.869140625\n",
      "Iteration 18120 Training loss 0.008181721903383732 Validation loss 0.012426537461578846 Accuracy 0.86962890625\n",
      "Iteration 18130 Training loss 0.008990374393761158 Validation loss 0.012272033840417862 Accuracy 0.87109375\n",
      "Iteration 18140 Training loss 0.007747245486825705 Validation loss 0.011739542707800865 Accuracy 0.87646484375\n",
      "Iteration 18150 Training loss 0.0079546719789505 Validation loss 0.0117444833740592 Accuracy 0.87646484375\n",
      "Iteration 18160 Training loss 0.006859526038169861 Validation loss 0.012401437386870384 Accuracy 0.8701171875\n",
      "Iteration 18170 Training loss 0.0070600975304841995 Validation loss 0.011886547319591045 Accuracy 0.87548828125\n",
      "Iteration 18180 Training loss 0.007074906025081873 Validation loss 0.012003026902675629 Accuracy 0.87353515625\n",
      "Iteration 18190 Training loss 0.007680376525968313 Validation loss 0.011860900558531284 Accuracy 0.87548828125\n",
      "Iteration 18200 Training loss 0.007672120351344347 Validation loss 0.011768299154937267 Accuracy 0.876953125\n",
      "Iteration 18210 Training loss 0.0066177258267998695 Validation loss 0.01173307839781046 Accuracy 0.87646484375\n",
      "Iteration 18220 Training loss 0.007035407237708569 Validation loss 0.012111213058233261 Accuracy 0.8720703125\n",
      "Iteration 18230 Training loss 0.008169718086719513 Validation loss 0.011752692982554436 Accuracy 0.8759765625\n",
      "Iteration 18240 Training loss 0.0078125 Validation loss 0.011797379702329636 Accuracy 0.8759765625\n",
      "Iteration 18250 Training loss 0.0068857562728226185 Validation loss 0.01194449421018362 Accuracy 0.8740234375\n",
      "Iteration 18260 Training loss 0.006885294336825609 Validation loss 0.012602180242538452 Accuracy 0.8671875\n",
      "Iteration 18270 Training loss 0.00910615362226963 Validation loss 0.013319894671440125 Accuracy 0.86083984375\n",
      "Iteration 18280 Training loss 0.008034538477659225 Validation loss 0.012135075405240059 Accuracy 0.873046875\n",
      "Iteration 18290 Training loss 0.006801027338951826 Validation loss 0.011752571910619736 Accuracy 0.87646484375\n",
      "Iteration 18300 Training loss 0.009568736888468266 Validation loss 0.011796632781624794 Accuracy 0.876953125\n",
      "Iteration 18310 Training loss 0.005910410080105066 Validation loss 0.011609064415097237 Accuracy 0.87841796875\n",
      "Iteration 18320 Training loss 0.006909174378961325 Validation loss 0.012862292118370533 Accuracy 0.8662109375\n",
      "Iteration 18330 Training loss 0.0066212196834385395 Validation loss 0.011631371453404427 Accuracy 0.87890625\n",
      "Iteration 18340 Training loss 0.008220324292778969 Validation loss 0.011935580521821976 Accuracy 0.87451171875\n",
      "Iteration 18350 Training loss 0.006179032381623983 Validation loss 0.011674482375383377 Accuracy 0.87744140625\n",
      "Iteration 18360 Training loss 0.008706141263246536 Validation loss 0.011878417804837227 Accuracy 0.875\n",
      "Iteration 18370 Training loss 0.009014464914798737 Validation loss 0.01196647435426712 Accuracy 0.8740234375\n",
      "Iteration 18380 Training loss 0.008069084025919437 Validation loss 0.01205923780798912 Accuracy 0.87255859375\n",
      "Iteration 18390 Training loss 0.008259657770395279 Validation loss 0.011774053797125816 Accuracy 0.87646484375\n",
      "Iteration 18400 Training loss 0.006334988866001368 Validation loss 0.0117386095225811 Accuracy 0.8759765625\n",
      "Iteration 18410 Training loss 0.010531652718782425 Validation loss 0.012075529433786869 Accuracy 0.87255859375\n",
      "Iteration 18420 Training loss 0.00827227532863617 Validation loss 0.011882644146680832 Accuracy 0.8759765625\n",
      "Iteration 18430 Training loss 0.007283117156475782 Validation loss 0.011871681548655033 Accuracy 0.875\n",
      "Iteration 18440 Training loss 0.009508652612566948 Validation loss 0.012298507615923882 Accuracy 0.8701171875\n",
      "Iteration 18450 Training loss 0.007674511522054672 Validation loss 0.01169058121740818 Accuracy 0.87744140625\n",
      "Iteration 18460 Training loss 0.008678282611072063 Validation loss 0.012228273786604404 Accuracy 0.87158203125\n",
      "Iteration 18470 Training loss 0.007969620637595654 Validation loss 0.0117736104875803 Accuracy 0.876953125\n",
      "Iteration 18480 Training loss 0.0069429995492100716 Validation loss 0.012099777348339558 Accuracy 0.8740234375\n",
      "Iteration 18490 Training loss 0.00835790578275919 Validation loss 0.011981461197137833 Accuracy 0.87451171875\n",
      "Iteration 18500 Training loss 0.008073294535279274 Validation loss 0.012096303515136242 Accuracy 0.873046875\n",
      "Iteration 18510 Training loss 0.007405128329992294 Validation loss 0.011914951726794243 Accuracy 0.875\n",
      "Iteration 18520 Training loss 0.0074620600789785385 Validation loss 0.011729689314961433 Accuracy 0.8779296875\n",
      "Iteration 18530 Training loss 0.008675595745444298 Validation loss 0.01252862997353077 Accuracy 0.86962890625\n",
      "Iteration 18540 Training loss 0.0075951386243104935 Validation loss 0.011644147336483002 Accuracy 0.876953125\n",
      "Iteration 18550 Training loss 0.008378718979656696 Validation loss 0.011945459060370922 Accuracy 0.87451171875\n",
      "Iteration 18560 Training loss 0.0064194099977612495 Validation loss 0.012004769407212734 Accuracy 0.87353515625\n",
      "Iteration 18570 Training loss 0.008063341490924358 Validation loss 0.011722290888428688 Accuracy 0.87646484375\n",
      "Iteration 18580 Training loss 0.006853972561657429 Validation loss 0.011879938654601574 Accuracy 0.87548828125\n",
      "Iteration 18590 Training loss 0.00751544488593936 Validation loss 0.012217424809932709 Accuracy 0.8720703125\n",
      "Iteration 18600 Training loss 0.0068860347382724285 Validation loss 0.012116232886910439 Accuracy 0.87353515625\n",
      "Iteration 18610 Training loss 0.00802088063210249 Validation loss 0.012084435671567917 Accuracy 0.873046875\n",
      "Iteration 18620 Training loss 0.009090578183531761 Validation loss 0.012310079298913479 Accuracy 0.87109375\n",
      "Iteration 18630 Training loss 0.007941159419715405 Validation loss 0.011930477805435658 Accuracy 0.8740234375\n",
      "Iteration 18640 Training loss 0.007355549372732639 Validation loss 0.011724605225026608 Accuracy 0.876953125\n",
      "Iteration 18650 Training loss 0.006484985817223787 Validation loss 0.011800155974924564 Accuracy 0.8759765625\n",
      "Iteration 18660 Training loss 0.009784079156816006 Validation loss 0.0123407281935215 Accuracy 0.87060546875\n",
      "Iteration 18670 Training loss 0.007342070806771517 Validation loss 0.011765590868890285 Accuracy 0.87646484375\n",
      "Iteration 18680 Training loss 0.00796285830438137 Validation loss 0.012524472549557686 Accuracy 0.86865234375\n",
      "Iteration 18690 Training loss 0.010457471013069153 Validation loss 0.012294970452785492 Accuracy 0.87158203125\n",
      "Iteration 18700 Training loss 0.008706298656761646 Validation loss 0.012815441936254501 Accuracy 0.8662109375\n",
      "Iteration 18710 Training loss 0.00903693400323391 Validation loss 0.012075487524271011 Accuracy 0.873046875\n",
      "Iteration 18720 Training loss 0.0068754879757761955 Validation loss 0.011754550039768219 Accuracy 0.8759765625\n",
      "Iteration 18730 Training loss 0.00871837418526411 Validation loss 0.012804626487195492 Accuracy 0.86572265625\n",
      "Iteration 18740 Training loss 0.007221119944006205 Validation loss 0.011492013931274414 Accuracy 0.87890625\n",
      "Iteration 18750 Training loss 0.007065075449645519 Validation loss 0.011574339121580124 Accuracy 0.8779296875\n",
      "Iteration 18760 Training loss 0.008453615941107273 Validation loss 0.011963846161961555 Accuracy 0.87451171875\n",
      "Iteration 18770 Training loss 0.0070107802748680115 Validation loss 0.012177504599094391 Accuracy 0.8720703125\n",
      "Iteration 18780 Training loss 0.006266261916607618 Validation loss 0.012043551541864872 Accuracy 0.87353515625\n",
      "Iteration 18790 Training loss 0.007479474414139986 Validation loss 0.011577568016946316 Accuracy 0.8779296875\n",
      "Iteration 18800 Training loss 0.007389547303318977 Validation loss 0.011568473652005196 Accuracy 0.87890625\n",
      "Iteration 18810 Training loss 0.007608184590935707 Validation loss 0.011806382797658443 Accuracy 0.87646484375\n",
      "Iteration 18820 Training loss 0.009100827388465405 Validation loss 0.011893603019416332 Accuracy 0.8740234375\n",
      "Iteration 18830 Training loss 0.007191701792180538 Validation loss 0.011668730527162552 Accuracy 0.87744140625\n",
      "Iteration 18840 Training loss 0.006246963515877724 Validation loss 0.011748671531677246 Accuracy 0.87744140625\n",
      "Iteration 18850 Training loss 0.0066574434749782085 Validation loss 0.012033598497509956 Accuracy 0.87353515625\n",
      "Iteration 18860 Training loss 0.005308076273649931 Validation loss 0.012037908658385277 Accuracy 0.87353515625\n",
      "Iteration 18870 Training loss 0.00841104332357645 Validation loss 0.01158563420176506 Accuracy 0.87841796875\n",
      "Iteration 18880 Training loss 0.006703255698084831 Validation loss 0.011906718835234642 Accuracy 0.87451171875\n",
      "Iteration 18890 Training loss 0.0066161383874714375 Validation loss 0.012018579989671707 Accuracy 0.87353515625\n",
      "Iteration 18900 Training loss 0.008351899683475494 Validation loss 0.011839966289699078 Accuracy 0.875\n",
      "Iteration 18910 Training loss 0.009633899666368961 Validation loss 0.012416188605129719 Accuracy 0.86865234375\n",
      "Iteration 18920 Training loss 0.004647617693990469 Validation loss 0.012132811360061169 Accuracy 0.873046875\n",
      "Iteration 18930 Training loss 0.007018357049673796 Validation loss 0.011780700646340847 Accuracy 0.8759765625\n",
      "Iteration 18940 Training loss 0.00656375614926219 Validation loss 0.011882215738296509 Accuracy 0.87548828125\n",
      "Iteration 18950 Training loss 0.008174218237400055 Validation loss 0.012903050519526005 Accuracy 0.86474609375\n",
      "Iteration 18960 Training loss 0.0077726515009999275 Validation loss 0.012487977743148804 Accuracy 0.8681640625\n",
      "Iteration 18970 Training loss 0.006332061719149351 Validation loss 0.011886530555784702 Accuracy 0.87451171875\n",
      "Iteration 18980 Training loss 0.0070452382788062096 Validation loss 0.011939474381506443 Accuracy 0.87451171875\n",
      "Iteration 18990 Training loss 0.005691764876246452 Validation loss 0.011927219107747078 Accuracy 0.87451171875\n",
      "Iteration 19000 Training loss 0.00784788466989994 Validation loss 0.012420278042554855 Accuracy 0.8701171875\n",
      "Iteration 19010 Training loss 0.007722118403762579 Validation loss 0.01212526299059391 Accuracy 0.87255859375\n",
      "Iteration 19020 Training loss 0.007969746366143227 Validation loss 0.011951153166592121 Accuracy 0.875\n",
      "Iteration 19030 Training loss 0.006764683406800032 Validation loss 0.011867714114487171 Accuracy 0.875\n",
      "Iteration 19040 Training loss 0.007536106742918491 Validation loss 0.01234267558902502 Accuracy 0.869140625\n",
      "Iteration 19050 Training loss 0.008762981742620468 Validation loss 0.01217910461127758 Accuracy 0.8720703125\n",
      "Iteration 19060 Training loss 0.009591375477612019 Validation loss 0.012453930452466011 Accuracy 0.86865234375\n",
      "Iteration 19070 Training loss 0.009099331684410572 Validation loss 0.012080961838364601 Accuracy 0.873046875\n",
      "Iteration 19080 Training loss 0.008079187013208866 Validation loss 0.012567245401442051 Accuracy 0.86865234375\n",
      "Iteration 19090 Training loss 0.007200323045253754 Validation loss 0.011798180639743805 Accuracy 0.87646484375\n",
      "Iteration 19100 Training loss 0.006351714953780174 Validation loss 0.011950796470046043 Accuracy 0.8740234375\n",
      "Iteration 19110 Training loss 0.006351579446345568 Validation loss 0.012014943175017834 Accuracy 0.8740234375\n",
      "Iteration 19120 Training loss 0.008815468288958073 Validation loss 0.012005140073597431 Accuracy 0.87353515625\n",
      "Iteration 19130 Training loss 0.00851703155785799 Validation loss 0.011773434467613697 Accuracy 0.876953125\n",
      "Iteration 19140 Training loss 0.008383977226912975 Validation loss 0.011916120536625385 Accuracy 0.875\n",
      "Iteration 19150 Training loss 0.007401794195175171 Validation loss 0.011883101426064968 Accuracy 0.87548828125\n",
      "Iteration 19160 Training loss 0.007772975135594606 Validation loss 0.011843200773000717 Accuracy 0.875\n",
      "Iteration 19170 Training loss 0.009675289504230022 Validation loss 0.01203538291156292 Accuracy 0.87353515625\n",
      "Iteration 19180 Training loss 0.008984844200313091 Validation loss 0.01222524419426918 Accuracy 0.8720703125\n",
      "Iteration 19190 Training loss 0.007823798805475235 Validation loss 0.012048301286995411 Accuracy 0.8720703125\n",
      "Iteration 19200 Training loss 0.007891477085649967 Validation loss 0.012080871500074863 Accuracy 0.8720703125\n",
      "Iteration 19210 Training loss 0.005092563573271036 Validation loss 0.011401988565921783 Accuracy 0.8798828125\n",
      "Iteration 19220 Training loss 0.007646540179848671 Validation loss 0.011744786985218525 Accuracy 0.87646484375\n",
      "Iteration 19230 Training loss 0.008512710221111774 Validation loss 0.011913646943867207 Accuracy 0.875\n",
      "Iteration 19240 Training loss 0.007695774082094431 Validation loss 0.011683649383485317 Accuracy 0.8779296875\n",
      "Iteration 19250 Training loss 0.006910438649356365 Validation loss 0.011788374744355679 Accuracy 0.87646484375\n",
      "Iteration 19260 Training loss 0.007820641621947289 Validation loss 0.012193683534860611 Accuracy 0.8720703125\n",
      "Iteration 19270 Training loss 0.007775815203785896 Validation loss 0.012103567831218243 Accuracy 0.87255859375\n",
      "Iteration 19280 Training loss 0.008685164153575897 Validation loss 0.012096766382455826 Accuracy 0.87353515625\n",
      "Iteration 19290 Training loss 0.006349537987262011 Validation loss 0.012505356222391129 Accuracy 0.8681640625\n",
      "Iteration 19300 Training loss 0.004512937273830175 Validation loss 0.012296756729483604 Accuracy 0.87109375\n",
      "Iteration 19310 Training loss 0.006832540035247803 Validation loss 0.01154386904090643 Accuracy 0.87841796875\n",
      "Iteration 19320 Training loss 0.006574404425919056 Validation loss 0.012075236067175865 Accuracy 0.8720703125\n",
      "Iteration 19330 Training loss 0.007648416329175234 Validation loss 0.011720250360667706 Accuracy 0.87646484375\n",
      "Iteration 19340 Training loss 0.006021862383931875 Validation loss 0.011638158932328224 Accuracy 0.8779296875\n",
      "Iteration 19350 Training loss 0.008048615418374538 Validation loss 0.011713996529579163 Accuracy 0.87646484375\n",
      "Iteration 19360 Training loss 0.00750813540071249 Validation loss 0.011676506139338017 Accuracy 0.87646484375\n",
      "Iteration 19370 Training loss 0.005637718830257654 Validation loss 0.011948478408157825 Accuracy 0.8740234375\n",
      "Iteration 19380 Training loss 0.009861159138381481 Validation loss 0.012294615618884563 Accuracy 0.87060546875\n",
      "Iteration 19390 Training loss 0.0068413023836910725 Validation loss 0.011735102161765099 Accuracy 0.87646484375\n",
      "Iteration 19400 Training loss 0.008043664507567883 Validation loss 0.011823190376162529 Accuracy 0.87548828125\n",
      "Iteration 19410 Training loss 0.009220954962074757 Validation loss 0.01276672724634409 Accuracy 0.8662109375\n",
      "Iteration 19420 Training loss 0.006435157265514135 Validation loss 0.012070500291883945 Accuracy 0.87255859375\n",
      "Iteration 19430 Training loss 0.007664923090487719 Validation loss 0.01184156071394682 Accuracy 0.875\n",
      "Iteration 19440 Training loss 0.00689526554197073 Validation loss 0.01192951388657093 Accuracy 0.87451171875\n",
      "Iteration 19450 Training loss 0.008298822678625584 Validation loss 0.011705761775374413 Accuracy 0.87646484375\n",
      "Iteration 19460 Training loss 0.006978444289416075 Validation loss 0.011709809303283691 Accuracy 0.87744140625\n",
      "Iteration 19470 Training loss 0.006688236258924007 Validation loss 0.011955702677369118 Accuracy 0.875\n",
      "Iteration 19480 Training loss 0.008562058210372925 Validation loss 0.011790317483246326 Accuracy 0.875\n",
      "Iteration 19490 Training loss 0.009514672681689262 Validation loss 0.012304704636335373 Accuracy 0.87060546875\n",
      "Iteration 19500 Training loss 0.00872746016830206 Validation loss 0.011751098558306694 Accuracy 0.8759765625\n",
      "Iteration 19510 Training loss 0.006777188740670681 Validation loss 0.012036584317684174 Accuracy 0.8720703125\n",
      "Iteration 19520 Training loss 0.007811355404555798 Validation loss 0.012635739520192146 Accuracy 0.86669921875\n",
      "Iteration 19530 Training loss 0.0055406889878213406 Validation loss 0.011937489733099937 Accuracy 0.8740234375\n",
      "Iteration 19540 Training loss 0.008162273094058037 Validation loss 0.012090831995010376 Accuracy 0.8740234375\n",
      "Iteration 19550 Training loss 0.007501406129449606 Validation loss 0.012089838273823261 Accuracy 0.87255859375\n",
      "Iteration 19560 Training loss 0.006496045272797346 Validation loss 0.011513185687363148 Accuracy 0.87939453125\n",
      "Iteration 19570 Training loss 0.007779345847666264 Validation loss 0.012248864397406578 Accuracy 0.87060546875\n",
      "Iteration 19580 Training loss 0.007253782823681831 Validation loss 0.012166646309196949 Accuracy 0.8720703125\n",
      "Iteration 19590 Training loss 0.006576459389179945 Validation loss 0.011901875026524067 Accuracy 0.875\n",
      "Iteration 19600 Training loss 0.006442081183195114 Validation loss 0.011720802634954453 Accuracy 0.87744140625\n",
      "Iteration 19610 Training loss 0.010024857707321644 Validation loss 0.012746283784508705 Accuracy 0.8662109375\n",
      "Iteration 19620 Training loss 0.006269244942814112 Validation loss 0.011902897618710995 Accuracy 0.87451171875\n",
      "Iteration 19630 Training loss 0.006492486223578453 Validation loss 0.01207934319972992 Accuracy 0.87353515625\n",
      "Iteration 19640 Training loss 0.007371225859969854 Validation loss 0.011886459775269032 Accuracy 0.87451171875\n",
      "Iteration 19650 Training loss 0.006269598379731178 Validation loss 0.01243369746953249 Accuracy 0.8701171875\n",
      "Iteration 19660 Training loss 0.010413940995931625 Validation loss 0.01223849318921566 Accuracy 0.87109375\n",
      "Iteration 19670 Training loss 0.008840561844408512 Validation loss 0.011875711381435394 Accuracy 0.875\n",
      "Iteration 19680 Training loss 0.007725599687546492 Validation loss 0.01192252617329359 Accuracy 0.87548828125\n",
      "Iteration 19690 Training loss 0.005518291611224413 Validation loss 0.011417378671467304 Accuracy 0.8798828125\n",
      "Iteration 19700 Training loss 0.010946174152195454 Validation loss 0.011766435578465462 Accuracy 0.87744140625\n",
      "Iteration 19710 Training loss 0.007583736442029476 Validation loss 0.01159963384270668 Accuracy 0.8779296875\n",
      "Iteration 19720 Training loss 0.008416157215833664 Validation loss 0.011338203214108944 Accuracy 0.88134765625\n",
      "Iteration 19730 Training loss 0.006259313318878412 Validation loss 0.011905219405889511 Accuracy 0.875\n",
      "Iteration 19740 Training loss 0.007983994670212269 Validation loss 0.012357831932604313 Accuracy 0.87060546875\n",
      "Iteration 19750 Training loss 0.005252046510577202 Validation loss 0.011866888031363487 Accuracy 0.875\n",
      "Iteration 19760 Training loss 0.008044615387916565 Validation loss 0.01230328343808651 Accuracy 0.87109375\n",
      "Iteration 19770 Training loss 0.00727566285058856 Validation loss 0.011668302118778229 Accuracy 0.87744140625\n",
      "Iteration 19780 Training loss 0.007453483995050192 Validation loss 0.011805463582277298 Accuracy 0.875\n",
      "Iteration 19790 Training loss 0.006250898819416761 Validation loss 0.011798289604485035 Accuracy 0.87548828125\n",
      "Iteration 19800 Training loss 0.004783414304256439 Validation loss 0.01160119567066431 Accuracy 0.87841796875\n",
      "Iteration 19810 Training loss 0.0066330875270068645 Validation loss 0.011585869826376438 Accuracy 0.87841796875\n",
      "Iteration 19820 Training loss 0.0060520414263010025 Validation loss 0.011819418519735336 Accuracy 0.8759765625\n",
      "Iteration 19830 Training loss 0.005465890280902386 Validation loss 0.011541547253727913 Accuracy 0.87890625\n",
      "Iteration 19840 Training loss 0.00895907822996378 Validation loss 0.011606015264987946 Accuracy 0.87744140625\n",
      "Iteration 19850 Training loss 0.007196292281150818 Validation loss 0.011756069026887417 Accuracy 0.875\n",
      "Iteration 19860 Training loss 0.0070069339126348495 Validation loss 0.011581989005208015 Accuracy 0.87890625\n",
      "Iteration 19870 Training loss 0.009105204604566097 Validation loss 0.012462271377444267 Accuracy 0.86865234375\n",
      "Iteration 19880 Training loss 0.00758237624540925 Validation loss 0.011660898104310036 Accuracy 0.87744140625\n",
      "Iteration 19890 Training loss 0.007116631604731083 Validation loss 0.011980768293142319 Accuracy 0.87353515625\n",
      "Iteration 19900 Training loss 0.0074494159780442715 Validation loss 0.01188630610704422 Accuracy 0.87548828125\n",
      "Iteration 19910 Training loss 0.005803808569908142 Validation loss 0.011528229340910912 Accuracy 0.8779296875\n",
      "Iteration 19920 Training loss 0.006519509479403496 Validation loss 0.011808296665549278 Accuracy 0.875\n",
      "Iteration 19930 Training loss 0.006135799456387758 Validation loss 0.011886940337717533 Accuracy 0.87451171875\n",
      "Iteration 19940 Training loss 0.007838474586606026 Validation loss 0.011574533767998219 Accuracy 0.87841796875\n",
      "Iteration 19950 Training loss 0.0060670520178973675 Validation loss 0.011607257649302483 Accuracy 0.87744140625\n",
      "Iteration 19960 Training loss 0.006034668069332838 Validation loss 0.011533461511135101 Accuracy 0.87939453125\n",
      "Iteration 19970 Training loss 0.007691734004765749 Validation loss 0.011621088720858097 Accuracy 0.8779296875\n",
      "Iteration 19980 Training loss 0.005817682947963476 Validation loss 0.011771904304623604 Accuracy 0.876953125\n",
      "Iteration 19990 Training loss 0.008314733393490314 Validation loss 0.012217792682349682 Accuracy 0.8720703125\n",
      "Iteration 20000 Training loss 0.006352282129228115 Validation loss 0.011400651186704636 Accuracy 0.87939453125\n",
      "Iteration 20010 Training loss 0.005363464821130037 Validation loss 0.012062731198966503 Accuracy 0.87353515625\n",
      "Iteration 20020 Training loss 0.007568892557173967 Validation loss 0.01145137194544077 Accuracy 0.87939453125\n",
      "Iteration 20030 Training loss 0.008243635296821594 Validation loss 0.01169281080365181 Accuracy 0.87646484375\n",
      "Iteration 20040 Training loss 0.007052088156342506 Validation loss 0.011893988586962223 Accuracy 0.87451171875\n",
      "Iteration 20050 Training loss 0.007121838629245758 Validation loss 0.011749987490475178 Accuracy 0.8759765625\n",
      "Iteration 20060 Training loss 0.007856156677007675 Validation loss 0.011401528492569923 Accuracy 0.88037109375\n",
      "Iteration 20070 Training loss 0.005072084255516529 Validation loss 0.011406774632632732 Accuracy 0.87939453125\n",
      "Iteration 20080 Training loss 0.006982236634939909 Validation loss 0.012004205957055092 Accuracy 0.87353515625\n",
      "Iteration 20090 Training loss 0.006063316483050585 Validation loss 0.011508197523653507 Accuracy 0.87890625\n",
      "Iteration 20100 Training loss 0.010139591060578823 Validation loss 0.012349067255854607 Accuracy 0.869140625\n",
      "Iteration 20110 Training loss 0.00888932403177023 Validation loss 0.011512376368045807 Accuracy 0.87890625\n",
      "Iteration 20120 Training loss 0.006830182857811451 Validation loss 0.012074630707502365 Accuracy 0.87255859375\n",
      "Iteration 20130 Training loss 0.005717205815017223 Validation loss 0.011604768224060535 Accuracy 0.87744140625\n",
      "Iteration 20140 Training loss 0.007941663265228271 Validation loss 0.01178357470780611 Accuracy 0.87744140625\n",
      "Iteration 20150 Training loss 0.006918226834386587 Validation loss 0.012080592103302479 Accuracy 0.87353515625\n",
      "Iteration 20160 Training loss 0.007973626255989075 Validation loss 0.011754176579415798 Accuracy 0.87646484375\n",
      "Iteration 20170 Training loss 0.006810352206230164 Validation loss 0.011801783926784992 Accuracy 0.87646484375\n",
      "Iteration 20180 Training loss 0.005919657181948423 Validation loss 0.012067156843841076 Accuracy 0.87353515625\n",
      "Iteration 20190 Training loss 0.008270172402262688 Validation loss 0.012132614850997925 Accuracy 0.873046875\n",
      "Iteration 20200 Training loss 0.004626660142093897 Validation loss 0.011674684472382069 Accuracy 0.87744140625\n",
      "Iteration 20210 Training loss 0.0072489515878260136 Validation loss 0.011887497268617153 Accuracy 0.87451171875\n",
      "Iteration 20220 Training loss 0.006479061674326658 Validation loss 0.012056464329361916 Accuracy 0.8720703125\n",
      "Iteration 20230 Training loss 0.006829905789345503 Validation loss 0.011648600921034813 Accuracy 0.876953125\n",
      "Iteration 20240 Training loss 0.006996419280767441 Validation loss 0.011545702815055847 Accuracy 0.87841796875\n",
      "Iteration 20250 Training loss 0.0060584088787436485 Validation loss 0.01240853127092123 Accuracy 0.86962890625\n",
      "Iteration 20260 Training loss 0.007705168332904577 Validation loss 0.012117165140807629 Accuracy 0.873046875\n",
      "Iteration 20270 Training loss 0.006214364431798458 Validation loss 0.011664699763059616 Accuracy 0.876953125\n",
      "Iteration 20280 Training loss 0.009027284570038319 Validation loss 0.011612013913691044 Accuracy 0.8779296875\n",
      "Iteration 20290 Training loss 0.006399883423000574 Validation loss 0.01171521469950676 Accuracy 0.876953125\n",
      "Iteration 20300 Training loss 0.008812684565782547 Validation loss 0.012079733423888683 Accuracy 0.87255859375\n",
      "Iteration 20310 Training loss 0.00748441182076931 Validation loss 0.012082833796739578 Accuracy 0.873046875\n",
      "Iteration 20320 Training loss 0.005073782987892628 Validation loss 0.011570774018764496 Accuracy 0.87841796875\n",
      "Iteration 20330 Training loss 0.007123475428670645 Validation loss 0.011925782077014446 Accuracy 0.87451171875\n",
      "Iteration 20340 Training loss 0.0054375589825212955 Validation loss 0.011713105253875256 Accuracy 0.87744140625\n",
      "Iteration 20350 Training loss 0.00860959105193615 Validation loss 0.01148940622806549 Accuracy 0.87890625\n",
      "Iteration 20360 Training loss 0.008760361932218075 Validation loss 0.012100549414753914 Accuracy 0.873046875\n",
      "Iteration 20370 Training loss 0.006704936269670725 Validation loss 0.011802866123616695 Accuracy 0.8759765625\n",
      "Iteration 20380 Training loss 0.0076678963378071785 Validation loss 0.011626798659563065 Accuracy 0.8779296875\n",
      "Iteration 20390 Training loss 0.005967423785477877 Validation loss 0.011376645416021347 Accuracy 0.88037109375\n",
      "Iteration 20400 Training loss 0.006379289086908102 Validation loss 0.011609865352511406 Accuracy 0.8779296875\n",
      "Iteration 20410 Training loss 0.00796347577124834 Validation loss 0.011598652228713036 Accuracy 0.87744140625\n",
      "Iteration 20420 Training loss 0.006154251750558615 Validation loss 0.011466510593891144 Accuracy 0.87890625\n",
      "Iteration 20430 Training loss 0.009873430244624615 Validation loss 0.012921166606247425 Accuracy 0.86376953125\n",
      "Iteration 20440 Training loss 0.006806838326156139 Validation loss 0.011430758982896805 Accuracy 0.8798828125\n",
      "Iteration 20450 Training loss 0.010488289408385754 Validation loss 0.012465540319681168 Accuracy 0.869140625\n",
      "Iteration 20460 Training loss 0.007955145090818405 Validation loss 0.01214428711682558 Accuracy 0.873046875\n",
      "Iteration 20470 Training loss 0.005833802279084921 Validation loss 0.012573298066854477 Accuracy 0.8671875\n",
      "Iteration 20480 Training loss 0.007533869240432978 Validation loss 0.011973352171480656 Accuracy 0.873046875\n",
      "Iteration 20490 Training loss 0.007126061711460352 Validation loss 0.011385058052837849 Accuracy 0.87939453125\n",
      "Iteration 20500 Training loss 0.006382415536791086 Validation loss 0.012005601078271866 Accuracy 0.87451171875\n",
      "Iteration 20510 Training loss 0.0075521813705563545 Validation loss 0.011913737282156944 Accuracy 0.875\n",
      "Iteration 20520 Training loss 0.006485556252300739 Validation loss 0.01167260855436325 Accuracy 0.8779296875\n",
      "Iteration 20530 Training loss 0.004726841580122709 Validation loss 0.011610455811023712 Accuracy 0.8779296875\n",
      "Iteration 20540 Training loss 0.006911533419042826 Validation loss 0.011857946403324604 Accuracy 0.8759765625\n",
      "Iteration 20550 Training loss 0.00712865125387907 Validation loss 0.011642700992524624 Accuracy 0.87646484375\n",
      "Iteration 20560 Training loss 0.009192044846713543 Validation loss 0.011784793809056282 Accuracy 0.875\n",
      "Iteration 20570 Training loss 0.008674520999193192 Validation loss 0.012487963773310184 Accuracy 0.869140625\n",
      "Iteration 20580 Training loss 0.006225828547030687 Validation loss 0.01167727168649435 Accuracy 0.87744140625\n",
      "Iteration 20590 Training loss 0.005467962007969618 Validation loss 0.012015873566269875 Accuracy 0.873046875\n",
      "Iteration 20600 Training loss 0.004976952914148569 Validation loss 0.011588489636778831 Accuracy 0.87841796875\n",
      "Iteration 20610 Training loss 0.005116116721183062 Validation loss 0.011591327376663685 Accuracy 0.87841796875\n",
      "Iteration 20620 Training loss 0.008408918976783752 Validation loss 0.01216015126556158 Accuracy 0.8720703125\n",
      "Iteration 20630 Training loss 0.005438619758933783 Validation loss 0.011856178753077984 Accuracy 0.875\n",
      "Iteration 20640 Training loss 0.0070256562903523445 Validation loss 0.011838178150355816 Accuracy 0.87548828125\n",
      "Iteration 20650 Training loss 0.006804533768445253 Validation loss 0.011631252244114876 Accuracy 0.87744140625\n",
      "Iteration 20660 Training loss 0.009836097247898579 Validation loss 0.01186366192996502 Accuracy 0.87548828125\n",
      "Iteration 20670 Training loss 0.00551576679572463 Validation loss 0.011444096453487873 Accuracy 0.8798828125\n",
      "Iteration 20680 Training loss 0.004408481065183878 Validation loss 0.011965048499405384 Accuracy 0.87451171875\n",
      "Iteration 20690 Training loss 0.0075209173373878 Validation loss 0.01202132273465395 Accuracy 0.8720703125\n",
      "Iteration 20700 Training loss 0.005143304355442524 Validation loss 0.011659030802547932 Accuracy 0.87841796875\n",
      "Iteration 20710 Training loss 0.008225549943745136 Validation loss 0.011925749480724335 Accuracy 0.875\n",
      "Iteration 20720 Training loss 0.004957871045917273 Validation loss 0.011811408214271069 Accuracy 0.8759765625\n",
      "Iteration 20730 Training loss 0.006920411251485348 Validation loss 0.0118557158857584 Accuracy 0.875\n",
      "Iteration 20740 Training loss 0.00619810214266181 Validation loss 0.011818023398518562 Accuracy 0.87548828125\n",
      "Iteration 20750 Training loss 0.007119977846741676 Validation loss 0.012181092984974384 Accuracy 0.87158203125\n",
      "Iteration 20760 Training loss 0.007822506129741669 Validation loss 0.012569306418299675 Accuracy 0.86767578125\n",
      "Iteration 20770 Training loss 0.007530753966420889 Validation loss 0.01206537801772356 Accuracy 0.87353515625\n",
      "Iteration 20780 Training loss 0.006740823853760958 Validation loss 0.011646205559372902 Accuracy 0.8779296875\n",
      "Iteration 20790 Training loss 0.008442948572337627 Validation loss 0.012385229580104351 Accuracy 0.86962890625\n",
      "Iteration 20800 Training loss 0.005634988192468882 Validation loss 0.011610081419348717 Accuracy 0.87841796875\n",
      "Iteration 20810 Training loss 0.0071283914148807526 Validation loss 0.011805739253759384 Accuracy 0.8759765625\n",
      "Iteration 20820 Training loss 0.0063687036745250225 Validation loss 0.01178622804582119 Accuracy 0.87646484375\n",
      "Iteration 20830 Training loss 0.006866881158202887 Validation loss 0.01184119563549757 Accuracy 0.8759765625\n",
      "Iteration 20840 Training loss 0.006431824993342161 Validation loss 0.011819939129054546 Accuracy 0.87548828125\n",
      "Iteration 20850 Training loss 0.006137824151664972 Validation loss 0.011289136484265327 Accuracy 0.88134765625\n",
      "Iteration 20860 Training loss 0.006795684807002544 Validation loss 0.011264531873166561 Accuracy 0.880859375\n",
      "Iteration 20870 Training loss 0.006099455524235964 Validation loss 0.011662842705845833 Accuracy 0.87744140625\n",
      "Iteration 20880 Training loss 0.007454556878656149 Validation loss 0.011416415683925152 Accuracy 0.8798828125\n",
      "Iteration 20890 Training loss 0.008346877060830593 Validation loss 0.012279821559786797 Accuracy 0.87060546875\n",
      "Iteration 20900 Training loss 0.004572523757815361 Validation loss 0.011728779412806034 Accuracy 0.8759765625\n",
      "Iteration 20910 Training loss 0.008064857684075832 Validation loss 0.011677619069814682 Accuracy 0.876953125\n",
      "Iteration 20920 Training loss 0.005522776860743761 Validation loss 0.011394632048904896 Accuracy 0.8798828125\n",
      "Iteration 20930 Training loss 0.00529453344643116 Validation loss 0.011206453666090965 Accuracy 0.8818359375\n",
      "Iteration 20940 Training loss 0.00609762966632843 Validation loss 0.011474044993519783 Accuracy 0.87939453125\n",
      "Iteration 20950 Training loss 0.008936544880270958 Validation loss 0.011994938366115093 Accuracy 0.873046875\n",
      "Iteration 20960 Training loss 0.007445622701197863 Validation loss 0.011633068323135376 Accuracy 0.8779296875\n",
      "Iteration 20970 Training loss 0.0063543617725372314 Validation loss 0.011690656654536724 Accuracy 0.87646484375\n",
      "Iteration 20980 Training loss 0.007134765386581421 Validation loss 0.011937580071389675 Accuracy 0.87353515625\n",
      "Iteration 20990 Training loss 0.0070445905439555645 Validation loss 0.011394601315259933 Accuracy 0.8798828125\n",
      "Iteration 21000 Training loss 0.005740034859627485 Validation loss 0.011730806902050972 Accuracy 0.87548828125\n",
      "Iteration 21010 Training loss 0.006967534311115742 Validation loss 0.011381813324987888 Accuracy 0.8798828125\n",
      "Iteration 21020 Training loss 0.005727672949433327 Validation loss 0.012485815212130547 Accuracy 0.8681640625\n",
      "Iteration 21030 Training loss 0.005685076117515564 Validation loss 0.011473161168396473 Accuracy 0.87890625\n",
      "Iteration 21040 Training loss 0.00592609541490674 Validation loss 0.01148407906293869 Accuracy 0.8798828125\n",
      "Iteration 21050 Training loss 0.007339395582675934 Validation loss 0.011627458967268467 Accuracy 0.87744140625\n",
      "Iteration 21060 Training loss 0.009000862017273903 Validation loss 0.011921325698494911 Accuracy 0.875\n",
      "Iteration 21070 Training loss 0.007437131367623806 Validation loss 0.011686638928949833 Accuracy 0.876953125\n",
      "Iteration 21080 Training loss 0.008262427523732185 Validation loss 0.01165554579347372 Accuracy 0.8779296875\n",
      "Iteration 21090 Training loss 0.007801157422363758 Validation loss 0.01160862110555172 Accuracy 0.8779296875\n",
      "Iteration 21100 Training loss 0.005707354750484228 Validation loss 0.011384177021682262 Accuracy 0.88037109375\n",
      "Iteration 21110 Training loss 0.007214664947241545 Validation loss 0.012255944311618805 Accuracy 0.87109375\n",
      "Iteration 21120 Training loss 0.007766688708215952 Validation loss 0.011530219577252865 Accuracy 0.87841796875\n",
      "Iteration 21130 Training loss 0.006327853538095951 Validation loss 0.011719447560608387 Accuracy 0.8759765625\n",
      "Iteration 21140 Training loss 0.0056572542525827885 Validation loss 0.011361398734152317 Accuracy 0.880859375\n",
      "Iteration 21150 Training loss 0.006688543129712343 Validation loss 0.01147528551518917 Accuracy 0.87939453125\n",
      "Iteration 21160 Training loss 0.006722385995090008 Validation loss 0.01124102994799614 Accuracy 0.88134765625\n",
      "Iteration 21170 Training loss 0.007178655359894037 Validation loss 0.011479929089546204 Accuracy 0.8798828125\n",
      "Iteration 21180 Training loss 0.008383908309042454 Validation loss 0.011329533532261848 Accuracy 0.880859375\n",
      "Iteration 21190 Training loss 0.007698180619627237 Validation loss 0.011944944970309734 Accuracy 0.87451171875\n",
      "Iteration 21200 Training loss 0.007838508114218712 Validation loss 0.012314794585108757 Accuracy 0.87060546875\n",
      "Iteration 21210 Training loss 0.007203333545476198 Validation loss 0.011670436710119247 Accuracy 0.876953125\n",
      "Iteration 21220 Training loss 0.008291076868772507 Validation loss 0.012074189260601997 Accuracy 0.87255859375\n",
      "Iteration 21230 Training loss 0.007538847625255585 Validation loss 0.01218628603965044 Accuracy 0.87060546875\n",
      "Iteration 21240 Training loss 0.006944649387151003 Validation loss 0.012081527151167393 Accuracy 0.87255859375\n",
      "Iteration 21250 Training loss 0.005592842120677233 Validation loss 0.011538260616362095 Accuracy 0.87939453125\n",
      "Iteration 21260 Training loss 0.008474904112517834 Validation loss 0.011937406845390797 Accuracy 0.8740234375\n",
      "Iteration 21270 Training loss 0.0066657885909080505 Validation loss 0.011547885835170746 Accuracy 0.87890625\n",
      "Iteration 21280 Training loss 0.0058568790555000305 Validation loss 0.011631330475211143 Accuracy 0.87744140625\n",
      "Iteration 21290 Training loss 0.006352369673550129 Validation loss 0.0112537182867527 Accuracy 0.8818359375\n",
      "Iteration 21300 Training loss 0.0057732099667191505 Validation loss 0.01134719792753458 Accuracy 0.880859375\n",
      "Iteration 21310 Training loss 0.007108307909220457 Validation loss 0.01185719296336174 Accuracy 0.87548828125\n",
      "Iteration 21320 Training loss 0.006698837038129568 Validation loss 0.011733978986740112 Accuracy 0.87646484375\n",
      "Iteration 21330 Training loss 0.006191717926412821 Validation loss 0.01209142804145813 Accuracy 0.873046875\n",
      "Iteration 21340 Training loss 0.005807476118206978 Validation loss 0.011378027498722076 Accuracy 0.8798828125\n",
      "Iteration 21350 Training loss 0.00559152290225029 Validation loss 0.011430710554122925 Accuracy 0.87939453125\n",
      "Iteration 21360 Training loss 0.009190069511532784 Validation loss 0.011940049938857555 Accuracy 0.87353515625\n",
      "Iteration 21370 Training loss 0.0059542362578213215 Validation loss 0.011360879987478256 Accuracy 0.8798828125\n",
      "Iteration 21380 Training loss 0.007296995725482702 Validation loss 0.01200046855956316 Accuracy 0.8740234375\n",
      "Iteration 21390 Training loss 0.008269188925623894 Validation loss 0.011766967363655567 Accuracy 0.87646484375\n",
      "Iteration 21400 Training loss 0.006903535220772028 Validation loss 0.011571744456887245 Accuracy 0.87939453125\n",
      "Iteration 21410 Training loss 0.006398222409188747 Validation loss 0.011590500362217426 Accuracy 0.8779296875\n",
      "Iteration 21420 Training loss 0.0056906635873019695 Validation loss 0.011765656061470509 Accuracy 0.8759765625\n",
      "Iteration 21430 Training loss 0.00477992556989193 Validation loss 0.011624060571193695 Accuracy 0.87744140625\n",
      "Iteration 21440 Training loss 0.007037846837192774 Validation loss 0.011979104951024055 Accuracy 0.875\n",
      "Iteration 21450 Training loss 0.005939450580626726 Validation loss 0.011719190515577793 Accuracy 0.876953125\n",
      "Iteration 21460 Training loss 0.005497191566973925 Validation loss 0.011609903536736965 Accuracy 0.8779296875\n",
      "Iteration 21470 Training loss 0.008904078043997288 Validation loss 0.01274939626455307 Accuracy 0.86572265625\n",
      "Iteration 21480 Training loss 0.005493914242833853 Validation loss 0.011202709749341011 Accuracy 0.88232421875\n",
      "Iteration 21490 Training loss 0.004859123378992081 Validation loss 0.01114602293819189 Accuracy 0.88232421875\n",
      "Iteration 21500 Training loss 0.008249271661043167 Validation loss 0.013582802377641201 Accuracy 0.85693359375\n",
      "Iteration 21510 Training loss 0.004936514422297478 Validation loss 0.0114812720566988 Accuracy 0.8779296875\n",
      "Iteration 21520 Training loss 0.006892061326652765 Validation loss 0.011535807512700558 Accuracy 0.87890625\n",
      "Iteration 21530 Training loss 0.006398672703653574 Validation loss 0.011441564187407494 Accuracy 0.87939453125\n",
      "Iteration 21540 Training loss 0.00628669373691082 Validation loss 0.012311508879065514 Accuracy 0.87158203125\n",
      "Iteration 21550 Training loss 0.006962586659938097 Validation loss 0.011891821399331093 Accuracy 0.87548828125\n",
      "Iteration 21560 Training loss 0.005905600264668465 Validation loss 0.012177892960608006 Accuracy 0.87158203125\n",
      "Iteration 21570 Training loss 0.006108326371759176 Validation loss 0.011433462612330914 Accuracy 0.8798828125\n",
      "Iteration 21580 Training loss 0.006224223878234625 Validation loss 0.011861502192914486 Accuracy 0.875\n",
      "Iteration 21590 Training loss 0.004973383154720068 Validation loss 0.011333060450851917 Accuracy 0.880859375\n",
      "Iteration 21600 Training loss 0.006169362459331751 Validation loss 0.011632567271590233 Accuracy 0.87744140625\n",
      "Iteration 21610 Training loss 0.005914566107094288 Validation loss 0.011227443814277649 Accuracy 0.8818359375\n",
      "Iteration 21620 Training loss 0.007495852652937174 Validation loss 0.011364434845745564 Accuracy 0.88037109375\n",
      "Iteration 21630 Training loss 0.006840101908892393 Validation loss 0.011607302352786064 Accuracy 0.87841796875\n",
      "Iteration 21640 Training loss 0.008340508677065372 Validation loss 0.011332012712955475 Accuracy 0.88037109375\n",
      "Iteration 21650 Training loss 0.008813257329165936 Validation loss 0.011347094550728798 Accuracy 0.8798828125\n",
      "Iteration 21660 Training loss 0.007692382670938969 Validation loss 0.011539997532963753 Accuracy 0.87841796875\n",
      "Iteration 21670 Training loss 0.006021992303431034 Validation loss 0.011526759713888168 Accuracy 0.87890625\n",
      "Iteration 21680 Training loss 0.005285362713038921 Validation loss 0.0113442437723279 Accuracy 0.88037109375\n",
      "Iteration 21690 Training loss 0.006025717128068209 Validation loss 0.01135936751961708 Accuracy 0.88037109375\n",
      "Iteration 21700 Training loss 0.0070494296960532665 Validation loss 0.01139053050428629 Accuracy 0.880859375\n",
      "Iteration 21710 Training loss 0.008107578381896019 Validation loss 0.011833393014967442 Accuracy 0.87451171875\n",
      "Iteration 21720 Training loss 0.006121294107288122 Validation loss 0.011819287203252316 Accuracy 0.875\n",
      "Iteration 21730 Training loss 0.0102214515209198 Validation loss 0.011714433319866657 Accuracy 0.87744140625\n",
      "Iteration 21740 Training loss 0.005816176068037748 Validation loss 0.011501840315759182 Accuracy 0.87890625\n",
      "Iteration 21750 Training loss 0.007188587915152311 Validation loss 0.011669527739286423 Accuracy 0.87646484375\n",
      "Iteration 21760 Training loss 0.007332461420446634 Validation loss 0.011405065655708313 Accuracy 0.87939453125\n",
      "Iteration 21770 Training loss 0.006010090932250023 Validation loss 0.011551725678145885 Accuracy 0.8779296875\n",
      "Iteration 21780 Training loss 0.005773287266492844 Validation loss 0.011566807515919209 Accuracy 0.87890625\n",
      "Iteration 21790 Training loss 0.00789420586079359 Validation loss 0.012297326698899269 Accuracy 0.8701171875\n",
      "Iteration 21800 Training loss 0.007521314080804586 Validation loss 0.0116495992988348 Accuracy 0.87841796875\n",
      "Iteration 21810 Training loss 0.006071270443499088 Validation loss 0.011575681157410145 Accuracy 0.87890625\n",
      "Iteration 21820 Training loss 0.006763548590242863 Validation loss 0.011901666410267353 Accuracy 0.8740234375\n",
      "Iteration 21830 Training loss 0.007111165206879377 Validation loss 0.011635707691311836 Accuracy 0.87744140625\n",
      "Iteration 21840 Training loss 0.007064598146826029 Validation loss 0.011684870347380638 Accuracy 0.87744140625\n",
      "Iteration 21850 Training loss 0.006900899112224579 Validation loss 0.011435167863965034 Accuracy 0.8798828125\n",
      "Iteration 21860 Training loss 0.006984742358326912 Validation loss 0.011311415582895279 Accuracy 0.880859375\n",
      "Iteration 21870 Training loss 0.007495645899325609 Validation loss 0.012120900675654411 Accuracy 0.87255859375\n",
      "Iteration 21880 Training loss 0.006689752917736769 Validation loss 0.01150477398186922 Accuracy 0.87841796875\n",
      "Iteration 21890 Training loss 0.007782142609357834 Validation loss 0.012052162550389767 Accuracy 0.8740234375\n",
      "Iteration 21900 Training loss 0.0060417549684643745 Validation loss 0.011261441744863987 Accuracy 0.88134765625\n",
      "Iteration 21910 Training loss 0.006607873365283012 Validation loss 0.011874807067215443 Accuracy 0.87451171875\n",
      "Iteration 21920 Training loss 0.007052567787468433 Validation loss 0.011614159680902958 Accuracy 0.87841796875\n",
      "Iteration 21930 Training loss 0.00917354691773653 Validation loss 0.012731580063700676 Accuracy 0.86669921875\n",
      "Iteration 21940 Training loss 0.004525294061750174 Validation loss 0.011275977827608585 Accuracy 0.8818359375\n",
      "Iteration 21950 Training loss 0.006018310319632292 Validation loss 0.011506891809403896 Accuracy 0.87890625\n",
      "Iteration 21960 Training loss 0.008158715441823006 Validation loss 0.011697784066200256 Accuracy 0.87646484375\n",
      "Iteration 21970 Training loss 0.006711573339998722 Validation loss 0.012067957781255245 Accuracy 0.87158203125\n",
      "Iteration 21980 Training loss 0.007310068234801292 Validation loss 0.011813928373157978 Accuracy 0.8759765625\n",
      "Iteration 21990 Training loss 0.005067609250545502 Validation loss 0.011575629934668541 Accuracy 0.8779296875\n",
      "Iteration 22000 Training loss 0.007485285401344299 Validation loss 0.01186361163854599 Accuracy 0.8740234375\n",
      "Iteration 22010 Training loss 0.007853452116250992 Validation loss 0.01178718265146017 Accuracy 0.8759765625\n",
      "Iteration 22020 Training loss 0.006173360161483288 Validation loss 0.01170973852276802 Accuracy 0.8759765625\n",
      "Iteration 22030 Training loss 0.006322323344647884 Validation loss 0.01171646174043417 Accuracy 0.875\n",
      "Iteration 22040 Training loss 0.00730010075494647 Validation loss 0.011354584246873856 Accuracy 0.87890625\n",
      "Iteration 22050 Training loss 0.006703244522213936 Validation loss 0.011686627753078938 Accuracy 0.87646484375\n",
      "Iteration 22060 Training loss 0.0071447547525167465 Validation loss 0.011411169543862343 Accuracy 0.8798828125\n",
      "Iteration 22070 Training loss 0.0038456039037555456 Validation loss 0.011142470873892307 Accuracy 0.88232421875\n",
      "Iteration 22080 Training loss 0.00509046483784914 Validation loss 0.011343107558786869 Accuracy 0.88037109375\n",
      "Iteration 22090 Training loss 0.0051116603426635265 Validation loss 0.011781793087720871 Accuracy 0.8759765625\n",
      "Iteration 22100 Training loss 0.008982863277196884 Validation loss 0.011820285581052303 Accuracy 0.87548828125\n",
      "Iteration 22110 Training loss 0.006782062351703644 Validation loss 0.011629397049546242 Accuracy 0.876953125\n",
      "Iteration 22120 Training loss 0.006461183074861765 Validation loss 0.011962086893618107 Accuracy 0.87353515625\n",
      "Iteration 22130 Training loss 0.005151183810085058 Validation loss 0.011531169526278973 Accuracy 0.87890625\n",
      "Iteration 22140 Training loss 0.009500261396169662 Validation loss 0.011932404711842537 Accuracy 0.87451171875\n",
      "Iteration 22150 Training loss 0.007461948320269585 Validation loss 0.011115227825939655 Accuracy 0.88232421875\n",
      "Iteration 22160 Training loss 0.0064300899393856525 Validation loss 0.012319513596594334 Accuracy 0.86962890625\n",
      "Iteration 22170 Training loss 0.005360629875212908 Validation loss 0.011281519196927547 Accuracy 0.880859375\n",
      "Iteration 22180 Training loss 0.008816815912723541 Validation loss 0.012223134748637676 Accuracy 0.87109375\n",
      "Iteration 22190 Training loss 0.006340446416288614 Validation loss 0.01136691588908434 Accuracy 0.8798828125\n",
      "Iteration 22200 Training loss 0.006024359259754419 Validation loss 0.011318706907331944 Accuracy 0.880859375\n",
      "Iteration 22210 Training loss 0.007271686568856239 Validation loss 0.011705739423632622 Accuracy 0.8759765625\n",
      "Iteration 22220 Training loss 0.009577176533639431 Validation loss 0.013255920261144638 Accuracy 0.85986328125\n",
      "Iteration 22230 Training loss 0.006197106558829546 Validation loss 0.011490714736282825 Accuracy 0.87841796875\n",
      "Iteration 22240 Training loss 0.006731685716658831 Validation loss 0.011214599013328552 Accuracy 0.88232421875\n",
      "Iteration 22250 Training loss 0.006036300677806139 Validation loss 0.011538698337972164 Accuracy 0.8779296875\n",
      "Iteration 22260 Training loss 0.007353529334068298 Validation loss 0.01129046268761158 Accuracy 0.88134765625\n",
      "Iteration 22270 Training loss 0.006563976872712374 Validation loss 0.011248906143009663 Accuracy 0.88134765625\n",
      "Iteration 22280 Training loss 0.006198596209287643 Validation loss 0.011362974531948566 Accuracy 0.88037109375\n",
      "Iteration 22290 Training loss 0.007464478723704815 Validation loss 0.011573593132197857 Accuracy 0.8779296875\n",
      "Iteration 22300 Training loss 0.0067467051558196545 Validation loss 0.01142838690429926 Accuracy 0.8798828125\n",
      "Iteration 22310 Training loss 0.0051316265016794205 Validation loss 0.011598212644457817 Accuracy 0.87744140625\n",
      "Iteration 22320 Training loss 0.00551370345056057 Validation loss 0.01141311228275299 Accuracy 0.8798828125\n",
      "Iteration 22330 Training loss 0.006915983743965626 Validation loss 0.011278152465820312 Accuracy 0.88134765625\n",
      "Iteration 22340 Training loss 0.004718095064163208 Validation loss 0.011136362329125404 Accuracy 0.88232421875\n",
      "Iteration 22350 Training loss 0.005332102999091148 Validation loss 0.011324831284582615 Accuracy 0.880859375\n",
      "Iteration 22360 Training loss 0.007257335353642702 Validation loss 0.011408663354814053 Accuracy 0.8798828125\n",
      "Iteration 22370 Training loss 0.007481763605028391 Validation loss 0.011461556889116764 Accuracy 0.87939453125\n",
      "Iteration 22380 Training loss 0.008754092268645763 Validation loss 0.01153438538312912 Accuracy 0.87890625\n",
      "Iteration 22390 Training loss 0.007146343123167753 Validation loss 0.011664772406220436 Accuracy 0.876953125\n",
      "Iteration 22400 Training loss 0.004988780245184898 Validation loss 0.011369766667485237 Accuracy 0.88037109375\n",
      "Iteration 22410 Training loss 0.005092863459140062 Validation loss 0.011277562938630581 Accuracy 0.88037109375\n",
      "Iteration 22420 Training loss 0.006883824709802866 Validation loss 0.01118677482008934 Accuracy 0.88134765625\n",
      "Iteration 22430 Training loss 0.00687751779332757 Validation loss 0.011540091596543789 Accuracy 0.8779296875\n",
      "Iteration 22440 Training loss 0.005839757155627012 Validation loss 0.011335006915032864 Accuracy 0.87939453125\n",
      "Iteration 22450 Training loss 0.005835818592458963 Validation loss 0.011806783266365528 Accuracy 0.87548828125\n",
      "Iteration 22460 Training loss 0.008365200832486153 Validation loss 0.012186619453132153 Accuracy 0.87109375\n",
      "Iteration 22470 Training loss 0.006461500655859709 Validation loss 0.011473177000880241 Accuracy 0.87841796875\n",
      "Iteration 22480 Training loss 0.00695995707064867 Validation loss 0.011601803824305534 Accuracy 0.87890625\n",
      "Iteration 22490 Training loss 0.00519119156524539 Validation loss 0.011561362072825432 Accuracy 0.8779296875\n",
      "Iteration 22500 Training loss 0.006442566402256489 Validation loss 0.011596680618822575 Accuracy 0.8779296875\n",
      "Iteration 22510 Training loss 0.005283366423100233 Validation loss 0.011778061278164387 Accuracy 0.8759765625\n",
      "Iteration 22520 Training loss 0.007112703286111355 Validation loss 0.011455628089606762 Accuracy 0.87939453125\n",
      "Iteration 22530 Training loss 0.00718674948439002 Validation loss 0.011516178958117962 Accuracy 0.87841796875\n",
      "Iteration 22540 Training loss 0.007472315337508917 Validation loss 0.012553123757243156 Accuracy 0.8671875\n",
      "Iteration 22550 Training loss 0.008036904968321323 Validation loss 0.011236213147640228 Accuracy 0.8828125\n",
      "Iteration 22560 Training loss 0.003784920321777463 Validation loss 0.011383556760847569 Accuracy 0.8779296875\n",
      "Iteration 22570 Training loss 0.006931766401976347 Validation loss 0.011308911256492138 Accuracy 0.87939453125\n",
      "Iteration 22580 Training loss 0.006544387899339199 Validation loss 0.011534023098647594 Accuracy 0.87841796875\n",
      "Iteration 22590 Training loss 0.007689665537327528 Validation loss 0.011193196289241314 Accuracy 0.8818359375\n",
      "Iteration 22600 Training loss 0.005699389148503542 Validation loss 0.011508532799780369 Accuracy 0.87841796875\n",
      "Iteration 22610 Training loss 0.0046998788602650166 Validation loss 0.011302340775728226 Accuracy 0.88037109375\n",
      "Iteration 22620 Training loss 0.005305654369294643 Validation loss 0.011278335005044937 Accuracy 0.880859375\n",
      "Iteration 22630 Training loss 0.0060925609432160854 Validation loss 0.011773314327001572 Accuracy 0.8759765625\n",
      "Iteration 22640 Training loss 0.005587513092905283 Validation loss 0.01154824998229742 Accuracy 0.8779296875\n",
      "Iteration 22650 Training loss 0.003790859831497073 Validation loss 0.011328835971653461 Accuracy 0.8798828125\n",
      "Iteration 22660 Training loss 0.0069801090285182 Validation loss 0.011367116123437881 Accuracy 0.88037109375\n",
      "Iteration 22670 Training loss 0.005758407525718212 Validation loss 0.011732607148587704 Accuracy 0.87548828125\n",
      "Iteration 22680 Training loss 0.008139865472912788 Validation loss 0.011408703401684761 Accuracy 0.87939453125\n",
      "Iteration 22690 Training loss 0.009560180827975273 Validation loss 0.014180552214384079 Accuracy 0.8515625\n",
      "Iteration 22700 Training loss 0.00577856320887804 Validation loss 0.011616818606853485 Accuracy 0.8779296875\n",
      "Iteration 22710 Training loss 0.005522649735212326 Validation loss 0.011347794905304909 Accuracy 0.88037109375\n",
      "Iteration 22720 Training loss 0.006405968684703112 Validation loss 0.011438670568168163 Accuracy 0.87939453125\n",
      "Iteration 22730 Training loss 0.005879055708646774 Validation loss 0.011477958410978317 Accuracy 0.87890625\n",
      "Iteration 22740 Training loss 0.006773513741791248 Validation loss 0.01164157409220934 Accuracy 0.87744140625\n",
      "Iteration 22750 Training loss 0.006224830634891987 Validation loss 0.011731859296560287 Accuracy 0.876953125\n",
      "Iteration 22760 Training loss 0.004398013930767775 Validation loss 0.011562065221369267 Accuracy 0.8779296875\n",
      "Iteration 22770 Training loss 0.004872904159128666 Validation loss 0.011762261390686035 Accuracy 0.87548828125\n",
      "Iteration 22780 Training loss 0.006891642697155476 Validation loss 0.011401491239666939 Accuracy 0.87939453125\n",
      "Iteration 22790 Training loss 0.005543590523302555 Validation loss 0.011326666921377182 Accuracy 0.88037109375\n",
      "Iteration 22800 Training loss 0.006571026984602213 Validation loss 0.011340485885739326 Accuracy 0.88037109375\n",
      "Iteration 22810 Training loss 0.006080931052565575 Validation loss 0.011500908061861992 Accuracy 0.87939453125\n",
      "Iteration 22820 Training loss 0.0073065729811787605 Validation loss 0.011315295472741127 Accuracy 0.880859375\n",
      "Iteration 22830 Training loss 0.006641559302806854 Validation loss 0.011746341362595558 Accuracy 0.8759765625\n",
      "Iteration 22840 Training loss 0.004758570343255997 Validation loss 0.011570378206670284 Accuracy 0.87841796875\n",
      "Iteration 22850 Training loss 0.00753673305734992 Validation loss 0.01170413289219141 Accuracy 0.87646484375\n",
      "Iteration 22860 Training loss 0.006437886040657759 Validation loss 0.011352374218404293 Accuracy 0.87939453125\n",
      "Iteration 22870 Training loss 0.006129962392151356 Validation loss 0.011511021293699741 Accuracy 0.87890625\n",
      "Iteration 22880 Training loss 0.005415989551693201 Validation loss 0.011207293719053268 Accuracy 0.8818359375\n",
      "Iteration 22890 Training loss 0.005920459516346455 Validation loss 0.011308925226330757 Accuracy 0.8818359375\n",
      "Iteration 22900 Training loss 0.005421197973191738 Validation loss 0.011631619185209274 Accuracy 0.87744140625\n",
      "Iteration 22910 Training loss 0.004064490087330341 Validation loss 0.01171969249844551 Accuracy 0.87646484375\n",
      "Iteration 22920 Training loss 0.005881635472178459 Validation loss 0.011272462084889412 Accuracy 0.88037109375\n",
      "Iteration 22930 Training loss 0.005036083981394768 Validation loss 0.011475580744445324 Accuracy 0.87939453125\n",
      "Iteration 22940 Training loss 0.005985477473586798 Validation loss 0.011367897503077984 Accuracy 0.87939453125\n",
      "Iteration 22950 Training loss 0.007647308986634016 Validation loss 0.011772875674068928 Accuracy 0.8759765625\n",
      "Iteration 22960 Training loss 0.0063715120777487755 Validation loss 0.011843744665384293 Accuracy 0.87548828125\n",
      "Iteration 22970 Training loss 0.008070160634815693 Validation loss 0.011594901792705059 Accuracy 0.8779296875\n",
      "Iteration 22980 Training loss 0.006006844807416201 Validation loss 0.011520031839609146 Accuracy 0.87744140625\n",
      "Iteration 22990 Training loss 0.008404811844229698 Validation loss 0.011704184114933014 Accuracy 0.8759765625\n",
      "Iteration 23000 Training loss 0.00778901157900691 Validation loss 0.011764715425670147 Accuracy 0.87646484375\n",
      "Iteration 23010 Training loss 0.004564139526337385 Validation loss 0.011437159962952137 Accuracy 0.87939453125\n",
      "Iteration 23020 Training loss 0.006847096607089043 Validation loss 0.011690954677760601 Accuracy 0.87646484375\n",
      "Iteration 23030 Training loss 0.008566939271986485 Validation loss 0.011344840750098228 Accuracy 0.880859375\n",
      "Iteration 23040 Training loss 0.006311634089797735 Validation loss 0.011628687381744385 Accuracy 0.87841796875\n",
      "Iteration 23050 Training loss 0.0059472001157701015 Validation loss 0.01159678678959608 Accuracy 0.87890625\n",
      "Iteration 23060 Training loss 0.0043376656249165535 Validation loss 0.01140008494257927 Accuracy 0.87841796875\n",
      "Iteration 23070 Training loss 0.004712016321718693 Validation loss 0.011600231751799583 Accuracy 0.87548828125\n",
      "Iteration 23080 Training loss 0.006567493546754122 Validation loss 0.011214987374842167 Accuracy 0.8818359375\n",
      "Iteration 23090 Training loss 0.006772542372345924 Validation loss 0.011332379654049873 Accuracy 0.88037109375\n",
      "Iteration 23100 Training loss 0.004667033441364765 Validation loss 0.011153693310916424 Accuracy 0.8818359375\n",
      "Iteration 23110 Training loss 0.007460144814103842 Validation loss 0.011195347644388676 Accuracy 0.8828125\n",
      "Iteration 23120 Training loss 0.006997985299676657 Validation loss 0.011189958080649376 Accuracy 0.88232421875\n",
      "Iteration 23130 Training loss 0.006013388279825449 Validation loss 0.011289757676422596 Accuracy 0.880859375\n",
      "Iteration 23140 Training loss 0.005314499139785767 Validation loss 0.011327018961310387 Accuracy 0.880859375\n",
      "Iteration 23150 Training loss 0.007265964988619089 Validation loss 0.01141269039362669 Accuracy 0.87890625\n",
      "Iteration 23160 Training loss 0.007708193268626928 Validation loss 0.011346716433763504 Accuracy 0.88037109375\n",
      "Iteration 23170 Training loss 0.004403707571327686 Validation loss 0.011246266774833202 Accuracy 0.8818359375\n",
      "Iteration 23180 Training loss 0.006600277964025736 Validation loss 0.01237691380083561 Accuracy 0.86962890625\n",
      "Iteration 23190 Training loss 0.0051328083500266075 Validation loss 0.01166363526135683 Accuracy 0.87646484375\n",
      "Iteration 23200 Training loss 0.00818631425499916 Validation loss 0.012051326222717762 Accuracy 0.87255859375\n",
      "Iteration 23210 Training loss 0.006191923748701811 Validation loss 0.011803208850324154 Accuracy 0.87548828125\n",
      "Iteration 23220 Training loss 0.0060570817440748215 Validation loss 0.011330763809382915 Accuracy 0.88037109375\n",
      "Iteration 23230 Training loss 0.005168805830180645 Validation loss 0.011340558528900146 Accuracy 0.87939453125\n",
      "Iteration 23240 Training loss 0.005250024609267712 Validation loss 0.01154242642223835 Accuracy 0.8779296875\n",
      "Iteration 23250 Training loss 0.005953578744083643 Validation loss 0.011515275575220585 Accuracy 0.87841796875\n",
      "Iteration 23260 Training loss 0.004194584675133228 Validation loss 0.01124765444546938 Accuracy 0.880859375\n",
      "Iteration 23270 Training loss 0.003955502063035965 Validation loss 0.011198215186595917 Accuracy 0.8818359375\n",
      "Iteration 23280 Training loss 0.006012941710650921 Validation loss 0.011666483245790005 Accuracy 0.8759765625\n",
      "Iteration 23290 Training loss 0.0059739635325968266 Validation loss 0.011427517980337143 Accuracy 0.87890625\n",
      "Iteration 23300 Training loss 0.005972014274448156 Validation loss 0.012090911157429218 Accuracy 0.87158203125\n",
      "Iteration 23310 Training loss 0.004844483453780413 Validation loss 0.011553146876394749 Accuracy 0.876953125\n",
      "Iteration 23320 Training loss 0.007928543724119663 Validation loss 0.01144914049655199 Accuracy 0.87890625\n",
      "Iteration 23330 Training loss 0.006347058340907097 Validation loss 0.01122279278934002 Accuracy 0.88134765625\n",
      "Iteration 23340 Training loss 0.003964012023061514 Validation loss 0.011337313801050186 Accuracy 0.87939453125\n",
      "Iteration 23350 Training loss 0.006573000457137823 Validation loss 0.011642209254205227 Accuracy 0.876953125\n",
      "Iteration 23360 Training loss 0.005135662388056517 Validation loss 0.011208991520106792 Accuracy 0.88134765625\n",
      "Iteration 23370 Training loss 0.008536768145859241 Validation loss 0.011725779622793198 Accuracy 0.87548828125\n",
      "Iteration 23380 Training loss 0.007211880758404732 Validation loss 0.01166341919451952 Accuracy 0.876953125\n",
      "Iteration 23390 Training loss 0.0049354941584169865 Validation loss 0.011201185174286366 Accuracy 0.880859375\n",
      "Iteration 23400 Training loss 0.007386564742773771 Validation loss 0.012160449288785458 Accuracy 0.87158203125\n",
      "Iteration 23410 Training loss 0.006168826948851347 Validation loss 0.011344562284648418 Accuracy 0.8798828125\n",
      "Iteration 23420 Training loss 0.004290047101676464 Validation loss 0.011353116482496262 Accuracy 0.87939453125\n",
      "Iteration 23430 Training loss 0.005188977345824242 Validation loss 0.01151090394705534 Accuracy 0.87890625\n",
      "Iteration 23440 Training loss 0.004756886046379805 Validation loss 0.011369041167199612 Accuracy 0.87939453125\n",
      "Iteration 23450 Training loss 0.0058000716380774975 Validation loss 0.011343976482748985 Accuracy 0.880859375\n",
      "Iteration 23460 Training loss 0.005199664272367954 Validation loss 0.011467025615274906 Accuracy 0.8798828125\n",
      "Iteration 23470 Training loss 0.006534601096063852 Validation loss 0.011190755292773247 Accuracy 0.88134765625\n",
      "Iteration 23480 Training loss 0.006754857487976551 Validation loss 0.011778159067034721 Accuracy 0.87548828125\n",
      "Iteration 23490 Training loss 0.005117663647979498 Validation loss 0.011203691363334656 Accuracy 0.8828125\n",
      "Iteration 23500 Training loss 0.0075325132347643375 Validation loss 0.01140100322663784 Accuracy 0.87841796875\n",
      "Iteration 23510 Training loss 0.005691662896424532 Validation loss 0.011482122354209423 Accuracy 0.87841796875\n",
      "Iteration 23520 Training loss 0.0058796461671590805 Validation loss 0.011750828474760056 Accuracy 0.87548828125\n",
      "Iteration 23530 Training loss 0.005703585222363472 Validation loss 0.011203168891370296 Accuracy 0.88134765625\n",
      "Iteration 23540 Training loss 0.007033621426671743 Validation loss 0.011309808120131493 Accuracy 0.88037109375\n",
      "Iteration 23550 Training loss 0.004774114117026329 Validation loss 0.011084910482168198 Accuracy 0.88232421875\n",
      "Iteration 23560 Training loss 0.008030342869460583 Validation loss 0.011959929019212723 Accuracy 0.87451171875\n",
      "Iteration 23570 Training loss 0.005448942072689533 Validation loss 0.011593746952712536 Accuracy 0.8759765625\n",
      "Iteration 23580 Training loss 0.0052994596771895885 Validation loss 0.01115456409752369 Accuracy 0.8818359375\n",
      "Iteration 23590 Training loss 0.004599258303642273 Validation loss 0.011422308161854744 Accuracy 0.87890625\n",
      "Iteration 23600 Training loss 0.007297387346625328 Validation loss 0.012015104293823242 Accuracy 0.873046875\n",
      "Iteration 23610 Training loss 0.007341365329921246 Validation loss 0.01137690432369709 Accuracy 0.87939453125\n",
      "Iteration 23620 Training loss 0.007268806453794241 Validation loss 0.011259239166975021 Accuracy 0.88037109375\n",
      "Iteration 23630 Training loss 0.006423856131732464 Validation loss 0.01135995052754879 Accuracy 0.87939453125\n",
      "Iteration 23640 Training loss 0.004320111125707626 Validation loss 0.011463739909231663 Accuracy 0.87890625\n",
      "Iteration 23650 Training loss 0.00487100426107645 Validation loss 0.011425409466028214 Accuracy 0.8798828125\n",
      "Iteration 23660 Training loss 0.006625285837799311 Validation loss 0.011447597295045853 Accuracy 0.87841796875\n",
      "Iteration 23670 Training loss 0.0062606073915958405 Validation loss 0.011387133039534092 Accuracy 0.87939453125\n",
      "Iteration 23680 Training loss 0.005451090633869171 Validation loss 0.011278693564236164 Accuracy 0.88037109375\n",
      "Iteration 23690 Training loss 0.006203757133334875 Validation loss 0.011793903075158596 Accuracy 0.87548828125\n",
      "Iteration 23700 Training loss 0.005534776020795107 Validation loss 0.011384783312678337 Accuracy 0.8798828125\n",
      "Iteration 23710 Training loss 0.0050302487798035145 Validation loss 0.011410290375351906 Accuracy 0.87939453125\n",
      "Iteration 23720 Training loss 0.005219542887061834 Validation loss 0.011386530473828316 Accuracy 0.8798828125\n",
      "Iteration 23730 Training loss 0.0055654626339674 Validation loss 0.01152213104069233 Accuracy 0.8779296875\n",
      "Iteration 23740 Training loss 0.007366048637777567 Validation loss 0.01175668928772211 Accuracy 0.875\n",
      "Iteration 23750 Training loss 0.006382203195244074 Validation loss 0.011330908164381981 Accuracy 0.8798828125\n",
      "Iteration 23760 Training loss 0.005933166481554508 Validation loss 0.011697525158524513 Accuracy 0.8759765625\n",
      "Iteration 23770 Training loss 0.004572635050863028 Validation loss 0.011859237216413021 Accuracy 0.87451171875\n",
      "Iteration 23780 Training loss 0.0053463527001440525 Validation loss 0.011266598477959633 Accuracy 0.8818359375\n",
      "Iteration 23790 Training loss 0.007053145207464695 Validation loss 0.011848862282931805 Accuracy 0.87451171875\n",
      "Iteration 23800 Training loss 0.004727391991764307 Validation loss 0.011241584084928036 Accuracy 0.88037109375\n",
      "Iteration 23810 Training loss 0.0055258674547076225 Validation loss 0.011493390426039696 Accuracy 0.8779296875\n",
      "Iteration 23820 Training loss 0.003462341846898198 Validation loss 0.011372409760951996 Accuracy 0.880859375\n",
      "Iteration 23830 Training loss 0.006048567593097687 Validation loss 0.011503560468554497 Accuracy 0.87841796875\n",
      "Iteration 23840 Training loss 0.0056546833366155624 Validation loss 0.011398116126656532 Accuracy 0.87939453125\n",
      "Iteration 23850 Training loss 0.006019349209964275 Validation loss 0.011301911436021328 Accuracy 0.880859375\n",
      "Iteration 23860 Training loss 0.004974715411663055 Validation loss 0.011605174280703068 Accuracy 0.8779296875\n",
      "Iteration 23870 Training loss 0.007296114694327116 Validation loss 0.01123534794896841 Accuracy 0.880859375\n",
      "Iteration 23880 Training loss 0.007011507637798786 Validation loss 0.011377601884305477 Accuracy 0.8798828125\n",
      "Iteration 23890 Training loss 0.005189636722207069 Validation loss 0.010970031842589378 Accuracy 0.884765625\n",
      "Iteration 23900 Training loss 0.004924365784972906 Validation loss 0.011244083754718304 Accuracy 0.88134765625\n",
      "Iteration 23910 Training loss 0.004892606753855944 Validation loss 0.01113271713256836 Accuracy 0.88134765625\n",
      "Iteration 23920 Training loss 0.006860149092972279 Validation loss 0.01133290957659483 Accuracy 0.88037109375\n",
      "Iteration 23930 Training loss 0.00637606019154191 Validation loss 0.011268538422882557 Accuracy 0.880859375\n",
      "Iteration 23940 Training loss 0.005752298515290022 Validation loss 0.011028463952243328 Accuracy 0.8828125\n",
      "Iteration 23950 Training loss 0.007160963024944067 Validation loss 0.011098158545792103 Accuracy 0.8828125\n",
      "Iteration 23960 Training loss 0.005478359293192625 Validation loss 0.01132172904908657 Accuracy 0.88037109375\n",
      "Iteration 23970 Training loss 0.006752896588295698 Validation loss 0.011695807799696922 Accuracy 0.87646484375\n",
      "Iteration 23980 Training loss 0.006746791768819094 Validation loss 0.011419896967709064 Accuracy 0.87890625\n",
      "Iteration 23990 Training loss 0.00644800066947937 Validation loss 0.011848402209579945 Accuracy 0.87548828125\n",
      "Iteration 24000 Training loss 0.004600547719746828 Validation loss 0.011591295711696148 Accuracy 0.8779296875\n",
      "Iteration 24010 Training loss 0.005347772501409054 Validation loss 0.011154791340231895 Accuracy 0.8818359375\n",
      "Iteration 24020 Training loss 0.007756727747619152 Validation loss 0.011960526928305626 Accuracy 0.8740234375\n",
      "Iteration 24030 Training loss 0.005816158372908831 Validation loss 0.011452562175691128 Accuracy 0.87890625\n",
      "Iteration 24040 Training loss 0.005318488925695419 Validation loss 0.011430365964770317 Accuracy 0.87939453125\n",
      "Iteration 24050 Training loss 0.006230452563613653 Validation loss 0.011641977354884148 Accuracy 0.876953125\n",
      "Iteration 24060 Training loss 0.00643776124343276 Validation loss 0.01125511433929205 Accuracy 0.88037109375\n",
      "Iteration 24070 Training loss 0.004783631768077612 Validation loss 0.011243323795497417 Accuracy 0.880859375\n",
      "Iteration 24080 Training loss 0.005814108531922102 Validation loss 0.011286857537925243 Accuracy 0.87939453125\n",
      "Iteration 24090 Training loss 0.0073339669033885 Validation loss 0.011347121559083462 Accuracy 0.8798828125\n",
      "Iteration 24100 Training loss 0.005229657981544733 Validation loss 0.011327543295919895 Accuracy 0.8798828125\n",
      "Iteration 24110 Training loss 0.006173672154545784 Validation loss 0.011108212172985077 Accuracy 0.88330078125\n",
      "Iteration 24120 Training loss 0.0052602761425077915 Validation loss 0.010860887356102467 Accuracy 0.88623046875\n",
      "Iteration 24130 Training loss 0.004802832845598459 Validation loss 0.011321193538606167 Accuracy 0.88037109375\n",
      "Iteration 24140 Training loss 0.006070921663194895 Validation loss 0.011300421319901943 Accuracy 0.88037109375\n",
      "Iteration 24150 Training loss 0.00474745174869895 Validation loss 0.011455628089606762 Accuracy 0.87890625\n",
      "Iteration 24160 Training loss 0.005945087876170874 Validation loss 0.011613612063229084 Accuracy 0.8759765625\n",
      "Iteration 24170 Training loss 0.005535897333174944 Validation loss 0.011692618019878864 Accuracy 0.876953125\n",
      "Iteration 24180 Training loss 0.006502246949821711 Validation loss 0.0110784862190485 Accuracy 0.8828125\n",
      "Iteration 24190 Training loss 0.005791733507066965 Validation loss 0.01093988586217165 Accuracy 0.8837890625\n",
      "Iteration 24200 Training loss 0.003597254166379571 Validation loss 0.011180775240063667 Accuracy 0.880859375\n",
      "Iteration 24210 Training loss 0.006683776620775461 Validation loss 0.011190441437065601 Accuracy 0.88134765625\n",
      "Iteration 24220 Training loss 0.0065296851098537445 Validation loss 0.011382775381207466 Accuracy 0.8798828125\n",
      "Iteration 24230 Training loss 0.005770583637058735 Validation loss 0.010939422063529491 Accuracy 0.8837890625\n",
      "Iteration 24240 Training loss 0.00565903028473258 Validation loss 0.011198075488209724 Accuracy 0.88134765625\n",
      "Iteration 24250 Training loss 0.004757055547088385 Validation loss 0.011925515718758106 Accuracy 0.8740234375\n",
      "Iteration 24260 Training loss 0.0059519256465137005 Validation loss 0.011331713758409023 Accuracy 0.88037109375\n",
      "Iteration 24270 Training loss 0.0052511016838252544 Validation loss 0.01155292708426714 Accuracy 0.8779296875\n",
      "Iteration 24280 Training loss 0.005878634750843048 Validation loss 0.011467362754046917 Accuracy 0.87841796875\n",
      "Iteration 24290 Training loss 0.006556034553796053 Validation loss 0.011107392609119415 Accuracy 0.8828125\n",
      "Iteration 24300 Training loss 0.006433095782995224 Validation loss 0.011909694410860538 Accuracy 0.8740234375\n",
      "Iteration 24310 Training loss 0.004876669030636549 Validation loss 0.011190858669579029 Accuracy 0.88232421875\n",
      "Iteration 24320 Training loss 0.006349456496536732 Validation loss 0.011711006984114647 Accuracy 0.87744140625\n",
      "Iteration 24330 Training loss 0.006381376646459103 Validation loss 0.011255266144871712 Accuracy 0.880859375\n",
      "Iteration 24340 Training loss 0.0050158314406871796 Validation loss 0.011337419040501118 Accuracy 0.88037109375\n",
      "Iteration 24350 Training loss 0.006359962280839682 Validation loss 0.011160004884004593 Accuracy 0.8818359375\n",
      "Iteration 24360 Training loss 0.006078408565372229 Validation loss 0.01124454103410244 Accuracy 0.8818359375\n",
      "Iteration 24370 Training loss 0.005143311340361834 Validation loss 0.011175860650837421 Accuracy 0.8818359375\n",
      "Iteration 24380 Training loss 0.004692140966653824 Validation loss 0.010857343673706055 Accuracy 0.88623046875\n",
      "Iteration 24390 Training loss 0.005107086151838303 Validation loss 0.010933826677501202 Accuracy 0.884765625\n",
      "Iteration 24400 Training loss 0.0071876090951263905 Validation loss 0.011418454349040985 Accuracy 0.88037109375\n",
      "Iteration 24410 Training loss 0.005629583727568388 Validation loss 0.01110456045717001 Accuracy 0.880859375\n",
      "Iteration 24420 Training loss 0.0069784140214324 Validation loss 0.011559340171515942 Accuracy 0.8779296875\n",
      "Iteration 24430 Training loss 0.005936130881309509 Validation loss 0.011250772513449192 Accuracy 0.880859375\n",
      "Iteration 24440 Training loss 0.005385276861488819 Validation loss 0.011202231980860233 Accuracy 0.880859375\n",
      "Iteration 24450 Training loss 0.0034586472902446985 Validation loss 0.011250670999288559 Accuracy 0.8818359375\n",
      "Iteration 24460 Training loss 0.006620398722589016 Validation loss 0.011058148927986622 Accuracy 0.8828125\n",
      "Iteration 24470 Training loss 0.006630444899201393 Validation loss 0.011320756748318672 Accuracy 0.88037109375\n",
      "Iteration 24480 Training loss 0.005375719629228115 Validation loss 0.011251899413764477 Accuracy 0.880859375\n",
      "Iteration 24490 Training loss 0.006084328982979059 Validation loss 0.011305754072964191 Accuracy 0.88037109375\n",
      "Iteration 24500 Training loss 0.005001165438443422 Validation loss 0.011503363959491253 Accuracy 0.8779296875\n",
      "Iteration 24510 Training loss 0.004705867730081081 Validation loss 0.011132074519991875 Accuracy 0.8818359375\n",
      "Iteration 24520 Training loss 0.004893324337899685 Validation loss 0.010910222306847572 Accuracy 0.88427734375\n",
      "Iteration 24530 Training loss 0.006098881829530001 Validation loss 0.011230863630771637 Accuracy 0.880859375\n",
      "Iteration 24540 Training loss 0.007306706625968218 Validation loss 0.01096031442284584 Accuracy 0.8837890625\n",
      "Iteration 24550 Training loss 0.007234023418277502 Validation loss 0.010881594382226467 Accuracy 0.8837890625\n",
      "Iteration 24560 Training loss 0.00581017741933465 Validation loss 0.011154109612107277 Accuracy 0.880859375\n",
      "Iteration 24570 Training loss 0.00519912876188755 Validation loss 0.011261220090091228 Accuracy 0.88037109375\n",
      "Iteration 24580 Training loss 0.006934393662959337 Validation loss 0.011216940358281136 Accuracy 0.880859375\n",
      "Iteration 24590 Training loss 0.007538145408034325 Validation loss 0.011973625048995018 Accuracy 0.87255859375\n",
      "Iteration 24600 Training loss 0.005467874929308891 Validation loss 0.011123981326818466 Accuracy 0.8818359375\n",
      "Iteration 24610 Training loss 0.007115209475159645 Validation loss 0.011251840740442276 Accuracy 0.880859375\n",
      "Iteration 24620 Training loss 0.004677434451878071 Validation loss 0.011349259875714779 Accuracy 0.87890625\n",
      "Iteration 24630 Training loss 0.006456897594034672 Validation loss 0.01152073498815298 Accuracy 0.87890625\n",
      "Iteration 24640 Training loss 0.005051294341683388 Validation loss 0.011365729384124279 Accuracy 0.87939453125\n",
      "Iteration 24650 Training loss 0.005621397402137518 Validation loss 0.011439790017902851 Accuracy 0.87890625\n",
      "Iteration 24660 Training loss 0.00570332258939743 Validation loss 0.011264672502875328 Accuracy 0.880859375\n",
      "Iteration 24670 Training loss 0.004387849010527134 Validation loss 0.011243246495723724 Accuracy 0.88134765625\n",
      "Iteration 24680 Training loss 0.004713596310466528 Validation loss 0.011182179674506187 Accuracy 0.8818359375\n",
      "Iteration 24690 Training loss 0.0059768143109977245 Validation loss 0.010827950201928616 Accuracy 0.88525390625\n",
      "Iteration 24700 Training loss 0.006374781019985676 Validation loss 0.011497315019369125 Accuracy 0.87890625\n",
      "Iteration 24710 Training loss 0.005403994116932154 Validation loss 0.010956876911222935 Accuracy 0.884765625\n",
      "Iteration 24720 Training loss 0.0069570583291351795 Validation loss 0.01170971617102623 Accuracy 0.8759765625\n",
      "Iteration 24730 Training loss 0.00513302581384778 Validation loss 0.011206962168216705 Accuracy 0.88134765625\n",
      "Iteration 24740 Training loss 0.006398025434464216 Validation loss 0.011233515106141567 Accuracy 0.88134765625\n",
      "Iteration 24750 Training loss 0.00634120823815465 Validation loss 0.011496801860630512 Accuracy 0.87841796875\n",
      "Iteration 24760 Training loss 0.0066696940921247005 Validation loss 0.011293971911072731 Accuracy 0.88037109375\n",
      "Iteration 24770 Training loss 0.0068624489940702915 Validation loss 0.01143475342541933 Accuracy 0.87890625\n",
      "Iteration 24780 Training loss 0.004690384026616812 Validation loss 0.011169237084686756 Accuracy 0.8818359375\n",
      "Iteration 24790 Training loss 0.004987700842320919 Validation loss 0.011510219424962997 Accuracy 0.8779296875\n",
      "Iteration 24800 Training loss 0.006561496760696173 Validation loss 0.011158288456499577 Accuracy 0.880859375\n",
      "Iteration 24810 Training loss 0.004457561299204826 Validation loss 0.011249467730522156 Accuracy 0.8798828125\n",
      "Iteration 24820 Training loss 0.004281299654394388 Validation loss 0.011469386518001556 Accuracy 0.87744140625\n",
      "Iteration 24830 Training loss 0.005192657001316547 Validation loss 0.011403136886656284 Accuracy 0.87939453125\n",
      "Iteration 24840 Training loss 0.0049584731459617615 Validation loss 0.011020155623555183 Accuracy 0.88330078125\n",
      "Iteration 24850 Training loss 0.007756985258311033 Validation loss 0.01181689091026783 Accuracy 0.87451171875\n",
      "Iteration 24860 Training loss 0.0060426462441682816 Validation loss 0.011345912702381611 Accuracy 0.87939453125\n",
      "Iteration 24870 Training loss 0.0065607032738626 Validation loss 0.011302878148853779 Accuracy 0.8798828125\n",
      "Iteration 24880 Training loss 0.006784564815461636 Validation loss 0.011342334561049938 Accuracy 0.8798828125\n",
      "Iteration 24890 Training loss 0.0064873467199504375 Validation loss 0.012021845206618309 Accuracy 0.87255859375\n",
      "Iteration 24900 Training loss 0.00394819863140583 Validation loss 0.01116734929382801 Accuracy 0.88134765625\n",
      "Iteration 24910 Training loss 0.006282932125031948 Validation loss 0.011067809537053108 Accuracy 0.88330078125\n",
      "Iteration 24920 Training loss 0.005817696917802095 Validation loss 0.01136422622948885 Accuracy 0.87890625\n",
      "Iteration 24930 Training loss 0.005348519422113895 Validation loss 0.011214353144168854 Accuracy 0.88134765625\n",
      "Iteration 24940 Training loss 0.006258323788642883 Validation loss 0.011654943227767944 Accuracy 0.87646484375\n",
      "Iteration 24950 Training loss 0.005790241993963718 Validation loss 0.01099538803100586 Accuracy 0.88232421875\n",
      "Iteration 24960 Training loss 0.0064511289820075035 Validation loss 0.010946615599095821 Accuracy 0.884765625\n",
      "Iteration 24970 Training loss 0.005893938709050417 Validation loss 0.011129393242299557 Accuracy 0.8818359375\n",
      "Iteration 24980 Training loss 0.005687374621629715 Validation loss 0.011199058964848518 Accuracy 0.88134765625\n",
      "Iteration 24990 Training loss 0.007528623566031456 Validation loss 0.011249669827520847 Accuracy 0.880859375\n",
      "Iteration 25000 Training loss 0.006372474599629641 Validation loss 0.011518171988427639 Accuracy 0.87744140625\n",
      "Iteration 25010 Training loss 0.005812598392367363 Validation loss 0.010896340012550354 Accuracy 0.8837890625\n",
      "Iteration 25020 Training loss 0.005201444029808044 Validation loss 0.011020154692232609 Accuracy 0.8828125\n",
      "Iteration 25030 Training loss 0.005161607638001442 Validation loss 0.011217092163860798 Accuracy 0.8798828125\n",
      "Iteration 25040 Training loss 0.00522985216230154 Validation loss 0.01132748182862997 Accuracy 0.8798828125\n",
      "Iteration 25050 Training loss 0.005143207497894764 Validation loss 0.010864474810659885 Accuracy 0.88427734375\n",
      "Iteration 25060 Training loss 0.004263146780431271 Validation loss 0.01152748428285122 Accuracy 0.87744140625\n",
      "Iteration 25070 Training loss 0.005575637798756361 Validation loss 0.011916354298591614 Accuracy 0.8740234375\n",
      "Iteration 25080 Training loss 0.005352507345378399 Validation loss 0.011151948943734169 Accuracy 0.8818359375\n",
      "Iteration 25090 Training loss 0.007237199228256941 Validation loss 0.011078915558755398 Accuracy 0.88232421875\n",
      "Iteration 25100 Training loss 0.006141869816929102 Validation loss 0.011096864007413387 Accuracy 0.88330078125\n",
      "Iteration 25110 Training loss 0.005540969781577587 Validation loss 0.01104117650538683 Accuracy 0.88330078125\n",
      "Iteration 25120 Training loss 0.005363295786082745 Validation loss 0.010961120016872883 Accuracy 0.88330078125\n",
      "Iteration 25130 Training loss 0.0031245702411979437 Validation loss 0.011252478696405888 Accuracy 0.880859375\n",
      "Iteration 25140 Training loss 0.00584949366748333 Validation loss 0.011077575385570526 Accuracy 0.8818359375\n",
      "Iteration 25150 Training loss 0.005403559189289808 Validation loss 0.010997326113283634 Accuracy 0.8837890625\n",
      "Iteration 25160 Training loss 0.005666601937264204 Validation loss 0.0118342200294137 Accuracy 0.87451171875\n",
      "Iteration 25170 Training loss 0.005334780085831881 Validation loss 0.011360883712768555 Accuracy 0.8798828125\n",
      "Iteration 25180 Training loss 0.005386426113545895 Validation loss 0.011340227909386158 Accuracy 0.87939453125\n",
      "Iteration 25190 Training loss 0.0068260664120316505 Validation loss 0.011340086348354816 Accuracy 0.8798828125\n",
      "Iteration 25200 Training loss 0.005912848748266697 Validation loss 0.011365678161382675 Accuracy 0.87939453125\n",
      "Iteration 25210 Training loss 0.004970676265656948 Validation loss 0.01136048510670662 Accuracy 0.87841796875\n",
      "Iteration 25220 Training loss 0.005916281137615442 Validation loss 0.011178278364241123 Accuracy 0.8818359375\n",
      "Iteration 25230 Training loss 0.005204773973673582 Validation loss 0.012113810516893864 Accuracy 0.8720703125\n",
      "Iteration 25240 Training loss 0.004768876358866692 Validation loss 0.01114142406731844 Accuracy 0.88330078125\n",
      "Iteration 25250 Training loss 0.007157471030950546 Validation loss 0.011250726878643036 Accuracy 0.8818359375\n",
      "Iteration 25260 Training loss 0.004990353714674711 Validation loss 0.011426370590925217 Accuracy 0.87939453125\n",
      "Iteration 25270 Training loss 0.006404608488082886 Validation loss 0.011563141830265522 Accuracy 0.8779296875\n",
      "Iteration 25280 Training loss 0.004443135112524033 Validation loss 0.011299023404717445 Accuracy 0.88134765625\n",
      "Iteration 25290 Training loss 0.006757747381925583 Validation loss 0.011300874873995781 Accuracy 0.88037109375\n",
      "Iteration 25300 Training loss 0.004425133112818003 Validation loss 0.011265002191066742 Accuracy 0.88037109375\n",
      "Iteration 25310 Training loss 0.0048441109247505665 Validation loss 0.011090031825006008 Accuracy 0.8828125\n",
      "Iteration 25320 Training loss 0.007293534930795431 Validation loss 0.010879768058657646 Accuracy 0.88525390625\n",
      "Iteration 25330 Training loss 0.00572267547249794 Validation loss 0.011220204643905163 Accuracy 0.8818359375\n",
      "Iteration 25340 Training loss 0.004110709298402071 Validation loss 0.011122814379632473 Accuracy 0.8828125\n",
      "Iteration 25350 Training loss 0.004745895508676767 Validation loss 0.011193771846592426 Accuracy 0.8818359375\n",
      "Iteration 25360 Training loss 0.004625602625310421 Validation loss 0.010953725315630436 Accuracy 0.88427734375\n",
      "Iteration 25370 Training loss 0.005491004791110754 Validation loss 0.011294455267488956 Accuracy 0.880859375\n",
      "Iteration 25380 Training loss 0.005148274824023247 Validation loss 0.01135250460356474 Accuracy 0.8798828125\n",
      "Iteration 25390 Training loss 0.006250485777854919 Validation loss 0.011068466119468212 Accuracy 0.8828125\n",
      "Iteration 25400 Training loss 0.005206177476793528 Validation loss 0.011273227632045746 Accuracy 0.880859375\n",
      "Iteration 25410 Training loss 0.006654779426753521 Validation loss 0.01134247612208128 Accuracy 0.88037109375\n",
      "Iteration 25420 Training loss 0.005071216728538275 Validation loss 0.011544787324965 Accuracy 0.87744140625\n",
      "Iteration 25430 Training loss 0.004198166541755199 Validation loss 0.01134578324854374 Accuracy 0.87939453125\n",
      "Iteration 25440 Training loss 0.004843488801270723 Validation loss 0.011750133708119392 Accuracy 0.87451171875\n",
      "Iteration 25450 Training loss 0.0077122291550040245 Validation loss 0.011586007662117481 Accuracy 0.876953125\n",
      "Iteration 25460 Training loss 0.004576416220515966 Validation loss 0.011207750998437405 Accuracy 0.880859375\n",
      "Iteration 25470 Training loss 0.004688891116529703 Validation loss 0.011004908010363579 Accuracy 0.8828125\n",
      "Iteration 25480 Training loss 0.0047404649667441845 Validation loss 0.010985798202455044 Accuracy 0.8828125\n",
      "Iteration 25490 Training loss 0.005313475616276264 Validation loss 0.01116293203085661 Accuracy 0.88037109375\n",
      "Iteration 25500 Training loss 0.007574823219329119 Validation loss 0.011430444195866585 Accuracy 0.8798828125\n",
      "Iteration 25510 Training loss 0.005887867417186499 Validation loss 0.011178957298398018 Accuracy 0.8818359375\n",
      "Iteration 25520 Training loss 0.005171956494450569 Validation loss 0.010913505218923092 Accuracy 0.88427734375\n",
      "Iteration 25530 Training loss 0.005009529180824757 Validation loss 0.011064340360462666 Accuracy 0.88232421875\n",
      "Iteration 25540 Training loss 0.006971403490751982 Validation loss 0.01147589460015297 Accuracy 0.8779296875\n",
      "Iteration 25550 Training loss 0.005972057115286589 Validation loss 0.011288043111562729 Accuracy 0.8798828125\n",
      "Iteration 25560 Training loss 0.005165891721844673 Validation loss 0.011239930987358093 Accuracy 0.88134765625\n",
      "Iteration 25570 Training loss 0.006078050471842289 Validation loss 0.01115898322314024 Accuracy 0.88134765625\n",
      "Iteration 25580 Training loss 0.005242117680609226 Validation loss 0.01095866784453392 Accuracy 0.8837890625\n",
      "Iteration 25590 Training loss 0.006490897387266159 Validation loss 0.011137858964502811 Accuracy 0.8818359375\n",
      "Iteration 25600 Training loss 0.004758454393595457 Validation loss 0.011024235747754574 Accuracy 0.8818359375\n",
      "Iteration 25610 Training loss 0.006550464779138565 Validation loss 0.011048915795981884 Accuracy 0.88232421875\n",
      "Iteration 25620 Training loss 0.005693239625543356 Validation loss 0.011000175960361958 Accuracy 0.88330078125\n",
      "Iteration 25630 Training loss 0.006360976956784725 Validation loss 0.010835091583430767 Accuracy 0.88427734375\n",
      "Iteration 25640 Training loss 0.005718565545976162 Validation loss 0.011215523816645145 Accuracy 0.8798828125\n",
      "Iteration 25650 Training loss 0.004883980844169855 Validation loss 0.011080966331064701 Accuracy 0.8818359375\n",
      "Iteration 25660 Training loss 0.006239467766135931 Validation loss 0.011225695721805096 Accuracy 0.88037109375\n",
      "Iteration 25670 Training loss 0.005310805048793554 Validation loss 0.010907917283475399 Accuracy 0.8837890625\n",
      "Iteration 25680 Training loss 0.0059577603824436665 Validation loss 0.011557218618690968 Accuracy 0.876953125\n",
      "Iteration 25690 Training loss 0.005767494905740023 Validation loss 0.011411735787987709 Accuracy 0.87841796875\n",
      "Iteration 25700 Training loss 0.008686322718858719 Validation loss 0.011248796246945858 Accuracy 0.880859375\n",
      "Iteration 25710 Training loss 0.0072362045757472515 Validation loss 0.011104545556008816 Accuracy 0.8828125\n",
      "Iteration 25720 Training loss 0.004662193823605776 Validation loss 0.011301998980343342 Accuracy 0.8798828125\n",
      "Iteration 25730 Training loss 0.006207944359630346 Validation loss 0.012193085625767708 Accuracy 0.8701171875\n",
      "Iteration 25740 Training loss 0.005679024383425713 Validation loss 0.011097900569438934 Accuracy 0.8818359375\n",
      "Iteration 25750 Training loss 0.004614075180143118 Validation loss 0.011132831685245037 Accuracy 0.88232421875\n",
      "Iteration 25760 Training loss 0.005683566443622112 Validation loss 0.01104439701884985 Accuracy 0.88330078125\n",
      "Iteration 25770 Training loss 0.007741156034171581 Validation loss 0.011660926043987274 Accuracy 0.876953125\n",
      "Iteration 25780 Training loss 0.005223811138421297 Validation loss 0.01092491764575243 Accuracy 0.88330078125\n",
      "Iteration 25790 Training loss 0.006336522288620472 Validation loss 0.011233210563659668 Accuracy 0.880859375\n",
      "Iteration 25800 Training loss 0.005423230584710836 Validation loss 0.011585223488509655 Accuracy 0.87890625\n",
      "Iteration 25810 Training loss 0.0053069400601089 Validation loss 0.01092914491891861 Accuracy 0.88427734375\n",
      "Iteration 25820 Training loss 0.00593979237601161 Validation loss 0.011164813302457333 Accuracy 0.88134765625\n",
      "Iteration 25830 Training loss 0.006608627270907164 Validation loss 0.011214611120522022 Accuracy 0.88037109375\n",
      "Iteration 25840 Training loss 0.005395283456891775 Validation loss 0.011462738737463951 Accuracy 0.8779296875\n",
      "Iteration 25850 Training loss 0.00550390500575304 Validation loss 0.011051804758608341 Accuracy 0.8818359375\n",
      "Iteration 25860 Training loss 0.0049636103212833405 Validation loss 0.011079523712396622 Accuracy 0.88330078125\n",
      "Iteration 25870 Training loss 0.00479381438344717 Validation loss 0.010934868827462196 Accuracy 0.8828125\n",
      "Iteration 25880 Training loss 0.005202895496040583 Validation loss 0.010883975774049759 Accuracy 0.884765625\n",
      "Iteration 25890 Training loss 0.0049512977711856365 Validation loss 0.010916030965745449 Accuracy 0.88330078125\n",
      "Iteration 25900 Training loss 0.005549461115151644 Validation loss 0.010928820818662643 Accuracy 0.8837890625\n",
      "Iteration 25910 Training loss 0.0046484763734042645 Validation loss 0.010838341899216175 Accuracy 0.88525390625\n",
      "Iteration 25920 Training loss 0.004641344770789146 Validation loss 0.01102191861718893 Accuracy 0.88330078125\n",
      "Iteration 25930 Training loss 0.002886800328269601 Validation loss 0.010960434563457966 Accuracy 0.8837890625\n",
      "Iteration 25940 Training loss 0.004673364106565714 Validation loss 0.011259675025939941 Accuracy 0.8798828125\n",
      "Iteration 25950 Training loss 0.004886422771960497 Validation loss 0.01127680018544197 Accuracy 0.87890625\n",
      "Iteration 25960 Training loss 0.006545765325427055 Validation loss 0.011181039735674858 Accuracy 0.88037109375\n",
      "Iteration 25970 Training loss 0.005239682272076607 Validation loss 0.01093112863600254 Accuracy 0.8837890625\n",
      "Iteration 25980 Training loss 0.004520911257714033 Validation loss 0.010932750068604946 Accuracy 0.8837890625\n",
      "Iteration 25990 Training loss 0.004779407288879156 Validation loss 0.011195268481969833 Accuracy 0.88134765625\n",
      "Iteration 26000 Training loss 0.005349168088287115 Validation loss 0.01106173824518919 Accuracy 0.88232421875\n",
      "Iteration 26010 Training loss 0.006670372094959021 Validation loss 0.012107728980481625 Accuracy 0.87255859375\n",
      "Iteration 26020 Training loss 0.005841778591275215 Validation loss 0.011413670144975185 Accuracy 0.87841796875\n",
      "Iteration 26030 Training loss 0.004814543295651674 Validation loss 0.011244622059166431 Accuracy 0.88037109375\n",
      "Iteration 26040 Training loss 0.0056967418640851974 Validation loss 0.011079628951847553 Accuracy 0.8818359375\n",
      "Iteration 26050 Training loss 0.0052225785329937935 Validation loss 0.011811915785074234 Accuracy 0.87451171875\n",
      "Iteration 26060 Training loss 0.0051148422062397 Validation loss 0.011514213867485523 Accuracy 0.87841796875\n",
      "Iteration 26070 Training loss 0.004699349869042635 Validation loss 0.010955854319036007 Accuracy 0.88330078125\n",
      "Iteration 26080 Training loss 0.006000572349876165 Validation loss 0.01122818049043417 Accuracy 0.880859375\n",
      "Iteration 26090 Training loss 0.005173432175070047 Validation loss 0.011257362551987171 Accuracy 0.880859375\n",
      "Iteration 26100 Training loss 0.004693891387432814 Validation loss 0.011377746239304543 Accuracy 0.87939453125\n",
      "Iteration 26110 Training loss 0.004803462885320187 Validation loss 0.011311247013509274 Accuracy 0.87890625\n",
      "Iteration 26120 Training loss 0.0056985169649124146 Validation loss 0.01113925501704216 Accuracy 0.88232421875\n",
      "Iteration 26130 Training loss 0.003786541521549225 Validation loss 0.011175709776580334 Accuracy 0.88134765625\n",
      "Iteration 26140 Training loss 0.005770216695964336 Validation loss 0.011321243830025196 Accuracy 0.8798828125\n",
      "Iteration 26150 Training loss 0.006366393994539976 Validation loss 0.011412855237722397 Accuracy 0.87939453125\n",
      "Iteration 26160 Training loss 0.006235078908503056 Validation loss 0.011517436243593693 Accuracy 0.87744140625\n",
      "Iteration 26170 Training loss 0.005567519925534725 Validation loss 0.011060371063649654 Accuracy 0.8828125\n",
      "Iteration 26180 Training loss 0.00487801618874073 Validation loss 0.010997067205607891 Accuracy 0.88232421875\n",
      "Iteration 26190 Training loss 0.007212381809949875 Validation loss 0.011869979090988636 Accuracy 0.87255859375\n",
      "Iteration 26200 Training loss 0.005756725091487169 Validation loss 0.011708999052643776 Accuracy 0.875\n",
      "Iteration 26210 Training loss 0.0043949284590780735 Validation loss 0.010839594528079033 Accuracy 0.884765625\n",
      "Iteration 26220 Training loss 0.0035611381754279137 Validation loss 0.01086411066353321 Accuracy 0.8837890625\n",
      "Iteration 26230 Training loss 0.004852428566664457 Validation loss 0.011061306111514568 Accuracy 0.88330078125\n",
      "Iteration 26240 Training loss 0.005589671898633242 Validation loss 0.011142644099891186 Accuracy 0.880859375\n",
      "Iteration 26250 Training loss 0.005856398027390242 Validation loss 0.010986877605319023 Accuracy 0.8837890625\n",
      "Iteration 26260 Training loss 0.004433934576809406 Validation loss 0.011140938848257065 Accuracy 0.880859375\n",
      "Iteration 26270 Training loss 0.006011616438627243 Validation loss 0.010876638814806938 Accuracy 0.88427734375\n",
      "Iteration 26280 Training loss 0.0053406828083097935 Validation loss 0.011171817779541016 Accuracy 0.88134765625\n",
      "Iteration 26290 Training loss 0.003331353422254324 Validation loss 0.011308128014206886 Accuracy 0.87939453125\n",
      "Iteration 26300 Training loss 0.008427945896983147 Validation loss 0.011318236589431763 Accuracy 0.8798828125\n",
      "Iteration 26310 Training loss 0.007620662450790405 Validation loss 0.011141443625092506 Accuracy 0.8818359375\n",
      "Iteration 26320 Training loss 0.006797126494348049 Validation loss 0.011182731948792934 Accuracy 0.88037109375\n",
      "Iteration 26330 Training loss 0.0061798300594091415 Validation loss 0.011286723427474499 Accuracy 0.8798828125\n",
      "Iteration 26340 Training loss 0.0050215995870530605 Validation loss 0.011398936621844769 Accuracy 0.87744140625\n",
      "Iteration 26350 Training loss 0.005323693621903658 Validation loss 0.011195468716323376 Accuracy 0.8798828125\n",
      "Iteration 26360 Training loss 0.004961507394909859 Validation loss 0.011187968775629997 Accuracy 0.880859375\n",
      "Iteration 26370 Training loss 0.004943726118654013 Validation loss 0.011540137231349945 Accuracy 0.876953125\n",
      "Iteration 26380 Training loss 0.006112988572567701 Validation loss 0.011080997996032238 Accuracy 0.88232421875\n",
      "Iteration 26390 Training loss 0.005097391549497843 Validation loss 0.011430683545768261 Accuracy 0.87841796875\n",
      "Iteration 26400 Training loss 0.007065360434353352 Validation loss 0.011772455647587776 Accuracy 0.87548828125\n",
      "Iteration 26410 Training loss 0.007034109905362129 Validation loss 0.01185954175889492 Accuracy 0.87353515625\n",
      "Iteration 26420 Training loss 0.004274987615644932 Validation loss 0.011066201142966747 Accuracy 0.88330078125\n",
      "Iteration 26430 Training loss 0.004919034894555807 Validation loss 0.011898425407707691 Accuracy 0.87353515625\n",
      "Iteration 26440 Training loss 0.00426126504316926 Validation loss 0.011066913604736328 Accuracy 0.8828125\n",
      "Iteration 26450 Training loss 0.0050368355587124825 Validation loss 0.010944782756268978 Accuracy 0.8837890625\n",
      "Iteration 26460 Training loss 0.003908409271389246 Validation loss 0.011206689290702343 Accuracy 0.880859375\n",
      "Iteration 26470 Training loss 0.006517508532851934 Validation loss 0.011129946447908878 Accuracy 0.8818359375\n",
      "Iteration 26480 Training loss 0.005975072272121906 Validation loss 0.011092898435890675 Accuracy 0.88134765625\n",
      "Iteration 26490 Training loss 0.007658636197447777 Validation loss 0.01162275392562151 Accuracy 0.875\n",
      "Iteration 26500 Training loss 0.005411970894783735 Validation loss 0.010874217376112938 Accuracy 0.88525390625\n",
      "Iteration 26510 Training loss 0.006285542622208595 Validation loss 0.011132277548313141 Accuracy 0.88134765625\n",
      "Iteration 26520 Training loss 0.006442971061915159 Validation loss 0.011190295219421387 Accuracy 0.88037109375\n",
      "Iteration 26530 Training loss 0.005426185671240091 Validation loss 0.011123810894787312 Accuracy 0.880859375\n",
      "Iteration 26540 Training loss 0.005539478734135628 Validation loss 0.011117157526314259 Accuracy 0.88037109375\n",
      "Iteration 26550 Training loss 0.006138913333415985 Validation loss 0.011281827464699745 Accuracy 0.87939453125\n",
      "Iteration 26560 Training loss 0.004705038852989674 Validation loss 0.011069275438785553 Accuracy 0.88330078125\n",
      "Iteration 26570 Training loss 0.0055884006433188915 Validation loss 0.011570102535188198 Accuracy 0.8779296875\n",
      "Iteration 26580 Training loss 0.004799301270395517 Validation loss 0.011275463737547398 Accuracy 0.880859375\n",
      "Iteration 26590 Training loss 0.005234428681433201 Validation loss 0.011014132760465145 Accuracy 0.88330078125\n",
      "Iteration 26600 Training loss 0.004423188976943493 Validation loss 0.011148888617753983 Accuracy 0.8818359375\n",
      "Iteration 26610 Training loss 0.0053662522695958614 Validation loss 0.011947043240070343 Accuracy 0.87353515625\n",
      "Iteration 26620 Training loss 0.005244264379143715 Validation loss 0.01114291325211525 Accuracy 0.88134765625\n",
      "Iteration 26630 Training loss 0.005721372086554766 Validation loss 0.011548295617103577 Accuracy 0.87744140625\n",
      "Iteration 26640 Training loss 0.006223181262612343 Validation loss 0.010978435166180134 Accuracy 0.88330078125\n",
      "Iteration 26650 Training loss 0.005184420850127935 Validation loss 0.011186005547642708 Accuracy 0.880859375\n",
      "Iteration 26660 Training loss 0.004155660048127174 Validation loss 0.011230690404772758 Accuracy 0.880859375\n",
      "Iteration 26670 Training loss 0.004797295201569796 Validation loss 0.011290272697806358 Accuracy 0.87939453125\n",
      "Iteration 26680 Training loss 0.004770590458065271 Validation loss 0.011385010555386543 Accuracy 0.87841796875\n",
      "Iteration 26690 Training loss 0.005952535662800074 Validation loss 0.01135337632149458 Accuracy 0.87939453125\n",
      "Iteration 26700 Training loss 0.005431478377431631 Validation loss 0.011691935360431671 Accuracy 0.8759765625\n",
      "Iteration 26710 Training loss 0.003603110322728753 Validation loss 0.011023161001503468 Accuracy 0.88330078125\n",
      "Iteration 26720 Training loss 0.006787446793168783 Validation loss 0.011798310093581676 Accuracy 0.87451171875\n",
      "Iteration 26730 Training loss 0.007615386042743921 Validation loss 0.011051176115870476 Accuracy 0.8828125\n",
      "Iteration 26740 Training loss 0.004404344130307436 Validation loss 0.011212228797376156 Accuracy 0.88037109375\n",
      "Iteration 26750 Training loss 0.004830149933695793 Validation loss 0.01119996514171362 Accuracy 0.88134765625\n",
      "Iteration 26760 Training loss 0.0059267678298056126 Validation loss 0.010880974121391773 Accuracy 0.8837890625\n",
      "Iteration 26770 Training loss 0.004239608068019152 Validation loss 0.011011655442416668 Accuracy 0.88330078125\n",
      "Iteration 26780 Training loss 0.006265608593821526 Validation loss 0.011213457211852074 Accuracy 0.87939453125\n",
      "Iteration 26790 Training loss 0.004752474371343851 Validation loss 0.010767311789095402 Accuracy 0.8857421875\n",
      "Iteration 26800 Training loss 0.004697691183537245 Validation loss 0.011776800267398357 Accuracy 0.87548828125\n",
      "Iteration 26810 Training loss 0.004457315895706415 Validation loss 0.010918735526502132 Accuracy 0.88427734375\n",
      "Iteration 26820 Training loss 0.006168060004711151 Validation loss 0.01109224371612072 Accuracy 0.88232421875\n",
      "Iteration 26830 Training loss 0.00661698542535305 Validation loss 0.010917961597442627 Accuracy 0.88427734375\n",
      "Iteration 26840 Training loss 0.0064738779328763485 Validation loss 0.010850315913558006 Accuracy 0.88525390625\n",
      "Iteration 26850 Training loss 0.005506459623575211 Validation loss 0.010834179818630219 Accuracy 0.88427734375\n",
      "Iteration 26860 Training loss 0.004544908180832863 Validation loss 0.010954523459076881 Accuracy 0.88330078125\n",
      "Iteration 26870 Training loss 0.00404321076348424 Validation loss 0.010952741838991642 Accuracy 0.88427734375\n",
      "Iteration 26880 Training loss 0.005955576431006193 Validation loss 0.01110438909381628 Accuracy 0.8828125\n",
      "Iteration 26890 Training loss 0.005757007747888565 Validation loss 0.010824843309819698 Accuracy 0.8857421875\n",
      "Iteration 26900 Training loss 0.005885799881070852 Validation loss 0.010801387950778008 Accuracy 0.884765625\n",
      "Iteration 26910 Training loss 0.003625662764534354 Validation loss 0.010888608172535896 Accuracy 0.88427734375\n",
      "Iteration 26920 Training loss 0.005448435433208942 Validation loss 0.010702180676162243 Accuracy 0.88623046875\n",
      "Iteration 26930 Training loss 0.004670523572713137 Validation loss 0.011225228197872639 Accuracy 0.880859375\n",
      "Iteration 26940 Training loss 0.00635526655241847 Validation loss 0.011310659348964691 Accuracy 0.8798828125\n",
      "Iteration 26950 Training loss 0.004254254978150129 Validation loss 0.010821818374097347 Accuracy 0.884765625\n",
      "Iteration 26960 Training loss 0.006223613861948252 Validation loss 0.011018476448953152 Accuracy 0.88330078125\n",
      "Iteration 26970 Training loss 0.005465326365083456 Validation loss 0.011132234707474709 Accuracy 0.880859375\n",
      "Iteration 26980 Training loss 0.004806065931916237 Validation loss 0.011069882661104202 Accuracy 0.88232421875\n",
      "Iteration 26990 Training loss 0.006174550857394934 Validation loss 0.011331393383443356 Accuracy 0.8798828125\n",
      "Iteration 27000 Training loss 0.004043081775307655 Validation loss 0.011375895701348782 Accuracy 0.87890625\n",
      "Iteration 27010 Training loss 0.0036227498203516006 Validation loss 0.011232323944568634 Accuracy 0.88134765625\n",
      "Iteration 27020 Training loss 0.007316750939935446 Validation loss 0.011655513197183609 Accuracy 0.87451171875\n",
      "Iteration 27030 Training loss 0.005680416245013475 Validation loss 0.011230438016355038 Accuracy 0.88134765625\n",
      "Iteration 27040 Training loss 0.0054906392470002174 Validation loss 0.011038051918148994 Accuracy 0.88232421875\n",
      "Iteration 27050 Training loss 0.005374914035201073 Validation loss 0.011346293613314629 Accuracy 0.87939453125\n",
      "Iteration 27060 Training loss 0.004872848745435476 Validation loss 0.010876083746552467 Accuracy 0.88427734375\n",
      "Iteration 27070 Training loss 0.006061345338821411 Validation loss 0.010996844619512558 Accuracy 0.8828125\n",
      "Iteration 27080 Training loss 0.005417818669229746 Validation loss 0.011091078631579876 Accuracy 0.88232421875\n",
      "Iteration 27090 Training loss 0.00513048842549324 Validation loss 0.011384009383618832 Accuracy 0.87890625\n",
      "Iteration 27100 Training loss 0.0042753443121910095 Validation loss 0.011224107816815376 Accuracy 0.88037109375\n",
      "Iteration 27110 Training loss 0.0061539942398667336 Validation loss 0.011227519251406193 Accuracy 0.880859375\n",
      "Iteration 27120 Training loss 0.005360075738281012 Validation loss 0.01106232963502407 Accuracy 0.88232421875\n",
      "Iteration 27130 Training loss 0.005106134805828333 Validation loss 0.011134248226881027 Accuracy 0.8818359375\n",
      "Iteration 27140 Training loss 0.007028433494269848 Validation loss 0.012339483015239239 Accuracy 0.86962890625\n",
      "Iteration 27150 Training loss 0.004150213673710823 Validation loss 0.010780123993754387 Accuracy 0.88427734375\n",
      "Iteration 27160 Training loss 0.0056368084624409676 Validation loss 0.01111684087663889 Accuracy 0.8818359375\n",
      "Iteration 27170 Training loss 0.003547891043126583 Validation loss 0.010864026844501495 Accuracy 0.884765625\n",
      "Iteration 27180 Training loss 0.002749307546764612 Validation loss 0.011048378422856331 Accuracy 0.8818359375\n",
      "Iteration 27190 Training loss 0.00540131377056241 Validation loss 0.011053097434341908 Accuracy 0.88232421875\n",
      "Iteration 27200 Training loss 0.0038916263729333878 Validation loss 0.010987850837409496 Accuracy 0.88330078125\n",
      "Iteration 27210 Training loss 0.005352489184588194 Validation loss 0.011209296993911266 Accuracy 0.8798828125\n",
      "Iteration 27220 Training loss 0.005256050731986761 Validation loss 0.011115597561001778 Accuracy 0.88330078125\n",
      "Iteration 27230 Training loss 0.005541648715734482 Validation loss 0.010898921638727188 Accuracy 0.88427734375\n",
      "Iteration 27240 Training loss 0.0046874526888132095 Validation loss 0.011208419688045979 Accuracy 0.88134765625\n",
      "Iteration 27250 Training loss 0.00525695038959384 Validation loss 0.010832848958671093 Accuracy 0.884765625\n",
      "Iteration 27260 Training loss 0.005780231207609177 Validation loss 0.011212844401597977 Accuracy 0.88037109375\n",
      "Iteration 27270 Training loss 0.005729508586227894 Validation loss 0.011359131895005703 Accuracy 0.8798828125\n",
      "Iteration 27280 Training loss 0.005358690395951271 Validation loss 0.010939416475594044 Accuracy 0.88427734375\n",
      "Iteration 27290 Training loss 0.005854861345142126 Validation loss 0.011313016526401043 Accuracy 0.8798828125\n",
      "Iteration 27300 Training loss 0.00635361485183239 Validation loss 0.01208356861025095 Accuracy 0.8720703125\n",
      "Iteration 27310 Training loss 0.004889746196568012 Validation loss 0.011041130870580673 Accuracy 0.88232421875\n",
      "Iteration 27320 Training loss 0.0047821709886193275 Validation loss 0.011360586620867252 Accuracy 0.87939453125\n",
      "Iteration 27330 Training loss 0.004295523278415203 Validation loss 0.01107199676334858 Accuracy 0.88232421875\n",
      "Iteration 27340 Training loss 0.005111014470458031 Validation loss 0.011295346543192863 Accuracy 0.88037109375\n",
      "Iteration 27350 Training loss 0.004597911611199379 Validation loss 0.011341147124767303 Accuracy 0.87939453125\n",
      "Iteration 27360 Training loss 0.00632822047919035 Validation loss 0.01196176279336214 Accuracy 0.8740234375\n",
      "Iteration 27370 Training loss 0.004603330045938492 Validation loss 0.011000003665685654 Accuracy 0.8828125\n",
      "Iteration 27380 Training loss 0.003877770621329546 Validation loss 0.010993754491209984 Accuracy 0.8837890625\n",
      "Iteration 27390 Training loss 0.0051673599518835545 Validation loss 0.01089017279446125 Accuracy 0.884765625\n",
      "Iteration 27400 Training loss 0.0062776184640824795 Validation loss 0.011085270904004574 Accuracy 0.88134765625\n",
      "Iteration 27410 Training loss 0.005701085552573204 Validation loss 0.012014348059892654 Accuracy 0.8720703125\n",
      "Iteration 27420 Training loss 0.0065283579751849174 Validation loss 0.01117488369345665 Accuracy 0.88037109375\n",
      "Iteration 27430 Training loss 0.005520826205611229 Validation loss 0.010920969769358635 Accuracy 0.8837890625\n",
      "Iteration 27440 Training loss 0.004792462568730116 Validation loss 0.01106999721378088 Accuracy 0.8818359375\n",
      "Iteration 27450 Training loss 0.006613409146666527 Validation loss 0.011157792992889881 Accuracy 0.8818359375\n",
      "Iteration 27460 Training loss 0.004457365721464157 Validation loss 0.011219008825719357 Accuracy 0.88037109375\n",
      "Iteration 27470 Training loss 0.004604043439030647 Validation loss 0.010810856707394123 Accuracy 0.88525390625\n",
      "Iteration 27480 Training loss 0.006729340646415949 Validation loss 0.01146544050425291 Accuracy 0.8779296875\n",
      "Iteration 27490 Training loss 0.0044885254465043545 Validation loss 0.010883903130888939 Accuracy 0.884765625\n",
      "Iteration 27500 Training loss 0.004241100512444973 Validation loss 0.010866941884160042 Accuracy 0.88427734375\n",
      "Iteration 27510 Training loss 0.007794239092618227 Validation loss 0.011388509534299374 Accuracy 0.87841796875\n",
      "Iteration 27520 Training loss 0.003915691748261452 Validation loss 0.010976816527545452 Accuracy 0.8837890625\n",
      "Iteration 27530 Training loss 0.005472322925925255 Validation loss 0.011446710675954819 Accuracy 0.8779296875\n",
      "Iteration 27540 Training loss 0.005256186239421368 Validation loss 0.0112908398732543 Accuracy 0.87939453125\n",
      "Iteration 27550 Training loss 0.004151526372879744 Validation loss 0.011005544103682041 Accuracy 0.88232421875\n",
      "Iteration 27560 Training loss 0.005835341289639473 Validation loss 0.011228090152144432 Accuracy 0.88232421875\n",
      "Iteration 27570 Training loss 0.005724437069147825 Validation loss 0.011000090278685093 Accuracy 0.88232421875\n",
      "Iteration 27580 Training loss 0.005856883712112904 Validation loss 0.011162499897181988 Accuracy 0.88037109375\n",
      "Iteration 27590 Training loss 0.004410797730088234 Validation loss 0.011299439705908298 Accuracy 0.87939453125\n",
      "Iteration 27600 Training loss 0.005565624218434095 Validation loss 0.010929304175078869 Accuracy 0.8837890625\n",
      "Iteration 27610 Training loss 0.004124315455555916 Validation loss 0.01089389342814684 Accuracy 0.88330078125\n",
      "Iteration 27620 Training loss 0.005253514740616083 Validation loss 0.011051164008677006 Accuracy 0.88330078125\n",
      "Iteration 27630 Training loss 0.006374063901603222 Validation loss 0.011945605278015137 Accuracy 0.873046875\n",
      "Iteration 27640 Training loss 0.005329711362719536 Validation loss 0.011268211528658867 Accuracy 0.880859375\n",
      "Iteration 27650 Training loss 0.004259663168340921 Validation loss 0.011256217025220394 Accuracy 0.87939453125\n",
      "Iteration 27660 Training loss 0.0037612037267535925 Validation loss 0.010773449204862118 Accuracy 0.88525390625\n",
      "Iteration 27670 Training loss 0.0036457062233239412 Validation loss 0.01072259247303009 Accuracy 0.88525390625\n",
      "Iteration 27680 Training loss 0.006772899534553289 Validation loss 0.011214192025363445 Accuracy 0.88037109375\n",
      "Iteration 27690 Training loss 0.003748226212337613 Validation loss 0.011019157245755196 Accuracy 0.88232421875\n",
      "Iteration 27700 Training loss 0.005580116994678974 Validation loss 0.01149632129818201 Accuracy 0.87841796875\n",
      "Iteration 27710 Training loss 0.003924737684428692 Validation loss 0.011003187857568264 Accuracy 0.8837890625\n",
      "Iteration 27720 Training loss 0.0051092179492115974 Validation loss 0.010987192392349243 Accuracy 0.88232421875\n",
      "Iteration 27730 Training loss 0.005013908259570599 Validation loss 0.011298705823719501 Accuracy 0.87890625\n",
      "Iteration 27740 Training loss 0.004851021803915501 Validation loss 0.010982717387378216 Accuracy 0.8828125\n",
      "Iteration 27750 Training loss 0.005994326435029507 Validation loss 0.011205244809389114 Accuracy 0.8798828125\n",
      "Iteration 27760 Training loss 0.005382436793297529 Validation loss 0.010918458923697472 Accuracy 0.88330078125\n",
      "Iteration 27770 Training loss 0.005357707850635052 Validation loss 0.01154684741050005 Accuracy 0.87646484375\n",
      "Iteration 27780 Training loss 0.005860883742570877 Validation loss 0.011013959534466267 Accuracy 0.88232421875\n",
      "Iteration 27790 Training loss 0.004486911930143833 Validation loss 0.011073002591729164 Accuracy 0.8818359375\n",
      "Iteration 27800 Training loss 0.004481895361095667 Validation loss 0.011067553423345089 Accuracy 0.88232421875\n",
      "Iteration 27810 Training loss 0.004418271128088236 Validation loss 0.011140900664031506 Accuracy 0.8818359375\n",
      "Iteration 27820 Training loss 0.00449117599055171 Validation loss 0.011079687625169754 Accuracy 0.8818359375\n",
      "Iteration 27830 Training loss 0.005535340402275324 Validation loss 0.011278863064944744 Accuracy 0.8798828125\n",
      "Iteration 27840 Training loss 0.005227589979767799 Validation loss 0.010974748060107231 Accuracy 0.8818359375\n",
      "Iteration 27850 Training loss 0.004351702984422445 Validation loss 0.011171088553965092 Accuracy 0.88134765625\n",
      "Iteration 27860 Training loss 0.005266491789370775 Validation loss 0.011005955748260021 Accuracy 0.8828125\n",
      "Iteration 27870 Training loss 0.004911038558930159 Validation loss 0.0113276531919837 Accuracy 0.87939453125\n",
      "Iteration 27880 Training loss 0.004202147945761681 Validation loss 0.010962736792862415 Accuracy 0.8837890625\n",
      "Iteration 27890 Training loss 0.0035464500542730093 Validation loss 0.01094979327172041 Accuracy 0.8828125\n",
      "Iteration 27900 Training loss 0.004510644357651472 Validation loss 0.0109309833496809 Accuracy 0.8837890625\n",
      "Iteration 27910 Training loss 0.004688594024628401 Validation loss 0.01106491219252348 Accuracy 0.88232421875\n",
      "Iteration 27920 Training loss 0.004745951388031244 Validation loss 0.010911569930613041 Accuracy 0.88427734375\n",
      "Iteration 27930 Training loss 0.00429244851693511 Validation loss 0.011031122878193855 Accuracy 0.8828125\n",
      "Iteration 27940 Training loss 0.004964754916727543 Validation loss 0.011195134371519089 Accuracy 0.880859375\n",
      "Iteration 27950 Training loss 0.005853645969182253 Validation loss 0.010790802538394928 Accuracy 0.88525390625\n",
      "Iteration 27960 Training loss 0.004652021918445826 Validation loss 0.010973210446536541 Accuracy 0.88232421875\n",
      "Iteration 27970 Training loss 0.0052507189102470875 Validation loss 0.010758544318377972 Accuracy 0.884765625\n",
      "Iteration 27980 Training loss 0.005317081231623888 Validation loss 0.010765896178781986 Accuracy 0.884765625\n",
      "Iteration 27990 Training loss 0.005901432130485773 Validation loss 0.01106290239840746 Accuracy 0.8828125\n",
      "Iteration 28000 Training loss 0.004947939421981573 Validation loss 0.01110240537673235 Accuracy 0.8818359375\n",
      "Iteration 28010 Training loss 0.005793644115328789 Validation loss 0.011381011456251144 Accuracy 0.8798828125\n",
      "Iteration 28020 Training loss 0.004379419144243002 Validation loss 0.0112595334649086 Accuracy 0.88037109375\n",
      "Iteration 28030 Training loss 0.005813155323266983 Validation loss 0.010902036912739277 Accuracy 0.8837890625\n",
      "Iteration 28040 Training loss 0.004822810646146536 Validation loss 0.011099305003881454 Accuracy 0.88232421875\n",
      "Iteration 28050 Training loss 0.003922098316252232 Validation loss 0.011043607257306576 Accuracy 0.88232421875\n",
      "Iteration 28060 Training loss 0.004898310638964176 Validation loss 0.011005409061908722 Accuracy 0.88330078125\n",
      "Iteration 28070 Training loss 0.00511307455599308 Validation loss 0.011050600558519363 Accuracy 0.88232421875\n",
      "Iteration 28080 Training loss 0.00618847506120801 Validation loss 0.010923256166279316 Accuracy 0.88427734375\n",
      "Iteration 28090 Training loss 0.004431604407727718 Validation loss 0.011022782884538174 Accuracy 0.8828125\n",
      "Iteration 28100 Training loss 0.005681330803781748 Validation loss 0.010950685478746891 Accuracy 0.88232421875\n",
      "Iteration 28110 Training loss 0.004399665631353855 Validation loss 0.011131836101412773 Accuracy 0.880859375\n",
      "Iteration 28120 Training loss 0.005556171294301748 Validation loss 0.011213310062885284 Accuracy 0.88037109375\n",
      "Iteration 28130 Training loss 0.00454375334084034 Validation loss 0.010932880453765392 Accuracy 0.8828125\n",
      "Iteration 28140 Training loss 0.006109870038926601 Validation loss 0.011248082853853703 Accuracy 0.880859375\n",
      "Iteration 28150 Training loss 0.0045554605312645435 Validation loss 0.010897265747189522 Accuracy 0.88427734375\n",
      "Iteration 28160 Training loss 0.003792235627770424 Validation loss 0.010876552201807499 Accuracy 0.8837890625\n",
      "Iteration 28170 Training loss 0.0042882137931883335 Validation loss 0.01110800914466381 Accuracy 0.8818359375\n",
      "Iteration 28180 Training loss 0.005446983501315117 Validation loss 0.01131710410118103 Accuracy 0.87890625\n",
      "Iteration 28190 Training loss 0.004809068515896797 Validation loss 0.010946808382868767 Accuracy 0.88232421875\n",
      "Iteration 28200 Training loss 0.005283923353999853 Validation loss 0.011033608578145504 Accuracy 0.8828125\n",
      "Iteration 28210 Training loss 0.005391646642237902 Validation loss 0.010751032270491123 Accuracy 0.88427734375\n",
      "Iteration 28220 Training loss 0.006467424798756838 Validation loss 0.010868747718632221 Accuracy 0.8837890625\n",
      "Iteration 28230 Training loss 0.004291364457458258 Validation loss 0.010971041396260262 Accuracy 0.88525390625\n",
      "Iteration 28240 Training loss 0.004259697627276182 Validation loss 0.011063042096793652 Accuracy 0.8828125\n",
      "Iteration 28250 Training loss 0.005431088153272867 Validation loss 0.011144401505589485 Accuracy 0.88232421875\n",
      "Iteration 28260 Training loss 0.004618443548679352 Validation loss 0.011346996761858463 Accuracy 0.87939453125\n",
      "Iteration 28270 Training loss 0.005095801781862974 Validation loss 0.011075767688453197 Accuracy 0.88232421875\n",
      "Iteration 28280 Training loss 0.005697701591998339 Validation loss 0.011569526046514511 Accuracy 0.87548828125\n",
      "Iteration 28290 Training loss 0.0041577741503715515 Validation loss 0.011385414749383926 Accuracy 0.87841796875\n",
      "Iteration 28300 Training loss 0.0040849014185369015 Validation loss 0.011097882874310017 Accuracy 0.880859375\n",
      "Iteration 28310 Training loss 0.005493483040481806 Validation loss 0.01126483827829361 Accuracy 0.87939453125\n",
      "Iteration 28320 Training loss 0.004801207687705755 Validation loss 0.011183654889464378 Accuracy 0.88037109375\n",
      "Iteration 28330 Training loss 0.004165909253060818 Validation loss 0.010990846902132034 Accuracy 0.8828125\n",
      "Iteration 28340 Training loss 0.0066460659727454185 Validation loss 0.01104779914021492 Accuracy 0.88232421875\n",
      "Iteration 28350 Training loss 0.004406424704939127 Validation loss 0.011405756697058678 Accuracy 0.87890625\n",
      "Iteration 28360 Training loss 0.005086932796984911 Validation loss 0.011426940560340881 Accuracy 0.87890625\n",
      "Iteration 28370 Training loss 0.005164043977856636 Validation loss 0.010907694697380066 Accuracy 0.88427734375\n",
      "Iteration 28380 Training loss 0.005159869324415922 Validation loss 0.010996767319738865 Accuracy 0.88330078125\n",
      "Iteration 28390 Training loss 0.003983297385275364 Validation loss 0.01123387273401022 Accuracy 0.88037109375\n",
      "Iteration 28400 Training loss 0.005207187030464411 Validation loss 0.01106530986726284 Accuracy 0.88232421875\n",
      "Iteration 28410 Training loss 0.005449590738862753 Validation loss 0.011303380131721497 Accuracy 0.8798828125\n",
      "Iteration 28420 Training loss 0.004256652668118477 Validation loss 0.010951826348900795 Accuracy 0.88330078125\n",
      "Iteration 28430 Training loss 0.00470580393448472 Validation loss 0.010917318053543568 Accuracy 0.88330078125\n",
      "Iteration 28440 Training loss 0.004615080542862415 Validation loss 0.011006530374288559 Accuracy 0.88232421875\n",
      "Iteration 28450 Training loss 0.0051872399635612965 Validation loss 0.01109373476356268 Accuracy 0.880859375\n",
      "Iteration 28460 Training loss 0.0049665640108287334 Validation loss 0.010885195806622505 Accuracy 0.8828125\n",
      "Iteration 28470 Training loss 0.005640088114887476 Validation loss 0.010919333435595036 Accuracy 0.8837890625\n",
      "Iteration 28480 Training loss 0.003942679613828659 Validation loss 0.010907504707574844 Accuracy 0.8837890625\n",
      "Iteration 28490 Training loss 0.005104838404804468 Validation loss 0.01122439093887806 Accuracy 0.88037109375\n",
      "Iteration 28500 Training loss 0.00361830135807395 Validation loss 0.010841213166713715 Accuracy 0.88525390625\n",
      "Iteration 28510 Training loss 0.005151445511728525 Validation loss 0.011667444370687008 Accuracy 0.8759765625\n",
      "Iteration 28520 Training loss 0.004602709319442511 Validation loss 0.011019177734851837 Accuracy 0.8828125\n",
      "Iteration 28530 Training loss 0.006860754452645779 Validation loss 0.010986250825226307 Accuracy 0.8828125\n",
      "Iteration 28540 Training loss 0.005520194303244352 Validation loss 0.010988626629114151 Accuracy 0.8818359375\n",
      "Iteration 28550 Training loss 0.004905024543404579 Validation loss 0.011045871302485466 Accuracy 0.88134765625\n",
      "Iteration 28560 Training loss 0.0035077754873782396 Validation loss 0.011047961190342903 Accuracy 0.8818359375\n",
      "Iteration 28570 Training loss 0.004861171822994947 Validation loss 0.01137405727058649 Accuracy 0.8779296875\n",
      "Iteration 28580 Training loss 0.004120498429983854 Validation loss 0.010854172520339489 Accuracy 0.8837890625\n",
      "Iteration 28590 Training loss 0.00541830575093627 Validation loss 0.010963872075080872 Accuracy 0.88232421875\n",
      "Iteration 28600 Training loss 0.004125514999032021 Validation loss 0.010748371481895447 Accuracy 0.884765625\n",
      "Iteration 28610 Training loss 0.00545946229249239 Validation loss 0.010816274210810661 Accuracy 0.884765625\n",
      "Iteration 28620 Training loss 0.00480540469288826 Validation loss 0.010952609591186047 Accuracy 0.88232421875\n",
      "Iteration 28630 Training loss 0.004811848048120737 Validation loss 0.01089537050575018 Accuracy 0.88330078125\n",
      "Iteration 28640 Training loss 0.004644846543669701 Validation loss 0.01098434254527092 Accuracy 0.88232421875\n",
      "Iteration 28650 Training loss 0.004090689122676849 Validation loss 0.011200145818293095 Accuracy 0.88037109375\n",
      "Iteration 28660 Training loss 0.004809171427041292 Validation loss 0.010865497402846813 Accuracy 0.88427734375\n",
      "Iteration 28670 Training loss 0.005298596806824207 Validation loss 0.0110007980838418 Accuracy 0.8818359375\n",
      "Iteration 28680 Training loss 0.00327684567309916 Validation loss 0.01102948933839798 Accuracy 0.8818359375\n",
      "Iteration 28690 Training loss 0.0057816957123577595 Validation loss 0.011192354373633862 Accuracy 0.880859375\n",
      "Iteration 28700 Training loss 0.0036620839964598417 Validation loss 0.010762382298707962 Accuracy 0.884765625\n",
      "Iteration 28710 Training loss 0.004014180041849613 Validation loss 0.010743862017989159 Accuracy 0.88427734375\n",
      "Iteration 28720 Training loss 0.004961386322975159 Validation loss 0.010765431448817253 Accuracy 0.88525390625\n",
      "Iteration 28730 Training loss 0.006151181645691395 Validation loss 0.011344509199261665 Accuracy 0.87939453125\n",
      "Iteration 28740 Training loss 0.00397305004298687 Validation loss 0.010827829129993916 Accuracy 0.88427734375\n",
      "Iteration 28750 Training loss 0.0036100351717323065 Validation loss 0.011050558649003506 Accuracy 0.88232421875\n",
      "Iteration 28760 Training loss 0.005684088449925184 Validation loss 0.011155177839100361 Accuracy 0.88134765625\n",
      "Iteration 28770 Training loss 0.006331870798021555 Validation loss 0.010996829718351364 Accuracy 0.88232421875\n",
      "Iteration 28780 Training loss 0.0036540997680276632 Validation loss 0.011133622378110886 Accuracy 0.880859375\n",
      "Iteration 28790 Training loss 0.006142125464975834 Validation loss 0.01129031553864479 Accuracy 0.87939453125\n",
      "Iteration 28800 Training loss 0.004134297836571932 Validation loss 0.010892106220126152 Accuracy 0.88330078125\n",
      "Iteration 28810 Training loss 0.003876802511513233 Validation loss 0.011134113185107708 Accuracy 0.880859375\n",
      "Iteration 28820 Training loss 0.004829861223697662 Validation loss 0.011199726723134518 Accuracy 0.88037109375\n",
      "Iteration 28830 Training loss 0.004177686758339405 Validation loss 0.011115841567516327 Accuracy 0.8818359375\n",
      "Iteration 28840 Training loss 0.0041363537311553955 Validation loss 0.011281400918960571 Accuracy 0.87890625\n",
      "Iteration 28850 Training loss 0.005697344895452261 Validation loss 0.011146562173962593 Accuracy 0.8818359375\n",
      "Iteration 28860 Training loss 0.004641234874725342 Validation loss 0.011197532527148724 Accuracy 0.88037109375\n",
      "Iteration 28870 Training loss 0.003934988286346197 Validation loss 0.011141098104417324 Accuracy 0.880859375\n",
      "Iteration 28880 Training loss 0.0068487077951431274 Validation loss 0.011490408331155777 Accuracy 0.8779296875\n",
      "Iteration 28890 Training loss 0.006351771764457226 Validation loss 0.01150039304047823 Accuracy 0.8759765625\n",
      "Iteration 28900 Training loss 0.004801206290721893 Validation loss 0.011047950014472008 Accuracy 0.8818359375\n",
      "Iteration 28910 Training loss 0.006008657161146402 Validation loss 0.0113834822550416 Accuracy 0.87744140625\n",
      "Iteration 28920 Training loss 0.00555424252524972 Validation loss 0.01095940638333559 Accuracy 0.88232421875\n",
      "Iteration 28930 Training loss 0.003813267918303609 Validation loss 0.010777898132801056 Accuracy 0.88427734375\n",
      "Iteration 28940 Training loss 0.003948742989450693 Validation loss 0.011138440109789371 Accuracy 0.880859375\n",
      "Iteration 28950 Training loss 0.004271712154150009 Validation loss 0.011200137436389923 Accuracy 0.8798828125\n",
      "Iteration 28960 Training loss 0.005235549062490463 Validation loss 0.011177253909409046 Accuracy 0.880859375\n",
      "Iteration 28970 Training loss 0.005456211045384407 Validation loss 0.011564718559384346 Accuracy 0.87744140625\n",
      "Iteration 28980 Training loss 0.005672745406627655 Validation loss 0.01091886218637228 Accuracy 0.88330078125\n",
      "Iteration 28990 Training loss 0.0036323030944913626 Validation loss 0.011132352985441685 Accuracy 0.880859375\n",
      "Iteration 29000 Training loss 0.00405650632455945 Validation loss 0.010766101069748402 Accuracy 0.88427734375\n",
      "Iteration 29010 Training loss 0.003314228728413582 Validation loss 0.011183436959981918 Accuracy 0.880859375\n",
      "Iteration 29020 Training loss 0.005099175032228231 Validation loss 0.010903107933700085 Accuracy 0.8837890625\n",
      "Iteration 29030 Training loss 0.00535815441980958 Validation loss 0.011292116716504097 Accuracy 0.87939453125\n",
      "Iteration 29040 Training loss 0.005914564710110426 Validation loss 0.010891476646065712 Accuracy 0.8828125\n",
      "Iteration 29050 Training loss 0.005682336166501045 Validation loss 0.011236955411732197 Accuracy 0.88037109375\n",
      "Iteration 29060 Training loss 0.0036802678368985653 Validation loss 0.011135797016322613 Accuracy 0.8818359375\n",
      "Iteration 29070 Training loss 0.004031247925013304 Validation loss 0.011079326272010803 Accuracy 0.8818359375\n",
      "Iteration 29080 Training loss 0.0025805537588894367 Validation loss 0.011080103926360607 Accuracy 0.8818359375\n",
      "Iteration 29090 Training loss 0.003921968396753073 Validation loss 0.01097810734063387 Accuracy 0.8818359375\n",
      "Iteration 29100 Training loss 0.004872637335211039 Validation loss 0.011127748526632786 Accuracy 0.88134765625\n",
      "Iteration 29110 Training loss 0.0037942505441606045 Validation loss 0.011075041256844997 Accuracy 0.8828125\n",
      "Iteration 29120 Training loss 0.005017537623643875 Validation loss 0.011194157414138317 Accuracy 0.88037109375\n",
      "Iteration 29130 Training loss 0.004899764433503151 Validation loss 0.011385851539671421 Accuracy 0.87890625\n",
      "Iteration 29140 Training loss 0.004166432190686464 Validation loss 0.010790873318910599 Accuracy 0.884765625\n",
      "Iteration 29150 Training loss 0.0037413793615996838 Validation loss 0.011075391434133053 Accuracy 0.8818359375\n",
      "Iteration 29160 Training loss 0.005044138059020042 Validation loss 0.011265900917351246 Accuracy 0.8798828125\n",
      "Iteration 29170 Training loss 0.00462509086355567 Validation loss 0.010872473008930683 Accuracy 0.884765625\n",
      "Iteration 29180 Training loss 0.003961421083658934 Validation loss 0.011081557720899582 Accuracy 0.880859375\n",
      "Iteration 29190 Training loss 0.005435576196759939 Validation loss 0.011047432199120522 Accuracy 0.8818359375\n",
      "Iteration 29200 Training loss 0.0048913415521383286 Validation loss 0.011151987127959728 Accuracy 0.880859375\n",
      "Iteration 29210 Training loss 0.004297333303838968 Validation loss 0.010988808237016201 Accuracy 0.8828125\n",
      "Iteration 29220 Training loss 0.004203586373478174 Validation loss 0.011151708662509918 Accuracy 0.88037109375\n",
      "Iteration 29230 Training loss 0.004407237749546766 Validation loss 0.011103703640401363 Accuracy 0.88232421875\n",
      "Iteration 29240 Training loss 0.004879770800471306 Validation loss 0.010969365015625954 Accuracy 0.88232421875\n",
      "Iteration 29250 Training loss 0.005294289905577898 Validation loss 0.011200662702322006 Accuracy 0.87939453125\n",
      "Iteration 29260 Training loss 0.006267928518354893 Validation loss 0.011595477350056171 Accuracy 0.87548828125\n",
      "Iteration 29270 Training loss 0.005323545541614294 Validation loss 0.010993454605340958 Accuracy 0.8828125\n",
      "Iteration 29280 Training loss 0.003677181201055646 Validation loss 0.011054587550461292 Accuracy 0.88232421875\n",
      "Iteration 29290 Training loss 0.003999378066509962 Validation loss 0.011251210235059261 Accuracy 0.8798828125\n",
      "Iteration 29300 Training loss 0.003266130341216922 Validation loss 0.010731403715908527 Accuracy 0.8857421875\n",
      "Iteration 29310 Training loss 0.0038174211513251066 Validation loss 0.010893022641539574 Accuracy 0.88330078125\n",
      "Iteration 29320 Training loss 0.00438288738951087 Validation loss 0.010947063565254211 Accuracy 0.8828125\n",
      "Iteration 29330 Training loss 0.004472789820283651 Validation loss 0.010976869612932205 Accuracy 0.8828125\n",
      "Iteration 29340 Training loss 0.00496105058118701 Validation loss 0.01067350897938013 Accuracy 0.8857421875\n",
      "Iteration 29350 Training loss 0.004346616566181183 Validation loss 0.010921883396804333 Accuracy 0.8828125\n",
      "Iteration 29360 Training loss 0.0046560377813875675 Validation loss 0.011087138205766678 Accuracy 0.8818359375\n",
      "Iteration 29370 Training loss 0.005729490891098976 Validation loss 0.011270038783550262 Accuracy 0.87939453125\n",
      "Iteration 29380 Training loss 0.005988226737827063 Validation loss 0.011153627187013626 Accuracy 0.8818359375\n",
      "Iteration 29390 Training loss 0.0051888711750507355 Validation loss 0.01084721926599741 Accuracy 0.884765625\n",
      "Iteration 29400 Training loss 0.004870621953159571 Validation loss 0.010841774754226208 Accuracy 0.884765625\n",
      "Iteration 29410 Training loss 0.004820794332772493 Validation loss 0.010760756209492683 Accuracy 0.88525390625\n",
      "Iteration 29420 Training loss 0.0048086936585605145 Validation loss 0.01137343980371952 Accuracy 0.87939453125\n",
      "Iteration 29430 Training loss 0.00460700923576951 Validation loss 0.01082959771156311 Accuracy 0.88427734375\n",
      "Iteration 29440 Training loss 0.004617720376700163 Validation loss 0.010839556343853474 Accuracy 0.884765625\n",
      "Iteration 29450 Training loss 0.006499655544757843 Validation loss 0.010664314031600952 Accuracy 0.88623046875\n",
      "Iteration 29460 Training loss 0.0033231403212994337 Validation loss 0.010824068449437618 Accuracy 0.8837890625\n",
      "Iteration 29470 Training loss 0.0034663076512515545 Validation loss 0.010899035260081291 Accuracy 0.88330078125\n",
      "Iteration 29480 Training loss 0.005978791508823633 Validation loss 0.010706898756325245 Accuracy 0.88720703125\n",
      "Iteration 29490 Training loss 0.005441118963062763 Validation loss 0.0108480928465724 Accuracy 0.8837890625\n",
      "Iteration 29500 Training loss 0.004265658091753721 Validation loss 0.010906178504228592 Accuracy 0.8837890625\n",
      "Iteration 29510 Training loss 0.004104589577764273 Validation loss 0.010968572460114956 Accuracy 0.88330078125\n",
      "Iteration 29520 Training loss 0.004649971146136522 Validation loss 0.010766535066068172 Accuracy 0.884765625\n",
      "Iteration 29530 Training loss 0.005391914863139391 Validation loss 0.010820573195815086 Accuracy 0.884765625\n",
      "Iteration 29540 Training loss 0.00465340306982398 Validation loss 0.011075317859649658 Accuracy 0.8818359375\n",
      "Iteration 29550 Training loss 0.0035346876829862595 Validation loss 0.010991334915161133 Accuracy 0.8818359375\n",
      "Iteration 29560 Training loss 0.0040864297188818455 Validation loss 0.010767140425741673 Accuracy 0.8857421875\n",
      "Iteration 29570 Training loss 0.0035432903096079826 Validation loss 0.01096492912620306 Accuracy 0.88330078125\n",
      "Iteration 29580 Training loss 0.005223878659307957 Validation loss 0.011111181229352951 Accuracy 0.88037109375\n",
      "Iteration 29590 Training loss 0.004685277119278908 Validation loss 0.01076614111661911 Accuracy 0.8857421875\n",
      "Iteration 29600 Training loss 0.0036129786167293787 Validation loss 0.010852540843188763 Accuracy 0.884765625\n",
      "Iteration 29610 Training loss 0.00498345447704196 Validation loss 0.010827071033418179 Accuracy 0.8837890625\n",
      "Iteration 29620 Training loss 0.005173817276954651 Validation loss 0.010839779861271381 Accuracy 0.884765625\n",
      "Iteration 29630 Training loss 0.004763668868690729 Validation loss 0.011029540561139584 Accuracy 0.88330078125\n",
      "Iteration 29640 Training loss 0.006248741876333952 Validation loss 0.011001154780387878 Accuracy 0.88232421875\n",
      "Iteration 29650 Training loss 0.004749090876430273 Validation loss 0.010977184399962425 Accuracy 0.8828125\n",
      "Iteration 29660 Training loss 0.0037389148492366076 Validation loss 0.011077712289988995 Accuracy 0.88134765625\n",
      "Iteration 29670 Training loss 0.00371455866843462 Validation loss 0.011109672486782074 Accuracy 0.8818359375\n",
      "Iteration 29680 Training loss 0.005847801920026541 Validation loss 0.011416954919695854 Accuracy 0.87841796875\n",
      "Iteration 29690 Training loss 0.004723937716335058 Validation loss 0.010961068794131279 Accuracy 0.8837890625\n",
      "Iteration 29700 Training loss 0.004809125792235136 Validation loss 0.011259269900619984 Accuracy 0.880859375\n",
      "Iteration 29710 Training loss 0.0028604702092707157 Validation loss 0.010846535675227642 Accuracy 0.88427734375\n",
      "Iteration 29720 Training loss 0.005786539521068335 Validation loss 0.010702221654355526 Accuracy 0.88623046875\n",
      "Iteration 29730 Training loss 0.00378051376901567 Validation loss 0.010837818495929241 Accuracy 0.88427734375\n",
      "Iteration 29740 Training loss 0.0045422702096402645 Validation loss 0.010702555999159813 Accuracy 0.88623046875\n",
      "Iteration 29750 Training loss 0.003934090957045555 Validation loss 0.011055574752390385 Accuracy 0.88037109375\n",
      "Iteration 29760 Training loss 0.005159594584256411 Validation loss 0.010703378356993198 Accuracy 0.88623046875\n",
      "Iteration 29770 Training loss 0.00542704900726676 Validation loss 0.010880931280553341 Accuracy 0.8837890625\n",
      "Iteration 29780 Training loss 0.0037199973594397306 Validation loss 0.01085752248764038 Accuracy 0.8837890625\n",
      "Iteration 29790 Training loss 0.004398445598781109 Validation loss 0.010981088504195213 Accuracy 0.88232421875\n",
      "Iteration 29800 Training loss 0.005824640393257141 Validation loss 0.011087482795119286 Accuracy 0.88232421875\n",
      "Iteration 29810 Training loss 0.005424892529845238 Validation loss 0.0108941113576293 Accuracy 0.88427734375\n",
      "Iteration 29820 Training loss 0.0056408606469631195 Validation loss 0.011320584453642368 Accuracy 0.88037109375\n",
      "Iteration 29830 Training loss 0.006112379487603903 Validation loss 0.011063388548791409 Accuracy 0.88232421875\n",
      "Iteration 29840 Training loss 0.005272905807942152 Validation loss 0.011249138042330742 Accuracy 0.8798828125\n",
      "Iteration 29850 Training loss 0.004919779486954212 Validation loss 0.010901418514549732 Accuracy 0.8837890625\n",
      "Iteration 29860 Training loss 0.004383999388664961 Validation loss 0.010956364683806896 Accuracy 0.8828125\n",
      "Iteration 29870 Training loss 0.004110144451260567 Validation loss 0.010914365760982037 Accuracy 0.88232421875\n",
      "Iteration 29880 Training loss 0.0033323729876428843 Validation loss 0.010741925798356533 Accuracy 0.8837890625\n",
      "Iteration 29890 Training loss 0.004348321817815304 Validation loss 0.01072611752897501 Accuracy 0.884765625\n",
      "Iteration 29900 Training loss 0.005062439013272524 Validation loss 0.0107295336201787 Accuracy 0.8857421875\n",
      "Iteration 29910 Training loss 0.005188846029341221 Validation loss 0.010807372629642487 Accuracy 0.884765625\n",
      "Iteration 29920 Training loss 0.005294200498610735 Validation loss 0.010649084113538265 Accuracy 0.88623046875\n",
      "Iteration 29930 Training loss 0.004860466346144676 Validation loss 0.010644831694662571 Accuracy 0.88525390625\n",
      "Iteration 29940 Training loss 0.0028164274990558624 Validation loss 0.01081504113972187 Accuracy 0.88427734375\n",
      "Iteration 29950 Training loss 0.004983908962458372 Validation loss 0.011016580276191235 Accuracy 0.88134765625\n",
      "Iteration 29960 Training loss 0.004544044379144907 Validation loss 0.011117368936538696 Accuracy 0.880859375\n",
      "Iteration 29970 Training loss 0.0036456845700740814 Validation loss 0.01147610042244196 Accuracy 0.8779296875\n",
      "Iteration 29980 Training loss 0.003942081239074469 Validation loss 0.01095989253371954 Accuracy 0.8828125\n",
      "Iteration 29990 Training loss 0.005808466579765081 Validation loss 0.011275500059127808 Accuracy 0.87890625\n",
      "Iteration 30000 Training loss 0.0055571370758116245 Validation loss 0.011869830079376698 Accuracy 0.87353515625\n",
      "Iteration 30010 Training loss 0.005714822094887495 Validation loss 0.0111030712723732 Accuracy 0.880859375\n",
      "Iteration 30020 Training loss 0.004373088013380766 Validation loss 0.010882353410124779 Accuracy 0.8837890625\n",
      "Iteration 30030 Training loss 0.004284994211047888 Validation loss 0.010833202861249447 Accuracy 0.8857421875\n",
      "Iteration 30040 Training loss 0.003500918624922633 Validation loss 0.011037897318601608 Accuracy 0.8818359375\n",
      "Iteration 30050 Training loss 0.0030660470947623253 Validation loss 0.01085782703012228 Accuracy 0.8837890625\n",
      "Iteration 30060 Training loss 0.004104133695363998 Validation loss 0.010947726666927338 Accuracy 0.88330078125\n",
      "Iteration 30070 Training loss 0.004852416459470987 Validation loss 0.01101107057183981 Accuracy 0.88232421875\n",
      "Iteration 30080 Training loss 0.003367512719705701 Validation loss 0.010608881711959839 Accuracy 0.88720703125\n",
      "Iteration 30090 Training loss 0.00459663849323988 Validation loss 0.010999553836882114 Accuracy 0.88232421875\n",
      "Iteration 30100 Training loss 0.004271337296813726 Validation loss 0.01069981511682272 Accuracy 0.884765625\n",
      "Iteration 30110 Training loss 0.005261733662337065 Validation loss 0.01085656601935625 Accuracy 0.88427734375\n",
      "Iteration 30120 Training loss 0.0056141954846680164 Validation loss 0.010841130279004574 Accuracy 0.88427734375\n",
      "Iteration 30130 Training loss 0.004444010555744171 Validation loss 0.010769783519208431 Accuracy 0.8857421875\n",
      "Iteration 30140 Training loss 0.007100529968738556 Validation loss 0.011194844730198383 Accuracy 0.880859375\n",
      "Iteration 30150 Training loss 0.004325644578784704 Validation loss 0.010955717414617538 Accuracy 0.8828125\n",
      "Iteration 30160 Training loss 0.005576052237302065 Validation loss 0.01101616770029068 Accuracy 0.8818359375\n",
      "Iteration 30170 Training loss 0.004341470543295145 Validation loss 0.010674107819795609 Accuracy 0.88671875\n",
      "Iteration 30180 Training loss 0.005846785847097635 Validation loss 0.011184696108102798 Accuracy 0.880859375\n",
      "Iteration 30190 Training loss 0.004787237849086523 Validation loss 0.011059770360589027 Accuracy 0.8818359375\n",
      "Iteration 30200 Training loss 0.005385789088904858 Validation loss 0.011527501046657562 Accuracy 0.87646484375\n",
      "Iteration 30210 Training loss 0.004955342970788479 Validation loss 0.01089449692517519 Accuracy 0.8837890625\n",
      "Iteration 30220 Training loss 0.003965455107390881 Validation loss 0.01071945484727621 Accuracy 0.8857421875\n",
      "Iteration 30230 Training loss 0.003954870626330376 Validation loss 0.010996975004673004 Accuracy 0.88330078125\n",
      "Iteration 30240 Training loss 0.003976465202867985 Validation loss 0.010825044475495815 Accuracy 0.88330078125\n",
      "Iteration 30250 Training loss 0.003460913896560669 Validation loss 0.010758018121123314 Accuracy 0.884765625\n",
      "Iteration 30260 Training loss 0.003097300184890628 Validation loss 0.01087441947311163 Accuracy 0.88525390625\n",
      "Iteration 30270 Training loss 0.004805384203791618 Validation loss 0.01066604070365429 Accuracy 0.8857421875\n",
      "Iteration 30280 Training loss 0.004483513999730349 Validation loss 0.010979191400110722 Accuracy 0.88330078125\n",
      "Iteration 30290 Training loss 0.005656997673213482 Validation loss 0.010975339449942112 Accuracy 0.88232421875\n",
      "Iteration 30300 Training loss 0.0047288015484809875 Validation loss 0.010857603512704372 Accuracy 0.884765625\n",
      "Iteration 30310 Training loss 0.005536925513297319 Validation loss 0.010691729374229908 Accuracy 0.88671875\n",
      "Iteration 30320 Training loss 0.005858779884874821 Validation loss 0.0110005559399724 Accuracy 0.88330078125\n",
      "Iteration 30330 Training loss 0.004621861036866903 Validation loss 0.010810361243784428 Accuracy 0.884765625\n",
      "Iteration 30340 Training loss 0.005277101881802082 Validation loss 0.010860146023333073 Accuracy 0.8837890625\n",
      "Iteration 30350 Training loss 0.0055312784388661385 Validation loss 0.010860557667911053 Accuracy 0.88427734375\n",
      "Iteration 30360 Training loss 0.005863516591489315 Validation loss 0.011123303323984146 Accuracy 0.880859375\n",
      "Iteration 30370 Training loss 0.0047205109149217606 Validation loss 0.010982565581798553 Accuracy 0.8828125\n",
      "Iteration 30380 Training loss 0.004967504646629095 Validation loss 0.010915413498878479 Accuracy 0.8837890625\n",
      "Iteration 30390 Training loss 0.004217574838548899 Validation loss 0.01079352293163538 Accuracy 0.88427734375\n",
      "Iteration 30400 Training loss 0.004842076450586319 Validation loss 0.010903866961598396 Accuracy 0.88330078125\n",
      "Iteration 30410 Training loss 0.0041956715285778046 Validation loss 0.010712802410125732 Accuracy 0.8837890625\n",
      "Iteration 30420 Training loss 0.004978125914931297 Validation loss 0.010816018097102642 Accuracy 0.88330078125\n",
      "Iteration 30430 Training loss 0.004496362060308456 Validation loss 0.010592514649033546 Accuracy 0.88671875\n",
      "Iteration 30440 Training loss 0.006684437859803438 Validation loss 0.011133923195302486 Accuracy 0.88134765625\n",
      "Iteration 30450 Training loss 0.004554123617708683 Validation loss 0.011086962185800076 Accuracy 0.880859375\n",
      "Iteration 30460 Training loss 0.003396997693926096 Validation loss 0.010990693233907223 Accuracy 0.8828125\n",
      "Iteration 30470 Training loss 0.004300704225897789 Validation loss 0.010630575940012932 Accuracy 0.8857421875\n",
      "Iteration 30480 Training loss 0.0048478394746780396 Validation loss 0.010890546254813671 Accuracy 0.8837890625\n",
      "Iteration 30490 Training loss 0.004392225760966539 Validation loss 0.010654906742274761 Accuracy 0.88525390625\n",
      "Iteration 30500 Training loss 0.0038435962051153183 Validation loss 0.010886437259614468 Accuracy 0.8837890625\n",
      "Iteration 30510 Training loss 0.005944414529949427 Validation loss 0.010833202861249447 Accuracy 0.8857421875\n",
      "Iteration 30520 Training loss 0.0024626993108540773 Validation loss 0.010895822197198868 Accuracy 0.88330078125\n",
      "Iteration 30530 Training loss 0.0038705470506101847 Validation loss 0.01062572468072176 Accuracy 0.88623046875\n",
      "Iteration 30540 Training loss 0.003690283978357911 Validation loss 0.010708806104958057 Accuracy 0.88671875\n",
      "Iteration 30550 Training loss 0.004745991434901953 Validation loss 0.010683168657124043 Accuracy 0.88623046875\n",
      "Iteration 30560 Training loss 0.00585869699716568 Validation loss 0.010802575387060642 Accuracy 0.884765625\n",
      "Iteration 30570 Training loss 0.00424750242382288 Validation loss 0.011023999191820621 Accuracy 0.8837890625\n",
      "Iteration 30580 Training loss 0.006565842777490616 Validation loss 0.010855128988623619 Accuracy 0.884765625\n",
      "Iteration 30590 Training loss 0.0036851107142865658 Validation loss 0.010819641873240471 Accuracy 0.88330078125\n",
      "Iteration 30600 Training loss 0.005634370259940624 Validation loss 0.011338585987687111 Accuracy 0.8779296875\n",
      "Iteration 30610 Training loss 0.0038047574926167727 Validation loss 0.010940180160105228 Accuracy 0.88330078125\n",
      "Iteration 30620 Training loss 0.003904378740116954 Validation loss 0.01082814671099186 Accuracy 0.88427734375\n",
      "Iteration 30630 Training loss 0.004116291645914316 Validation loss 0.010960829444229603 Accuracy 0.88330078125\n",
      "Iteration 30640 Training loss 0.004044858738780022 Validation loss 0.010911151766777039 Accuracy 0.8837890625\n",
      "Iteration 30650 Training loss 0.004144128877669573 Validation loss 0.010751012712717056 Accuracy 0.884765625\n",
      "Iteration 30660 Training loss 0.005892462562769651 Validation loss 0.011613293550908566 Accuracy 0.876953125\n",
      "Iteration 30670 Training loss 0.0033771656453609467 Validation loss 0.011076798662543297 Accuracy 0.88134765625\n",
      "Iteration 30680 Training loss 0.005098687019199133 Validation loss 0.010712088085711002 Accuracy 0.88623046875\n",
      "Iteration 30690 Training loss 0.0035050385631620884 Validation loss 0.011135891079902649 Accuracy 0.88037109375\n",
      "Iteration 30700 Training loss 0.004170111380517483 Validation loss 0.010837907902896404 Accuracy 0.8837890625\n",
      "Iteration 30710 Training loss 0.005583995021879673 Validation loss 0.010946308262646198 Accuracy 0.88232421875\n",
      "Iteration 30720 Training loss 0.0042596557177603245 Validation loss 0.011245915666222572 Accuracy 0.8798828125\n",
      "Iteration 30730 Training loss 0.004634143318980932 Validation loss 0.011512823402881622 Accuracy 0.87646484375\n",
      "Iteration 30740 Training loss 0.002066435292363167 Validation loss 0.010797392576932907 Accuracy 0.88427734375\n",
      "Iteration 30750 Training loss 0.0051337555050849915 Validation loss 0.01083255372941494 Accuracy 0.8837890625\n",
      "Iteration 30760 Training loss 0.0041565606370568275 Validation loss 0.010752313770353794 Accuracy 0.8857421875\n",
      "Iteration 30770 Training loss 0.0049967519007623196 Validation loss 0.010758311487734318 Accuracy 0.8857421875\n",
      "Iteration 30780 Training loss 0.004947444424033165 Validation loss 0.010804432444274426 Accuracy 0.88330078125\n",
      "Iteration 30790 Training loss 0.0029918975196778774 Validation loss 0.01086256094276905 Accuracy 0.8837890625\n",
      "Iteration 30800 Training loss 0.004376294557005167 Validation loss 0.011281008832156658 Accuracy 0.88037109375\n",
      "Iteration 30810 Training loss 0.0034801498986780643 Validation loss 0.010800573043525219 Accuracy 0.884765625\n",
      "Iteration 30820 Training loss 0.00543979974463582 Validation loss 0.010690995492041111 Accuracy 0.8857421875\n",
      "Iteration 30830 Training loss 0.00448132399469614 Validation loss 0.010658873245120049 Accuracy 0.88623046875\n",
      "Iteration 30840 Training loss 0.0048216562718153 Validation loss 0.010716731660068035 Accuracy 0.88525390625\n",
      "Iteration 30850 Training loss 0.004078918602317572 Validation loss 0.010751930996775627 Accuracy 0.884765625\n",
      "Iteration 30860 Training loss 0.004335327539592981 Validation loss 0.011265846900641918 Accuracy 0.8798828125\n",
      "Iteration 30870 Training loss 0.00432689068838954 Validation loss 0.010787195526063442 Accuracy 0.8837890625\n",
      "Iteration 30880 Training loss 0.004119638353586197 Validation loss 0.010681398212909698 Accuracy 0.88671875\n",
      "Iteration 30890 Training loss 0.004441136494278908 Validation loss 0.01093828584998846 Accuracy 0.88330078125\n",
      "Iteration 30900 Training loss 0.0041491081938147545 Validation loss 0.010844831354916096 Accuracy 0.8837890625\n",
      "Iteration 30910 Training loss 0.004953906871378422 Validation loss 0.011216589249670506 Accuracy 0.8798828125\n",
      "Iteration 30920 Training loss 0.004574904218316078 Validation loss 0.01067846268415451 Accuracy 0.88623046875\n",
      "Iteration 30930 Training loss 0.003986324183642864 Validation loss 0.010831418447196484 Accuracy 0.88427734375\n",
      "Iteration 30940 Training loss 0.003992181736975908 Validation loss 0.010819591581821442 Accuracy 0.88427734375\n",
      "Iteration 30950 Training loss 0.003853201400488615 Validation loss 0.010920140892267227 Accuracy 0.88232421875\n",
      "Iteration 30960 Training loss 0.006228045094758272 Validation loss 0.010808904655277729 Accuracy 0.884765625\n",
      "Iteration 30970 Training loss 0.0036003892309963703 Validation loss 0.011134328320622444 Accuracy 0.88037109375\n",
      "Iteration 30980 Training loss 0.0031995840836316347 Validation loss 0.010926916263997555 Accuracy 0.8828125\n",
      "Iteration 30990 Training loss 0.005498308688402176 Validation loss 0.010971122421324253 Accuracy 0.88232421875\n",
      "Iteration 31000 Training loss 0.0033174988348037004 Validation loss 0.011191142722964287 Accuracy 0.8798828125\n",
      "Iteration 31010 Training loss 0.005351154133677483 Validation loss 0.011119348928332329 Accuracy 0.8818359375\n",
      "Iteration 31020 Training loss 0.005897365976125002 Validation loss 0.01095669623464346 Accuracy 0.8837890625\n",
      "Iteration 31030 Training loss 0.00344702391885221 Validation loss 0.010747460648417473 Accuracy 0.88623046875\n",
      "Iteration 31040 Training loss 0.004480310250073671 Validation loss 0.010975395329296589 Accuracy 0.8828125\n",
      "Iteration 31050 Training loss 0.004880865570157766 Validation loss 0.010812930762767792 Accuracy 0.884765625\n",
      "Iteration 31060 Training loss 0.00512280547991395 Validation loss 0.010911780409514904 Accuracy 0.8837890625\n",
      "Iteration 31070 Training loss 0.004191204905509949 Validation loss 0.010958444327116013 Accuracy 0.88330078125\n",
      "Iteration 31080 Training loss 0.0055156610906124115 Validation loss 0.010645892471075058 Accuracy 0.88623046875\n",
      "Iteration 31090 Training loss 0.0042665572836995125 Validation loss 0.011440690606832504 Accuracy 0.87939453125\n",
      "Iteration 31100 Training loss 0.0053774514235556126 Validation loss 0.01100255362689495 Accuracy 0.88232421875\n",
      "Iteration 31110 Training loss 0.00381491519510746 Validation loss 0.010801617987453938 Accuracy 0.884765625\n",
      "Iteration 31120 Training loss 0.003640042617917061 Validation loss 0.011076935566961765 Accuracy 0.8818359375\n",
      "Iteration 31130 Training loss 0.005033995024859905 Validation loss 0.010526652447879314 Accuracy 0.88720703125\n",
      "Iteration 31140 Training loss 0.00453717028722167 Validation loss 0.01061245333403349 Accuracy 0.88720703125\n",
      "Iteration 31150 Training loss 0.005006662104278803 Validation loss 0.010877924971282482 Accuracy 0.8828125\n",
      "Iteration 31160 Training loss 0.0036060635466128588 Validation loss 0.010730107314884663 Accuracy 0.88525390625\n",
      "Iteration 31170 Training loss 0.003551291534677148 Validation loss 0.011271169409155846 Accuracy 0.87939453125\n",
      "Iteration 31180 Training loss 0.003935503773391247 Validation loss 0.010549189522862434 Accuracy 0.8857421875\n",
      "Iteration 31190 Training loss 0.0044927699491381645 Validation loss 0.011083110235631466 Accuracy 0.88232421875\n",
      "Iteration 31200 Training loss 0.004403355065733194 Validation loss 0.010840934701263905 Accuracy 0.8837890625\n",
      "Iteration 31210 Training loss 0.004344445653259754 Validation loss 0.010786877013742924 Accuracy 0.88427734375\n",
      "Iteration 31220 Training loss 0.005160504020750523 Validation loss 0.010979914106428623 Accuracy 0.88232421875\n",
      "Iteration 31230 Training loss 0.0049318671226501465 Validation loss 0.011100558564066887 Accuracy 0.88134765625\n",
      "Iteration 31240 Training loss 0.004139300901442766 Validation loss 0.010806004516780376 Accuracy 0.88525390625\n",
      "Iteration 31250 Training loss 0.005071793217211962 Validation loss 0.011119483038783073 Accuracy 0.88134765625\n",
      "Iteration 31260 Training loss 0.005756495054811239 Validation loss 0.011098467744886875 Accuracy 0.8818359375\n",
      "Iteration 31270 Training loss 0.003790505463257432 Validation loss 0.010680925101041794 Accuracy 0.884765625\n",
      "Iteration 31280 Training loss 0.0039028720930218697 Validation loss 0.011047233827412128 Accuracy 0.8828125\n",
      "Iteration 31290 Training loss 0.004551181569695473 Validation loss 0.011069483123719692 Accuracy 0.8818359375\n",
      "Iteration 31300 Training loss 0.006047310307621956 Validation loss 0.011002499610185623 Accuracy 0.88232421875\n",
      "Iteration 31310 Training loss 0.006276139989495277 Validation loss 0.010976062156260014 Accuracy 0.8837890625\n",
      "Iteration 31320 Training loss 0.004292989149689674 Validation loss 0.010937334038317204 Accuracy 0.8828125\n",
      "Iteration 31330 Training loss 0.0033729188144207 Validation loss 0.010959445498883724 Accuracy 0.88330078125\n",
      "Iteration 31340 Training loss 0.005148808006197214 Validation loss 0.010937614366412163 Accuracy 0.88232421875\n",
      "Iteration 31350 Training loss 0.004703248851001263 Validation loss 0.011143231764435768 Accuracy 0.88134765625\n",
      "Iteration 31360 Training loss 0.005174565594643354 Validation loss 0.011216484010219574 Accuracy 0.88037109375\n",
      "Iteration 31370 Training loss 0.005361303687095642 Validation loss 0.011249919421970844 Accuracy 0.8798828125\n",
      "Iteration 31380 Training loss 0.004873988218605518 Validation loss 0.01100040040910244 Accuracy 0.8818359375\n",
      "Iteration 31390 Training loss 0.004949276335537434 Validation loss 0.011036536656320095 Accuracy 0.88134765625\n",
      "Iteration 31400 Training loss 0.005103531293570995 Validation loss 0.010822062380611897 Accuracy 0.8837890625\n",
      "Iteration 31410 Training loss 0.004201948177069426 Validation loss 0.010819323360919952 Accuracy 0.88525390625\n",
      "Iteration 31420 Training loss 0.005579747259616852 Validation loss 0.011144174262881279 Accuracy 0.880859375\n",
      "Iteration 31430 Training loss 0.0034258426167070866 Validation loss 0.010769755579531193 Accuracy 0.88671875\n",
      "Iteration 31440 Training loss 0.002971055917441845 Validation loss 0.010963900946080685 Accuracy 0.88330078125\n",
      "Iteration 31450 Training loss 0.006104466505348682 Validation loss 0.011081363074481487 Accuracy 0.8818359375\n",
      "Iteration 31460 Training loss 0.005927270278334618 Validation loss 0.010842588730156422 Accuracy 0.8837890625\n",
      "Iteration 31470 Training loss 0.0046914624981582165 Validation loss 0.011046637780964375 Accuracy 0.8818359375\n",
      "Iteration 31480 Training loss 0.005100517999380827 Validation loss 0.011243682354688644 Accuracy 0.8798828125\n",
      "Iteration 31490 Training loss 0.004168997053056955 Validation loss 0.011024422012269497 Accuracy 0.8828125\n",
      "Iteration 31500 Training loss 0.004140152595937252 Validation loss 0.010946877300739288 Accuracy 0.8828125\n",
      "Iteration 31510 Training loss 0.005562166683375835 Validation loss 0.010935490019619465 Accuracy 0.8837890625\n",
      "Iteration 31520 Training loss 0.004180236253887415 Validation loss 0.010804391466081142 Accuracy 0.88427734375\n",
      "Iteration 31530 Training loss 0.0056005967780947685 Validation loss 0.010894659906625748 Accuracy 0.88330078125\n",
      "Iteration 31540 Training loss 0.004716651979833841 Validation loss 0.010644704103469849 Accuracy 0.88671875\n",
      "Iteration 31550 Training loss 0.005273770075291395 Validation loss 0.01086992397904396 Accuracy 0.8828125\n",
      "Iteration 31560 Training loss 0.0051124864257872105 Validation loss 0.010757612995803356 Accuracy 0.8857421875\n",
      "Iteration 31570 Training loss 0.0037599713541567326 Validation loss 0.01075740810483694 Accuracy 0.8857421875\n",
      "Iteration 31580 Training loss 0.003972606733441353 Validation loss 0.01064188964664936 Accuracy 0.88671875\n",
      "Iteration 31590 Training loss 0.005405975505709648 Validation loss 0.011043749749660492 Accuracy 0.88232421875\n",
      "Iteration 31600 Training loss 0.004232839681208134 Validation loss 0.011001062579452991 Accuracy 0.88232421875\n",
      "Iteration 31610 Training loss 0.005694124381989241 Validation loss 0.010914214886724949 Accuracy 0.8837890625\n",
      "Iteration 31620 Training loss 0.005886517930775881 Validation loss 0.010937709361314774 Accuracy 0.8837890625\n",
      "Iteration 31630 Training loss 0.004115065094083548 Validation loss 0.011238094419240952 Accuracy 0.880859375\n",
      "Iteration 31640 Training loss 0.005061344243586063 Validation loss 0.010696006007492542 Accuracy 0.8857421875\n",
      "Iteration 31650 Training loss 0.00429181195795536 Validation loss 0.010979472659528255 Accuracy 0.8828125\n",
      "Iteration 31660 Training loss 0.002146426821127534 Validation loss 0.010686774738132954 Accuracy 0.88623046875\n",
      "Iteration 31670 Training loss 0.006371852941811085 Validation loss 0.01157389860600233 Accuracy 0.8759765625\n",
      "Iteration 31680 Training loss 0.005767158232629299 Validation loss 0.011585701256990433 Accuracy 0.876953125\n",
      "Iteration 31690 Training loss 0.0047712200321257114 Validation loss 0.010905612260103226 Accuracy 0.88330078125\n",
      "Iteration 31700 Training loss 0.0038234605453908443 Validation loss 0.010984336026012897 Accuracy 0.8837890625\n",
      "Iteration 31710 Training loss 0.003551566507667303 Validation loss 0.011469691060483456 Accuracy 0.87744140625\n",
      "Iteration 31720 Training loss 0.003553427755832672 Validation loss 0.010931521654129028 Accuracy 0.88330078125\n",
      "Iteration 31730 Training loss 0.004531043581664562 Validation loss 0.011172887869179249 Accuracy 0.88037109375\n",
      "Iteration 31740 Training loss 0.0045265681110322475 Validation loss 0.010943254455924034 Accuracy 0.88330078125\n",
      "Iteration 31750 Training loss 0.004396943841129541 Validation loss 0.011124537326395512 Accuracy 0.88232421875\n",
      "Iteration 31760 Training loss 0.00582940923050046 Validation loss 0.010824312455952168 Accuracy 0.884765625\n",
      "Iteration 31770 Training loss 0.005281899590045214 Validation loss 0.010935906320810318 Accuracy 0.88330078125\n",
      "Iteration 31780 Training loss 0.003330484963953495 Validation loss 0.010982739739120007 Accuracy 0.8828125\n",
      "Iteration 31790 Training loss 0.0043899305164813995 Validation loss 0.010623305104672909 Accuracy 0.88623046875\n",
      "Iteration 31800 Training loss 0.0028585351537913084 Validation loss 0.010832448489964008 Accuracy 0.88427734375\n",
      "Iteration 31810 Training loss 0.002778795547783375 Validation loss 0.010834981687366962 Accuracy 0.88427734375\n",
      "Iteration 31820 Training loss 0.004609460011124611 Validation loss 0.011016464792191982 Accuracy 0.8818359375\n",
      "Iteration 31830 Training loss 0.004451681859791279 Validation loss 0.010761188343167305 Accuracy 0.8857421875\n",
      "Iteration 31840 Training loss 0.005183685105293989 Validation loss 0.010913964360952377 Accuracy 0.8837890625\n",
      "Iteration 31850 Training loss 0.0030396778602153063 Validation loss 0.011103067547082901 Accuracy 0.880859375\n",
      "Iteration 31860 Training loss 0.001838846830651164 Validation loss 0.01120965089648962 Accuracy 0.88037109375\n",
      "Iteration 31870 Training loss 0.004107271321117878 Validation loss 0.010671098716557026 Accuracy 0.88525390625\n",
      "Iteration 31880 Training loss 0.004328918643295765 Validation loss 0.010706956498324871 Accuracy 0.88525390625\n",
      "Iteration 31890 Training loss 0.005884375423192978 Validation loss 0.010717513971030712 Accuracy 0.88623046875\n",
      "Iteration 31900 Training loss 0.0038470663130283356 Validation loss 0.011075077578425407 Accuracy 0.88232421875\n",
      "Iteration 31910 Training loss 0.004034124314785004 Validation loss 0.011058583855628967 Accuracy 0.88134765625\n",
      "Iteration 31920 Training loss 0.004646408837288618 Validation loss 0.01088693831115961 Accuracy 0.8837890625\n",
      "Iteration 31930 Training loss 0.004672906827181578 Validation loss 0.010793942958116531 Accuracy 0.88525390625\n",
      "Iteration 31940 Training loss 0.003896568203344941 Validation loss 0.010746069252490997 Accuracy 0.884765625\n",
      "Iteration 31950 Training loss 0.004347424954175949 Validation loss 0.010620578192174435 Accuracy 0.88623046875\n",
      "Iteration 31960 Training loss 0.004475774709135294 Validation loss 0.010604835115373135 Accuracy 0.88623046875\n",
      "Iteration 31970 Training loss 0.005243619438260794 Validation loss 0.010564272291958332 Accuracy 0.88671875\n",
      "Iteration 31980 Training loss 0.004397282842546701 Validation loss 0.011307724751532078 Accuracy 0.87890625\n",
      "Iteration 31990 Training loss 0.004720652941614389 Validation loss 0.011217696592211723 Accuracy 0.88037109375\n",
      "Iteration 32000 Training loss 0.0030243650544434786 Validation loss 0.010939721018075943 Accuracy 0.8828125\n",
      "Iteration 32010 Training loss 0.003833167953416705 Validation loss 0.011027218773961067 Accuracy 0.8818359375\n",
      "Iteration 32020 Training loss 0.0051598562858998775 Validation loss 0.010839283466339111 Accuracy 0.884765625\n",
      "Iteration 32030 Training loss 0.004009635653346777 Validation loss 0.010837763547897339 Accuracy 0.88427734375\n",
      "Iteration 32040 Training loss 0.0032982202246785164 Validation loss 0.0108168451115489 Accuracy 0.88427734375\n",
      "Iteration 32050 Training loss 0.0031099007464945316 Validation loss 0.010658476501703262 Accuracy 0.88671875\n",
      "Iteration 32060 Training loss 0.005254153162240982 Validation loss 0.010820351541042328 Accuracy 0.8837890625\n",
      "Iteration 32070 Training loss 0.00428271247074008 Validation loss 0.010645834729075432 Accuracy 0.88623046875\n",
      "Iteration 32080 Training loss 0.0037310628686100245 Validation loss 0.010670309886336327 Accuracy 0.88671875\n",
      "Iteration 32090 Training loss 0.004065487068146467 Validation loss 0.01103740930557251 Accuracy 0.88232421875\n",
      "Iteration 32100 Training loss 0.0032278571743518114 Validation loss 0.010674002580344677 Accuracy 0.88623046875\n",
      "Iteration 32110 Training loss 0.00591536657884717 Validation loss 0.01060159970074892 Accuracy 0.88623046875\n",
      "Iteration 32120 Training loss 0.004613233730196953 Validation loss 0.010773643851280212 Accuracy 0.884765625\n",
      "Iteration 32130 Training loss 0.004273300990462303 Validation loss 0.011098110117018223 Accuracy 0.88037109375\n",
      "Iteration 32140 Training loss 0.0032055103220045567 Validation loss 0.010970944538712502 Accuracy 0.8828125\n",
      "Iteration 32150 Training loss 0.004330867901444435 Validation loss 0.010842876508831978 Accuracy 0.88427734375\n",
      "Iteration 32160 Training loss 0.005151910707354546 Validation loss 0.011028554290533066 Accuracy 0.8828125\n",
      "Iteration 32170 Training loss 0.0042187818326056 Validation loss 0.010905536822974682 Accuracy 0.88232421875\n",
      "Iteration 32180 Training loss 0.0053675477392971516 Validation loss 0.010987752117216587 Accuracy 0.88232421875\n",
      "Iteration 32190 Training loss 0.0043939449824392796 Validation loss 0.010901431553065777 Accuracy 0.8818359375\n",
      "Iteration 32200 Training loss 0.004317714832723141 Validation loss 0.010707603767514229 Accuracy 0.8857421875\n",
      "Iteration 32210 Training loss 0.004662804771214724 Validation loss 0.010733617469668388 Accuracy 0.884765625\n",
      "Iteration 32220 Training loss 0.005536725278943777 Validation loss 0.010583207942545414 Accuracy 0.88671875\n",
      "Iteration 32230 Training loss 0.004609308205544949 Validation loss 0.010855416767299175 Accuracy 0.88427734375\n",
      "Iteration 32240 Training loss 0.00491061108186841 Validation loss 0.011078180745244026 Accuracy 0.8818359375\n",
      "Iteration 32250 Training loss 0.004225555807352066 Validation loss 0.011333678849041462 Accuracy 0.87890625\n",
      "Iteration 32260 Training loss 0.003920887131243944 Validation loss 0.01071077398955822 Accuracy 0.8857421875\n",
      "Iteration 32270 Training loss 0.004407515749335289 Validation loss 0.010547948069870472 Accuracy 0.8876953125\n",
      "Iteration 32280 Training loss 0.006479048170149326 Validation loss 0.011335753835737705 Accuracy 0.8798828125\n",
      "Iteration 32290 Training loss 0.00478538079187274 Validation loss 0.01086372323334217 Accuracy 0.88427734375\n",
      "Iteration 32300 Training loss 0.006258392706513405 Validation loss 0.011302500031888485 Accuracy 0.87939453125\n",
      "Iteration 32310 Training loss 0.0043335240334272385 Validation loss 0.011119249276816845 Accuracy 0.8818359375\n",
      "Iteration 32320 Training loss 0.0026934235356748104 Validation loss 0.010931984521448612 Accuracy 0.88232421875\n",
      "Iteration 32330 Training loss 0.0046277763321995735 Validation loss 0.010832516476511955 Accuracy 0.8837890625\n",
      "Iteration 32340 Training loss 0.004722808953374624 Validation loss 0.01085522398352623 Accuracy 0.88427734375\n",
      "Iteration 32350 Training loss 0.004049745388329029 Validation loss 0.010831057094037533 Accuracy 0.884765625\n",
      "Iteration 32360 Training loss 0.0038746611680835485 Validation loss 0.01123164501041174 Accuracy 0.87939453125\n",
      "Iteration 32370 Training loss 0.004919133614748716 Validation loss 0.011224939487874508 Accuracy 0.88037109375\n",
      "Iteration 32380 Training loss 0.003970598801970482 Validation loss 0.01081396546214819 Accuracy 0.8837890625\n",
      "Iteration 32390 Training loss 0.005789207294583321 Validation loss 0.010752072557806969 Accuracy 0.884765625\n",
      "Iteration 32400 Training loss 0.004466718528419733 Validation loss 0.011045686900615692 Accuracy 0.88232421875\n",
      "Iteration 32410 Training loss 0.004132927395403385 Validation loss 0.010875711217522621 Accuracy 0.8828125\n",
      "Iteration 32420 Training loss 0.002871833508834243 Validation loss 0.010747932828962803 Accuracy 0.884765625\n",
      "Iteration 32430 Training loss 0.0033283112570643425 Validation loss 0.010711432434618473 Accuracy 0.88525390625\n",
      "Iteration 32440 Training loss 0.00601611565798521 Validation loss 0.011057589203119278 Accuracy 0.8818359375\n",
      "Iteration 32450 Training loss 0.005084620323032141 Validation loss 0.011022454127669334 Accuracy 0.8818359375\n",
      "Iteration 32460 Training loss 0.004398518707603216 Validation loss 0.01092294417321682 Accuracy 0.88330078125\n",
      "Iteration 32470 Training loss 0.004408855922520161 Validation loss 0.01081507746130228 Accuracy 0.88427734375\n",
      "Iteration 32480 Training loss 0.004215228836983442 Validation loss 0.010681762360036373 Accuracy 0.88671875\n",
      "Iteration 32490 Training loss 0.0050824303179979324 Validation loss 0.010984404012560844 Accuracy 0.88330078125\n",
      "Iteration 32500 Training loss 0.003947945777326822 Validation loss 0.01078780461102724 Accuracy 0.884765625\n",
      "Iteration 32510 Training loss 0.00476955808699131 Validation loss 0.010938703082501888 Accuracy 0.8828125\n",
      "Iteration 32520 Training loss 0.004354530479758978 Validation loss 0.011225280351936817 Accuracy 0.880859375\n",
      "Iteration 32530 Training loss 0.0044415900483727455 Validation loss 0.010677271522581577 Accuracy 0.8857421875\n",
      "Iteration 32540 Training loss 0.003996229264885187 Validation loss 0.010909689590334892 Accuracy 0.88427734375\n",
      "Iteration 32550 Training loss 0.003482840722426772 Validation loss 0.010619720444083214 Accuracy 0.88671875\n",
      "Iteration 32560 Training loss 0.00462913466617465 Validation loss 0.0106849679723382 Accuracy 0.88671875\n",
      "Iteration 32570 Training loss 0.004471719264984131 Validation loss 0.010913119651377201 Accuracy 0.88330078125\n",
      "Iteration 32580 Training loss 0.0037855191621929407 Validation loss 0.01047446858137846 Accuracy 0.88916015625\n",
      "Iteration 32590 Training loss 0.004434030503034592 Validation loss 0.010901901870965958 Accuracy 0.88330078125\n",
      "Iteration 32600 Training loss 0.004555427003651857 Validation loss 0.011055037379264832 Accuracy 0.8818359375\n",
      "Iteration 32610 Training loss 0.004556297790259123 Validation loss 0.010918264277279377 Accuracy 0.8828125\n",
      "Iteration 32620 Training loss 0.003432704834267497 Validation loss 0.010929894633591175 Accuracy 0.8828125\n",
      "Iteration 32630 Training loss 0.005237719044089317 Validation loss 0.010951080359518528 Accuracy 0.8828125\n",
      "Iteration 32640 Training loss 0.0061220149509608746 Validation loss 0.010952351614832878 Accuracy 0.8837890625\n",
      "Iteration 32650 Training loss 0.005278280004858971 Validation loss 0.01081344299018383 Accuracy 0.88330078125\n",
      "Iteration 32660 Training loss 0.004018843173980713 Validation loss 0.011071174405515194 Accuracy 0.88134765625\n",
      "Iteration 32670 Training loss 0.0043284217827022076 Validation loss 0.010993212461471558 Accuracy 0.8818359375\n",
      "Iteration 32680 Training loss 0.004761618562042713 Validation loss 0.011094614863395691 Accuracy 0.88232421875\n",
      "Iteration 32690 Training loss 0.004370864015072584 Validation loss 0.010781490243971348 Accuracy 0.884765625\n",
      "Iteration 32700 Training loss 0.003274446353316307 Validation loss 0.011022888123989105 Accuracy 0.8818359375\n",
      "Iteration 32710 Training loss 0.004327783361077309 Validation loss 0.010973017662763596 Accuracy 0.8828125\n",
      "Iteration 32720 Training loss 0.005031533073633909 Validation loss 0.01131435763090849 Accuracy 0.8798828125\n",
      "Iteration 32730 Training loss 0.0034304093569517136 Validation loss 0.010806691832840443 Accuracy 0.884765625\n",
      "Iteration 32740 Training loss 0.005746681243181229 Validation loss 0.01109572034329176 Accuracy 0.8818359375\n",
      "Iteration 32750 Training loss 0.005088745150715113 Validation loss 0.010988202877342701 Accuracy 0.88232421875\n",
      "Iteration 32760 Training loss 0.004674089141190052 Validation loss 0.011225550435483456 Accuracy 0.8798828125\n",
      "Iteration 32770 Training loss 0.00602866243571043 Validation loss 0.011137118563055992 Accuracy 0.880859375\n",
      "Iteration 32780 Training loss 0.0038157710805535316 Validation loss 0.010998928919434547 Accuracy 0.88330078125\n",
      "Iteration 32790 Training loss 0.004067955072969198 Validation loss 0.011027832515537739 Accuracy 0.88232421875\n",
      "Iteration 32800 Training loss 0.0054297661408782005 Validation loss 0.011040979996323586 Accuracy 0.88330078125\n",
      "Iteration 32810 Training loss 0.003435928840190172 Validation loss 0.010787682607769966 Accuracy 0.88427734375\n",
      "Iteration 32820 Training loss 0.0046866838820278645 Validation loss 0.010835767723619938 Accuracy 0.8837890625\n",
      "Iteration 32830 Training loss 0.005198596976697445 Validation loss 0.011068952269852161 Accuracy 0.88232421875\n",
      "Iteration 32840 Training loss 0.0037758664693683386 Validation loss 0.011029308661818504 Accuracy 0.88232421875\n",
      "Iteration 32850 Training loss 0.003656294196844101 Validation loss 0.01070279348641634 Accuracy 0.8857421875\n",
      "Iteration 32860 Training loss 0.003252635942772031 Validation loss 0.011233613826334476 Accuracy 0.87890625\n",
      "Iteration 32870 Training loss 0.003752944292500615 Validation loss 0.01064382866024971 Accuracy 0.88623046875\n",
      "Iteration 32880 Training loss 0.004136998206377029 Validation loss 0.010831503197550774 Accuracy 0.88427734375\n",
      "Iteration 32890 Training loss 0.0038380834739655256 Validation loss 0.010724467225372791 Accuracy 0.88525390625\n",
      "Iteration 32900 Training loss 0.004727063234895468 Validation loss 0.010746709071099758 Accuracy 0.88427734375\n",
      "Iteration 32910 Training loss 0.004604453220963478 Validation loss 0.011143451556563377 Accuracy 0.880859375\n",
      "Iteration 32920 Training loss 0.004631541669368744 Validation loss 0.010793864727020264 Accuracy 0.884765625\n",
      "Iteration 32930 Training loss 0.003627423895522952 Validation loss 0.01108162198215723 Accuracy 0.8818359375\n",
      "Iteration 32940 Training loss 0.003569595282897353 Validation loss 0.010862451046705246 Accuracy 0.8837890625\n",
      "Iteration 32950 Training loss 0.004242564085870981 Validation loss 0.010836588218808174 Accuracy 0.884765625\n",
      "Iteration 32960 Training loss 0.0038186025340110064 Validation loss 0.010680194944143295 Accuracy 0.88525390625\n",
      "Iteration 32970 Training loss 0.003839680226519704 Validation loss 0.010763873346149921 Accuracy 0.88525390625\n",
      "Iteration 32980 Training loss 0.0033206010702997446 Validation loss 0.0107376454398036 Accuracy 0.88623046875\n",
      "Iteration 32990 Training loss 0.003950855694711208 Validation loss 0.010628225281834602 Accuracy 0.88671875\n",
      "Iteration 33000 Training loss 0.0024702849332243204 Validation loss 0.010701566003262997 Accuracy 0.88427734375\n",
      "Iteration 33010 Training loss 0.003848726861178875 Validation loss 0.011030180379748344 Accuracy 0.8837890625\n",
      "Iteration 33020 Training loss 0.00515358243137598 Validation loss 0.010735942982137203 Accuracy 0.88525390625\n",
      "Iteration 33030 Training loss 0.00373170361854136 Validation loss 0.01097936648875475 Accuracy 0.88232421875\n",
      "Iteration 33040 Training loss 0.0046373396180570126 Validation loss 0.011101544834673405 Accuracy 0.880859375\n",
      "Iteration 33050 Training loss 0.005802375730127096 Validation loss 0.011002451181411743 Accuracy 0.88232421875\n",
      "Iteration 33060 Training loss 0.0035446512047201395 Validation loss 0.010946481488645077 Accuracy 0.8828125\n",
      "Iteration 33070 Training loss 0.0039151799865067005 Validation loss 0.010595700703561306 Accuracy 0.88720703125\n",
      "Iteration 33080 Training loss 0.003070816630497575 Validation loss 0.010642502456903458 Accuracy 0.8857421875\n",
      "Iteration 33090 Training loss 0.006220775656402111 Validation loss 0.010798564180731773 Accuracy 0.88427734375\n",
      "Iteration 33100 Training loss 0.0028962958604097366 Validation loss 0.010961931198835373 Accuracy 0.88330078125\n",
      "Iteration 33110 Training loss 0.0028196556959301233 Validation loss 0.010956325568258762 Accuracy 0.88232421875\n",
      "Iteration 33120 Training loss 0.0028297773096710443 Validation loss 0.010607575066387653 Accuracy 0.88623046875\n",
      "Iteration 33130 Training loss 0.003209441900253296 Validation loss 0.010991591960191727 Accuracy 0.8818359375\n",
      "Iteration 33140 Training loss 0.0029535649809986353 Validation loss 0.011105154640972614 Accuracy 0.88134765625\n",
      "Iteration 33150 Training loss 0.0040715476498007774 Validation loss 0.010852055624127388 Accuracy 0.884765625\n",
      "Iteration 33160 Training loss 0.004994231276214123 Validation loss 0.011163346469402313 Accuracy 0.88134765625\n",
      "Iteration 33170 Training loss 0.005246736574918032 Validation loss 0.011387697421014309 Accuracy 0.87890625\n",
      "Iteration 33180 Training loss 0.0037345504388213158 Validation loss 0.01078566163778305 Accuracy 0.884765625\n",
      "Iteration 33190 Training loss 0.0033912425860762596 Validation loss 0.010744764469563961 Accuracy 0.884765625\n",
      "Iteration 33200 Training loss 0.003615268040448427 Validation loss 0.010980380699038506 Accuracy 0.88232421875\n",
      "Iteration 33210 Training loss 0.004025888629257679 Validation loss 0.010692574083805084 Accuracy 0.8857421875\n",
      "Iteration 33220 Training loss 0.00352338794618845 Validation loss 0.010929650627076626 Accuracy 0.8837890625\n",
      "Iteration 33230 Training loss 0.003633905667811632 Validation loss 0.010690322145819664 Accuracy 0.8857421875\n",
      "Iteration 33240 Training loss 0.003989413380622864 Validation loss 0.010916011407971382 Accuracy 0.88330078125\n",
      "Iteration 33250 Training loss 0.0045518456026911736 Validation loss 0.010860924609005451 Accuracy 0.8837890625\n",
      "Iteration 33260 Training loss 0.003612356260418892 Validation loss 0.010630689561367035 Accuracy 0.88720703125\n",
      "Iteration 33270 Training loss 0.002819906920194626 Validation loss 0.01087268814444542 Accuracy 0.8837890625\n",
      "Iteration 33280 Training loss 0.004695784766227007 Validation loss 0.010846245102584362 Accuracy 0.8837890625\n",
      "Iteration 33290 Training loss 0.0039842319674789906 Validation loss 0.01081534381955862 Accuracy 0.8837890625\n",
      "Iteration 33300 Training loss 0.005377980414777994 Validation loss 0.010784359648823738 Accuracy 0.8837890625\n",
      "Iteration 33310 Training loss 0.0042716036550700665 Validation loss 0.011012404225766659 Accuracy 0.8818359375\n",
      "Iteration 33320 Training loss 0.004128015134483576 Validation loss 0.010639316402375698 Accuracy 0.8857421875\n",
      "Iteration 33330 Training loss 0.005490334704518318 Validation loss 0.010733290575444698 Accuracy 0.8857421875\n",
      "Iteration 33340 Training loss 0.004314555320888758 Validation loss 0.010697780176997185 Accuracy 0.884765625\n",
      "Iteration 33350 Training loss 0.003423704532906413 Validation loss 0.010586815886199474 Accuracy 0.88671875\n",
      "Iteration 33360 Training loss 0.0035565050784498453 Validation loss 0.010712162591516972 Accuracy 0.884765625\n",
      "Iteration 33370 Training loss 0.0022400866728276014 Validation loss 0.010749934241175652 Accuracy 0.884765625\n",
      "Iteration 33380 Training loss 0.0033880623523145914 Validation loss 0.010682749561965466 Accuracy 0.8857421875\n",
      "Iteration 33390 Training loss 0.004072363488376141 Validation loss 0.010782182216644287 Accuracy 0.88525390625\n",
      "Iteration 33400 Training loss 0.0034743594005703926 Validation loss 0.010934404097497463 Accuracy 0.8837890625\n",
      "Iteration 33410 Training loss 0.004012517165392637 Validation loss 0.010658390820026398 Accuracy 0.88671875\n",
      "Iteration 33420 Training loss 0.003746998030692339 Validation loss 0.010522580705583096 Accuracy 0.88720703125\n",
      "Iteration 33430 Training loss 0.0034578489139676094 Validation loss 0.010604627430438995 Accuracy 0.88720703125\n",
      "Iteration 33440 Training loss 0.00491076335310936 Validation loss 0.010810215026140213 Accuracy 0.88427734375\n",
      "Iteration 33450 Training loss 0.0031140141654759645 Validation loss 0.011228105053305626 Accuracy 0.87939453125\n",
      "Iteration 33460 Training loss 0.005564397666603327 Validation loss 0.010912366211414337 Accuracy 0.8828125\n",
      "Iteration 33470 Training loss 0.003958546556532383 Validation loss 0.010699967853724957 Accuracy 0.884765625\n",
      "Iteration 33480 Training loss 0.00397328520193696 Validation loss 0.010783437639474869 Accuracy 0.8837890625\n",
      "Iteration 33490 Training loss 0.004665007349103689 Validation loss 0.010900049470365047 Accuracy 0.88330078125\n",
      "Iteration 33500 Training loss 0.0027594028506428003 Validation loss 0.010959292761981487 Accuracy 0.8837890625\n",
      "Iteration 33510 Training loss 0.0031212009489536285 Validation loss 0.010982140898704529 Accuracy 0.88232421875\n",
      "Iteration 33520 Training loss 0.004973439034074545 Validation loss 0.010950585827231407 Accuracy 0.88232421875\n",
      "Iteration 33530 Training loss 0.004183343145996332 Validation loss 0.010821832343935966 Accuracy 0.88330078125\n",
      "Iteration 33540 Training loss 0.00406405096873641 Validation loss 0.010776513256132603 Accuracy 0.884765625\n",
      "Iteration 33550 Training loss 0.0049546463415026665 Validation loss 0.010931716300547123 Accuracy 0.88330078125\n",
      "Iteration 33560 Training loss 0.00440972251817584 Validation loss 0.010842653922736645 Accuracy 0.88427734375\n",
      "Iteration 33570 Training loss 0.00421183230355382 Validation loss 0.010635441169142723 Accuracy 0.88671875\n",
      "Iteration 33580 Training loss 0.005340487230569124 Validation loss 0.01090560108423233 Accuracy 0.8828125\n",
      "Iteration 33590 Training loss 0.003312312066555023 Validation loss 0.010805658996105194 Accuracy 0.88525390625\n",
      "Iteration 33600 Training loss 0.0039109098725020885 Validation loss 0.010908647440373898 Accuracy 0.8837890625\n",
      "Iteration 33610 Training loss 0.003929703030735254 Validation loss 0.010831701569259167 Accuracy 0.8837890625\n",
      "Iteration 33620 Training loss 0.003934184554964304 Validation loss 0.010723410174250603 Accuracy 0.8857421875\n",
      "Iteration 33630 Training loss 0.00501468637958169 Validation loss 0.010816257447004318 Accuracy 0.8837890625\n",
      "Iteration 33640 Training loss 0.005272608250379562 Validation loss 0.01111269649118185 Accuracy 0.880859375\n",
      "Iteration 33650 Training loss 0.004048841539770365 Validation loss 0.010752038098871708 Accuracy 0.88525390625\n",
      "Iteration 33660 Training loss 0.0039907703176140785 Validation loss 0.010730346664786339 Accuracy 0.88525390625\n",
      "Iteration 33670 Training loss 0.004992042668163776 Validation loss 0.011036170646548271 Accuracy 0.8818359375\n",
      "Iteration 33680 Training loss 0.003280091565102339 Validation loss 0.010715551674365997 Accuracy 0.88623046875\n",
      "Iteration 33690 Training loss 0.004729371517896652 Validation loss 0.010923155583441257 Accuracy 0.88330078125\n",
      "Iteration 33700 Training loss 0.0033117153216153383 Validation loss 0.010649400763213634 Accuracy 0.8857421875\n",
      "Iteration 33710 Training loss 0.003945090342313051 Validation loss 0.010694246739149094 Accuracy 0.8857421875\n",
      "Iteration 33720 Training loss 0.003367058001458645 Validation loss 0.010661757551133633 Accuracy 0.88623046875\n",
      "Iteration 33730 Training loss 0.0062856064178049564 Validation loss 0.011172765865921974 Accuracy 0.8818359375\n",
      "Iteration 33740 Training loss 0.0036620476748794317 Validation loss 0.010825887322425842 Accuracy 0.88427734375\n",
      "Iteration 33750 Training loss 0.003660490270704031 Validation loss 0.010839630849659443 Accuracy 0.8837890625\n",
      "Iteration 33760 Training loss 0.004040485247969627 Validation loss 0.0106351962313056 Accuracy 0.88623046875\n",
      "Iteration 33770 Training loss 0.003917057532817125 Validation loss 0.0108854491263628 Accuracy 0.8828125\n",
      "Iteration 33780 Training loss 0.004948843736201525 Validation loss 0.010818704962730408 Accuracy 0.884765625\n",
      "Iteration 33790 Training loss 0.004474584013223648 Validation loss 0.011019296944141388 Accuracy 0.8818359375\n",
      "Iteration 33800 Training loss 0.004848725162446499 Validation loss 0.01060874480754137 Accuracy 0.88818359375\n",
      "Iteration 33810 Training loss 0.004237025510519743 Validation loss 0.01072351448237896 Accuracy 0.8857421875\n",
      "Iteration 33820 Training loss 0.004263861104846001 Validation loss 0.010829446837306023 Accuracy 0.88427734375\n",
      "Iteration 33830 Training loss 0.0051608942449092865 Validation loss 0.01068209670484066 Accuracy 0.8857421875\n",
      "Iteration 33840 Training loss 0.004095019306987524 Validation loss 0.01072494313120842 Accuracy 0.88427734375\n",
      "Iteration 33850 Training loss 0.0028011936228722334 Validation loss 0.010833901353180408 Accuracy 0.88427734375\n",
      "Iteration 33860 Training loss 0.004279653541743755 Validation loss 0.010736840777099133 Accuracy 0.884765625\n",
      "Iteration 33870 Training loss 0.003970600198954344 Validation loss 0.011204764246940613 Accuracy 0.88037109375\n",
      "Iteration 33880 Training loss 0.0027757962234318256 Validation loss 0.010641010478138924 Accuracy 0.8857421875\n",
      "Iteration 33890 Training loss 0.004497618414461613 Validation loss 0.010746045038104057 Accuracy 0.8857421875\n",
      "Iteration 33900 Training loss 0.003988051787018776 Validation loss 0.01073980238288641 Accuracy 0.88525390625\n",
      "Iteration 33910 Training loss 0.002602776512503624 Validation loss 0.010684389621019363 Accuracy 0.88525390625\n",
      "Iteration 33920 Training loss 0.0033453062642365694 Validation loss 0.010843398049473763 Accuracy 0.8837890625\n",
      "Iteration 33930 Training loss 0.00425497954711318 Validation loss 0.011124026030302048 Accuracy 0.880859375\n",
      "Iteration 33940 Training loss 0.004085863009095192 Validation loss 0.010922165587544441 Accuracy 0.88427734375\n",
      "Iteration 33950 Training loss 0.004977871663868427 Validation loss 0.011244389228522778 Accuracy 0.88037109375\n",
      "Iteration 33960 Training loss 0.004596530459821224 Validation loss 0.010808886028826237 Accuracy 0.88427734375\n",
      "Iteration 33970 Training loss 0.003196943551301956 Validation loss 0.010860572569072247 Accuracy 0.88427734375\n",
      "Iteration 33980 Training loss 0.0043840110301971436 Validation loss 0.010746188461780548 Accuracy 0.88525390625\n",
      "Iteration 33990 Training loss 0.004258597269654274 Validation loss 0.011094927787780762 Accuracy 0.88232421875\n",
      "Iteration 34000 Training loss 0.0028519846964627504 Validation loss 0.010766090825200081 Accuracy 0.88525390625\n",
      "Iteration 34010 Training loss 0.0036055666860193014 Validation loss 0.010777665302157402 Accuracy 0.884765625\n",
      "Iteration 34020 Training loss 0.0017296220175921917 Validation loss 0.010636847466230392 Accuracy 0.88623046875\n",
      "Iteration 34030 Training loss 0.00436015147715807 Validation loss 0.010872932150959969 Accuracy 0.8837890625\n",
      "Iteration 34040 Training loss 0.00415085582062602 Validation loss 0.010770458728075027 Accuracy 0.8837890625\n",
      "Iteration 34050 Training loss 0.0037489801179617643 Validation loss 0.011028881184756756 Accuracy 0.88232421875\n",
      "Iteration 34060 Training loss 0.0027568244840949774 Validation loss 0.010524805635213852 Accuracy 0.88818359375\n",
      "Iteration 34070 Training loss 0.003652723506093025 Validation loss 0.01081712357699871 Accuracy 0.88427734375\n",
      "Iteration 34080 Training loss 0.00505864666774869 Validation loss 0.010964835993945599 Accuracy 0.88330078125\n",
      "Iteration 34090 Training loss 0.003291698405519128 Validation loss 0.01071025338023901 Accuracy 0.8857421875\n",
      "Iteration 34100 Training loss 0.0033863417338579893 Validation loss 0.010668774135410786 Accuracy 0.88427734375\n",
      "Iteration 34110 Training loss 0.0031702760607004166 Validation loss 0.010485044680535793 Accuracy 0.88720703125\n",
      "Iteration 34120 Training loss 0.004793370608240366 Validation loss 0.010672868229448795 Accuracy 0.88525390625\n",
      "Iteration 34130 Training loss 0.004195982590317726 Validation loss 0.010607192292809486 Accuracy 0.88671875\n",
      "Iteration 34140 Training loss 0.004721298813819885 Validation loss 0.011131828650832176 Accuracy 0.88037109375\n",
      "Iteration 34150 Training loss 0.003983092028647661 Validation loss 0.010526093654334545 Accuracy 0.88720703125\n",
      "Iteration 34160 Training loss 0.0034550291020423174 Validation loss 0.010598180815577507 Accuracy 0.88720703125\n",
      "Iteration 34170 Training loss 0.004159938544034958 Validation loss 0.010663808323442936 Accuracy 0.88525390625\n",
      "Iteration 34180 Training loss 0.0037573804147541523 Validation loss 0.010661930777132511 Accuracy 0.88671875\n",
      "Iteration 34190 Training loss 0.004542081151157618 Validation loss 0.010516955517232418 Accuracy 0.88720703125\n",
      "Iteration 34200 Training loss 0.001858071656897664 Validation loss 0.01075170747935772 Accuracy 0.88427734375\n",
      "Iteration 34210 Training loss 0.004519709385931492 Validation loss 0.010605229996144772 Accuracy 0.88671875\n",
      "Iteration 34220 Training loss 0.003032634500414133 Validation loss 0.010625412687659264 Accuracy 0.8857421875\n",
      "Iteration 34230 Training loss 0.003985768184065819 Validation loss 0.010761383920907974 Accuracy 0.88525390625\n",
      "Iteration 34240 Training loss 0.004193683620542288 Validation loss 0.010441751219332218 Accuracy 0.88916015625\n",
      "Iteration 34250 Training loss 0.00462412741035223 Validation loss 0.01086872536689043 Accuracy 0.8828125\n",
      "Iteration 34260 Training loss 0.0032750011887401342 Validation loss 0.010779890231788158 Accuracy 0.884765625\n",
      "Iteration 34270 Training loss 0.004586648195981979 Validation loss 0.010794973000884056 Accuracy 0.88427734375\n",
      "Iteration 34280 Training loss 0.003568991320207715 Validation loss 0.010900571011006832 Accuracy 0.88330078125\n",
      "Iteration 34290 Training loss 0.0024436863604933023 Validation loss 0.010609986260533333 Accuracy 0.88623046875\n",
      "Iteration 34300 Training loss 0.0033922067377716303 Validation loss 0.010955603793263435 Accuracy 0.8828125\n",
      "Iteration 34310 Training loss 0.004014408215880394 Validation loss 0.01072665210813284 Accuracy 0.884765625\n",
      "Iteration 34320 Training loss 0.003992513753473759 Validation loss 0.01090798806399107 Accuracy 0.88330078125\n",
      "Iteration 34330 Training loss 0.00444211158901453 Validation loss 0.010740506462752819 Accuracy 0.8857421875\n",
      "Iteration 34340 Training loss 0.0037417407147586346 Validation loss 0.010817743837833405 Accuracy 0.88525390625\n",
      "Iteration 34350 Training loss 0.0027181352488696575 Validation loss 0.011095789261162281 Accuracy 0.8818359375\n",
      "Iteration 34360 Training loss 0.003975201863795519 Validation loss 0.011006074957549572 Accuracy 0.88232421875\n",
      "Iteration 34370 Training loss 0.00428090849891305 Validation loss 0.010656053200364113 Accuracy 0.88720703125\n",
      "Iteration 34380 Training loss 0.0040893107652664185 Validation loss 0.010834538377821445 Accuracy 0.88427734375\n",
      "Iteration 34390 Training loss 0.0050042057409882545 Validation loss 0.010896110907196999 Accuracy 0.8828125\n",
      "Iteration 34400 Training loss 0.0040376088581979275 Validation loss 0.010442324914038181 Accuracy 0.88818359375\n",
      "Iteration 34410 Training loss 0.0038953889161348343 Validation loss 0.01076117530465126 Accuracy 0.8857421875\n",
      "Iteration 34420 Training loss 0.0038531608879566193 Validation loss 0.010653247125446796 Accuracy 0.88671875\n",
      "Iteration 34430 Training loss 0.0032801583874970675 Validation loss 0.010821598581969738 Accuracy 0.884765625\n",
      "Iteration 34440 Training loss 0.0048348503187298775 Validation loss 0.01056670118123293 Accuracy 0.88671875\n",
      "Iteration 34450 Training loss 0.003610792802646756 Validation loss 0.010480890981853008 Accuracy 0.8876953125\n",
      "Iteration 34460 Training loss 0.006085254717618227 Validation loss 0.01067467499524355 Accuracy 0.88671875\n",
      "Iteration 34470 Training loss 0.005653237458318472 Validation loss 0.01096299383789301 Accuracy 0.8837890625\n",
      "Iteration 34480 Training loss 0.003356215776875615 Validation loss 0.01052769459784031 Accuracy 0.88671875\n",
      "Iteration 34490 Training loss 0.004163626115769148 Validation loss 0.010639604181051254 Accuracy 0.88671875\n",
      "Iteration 34500 Training loss 0.001967987045645714 Validation loss 0.010869953781366348 Accuracy 0.88427734375\n",
      "Iteration 34510 Training loss 0.003410096513107419 Validation loss 0.010427983477711678 Accuracy 0.8876953125\n",
      "Iteration 34520 Training loss 0.004115809220820665 Validation loss 0.010609985329210758 Accuracy 0.88623046875\n",
      "Iteration 34530 Training loss 0.004317977465689182 Validation loss 0.010569603182375431 Accuracy 0.88623046875\n",
      "Iteration 34540 Training loss 0.005076994653791189 Validation loss 0.010714193806052208 Accuracy 0.8857421875\n",
      "Iteration 34550 Training loss 0.003717706073075533 Validation loss 0.010690978728234768 Accuracy 0.8857421875\n",
      "Iteration 34560 Training loss 0.005011768080294132 Validation loss 0.010934468358755112 Accuracy 0.8828125\n",
      "Iteration 34570 Training loss 0.004589442629367113 Validation loss 0.01077751163393259 Accuracy 0.88427734375\n",
      "Iteration 34580 Training loss 0.004808942321687937 Validation loss 0.010731556452810764 Accuracy 0.88525390625\n",
      "Iteration 34590 Training loss 0.004740843083709478 Validation loss 0.010504860430955887 Accuracy 0.88720703125\n",
      "Iteration 34600 Training loss 0.00416801730170846 Validation loss 0.010722805745899677 Accuracy 0.8857421875\n",
      "Iteration 34610 Training loss 0.005309318657964468 Validation loss 0.010738274082541466 Accuracy 0.8857421875\n",
      "Iteration 34620 Training loss 0.003459185129031539 Validation loss 0.010609938763082027 Accuracy 0.88671875\n",
      "Iteration 34630 Training loss 0.0034425207413733006 Validation loss 0.010495198890566826 Accuracy 0.88720703125\n",
      "Iteration 34640 Training loss 0.002729948377236724 Validation loss 0.010597281157970428 Accuracy 0.88671875\n",
      "Iteration 34650 Training loss 0.004304047673940659 Validation loss 0.010657568462193012 Accuracy 0.88671875\n",
      "Iteration 34660 Training loss 0.0035735988058149815 Validation loss 0.01092507317662239 Accuracy 0.88427734375\n",
      "Iteration 34670 Training loss 0.004146484192460775 Validation loss 0.01076589897274971 Accuracy 0.884765625\n",
      "Iteration 34680 Training loss 0.004020096268504858 Validation loss 0.010549260303378105 Accuracy 0.88623046875\n",
      "Iteration 34690 Training loss 0.004308838397264481 Validation loss 0.010702072642743587 Accuracy 0.88525390625\n",
      "Iteration 34700 Training loss 0.004919493570923805 Validation loss 0.010620733723044395 Accuracy 0.88623046875\n",
      "Iteration 34710 Training loss 0.004176286514848471 Validation loss 0.010652289725840092 Accuracy 0.8857421875\n",
      "Iteration 34720 Training loss 0.005780432373285294 Validation loss 0.010713648051023483 Accuracy 0.88427734375\n",
      "Iteration 34730 Training loss 0.003936357796192169 Validation loss 0.01089477725327015 Accuracy 0.8837890625\n",
      "Iteration 34740 Training loss 0.003492276882752776 Validation loss 0.010961857624351978 Accuracy 0.88330078125\n",
      "Iteration 34750 Training loss 0.003210330381989479 Validation loss 0.010633722878992558 Accuracy 0.88671875\n",
      "Iteration 34760 Training loss 0.0035892436280846596 Validation loss 0.010918903164565563 Accuracy 0.8828125\n",
      "Iteration 34770 Training loss 0.0035944196861237288 Validation loss 0.010764162056148052 Accuracy 0.884765625\n",
      "Iteration 34780 Training loss 0.004044190514832735 Validation loss 0.01085766963660717 Accuracy 0.88330078125\n",
      "Iteration 34790 Training loss 0.0030972864478826523 Validation loss 0.010578930377960205 Accuracy 0.88720703125\n",
      "Iteration 34800 Training loss 0.0044835819862782955 Validation loss 0.010903632268309593 Accuracy 0.8828125\n",
      "Iteration 34810 Training loss 0.003939968068152666 Validation loss 0.010705915279686451 Accuracy 0.8857421875\n",
      "Iteration 34820 Training loss 0.002311276737600565 Validation loss 0.010807658545672894 Accuracy 0.8837890625\n",
      "Iteration 34830 Training loss 0.004708480555564165 Validation loss 0.010857131332159042 Accuracy 0.8837890625\n",
      "Iteration 34840 Training loss 0.0028771287761628628 Validation loss 0.010608497075736523 Accuracy 0.88671875\n",
      "Iteration 34850 Training loss 0.0038815808948129416 Validation loss 0.010982723906636238 Accuracy 0.8828125\n",
      "Iteration 34860 Training loss 0.0033305182587355375 Validation loss 0.010748068802058697 Accuracy 0.8857421875\n",
      "Iteration 34870 Training loss 0.004664006642997265 Validation loss 0.010744498111307621 Accuracy 0.88525390625\n",
      "Iteration 34880 Training loss 0.0038612298667430878 Validation loss 0.010767120867967606 Accuracy 0.884765625\n",
      "Iteration 34890 Training loss 0.0039131250232458115 Validation loss 0.010638583451509476 Accuracy 0.88525390625\n",
      "Iteration 34900 Training loss 0.005134129896759987 Validation loss 0.010812672786414623 Accuracy 0.8857421875\n",
      "Iteration 34910 Training loss 0.0021646814420819283 Validation loss 0.010512595064938068 Accuracy 0.88818359375\n",
      "Iteration 34920 Training loss 0.003420647932216525 Validation loss 0.010934839025139809 Accuracy 0.88330078125\n",
      "Iteration 34930 Training loss 0.005173624958842993 Validation loss 0.011104047298431396 Accuracy 0.88037109375\n",
      "Iteration 34940 Training loss 0.004802670329809189 Validation loss 0.01128776092082262 Accuracy 0.87939453125\n",
      "Iteration 34950 Training loss 0.0044008283875882626 Validation loss 0.010984046384692192 Accuracy 0.8828125\n",
      "Iteration 34960 Training loss 0.0032494785264134407 Validation loss 0.01063966192305088 Accuracy 0.8857421875\n",
      "Iteration 34970 Training loss 0.004094126168638468 Validation loss 0.01080322265625 Accuracy 0.884765625\n",
      "Iteration 34980 Training loss 0.002725728088989854 Validation loss 0.01106314454227686 Accuracy 0.88232421875\n",
      "Iteration 34990 Training loss 0.003907849546521902 Validation loss 0.010877592489123344 Accuracy 0.88427734375\n",
      "Iteration 35000 Training loss 0.003981495276093483 Validation loss 0.01068701408803463 Accuracy 0.88623046875\n",
      "Iteration 35010 Training loss 0.0036080130375921726 Validation loss 0.010718139819800854 Accuracy 0.88623046875\n",
      "Iteration 35020 Training loss 0.00280218874104321 Validation loss 0.010534345172345638 Accuracy 0.88818359375\n",
      "Iteration 35030 Training loss 0.003842848353087902 Validation loss 0.010842076502740383 Accuracy 0.88427734375\n",
      "Iteration 35040 Training loss 0.00641426257789135 Validation loss 0.011155541986227036 Accuracy 0.88134765625\n",
      "Iteration 35050 Training loss 0.003977867774665356 Validation loss 0.011250981129705906 Accuracy 0.88037109375\n",
      "Iteration 35060 Training loss 0.0031793073285371065 Validation loss 0.010710970498621464 Accuracy 0.884765625\n",
      "Iteration 35070 Training loss 0.0036270199343562126 Validation loss 0.010585074312984943 Accuracy 0.88623046875\n",
      "Iteration 35080 Training loss 0.003745259018614888 Validation loss 0.010890553705394268 Accuracy 0.8828125\n",
      "Iteration 35090 Training loss 0.002470561536028981 Validation loss 0.01086230669170618 Accuracy 0.8837890625\n",
      "Iteration 35100 Training loss 0.003844399005174637 Validation loss 0.01087123341858387 Accuracy 0.8837890625\n",
      "Iteration 35110 Training loss 0.002966483822092414 Validation loss 0.010751388035714626 Accuracy 0.884765625\n",
      "Iteration 35120 Training loss 0.0035536193754523993 Validation loss 0.010607787407934666 Accuracy 0.88720703125\n",
      "Iteration 35130 Training loss 0.00529605895280838 Validation loss 0.010611995123326778 Accuracy 0.88671875\n",
      "Iteration 35140 Training loss 0.004225662909448147 Validation loss 0.010425186716020107 Accuracy 0.888671875\n",
      "Iteration 35150 Training loss 0.004727164749056101 Validation loss 0.010662255808711052 Accuracy 0.88623046875\n",
      "Iteration 35160 Training loss 0.003032383508980274 Validation loss 0.010470635257661343 Accuracy 0.8876953125\n",
      "Iteration 35170 Training loss 0.004634442739188671 Validation loss 0.01043753232806921 Accuracy 0.888671875\n",
      "Iteration 35180 Training loss 0.004464397206902504 Validation loss 0.010544857010245323 Accuracy 0.88623046875\n",
      "Iteration 35190 Training loss 0.0053275274112820625 Validation loss 0.010654345154762268 Accuracy 0.88671875\n",
      "Iteration 35200 Training loss 0.003310216823592782 Validation loss 0.010842653922736645 Accuracy 0.8837890625\n",
      "Iteration 35210 Training loss 0.0031639747321605682 Validation loss 0.01067531667649746 Accuracy 0.88671875\n",
      "Iteration 35220 Training loss 0.003978177439421415 Validation loss 0.010682487860321999 Accuracy 0.88525390625\n",
      "Iteration 35230 Training loss 0.0035616550594568253 Validation loss 0.010660006664693356 Accuracy 0.88623046875\n",
      "Iteration 35240 Training loss 0.003831845475360751 Validation loss 0.01059463620185852 Accuracy 0.88623046875\n",
      "Iteration 35250 Training loss 0.0029408843256533146 Validation loss 0.0105547234416008 Accuracy 0.8876953125\n",
      "Iteration 35260 Training loss 0.0040196385234594345 Validation loss 0.010698825120925903 Accuracy 0.8857421875\n",
      "Iteration 35270 Training loss 0.004745462443679571 Validation loss 0.01077356655150652 Accuracy 0.884765625\n",
      "Iteration 35280 Training loss 0.0028672395274043083 Validation loss 0.01065797172486782 Accuracy 0.88525390625\n",
      "Iteration 35290 Training loss 0.003915665205568075 Validation loss 0.010874169878661633 Accuracy 0.88427734375\n",
      "Iteration 35300 Training loss 0.002381540834903717 Validation loss 0.010798361152410507 Accuracy 0.88427734375\n",
      "Iteration 35310 Training loss 0.003231555223464966 Validation loss 0.01090123038738966 Accuracy 0.8837890625\n",
      "Iteration 35320 Training loss 0.0034168274141848087 Validation loss 0.01069610845297575 Accuracy 0.88623046875\n",
      "Iteration 35330 Training loss 0.004374855197966099 Validation loss 0.010688964277505875 Accuracy 0.8857421875\n",
      "Iteration 35340 Training loss 0.0033879552502185106 Validation loss 0.011181759648025036 Accuracy 0.88037109375\n",
      "Iteration 35350 Training loss 0.0035274671390652657 Validation loss 0.01053603459149599 Accuracy 0.8857421875\n",
      "Iteration 35360 Training loss 0.0034938398748636246 Validation loss 0.010507708415389061 Accuracy 0.88720703125\n",
      "Iteration 35370 Training loss 0.003633936634287238 Validation loss 0.010785551741719246 Accuracy 0.884765625\n",
      "Iteration 35380 Training loss 0.004308589734137058 Validation loss 0.010527340695261955 Accuracy 0.88720703125\n",
      "Iteration 35390 Training loss 0.003659340785816312 Validation loss 0.010855874046683311 Accuracy 0.8837890625\n",
      "Iteration 35400 Training loss 0.004242760129272938 Validation loss 0.010735002346336842 Accuracy 0.88427734375\n",
      "Iteration 35410 Training loss 0.0028201246168464422 Validation loss 0.010495348833501339 Accuracy 0.8876953125\n",
      "Iteration 35420 Training loss 0.0035618040710687637 Validation loss 0.010648906230926514 Accuracy 0.88671875\n",
      "Iteration 35430 Training loss 0.004679340403527021 Validation loss 0.010602914728224277 Accuracy 0.88623046875\n",
      "Iteration 35440 Training loss 0.003342516254633665 Validation loss 0.010743907652795315 Accuracy 0.8857421875\n",
      "Iteration 35450 Training loss 0.004330274648964405 Validation loss 0.010674314573407173 Accuracy 0.88671875\n",
      "Iteration 35460 Training loss 0.003273705020546913 Validation loss 0.010626726783812046 Accuracy 0.88623046875\n",
      "Iteration 35470 Training loss 0.00408944021910429 Validation loss 0.010707796551287174 Accuracy 0.8837890625\n",
      "Iteration 35480 Training loss 0.003012055065482855 Validation loss 0.010656788945198059 Accuracy 0.88623046875\n",
      "Iteration 35490 Training loss 0.004463179036974907 Validation loss 0.010697314515709877 Accuracy 0.884765625\n",
      "Iteration 35500 Training loss 0.0031552205327898264 Validation loss 0.010452832095324993 Accuracy 0.88720703125\n",
      "Iteration 35510 Training loss 0.004248297773301601 Validation loss 0.010600988753139973 Accuracy 0.88623046875\n",
      "Iteration 35520 Training loss 0.003913385793566704 Validation loss 0.010506799444556236 Accuracy 0.88720703125\n",
      "Iteration 35530 Training loss 0.003164831083267927 Validation loss 0.010568317957222462 Accuracy 0.88623046875\n",
      "Iteration 35540 Training loss 0.00512858759611845 Validation loss 0.010705034248530865 Accuracy 0.884765625\n",
      "Iteration 35550 Training loss 0.002965874271467328 Validation loss 0.01047747302800417 Accuracy 0.8876953125\n",
      "Iteration 35560 Training loss 0.0036661310587078333 Validation loss 0.010669810697436333 Accuracy 0.88525390625\n",
      "Iteration 35570 Training loss 0.0030068291816860437 Validation loss 0.010879010893404484 Accuracy 0.88330078125\n",
      "Iteration 35580 Training loss 0.003227442502975464 Validation loss 0.010730606503784657 Accuracy 0.88623046875\n",
      "Iteration 35590 Training loss 0.004942281637340784 Validation loss 0.01073923148214817 Accuracy 0.88525390625\n",
      "Iteration 35600 Training loss 0.0043551078997552395 Validation loss 0.010582407005131245 Accuracy 0.88671875\n",
      "Iteration 35610 Training loss 0.004343243781477213 Validation loss 0.010911609046161175 Accuracy 0.8837890625\n",
      "Iteration 35620 Training loss 0.004098216537386179 Validation loss 0.011025781743228436 Accuracy 0.88232421875\n",
      "Iteration 35630 Training loss 0.003904384560883045 Validation loss 0.010722331702709198 Accuracy 0.884765625\n",
      "Iteration 35640 Training loss 0.003676195628941059 Validation loss 0.0110397320240736 Accuracy 0.8818359375\n",
      "Iteration 35650 Training loss 0.004716707859188318 Validation loss 0.01089246105402708 Accuracy 0.8837890625\n",
      "Iteration 35660 Training loss 0.00428858632221818 Validation loss 0.010914078913629055 Accuracy 0.8837890625\n",
      "Iteration 35670 Training loss 0.003790342016145587 Validation loss 0.010810400359332561 Accuracy 0.8837890625\n",
      "Iteration 35680 Training loss 0.004402414429932833 Validation loss 0.010782519355416298 Accuracy 0.88427734375\n",
      "Iteration 35690 Training loss 0.004254714585840702 Validation loss 0.010564088821411133 Accuracy 0.88623046875\n",
      "Iteration 35700 Training loss 0.004516506567597389 Validation loss 0.010675751604139805 Accuracy 0.88525390625\n",
      "Iteration 35710 Training loss 0.003223998239263892 Validation loss 0.010823879390954971 Accuracy 0.88330078125\n",
      "Iteration 35720 Training loss 0.004135398659855127 Validation loss 0.0107726464048028 Accuracy 0.8837890625\n",
      "Iteration 35730 Training loss 0.0037541782949119806 Validation loss 0.010815690271556377 Accuracy 0.88427734375\n",
      "Iteration 35740 Training loss 0.003243156475946307 Validation loss 0.010712952353060246 Accuracy 0.88525390625\n",
      "Iteration 35750 Training loss 0.0035531367175281048 Validation loss 0.010860729962587357 Accuracy 0.8837890625\n",
      "Iteration 35760 Training loss 0.003567994339391589 Validation loss 0.010809347964823246 Accuracy 0.884765625\n",
      "Iteration 35770 Training loss 0.003509103087708354 Validation loss 0.010738871991634369 Accuracy 0.88525390625\n",
      "Iteration 35780 Training loss 0.002556792227551341 Validation loss 0.010679779574275017 Accuracy 0.88623046875\n",
      "Iteration 35790 Training loss 0.0034927462693303823 Validation loss 0.011097094044089317 Accuracy 0.8818359375\n",
      "Iteration 35800 Training loss 0.00456753745675087 Validation loss 0.010815047658979893 Accuracy 0.8837890625\n",
      "Iteration 35810 Training loss 0.0046477061696350574 Validation loss 0.010682868771255016 Accuracy 0.88525390625\n",
      "Iteration 35820 Training loss 0.0033061103895306587 Validation loss 0.010949073359370232 Accuracy 0.8837890625\n",
      "Iteration 35830 Training loss 0.0036126377526670694 Validation loss 0.010448700748383999 Accuracy 0.88916015625\n",
      "Iteration 35840 Training loss 0.0023154255468398333 Validation loss 0.01053767092525959 Accuracy 0.8876953125\n",
      "Iteration 35850 Training loss 0.0031691938638687134 Validation loss 0.01044089812785387 Accuracy 0.88818359375\n",
      "Iteration 35860 Training loss 0.0027825774159282446 Validation loss 0.010731462389230728 Accuracy 0.88427734375\n",
      "Iteration 35870 Training loss 0.003668418852612376 Validation loss 0.010799895040690899 Accuracy 0.88330078125\n",
      "Iteration 35880 Training loss 0.00515328673645854 Validation loss 0.010698813013732433 Accuracy 0.8857421875\n",
      "Iteration 35890 Training loss 0.002477058907970786 Validation loss 0.010473818518221378 Accuracy 0.88720703125\n",
      "Iteration 35900 Training loss 0.0043205395340919495 Validation loss 0.010753671638667583 Accuracy 0.884765625\n",
      "Iteration 35910 Training loss 0.0032395769376307726 Validation loss 0.010964426212012768 Accuracy 0.88232421875\n",
      "Iteration 35920 Training loss 0.004010110627859831 Validation loss 0.010741855017840862 Accuracy 0.88525390625\n",
      "Iteration 35930 Training loss 0.0024640585761517286 Validation loss 0.010532980784773827 Accuracy 0.88671875\n",
      "Iteration 35940 Training loss 0.002332933945581317 Validation loss 0.010450766421854496 Accuracy 0.88818359375\n",
      "Iteration 35950 Training loss 0.00326215079985559 Validation loss 0.010395695455372334 Accuracy 0.88818359375\n",
      "Iteration 35960 Training loss 0.004498028662055731 Validation loss 0.010735532268881798 Accuracy 0.88525390625\n",
      "Iteration 35970 Training loss 0.003636040957644582 Validation loss 0.010647078976035118 Accuracy 0.88623046875\n",
      "Iteration 35980 Training loss 0.002964960178360343 Validation loss 0.010583302937448025 Accuracy 0.88623046875\n",
      "Iteration 35990 Training loss 0.004759072791785002 Validation loss 0.010514536872506142 Accuracy 0.88818359375\n",
      "Iteration 36000 Training loss 0.0031433654949069023 Validation loss 0.010600373148918152 Accuracy 0.8876953125\n",
      "Iteration 36010 Training loss 0.002913164673373103 Validation loss 0.010679539293050766 Accuracy 0.88525390625\n",
      "Iteration 36020 Training loss 0.002736894879490137 Validation loss 0.01075492613017559 Accuracy 0.88427734375\n",
      "Iteration 36030 Training loss 0.005297939293086529 Validation loss 0.011054903268814087 Accuracy 0.8828125\n",
      "Iteration 36040 Training loss 0.0038161328993737698 Validation loss 0.011012006551027298 Accuracy 0.88134765625\n",
      "Iteration 36050 Training loss 0.00426826998591423 Validation loss 0.010664111003279686 Accuracy 0.88671875\n",
      "Iteration 36060 Training loss 0.004576284438371658 Validation loss 0.010954853147268295 Accuracy 0.88232421875\n",
      "Iteration 36070 Training loss 0.004718192853033543 Validation loss 0.010662108659744263 Accuracy 0.8857421875\n",
      "Iteration 36080 Training loss 0.001706258743070066 Validation loss 0.01060121413320303 Accuracy 0.88720703125\n",
      "Iteration 36090 Training loss 0.004346083849668503 Validation loss 0.010720604099333286 Accuracy 0.8857421875\n",
      "Iteration 36100 Training loss 0.0040605501271784306 Validation loss 0.010774074122309685 Accuracy 0.88427734375\n",
      "Iteration 36110 Training loss 0.004487066064029932 Validation loss 0.01092839241027832 Accuracy 0.8828125\n",
      "Iteration 36120 Training loss 0.004545832052826881 Validation loss 0.010882368311285973 Accuracy 0.8828125\n",
      "Iteration 36130 Training loss 0.003543820697814226 Validation loss 0.01064027938991785 Accuracy 0.884765625\n",
      "Iteration 36140 Training loss 0.00334048829972744 Validation loss 0.010553374886512756 Accuracy 0.88623046875\n",
      "Iteration 36150 Training loss 0.0036991785746067762 Validation loss 0.01085322443395853 Accuracy 0.8837890625\n",
      "Iteration 36160 Training loss 0.002913795178756118 Validation loss 0.010701720602810383 Accuracy 0.8857421875\n",
      "Iteration 36170 Training loss 0.0021013172809034586 Validation loss 0.010498707182705402 Accuracy 0.88818359375\n",
      "Iteration 36180 Training loss 0.003141904715448618 Validation loss 0.010562984272837639 Accuracy 0.8876953125\n",
      "Iteration 36190 Training loss 0.0038991656620055437 Validation loss 0.010567864403128624 Accuracy 0.8876953125\n",
      "Iteration 36200 Training loss 0.002814892213791609 Validation loss 0.01072164811193943 Accuracy 0.8857421875\n",
      "Iteration 36210 Training loss 0.003918673377484083 Validation loss 0.010664449073374271 Accuracy 0.88671875\n",
      "Iteration 36220 Training loss 0.00341430539265275 Validation loss 0.010689537972211838 Accuracy 0.8857421875\n",
      "Iteration 36230 Training loss 0.0042053209617733955 Validation loss 0.010679694823920727 Accuracy 0.8857421875\n",
      "Iteration 36240 Training loss 0.003965457901358604 Validation loss 0.010473400354385376 Accuracy 0.88720703125\n",
      "Iteration 36250 Training loss 0.004900349769741297 Validation loss 0.010853617452085018 Accuracy 0.8837890625\n",
      "Iteration 36260 Training loss 0.0032997499220073223 Validation loss 0.010549607686698437 Accuracy 0.8876953125\n",
      "Iteration 36270 Training loss 0.004997747018933296 Validation loss 0.010669332928955555 Accuracy 0.884765625\n",
      "Iteration 36280 Training loss 0.003384241135790944 Validation loss 0.010688884183764458 Accuracy 0.88671875\n",
      "Iteration 36290 Training loss 0.0052446285262703896 Validation loss 0.010696444660425186 Accuracy 0.88525390625\n",
      "Iteration 36300 Training loss 0.004410683177411556 Validation loss 0.010656972415745258 Accuracy 0.88623046875\n",
      "Iteration 36310 Training loss 0.003532655304297805 Validation loss 0.010616518557071686 Accuracy 0.88671875\n",
      "Iteration 36320 Training loss 0.004749819170683622 Validation loss 0.010588652454316616 Accuracy 0.88720703125\n",
      "Iteration 36330 Training loss 0.0020876999478787184 Validation loss 0.010667634196579456 Accuracy 0.88525390625\n",
      "Iteration 36340 Training loss 0.004750593099743128 Validation loss 0.010665018111467361 Accuracy 0.88623046875\n",
      "Iteration 36350 Training loss 0.0038364154752343893 Validation loss 0.010970763862133026 Accuracy 0.88232421875\n",
      "Iteration 36360 Training loss 0.003897078800946474 Validation loss 0.010768495500087738 Accuracy 0.88525390625\n",
      "Iteration 36370 Training loss 0.002745910082012415 Validation loss 0.010764742270112038 Accuracy 0.8837890625\n",
      "Iteration 36380 Training loss 0.005975375883281231 Validation loss 0.010946597903966904 Accuracy 0.8837890625\n",
      "Iteration 36390 Training loss 0.004983499646186829 Validation loss 0.01054054033011198 Accuracy 0.88818359375\n",
      "Iteration 36400 Training loss 0.0034635858610272408 Validation loss 0.010764660313725471 Accuracy 0.8837890625\n",
      "Iteration 36410 Training loss 0.005143146030604839 Validation loss 0.010645104572176933 Accuracy 0.88671875\n",
      "Iteration 36420 Training loss 0.0028082123026251793 Validation loss 0.010534721426665783 Accuracy 0.88720703125\n",
      "Iteration 36430 Training loss 0.004235481843352318 Validation loss 0.010585547424852848 Accuracy 0.8857421875\n",
      "Iteration 36440 Training loss 0.0032279007136821747 Validation loss 0.010467428714036942 Accuracy 0.88720703125\n",
      "Iteration 36450 Training loss 0.0049466597847640514 Validation loss 0.010805011726915836 Accuracy 0.884765625\n",
      "Iteration 36460 Training loss 0.0033675094600766897 Validation loss 0.010595371015369892 Accuracy 0.88623046875\n",
      "Iteration 36470 Training loss 0.0038405098021030426 Validation loss 0.010674513876438141 Accuracy 0.8857421875\n",
      "Iteration 36480 Training loss 0.003650421043857932 Validation loss 0.010452516376972198 Accuracy 0.88818359375\n",
      "Iteration 36490 Training loss 0.0035870710853487253 Validation loss 0.01053654309362173 Accuracy 0.88671875\n",
      "Iteration 36500 Training loss 0.0029753046110272408 Validation loss 0.01063003670424223 Accuracy 0.88720703125\n",
      "Iteration 36510 Training loss 0.0028548771515488625 Validation loss 0.010499925352633 Accuracy 0.88671875\n",
      "Iteration 36520 Training loss 0.0026308957021683455 Validation loss 0.010522028431296349 Accuracy 0.88818359375\n",
      "Iteration 36530 Training loss 0.0035645095631480217 Validation loss 0.010447812266647816 Accuracy 0.88818359375\n",
      "Iteration 36540 Training loss 0.0025277745444327593 Validation loss 0.010452847927808762 Accuracy 0.8876953125\n",
      "Iteration 36550 Training loss 0.003618543967604637 Validation loss 0.010542514733970165 Accuracy 0.88720703125\n",
      "Iteration 36560 Training loss 0.005518861580640078 Validation loss 0.010784158483147621 Accuracy 0.884765625\n",
      "Iteration 36570 Training loss 0.005025338847190142 Validation loss 0.010609987191855907 Accuracy 0.8876953125\n",
      "Iteration 36580 Training loss 0.004529902711510658 Validation loss 0.01079582516103983 Accuracy 0.88427734375\n",
      "Iteration 36590 Training loss 0.0038801743648946285 Validation loss 0.010475724004209042 Accuracy 0.88720703125\n",
      "Iteration 36600 Training loss 0.003690374316647649 Validation loss 0.0105948681011796 Accuracy 0.88623046875\n",
      "Iteration 36610 Training loss 0.002808949211612344 Validation loss 0.010572419501841068 Accuracy 0.8876953125\n",
      "Iteration 36620 Training loss 0.003008126514032483 Validation loss 0.01048513688147068 Accuracy 0.88720703125\n",
      "Iteration 36630 Training loss 0.004726713988929987 Validation loss 0.010451732203364372 Accuracy 0.88818359375\n",
      "Iteration 36640 Training loss 0.0021559062879532576 Validation loss 0.010587800294160843 Accuracy 0.88671875\n",
      "Iteration 36650 Training loss 0.0031345500610768795 Validation loss 0.010567289777100086 Accuracy 0.88671875\n",
      "Iteration 36660 Training loss 0.003288974752649665 Validation loss 0.010645541362464428 Accuracy 0.88671875\n",
      "Iteration 36670 Training loss 0.0036397892981767654 Validation loss 0.010549964383244514 Accuracy 0.88623046875\n",
      "Iteration 36680 Training loss 0.0030620836187154055 Validation loss 0.010647309012711048 Accuracy 0.88623046875\n",
      "Iteration 36690 Training loss 0.0024291719309985638 Validation loss 0.01060700137168169 Accuracy 0.88525390625\n",
      "Iteration 36700 Training loss 0.004054227378219366 Validation loss 0.01116152759641409 Accuracy 0.880859375\n",
      "Iteration 36710 Training loss 0.0037848185747861862 Validation loss 0.010802402161061764 Accuracy 0.88330078125\n",
      "Iteration 36720 Training loss 0.003934698179364204 Validation loss 0.010786386206746101 Accuracy 0.8837890625\n",
      "Iteration 36730 Training loss 0.0032127241138368845 Validation loss 0.011233046650886536 Accuracy 0.87939453125\n",
      "Iteration 36740 Training loss 0.003515304531902075 Validation loss 0.010683493688702583 Accuracy 0.88525390625\n",
      "Iteration 36750 Training loss 0.0031018052250146866 Validation loss 0.010721257887780666 Accuracy 0.8857421875\n",
      "Iteration 36760 Training loss 0.004504823125898838 Validation loss 0.01087319664657116 Accuracy 0.8837890625\n",
      "Iteration 36770 Training loss 0.0031502211932092905 Validation loss 0.010936353355646133 Accuracy 0.8818359375\n",
      "Iteration 36780 Training loss 0.002643260871991515 Validation loss 0.010778583586215973 Accuracy 0.88330078125\n",
      "Iteration 36790 Training loss 0.0035762032493948936 Validation loss 0.010596193373203278 Accuracy 0.8876953125\n",
      "Iteration 36800 Training loss 0.005096543580293655 Validation loss 0.010439193807542324 Accuracy 0.88720703125\n",
      "Iteration 36810 Training loss 0.0031807583291083574 Validation loss 0.010508796200156212 Accuracy 0.88818359375\n",
      "Iteration 36820 Training loss 0.0036184126511216164 Validation loss 0.010533151216804981 Accuracy 0.88720703125\n",
      "Iteration 36830 Training loss 0.0034766909666359425 Validation loss 0.010723583400249481 Accuracy 0.8857421875\n",
      "Iteration 36840 Training loss 0.0035039717331528664 Validation loss 0.01037876307964325 Accuracy 0.88818359375\n",
      "Iteration 36850 Training loss 0.00441662035882473 Validation loss 0.011686046607792377 Accuracy 0.875\n",
      "Iteration 36860 Training loss 0.0033856555819511414 Validation loss 0.01110110804438591 Accuracy 0.88134765625\n",
      "Iteration 36870 Training loss 0.0046646250411868095 Validation loss 0.010948696173727512 Accuracy 0.88330078125\n",
      "Iteration 36880 Training loss 0.002405497245490551 Validation loss 0.010484604164958 Accuracy 0.88818359375\n",
      "Iteration 36890 Training loss 0.004893817938864231 Validation loss 0.01047083456069231 Accuracy 0.8876953125\n",
      "Iteration 36900 Training loss 0.0040029073134064674 Validation loss 0.010579860769212246 Accuracy 0.8857421875\n",
      "Iteration 36910 Training loss 0.00433652987703681 Validation loss 0.010711058974266052 Accuracy 0.88427734375\n",
      "Iteration 36920 Training loss 0.002956278854981065 Validation loss 0.010560316033661366 Accuracy 0.88623046875\n",
      "Iteration 36930 Training loss 0.0018117781728506088 Validation loss 0.010504266247153282 Accuracy 0.88671875\n",
      "Iteration 36940 Training loss 0.0033471330534666777 Validation loss 0.010594572871923447 Accuracy 0.88671875\n",
      "Iteration 36950 Training loss 0.003323320997878909 Validation loss 0.0105109428986907 Accuracy 0.8876953125\n",
      "Iteration 36960 Training loss 0.003281545592471957 Validation loss 0.010426892898976803 Accuracy 0.88818359375\n",
      "Iteration 36970 Training loss 0.00432689068838954 Validation loss 0.010522001422941685 Accuracy 0.88720703125\n",
      "Iteration 36980 Training loss 0.004034731071442366 Validation loss 0.010490244254469872 Accuracy 0.88818359375\n",
      "Iteration 36990 Training loss 0.003364389296621084 Validation loss 0.010779025964438915 Accuracy 0.88427734375\n",
      "Iteration 37000 Training loss 0.003961008507758379 Validation loss 0.010713068768382072 Accuracy 0.884765625\n",
      "Iteration 37010 Training loss 0.004997149109840393 Validation loss 0.010623627342283726 Accuracy 0.88525390625\n",
      "Iteration 37020 Training loss 0.003535287221893668 Validation loss 0.010609726421535015 Accuracy 0.88623046875\n",
      "Iteration 37030 Training loss 0.003262878395617008 Validation loss 0.010532796382904053 Accuracy 0.8876953125\n",
      "Iteration 37040 Training loss 0.002706727012991905 Validation loss 0.010819421149790287 Accuracy 0.88330078125\n",
      "Iteration 37050 Training loss 0.00424817344173789 Validation loss 0.010573591105639935 Accuracy 0.8876953125\n",
      "Iteration 37060 Training loss 0.003662639996036887 Validation loss 0.010624023154377937 Accuracy 0.88525390625\n",
      "Iteration 37070 Training loss 0.0024082388263195753 Validation loss 0.010544675402343273 Accuracy 0.88720703125\n",
      "Iteration 37080 Training loss 0.0046987091191112995 Validation loss 0.010640563443303108 Accuracy 0.88525390625\n",
      "Iteration 37090 Training loss 0.003791200928390026 Validation loss 0.01094920001924038 Accuracy 0.88232421875\n",
      "Iteration 37100 Training loss 0.003659999230876565 Validation loss 0.01080382987856865 Accuracy 0.88427734375\n",
      "Iteration 37110 Training loss 0.0039057445246726274 Validation loss 0.010600383393466473 Accuracy 0.88525390625\n",
      "Iteration 37120 Training loss 0.0031272820197045803 Validation loss 0.01057686097919941 Accuracy 0.8857421875\n",
      "Iteration 37130 Training loss 0.0036498294211924076 Validation loss 0.010810297913849354 Accuracy 0.88427734375\n",
      "Iteration 37140 Training loss 0.004003135487437248 Validation loss 0.01063950452953577 Accuracy 0.884765625\n",
      "Iteration 37150 Training loss 0.004610702395439148 Validation loss 0.010564287193119526 Accuracy 0.88623046875\n",
      "Iteration 37160 Training loss 0.004232617095112801 Validation loss 0.010647531598806381 Accuracy 0.8857421875\n",
      "Iteration 37170 Training loss 0.004002900328487158 Validation loss 0.010456358082592487 Accuracy 0.88818359375\n",
      "Iteration 37180 Training loss 0.003917937632650137 Validation loss 0.010878002271056175 Accuracy 0.88330078125\n",
      "Iteration 37190 Training loss 0.0051751951687037945 Validation loss 0.010550129227340221 Accuracy 0.88720703125\n",
      "Iteration 37200 Training loss 0.0026402289513498545 Validation loss 0.010836860164999962 Accuracy 0.884765625\n",
      "Iteration 37210 Training loss 0.003246481530368328 Validation loss 0.010548161342740059 Accuracy 0.888671875\n",
      "Iteration 37220 Training loss 0.0032244049943983555 Validation loss 0.010507483035326004 Accuracy 0.88818359375\n",
      "Iteration 37230 Training loss 0.004416974261403084 Validation loss 0.010845121927559376 Accuracy 0.8828125\n",
      "Iteration 37240 Training loss 0.003535239491611719 Validation loss 0.010452455841004848 Accuracy 0.88671875\n",
      "Iteration 37250 Training loss 0.0031541213393211365 Validation loss 0.010538851842284203 Accuracy 0.88720703125\n",
      "Iteration 37260 Training loss 0.003965701442211866 Validation loss 0.010694440454244614 Accuracy 0.884765625\n",
      "Iteration 37270 Training loss 0.004006276372820139 Validation loss 0.010652021504938602 Accuracy 0.88525390625\n",
      "Iteration 37280 Training loss 0.004383698105812073 Validation loss 0.011011259630322456 Accuracy 0.88232421875\n",
      "Iteration 37290 Training loss 0.0043901847675442696 Validation loss 0.010709855705499649 Accuracy 0.8857421875\n",
      "Iteration 37300 Training loss 0.0039505017921328545 Validation loss 0.010536826215684414 Accuracy 0.88671875\n",
      "Iteration 37310 Training loss 0.004203529097139835 Validation loss 0.010738427750766277 Accuracy 0.88525390625\n",
      "Iteration 37320 Training loss 0.0041161589324474335 Validation loss 0.010642286390066147 Accuracy 0.8857421875\n",
      "Iteration 37330 Training loss 0.0037394026294350624 Validation loss 0.010708007961511612 Accuracy 0.88623046875\n",
      "Iteration 37340 Training loss 0.0032404428347945213 Validation loss 0.010768307372927666 Accuracy 0.8837890625\n",
      "Iteration 37350 Training loss 0.0036064572632312775 Validation loss 0.010630921460688114 Accuracy 0.88671875\n",
      "Iteration 37360 Training loss 0.003570991102606058 Validation loss 0.010744232684373856 Accuracy 0.884765625\n",
      "Iteration 37370 Training loss 0.0038987458683550358 Validation loss 0.011299557983875275 Accuracy 0.87890625\n",
      "Iteration 37380 Training loss 0.004890033509582281 Validation loss 0.010729709640145302 Accuracy 0.88427734375\n",
      "Iteration 37390 Training loss 0.0032577801030129194 Validation loss 0.010982939042150974 Accuracy 0.88330078125\n",
      "Iteration 37400 Training loss 0.0037068582605570555 Validation loss 0.010750843212008476 Accuracy 0.884765625\n",
      "Iteration 37410 Training loss 0.0036586569622159004 Validation loss 0.010470363311469555 Accuracy 0.88720703125\n",
      "Iteration 37420 Training loss 0.003995047416538 Validation loss 0.010513920336961746 Accuracy 0.88818359375\n",
      "Iteration 37430 Training loss 0.004433594178408384 Validation loss 0.01068514958024025 Accuracy 0.884765625\n",
      "Iteration 37440 Training loss 0.003727738745510578 Validation loss 0.010722717270255089 Accuracy 0.88525390625\n",
      "Iteration 37450 Training loss 0.0034856058191508055 Validation loss 0.01058153249323368 Accuracy 0.88671875\n",
      "Iteration 37460 Training loss 0.003018500516191125 Validation loss 0.010678612627089024 Accuracy 0.88525390625\n",
      "Iteration 37470 Training loss 0.004217133857309818 Validation loss 0.010907838121056557 Accuracy 0.88232421875\n",
      "Iteration 37480 Training loss 0.0035242519807070494 Validation loss 0.010516567155718803 Accuracy 0.88720703125\n",
      "Iteration 37490 Training loss 0.0029933967161923647 Validation loss 0.010775861330330372 Accuracy 0.8837890625\n",
      "Iteration 37500 Training loss 0.0037544993683695793 Validation loss 0.010646282695233822 Accuracy 0.88671875\n",
      "Iteration 37510 Training loss 0.003993472550064325 Validation loss 0.010481511242687702 Accuracy 0.888671875\n",
      "Iteration 37520 Training loss 0.00447850814089179 Validation loss 0.010606725700199604 Accuracy 0.88671875\n",
      "Iteration 37530 Training loss 0.0056058610789477825 Validation loss 0.010582651011645794 Accuracy 0.8857421875\n",
      "Iteration 37540 Training loss 0.002821657108142972 Validation loss 0.01064027938991785 Accuracy 0.8857421875\n",
      "Iteration 37550 Training loss 0.004122632090002298 Validation loss 0.010801437310874462 Accuracy 0.88330078125\n",
      "Iteration 37560 Training loss 0.00398651510477066 Validation loss 0.010767362080514431 Accuracy 0.88427734375\n",
      "Iteration 37570 Training loss 0.003146166680380702 Validation loss 0.010675412602722645 Accuracy 0.88525390625\n",
      "Iteration 37580 Training loss 0.0038809317629784346 Validation loss 0.010751105844974518 Accuracy 0.88427734375\n",
      "Iteration 37590 Training loss 0.002578369341790676 Validation loss 0.010658244602382183 Accuracy 0.88720703125\n",
      "Iteration 37600 Training loss 0.0032942993566393852 Validation loss 0.010631526820361614 Accuracy 0.8857421875\n",
      "Iteration 37610 Training loss 0.004140867386013269 Validation loss 0.010680011473596096 Accuracy 0.884765625\n",
      "Iteration 37620 Training loss 0.0036600609309971333 Validation loss 0.01060886774212122 Accuracy 0.8857421875\n",
      "Iteration 37630 Training loss 0.003338885260745883 Validation loss 0.010487094521522522 Accuracy 0.88720703125\n",
      "Iteration 37640 Training loss 0.0036303100641816854 Validation loss 0.010422454215586185 Accuracy 0.88720703125\n",
      "Iteration 37650 Training loss 0.004365050233900547 Validation loss 0.010446845553815365 Accuracy 0.88720703125\n",
      "Iteration 37660 Training loss 0.004356499295681715 Validation loss 0.01054504606872797 Accuracy 0.8876953125\n",
      "Iteration 37670 Training loss 0.004602814093232155 Validation loss 0.010723900981247425 Accuracy 0.884765625\n",
      "Iteration 37680 Training loss 0.002125704428181052 Validation loss 0.010815300047397614 Accuracy 0.88427734375\n",
      "Iteration 37690 Training loss 0.0035123019479215145 Validation loss 0.010619091801345348 Accuracy 0.88623046875\n",
      "Iteration 37700 Training loss 0.004286566283553839 Validation loss 0.010620457120239735 Accuracy 0.8857421875\n",
      "Iteration 37710 Training loss 0.0034852097742259502 Validation loss 0.010747582651674747 Accuracy 0.8857421875\n",
      "Iteration 37720 Training loss 0.0038563834968954325 Validation loss 0.010925800539553165 Accuracy 0.88232421875\n",
      "Iteration 37730 Training loss 0.0051120249554514885 Validation loss 0.01070313062518835 Accuracy 0.88623046875\n",
      "Iteration 37740 Training loss 0.0036035366356372833 Validation loss 0.010756725445389748 Accuracy 0.88427734375\n",
      "Iteration 37750 Training loss 0.003705583745613694 Validation loss 0.010596963576972485 Accuracy 0.88623046875\n",
      "Iteration 37760 Training loss 0.003117319429293275 Validation loss 0.01092598121613264 Accuracy 0.88330078125\n",
      "Iteration 37770 Training loss 0.003037021029740572 Validation loss 0.010520190000534058 Accuracy 0.88720703125\n",
      "Iteration 37780 Training loss 0.004702832084149122 Validation loss 0.010448740795254707 Accuracy 0.8876953125\n",
      "Iteration 37790 Training loss 0.0042391205206513405 Validation loss 0.010627382434904575 Accuracy 0.8857421875\n",
      "Iteration 37800 Training loss 0.003088713390752673 Validation loss 0.010625869035720825 Accuracy 0.88720703125\n",
      "Iteration 37810 Training loss 0.0031027754303067923 Validation loss 0.010796216316521168 Accuracy 0.88525390625\n",
      "Iteration 37820 Training loss 0.0031421356834471226 Validation loss 0.01059165969491005 Accuracy 0.88671875\n",
      "Iteration 37830 Training loss 0.004808933939784765 Validation loss 0.010522907599806786 Accuracy 0.888671875\n",
      "Iteration 37840 Training loss 0.0036423015408217907 Validation loss 0.010661057196557522 Accuracy 0.88623046875\n",
      "Iteration 37850 Training loss 0.0045562186278402805 Validation loss 0.011422641575336456 Accuracy 0.8779296875\n",
      "Iteration 37860 Training loss 0.004710226319730282 Validation loss 0.010683572851121426 Accuracy 0.884765625\n",
      "Iteration 37870 Training loss 0.003971883561462164 Validation loss 0.011019467376172543 Accuracy 0.88330078125\n",
      "Iteration 37880 Training loss 0.002300113206729293 Validation loss 0.010607216507196426 Accuracy 0.88671875\n",
      "Iteration 37890 Training loss 0.0032912814058363438 Validation loss 0.010562614537775517 Accuracy 0.8857421875\n",
      "Iteration 37900 Training loss 0.0041782367043197155 Validation loss 0.010669996030628681 Accuracy 0.8857421875\n",
      "Iteration 37910 Training loss 0.004652644041925669 Validation loss 0.011046161875128746 Accuracy 0.88232421875\n",
      "Iteration 37920 Training loss 0.004467313177883625 Validation loss 0.01059491466730833 Accuracy 0.88671875\n",
      "Iteration 37930 Training loss 0.0020586741156876087 Validation loss 0.010609319433569908 Accuracy 0.88671875\n",
      "Iteration 37940 Training loss 0.0020685617346316576 Validation loss 0.010604549199342728 Accuracy 0.88623046875\n",
      "Iteration 37950 Training loss 0.004252976272255182 Validation loss 0.010703889653086662 Accuracy 0.8857421875\n",
      "Iteration 37960 Training loss 0.0030208893585950136 Validation loss 0.010787751525640488 Accuracy 0.88427734375\n",
      "Iteration 37970 Training loss 0.003691550809890032 Validation loss 0.010842448100447655 Accuracy 0.8828125\n",
      "Iteration 37980 Training loss 0.0033977653365582228 Validation loss 0.010632358491420746 Accuracy 0.88671875\n",
      "Iteration 37990 Training loss 0.0033776273485273123 Validation loss 0.010558822192251682 Accuracy 0.88720703125\n",
      "Iteration 38000 Training loss 0.003283450612798333 Validation loss 0.010550686158239841 Accuracy 0.88525390625\n",
      "Iteration 38010 Training loss 0.003166444832459092 Validation loss 0.01064386684447527 Accuracy 0.88525390625\n",
      "Iteration 38020 Training loss 0.0030150546226650476 Validation loss 0.010587207041680813 Accuracy 0.88671875\n",
      "Iteration 38030 Training loss 0.003317242953926325 Validation loss 0.010710072703659534 Accuracy 0.88427734375\n",
      "Iteration 38040 Training loss 0.0025404023472219706 Validation loss 0.010833099484443665 Accuracy 0.884765625\n",
      "Iteration 38050 Training loss 0.0038750043604522943 Validation loss 0.01073953602463007 Accuracy 0.88525390625\n",
      "Iteration 38060 Training loss 0.004087282344698906 Validation loss 0.010910236276686192 Accuracy 0.88330078125\n",
      "Iteration 38070 Training loss 0.002324976958334446 Validation loss 0.010686825960874557 Accuracy 0.8857421875\n",
      "Iteration 38080 Training loss 0.003634524531662464 Validation loss 0.010820417664945126 Accuracy 0.88427734375\n",
      "Iteration 38090 Training loss 0.0035761012695729733 Validation loss 0.011039015837013721 Accuracy 0.88232421875\n",
      "Iteration 38100 Training loss 0.003947321325540543 Validation loss 0.010912487283349037 Accuracy 0.8828125\n",
      "Iteration 38110 Training loss 0.0038529480807483196 Validation loss 0.01060129888355732 Accuracy 0.88671875\n",
      "Iteration 38120 Training loss 0.0036317382473498583 Validation loss 0.010663461871445179 Accuracy 0.8857421875\n",
      "Iteration 38130 Training loss 0.0036366386339068413 Validation loss 0.011160634458065033 Accuracy 0.8798828125\n",
      "Iteration 38140 Training loss 0.002436799695715308 Validation loss 0.010523030534386635 Accuracy 0.88623046875\n",
      "Iteration 38150 Training loss 0.0027501494623720646 Validation loss 0.010603019967675209 Accuracy 0.88671875\n",
      "Iteration 38160 Training loss 0.0026678680442273617 Validation loss 0.010580339469015598 Accuracy 0.88720703125\n",
      "Iteration 38170 Training loss 0.004192854277789593 Validation loss 0.01053448673337698 Accuracy 0.8876953125\n",
      "Iteration 38180 Training loss 0.002716552698984742 Validation loss 0.01053442619740963 Accuracy 0.88720703125\n",
      "Iteration 38190 Training loss 0.003653552383184433 Validation loss 0.01074920129030943 Accuracy 0.88525390625\n",
      "Iteration 38200 Training loss 0.0038314450066536665 Validation loss 0.010472873225808144 Accuracy 0.88818359375\n",
      "Iteration 38210 Training loss 0.003355196909978986 Validation loss 0.010836632922291756 Accuracy 0.88330078125\n",
      "Iteration 38220 Training loss 0.005099277477711439 Validation loss 0.011005991138517857 Accuracy 0.8818359375\n",
      "Iteration 38230 Training loss 0.0018167493399232626 Validation loss 0.010674321092665195 Accuracy 0.88525390625\n",
      "Iteration 38240 Training loss 0.004565126728266478 Validation loss 0.010943290777504444 Accuracy 0.88232421875\n",
      "Iteration 38250 Training loss 0.003393748542293906 Validation loss 0.01074210088700056 Accuracy 0.8837890625\n",
      "Iteration 38260 Training loss 0.0034681595861911774 Validation loss 0.010858545079827309 Accuracy 0.8828125\n",
      "Iteration 38270 Training loss 0.003275701543316245 Validation loss 0.010813955217599869 Accuracy 0.8837890625\n",
      "Iteration 38280 Training loss 0.0030336726922541857 Validation loss 0.01096712239086628 Accuracy 0.88232421875\n",
      "Iteration 38290 Training loss 0.00290341442450881 Validation loss 0.010724847204983234 Accuracy 0.8857421875\n",
      "Iteration 38300 Training loss 0.0040395124815404415 Validation loss 0.010860797949135303 Accuracy 0.8837890625\n",
      "Iteration 38310 Training loss 0.0031119021587073803 Validation loss 0.010670582763850689 Accuracy 0.88525390625\n",
      "Iteration 38320 Training loss 0.004061958286911249 Validation loss 0.010509251616895199 Accuracy 0.88720703125\n",
      "Iteration 38330 Training loss 0.0039876773953437805 Validation loss 0.010517570190131664 Accuracy 0.88818359375\n",
      "Iteration 38340 Training loss 0.003558716969564557 Validation loss 0.01064886711537838 Accuracy 0.88525390625\n",
      "Iteration 38350 Training loss 0.002662437269464135 Validation loss 0.0106767937541008 Accuracy 0.8857421875\n",
      "Iteration 38360 Training loss 0.004326925612986088 Validation loss 0.010579402558505535 Accuracy 0.88671875\n",
      "Iteration 38370 Training loss 0.003725383197888732 Validation loss 0.010482389479875565 Accuracy 0.888671875\n",
      "Iteration 38380 Training loss 0.003114475170150399 Validation loss 0.010791247710585594 Accuracy 0.8857421875\n",
      "Iteration 38390 Training loss 0.003494354197755456 Validation loss 0.010810251347720623 Accuracy 0.88427734375\n",
      "Iteration 38400 Training loss 0.003845602972432971 Validation loss 0.01064449641853571 Accuracy 0.8857421875\n",
      "Iteration 38410 Training loss 0.004366748034954071 Validation loss 0.010467593558132648 Accuracy 0.88720703125\n",
      "Iteration 38420 Training loss 0.0029694270342588425 Validation loss 0.01035957783460617 Accuracy 0.888671875\n",
      "Iteration 38430 Training loss 0.002380231162533164 Validation loss 0.010467697866261005 Accuracy 0.8876953125\n",
      "Iteration 38440 Training loss 0.003615049412474036 Validation loss 0.010628278367221355 Accuracy 0.88623046875\n",
      "Iteration 38450 Training loss 0.0020861581433564425 Validation loss 0.010760807432234287 Accuracy 0.8837890625\n",
      "Iteration 38460 Training loss 0.0046247257851064205 Validation loss 0.01082330197095871 Accuracy 0.8837890625\n",
      "Iteration 38470 Training loss 0.0028113338630646467 Validation loss 0.010721527971327305 Accuracy 0.88427734375\n",
      "Iteration 38480 Training loss 0.005195183213800192 Validation loss 0.01072840578854084 Accuracy 0.884765625\n",
      "Iteration 38490 Training loss 0.0038014843594282866 Validation loss 0.010922367684543133 Accuracy 0.88330078125\n",
      "Iteration 38500 Training loss 0.0027657002210617065 Validation loss 0.010704504325985909 Accuracy 0.88525390625\n",
      "Iteration 38510 Training loss 0.004537297412753105 Validation loss 0.010768935084342957 Accuracy 0.884765625\n",
      "Iteration 38520 Training loss 0.0036546699702739716 Validation loss 0.010759929195046425 Accuracy 0.88427734375\n",
      "Iteration 38530 Training loss 0.004576004110276699 Validation loss 0.010577838867902756 Accuracy 0.88671875\n",
      "Iteration 38540 Training loss 0.003869539825245738 Validation loss 0.010756686329841614 Accuracy 0.884765625\n",
      "Iteration 38550 Training loss 0.00416981428861618 Validation loss 0.010797380469739437 Accuracy 0.8857421875\n",
      "Iteration 38560 Training loss 0.004815931431949139 Validation loss 0.010628764517605305 Accuracy 0.88623046875\n",
      "Iteration 38570 Training loss 0.003984087612479925 Validation loss 0.010605412535369396 Accuracy 0.88671875\n",
      "Iteration 38580 Training loss 0.004251193720847368 Validation loss 0.0105888731777668 Accuracy 0.88720703125\n",
      "Iteration 38590 Training loss 0.003795205382630229 Validation loss 0.010788927786052227 Accuracy 0.8837890625\n",
      "Iteration 38600 Training loss 0.0028551616705954075 Validation loss 0.010858459398150444 Accuracy 0.8837890625\n",
      "Iteration 38610 Training loss 0.002626527799293399 Validation loss 0.010575901716947556 Accuracy 0.88623046875\n",
      "Iteration 38620 Training loss 0.0036638646852225065 Validation loss 0.01075099129229784 Accuracy 0.884765625\n",
      "Iteration 38630 Training loss 0.003492397954687476 Validation loss 0.010620256885886192 Accuracy 0.88671875\n",
      "Iteration 38640 Training loss 0.0030465442687273026 Validation loss 0.010714645497500896 Accuracy 0.884765625\n",
      "Iteration 38650 Training loss 0.0031649747397750616 Validation loss 0.010569647885859013 Accuracy 0.88671875\n",
      "Iteration 38660 Training loss 0.003720587119460106 Validation loss 0.010593099519610405 Accuracy 0.88623046875\n",
      "Iteration 38670 Training loss 0.0037909781094640493 Validation loss 0.011515643447637558 Accuracy 0.876953125\n",
      "Iteration 38680 Training loss 0.0041130841709673405 Validation loss 0.010858435183763504 Accuracy 0.884765625\n",
      "Iteration 38690 Training loss 0.003254098119214177 Validation loss 0.010687775909900665 Accuracy 0.88525390625\n",
      "Iteration 38700 Training loss 0.004475393332540989 Validation loss 0.010535904206335545 Accuracy 0.88720703125\n",
      "Iteration 38710 Training loss 0.0033167852088809013 Validation loss 0.010749168694019318 Accuracy 0.88427734375\n",
      "Iteration 38720 Training loss 0.003888703417032957 Validation loss 0.010743539780378342 Accuracy 0.8837890625\n",
      "Iteration 38730 Training loss 0.004457918927073479 Validation loss 0.010559199377894402 Accuracy 0.8857421875\n",
      "Iteration 38740 Training loss 0.0035443201195448637 Validation loss 0.010799044743180275 Accuracy 0.88330078125\n",
      "Iteration 38750 Training loss 0.0028360493015497923 Validation loss 0.010738986544311047 Accuracy 0.884765625\n",
      "Iteration 38760 Training loss 0.004884601105004549 Validation loss 0.011001838371157646 Accuracy 0.88232421875\n",
      "Iteration 38770 Training loss 0.003336676862090826 Validation loss 0.010644695721566677 Accuracy 0.88525390625\n",
      "Iteration 38780 Training loss 0.002308742143213749 Validation loss 0.010633120313286781 Accuracy 0.8857421875\n",
      "Iteration 38790 Training loss 0.0025168394204229116 Validation loss 0.010489342734217644 Accuracy 0.88818359375\n",
      "Iteration 38800 Training loss 0.0030461056157946587 Validation loss 0.010689465329051018 Accuracy 0.884765625\n",
      "Iteration 38810 Training loss 0.00353904883377254 Validation loss 0.010574016720056534 Accuracy 0.8857421875\n",
      "Iteration 38820 Training loss 0.004069805610924959 Validation loss 0.010915185324847698 Accuracy 0.88232421875\n",
      "Iteration 38830 Training loss 0.0034076543524861336 Validation loss 0.010772996582090855 Accuracy 0.884765625\n",
      "Iteration 38840 Training loss 0.003599137533456087 Validation loss 0.010883285664021969 Accuracy 0.88427734375\n",
      "Iteration 38850 Training loss 0.0033268723636865616 Validation loss 0.010726659558713436 Accuracy 0.884765625\n",
      "Iteration 38860 Training loss 0.0040077636949718 Validation loss 0.01077463012188673 Accuracy 0.8837890625\n",
      "Iteration 38870 Training loss 0.00408592913299799 Validation loss 0.010699206031858921 Accuracy 0.88525390625\n",
      "Iteration 38880 Training loss 0.005125327035784721 Validation loss 0.010648421011865139 Accuracy 0.8857421875\n",
      "Iteration 38890 Training loss 0.0027555800043046474 Validation loss 0.010689843446016312 Accuracy 0.884765625\n",
      "Iteration 38900 Training loss 0.0043998840264976025 Validation loss 0.010327682830393314 Accuracy 0.88916015625\n",
      "Iteration 38910 Training loss 0.0033967820927500725 Validation loss 0.010844760574400425 Accuracy 0.88427734375\n",
      "Iteration 38920 Training loss 0.003041981952264905 Validation loss 0.01042991690337658 Accuracy 0.888671875\n",
      "Iteration 38930 Training loss 0.0031875520944595337 Validation loss 0.010720184072852135 Accuracy 0.88427734375\n",
      "Iteration 38940 Training loss 0.0037135162856429815 Validation loss 0.010945209302008152 Accuracy 0.8818359375\n",
      "Iteration 38950 Training loss 0.003473786637187004 Validation loss 0.01051116082817316 Accuracy 0.88671875\n",
      "Iteration 38960 Training loss 0.00553105166181922 Validation loss 0.010580983944237232 Accuracy 0.8857421875\n",
      "Iteration 38970 Training loss 0.002816767431795597 Validation loss 0.01093541830778122 Accuracy 0.88232421875\n",
      "Iteration 38980 Training loss 0.00343882292509079 Validation loss 0.010486718267202377 Accuracy 0.88720703125\n",
      "Iteration 38990 Training loss 0.003202166873961687 Validation loss 0.010573266074061394 Accuracy 0.8876953125\n",
      "Iteration 39000 Training loss 0.0034555974416434765 Validation loss 0.01072537899017334 Accuracy 0.884765625\n",
      "Iteration 39010 Training loss 0.0027226749807596207 Validation loss 0.010747743770480156 Accuracy 0.88427734375\n",
      "Iteration 39020 Training loss 0.004624071065336466 Validation loss 0.010654793120920658 Accuracy 0.8857421875\n",
      "Iteration 39030 Training loss 0.0036578059662133455 Validation loss 0.010789123363792896 Accuracy 0.88330078125\n",
      "Iteration 39040 Training loss 0.002936403267085552 Validation loss 0.010695637203752995 Accuracy 0.88525390625\n",
      "Iteration 39050 Training loss 0.0029767360538244247 Validation loss 0.010833905078470707 Accuracy 0.8837890625\n",
      "Iteration 39060 Training loss 0.003586571430787444 Validation loss 0.010648217052221298 Accuracy 0.8857421875\n",
      "Iteration 39070 Training loss 0.004543783143162727 Validation loss 0.01056376937776804 Accuracy 0.88623046875\n",
      "Iteration 39080 Training loss 0.004060425329953432 Validation loss 0.01100127398967743 Accuracy 0.8828125\n",
      "Iteration 39090 Training loss 0.0038152243942022324 Validation loss 0.01053739245980978 Accuracy 0.88671875\n",
      "Iteration 39100 Training loss 0.0024856191594153643 Validation loss 0.010464340448379517 Accuracy 0.89013671875\n",
      "Iteration 39110 Training loss 0.0036447092425078154 Validation loss 0.010573217645287514 Accuracy 0.88623046875\n",
      "Iteration 39120 Training loss 0.003793301759287715 Validation loss 0.010586293414235115 Accuracy 0.88720703125\n",
      "Iteration 39130 Training loss 0.0036854168865829706 Validation loss 0.010600639507174492 Accuracy 0.88720703125\n",
      "Iteration 39140 Training loss 0.002745263045653701 Validation loss 0.010778809897601604 Accuracy 0.88427734375\n",
      "Iteration 39150 Training loss 0.004008294548839331 Validation loss 0.010910493321716785 Accuracy 0.8828125\n",
      "Iteration 39160 Training loss 0.003788375062867999 Validation loss 0.010743621736764908 Accuracy 0.884765625\n",
      "Iteration 39170 Training loss 0.00333011825568974 Validation loss 0.01057229470461607 Accuracy 0.88671875\n",
      "Iteration 39180 Training loss 0.0043601891957223415 Validation loss 0.01065811701118946 Accuracy 0.88525390625\n",
      "Iteration 39190 Training loss 0.004928035195916891 Validation loss 0.010635239072144032 Accuracy 0.88623046875\n",
      "Iteration 39200 Training loss 0.003250826383009553 Validation loss 0.010430478490889072 Accuracy 0.8876953125\n",
      "Iteration 39210 Training loss 0.00424566213041544 Validation loss 0.010688056237995625 Accuracy 0.88427734375\n",
      "Iteration 39220 Training loss 0.0038330911193042994 Validation loss 0.010556302033364773 Accuracy 0.88525390625\n",
      "Iteration 39230 Training loss 0.004229987971484661 Validation loss 0.010592155158519745 Accuracy 0.88671875\n",
      "Iteration 39240 Training loss 0.0038112178444862366 Validation loss 0.010655059479176998 Accuracy 0.88623046875\n",
      "Iteration 39250 Training loss 0.0028468307573348284 Validation loss 0.010585475713014603 Accuracy 0.88671875\n",
      "Iteration 39260 Training loss 0.005318204406648874 Validation loss 0.010628288611769676 Accuracy 0.88671875\n",
      "Iteration 39270 Training loss 0.0051565999165177345 Validation loss 0.010605390183627605 Accuracy 0.88720703125\n",
      "Iteration 39280 Training loss 0.003156309947371483 Validation loss 0.010645515285432339 Accuracy 0.88623046875\n",
      "Iteration 39290 Training loss 0.0032648316118866205 Validation loss 0.010638884268701077 Accuracy 0.8857421875\n",
      "Iteration 39300 Training loss 0.0030575015116482973 Validation loss 0.010617957450449467 Accuracy 0.8857421875\n",
      "Iteration 39310 Training loss 0.004032548051327467 Validation loss 0.010721420869231224 Accuracy 0.88427734375\n",
      "Iteration 39320 Training loss 0.002322872867807746 Validation loss 0.010511535219848156 Accuracy 0.88720703125\n",
      "Iteration 39330 Training loss 0.0045610531233251095 Validation loss 0.010931923985481262 Accuracy 0.88330078125\n",
      "Iteration 39340 Training loss 0.0021571884863078594 Validation loss 0.010676494799554348 Accuracy 0.8857421875\n",
      "Iteration 39350 Training loss 0.0030035963281989098 Validation loss 0.010579206049442291 Accuracy 0.88671875\n",
      "Iteration 39360 Training loss 0.0042579518631100655 Validation loss 0.010593881830573082 Accuracy 0.88623046875\n",
      "Iteration 39370 Training loss 0.0028196393977850676 Validation loss 0.0109392199665308 Accuracy 0.8828125\n",
      "Iteration 39380 Training loss 0.0031021670438349247 Validation loss 0.010768632404506207 Accuracy 0.884765625\n",
      "Iteration 39390 Training loss 0.002761944429948926 Validation loss 0.010646602138876915 Accuracy 0.8857421875\n",
      "Iteration 39400 Training loss 0.0038145091384649277 Validation loss 0.010596498847007751 Accuracy 0.88720703125\n",
      "Iteration 39410 Training loss 0.004161476157605648 Validation loss 0.010454026982188225 Accuracy 0.88818359375\n",
      "Iteration 39420 Training loss 0.003258609212934971 Validation loss 0.01051806379109621 Accuracy 0.88671875\n",
      "Iteration 39430 Training loss 0.003719690954312682 Validation loss 0.010469655506312847 Accuracy 0.88720703125\n",
      "Iteration 39440 Training loss 0.0031267786398530006 Validation loss 0.010482761077582836 Accuracy 0.88818359375\n",
      "Iteration 39450 Training loss 0.0036000232212245464 Validation loss 0.010636693798005581 Accuracy 0.88623046875\n",
      "Iteration 39460 Training loss 0.002500948030501604 Validation loss 0.010538190603256226 Accuracy 0.88623046875\n",
      "Iteration 39470 Training loss 0.004010354168713093 Validation loss 0.010540316812694073 Accuracy 0.8857421875\n",
      "Iteration 39480 Training loss 0.0056940470822155476 Validation loss 0.010693151503801346 Accuracy 0.88623046875\n",
      "Iteration 39490 Training loss 0.004072819370776415 Validation loss 0.010725453495979309 Accuracy 0.8837890625\n",
      "Iteration 39500 Training loss 0.003826087573543191 Validation loss 0.01074831560254097 Accuracy 0.884765625\n",
      "Iteration 39510 Training loss 0.0038278503343462944 Validation loss 0.010663913562893867 Accuracy 0.8857421875\n",
      "Iteration 39520 Training loss 0.0033505649771541357 Validation loss 0.010868347249925137 Accuracy 0.88330078125\n",
      "Iteration 39530 Training loss 0.004203429911285639 Validation loss 0.010407939553260803 Accuracy 0.88720703125\n",
      "Iteration 39540 Training loss 0.0013243848225101829 Validation loss 0.0105648348107934 Accuracy 0.88623046875\n",
      "Iteration 39550 Training loss 0.004151068162173033 Validation loss 0.010887271724641323 Accuracy 0.8828125\n",
      "Iteration 39560 Training loss 0.0037252248730510473 Validation loss 0.010571635328233242 Accuracy 0.88720703125\n",
      "Iteration 39570 Training loss 0.0037044486962258816 Validation loss 0.010334601625800133 Accuracy 0.8896484375\n",
      "Iteration 39580 Training loss 0.003945571836084127 Validation loss 0.010916116647422314 Accuracy 0.8828125\n",
      "Iteration 39590 Training loss 0.003862628946080804 Validation loss 0.010462557896971703 Accuracy 0.8876953125\n",
      "Iteration 39600 Training loss 0.004884028807282448 Validation loss 0.010620019398629665 Accuracy 0.88525390625\n",
      "Iteration 39610 Training loss 0.003414260921999812 Validation loss 0.010564906522631645 Accuracy 0.88623046875\n",
      "Iteration 39620 Training loss 0.00335154146887362 Validation loss 0.010655570775270462 Accuracy 0.88525390625\n",
      "Iteration 39630 Training loss 0.0026743023190647364 Validation loss 0.010726832784712315 Accuracy 0.884765625\n",
      "Iteration 39640 Training loss 0.0032402172219008207 Validation loss 0.010504481382668018 Accuracy 0.8876953125\n",
      "Iteration 39650 Training loss 0.003446707036346197 Validation loss 0.010590878315269947 Accuracy 0.88671875\n",
      "Iteration 39660 Training loss 0.0024602890480309725 Validation loss 0.01048998348414898 Accuracy 0.888671875\n",
      "Iteration 39670 Training loss 0.0019973537418991327 Validation loss 0.010550222359597683 Accuracy 0.8857421875\n",
      "Iteration 39680 Training loss 0.0028575139585882425 Validation loss 0.01052919402718544 Accuracy 0.88671875\n",
      "Iteration 39690 Training loss 0.0032278457656502724 Validation loss 0.010519077070057392 Accuracy 0.8876953125\n",
      "Iteration 39700 Training loss 0.004522078670561314 Validation loss 0.010529948398470879 Accuracy 0.88623046875\n",
      "Iteration 39710 Training loss 0.0021544895134866238 Validation loss 0.010518795810639858 Accuracy 0.88720703125\n",
      "Iteration 39720 Training loss 0.003415925893932581 Validation loss 0.01035167183727026 Accuracy 0.888671875\n",
      "Iteration 39730 Training loss 0.003588679013773799 Validation loss 0.010569341480731964 Accuracy 0.88671875\n",
      "Iteration 39740 Training loss 0.0023511273320764303 Validation loss 0.010707632638514042 Accuracy 0.884765625\n",
      "Iteration 39750 Training loss 0.0035697754938155413 Validation loss 0.010547717101871967 Accuracy 0.884765625\n",
      "Iteration 39760 Training loss 0.004126731306314468 Validation loss 0.010524886660277843 Accuracy 0.88623046875\n",
      "Iteration 39770 Training loss 0.003344203345477581 Validation loss 0.010578560642898083 Accuracy 0.8857421875\n",
      "Iteration 39780 Training loss 0.003594173351302743 Validation loss 0.010839146561920643 Accuracy 0.8828125\n",
      "Iteration 39790 Training loss 0.003012138418853283 Validation loss 0.010426503606140614 Accuracy 0.888671875\n",
      "Iteration 39800 Training loss 0.0030063348822295666 Validation loss 0.010553944855928421 Accuracy 0.88623046875\n",
      "Iteration 39810 Training loss 0.004232020117342472 Validation loss 0.010591225698590279 Accuracy 0.8857421875\n",
      "Iteration 39820 Training loss 0.0025132920127362013 Validation loss 0.01079030241817236 Accuracy 0.8837890625\n",
      "Iteration 39830 Training loss 0.0032627484761178493 Validation loss 0.010645166039466858 Accuracy 0.88623046875\n",
      "Iteration 39840 Training loss 0.002373314229771495 Validation loss 0.01079979445785284 Accuracy 0.88427734375\n",
      "Iteration 39850 Training loss 0.003565336111932993 Validation loss 0.0104291420429945 Accuracy 0.88916015625\n",
      "Iteration 39860 Training loss 0.00393351586535573 Validation loss 0.010583817958831787 Accuracy 0.88671875\n",
      "Iteration 39870 Training loss 0.004595127888023853 Validation loss 0.010563519783318043 Accuracy 0.8857421875\n",
      "Iteration 39880 Training loss 0.004059819038957357 Validation loss 0.010610434226691723 Accuracy 0.88623046875\n",
      "Iteration 39890 Training loss 0.0038888179697096348 Validation loss 0.010840597562491894 Accuracy 0.8837890625\n",
      "Iteration 39900 Training loss 0.003775113495066762 Validation loss 0.010638066567480564 Accuracy 0.88623046875\n",
      "Iteration 39910 Training loss 0.005087627563625574 Validation loss 0.010674345307052135 Accuracy 0.8857421875\n",
      "Iteration 39920 Training loss 0.002112498041242361 Validation loss 0.010452472604811192 Accuracy 0.88818359375\n",
      "Iteration 39930 Training loss 0.002792397513985634 Validation loss 0.010592907667160034 Accuracy 0.88623046875\n",
      "Iteration 39940 Training loss 0.002066425746306777 Validation loss 0.01062039379030466 Accuracy 0.88623046875\n",
      "Iteration 39950 Training loss 0.003885383950546384 Validation loss 0.010620763525366783 Accuracy 0.8857421875\n",
      "Iteration 39960 Training loss 0.0032437751069664955 Validation loss 0.010563774034380913 Accuracy 0.88720703125\n",
      "Iteration 39970 Training loss 0.0036304278764873743 Validation loss 0.010619589127600193 Accuracy 0.88720703125\n",
      "Iteration 39980 Training loss 0.003244666149839759 Validation loss 0.010544247925281525 Accuracy 0.88720703125\n",
      "Iteration 39990 Training loss 0.004197100177407265 Validation loss 0.010595775209367275 Accuracy 0.88623046875\n",
      "Iteration 40000 Training loss 0.002836614614352584 Validation loss 0.010639062151312828 Accuracy 0.8857421875\n",
      "Iteration 40010 Training loss 0.00427358178421855 Validation loss 0.010506569407880306 Accuracy 0.8876953125\n",
      "Iteration 40020 Training loss 0.00451639574021101 Validation loss 0.010625092312693596 Accuracy 0.88671875\n",
      "Iteration 40030 Training loss 0.004770142026245594 Validation loss 0.010964804328978062 Accuracy 0.8828125\n",
      "Iteration 40040 Training loss 0.00330475065857172 Validation loss 0.0109883863478899 Accuracy 0.8828125\n",
      "Iteration 40050 Training loss 0.0027185098733752966 Validation loss 0.01071347389370203 Accuracy 0.884765625\n",
      "Iteration 40060 Training loss 0.0045564924366772175 Validation loss 0.010518213734030724 Accuracy 0.88671875\n",
      "Iteration 40070 Training loss 0.0032919333316385746 Validation loss 0.010630810633301735 Accuracy 0.88623046875\n",
      "Iteration 40080 Training loss 0.0028452554251998663 Validation loss 0.010602802038192749 Accuracy 0.88671875\n",
      "Iteration 40090 Training loss 0.003756697988137603 Validation loss 0.010740119032561779 Accuracy 0.88525390625\n",
      "Iteration 40100 Training loss 0.003330649109557271 Validation loss 0.010701139457523823 Accuracy 0.884765625\n",
      "Iteration 40110 Training loss 0.0024682343937456608 Validation loss 0.01065431535243988 Accuracy 0.884765625\n",
      "Iteration 40120 Training loss 0.0035872445441782475 Validation loss 0.010747360996901989 Accuracy 0.88525390625\n",
      "Iteration 40130 Training loss 0.004650440067052841 Validation loss 0.010656159371137619 Accuracy 0.8857421875\n",
      "Iteration 40140 Training loss 0.0035957631189376116 Validation loss 0.01050181407481432 Accuracy 0.88720703125\n",
      "Iteration 40150 Training loss 0.0032265586778521538 Validation loss 0.010835298337042332 Accuracy 0.88427734375\n",
      "Iteration 40160 Training loss 0.002808628138154745 Validation loss 0.0106477877125144 Accuracy 0.88623046875\n",
      "Iteration 40170 Training loss 0.003752220654860139 Validation loss 0.010434753261506557 Accuracy 0.88818359375\n",
      "Iteration 40180 Training loss 0.004699429962784052 Validation loss 0.010758347809314728 Accuracy 0.88525390625\n",
      "Iteration 40190 Training loss 0.0030046964529901743 Validation loss 0.010651188902556896 Accuracy 0.8857421875\n",
      "Iteration 40200 Training loss 0.0028063070494681597 Validation loss 0.01059054397046566 Accuracy 0.88623046875\n",
      "Iteration 40210 Training loss 0.0029611866921186447 Validation loss 0.010655578225851059 Accuracy 0.88525390625\n",
      "Iteration 40220 Training loss 0.0025093546137213707 Validation loss 0.010786342434585094 Accuracy 0.88427734375\n",
      "Iteration 40230 Training loss 0.0029007066041231155 Validation loss 0.010497340932488441 Accuracy 0.88720703125\n",
      "Iteration 40240 Training loss 0.002915454562753439 Validation loss 0.010674619115889072 Accuracy 0.88525390625\n",
      "Iteration 40250 Training loss 0.0028702309355139732 Validation loss 0.010478480719029903 Accuracy 0.8876953125\n",
      "Iteration 40260 Training loss 0.003599221585318446 Validation loss 0.01053929328918457 Accuracy 0.8876953125\n",
      "Iteration 40270 Training loss 0.004830625373870134 Validation loss 0.010530691593885422 Accuracy 0.88720703125\n",
      "Iteration 40280 Training loss 0.004021535161882639 Validation loss 0.010730095207691193 Accuracy 0.8857421875\n",
      "Iteration 40290 Training loss 0.0017114918446168303 Validation loss 0.01077255792915821 Accuracy 0.88427734375\n",
      "Iteration 40300 Training loss 0.002852127654477954 Validation loss 0.010782585479319096 Accuracy 0.884765625\n",
      "Iteration 40310 Training loss 0.0022843715269118547 Validation loss 0.01080272812396288 Accuracy 0.884765625\n",
      "Iteration 40320 Training loss 0.0035613069776445627 Validation loss 0.010700619779527187 Accuracy 0.884765625\n",
      "Iteration 40330 Training loss 0.0028170673176646233 Validation loss 0.010551277548074722 Accuracy 0.88623046875\n",
      "Iteration 40340 Training loss 0.003317885100841522 Validation loss 0.010472746565937996 Accuracy 0.88818359375\n",
      "Iteration 40350 Training loss 0.0037810001522302628 Validation loss 0.010671176947653294 Accuracy 0.88525390625\n",
      "Iteration 40360 Training loss 0.0038341209292411804 Validation loss 0.010502436198294163 Accuracy 0.88720703125\n",
      "Iteration 40370 Training loss 0.0025336304679512978 Validation loss 0.010628681629896164 Accuracy 0.88623046875\n",
      "Iteration 40380 Training loss 0.0032957051880657673 Validation loss 0.01068259496241808 Accuracy 0.8857421875\n",
      "Iteration 40390 Training loss 0.002208341844379902 Validation loss 0.01046714000403881 Accuracy 0.8876953125\n",
      "Iteration 40400 Training loss 0.003410645527765155 Validation loss 0.01042325422167778 Accuracy 0.88671875\n",
      "Iteration 40410 Training loss 0.0035795026924461126 Validation loss 0.010470491833984852 Accuracy 0.8876953125\n",
      "Iteration 40420 Training loss 0.003306854283437133 Validation loss 0.010516691952943802 Accuracy 0.88720703125\n",
      "Iteration 40430 Training loss 0.003495490411296487 Validation loss 0.010598844848573208 Accuracy 0.8857421875\n",
      "Iteration 40440 Training loss 0.0038188821636140347 Validation loss 0.010490795597434044 Accuracy 0.88671875\n",
      "Iteration 40450 Training loss 0.00433260016143322 Validation loss 0.010828261263668537 Accuracy 0.88330078125\n",
      "Iteration 40460 Training loss 0.0035063812974840403 Validation loss 0.010611915960907936 Accuracy 0.88671875\n",
      "Iteration 40470 Training loss 0.0028658227529376745 Validation loss 0.010657164268195629 Accuracy 0.88623046875\n",
      "Iteration 40480 Training loss 0.003019805531948805 Validation loss 0.01059997733682394 Accuracy 0.88671875\n",
      "Iteration 40490 Training loss 0.0030364326667040586 Validation loss 0.010646759532392025 Accuracy 0.88671875\n",
      "Iteration 40500 Training loss 0.0041652568615973 Validation loss 0.010638908483088017 Accuracy 0.88525390625\n",
      "Iteration 40510 Training loss 0.0036488256882876158 Validation loss 0.010752245783805847 Accuracy 0.8837890625\n",
      "Iteration 40520 Training loss 0.00408782996237278 Validation loss 0.010624195449054241 Accuracy 0.8857421875\n",
      "Iteration 40530 Training loss 0.0031436483841389418 Validation loss 0.010763506405055523 Accuracy 0.884765625\n",
      "Iteration 40540 Training loss 0.0029381497297436 Validation loss 0.010803820565342903 Accuracy 0.88330078125\n",
      "Iteration 40550 Training loss 0.0029556022491306067 Validation loss 0.010471496731042862 Accuracy 0.88818359375\n",
      "Iteration 40560 Training loss 0.003646650817245245 Validation loss 0.010818996466696262 Accuracy 0.884765625\n",
      "Iteration 40570 Training loss 0.0034241981338709593 Validation loss 0.010528487153351307 Accuracy 0.88720703125\n",
      "Iteration 40580 Training loss 0.0052858623676002026 Validation loss 0.010693207383155823 Accuracy 0.884765625\n",
      "Iteration 40590 Training loss 0.00457375030964613 Validation loss 0.010643614456057549 Accuracy 0.88525390625\n",
      "Iteration 40600 Training loss 0.00189885008148849 Validation loss 0.010548378340899944 Accuracy 0.88623046875\n",
      "Iteration 40610 Training loss 0.002959931269288063 Validation loss 0.010514108464121819 Accuracy 0.88671875\n",
      "Iteration 40620 Training loss 0.004103674087673426 Validation loss 0.0106569929048419 Accuracy 0.88525390625\n",
      "Iteration 40630 Training loss 0.0030605217907577753 Validation loss 0.010732526890933514 Accuracy 0.88427734375\n",
      "Iteration 40640 Training loss 0.0024204482324421406 Validation loss 0.010652396827936172 Accuracy 0.8857421875\n",
      "Iteration 40650 Training loss 0.003313753055408597 Validation loss 0.010816876776516438 Accuracy 0.884765625\n",
      "Iteration 40660 Training loss 0.0033935876563191414 Validation loss 0.010613051243126392 Accuracy 0.88671875\n",
      "Iteration 40670 Training loss 0.002487583551555872 Validation loss 0.010615305975079536 Accuracy 0.88525390625\n",
      "Iteration 40680 Training loss 0.0024404097348451614 Validation loss 0.010622415691614151 Accuracy 0.88623046875\n",
      "Iteration 40690 Training loss 0.004164768382906914 Validation loss 0.010767382569611073 Accuracy 0.884765625\n",
      "Iteration 40700 Training loss 0.0034513319842517376 Validation loss 0.010797567665576935 Accuracy 0.88427734375\n",
      "Iteration 40710 Training loss 0.003384543815627694 Validation loss 0.010546527802944183 Accuracy 0.88671875\n",
      "Iteration 40720 Training loss 0.004559484776109457 Validation loss 0.010510292835533619 Accuracy 0.88671875\n",
      "Iteration 40730 Training loss 0.004016471095383167 Validation loss 0.010604765266180038 Accuracy 0.8857421875\n",
      "Iteration 40740 Training loss 0.0023098820820450783 Validation loss 0.010671067051589489 Accuracy 0.8857421875\n",
      "Iteration 40750 Training loss 0.003104679984971881 Validation loss 0.010581552051007748 Accuracy 0.88720703125\n",
      "Iteration 40760 Training loss 0.0033429490868002176 Validation loss 0.01096897479146719 Accuracy 0.8828125\n",
      "Iteration 40770 Training loss 0.003799624741077423 Validation loss 0.010695432312786579 Accuracy 0.884765625\n",
      "Iteration 40780 Training loss 0.003093745093792677 Validation loss 0.01084861345589161 Accuracy 0.884765625\n",
      "Iteration 40790 Training loss 0.0042871031910181046 Validation loss 0.01085218321532011 Accuracy 0.8837890625\n",
      "Iteration 40800 Training loss 0.0039395117200911045 Validation loss 0.010848186910152435 Accuracy 0.8837890625\n",
      "Iteration 40810 Training loss 0.0029676761478185654 Validation loss 0.01070837490260601 Accuracy 0.8857421875\n",
      "Iteration 40820 Training loss 0.003394907806068659 Validation loss 0.010784853249788284 Accuracy 0.88427734375\n",
      "Iteration 40830 Training loss 0.0037360338028520346 Validation loss 0.01069994643330574 Accuracy 0.8857421875\n",
      "Iteration 40840 Training loss 0.0021998323500156403 Validation loss 0.01058039627969265 Accuracy 0.88623046875\n",
      "Iteration 40850 Training loss 0.0032771260011941195 Validation loss 0.010606491006910801 Accuracy 0.88671875\n",
      "Iteration 40860 Training loss 0.004446040373295546 Validation loss 0.010472076013684273 Accuracy 0.8876953125\n",
      "Iteration 40870 Training loss 0.002968978602439165 Validation loss 0.01041122991591692 Accuracy 0.8876953125\n",
      "Iteration 40880 Training loss 0.0034339402336627245 Validation loss 0.010599249042570591 Accuracy 0.88671875\n",
      "Iteration 40890 Training loss 0.003742202650755644 Validation loss 0.010711646638810635 Accuracy 0.88525390625\n",
      "Iteration 40900 Training loss 0.0027755647897720337 Validation loss 0.0106497248634696 Accuracy 0.88720703125\n",
      "Iteration 40910 Training loss 0.003404265036806464 Validation loss 0.01059701107442379 Accuracy 0.88623046875\n",
      "Iteration 40920 Training loss 0.0014502271078526974 Validation loss 0.010660332627594471 Accuracy 0.884765625\n",
      "Iteration 40930 Training loss 0.0034897199366241693 Validation loss 0.0107234762981534 Accuracy 0.88525390625\n",
      "Iteration 40940 Training loss 0.0034427859354764223 Validation loss 0.010823322460055351 Accuracy 0.8837890625\n",
      "Iteration 40950 Training loss 0.0032705001067370176 Validation loss 0.010752084665000439 Accuracy 0.88525390625\n",
      "Iteration 40960 Training loss 0.002432272769510746 Validation loss 0.010663686320185661 Accuracy 0.88623046875\n",
      "Iteration 40970 Training loss 0.002293815603479743 Validation loss 0.010454664006829262 Accuracy 0.888671875\n",
      "Iteration 40980 Training loss 0.0037370878271758556 Validation loss 0.010568766854703426 Accuracy 0.88720703125\n",
      "Iteration 40990 Training loss 0.003060116432607174 Validation loss 0.01077550183981657 Accuracy 0.8837890625\n",
      "Iteration 41000 Training loss 0.0037830988876521587 Validation loss 0.010780119337141514 Accuracy 0.8837890625\n",
      "Iteration 41010 Training loss 0.0026359213516116142 Validation loss 0.01089339330792427 Accuracy 0.88330078125\n",
      "Iteration 41020 Training loss 0.0029534136410802603 Validation loss 0.010773235000669956 Accuracy 0.88427734375\n",
      "Iteration 41030 Training loss 0.004293100442737341 Validation loss 0.010685576125979424 Accuracy 0.88525390625\n",
      "Iteration 41040 Training loss 0.0045505003072321415 Validation loss 0.01068231649696827 Accuracy 0.88525390625\n",
      "Iteration 41050 Training loss 0.004169030115008354 Validation loss 0.010887746699154377 Accuracy 0.88330078125\n",
      "Iteration 41060 Training loss 0.003307654056698084 Validation loss 0.010800783522427082 Accuracy 0.884765625\n",
      "Iteration 41070 Training loss 0.0036416491493582726 Validation loss 0.010926386341452599 Accuracy 0.88232421875\n",
      "Iteration 41080 Training loss 0.002980851801112294 Validation loss 0.010659884661436081 Accuracy 0.88525390625\n",
      "Iteration 41090 Training loss 0.0030901883728802204 Validation loss 0.010712158866226673 Accuracy 0.884765625\n",
      "Iteration 41100 Training loss 0.003991662058979273 Validation loss 0.010712251998484135 Accuracy 0.88427734375\n",
      "Iteration 41110 Training loss 0.0035188954789191484 Validation loss 0.010558434762060642 Accuracy 0.88623046875\n",
      "Iteration 41120 Training loss 0.0023097421508282423 Validation loss 0.01072897668927908 Accuracy 0.8837890625\n",
      "Iteration 41130 Training loss 0.002893885364755988 Validation loss 0.010580367408692837 Accuracy 0.88623046875\n",
      "Iteration 41140 Training loss 0.0031289909966289997 Validation loss 0.010923395864665508 Accuracy 0.8837890625\n",
      "Iteration 41150 Training loss 0.002603123662993312 Validation loss 0.010592589154839516 Accuracy 0.88623046875\n",
      "Iteration 41160 Training loss 0.0028299991972744465 Validation loss 0.01059455331414938 Accuracy 0.88623046875\n",
      "Iteration 41170 Training loss 0.0031495278235524893 Validation loss 0.010694452561438084 Accuracy 0.8857421875\n",
      "Iteration 41180 Training loss 0.003720140317454934 Validation loss 0.010648438706994057 Accuracy 0.88623046875\n",
      "Iteration 41190 Training loss 0.003783336840569973 Validation loss 0.010637402534484863 Accuracy 0.88623046875\n",
      "Iteration 41200 Training loss 0.0035959421657025814 Validation loss 0.010751117952167988 Accuracy 0.88525390625\n",
      "Iteration 41210 Training loss 0.0042458935640752316 Validation loss 0.010650381445884705 Accuracy 0.8857421875\n",
      "Iteration 41220 Training loss 0.003421142464503646 Validation loss 0.010698300786316395 Accuracy 0.88525390625\n",
      "Iteration 41230 Training loss 0.0021571265533566475 Validation loss 0.010513232089579105 Accuracy 0.88671875\n",
      "Iteration 41240 Training loss 0.0022339774295687675 Validation loss 0.010744321160018444 Accuracy 0.88525390625\n",
      "Iteration 41250 Training loss 0.0021968712098896503 Validation loss 0.010637934319674969 Accuracy 0.88623046875\n",
      "Iteration 41260 Training loss 0.0031502668280154467 Validation loss 0.01075078547000885 Accuracy 0.88427734375\n",
      "Iteration 41270 Training loss 0.003044659271836281 Validation loss 0.010531813837587833 Accuracy 0.88720703125\n",
      "Iteration 41280 Training loss 0.003308027284219861 Validation loss 0.010831309482455254 Accuracy 0.8837890625\n",
      "Iteration 41290 Training loss 0.0032473027240484953 Validation loss 0.010776343755424023 Accuracy 0.8837890625\n",
      "Iteration 41300 Training loss 0.0029739532619714737 Validation loss 0.010647691786289215 Accuracy 0.88623046875\n",
      "Iteration 41310 Training loss 0.002755463821813464 Validation loss 0.010728497989475727 Accuracy 0.88427734375\n",
      "Iteration 41320 Training loss 0.003055589273571968 Validation loss 0.010764280334115028 Accuracy 0.88427734375\n",
      "Iteration 41330 Training loss 0.0035362732596695423 Validation loss 0.010864883661270142 Accuracy 0.8837890625\n",
      "Iteration 41340 Training loss 0.002687175292521715 Validation loss 0.010921475477516651 Accuracy 0.88232421875\n",
      "Iteration 41350 Training loss 0.0018192599527537823 Validation loss 0.01063476037234068 Accuracy 0.88623046875\n",
      "Iteration 41360 Training loss 0.004210844170302153 Validation loss 0.010941432788968086 Accuracy 0.88232421875\n",
      "Iteration 41370 Training loss 0.0026171961799263954 Validation loss 0.010772615671157837 Accuracy 0.884765625\n",
      "Iteration 41380 Training loss 0.003437890438362956 Validation loss 0.010796642862260342 Accuracy 0.8837890625\n",
      "Iteration 41390 Training loss 0.0031391994562000036 Validation loss 0.010748250409960747 Accuracy 0.88427734375\n",
      "Iteration 41400 Training loss 0.003350300481542945 Validation loss 0.010692162439227104 Accuracy 0.88525390625\n",
      "Iteration 41410 Training loss 0.002339232712984085 Validation loss 0.01061935257166624 Accuracy 0.884765625\n",
      "Iteration 41420 Training loss 0.0024925905745476484 Validation loss 0.010946283116936684 Accuracy 0.88232421875\n",
      "Iteration 41430 Training loss 0.0045610396191477776 Validation loss 0.010832538828253746 Accuracy 0.88427734375\n",
      "Iteration 41440 Training loss 0.0025900413747876883 Validation loss 0.010632151737809181 Accuracy 0.88525390625\n",
      "Iteration 41450 Training loss 0.0036383557599037886 Validation loss 0.010619803331792355 Accuracy 0.88720703125\n",
      "Iteration 41460 Training loss 0.0035364069044589996 Validation loss 0.010859690606594086 Accuracy 0.88427734375\n",
      "Iteration 41470 Training loss 0.003372395411133766 Validation loss 0.010672216303646564 Accuracy 0.88623046875\n",
      "Iteration 41480 Training loss 0.0022353988606482744 Validation loss 0.010494736023247242 Accuracy 0.88720703125\n",
      "Iteration 41490 Training loss 0.0023872521705925465 Validation loss 0.010720238089561462 Accuracy 0.8857421875\n",
      "Iteration 41500 Training loss 0.004415706731379032 Validation loss 0.010591485537588596 Accuracy 0.88623046875\n",
      "Iteration 41510 Training loss 0.0037292626220732927 Validation loss 0.011002550832927227 Accuracy 0.88134765625\n",
      "Iteration 41520 Training loss 0.0027780162636190653 Validation loss 0.010700507089495659 Accuracy 0.884765625\n",
      "Iteration 41530 Training loss 0.003491349518299103 Validation loss 0.010625583119690418 Accuracy 0.88525390625\n",
      "Iteration 41540 Training loss 0.0026023725513368845 Validation loss 0.010608375072479248 Accuracy 0.88671875\n",
      "Iteration 41550 Training loss 0.003959557507187128 Validation loss 0.011061913333833218 Accuracy 0.88232421875\n",
      "Iteration 41560 Training loss 0.003603869117796421 Validation loss 0.010793260298669338 Accuracy 0.8837890625\n",
      "Iteration 41570 Training loss 0.0029386768583208323 Validation loss 0.01096758060157299 Accuracy 0.8828125\n",
      "Iteration 41580 Training loss 0.00352676585316658 Validation loss 0.010614901781082153 Accuracy 0.88671875\n",
      "Iteration 41590 Training loss 0.002261608373373747 Validation loss 0.010671276599168777 Accuracy 0.88623046875\n",
      "Iteration 41600 Training loss 0.0021839295513927937 Validation loss 0.01075784396380186 Accuracy 0.8857421875\n",
      "Iteration 41610 Training loss 0.003814839059486985 Validation loss 0.010620568878948689 Accuracy 0.88671875\n",
      "Iteration 41620 Training loss 0.003440784988924861 Validation loss 0.010634960606694221 Accuracy 0.8857421875\n",
      "Iteration 41630 Training loss 0.0029181893914937973 Validation loss 0.010591474361717701 Accuracy 0.8857421875\n",
      "Iteration 41640 Training loss 0.003942484501749277 Validation loss 0.010630734264850616 Accuracy 0.8857421875\n",
      "Iteration 41650 Training loss 0.002804777817800641 Validation loss 0.010697155259549618 Accuracy 0.88427734375\n",
      "Iteration 41660 Training loss 0.0028932609129697084 Validation loss 0.010635926388204098 Accuracy 0.8857421875\n",
      "Iteration 41670 Training loss 0.002492086496204138 Validation loss 0.0106996214017272 Accuracy 0.884765625\n",
      "Iteration 41680 Training loss 0.0030134429689496756 Validation loss 0.010698098689317703 Accuracy 0.88330078125\n",
      "Iteration 41690 Training loss 0.002395401708781719 Validation loss 0.01069231890141964 Accuracy 0.884765625\n",
      "Iteration 41700 Training loss 0.002591641154140234 Validation loss 0.010476657189428806 Accuracy 0.88671875\n",
      "Iteration 41710 Training loss 0.004524221643805504 Validation loss 0.010640178807079792 Accuracy 0.88671875\n",
      "Iteration 41720 Training loss 0.002788958139717579 Validation loss 0.0106258699670434 Accuracy 0.8857421875\n",
      "Iteration 41730 Training loss 0.0035730688832700253 Validation loss 0.010821767151355743 Accuracy 0.8837890625\n",
      "Iteration 41740 Training loss 0.003704568836838007 Validation loss 0.010769772343337536 Accuracy 0.88525390625\n",
      "Iteration 41750 Training loss 0.002457505324855447 Validation loss 0.011003940366208553 Accuracy 0.8818359375\n",
      "Iteration 41760 Training loss 0.0027509257197380066 Validation loss 0.010853984393179417 Accuracy 0.88330078125\n",
      "Iteration 41770 Training loss 0.004554195795208216 Validation loss 0.011244344525039196 Accuracy 0.87939453125\n",
      "Iteration 41780 Training loss 0.004470594227313995 Validation loss 0.010769047774374485 Accuracy 0.884765625\n",
      "Iteration 41790 Training loss 0.0026572588831186295 Validation loss 0.010826320387423038 Accuracy 0.8837890625\n",
      "Iteration 41800 Training loss 0.0031262950506061316 Validation loss 0.010599174536764622 Accuracy 0.8857421875\n",
      "Iteration 41810 Training loss 0.004190958570688963 Validation loss 0.010910915210843086 Accuracy 0.88330078125\n",
      "Iteration 41820 Training loss 0.003638721536844969 Validation loss 0.01072815153747797 Accuracy 0.88525390625\n",
      "Iteration 41830 Training loss 0.00420098751783371 Validation loss 0.010802744887769222 Accuracy 0.88330078125\n",
      "Iteration 41840 Training loss 0.002288175979629159 Validation loss 0.010678809136152267 Accuracy 0.88525390625\n",
      "Iteration 41850 Training loss 0.0020791208371520042 Validation loss 0.010535912588238716 Accuracy 0.88525390625\n",
      "Iteration 41860 Training loss 0.0029612125363200903 Validation loss 0.010612688027322292 Accuracy 0.8857421875\n",
      "Iteration 41870 Training loss 0.004807450342923403 Validation loss 0.010814514011144638 Accuracy 0.8828125\n",
      "Iteration 41880 Training loss 0.003993681166321039 Validation loss 0.010639284737408161 Accuracy 0.8857421875\n",
      "Iteration 41890 Training loss 0.0032380828633904457 Validation loss 0.011091701686382294 Accuracy 0.88134765625\n",
      "Iteration 41900 Training loss 0.004058287478983402 Validation loss 0.010719245299696922 Accuracy 0.88427734375\n",
      "Iteration 41910 Training loss 0.0030420233961194754 Validation loss 0.010537548922002316 Accuracy 0.88671875\n",
      "Iteration 41920 Training loss 0.0028746582102030516 Validation loss 0.01073567382991314 Accuracy 0.8857421875\n",
      "Iteration 41930 Training loss 0.0026491691824048758 Validation loss 0.010626226663589478 Accuracy 0.8857421875\n",
      "Iteration 41940 Training loss 0.0028626041021198034 Validation loss 0.01063457503914833 Accuracy 0.88623046875\n",
      "Iteration 41950 Training loss 0.003211554139852524 Validation loss 0.01062957476824522 Accuracy 0.8857421875\n",
      "Iteration 41960 Training loss 0.002386959735304117 Validation loss 0.010677204467356205 Accuracy 0.88427734375\n",
      "Iteration 41970 Training loss 0.0036548771895468235 Validation loss 0.010738900862634182 Accuracy 0.88525390625\n",
      "Iteration 41980 Training loss 0.0028545341920107603 Validation loss 0.01066212821751833 Accuracy 0.8857421875\n",
      "Iteration 41990 Training loss 0.0035988858435302973 Validation loss 0.010422115214169025 Accuracy 0.8876953125\n",
      "Iteration 42000 Training loss 0.002936822595074773 Validation loss 0.01069143321365118 Accuracy 0.8857421875\n",
      "Iteration 42010 Training loss 0.004044622182846069 Validation loss 0.0109476363286376 Accuracy 0.8828125\n",
      "Iteration 42020 Training loss 0.00334080308675766 Validation loss 0.010759343393146992 Accuracy 0.88525390625\n",
      "Iteration 42030 Training loss 0.0033393395133316517 Validation loss 0.010883122682571411 Accuracy 0.8837890625\n",
      "Iteration 42040 Training loss 0.003142630448564887 Validation loss 0.010743080638349056 Accuracy 0.884765625\n",
      "Iteration 42050 Training loss 0.0034579080529510975 Validation loss 0.010769009590148926 Accuracy 0.8857421875\n",
      "Iteration 42060 Training loss 0.004460587166249752 Validation loss 0.010579644702374935 Accuracy 0.88623046875\n",
      "Iteration 42070 Training loss 0.004100378602743149 Validation loss 0.010672803968191147 Accuracy 0.88623046875\n",
      "Iteration 42080 Training loss 0.004099609330296516 Validation loss 0.0107363136485219 Accuracy 0.88427734375\n",
      "Iteration 42090 Training loss 0.002262985799461603 Validation loss 0.010580392554402351 Accuracy 0.88671875\n",
      "Iteration 42100 Training loss 0.003747670678421855 Validation loss 0.010652143508195877 Accuracy 0.8857421875\n",
      "Iteration 42110 Training loss 0.004047504160553217 Validation loss 0.010635686106979847 Accuracy 0.88623046875\n",
      "Iteration 42120 Training loss 0.002629151102155447 Validation loss 0.010506141930818558 Accuracy 0.8876953125\n",
      "Iteration 42130 Training loss 0.0030314859468489885 Validation loss 0.010555736720561981 Accuracy 0.88671875\n",
      "Iteration 42140 Training loss 0.0024497625418007374 Validation loss 0.010436005890369415 Accuracy 0.888671875\n",
      "Iteration 42150 Training loss 0.0025334390811622143 Validation loss 0.010473030619323254 Accuracy 0.8876953125\n",
      "Iteration 42160 Training loss 0.004409588873386383 Validation loss 0.010661615990102291 Accuracy 0.8857421875\n",
      "Iteration 42170 Training loss 0.0030782960820943117 Validation loss 0.010742985643446445 Accuracy 0.884765625\n",
      "Iteration 42180 Training loss 0.0030461647547781467 Validation loss 0.011001252569258213 Accuracy 0.880859375\n",
      "Iteration 42190 Training loss 0.0037092934362590313 Validation loss 0.010914742946624756 Accuracy 0.88330078125\n",
      "Iteration 42200 Training loss 0.0028340346179902554 Validation loss 0.010706650093197823 Accuracy 0.8837890625\n",
      "Iteration 42210 Training loss 0.003455501515418291 Validation loss 0.010740404017269611 Accuracy 0.88427734375\n",
      "Iteration 42220 Training loss 0.0025658474769443274 Validation loss 0.010552925989031792 Accuracy 0.88671875\n",
      "Iteration 42230 Training loss 0.003155280603095889 Validation loss 0.010743297636508942 Accuracy 0.88427734375\n",
      "Iteration 42240 Training loss 0.0020761892665177584 Validation loss 0.010705114342272282 Accuracy 0.88427734375\n",
      "Iteration 42250 Training loss 0.002841288223862648 Validation loss 0.010839150287210941 Accuracy 0.8837890625\n",
      "Iteration 42260 Training loss 0.0027945854235440493 Validation loss 0.010883810929954052 Accuracy 0.88330078125\n",
      "Iteration 42270 Training loss 0.0024814873468130827 Validation loss 0.010616803541779518 Accuracy 0.88623046875\n",
      "Iteration 42280 Training loss 0.0032734028063714504 Validation loss 0.010633294470608234 Accuracy 0.88525390625\n",
      "Iteration 42290 Training loss 0.002731210784986615 Validation loss 0.010697778314352036 Accuracy 0.8857421875\n",
      "Iteration 42300 Training loss 0.0035499476362019777 Validation loss 0.010647948831319809 Accuracy 0.88623046875\n",
      "Iteration 42310 Training loss 0.0030114827677607536 Validation loss 0.010818415321409702 Accuracy 0.88330078125\n",
      "Iteration 42320 Training loss 0.002440708689391613 Validation loss 0.010674175806343555 Accuracy 0.88623046875\n",
      "Iteration 42330 Training loss 0.003990080673247576 Validation loss 0.010749008506536484 Accuracy 0.884765625\n",
      "Iteration 42340 Training loss 0.0027719473000615835 Validation loss 0.010771789588034153 Accuracy 0.88427734375\n",
      "Iteration 42350 Training loss 0.0036969135981053114 Validation loss 0.010544662363827229 Accuracy 0.88671875\n",
      "Iteration 42360 Training loss 0.003427717834711075 Validation loss 0.010583911091089249 Accuracy 0.88525390625\n",
      "Iteration 42370 Training loss 0.002784150652587414 Validation loss 0.010592248290777206 Accuracy 0.88623046875\n",
      "Iteration 42380 Training loss 0.0026150369085371494 Validation loss 0.010433424264192581 Accuracy 0.8876953125\n",
      "Iteration 42390 Training loss 0.004155807662755251 Validation loss 0.01078000571578741 Accuracy 0.88427734375\n",
      "Iteration 42400 Training loss 0.0013080304488539696 Validation loss 0.010541405528783798 Accuracy 0.88623046875\n",
      "Iteration 42410 Training loss 0.003493159543722868 Validation loss 0.010928263887763023 Accuracy 0.88330078125\n",
      "Iteration 42420 Training loss 0.003518175333738327 Validation loss 0.011120337061583996 Accuracy 0.8798828125\n",
      "Iteration 42430 Training loss 0.0034513480495661497 Validation loss 0.011004838161170483 Accuracy 0.880859375\n",
      "Iteration 42440 Training loss 0.003785465843975544 Validation loss 0.010544384829699993 Accuracy 0.88671875\n",
      "Iteration 42450 Training loss 0.0029907994903624058 Validation loss 0.010696807876229286 Accuracy 0.88525390625\n",
      "Iteration 42460 Training loss 0.002635152079164982 Validation loss 0.01059782411903143 Accuracy 0.88525390625\n",
      "Iteration 42470 Training loss 0.002654146635904908 Validation loss 0.010662916116416454 Accuracy 0.88525390625\n",
      "Iteration 42480 Training loss 0.003088184632360935 Validation loss 0.01075603999197483 Accuracy 0.884765625\n",
      "Iteration 42490 Training loss 0.0027314515318721533 Validation loss 0.010793562978506088 Accuracy 0.88427734375\n",
      "Iteration 42500 Training loss 0.0034173668827861547 Validation loss 0.010674611665308475 Accuracy 0.88623046875\n",
      "Iteration 42510 Training loss 0.003754282835870981 Validation loss 0.010586857795715332 Accuracy 0.88623046875\n",
      "Iteration 42520 Training loss 0.002787795616313815 Validation loss 0.010804645717144012 Accuracy 0.88330078125\n",
      "Iteration 42530 Training loss 0.002767602214589715 Validation loss 0.010591519065201283 Accuracy 0.8857421875\n",
      "Iteration 42540 Training loss 0.003491208888590336 Validation loss 0.01067805103957653 Accuracy 0.88623046875\n",
      "Iteration 42550 Training loss 0.003669451456516981 Validation loss 0.010637974366545677 Accuracy 0.88720703125\n",
      "Iteration 42560 Training loss 0.003524972591549158 Validation loss 0.01061574462801218 Accuracy 0.88671875\n",
      "Iteration 42570 Training loss 0.0033930556382983923 Validation loss 0.01069639902561903 Accuracy 0.88525390625\n",
      "Iteration 42580 Training loss 0.003189884126186371 Validation loss 0.010728956200182438 Accuracy 0.884765625\n",
      "Iteration 42590 Training loss 0.0025037764571607113 Validation loss 0.010698121972382069 Accuracy 0.884765625\n",
      "Iteration 42600 Training loss 0.003607391845434904 Validation loss 0.010654745623469353 Accuracy 0.88623046875\n",
      "Iteration 42610 Training loss 0.0028226501308381557 Validation loss 0.010737945325672626 Accuracy 0.8857421875\n",
      "Iteration 42620 Training loss 0.002996055642142892 Validation loss 0.011030364781618118 Accuracy 0.880859375\n",
      "Iteration 42630 Training loss 0.0021531269885599613 Validation loss 0.010715317912399769 Accuracy 0.88427734375\n",
      "Iteration 42640 Training loss 0.0026533729396760464 Validation loss 0.011090063489973545 Accuracy 0.88134765625\n",
      "Iteration 42650 Training loss 0.003003116464242339 Validation loss 0.010592186823487282 Accuracy 0.88623046875\n",
      "Iteration 42660 Training loss 0.0027677242178469896 Validation loss 0.010641233995556831 Accuracy 0.88525390625\n",
      "Iteration 42670 Training loss 0.0028964527882635593 Validation loss 0.01073632575571537 Accuracy 0.88525390625\n",
      "Iteration 42680 Training loss 0.003010808490216732 Validation loss 0.010701579041779041 Accuracy 0.88427734375\n",
      "Iteration 42690 Training loss 0.0026590926572680473 Validation loss 0.010786451399326324 Accuracy 0.88232421875\n",
      "Iteration 42700 Training loss 0.003334522247314453 Validation loss 0.010747583582997322 Accuracy 0.8837890625\n",
      "Iteration 42710 Training loss 0.0031206670682877302 Validation loss 0.010584685951471329 Accuracy 0.8857421875\n",
      "Iteration 42720 Training loss 0.004616208840161562 Validation loss 0.0105296541005373 Accuracy 0.88671875\n",
      "Iteration 42730 Training loss 0.004210519138723612 Validation loss 0.010423803701996803 Accuracy 0.88720703125\n",
      "Iteration 42740 Training loss 0.0049119084142148495 Validation loss 0.010648053139448166 Accuracy 0.88623046875\n",
      "Iteration 42750 Training loss 0.002217611065134406 Validation loss 0.0106447022408247 Accuracy 0.8857421875\n",
      "Iteration 42760 Training loss 0.0018104114569723606 Validation loss 0.010841520503163338 Accuracy 0.88427734375\n",
      "Iteration 42770 Training loss 0.0033669264521449804 Validation loss 0.01062835194170475 Accuracy 0.88623046875\n",
      "Iteration 42780 Training loss 0.0016395162092521787 Validation loss 0.01054171472787857 Accuracy 0.88671875\n",
      "Iteration 42790 Training loss 0.0023689132649451494 Validation loss 0.010792125016450882 Accuracy 0.884765625\n",
      "Iteration 42800 Training loss 0.0038198058027774096 Validation loss 0.01072762068361044 Accuracy 0.88525390625\n",
      "Iteration 42810 Training loss 0.0024745273403823376 Validation loss 0.010750995948910713 Accuracy 0.8857421875\n",
      "Iteration 42820 Training loss 0.003002341603860259 Validation loss 0.010734904557466507 Accuracy 0.8857421875\n",
      "Iteration 42830 Training loss 0.0033458583056926727 Validation loss 0.010704242624342442 Accuracy 0.88525390625\n",
      "Iteration 42840 Training loss 0.002028099028393626 Validation loss 0.010544994845986366 Accuracy 0.88720703125\n",
      "Iteration 42850 Training loss 0.0023377889301627874 Validation loss 0.010570197366178036 Accuracy 0.88623046875\n",
      "Iteration 42860 Training loss 0.004820171277970076 Validation loss 0.010546262376010418 Accuracy 0.88671875\n",
      "Iteration 42870 Training loss 0.0030791577883064747 Validation loss 0.010462351143360138 Accuracy 0.88818359375\n",
      "Iteration 42880 Training loss 0.004049114417284727 Validation loss 0.010346520692110062 Accuracy 0.88916015625\n",
      "Iteration 42890 Training loss 0.0017461908282712102 Validation loss 0.01043880358338356 Accuracy 0.888671875\n",
      "Iteration 42900 Training loss 0.003010490443557501 Validation loss 0.010615898296236992 Accuracy 0.8857421875\n",
      "Iteration 42910 Training loss 0.003788809757679701 Validation loss 0.010580925270915031 Accuracy 0.88720703125\n",
      "Iteration 42920 Training loss 0.0021772682666778564 Validation loss 0.010586635209619999 Accuracy 0.88720703125\n",
      "Iteration 42930 Training loss 0.0038592801429331303 Validation loss 0.0107272919267416 Accuracy 0.88427734375\n",
      "Iteration 42940 Training loss 0.003994669299572706 Validation loss 0.010662783868610859 Accuracy 0.8857421875\n",
      "Iteration 42950 Training loss 0.0026426336262375116 Validation loss 0.010681310668587685 Accuracy 0.884765625\n",
      "Iteration 42960 Training loss 0.003272591158747673 Validation loss 0.010518956929445267 Accuracy 0.88671875\n",
      "Iteration 42970 Training loss 0.00347473518922925 Validation loss 0.010634389705955982 Accuracy 0.884765625\n",
      "Iteration 42980 Training loss 0.002759831491857767 Validation loss 0.010630137287080288 Accuracy 0.8857421875\n",
      "Iteration 42990 Training loss 0.002541032386943698 Validation loss 0.010669445618987083 Accuracy 0.88623046875\n",
      "Iteration 43000 Training loss 0.0026532700285315514 Validation loss 0.010747209191322327 Accuracy 0.88427734375\n",
      "Iteration 43010 Training loss 0.0030459905974566936 Validation loss 0.010611024685204029 Accuracy 0.88623046875\n",
      "Iteration 43020 Training loss 0.0028242364060133696 Validation loss 0.010562499985098839 Accuracy 0.88671875\n",
      "Iteration 43030 Training loss 0.002178437076508999 Validation loss 0.010680732317268848 Accuracy 0.884765625\n",
      "Iteration 43040 Training loss 0.0032406558748334646 Validation loss 0.010646463371813297 Accuracy 0.88525390625\n",
      "Iteration 43050 Training loss 0.0021759257651865482 Validation loss 0.01068168692290783 Accuracy 0.88427734375\n",
      "Iteration 43060 Training loss 0.0035285570193082094 Validation loss 0.010835709981620312 Accuracy 0.88427734375\n",
      "Iteration 43070 Training loss 0.002564008114859462 Validation loss 0.010774033144116402 Accuracy 0.88427734375\n",
      "Iteration 43080 Training loss 0.0031943332869559526 Validation loss 0.010539577342569828 Accuracy 0.8876953125\n",
      "Iteration 43090 Training loss 0.0032045592088252306 Validation loss 0.010727724060416222 Accuracy 0.88525390625\n",
      "Iteration 43100 Training loss 0.0038654538802802563 Validation loss 0.010631939396262169 Accuracy 0.8857421875\n",
      "Iteration 43110 Training loss 0.0036169083323329687 Validation loss 0.010697347111999989 Accuracy 0.8857421875\n",
      "Iteration 43120 Training loss 0.0037448813673108816 Validation loss 0.010768145322799683 Accuracy 0.88427734375\n",
      "Iteration 43130 Training loss 0.0027234998997300863 Validation loss 0.010892525315284729 Accuracy 0.8828125\n",
      "Iteration 43140 Training loss 0.0025530795101076365 Validation loss 0.01064242608845234 Accuracy 0.884765625\n",
      "Iteration 43150 Training loss 0.00262086046859622 Validation loss 0.010648391209542751 Accuracy 0.8857421875\n",
      "Iteration 43160 Training loss 0.00379352574236691 Validation loss 0.010776229202747345 Accuracy 0.88525390625\n",
      "Iteration 43170 Training loss 0.0035548838786780834 Validation loss 0.010767641477286816 Accuracy 0.88525390625\n",
      "Iteration 43180 Training loss 0.0030283050145953894 Validation loss 0.010597418062388897 Accuracy 0.8857421875\n",
      "Iteration 43190 Training loss 0.0041106888093054295 Validation loss 0.01063974667340517 Accuracy 0.88623046875\n",
      "Iteration 43200 Training loss 0.0037712426856160164 Validation loss 0.010643655434250832 Accuracy 0.88525390625\n",
      "Iteration 43210 Training loss 0.0031903653871268034 Validation loss 0.010542700067162514 Accuracy 0.88623046875\n",
      "Iteration 43220 Training loss 0.003776431316509843 Validation loss 0.010543324053287506 Accuracy 0.8876953125\n",
      "Iteration 43230 Training loss 0.002426769118756056 Validation loss 0.010701015591621399 Accuracy 0.8857421875\n",
      "Iteration 43240 Training loss 0.0036855051293969154 Validation loss 0.011320219375193119 Accuracy 0.87841796875\n",
      "Iteration 43250 Training loss 0.003759271465241909 Validation loss 0.010636474005877972 Accuracy 0.88671875\n",
      "Iteration 43260 Training loss 0.00259147840552032 Validation loss 0.010594562627375126 Accuracy 0.88525390625\n",
      "Iteration 43270 Training loss 0.0028511567506939173 Validation loss 0.010685000568628311 Accuracy 0.88427734375\n",
      "Iteration 43280 Training loss 0.003783458610996604 Validation loss 0.01057412289083004 Accuracy 0.8857421875\n",
      "Iteration 43290 Training loss 0.003392287530004978 Validation loss 0.010638942942023277 Accuracy 0.88525390625\n",
      "Iteration 43300 Training loss 0.0037529771216213703 Validation loss 0.010472983121871948 Accuracy 0.88720703125\n",
      "Iteration 43310 Training loss 0.0027059824205935 Validation loss 0.010464834049344063 Accuracy 0.88720703125\n",
      "Iteration 43320 Training loss 0.0021741287782788277 Validation loss 0.01071704551577568 Accuracy 0.8857421875\n",
      "Iteration 43330 Training loss 0.003521881066262722 Validation loss 0.010545369237661362 Accuracy 0.88720703125\n",
      "Iteration 43340 Training loss 0.0024445110466331244 Validation loss 0.010745839215815067 Accuracy 0.8857421875\n",
      "Iteration 43350 Training loss 0.0019077910110354424 Validation loss 0.010498955845832825 Accuracy 0.8876953125\n",
      "Iteration 43360 Training loss 0.002843189751729369 Validation loss 0.010414795018732548 Accuracy 0.8876953125\n",
      "Iteration 43370 Training loss 0.0022045571822673082 Validation loss 0.010650199837982655 Accuracy 0.8857421875\n",
      "Iteration 43380 Training loss 0.002639778656885028 Validation loss 0.010718943551182747 Accuracy 0.88427734375\n",
      "Iteration 43390 Training loss 0.0028830391820520163 Validation loss 0.01077308040112257 Accuracy 0.88427734375\n",
      "Iteration 43400 Training loss 0.00339674623683095 Validation loss 0.010667025111615658 Accuracy 0.88525390625\n",
      "Iteration 43410 Training loss 0.0032579945400357246 Validation loss 0.010794579982757568 Accuracy 0.88525390625\n",
      "Iteration 43420 Training loss 0.0026601182762533426 Validation loss 0.01076617743819952 Accuracy 0.88330078125\n",
      "Iteration 43430 Training loss 0.004529840312898159 Validation loss 0.010618251748383045 Accuracy 0.8857421875\n",
      "Iteration 43440 Training loss 0.004071689676493406 Validation loss 0.010673449374735355 Accuracy 0.884765625\n",
      "Iteration 43450 Training loss 0.0034798826090991497 Validation loss 0.010897309519350529 Accuracy 0.8818359375\n",
      "Iteration 43460 Training loss 0.0031682541593909264 Validation loss 0.01081271655857563 Accuracy 0.8828125\n",
      "Iteration 43470 Training loss 0.004456689581274986 Validation loss 0.01067390851676464 Accuracy 0.8857421875\n",
      "Iteration 43480 Training loss 0.0028034579008817673 Validation loss 0.010767862200737 Accuracy 0.88525390625\n",
      "Iteration 43490 Training loss 0.003684673923999071 Validation loss 0.01069034356623888 Accuracy 0.8857421875\n",
      "Iteration 43500 Training loss 0.002853497164323926 Validation loss 0.0106031633913517 Accuracy 0.88623046875\n",
      "Iteration 43510 Training loss 0.0030095456168055534 Validation loss 0.01068070251494646 Accuracy 0.884765625\n",
      "Iteration 43520 Training loss 0.0038725254125893116 Validation loss 0.010524146258831024 Accuracy 0.88720703125\n",
      "Iteration 43530 Training loss 0.0026685886550694704 Validation loss 0.010595392435789108 Accuracy 0.88623046875\n",
      "Iteration 43540 Training loss 0.002577745821326971 Validation loss 0.010759394615888596 Accuracy 0.8837890625\n",
      "Iteration 43550 Training loss 0.0028490759432315826 Validation loss 0.010505951941013336 Accuracy 0.88671875\n",
      "Iteration 43560 Training loss 0.002433009212836623 Validation loss 0.010573063977062702 Accuracy 0.88671875\n",
      "Iteration 43570 Training loss 0.003622195916250348 Validation loss 0.010596295818686485 Accuracy 0.88623046875\n",
      "Iteration 43580 Training loss 0.0034555222373455763 Validation loss 0.01067711878567934 Accuracy 0.8857421875\n",
      "Iteration 43590 Training loss 0.002529617166146636 Validation loss 0.010499111376702785 Accuracy 0.88818359375\n",
      "Iteration 43600 Training loss 0.0026781968772411346 Validation loss 0.010743198916316032 Accuracy 0.8837890625\n",
      "Iteration 43610 Training loss 0.002972551854327321 Validation loss 0.010772584937512875 Accuracy 0.8837890625\n",
      "Iteration 43620 Training loss 0.004266245290637016 Validation loss 0.010623800568282604 Accuracy 0.88623046875\n",
      "Iteration 43630 Training loss 0.0035642993170768023 Validation loss 0.010672258213162422 Accuracy 0.88427734375\n",
      "Iteration 43640 Training loss 0.0026714708656072617 Validation loss 0.010586386546492577 Accuracy 0.88623046875\n",
      "Iteration 43650 Training loss 0.0017719307215884328 Validation loss 0.010686306282877922 Accuracy 0.884765625\n",
      "Iteration 43660 Training loss 0.004111915826797485 Validation loss 0.010520871728658676 Accuracy 0.8876953125\n",
      "Iteration 43670 Training loss 0.00234472774900496 Validation loss 0.010531923733651638 Accuracy 0.88623046875\n",
      "Iteration 43680 Training loss 0.0028314103838056326 Validation loss 0.010699720121920109 Accuracy 0.884765625\n",
      "Iteration 43690 Training loss 0.0035121766850352287 Validation loss 0.010665380395948887 Accuracy 0.88671875\n",
      "Iteration 43700 Training loss 0.004745837301015854 Validation loss 0.010650424286723137 Accuracy 0.88623046875\n",
      "Iteration 43710 Training loss 0.004213080741465092 Validation loss 0.01070246659219265 Accuracy 0.8857421875\n",
      "Iteration 43720 Training loss 0.0038109777960926294 Validation loss 0.010571049526333809 Accuracy 0.88623046875\n",
      "Iteration 43730 Training loss 0.0025257703382521868 Validation loss 0.010692643001675606 Accuracy 0.88623046875\n",
      "Iteration 43740 Training loss 0.003246262203902006 Validation loss 0.010502837598323822 Accuracy 0.88720703125\n",
      "Iteration 43750 Training loss 0.0022925653029233217 Validation loss 0.01058558002114296 Accuracy 0.8857421875\n",
      "Iteration 43760 Training loss 0.0021510536316782236 Validation loss 0.010627989657223225 Accuracy 0.88671875\n",
      "Iteration 43770 Training loss 0.0036628853995352983 Validation loss 0.010687639936804771 Accuracy 0.88427734375\n",
      "Iteration 43780 Training loss 0.0028595682233572006 Validation loss 0.010640610940754414 Accuracy 0.8857421875\n",
      "Iteration 43790 Training loss 0.0032507088035345078 Validation loss 0.010623457841575146 Accuracy 0.884765625\n",
      "Iteration 43800 Training loss 0.0034427775535732508 Validation loss 0.010717760771512985 Accuracy 0.884765625\n",
      "Iteration 43810 Training loss 0.0038962687831372023 Validation loss 0.010745061561465263 Accuracy 0.88427734375\n",
      "Iteration 43820 Training loss 0.0035376872401684523 Validation loss 0.010861112736165524 Accuracy 0.88330078125\n",
      "Iteration 43830 Training loss 0.0037728403694927692 Validation loss 0.010938824154436588 Accuracy 0.8828125\n",
      "Iteration 43840 Training loss 0.002412542700767517 Validation loss 0.010694729164242744 Accuracy 0.88525390625\n",
      "Iteration 43850 Training loss 0.0036924686282873154 Validation loss 0.01066662184894085 Accuracy 0.8857421875\n",
      "Iteration 43860 Training loss 0.002308448078110814 Validation loss 0.010577099397778511 Accuracy 0.8857421875\n",
      "Iteration 43870 Training loss 0.003142639761790633 Validation loss 0.01076273899525404 Accuracy 0.884765625\n",
      "Iteration 43880 Training loss 0.0028451576363295317 Validation loss 0.010791991837322712 Accuracy 0.8837890625\n",
      "Iteration 43890 Training loss 0.002769823418930173 Validation loss 0.010699198581278324 Accuracy 0.88427734375\n",
      "Iteration 43900 Training loss 0.00337530137039721 Validation loss 0.010701064020395279 Accuracy 0.88525390625\n",
      "Iteration 43910 Training loss 0.003933677449822426 Validation loss 0.010757463052868843 Accuracy 0.88330078125\n",
      "Iteration 43920 Training loss 0.0035400851629674435 Validation loss 0.010805265977978706 Accuracy 0.88427734375\n",
      "Iteration 43930 Training loss 0.0021950984373688698 Validation loss 0.010741034522652626 Accuracy 0.884765625\n",
      "Iteration 43940 Training loss 0.003022278193384409 Validation loss 0.01087250467389822 Accuracy 0.88232421875\n",
      "Iteration 43950 Training loss 0.0022665958385914564 Validation loss 0.010695111937820911 Accuracy 0.88525390625\n",
      "Iteration 43960 Training loss 0.004359654616564512 Validation loss 0.010777927935123444 Accuracy 0.88623046875\n",
      "Iteration 43970 Training loss 0.0030180790927261114 Validation loss 0.010852071456611156 Accuracy 0.88330078125\n",
      "Iteration 43980 Training loss 0.002497021108865738 Validation loss 0.01081538014113903 Accuracy 0.88330078125\n",
      "Iteration 43990 Training loss 0.0028980127535760403 Validation loss 0.010693512856960297 Accuracy 0.88525390625\n",
      "Iteration 44000 Training loss 0.002512105042114854 Validation loss 0.010733582079410553 Accuracy 0.88525390625\n",
      "Iteration 44010 Training loss 0.0034256966318935156 Validation loss 0.010709100402891636 Accuracy 0.884765625\n",
      "Iteration 44020 Training loss 0.0036209067329764366 Validation loss 0.010648042894899845 Accuracy 0.88623046875\n",
      "Iteration 44030 Training loss 0.0032602993305772543 Validation loss 0.010763893835246563 Accuracy 0.884765625\n",
      "Iteration 44040 Training loss 0.0037086522206664085 Validation loss 0.010886513628065586 Accuracy 0.88232421875\n",
      "Iteration 44050 Training loss 0.0032856445759534836 Validation loss 0.010790151543915272 Accuracy 0.88427734375\n",
      "Iteration 44060 Training loss 0.003955204505473375 Validation loss 0.010928626172244549 Accuracy 0.8828125\n",
      "Iteration 44070 Training loss 0.0031229699961841106 Validation loss 0.010866200551390648 Accuracy 0.8828125\n",
      "Iteration 44080 Training loss 0.002830593613907695 Validation loss 0.01096384972333908 Accuracy 0.88232421875\n",
      "Iteration 44090 Training loss 0.004313889890909195 Validation loss 0.010949301533401012 Accuracy 0.8818359375\n",
      "Iteration 44100 Training loss 0.0022802178282290697 Validation loss 0.011051452718675137 Accuracy 0.8818359375\n",
      "Iteration 44110 Training loss 0.003654740983620286 Validation loss 0.010871664620935917 Accuracy 0.8828125\n",
      "Iteration 44120 Training loss 0.002663949504494667 Validation loss 0.010853917337954044 Accuracy 0.88330078125\n",
      "Iteration 44130 Training loss 0.003414709120988846 Validation loss 0.010940446518361568 Accuracy 0.8828125\n",
      "Iteration 44140 Training loss 0.0034411961678415537 Validation loss 0.010880221612751484 Accuracy 0.88330078125\n",
      "Iteration 44150 Training loss 0.0035450260620564222 Validation loss 0.010877043008804321 Accuracy 0.8837890625\n",
      "Iteration 44160 Training loss 0.003397765103727579 Validation loss 0.010882665403187275 Accuracy 0.8837890625\n",
      "Iteration 44170 Training loss 0.0035241087898612022 Validation loss 0.010874731466174126 Accuracy 0.8837890625\n",
      "Iteration 44180 Training loss 0.00297885132022202 Validation loss 0.010846978053450584 Accuracy 0.8837890625\n",
      "Iteration 44190 Training loss 0.0028962395153939724 Validation loss 0.011125299148261547 Accuracy 0.880859375\n",
      "Iteration 44200 Training loss 0.0029234387911856174 Validation loss 0.01088419184088707 Accuracy 0.88330078125\n",
      "Iteration 44210 Training loss 0.0035950113087892532 Validation loss 0.010866757482290268 Accuracy 0.8837890625\n",
      "Iteration 44220 Training loss 0.0028011782560497522 Validation loss 0.01082372572273016 Accuracy 0.8837890625\n",
      "Iteration 44230 Training loss 0.003782095853239298 Validation loss 0.010823319666087627 Accuracy 0.88330078125\n",
      "Iteration 44240 Training loss 0.0026539727114140987 Validation loss 0.010724836960434914 Accuracy 0.884765625\n",
      "Iteration 44250 Training loss 0.002717049792408943 Validation loss 0.010818066075444221 Accuracy 0.8837890625\n",
      "Iteration 44260 Training loss 0.002657763659954071 Validation loss 0.010561729781329632 Accuracy 0.88623046875\n",
      "Iteration 44270 Training loss 0.0028130861464887857 Validation loss 0.010643956251442432 Accuracy 0.8857421875\n",
      "Iteration 44280 Training loss 0.0031628322321921587 Validation loss 0.010536924935877323 Accuracy 0.88720703125\n",
      "Iteration 44290 Training loss 0.004038010723888874 Validation loss 0.010809484869241714 Accuracy 0.8837890625\n",
      "Iteration 44300 Training loss 0.0031004950869828463 Validation loss 0.010729154571890831 Accuracy 0.884765625\n",
      "Iteration 44310 Training loss 0.0033865601290017366 Validation loss 0.010621354915201664 Accuracy 0.88623046875\n",
      "Iteration 44320 Training loss 0.002455378184095025 Validation loss 0.010658567771315575 Accuracy 0.88623046875\n",
      "Iteration 44330 Training loss 0.002317163860425353 Validation loss 0.010768922977149487 Accuracy 0.8837890625\n",
      "Iteration 44340 Training loss 0.004080381244421005 Validation loss 0.010721425525844097 Accuracy 0.88427734375\n",
      "Iteration 44350 Training loss 0.003102795220911503 Validation loss 0.010734504088759422 Accuracy 0.8837890625\n",
      "Iteration 44360 Training loss 0.0030501035507768393 Validation loss 0.010931672528386116 Accuracy 0.8818359375\n",
      "Iteration 44370 Training loss 0.004327910020947456 Validation loss 0.010705417022109032 Accuracy 0.88525390625\n",
      "Iteration 44380 Training loss 0.002195813925936818 Validation loss 0.010757277719676495 Accuracy 0.88427734375\n",
      "Iteration 44390 Training loss 0.0025789164938032627 Validation loss 0.010804273188114166 Accuracy 0.8837890625\n",
      "Iteration 44400 Training loss 0.002947867615148425 Validation loss 0.010615619830787182 Accuracy 0.88671875\n",
      "Iteration 44410 Training loss 0.003809420159086585 Validation loss 0.010649366304278374 Accuracy 0.884765625\n",
      "Iteration 44420 Training loss 0.004162325523793697 Validation loss 0.010635319165885448 Accuracy 0.88427734375\n",
      "Iteration 44430 Training loss 0.002702775876969099 Validation loss 0.01056775264441967 Accuracy 0.88623046875\n",
      "Iteration 44440 Training loss 0.002284494461491704 Validation loss 0.01056625321507454 Accuracy 0.88623046875\n",
      "Iteration 44450 Training loss 0.0033547459170222282 Validation loss 0.010759560391306877 Accuracy 0.884765625\n",
      "Iteration 44460 Training loss 0.003619581228122115 Validation loss 0.010731554590165615 Accuracy 0.8837890625\n",
      "Iteration 44470 Training loss 0.0036875600926578045 Validation loss 0.010889487341046333 Accuracy 0.88232421875\n",
      "Iteration 44480 Training loss 0.003240046091377735 Validation loss 0.01062419917434454 Accuracy 0.88623046875\n",
      "Iteration 44490 Training loss 0.002787858946248889 Validation loss 0.01079415250569582 Accuracy 0.88330078125\n",
      "Iteration 44500 Training loss 0.0036897265817970037 Validation loss 0.010766876861453056 Accuracy 0.8837890625\n",
      "Iteration 44510 Training loss 0.002755426801741123 Validation loss 0.01075101736932993 Accuracy 0.88427734375\n",
      "Iteration 44520 Training loss 0.0036651541013270617 Validation loss 0.010650415904819965 Accuracy 0.884765625\n",
      "Iteration 44530 Training loss 0.0044106608256697655 Validation loss 0.01097075641155243 Accuracy 0.88232421875\n",
      "Iteration 44540 Training loss 0.0037321129348129034 Validation loss 0.010707821696996689 Accuracy 0.884765625\n",
      "Iteration 44550 Training loss 0.003033902496099472 Validation loss 0.010684213601052761 Accuracy 0.884765625\n",
      "Iteration 44560 Training loss 0.004189174622297287 Validation loss 0.010703892447054386 Accuracy 0.884765625\n",
      "Iteration 44570 Training loss 0.0024560189340263605 Validation loss 0.010707658715546131 Accuracy 0.88525390625\n",
      "Iteration 44580 Training loss 0.00387491169385612 Validation loss 0.01073025818914175 Accuracy 0.88525390625\n",
      "Iteration 44590 Training loss 0.004191360902041197 Validation loss 0.010605852119624615 Accuracy 0.88623046875\n",
      "Iteration 44600 Training loss 0.0030172092374414206 Validation loss 0.010613112710416317 Accuracy 0.8857421875\n",
      "Iteration 44610 Training loss 0.002283101435750723 Validation loss 0.010639730840921402 Accuracy 0.88525390625\n",
      "Iteration 44620 Training loss 0.0021754922345280647 Validation loss 0.010667752474546432 Accuracy 0.88525390625\n",
      "Iteration 44630 Training loss 0.0024539176374673843 Validation loss 0.010643496178090572 Accuracy 0.884765625\n",
      "Iteration 44640 Training loss 0.004407824017107487 Validation loss 0.010679470375180244 Accuracy 0.884765625\n",
      "Iteration 44650 Training loss 0.0038196074310690165 Validation loss 0.010652459226548672 Accuracy 0.88427734375\n",
      "Iteration 44660 Training loss 0.0016405553324148059 Validation loss 0.010804361663758755 Accuracy 0.88427734375\n",
      "Iteration 44670 Training loss 0.002403398510068655 Validation loss 0.010665043257176876 Accuracy 0.8857421875\n",
      "Iteration 44680 Training loss 0.0037878158036619425 Validation loss 0.010564611293375492 Accuracy 0.88623046875\n",
      "Iteration 44690 Training loss 0.0022808751091361046 Validation loss 0.010521230287849903 Accuracy 0.88720703125\n",
      "Iteration 44700 Training loss 0.003575672395527363 Validation loss 0.010667726397514343 Accuracy 0.88525390625\n",
      "Iteration 44710 Training loss 0.0036804755218327045 Validation loss 0.010742207057774067 Accuracy 0.88427734375\n",
      "Iteration 44720 Training loss 0.003103247843682766 Validation loss 0.010677658021450043 Accuracy 0.88623046875\n",
      "Iteration 44730 Training loss 0.0030299376230686903 Validation loss 0.010848430916666985 Accuracy 0.88232421875\n",
      "Iteration 44740 Training loss 0.0035578873939812183 Validation loss 0.010843581520020962 Accuracy 0.88330078125\n",
      "Iteration 44750 Training loss 0.003666250267997384 Validation loss 0.010864541865885258 Accuracy 0.88330078125\n",
      "Iteration 44760 Training loss 0.0035083817783743143 Validation loss 0.010675141587853432 Accuracy 0.8857421875\n",
      "Iteration 44770 Training loss 0.0028246999718248844 Validation loss 0.0106230853125453 Accuracy 0.88623046875\n",
      "Iteration 44780 Training loss 0.0035392700228840113 Validation loss 0.010686174035072327 Accuracy 0.88427734375\n",
      "Iteration 44790 Training loss 0.0017640263540670276 Validation loss 0.010714098811149597 Accuracy 0.884765625\n",
      "Iteration 44800 Training loss 0.0036121548619121313 Validation loss 0.010608985088765621 Accuracy 0.88671875\n",
      "Iteration 44810 Training loss 0.003244862426072359 Validation loss 0.010765539482235909 Accuracy 0.8837890625\n",
      "Iteration 44820 Training loss 0.0042574149556458 Validation loss 0.010768609121441841 Accuracy 0.884765625\n",
      "Iteration 44830 Training loss 0.002694072900339961 Validation loss 0.010656364262104034 Accuracy 0.8857421875\n",
      "Iteration 44840 Training loss 0.001963603077456355 Validation loss 0.010675644502043724 Accuracy 0.884765625\n",
      "Iteration 44850 Training loss 0.002518025226891041 Validation loss 0.0108222970739007 Accuracy 0.8837890625\n",
      "Iteration 44860 Training loss 0.0034349130000919104 Validation loss 0.010640480555593967 Accuracy 0.88671875\n",
      "Iteration 44870 Training loss 0.003440509084612131 Validation loss 0.010813465341925621 Accuracy 0.8837890625\n",
      "Iteration 44880 Training loss 0.003465634537860751 Validation loss 0.010851128026843071 Accuracy 0.8828125\n",
      "Iteration 44890 Training loss 0.001839072210714221 Validation loss 0.01072730217128992 Accuracy 0.88623046875\n",
      "Iteration 44900 Training loss 0.00318328058347106 Validation loss 0.010649381205439568 Accuracy 0.8857421875\n",
      "Iteration 44910 Training loss 0.003437418257817626 Validation loss 0.010613063350319862 Accuracy 0.88623046875\n",
      "Iteration 44920 Training loss 0.0034428329672664404 Validation loss 0.01070384494960308 Accuracy 0.884765625\n",
      "Iteration 44930 Training loss 0.0033459924161434174 Validation loss 0.010735644027590752 Accuracy 0.8837890625\n",
      "Iteration 44940 Training loss 0.0033011646009981632 Validation loss 0.01065994892269373 Accuracy 0.8857421875\n",
      "Iteration 44950 Training loss 0.0026141004636883736 Validation loss 0.010769728571176529 Accuracy 0.88427734375\n",
      "Iteration 44960 Training loss 0.003050577826797962 Validation loss 0.010837206616997719 Accuracy 0.88427734375\n",
      "Iteration 44970 Training loss 0.002783013042062521 Validation loss 0.010654189623892307 Accuracy 0.8857421875\n",
      "Iteration 44980 Training loss 0.0024897686671465635 Validation loss 0.010667516849935055 Accuracy 0.8857421875\n",
      "Iteration 44990 Training loss 0.004751409869641066 Validation loss 0.010816887021064758 Accuracy 0.8837890625\n",
      "Iteration 45000 Training loss 0.0020667097996920347 Validation loss 0.010791923850774765 Accuracy 0.884765625\n",
      "Iteration 45010 Training loss 0.002640285762026906 Validation loss 0.010835863649845123 Accuracy 0.8837890625\n",
      "Iteration 45020 Training loss 0.004318063147366047 Validation loss 0.010835262015461922 Accuracy 0.88330078125\n",
      "Iteration 45030 Training loss 0.0021837169770151377 Validation loss 0.010721943341195583 Accuracy 0.88427734375\n",
      "Iteration 45040 Training loss 0.0012258682399988174 Validation loss 0.010789615102112293 Accuracy 0.8837890625\n",
      "Iteration 45050 Training loss 0.003808677662163973 Validation loss 0.010636902414262295 Accuracy 0.8857421875\n",
      "Iteration 45060 Training loss 0.0030562984757125378 Validation loss 0.010836580768227577 Accuracy 0.8828125\n",
      "Iteration 45070 Training loss 0.0026778888422995806 Validation loss 0.010679514147341251 Accuracy 0.88525390625\n",
      "Iteration 45080 Training loss 0.002651395509019494 Validation loss 0.010790424421429634 Accuracy 0.8837890625\n",
      "Iteration 45090 Training loss 0.0027414073701947927 Validation loss 0.010990936309099197 Accuracy 0.8818359375\n",
      "Iteration 45100 Training loss 0.004281431902199984 Validation loss 0.010818964801728725 Accuracy 0.88427734375\n",
      "Iteration 45110 Training loss 0.002522441791370511 Validation loss 0.01100603025406599 Accuracy 0.8818359375\n",
      "Iteration 45120 Training loss 0.003300166456028819 Validation loss 0.010769295506179333 Accuracy 0.88427734375\n",
      "Iteration 45130 Training loss 0.00192247424274683 Validation loss 0.01064543891698122 Accuracy 0.88671875\n",
      "Iteration 45140 Training loss 0.0027269073761999607 Validation loss 0.01067738514393568 Accuracy 0.884765625\n",
      "Iteration 45150 Training loss 0.003050445578992367 Validation loss 0.010769503191113472 Accuracy 0.8837890625\n",
      "Iteration 45160 Training loss 0.0024618152529001236 Validation loss 0.010876896791160107 Accuracy 0.88330078125\n",
      "Iteration 45170 Training loss 0.0023879350628703833 Validation loss 0.010808732360601425 Accuracy 0.884765625\n",
      "Iteration 45180 Training loss 0.0032485343981534243 Validation loss 0.010633922182023525 Accuracy 0.88623046875\n",
      "Iteration 45190 Training loss 0.002670773770660162 Validation loss 0.010738571174442768 Accuracy 0.884765625\n",
      "Iteration 45200 Training loss 0.003530242945998907 Validation loss 0.010791592299938202 Accuracy 0.88427734375\n",
      "Iteration 45210 Training loss 0.0038316077552735806 Validation loss 0.01085314154624939 Accuracy 0.88330078125\n",
      "Iteration 45220 Training loss 0.0017229049699380994 Validation loss 0.010705218650400639 Accuracy 0.88427734375\n",
      "Iteration 45230 Training loss 0.003910844214260578 Validation loss 0.010783986188471317 Accuracy 0.88427734375\n",
      "Iteration 45240 Training loss 0.004503238946199417 Validation loss 0.01061108335852623 Accuracy 0.88623046875\n",
      "Iteration 45250 Training loss 0.003059124806895852 Validation loss 0.010742979124188423 Accuracy 0.88427734375\n",
      "Iteration 45260 Training loss 0.002301018452271819 Validation loss 0.010692699812352657 Accuracy 0.884765625\n",
      "Iteration 45270 Training loss 0.0021465958561748266 Validation loss 0.01067445520311594 Accuracy 0.8837890625\n",
      "Iteration 45280 Training loss 0.002993667032569647 Validation loss 0.010560528375208378 Accuracy 0.88671875\n",
      "Iteration 45290 Training loss 0.002916737226769328 Validation loss 0.01057110819965601 Accuracy 0.88623046875\n",
      "Iteration 45300 Training loss 0.003821259131655097 Validation loss 0.010783585719764233 Accuracy 0.88330078125\n",
      "Iteration 45310 Training loss 0.002457896713167429 Validation loss 0.010722039267420769 Accuracy 0.884765625\n",
      "Iteration 45320 Training loss 0.002266732743009925 Validation loss 0.01089642196893692 Accuracy 0.88232421875\n",
      "Iteration 45330 Training loss 0.0033242113422602415 Validation loss 0.010906166397035122 Accuracy 0.88232421875\n",
      "Iteration 45340 Training loss 0.002975670387968421 Validation loss 0.010599552653729916 Accuracy 0.88623046875\n",
      "Iteration 45350 Training loss 0.002264341339468956 Validation loss 0.010766196064651012 Accuracy 0.88427734375\n",
      "Iteration 45360 Training loss 0.0030062971636652946 Validation loss 0.010708678513765335 Accuracy 0.8837890625\n",
      "Iteration 45370 Training loss 0.003424611408263445 Validation loss 0.010667930357158184 Accuracy 0.88525390625\n",
      "Iteration 45380 Training loss 0.003345800330862403 Validation loss 0.010647187009453773 Accuracy 0.88525390625\n",
      "Iteration 45390 Training loss 0.0026587373577058315 Validation loss 0.010679648257791996 Accuracy 0.88330078125\n",
      "Iteration 45400 Training loss 0.0034594624303281307 Validation loss 0.010694297961890697 Accuracy 0.8857421875\n",
      "Iteration 45410 Training loss 0.003319180104881525 Validation loss 0.01060060877352953 Accuracy 0.8857421875\n",
      "Iteration 45420 Training loss 0.00329402438364923 Validation loss 0.010628610849380493 Accuracy 0.8857421875\n",
      "Iteration 45430 Training loss 0.003040400566533208 Validation loss 0.010786280035972595 Accuracy 0.88525390625\n",
      "Iteration 45440 Training loss 0.003648871323093772 Validation loss 0.010644521564245224 Accuracy 0.88623046875\n",
      "Iteration 45450 Training loss 0.0026831740979105234 Validation loss 0.010602721944451332 Accuracy 0.8857421875\n",
      "Iteration 45460 Training loss 0.0025508450344204903 Validation loss 0.010755534283816814 Accuracy 0.88427734375\n",
      "Iteration 45470 Training loss 0.0031792912632226944 Validation loss 0.01081449817866087 Accuracy 0.88427734375\n",
      "Iteration 45480 Training loss 0.0035040511284023523 Validation loss 0.01079173106700182 Accuracy 0.88427734375\n",
      "Iteration 45490 Training loss 0.0024586825165897608 Validation loss 0.010575971566140652 Accuracy 0.88671875\n",
      "Iteration 45500 Training loss 0.002722501754760742 Validation loss 0.010671116411685944 Accuracy 0.88525390625\n",
      "Iteration 45510 Training loss 0.002737971255555749 Validation loss 0.01067936047911644 Accuracy 0.884765625\n",
      "Iteration 45520 Training loss 0.0026229186914861202 Validation loss 0.010706447064876556 Accuracy 0.88427734375\n",
      "Iteration 45530 Training loss 0.003564703045412898 Validation loss 0.010726364329457283 Accuracy 0.884765625\n",
      "Iteration 45540 Training loss 0.0034086285158991814 Validation loss 0.010649469681084156 Accuracy 0.88427734375\n",
      "Iteration 45550 Training loss 0.0026155286468565464 Validation loss 0.010712064802646637 Accuracy 0.8857421875\n",
      "Iteration 45560 Training loss 0.0034470907412469387 Validation loss 0.010632108896970749 Accuracy 0.88623046875\n",
      "Iteration 45570 Training loss 0.004327406641095877 Validation loss 0.01063541229814291 Accuracy 0.8857421875\n",
      "Iteration 45580 Training loss 0.0029190084896981716 Validation loss 0.010932397097349167 Accuracy 0.88330078125\n",
      "Iteration 45590 Training loss 0.002952211769297719 Validation loss 0.010861757211387157 Accuracy 0.8828125\n",
      "Iteration 45600 Training loss 0.0024112078826874495 Validation loss 0.010618966072797775 Accuracy 0.8857421875\n",
      "Iteration 45610 Training loss 0.003372492967173457 Validation loss 0.010660534724593163 Accuracy 0.88671875\n",
      "Iteration 45620 Training loss 0.003841657657176256 Validation loss 0.0106187229976058 Accuracy 0.88623046875\n",
      "Iteration 45630 Training loss 0.003960391506552696 Validation loss 0.010616512969136238 Accuracy 0.8857421875\n",
      "Iteration 45640 Training loss 0.0029963203705847263 Validation loss 0.010677996091544628 Accuracy 0.8857421875\n",
      "Iteration 45650 Training loss 0.002840779023244977 Validation loss 0.010625075548887253 Accuracy 0.88623046875\n",
      "Iteration 45660 Training loss 0.0029201454017311335 Validation loss 0.010648192837834358 Accuracy 0.88623046875\n",
      "Iteration 45670 Training loss 0.0026001017540693283 Validation loss 0.010563777759671211 Accuracy 0.8876953125\n",
      "Iteration 45680 Training loss 0.0031783466693013906 Validation loss 0.010709631256759167 Accuracy 0.884765625\n",
      "Iteration 45690 Training loss 0.0017618306446820498 Validation loss 0.010708294808864594 Accuracy 0.8857421875\n",
      "Iteration 45700 Training loss 0.003973101731389761 Validation loss 0.010741114616394043 Accuracy 0.884765625\n",
      "Iteration 45710 Training loss 0.0036879493854939938 Validation loss 0.010700048878788948 Accuracy 0.884765625\n",
      "Iteration 45720 Training loss 0.002075828844681382 Validation loss 0.01047674659639597 Accuracy 0.8876953125\n",
      "Iteration 45730 Training loss 0.004341174848377705 Validation loss 0.010502099059522152 Accuracy 0.88720703125\n",
      "Iteration 45740 Training loss 0.0026512013282626867 Validation loss 0.010549969971179962 Accuracy 0.8857421875\n",
      "Iteration 45750 Training loss 0.0024588550440967083 Validation loss 0.010497288778424263 Accuracy 0.88671875\n",
      "Iteration 45760 Training loss 0.0018623938085511327 Validation loss 0.010470923967659473 Accuracy 0.88720703125\n",
      "Iteration 45770 Training loss 0.0019345175242051482 Validation loss 0.010656323283910751 Accuracy 0.88427734375\n",
      "Iteration 45780 Training loss 0.003353756619617343 Validation loss 0.010605888441205025 Accuracy 0.88623046875\n",
      "Iteration 45790 Training loss 0.0022538634948432446 Validation loss 0.010692003183066845 Accuracy 0.884765625\n",
      "Iteration 45800 Training loss 0.0025022891350090504 Validation loss 0.01069649402052164 Accuracy 0.884765625\n",
      "Iteration 45810 Training loss 0.0035629933699965477 Validation loss 0.010709199123084545 Accuracy 0.8857421875\n",
      "Iteration 45820 Training loss 0.0030204078648239374 Validation loss 0.010584477335214615 Accuracy 0.88623046875\n",
      "Iteration 45830 Training loss 0.0031884415075182915 Validation loss 0.010548052377998829 Accuracy 0.8857421875\n",
      "Iteration 45840 Training loss 0.004026284907013178 Validation loss 0.01072765700519085 Accuracy 0.88427734375\n",
      "Iteration 45850 Training loss 0.0030915418174117804 Validation loss 0.010641541332006454 Accuracy 0.8857421875\n",
      "Iteration 45860 Training loss 0.0026876700576394796 Validation loss 0.010976763442158699 Accuracy 0.88232421875\n",
      "Iteration 45870 Training loss 0.004040507134050131 Validation loss 0.010662609711289406 Accuracy 0.88525390625\n",
      "Iteration 45880 Training loss 0.002721525263041258 Validation loss 0.01064352411776781 Accuracy 0.8857421875\n",
      "Iteration 45890 Training loss 0.002312122378498316 Validation loss 0.01065151672810316 Accuracy 0.8857421875\n",
      "Iteration 45900 Training loss 0.0032766987569630146 Validation loss 0.010611169040203094 Accuracy 0.8857421875\n",
      "Iteration 45910 Training loss 0.003195713274180889 Validation loss 0.01057208701968193 Accuracy 0.88623046875\n",
      "Iteration 45920 Training loss 0.003126103663817048 Validation loss 0.010563737712800503 Accuracy 0.88671875\n",
      "Iteration 45930 Training loss 0.0032471735030412674 Validation loss 0.010646024718880653 Accuracy 0.88525390625\n",
      "Iteration 45940 Training loss 0.002657700562849641 Validation loss 0.010445191524922848 Accuracy 0.88623046875\n",
      "Iteration 45950 Training loss 0.0033345979172736406 Validation loss 0.010501083917915821 Accuracy 0.8876953125\n",
      "Iteration 45960 Training loss 0.003528304398059845 Validation loss 0.01062462106347084 Accuracy 0.8857421875\n",
      "Iteration 45970 Training loss 0.002732990775257349 Validation loss 0.01065275352448225 Accuracy 0.884765625\n",
      "Iteration 45980 Training loss 0.0033653220161795616 Validation loss 0.01066880114376545 Accuracy 0.88623046875\n",
      "Iteration 45990 Training loss 0.0027103631291538477 Validation loss 0.010662633925676346 Accuracy 0.88623046875\n",
      "Iteration 46000 Training loss 0.003488316433504224 Validation loss 0.010732104070484638 Accuracy 0.884765625\n",
      "Iteration 46010 Training loss 0.002805037423968315 Validation loss 0.010761374607682228 Accuracy 0.8837890625\n",
      "Iteration 46020 Training loss 0.0032350250985473394 Validation loss 0.010639560408890247 Accuracy 0.884765625\n",
      "Iteration 46030 Training loss 0.0035462912637740374 Validation loss 0.01061417069286108 Accuracy 0.88623046875\n",
      "Iteration 46040 Training loss 0.0036669180262833834 Validation loss 0.010603569447994232 Accuracy 0.8857421875\n",
      "Iteration 46050 Training loss 0.002487247809767723 Validation loss 0.010940623469650745 Accuracy 0.88330078125\n",
      "Iteration 46060 Training loss 0.0026321744080632925 Validation loss 0.010749051347374916 Accuracy 0.88525390625\n",
      "Iteration 46070 Training loss 0.0026678936555981636 Validation loss 0.010593929328024387 Accuracy 0.8857421875\n",
      "Iteration 46080 Training loss 0.0033664193470031023 Validation loss 0.010450365953147411 Accuracy 0.88720703125\n",
      "Iteration 46090 Training loss 0.0022549189161509275 Validation loss 0.01055973581969738 Accuracy 0.88623046875\n",
      "Iteration 46100 Training loss 0.0036452794447541237 Validation loss 0.010502348653972149 Accuracy 0.88720703125\n",
      "Iteration 46110 Training loss 0.002260893117636442 Validation loss 0.010546398349106312 Accuracy 0.88720703125\n",
      "Iteration 46120 Training loss 0.0018954552942886949 Validation loss 0.010546904057264328 Accuracy 0.88671875\n",
      "Iteration 46130 Training loss 0.0026301604229956865 Validation loss 0.010535228997468948 Accuracy 0.88623046875\n",
      "Iteration 46140 Training loss 0.002992387395352125 Validation loss 0.011001543141901493 Accuracy 0.88134765625\n",
      "Iteration 46150 Training loss 0.004831735510379076 Validation loss 0.010808691382408142 Accuracy 0.8837890625\n",
      "Iteration 46160 Training loss 0.00388797908090055 Validation loss 0.010725655592978 Accuracy 0.8837890625\n",
      "Iteration 46170 Training loss 0.00253412127494812 Validation loss 0.010698198340833187 Accuracy 0.88427734375\n",
      "Iteration 46180 Training loss 0.003550122957676649 Validation loss 0.010873730294406414 Accuracy 0.88232421875\n",
      "Iteration 46190 Training loss 0.003514606738463044 Validation loss 0.010757924988865852 Accuracy 0.88427734375\n",
      "Iteration 46200 Training loss 0.0038051337469369173 Validation loss 0.010595371015369892 Accuracy 0.88671875\n",
      "Iteration 46210 Training loss 0.003125250805169344 Validation loss 0.010796916671097279 Accuracy 0.88427734375\n",
      "Iteration 46220 Training loss 0.0023483692202717066 Validation loss 0.010684069246053696 Accuracy 0.88427734375\n",
      "Iteration 46230 Training loss 0.0023584100417792797 Validation loss 0.010641832835972309 Accuracy 0.88525390625\n",
      "Iteration 46240 Training loss 0.003966503776609898 Validation loss 0.010748139582574368 Accuracy 0.884765625\n",
      "Iteration 46250 Training loss 0.0032735855784267187 Validation loss 0.010559627786278725 Accuracy 0.88671875\n",
      "Iteration 46260 Training loss 0.002494300249963999 Validation loss 0.010510588064789772 Accuracy 0.88671875\n",
      "Iteration 46270 Training loss 0.0025604753755033016 Validation loss 0.010545508936047554 Accuracy 0.88671875\n",
      "Iteration 46280 Training loss 0.0029629208147525787 Validation loss 0.010622845962643623 Accuracy 0.88623046875\n",
      "Iteration 46290 Training loss 0.0034265187568962574 Validation loss 0.010725614614784718 Accuracy 0.884765625\n",
      "Iteration 46300 Training loss 0.0034205468837171793 Validation loss 0.01069931872189045 Accuracy 0.88427734375\n",
      "Iteration 46310 Training loss 0.003999061416834593 Validation loss 0.010741411708295345 Accuracy 0.88330078125\n",
      "Iteration 46320 Training loss 0.0022876497823745012 Validation loss 0.010516483336687088 Accuracy 0.88720703125\n",
      "Iteration 46330 Training loss 0.0035074881743639708 Validation loss 0.010739143006503582 Accuracy 0.8837890625\n",
      "Iteration 46340 Training loss 0.0032133820932358503 Validation loss 0.010618571192026138 Accuracy 0.8857421875\n",
      "Iteration 46350 Training loss 0.0028420593589544296 Validation loss 0.010545242577791214 Accuracy 0.88671875\n",
      "Iteration 46360 Training loss 0.002946866676211357 Validation loss 0.01053601410239935 Accuracy 0.88671875\n",
      "Iteration 46370 Training loss 0.002447958802804351 Validation loss 0.010616001673042774 Accuracy 0.88525390625\n",
      "Iteration 46380 Training loss 0.003235338255763054 Validation loss 0.01075062993913889 Accuracy 0.8837890625\n",
      "Iteration 46390 Training loss 0.0032484473194926977 Validation loss 0.010660853236913681 Accuracy 0.884765625\n",
      "Iteration 46400 Training loss 0.003626585006713867 Validation loss 0.010681159794330597 Accuracy 0.8857421875\n",
      "Iteration 46410 Training loss 0.002501749899238348 Validation loss 0.010660684667527676 Accuracy 0.88525390625\n",
      "Iteration 46420 Training loss 0.002497608307749033 Validation loss 0.010574137791991234 Accuracy 0.8857421875\n",
      "Iteration 46430 Training loss 0.0024536065757274628 Validation loss 0.010541762225329876 Accuracy 0.8876953125\n",
      "Iteration 46440 Training loss 0.004521807190030813 Validation loss 0.010699589736759663 Accuracy 0.88427734375\n",
      "Iteration 46450 Training loss 0.003609316423535347 Validation loss 0.010648617520928383 Accuracy 0.8857421875\n",
      "Iteration 46460 Training loss 0.0028879616875201464 Validation loss 0.010578102432191372 Accuracy 0.88671875\n",
      "Iteration 46470 Training loss 0.0021162747871130705 Validation loss 0.010565186850726604 Accuracy 0.88623046875\n",
      "Iteration 46480 Training loss 0.002656711498275399 Validation loss 0.010519037023186684 Accuracy 0.88720703125\n",
      "Iteration 46490 Training loss 0.0029971497133374214 Validation loss 0.010586102493107319 Accuracy 0.88623046875\n",
      "Iteration 46500 Training loss 0.003131358651444316 Validation loss 0.010579507797956467 Accuracy 0.88671875\n",
      "Iteration 46510 Training loss 0.0042274403385818005 Validation loss 0.010714779607951641 Accuracy 0.8857421875\n",
      "Iteration 46520 Training loss 0.003472070675343275 Validation loss 0.010704835876822472 Accuracy 0.8857421875\n",
      "Iteration 46530 Training loss 0.003802467370405793 Validation loss 0.01059689186513424 Accuracy 0.88623046875\n",
      "Iteration 46540 Training loss 0.002613379154354334 Validation loss 0.010789713822305202 Accuracy 0.88330078125\n",
      "Iteration 46550 Training loss 0.003062991425395012 Validation loss 0.0105734933167696 Accuracy 0.88623046875\n",
      "Iteration 46560 Training loss 0.003757453989237547 Validation loss 0.010542758740484715 Accuracy 0.88720703125\n",
      "Iteration 46570 Training loss 0.0032337941229343414 Validation loss 0.010488582774996758 Accuracy 0.88720703125\n",
      "Iteration 46580 Training loss 0.0018217687029391527 Validation loss 0.010810773819684982 Accuracy 0.8837890625\n",
      "Iteration 46590 Training loss 0.0035075212363153696 Validation loss 0.010631545446813107 Accuracy 0.8857421875\n",
      "Iteration 46600 Training loss 0.0024983463808894157 Validation loss 0.010663656517863274 Accuracy 0.884765625\n",
      "Iteration 46610 Training loss 0.002731866203248501 Validation loss 0.010554623790085316 Accuracy 0.88671875\n",
      "Iteration 46620 Training loss 0.003799266181886196 Validation loss 0.010565795935690403 Accuracy 0.8876953125\n",
      "Iteration 46630 Training loss 0.0024658117908984423 Validation loss 0.010551664978265762 Accuracy 0.88671875\n",
      "Iteration 46640 Training loss 0.002894399920478463 Validation loss 0.010612624697387218 Accuracy 0.88525390625\n",
      "Iteration 46650 Training loss 0.0013337335549294949 Validation loss 0.010593121871352196 Accuracy 0.8857421875\n",
      "Iteration 46660 Training loss 0.002382742241024971 Validation loss 0.010423600673675537 Accuracy 0.8876953125\n",
      "Iteration 46670 Training loss 0.0038594261277467012 Validation loss 0.010688337497413158 Accuracy 0.884765625\n",
      "Iteration 46680 Training loss 0.002317443024367094 Validation loss 0.01067106518894434 Accuracy 0.88427734375\n",
      "Iteration 46690 Training loss 0.003882434917613864 Validation loss 0.010683317668735981 Accuracy 0.8837890625\n",
      "Iteration 46700 Training loss 0.0043048863299191 Validation loss 0.010675163008272648 Accuracy 0.88525390625\n",
      "Iteration 46710 Training loss 0.0031312720384448767 Validation loss 0.010522919707000256 Accuracy 0.88671875\n",
      "Iteration 46720 Training loss 0.002996098715811968 Validation loss 0.010841473937034607 Accuracy 0.8828125\n",
      "Iteration 46730 Training loss 0.0028756256215274334 Validation loss 0.010534035041928291 Accuracy 0.8857421875\n",
      "Iteration 46740 Training loss 0.0036767113488167524 Validation loss 0.010586259886622429 Accuracy 0.88623046875\n",
      "Iteration 46750 Training loss 0.002315480262041092 Validation loss 0.010580104775726795 Accuracy 0.88720703125\n",
      "Iteration 46760 Training loss 0.0027745275292545557 Validation loss 0.010619941167533398 Accuracy 0.88623046875\n",
      "Iteration 46770 Training loss 0.0035703766625374556 Validation loss 0.010669920593500137 Accuracy 0.8857421875\n",
      "Iteration 46780 Training loss 0.0037643788382411003 Validation loss 0.010603436268866062 Accuracy 0.8857421875\n",
      "Iteration 46790 Training loss 0.002846835181117058 Validation loss 0.01080983504652977 Accuracy 0.88427734375\n",
      "Iteration 46800 Training loss 0.002615779871121049 Validation loss 0.010661994107067585 Accuracy 0.884765625\n",
      "Iteration 46810 Training loss 0.0029149604961276054 Validation loss 0.010542390868067741 Accuracy 0.8876953125\n",
      "Iteration 46820 Training loss 0.0026804995723068714 Validation loss 0.010503539815545082 Accuracy 0.88720703125\n",
      "Iteration 46830 Training loss 0.0026088058948516846 Validation loss 0.010755038820207119 Accuracy 0.8837890625\n",
      "Iteration 46840 Training loss 0.003157292725518346 Validation loss 0.010575531050562859 Accuracy 0.88623046875\n",
      "Iteration 46850 Training loss 0.0028719825204461813 Validation loss 0.010589104145765305 Accuracy 0.8857421875\n",
      "Iteration 46860 Training loss 0.002878683153539896 Validation loss 0.010614853352308273 Accuracy 0.8857421875\n",
      "Iteration 46870 Training loss 0.0020832130685448647 Validation loss 0.010475552640855312 Accuracy 0.88623046875\n",
      "Iteration 46880 Training loss 0.0036197120789438486 Validation loss 0.01058349572122097 Accuracy 0.8857421875\n",
      "Iteration 46890 Training loss 0.0013768603093922138 Validation loss 0.0106499707326293 Accuracy 0.88623046875\n",
      "Iteration 46900 Training loss 0.002905289875343442 Validation loss 0.010617121122777462 Accuracy 0.88623046875\n",
      "Iteration 46910 Training loss 0.0028535390738397837 Validation loss 0.010551905259490013 Accuracy 0.8857421875\n",
      "Iteration 46920 Training loss 0.003590567270293832 Validation loss 0.010659173130989075 Accuracy 0.884765625\n",
      "Iteration 46930 Training loss 0.0027361796237528324 Validation loss 0.010650619864463806 Accuracy 0.88671875\n",
      "Iteration 46940 Training loss 0.003247643820941448 Validation loss 0.010681340470910072 Accuracy 0.8857421875\n",
      "Iteration 46950 Training loss 0.002955281175673008 Validation loss 0.01042894460260868 Accuracy 0.888671875\n",
      "Iteration 46960 Training loss 0.002515546977519989 Validation loss 0.010682700201869011 Accuracy 0.884765625\n",
      "Iteration 46970 Training loss 0.001831109868362546 Validation loss 0.010723095387220383 Accuracy 0.884765625\n",
      "Iteration 46980 Training loss 0.0017731329426169395 Validation loss 0.010595527477562428 Accuracy 0.88623046875\n",
      "Iteration 46990 Training loss 0.004488333128392696 Validation loss 0.010590575635433197 Accuracy 0.88671875\n",
      "Iteration 47000 Training loss 0.00236231810413301 Validation loss 0.01051232311874628 Accuracy 0.88720703125\n",
      "Iteration 47010 Training loss 0.003586034057661891 Validation loss 0.010589852929115295 Accuracy 0.88671875\n",
      "Iteration 47020 Training loss 0.0021386388689279556 Validation loss 0.010585525073111057 Accuracy 0.88623046875\n",
      "Iteration 47030 Training loss 0.0034250481985509396 Validation loss 0.010755037888884544 Accuracy 0.88427734375\n",
      "Iteration 47040 Training loss 0.0013707205653190613 Validation loss 0.010588874109089375 Accuracy 0.88623046875\n",
      "Iteration 47050 Training loss 0.0015000645071268082 Validation loss 0.010700041428208351 Accuracy 0.884765625\n",
      "Iteration 47060 Training loss 0.0023743135388940573 Validation loss 0.010746713727712631 Accuracy 0.88427734375\n",
      "Iteration 47070 Training loss 0.003798243124037981 Validation loss 0.010601615533232689 Accuracy 0.88623046875\n",
      "Iteration 47080 Training loss 0.0027732616290450096 Validation loss 0.01054719090461731 Accuracy 0.88671875\n",
      "Iteration 47090 Training loss 0.0027381298132240772 Validation loss 0.010531044565141201 Accuracy 0.88671875\n",
      "Iteration 47100 Training loss 0.004075719974935055 Validation loss 0.010498714633286 Accuracy 0.88671875\n",
      "Iteration 47110 Training loss 0.003529016859829426 Validation loss 0.010502568446099758 Accuracy 0.88671875\n",
      "Iteration 47120 Training loss 0.003248011227697134 Validation loss 0.010609200224280357 Accuracy 0.8857421875\n",
      "Iteration 47130 Training loss 0.003752522636204958 Validation loss 0.010593860410153866 Accuracy 0.8857421875\n",
      "Iteration 47140 Training loss 0.0018243941012769938 Validation loss 0.010632709600031376 Accuracy 0.88525390625\n",
      "Iteration 47150 Training loss 0.0036640542093664408 Validation loss 0.010595622472465038 Accuracy 0.8857421875\n",
      "Iteration 47160 Training loss 0.003060293849557638 Validation loss 0.010888908058404922 Accuracy 0.8818359375\n",
      "Iteration 47170 Training loss 0.0031003712210804224 Validation loss 0.010546342469751835 Accuracy 0.88623046875\n",
      "Iteration 47180 Training loss 0.003332492895424366 Validation loss 0.010677228681743145 Accuracy 0.884765625\n",
      "Iteration 47190 Training loss 0.0017948998138308525 Validation loss 0.010610253550112247 Accuracy 0.884765625\n",
      "Iteration 47200 Training loss 0.004502652212977409 Validation loss 0.01062515377998352 Accuracy 0.8857421875\n",
      "Iteration 47210 Training loss 0.0024888371117413044 Validation loss 0.01060250960290432 Accuracy 0.8857421875\n",
      "Iteration 47220 Training loss 0.0029907021671533585 Validation loss 0.010545581579208374 Accuracy 0.88671875\n",
      "Iteration 47230 Training loss 0.0023055949714034796 Validation loss 0.010658207349479198 Accuracy 0.88525390625\n",
      "Iteration 47240 Training loss 0.003221237799152732 Validation loss 0.010485460981726646 Accuracy 0.88720703125\n",
      "Iteration 47250 Training loss 0.0032185057643800974 Validation loss 0.01064950879663229 Accuracy 0.8857421875\n",
      "Iteration 47260 Training loss 0.002472875639796257 Validation loss 0.010611738078296185 Accuracy 0.8857421875\n",
      "Iteration 47270 Training loss 0.002640959108248353 Validation loss 0.010513263754546642 Accuracy 0.88671875\n",
      "Iteration 47280 Training loss 0.0029321624897420406 Validation loss 0.010711041279137135 Accuracy 0.884765625\n",
      "Iteration 47290 Training loss 0.0033390820026397705 Validation loss 0.010569443926215172 Accuracy 0.88525390625\n",
      "Iteration 47300 Training loss 0.0031094455625861883 Validation loss 0.010713593102991581 Accuracy 0.88427734375\n",
      "Iteration 47310 Training loss 0.0033776997588574886 Validation loss 0.010710449889302254 Accuracy 0.8857421875\n",
      "Iteration 47320 Training loss 0.003083058400079608 Validation loss 0.010452842339873314 Accuracy 0.8876953125\n",
      "Iteration 47330 Training loss 0.002769644372165203 Validation loss 0.01048262882977724 Accuracy 0.8876953125\n",
      "Iteration 47340 Training loss 0.0022486152593046427 Validation loss 0.010519656352698803 Accuracy 0.88671875\n",
      "Iteration 47350 Training loss 0.0017588261980563402 Validation loss 0.010540680028498173 Accuracy 0.88671875\n",
      "Iteration 47360 Training loss 0.0038271681405603886 Validation loss 0.010619438253343105 Accuracy 0.88623046875\n",
      "Iteration 47370 Training loss 0.002448985818773508 Validation loss 0.010497893206775188 Accuracy 0.88720703125\n",
      "Iteration 47380 Training loss 0.0038184309378266335 Validation loss 0.010568750090897083 Accuracy 0.88720703125\n",
      "Iteration 47390 Training loss 0.002456821035593748 Validation loss 0.010746294632554054 Accuracy 0.88427734375\n",
      "Iteration 47400 Training loss 0.0025982793886214495 Validation loss 0.010677553713321686 Accuracy 0.8857421875\n",
      "Iteration 47410 Training loss 0.0026207375340163708 Validation loss 0.010474154725670815 Accuracy 0.88671875\n",
      "Iteration 47420 Training loss 0.0028007959481328726 Validation loss 0.010634618811309338 Accuracy 0.8857421875\n",
      "Iteration 47430 Training loss 0.0025160349905490875 Validation loss 0.010556429624557495 Accuracy 0.88623046875\n",
      "Iteration 47440 Training loss 0.002462879056110978 Validation loss 0.010707961395382881 Accuracy 0.884765625\n",
      "Iteration 47450 Training loss 0.0015315004857257009 Validation loss 0.010594573803246021 Accuracy 0.88525390625\n",
      "Iteration 47460 Training loss 0.00270366040058434 Validation loss 0.010603837668895721 Accuracy 0.88525390625\n",
      "Iteration 47470 Training loss 0.0030940035358071327 Validation loss 0.010722550563514233 Accuracy 0.88427734375\n",
      "Iteration 47480 Training loss 0.002140115248039365 Validation loss 0.010616361163556576 Accuracy 0.88720703125\n",
      "Iteration 47490 Training loss 0.003799125086516142 Validation loss 0.010561731643974781 Accuracy 0.88671875\n",
      "Iteration 47500 Training loss 0.0028730311896651983 Validation loss 0.010595292784273624 Accuracy 0.88623046875\n",
      "Iteration 47510 Training loss 0.002498872112482786 Validation loss 0.010573217645287514 Accuracy 0.88525390625\n",
      "Iteration 47520 Training loss 0.0020866079721599817 Validation loss 0.010517743416130543 Accuracy 0.88623046875\n",
      "Iteration 47530 Training loss 0.0020903090480715036 Validation loss 0.01045914739370346 Accuracy 0.88671875\n",
      "Iteration 47540 Training loss 0.0017710143001750112 Validation loss 0.01078063901513815 Accuracy 0.8837890625\n",
      "Iteration 47550 Training loss 0.0036219602916389704 Validation loss 0.010706649161875248 Accuracy 0.88427734375\n",
      "Iteration 47560 Training loss 0.0032976798247545958 Validation loss 0.010572842322289944 Accuracy 0.8857421875\n",
      "Iteration 47570 Training loss 0.003107996191829443 Validation loss 0.010650807060301304 Accuracy 0.884765625\n",
      "Iteration 47580 Training loss 0.0014343161601573229 Validation loss 0.010645254515111446 Accuracy 0.88671875\n",
      "Iteration 47590 Training loss 0.0031416791025549173 Validation loss 0.010672138072550297 Accuracy 0.8857421875\n",
      "Iteration 47600 Training loss 0.002444647252559662 Validation loss 0.01054073590785265 Accuracy 0.8876953125\n",
      "Iteration 47610 Training loss 0.0022924768272787333 Validation loss 0.010507592931389809 Accuracy 0.88818359375\n",
      "Iteration 47620 Training loss 0.003298684488981962 Validation loss 0.010686286725103855 Accuracy 0.88427734375\n",
      "Iteration 47630 Training loss 0.0024112656246870756 Validation loss 0.010650224052369595 Accuracy 0.884765625\n",
      "Iteration 47640 Training loss 0.0028595509938895702 Validation loss 0.010594514198601246 Accuracy 0.88671875\n",
      "Iteration 47650 Training loss 0.001999139552935958 Validation loss 0.010557188652455807 Accuracy 0.88720703125\n",
      "Iteration 47660 Training loss 0.003270356683060527 Validation loss 0.01060470100492239 Accuracy 0.88623046875\n",
      "Iteration 47670 Training loss 0.0035857383627444506 Validation loss 0.010805838741362095 Accuracy 0.8837890625\n",
      "Iteration 47680 Training loss 0.0020721061155200005 Validation loss 0.010614927858114243 Accuracy 0.88623046875\n",
      "Iteration 47690 Training loss 0.0036363492254167795 Validation loss 0.010633856989443302 Accuracy 0.88623046875\n",
      "Iteration 47700 Training loss 0.0035347016528248787 Validation loss 0.010666518472135067 Accuracy 0.884765625\n",
      "Iteration 47710 Training loss 0.002303775865584612 Validation loss 0.01067446917295456 Accuracy 0.884765625\n",
      "Iteration 47720 Training loss 0.0035427003167569637 Validation loss 0.010573158040642738 Accuracy 0.88623046875\n",
      "Iteration 47730 Training loss 0.003908445592969656 Validation loss 0.010683475993573666 Accuracy 0.8857421875\n",
      "Iteration 47740 Training loss 0.00277478015050292 Validation loss 0.010782185010612011 Accuracy 0.8828125\n",
      "Iteration 47750 Training loss 0.004121854901313782 Validation loss 0.010564633645117283 Accuracy 0.88623046875\n",
      "Iteration 47760 Training loss 0.0026317767333239317 Validation loss 0.010544904507696629 Accuracy 0.88720703125\n",
      "Iteration 47770 Training loss 0.002240363974124193 Validation loss 0.010538166388869286 Accuracy 0.88623046875\n",
      "Iteration 47780 Training loss 0.0032219234853982925 Validation loss 0.010631591081619263 Accuracy 0.88427734375\n",
      "Iteration 47790 Training loss 0.0037711579352617264 Validation loss 0.01063007302582264 Accuracy 0.88525390625\n",
      "Iteration 47800 Training loss 0.003347602905705571 Validation loss 0.010598713532090187 Accuracy 0.88623046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1076cb410>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47810 Training loss 0.002809042576700449 Validation loss 0.010421793907880783 Accuracy 0.88818359375\n",
      "Iteration 47820 Training loss 0.00301810703240335 Validation loss 0.010476169176399708 Accuracy 0.8876953125\n",
      "Iteration 47830 Training loss 0.002794896485283971 Validation loss 0.010643944144248962 Accuracy 0.8857421875\n",
      "Iteration 47840 Training loss 0.0017396865878254175 Validation loss 0.010600919835269451 Accuracy 0.8857421875\n",
      "Iteration 47850 Training loss 0.0034045397769659758 Validation loss 0.01056494377553463 Accuracy 0.88623046875\n",
      "Iteration 47860 Training loss 0.002637468511238694 Validation loss 0.010590159334242344 Accuracy 0.8857421875\n",
      "Iteration 47870 Training loss 0.002878663595765829 Validation loss 0.010597501881420612 Accuracy 0.88525390625\n",
      "Iteration 47880 Training loss 0.0023843084927648306 Validation loss 0.01059157308191061 Accuracy 0.88623046875\n",
      "Iteration 47890 Training loss 0.0028588958084583282 Validation loss 0.010633093304932117 Accuracy 0.88525390625\n",
      "Iteration 47900 Training loss 0.0021616281010210514 Validation loss 0.010568547993898392 Accuracy 0.88671875\n",
      "Iteration 47910 Training loss 0.002831535879522562 Validation loss 0.010472903959453106 Accuracy 0.88671875\n",
      "Iteration 47920 Training loss 0.0021815267391502857 Validation loss 0.010675854980945587 Accuracy 0.8857421875\n",
      "Iteration 47930 Training loss 0.0013649514876306057 Validation loss 0.010603578761219978 Accuracy 0.88671875\n",
      "Iteration 47940 Training loss 0.002471331274136901 Validation loss 0.010615314356982708 Accuracy 0.88623046875\n",
      "Iteration 47950 Training loss 0.0028619030490517616 Validation loss 0.01047096773982048 Accuracy 0.88818359375\n",
      "Iteration 47960 Training loss 0.0030491482466459274 Validation loss 0.01056408416479826 Accuracy 0.88671875\n",
      "Iteration 47970 Training loss 0.004335252568125725 Validation loss 0.010694711469113827 Accuracy 0.88623046875\n",
      "Iteration 47980 Training loss 0.0032203339505940676 Validation loss 0.010610492900013924 Accuracy 0.88623046875\n",
      "Iteration 47990 Training loss 0.002455121371895075 Validation loss 0.010585837066173553 Accuracy 0.8876953125\n",
      "Iteration 48000 Training loss 0.00315055251121521 Validation loss 0.010545789264142513 Accuracy 0.88720703125\n",
      "Iteration 48010 Training loss 0.002479559276252985 Validation loss 0.010605673305690289 Accuracy 0.8857421875\n",
      "Iteration 48020 Training loss 0.0035923710092902184 Validation loss 0.010660293512046337 Accuracy 0.88525390625\n",
      "Iteration 48030 Training loss 0.0028647335711866617 Validation loss 0.010672379285097122 Accuracy 0.8857421875\n",
      "Iteration 48040 Training loss 0.0032703338656574488 Validation loss 0.010683786123991013 Accuracy 0.88525390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1076cb410>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hugovidal/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48050 Training loss 0.002422309946268797 Validation loss 0.010584050789475441 Accuracy 0.88671875\n",
      "Iteration 48060 Training loss 0.0025670321192592382 Validation loss 0.010744591243565083 Accuracy 0.884765625\n",
      "Iteration 48070 Training loss 0.0024934944231063128 Validation loss 0.010549862869083881 Accuracy 0.88818359375\n",
      "Iteration 48080 Training loss 0.004410143010318279 Validation loss 0.010523930191993713 Accuracy 0.88671875\n",
      "Iteration 48090 Training loss 0.0028228263836354017 Validation loss 0.010511358268558979 Accuracy 0.88720703125\n",
      "Iteration 48100 Training loss 0.0037939916364848614 Validation loss 0.010562456212937832 Accuracy 0.88623046875\n",
      "Iteration 48110 Training loss 0.0027133130934089422 Validation loss 0.01059702504426241 Accuracy 0.88623046875\n",
      "Iteration 48120 Training loss 0.002325055655092001 Validation loss 0.010800277814269066 Accuracy 0.88330078125\n",
      "Iteration 48130 Training loss 0.002635604003444314 Validation loss 0.010525689460337162 Accuracy 0.88623046875\n",
      "Iteration 48140 Training loss 0.0025779304560273886 Validation loss 0.010506141930818558 Accuracy 0.88720703125\n",
      "Iteration 48150 Training loss 0.001963868038728833 Validation loss 0.010509156621992588 Accuracy 0.88671875\n",
      "Iteration 48160 Training loss 0.0024368518497794867 Validation loss 0.0105843311175704 Accuracy 0.88623046875\n",
      "Iteration 48170 Training loss 0.003071711864322424 Validation loss 0.010729309171438217 Accuracy 0.88525390625\n",
      "Iteration 48180 Training loss 0.004528329707682133 Validation loss 0.010874398984014988 Accuracy 0.8837890625\n",
      "Iteration 48190 Training loss 0.001718376181088388 Validation loss 0.010500144213438034 Accuracy 0.88818359375\n",
      "Iteration 48200 Training loss 0.0025741676799952984 Validation loss 0.010527810081839561 Accuracy 0.88671875\n",
      "Iteration 48210 Training loss 0.0018373536877334118 Validation loss 0.010510922409594059 Accuracy 0.88818359375\n",
      "Iteration 48220 Training loss 0.002974704373627901 Validation loss 0.01090067345649004 Accuracy 0.88330078125\n",
      "Iteration 48230 Training loss 0.002166483551263809 Validation loss 0.010674023069441319 Accuracy 0.88720703125\n",
      "Iteration 48240 Training loss 0.003933171276003122 Validation loss 0.010626537725329399 Accuracy 0.8857421875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_3_trained_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 258\u001b[39m, in \u001b[36mthree_layer_NN.train_layers\u001b[39m\u001b[34m(self, x_train, y_train, x_valid, y_valid, kappa, lr, reg1, reg2, reg3, eps_init, fraction_batch, observation_rate, train_layer_1, train_layer_2, train_layer_3)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\u001b[39;00m\n\u001b[32m    255\u001b[39m \n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# Calcul des gradients\u001b[39;00m\n\u001b[32m    257\u001b[39m grad_output = (output - y_minibatch).to(dtype)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m grad_z3 = (torch.einsum(\u001b[33m'\u001b[39m\u001b[33mno,noz->nz\u001b[39m\u001b[33m'\u001b[39m,grad_output,\u001b[43msoftmax_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m)).to(dtype) \u001b[38;5;66;03m# shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\u001b[39;00m\n\u001b[32m    259\u001b[39m grad_h2 = (torch.mm(grad_z3, \u001b[38;5;28mself\u001b[39m.W3)).to(dtype) \u001b[38;5;66;03m# shape (n_data, hidden_2_size)\u001b[39;00m\n\u001b[32m    260\u001b[39m grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) \u001b[38;5;66;03m# shape(n_data, hidden_2_size)         \u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36msoftmax_derivative\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m     22\u001b[39m jacobians = torch.zeros(n, C, C, dtype=s.dtype).to(device) \u001b[38;5;66;03m# Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = drive de softmax au logit j du ime batch par rapport au logit k du mme batch\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):  \u001b[38;5;66;03m# Pour chaque chantillon du batch, on calcule la jacobienne de softmax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     si = \u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape (C, 1) # vecteur softmax(logits) pour la i-me donne du batch\u001b[39;00m\n\u001b[32m     25\u001b[39m     jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) \u001b[38;5;66;03m# calcul de la jacobienne (matrice des drives croises) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jacobians\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_3_trained_layer.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44572e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained = three_layer_NN(784, 2048, 2048, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553f1ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_3_layer_1_untrained' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_3_layer_1_untrained\u001b[49m.train_layers(x_train, y_train, x_valid, y_valid, \u001b[32m2\u001b[39m, \u001b[32m1e-3\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m0.01\u001b[39m, \u001b[32m10\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_3_layer_1_untrained' is not defined"
     ]
    }
   ],
   "source": [
    "model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-4, 0, 0, 0, 1, 0.01, 10, True, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a65f0eff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'binary_model_2_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m date = \u001b[33m\"\u001b[39m\u001b[33m01_05_25_\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save :\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     model = \u001b[43mbinary_model_2_layer\u001b[49m\n\u001b[32m      6\u001b[39m     model_name = \u001b[33m\"\u001b[39m\u001b[33mCIFAR10_model_(3072+2048+1)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m     save_path = \u001b[33m\"\u001b[39m\u001b[33mClassifiers/\u001b[39m\u001b[33m\"\u001b[39m + date + model_name + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'binary_model_2_layer' is not defined"
     ]
    }
   ],
   "source": [
    "save = True\n",
    "date = \"01_05_25_\"\n",
    "if save :\n",
    "    # Save the model\n",
    "    model = binary_model_2_layer\n",
    "    model_name = \"CIFAR10_model_(3072+2048+1)\"\n",
    "    save_path = \"Classifiers/\" + date + model_name + \"/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model , save_path + model_name + \".pt\")\n",
    "\n",
    "\n",
    "    # Save Architecture\n",
    "    with open(save_path + \"architecture.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model.architecture + str(model.training_time))\n",
    "\n",
    "    # Save performances of the model\n",
    "    os.makedirs(save_path + \"figures/\", exist_ok=True) \n",
    "    # Plot accuracy = f(n)\n",
    "    plt.plot(np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, len(model.accuracy_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory)) \n",
    "    np.savetxt(save_path +\"figures/accuracy_of_\" + model_name + \".txt\", data, delimiter =\",\", header=\"n,accuracy\")\n",
    "    plt.show() \n",
    "    \n",
    "    # Plot training and validation loss = f(n)\n",
    "    plt.plot(np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(np.linspace(0,len(model.validation_loss_trajectory)*model.observation_rate, len(model.validation_loss_trajectory)), model.validation_loss_trajectory, label=\"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, len(model.training_loss_trajectory)*model.observation_rate)\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/loss_training_\" + model_name + \".txt\", data, delimiter=\",\", header=\"n, training_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot accuracy = f(kappa)\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.accuracy_trajectory)*model.observation_rate+1, len(model.accuracy_trajectory))]\n",
    "    plt.plot(kappa, model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((kappa, model.accuracy_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_accuracy_\" + model_name + \".txt\", data, delimiter=\",\", header=\"kappa, accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory))]\n",
    "    plt.plot(kappa, model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(kappa, model.validation_loss_trajectory, label = \"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data_training = np.column_stack((kappa, model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_training_\" + model_name + \".txt\", data_training, delimiter=\",\", header=\"kappa, training_loss\")\n",
    "    data_validation = np.column_stack((kappa, model.validation_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_validation_\" + model_name + \".txt\", data_validation, delimiter=\",\", header=\"kappa, validation_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"datas\\models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "tensor(0.8692)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import model and plot performances\n",
    "\n",
    "model_name = \"model_2_layer_trained_save_1\"\n",
    "assessed_model = torch.load(\"datas/models/model_2_layer_save_1.pt\", weights_only=False)\n",
    "\n",
    "# Details of the model\n",
    "print(assessed_model.architecture)\n",
    "\n",
    "# Plots of performances\n",
    "accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(accuracy)\n",
    "kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_accuracy)\n",
    "loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(loss)\n",
    "kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_loss)\n",
    "plt.show()\n",
    "\n",
    "# Import datas\n",
    "accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8346c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).to(dtype)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), dj softmax\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque chantillon\n",
    "    \"\"\"\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = drive de softmax au logit j du ime batch par rapport au logit k du mme batch\n",
    "    for i in range(n):  # Pour chaque chantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-me donne du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des drives croises) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "\n",
    "\n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, esp_init = 1, fraction_batch=0.01):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = esp_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = esp_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # # hidden neurons layer 2\n",
    "        z3 = torch.mm(self.W3, h2.t() + self.b3)\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajust en fonction du seuil kappa_eff d'apprentissage des donnes.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, coef_iter = 1, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=0.2, fraction_batch=1e-2, train_layer1=True, train_layer2=True, train_layer3=True):\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        kappa_max = 1 + coef_iter\n",
    "        max_iter = self.input_dimension**(kappa_max)\n",
    "        print(\"max_iter\", max_iter)\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_minibatches = int(max_iter / self.fraction_batch) # Nombre de minibatches utiliss pour l'apprentissage de la premire couche\n",
    "        for i in range(N_minibatches):\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            # Calcul de la prdiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            # Suivi de l'apprentissage\n",
    "            if i % 100 == 0:\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2, dim=0) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                overall_training_loss = torch.mean(training_loss,dim=0)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2, dim=0)\n",
    "                overall_validation_loss = torch.mean(validation_loss,dim=0)\n",
    "                self.training_loss_trajectory.append(overall_training_loss.item())\n",
    "                self.validation_loss_trajectory.append(overall_validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", overall_training_loss.item(), \"Validation loss\", overall_validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0] # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = torch.mean(grad_z1, dim=0).unsqueeze(1) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0] # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = torch.mean(grad_z2, dim=0).unsqueeze(1)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            self.W1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/self.eps_init**2 + self.reg1*self.W1) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "            self.b1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/self.eps_init**2 + self.reg1*self.b1)\n",
    "            self.W2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/self.eps_init**2 +self.reg2*self.W2)\n",
    "            self.b2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/self.eps_init**2 + self.reg2*self.b2)\n",
    "        return \"Training done\"\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
