{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337dd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cb0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# improve the ploting style\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "rcParams['font.size'] = 18\n",
    "rcParams['mathtext.fontset'] = 'stix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba5a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_data = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) # Apply the (x - mean)/var operation on the components of the data # if x is in [0,1] then Normalise(x) is in [-1,1] # is applied on the three channels RGB\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feff3039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "dtype = torch.float32\n",
    "trainset = torchvision.datasets.CIFAR10(root = './datas', train= True, download = True, transform = transform_data)\n",
    "validset = torchvision.datasets.CIFAR10(root = './datas', train = False, download = True, transform = transform_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 1)\n",
    "\n",
    "x_train_raw, y_train_raw, x_valid_raw, y_valid_raw = torch.tensor(trainset.data, ), torch.tensor(trainset.targets), torch.tensor(validset.data), torch.tensor(validset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1ded496",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = (x_train_raw.reshape(x_train_raw.shape[0], x_train_raw.shape[1]*x_train_raw.shape[2]*x_train_raw.shape[3])).to(dtype), x_valid_raw.reshape(x_valid_raw.shape[0], x_valid_raw.shape[1]*x_valid_raw.shape[2]*x_valid_raw.shape[3]).to(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8baed1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 59.,  62.,  63.,  43.,  46.,  45.,  50.,  48.,  43.,  68.,  54.,  42.,\n",
      "         98.,  73.,  52., 119.,  91.,  63., 139., 107.,  75., 145., 110.,  80.,\n",
      "        149., 117.,  89., 149., 120.,  93., 131., 103.])\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0,0:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "329b3d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 32, 32, 3]) torch.Size([50000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(x_train_raw.shape, y_train_raw.shape)\n",
    "plt.imshow(x_train_raw[17])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7698be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 10]) torch.Size([10000, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40000, 3072]) torch.Size([40000, 1]) tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) torch.Size([8000, 3072]) torch.Size([8000, 1])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "# Modification du format des données shape (n_data,1) -> (n_data, n_classes)\n",
    "y_train = torch.zeros((y_train_raw.shape[0], torch.max(y_train_raw)+1))\n",
    "for i, y in enumerate(y_train_raw):\n",
    "    j = int(y.item())\n",
    "    y_train[i,j] = 1\n",
    "\n",
    "y_valid = torch.zeros((y_valid_raw.shape[0], torch.max(y_valid_raw)+1))\n",
    "for i,y in enumerate(y_valid_raw):\n",
    "    j = int(y.item())\n",
    "    y_valid[i,j] = 1 \n",
    "print(y_train.shape, y_valid.shape)\n",
    "\n",
    "# Binary reduction of the classes # To avoid using softmax, we regroup classes in two classes\n",
    "class_binary_reduction = True\n",
    "determination_des_classes = True\n",
    "if class_binary_reduction :\n",
    "    if determination_des_classes :\n",
    "        # Determination des classes\n",
    "        class_list = []\n",
    "        class_index = 0\n",
    "        for i in range (x_train_raw.shape[0]):\n",
    "            if y_train[i, class_index] == 1:\n",
    "                class_list.append(x_train_raw[i])\n",
    "                class_index += 1\n",
    "            if len(class_list) == len(y_train[0]):\n",
    "                break\n",
    "        for i, image in enumerate(class_list):\n",
    "            plt.subplot(2, int(len(class_list)/2+1),i+1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "        plt.show()\n",
    "                \n",
    "    classe1 = [0, 1, 8, 9]\n",
    "    classe2 = [2, 3, 4, 5]\n",
    "\n",
    "    # Création des masques pour les échantillons appartenant à ces classes\n",
    "    mask_classe1_train = y_train[:, classe1].sum(dim=1) > 0  # True si appartient à classe1\n",
    "    mask_classe2_train = y_train[:, classe2].sum(dim=1) > 0  # True si appartient à classe2\n",
    "    \n",
    "    mask_classe1_valid = y_valid[:, classe1].sum(dim=1) > 0\n",
    "    mask_classe2_valid = y_valid[:, classe2].sum(dim=1) > 0\n",
    "    \n",
    "    # Filtrage des exemples concernés\n",
    "    mask_train = torch.logical_or(mask_classe1_train, mask_classe2_train)\n",
    "    mask_valid = torch.logical_or(mask_classe1_valid, mask_classe2_valid)\n",
    "    x_train, y_train = x_train[mask_train], y_train[mask_train]\n",
    "    x_valid, y_valid = x_valid[mask_valid], y_valid[mask_valid]\n",
    "\n",
    "    # Création du vecteur de labels binaires (1 pour classe1, 0 pour classe2)\n",
    "    y_train = (y_train[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    y_valid = (y_valid[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    \n",
    "    # Avec tanh\n",
    "    \n",
    "    # x_train = 2*(x_train-0.5)\n",
    "    # y_train = 2*(y_train-0.5)\n",
    "    # x_valid = 2*(x_valid-0.5)\n",
    "    # y_valid = 2*(y_valid-0.5)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, y_train[0:10], x_valid.shape, y_valid.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45060724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing on :  mps\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Computing on : \", device)\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0).to(device),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).type(dtype).to(device)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), déjà softmaxé\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque échantillon\n",
    "    \"\"\"\n",
    "    s = s.to(device)\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype).to(device) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\n",
    "    for i in range(n):  # Pour chaque échantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return torch.exp(-x)/((1 + torch.exp(-x))**2)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - (torch.tanh(x)**2)).to(device)\n",
    "\n",
    "class two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output); grad_z2  = grad_z2.to(dtype) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée de l'entraînement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "   \n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, number_of_classes,lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(three_layer_NN,self).__init__()\n",
    "        # Initialisation des propriétés du réseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la propriété durée d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du réseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(number_of_classes, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t()\n",
    "        output = self.softmax(z3) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = (torch.einsum('no,noz->nz',grad_output,softmax_derivative(output))).to(dtype) # shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "\n",
    "class binary_classification_two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(1, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = torch.sigmoid(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            #print(output[0:5], z2[0:5], h1[0:5], z1[0:5])\n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                print(\"Output\", output[20:22])\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = grad_output*sigmoid_derivative(z2); grad_z2  = grad_z2.to(dtype) # shape(n_data, 1)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # if i == 250:\n",
    "            #     break\n",
    "            # print(\"grad_output\" ,grad_output[0:2,0])            \n",
    "            # print(\"grad_z2\",grad_z2[0:2,0]) \n",
    "            # print(\"grad_h1\",grad_h1[0:2,0])\n",
    "            # print(\"grad_z1\", grad_z1[0:2,0] )\n",
    "            # print(\"x_minibatch\", torch.mean(x_minibatch[0]))\n",
    "            # print(\"grad_W1\",grad_W1[0:2,0])\n",
    "            # print(\"mean W1\", torch.mean(grad_W1))            \n",
    "            # print(\"grad_b1\",grad_b1[0:2,0])\n",
    "            # print(\"grad_W2\",grad_W2[0:2,0])\n",
    "            # print(\"grad_b2\",grad_b2[0:2,0])\n",
    "            # print(\"W1\",self.W1[0:2,0])\n",
    "            # print(\"W2\",self.W2[0:2,0])\n",
    "            # print(\"b1\",self.b1[0:2,0])\n",
    "            # print(\"b2\",self.b2[0:2,0])\n",
    "            # break\n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la durée de l'entraînement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "class binary_classification_three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_three_layer_NN,self).__init__()\n",
    "        # Initialisation des propriétés du réseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la propriété durée d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du réseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(1, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, hidden_2_size ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t() # shape (n_data, 1)\n",
    "        output = sigmoid(z3) # output layer # shape (n_data, 1)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage aléatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            \n",
    "            # Calcul de la prédiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'échantillonnage dépend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps lié à la sauvegarde des données d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = grad_output*sigmoid_derivative(z3).to(dtype) # shape (n_data, 1) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "            \n",
    "            # if i == 12000:\n",
    "            #     break\n",
    "        \n",
    "        # Calcul de la durée d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36584c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_2_layer = binary_classification_two_layer_NN(3072, 2048, eps_init = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57236d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 1.6, the number of datas used for the training is 38007905.54899443 and the number of iterations is 95019.\n",
      "Output tensor([[0.5344],\n",
      "        [0.5021]], device='mps:0')\n",
      "Iteration 0 Training loss 0.12433286756277084 Validation loss 0.1240895688533783 Accuracy 0.515625\n",
      "Output tensor([[0.4391],\n",
      "        [0.4732]], device='mps:0')\n",
      "Iteration 10 Training loss 0.11404707282781601 Validation loss 0.11162389069795609 Accuracy 0.7252500057220459\n",
      "Output tensor([[0.4721],\n",
      "        [0.5167]], device='mps:0')\n",
      "Iteration 20 Training loss 0.10335248708724976 Validation loss 0.10402999073266983 Accuracy 0.7307500243186951\n",
      "Output tensor([[0.4369],\n",
      "        [0.6140]], device='mps:0')\n",
      "Iteration 30 Training loss 0.09915754944086075 Validation loss 0.09918401390314102 Accuracy 0.7452500462532043\n",
      "Output tensor([[0.6085],\n",
      "        [0.5614]], device='mps:0')\n",
      "Iteration 40 Training loss 0.09927006810903549 Validation loss 0.09629858285188675 Accuracy 0.7383750081062317\n",
      "Output tensor([[0.4795],\n",
      "        [0.2637]], device='mps:0')\n",
      "Iteration 50 Training loss 0.09213077276945114 Validation loss 0.09358913451433182 Accuracy 0.752875030040741\n",
      "Output tensor([[0.7818],\n",
      "        [0.4871]], device='mps:0')\n",
      "Iteration 60 Training loss 0.08727472275495529 Validation loss 0.09209339320659637 Accuracy 0.749500036239624\n",
      "Output tensor([[0.5581],\n",
      "        [0.8507]], device='mps:0')\n",
      "Iteration 70 Training loss 0.09135264903306961 Validation loss 0.09062419831752777 Accuracy 0.7551250457763672\n",
      "Output tensor([[0.3197],\n",
      "        [0.5732]], device='mps:0')\n",
      "Iteration 80 Training loss 0.08572746068239212 Validation loss 0.08925870805978775 Accuracy 0.7585000395774841\n",
      "Output tensor([[0.7404],\n",
      "        [0.5547]], device='mps:0')\n",
      "Iteration 90 Training loss 0.0861709788441658 Validation loss 0.08822322636842728 Accuracy 0.7576250433921814\n",
      "Output tensor([[0.6938],\n",
      "        [0.5734]], device='mps:0')\n",
      "Iteration 100 Training loss 0.09670443087816238 Validation loss 0.0890803337097168 Accuracy 0.749875009059906\n",
      "Output tensor([[0.2990],\n",
      "        [0.2816]], device='mps:0')\n",
      "Iteration 110 Training loss 0.08371652662754059 Validation loss 0.08658962696790695 Accuracy 0.7616250514984131\n",
      "Output tensor([[0.1532],\n",
      "        [0.8340]], device='mps:0')\n",
      "Iteration 120 Training loss 0.08134427666664124 Validation loss 0.08596419543027878 Accuracy 0.7628750205039978\n",
      "Output tensor([[0.4969],\n",
      "        [0.1889]], device='mps:0')\n",
      "Iteration 130 Training loss 0.08843337744474411 Validation loss 0.08532661199569702 Accuracy 0.764875054359436\n",
      "Output tensor([[0.6735],\n",
      "        [0.9329]], device='mps:0')\n",
      "Iteration 140 Training loss 0.08493693917989731 Validation loss 0.08479209989309311 Accuracy 0.7661250233650208\n",
      "Output tensor([[0.8404],\n",
      "        [0.4305]], device='mps:0')\n",
      "Iteration 150 Training loss 0.08050916343927383 Validation loss 0.08441176265478134 Accuracy 0.7676250338554382\n",
      "Output tensor([[0.6469],\n",
      "        [0.5240]], device='mps:0')\n",
      "Iteration 160 Training loss 0.08866854757070541 Validation loss 0.08382421731948853 Accuracy 0.768625020980835\n",
      "Output tensor([[0.3576],\n",
      "        [0.0789]], device='mps:0')\n",
      "Iteration 170 Training loss 0.07931463420391083 Validation loss 0.0833202451467514 Accuracy 0.7712500095367432\n",
      "Output tensor([[0.5998],\n",
      "        [0.2170]], device='mps:0')\n",
      "Iteration 180 Training loss 0.07942015677690506 Validation loss 0.08293014764785767 Accuracy 0.7708750367164612\n",
      "Output tensor([[0.2068],\n",
      "        [0.2578]], device='mps:0')\n",
      "Iteration 190 Training loss 0.08840367197990417 Validation loss 0.08312178403139114 Accuracy 0.768750011920929\n",
      "Output tensor([[0.1002],\n",
      "        [0.5714]], device='mps:0')\n",
      "Iteration 200 Training loss 0.08319585770368576 Validation loss 0.08255688101053238 Accuracy 0.7700000405311584\n",
      "Output tensor([[0.4535],\n",
      "        [0.7067]], device='mps:0')\n",
      "Iteration 210 Training loss 0.08458829671144485 Validation loss 0.08228807896375656 Accuracy 0.7715000510215759\n",
      "Output tensor([[0.6481],\n",
      "        [0.7816]], device='mps:0')\n",
      "Iteration 220 Training loss 0.07559886574745178 Validation loss 0.08174916356801987 Accuracy 0.7737500071525574\n",
      "Output tensor([[0.8825],\n",
      "        [0.2205]], device='mps:0')\n",
      "Iteration 230 Training loss 0.08297380059957504 Validation loss 0.08135289698839188 Accuracy 0.7746250629425049\n",
      "Output tensor([[0.4016],\n",
      "        [0.7433]], device='mps:0')\n",
      "Iteration 240 Training loss 0.07626183331012726 Validation loss 0.0819292813539505 Accuracy 0.7713750600814819\n",
      "Output tensor([[0.3897],\n",
      "        [0.9025]], device='mps:0')\n",
      "Iteration 250 Training loss 0.0779356062412262 Validation loss 0.0809139758348465 Accuracy 0.7765000462532043\n",
      "Output tensor([[0.1293],\n",
      "        [0.8636]], device='mps:0')\n",
      "Iteration 260 Training loss 0.08917102962732315 Validation loss 0.08061925321817398 Accuracy 0.7772500514984131\n",
      "Output tensor([[0.8462],\n",
      "        [0.9209]], device='mps:0')\n",
      "Iteration 270 Training loss 0.07666973769664764 Validation loss 0.08053728193044662 Accuracy 0.7770000100135803\n",
      "Output tensor([[0.1839],\n",
      "        [0.4111]], device='mps:0')\n",
      "Iteration 280 Training loss 0.077266626060009 Validation loss 0.08020364493131638 Accuracy 0.7781250476837158\n",
      "Output tensor([[0.1382],\n",
      "        [0.4203]], device='mps:0')\n",
      "Iteration 290 Training loss 0.08199693262577057 Validation loss 0.07997613400220871 Accuracy 0.7788750529289246\n",
      "Output tensor([[0.7212],\n",
      "        [0.8298]], device='mps:0')\n",
      "Iteration 300 Training loss 0.08007360249757767 Validation loss 0.07987406104803085 Accuracy 0.7787500619888306\n",
      "Output tensor([[0.7085],\n",
      "        [0.7385]], device='mps:0')\n",
      "Iteration 310 Training loss 0.08116361498832703 Validation loss 0.07964933663606644 Accuracy 0.7795000076293945\n",
      "Output tensor([[0.0835],\n",
      "        [0.2219]], device='mps:0')\n",
      "Iteration 320 Training loss 0.08127428591251373 Validation loss 0.07984858006238937 Accuracy 0.7781250476837158\n",
      "Output tensor([[0.6011],\n",
      "        [0.3060]], device='mps:0')\n",
      "Iteration 330 Training loss 0.07896602898836136 Validation loss 0.07939156144857407 Accuracy 0.7795000076293945\n",
      "Output tensor([[0.2899],\n",
      "        [0.0474]], device='mps:0')\n",
      "Iteration 340 Training loss 0.07621680200099945 Validation loss 0.07915228605270386 Accuracy 0.780500054359436\n",
      "Output tensor([[0.3788],\n",
      "        [0.1795]], device='mps:0')\n",
      "Iteration 350 Training loss 0.0756276398897171 Validation loss 0.07904184609651566 Accuracy 0.7815000414848328\n",
      "Output tensor([[0.5852],\n",
      "        [0.6535]], device='mps:0')\n",
      "Iteration 360 Training loss 0.07740487158298492 Validation loss 0.07934679090976715 Accuracy 0.781125009059906\n",
      "Output tensor([[0.5364],\n",
      "        [0.3952]], device='mps:0')\n",
      "Iteration 370 Training loss 0.07532072067260742 Validation loss 0.0787622258067131 Accuracy 0.7815000414848328\n",
      "Output tensor([[0.8408],\n",
      "        [0.6562]], device='mps:0')\n",
      "Iteration 380 Training loss 0.0817093700170517 Validation loss 0.07885893434286118 Accuracy 0.7821250557899475\n",
      "Output tensor([[0.0754],\n",
      "        [0.2843]], device='mps:0')\n",
      "Iteration 390 Training loss 0.07434742897748947 Validation loss 0.07845260947942734 Accuracy 0.7823750376701355\n",
      "Output tensor([[0.8539],\n",
      "        [0.8575]], device='mps:0')\n",
      "Iteration 400 Training loss 0.07726363092660904 Validation loss 0.07897303998470306 Accuracy 0.7821250557899475\n",
      "Output tensor([[0.5997],\n",
      "        [0.7142]], device='mps:0')\n",
      "Iteration 410 Training loss 0.08031585812568665 Validation loss 0.07832901179790497 Accuracy 0.7846250534057617\n",
      "Output tensor([[0.7257],\n",
      "        [0.2830]], device='mps:0')\n",
      "Iteration 420 Training loss 0.078406423330307 Validation loss 0.07808299362659454 Accuracy 0.7848750352859497\n",
      "Output tensor([[0.6719],\n",
      "        [0.2070]], device='mps:0')\n",
      "Iteration 430 Training loss 0.07505976408720016 Validation loss 0.07815968990325928 Accuracy 0.784250020980835\n",
      "Output tensor([[0.6825],\n",
      "        [0.0845]], device='mps:0')\n",
      "Iteration 440 Training loss 0.07247135043144226 Validation loss 0.07804728299379349 Accuracy 0.7855000495910645\n",
      "Output tensor([[0.6176],\n",
      "        [0.2011]], device='mps:0')\n",
      "Iteration 450 Training loss 0.08036798238754272 Validation loss 0.07782309502363205 Accuracy 0.7846250534057617\n",
      "Output tensor([[0.5555],\n",
      "        [0.5067]], device='mps:0')\n",
      "Iteration 460 Training loss 0.0767764151096344 Validation loss 0.07787445187568665 Accuracy 0.7848750352859497\n",
      "Output tensor([[0.4850],\n",
      "        [0.8137]], device='mps:0')\n",
      "Iteration 470 Training loss 0.07346916198730469 Validation loss 0.07758697867393494 Accuracy 0.7860000133514404\n",
      "Output tensor([[0.4460],\n",
      "        [0.6257]], device='mps:0')\n",
      "Iteration 480 Training loss 0.07133984565734863 Validation loss 0.07744374126195908 Accuracy 0.7872500419616699\n",
      "Output tensor([[0.9497],\n",
      "        [0.1487]], device='mps:0')\n",
      "Iteration 490 Training loss 0.07259008288383484 Validation loss 0.0773908868432045 Accuracy 0.7862500548362732\n",
      "Output tensor([[0.4788],\n",
      "        [0.7689]], device='mps:0')\n",
      "Iteration 500 Training loss 0.07093146443367004 Validation loss 0.07736743241548538 Accuracy 0.7862500548362732\n",
      "Output tensor([[0.3619],\n",
      "        [0.5673]], device='mps:0')\n",
      "Iteration 510 Training loss 0.07241106778383255 Validation loss 0.07717613875865936 Accuracy 0.7867500185966492\n",
      "Output tensor([[0.8925],\n",
      "        [0.2831]], device='mps:0')\n",
      "Iteration 520 Training loss 0.08516723662614822 Validation loss 0.07734259217977524 Accuracy 0.7861250638961792\n",
      "Output tensor([[0.2542],\n",
      "        [0.0907]], device='mps:0')\n",
      "Iteration 530 Training loss 0.07124915719032288 Validation loss 0.07730267196893692 Accuracy 0.7866250276565552\n",
      "Output tensor([[0.2151],\n",
      "        [0.3558]], device='mps:0')\n",
      "Iteration 540 Training loss 0.0837705135345459 Validation loss 0.07693782448768616 Accuracy 0.7880000472068787\n",
      "Output tensor([[0.3883],\n",
      "        [0.3915]], device='mps:0')\n",
      "Iteration 550 Training loss 0.06980577111244202 Validation loss 0.07688812166452408 Accuracy 0.7881250381469727\n",
      "Output tensor([[0.2239],\n",
      "        [0.8068]], device='mps:0')\n",
      "Iteration 560 Training loss 0.07637595385313034 Validation loss 0.07678262889385223 Accuracy 0.7887500524520874\n",
      "Output tensor([[0.2383],\n",
      "        [0.1220]], device='mps:0')\n",
      "Iteration 570 Training loss 0.07264654338359833 Validation loss 0.07681669294834137 Accuracy 0.7882500290870667\n",
      "Output tensor([[0.5725],\n",
      "        [0.2979]], device='mps:0')\n",
      "Iteration 580 Training loss 0.08308437466621399 Validation loss 0.0767836645245552 Accuracy 0.7887500524520874\n",
      "Output tensor([[0.1717],\n",
      "        [0.2145]], device='mps:0')\n",
      "Iteration 590 Training loss 0.0769384577870369 Validation loss 0.07657165825366974 Accuracy 0.7885000109672546\n",
      "Output tensor([[0.1619],\n",
      "        [0.4039]], device='mps:0')\n",
      "Iteration 600 Training loss 0.07744313776493073 Validation loss 0.07659541815519333 Accuracy 0.7892500162124634\n",
      "Output tensor([[0.4299],\n",
      "        [0.2781]], device='mps:0')\n",
      "Iteration 610 Training loss 0.08348304778337479 Validation loss 0.0764630138874054 Accuracy 0.7893750667572021\n",
      "Output tensor([[0.5273],\n",
      "        [0.5147]], device='mps:0')\n",
      "Iteration 620 Training loss 0.085294209420681 Validation loss 0.07637672871351242 Accuracy 0.7900000214576721\n",
      "Output tensor([[0.2894],\n",
      "        [0.0121]], device='mps:0')\n",
      "Iteration 630 Training loss 0.06823547184467316 Validation loss 0.0763816386461258 Accuracy 0.7895000576972961\n",
      "Output tensor([[0.2037],\n",
      "        [0.6772]], device='mps:0')\n",
      "Iteration 640 Training loss 0.07226604968309402 Validation loss 0.07633444666862488 Accuracy 0.7895000576972961\n",
      "Output tensor([[0.9515],\n",
      "        [0.3341]], device='mps:0')\n",
      "Iteration 650 Training loss 0.07194880396127701 Validation loss 0.07628031075000763 Accuracy 0.7895000576972961\n",
      "Output tensor([[0.3802],\n",
      "        [0.1422]], device='mps:0')\n",
      "Iteration 660 Training loss 0.07371105253696442 Validation loss 0.07623795419931412 Accuracy 0.7906250357627869\n",
      "Output tensor([[0.4547],\n",
      "        [0.6031]], device='mps:0')\n",
      "Iteration 670 Training loss 0.07687730342149734 Validation loss 0.07607288658618927 Accuracy 0.7893750667572021\n",
      "Output tensor([[0.6442],\n",
      "        [0.8842]], device='mps:0')\n",
      "Iteration 680 Training loss 0.07692408561706543 Validation loss 0.07604935020208359 Accuracy 0.7895000576972961\n",
      "Output tensor([[0.3361],\n",
      "        [0.3997]], device='mps:0')\n",
      "Iteration 690 Training loss 0.06668495386838913 Validation loss 0.07597677409648895 Accuracy 0.7902500629425049\n",
      "Output tensor([[0.4399],\n",
      "        [0.7327]], device='mps:0')\n",
      "Iteration 700 Training loss 0.07804293185472488 Validation loss 0.07593367993831635 Accuracy 0.7912500500679016\n",
      "Output tensor([[0.7923],\n",
      "        [0.7789]], device='mps:0')\n",
      "Iteration 710 Training loss 0.07440976798534393 Validation loss 0.07586710900068283 Accuracy 0.7917500138282776\n",
      "Output tensor([[0.6515],\n",
      "        [0.8688]], device='mps:0')\n",
      "Iteration 720 Training loss 0.06786396354436874 Validation loss 0.07590508460998535 Accuracy 0.7907500267028809\n",
      "Output tensor([[0.1923],\n",
      "        [0.8494]], device='mps:0')\n",
      "Iteration 730 Training loss 0.08205783367156982 Validation loss 0.07581828534603119 Accuracy 0.7921250462532043\n",
      "Output tensor([[0.4547],\n",
      "        [0.0660]], device='mps:0')\n",
      "Iteration 740 Training loss 0.07806207239627838 Validation loss 0.07572845369577408 Accuracy 0.7921250462532043\n",
      "Output tensor([[0.6919],\n",
      "        [0.3671]], device='mps:0')\n",
      "Iteration 750 Training loss 0.08148784190416336 Validation loss 0.07585600018501282 Accuracy 0.7903750538825989\n",
      "Output tensor([[0.5991],\n",
      "        [0.1565]], device='mps:0')\n",
      "Iteration 760 Training loss 0.07397790998220444 Validation loss 0.07560725510120392 Accuracy 0.7922500371932983\n",
      "Output tensor([[0.9585],\n",
      "        [0.2119]], device='mps:0')\n",
      "Iteration 770 Training loss 0.08156248927116394 Validation loss 0.07560023665428162 Accuracy 0.7931250333786011\n",
      "Output tensor([[0.8466],\n",
      "        [0.1121]], device='mps:0')\n",
      "Iteration 780 Training loss 0.07591158896684647 Validation loss 0.07558742165565491 Accuracy 0.7921250462532043\n",
      "Output tensor([[0.9031],\n",
      "        [0.2845]], device='mps:0')\n",
      "Iteration 790 Training loss 0.06736941635608673 Validation loss 0.07548622041940689 Accuracy 0.7937500476837158\n",
      "Output tensor([[0.1640],\n",
      "        [0.7816]], device='mps:0')\n",
      "Iteration 800 Training loss 0.07678914815187454 Validation loss 0.07541928440332413 Accuracy 0.7942500114440918\n",
      "Output tensor([[0.0628],\n",
      "        [0.4266]], device='mps:0')\n",
      "Iteration 810 Training loss 0.07251621037721634 Validation loss 0.07557371258735657 Accuracy 0.7917500138282776\n",
      "Output tensor([[0.3074],\n",
      "        [0.9318]], device='mps:0')\n",
      "Iteration 820 Training loss 0.0764058455824852 Validation loss 0.0754355788230896 Accuracy 0.7932500243186951\n",
      "Output tensor([[0.8415],\n",
      "        [0.8978]], device='mps:0')\n",
      "Iteration 830 Training loss 0.07470017671585083 Validation loss 0.07532864809036255 Accuracy 0.7937500476837158\n",
      "Output tensor([[0.1716],\n",
      "        [0.8527]], device='mps:0')\n",
      "Iteration 840 Training loss 0.07780986279249191 Validation loss 0.0752514973282814 Accuracy 0.7928750514984131\n",
      "Output tensor([[0.8697],\n",
      "        [0.8213]], device='mps:0')\n",
      "Iteration 850 Training loss 0.07269804924726486 Validation loss 0.07528787106275558 Accuracy 0.7932500243186951\n",
      "Output tensor([[0.8603],\n",
      "        [0.0649]], device='mps:0')\n",
      "Iteration 860 Training loss 0.08024942129850388 Validation loss 0.07529286295175552 Accuracy 0.7932500243186951\n",
      "Output tensor([[0.3392],\n",
      "        [0.6439]], device='mps:0')\n",
      "Iteration 870 Training loss 0.08034621924161911 Validation loss 0.07512688636779785 Accuracy 0.7950000166893005\n",
      "Output tensor([[0.5095],\n",
      "        [0.1200]], device='mps:0')\n",
      "Iteration 880 Training loss 0.07220374047756195 Validation loss 0.07516808807849884 Accuracy 0.7917500138282776\n",
      "Output tensor([[0.8447],\n",
      "        [0.0425]], device='mps:0')\n",
      "Iteration 890 Training loss 0.07560709863901138 Validation loss 0.07514261454343796 Accuracy 0.7936250567436218\n",
      "Output tensor([[0.1963],\n",
      "        [0.7390]], device='mps:0')\n",
      "Iteration 900 Training loss 0.07072863727807999 Validation loss 0.07497937232255936 Accuracy 0.7957500219345093\n",
      "Output tensor([[0.5064],\n",
      "        [0.7730]], device='mps:0')\n",
      "Iteration 910 Training loss 0.08342824876308441 Validation loss 0.07493308186531067 Accuracy 0.796625018119812\n",
      "Output tensor([[0.4151],\n",
      "        [0.7978]], device='mps:0')\n",
      "Iteration 920 Training loss 0.08410439640283585 Validation loss 0.0749121829867363 Accuracy 0.7955000400543213\n",
      "Output tensor([[0.0759],\n",
      "        [0.7138]], device='mps:0')\n",
      "Iteration 930 Training loss 0.07354661822319031 Validation loss 0.0748637393116951 Accuracy 0.7958750128746033\n",
      "Output tensor([[0.1291],\n",
      "        [0.6157]], device='mps:0')\n",
      "Iteration 940 Training loss 0.06951632350683212 Validation loss 0.07486581802368164 Accuracy 0.7938750386238098\n",
      "Output tensor([[0.0775],\n",
      "        [0.0054]], device='mps:0')\n",
      "Iteration 950 Training loss 0.07352767884731293 Validation loss 0.07481373846530914 Accuracy 0.7933750152587891\n",
      "Output tensor([[0.5868],\n",
      "        [0.0943]], device='mps:0')\n",
      "Iteration 960 Training loss 0.07887961715459824 Validation loss 0.07494077831506729 Accuracy 0.7943750619888306\n",
      "Output tensor([[0.7077],\n",
      "        [0.2359]], device='mps:0')\n",
      "Iteration 970 Training loss 0.07643565535545349 Validation loss 0.07472658157348633 Accuracy 0.79625004529953\n",
      "Output tensor([[0.7607],\n",
      "        [0.3587]], device='mps:0')\n",
      "Iteration 980 Training loss 0.07575272768735886 Validation loss 0.07468482106924057 Accuracy 0.7945000529289246\n",
      "Output tensor([[0.6159],\n",
      "        [0.2922]], device='mps:0')\n",
      "Iteration 990 Training loss 0.07567840069532394 Validation loss 0.07472563534975052 Accuracy 0.7958750128746033\n",
      "Output tensor([[0.9695],\n",
      "        [0.3210]], device='mps:0')\n",
      "Iteration 1000 Training loss 0.07464704662561417 Validation loss 0.07463739067316055 Accuracy 0.796125054359436\n",
      "Output tensor([[0.1055],\n",
      "        [0.8192]], device='mps:0')\n",
      "Iteration 1010 Training loss 0.07300276309251785 Validation loss 0.074607715010643 Accuracy 0.7957500219345093\n",
      "Output tensor([[0.1065],\n",
      "        [0.6926]], device='mps:0')\n",
      "Iteration 1020 Training loss 0.07766785472631454 Validation loss 0.07467776536941528 Accuracy 0.7950000166893005\n",
      "Output tensor([[0.0296],\n",
      "        [0.4685]], device='mps:0')\n",
      "Iteration 1030 Training loss 0.07076675444841385 Validation loss 0.07456333935260773 Accuracy 0.7955000400543213\n",
      "Output tensor([[0.6000],\n",
      "        [0.1224]], device='mps:0')\n",
      "Iteration 1040 Training loss 0.07504063099622726 Validation loss 0.07456908375024796 Accuracy 0.7955000400543213\n",
      "Output tensor([[0.3641],\n",
      "        [0.9423]], device='mps:0')\n",
      "Iteration 1050 Training loss 0.07212288677692413 Validation loss 0.07482878118753433 Accuracy 0.7953750491142273\n",
      "Output tensor([[0.2701],\n",
      "        [0.3047]], device='mps:0')\n",
      "Iteration 1060 Training loss 0.07486613839864731 Validation loss 0.07448698580265045 Accuracy 0.7955000400543213\n",
      "Output tensor([[0.1800],\n",
      "        [0.9721]], device='mps:0')\n",
      "Iteration 1070 Training loss 0.08073878288269043 Validation loss 0.07493074983358383 Accuracy 0.7952500581741333\n",
      "Output tensor([[0.3066],\n",
      "        [0.1105]], device='mps:0')\n",
      "Iteration 1080 Training loss 0.07013443112373352 Validation loss 0.07442427426576614 Accuracy 0.7971250414848328\n",
      "Output tensor([[0.7372],\n",
      "        [0.1350]], device='mps:0')\n",
      "Iteration 1090 Training loss 0.08141493797302246 Validation loss 0.07463513314723969 Accuracy 0.7920000553131104\n",
      "Output tensor([[0.9474],\n",
      "        [0.3965]], device='mps:0')\n",
      "Iteration 1100 Training loss 0.06510810554027557 Validation loss 0.0743277296423912 Accuracy 0.796625018119812\n",
      "Output tensor([[0.6161],\n",
      "        [0.9829]], device='mps:0')\n",
      "Iteration 1110 Training loss 0.07999995350837708 Validation loss 0.07447274774312973 Accuracy 0.7936250567436218\n",
      "Output tensor([[0.4319],\n",
      "        [0.9941]], device='mps:0')\n",
      "Iteration 1120 Training loss 0.07519682496786118 Validation loss 0.07434550672769547 Accuracy 0.79625004529953\n",
      "Output tensor([[0.2088],\n",
      "        [0.5865]], device='mps:0')\n",
      "Iteration 1130 Training loss 0.07090545445680618 Validation loss 0.07433070987462997 Accuracy 0.7971250414848328\n",
      "Output tensor([[0.9531],\n",
      "        [0.8693]], device='mps:0')\n",
      "Iteration 1140 Training loss 0.07009939849376678 Validation loss 0.07426770776510239 Accuracy 0.7971250414848328\n",
      "Output tensor([[0.4273],\n",
      "        [0.2528]], device='mps:0')\n",
      "Iteration 1150 Training loss 0.06990943104028702 Validation loss 0.07468704134225845 Accuracy 0.7942500114440918\n",
      "Output tensor([[0.9539],\n",
      "        [0.5622]], device='mps:0')\n",
      "Iteration 1160 Training loss 0.07798594981431961 Validation loss 0.0741814449429512 Accuracy 0.796000063419342\n",
      "Output tensor([[0.9891],\n",
      "        [0.7260]], device='mps:0')\n",
      "Iteration 1170 Training loss 0.061402373015880585 Validation loss 0.07427777349948883 Accuracy 0.7978750467300415\n",
      "Output tensor([[0.0340],\n",
      "        [0.6693]], device='mps:0')\n",
      "Iteration 1180 Training loss 0.06694590300321579 Validation loss 0.07408981770277023 Accuracy 0.796500027179718\n",
      "Output tensor([[0.2036],\n",
      "        [0.2302]], device='mps:0')\n",
      "Iteration 1190 Training loss 0.07815732061862946 Validation loss 0.07406024634838104 Accuracy 0.7980000376701355\n",
      "Output tensor([[0.7127],\n",
      "        [0.0202]], device='mps:0')\n",
      "Iteration 1200 Training loss 0.07427113503217697 Validation loss 0.0740242525935173 Accuracy 0.7976250648498535\n",
      "Output tensor([[0.1476],\n",
      "        [0.0998]], device='mps:0')\n",
      "Iteration 1210 Training loss 0.0772598460316658 Validation loss 0.07403646409511566 Accuracy 0.7975000143051147\n",
      "Output tensor([[0.4036],\n",
      "        [0.6404]], device='mps:0')\n",
      "Iteration 1220 Training loss 0.0692506954073906 Validation loss 0.07400846481323242 Accuracy 0.7970000505447388\n",
      "Output tensor([[0.2875],\n",
      "        [0.9311]], device='mps:0')\n",
      "Iteration 1230 Training loss 0.07396054267883301 Validation loss 0.07400607317686081 Accuracy 0.7973750233650208\n",
      "Output tensor([[0.8877],\n",
      "        [0.7367]], device='mps:0')\n",
      "Iteration 1240 Training loss 0.07338416576385498 Validation loss 0.07390730082988739 Accuracy 0.7976250648498535\n",
      "Output tensor([[0.8078],\n",
      "        [0.5486]], device='mps:0')\n",
      "Iteration 1250 Training loss 0.059907495975494385 Validation loss 0.07387901842594147 Accuracy 0.7970000505447388\n",
      "Output tensor([[0.2415],\n",
      "        [0.6227]], device='mps:0')\n",
      "Iteration 1260 Training loss 0.0732458084821701 Validation loss 0.0738588273525238 Accuracy 0.7972500324249268\n",
      "Output tensor([[0.8906],\n",
      "        [0.3892]], device='mps:0')\n",
      "Iteration 1270 Training loss 0.07724697142839432 Validation loss 0.07382303476333618 Accuracy 0.7980000376701355\n",
      "Output tensor([[0.5581],\n",
      "        [0.5974]], device='mps:0')\n",
      "Iteration 1280 Training loss 0.07578261196613312 Validation loss 0.07379854470491409 Accuracy 0.7976250648498535\n",
      "Output tensor([[0.0797],\n",
      "        [0.3661]], device='mps:0')\n",
      "Iteration 1290 Training loss 0.06083479896187782 Validation loss 0.07389684021472931 Accuracy 0.7980000376701355\n",
      "Output tensor([[0.7971],\n",
      "        [0.8268]], device='mps:0')\n",
      "Iteration 1300 Training loss 0.08109580725431442 Validation loss 0.0737769827246666 Accuracy 0.7976250648498535\n",
      "Output tensor([[0.0176],\n",
      "        [0.3042]], device='mps:0')\n",
      "Iteration 1310 Training loss 0.07014521211385727 Validation loss 0.07412828505039215 Accuracy 0.796000063419342\n",
      "Output tensor([[0.3331],\n",
      "        [0.4992]], device='mps:0')\n",
      "Iteration 1320 Training loss 0.060270193964242935 Validation loss 0.0737181305885315 Accuracy 0.7973750233650208\n",
      "Output tensor([[0.8142],\n",
      "        [0.6210]], device='mps:0')\n",
      "Iteration 1330 Training loss 0.07647783309221268 Validation loss 0.07368940860033035 Accuracy 0.7972500324249268\n",
      "Output tensor([[0.8761],\n",
      "        [0.6745]], device='mps:0')\n",
      "Iteration 1340 Training loss 0.07655058056116104 Validation loss 0.07367264479398727 Accuracy 0.7975000143051147\n",
      "Output tensor([[0.3091],\n",
      "        [0.0741]], device='mps:0')\n",
      "Iteration 1350 Training loss 0.07531390339136124 Validation loss 0.0737072303891182 Accuracy 0.799250066280365\n",
      "Output tensor([[0.5477],\n",
      "        [0.8590]], device='mps:0')\n",
      "Iteration 1360 Training loss 0.08012363314628601 Validation loss 0.0736655741930008 Accuracy 0.799250066280365\n",
      "Output tensor([[0.2703],\n",
      "        [0.9209]], device='mps:0')\n",
      "Iteration 1370 Training loss 0.07037203758955002 Validation loss 0.07358429580926895 Accuracy 0.7980000376701355\n",
      "Output tensor([[0.5137],\n",
      "        [0.3591]], device='mps:0')\n",
      "Iteration 1380 Training loss 0.06661106646060944 Validation loss 0.07358188182115555 Accuracy 0.7980000376701355\n",
      "Output tensor([[0.8774],\n",
      "        [0.0649]], device='mps:0')\n",
      "Iteration 1390 Training loss 0.07519428431987762 Validation loss 0.07352543622255325 Accuracy 0.7981250286102295\n",
      "Output tensor([[0.1615],\n",
      "        [0.2083]], device='mps:0')\n",
      "Iteration 1400 Training loss 0.06762014329433441 Validation loss 0.07354814559221268 Accuracy 0.799250066280365\n",
      "Output tensor([[0.7097],\n",
      "        [0.5442]], device='mps:0')\n",
      "Iteration 1410 Training loss 0.06755675375461578 Validation loss 0.07346867769956589 Accuracy 0.7986250519752502\n",
      "Output tensor([[0.0238],\n",
      "        [0.1238]], device='mps:0')\n",
      "Iteration 1420 Training loss 0.07318789511919022 Validation loss 0.073463574051857 Accuracy 0.7982500195503235\n",
      "Output tensor([[0.4928],\n",
      "        [0.1571]], device='mps:0')\n",
      "Iteration 1430 Training loss 0.06492063403129578 Validation loss 0.07354677468538284 Accuracy 0.7976250648498535\n",
      "Output tensor([[0.0695],\n",
      "        [0.3491]], device='mps:0')\n",
      "Iteration 1440 Training loss 0.07180283218622208 Validation loss 0.07342763245105743 Accuracy 0.7983750104904175\n",
      "Output tensor([[0.3248],\n",
      "        [0.0602]], device='mps:0')\n",
      "Iteration 1450 Training loss 0.07756771147251129 Validation loss 0.0733896791934967 Accuracy 0.7987500429153442\n",
      "Output tensor([[0.5952],\n",
      "        [0.0552]], device='mps:0')\n",
      "Iteration 1460 Training loss 0.0772116482257843 Validation loss 0.07347466051578522 Accuracy 0.7990000247955322\n",
      "Output tensor([[0.9627],\n",
      "        [0.0758]], device='mps:0')\n",
      "Iteration 1470 Training loss 0.06705696880817413 Validation loss 0.0734056755900383 Accuracy 0.799500048160553\n",
      "Output tensor([[0.4780],\n",
      "        [0.8109]], device='mps:0')\n",
      "Iteration 1480 Training loss 0.07337687164545059 Validation loss 0.07331857830286026 Accuracy 0.7985000610351562\n",
      "Output tensor([[0.8624],\n",
      "        [0.0326]], device='mps:0')\n",
      "Iteration 1490 Training loss 0.06967154890298843 Validation loss 0.07355553656816483 Accuracy 0.7980000376701355\n",
      "Output tensor([[0.0842],\n",
      "        [0.2380]], device='mps:0')\n",
      "Iteration 1500 Training loss 0.07720591127872467 Validation loss 0.07325185090303421 Accuracy 0.799500048160553\n",
      "Output tensor([[0.5688],\n",
      "        [0.5416]], device='mps:0')\n",
      "Iteration 1510 Training loss 0.06867708265781403 Validation loss 0.07324350625276566 Accuracy 0.7988750338554382\n",
      "Output tensor([[0.9094],\n",
      "        [0.9981]], device='mps:0')\n",
      "Iteration 1520 Training loss 0.07854597270488739 Validation loss 0.07364159822463989 Accuracy 0.796750009059906\n",
      "Output tensor([[0.7871],\n",
      "        [0.8684]], device='mps:0')\n",
      "Iteration 1530 Training loss 0.07111911475658417 Validation loss 0.07318852841854095 Accuracy 0.799375057220459\n",
      "Output tensor([[0.5605],\n",
      "        [0.8662]], device='mps:0')\n",
      "Iteration 1540 Training loss 0.07082764804363251 Validation loss 0.07316446304321289 Accuracy 0.799625039100647\n",
      "Output tensor([[0.0750],\n",
      "        [0.3067]], device='mps:0')\n",
      "Iteration 1550 Training loss 0.07144270837306976 Validation loss 0.07322949916124344 Accuracy 0.799750030040741\n",
      "Output tensor([[0.9349],\n",
      "        [0.6205]], device='mps:0')\n",
      "Iteration 1560 Training loss 0.07283363491296768 Validation loss 0.07312572002410889 Accuracy 0.800000011920929\n",
      "Output tensor([[0.9368],\n",
      "        [0.6291]], device='mps:0')\n",
      "Iteration 1570 Training loss 0.0812034010887146 Validation loss 0.07313493639230728 Accuracy 0.799250066280365\n",
      "Output tensor([[0.4898],\n",
      "        [0.8396]], device='mps:0')\n",
      "Iteration 1580 Training loss 0.06708397716283798 Validation loss 0.07309520244598389 Accuracy 0.799375057220459\n",
      "Output tensor([[0.3291],\n",
      "        [0.9279]], device='mps:0')\n",
      "Iteration 1590 Training loss 0.07270306348800659 Validation loss 0.07310585677623749 Accuracy 0.8001250624656677\n",
      "Output tensor([[0.0771],\n",
      "        [0.3201]], device='mps:0')\n",
      "Iteration 1600 Training loss 0.07015281170606613 Validation loss 0.07316751778125763 Accuracy 0.800000011920929\n",
      "Output tensor([[0.2579],\n",
      "        [0.7819]], device='mps:0')\n",
      "Iteration 1610 Training loss 0.07924983650445938 Validation loss 0.07302321493625641 Accuracy 0.8002500534057617\n",
      "Output tensor([[0.9557],\n",
      "        [0.8893]], device='mps:0')\n",
      "Iteration 1620 Training loss 0.07473532110452652 Validation loss 0.07307527214288712 Accuracy 0.799625039100647\n",
      "Output tensor([[0.3237],\n",
      "        [0.5847]], device='mps:0')\n",
      "Iteration 1630 Training loss 0.06950698792934418 Validation loss 0.07298430800437927 Accuracy 0.8001250624656677\n",
      "Output tensor([[0.1244],\n",
      "        [0.6693]], device='mps:0')\n",
      "Iteration 1640 Training loss 0.07832794636487961 Validation loss 0.07299447804689407 Accuracy 0.799875020980835\n",
      "Output tensor([[0.1431],\n",
      "        [0.2029]], device='mps:0')\n",
      "Iteration 1650 Training loss 0.07845896482467651 Validation loss 0.07337228953838348 Accuracy 0.7972500324249268\n",
      "Output tensor([[0.1450],\n",
      "        [0.7409]], device='mps:0')\n",
      "Iteration 1660 Training loss 0.06961114704608917 Validation loss 0.07306978106498718 Accuracy 0.8003750443458557\n",
      "Output tensor([[0.9327],\n",
      "        [0.1064]], device='mps:0')\n",
      "Iteration 1670 Training loss 0.07222636044025421 Validation loss 0.07291649281978607 Accuracy 0.8002500534057617\n",
      "Output tensor([[0.1246],\n",
      "        [0.9156]], device='mps:0')\n",
      "Iteration 1680 Training loss 0.08961698412895203 Validation loss 0.07304703444242477 Accuracy 0.8001250624656677\n",
      "Output tensor([[0.3883],\n",
      "        [0.5775]], device='mps:0')\n",
      "Iteration 1690 Training loss 0.06891009211540222 Validation loss 0.07284118980169296 Accuracy 0.8008750677108765\n",
      "Output tensor([[0.8933],\n",
      "        [0.4083]], device='mps:0')\n",
      "Iteration 1700 Training loss 0.0651879757642746 Validation loss 0.07283589988946915 Accuracy 0.8013750314712524\n",
      "Output tensor([[0.3442],\n",
      "        [0.2661]], device='mps:0')\n",
      "Iteration 1710 Training loss 0.06936315447092056 Validation loss 0.07281126827001572 Accuracy 0.8007500171661377\n",
      "Output tensor([[0.6316],\n",
      "        [0.3779]], device='mps:0')\n",
      "Iteration 1720 Training loss 0.07173517346382141 Validation loss 0.07293005287647247 Accuracy 0.8003750443458557\n",
      "Output tensor([[0.5028],\n",
      "        [0.5801]], device='mps:0')\n",
      "Iteration 1730 Training loss 0.0794907808303833 Validation loss 0.07286161184310913 Accuracy 0.8002500534057617\n",
      "Output tensor([[0.8409],\n",
      "        [0.9088]], device='mps:0')\n",
      "Iteration 1740 Training loss 0.06519173830747604 Validation loss 0.07275833189487457 Accuracy 0.8005000352859497\n",
      "Output tensor([[0.9649],\n",
      "        [0.9365]], device='mps:0')\n",
      "Iteration 1750 Training loss 0.06827922910451889 Validation loss 0.07279359549283981 Accuracy 0.8006250262260437\n",
      "Output tensor([[0.4656],\n",
      "        [0.3457]], device='mps:0')\n",
      "Iteration 1760 Training loss 0.07421106100082397 Validation loss 0.07272446155548096 Accuracy 0.8011250495910645\n",
      "Output tensor([[0.0507],\n",
      "        [0.3058]], device='mps:0')\n",
      "Iteration 1770 Training loss 0.07111947238445282 Validation loss 0.07278048992156982 Accuracy 0.8003750443458557\n",
      "Output tensor([[0.9497],\n",
      "        [0.9612]], device='mps:0')\n",
      "Iteration 1780 Training loss 0.07357639819383621 Validation loss 0.07269978523254395 Accuracy 0.8008750677108765\n",
      "Output tensor([[0.6906],\n",
      "        [0.9461]], device='mps:0')\n",
      "Iteration 1790 Training loss 0.062023647129535675 Validation loss 0.0728173553943634 Accuracy 0.8003750443458557\n",
      "Output tensor([[0.3159],\n",
      "        [0.0194]], device='mps:0')\n",
      "Iteration 1800 Training loss 0.07943695783615112 Validation loss 0.07262245565652847 Accuracy 0.8012500405311584\n",
      "Output tensor([[0.1205],\n",
      "        [0.3745]], device='mps:0')\n",
      "Iteration 1810 Training loss 0.07424850016832352 Validation loss 0.0726095661520958 Accuracy 0.8015000224113464\n",
      "Output tensor([[0.7913],\n",
      "        [0.1527]], device='mps:0')\n",
      "Iteration 1820 Training loss 0.0774427279829979 Validation loss 0.07272278517484665 Accuracy 0.799500048160553\n",
      "Output tensor([[0.9322],\n",
      "        [0.7998]], device='mps:0')\n",
      "Iteration 1830 Training loss 0.0737823024392128 Validation loss 0.0725877434015274 Accuracy 0.8011250495910645\n",
      "Output tensor([[0.6597],\n",
      "        [0.3514]], device='mps:0')\n",
      "Iteration 1840 Training loss 0.07554923743009567 Validation loss 0.07255063205957413 Accuracy 0.8016250133514404\n",
      "Output tensor([[0.0992],\n",
      "        [0.4745]], device='mps:0')\n",
      "Iteration 1850 Training loss 0.0708804801106453 Validation loss 0.07266615331172943 Accuracy 0.8001250624656677\n",
      "Output tensor([[0.7205],\n",
      "        [0.1213]], device='mps:0')\n",
      "Iteration 1860 Training loss 0.07415597140789032 Validation loss 0.07254046946763992 Accuracy 0.8011250495910645\n",
      "Output tensor([[0.9769],\n",
      "        [0.5129]], device='mps:0')\n",
      "Iteration 1870 Training loss 0.07792697846889496 Validation loss 0.07258838415145874 Accuracy 0.8012500405311584\n",
      "Output tensor([[0.7990],\n",
      "        [0.1114]], device='mps:0')\n",
      "Iteration 1880 Training loss 0.07243184000253677 Validation loss 0.07249782234430313 Accuracy 0.8022500276565552\n",
      "Output tensor([[0.7172],\n",
      "        [0.2731]], device='mps:0')\n",
      "Iteration 1890 Training loss 0.07355731725692749 Validation loss 0.0724860206246376 Accuracy 0.8023750185966492\n",
      "Output tensor([[0.2290],\n",
      "        [0.3252]], device='mps:0')\n",
      "Iteration 1900 Training loss 0.08286876231431961 Validation loss 0.07262718677520752 Accuracy 0.8011250495910645\n",
      "Output tensor([[0.1795],\n",
      "        [0.9428]], device='mps:0')\n",
      "Iteration 1910 Training loss 0.06716727465391159 Validation loss 0.07248052954673767 Accuracy 0.8015000224113464\n",
      "Output tensor([[0.2473],\n",
      "        [0.3755]], device='mps:0')\n",
      "Iteration 1920 Training loss 0.0672885999083519 Validation loss 0.07243771106004715 Accuracy 0.8021250367164612\n",
      "Output tensor([[0.6071],\n",
      "        [0.0147]], device='mps:0')\n",
      "Iteration 1930 Training loss 0.07655434310436249 Validation loss 0.07241974025964737 Accuracy 0.8025000095367432\n",
      "Output tensor([[0.9874],\n",
      "        [0.9658]], device='mps:0')\n",
      "Iteration 1940 Training loss 0.06949544697999954 Validation loss 0.07241151481866837 Accuracy 0.8022500276565552\n",
      "Output tensor([[0.1667],\n",
      "        [0.7397]], device='mps:0')\n",
      "Iteration 1950 Training loss 0.08143427968025208 Validation loss 0.07259028404951096 Accuracy 0.8008750677108765\n",
      "Output tensor([[0.8293],\n",
      "        [0.9718]], device='mps:0')\n",
      "Iteration 1960 Training loss 0.06598913669586182 Validation loss 0.07250649482011795 Accuracy 0.8011250495910645\n",
      "Output tensor([[0.1528],\n",
      "        [0.7671]], device='mps:0')\n",
      "Iteration 1970 Training loss 0.07687385380268097 Validation loss 0.07234031707048416 Accuracy 0.8020000457763672\n",
      "Output tensor([[0.2621],\n",
      "        [0.9362]], device='mps:0')\n",
      "Iteration 1980 Training loss 0.06862354278564453 Validation loss 0.07253071665763855 Accuracy 0.8006250262260437\n",
      "Output tensor([[0.4077],\n",
      "        [0.6950]], device='mps:0')\n",
      "Iteration 1990 Training loss 0.0716014876961708 Validation loss 0.072740338742733 Accuracy 0.799875020980835\n",
      "Output tensor([[0.9341],\n",
      "        [0.9753]], device='mps:0')\n",
      "Iteration 2000 Training loss 0.07359746843576431 Validation loss 0.0723729133605957 Accuracy 0.8008750677108765\n",
      "Output tensor([[0.8785],\n",
      "        [0.3741]], device='mps:0')\n",
      "Iteration 2010 Training loss 0.08380819857120514 Validation loss 0.07225483655929565 Accuracy 0.8025000095367432\n",
      "Output tensor([[0.1568],\n",
      "        [0.1416]], device='mps:0')\n",
      "Iteration 2020 Training loss 0.06555306911468506 Validation loss 0.07235807925462723 Accuracy 0.8006250262260437\n",
      "Output tensor([[0.1153],\n",
      "        [0.0725]], device='mps:0')\n",
      "Iteration 2030 Training loss 0.06758779287338257 Validation loss 0.07223226875066757 Accuracy 0.8022500276565552\n",
      "Output tensor([[0.4995],\n",
      "        [0.1973]], device='mps:0')\n",
      "Iteration 2040 Training loss 0.06881046295166016 Validation loss 0.07220775634050369 Accuracy 0.8030000329017639\n",
      "Output tensor([[0.9311],\n",
      "        [0.6070]], device='mps:0')\n",
      "Iteration 2050 Training loss 0.06673603504896164 Validation loss 0.07221227139234543 Accuracy 0.8026250600814819\n",
      "Output tensor([[0.1802],\n",
      "        [0.6273]], device='mps:0')\n",
      "Iteration 2060 Training loss 0.06502378731966019 Validation loss 0.07217706739902496 Accuracy 0.8033750653266907\n",
      "Output tensor([[0.1220],\n",
      "        [0.1118]], device='mps:0')\n",
      "Iteration 2070 Training loss 0.06799276918172836 Validation loss 0.07219880819320679 Accuracy 0.8023750185966492\n",
      "Output tensor([[0.0690],\n",
      "        [0.5871]], device='mps:0')\n",
      "Iteration 2080 Training loss 0.06628093868494034 Validation loss 0.07215233147144318 Accuracy 0.8030000329017639\n",
      "Output tensor([[0.2411],\n",
      "        [0.3364]], device='mps:0')\n",
      "Iteration 2090 Training loss 0.0722719207406044 Validation loss 0.07213374972343445 Accuracy 0.8026250600814819\n",
      "Output tensor([[0.4918],\n",
      "        [0.7430]], device='mps:0')\n",
      "Iteration 2100 Training loss 0.07173425704240799 Validation loss 0.07232402265071869 Accuracy 0.8010000586509705\n",
      "Output tensor([[0.7099],\n",
      "        [0.0259]], device='mps:0')\n",
      "Iteration 2110 Training loss 0.0763189047574997 Validation loss 0.07208991795778275 Accuracy 0.8035000562667847\n",
      "Output tensor([[0.0807],\n",
      "        [0.8247]], device='mps:0')\n",
      "Iteration 2120 Training loss 0.07224269956350327 Validation loss 0.07208720594644547 Accuracy 0.8028750419616699\n",
      "Output tensor([[0.9231],\n",
      "        [0.4422]], device='mps:0')\n",
      "Iteration 2130 Training loss 0.07099595665931702 Validation loss 0.07205027341842651 Accuracy 0.8031250238418579\n",
      "Output tensor([[0.8330],\n",
      "        [0.1221]], device='mps:0')\n",
      "Iteration 2140 Training loss 0.07551346719264984 Validation loss 0.07220204919576645 Accuracy 0.8022500276565552\n",
      "Output tensor([[0.3377],\n",
      "        [0.2635]], device='mps:0')\n",
      "Iteration 2150 Training loss 0.07127164304256439 Validation loss 0.07211974263191223 Accuracy 0.8023750185966492\n",
      "Output tensor([[0.5088],\n",
      "        [0.9865]], device='mps:0')\n",
      "Iteration 2160 Training loss 0.08003732562065125 Validation loss 0.07200619578361511 Accuracy 0.8035000562667847\n",
      "Output tensor([[0.1596],\n",
      "        [0.9881]], device='mps:0')\n",
      "Iteration 2170 Training loss 0.07671540230512619 Validation loss 0.07197720557451248 Accuracy 0.8040000200271606\n",
      "Output tensor([[0.9039],\n",
      "        [0.1327]], device='mps:0')\n",
      "Iteration 2180 Training loss 0.07381930947303772 Validation loss 0.07197321206331253 Accuracy 0.8037500381469727\n",
      "Output tensor([[0.0453],\n",
      "        [0.9732]], device='mps:0')\n",
      "Iteration 2190 Training loss 0.06595980376005173 Validation loss 0.07196877896785736 Accuracy 0.8037500381469727\n",
      "Output tensor([[0.5209],\n",
      "        [0.2024]], device='mps:0')\n",
      "Iteration 2200 Training loss 0.07599250972270966 Validation loss 0.07192344963550568 Accuracy 0.8038750290870667\n",
      "Output tensor([[0.3539],\n",
      "        [0.5073]], device='mps:0')\n",
      "Iteration 2210 Training loss 0.07663498818874359 Validation loss 0.07245147228240967 Accuracy 0.8011250495910645\n",
      "Output tensor([[0.2046],\n",
      "        [0.4824]], device='mps:0')\n",
      "Iteration 2220 Training loss 0.06670481711626053 Validation loss 0.07189071923494339 Accuracy 0.8038750290870667\n",
      "Output tensor([[0.3797],\n",
      "        [0.2759]], device='mps:0')\n",
      "Iteration 2230 Training loss 0.07115988433361053 Validation loss 0.072130486369133 Accuracy 0.8031250238418579\n",
      "Output tensor([[0.8274],\n",
      "        [0.0293]], device='mps:0')\n",
      "Iteration 2240 Training loss 0.07393074035644531 Validation loss 0.07186691462993622 Accuracy 0.8038750290870667\n",
      "Output tensor([[0.8212],\n",
      "        [0.8249]], device='mps:0')\n",
      "Iteration 2250 Training loss 0.06855104118585587 Validation loss 0.07198727875947952 Accuracy 0.8021250367164612\n",
      "Output tensor([[0.1656],\n",
      "        [0.1306]], device='mps:0')\n",
      "Iteration 2260 Training loss 0.07558076828718185 Validation loss 0.07181980460882187 Accuracy 0.8042500615119934\n",
      "Output tensor([[0.2081],\n",
      "        [0.8909]], device='mps:0')\n",
      "Iteration 2270 Training loss 0.077248215675354 Validation loss 0.07183792442083359 Accuracy 0.8035000562667847\n",
      "Output tensor([[0.8464],\n",
      "        [0.1318]], device='mps:0')\n",
      "Iteration 2280 Training loss 0.06249658390879631 Validation loss 0.07179823517799377 Accuracy 0.8036250472068787\n",
      "Output tensor([[0.1797],\n",
      "        [0.4301]], device='mps:0')\n",
      "Iteration 2290 Training loss 0.07072269916534424 Validation loss 0.07179538905620575 Accuracy 0.8041250109672546\n",
      "Output tensor([[0.0133],\n",
      "        [0.9838]], device='mps:0')\n",
      "Iteration 2300 Training loss 0.07614535838365555 Validation loss 0.07188230752944946 Accuracy 0.8030000329017639\n",
      "Output tensor([[0.5492],\n",
      "        [0.7090]], device='mps:0')\n",
      "Iteration 2310 Training loss 0.0741608664393425 Validation loss 0.07275976985692978 Accuracy 0.7975000143051147\n",
      "Output tensor([[0.5342],\n",
      "        [0.0337]], device='mps:0')\n",
      "Iteration 2320 Training loss 0.0660940483212471 Validation loss 0.07179389894008636 Accuracy 0.8037500381469727\n",
      "Output tensor([[0.0119],\n",
      "        [0.1307]], device='mps:0')\n",
      "Iteration 2330 Training loss 0.06735806912183762 Validation loss 0.07173309475183487 Accuracy 0.8046250343322754\n",
      "Output tensor([[0.0245],\n",
      "        [0.9133]], device='mps:0')\n",
      "Iteration 2340 Training loss 0.07414866983890533 Validation loss 0.07170889526605606 Accuracy 0.8051250576972961\n",
      "Output tensor([[0.2037],\n",
      "        [0.7252]], device='mps:0')\n",
      "Iteration 2350 Training loss 0.07069145888090134 Validation loss 0.07169586420059204 Accuracy 0.8036250472068787\n",
      "Output tensor([[0.7006],\n",
      "        [0.8746]], device='mps:0')\n",
      "Iteration 2360 Training loss 0.06527034938335419 Validation loss 0.07176956534385681 Accuracy 0.8037500381469727\n",
      "Output tensor([[0.0427],\n",
      "        [0.1316]], device='mps:0')\n",
      "Iteration 2370 Training loss 0.06141938641667366 Validation loss 0.07194595783948898 Accuracy 0.8030000329017639\n",
      "Output tensor([[0.5558],\n",
      "        [0.4020]], device='mps:0')\n",
      "Iteration 2380 Training loss 0.0776265487074852 Validation loss 0.07164912670850754 Accuracy 0.8040000200271606\n",
      "Output tensor([[0.9160],\n",
      "        [0.4158]], device='mps:0')\n",
      "Iteration 2390 Training loss 0.08168601989746094 Validation loss 0.07163447886705399 Accuracy 0.8040000200271606\n",
      "Output tensor([[0.4071],\n",
      "        [0.1338]], device='mps:0')\n",
      "Iteration 2400 Training loss 0.07130150496959686 Validation loss 0.07174161076545715 Accuracy 0.8038750290870667\n",
      "Output tensor([[0.2583],\n",
      "        [0.5823]], device='mps:0')\n",
      "Iteration 2410 Training loss 0.07817397266626358 Validation loss 0.07167835533618927 Accuracy 0.8036250472068787\n",
      "Output tensor([[0.4321],\n",
      "        [0.7529]], device='mps:0')\n",
      "Iteration 2420 Training loss 0.06964628398418427 Validation loss 0.07159782201051712 Accuracy 0.8056250214576721\n",
      "Output tensor([[0.2491],\n",
      "        [0.3976]], device='mps:0')\n",
      "Iteration 2430 Training loss 0.0736565813422203 Validation loss 0.07172886282205582 Accuracy 0.8033750653266907\n",
      "Output tensor([[0.9093],\n",
      "        [0.8072]], device='mps:0')\n",
      "Iteration 2440 Training loss 0.07637911289930344 Validation loss 0.07159146666526794 Accuracy 0.8056250214576721\n",
      "Output tensor([[0.2404],\n",
      "        [0.7395]], device='mps:0')\n",
      "Iteration 2450 Training loss 0.07434078305959702 Validation loss 0.07153984904289246 Accuracy 0.8051250576972961\n",
      "Output tensor([[0.3522],\n",
      "        [0.8903]], device='mps:0')\n",
      "Iteration 2460 Training loss 0.07503265887498856 Validation loss 0.07169460505247116 Accuracy 0.8030000329017639\n",
      "Output tensor([[0.5871],\n",
      "        [0.4347]], device='mps:0')\n",
      "Iteration 2470 Training loss 0.07198643684387207 Validation loss 0.07150354236364365 Accuracy 0.8056250214576721\n",
      "Output tensor([[0.8462],\n",
      "        [0.1798]], device='mps:0')\n",
      "Iteration 2480 Training loss 0.06729480624198914 Validation loss 0.07149217277765274 Accuracy 0.8056250214576721\n",
      "Output tensor([[0.0464],\n",
      "        [0.9619]], device='mps:0')\n",
      "Iteration 2490 Training loss 0.0703515112400055 Validation loss 0.0715518370270729 Accuracy 0.8046250343322754\n",
      "Output tensor([[0.5277],\n",
      "        [0.5408]], device='mps:0')\n",
      "Iteration 2500 Training loss 0.07166991382837296 Validation loss 0.07146841287612915 Accuracy 0.8053750395774841\n",
      "Output tensor([[0.8398],\n",
      "        [0.9031]], device='mps:0')\n",
      "Iteration 2510 Training loss 0.07385995984077454 Validation loss 0.07148875296115875 Accuracy 0.8058750629425049\n",
      "Output tensor([[0.9099],\n",
      "        [0.8866]], device='mps:0')\n",
      "Iteration 2520 Training loss 0.06750781834125519 Validation loss 0.07154541462659836 Accuracy 0.8048750162124634\n",
      "Output tensor([[0.2309],\n",
      "        [0.0330]], device='mps:0')\n",
      "Iteration 2530 Training loss 0.07362902909517288 Validation loss 0.0714312493801117 Accuracy 0.8060000538825989\n",
      "Output tensor([[0.5723],\n",
      "        [0.2650]], device='mps:0')\n",
      "Iteration 2540 Training loss 0.06118415296077728 Validation loss 0.07142217457294464 Accuracy 0.8046250343322754\n",
      "Output tensor([[0.9334],\n",
      "        [0.9545]], device='mps:0')\n",
      "Iteration 2550 Training loss 0.06857027113437653 Validation loss 0.07146511971950531 Accuracy 0.8053750395774841\n",
      "Output tensor([[0.5319],\n",
      "        [0.9030]], device='mps:0')\n",
      "Iteration 2560 Training loss 0.07552061229944229 Validation loss 0.07149624824523926 Accuracy 0.8040000200271606\n",
      "Output tensor([[0.3741],\n",
      "        [0.2654]], device='mps:0')\n",
      "Iteration 2570 Training loss 0.07243335247039795 Validation loss 0.07147352397441864 Accuracy 0.8046250343322754\n",
      "Output tensor([[0.3269],\n",
      "        [0.6316]], device='mps:0')\n",
      "Iteration 2580 Training loss 0.07190253585577011 Validation loss 0.0713723823428154 Accuracy 0.8063750267028809\n",
      "Output tensor([[0.0316],\n",
      "        [0.5377]], device='mps:0')\n",
      "Iteration 2590 Training loss 0.06363718956708908 Validation loss 0.07134630531072617 Accuracy 0.8057500123977661\n",
      "Output tensor([[0.0429],\n",
      "        [0.2227]], device='mps:0')\n",
      "Iteration 2600 Training loss 0.07393930107355118 Validation loss 0.07159405946731567 Accuracy 0.8051250576972961\n",
      "Output tensor([[0.6008],\n",
      "        [0.1594]], device='mps:0')\n",
      "Iteration 2610 Training loss 0.06496591866016388 Validation loss 0.07135798037052155 Accuracy 0.8055000305175781\n",
      "Output tensor([[0.3771],\n",
      "        [0.7803]], device='mps:0')\n",
      "Iteration 2620 Training loss 0.0686587542295456 Validation loss 0.07133272290229797 Accuracy 0.8060000538825989\n",
      "Output tensor([[0.7099],\n",
      "        [0.1764]], device='mps:0')\n",
      "Iteration 2630 Training loss 0.06645798683166504 Validation loss 0.07128969579935074 Accuracy 0.8053750395774841\n",
      "Output tensor([[0.3367],\n",
      "        [0.9821]], device='mps:0')\n",
      "Iteration 2640 Training loss 0.07421871274709702 Validation loss 0.07127745449542999 Accuracy 0.8048750162124634\n",
      "Output tensor([[0.2823],\n",
      "        [0.9303]], device='mps:0')\n",
      "Iteration 2650 Training loss 0.07204084098339081 Validation loss 0.0713605210185051 Accuracy 0.8060000538825989\n",
      "Output tensor([[0.0531],\n",
      "        [0.8900]], device='mps:0')\n",
      "Iteration 2660 Training loss 0.07595133781433105 Validation loss 0.0713159441947937 Accuracy 0.8065000176429749\n",
      "Output tensor([[0.6974],\n",
      "        [0.7684]], device='mps:0')\n",
      "Iteration 2670 Training loss 0.07427763938903809 Validation loss 0.07128606736660004 Accuracy 0.8052500486373901\n",
      "Output tensor([[0.1416],\n",
      "        [0.1436]], device='mps:0')\n",
      "Iteration 2680 Training loss 0.0811435878276825 Validation loss 0.0712578296661377 Accuracy 0.8073750138282776\n",
      "Output tensor([[0.0534],\n",
      "        [0.1820]], device='mps:0')\n",
      "Iteration 2690 Training loss 0.06831523030996323 Validation loss 0.07121577113866806 Accuracy 0.8062500357627869\n",
      "Output tensor([[0.0675],\n",
      "        [0.5173]], device='mps:0')\n",
      "Iteration 2700 Training loss 0.07230356335639954 Validation loss 0.07130254060029984 Accuracy 0.8068750500679016\n",
      "Output tensor([[0.7369],\n",
      "        [0.2330]], device='mps:0')\n",
      "Iteration 2710 Training loss 0.07397595047950745 Validation loss 0.07120189070701599 Accuracy 0.8071250319480896\n",
      "Output tensor([[0.0199],\n",
      "        [0.8447]], device='mps:0')\n",
      "Iteration 2720 Training loss 0.06183086708188057 Validation loss 0.071303591132164 Accuracy 0.8071250319480896\n",
      "Output tensor([[0.0783],\n",
      "        [0.9714]], device='mps:0')\n",
      "Iteration 2730 Training loss 0.06775423139333725 Validation loss 0.07115626335144043 Accuracy 0.8067500591278076\n",
      "Output tensor([[0.1826],\n",
      "        [0.8825]], device='mps:0')\n",
      "Iteration 2740 Training loss 0.07076858729124069 Validation loss 0.07114788144826889 Accuracy 0.8053750395774841\n",
      "Output tensor([[0.0968],\n",
      "        [0.1410]], device='mps:0')\n",
      "Iteration 2750 Training loss 0.07376763969659805 Validation loss 0.07116300612688065 Accuracy 0.8077500462532043\n",
      "Output tensor([[0.2285],\n",
      "        [0.9458]], device='mps:0')\n",
      "Iteration 2760 Training loss 0.07756669074296951 Validation loss 0.07109813392162323 Accuracy 0.8065000176429749\n",
      "Output tensor([[0.3249],\n",
      "        [0.3862]], device='mps:0')\n",
      "Iteration 2770 Training loss 0.0758247971534729 Validation loss 0.07109571993350983 Accuracy 0.8072500228881836\n",
      "Output tensor([[0.6302],\n",
      "        [0.9101]], device='mps:0')\n",
      "Iteration 2780 Training loss 0.06919430941343307 Validation loss 0.07116241753101349 Accuracy 0.8050000667572021\n",
      "Output tensor([[0.9798],\n",
      "        [0.8833]], device='mps:0')\n",
      "Iteration 2790 Training loss 0.06526436656713486 Validation loss 0.07105408608913422 Accuracy 0.8063750267028809\n",
      "Output tensor([[0.2684],\n",
      "        [0.6111]], device='mps:0')\n",
      "Iteration 2800 Training loss 0.06380506604909897 Validation loss 0.07108652591705322 Accuracy 0.8076250553131104\n",
      "Output tensor([[0.3051],\n",
      "        [0.5645]], device='mps:0')\n",
      "Iteration 2810 Training loss 0.0685834139585495 Validation loss 0.07107797265052795 Accuracy 0.8047500252723694\n",
      "Output tensor([[0.9603],\n",
      "        [0.8520]], device='mps:0')\n",
      "Iteration 2820 Training loss 0.06876394897699356 Validation loss 0.07124766707420349 Accuracy 0.8053750395774841\n",
      "Output tensor([[0.5342],\n",
      "        [0.7843]], device='mps:0')\n",
      "Iteration 2830 Training loss 0.06644400209188461 Validation loss 0.0710667222738266 Accuracy 0.8042500615119934\n",
      "Output tensor([[0.3377],\n",
      "        [0.8961]], device='mps:0')\n",
      "Iteration 2840 Training loss 0.07110892981290817 Validation loss 0.0710354894399643 Accuracy 0.8082500100135803\n",
      "Output tensor([[0.4044],\n",
      "        [0.0900]], device='mps:0')\n",
      "Iteration 2850 Training loss 0.07100515812635422 Validation loss 0.0710025206208229 Accuracy 0.8077500462532043\n",
      "Output tensor([[0.0628],\n",
      "        [0.9508]], device='mps:0')\n",
      "Iteration 2860 Training loss 0.07580307871103287 Validation loss 0.0709904208779335 Accuracy 0.8058750629425049\n",
      "Output tensor([[0.9366],\n",
      "        [0.5278]], device='mps:0')\n",
      "Iteration 2870 Training loss 0.0767572745680809 Validation loss 0.07139905542135239 Accuracy 0.8043750524520874\n",
      "Output tensor([[0.1393],\n",
      "        [0.9415]], device='mps:0')\n",
      "Iteration 2880 Training loss 0.061707753688097 Validation loss 0.07098599523305893 Accuracy 0.8076250553131104\n",
      "Output tensor([[0.4488],\n",
      "        [0.7540]], device='mps:0')\n",
      "Iteration 2890 Training loss 0.06771501898765564 Validation loss 0.0709393173456192 Accuracy 0.8071250319480896\n",
      "Output tensor([[0.9527],\n",
      "        [0.8104]], device='mps:0')\n",
      "Iteration 2900 Training loss 0.06469161808490753 Validation loss 0.07092663645744324 Accuracy 0.8076250553131104\n",
      "Output tensor([[0.2603],\n",
      "        [0.2380]], device='mps:0')\n",
      "Iteration 2910 Training loss 0.07202520966529846 Validation loss 0.07091757655143738 Accuracy 0.8060000538825989\n",
      "Output tensor([[0.3114],\n",
      "        [0.6854]], device='mps:0')\n",
      "Iteration 2920 Training loss 0.06889822334051132 Validation loss 0.07089226692914963 Accuracy 0.8063750267028809\n",
      "Output tensor([[0.8347],\n",
      "        [0.0157]], device='mps:0')\n",
      "Iteration 2930 Training loss 0.0740799605846405 Validation loss 0.07094404846429825 Accuracy 0.8051250576972961\n",
      "Output tensor([[0.0671],\n",
      "        [0.0983]], device='mps:0')\n",
      "Iteration 2940 Training loss 0.07032223045825958 Validation loss 0.07095524668693542 Accuracy 0.8051250576972961\n",
      "Output tensor([[0.3708],\n",
      "        [0.8771]], device='mps:0')\n",
      "Iteration 2950 Training loss 0.07193545252084732 Validation loss 0.07091383635997772 Accuracy 0.8047500252723694\n",
      "Output tensor([[0.7633],\n",
      "        [0.8908]], device='mps:0')\n",
      "Iteration 2960 Training loss 0.06423471122980118 Validation loss 0.07086645066738129 Accuracy 0.8055000305175781\n",
      "Output tensor([[0.1513],\n",
      "        [0.3500]], device='mps:0')\n",
      "Iteration 2970 Training loss 0.07194217294454575 Validation loss 0.07133404910564423 Accuracy 0.8042500615119934\n",
      "Output tensor([[0.5141],\n",
      "        [0.6612]], device='mps:0')\n",
      "Iteration 2980 Training loss 0.06822986155748367 Validation loss 0.07110006362199783 Accuracy 0.8033750653266907\n",
      "Output tensor([[0.8045],\n",
      "        [0.0694]], device='mps:0')\n",
      "Iteration 2990 Training loss 0.07414422184228897 Validation loss 0.07082897424697876 Accuracy 0.8080000281333923\n",
      "Output tensor([[0.3397],\n",
      "        [0.9459]], device='mps:0')\n",
      "Iteration 3000 Training loss 0.06551699340343475 Validation loss 0.07089019566774368 Accuracy 0.8080000281333923\n",
      "Output tensor([[0.7885],\n",
      "        [0.8648]], device='mps:0')\n",
      "Iteration 3010 Training loss 0.0705757737159729 Validation loss 0.07128410786390305 Accuracy 0.8037500381469727\n",
      "Output tensor([[0.8541],\n",
      "        [0.2781]], device='mps:0')\n",
      "Iteration 3020 Training loss 0.06438671797513962 Validation loss 0.070790134370327 Accuracy 0.8057500123977661\n",
      "Output tensor([[0.4727],\n",
      "        [0.7263]], device='mps:0')\n",
      "Iteration 3030 Training loss 0.07329613715410233 Validation loss 0.07075882703065872 Accuracy 0.8086250424385071\n",
      "Output tensor([[0.3155],\n",
      "        [0.6673]], device='mps:0')\n",
      "Iteration 3040 Training loss 0.06796514242887497 Validation loss 0.07079683244228363 Accuracy 0.8081250190734863\n",
      "Output tensor([[0.2065],\n",
      "        [0.2618]], device='mps:0')\n",
      "Iteration 3050 Training loss 0.07314956188201904 Validation loss 0.07117786258459091 Accuracy 0.8045000433921814\n",
      "Output tensor([[0.9881],\n",
      "        [0.1335]], device='mps:0')\n",
      "Iteration 3060 Training loss 0.06945326179265976 Validation loss 0.07071353495121002 Accuracy 0.8081250190734863\n",
      "Output tensor([[0.5971],\n",
      "        [0.6962]], device='mps:0')\n",
      "Iteration 3070 Training loss 0.06511218100786209 Validation loss 0.0707000195980072 Accuracy 0.8078750371932983\n",
      "Output tensor([[0.0231],\n",
      "        [0.0691]], device='mps:0')\n",
      "Iteration 3080 Training loss 0.07990111410617828 Validation loss 0.07089799642562866 Accuracy 0.8063750267028809\n",
      "Output tensor([[0.3152],\n",
      "        [0.3336]], device='mps:0')\n",
      "Iteration 3090 Training loss 0.0648021325469017 Validation loss 0.07066750526428223 Accuracy 0.8078750371932983\n",
      "Output tensor([[0.8753],\n",
      "        [0.6054]], device='mps:0')\n",
      "Iteration 3100 Training loss 0.06975910067558289 Validation loss 0.07089059799909592 Accuracy 0.8061250448226929\n",
      "Output tensor([[0.7684],\n",
      "        [0.2991]], device='mps:0')\n",
      "Iteration 3110 Training loss 0.06136540323495865 Validation loss 0.07088290899991989 Accuracy 0.8045000433921814\n",
      "Output tensor([[0.1576],\n",
      "        [0.8437]], device='mps:0')\n",
      "Iteration 3120 Training loss 0.06893758475780487 Validation loss 0.07069242000579834 Accuracy 0.8080000281333923\n",
      "Output tensor([[0.8096],\n",
      "        [0.8963]], device='mps:0')\n",
      "Iteration 3130 Training loss 0.07572164386510849 Validation loss 0.07071841508150101 Accuracy 0.8048750162124634\n",
      "Output tensor([[0.8851],\n",
      "        [0.1053]], device='mps:0')\n",
      "Iteration 3140 Training loss 0.06620427221059799 Validation loss 0.07061287015676498 Accuracy 0.8060000538825989\n",
      "Output tensor([[0.0767],\n",
      "        [0.1094]], device='mps:0')\n",
      "Iteration 3150 Training loss 0.06712035089731216 Validation loss 0.0706249326467514 Accuracy 0.8082500100135803\n",
      "Output tensor([[0.4669],\n",
      "        [0.5920]], device='mps:0')\n",
      "Iteration 3160 Training loss 0.07224144786596298 Validation loss 0.07058887183666229 Accuracy 0.8080000281333923\n",
      "Output tensor([[0.0544],\n",
      "        [0.6978]], device='mps:0')\n",
      "Iteration 3170 Training loss 0.08546144515275955 Validation loss 0.07066244632005692 Accuracy 0.8088750243186951\n",
      "Output tensor([[0.9726],\n",
      "        [0.0414]], device='mps:0')\n",
      "Iteration 3180 Training loss 0.066429004073143 Validation loss 0.07055985182523727 Accuracy 0.8083750605583191\n",
      "Output tensor([[0.3650],\n",
      "        [0.3363]], device='mps:0')\n",
      "Iteration 3190 Training loss 0.06251160800457001 Validation loss 0.07059977948665619 Accuracy 0.8085000514984131\n",
      "Output tensor([[0.2387],\n",
      "        [0.8165]], device='mps:0')\n",
      "Iteration 3200 Training loss 0.0684080645442009 Validation loss 0.07053936272859573 Accuracy 0.8083750605583191\n",
      "Output tensor([[0.8089],\n",
      "        [0.6889]], device='mps:0')\n",
      "Iteration 3210 Training loss 0.06133577227592468 Validation loss 0.07055580615997314 Accuracy 0.8085000514984131\n",
      "Output tensor([[0.7184],\n",
      "        [0.8071]], device='mps:0')\n",
      "Iteration 3220 Training loss 0.07132487744092941 Validation loss 0.07065355032682419 Accuracy 0.8082500100135803\n",
      "Output tensor([[0.9797],\n",
      "        [0.3848]], device='mps:0')\n",
      "Iteration 3230 Training loss 0.06384182721376419 Validation loss 0.07050717622041702 Accuracy 0.8090000152587891\n",
      "Output tensor([[0.5243],\n",
      "        [0.2279]], device='mps:0')\n",
      "Iteration 3240 Training loss 0.07066420465707779 Validation loss 0.07047789543867111 Accuracy 0.8076250553131104\n",
      "Output tensor([[0.8589],\n",
      "        [0.4415]], device='mps:0')\n",
      "Iteration 3250 Training loss 0.06597915291786194 Validation loss 0.07078702002763748 Accuracy 0.8041250109672546\n",
      "Output tensor([[0.8436],\n",
      "        [0.4876]], device='mps:0')\n",
      "Iteration 3260 Training loss 0.0685199499130249 Validation loss 0.0704822987318039 Accuracy 0.8091250658035278\n",
      "Output tensor([[0.9883],\n",
      "        [0.2230]], device='mps:0')\n",
      "Iteration 3270 Training loss 0.06919887661933899 Validation loss 0.0704551488161087 Accuracy 0.8072500228881836\n",
      "Output tensor([[0.1041],\n",
      "        [0.1547]], device='mps:0')\n",
      "Iteration 3280 Training loss 0.06909984350204468 Validation loss 0.07045384496450424 Accuracy 0.8090000152587891\n",
      "Output tensor([[0.8296],\n",
      "        [0.9741]], device='mps:0')\n",
      "Iteration 3290 Training loss 0.060503702610731125 Validation loss 0.07044461369514465 Accuracy 0.8068750500679016\n",
      "Output tensor([[0.1896],\n",
      "        [0.1165]], device='mps:0')\n",
      "Iteration 3300 Training loss 0.07081598043441772 Validation loss 0.07049982249736786 Accuracy 0.8088750243186951\n",
      "Output tensor([[0.9567],\n",
      "        [0.6456]], device='mps:0')\n",
      "Iteration 3310 Training loss 0.06436294317245483 Validation loss 0.07041283696889877 Accuracy 0.8092500567436218\n",
      "Output tensor([[0.8993],\n",
      "        [0.2421]], device='mps:0')\n",
      "Iteration 3320 Training loss 0.0646803006529808 Validation loss 0.07052600383758545 Accuracy 0.8087500333786011\n",
      "Output tensor([[0.8390],\n",
      "        [0.5548]], device='mps:0')\n",
      "Iteration 3330 Training loss 0.06543446332216263 Validation loss 0.07039324194192886 Accuracy 0.8070000410079956\n",
      "Output tensor([[0.9151],\n",
      "        [0.1349]], device='mps:0')\n",
      "Iteration 3340 Training loss 0.07314196974039078 Validation loss 0.07036103308200836 Accuracy 0.8077500462532043\n",
      "Output tensor([[0.0557],\n",
      "        [0.0591]], device='mps:0')\n",
      "Iteration 3350 Training loss 0.07295747101306915 Validation loss 0.07036088407039642 Accuracy 0.8090000152587891\n",
      "Output tensor([[0.6272],\n",
      "        [0.1465]], device='mps:0')\n",
      "Iteration 3360 Training loss 0.0745813176035881 Validation loss 0.07041297852993011 Accuracy 0.8090000152587891\n",
      "Output tensor([[0.0694],\n",
      "        [0.1767]], device='mps:0')\n",
      "Iteration 3370 Training loss 0.07101976126432419 Validation loss 0.07033269107341766 Accuracy 0.8081250190734863\n",
      "Output tensor([[0.9193],\n",
      "        [0.6508]], device='mps:0')\n",
      "Iteration 3380 Training loss 0.06322451680898666 Validation loss 0.07031932473182678 Accuracy 0.8080000281333923\n",
      "Output tensor([[0.1576],\n",
      "        [0.8074]], device='mps:0')\n",
      "Iteration 3390 Training loss 0.06689877063035965 Validation loss 0.07033274322748184 Accuracy 0.8070000410079956\n",
      "Output tensor([[0.5815],\n",
      "        [0.9428]], device='mps:0')\n",
      "Iteration 3400 Training loss 0.06920282542705536 Validation loss 0.0705682709813118 Accuracy 0.8075000643730164\n",
      "Output tensor([[0.8770],\n",
      "        [0.7272]], device='mps:0')\n",
      "Iteration 3410 Training loss 0.061718907207250595 Validation loss 0.07033275812864304 Accuracy 0.8095000386238098\n",
      "Output tensor([[0.8175],\n",
      "        [0.0766]], device='mps:0')\n",
      "Iteration 3420 Training loss 0.060463860630989075 Validation loss 0.07030656933784485 Accuracy 0.8097500205039978\n",
      "Output tensor([[0.6368],\n",
      "        [0.9684]], device='mps:0')\n",
      "Iteration 3430 Training loss 0.062382668256759644 Validation loss 0.07045403122901917 Accuracy 0.8057500123977661\n",
      "Output tensor([[0.2022],\n",
      "        [0.0238]], device='mps:0')\n",
      "Iteration 3440 Training loss 0.06345571577548981 Validation loss 0.07043715566396713 Accuracy 0.8077500462532043\n",
      "Output tensor([[0.7052],\n",
      "        [0.8007]], device='mps:0')\n",
      "Iteration 3450 Training loss 0.07294376939535141 Validation loss 0.07050668448209763 Accuracy 0.8043750524520874\n",
      "Output tensor([[0.2027],\n",
      "        [0.2720]], device='mps:0')\n",
      "Iteration 3460 Training loss 0.07604758441448212 Validation loss 0.07031752169132233 Accuracy 0.8061250448226929\n",
      "Output tensor([[0.5247],\n",
      "        [0.6495]], device='mps:0')\n",
      "Iteration 3470 Training loss 0.07175634056329727 Validation loss 0.0702868402004242 Accuracy 0.8101250529289246\n",
      "Output tensor([[0.1567],\n",
      "        [0.9706]], device='mps:0')\n",
      "Iteration 3480 Training loss 0.07239440083503723 Validation loss 0.07020091265439987 Accuracy 0.8090000152587891\n",
      "Output tensor([[0.7097],\n",
      "        [0.7066]], device='mps:0')\n",
      "Iteration 3490 Training loss 0.06844712048768997 Validation loss 0.07023218274116516 Accuracy 0.8095000386238098\n",
      "Output tensor([[0.9408],\n",
      "        [0.9239]], device='mps:0')\n",
      "Iteration 3500 Training loss 0.06950095295906067 Validation loss 0.07043056190013885 Accuracy 0.8076250553131104\n",
      "Output tensor([[0.3281],\n",
      "        [0.2734]], device='mps:0')\n",
      "Iteration 3510 Training loss 0.07394599914550781 Validation loss 0.07033725082874298 Accuracy 0.8061250448226929\n",
      "Output tensor([[0.0794],\n",
      "        [0.4865]], device='mps:0')\n",
      "Iteration 3520 Training loss 0.06716273725032806 Validation loss 0.07041136920452118 Accuracy 0.8076250553131104\n",
      "Output tensor([[0.7917],\n",
      "        [0.1800]], device='mps:0')\n",
      "Iteration 3530 Training loss 0.07465135306119919 Validation loss 0.07013338059186935 Accuracy 0.8093750476837158\n",
      "Output tensor([[0.8051],\n",
      "        [0.0240]], device='mps:0')\n",
      "Iteration 3540 Training loss 0.07051970809698105 Validation loss 0.07012098282575607 Accuracy 0.8100000619888306\n",
      "Output tensor([[0.8039],\n",
      "        [0.4286]], device='mps:0')\n",
      "Iteration 3550 Training loss 0.06558460742235184 Validation loss 0.07016492635011673 Accuracy 0.8102500438690186\n",
      "Output tensor([[0.7909],\n",
      "        [0.7436]], device='mps:0')\n",
      "Iteration 3560 Training loss 0.06879940629005432 Validation loss 0.07029330730438232 Accuracy 0.8060000538825989\n",
      "Output tensor([[0.0327],\n",
      "        [0.0261]], device='mps:0')\n",
      "Iteration 3570 Training loss 0.07350485771894455 Validation loss 0.0700969249010086 Accuracy 0.8078750371932983\n",
      "Output tensor([[0.4024],\n",
      "        [0.1457]], device='mps:0')\n",
      "Iteration 3580 Training loss 0.07056402415037155 Validation loss 0.07030410319566727 Accuracy 0.8056250214576721\n",
      "Output tensor([[0.1190],\n",
      "        [0.9111]], device='mps:0')\n",
      "Iteration 3590 Training loss 0.0597880594432354 Validation loss 0.07010498642921448 Accuracy 0.8103750348091125\n",
      "Output tensor([[0.7735],\n",
      "        [0.4664]], device='mps:0')\n",
      "Iteration 3600 Training loss 0.068499356508255 Validation loss 0.0701274424791336 Accuracy 0.8101250529289246\n",
      "Output tensor([[0.9611],\n",
      "        [0.7386]], device='mps:0')\n",
      "Iteration 3610 Training loss 0.07184390723705292 Validation loss 0.07018817216157913 Accuracy 0.8097500205039978\n",
      "Output tensor([[0.8090],\n",
      "        [0.7058]], device='mps:0')\n",
      "Iteration 3620 Training loss 0.0628676787018776 Validation loss 0.07008882611989975 Accuracy 0.8105000257492065\n",
      "Output tensor([[0.9322],\n",
      "        [0.0084]], device='mps:0')\n",
      "Iteration 3630 Training loss 0.08279279619455338 Validation loss 0.07001886516809464 Accuracy 0.8101250529289246\n",
      "Output tensor([[0.1286],\n",
      "        [0.4697]], device='mps:0')\n",
      "Iteration 3640 Training loss 0.07281660288572311 Validation loss 0.07060210406780243 Accuracy 0.8032500147819519\n",
      "Output tensor([[0.4056],\n",
      "        [0.9304]], device='mps:0')\n",
      "Iteration 3650 Training loss 0.07147545367479324 Validation loss 0.07018857449293137 Accuracy 0.8063750267028809\n",
      "Output tensor([[0.0611],\n",
      "        [0.4198]], device='mps:0')\n",
      "Iteration 3660 Training loss 0.0624348446726799 Validation loss 0.06999155133962631 Accuracy 0.8088750243186951\n",
      "Output tensor([[0.4659],\n",
      "        [0.9899]], device='mps:0')\n",
      "Iteration 3670 Training loss 0.0676848292350769 Validation loss 0.07010969519615173 Accuracy 0.8067500591278076\n",
      "Output tensor([[0.9023],\n",
      "        [0.0754]], device='mps:0')\n",
      "Iteration 3680 Training loss 0.07274601608514786 Validation loss 0.06998830288648605 Accuracy 0.8105000257492065\n",
      "Output tensor([[0.6717],\n",
      "        [0.9548]], device='mps:0')\n",
      "Iteration 3690 Training loss 0.07539137452840805 Validation loss 0.07000254839658737 Accuracy 0.8072500228881836\n",
      "Output tensor([[0.1040],\n",
      "        [0.3293]], device='mps:0')\n",
      "Iteration 3700 Training loss 0.07908783107995987 Validation loss 0.07025279849767685 Accuracy 0.8080000281333923\n",
      "Output tensor([[0.1718],\n",
      "        [0.9702]], device='mps:0')\n",
      "Iteration 3710 Training loss 0.065874382853508 Validation loss 0.07021362334489822 Accuracy 0.8080000281333923\n",
      "Output tensor([[0.5561],\n",
      "        [0.2745]], device='mps:0')\n",
      "Iteration 3720 Training loss 0.07747885584831238 Validation loss 0.06992046535015106 Accuracy 0.8107500672340393\n",
      "Output tensor([[0.6703],\n",
      "        [0.6084]], device='mps:0')\n",
      "Iteration 3730 Training loss 0.07521265000104904 Validation loss 0.0701303631067276 Accuracy 0.8058750629425049\n",
      "Output tensor([[0.1894],\n",
      "        [0.9966]], device='mps:0')\n",
      "Iteration 3740 Training loss 0.06955192238092422 Validation loss 0.0698915421962738 Accuracy 0.8093750476837158\n",
      "Output tensor([[0.8106],\n",
      "        [0.8532]], device='mps:0')\n",
      "Iteration 3750 Training loss 0.0790235623717308 Validation loss 0.06991709023714066 Accuracy 0.812000036239624\n",
      "Output tensor([[0.0240],\n",
      "        [0.6718]], device='mps:0')\n",
      "Iteration 3760 Training loss 0.053680237382650375 Validation loss 0.06988826394081116 Accuracy 0.8112500309944153\n",
      "Output tensor([[0.0788],\n",
      "        [0.6777]], device='mps:0')\n",
      "Iteration 3770 Training loss 0.07400567829608917 Validation loss 0.07009001821279526 Accuracy 0.8083750605583191\n",
      "Output tensor([[0.9048],\n",
      "        [0.3703]], device='mps:0')\n",
      "Iteration 3780 Training loss 0.07877127081155777 Validation loss 0.06999471038579941 Accuracy 0.8062500357627869\n",
      "Output tensor([[0.0135],\n",
      "        [0.7523]], device='mps:0')\n",
      "Iteration 3790 Training loss 0.06912098824977875 Validation loss 0.0701095461845398 Accuracy 0.8078750371932983\n",
      "Output tensor([[0.1742],\n",
      "        [0.4619]], device='mps:0')\n",
      "Iteration 3800 Training loss 0.07178409397602081 Validation loss 0.06985148787498474 Accuracy 0.8081250190734863\n",
      "Output tensor([[0.9060],\n",
      "        [0.0463]], device='mps:0')\n",
      "Iteration 3810 Training loss 0.06453912705183029 Validation loss 0.06993843615055084 Accuracy 0.8095000386238098\n",
      "Output tensor([[0.2047],\n",
      "        [0.9722]], device='mps:0')\n",
      "Iteration 3820 Training loss 0.06770751625299454 Validation loss 0.06980909407138824 Accuracy 0.8110000491142273\n",
      "Output tensor([[0.5201],\n",
      "        [0.3963]], device='mps:0')\n",
      "Iteration 3830 Training loss 0.07014592736959457 Validation loss 0.06987608969211578 Accuracy 0.8103750348091125\n",
      "Output tensor([[0.2455],\n",
      "        [0.9628]], device='mps:0')\n",
      "Iteration 3840 Training loss 0.06668354570865631 Validation loss 0.06978980451822281 Accuracy 0.8112500309944153\n",
      "Output tensor([[0.9885],\n",
      "        [0.3937]], device='mps:0')\n",
      "Iteration 3850 Training loss 0.06480726599693298 Validation loss 0.06982950866222382 Accuracy 0.8110000491142273\n",
      "Output tensor([[0.3340],\n",
      "        [0.2325]], device='mps:0')\n",
      "Iteration 3860 Training loss 0.07068988680839539 Validation loss 0.0697677955031395 Accuracy 0.8103750348091125\n",
      "Output tensor([[0.0710],\n",
      "        [0.8207]], device='mps:0')\n",
      "Iteration 3870 Training loss 0.07669511437416077 Validation loss 0.06978888809680939 Accuracy 0.8086250424385071\n",
      "Output tensor([[0.7654],\n",
      "        [0.7165]], device='mps:0')\n",
      "Iteration 3880 Training loss 0.06772340834140778 Validation loss 0.06983661651611328 Accuracy 0.8077500462532043\n",
      "Output tensor([[0.3534],\n",
      "        [0.2655]], device='mps:0')\n",
      "Iteration 3890 Training loss 0.0768037661910057 Validation loss 0.06979936361312866 Accuracy 0.8080000281333923\n",
      "Output tensor([[0.9555],\n",
      "        [0.8036]], device='mps:0')\n",
      "Iteration 3900 Training loss 0.07181525975465775 Validation loss 0.06978781521320343 Accuracy 0.8112500309944153\n",
      "Output tensor([[0.8057],\n",
      "        [0.3183]], device='mps:0')\n",
      "Iteration 3910 Training loss 0.06655895709991455 Validation loss 0.06982693821191788 Accuracy 0.8082500100135803\n",
      "Output tensor([[0.1222],\n",
      "        [0.7871]], device='mps:0')\n",
      "Iteration 3920 Training loss 0.07061523199081421 Validation loss 0.06973589956760406 Accuracy 0.8115000128746033\n",
      "Output tensor([[0.7821],\n",
      "        [0.9634]], device='mps:0')\n",
      "Iteration 3930 Training loss 0.06897558271884918 Validation loss 0.07004565745592117 Accuracy 0.8048750162124634\n",
      "Output tensor([[0.0319],\n",
      "        [0.8091]], device='mps:0')\n",
      "Iteration 3940 Training loss 0.07249247282743454 Validation loss 0.06984074413776398 Accuracy 0.8096250295639038\n",
      "Output tensor([[0.1300],\n",
      "        [0.3365]], device='mps:0')\n",
      "Iteration 3950 Training loss 0.07374938577413559 Validation loss 0.0697549507021904 Accuracy 0.8082500100135803\n",
      "Output tensor([[0.4089],\n",
      "        [0.2910]], device='mps:0')\n",
      "Iteration 3960 Training loss 0.0800996795296669 Validation loss 0.06967653334140778 Accuracy 0.811625063419342\n",
      "Output tensor([[0.9768],\n",
      "        [0.7506]], device='mps:0')\n",
      "Iteration 3970 Training loss 0.07023362070322037 Validation loss 0.0696636363863945 Accuracy 0.812000036239624\n",
      "Output tensor([[0.5132],\n",
      "        [0.0104]], device='mps:0')\n",
      "Iteration 3980 Training loss 0.06739343702793121 Validation loss 0.06964076310396194 Accuracy 0.8102500438690186\n",
      "Output tensor([[0.7619],\n",
      "        [0.2178]], device='mps:0')\n",
      "Iteration 3990 Training loss 0.06905310600996017 Validation loss 0.0696348026394844 Accuracy 0.8096250295639038\n",
      "Output tensor([[0.2688],\n",
      "        [0.6768]], device='mps:0')\n",
      "Iteration 4000 Training loss 0.06787636876106262 Validation loss 0.06983309239149094 Accuracy 0.8093750476837158\n",
      "Output tensor([[0.8930],\n",
      "        [0.3729]], device='mps:0')\n",
      "Iteration 4010 Training loss 0.06753703951835632 Validation loss 0.0696568489074707 Accuracy 0.8127500414848328\n",
      "Output tensor([[0.2310],\n",
      "        [0.8183]], device='mps:0')\n",
      "Iteration 4020 Training loss 0.060573797672986984 Validation loss 0.06961125880479813 Accuracy 0.811750054359436\n",
      "Output tensor([[0.7620],\n",
      "        [0.4754]], device='mps:0')\n",
      "Iteration 4030 Training loss 0.060749080032110214 Validation loss 0.06985806673765182 Accuracy 0.8063750267028809\n",
      "Output tensor([[0.8754],\n",
      "        [0.0909]], device='mps:0')\n",
      "Iteration 4040 Training loss 0.06516256928443909 Validation loss 0.069610096514225 Accuracy 0.8090000152587891\n",
      "Output tensor([[0.8521],\n",
      "        [0.0138]], device='mps:0')\n",
      "Iteration 4050 Training loss 0.07042454928159714 Validation loss 0.06961703300476074 Accuracy 0.8126250505447388\n",
      "Output tensor([[0.6293],\n",
      "        [0.8659]], device='mps:0')\n",
      "Iteration 4060 Training loss 0.0639047771692276 Validation loss 0.06977774202823639 Accuracy 0.8083750605583191\n",
      "Output tensor([[0.6282],\n",
      "        [0.6115]], device='mps:0')\n",
      "Iteration 4070 Training loss 0.06845320761203766 Validation loss 0.06973422318696976 Accuracy 0.8078750371932983\n",
      "Output tensor([[0.8038],\n",
      "        [0.1579]], device='mps:0')\n",
      "Iteration 4080 Training loss 0.07102962583303452 Validation loss 0.06961969286203384 Accuracy 0.81187504529953\n",
      "Output tensor([[0.8960],\n",
      "        [0.5874]], device='mps:0')\n",
      "Iteration 4090 Training loss 0.07065770775079727 Validation loss 0.06954440474510193 Accuracy 0.8103750348091125\n",
      "Output tensor([[0.4424],\n",
      "        [0.9127]], device='mps:0')\n",
      "Iteration 4100 Training loss 0.0754956305027008 Validation loss 0.06970888376235962 Accuracy 0.8076250553131104\n",
      "Output tensor([[0.8752],\n",
      "        [0.4954]], device='mps:0')\n",
      "Iteration 4110 Training loss 0.06802145391702652 Validation loss 0.06955024600028992 Accuracy 0.8095000386238098\n",
      "Output tensor([[0.0038],\n",
      "        [0.2427]], device='mps:0')\n",
      "Iteration 4120 Training loss 0.05738852545619011 Validation loss 0.06950940191745758 Accuracy 0.8113750219345093\n",
      "Output tensor([[0.0164],\n",
      "        [0.9944]], device='mps:0')\n",
      "Iteration 4130 Training loss 0.06926826387643814 Validation loss 0.0695611760020256 Accuracy 0.8088750243186951\n",
      "Output tensor([[0.9760],\n",
      "        [0.6777]], device='mps:0')\n",
      "Iteration 4140 Training loss 0.0730508491396904 Validation loss 0.06949162483215332 Accuracy 0.812125027179718\n",
      "Output tensor([[0.2908],\n",
      "        [0.0976]], device='mps:0')\n",
      "Iteration 4150 Training loss 0.0749523788690567 Validation loss 0.06949009001255035 Accuracy 0.8125000596046448\n",
      "Output tensor([[0.5404],\n",
      "        [0.1533]], device='mps:0')\n",
      "Iteration 4160 Training loss 0.06046466529369354 Validation loss 0.06948306411504745 Accuracy 0.8105000257492065\n",
      "Output tensor([[0.0764],\n",
      "        [0.2051]], device='mps:0')\n",
      "Iteration 4170 Training loss 0.06499927490949631 Validation loss 0.06956219673156738 Accuracy 0.8112500309944153\n",
      "Output tensor([[0.0509],\n",
      "        [0.2102]], device='mps:0')\n",
      "Iteration 4180 Training loss 0.07983644306659698 Validation loss 0.06970218569040298 Accuracy 0.8086250424385071\n",
      "Output tensor([[0.3766],\n",
      "        [0.9443]], device='mps:0')\n",
      "Iteration 4190 Training loss 0.06980478763580322 Validation loss 0.06987833231687546 Accuracy 0.8092500567436218\n",
      "Output tensor([[0.9350],\n",
      "        [0.9481]], device='mps:0')\n",
      "Iteration 4200 Training loss 0.06084871664643288 Validation loss 0.06942834705114365 Accuracy 0.8108750581741333\n",
      "Output tensor([[0.9421],\n",
      "        [0.9190]], device='mps:0')\n",
      "Iteration 4210 Training loss 0.06473953276872635 Validation loss 0.06941859424114227 Accuracy 0.812000036239624\n",
      "Output tensor([[0.8946],\n",
      "        [0.4068]], device='mps:0')\n",
      "Iteration 4220 Training loss 0.06478320807218552 Validation loss 0.06949368119239807 Accuracy 0.811750054359436\n",
      "Output tensor([[0.7063],\n",
      "        [0.0764]], device='mps:0')\n",
      "Iteration 4230 Training loss 0.06353990733623505 Validation loss 0.06942207366228104 Accuracy 0.8125000596046448\n",
      "Output tensor([[0.0295],\n",
      "        [0.1873]], device='mps:0')\n",
      "Iteration 4240 Training loss 0.06206068396568298 Validation loss 0.06985803693532944 Accuracy 0.8088750243186951\n",
      "Output tensor([[0.2775],\n",
      "        [0.2927]], device='mps:0')\n",
      "Iteration 4250 Training loss 0.06970357149839401 Validation loss 0.06937600672245026 Accuracy 0.811750054359436\n",
      "Output tensor([[0.0772],\n",
      "        [0.3664]], device='mps:0')\n",
      "Iteration 4260 Training loss 0.06898229569196701 Validation loss 0.0693797916173935 Accuracy 0.81187504529953\n",
      "Output tensor([[0.8079],\n",
      "        [0.0223]], device='mps:0')\n",
      "Iteration 4270 Training loss 0.07010224461555481 Validation loss 0.06942618638277054 Accuracy 0.81187504529953\n",
      "Output tensor([[0.8828],\n",
      "        [0.0641]], device='mps:0')\n",
      "Iteration 4280 Training loss 0.07310605049133301 Validation loss 0.06935857236385345 Accuracy 0.812000036239624\n",
      "Output tensor([[0.0673],\n",
      "        [0.9340]], device='mps:0')\n",
      "Iteration 4290 Training loss 0.0805559828877449 Validation loss 0.06952126324176788 Accuracy 0.8097500205039978\n",
      "Output tensor([[0.1048],\n",
      "        [0.0971]], device='mps:0')\n",
      "Iteration 4300 Training loss 0.06498314440250397 Validation loss 0.0694047063589096 Accuracy 0.811750054359436\n",
      "Output tensor([[0.0977],\n",
      "        [0.3122]], device='mps:0')\n",
      "Iteration 4310 Training loss 0.06196705251932144 Validation loss 0.06932840496301651 Accuracy 0.8115000128746033\n",
      "Output tensor([[0.2229],\n",
      "        [0.5873]], device='mps:0')\n",
      "Iteration 4320 Training loss 0.061059024184942245 Validation loss 0.06932616978883743 Accuracy 0.8113750219345093\n",
      "Output tensor([[0.2135],\n",
      "        [0.7781]], device='mps:0')\n",
      "Iteration 4330 Training loss 0.06813953071832657 Validation loss 0.0693664625287056 Accuracy 0.8105000257492065\n",
      "Output tensor([[0.2290],\n",
      "        [0.6543]], device='mps:0')\n",
      "Iteration 4340 Training loss 0.07007649540901184 Validation loss 0.06960868835449219 Accuracy 0.8098750114440918\n",
      "Output tensor([[0.0829],\n",
      "        [0.2239]], device='mps:0')\n",
      "Iteration 4350 Training loss 0.06242206320166588 Validation loss 0.06929074227809906 Accuracy 0.81187504529953\n",
      "Output tensor([[0.9210],\n",
      "        [0.7867]], device='mps:0')\n",
      "Iteration 4360 Training loss 0.07287147641181946 Validation loss 0.06931605190038681 Accuracy 0.812375009059906\n",
      "Output tensor([[0.9161],\n",
      "        [0.3942]], device='mps:0')\n",
      "Iteration 4370 Training loss 0.07866602391004562 Validation loss 0.06928123533725739 Accuracy 0.8113750219345093\n",
      "Output tensor([[0.8575],\n",
      "        [0.4630]], device='mps:0')\n",
      "Iteration 4380 Training loss 0.06309829652309418 Validation loss 0.06933456659317017 Accuracy 0.812125027179718\n",
      "Output tensor([[0.9405],\n",
      "        [0.8263]], device='mps:0')\n",
      "Iteration 4390 Training loss 0.07463063299655914 Validation loss 0.06924058496952057 Accuracy 0.811625063419342\n",
      "Output tensor([[0.9992],\n",
      "        [0.7036]], device='mps:0')\n",
      "Iteration 4400 Training loss 0.06944376975297928 Validation loss 0.06925340741872787 Accuracy 0.8125000596046448\n",
      "Output tensor([[0.9166],\n",
      "        [0.6023]], device='mps:0')\n",
      "Iteration 4410 Training loss 0.06410497426986694 Validation loss 0.06927765160799026 Accuracy 0.8107500672340393\n",
      "Output tensor([[0.9819],\n",
      "        [0.1201]], device='mps:0')\n",
      "Iteration 4420 Training loss 0.07446721196174622 Validation loss 0.06921275705099106 Accuracy 0.812125027179718\n",
      "Output tensor([[0.1571],\n",
      "        [0.6592]], device='mps:0')\n",
      "Iteration 4430 Training loss 0.070464126765728 Validation loss 0.06919733434915543 Accuracy 0.812375009059906\n",
      "Output tensor([[0.1424],\n",
      "        [0.7822]], device='mps:0')\n",
      "Iteration 4440 Training loss 0.07482650876045227 Validation loss 0.06933485716581345 Accuracy 0.8108750581741333\n",
      "Output tensor([[0.9446],\n",
      "        [0.6288]], device='mps:0')\n",
      "Iteration 4450 Training loss 0.0738920196890831 Validation loss 0.0691928118467331 Accuracy 0.8113750219345093\n",
      "Output tensor([[0.9515],\n",
      "        [0.6803]], device='mps:0')\n",
      "Iteration 4460 Training loss 0.06190400570631027 Validation loss 0.06919094920158386 Accuracy 0.8113750219345093\n",
      "Output tensor([[0.9463],\n",
      "        [0.0677]], device='mps:0')\n",
      "Iteration 4470 Training loss 0.07098954916000366 Validation loss 0.06938820332288742 Accuracy 0.8085000514984131\n",
      "Output tensor([[0.4301],\n",
      "        [0.1738]], device='mps:0')\n",
      "Iteration 4480 Training loss 0.06588497012853622 Validation loss 0.06931201368570328 Accuracy 0.8106250166893005\n",
      "Output tensor([[0.4534],\n",
      "        [0.9008]], device='mps:0')\n",
      "Iteration 4490 Training loss 0.06245305389165878 Validation loss 0.06915909051895142 Accuracy 0.811750054359436\n",
      "Output tensor([[0.3746],\n",
      "        [0.7818]], device='mps:0')\n",
      "Iteration 4500 Training loss 0.06862933188676834 Validation loss 0.06915359944105148 Accuracy 0.8125000596046448\n",
      "Output tensor([[0.1870],\n",
      "        [0.8454]], device='mps:0')\n",
      "Iteration 4510 Training loss 0.06604789942502975 Validation loss 0.06914329528808594 Accuracy 0.81187504529953\n",
      "Output tensor([[0.9528],\n",
      "        [0.9773]], device='mps:0')\n",
      "Iteration 4520 Training loss 0.05836757645010948 Validation loss 0.06935669481754303 Accuracy 0.8078750371932983\n",
      "Output tensor([[0.5907],\n",
      "        [0.2984]], device='mps:0')\n",
      "Iteration 4530 Training loss 0.06660987436771393 Validation loss 0.06912379711866379 Accuracy 0.812250018119812\n",
      "Output tensor([[0.9123],\n",
      "        [0.2546]], device='mps:0')\n",
      "Iteration 4540 Training loss 0.06754177063703537 Validation loss 0.06954500079154968 Accuracy 0.8096250295639038\n",
      "Output tensor([[0.7484],\n",
      "        [0.8142]], device='mps:0')\n",
      "Iteration 4550 Training loss 0.06371982395648956 Validation loss 0.06911280751228333 Accuracy 0.812250018119812\n",
      "Output tensor([[0.0484],\n",
      "        [0.0991]], device='mps:0')\n",
      "Iteration 4560 Training loss 0.06984636932611465 Validation loss 0.06910017132759094 Accuracy 0.8115000128746033\n",
      "Output tensor([[0.8420],\n",
      "        [0.0148]], device='mps:0')\n",
      "Iteration 4570 Training loss 0.07098746299743652 Validation loss 0.06908870488405228 Accuracy 0.8125000596046448\n",
      "Output tensor([[0.6301],\n",
      "        [0.2448]], device='mps:0')\n",
      "Iteration 4580 Training loss 0.060972221195697784 Validation loss 0.06915345788002014 Accuracy 0.8125000596046448\n",
      "Output tensor([[0.9481],\n",
      "        [0.6167]], device='mps:0')\n",
      "Iteration 4590 Training loss 0.0689067468047142 Validation loss 0.06908688694238663 Accuracy 0.812375009059906\n",
      "Output tensor([[0.5453],\n",
      "        [0.7116]], device='mps:0')\n",
      "Iteration 4600 Training loss 0.07471484690904617 Validation loss 0.06914345175027847 Accuracy 0.8130000233650208\n",
      "Output tensor([[0.8282],\n",
      "        [0.3239]], device='mps:0')\n",
      "Iteration 4610 Training loss 0.0764288455247879 Validation loss 0.06909643858671188 Accuracy 0.8105000257492065\n",
      "Output tensor([[0.9681],\n",
      "        [0.8171]], device='mps:0')\n",
      "Iteration 4620 Training loss 0.06081252917647362 Validation loss 0.06911035627126694 Accuracy 0.8107500672340393\n",
      "Output tensor([[0.9107],\n",
      "        [0.5106]], device='mps:0')\n",
      "Iteration 4630 Training loss 0.0746508538722992 Validation loss 0.06950676441192627 Accuracy 0.8077500462532043\n",
      "Output tensor([[0.3389],\n",
      "        [0.1228]], device='mps:0')\n",
      "Iteration 4640 Training loss 0.06649213284254074 Validation loss 0.06918331980705261 Accuracy 0.81187504529953\n",
      "Output tensor([[0.9545],\n",
      "        [0.0636]], device='mps:0')\n",
      "Iteration 4650 Training loss 0.06556544452905655 Validation loss 0.06901373714208603 Accuracy 0.8113750219345093\n",
      "Output tensor([[0.8198],\n",
      "        [0.6629]], device='mps:0')\n",
      "Iteration 4660 Training loss 0.05815274268388748 Validation loss 0.06900367885828018 Accuracy 0.812375009059906\n",
      "Output tensor([[0.1664],\n",
      "        [0.0448]], device='mps:0')\n",
      "Iteration 4670 Training loss 0.07246503233909607 Validation loss 0.06899489462375641 Accuracy 0.8126250505447388\n",
      "Output tensor([[0.0329],\n",
      "        [0.5089]], device='mps:0')\n",
      "Iteration 4680 Training loss 0.06928335875272751 Validation loss 0.06909435242414474 Accuracy 0.8126250505447388\n",
      "Output tensor([[0.9204],\n",
      "        [0.5897]], device='mps:0')\n",
      "Iteration 4690 Training loss 0.06488411873579025 Validation loss 0.06899243593215942 Accuracy 0.812000036239624\n",
      "Output tensor([[0.4795],\n",
      "        [0.9578]], device='mps:0')\n",
      "Iteration 4700 Training loss 0.06846434623003006 Validation loss 0.06959682703018188 Accuracy 0.8088750243186951\n",
      "Output tensor([[0.8507],\n",
      "        [0.9918]], device='mps:0')\n",
      "Iteration 4710 Training loss 0.07605131715536118 Validation loss 0.06896764039993286 Accuracy 0.8113750219345093\n",
      "Output tensor([[0.9164],\n",
      "        [0.7947]], device='mps:0')\n",
      "Iteration 4720 Training loss 0.07054447382688522 Validation loss 0.06896031647920609 Accuracy 0.811625063419342\n",
      "Output tensor([[0.3926],\n",
      "        [0.0522]], device='mps:0')\n",
      "Iteration 4730 Training loss 0.07847485691308975 Validation loss 0.0689353197813034 Accuracy 0.812000036239624\n",
      "Output tensor([[0.8872],\n",
      "        [0.7539]], device='mps:0')\n",
      "Iteration 4740 Training loss 0.06834575533866882 Validation loss 0.06895004957914352 Accuracy 0.8125000596046448\n",
      "Output tensor([[0.0084],\n",
      "        [0.9957]], device='mps:0')\n",
      "Iteration 4750 Training loss 0.0646827220916748 Validation loss 0.06907408684492111 Accuracy 0.8100000619888306\n",
      "Output tensor([[0.2274],\n",
      "        [0.0535]], device='mps:0')\n",
      "Iteration 4760 Training loss 0.0807151198387146 Validation loss 0.06893078237771988 Accuracy 0.812375009059906\n",
      "Output tensor([[0.8001],\n",
      "        [0.5150]], device='mps:0')\n",
      "Iteration 4770 Training loss 0.05454031005501747 Validation loss 0.0688888356089592 Accuracy 0.812375009059906\n",
      "Output tensor([[0.8245],\n",
      "        [0.4968]], device='mps:0')\n",
      "Iteration 4780 Training loss 0.06962523609399796 Validation loss 0.06904532015323639 Accuracy 0.812250018119812\n",
      "Output tensor([[0.3964],\n",
      "        [0.2974]], device='mps:0')\n",
      "Iteration 4790 Training loss 0.07065653800964355 Validation loss 0.06895419955253601 Accuracy 0.8131250143051147\n",
      "Output tensor([[0.6800],\n",
      "        [0.2430]], device='mps:0')\n",
      "Iteration 4800 Training loss 0.06901220977306366 Validation loss 0.06888218969106674 Accuracy 0.812125027179718\n",
      "Output tensor([[0.2003],\n",
      "        [0.9115]], device='mps:0')\n",
      "Iteration 4810 Training loss 0.064377561211586 Validation loss 0.06886748224496841 Accuracy 0.812125027179718\n",
      "Output tensor([[0.0324],\n",
      "        [0.0965]], device='mps:0')\n",
      "Iteration 4820 Training loss 0.07067961990833282 Validation loss 0.06891877204179764 Accuracy 0.8131250143051147\n",
      "Output tensor([[0.0391],\n",
      "        [0.6161]], device='mps:0')\n",
      "Iteration 4830 Training loss 0.06628921627998352 Validation loss 0.06888840347528458 Accuracy 0.8127500414848328\n",
      "Output tensor([[0.9945],\n",
      "        [0.9401]], device='mps:0')\n",
      "Iteration 4840 Training loss 0.07077278941869736 Validation loss 0.06945774704217911 Accuracy 0.8091250658035278\n",
      "Output tensor([[0.0702],\n",
      "        [0.6983]], device='mps:0')\n",
      "Iteration 4850 Training loss 0.0683090090751648 Validation loss 0.06889674812555313 Accuracy 0.811750054359436\n",
      "Output tensor([[0.7563],\n",
      "        [0.2540]], device='mps:0')\n",
      "Iteration 4860 Training loss 0.06582649797201157 Validation loss 0.06920535862445831 Accuracy 0.8102500438690186\n",
      "Output tensor([[0.9830],\n",
      "        [0.4842]], device='mps:0')\n",
      "Iteration 4870 Training loss 0.0664401724934578 Validation loss 0.0688578188419342 Accuracy 0.8115000128746033\n",
      "Output tensor([[0.7597],\n",
      "        [0.7688]], device='mps:0')\n",
      "Iteration 4880 Training loss 0.0651313066482544 Validation loss 0.06879369169473648 Accuracy 0.812375009059906\n",
      "Output tensor([[0.3579],\n",
      "        [0.6652]], device='mps:0')\n",
      "Iteration 4890 Training loss 0.06233774870634079 Validation loss 0.06878800690174103 Accuracy 0.8133750557899475\n",
      "Output tensor([[0.4974],\n",
      "        [0.3808]], device='mps:0')\n",
      "Iteration 4900 Training loss 0.06944029033184052 Validation loss 0.06884989142417908 Accuracy 0.8107500672340393\n",
      "Output tensor([[0.0831],\n",
      "        [0.2652]], device='mps:0')\n",
      "Iteration 4910 Training loss 0.0682014524936676 Validation loss 0.06915848702192307 Accuracy 0.8105000257492065\n",
      "Output tensor([[0.2674],\n",
      "        [0.2286]], device='mps:0')\n",
      "Iteration 4920 Training loss 0.07282543182373047 Validation loss 0.06891132891178131 Accuracy 0.812125027179718\n",
      "Output tensor([[0.3275],\n",
      "        [0.2377]], device='mps:0')\n",
      "Iteration 4930 Training loss 0.0664263591170311 Validation loss 0.06880886852741241 Accuracy 0.8133750557899475\n",
      "Output tensor([[0.2200],\n",
      "        [0.9148]], device='mps:0')\n",
      "Iteration 4940 Training loss 0.06936617940664291 Validation loss 0.06880279630422592 Accuracy 0.8131250143051147\n",
      "Output tensor([[0.6754],\n",
      "        [0.2564]], device='mps:0')\n",
      "Iteration 4950 Training loss 0.06938682496547699 Validation loss 0.06890028715133667 Accuracy 0.8126250505447388\n",
      "Output tensor([[0.0961],\n",
      "        [0.8982]], device='mps:0')\n",
      "Iteration 4960 Training loss 0.069815993309021 Validation loss 0.06874284893274307 Accuracy 0.812250018119812\n",
      "Output tensor([[0.3072],\n",
      "        [0.0542]], device='mps:0')\n",
      "Iteration 4970 Training loss 0.06444404274225235 Validation loss 0.0687488466501236 Accuracy 0.8126250505447388\n",
      "Output tensor([[0.1498],\n",
      "        [0.9398]], device='mps:0')\n",
      "Iteration 4980 Training loss 0.06702770292758942 Validation loss 0.06871917098760605 Accuracy 0.8125000596046448\n",
      "Output tensor([[0.8665],\n",
      "        [0.2707]], device='mps:0')\n",
      "Iteration 4990 Training loss 0.06446217745542526 Validation loss 0.06870480626821518 Accuracy 0.812125027179718\n",
      "Output tensor([[0.8674],\n",
      "        [0.4614]], device='mps:0')\n",
      "Iteration 5000 Training loss 0.06678929924964905 Validation loss 0.06874408572912216 Accuracy 0.8132500648498535\n",
      "Output tensor([[0.2630],\n",
      "        [0.9796]], device='mps:0')\n",
      "Iteration 5010 Training loss 0.07234559953212738 Validation loss 0.0688055232167244 Accuracy 0.8135000467300415\n",
      "Output tensor([[0.5343],\n",
      "        [0.8550]], device='mps:0')\n",
      "Iteration 5020 Training loss 0.06128157302737236 Validation loss 0.06867428869009018 Accuracy 0.8128750324249268\n",
      "Output tensor([[0.0609],\n",
      "        [0.6272]], device='mps:0')\n",
      "Iteration 5030 Training loss 0.07073161751031876 Validation loss 0.06874838471412659 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.4730],\n",
      "        [0.7633]], device='mps:0')\n",
      "Iteration 5040 Training loss 0.06073390692472458 Validation loss 0.06870981305837631 Accuracy 0.8131250143051147\n",
      "Output tensor([[0.0211],\n",
      "        [0.6492]], device='mps:0')\n",
      "Iteration 5050 Training loss 0.06524058431386948 Validation loss 0.06869278848171234 Accuracy 0.8127500414848328\n",
      "Output tensor([[0.9118],\n",
      "        [0.9502]], device='mps:0')\n",
      "Iteration 5060 Training loss 0.07417766749858856 Validation loss 0.06912402808666229 Accuracy 0.8096250295639038\n",
      "Output tensor([[0.2627],\n",
      "        [0.1728]], device='mps:0')\n",
      "Iteration 5070 Training loss 0.0642617866396904 Validation loss 0.06893862783908844 Accuracy 0.812250018119812\n",
      "Output tensor([[0.6549],\n",
      "        [0.2585]], device='mps:0')\n",
      "Iteration 5080 Training loss 0.06826405227184296 Validation loss 0.06866147369146347 Accuracy 0.8135000467300415\n",
      "Output tensor([[0.5455],\n",
      "        [0.9171]], device='mps:0')\n",
      "Iteration 5090 Training loss 0.058959588408470154 Validation loss 0.06864234805107117 Accuracy 0.8132500648498535\n",
      "Output tensor([[0.4459],\n",
      "        [0.9947]], device='mps:0')\n",
      "Iteration 5100 Training loss 0.07546130567789078 Validation loss 0.06863497942686081 Accuracy 0.8127500414848328\n",
      "Output tensor([[0.8153],\n",
      "        [0.8713]], device='mps:0')\n",
      "Iteration 5110 Training loss 0.07463353872299194 Validation loss 0.06865370273590088 Accuracy 0.812375009059906\n",
      "Output tensor([[0.2772],\n",
      "        [0.1584]], device='mps:0')\n",
      "Iteration 5120 Training loss 0.08015871793031693 Validation loss 0.06869256496429443 Accuracy 0.8133750557899475\n",
      "Output tensor([[0.2809],\n",
      "        [0.0948]], device='mps:0')\n",
      "Iteration 5130 Training loss 0.061723753809928894 Validation loss 0.06868475675582886 Accuracy 0.8113750219345093\n",
      "Output tensor([[0.1256],\n",
      "        [0.9876]], device='mps:0')\n",
      "Iteration 5140 Training loss 0.06691747158765793 Validation loss 0.06870129704475403 Accuracy 0.8127500414848328\n",
      "Output tensor([[0.6301],\n",
      "        [0.2392]], device='mps:0')\n",
      "Iteration 5150 Training loss 0.06721657514572144 Validation loss 0.06870459765195847 Accuracy 0.811750054359436\n",
      "Output tensor([[0.2313],\n",
      "        [0.4223]], device='mps:0')\n",
      "Iteration 5160 Training loss 0.06643233448266983 Validation loss 0.0686430037021637 Accuracy 0.8131250143051147\n",
      "Output tensor([[0.7860],\n",
      "        [0.1357]], device='mps:0')\n",
      "Iteration 5170 Training loss 0.06739387661218643 Validation loss 0.06876242160797119 Accuracy 0.812250018119812\n",
      "Output tensor([[0.5744],\n",
      "        [0.1201]], device='mps:0')\n",
      "Iteration 5180 Training loss 0.06744330376386642 Validation loss 0.06859805434942245 Accuracy 0.812125027179718\n",
      "Output tensor([[0.5649],\n",
      "        [0.4959]], device='mps:0')\n",
      "Iteration 5190 Training loss 0.05912592634558678 Validation loss 0.0686730369925499 Accuracy 0.8130000233650208\n",
      "Output tensor([[0.1932],\n",
      "        [0.8153]], device='mps:0')\n",
      "Iteration 5200 Training loss 0.0768294632434845 Validation loss 0.06866386532783508 Accuracy 0.8130000233650208\n",
      "Output tensor([[0.1138],\n",
      "        [0.1732]], device='mps:0')\n",
      "Iteration 5210 Training loss 0.06782611459493637 Validation loss 0.06873214244842529 Accuracy 0.811750054359436\n",
      "Output tensor([[0.9615],\n",
      "        [0.5049]], device='mps:0')\n",
      "Iteration 5220 Training loss 0.05904616042971611 Validation loss 0.06852570176124573 Accuracy 0.8141250610351562\n",
      "Output tensor([[0.7791],\n",
      "        [0.1279]], device='mps:0')\n",
      "Iteration 5230 Training loss 0.06491531431674957 Validation loss 0.06859572231769562 Accuracy 0.8111250400543213\n",
      "Output tensor([[0.3067],\n",
      "        [0.3441]], device='mps:0')\n",
      "Iteration 5240 Training loss 0.0658942312002182 Validation loss 0.06874535977840424 Accuracy 0.8127500414848328\n",
      "Output tensor([[0.6103],\n",
      "        [0.5650]], device='mps:0')\n",
      "Iteration 5250 Training loss 0.062390267848968506 Validation loss 0.06849687546491623 Accuracy 0.8135000467300415\n",
      "Output tensor([[0.9312],\n",
      "        [0.9663]], device='mps:0')\n",
      "Iteration 5260 Training loss 0.07445752620697021 Validation loss 0.06853223592042923 Accuracy 0.8127500414848328\n",
      "Output tensor([[0.9435],\n",
      "        [0.5533]], device='mps:0')\n",
      "Iteration 5270 Training loss 0.06870654970407486 Validation loss 0.0685526430606842 Accuracy 0.8137500286102295\n",
      "Output tensor([[0.9861],\n",
      "        [0.2950]], device='mps:0')\n",
      "Iteration 5280 Training loss 0.06469207257032394 Validation loss 0.06846582144498825 Accuracy 0.8132500648498535\n",
      "Output tensor([[0.7055],\n",
      "        [0.7949]], device='mps:0')\n",
      "Iteration 5290 Training loss 0.05614577233791351 Validation loss 0.0686115175485611 Accuracy 0.8110000491142273\n",
      "Output tensor([[0.1514],\n",
      "        [0.8914]], device='mps:0')\n",
      "Iteration 5300 Training loss 0.06690653413534164 Validation loss 0.06860247254371643 Accuracy 0.812375009059906\n",
      "Output tensor([[0.0780],\n",
      "        [0.8944]], device='mps:0')\n",
      "Iteration 5310 Training loss 0.06505633145570755 Validation loss 0.06845792382955551 Accuracy 0.8136250376701355\n",
      "Output tensor([[0.7214],\n",
      "        [0.9361]], device='mps:0')\n",
      "Iteration 5320 Training loss 0.06505253911018372 Validation loss 0.06855425983667374 Accuracy 0.8128750324249268\n",
      "Output tensor([[0.1340],\n",
      "        [0.5931]], device='mps:0')\n",
      "Iteration 5330 Training loss 0.07570695877075195 Validation loss 0.06844070553779602 Accuracy 0.8141250610351562\n",
      "Output tensor([[0.6683],\n",
      "        [0.6563]], device='mps:0')\n",
      "Iteration 5340 Training loss 0.06445425003767014 Validation loss 0.06849972903728485 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.1470],\n",
      "        [0.7038]], device='mps:0')\n",
      "Iteration 5350 Training loss 0.06423778086900711 Validation loss 0.06968878209590912 Accuracy 0.8082500100135803\n",
      "Output tensor([[0.0793],\n",
      "        [0.9793]], device='mps:0')\n",
      "Iteration 5360 Training loss 0.0684271976351738 Validation loss 0.06861227005720139 Accuracy 0.812250018119812\n",
      "Output tensor([[0.7676],\n",
      "        [0.0806]], device='mps:0')\n",
      "Iteration 5370 Training loss 0.06524855643510818 Validation loss 0.06862416863441467 Accuracy 0.8128750324249268\n",
      "Output tensor([[0.6651],\n",
      "        [0.9379]], device='mps:0')\n",
      "Iteration 5380 Training loss 0.07050430774688721 Validation loss 0.06845657527446747 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.4126],\n",
      "        [0.0921]], device='mps:0')\n",
      "Iteration 5390 Training loss 0.06723088771104813 Validation loss 0.06870470196008682 Accuracy 0.8107500672340393\n",
      "Output tensor([[0.8012],\n",
      "        [0.4461]], device='mps:0')\n",
      "Iteration 5400 Training loss 0.07193614542484283 Validation loss 0.06845167279243469 Accuracy 0.8145000338554382\n",
      "Output tensor([[0.9586],\n",
      "        [0.9698]], device='mps:0')\n",
      "Iteration 5410 Training loss 0.06476492434740067 Validation loss 0.0684073343873024 Accuracy 0.8138750195503235\n",
      "Output tensor([[0.3504],\n",
      "        [0.7867]], device='mps:0')\n",
      "Iteration 5420 Training loss 0.0655689686536789 Validation loss 0.06866620481014252 Accuracy 0.8106250166893005\n",
      "Output tensor([[0.1686],\n",
      "        [0.8535]], device='mps:0')\n",
      "Iteration 5430 Training loss 0.06944697350263596 Validation loss 0.06837888062000275 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.6287],\n",
      "        [0.9321]], device='mps:0')\n",
      "Iteration 5440 Training loss 0.07274097949266434 Validation loss 0.06835383176803589 Accuracy 0.8145000338554382\n",
      "Output tensor([[0.9031],\n",
      "        [0.6628]], device='mps:0')\n",
      "Iteration 5450 Training loss 0.06343796104192734 Validation loss 0.06832921504974365 Accuracy 0.8136250376701355\n",
      "Output tensor([[0.8701],\n",
      "        [0.9895]], device='mps:0')\n",
      "Iteration 5460 Training loss 0.06583388894796371 Validation loss 0.0683712512254715 Accuracy 0.8146250247955322\n",
      "Output tensor([[0.0032],\n",
      "        [0.3110]], device='mps:0')\n",
      "Iteration 5470 Training loss 0.07212968915700912 Validation loss 0.06832855194807053 Accuracy 0.8136250376701355\n",
      "Output tensor([[0.0510],\n",
      "        [0.0778]], device='mps:0')\n",
      "Iteration 5480 Training loss 0.0685800090432167 Validation loss 0.06834094226360321 Accuracy 0.8142500519752502\n",
      "Output tensor([[0.8391],\n",
      "        [0.0189]], device='mps:0')\n",
      "Iteration 5490 Training loss 0.06821475178003311 Validation loss 0.06847139447927475 Accuracy 0.8127500414848328\n",
      "Output tensor([[0.1556],\n",
      "        [0.8581]], device='mps:0')\n",
      "Iteration 5500 Training loss 0.06926297396421432 Validation loss 0.06831898540258408 Accuracy 0.8132500648498535\n",
      "Output tensor([[0.0884],\n",
      "        [0.0200]], device='mps:0')\n",
      "Iteration 5510 Training loss 0.06787370145320892 Validation loss 0.06830481439828873 Accuracy 0.8147500157356262\n",
      "Output tensor([[0.3480],\n",
      "        [0.1784]], device='mps:0')\n",
      "Iteration 5520 Training loss 0.060259923338890076 Validation loss 0.06835243850946426 Accuracy 0.8138750195503235\n",
      "Output tensor([[0.4027],\n",
      "        [0.6661]], device='mps:0')\n",
      "Iteration 5530 Training loss 0.06831078231334686 Validation loss 0.06839551776647568 Accuracy 0.8132500648498535\n",
      "Output tensor([[0.0297],\n",
      "        [0.0129]], device='mps:0')\n",
      "Iteration 5540 Training loss 0.07601132988929749 Validation loss 0.06835615634918213 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.8013],\n",
      "        [0.9638]], device='mps:0')\n",
      "Iteration 5550 Training loss 0.06392115354537964 Validation loss 0.06848204135894775 Accuracy 0.8112500309944153\n",
      "Output tensor([[0.9158],\n",
      "        [0.1727]], device='mps:0')\n",
      "Iteration 5560 Training loss 0.06904131174087524 Validation loss 0.06859780102968216 Accuracy 0.8126250505447388\n",
      "Output tensor([[0.6988],\n",
      "        [0.0478]], device='mps:0')\n",
      "Iteration 5570 Training loss 0.07088356465101242 Validation loss 0.06921301782131195 Accuracy 0.8098750114440918\n",
      "Output tensor([[0.2544],\n",
      "        [0.0602]], device='mps:0')\n",
      "Iteration 5580 Training loss 0.06848859786987305 Validation loss 0.06825249642133713 Accuracy 0.8136250376701355\n",
      "Output tensor([[0.4980],\n",
      "        [0.0917]], device='mps:0')\n",
      "Iteration 5590 Training loss 0.07026142627000809 Validation loss 0.06824501603841782 Accuracy 0.815000057220459\n",
      "Output tensor([[0.8123],\n",
      "        [0.1054]], device='mps:0')\n",
      "Iteration 5600 Training loss 0.06792275607585907 Validation loss 0.06825561076402664 Accuracy 0.8141250610351562\n",
      "Output tensor([[0.1727],\n",
      "        [0.0794]], device='mps:0')\n",
      "Iteration 5610 Training loss 0.06565584242343903 Validation loss 0.0682402104139328 Accuracy 0.8146250247955322\n",
      "Output tensor([[0.9359],\n",
      "        [0.0629]], device='mps:0')\n",
      "Iteration 5620 Training loss 0.06412532180547714 Validation loss 0.06823887676000595 Accuracy 0.8145000338554382\n",
      "Output tensor([[0.2968],\n",
      "        [0.1589]], device='mps:0')\n",
      "Iteration 5630 Training loss 0.0672786682844162 Validation loss 0.06824120879173279 Accuracy 0.8142500519752502\n",
      "Output tensor([[0.2951],\n",
      "        [0.1095]], device='mps:0')\n",
      "Iteration 5640 Training loss 0.06897760927677155 Validation loss 0.06830644607543945 Accuracy 0.8135000467300415\n",
      "Output tensor([[0.0529],\n",
      "        [0.7813]], device='mps:0')\n",
      "Iteration 5650 Training loss 0.07940182834863663 Validation loss 0.06822862476110458 Accuracy 0.815000057220459\n",
      "Output tensor([[0.7668],\n",
      "        [0.4819]], device='mps:0')\n",
      "Iteration 5660 Training loss 0.06608882546424866 Validation loss 0.0682314783334732 Accuracy 0.8137500286102295\n",
      "Output tensor([[0.7513],\n",
      "        [0.7862]], device='mps:0')\n",
      "Iteration 5670 Training loss 0.06634162366390228 Validation loss 0.06817691773176193 Accuracy 0.8138750195503235\n",
      "Output tensor([[0.9100],\n",
      "        [0.2912]], device='mps:0')\n",
      "Iteration 5680 Training loss 0.06218384578824043 Validation loss 0.0682409480214119 Accuracy 0.8138750195503235\n",
      "Output tensor([[0.1617],\n",
      "        [0.4085]], device='mps:0')\n",
      "Iteration 5690 Training loss 0.07335929572582245 Validation loss 0.06846041977405548 Accuracy 0.8110000491142273\n",
      "Output tensor([[0.7501],\n",
      "        [0.9449]], device='mps:0')\n",
      "Iteration 5700 Training loss 0.07106482237577438 Validation loss 0.06815452128648758 Accuracy 0.8146250247955322\n",
      "Output tensor([[0.0478],\n",
      "        [0.1770]], device='mps:0')\n",
      "Iteration 5710 Training loss 0.06538891047239304 Validation loss 0.06819302588701248 Accuracy 0.8143750429153442\n",
      "Output tensor([[0.9019],\n",
      "        [0.8335]], device='mps:0')\n",
      "Iteration 5720 Training loss 0.060411300510168076 Validation loss 0.06814192235469818 Accuracy 0.8142500519752502\n",
      "Output tensor([[0.8561],\n",
      "        [0.0775]], device='mps:0')\n",
      "Iteration 5730 Training loss 0.06603887677192688 Validation loss 0.06824298948049545 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.4563],\n",
      "        [0.6686]], device='mps:0')\n",
      "Iteration 5740 Training loss 0.07419554889202118 Validation loss 0.06812520325183868 Accuracy 0.8141250610351562\n",
      "Output tensor([[0.3915],\n",
      "        [0.9275]], device='mps:0')\n",
      "Iteration 5750 Training loss 0.06437908113002777 Validation loss 0.06814347952604294 Accuracy 0.815375030040741\n",
      "Output tensor([[0.7771],\n",
      "        [0.0341]], device='mps:0')\n",
      "Iteration 5760 Training loss 0.06973898410797119 Validation loss 0.06811506301164627 Accuracy 0.8142500519752502\n",
      "Output tensor([[0.8332],\n",
      "        [0.0461]], device='mps:0')\n",
      "Iteration 5770 Training loss 0.06366347521543503 Validation loss 0.06823433935642242 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.7519],\n",
      "        [0.7590]], device='mps:0')\n",
      "Iteration 5780 Training loss 0.07487456500530243 Validation loss 0.06809181720018387 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.9253],\n",
      "        [0.3055]], device='mps:0')\n",
      "Iteration 5790 Training loss 0.0727636069059372 Validation loss 0.06808614730834961 Accuracy 0.8145000338554382\n",
      "Output tensor([[0.0128],\n",
      "        [0.3248]], device='mps:0')\n",
      "Iteration 5800 Training loss 0.06597426533699036 Validation loss 0.06816869229078293 Accuracy 0.8133750557899475\n",
      "Output tensor([[0.9442],\n",
      "        [0.5954]], device='mps:0')\n",
      "Iteration 5810 Training loss 0.06105988845229149 Validation loss 0.06808263063430786 Accuracy 0.815375030040741\n",
      "Output tensor([[0.3464],\n",
      "        [0.2344]], device='mps:0')\n",
      "Iteration 5820 Training loss 0.0652584657073021 Validation loss 0.06805963814258575 Accuracy 0.815625011920929\n",
      "Output tensor([[0.9766],\n",
      "        [0.8542]], device='mps:0')\n",
      "Iteration 5830 Training loss 0.06766585260629654 Validation loss 0.06812605261802673 Accuracy 0.8146250247955322\n",
      "Output tensor([[0.2681],\n",
      "        [0.4466]], device='mps:0')\n",
      "Iteration 5840 Training loss 0.06794339418411255 Validation loss 0.06805945187807083 Accuracy 0.815125048160553\n",
      "Output tensor([[0.8587],\n",
      "        [0.1989]], device='mps:0')\n",
      "Iteration 5850 Training loss 0.06621313095092773 Validation loss 0.06804794818162918 Accuracy 0.815625011920929\n",
      "Output tensor([[0.6382],\n",
      "        [0.7023]], device='mps:0')\n",
      "Iteration 5860 Training loss 0.07055175304412842 Validation loss 0.06804738938808441 Accuracy 0.815500020980835\n",
      "Output tensor([[0.4614],\n",
      "        [0.1353]], device='mps:0')\n",
      "Iteration 5870 Training loss 0.06385436654090881 Validation loss 0.06802952289581299 Accuracy 0.815000057220459\n",
      "Output tensor([[0.7362],\n",
      "        [0.6670]], device='mps:0')\n",
      "Iteration 5880 Training loss 0.06848779320716858 Validation loss 0.06803284585475922 Accuracy 0.815625011920929\n",
      "Output tensor([[0.9665],\n",
      "        [0.8001]], device='mps:0')\n",
      "Iteration 5890 Training loss 0.062623530626297 Validation loss 0.0680566132068634 Accuracy 0.8158750534057617\n",
      "Output tensor([[0.0762],\n",
      "        [0.9095]], device='mps:0')\n",
      "Iteration 5900 Training loss 0.063056580722332 Validation loss 0.06804266571998596 Accuracy 0.815250039100647\n",
      "Output tensor([[0.6381],\n",
      "        [0.7017]], device='mps:0')\n",
      "Iteration 5910 Training loss 0.06944604218006134 Validation loss 0.06800917536020279 Accuracy 0.815625011920929\n",
      "Output tensor([[0.4473],\n",
      "        [0.0680]], device='mps:0')\n",
      "Iteration 5920 Training loss 0.0685197189450264 Validation loss 0.06804531067609787 Accuracy 0.8147500157356262\n",
      "Output tensor([[0.0138],\n",
      "        [0.0754]], device='mps:0')\n",
      "Iteration 5930 Training loss 0.06449013203382492 Validation loss 0.0679858848452568 Accuracy 0.814875066280365\n",
      "Output tensor([[0.9846],\n",
      "        [0.0778]], device='mps:0')\n",
      "Iteration 5940 Training loss 0.06003463640809059 Validation loss 0.067989781498909 Accuracy 0.8145000338554382\n",
      "Output tensor([[0.2690],\n",
      "        [0.6904]], device='mps:0')\n",
      "Iteration 5950 Training loss 0.05987560749053955 Validation loss 0.06797482818365097 Accuracy 0.8147500157356262\n",
      "Output tensor([[0.9120],\n",
      "        [0.3049]], device='mps:0')\n",
      "Iteration 5960 Training loss 0.06690200418233871 Validation loss 0.06806473433971405 Accuracy 0.8137500286102295\n",
      "Output tensor([[0.6694],\n",
      "        [0.0412]], device='mps:0')\n",
      "Iteration 5970 Training loss 0.06300508230924606 Validation loss 0.06794686615467072 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.2119],\n",
      "        [0.1485]], device='mps:0')\n",
      "Iteration 5980 Training loss 0.0666562169790268 Validation loss 0.06812138110399246 Accuracy 0.8145000338554382\n",
      "Output tensor([[0.4631],\n",
      "        [0.8187]], device='mps:0')\n",
      "Iteration 5990 Training loss 0.07188212871551514 Validation loss 0.06793637573719025 Accuracy 0.8147500157356262\n",
      "Output tensor([[0.6352],\n",
      "        [0.6221]], device='mps:0')\n",
      "Iteration 6000 Training loss 0.06476063281297684 Validation loss 0.06791941821575165 Accuracy 0.815250039100647\n",
      "Output tensor([[0.3192],\n",
      "        [0.9482]], device='mps:0')\n",
      "Iteration 6010 Training loss 0.06445511430501938 Validation loss 0.06800549477338791 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.2512],\n",
      "        [0.4754]], device='mps:0')\n",
      "Iteration 6020 Training loss 0.06256892532110214 Validation loss 0.0680883526802063 Accuracy 0.8141250610351562\n",
      "Output tensor([[0.3423],\n",
      "        [0.3752]], device='mps:0')\n",
      "Iteration 6030 Training loss 0.05983714759349823 Validation loss 0.06810196489095688 Accuracy 0.8138750195503235\n",
      "Output tensor([[0.2935],\n",
      "        [0.1627]], device='mps:0')\n",
      "Iteration 6040 Training loss 0.07276985049247742 Validation loss 0.06789163500070572 Accuracy 0.8157500624656677\n",
      "Output tensor([[0.1837],\n",
      "        [0.8794]], device='mps:0')\n",
      "Iteration 6050 Training loss 0.06781759113073349 Validation loss 0.06788896024227142 Accuracy 0.8157500624656677\n",
      "Output tensor([[0.7245],\n",
      "        [0.2830]], device='mps:0')\n",
      "Iteration 6060 Training loss 0.06090429797768593 Validation loss 0.06790102273225784 Accuracy 0.815625011920929\n",
      "Output tensor([[0.1216],\n",
      "        [0.8205]], device='mps:0')\n",
      "Iteration 6070 Training loss 0.06280195713043213 Validation loss 0.06786992400884628 Accuracy 0.8157500624656677\n",
      "Output tensor([[0.7605],\n",
      "        [0.7693]], device='mps:0')\n",
      "Iteration 6080 Training loss 0.06611767411231995 Validation loss 0.06806230545043945 Accuracy 0.8137500286102295\n",
      "Output tensor([[0.1338],\n",
      "        [0.7933]], device='mps:0')\n",
      "Iteration 6090 Training loss 0.062107715755701065 Validation loss 0.0678577572107315 Accuracy 0.815500020980835\n",
      "Output tensor([[0.3414],\n",
      "        [0.4165]], device='mps:0')\n",
      "Iteration 6100 Training loss 0.06512908637523651 Validation loss 0.06795505434274673 Accuracy 0.8145000338554382\n",
      "Output tensor([[0.3526],\n",
      "        [0.9084]], device='mps:0')\n",
      "Iteration 6110 Training loss 0.06980659067630768 Validation loss 0.06792621314525604 Accuracy 0.815375030040741\n",
      "Output tensor([[0.3242],\n",
      "        [0.9494]], device='mps:0')\n",
      "Iteration 6120 Training loss 0.07547459751367569 Validation loss 0.06802025437355042 Accuracy 0.8128750324249268\n",
      "Output tensor([[0.9460],\n",
      "        [0.5564]], device='mps:0')\n",
      "Iteration 6130 Training loss 0.07799138128757477 Validation loss 0.0678124874830246 Accuracy 0.8157500624656677\n",
      "Output tensor([[0.5509],\n",
      "        [0.0497]], device='mps:0')\n",
      "Iteration 6140 Training loss 0.06113250553607941 Validation loss 0.06801581382751465 Accuracy 0.8137500286102295\n",
      "Output tensor([[0.2067],\n",
      "        [0.8549]], device='mps:0')\n",
      "Iteration 6150 Training loss 0.07011665403842926 Validation loss 0.0682961717247963 Accuracy 0.8126250505447388\n",
      "Output tensor([[0.8773],\n",
      "        [0.6290]], device='mps:0')\n",
      "Iteration 6160 Training loss 0.057979173958301544 Validation loss 0.06780873984098434 Accuracy 0.815250039100647\n",
      "Output tensor([[0.0087],\n",
      "        [0.7273]], device='mps:0')\n",
      "Iteration 6170 Training loss 0.06715689599514008 Validation loss 0.06778968870639801 Accuracy 0.8167500495910645\n",
      "Output tensor([[0.0912],\n",
      "        [0.9750]], device='mps:0')\n",
      "Iteration 6180 Training loss 0.0757276639342308 Validation loss 0.06792019307613373 Accuracy 0.8137500286102295\n",
      "Output tensor([[0.0952],\n",
      "        [0.1022]], device='mps:0')\n",
      "Iteration 6190 Training loss 0.06590374559164047 Validation loss 0.06777632236480713 Accuracy 0.8161250352859497\n",
      "Output tensor([[0.9431],\n",
      "        [0.8540]], device='mps:0')\n",
      "Iteration 6200 Training loss 0.06184113398194313 Validation loss 0.06787984073162079 Accuracy 0.8145000338554382\n",
      "Output tensor([[0.2588],\n",
      "        [0.4901]], device='mps:0')\n",
      "Iteration 6210 Training loss 0.07071079313755035 Validation loss 0.06781843304634094 Accuracy 0.8146250247955322\n",
      "Output tensor([[0.9429],\n",
      "        [0.8166]], device='mps:0')\n",
      "Iteration 6220 Training loss 0.06749578565359116 Validation loss 0.06784810870885849 Accuracy 0.8147500157356262\n",
      "Output tensor([[0.4930],\n",
      "        [0.2081]], device='mps:0')\n",
      "Iteration 6230 Training loss 0.07316257804632187 Validation loss 0.06775610148906708 Accuracy 0.8162500262260437\n",
      "Output tensor([[0.4261],\n",
      "        [0.9438]], device='mps:0')\n",
      "Iteration 6240 Training loss 0.06861036270856857 Validation loss 0.06779548525810242 Accuracy 0.8162500262260437\n",
      "Output tensor([[0.9755],\n",
      "        [0.3124]], device='mps:0')\n",
      "Iteration 6250 Training loss 0.06913899630308151 Validation loss 0.06786806136369705 Accuracy 0.8142500519752502\n",
      "Output tensor([[0.8639],\n",
      "        [0.5546]], device='mps:0')\n",
      "Iteration 6260 Training loss 0.08213170617818832 Validation loss 0.06775078922510147 Accuracy 0.815375030040741\n",
      "Output tensor([[0.0739],\n",
      "        [0.1820]], device='mps:0')\n",
      "Iteration 6270 Training loss 0.06364400684833527 Validation loss 0.06782426685094833 Accuracy 0.815000057220459\n",
      "Output tensor([[0.2716],\n",
      "        [0.0185]], device='mps:0')\n",
      "Iteration 6280 Training loss 0.06637051701545715 Validation loss 0.0677051842212677 Accuracy 0.8158750534057617\n",
      "Output tensor([[0.2868],\n",
      "        [0.1167]], device='mps:0')\n",
      "Iteration 6290 Training loss 0.07011065632104874 Validation loss 0.0678015649318695 Accuracy 0.8157500624656677\n",
      "Output tensor([[0.2959],\n",
      "        [0.4187]], device='mps:0')\n",
      "Iteration 6300 Training loss 0.07122308760881424 Validation loss 0.0676867738366127 Accuracy 0.815500020980835\n",
      "Output tensor([[0.1042],\n",
      "        [0.6631]], device='mps:0')\n",
      "Iteration 6310 Training loss 0.06089559569954872 Validation loss 0.0677507072687149 Accuracy 0.815500020980835\n",
      "Output tensor([[0.4485],\n",
      "        [0.2562]], device='mps:0')\n",
      "Iteration 6320 Training loss 0.06545579433441162 Validation loss 0.06787126511335373 Accuracy 0.815125048160553\n",
      "Output tensor([[0.9940],\n",
      "        [0.9062]], device='mps:0')\n",
      "Iteration 6330 Training loss 0.06401382386684418 Validation loss 0.06769008934497833 Accuracy 0.8165000677108765\n",
      "Output tensor([[0.1470],\n",
      "        [0.1803]], device='mps:0')\n",
      "Iteration 6340 Training loss 0.06461859494447708 Validation loss 0.06771493703126907 Accuracy 0.8165000677108765\n",
      "Output tensor([[0.7616],\n",
      "        [0.1598]], device='mps:0')\n",
      "Iteration 6350 Training loss 0.06735146790742874 Validation loss 0.06784222275018692 Accuracy 0.8142500519752502\n",
      "Output tensor([[0.4556],\n",
      "        [0.1848]], device='mps:0')\n",
      "Iteration 6360 Training loss 0.07338526099920273 Validation loss 0.06767985224723816 Accuracy 0.8162500262260437\n",
      "Output tensor([[0.7369],\n",
      "        [0.9473]], device='mps:0')\n",
      "Iteration 6370 Training loss 0.0616922564804554 Validation loss 0.06769963353872299 Accuracy 0.815500020980835\n",
      "Output tensor([[0.1202],\n",
      "        [0.2012]], device='mps:0')\n",
      "Iteration 6380 Training loss 0.06861832737922668 Validation loss 0.06764265894889832 Accuracy 0.815625011920929\n",
      "Output tensor([[0.5543],\n",
      "        [0.6933]], device='mps:0')\n",
      "Iteration 6390 Training loss 0.068990059196949 Validation loss 0.06775230914354324 Accuracy 0.8142500519752502\n",
      "Output tensor([[0.3738],\n",
      "        [0.1669]], device='mps:0')\n",
      "Iteration 6400 Training loss 0.06890899688005447 Validation loss 0.0676022544503212 Accuracy 0.815500020980835\n",
      "Output tensor([[0.0493],\n",
      "        [0.4972]], device='mps:0')\n",
      "Iteration 6410 Training loss 0.06655743718147278 Validation loss 0.06759488582611084 Accuracy 0.8160000443458557\n",
      "Output tensor([[0.9384],\n",
      "        [0.0968]], device='mps:0')\n",
      "Iteration 6420 Training loss 0.06316301971673965 Validation loss 0.06762082129716873 Accuracy 0.8163750171661377\n",
      "Output tensor([[0.6759],\n",
      "        [0.1754]], device='mps:0')\n",
      "Iteration 6430 Training loss 0.06380895525217056 Validation loss 0.0678727999329567 Accuracy 0.8127500414848328\n",
      "Output tensor([[0.9393],\n",
      "        [0.1872]], device='mps:0')\n",
      "Iteration 6440 Training loss 0.07287343591451645 Validation loss 0.06763681769371033 Accuracy 0.8157500624656677\n",
      "Output tensor([[0.8829],\n",
      "        [0.9030]], device='mps:0')\n",
      "Iteration 6450 Training loss 0.06639949977397919 Validation loss 0.06795891374349594 Accuracy 0.8138750195503235\n",
      "Output tensor([[0.5153],\n",
      "        [0.8267]], device='mps:0')\n",
      "Iteration 6460 Training loss 0.0645216554403305 Validation loss 0.06761328130960464 Accuracy 0.815500020980835\n",
      "Output tensor([[0.1839],\n",
      "        [0.1587]], device='mps:0')\n",
      "Iteration 6470 Training loss 0.0626881793141365 Validation loss 0.06756250560283661 Accuracy 0.8160000443458557\n",
      "Output tensor([[0.9503],\n",
      "        [0.0363]], device='mps:0')\n",
      "Iteration 6480 Training loss 0.060028646141290665 Validation loss 0.06762787699699402 Accuracy 0.815250039100647\n",
      "Output tensor([[0.8820],\n",
      "        [0.0417]], device='mps:0')\n",
      "Iteration 6490 Training loss 0.0671730563044548 Validation loss 0.06757627427577972 Accuracy 0.8158750534057617\n",
      "Output tensor([[0.8285],\n",
      "        [0.2737]], device='mps:0')\n",
      "Iteration 6500 Training loss 0.06824198365211487 Validation loss 0.06754005700349808 Accuracy 0.8160000443458557\n",
      "Output tensor([[0.0030],\n",
      "        [0.9701]], device='mps:0')\n",
      "Iteration 6510 Training loss 0.06901458650827408 Validation loss 0.06762003153562546 Accuracy 0.815375030040741\n",
      "Output tensor([[0.4138],\n",
      "        [0.2285]], device='mps:0')\n",
      "Iteration 6520 Training loss 0.06691143661737442 Validation loss 0.06751846522092819 Accuracy 0.815625011920929\n",
      "Output tensor([[0.5993],\n",
      "        [0.1512]], device='mps:0')\n",
      "Iteration 6530 Training loss 0.06339707970619202 Validation loss 0.06758932769298553 Accuracy 0.8147500157356262\n",
      "Output tensor([[0.5598],\n",
      "        [0.0449]], device='mps:0')\n",
      "Iteration 6540 Training loss 0.06775427609682083 Validation loss 0.06754542887210846 Accuracy 0.815625011920929\n",
      "Output tensor([[0.3129],\n",
      "        [0.9523]], device='mps:0')\n",
      "Iteration 6550 Training loss 0.061770420521497726 Validation loss 0.06790265440940857 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.7832],\n",
      "        [0.8925]], device='mps:0')\n",
      "Iteration 6560 Training loss 0.06673695147037506 Validation loss 0.06751955300569534 Accuracy 0.8158750534057617\n",
      "Output tensor([[0.7401],\n",
      "        [0.2150]], device='mps:0')\n",
      "Iteration 6570 Training loss 0.06784016638994217 Validation loss 0.0676615908741951 Accuracy 0.8140000104904175\n",
      "Output tensor([[0.2166],\n",
      "        [0.3559]], device='mps:0')\n",
      "Iteration 6580 Training loss 0.06897898018360138 Validation loss 0.06770455092191696 Accuracy 0.8145000338554382\n",
      "Output tensor([[0.1952],\n",
      "        [0.4933]], device='mps:0')\n",
      "Iteration 6590 Training loss 0.06303875148296356 Validation loss 0.06761447340250015 Accuracy 0.8142500519752502\n",
      "Output tensor([[0.1997],\n",
      "        [0.4857]], device='mps:0')\n",
      "Iteration 6600 Training loss 0.06858950108289719 Validation loss 0.06746768206357956 Accuracy 0.8163750171661377\n",
      "Output tensor([[0.4574],\n",
      "        [0.2091]], device='mps:0')\n",
      "Iteration 6610 Training loss 0.05798475071787834 Validation loss 0.06747584789991379 Accuracy 0.8161250352859497\n",
      "Output tensor([[0.5626],\n",
      "        [0.5910]], device='mps:0')\n",
      "Iteration 6620 Training loss 0.06996709853410721 Validation loss 0.06745284050703049 Accuracy 0.815375030040741\n",
      "Output tensor([[0.0738],\n",
      "        [0.8715]], device='mps:0')\n",
      "Iteration 6630 Training loss 0.06449172645807266 Validation loss 0.0674588680267334 Accuracy 0.815625011920929\n",
      "Output tensor([[0.1666],\n",
      "        [0.9317]], device='mps:0')\n",
      "Iteration 6640 Training loss 0.07675626873970032 Validation loss 0.06746016442775726 Accuracy 0.815500020980835\n",
      "Output tensor([[0.1925],\n",
      "        [0.9887]], device='mps:0')\n",
      "Iteration 6650 Training loss 0.07181214541196823 Validation loss 0.06754088401794434 Accuracy 0.8163750171661377\n",
      "Output tensor([[0.9169],\n",
      "        [0.5196]], device='mps:0')\n",
      "Iteration 6660 Training loss 0.06807776540517807 Validation loss 0.06745744496583939 Accuracy 0.8166250586509705\n",
      "Output tensor([[0.3960],\n",
      "        [0.0187]], device='mps:0')\n",
      "Iteration 6670 Training loss 0.06234835460782051 Validation loss 0.06760897487401962 Accuracy 0.8141250610351562\n",
      "Output tensor([[0.0651],\n",
      "        [0.7131]], device='mps:0')\n",
      "Iteration 6680 Training loss 0.07562649250030518 Validation loss 0.06780049204826355 Accuracy 0.8133750557899475\n",
      "Output tensor([[0.8027],\n",
      "        [0.2826]], device='mps:0')\n",
      "Iteration 6690 Training loss 0.06238432601094246 Validation loss 0.06739688664674759 Accuracy 0.8165000677108765\n",
      "Output tensor([[0.8835],\n",
      "        [0.1062]], device='mps:0')\n",
      "Iteration 6700 Training loss 0.07210254669189453 Validation loss 0.06738734990358353 Accuracy 0.8165000677108765\n",
      "Output tensor([[0.1112],\n",
      "        [0.3366]], device='mps:0')\n",
      "Iteration 6710 Training loss 0.06729011982679367 Validation loss 0.06740634888410568 Accuracy 0.8162500262260437\n",
      "Output tensor([[0.6039],\n",
      "        [0.9348]], device='mps:0')\n",
      "Iteration 6720 Training loss 0.07620938867330551 Validation loss 0.06749064475297928 Accuracy 0.8165000677108765\n",
      "Output tensor([[0.0302],\n",
      "        [0.9587]], device='mps:0')\n",
      "Iteration 6730 Training loss 0.07091467827558517 Validation loss 0.06740779429674149 Accuracy 0.8162500262260437\n",
      "Output tensor([[0.3124],\n",
      "        [0.4894]], device='mps:0')\n",
      "Iteration 6740 Training loss 0.06092507392168045 Validation loss 0.06743359565734863 Accuracy 0.8166250586509705\n",
      "Output tensor([[0.1559],\n",
      "        [0.7066]], device='mps:0')\n",
      "Iteration 6750 Training loss 0.06764113903045654 Validation loss 0.06734302639961243 Accuracy 0.8161250352859497\n",
      "Output tensor([[0.0357],\n",
      "        [0.6448]], device='mps:0')\n",
      "Iteration 6760 Training loss 0.06703749299049377 Validation loss 0.06733696162700653 Accuracy 0.8168750405311584\n",
      "Output tensor([[0.2765],\n",
      "        [0.7748]], device='mps:0')\n",
      "Iteration 6770 Training loss 0.06458678841590881 Validation loss 0.06738188862800598 Accuracy 0.8165000677108765\n",
      "Output tensor([[0.7654],\n",
      "        [0.3778]], device='mps:0')\n",
      "Iteration 6780 Training loss 0.07482846081256866 Validation loss 0.06732013076543808 Accuracy 0.8160000443458557\n",
      "Output tensor([[0.4799],\n",
      "        [0.8328]], device='mps:0')\n",
      "Iteration 6790 Training loss 0.06463782489299774 Validation loss 0.06742706149816513 Accuracy 0.815125048160553\n",
      "Output tensor([[0.5784],\n",
      "        [0.5905]], device='mps:0')\n",
      "Iteration 6800 Training loss 0.0669945552945137 Validation loss 0.06734025478363037 Accuracy 0.8170000314712524\n",
      "Output tensor([[0.9519],\n",
      "        [0.4368]], device='mps:0')\n",
      "Iteration 6810 Training loss 0.06376306712627411 Validation loss 0.06747648864984512 Accuracy 0.815375030040741\n",
      "Output tensor([[0.2510],\n",
      "        [0.9136]], device='mps:0')\n",
      "Iteration 6820 Training loss 0.06652653217315674 Validation loss 0.06740780174732208 Accuracy 0.8160000443458557\n",
      "Output tensor([[0.2109],\n",
      "        [0.8759]], device='mps:0')\n",
      "Iteration 6830 Training loss 0.059145741164684296 Validation loss 0.06728082150220871 Accuracy 0.8165000677108765\n",
      "Output tensor([[0.2864],\n",
      "        [0.9795]], device='mps:0')\n",
      "Iteration 6840 Training loss 0.06104643642902374 Validation loss 0.06736190617084503 Accuracy 0.815625011920929\n",
      "Output tensor([[0.6697],\n",
      "        [0.7911]], device='mps:0')\n",
      "Iteration 6850 Training loss 0.0676041767001152 Validation loss 0.0672878623008728 Accuracy 0.8168750405311584\n",
      "Output tensor([[0.2757],\n",
      "        [0.2507]], device='mps:0')\n",
      "Iteration 6860 Training loss 0.06294452399015427 Validation loss 0.0675089880824089 Accuracy 0.815375030040741\n",
      "Output tensor([[0.4799],\n",
      "        [0.9538]], device='mps:0')\n",
      "Iteration 6870 Training loss 0.06870537251234055 Validation loss 0.06726542860269547 Accuracy 0.8173750638961792\n",
      "Output tensor([[0.4744],\n",
      "        [0.6244]], device='mps:0')\n",
      "Iteration 6880 Training loss 0.06884919852018356 Validation loss 0.06732607632875443 Accuracy 0.8165000677108765\n",
      "Output tensor([[0.2452],\n",
      "        [0.3214]], device='mps:0')\n",
      "Iteration 6890 Training loss 0.06627500057220459 Validation loss 0.0672527402639389 Accuracy 0.8176250457763672\n",
      "Output tensor([[0.0515],\n",
      "        [0.9113]], device='mps:0')\n",
      "Iteration 6900 Training loss 0.07395916432142258 Validation loss 0.06751161813735962 Accuracy 0.815500020980835\n",
      "Output tensor([[0.9316],\n",
      "        [0.7156]], device='mps:0')\n",
      "Iteration 6910 Training loss 0.06435710936784744 Validation loss 0.06725525110960007 Accuracy 0.8158750534057617\n",
      "Output tensor([[0.6889],\n",
      "        [0.5962]], device='mps:0')\n",
      "Iteration 6920 Training loss 0.059231746941804886 Validation loss 0.06748545914888382 Accuracy 0.8158750534057617\n",
      "Output tensor([[0.2580],\n",
      "        [0.9246]], device='mps:0')\n",
      "Iteration 6930 Training loss 0.06692550331354141 Validation loss 0.06761793792247772 Accuracy 0.8142500519752502\n",
      "Output tensor([[0.4285],\n",
      "        [0.9487]], device='mps:0')\n",
      "Iteration 6940 Training loss 0.06727783381938934 Validation loss 0.06722505390644073 Accuracy 0.8178750276565552\n",
      "Output tensor([[0.1916],\n",
      "        [0.9688]], device='mps:0')\n",
      "Iteration 6950 Training loss 0.058174990117549896 Validation loss 0.06727274507284164 Accuracy 0.8162500262260437\n",
      "Output tensor([[0.6925],\n",
      "        [0.1231]], device='mps:0')\n",
      "Iteration 6960 Training loss 0.06602101773023605 Validation loss 0.06724173575639725 Accuracy 0.8167500495910645\n",
      "Output tensor([[0.4546],\n",
      "        [0.8680]], device='mps:0')\n",
      "Iteration 6970 Training loss 0.06333605200052261 Validation loss 0.06725271046161652 Accuracy 0.8161250352859497\n",
      "Output tensor([[0.4299],\n",
      "        [0.6834]], device='mps:0')\n",
      "Iteration 6980 Training loss 0.06602593511343002 Validation loss 0.06720924377441406 Accuracy 0.8170000314712524\n",
      "Output tensor([[0.6395],\n",
      "        [0.4793]], device='mps:0')\n",
      "Iteration 6990 Training loss 0.0674329623579979 Validation loss 0.06719855964183807 Accuracy 0.8170000314712524\n",
      "Output tensor([[0.9221],\n",
      "        [0.0833]], device='mps:0')\n",
      "Iteration 7000 Training loss 0.06396803259849548 Validation loss 0.06721800565719604 Accuracy 0.8166250586509705\n",
      "Output tensor([[0.9513],\n",
      "        [0.6121]], device='mps:0')\n",
      "Iteration 7010 Training loss 0.07241226732730865 Validation loss 0.06737177073955536 Accuracy 0.815500020980835\n",
      "Output tensor([[0.0685],\n",
      "        [0.3081]], device='mps:0')\n",
      "Iteration 7020 Training loss 0.06549927592277527 Validation loss 0.06734617054462433 Accuracy 0.815500020980835\n",
      "Output tensor([[0.3662],\n",
      "        [0.9937]], device='mps:0')\n",
      "Iteration 7030 Training loss 0.05873088538646698 Validation loss 0.06725732982158661 Accuracy 0.8158750534057617\n",
      "Output tensor([[0.1004],\n",
      "        [0.1321]], device='mps:0')\n",
      "Iteration 7040 Training loss 0.06910862028598785 Validation loss 0.06726068258285522 Accuracy 0.8167500495910645\n",
      "Output tensor([[0.7136],\n",
      "        [0.0653]], device='mps:0')\n",
      "Iteration 7050 Training loss 0.06828439235687256 Validation loss 0.06716403365135193 Accuracy 0.8161250352859497\n",
      "Output tensor([[0.9156],\n",
      "        [0.0774]], device='mps:0')\n",
      "Iteration 7060 Training loss 0.06389112025499344 Validation loss 0.06734266132116318 Accuracy 0.8162500262260437\n",
      "Output tensor([[0.0149],\n",
      "        [0.9010]], device='mps:0')\n",
      "Iteration 7070 Training loss 0.06391550600528717 Validation loss 0.06719931215047836 Accuracy 0.8167500495910645\n",
      "Output tensor([[0.9663],\n",
      "        [0.7370]], device='mps:0')\n",
      "Iteration 7080 Training loss 0.06644585728645325 Validation loss 0.06713985651731491 Accuracy 0.8177500367164612\n",
      "Output tensor([[0.2971],\n",
      "        [0.2382]], device='mps:0')\n",
      "Iteration 7090 Training loss 0.06544160097837448 Validation loss 0.06712314486503601 Accuracy 0.8177500367164612\n",
      "Output tensor([[0.2395],\n",
      "        [0.1387]], device='mps:0')\n",
      "Iteration 7100 Training loss 0.07245561480522156 Validation loss 0.06713495403528214 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.7030],\n",
      "        [0.0662]], device='mps:0')\n",
      "Iteration 7110 Training loss 0.07587791979312897 Validation loss 0.06721954047679901 Accuracy 0.8166250586509705\n",
      "Output tensor([[0.1464],\n",
      "        [0.7065]], device='mps:0')\n",
      "Iteration 7120 Training loss 0.06271292269229889 Validation loss 0.06722842901945114 Accuracy 0.815625011920929\n",
      "Output tensor([[0.0731],\n",
      "        [0.9005]], device='mps:0')\n",
      "Iteration 7130 Training loss 0.06767233461141586 Validation loss 0.06715674698352814 Accuracy 0.8171250224113464\n",
      "Output tensor([[0.5440],\n",
      "        [0.6621]], device='mps:0')\n",
      "Iteration 7140 Training loss 0.058307476341724396 Validation loss 0.06722969561815262 Accuracy 0.815500020980835\n",
      "Output tensor([[0.9307],\n",
      "        [0.3024]], device='mps:0')\n",
      "Iteration 7150 Training loss 0.06525157392024994 Validation loss 0.06722470372915268 Accuracy 0.8166250586509705\n",
      "Output tensor([[0.1141],\n",
      "        [0.4966]], device='mps:0')\n",
      "Iteration 7160 Training loss 0.06150369346141815 Validation loss 0.0671028196811676 Accuracy 0.8173750638961792\n",
      "Output tensor([[0.4774],\n",
      "        [0.4125]], device='mps:0')\n",
      "Iteration 7170 Training loss 0.06550999730825424 Validation loss 0.0673530176281929 Accuracy 0.815500020980835\n",
      "Output tensor([[0.1183],\n",
      "        [0.3335]], device='mps:0')\n",
      "Iteration 7180 Training loss 0.05725516751408577 Validation loss 0.06709837913513184 Accuracy 0.8166250586509705\n",
      "Output tensor([[0.3712],\n",
      "        [0.2012]], device='mps:0')\n",
      "Iteration 7190 Training loss 0.06997735053300858 Validation loss 0.06709809601306915 Accuracy 0.8176250457763672\n",
      "Output tensor([[0.1261],\n",
      "        [0.7293]], device='mps:0')\n",
      "Iteration 7200 Training loss 0.06419520080089569 Validation loss 0.06710727512836456 Accuracy 0.8165000677108765\n",
      "Output tensor([[0.6757],\n",
      "        [0.3685]], device='mps:0')\n",
      "Iteration 7210 Training loss 0.0698268860578537 Validation loss 0.06712102144956589 Accuracy 0.8173750638961792\n",
      "Output tensor([[0.8495],\n",
      "        [0.9130]], device='mps:0')\n",
      "Iteration 7220 Training loss 0.0673709288239479 Validation loss 0.06707093119621277 Accuracy 0.8171250224113464\n",
      "Output tensor([[0.8390],\n",
      "        [0.1991]], device='mps:0')\n",
      "Iteration 7230 Training loss 0.06033732369542122 Validation loss 0.06736525148153305 Accuracy 0.8147500157356262\n",
      "Output tensor([[0.0941],\n",
      "        [0.8948]], device='mps:0')\n",
      "Iteration 7240 Training loss 0.05847826972603798 Validation loss 0.06717372685670853 Accuracy 0.8158750534057617\n",
      "Output tensor([[0.5529],\n",
      "        [0.8784]], device='mps:0')\n",
      "Iteration 7250 Training loss 0.06152617186307907 Validation loss 0.06706571578979492 Accuracy 0.8163750171661377\n",
      "Output tensor([[0.8226],\n",
      "        [0.3129]], device='mps:0')\n",
      "Iteration 7260 Training loss 0.06359976530075073 Validation loss 0.06707846373319626 Accuracy 0.8171250224113464\n",
      "Output tensor([[0.1239],\n",
      "        [0.4095]], device='mps:0')\n",
      "Iteration 7270 Training loss 0.06299083679914474 Validation loss 0.06702517718076706 Accuracy 0.8181250095367432\n",
      "Output tensor([[0.9966],\n",
      "        [0.3109]], device='mps:0')\n",
      "Iteration 7280 Training loss 0.0681772530078888 Validation loss 0.06703384965658188 Accuracy 0.8168750405311584\n",
      "Output tensor([[0.9273],\n",
      "        [0.0197]], device='mps:0')\n",
      "Iteration 7290 Training loss 0.0690600797533989 Validation loss 0.06702229380607605 Accuracy 0.8168750405311584\n",
      "Output tensor([[0.5034],\n",
      "        [0.6013]], device='mps:0')\n",
      "Iteration 7300 Training loss 0.0666753500699997 Validation loss 0.06700040400028229 Accuracy 0.8176250457763672\n",
      "Output tensor([[0.7280],\n",
      "        [0.9833]], device='mps:0')\n",
      "Iteration 7310 Training loss 0.06553792953491211 Validation loss 0.06704410165548325 Accuracy 0.8162500262260437\n",
      "Output tensor([[0.8273],\n",
      "        [0.1659]], device='mps:0')\n",
      "Iteration 7320 Training loss 0.06689482182264328 Validation loss 0.06711488962173462 Accuracy 0.8158750534057617\n",
      "Output tensor([[0.6732],\n",
      "        [0.9554]], device='mps:0')\n",
      "Iteration 7330 Training loss 0.0673472136259079 Validation loss 0.06704221665859222 Accuracy 0.8176250457763672\n",
      "Output tensor([[0.3268],\n",
      "        [0.3077]], device='mps:0')\n",
      "Iteration 7340 Training loss 0.07164201885461807 Validation loss 0.06717821210622787 Accuracy 0.8161250352859497\n",
      "Output tensor([[0.7823],\n",
      "        [0.4494]], device='mps:0')\n",
      "Iteration 7350 Training loss 0.06444390118122101 Validation loss 0.06715508550405502 Accuracy 0.8162500262260437\n",
      "Output tensor([[0.9351],\n",
      "        [0.1491]], device='mps:0')\n",
      "Iteration 7360 Training loss 0.07376094162464142 Validation loss 0.06698618084192276 Accuracy 0.8178750276565552\n",
      "Output tensor([[0.9009],\n",
      "        [0.1881]], device='mps:0')\n",
      "Iteration 7370 Training loss 0.061791472136974335 Validation loss 0.06697952747344971 Accuracy 0.8181250095367432\n",
      "Output tensor([[0.0903],\n",
      "        [0.9546]], device='mps:0')\n",
      "Iteration 7380 Training loss 0.06829500198364258 Validation loss 0.06698937714099884 Accuracy 0.8176250457763672\n",
      "Output tensor([[0.9360],\n",
      "        [0.3159]], device='mps:0')\n",
      "Iteration 7390 Training loss 0.064289890229702 Validation loss 0.06695661693811417 Accuracy 0.8186250329017639\n",
      "Output tensor([[0.7068],\n",
      "        [0.9703]], device='mps:0')\n",
      "Iteration 7400 Training loss 0.06664606183767319 Validation loss 0.06694582104682922 Accuracy 0.8181250095367432\n",
      "Output tensor([[0.3952],\n",
      "        [0.1980]], device='mps:0')\n",
      "Iteration 7410 Training loss 0.0642002671957016 Validation loss 0.06696377694606781 Accuracy 0.8168750405311584\n",
      "Output tensor([[0.1036],\n",
      "        [0.0338]], device='mps:0')\n",
      "Iteration 7420 Training loss 0.0711631327867508 Validation loss 0.06699507683515549 Accuracy 0.8177500367164612\n",
      "Output tensor([[0.7953],\n",
      "        [0.2193]], device='mps:0')\n",
      "Iteration 7430 Training loss 0.06573991477489471 Validation loss 0.06698985397815704 Accuracy 0.8177500367164612\n",
      "Output tensor([[0.9315],\n",
      "        [0.6288]], device='mps:0')\n",
      "Iteration 7440 Training loss 0.06527022272348404 Validation loss 0.06697456538677216 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.0321],\n",
      "        [0.9275]], device='mps:0')\n",
      "Iteration 7450 Training loss 0.0686793252825737 Validation loss 0.06693267822265625 Accuracy 0.8178750276565552\n",
      "Output tensor([[0.3604],\n",
      "        [0.5072]], device='mps:0')\n",
      "Iteration 7460 Training loss 0.07087244838476181 Validation loss 0.06693990528583527 Accuracy 0.8177500367164612\n",
      "Output tensor([[0.8287],\n",
      "        [0.4202]], device='mps:0')\n",
      "Iteration 7470 Training loss 0.07072026282548904 Validation loss 0.06762930750846863 Accuracy 0.815125048160553\n",
      "Output tensor([[0.3441],\n",
      "        [0.2736]], device='mps:0')\n",
      "Iteration 7480 Training loss 0.06579788774251938 Validation loss 0.0669640451669693 Accuracy 0.8176250457763672\n",
      "Output tensor([[0.2730],\n",
      "        [0.6709]], device='mps:0')\n",
      "Iteration 7490 Training loss 0.07181962579488754 Validation loss 0.066910021007061 Accuracy 0.8177500367164612\n",
      "Output tensor([[0.9301],\n",
      "        [0.1382]], device='mps:0')\n",
      "Iteration 7500 Training loss 0.07135014981031418 Validation loss 0.06692399829626083 Accuracy 0.8175000548362732\n",
      "Output tensor([[0.7090],\n",
      "        [0.5850]], device='mps:0')\n",
      "Iteration 7510 Training loss 0.06560884416103363 Validation loss 0.06689830124378204 Accuracy 0.8178750276565552\n",
      "Output tensor([[0.3879],\n",
      "        [0.6394]], device='mps:0')\n",
      "Iteration 7520 Training loss 0.06534473598003387 Validation loss 0.06689395755529404 Accuracy 0.8183750510215759\n",
      "Output tensor([[0.5062],\n",
      "        [0.4519]], device='mps:0')\n",
      "Iteration 7530 Training loss 0.06534033268690109 Validation loss 0.06689312309026718 Accuracy 0.8183750510215759\n",
      "Output tensor([[0.0865],\n",
      "        [0.2127]], device='mps:0')\n",
      "Iteration 7540 Training loss 0.06244400516152382 Validation loss 0.06689038127660751 Accuracy 0.8186250329017639\n",
      "Output tensor([[0.9810],\n",
      "        [0.8708]], device='mps:0')\n",
      "Iteration 7550 Training loss 0.06386248767375946 Validation loss 0.06701036542654037 Accuracy 0.8160000443458557\n",
      "Output tensor([[0.9521],\n",
      "        [0.5228]], device='mps:0')\n",
      "Iteration 7560 Training loss 0.06888537108898163 Validation loss 0.06695649772882462 Accuracy 0.8166250586509705\n",
      "Output tensor([[0.1889],\n",
      "        [0.7869]], device='mps:0')\n",
      "Iteration 7570 Training loss 0.059755172580480576 Validation loss 0.06692621111869812 Accuracy 0.8173750638961792\n",
      "Output tensor([[0.0660],\n",
      "        [0.6328]], device='mps:0')\n",
      "Iteration 7580 Training loss 0.06746950000524521 Validation loss 0.0671318843960762 Accuracy 0.815375030040741\n",
      "Output tensor([[0.0770],\n",
      "        [0.7933]], device='mps:0')\n",
      "Iteration 7590 Training loss 0.06328192353248596 Validation loss 0.06687719374895096 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.5649],\n",
      "        [0.6870]], device='mps:0')\n",
      "Iteration 7600 Training loss 0.06097710505127907 Validation loss 0.06700827181339264 Accuracy 0.8163750171661377\n",
      "Output tensor([[0.8846],\n",
      "        [0.2924]], device='mps:0')\n",
      "Iteration 7610 Training loss 0.06682858616113663 Validation loss 0.06684467196464539 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.1789],\n",
      "        [0.9360]], device='mps:0')\n",
      "Iteration 7620 Training loss 0.07068847119808197 Validation loss 0.06686332076787949 Accuracy 0.8165000677108765\n",
      "Output tensor([[0.8451],\n",
      "        [0.3059]], device='mps:0')\n",
      "Iteration 7630 Training loss 0.06822627037763596 Validation loss 0.06697152554988861 Accuracy 0.8167500495910645\n",
      "Output tensor([[0.0458],\n",
      "        [0.0528]], device='mps:0')\n",
      "Iteration 7640 Training loss 0.07165578007698059 Validation loss 0.06691651791334152 Accuracy 0.8166250586509705\n",
      "Output tensor([[0.3864],\n",
      "        [0.2095]], device='mps:0')\n",
      "Iteration 7650 Training loss 0.06945555657148361 Validation loss 0.06683695316314697 Accuracy 0.8173750638961792\n",
      "Output tensor([[0.1364],\n",
      "        [0.0604]], device='mps:0')\n",
      "Iteration 7660 Training loss 0.06750908493995667 Validation loss 0.06681294739246368 Accuracy 0.8177500367164612\n",
      "Output tensor([[0.9109],\n",
      "        [0.6284]], device='mps:0')\n",
      "Iteration 7670 Training loss 0.06424527615308762 Validation loss 0.06681452691555023 Accuracy 0.8173750638961792\n",
      "Output tensor([[0.1237],\n",
      "        [0.0456]], device='mps:0')\n",
      "Iteration 7680 Training loss 0.06607653200626373 Validation loss 0.06686615943908691 Accuracy 0.8165000677108765\n",
      "Output tensor([[0.5057],\n",
      "        [0.4325]], device='mps:0')\n",
      "Iteration 7690 Training loss 0.0651443600654602 Validation loss 0.0667935162782669 Accuracy 0.8176250457763672\n",
      "Output tensor([[0.0819],\n",
      "        [0.0813]], device='mps:0')\n",
      "Iteration 7700 Training loss 0.06369778513908386 Validation loss 0.06697198003530502 Accuracy 0.8167500495910645\n",
      "Output tensor([[0.4841],\n",
      "        [0.6098]], device='mps:0')\n",
      "Iteration 7710 Training loss 0.06956996023654938 Validation loss 0.06692942976951599 Accuracy 0.8170000314712524\n",
      "Output tensor([[0.2302],\n",
      "        [0.1225]], device='mps:0')\n",
      "Iteration 7720 Training loss 0.06078040599822998 Validation loss 0.06679678708314896 Accuracy 0.8177500367164612\n",
      "Output tensor([[0.2071],\n",
      "        [0.0374]], device='mps:0')\n",
      "Iteration 7730 Training loss 0.0660117045044899 Validation loss 0.06679695099592209 Accuracy 0.8188750147819519\n",
      "Output tensor([[0.8694],\n",
      "        [0.0258]], device='mps:0')\n",
      "Iteration 7740 Training loss 0.07193505018949509 Validation loss 0.06679215282201767 Accuracy 0.8177500367164612\n",
      "Output tensor([[0.7344],\n",
      "        [0.1776]], device='mps:0')\n",
      "Iteration 7750 Training loss 0.06785151362419128 Validation loss 0.0667976588010788 Accuracy 0.8162500262260437\n",
      "Output tensor([[0.9123],\n",
      "        [0.9513]], device='mps:0')\n",
      "Iteration 7760 Training loss 0.06777038425207138 Validation loss 0.06737376749515533 Accuracy 0.815250039100647\n",
      "Output tensor([[0.1929],\n",
      "        [0.1464]], device='mps:0')\n",
      "Iteration 7770 Training loss 0.06781139969825745 Validation loss 0.06688287854194641 Accuracy 0.8185000419616699\n",
      "Output tensor([[0.0884],\n",
      "        [0.8224]], device='mps:0')\n",
      "Iteration 7780 Training loss 0.06552261114120483 Validation loss 0.06747951358556747 Accuracy 0.8146250247955322\n",
      "Output tensor([[0.9686],\n",
      "        [0.8189]], device='mps:0')\n",
      "Iteration 7790 Training loss 0.06916708499193192 Validation loss 0.06678738445043564 Accuracy 0.8168750405311584\n",
      "Output tensor([[0.1244],\n",
      "        [0.1723]], device='mps:0')\n",
      "Iteration 7800 Training loss 0.06886184960603714 Validation loss 0.06673483550548553 Accuracy 0.8191250562667847\n",
      "Output tensor([[0.1261],\n",
      "        [0.9087]], device='mps:0')\n",
      "Iteration 7810 Training loss 0.06751005351543427 Validation loss 0.06673578172922134 Accuracy 0.8185000419616699\n",
      "Output tensor([[0.2798],\n",
      "        [0.1630]], device='mps:0')\n",
      "Iteration 7820 Training loss 0.0656202882528305 Validation loss 0.06673726439476013 Accuracy 0.8178750276565552\n",
      "Output tensor([[0.9815],\n",
      "        [0.9685]], device='mps:0')\n",
      "Iteration 7830 Training loss 0.06389008462429047 Validation loss 0.06673039495944977 Accuracy 0.8178750276565552\n",
      "Output tensor([[0.7826],\n",
      "        [0.2132]], device='mps:0')\n",
      "Iteration 7840 Training loss 0.059209804981946945 Validation loss 0.0667518675327301 Accuracy 0.8177500367164612\n",
      "Output tensor([[0.3689],\n",
      "        [0.0070]], device='mps:0')\n",
      "Iteration 7850 Training loss 0.06534341722726822 Validation loss 0.06671556830406189 Accuracy 0.8182500600814819\n",
      "Output tensor([[0.3039],\n",
      "        [0.2851]], device='mps:0')\n",
      "Iteration 7860 Training loss 0.06743914633989334 Validation loss 0.06670724600553513 Accuracy 0.8190000653266907\n",
      "Output tensor([[0.2115],\n",
      "        [0.0701]], device='mps:0')\n",
      "Iteration 7870 Training loss 0.0625474601984024 Validation loss 0.06671121716499329 Accuracy 0.8185000419616699\n",
      "Output tensor([[0.3945],\n",
      "        [0.0943]], device='mps:0')\n",
      "Iteration 7880 Training loss 0.06711279600858688 Validation loss 0.06669788062572479 Accuracy 0.8191250562667847\n",
      "Output tensor([[0.0371],\n",
      "        [0.4141]], device='mps:0')\n",
      "Iteration 7890 Training loss 0.059428002685308456 Validation loss 0.06671149283647537 Accuracy 0.8175000548362732\n",
      "Output tensor([[0.9593],\n",
      "        [0.4651]], device='mps:0')\n",
      "Iteration 7900 Training loss 0.0653202086687088 Validation loss 0.06669137626886368 Accuracy 0.8188750147819519\n",
      "Output tensor([[0.8233],\n",
      "        [0.8166]], device='mps:0')\n",
      "Iteration 7910 Training loss 0.06411712616682053 Validation loss 0.06669286638498306 Accuracy 0.8186250329017639\n",
      "Output tensor([[0.2352],\n",
      "        [0.2714]], device='mps:0')\n",
      "Iteration 7920 Training loss 0.0641799345612526 Validation loss 0.06696026772260666 Accuracy 0.8157500624656677\n",
      "Output tensor([[0.9600],\n",
      "        [0.9066]], device='mps:0')\n",
      "Iteration 7930 Training loss 0.07244052737951279 Validation loss 0.06667580455541611 Accuracy 0.8182500600814819\n",
      "Output tensor([[0.5347],\n",
      "        [0.6210]], device='mps:0')\n",
      "Iteration 7940 Training loss 0.076427161693573 Validation loss 0.06671487540006638 Accuracy 0.8188750147819519\n",
      "Output tensor([[0.0623],\n",
      "        [0.9174]], device='mps:0')\n",
      "Iteration 7950 Training loss 0.06817138195037842 Validation loss 0.06665866076946259 Accuracy 0.8185000419616699\n",
      "Output tensor([[0.8130],\n",
      "        [0.1020]], device='mps:0')\n",
      "Iteration 7960 Training loss 0.06578423082828522 Validation loss 0.06667538732290268 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.9756],\n",
      "        [0.1622]], device='mps:0')\n",
      "Iteration 7970 Training loss 0.06614626944065094 Validation loss 0.06676553934812546 Accuracy 0.8183750510215759\n",
      "Output tensor([[0.0385],\n",
      "        [0.3591]], device='mps:0')\n",
      "Iteration 7980 Training loss 0.0672103613615036 Validation loss 0.06664146482944489 Accuracy 0.8188750147819519\n",
      "Output tensor([[0.8838],\n",
      "        [0.8118]], device='mps:0')\n",
      "Iteration 7990 Training loss 0.06648996472358704 Validation loss 0.06665049493312836 Accuracy 0.8181250095367432\n",
      "Output tensor([[0.7528],\n",
      "        [0.3289]], device='mps:0')\n",
      "Iteration 8000 Training loss 0.06313606351613998 Validation loss 0.06685712188482285 Accuracy 0.8173750638961792\n",
      "Output tensor([[0.2548],\n",
      "        [0.9177]], device='mps:0')\n",
      "Iteration 8010 Training loss 0.060319241136312485 Validation loss 0.06666028499603271 Accuracy 0.8178750276565552\n",
      "Output tensor([[0.8430],\n",
      "        [0.8146]], device='mps:0')\n",
      "Iteration 8020 Training loss 0.06951884925365448 Validation loss 0.06663817912340164 Accuracy 0.8176250457763672\n",
      "Output tensor([[0.7745],\n",
      "        [0.8984]], device='mps:0')\n",
      "Iteration 8030 Training loss 0.059316929429769516 Validation loss 0.06666935980319977 Accuracy 0.8183750510215759\n",
      "Output tensor([[0.9850],\n",
      "        [0.1299]], device='mps:0')\n",
      "Iteration 8040 Training loss 0.06646507233381271 Validation loss 0.06661832332611084 Accuracy 0.8181250095367432\n",
      "Output tensor([[0.2848],\n",
      "        [0.9598]], device='mps:0')\n",
      "Iteration 8050 Training loss 0.06399529427289963 Validation loss 0.06670328974723816 Accuracy 0.8177500367164612\n",
      "Output tensor([[0.2551],\n",
      "        [0.8863]], device='mps:0')\n",
      "Iteration 8060 Training loss 0.06406594067811966 Validation loss 0.06660889089107513 Accuracy 0.8181250095367432\n",
      "Output tensor([[0.0212],\n",
      "        [0.1622]], device='mps:0')\n",
      "Iteration 8070 Training loss 0.06224796175956726 Validation loss 0.06658319383859634 Accuracy 0.8186250329017639\n",
      "Output tensor([[0.0748],\n",
      "        [0.2688]], device='mps:0')\n",
      "Iteration 8080 Training loss 0.0695299506187439 Validation loss 0.06668895483016968 Accuracy 0.8186250329017639\n",
      "Output tensor([[0.7664],\n",
      "        [0.3634]], device='mps:0')\n",
      "Iteration 8090 Training loss 0.06536441296339035 Validation loss 0.06658709794282913 Accuracy 0.8190000653266907\n",
      "Output tensor([[0.8534],\n",
      "        [0.9198]], device='mps:0')\n",
      "Iteration 8100 Training loss 0.066582590341568 Validation loss 0.06700246036052704 Accuracy 0.815375030040741\n",
      "Output tensor([[0.4201],\n",
      "        [0.0132]], device='mps:0')\n",
      "Iteration 8110 Training loss 0.06808041781187057 Validation loss 0.06656491756439209 Accuracy 0.8187500238418579\n",
      "Output tensor([[0.6102],\n",
      "        [0.3936]], device='mps:0')\n",
      "Iteration 8120 Training loss 0.06072380766272545 Validation loss 0.06676328182220459 Accuracy 0.8172500133514404\n",
      "Output tensor([[0.5403],\n",
      "        [0.0607]], device='mps:0')\n",
      "Iteration 8130 Training loss 0.05848105624318123 Validation loss 0.06654513627290726 Accuracy 0.8197500109672546\n",
      "Output tensor([[0.1540],\n",
      "        [0.4173]], device='mps:0')\n",
      "Iteration 8140 Training loss 0.0656518042087555 Validation loss 0.06661131232976913 Accuracy 0.8182500600814819\n",
      "Output tensor([[0.1178],\n",
      "        [0.0577]], device='mps:0')\n",
      "Iteration 8150 Training loss 0.061814773827791214 Validation loss 0.06653933972120285 Accuracy 0.8196250200271606\n",
      "Output tensor([[0.4156],\n",
      "        [0.3619]], device='mps:0')\n",
      "Iteration 8160 Training loss 0.06661995500326157 Validation loss 0.06653554737567902 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.9276],\n",
      "        [0.0234]], device='mps:0')\n",
      "Iteration 8170 Training loss 0.06749043613672256 Validation loss 0.06660647690296173 Accuracy 0.8178750276565552\n",
      "Output tensor([[0.3453],\n",
      "        [0.0331]], device='mps:0')\n",
      "Iteration 8180 Training loss 0.06614122539758682 Validation loss 0.0665663555264473 Accuracy 0.8178750276565552\n",
      "Output tensor([[0.2641],\n",
      "        [0.7795]], device='mps:0')\n",
      "Iteration 8190 Training loss 0.06416718661785126 Validation loss 0.06664470583200455 Accuracy 0.8181250095367432\n",
      "Output tensor([[0.0929],\n",
      "        [0.0572]], device='mps:0')\n",
      "Iteration 8200 Training loss 0.0630187913775444 Validation loss 0.06663580983877182 Accuracy 0.8182500600814819\n",
      "Output tensor([[0.2815],\n",
      "        [0.4220]], device='mps:0')\n",
      "Iteration 8210 Training loss 0.07680557668209076 Validation loss 0.06666359305381775 Accuracy 0.8183750510215759\n",
      "Output tensor([[0.0124],\n",
      "        [0.7265]], device='mps:0')\n",
      "Iteration 8220 Training loss 0.0710572674870491 Validation loss 0.06649603694677353 Accuracy 0.8193750381469727\n",
      "Output tensor([[0.5866],\n",
      "        [0.8120]], device='mps:0')\n",
      "Iteration 8230 Training loss 0.06598584353923798 Validation loss 0.0665288195014 Accuracy 0.8181250095367432\n",
      "Output tensor([[0.1105],\n",
      "        [0.7881]], device='mps:0')\n",
      "Iteration 8240 Training loss 0.06891553103923798 Validation loss 0.06649243086576462 Accuracy 0.8193750381469727\n",
      "Output tensor([[0.8549],\n",
      "        [0.2376]], device='mps:0')\n",
      "Iteration 8250 Training loss 0.06625848263502121 Validation loss 0.06660299003124237 Accuracy 0.8183750510215759\n",
      "Output tensor([[0.8226],\n",
      "        [0.2074]], device='mps:0')\n",
      "Iteration 8260 Training loss 0.06706583499908447 Validation loss 0.0664810985326767 Accuracy 0.8191250562667847\n",
      "Output tensor([[0.0340],\n",
      "        [0.3709]], device='mps:0')\n",
      "Iteration 8270 Training loss 0.06882289797067642 Validation loss 0.0666264221072197 Accuracy 0.8178750276565552\n",
      "Output tensor([[0.3359],\n",
      "        [0.2856]], device='mps:0')\n",
      "Iteration 8280 Training loss 0.06685025244951248 Validation loss 0.06647209078073502 Accuracy 0.8190000653266907\n",
      "Output tensor([[0.0486],\n",
      "        [0.4845]], device='mps:0')\n",
      "Iteration 8290 Training loss 0.0625060647726059 Validation loss 0.06645052880048752 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.9781],\n",
      "        [0.2392]], device='mps:0')\n",
      "Iteration 8300 Training loss 0.05267114192247391 Validation loss 0.06644115597009659 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.8200],\n",
      "        [0.0644]], device='mps:0')\n",
      "Iteration 8310 Training loss 0.06564103811979294 Validation loss 0.06643080711364746 Accuracy 0.8200000524520874\n",
      "Output tensor([[0.6435],\n",
      "        [0.3539]], device='mps:0')\n",
      "Iteration 8320 Training loss 0.07429642230272293 Validation loss 0.06644133478403091 Accuracy 0.8191250562667847\n",
      "Output tensor([[0.3412],\n",
      "        [0.3231]], device='mps:0')\n",
      "Iteration 8330 Training loss 0.06659292429685593 Validation loss 0.06645523756742477 Accuracy 0.8187500238418579\n",
      "Output tensor([[0.2053],\n",
      "        [0.1411]], device='mps:0')\n",
      "Iteration 8340 Training loss 0.06451045721769333 Validation loss 0.06650196760892868 Accuracy 0.8180000185966492\n",
      "Output tensor([[0.9091],\n",
      "        [0.1958]], device='mps:0')\n",
      "Iteration 8350 Training loss 0.05803220719099045 Validation loss 0.06664199382066727 Accuracy 0.8173750638961792\n",
      "Output tensor([[0.3387],\n",
      "        [0.3596]], device='mps:0')\n",
      "Iteration 8360 Training loss 0.06474617123603821 Validation loss 0.06662604957818985 Accuracy 0.8173750638961792\n",
      "Output tensor([[0.0880],\n",
      "        [0.0063]], device='mps:0')\n",
      "Iteration 8370 Training loss 0.06479226797819138 Validation loss 0.06645675748586655 Accuracy 0.8182500600814819\n",
      "Output tensor([[0.6319],\n",
      "        [0.0915]], device='mps:0')\n",
      "Iteration 8380 Training loss 0.060403622686862946 Validation loss 0.06654423475265503 Accuracy 0.8177500367164612\n",
      "Output tensor([[0.3576],\n",
      "        [0.9377]], device='mps:0')\n",
      "Iteration 8390 Training loss 0.05915214493870735 Validation loss 0.06640830636024475 Accuracy 0.8200000524520874\n",
      "Output tensor([[0.3491],\n",
      "        [0.0804]], device='mps:0')\n",
      "Iteration 8400 Training loss 0.061966560781002045 Validation loss 0.06645183265209198 Accuracy 0.8186250329017639\n",
      "Output tensor([[0.3845],\n",
      "        [0.9575]], device='mps:0')\n",
      "Iteration 8410 Training loss 0.0676298663020134 Validation loss 0.06642807275056839 Accuracy 0.8186250329017639\n",
      "Output tensor([[0.9623],\n",
      "        [0.9615]], device='mps:0')\n",
      "Iteration 8420 Training loss 0.06652885675430298 Validation loss 0.06641615927219391 Accuracy 0.8186250329017639\n",
      "Output tensor([[0.1379],\n",
      "        [0.0159]], device='mps:0')\n",
      "Iteration 8430 Training loss 0.06314784288406372 Validation loss 0.06651917845010757 Accuracy 0.8181250095367432\n",
      "Output tensor([[0.9768],\n",
      "        [0.0719]], device='mps:0')\n",
      "Iteration 8440 Training loss 0.055723827332258224 Validation loss 0.06652520596981049 Accuracy 0.8177500367164612\n",
      "Output tensor([[0.8640],\n",
      "        [0.7776]], device='mps:0')\n",
      "Iteration 8450 Training loss 0.06912104040384293 Validation loss 0.06657031923532486 Accuracy 0.8171250224113464\n",
      "Output tensor([[0.0226],\n",
      "        [0.6289]], device='mps:0')\n",
      "Iteration 8460 Training loss 0.05860218033194542 Validation loss 0.06634635478258133 Accuracy 0.8207500576972961\n",
      "Output tensor([[0.9568],\n",
      "        [0.0744]], device='mps:0')\n",
      "Iteration 8470 Training loss 0.06515537202358246 Validation loss 0.06636139750480652 Accuracy 0.8192500472068787\n",
      "Output tensor([[0.8054],\n",
      "        [0.3867]], device='mps:0')\n",
      "Iteration 8480 Training loss 0.07583455741405487 Validation loss 0.06636258959770203 Accuracy 0.8190000653266907\n",
      "Output tensor([[0.9593],\n",
      "        [0.7169]], device='mps:0')\n",
      "Iteration 8490 Training loss 0.06478462368249893 Validation loss 0.06632878631353378 Accuracy 0.8207500576972961\n",
      "Output tensor([[0.0921],\n",
      "        [0.9829]], device='mps:0')\n",
      "Iteration 8500 Training loss 0.0622132271528244 Validation loss 0.06641464680433273 Accuracy 0.8186250329017639\n",
      "Output tensor([[0.0472],\n",
      "        [0.0687]], device='mps:0')\n",
      "Iteration 8510 Training loss 0.06015913933515549 Validation loss 0.06645350903272629 Accuracy 0.8188750147819519\n",
      "Output tensor([[0.1387],\n",
      "        [0.1613]], device='mps:0')\n",
      "Iteration 8520 Training loss 0.07059302926063538 Validation loss 0.06633695960044861 Accuracy 0.8198750615119934\n",
      "Output tensor([[0.0901],\n",
      "        [0.1271]], device='mps:0')\n",
      "Iteration 8530 Training loss 0.061828430742025375 Validation loss 0.06633149832487106 Accuracy 0.8197500109672546\n",
      "Output tensor([[0.3872],\n",
      "        [0.2289]], device='mps:0')\n",
      "Iteration 8540 Training loss 0.060719817876815796 Validation loss 0.06646393239498138 Accuracy 0.8178750276565552\n",
      "Output tensor([[0.6350],\n",
      "        [0.0096]], device='mps:0')\n",
      "Iteration 8550 Training loss 0.07178587466478348 Validation loss 0.06631705164909363 Accuracy 0.8203750252723694\n",
      "Output tensor([[0.8664],\n",
      "        [0.8703]], device='mps:0')\n",
      "Iteration 8560 Training loss 0.06747166812419891 Validation loss 0.06656359881162643 Accuracy 0.8166250586509705\n",
      "Output tensor([[0.6543],\n",
      "        [0.8438]], device='mps:0')\n",
      "Iteration 8570 Training loss 0.06860632449388504 Validation loss 0.0665183961391449 Accuracy 0.8170000314712524\n",
      "Output tensor([[0.0684],\n",
      "        [0.9760]], device='mps:0')\n",
      "Iteration 8580 Training loss 0.061085592955350876 Validation loss 0.06629738211631775 Accuracy 0.8207500576972961\n",
      "Output tensor([[0.0386],\n",
      "        [0.9296]], device='mps:0')\n",
      "Iteration 8590 Training loss 0.06552838534116745 Validation loss 0.06631550937891006 Accuracy 0.8203750252723694\n",
      "Output tensor([[0.0903],\n",
      "        [0.2367]], device='mps:0')\n",
      "Iteration 8600 Training loss 0.07657871395349503 Validation loss 0.06628837436437607 Accuracy 0.8207500576972961\n",
      "Output tensor([[0.6315],\n",
      "        [0.2792]], device='mps:0')\n",
      "Iteration 8610 Training loss 0.06630101054906845 Validation loss 0.06646078824996948 Accuracy 0.8190000653266907\n",
      "Output tensor([[0.0766],\n",
      "        [0.1742]], device='mps:0')\n",
      "Iteration 8620 Training loss 0.07871861010789871 Validation loss 0.066315196454525 Accuracy 0.8203750252723694\n",
      "Output tensor([[0.4056],\n",
      "        [0.3939]], device='mps:0')\n",
      "Iteration 8630 Training loss 0.06526941806077957 Validation loss 0.06631967425346375 Accuracy 0.8195000290870667\n",
      "Output tensor([[0.4293],\n",
      "        [0.1249]], device='mps:0')\n",
      "Iteration 8640 Training loss 0.06867149472236633 Validation loss 0.0662950947880745 Accuracy 0.8203750252723694\n",
      "Output tensor([[0.2880],\n",
      "        [0.8793]], device='mps:0')\n",
      "Iteration 8650 Training loss 0.07152842730283737 Validation loss 0.06628166884183884 Accuracy 0.8212500214576721\n",
      "Output tensor([[0.9045],\n",
      "        [0.2863]], device='mps:0')\n",
      "Iteration 8660 Training loss 0.06823312491178513 Validation loss 0.06629489362239838 Accuracy 0.8201250433921814\n",
      "Output tensor([[0.6437],\n",
      "        [0.9662]], device='mps:0')\n",
      "Iteration 8670 Training loss 0.06678254157304764 Validation loss 0.06657559424638748 Accuracy 0.8165000677108765\n",
      "Output tensor([[0.2362],\n",
      "        [0.9628]], device='mps:0')\n",
      "Iteration 8680 Training loss 0.06280098110437393 Validation loss 0.06625946611166 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.1736],\n",
      "        [0.3257]], device='mps:0')\n",
      "Iteration 8690 Training loss 0.06007920950651169 Validation loss 0.06652208417654037 Accuracy 0.8162500262260437\n",
      "Output tensor([[0.9625],\n",
      "        [0.2711]], device='mps:0')\n",
      "Iteration 8700 Training loss 0.05524131655693054 Validation loss 0.06631824374198914 Accuracy 0.8187500238418579\n",
      "Output tensor([[0.1401],\n",
      "        [0.6103]], device='mps:0')\n",
      "Iteration 8710 Training loss 0.06396738439798355 Validation loss 0.06630437821149826 Accuracy 0.8190000653266907\n",
      "Output tensor([[0.9184],\n",
      "        [0.3437]], device='mps:0')\n",
      "Iteration 8720 Training loss 0.06898772716522217 Validation loss 0.06640824675559998 Accuracy 0.8198750615119934\n",
      "Output tensor([[0.4353],\n",
      "        [0.9706]], device='mps:0')\n",
      "Iteration 8730 Training loss 0.06315009295940399 Validation loss 0.06623928993940353 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.0384],\n",
      "        [0.3668]], device='mps:0')\n",
      "Iteration 8740 Training loss 0.06431011110544205 Validation loss 0.06630926579236984 Accuracy 0.8187500238418579\n",
      "Output tensor([[0.7870],\n",
      "        [0.2829]], device='mps:0')\n",
      "Iteration 8750 Training loss 0.06478109955787659 Validation loss 0.0663897693157196 Accuracy 0.8198750615119934\n",
      "Output tensor([[0.1156],\n",
      "        [0.9188]], device='mps:0')\n",
      "Iteration 8760 Training loss 0.0706772655248642 Validation loss 0.06648964434862137 Accuracy 0.8181250095367432\n",
      "Output tensor([[0.1337],\n",
      "        [0.3312]], device='mps:0')\n",
      "Iteration 8770 Training loss 0.06333419680595398 Validation loss 0.0662091076374054 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.2088],\n",
      "        [0.9196]], device='mps:0')\n",
      "Iteration 8780 Training loss 0.06496443599462509 Validation loss 0.06634809821844101 Accuracy 0.8202500343322754\n",
      "Output tensor([[0.6261],\n",
      "        [0.7962]], device='mps:0')\n",
      "Iteration 8790 Training loss 0.06731830537319183 Validation loss 0.06619690358638763 Accuracy 0.8208750486373901\n",
      "Output tensor([[0.9592],\n",
      "        [0.0993]], device='mps:0')\n",
      "Iteration 8800 Training loss 0.0706881731748581 Validation loss 0.06635194271802902 Accuracy 0.8202500343322754\n",
      "Output tensor([[0.9628],\n",
      "        [0.1737]], device='mps:0')\n",
      "Iteration 8810 Training loss 0.06540771573781967 Validation loss 0.06624093651771545 Accuracy 0.8197500109672546\n",
      "Output tensor([[0.8175],\n",
      "        [0.1242]], device='mps:0')\n",
      "Iteration 8820 Training loss 0.05987014248967171 Validation loss 0.06618565320968628 Accuracy 0.8207500576972961\n",
      "Output tensor([[0.1619],\n",
      "        [0.9588]], device='mps:0')\n",
      "Iteration 8830 Training loss 0.06773751229047775 Validation loss 0.0669478327035904 Accuracy 0.8163750171661377\n",
      "Output tensor([[0.6311],\n",
      "        [0.0327]], device='mps:0')\n",
      "Iteration 8840 Training loss 0.06470637768507004 Validation loss 0.06674123555421829 Accuracy 0.8172500133514404\n",
      "Output tensor([[0.0498],\n",
      "        [0.8461]], device='mps:0')\n",
      "Iteration 8850 Training loss 0.05885697528719902 Validation loss 0.06620702147483826 Accuracy 0.8196250200271606\n",
      "Output tensor([[0.9772],\n",
      "        [0.0624]], device='mps:0')\n",
      "Iteration 8860 Training loss 0.05945264548063278 Validation loss 0.06615719944238663 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.8608],\n",
      "        [0.8554]], device='mps:0')\n",
      "Iteration 8870 Training loss 0.07281343638896942 Validation loss 0.06621187180280685 Accuracy 0.8191250562667847\n",
      "Output tensor([[0.4184],\n",
      "        [0.9566]], device='mps:0')\n",
      "Iteration 8880 Training loss 0.06553331017494202 Validation loss 0.06617734581232071 Accuracy 0.8193750381469727\n",
      "Output tensor([[0.2327],\n",
      "        [0.8247]], device='mps:0')\n",
      "Iteration 8890 Training loss 0.061741903424263 Validation loss 0.06618042290210724 Accuracy 0.8197500109672546\n",
      "Output tensor([[0.5611],\n",
      "        [0.7482]], device='mps:0')\n",
      "Iteration 8900 Training loss 0.059374045580625534 Validation loss 0.06612582504749298 Accuracy 0.8203750252723694\n",
      "Output tensor([[0.0197],\n",
      "        [0.0453]], device='mps:0')\n",
      "Iteration 8910 Training loss 0.06648826599121094 Validation loss 0.06614833325147629 Accuracy 0.8198750615119934\n",
      "Output tensor([[0.3062],\n",
      "        [0.8648]], device='mps:0')\n",
      "Iteration 8920 Training loss 0.057170361280441284 Validation loss 0.06612130999565125 Accuracy 0.8198750615119934\n",
      "Output tensor([[0.0495],\n",
      "        [0.3818]], device='mps:0')\n",
      "Iteration 8930 Training loss 0.06374072283506393 Validation loss 0.06611394137144089 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.3758],\n",
      "        [0.8807]], device='mps:0')\n",
      "Iteration 8940 Training loss 0.06267880648374557 Validation loss 0.06611093878746033 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.8364],\n",
      "        [0.1937]], device='mps:0')\n",
      "Iteration 8950 Training loss 0.052549950778484344 Validation loss 0.06609651446342468 Accuracy 0.8208750486373901\n",
      "Output tensor([[0.8188],\n",
      "        [0.5530]], device='mps:0')\n",
      "Iteration 8960 Training loss 0.07074622809886932 Validation loss 0.0661998838186264 Accuracy 0.8191250562667847\n",
      "Output tensor([[0.7033],\n",
      "        [0.9274]], device='mps:0')\n",
      "Iteration 8970 Training loss 0.07949338108301163 Validation loss 0.06609861552715302 Accuracy 0.8212500214576721\n",
      "Output tensor([[0.6935],\n",
      "        [0.2082]], device='mps:0')\n",
      "Iteration 8980 Training loss 0.06358996778726578 Validation loss 0.06609665602445602 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.0519],\n",
      "        [0.3152]], device='mps:0')\n",
      "Iteration 8990 Training loss 0.06290854513645172 Validation loss 0.06611400097608566 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.3519],\n",
      "        [0.8944]], device='mps:0')\n",
      "Iteration 9000 Training loss 0.06334342807531357 Validation loss 0.06610880047082901 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.0903],\n",
      "        [0.9612]], device='mps:0')\n",
      "Iteration 9010 Training loss 0.07061239331960678 Validation loss 0.06607803702354431 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.7630],\n",
      "        [0.1065]], device='mps:0')\n",
      "Iteration 9020 Training loss 0.06876502931118011 Validation loss 0.06608185172080994 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.0651],\n",
      "        [0.1226]], device='mps:0')\n",
      "Iteration 9030 Training loss 0.062326349318027496 Validation loss 0.06609556823968887 Accuracy 0.8200000524520874\n",
      "Output tensor([[0.8314],\n",
      "        [0.1627]], device='mps:0')\n",
      "Iteration 9040 Training loss 0.05924328789114952 Validation loss 0.06608503311872482 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.4185],\n",
      "        [0.6190]], device='mps:0')\n",
      "Iteration 9050 Training loss 0.06567505747079849 Validation loss 0.06608275324106216 Accuracy 0.8210000395774841\n",
      "Output tensor([[0.3756],\n",
      "        [0.2812]], device='mps:0')\n",
      "Iteration 9060 Training loss 0.07147800177335739 Validation loss 0.06605669856071472 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.5025],\n",
      "        [0.6327]], device='mps:0')\n",
      "Iteration 9070 Training loss 0.06783633679151535 Validation loss 0.06604934483766556 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.2049],\n",
      "        [0.5385]], device='mps:0')\n",
      "Iteration 9080 Training loss 0.0692162811756134 Validation loss 0.06608936190605164 Accuracy 0.8207500576972961\n",
      "Output tensor([[0.4062],\n",
      "        [0.0525]], device='mps:0')\n",
      "Iteration 9090 Training loss 0.06250886619091034 Validation loss 0.06612320244312286 Accuracy 0.8192500472068787\n",
      "Output tensor([[0.9080],\n",
      "        [0.0815]], device='mps:0')\n",
      "Iteration 9100 Training loss 0.06938831508159637 Validation loss 0.06606025993824005 Accuracy 0.8201250433921814\n",
      "Output tensor([[0.1774],\n",
      "        [0.9823]], device='mps:0')\n",
      "Iteration 9110 Training loss 0.07026876509189606 Validation loss 0.06604690849781036 Accuracy 0.8210000395774841\n",
      "Output tensor([[0.3649],\n",
      "        [0.2185]], device='mps:0')\n",
      "Iteration 9120 Training loss 0.06576003134250641 Validation loss 0.06608878076076508 Accuracy 0.8191250562667847\n",
      "Output tensor([[0.2100],\n",
      "        [0.1144]], device='mps:0')\n",
      "Iteration 9130 Training loss 0.060832567512989044 Validation loss 0.06603750586509705 Accuracy 0.8196250200271606\n",
      "Output tensor([[0.3094],\n",
      "        [0.1205]], device='mps:0')\n",
      "Iteration 9140 Training loss 0.06686988472938538 Validation loss 0.0661541000008583 Accuracy 0.8185000419616699\n",
      "Output tensor([[0.8731],\n",
      "        [0.6449]], device='mps:0')\n",
      "Iteration 9150 Training loss 0.06512868404388428 Validation loss 0.06604776531457901 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.1945],\n",
      "        [0.0745]], device='mps:0')\n",
      "Iteration 9160 Training loss 0.06339716911315918 Validation loss 0.06601035594940186 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.4854],\n",
      "        [0.9121]], device='mps:0')\n",
      "Iteration 9170 Training loss 0.06462246924638748 Validation loss 0.06599991023540497 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.1127],\n",
      "        [0.9723]], device='mps:0')\n",
      "Iteration 9180 Training loss 0.06589080393314362 Validation loss 0.06602538377046585 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.1359],\n",
      "        [0.9561]], device='mps:0')\n",
      "Iteration 9190 Training loss 0.07146947085857391 Validation loss 0.0660722404718399 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.6908],\n",
      "        [0.3781]], device='mps:0')\n",
      "Iteration 9200 Training loss 0.06601347774267197 Validation loss 0.06601447612047195 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.0270],\n",
      "        [0.0401]], device='mps:0')\n",
      "Iteration 9210 Training loss 0.07088211178779602 Validation loss 0.0659845694899559 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.1617],\n",
      "        [0.4898]], device='mps:0')\n",
      "Iteration 9220 Training loss 0.07125719636678696 Validation loss 0.0660443976521492 Accuracy 0.8203750252723694\n",
      "Output tensor([[0.9243],\n",
      "        [0.4202]], device='mps:0')\n",
      "Iteration 9230 Training loss 0.05810677260160446 Validation loss 0.06651399284601212 Accuracy 0.8170000314712524\n",
      "Output tensor([[0.6644],\n",
      "        [0.3131]], device='mps:0')\n",
      "Iteration 9240 Training loss 0.07159176468849182 Validation loss 0.06598719209432602 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.1831],\n",
      "        [0.7987]], device='mps:0')\n",
      "Iteration 9250 Training loss 0.06297089159488678 Validation loss 0.06604643911123276 Accuracy 0.8201250433921814\n",
      "Output tensor([[0.0986],\n",
      "        [0.0412]], device='mps:0')\n",
      "Iteration 9260 Training loss 0.05805176496505737 Validation loss 0.06604041904211044 Accuracy 0.8202500343322754\n",
      "Output tensor([[0.3934],\n",
      "        [0.1227]], device='mps:0')\n",
      "Iteration 9270 Training loss 0.05890434607863426 Validation loss 0.06603462249040604 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.3317],\n",
      "        [0.1554]], device='mps:0')\n",
      "Iteration 9280 Training loss 0.06521917134523392 Validation loss 0.06596444547176361 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.1265],\n",
      "        [0.8573]], device='mps:0')\n",
      "Iteration 9290 Training loss 0.06375747174024582 Validation loss 0.0662601888179779 Accuracy 0.8186250329017639\n",
      "Output tensor([[0.8690],\n",
      "        [0.3229]], device='mps:0')\n",
      "Iteration 9300 Training loss 0.05718449875712395 Validation loss 0.06595747917890549 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.2854],\n",
      "        [0.5028]], device='mps:0')\n",
      "Iteration 9310 Training loss 0.06683126091957092 Validation loss 0.06612014025449753 Accuracy 0.8190000653266907\n",
      "Output tensor([[0.3820],\n",
      "        [0.1132]], device='mps:0')\n",
      "Iteration 9320 Training loss 0.07274505496025085 Validation loss 0.06606192141771317 Accuracy 0.8200000524520874\n",
      "Output tensor([[0.4902],\n",
      "        [0.4809]], device='mps:0')\n",
      "Iteration 9330 Training loss 0.07443902641534805 Validation loss 0.06595275551080704 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.5369],\n",
      "        [0.8175]], device='mps:0')\n",
      "Iteration 9340 Training loss 0.06250409781932831 Validation loss 0.06594043225049973 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.2129],\n",
      "        [0.7560]], device='mps:0')\n",
      "Iteration 9350 Training loss 0.06396938115358353 Validation loss 0.06596434861421585 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.6264],\n",
      "        [0.9674]], device='mps:0')\n",
      "Iteration 9360 Training loss 0.0700925812125206 Validation loss 0.06602486968040466 Accuracy 0.8200000524520874\n",
      "Output tensor([[0.9845],\n",
      "        [0.6633]], device='mps:0')\n",
      "Iteration 9370 Training loss 0.06913603842258453 Validation loss 0.06592398881912231 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.3942],\n",
      "        [0.4252]], device='mps:0')\n",
      "Iteration 9380 Training loss 0.06422941386699677 Validation loss 0.0660034790635109 Accuracy 0.8201250433921814\n",
      "Output tensor([[0.3566],\n",
      "        [0.1988]], device='mps:0')\n",
      "Iteration 9390 Training loss 0.06326594948768616 Validation loss 0.0660010576248169 Accuracy 0.8201250433921814\n",
      "Output tensor([[0.1130],\n",
      "        [0.7187]], device='mps:0')\n",
      "Iteration 9400 Training loss 0.07139598578214645 Validation loss 0.06590033322572708 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.8736],\n",
      "        [0.5782]], device='mps:0')\n",
      "Iteration 9410 Training loss 0.07272032648324966 Validation loss 0.06590048968791962 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.0792],\n",
      "        [0.9379]], device='mps:0')\n",
      "Iteration 9420 Training loss 0.05700620636343956 Validation loss 0.06588832288980484 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.5898],\n",
      "        [0.7598]], device='mps:0')\n",
      "Iteration 9430 Training loss 0.06319428980350494 Validation loss 0.06587521731853485 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.2260],\n",
      "        [0.8707]], device='mps:0')\n",
      "Iteration 9440 Training loss 0.0656413584947586 Validation loss 0.06588704884052277 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.0607],\n",
      "        [0.4998]], device='mps:0')\n",
      "Iteration 9450 Training loss 0.06619402766227722 Validation loss 0.06591777503490448 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.7566],\n",
      "        [0.9885]], device='mps:0')\n",
      "Iteration 9460 Training loss 0.0709139034152031 Validation loss 0.06586848944425583 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.5058],\n",
      "        [0.9681]], device='mps:0')\n",
      "Iteration 9470 Training loss 0.07191238552331924 Validation loss 0.06585737317800522 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.3531],\n",
      "        [0.4052]], device='mps:0')\n",
      "Iteration 9480 Training loss 0.05874918773770332 Validation loss 0.0658499151468277 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.9554],\n",
      "        [0.0220]], device='mps:0')\n",
      "Iteration 9490 Training loss 0.06475520133972168 Validation loss 0.06591787934303284 Accuracy 0.8203750252723694\n",
      "Output tensor([[0.3860],\n",
      "        [0.1839]], device='mps:0')\n",
      "Iteration 9500 Training loss 0.06153876334428787 Validation loss 0.06584981083869934 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.9396],\n",
      "        [0.9914]], device='mps:0')\n",
      "Iteration 9510 Training loss 0.05991842970252037 Validation loss 0.06594257056713104 Accuracy 0.8196250200271606\n",
      "Output tensor([[0.5140],\n",
      "        [0.0028]], device='mps:0')\n",
      "Iteration 9520 Training loss 0.06597987562417984 Validation loss 0.06597412377595901 Accuracy 0.8210000395774841\n",
      "Output tensor([[0.6892],\n",
      "        [0.0474]], device='mps:0')\n",
      "Iteration 9530 Training loss 0.06307438760995865 Validation loss 0.06584108620882034 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.8236],\n",
      "        [0.9703]], device='mps:0')\n",
      "Iteration 9540 Training loss 0.07823167741298676 Validation loss 0.06610573083162308 Accuracy 0.8186250329017639\n",
      "Output tensor([[0.8962],\n",
      "        [0.0547]], device='mps:0')\n",
      "Iteration 9550 Training loss 0.06126639246940613 Validation loss 0.06588422507047653 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.4619],\n",
      "        [0.8969]], device='mps:0')\n",
      "Iteration 9560 Training loss 0.07475665211677551 Validation loss 0.06580851972103119 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.7793],\n",
      "        [0.8911]], device='mps:0')\n",
      "Iteration 9570 Training loss 0.05867726355791092 Validation loss 0.065800741314888 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.9351],\n",
      "        [0.3474]], device='mps:0')\n",
      "Iteration 9580 Training loss 0.06476771086454391 Validation loss 0.0657954216003418 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.0948],\n",
      "        [0.1570]], device='mps:0')\n",
      "Iteration 9590 Training loss 0.06551822274923325 Validation loss 0.06621002405881882 Accuracy 0.8190000653266907\n",
      "Output tensor([[0.0229],\n",
      "        [0.1768]], device='mps:0')\n",
      "Iteration 9600 Training loss 0.06271670013666153 Validation loss 0.06582856923341751 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.9914],\n",
      "        [0.4133]], device='mps:0')\n",
      "Iteration 9610 Training loss 0.07130461186170578 Validation loss 0.06582364439964294 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.1773],\n",
      "        [0.8467]], device='mps:0')\n",
      "Iteration 9620 Training loss 0.06468819826841354 Validation loss 0.06579864025115967 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.3381],\n",
      "        [0.7438]], device='mps:0')\n",
      "Iteration 9630 Training loss 0.06654142588376999 Validation loss 0.06576157361268997 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.0750],\n",
      "        [0.9315]], device='mps:0')\n",
      "Iteration 9640 Training loss 0.06421903520822525 Validation loss 0.06579705327749252 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.6644],\n",
      "        [0.2360]], device='mps:0')\n",
      "Iteration 9650 Training loss 0.06548157334327698 Validation loss 0.06602760404348373 Accuracy 0.8198750615119934\n",
      "Output tensor([[0.5922],\n",
      "        [0.4304]], device='mps:0')\n",
      "Iteration 9660 Training loss 0.06108718365430832 Validation loss 0.06583955138921738 Accuracy 0.8203750252723694\n",
      "Output tensor([[0.6170],\n",
      "        [0.9577]], device='mps:0')\n",
      "Iteration 9670 Training loss 0.06217724829912186 Validation loss 0.06582467257976532 Accuracy 0.8208750486373901\n",
      "Output tensor([[0.1870],\n",
      "        [0.8038]], device='mps:0')\n",
      "Iteration 9680 Training loss 0.06614828109741211 Validation loss 0.06600460410118103 Accuracy 0.8191250562667847\n",
      "Output tensor([[0.2862],\n",
      "        [0.2839]], device='mps:0')\n",
      "Iteration 9690 Training loss 0.072734534740448 Validation loss 0.0658377856016159 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.7263],\n",
      "        [0.7604]], device='mps:0')\n",
      "Iteration 9700 Training loss 0.07170075923204422 Validation loss 0.06574129313230515 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.0974],\n",
      "        [0.8776]], device='mps:0')\n",
      "Iteration 9710 Training loss 0.06614013761281967 Validation loss 0.06574646383523941 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.9172],\n",
      "        [0.0368]], device='mps:0')\n",
      "Iteration 9720 Training loss 0.05735549330711365 Validation loss 0.06582475453615189 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.4484],\n",
      "        [0.7566]], device='mps:0')\n",
      "Iteration 9730 Training loss 0.062009409070014954 Validation loss 0.06571047008037567 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.9493],\n",
      "        [0.4755]], device='mps:0')\n",
      "Iteration 9740 Training loss 0.06128222495317459 Validation loss 0.06577932089567184 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.8953],\n",
      "        [0.1861]], device='mps:0')\n",
      "Iteration 9750 Training loss 0.061346590518951416 Validation loss 0.06572605669498444 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.0247],\n",
      "        [0.2165]], device='mps:0')\n",
      "Iteration 9760 Training loss 0.05433257296681404 Validation loss 0.06574887037277222 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.0442],\n",
      "        [0.0445]], device='mps:0')\n",
      "Iteration 9770 Training loss 0.05997635796666145 Validation loss 0.0657840371131897 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.4363],\n",
      "        [0.7937]], device='mps:0')\n",
      "Iteration 9780 Training loss 0.0682322159409523 Validation loss 0.06571228057146072 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.2849],\n",
      "        [0.3164]], device='mps:0')\n",
      "Iteration 9790 Training loss 0.06423792243003845 Validation loss 0.06569067388772964 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.6620],\n",
      "        [0.4159]], device='mps:0')\n",
      "Iteration 9800 Training loss 0.06486667692661285 Validation loss 0.06582958996295929 Accuracy 0.8203750252723694\n",
      "Output tensor([[0.0183],\n",
      "        [0.0790]], device='mps:0')\n",
      "Iteration 9810 Training loss 0.062949538230896 Validation loss 0.06600408256053925 Accuracy 0.8191250562667847\n",
      "Output tensor([[0.9544],\n",
      "        [0.1576]], device='mps:0')\n",
      "Iteration 9820 Training loss 0.05780639126896858 Validation loss 0.06590443849563599 Accuracy 0.8187500238418579\n",
      "Output tensor([[0.6210],\n",
      "        [0.2566]], device='mps:0')\n",
      "Iteration 9830 Training loss 0.06509647518396378 Validation loss 0.0657835602760315 Accuracy 0.8203750252723694\n",
      "Output tensor([[0.2121],\n",
      "        [0.7135]], device='mps:0')\n",
      "Iteration 9840 Training loss 0.0642673596739769 Validation loss 0.06568031013011932 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.9216],\n",
      "        [0.1438]], device='mps:0')\n",
      "Iteration 9850 Training loss 0.06712550669908524 Validation loss 0.06572388112545013 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.6226],\n",
      "        [0.2159]], device='mps:0')\n",
      "Iteration 9860 Training loss 0.0645165741443634 Validation loss 0.06566064804792404 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.0803],\n",
      "        [0.9127]], device='mps:0')\n",
      "Iteration 9870 Training loss 0.06823711097240448 Validation loss 0.0656832903623581 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.2197],\n",
      "        [0.8871]], device='mps:0')\n",
      "Iteration 9880 Training loss 0.05924427881836891 Validation loss 0.0656891018152237 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.3992],\n",
      "        [0.3666]], device='mps:0')\n",
      "Iteration 9890 Training loss 0.06440569460391998 Validation loss 0.06568676978349686 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.0644],\n",
      "        [0.5455]], device='mps:0')\n",
      "Iteration 9900 Training loss 0.05480474233627319 Validation loss 0.06566012650728226 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.9334],\n",
      "        [0.5421]], device='mps:0')\n",
      "Iteration 9910 Training loss 0.06623514741659164 Validation loss 0.06568089872598648 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.3486],\n",
      "        [0.6407]], device='mps:0')\n",
      "Iteration 9920 Training loss 0.06694798916578293 Validation loss 0.0657980665564537 Accuracy 0.8210000395774841\n",
      "Output tensor([[0.7954],\n",
      "        [0.4344]], device='mps:0')\n",
      "Iteration 9930 Training loss 0.07170199602842331 Validation loss 0.06571724265813828 Accuracy 0.8212500214576721\n",
      "Output tensor([[0.2158],\n",
      "        [0.9728]], device='mps:0')\n",
      "Iteration 9940 Training loss 0.06482929736375809 Validation loss 0.0656457394361496 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.2302],\n",
      "        [0.5166]], device='mps:0')\n",
      "Iteration 9950 Training loss 0.06028204783797264 Validation loss 0.06563816964626312 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.0254],\n",
      "        [0.0388]], device='mps:0')\n",
      "Iteration 9960 Training loss 0.06694266200065613 Validation loss 0.065622977912426 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.4566],\n",
      "        [0.1193]], device='mps:0')\n",
      "Iteration 9970 Training loss 0.062067631632089615 Validation loss 0.06566702574491501 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.0252],\n",
      "        [0.0978]], device='mps:0')\n",
      "Iteration 9980 Training loss 0.06783846020698547 Validation loss 0.06562509387731552 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.3086],\n",
      "        [0.9880]], device='mps:0')\n",
      "Iteration 9990 Training loss 0.06714282929897308 Validation loss 0.06569959968328476 Accuracy 0.8208750486373901\n",
      "Output tensor([[0.6131],\n",
      "        [0.1255]], device='mps:0')\n",
      "Iteration 10000 Training loss 0.06770510226488113 Validation loss 0.06567015498876572 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.2446],\n",
      "        [0.9165]], device='mps:0')\n",
      "Iteration 10010 Training loss 0.06565546989440918 Validation loss 0.06583740562200546 Accuracy 0.8202500343322754\n",
      "Output tensor([[0.2356],\n",
      "        [0.1227]], device='mps:0')\n",
      "Iteration 10020 Training loss 0.06529399007558823 Validation loss 0.06576412916183472 Accuracy 0.8200000524520874\n",
      "Output tensor([[0.9505],\n",
      "        [0.0465]], device='mps:0')\n",
      "Iteration 10030 Training loss 0.05818226560950279 Validation loss 0.06559092551469803 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.1223],\n",
      "        [0.0462]], device='mps:0')\n",
      "Iteration 10040 Training loss 0.0686495378613472 Validation loss 0.06558820605278015 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.0102],\n",
      "        [0.6228]], device='mps:0')\n",
      "Iteration 10050 Training loss 0.05127940699458122 Validation loss 0.06561382859945297 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.2093],\n",
      "        [0.2229]], device='mps:0')\n",
      "Iteration 10060 Training loss 0.05598539113998413 Validation loss 0.06561186164617538 Accuracy 0.8212500214576721\n",
      "Output tensor([[0.8250],\n",
      "        [0.6972]], device='mps:0')\n",
      "Iteration 10070 Training loss 0.05860496312379837 Validation loss 0.06558073312044144 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.3059],\n",
      "        [0.7642]], device='mps:0')\n",
      "Iteration 10080 Training loss 0.07257453352212906 Validation loss 0.06558084487915039 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.0849],\n",
      "        [0.3955]], device='mps:0')\n",
      "Iteration 10090 Training loss 0.06888420134782791 Validation loss 0.06557342410087585 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.4363],\n",
      "        [0.7658]], device='mps:0')\n",
      "Iteration 10100 Training loss 0.0662057101726532 Validation loss 0.06566508114337921 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.0062],\n",
      "        [0.1441]], device='mps:0')\n",
      "Iteration 10110 Training loss 0.07094196230173111 Validation loss 0.0655689463019371 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.4083],\n",
      "        [0.2057]], device='mps:0')\n",
      "Iteration 10120 Training loss 0.06485050171613693 Validation loss 0.06556117534637451 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.1919],\n",
      "        [0.7162]], device='mps:0')\n",
      "Iteration 10130 Training loss 0.05994393303990364 Validation loss 0.06571231037378311 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.9710],\n",
      "        [0.3707]], device='mps:0')\n",
      "Iteration 10140 Training loss 0.062242817133665085 Validation loss 0.06558188796043396 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.9866],\n",
      "        [0.8880]], device='mps:0')\n",
      "Iteration 10150 Training loss 0.07069980353116989 Validation loss 0.06557301431894302 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.0388],\n",
      "        [0.7039]], device='mps:0')\n",
      "Iteration 10160 Training loss 0.0646292045712471 Validation loss 0.06554675847291946 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.7707],\n",
      "        [0.1123]], device='mps:0')\n",
      "Iteration 10170 Training loss 0.06583546847105026 Validation loss 0.06554535031318665 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.8083],\n",
      "        [0.6143]], device='mps:0')\n",
      "Iteration 10180 Training loss 0.05680401623249054 Validation loss 0.06554601341485977 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.1583],\n",
      "        [0.2352]], device='mps:0')\n",
      "Iteration 10190 Training loss 0.06414905190467834 Validation loss 0.06552537530660629 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.5645],\n",
      "        [0.2748]], device='mps:0')\n",
      "Iteration 10200 Training loss 0.06529552489519119 Validation loss 0.06551442295312881 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.0280],\n",
      "        [0.8504]], device='mps:0')\n",
      "Iteration 10210 Training loss 0.059303510934114456 Validation loss 0.06550822407007217 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.4246],\n",
      "        [0.6336]], device='mps:0')\n",
      "Iteration 10220 Training loss 0.06008230522274971 Validation loss 0.0655958354473114 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.2764],\n",
      "        [0.6155]], device='mps:0')\n",
      "Iteration 10230 Training loss 0.07353033870458603 Validation loss 0.06552289426326752 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.3546],\n",
      "        [0.8523]], device='mps:0')\n",
      "Iteration 10240 Training loss 0.06096617877483368 Validation loss 0.0655040517449379 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.9643],\n",
      "        [0.6639]], device='mps:0')\n",
      "Iteration 10250 Training loss 0.0625808909535408 Validation loss 0.06550052762031555 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.0145],\n",
      "        [0.1744]], device='mps:0')\n",
      "Iteration 10260 Training loss 0.055075328797101974 Validation loss 0.06554550677537918 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.8504],\n",
      "        [0.8706]], device='mps:0')\n",
      "Iteration 10270 Training loss 0.07425959408283234 Validation loss 0.06558220088481903 Accuracy 0.8212500214576721\n",
      "Output tensor([[0.3660],\n",
      "        [0.7690]], device='mps:0')\n",
      "Iteration 10280 Training loss 0.061501797288656235 Validation loss 0.06548097729682922 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.9043],\n",
      "        [0.0620]], device='mps:0')\n",
      "Iteration 10290 Training loss 0.06200845539569855 Validation loss 0.0655260682106018 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.4977],\n",
      "        [0.1686]], device='mps:0')\n",
      "Iteration 10300 Training loss 0.06678330153226852 Validation loss 0.06566478312015533 Accuracy 0.8201250433921814\n",
      "Output tensor([[0.3516],\n",
      "        [0.1870]], device='mps:0')\n",
      "Iteration 10310 Training loss 0.06274712085723877 Validation loss 0.0655035600066185 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.1181],\n",
      "        [0.9314]], device='mps:0')\n",
      "Iteration 10320 Training loss 0.06316414475440979 Validation loss 0.06549839675426483 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.6700],\n",
      "        [0.8938]], device='mps:0')\n",
      "Iteration 10330 Training loss 0.06273151934146881 Validation loss 0.06556949019432068 Accuracy 0.8202500343322754\n",
      "Output tensor([[0.0182],\n",
      "        [0.5543]], device='mps:0')\n",
      "Iteration 10340 Training loss 0.059101179242134094 Validation loss 0.06558415293693542 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.7102],\n",
      "        [0.3357]], device='mps:0')\n",
      "Iteration 10350 Training loss 0.06395043432712555 Validation loss 0.06557078659534454 Accuracy 0.8201250433921814\n",
      "Output tensor([[0.1805],\n",
      "        [0.3049]], device='mps:0')\n",
      "Iteration 10360 Training loss 0.06313075870275497 Validation loss 0.0654461532831192 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.2453],\n",
      "        [0.0424]], device='mps:0')\n",
      "Iteration 10370 Training loss 0.06258378177881241 Validation loss 0.06559698283672333 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.1248],\n",
      "        [0.9332]], device='mps:0')\n",
      "Iteration 10380 Training loss 0.05835530161857605 Validation loss 0.06543677300214767 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.8302],\n",
      "        [0.0940]], device='mps:0')\n",
      "Iteration 10390 Training loss 0.06517313420772552 Validation loss 0.06544918566942215 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.8316],\n",
      "        [0.1075]], device='mps:0')\n",
      "Iteration 10400 Training loss 0.058734796941280365 Validation loss 0.06554898619651794 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.7678],\n",
      "        [0.3030]], device='mps:0')\n",
      "Iteration 10410 Training loss 0.06399762630462646 Validation loss 0.0656341165304184 Accuracy 0.8198750615119934\n",
      "Output tensor([[0.5277],\n",
      "        [0.5783]], device='mps:0')\n",
      "Iteration 10420 Training loss 0.06896474957466125 Validation loss 0.0654386505484581 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.5240],\n",
      "        [0.8515]], device='mps:0')\n",
      "Iteration 10430 Training loss 0.0708061084151268 Validation loss 0.06548214703798294 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.1058],\n",
      "        [0.8432]], device='mps:0')\n",
      "Iteration 10440 Training loss 0.05678144842386246 Validation loss 0.06541508436203003 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.1033],\n",
      "        [0.2138]], device='mps:0')\n",
      "Iteration 10450 Training loss 0.06146649271249771 Validation loss 0.06541591137647629 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.7615],\n",
      "        [0.0577]], device='mps:0')\n",
      "Iteration 10460 Training loss 0.06979420781135559 Validation loss 0.0654856488108635 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.4574],\n",
      "        [0.6673]], device='mps:0')\n",
      "Iteration 10470 Training loss 0.05965496972203255 Validation loss 0.06540703773498535 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.9298],\n",
      "        [0.7818]], device='mps:0')\n",
      "Iteration 10480 Training loss 0.06677456945180893 Validation loss 0.06541429460048676 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.7102],\n",
      "        [0.0657]], device='mps:0')\n",
      "Iteration 10490 Training loss 0.045999325811862946 Validation loss 0.06541536003351212 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.9836],\n",
      "        [0.0910]], device='mps:0')\n",
      "Iteration 10500 Training loss 0.06723669171333313 Validation loss 0.06546264886856079 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.8506],\n",
      "        [0.0611]], device='mps:0')\n",
      "Iteration 10510 Training loss 0.061182860285043716 Validation loss 0.06543374061584473 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.2458],\n",
      "        [0.4919]], device='mps:0')\n",
      "Iteration 10520 Training loss 0.06495898216962814 Validation loss 0.06543543189764023 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.2561],\n",
      "        [0.3706]], device='mps:0')\n",
      "Iteration 10530 Training loss 0.06238212808966637 Validation loss 0.06551673263311386 Accuracy 0.8208750486373901\n",
      "Output tensor([[0.0484],\n",
      "        [0.1507]], device='mps:0')\n",
      "Iteration 10540 Training loss 0.0694202184677124 Validation loss 0.06538354605436325 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.8983],\n",
      "        [0.7438]], device='mps:0')\n",
      "Iteration 10550 Training loss 0.07013141363859177 Validation loss 0.06537467986345291 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.3609],\n",
      "        [0.3760]], device='mps:0')\n",
      "Iteration 10560 Training loss 0.06995980441570282 Validation loss 0.06536852568387985 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.0968],\n",
      "        [0.7926]], device='mps:0')\n",
      "Iteration 10570 Training loss 0.06539180874824524 Validation loss 0.06537003070116043 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.0275],\n",
      "        [0.5321]], device='mps:0')\n",
      "Iteration 10580 Training loss 0.06963454931974411 Validation loss 0.06539877504110336 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.0273],\n",
      "        [0.6732]], device='mps:0')\n",
      "Iteration 10590 Training loss 0.0685061514377594 Validation loss 0.06558340042829514 Accuracy 0.8207500576972961\n",
      "Output tensor([[0.9686],\n",
      "        [0.6125]], device='mps:0')\n",
      "Iteration 10600 Training loss 0.06706532835960388 Validation loss 0.06553199142217636 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.8771],\n",
      "        [0.9543]], device='mps:0')\n",
      "Iteration 10610 Training loss 0.06721041351556778 Validation loss 0.06533928215503693 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.5042],\n",
      "        [0.1534]], device='mps:0')\n",
      "Iteration 10620 Training loss 0.062590092420578 Validation loss 0.06536850333213806 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.0297],\n",
      "        [0.0487]], device='mps:0')\n",
      "Iteration 10630 Training loss 0.06209709122776985 Validation loss 0.06532680988311768 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.1005],\n",
      "        [0.9733]], device='mps:0')\n",
      "Iteration 10640 Training loss 0.06073900684714317 Validation loss 0.06531691551208496 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.0493],\n",
      "        [0.5012]], device='mps:0')\n",
      "Iteration 10650 Training loss 0.07405749708414078 Validation loss 0.0653267353773117 Accuracy 0.8221250176429749\n",
      "Output tensor([[0.8477],\n",
      "        [0.5692]], device='mps:0')\n",
      "Iteration 10660 Training loss 0.06486395001411438 Validation loss 0.06530854851007462 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.8767],\n",
      "        [0.8443]], device='mps:0')\n",
      "Iteration 10670 Training loss 0.06128614395856857 Validation loss 0.0656212791800499 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.8684],\n",
      "        [0.4783]], device='mps:0')\n",
      "Iteration 10680 Training loss 0.06306727230548859 Validation loss 0.0653245747089386 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.1235],\n",
      "        [0.1089]], device='mps:0')\n",
      "Iteration 10690 Training loss 0.07526502758264542 Validation loss 0.06529893726110458 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.9215],\n",
      "        [0.1828]], device='mps:0')\n",
      "Iteration 10700 Training loss 0.06661321222782135 Validation loss 0.06538325548171997 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.0331],\n",
      "        [0.4171]], device='mps:0')\n",
      "Iteration 10710 Training loss 0.06363105028867722 Validation loss 0.06529957801103592 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.4689],\n",
      "        [0.0066]], device='mps:0')\n",
      "Iteration 10720 Training loss 0.06283487379550934 Validation loss 0.06527215242385864 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.8695],\n",
      "        [0.6996]], device='mps:0')\n",
      "Iteration 10730 Training loss 0.05757186934351921 Validation loss 0.06528552621603012 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.0534],\n",
      "        [0.4488]], device='mps:0')\n",
      "Iteration 10740 Training loss 0.06203150004148483 Validation loss 0.06563090533018112 Accuracy 0.8207500576972961\n",
      "Output tensor([[0.5683],\n",
      "        [0.9167]], device='mps:0')\n",
      "Iteration 10750 Training loss 0.055236171931028366 Validation loss 0.06547696888446808 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.2636],\n",
      "        [0.2770]], device='mps:0')\n",
      "Iteration 10760 Training loss 0.06477817893028259 Validation loss 0.0653267353773117 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.5316],\n",
      "        [0.8251]], device='mps:0')\n",
      "Iteration 10770 Training loss 0.06696873903274536 Validation loss 0.06529460847377777 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.9574],\n",
      "        [0.9669]], device='mps:0')\n",
      "Iteration 10780 Training loss 0.06199496239423752 Validation loss 0.06530681252479553 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.0151],\n",
      "        [0.8597]], device='mps:0')\n",
      "Iteration 10790 Training loss 0.06677600741386414 Validation loss 0.06532563269138336 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.9108],\n",
      "        [0.8282]], device='mps:0')\n",
      "Iteration 10800 Training loss 0.05920892581343651 Validation loss 0.06525406241416931 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.8089],\n",
      "        [0.8298]], device='mps:0')\n",
      "Iteration 10810 Training loss 0.05675783008337021 Validation loss 0.06538564711809158 Accuracy 0.8216250538825989\n",
      "Output tensor([[0.1109],\n",
      "        [0.9310]], device='mps:0')\n",
      "Iteration 10820 Training loss 0.06473191827535629 Validation loss 0.06527095288038254 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.7249],\n",
      "        [0.7767]], device='mps:0')\n",
      "Iteration 10830 Training loss 0.06520387530326843 Validation loss 0.06526686251163483 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.0540],\n",
      "        [0.2626]], device='mps:0')\n",
      "Iteration 10840 Training loss 0.059820372611284256 Validation loss 0.0653114840388298 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.7864],\n",
      "        [0.2508]], device='mps:0')\n",
      "Iteration 10850 Training loss 0.06255540996789932 Validation loss 0.06522246450185776 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.0212],\n",
      "        [0.8317]], device='mps:0')\n",
      "Iteration 10860 Training loss 0.0680198147892952 Validation loss 0.06521189957857132 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.0474],\n",
      "        [0.7148]], device='mps:0')\n",
      "Iteration 10870 Training loss 0.0714898407459259 Validation loss 0.06522949039936066 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.7772],\n",
      "        [0.0155]], device='mps:0')\n",
      "Iteration 10880 Training loss 0.0711909830570221 Validation loss 0.06520405411720276 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.2459],\n",
      "        [0.1471]], device='mps:0')\n",
      "Iteration 10890 Training loss 0.07189545035362244 Validation loss 0.0652865469455719 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.6572],\n",
      "        [0.5897]], device='mps:0')\n",
      "Iteration 10900 Training loss 0.062449369579553604 Validation loss 0.06529553234577179 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.8715],\n",
      "        [0.4774]], device='mps:0')\n",
      "Iteration 10910 Training loss 0.0665789321064949 Validation loss 0.06518428027629852 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.9825],\n",
      "        [0.9699]], device='mps:0')\n",
      "Iteration 10920 Training loss 0.07611396908760071 Validation loss 0.0651986375451088 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.5519],\n",
      "        [0.9432]], device='mps:0')\n",
      "Iteration 10930 Training loss 0.05775344744324684 Validation loss 0.06536053866147995 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.8314],\n",
      "        [0.9599]], device='mps:0')\n",
      "Iteration 10940 Training loss 0.061136018484830856 Validation loss 0.0651838555932045 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.3653],\n",
      "        [0.8037]], device='mps:0')\n",
      "Iteration 10950 Training loss 0.06556906551122665 Validation loss 0.06530218571424484 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.2404],\n",
      "        [0.6999]], device='mps:0')\n",
      "Iteration 10960 Training loss 0.06763539463281631 Validation loss 0.06518624722957611 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.3250],\n",
      "        [0.3140]], device='mps:0')\n",
      "Iteration 10970 Training loss 0.05262145400047302 Validation loss 0.06517503410577774 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.1111],\n",
      "        [0.7334]], device='mps:0')\n",
      "Iteration 10980 Training loss 0.058340031653642654 Validation loss 0.06541036069393158 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.3632],\n",
      "        [0.3455]], device='mps:0')\n",
      "Iteration 10990 Training loss 0.06639571487903595 Validation loss 0.06516524404287338 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.9467],\n",
      "        [0.1013]], device='mps:0')\n",
      "Iteration 11000 Training loss 0.058165453374385834 Validation loss 0.06518050283193588 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.0574],\n",
      "        [0.2022]], device='mps:0')\n",
      "Iteration 11010 Training loss 0.06633339077234268 Validation loss 0.06515626609325409 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.1435],\n",
      "        [0.2917]], device='mps:0')\n",
      "Iteration 11020 Training loss 0.06124037504196167 Validation loss 0.0653030127286911 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.8742],\n",
      "        [0.6695]], device='mps:0')\n",
      "Iteration 11030 Training loss 0.056502386927604675 Validation loss 0.06532184779644012 Accuracy 0.8222500681877136\n",
      "Output tensor([[0.0713],\n",
      "        [0.8761]], device='mps:0')\n",
      "Iteration 11040 Training loss 0.06995736807584763 Validation loss 0.06515669822692871 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.1389],\n",
      "        [0.1754]], device='mps:0')\n",
      "Iteration 11050 Training loss 0.06261947005987167 Validation loss 0.06520915776491165 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.7351],\n",
      "        [0.9695]], device='mps:0')\n",
      "Iteration 11060 Training loss 0.07148616015911102 Validation loss 0.06583419442176819 Accuracy 0.8206250667572021\n",
      "Output tensor([[0.1625],\n",
      "        [0.7640]], device='mps:0')\n",
      "Iteration 11070 Training loss 0.0642225369811058 Validation loss 0.06514091044664383 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.1613],\n",
      "        [0.8473]], device='mps:0')\n",
      "Iteration 11080 Training loss 0.0562080480158329 Validation loss 0.06514359265565872 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.1822],\n",
      "        [0.7126]], device='mps:0')\n",
      "Iteration 11090 Training loss 0.05707169324159622 Validation loss 0.06515132635831833 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.0504],\n",
      "        [0.8381]], device='mps:0')\n",
      "Iteration 11100 Training loss 0.06110258027911186 Validation loss 0.06520596891641617 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.6165],\n",
      "        [0.3404]], device='mps:0')\n",
      "Iteration 11110 Training loss 0.07033650577068329 Validation loss 0.06513188779354095 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.8212],\n",
      "        [0.4202]], device='mps:0')\n",
      "Iteration 11120 Training loss 0.059792324900627136 Validation loss 0.06535331904888153 Accuracy 0.8213750123977661\n",
      "Output tensor([[0.0698],\n",
      "        [0.8657]], device='mps:0')\n",
      "Iteration 11130 Training loss 0.061712540686130524 Validation loss 0.06521999090909958 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.2828],\n",
      "        [0.7473]], device='mps:0')\n",
      "Iteration 11140 Training loss 0.06649693101644516 Validation loss 0.0651225745677948 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.9825],\n",
      "        [0.1138]], device='mps:0')\n",
      "Iteration 11150 Training loss 0.0569368340075016 Validation loss 0.06513408571481705 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.7272],\n",
      "        [0.7610]], device='mps:0')\n",
      "Iteration 11160 Training loss 0.059811752289533615 Validation loss 0.06517209112644196 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.3132],\n",
      "        [0.2133]], device='mps:0')\n",
      "Iteration 11170 Training loss 0.0639955922961235 Validation loss 0.06533417850732803 Accuracy 0.8211250305175781\n",
      "Output tensor([[0.2331],\n",
      "        [0.9386]], device='mps:0')\n",
      "Iteration 11180 Training loss 0.05979049578309059 Validation loss 0.06512769311666489 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.8149],\n",
      "        [0.8323]], device='mps:0')\n",
      "Iteration 11190 Training loss 0.06697412580251694 Validation loss 0.06509984284639359 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.7439],\n",
      "        [0.0746]], device='mps:0')\n",
      "Iteration 11200 Training loss 0.07006274163722992 Validation loss 0.0651971772313118 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.0951],\n",
      "        [0.1413]], device='mps:0')\n",
      "Iteration 11210 Training loss 0.06244809925556183 Validation loss 0.06509380787611008 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.8123],\n",
      "        [0.1703]], device='mps:0')\n",
      "Iteration 11220 Training loss 0.06463848054409027 Validation loss 0.06507658213376999 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.8768],\n",
      "        [0.9196]], device='mps:0')\n",
      "Iteration 11230 Training loss 0.06430451571941376 Validation loss 0.065155528485775 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.0763],\n",
      "        [0.0322]], device='mps:0')\n",
      "Iteration 11240 Training loss 0.06218070909380913 Validation loss 0.06543590128421783 Accuracy 0.8205000162124634\n",
      "Output tensor([[0.7601],\n",
      "        [0.9847]], device='mps:0')\n",
      "Iteration 11250 Training loss 0.0663408562541008 Validation loss 0.06511258333921432 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.2443],\n",
      "        [0.9422]], device='mps:0')\n",
      "Iteration 11260 Training loss 0.06408283859491348 Validation loss 0.06513530761003494 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.8054],\n",
      "        [0.9480]], device='mps:0')\n",
      "Iteration 11270 Training loss 0.06524113565683365 Validation loss 0.0650554969906807 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.0198],\n",
      "        [0.1338]], device='mps:0')\n",
      "Iteration 11280 Training loss 0.05977215990424156 Validation loss 0.06534221768379211 Accuracy 0.8212500214576721\n",
      "Output tensor([[0.8942],\n",
      "        [0.8575]], device='mps:0')\n",
      "Iteration 11290 Training loss 0.0698675736784935 Validation loss 0.06504301726818085 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.2551],\n",
      "        [0.9741]], device='mps:0')\n",
      "Iteration 11300 Training loss 0.06476102024316788 Validation loss 0.06505338847637177 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.9887],\n",
      "        [0.0524]], device='mps:0')\n",
      "Iteration 11310 Training loss 0.06308439373970032 Validation loss 0.06514627486467361 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.7113],\n",
      "        [0.6509]], device='mps:0')\n",
      "Iteration 11320 Training loss 0.06762907654047012 Validation loss 0.06506071239709854 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.9889],\n",
      "        [0.7251]], device='mps:0')\n",
      "Iteration 11330 Training loss 0.0694606751203537 Validation loss 0.06504005938768387 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.8875],\n",
      "        [0.0842]], device='mps:0')\n",
      "Iteration 11340 Training loss 0.061321429908275604 Validation loss 0.0652230829000473 Accuracy 0.8215000629425049\n",
      "Output tensor([[0.1939],\n",
      "        [0.9740]], device='mps:0')\n",
      "Iteration 11350 Training loss 0.05584409460425377 Validation loss 0.06506521999835968 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.6957],\n",
      "        [0.9645]], device='mps:0')\n",
      "Iteration 11360 Training loss 0.06466248631477356 Validation loss 0.06507475674152374 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.7941],\n",
      "        [0.4174]], device='mps:0')\n",
      "Iteration 11370 Training loss 0.06530866771936417 Validation loss 0.06503129750490189 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.1321],\n",
      "        [0.4577]], device='mps:0')\n",
      "Iteration 11380 Training loss 0.05487678572535515 Validation loss 0.0651182308793068 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.0496],\n",
      "        [0.2709]], device='mps:0')\n",
      "Iteration 11390 Training loss 0.06496812403202057 Validation loss 0.06502441316843033 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.5142],\n",
      "        [0.1718]], device='mps:0')\n",
      "Iteration 11400 Training loss 0.06687702238559723 Validation loss 0.06501039117574692 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.8813],\n",
      "        [0.8074]], device='mps:0')\n",
      "Iteration 11410 Training loss 0.06600188463926315 Validation loss 0.0650150328874588 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.2598],\n",
      "        [0.8912]], device='mps:0')\n",
      "Iteration 11420 Training loss 0.06287870556116104 Validation loss 0.06500332057476044 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.3738],\n",
      "        [0.2230]], device='mps:0')\n",
      "Iteration 11430 Training loss 0.06593789160251617 Validation loss 0.06518962234258652 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.3167],\n",
      "        [0.1040]], device='mps:0')\n",
      "Iteration 11440 Training loss 0.06441973894834518 Validation loss 0.06498908996582031 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.0629],\n",
      "        [0.7243]], device='mps:0')\n",
      "Iteration 11450 Training loss 0.05717729404568672 Validation loss 0.06499379873275757 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.6533],\n",
      "        [0.2664]], device='mps:0')\n",
      "Iteration 11460 Training loss 0.056487780064344406 Validation loss 0.06497877091169357 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.5272],\n",
      "        [0.0559]], device='mps:0')\n",
      "Iteration 11470 Training loss 0.057900331914424896 Validation loss 0.06501907855272293 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.9035],\n",
      "        [0.9331]], device='mps:0')\n",
      "Iteration 11480 Training loss 0.061073388904333115 Validation loss 0.0649813711643219 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.8281],\n",
      "        [0.1516]], device='mps:0')\n",
      "Iteration 11490 Training loss 0.0650687888264656 Validation loss 0.06498470157384872 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.2416],\n",
      "        [0.0459]], device='mps:0')\n",
      "Iteration 11500 Training loss 0.06616093218326569 Validation loss 0.06503882259130478 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.1835],\n",
      "        [0.0495]], device='mps:0')\n",
      "Iteration 11510 Training loss 0.06533420085906982 Validation loss 0.06495790928602219 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.2635],\n",
      "        [0.4441]], device='mps:0')\n",
      "Iteration 11520 Training loss 0.06593536585569382 Validation loss 0.06522245705127716 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.5796],\n",
      "        [0.9709]], device='mps:0')\n",
      "Iteration 11530 Training loss 0.07029027491807938 Validation loss 0.06495002657175064 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.5181],\n",
      "        [0.0657]], device='mps:0')\n",
      "Iteration 11540 Training loss 0.061619970947504044 Validation loss 0.06496752798557281 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.5219],\n",
      "        [0.3203]], device='mps:0')\n",
      "Iteration 11550 Training loss 0.06379083544015884 Validation loss 0.06511951237916946 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.4654],\n",
      "        [0.9559]], device='mps:0')\n",
      "Iteration 11560 Training loss 0.06202754005789757 Validation loss 0.06495065987110138 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.3574],\n",
      "        [0.1393]], device='mps:0')\n",
      "Iteration 11570 Training loss 0.06852405518293381 Validation loss 0.06499383598566055 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.8627],\n",
      "        [0.1171]], device='mps:0')\n",
      "Iteration 11580 Training loss 0.06333564221858978 Validation loss 0.0649462565779686 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.7457],\n",
      "        [0.3147]], device='mps:0')\n",
      "Iteration 11590 Training loss 0.06072978675365448 Validation loss 0.0654079020023346 Accuracy 0.8208750486373901\n",
      "Output tensor([[0.0648],\n",
      "        [0.7700]], device='mps:0')\n",
      "Iteration 11600 Training loss 0.06652709096670151 Validation loss 0.06492220610380173 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.0188],\n",
      "        [0.2069]], device='mps:0')\n",
      "Iteration 11610 Training loss 0.07022472470998764 Validation loss 0.0649472251534462 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.9345],\n",
      "        [0.8041]], device='mps:0')\n",
      "Iteration 11620 Training loss 0.06489186733961105 Validation loss 0.06493664532899857 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.4771],\n",
      "        [0.3383]], device='mps:0')\n",
      "Iteration 11630 Training loss 0.05959305167198181 Validation loss 0.06491275876760483 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.8751],\n",
      "        [0.0589]], device='mps:0')\n",
      "Iteration 11640 Training loss 0.06607963144779205 Validation loss 0.06490299105644226 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.4096],\n",
      "        [0.8882]], device='mps:0')\n",
      "Iteration 11650 Training loss 0.0713711827993393 Validation loss 0.0649520680308342 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.6714],\n",
      "        [0.3520]], device='mps:0')\n",
      "Iteration 11660 Training loss 0.06727644801139832 Validation loss 0.06489042937755585 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.4113],\n",
      "        [0.6112]], device='mps:0')\n",
      "Iteration 11670 Training loss 0.06583984941244125 Validation loss 0.06489109992980957 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.8390],\n",
      "        [0.2121]], device='mps:0')\n",
      "Iteration 11680 Training loss 0.06503525376319885 Validation loss 0.06488900631666183 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.1104],\n",
      "        [0.3640]], device='mps:0')\n",
      "Iteration 11690 Training loss 0.06526044756174088 Validation loss 0.06494896858930588 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.9446],\n",
      "        [0.7049]], device='mps:0')\n",
      "Iteration 11700 Training loss 0.06321849673986435 Validation loss 0.06491604447364807 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.6421],\n",
      "        [0.8592]], device='mps:0')\n",
      "Iteration 11710 Training loss 0.06260377168655396 Validation loss 0.06490518897771835 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.9570],\n",
      "        [0.9720]], device='mps:0')\n",
      "Iteration 11720 Training loss 0.06396334618330002 Validation loss 0.06489294022321701 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.2468],\n",
      "        [0.6162]], device='mps:0')\n",
      "Iteration 11730 Training loss 0.07616591453552246 Validation loss 0.06487312912940979 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.0557],\n",
      "        [0.0415]], device='mps:0')\n",
      "Iteration 11740 Training loss 0.06413745880126953 Validation loss 0.06499121338129044 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.7723],\n",
      "        [0.9419]], device='mps:0')\n",
      "Iteration 11750 Training loss 0.06611110270023346 Validation loss 0.06520234793424606 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.9778],\n",
      "        [0.7018]], device='mps:0')\n",
      "Iteration 11760 Training loss 0.07122722268104553 Validation loss 0.0648924931883812 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.9523],\n",
      "        [0.1770]], device='mps:0')\n",
      "Iteration 11770 Training loss 0.06508583575487137 Validation loss 0.06487991660833359 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.6566],\n",
      "        [0.1219]], device='mps:0')\n",
      "Iteration 11780 Training loss 0.06601385772228241 Validation loss 0.06488510966300964 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.8976],\n",
      "        [0.4537]], device='mps:0')\n",
      "Iteration 11790 Training loss 0.06292494386434555 Validation loss 0.06487981230020523 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.2184],\n",
      "        [0.1473]], device='mps:0')\n",
      "Iteration 11800 Training loss 0.06840266287326813 Validation loss 0.06486593186855316 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.8994],\n",
      "        [0.0985]], device='mps:0')\n",
      "Iteration 11810 Training loss 0.06579592078924179 Validation loss 0.0651000365614891 Accuracy 0.8217500448226929\n",
      "Output tensor([[0.7595],\n",
      "        [0.9357]], device='mps:0')\n",
      "Iteration 11820 Training loss 0.051393698900938034 Validation loss 0.06485437601804733 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.0744],\n",
      "        [0.0806]], device='mps:0')\n",
      "Iteration 11830 Training loss 0.06531672179698944 Validation loss 0.06489600241184235 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.4433],\n",
      "        [0.8788]], device='mps:0')\n",
      "Iteration 11840 Training loss 0.06828873604536057 Validation loss 0.06486719101667404 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.9387],\n",
      "        [0.3768]], device='mps:0')\n",
      "Iteration 11850 Training loss 0.06862199306488037 Validation loss 0.06484903395175934 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.0738],\n",
      "        [0.8783]], device='mps:0')\n",
      "Iteration 11860 Training loss 0.06383223086595535 Validation loss 0.06525009125471115 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.9482],\n",
      "        [0.7018]], device='mps:0')\n",
      "Iteration 11870 Training loss 0.0645970031619072 Validation loss 0.0648568719625473 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.4346],\n",
      "        [0.4961]], device='mps:0')\n",
      "Iteration 11880 Training loss 0.06179599463939667 Validation loss 0.06482795625925064 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.7093],\n",
      "        [0.1390]], device='mps:0')\n",
      "Iteration 11890 Training loss 0.06569936126470566 Validation loss 0.06482481956481934 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.7146],\n",
      "        [0.2052]], device='mps:0')\n",
      "Iteration 11900 Training loss 0.06781183928251266 Validation loss 0.06499214470386505 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.6453],\n",
      "        [0.9755]], device='mps:0')\n",
      "Iteration 11910 Training loss 0.057404518127441406 Validation loss 0.06480838358402252 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.6193],\n",
      "        [0.3059]], device='mps:0')\n",
      "Iteration 11920 Training loss 0.07620818167924881 Validation loss 0.06487168371677399 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.4874],\n",
      "        [0.7969]], device='mps:0')\n",
      "Iteration 11930 Training loss 0.06887992471456528 Validation loss 0.06503929197788239 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.5041],\n",
      "        [0.0297]], device='mps:0')\n",
      "Iteration 11940 Training loss 0.05299314484000206 Validation loss 0.06480371952056885 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.6479],\n",
      "        [0.0654]], device='mps:0')\n",
      "Iteration 11950 Training loss 0.06584446877241135 Validation loss 0.06482245773077011 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.2093],\n",
      "        [0.4347]], device='mps:0')\n",
      "Iteration 11960 Training loss 0.0767199769616127 Validation loss 0.06480107456445694 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.4229],\n",
      "        [0.5378]], device='mps:0')\n",
      "Iteration 11970 Training loss 0.06879483908414841 Validation loss 0.06490349024534225 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.7042],\n",
      "        [0.2574]], device='mps:0')\n",
      "Iteration 11980 Training loss 0.060327108949422836 Validation loss 0.06480404734611511 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.5552],\n",
      "        [0.9512]], device='mps:0')\n",
      "Iteration 11990 Training loss 0.06801218539476395 Validation loss 0.06492282450199127 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.0420],\n",
      "        [0.0274]], device='mps:0')\n",
      "Iteration 12000 Training loss 0.06276559829711914 Validation loss 0.06480211019515991 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.1217],\n",
      "        [0.0315]], device='mps:0')\n",
      "Iteration 12010 Training loss 0.05964082106947899 Validation loss 0.06482012569904327 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.2775],\n",
      "        [0.0227]], device='mps:0')\n",
      "Iteration 12020 Training loss 0.06427949666976929 Validation loss 0.06484755873680115 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.1074],\n",
      "        [0.9722]], device='mps:0')\n",
      "Iteration 12030 Training loss 0.0607830174267292 Validation loss 0.06477537006139755 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9309],\n",
      "        [0.3706]], device='mps:0')\n",
      "Iteration 12040 Training loss 0.07882414758205414 Validation loss 0.06479727476835251 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.0019],\n",
      "        [0.5718]], device='mps:0')\n",
      "Iteration 12050 Training loss 0.06943216174840927 Validation loss 0.06480147689580917 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.1628],\n",
      "        [0.9193]], device='mps:0')\n",
      "Iteration 12060 Training loss 0.0603557750582695 Validation loss 0.06479345262050629 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.1205],\n",
      "        [0.4666]], device='mps:0')\n",
      "Iteration 12070 Training loss 0.05389764532446861 Validation loss 0.0647517517209053 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.4426],\n",
      "        [0.6514]], device='mps:0')\n",
      "Iteration 12080 Training loss 0.06072093918919563 Validation loss 0.06476495414972305 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.9433],\n",
      "        [0.7109]], device='mps:0')\n",
      "Iteration 12090 Training loss 0.06088107451796532 Validation loss 0.06486538797616959 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.5507],\n",
      "        [0.9756]], device='mps:0')\n",
      "Iteration 12100 Training loss 0.06645803898572922 Validation loss 0.06475089490413666 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.5910],\n",
      "        [0.1249]], device='mps:0')\n",
      "Iteration 12110 Training loss 0.06546937674283981 Validation loss 0.0647369921207428 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.9780],\n",
      "        [0.4334]], device='mps:0')\n",
      "Iteration 12120 Training loss 0.05381591618061066 Validation loss 0.06473929435014725 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9394],\n",
      "        [0.0629]], device='mps:0')\n",
      "Iteration 12130 Training loss 0.06341702491044998 Validation loss 0.0647391527891159 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.0642],\n",
      "        [0.2082]], device='mps:0')\n",
      "Iteration 12140 Training loss 0.06530033051967621 Validation loss 0.064755380153656 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.9883],\n",
      "        [0.1284]], device='mps:0')\n",
      "Iteration 12150 Training loss 0.06355343014001846 Validation loss 0.06472316384315491 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.0273],\n",
      "        [0.9347]], device='mps:0')\n",
      "Iteration 12160 Training loss 0.06158154457807541 Validation loss 0.06489938497543335 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.8939],\n",
      "        [0.1236]], device='mps:0')\n",
      "Iteration 12170 Training loss 0.07104272395372391 Validation loss 0.06472407281398773 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.5803],\n",
      "        [0.3489]], device='mps:0')\n",
      "Iteration 12180 Training loss 0.0690702274441719 Validation loss 0.06473328918218613 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.2538],\n",
      "        [0.1602]], device='mps:0')\n",
      "Iteration 12190 Training loss 0.06599828600883484 Validation loss 0.06491798162460327 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.6622],\n",
      "        [0.0221]], device='mps:0')\n",
      "Iteration 12200 Training loss 0.06525175273418427 Validation loss 0.0647173821926117 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.4275],\n",
      "        [0.0100]], device='mps:0')\n",
      "Iteration 12210 Training loss 0.06559833139181137 Validation loss 0.06473273038864136 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.5842],\n",
      "        [0.9537]], device='mps:0')\n",
      "Iteration 12220 Training loss 0.06645550578832626 Validation loss 0.06469085067510605 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.0578],\n",
      "        [0.1841]], device='mps:0')\n",
      "Iteration 12230 Training loss 0.06341610848903656 Validation loss 0.06524048745632172 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.3005],\n",
      "        [0.0837]], device='mps:0')\n",
      "Iteration 12240 Training loss 0.05909913033246994 Validation loss 0.06488507241010666 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.0091],\n",
      "        [0.8415]], device='mps:0')\n",
      "Iteration 12250 Training loss 0.06340012699365616 Validation loss 0.06470201909542084 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.7952],\n",
      "        [0.0378]], device='mps:0')\n",
      "Iteration 12260 Training loss 0.06263931095600128 Validation loss 0.06490302085876465 Accuracy 0.8227500319480896\n",
      "Output tensor([[0.0539],\n",
      "        [0.9711]], device='mps:0')\n",
      "Iteration 12270 Training loss 0.06321196258068085 Validation loss 0.06477020680904388 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.1596],\n",
      "        [0.6484]], device='mps:0')\n",
      "Iteration 12280 Training loss 0.0630360096693039 Validation loss 0.06472300738096237 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.8654],\n",
      "        [0.1943]], device='mps:0')\n",
      "Iteration 12290 Training loss 0.07250050455331802 Validation loss 0.06475949287414551 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.4265],\n",
      "        [0.1301]], device='mps:0')\n",
      "Iteration 12300 Training loss 0.05422407016158104 Validation loss 0.06474830955266953 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.0757],\n",
      "        [0.6571]], device='mps:0')\n",
      "Iteration 12310 Training loss 0.062100332230329514 Validation loss 0.06466394662857056 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.5084],\n",
      "        [0.9918]], device='mps:0')\n",
      "Iteration 12320 Training loss 0.06516124308109283 Validation loss 0.06466055661439896 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.8846],\n",
      "        [0.8386]], device='mps:0')\n",
      "Iteration 12330 Training loss 0.057302325963974 Validation loss 0.06474082171916962 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5012],\n",
      "        [0.0354]], device='mps:0')\n",
      "Iteration 12340 Training loss 0.06215456873178482 Validation loss 0.06488003581762314 Accuracy 0.8230000138282776\n",
      "Output tensor([[0.5897],\n",
      "        [0.6626]], device='mps:0')\n",
      "Iteration 12350 Training loss 0.05997418239712715 Validation loss 0.06469421833753586 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.9407],\n",
      "        [0.9411]], device='mps:0')\n",
      "Iteration 12360 Training loss 0.06612095981836319 Validation loss 0.06464695185422897 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.3778],\n",
      "        [0.8575]], device='mps:0')\n",
      "Iteration 12370 Training loss 0.06423783302307129 Validation loss 0.0646452084183693 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.7221],\n",
      "        [0.8648]], device='mps:0')\n",
      "Iteration 12380 Training loss 0.0637897402048111 Validation loss 0.0647137314081192 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.4107],\n",
      "        [0.2907]], device='mps:0')\n",
      "Iteration 12390 Training loss 0.06236855313181877 Validation loss 0.06467515975236893 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.9785],\n",
      "        [0.8988]], device='mps:0')\n",
      "Iteration 12400 Training loss 0.06664591282606125 Validation loss 0.06483803689479828 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.2339],\n",
      "        [0.8245]], device='mps:0')\n",
      "Iteration 12410 Training loss 0.05998445302248001 Validation loss 0.06468918919563293 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.5731],\n",
      "        [0.2673]], device='mps:0')\n",
      "Iteration 12420 Training loss 0.06208784505724907 Validation loss 0.06484971940517426 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.1850],\n",
      "        [0.1756]], device='mps:0')\n",
      "Iteration 12430 Training loss 0.06600276380777359 Validation loss 0.0651358962059021 Accuracy 0.8218750357627869\n",
      "Output tensor([[0.4408],\n",
      "        [0.6421]], device='mps:0')\n",
      "Iteration 12440 Training loss 0.07140956819057465 Validation loss 0.06466901302337646 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.2534],\n",
      "        [0.5221]], device='mps:0')\n",
      "Iteration 12450 Training loss 0.06435997039079666 Validation loss 0.06471056491136551 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.8055],\n",
      "        [0.9889]], device='mps:0')\n",
      "Iteration 12460 Training loss 0.06878478080034256 Validation loss 0.06465232372283936 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.4552],\n",
      "        [0.9456]], device='mps:0')\n",
      "Iteration 12470 Training loss 0.07248716801404953 Validation loss 0.06463220715522766 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.5681],\n",
      "        [0.6754]], device='mps:0')\n",
      "Iteration 12480 Training loss 0.06857030838727951 Validation loss 0.0648641288280487 Accuracy 0.8225000500679016\n",
      "Output tensor([[0.0467],\n",
      "        [0.4439]], device='mps:0')\n",
      "Iteration 12490 Training loss 0.06796161830425262 Validation loss 0.0650329440832138 Accuracy 0.8220000267028809\n",
      "Output tensor([[0.6525],\n",
      "        [0.1614]], device='mps:0')\n",
      "Iteration 12500 Training loss 0.06602374464273453 Validation loss 0.06477763503789902 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.2025],\n",
      "        [0.5644]], device='mps:0')\n",
      "Iteration 12510 Training loss 0.06874538213014603 Validation loss 0.06464316695928574 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.8526],\n",
      "        [0.9490]], device='mps:0')\n",
      "Iteration 12520 Training loss 0.06398570537567139 Validation loss 0.06462594866752625 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.0059],\n",
      "        [0.0186]], device='mps:0')\n",
      "Iteration 12530 Training loss 0.0683990865945816 Validation loss 0.06460927426815033 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.6556],\n",
      "        [0.0347]], device='mps:0')\n",
      "Iteration 12540 Training loss 0.06392166763544083 Validation loss 0.06460617482662201 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.6108],\n",
      "        [0.0803]], device='mps:0')\n",
      "Iteration 12550 Training loss 0.06848612427711487 Validation loss 0.06465524435043335 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.3219],\n",
      "        [0.4330]], device='mps:0')\n",
      "Iteration 12560 Training loss 0.06549232453107834 Validation loss 0.06462789326906204 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.0912],\n",
      "        [0.3311]], device='mps:0')\n",
      "Iteration 12570 Training loss 0.0551164336502552 Validation loss 0.0646018534898758 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.3061],\n",
      "        [0.0509]], device='mps:0')\n",
      "Iteration 12580 Training loss 0.07499651610851288 Validation loss 0.06462214887142181 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.0806],\n",
      "        [0.5214]], device='mps:0')\n",
      "Iteration 12590 Training loss 0.06530238687992096 Validation loss 0.06460797786712646 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.5471],\n",
      "        [0.9084]], device='mps:0')\n",
      "Iteration 12600 Training loss 0.06252354383468628 Validation loss 0.06459229439496994 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.8737],\n",
      "        [0.0099]], device='mps:0')\n",
      "Iteration 12610 Training loss 0.07634533196687698 Validation loss 0.06462377309799194 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.2671],\n",
      "        [0.1036]], device='mps:0')\n",
      "Iteration 12620 Training loss 0.06791020929813385 Validation loss 0.06457545608282089 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.1647],\n",
      "        [0.0265]], device='mps:0')\n",
      "Iteration 12630 Training loss 0.05795780196785927 Validation loss 0.06470071524381638 Accuracy 0.8223750591278076\n",
      "Output tensor([[0.8841],\n",
      "        [0.2042]], device='mps:0')\n",
      "Iteration 12640 Training loss 0.07027693837881088 Validation loss 0.06460583209991455 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.7924],\n",
      "        [0.1393]], device='mps:0')\n",
      "Iteration 12650 Training loss 0.06560222804546356 Validation loss 0.06469985097646713 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.6021],\n",
      "        [0.9275]], device='mps:0')\n",
      "Iteration 12660 Training loss 0.06600213050842285 Validation loss 0.06455527245998383 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9369],\n",
      "        [0.5416]], device='mps:0')\n",
      "Iteration 12670 Training loss 0.0669536143541336 Validation loss 0.06457005441188812 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.1472],\n",
      "        [0.1433]], device='mps:0')\n",
      "Iteration 12680 Training loss 0.058953821659088135 Validation loss 0.06471332907676697 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.8270],\n",
      "        [0.2863]], device='mps:0')\n",
      "Iteration 12690 Training loss 0.06893332302570343 Validation loss 0.06458160281181335 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.4601],\n",
      "        [0.4340]], device='mps:0')\n",
      "Iteration 12700 Training loss 0.06237685680389404 Validation loss 0.06462765485048294 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.7682],\n",
      "        [0.1653]], device='mps:0')\n",
      "Iteration 12710 Training loss 0.06481488794088364 Validation loss 0.06475064158439636 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.4882],\n",
      "        [0.6949]], device='mps:0')\n",
      "Iteration 12720 Training loss 0.06169150397181511 Validation loss 0.06459323316812515 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.8847],\n",
      "        [0.2406]], device='mps:0')\n",
      "Iteration 12730 Training loss 0.054834041744470596 Validation loss 0.06453585624694824 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.8353],\n",
      "        [0.1657]], device='mps:0')\n",
      "Iteration 12740 Training loss 0.06340926140546799 Validation loss 0.06466351449489594 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.0216],\n",
      "        [0.6422]], device='mps:0')\n",
      "Iteration 12750 Training loss 0.07005448639392853 Validation loss 0.06453634798526764 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.9420],\n",
      "        [0.1779]], device='mps:0')\n",
      "Iteration 12760 Training loss 0.07000824809074402 Validation loss 0.0646277368068695 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.2727],\n",
      "        [0.5094]], device='mps:0')\n",
      "Iteration 12770 Training loss 0.06040553003549576 Validation loss 0.06452720612287521 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.0354],\n",
      "        [0.3154]], device='mps:0')\n",
      "Iteration 12780 Training loss 0.05927512049674988 Validation loss 0.06478425860404968 Accuracy 0.8226250410079956\n",
      "Output tensor([[0.0582],\n",
      "        [0.8771]], device='mps:0')\n",
      "Iteration 12790 Training loss 0.05493096262216568 Validation loss 0.06453081965446472 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.9118],\n",
      "        [0.6730]], device='mps:0')\n",
      "Iteration 12800 Training loss 0.0661795511841774 Validation loss 0.06460272520780563 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.0589],\n",
      "        [0.2346]], device='mps:0')\n",
      "Iteration 12810 Training loss 0.05779869854450226 Validation loss 0.0645349994301796 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.2030],\n",
      "        [0.4870]], device='mps:0')\n",
      "Iteration 12820 Training loss 0.06023632735013962 Validation loss 0.06467067450284958 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.6911],\n",
      "        [0.6931]], device='mps:0')\n",
      "Iteration 12830 Training loss 0.06257714331150055 Validation loss 0.0645747259259224 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.9358],\n",
      "        [0.6912]], device='mps:0')\n",
      "Iteration 12840 Training loss 0.06422523409128189 Validation loss 0.0646718442440033 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.3398],\n",
      "        [0.2066]], device='mps:0')\n",
      "Iteration 12850 Training loss 0.0701681524515152 Validation loss 0.06453545391559601 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.1560],\n",
      "        [0.2653]], device='mps:0')\n",
      "Iteration 12860 Training loss 0.05986175313591957 Validation loss 0.06464581191539764 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.4188],\n",
      "        [0.3899]], device='mps:0')\n",
      "Iteration 12870 Training loss 0.06430186331272125 Validation loss 0.06458182632923126 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.1224],\n",
      "        [0.3780]], device='mps:0')\n",
      "Iteration 12880 Training loss 0.07610409706830978 Validation loss 0.06460988521575928 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.9765],\n",
      "        [0.2227]], device='mps:0')\n",
      "Iteration 12890 Training loss 0.06447024643421173 Validation loss 0.06451092660427094 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.2217],\n",
      "        [0.0659]], device='mps:0')\n",
      "Iteration 12900 Training loss 0.05875546485185623 Validation loss 0.06452596932649612 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.6223],\n",
      "        [0.0951]], device='mps:0')\n",
      "Iteration 12910 Training loss 0.06024719029664993 Validation loss 0.06448033452033997 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.6625],\n",
      "        [0.4098]], device='mps:0')\n",
      "Iteration 12920 Training loss 0.06058423966169357 Validation loss 0.0645536407828331 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.8644],\n",
      "        [0.4653]], device='mps:0')\n",
      "Iteration 12930 Training loss 0.05469975247979164 Validation loss 0.06450531631708145 Accuracy 0.8228750228881836\n",
      "Output tensor([[0.0481],\n",
      "        [0.9155]], device='mps:0')\n",
      "Iteration 12940 Training loss 0.06722869724035263 Validation loss 0.06466943770647049 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.2359],\n",
      "        [0.6853]], device='mps:0')\n",
      "Iteration 12950 Training loss 0.056451376527547836 Validation loss 0.06445673108100891 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.7703],\n",
      "        [0.9429]], device='mps:0')\n",
      "Iteration 12960 Training loss 0.05444468930363655 Validation loss 0.06445291638374329 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.4836],\n",
      "        [0.9415]], device='mps:0')\n",
      "Iteration 12970 Training loss 0.06626521795988083 Validation loss 0.06482093781232834 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.7801],\n",
      "        [0.1955]], device='mps:0')\n",
      "Iteration 12980 Training loss 0.061037976294755936 Validation loss 0.06451740115880966 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.0038],\n",
      "        [0.9847]], device='mps:0')\n",
      "Iteration 12990 Training loss 0.06789308041334152 Validation loss 0.0644388273358345 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.0483],\n",
      "        [0.8861]], device='mps:0')\n",
      "Iteration 13000 Training loss 0.07030114531517029 Validation loss 0.06444282829761505 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.3124],\n",
      "        [0.6197]], device='mps:0')\n",
      "Iteration 13010 Training loss 0.06171540170907974 Validation loss 0.06450732797384262 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.7643],\n",
      "        [0.8759]], device='mps:0')\n",
      "Iteration 13020 Training loss 0.06182396784424782 Validation loss 0.06446593254804611 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.1950],\n",
      "        [0.4185]], device='mps:0')\n",
      "Iteration 13030 Training loss 0.06749042123556137 Validation loss 0.06451050937175751 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.9425],\n",
      "        [0.0559]], device='mps:0')\n",
      "Iteration 13040 Training loss 0.06164039298892021 Validation loss 0.06460294872522354 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.0910],\n",
      "        [0.2488]], device='mps:0')\n",
      "Iteration 13050 Training loss 0.06910203397274017 Validation loss 0.06444045156240463 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.7240],\n",
      "        [0.6489]], device='mps:0')\n",
      "Iteration 13060 Training loss 0.05879881605505943 Validation loss 0.06444968283176422 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.8734],\n",
      "        [0.2177]], device='mps:0')\n",
      "Iteration 13070 Training loss 0.056480783969163895 Validation loss 0.06447064876556396 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.1975],\n",
      "        [0.4726]], device='mps:0')\n",
      "Iteration 13080 Training loss 0.06981585174798965 Validation loss 0.06445111334323883 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.0016],\n",
      "        [0.2285]], device='mps:0')\n",
      "Iteration 13090 Training loss 0.07873131334781647 Validation loss 0.06442662328481674 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.8083],\n",
      "        [0.0788]], device='mps:0')\n",
      "Iteration 13100 Training loss 0.06431262940168381 Validation loss 0.06445629149675369 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.0574],\n",
      "        [0.8129]], device='mps:0')\n",
      "Iteration 13110 Training loss 0.07219913601875305 Validation loss 0.06442934274673462 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.8199],\n",
      "        [0.9423]], device='mps:0')\n",
      "Iteration 13120 Training loss 0.06810811907052994 Validation loss 0.06441136449575424 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.7731],\n",
      "        [0.9779]], device='mps:0')\n",
      "Iteration 13130 Training loss 0.05803004279732704 Validation loss 0.06470267474651337 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.2607],\n",
      "        [0.5826]], device='mps:0')\n",
      "Iteration 13140 Training loss 0.07280682772397995 Validation loss 0.0644497200846672 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.8114],\n",
      "        [0.0120]], device='mps:0')\n",
      "Iteration 13150 Training loss 0.06559155136346817 Validation loss 0.06440730392932892 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9885],\n",
      "        [0.0496]], device='mps:0')\n",
      "Iteration 13160 Training loss 0.06060756370425224 Validation loss 0.06453002989292145 Accuracy 0.8235000371932983\n",
      "Output tensor([[0.9652],\n",
      "        [0.9188]], device='mps:0')\n",
      "Iteration 13170 Training loss 0.05021732300519943 Validation loss 0.06452183425426483 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.9574],\n",
      "        [0.9138]], device='mps:0')\n",
      "Iteration 13180 Training loss 0.058934636414051056 Validation loss 0.06439056247472763 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.9631],\n",
      "        [0.9803]], device='mps:0')\n",
      "Iteration 13190 Training loss 0.06533320248126984 Validation loss 0.06448756903409958 Accuracy 0.8233750462532043\n",
      "Output tensor([[0.9561],\n",
      "        [0.7425]], device='mps:0')\n",
      "Iteration 13200 Training loss 0.05994356423616409 Validation loss 0.06440019607543945 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.8862],\n",
      "        [0.0696]], device='mps:0')\n",
      "Iteration 13210 Training loss 0.061590369790792465 Validation loss 0.06448028981685638 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.8455],\n",
      "        [0.8613]], device='mps:0')\n",
      "Iteration 13220 Training loss 0.06895028799772263 Validation loss 0.06438174843788147 Accuracy 0.8231250643730164\n",
      "Output tensor([[0.4550],\n",
      "        [0.7719]], device='mps:0')\n",
      "Iteration 13230 Training loss 0.0609295479953289 Validation loss 0.06456026434898376 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9727],\n",
      "        [0.5664]], device='mps:0')\n",
      "Iteration 13240 Training loss 0.06405787169933319 Validation loss 0.06437186151742935 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.5996],\n",
      "        [0.0622]], device='mps:0')\n",
      "Iteration 13250 Training loss 0.05999588966369629 Validation loss 0.06443649530410767 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.8352],\n",
      "        [0.8602]], device='mps:0')\n",
      "Iteration 13260 Training loss 0.06044255197048187 Validation loss 0.06441731005907059 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.1063],\n",
      "        [0.2431]], device='mps:0')\n",
      "Iteration 13270 Training loss 0.06695038825273514 Validation loss 0.06447606533765793 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.1739],\n",
      "        [0.9896]], device='mps:0')\n",
      "Iteration 13280 Training loss 0.06902290135622025 Validation loss 0.06435012817382812 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.9374],\n",
      "        [0.8517]], device='mps:0')\n",
      "Iteration 13290 Training loss 0.06588595360517502 Validation loss 0.06442669779062271 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.6383],\n",
      "        [0.1252]], device='mps:0')\n",
      "Iteration 13300 Training loss 0.07074609398841858 Validation loss 0.06447232514619827 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.1295],\n",
      "        [0.7560]], device='mps:0')\n",
      "Iteration 13310 Training loss 0.06643683463335037 Validation loss 0.06442836672067642 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.1613],\n",
      "        [0.7699]], device='mps:0')\n",
      "Iteration 13320 Training loss 0.058152731508016586 Validation loss 0.06434937566518784 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.1023],\n",
      "        [0.0336]], device='mps:0')\n",
      "Iteration 13330 Training loss 0.06906809657812119 Validation loss 0.06434024125337601 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.4302],\n",
      "        [0.2855]], device='mps:0')\n",
      "Iteration 13340 Training loss 0.060600440949201584 Validation loss 0.06435741484165192 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.9099],\n",
      "        [0.2138]], device='mps:0')\n",
      "Iteration 13350 Training loss 0.06442933529615402 Validation loss 0.06433156877756119 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.9733],\n",
      "        [0.7195]], device='mps:0')\n",
      "Iteration 13360 Training loss 0.06345923990011215 Validation loss 0.06440489739179611 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.3628],\n",
      "        [0.0806]], device='mps:0')\n",
      "Iteration 13370 Training loss 0.06741233915090561 Validation loss 0.06433246284723282 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9525],\n",
      "        [0.1150]], device='mps:0')\n",
      "Iteration 13380 Training loss 0.06156079098582268 Validation loss 0.06455795466899872 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.0742],\n",
      "        [0.2769]], device='mps:0')\n",
      "Iteration 13390 Training loss 0.07161188870668411 Validation loss 0.06437645852565765 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.2591],\n",
      "        [0.2556]], device='mps:0')\n",
      "Iteration 13400 Training loss 0.05714141204953194 Validation loss 0.06436500698328018 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.6357],\n",
      "        [0.3086]], device='mps:0')\n",
      "Iteration 13410 Training loss 0.06304717063903809 Validation loss 0.06429898738861084 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.9334],\n",
      "        [0.5929]], device='mps:0')\n",
      "Iteration 13420 Training loss 0.06433334946632385 Validation loss 0.06430790573358536 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.8814],\n",
      "        [0.3045]], device='mps:0')\n",
      "Iteration 13430 Training loss 0.06413277238607407 Validation loss 0.064396932721138 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.6043],\n",
      "        [0.5019]], device='mps:0')\n",
      "Iteration 13440 Training loss 0.06773602962493896 Validation loss 0.06431950628757477 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.6652],\n",
      "        [0.0489]], device='mps:0')\n",
      "Iteration 13450 Training loss 0.06648832559585571 Validation loss 0.06430178135633469 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.2791],\n",
      "        [0.7742]], device='mps:0')\n",
      "Iteration 13460 Training loss 0.06554223597049713 Validation loss 0.06438295543193817 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.0856],\n",
      "        [0.4124]], device='mps:0')\n",
      "Iteration 13470 Training loss 0.058113619685173035 Validation loss 0.06475018709897995 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.2097],\n",
      "        [0.0916]], device='mps:0')\n",
      "Iteration 13480 Training loss 0.05792861431837082 Validation loss 0.06433451920747757 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.5264],\n",
      "        [0.0659]], device='mps:0')\n",
      "Iteration 13490 Training loss 0.06888657063245773 Validation loss 0.06429584324359894 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.7895],\n",
      "        [0.2399]], device='mps:0')\n",
      "Iteration 13500 Training loss 0.06447560340166092 Validation loss 0.06428204476833344 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.8560],\n",
      "        [0.3569]], device='mps:0')\n",
      "Iteration 13510 Training loss 0.06341134756803513 Validation loss 0.06427928060293198 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.9755],\n",
      "        [0.3295]], device='mps:0')\n",
      "Iteration 13520 Training loss 0.05802038684487343 Validation loss 0.06427818536758423 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.9591],\n",
      "        [0.6501]], device='mps:0')\n",
      "Iteration 13530 Training loss 0.061627667397260666 Validation loss 0.06448442488908768 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.0163],\n",
      "        [0.3087]], device='mps:0')\n",
      "Iteration 13540 Training loss 0.06344834715127945 Validation loss 0.06428370624780655 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.9573],\n",
      "        [0.4836]], device='mps:0')\n",
      "Iteration 13550 Training loss 0.06000858172774315 Validation loss 0.0642663985490799 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.7098],\n",
      "        [0.9243]], device='mps:0')\n",
      "Iteration 13560 Training loss 0.06300227344036102 Validation loss 0.06434614211320877 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.8388],\n",
      "        [0.1478]], device='mps:0')\n",
      "Iteration 13570 Training loss 0.05956966429948807 Validation loss 0.06433825194835663 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.7921],\n",
      "        [0.6855]], device='mps:0')\n",
      "Iteration 13580 Training loss 0.0598626472055912 Validation loss 0.06430771201848984 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.0229],\n",
      "        [0.6888]], device='mps:0')\n",
      "Iteration 13590 Training loss 0.061274945735931396 Validation loss 0.06431619822978973 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.2423],\n",
      "        [0.8975]], device='mps:0')\n",
      "Iteration 13600 Training loss 0.056293219327926636 Validation loss 0.06429167836904526 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.5148],\n",
      "        [0.3911]], device='mps:0')\n",
      "Iteration 13610 Training loss 0.06146431341767311 Validation loss 0.06424154341220856 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.8697],\n",
      "        [0.9585]], device='mps:0')\n",
      "Iteration 13620 Training loss 0.05810578167438507 Validation loss 0.06428217142820358 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.5645],\n",
      "        [0.0270]], device='mps:0')\n",
      "Iteration 13630 Training loss 0.06852535903453827 Validation loss 0.0642291009426117 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.5494],\n",
      "        [0.7795]], device='mps:0')\n",
      "Iteration 13640 Training loss 0.06760790944099426 Validation loss 0.06430970132350922 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.6031],\n",
      "        [0.2384]], device='mps:0')\n",
      "Iteration 13650 Training loss 0.0638149231672287 Validation loss 0.06441499292850494 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.6169],\n",
      "        [0.3246]], device='mps:0')\n",
      "Iteration 13660 Training loss 0.0636499896645546 Validation loss 0.06422301381826401 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.7751],\n",
      "        [0.7866]], device='mps:0')\n",
      "Iteration 13670 Training loss 0.067386195063591 Validation loss 0.06423385441303253 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.7200],\n",
      "        [0.5339]], device='mps:0')\n",
      "Iteration 13680 Training loss 0.06506803631782532 Validation loss 0.06421150267124176 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.2166],\n",
      "        [0.1459]], device='mps:0')\n",
      "Iteration 13690 Training loss 0.06255233287811279 Validation loss 0.06440633535385132 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9084],\n",
      "        [0.8258]], device='mps:0')\n",
      "Iteration 13700 Training loss 0.06385313719511032 Validation loss 0.06440624594688416 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.8401],\n",
      "        [0.9248]], device='mps:0')\n",
      "Iteration 13710 Training loss 0.06372921913862228 Validation loss 0.06432171911001205 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.4169],\n",
      "        [0.9860]], device='mps:0')\n",
      "Iteration 13720 Training loss 0.05653364583849907 Validation loss 0.06432774662971497 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.8074],\n",
      "        [0.8959]], device='mps:0')\n",
      "Iteration 13730 Training loss 0.06732229143381119 Validation loss 0.06420906633138657 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.1549],\n",
      "        [0.8430]], device='mps:0')\n",
      "Iteration 13740 Training loss 0.05875017121434212 Validation loss 0.06419192254543304 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.0392],\n",
      "        [0.1431]], device='mps:0')\n",
      "Iteration 13750 Training loss 0.06455045193433762 Validation loss 0.06432103365659714 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.0172],\n",
      "        [0.9923]], device='mps:0')\n",
      "Iteration 13760 Training loss 0.06565327197313309 Validation loss 0.06449253112077713 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.8016],\n",
      "        [0.0456]], device='mps:0')\n",
      "Iteration 13770 Training loss 0.06398458033800125 Validation loss 0.06419443339109421 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.7487],\n",
      "        [0.6369]], device='mps:0')\n",
      "Iteration 13780 Training loss 0.06278640031814575 Validation loss 0.06432601064443588 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.9530],\n",
      "        [0.8601]], device='mps:0')\n",
      "Iteration 13790 Training loss 0.059500738978385925 Validation loss 0.06416981667280197 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.2364],\n",
      "        [0.7829]], device='mps:0')\n",
      "Iteration 13800 Training loss 0.05999104306101799 Validation loss 0.0643719956278801 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.6671],\n",
      "        [0.0865]], device='mps:0')\n",
      "Iteration 13810 Training loss 0.06193121150135994 Validation loss 0.0642896518111229 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.1481],\n",
      "        [0.1923]], device='mps:0')\n",
      "Iteration 13820 Training loss 0.059808142483234406 Validation loss 0.06418729573488235 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.3218],\n",
      "        [0.8895]], device='mps:0')\n",
      "Iteration 13830 Training loss 0.06356485933065414 Validation loss 0.06421186774969101 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.8657],\n",
      "        [0.3737]], device='mps:0')\n",
      "Iteration 13840 Training loss 0.06665348261594772 Validation loss 0.06427977979183197 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.9419],\n",
      "        [0.5282]], device='mps:0')\n",
      "Iteration 13850 Training loss 0.05493907630443573 Validation loss 0.06415966153144836 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.2530],\n",
      "        [0.5563]], device='mps:0')\n",
      "Iteration 13860 Training loss 0.05620664358139038 Validation loss 0.0641966462135315 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.9552],\n",
      "        [0.5979]], device='mps:0')\n",
      "Iteration 13870 Training loss 0.05981831252574921 Validation loss 0.06419003009796143 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.1160],\n",
      "        [0.0463]], device='mps:0')\n",
      "Iteration 13880 Training loss 0.06482643634080887 Validation loss 0.0642852932214737 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.2096],\n",
      "        [0.0708]], device='mps:0')\n",
      "Iteration 13890 Training loss 0.06540246307849884 Validation loss 0.0641554668545723 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.6785],\n",
      "        [0.8394]], device='mps:0')\n",
      "Iteration 13900 Training loss 0.06254775077104568 Validation loss 0.06413649022579193 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.7177],\n",
      "        [0.8631]], device='mps:0')\n",
      "Iteration 13910 Training loss 0.060860004276037216 Validation loss 0.06424500048160553 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.3356],\n",
      "        [0.3978]], device='mps:0')\n",
      "Iteration 13920 Training loss 0.060059648007154465 Validation loss 0.06418440490961075 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.1492],\n",
      "        [0.9210]], device='mps:0')\n",
      "Iteration 13930 Training loss 0.06482148170471191 Validation loss 0.06446550041437149 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.3297],\n",
      "        [0.0130]], device='mps:0')\n",
      "Iteration 13940 Training loss 0.06610018759965897 Validation loss 0.06420291215181351 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.2741],\n",
      "        [0.1985]], device='mps:0')\n",
      "Iteration 13950 Training loss 0.053926944732666016 Validation loss 0.06435727328062057 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.7094],\n",
      "        [0.9079]], device='mps:0')\n",
      "Iteration 13960 Training loss 0.06565409153699875 Validation loss 0.06416397541761398 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9595],\n",
      "        [0.6134]], device='mps:0')\n",
      "Iteration 13970 Training loss 0.05645313486456871 Validation loss 0.0641351118683815 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.0413],\n",
      "        [0.1330]], device='mps:0')\n",
      "Iteration 13980 Training loss 0.0646970346570015 Validation loss 0.06411955505609512 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.4458],\n",
      "        [0.1165]], device='mps:0')\n",
      "Iteration 13990 Training loss 0.06558512151241302 Validation loss 0.06416849046945572 Accuracy 0.8240000605583191\n",
      "Output tensor([[0.9389],\n",
      "        [0.6251]], device='mps:0')\n",
      "Iteration 14000 Training loss 0.06861764937639236 Validation loss 0.06413031369447708 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.0123],\n",
      "        [0.8918]], device='mps:0')\n",
      "Iteration 14010 Training loss 0.06894002854824066 Validation loss 0.064211905002594 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.0564],\n",
      "        [0.7731]], device='mps:0')\n",
      "Iteration 14020 Training loss 0.06741995364427567 Validation loss 0.06424829363822937 Accuracy 0.8241250514984131\n",
      "Output tensor([[0.0646],\n",
      "        [0.6739]], device='mps:0')\n",
      "Iteration 14030 Training loss 0.06508997082710266 Validation loss 0.06411567330360413 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.9418],\n",
      "        [0.0613]], device='mps:0')\n",
      "Iteration 14040 Training loss 0.06694138050079346 Validation loss 0.0640922263264656 Accuracy 0.8236250281333923\n",
      "Output tensor([[0.9050],\n",
      "        [0.1633]], device='mps:0')\n",
      "Iteration 14050 Training loss 0.0533539317548275 Validation loss 0.06409041583538055 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.2432],\n",
      "        [0.6663]], device='mps:0')\n",
      "Iteration 14060 Training loss 0.06779827177524567 Validation loss 0.06470856070518494 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.0207],\n",
      "        [0.4201]], device='mps:0')\n",
      "Iteration 14070 Training loss 0.06843308359384537 Validation loss 0.06416735798120499 Accuracy 0.8237500190734863\n",
      "Output tensor([[0.5626],\n",
      "        [0.9488]], device='mps:0')\n",
      "Iteration 14080 Training loss 0.05888745188713074 Validation loss 0.06412996351718903 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.8623],\n",
      "        [0.0960]], device='mps:0')\n",
      "Iteration 14090 Training loss 0.06362467259168625 Validation loss 0.0641135722398758 Accuracy 0.8232500553131104\n",
      "Output tensor([[0.4757],\n",
      "        [0.4330]], device='mps:0')\n",
      "Iteration 14100 Training loss 0.06180412694811821 Validation loss 0.06414645910263062 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.3793],\n",
      "        [0.5269]], device='mps:0')\n",
      "Iteration 14110 Training loss 0.05744680389761925 Validation loss 0.06406188011169434 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.1227],\n",
      "        [0.8770]], device='mps:0')\n",
      "Iteration 14120 Training loss 0.06122184544801712 Validation loss 0.06407856196165085 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.5302],\n",
      "        [0.6202]], device='mps:0')\n",
      "Iteration 14130 Training loss 0.053781766444444656 Validation loss 0.0641791895031929 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.9253],\n",
      "        [0.7171]], device='mps:0')\n",
      "Iteration 14140 Training loss 0.060180407017469406 Validation loss 0.06425853073596954 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.1563],\n",
      "        [0.5166]], device='mps:0')\n",
      "Iteration 14150 Training loss 0.06684519350528717 Validation loss 0.06405899673700333 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.3825],\n",
      "        [0.9451]], device='mps:0')\n",
      "Iteration 14160 Training loss 0.06170828640460968 Validation loss 0.06429100781679153 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.0390],\n",
      "        [0.5892]], device='mps:0')\n",
      "Iteration 14170 Training loss 0.06160949543118477 Validation loss 0.06407749652862549 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.8040],\n",
      "        [0.5013]], device='mps:0')\n",
      "Iteration 14180 Training loss 0.06791279464960098 Validation loss 0.06404849886894226 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.7684],\n",
      "        [0.9085]], device='mps:0')\n",
      "Iteration 14190 Training loss 0.06372938305139542 Validation loss 0.06408912688493729 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.9423],\n",
      "        [0.9932]], device='mps:0')\n",
      "Iteration 14200 Training loss 0.05633039027452469 Validation loss 0.06404506415128708 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.8042],\n",
      "        [0.5112]], device='mps:0')\n",
      "Iteration 14210 Training loss 0.06836839765310287 Validation loss 0.06416937708854675 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.4380],\n",
      "        [0.4783]], device='mps:0')\n",
      "Iteration 14220 Training loss 0.06100721284747124 Validation loss 0.06433743238449097 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.1556],\n",
      "        [0.9827]], device='mps:0')\n",
      "Iteration 14230 Training loss 0.06148364022374153 Validation loss 0.0640861988067627 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.9397],\n",
      "        [0.5403]], device='mps:0')\n",
      "Iteration 14240 Training loss 0.0593399703502655 Validation loss 0.06414979696273804 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.6726],\n",
      "        [0.8932]], device='mps:0')\n",
      "Iteration 14250 Training loss 0.06320151686668396 Validation loss 0.06405583769083023 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.3183],\n",
      "        [0.1396]], device='mps:0')\n",
      "Iteration 14260 Training loss 0.05783756449818611 Validation loss 0.0640520378947258 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.0997],\n",
      "        [0.9445]], device='mps:0')\n",
      "Iteration 14270 Training loss 0.06726274639368057 Validation loss 0.06422552466392517 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9650],\n",
      "        [0.9429]], device='mps:0')\n",
      "Iteration 14280 Training loss 0.06032637506723404 Validation loss 0.06415333598852158 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.6494],\n",
      "        [0.0390]], device='mps:0')\n",
      "Iteration 14290 Training loss 0.06676581501960754 Validation loss 0.0641496330499649 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.8878],\n",
      "        [0.9600]], device='mps:0')\n",
      "Iteration 14300 Training loss 0.06046401709318161 Validation loss 0.06416894495487213 Accuracy 0.8238750100135803\n",
      "Output tensor([[0.1818],\n",
      "        [0.0239]], device='mps:0')\n",
      "Iteration 14310 Training loss 0.06768763810396194 Validation loss 0.0640840232372284 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.1835],\n",
      "        [0.2760]], device='mps:0')\n",
      "Iteration 14320 Training loss 0.062456902116537094 Validation loss 0.06401801109313965 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.7287],\n",
      "        [0.0397]], device='mps:0')\n",
      "Iteration 14330 Training loss 0.057859256863594055 Validation loss 0.06400991231203079 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.1507],\n",
      "        [0.0906]], device='mps:0')\n",
      "Iteration 14340 Training loss 0.06374649703502655 Validation loss 0.0641007125377655 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.9651],\n",
      "        [0.0570]], device='mps:0')\n",
      "Iteration 14350 Training loss 0.06515839695930481 Validation loss 0.0640038475394249 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.3735],\n",
      "        [0.1615]], device='mps:0')\n",
      "Iteration 14360 Training loss 0.06731660664081573 Validation loss 0.06404358893632889 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.4396],\n",
      "        [0.9828]], device='mps:0')\n",
      "Iteration 14370 Training loss 0.05700485035777092 Validation loss 0.06409197300672531 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.9415],\n",
      "        [0.6974]], device='mps:0')\n",
      "Iteration 14380 Training loss 0.06434927880764008 Validation loss 0.06400831788778305 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.9118],\n",
      "        [0.4883]], device='mps:0')\n",
      "Iteration 14390 Training loss 0.062023963779211044 Validation loss 0.06427734345197678 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.0253],\n",
      "        [0.6703]], device='mps:0')\n",
      "Iteration 14400 Training loss 0.06620588898658752 Validation loss 0.0640421137213707 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.9252],\n",
      "        [0.9359]], device='mps:0')\n",
      "Iteration 14410 Training loss 0.061326853930950165 Validation loss 0.06406863778829575 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.9471],\n",
      "        [0.8075]], device='mps:0')\n",
      "Iteration 14420 Training loss 0.06101066619157791 Validation loss 0.06397981196641922 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.8124],\n",
      "        [0.0153]], device='mps:0')\n",
      "Iteration 14430 Training loss 0.066164031624794 Validation loss 0.06409891694784164 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.6683],\n",
      "        [0.7231]], device='mps:0')\n",
      "Iteration 14440 Training loss 0.056607436388731 Validation loss 0.06399024277925491 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.9011],\n",
      "        [0.4263]], device='mps:0')\n",
      "Iteration 14450 Training loss 0.060197360813617706 Validation loss 0.06411035358905792 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.3994],\n",
      "        [0.0957]], device='mps:0')\n",
      "Iteration 14460 Training loss 0.06330159306526184 Validation loss 0.0643651932477951 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.6917],\n",
      "        [0.4179]], device='mps:0')\n",
      "Iteration 14470 Training loss 0.06514830887317657 Validation loss 0.06394375115633011 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.8474],\n",
      "        [0.1501]], device='mps:0')\n",
      "Iteration 14480 Training loss 0.06233110651373863 Validation loss 0.06401749700307846 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.2275],\n",
      "        [0.3240]], device='mps:0')\n",
      "Iteration 14490 Training loss 0.059248197823762894 Validation loss 0.06401348859071732 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.1861],\n",
      "        [0.2988]], device='mps:0')\n",
      "Iteration 14500 Training loss 0.07206838577985764 Validation loss 0.06408155709505081 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.7293],\n",
      "        [0.0845]], device='mps:0')\n",
      "Iteration 14510 Training loss 0.06881469488143921 Validation loss 0.06440744549036026 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.0286],\n",
      "        [0.7946]], device='mps:0')\n",
      "Iteration 14520 Training loss 0.06049869954586029 Validation loss 0.06393489986658096 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.0436],\n",
      "        [0.9311]], device='mps:0')\n",
      "Iteration 14530 Training loss 0.06658172607421875 Validation loss 0.06425781548023224 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.7521],\n",
      "        [0.4584]], device='mps:0')\n",
      "Iteration 14540 Training loss 0.06708142161369324 Validation loss 0.06429797410964966 Accuracy 0.8242500424385071\n",
      "Output tensor([[0.3878],\n",
      "        [0.9916]], device='mps:0')\n",
      "Iteration 14550 Training loss 0.056848857551813126 Validation loss 0.06392286717891693 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.0948],\n",
      "        [0.8905]], device='mps:0')\n",
      "Iteration 14560 Training loss 0.061606425791978836 Validation loss 0.06396408379077911 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.8267],\n",
      "        [0.2000]], device='mps:0')\n",
      "Iteration 14570 Training loss 0.06452510505914688 Validation loss 0.06392654031515121 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9981],\n",
      "        [0.8523]], device='mps:0')\n",
      "Iteration 14580 Training loss 0.06956613808870316 Validation loss 0.06415556371212006 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.5070],\n",
      "        [0.3573]], device='mps:0')\n",
      "Iteration 14590 Training loss 0.06877794116735458 Validation loss 0.06417331099510193 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.0753],\n",
      "        [0.2721]], device='mps:0')\n",
      "Iteration 14600 Training loss 0.06864582747220993 Validation loss 0.06400689482688904 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.9878],\n",
      "        [0.3356]], device='mps:0')\n",
      "Iteration 14610 Training loss 0.06087310612201691 Validation loss 0.06396934390068054 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.2451],\n",
      "        [0.4706]], device='mps:0')\n",
      "Iteration 14620 Training loss 0.05178679898381233 Validation loss 0.06391604244709015 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.0215],\n",
      "        [0.0982]], device='mps:0')\n",
      "Iteration 14630 Training loss 0.06884303689002991 Validation loss 0.06408557295799255 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.9370],\n",
      "        [0.0238]], device='mps:0')\n",
      "Iteration 14640 Training loss 0.06374462693929672 Validation loss 0.06399562954902649 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.4491],\n",
      "        [0.0980]], device='mps:0')\n",
      "Iteration 14650 Training loss 0.06967995315790176 Validation loss 0.0639033243060112 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.9971],\n",
      "        [0.0943]], device='mps:0')\n",
      "Iteration 14660 Training loss 0.06156841665506363 Validation loss 0.06398071348667145 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.6783],\n",
      "        [0.3147]], device='mps:0')\n",
      "Iteration 14670 Training loss 0.06337057054042816 Validation loss 0.06390460580587387 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.9273],\n",
      "        [0.1890]], device='mps:0')\n",
      "Iteration 14680 Training loss 0.05856186896562576 Validation loss 0.06391950696706772 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.8886],\n",
      "        [0.1626]], device='mps:0')\n",
      "Iteration 14690 Training loss 0.05741250887513161 Validation loss 0.06406249850988388 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.4705],\n",
      "        [0.6933]], device='mps:0')\n",
      "Iteration 14700 Training loss 0.06486058235168457 Validation loss 0.06388629972934723 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0941],\n",
      "        [0.3727]], device='mps:0')\n",
      "Iteration 14710 Training loss 0.05499831959605217 Validation loss 0.06389354914426804 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.9588],\n",
      "        [0.5615]], device='mps:0')\n",
      "Iteration 14720 Training loss 0.055521029978990555 Validation loss 0.06419254839420319 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.6337],\n",
      "        [0.1905]], device='mps:0')\n",
      "Iteration 14730 Training loss 0.05968894809484482 Validation loss 0.0639803484082222 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.2010],\n",
      "        [0.8625]], device='mps:0')\n",
      "Iteration 14740 Training loss 0.052561718970537186 Validation loss 0.06390628218650818 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.3253],\n",
      "        [0.4568]], device='mps:0')\n",
      "Iteration 14750 Training loss 0.0643143579363823 Validation loss 0.06387214362621307 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.0460],\n",
      "        [0.0084]], device='mps:0')\n",
      "Iteration 14760 Training loss 0.06232362613081932 Validation loss 0.06388457119464874 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.7967],\n",
      "        [0.2946]], device='mps:0')\n",
      "Iteration 14770 Training loss 0.06546908617019653 Validation loss 0.06388938426971436 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.0770],\n",
      "        [0.4580]], device='mps:0')\n",
      "Iteration 14780 Training loss 0.0641913190484047 Validation loss 0.06394943594932556 Accuracy 0.8243750333786011\n",
      "Output tensor([[0.1513],\n",
      "        [0.9356]], device='mps:0')\n",
      "Iteration 14790 Training loss 0.06098802387714386 Validation loss 0.06396164000034332 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.6279],\n",
      "        [0.7334]], device='mps:0')\n",
      "Iteration 14800 Training loss 0.06296373158693314 Validation loss 0.06410632282495499 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.1094],\n",
      "        [0.6797]], device='mps:0')\n",
      "Iteration 14810 Training loss 0.06712186336517334 Validation loss 0.06388218700885773 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.9772],\n",
      "        [0.9611]], device='mps:0')\n",
      "Iteration 14820 Training loss 0.056924257427453995 Validation loss 0.06384005397558212 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.4061],\n",
      "        [0.8910]], device='mps:0')\n",
      "Iteration 14830 Training loss 0.06138334050774574 Validation loss 0.06386930495500565 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.2487],\n",
      "        [0.0751]], device='mps:0')\n",
      "Iteration 14840 Training loss 0.061827611178159714 Validation loss 0.06384273618459702 Accuracy 0.8245000243186951\n",
      "Output tensor([[0.5475],\n",
      "        [0.7279]], device='mps:0')\n",
      "Iteration 14850 Training loss 0.06092086806893349 Validation loss 0.06387712806463242 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.5402],\n",
      "        [0.9936]], device='mps:0')\n",
      "Iteration 14860 Training loss 0.0613405779004097 Validation loss 0.06387291848659515 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.8447],\n",
      "        [0.4578]], device='mps:0')\n",
      "Iteration 14870 Training loss 0.06507772952318192 Validation loss 0.06427434086799622 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.9769],\n",
      "        [0.8969]], device='mps:0')\n",
      "Iteration 14880 Training loss 0.0539497546851635 Validation loss 0.06386057287454605 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.1301],\n",
      "        [0.7548]], device='mps:0')\n",
      "Iteration 14890 Training loss 0.06164270639419556 Validation loss 0.06386088579893112 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.2481],\n",
      "        [0.7694]], device='mps:0')\n",
      "Iteration 14900 Training loss 0.05804817005991936 Validation loss 0.0638442113995552 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.9534],\n",
      "        [0.9851]], device='mps:0')\n",
      "Iteration 14910 Training loss 0.06272561103105545 Validation loss 0.06381238996982574 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.9858],\n",
      "        [0.3912]], device='mps:0')\n",
      "Iteration 14920 Training loss 0.06018326058983803 Validation loss 0.06402736902236938 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.1348],\n",
      "        [0.9005]], device='mps:0')\n",
      "Iteration 14930 Training loss 0.0610077902674675 Validation loss 0.06381333619356155 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.9145],\n",
      "        [0.6409]], device='mps:0')\n",
      "Iteration 14940 Training loss 0.054467372596263885 Validation loss 0.0638316348195076 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.5200],\n",
      "        [0.1890]], device='mps:0')\n",
      "Iteration 14950 Training loss 0.06009286269545555 Validation loss 0.06379630416631699 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.2081],\n",
      "        [0.8477]], device='mps:0')\n",
      "Iteration 14960 Training loss 0.06431015580892563 Validation loss 0.06379202008247375 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.9924],\n",
      "        [0.7003]], device='mps:0')\n",
      "Iteration 14970 Training loss 0.06464937329292297 Validation loss 0.06401020288467407 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0621],\n",
      "        [0.1569]], device='mps:0')\n",
      "Iteration 14980 Training loss 0.07538197189569473 Validation loss 0.06379386782646179 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.1602],\n",
      "        [0.8539]], device='mps:0')\n",
      "Iteration 14990 Training loss 0.0689627081155777 Validation loss 0.06378749012947083 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.8510],\n",
      "        [0.8142]], device='mps:0')\n",
      "Iteration 15000 Training loss 0.064854696393013 Validation loss 0.0640052855014801 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.6570],\n",
      "        [0.9577]], device='mps:0')\n",
      "Iteration 15010 Training loss 0.0680803656578064 Validation loss 0.06388300657272339 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.9859],\n",
      "        [0.1792]], device='mps:0')\n",
      "Iteration 15020 Training loss 0.05735541135072708 Validation loss 0.06380613148212433 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.0617],\n",
      "        [0.6733]], device='mps:0')\n",
      "Iteration 15030 Training loss 0.05560411512851715 Validation loss 0.06393270194530487 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.4964],\n",
      "        [0.7775]], device='mps:0')\n",
      "Iteration 15040 Training loss 0.067238450050354 Validation loss 0.06376753002405167 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.8679],\n",
      "        [0.3786]], device='mps:0')\n",
      "Iteration 15050 Training loss 0.06892048567533493 Validation loss 0.06379911303520203 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.4018],\n",
      "        [0.3125]], device='mps:0')\n",
      "Iteration 15060 Training loss 0.05687462165951729 Validation loss 0.06411999464035034 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.8379],\n",
      "        [0.9225]], device='mps:0')\n",
      "Iteration 15070 Training loss 0.06839322298765182 Validation loss 0.06381328403949738 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.5250],\n",
      "        [0.1327]], device='mps:0')\n",
      "Iteration 15080 Training loss 0.06001971662044525 Validation loss 0.06433847546577454 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.0665],\n",
      "        [0.9814]], device='mps:0')\n",
      "Iteration 15090 Training loss 0.060835253447294235 Validation loss 0.06382329016923904 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.7723],\n",
      "        [0.2294]], device='mps:0')\n",
      "Iteration 15100 Training loss 0.06289058178663254 Validation loss 0.06397861242294312 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.4101],\n",
      "        [0.2947]], device='mps:0')\n",
      "Iteration 15110 Training loss 0.061243314296007156 Validation loss 0.06379697471857071 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.3696],\n",
      "        [0.9717]], device='mps:0')\n",
      "Iteration 15120 Training loss 0.06003584712743759 Validation loss 0.06375481933355331 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.9472],\n",
      "        [0.9737]], device='mps:0')\n",
      "Iteration 15130 Training loss 0.058474089950323105 Validation loss 0.06377467513084412 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.5758],\n",
      "        [0.1652]], device='mps:0')\n",
      "Iteration 15140 Training loss 0.07219205051660538 Validation loss 0.0638192743062973 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.9807],\n",
      "        [0.3157]], device='mps:0')\n",
      "Iteration 15150 Training loss 0.058693792670965195 Validation loss 0.06383029371500015 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.7911],\n",
      "        [0.0793]], device='mps:0')\n",
      "Iteration 15160 Training loss 0.05886400118470192 Validation loss 0.06374748051166534 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.6011],\n",
      "        [0.7044]], device='mps:0')\n",
      "Iteration 15170 Training loss 0.07256115972995758 Validation loss 0.06383036077022552 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.5528],\n",
      "        [0.8820]], device='mps:0')\n",
      "Iteration 15180 Training loss 0.06659235060214996 Validation loss 0.06387179344892502 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.5842],\n",
      "        [0.6312]], device='mps:0')\n",
      "Iteration 15190 Training loss 0.06025585159659386 Validation loss 0.06380008906126022 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.9317],\n",
      "        [0.7808]], device='mps:0')\n",
      "Iteration 15200 Training loss 0.06627360731363297 Validation loss 0.0637354850769043 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.8501],\n",
      "        [0.6636]], device='mps:0')\n",
      "Iteration 15210 Training loss 0.062411051243543625 Validation loss 0.06383002549409866 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.9203],\n",
      "        [0.8671]], device='mps:0')\n",
      "Iteration 15220 Training loss 0.06476154178380966 Validation loss 0.06373100727796555 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.4557],\n",
      "        [0.6935]], device='mps:0')\n",
      "Iteration 15230 Training loss 0.07030510157346725 Validation loss 0.06386403739452362 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0909],\n",
      "        [0.2400]], device='mps:0')\n",
      "Iteration 15240 Training loss 0.06614427268505096 Validation loss 0.06395092606544495 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.1534],\n",
      "        [0.1172]], device='mps:0')\n",
      "Iteration 15250 Training loss 0.06236957386136055 Validation loss 0.0637471079826355 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.0309],\n",
      "        [0.4461]], device='mps:0')\n",
      "Iteration 15260 Training loss 0.06576140224933624 Validation loss 0.06376570463180542 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9119],\n",
      "        [0.0402]], device='mps:0')\n",
      "Iteration 15270 Training loss 0.06300359219312668 Validation loss 0.06377188116312027 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.7603],\n",
      "        [0.2153]], device='mps:0')\n",
      "Iteration 15280 Training loss 0.06346236169338226 Validation loss 0.06372600048780441 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.5459],\n",
      "        [0.9874]], device='mps:0')\n",
      "Iteration 15290 Training loss 0.06924614310264587 Validation loss 0.0637236163020134 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.3565],\n",
      "        [0.3056]], device='mps:0')\n",
      "Iteration 15300 Training loss 0.055850133299827576 Validation loss 0.0637243241071701 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.7860],\n",
      "        [0.1354]], device='mps:0')\n",
      "Iteration 15310 Training loss 0.0597747266292572 Validation loss 0.06378006190061569 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.7743],\n",
      "        [0.1542]], device='mps:0')\n",
      "Iteration 15320 Training loss 0.06427719444036484 Validation loss 0.06371516734361649 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.8018],\n",
      "        [0.9505]], device='mps:0')\n",
      "Iteration 15330 Training loss 0.050452906638383865 Validation loss 0.06372130662202835 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.6113],\n",
      "        [0.9421]], device='mps:0')\n",
      "Iteration 15340 Training loss 0.06424111127853394 Validation loss 0.06385207921266556 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.6077],\n",
      "        [0.3290]], device='mps:0')\n",
      "Iteration 15350 Training loss 0.07223957031965256 Validation loss 0.06369969993829727 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.8814],\n",
      "        [0.9076]], device='mps:0')\n",
      "Iteration 15360 Training loss 0.06165211647748947 Validation loss 0.06370700895786285 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.3864],\n",
      "        [0.8637]], device='mps:0')\n",
      "Iteration 15370 Training loss 0.055242691189050674 Validation loss 0.06370317190885544 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.7027],\n",
      "        [0.0277]], device='mps:0')\n",
      "Iteration 15380 Training loss 0.05759238451719284 Validation loss 0.06373906880617142 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.3246],\n",
      "        [0.2769]], device='mps:0')\n",
      "Iteration 15390 Training loss 0.06176538020372391 Validation loss 0.06368986517190933 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.6390],\n",
      "        [0.4730]], device='mps:0')\n",
      "Iteration 15400 Training loss 0.05884302780032158 Validation loss 0.06368832290172577 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.7381],\n",
      "        [0.2749]], device='mps:0')\n",
      "Iteration 15410 Training loss 0.06177540123462677 Validation loss 0.06377769261598587 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.8903],\n",
      "        [0.4842]], device='mps:0')\n",
      "Iteration 15420 Training loss 0.06215565651655197 Validation loss 0.06381478905677795 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.4481],\n",
      "        [0.7612]], device='mps:0')\n",
      "Iteration 15430 Training loss 0.06349091231822968 Validation loss 0.0637180507183075 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.4175],\n",
      "        [0.9064]], device='mps:0')\n",
      "Iteration 15440 Training loss 0.05546192079782486 Validation loss 0.06369684636592865 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.7656],\n",
      "        [0.6162]], device='mps:0')\n",
      "Iteration 15450 Training loss 0.05900246649980545 Validation loss 0.06379023939371109 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.1401],\n",
      "        [0.9390]], device='mps:0')\n",
      "Iteration 15460 Training loss 0.057993095368146896 Validation loss 0.06371592730283737 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.5438],\n",
      "        [0.0479]], device='mps:0')\n",
      "Iteration 15470 Training loss 0.0683063194155693 Validation loss 0.06367924809455872 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.7770],\n",
      "        [0.0163]], device='mps:0')\n",
      "Iteration 15480 Training loss 0.05749756842851639 Validation loss 0.06368467956781387 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9885],\n",
      "        [0.2870]], device='mps:0')\n",
      "Iteration 15490 Training loss 0.06798228621482849 Validation loss 0.06367524713277817 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.0412],\n",
      "        [0.9894]], device='mps:0')\n",
      "Iteration 15500 Training loss 0.06959231942892075 Validation loss 0.06366120278835297 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.1643],\n",
      "        [0.4343]], device='mps:0')\n",
      "Iteration 15510 Training loss 0.06587495654821396 Validation loss 0.06365406513214111 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.8524],\n",
      "        [0.1035]], device='mps:0')\n",
      "Iteration 15520 Training loss 0.06775902956724167 Validation loss 0.06366750597953796 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.1015],\n",
      "        [0.6504]], device='mps:0')\n",
      "Iteration 15530 Training loss 0.05999862402677536 Validation loss 0.06367786228656769 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.5917],\n",
      "        [0.7229]], device='mps:0')\n",
      "Iteration 15540 Training loss 0.06723865121603012 Validation loss 0.06366269290447235 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.4565],\n",
      "        [0.6967]], device='mps:0')\n",
      "Iteration 15550 Training loss 0.05835578218102455 Validation loss 0.06366190314292908 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.6572],\n",
      "        [0.9364]], device='mps:0')\n",
      "Iteration 15560 Training loss 0.06005530059337616 Validation loss 0.0636579617857933 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.9783],\n",
      "        [0.4816]], device='mps:0')\n",
      "Iteration 15570 Training loss 0.06793871521949768 Validation loss 0.06363137066364288 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.9363],\n",
      "        [0.7061]], device='mps:0')\n",
      "Iteration 15580 Training loss 0.060087837278842926 Validation loss 0.0636259913444519 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.3264],\n",
      "        [0.0751]], device='mps:0')\n",
      "Iteration 15590 Training loss 0.06797990202903748 Validation loss 0.06362392753362656 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.9422],\n",
      "        [0.8648]], device='mps:0')\n",
      "Iteration 15600 Training loss 0.061419982463121414 Validation loss 0.0639982596039772 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.6716],\n",
      "        [0.3368]], device='mps:0')\n",
      "Iteration 15610 Training loss 0.0641867145895958 Validation loss 0.06361615657806396 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.3503],\n",
      "        [0.1665]], device='mps:0')\n",
      "Iteration 15620 Training loss 0.07222207635641098 Validation loss 0.06374388188123703 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.8769],\n",
      "        [0.3544]], device='mps:0')\n",
      "Iteration 15630 Training loss 0.06633274257183075 Validation loss 0.0636105015873909 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.4723],\n",
      "        [0.5066]], device='mps:0')\n",
      "Iteration 15640 Training loss 0.060447901487350464 Validation loss 0.06361859291791916 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0297],\n",
      "        [0.2455]], device='mps:0')\n",
      "Iteration 15650 Training loss 0.055813007056713104 Validation loss 0.06360435485839844 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.1025],\n",
      "        [0.4533]], device='mps:0')\n",
      "Iteration 15660 Training loss 0.06597502529621124 Validation loss 0.06360632926225662 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.5689],\n",
      "        [0.2650]], device='mps:0')\n",
      "Iteration 15670 Training loss 0.06248735263943672 Validation loss 0.06359540671110153 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.7716],\n",
      "        [0.7807]], device='mps:0')\n",
      "Iteration 15680 Training loss 0.06252416223287582 Validation loss 0.0637136846780777 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.0461],\n",
      "        [0.8824]], device='mps:0')\n",
      "Iteration 15690 Training loss 0.061744146049022675 Validation loss 0.06361477077007294 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.7767],\n",
      "        [0.7694]], device='mps:0')\n",
      "Iteration 15700 Training loss 0.06547585129737854 Validation loss 0.06436947733163834 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.5631],\n",
      "        [0.8806]], device='mps:0')\n",
      "Iteration 15710 Training loss 0.06395234167575836 Validation loss 0.06359648704528809 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.6647],\n",
      "        [0.9486]], device='mps:0')\n",
      "Iteration 15720 Training loss 0.060775335878133774 Validation loss 0.0636986792087555 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.3338],\n",
      "        [0.2081]], device='mps:0')\n",
      "Iteration 15730 Training loss 0.05997859686613083 Validation loss 0.06357712298631668 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.8655],\n",
      "        [0.0379]], device='mps:0')\n",
      "Iteration 15740 Training loss 0.059910278767347336 Validation loss 0.06357695907354355 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.9959],\n",
      "        [0.9011]], device='mps:0')\n",
      "Iteration 15750 Training loss 0.06204831972718239 Validation loss 0.06358271837234497 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.6167],\n",
      "        [0.7033]], device='mps:0')\n",
      "Iteration 15760 Training loss 0.0681280717253685 Validation loss 0.06358186900615692 Accuracy 0.8246250152587891\n",
      "Output tensor([[0.0361],\n",
      "        [0.6017]], device='mps:0')\n",
      "Iteration 15770 Training loss 0.06242901831865311 Validation loss 0.06379864364862442 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.2371],\n",
      "        [0.8378]], device='mps:0')\n",
      "Iteration 15780 Training loss 0.06279683858156204 Validation loss 0.06357017904520035 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.2068],\n",
      "        [0.1464]], device='mps:0')\n",
      "Iteration 15790 Training loss 0.05717021971940994 Validation loss 0.06361951678991318 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.9799],\n",
      "        [0.1769]], device='mps:0')\n",
      "Iteration 15800 Training loss 0.05874224379658699 Validation loss 0.0635610967874527 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.9491],\n",
      "        [0.9550]], device='mps:0')\n",
      "Iteration 15810 Training loss 0.06182379648089409 Validation loss 0.0635892003774643 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.8234],\n",
      "        [0.5230]], device='mps:0')\n",
      "Iteration 15820 Training loss 0.06305721402168274 Validation loss 0.06355766206979752 Accuracy 0.8247500658035278\n",
      "Output tensor([[0.7647],\n",
      "        [0.4545]], device='mps:0')\n",
      "Iteration 15830 Training loss 0.05956953018903732 Validation loss 0.06365103274583817 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.3327],\n",
      "        [0.8379]], device='mps:0')\n",
      "Iteration 15840 Training loss 0.06939931958913803 Validation loss 0.06386619061231613 Accuracy 0.8248750567436218\n",
      "Output tensor([[0.6604],\n",
      "        [0.0652]], device='mps:0')\n",
      "Iteration 15850 Training loss 0.06520988792181015 Validation loss 0.06362088024616241 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.1378],\n",
      "        [0.7291]], device='mps:0')\n",
      "Iteration 15860 Training loss 0.06597825139760971 Validation loss 0.06354333460330963 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.7455],\n",
      "        [0.9772]], device='mps:0')\n",
      "Iteration 15870 Training loss 0.06146091967821121 Validation loss 0.06354371458292007 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.1033],\n",
      "        [0.1361]], device='mps:0')\n",
      "Iteration 15880 Training loss 0.06000816076993942 Validation loss 0.06363525241613388 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.8825],\n",
      "        [0.8191]], device='mps:0')\n",
      "Iteration 15890 Training loss 0.062152326107025146 Validation loss 0.0635811910033226 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.3428],\n",
      "        [0.9726]], device='mps:0')\n",
      "Iteration 15900 Training loss 0.0670318454504013 Validation loss 0.06363697350025177 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.1059],\n",
      "        [0.9726]], device='mps:0')\n",
      "Iteration 15910 Training loss 0.06754571944475174 Validation loss 0.06353715807199478 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.0704],\n",
      "        [0.7668]], device='mps:0')\n",
      "Iteration 15920 Training loss 0.07365527004003525 Validation loss 0.06360416859388351 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.0214],\n",
      "        [0.4504]], device='mps:0')\n",
      "Iteration 15930 Training loss 0.0532856211066246 Validation loss 0.06357371062040329 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.3008],\n",
      "        [0.1022]], device='mps:0')\n",
      "Iteration 15940 Training loss 0.06354159861803055 Validation loss 0.06352147459983826 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.8512],\n",
      "        [0.5853]], device='mps:0')\n",
      "Iteration 15950 Training loss 0.06088936701416969 Validation loss 0.06360796838998795 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.7399],\n",
      "        [0.9522]], device='mps:0')\n",
      "Iteration 15960 Training loss 0.0623243972659111 Validation loss 0.06353092938661575 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.8153],\n",
      "        [0.1323]], device='mps:0')\n",
      "Iteration 15970 Training loss 0.0668802261352539 Validation loss 0.06352544575929642 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.8510],\n",
      "        [0.3494]], device='mps:0')\n",
      "Iteration 15980 Training loss 0.05928373709321022 Validation loss 0.06353957206010818 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.9963],\n",
      "        [0.0407]], device='mps:0')\n",
      "Iteration 15990 Training loss 0.061331309378147125 Validation loss 0.06351049244403839 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.9964],\n",
      "        [0.5367]], device='mps:0')\n",
      "Iteration 16000 Training loss 0.055220287293195724 Validation loss 0.06354109942913055 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.8921],\n",
      "        [0.6800]], device='mps:0')\n",
      "Iteration 16010 Training loss 0.06693588942289352 Validation loss 0.06352928280830383 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.4117],\n",
      "        [0.2477]], device='mps:0')\n",
      "Iteration 16020 Training loss 0.06527134776115417 Validation loss 0.0635249987244606 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.7870],\n",
      "        [0.0221]], device='mps:0')\n",
      "Iteration 16030 Training loss 0.06164949759840965 Validation loss 0.06359799206256866 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.8020],\n",
      "        [0.0649]], device='mps:0')\n",
      "Iteration 16040 Training loss 0.06198893114924431 Validation loss 0.06352551281452179 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.3757],\n",
      "        [0.9238]], device='mps:0')\n",
      "Iteration 16050 Training loss 0.06259551644325256 Validation loss 0.063735231757164 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.2504],\n",
      "        [0.5891]], device='mps:0')\n",
      "Iteration 16060 Training loss 0.05939474329352379 Validation loss 0.0635611042380333 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.7858],\n",
      "        [0.5830]], device='mps:0')\n",
      "Iteration 16070 Training loss 0.056152019649744034 Validation loss 0.06357493996620178 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.6466],\n",
      "        [0.1811]], device='mps:0')\n",
      "Iteration 16080 Training loss 0.06071409210562706 Validation loss 0.06348874419927597 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.6404],\n",
      "        [0.1762]], device='mps:0')\n",
      "Iteration 16090 Training loss 0.06563817709684372 Validation loss 0.0635373517870903 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.9961],\n",
      "        [0.0114]], device='mps:0')\n",
      "Iteration 16100 Training loss 0.061504725366830826 Validation loss 0.06350871920585632 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.8370],\n",
      "        [0.3864]], device='mps:0')\n",
      "Iteration 16110 Training loss 0.0642634928226471 Validation loss 0.06357114762067795 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.3816],\n",
      "        [0.8354]], device='mps:0')\n",
      "Iteration 16120 Training loss 0.0662192702293396 Validation loss 0.06355150043964386 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.5011],\n",
      "        [0.1467]], device='mps:0')\n",
      "Iteration 16130 Training loss 0.06390821188688278 Validation loss 0.06348618119955063 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.6365],\n",
      "        [0.8242]], device='mps:0')\n",
      "Iteration 16140 Training loss 0.05659950152039528 Validation loss 0.06347990781068802 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.5194],\n",
      "        [0.2076]], device='mps:0')\n",
      "Iteration 16150 Training loss 0.06388459354639053 Validation loss 0.06355071067810059 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.8646],\n",
      "        [0.9071]], device='mps:0')\n",
      "Iteration 16160 Training loss 0.06143428757786751 Validation loss 0.06348913162946701 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.7279],\n",
      "        [0.9732]], device='mps:0')\n",
      "Iteration 16170 Training loss 0.060504063963890076 Validation loss 0.06354441493749619 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.1323],\n",
      "        [0.0762]], device='mps:0')\n",
      "Iteration 16180 Training loss 0.059965379536151886 Validation loss 0.06349029392004013 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.8196],\n",
      "        [0.9667]], device='mps:0')\n",
      "Iteration 16190 Training loss 0.06898252665996552 Validation loss 0.06345753371715546 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.7729],\n",
      "        [0.8276]], device='mps:0')\n",
      "Iteration 16200 Training loss 0.06115026772022247 Validation loss 0.06365545839071274 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9657],\n",
      "        [0.0619]], device='mps:0')\n",
      "Iteration 16210 Training loss 0.05783768370747566 Validation loss 0.06344494223594666 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.0336],\n",
      "        [0.6864]], device='mps:0')\n",
      "Iteration 16220 Training loss 0.05335066840052605 Validation loss 0.06344826519489288 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.4538],\n",
      "        [0.4577]], device='mps:0')\n",
      "Iteration 16230 Training loss 0.06355112046003342 Validation loss 0.06347816437482834 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.2644],\n",
      "        [0.6156]], device='mps:0')\n",
      "Iteration 16240 Training loss 0.0700836330652237 Validation loss 0.06350427865982056 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.1461],\n",
      "        [0.7477]], device='mps:0')\n",
      "Iteration 16250 Training loss 0.06221030652523041 Validation loss 0.0637567862868309 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.0606],\n",
      "        [0.8955]], device='mps:0')\n",
      "Iteration 16260 Training loss 0.06382662802934647 Validation loss 0.06346800923347473 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.1945],\n",
      "        [0.7136]], device='mps:0')\n",
      "Iteration 16270 Training loss 0.06051713973283768 Validation loss 0.06382910162210464 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.1015],\n",
      "        [0.9838]], device='mps:0')\n",
      "Iteration 16280 Training loss 0.06945089995861053 Validation loss 0.06344833225011826 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0308],\n",
      "        [0.4042]], device='mps:0')\n",
      "Iteration 16290 Training loss 0.06706742197275162 Validation loss 0.06349867582321167 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.9776],\n",
      "        [0.9135]], device='mps:0')\n",
      "Iteration 16300 Training loss 0.06860744953155518 Validation loss 0.06342407315969467 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.9521],\n",
      "        [0.9023]], device='mps:0')\n",
      "Iteration 16310 Training loss 0.06311678141355515 Validation loss 0.06344335526227951 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.7141],\n",
      "        [0.0203]], device='mps:0')\n",
      "Iteration 16320 Training loss 0.056278884410858154 Validation loss 0.06357229501008987 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.2365],\n",
      "        [0.0432]], device='mps:0')\n",
      "Iteration 16330 Training loss 0.06288221478462219 Validation loss 0.06341579556465149 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.1956],\n",
      "        [0.9726]], device='mps:0')\n",
      "Iteration 16340 Training loss 0.06373152881860733 Validation loss 0.063447505235672 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.0409],\n",
      "        [0.9949]], device='mps:0')\n",
      "Iteration 16350 Training loss 0.06261800974607468 Validation loss 0.06362780928611755 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.8535],\n",
      "        [0.1345]], device='mps:0')\n",
      "Iteration 16360 Training loss 0.06078338623046875 Validation loss 0.0634138435125351 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.8622],\n",
      "        [0.0379]], device='mps:0')\n",
      "Iteration 16370 Training loss 0.06001097708940506 Validation loss 0.06343722343444824 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.8457],\n",
      "        [0.8619]], device='mps:0')\n",
      "Iteration 16380 Training loss 0.07134418934583664 Validation loss 0.06342394649982452 Accuracy 0.8250000476837158\n",
      "Output tensor([[0.2309],\n",
      "        [0.9131]], device='mps:0')\n",
      "Iteration 16390 Training loss 0.05871818587183952 Validation loss 0.06343427300453186 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.9371],\n",
      "        [0.9905]], device='mps:0')\n",
      "Iteration 16400 Training loss 0.0667295753955841 Validation loss 0.06342253088951111 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.3800],\n",
      "        [0.7769]], device='mps:0')\n",
      "Iteration 16410 Training loss 0.06239546462893486 Validation loss 0.06340043991804123 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.8986],\n",
      "        [0.3430]], device='mps:0')\n",
      "Iteration 16420 Training loss 0.06326213479042053 Validation loss 0.06341341137886047 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9643],\n",
      "        [0.8082]], device='mps:0')\n",
      "Iteration 16430 Training loss 0.07345203310251236 Validation loss 0.06339789181947708 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.8751],\n",
      "        [0.6977]], device='mps:0')\n",
      "Iteration 16440 Training loss 0.055223431438207626 Validation loss 0.06344235688447952 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.2614],\n",
      "        [0.5972]], device='mps:0')\n",
      "Iteration 16450 Training loss 0.06587271392345428 Validation loss 0.0635770931839943 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.0409],\n",
      "        [0.7364]], device='mps:0')\n",
      "Iteration 16460 Training loss 0.0684155747294426 Validation loss 0.06360804289579391 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.5469],\n",
      "        [0.8985]], device='mps:0')\n",
      "Iteration 16470 Training loss 0.06250674277544022 Validation loss 0.0635642260313034 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.7756],\n",
      "        [0.0109]], device='mps:0')\n",
      "Iteration 16480 Training loss 0.06876372545957565 Validation loss 0.06343677639961243 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.5808],\n",
      "        [0.8528]], device='mps:0')\n",
      "Iteration 16490 Training loss 0.05468699708580971 Validation loss 0.0633942112326622 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.0561],\n",
      "        [0.9844]], device='mps:0')\n",
      "Iteration 16500 Training loss 0.06043582782149315 Validation loss 0.06338296085596085 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.9771],\n",
      "        [0.0127]], device='mps:0')\n",
      "Iteration 16510 Training loss 0.06420831382274628 Validation loss 0.06357398629188538 Accuracy 0.8256250619888306\n",
      "Output tensor([[0.1366],\n",
      "        [0.8283]], device='mps:0')\n",
      "Iteration 16520 Training loss 0.0575798861682415 Validation loss 0.06337997317314148 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.7007],\n",
      "        [0.9157]], device='mps:0')\n",
      "Iteration 16530 Training loss 0.0558573417365551 Validation loss 0.06371012330055237 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.3968],\n",
      "        [0.0612]], device='mps:0')\n",
      "Iteration 16540 Training loss 0.06437426805496216 Validation loss 0.06353672593832016 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.9412],\n",
      "        [0.8690]], device='mps:0')\n",
      "Iteration 16550 Training loss 0.06578564643859863 Validation loss 0.06346341967582703 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.1221],\n",
      "        [0.0929]], device='mps:0')\n",
      "Iteration 16560 Training loss 0.055044468492269516 Validation loss 0.06344538927078247 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.8846],\n",
      "        [0.0449]], device='mps:0')\n",
      "Iteration 16570 Training loss 0.06414651870727539 Validation loss 0.063545361161232 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.1925],\n",
      "        [0.6870]], device='mps:0')\n",
      "Iteration 16580 Training loss 0.06925696134567261 Validation loss 0.06349090486764908 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.9292],\n",
      "        [0.8807]], device='mps:0')\n",
      "Iteration 16590 Training loss 0.06432437151670456 Validation loss 0.06337504088878632 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.5142],\n",
      "        [0.1107]], device='mps:0')\n",
      "Iteration 16600 Training loss 0.05419820174574852 Validation loss 0.06337077170610428 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.9522],\n",
      "        [0.1713]], device='mps:0')\n",
      "Iteration 16610 Training loss 0.058852385729551315 Validation loss 0.06336178630590439 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.7276],\n",
      "        [0.5702]], device='mps:0')\n",
      "Iteration 16620 Training loss 0.06460926681756973 Validation loss 0.06367605179548264 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.0544],\n",
      "        [0.2202]], device='mps:0')\n",
      "Iteration 16630 Training loss 0.06284038722515106 Validation loss 0.06338127702474594 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.3910],\n",
      "        [0.0319]], device='mps:0')\n",
      "Iteration 16640 Training loss 0.06878074258565903 Validation loss 0.06345551460981369 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.8647],\n",
      "        [0.7984]], device='mps:0')\n",
      "Iteration 16650 Training loss 0.0616467259824276 Validation loss 0.06337154656648636 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.5173],\n",
      "        [0.9426]], device='mps:0')\n",
      "Iteration 16660 Training loss 0.055937573313713074 Validation loss 0.06335615366697311 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.3666],\n",
      "        [0.2748]], device='mps:0')\n",
      "Iteration 16670 Training loss 0.06032640486955643 Validation loss 0.06336825340986252 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.3867],\n",
      "        [0.1197]], device='mps:0')\n",
      "Iteration 16680 Training loss 0.05767955631017685 Validation loss 0.06335607916116714 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.0770],\n",
      "        [0.1569]], device='mps:0')\n",
      "Iteration 16690 Training loss 0.05552198365330696 Validation loss 0.06333549320697784 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.0140],\n",
      "        [0.9074]], device='mps:0')\n",
      "Iteration 16700 Training loss 0.06734871119260788 Validation loss 0.06336119025945663 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.9884],\n",
      "        [0.8359]], device='mps:0')\n",
      "Iteration 16710 Training loss 0.061110541224479675 Validation loss 0.06339873373508453 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.7920],\n",
      "        [0.9441]], device='mps:0')\n",
      "Iteration 16720 Training loss 0.06587866693735123 Validation loss 0.06332895904779434 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.2606],\n",
      "        [0.9779]], device='mps:0')\n",
      "Iteration 16730 Training loss 0.0651976689696312 Validation loss 0.06332539021968842 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.2604],\n",
      "        [0.7615]], device='mps:0')\n",
      "Iteration 16740 Training loss 0.05195329710841179 Validation loss 0.063373863697052 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.2954],\n",
      "        [0.5588]], device='mps:0')\n",
      "Iteration 16750 Training loss 0.06534753739833832 Validation loss 0.06332261115312576 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.0792],\n",
      "        [0.0759]], device='mps:0')\n",
      "Iteration 16760 Training loss 0.0626942440867424 Validation loss 0.06334835290908813 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.0154],\n",
      "        [0.9021]], device='mps:0')\n",
      "Iteration 16770 Training loss 0.04772062972187996 Validation loss 0.06333108991384506 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9856],\n",
      "        [0.7742]], device='mps:0')\n",
      "Iteration 16780 Training loss 0.06460078805685043 Validation loss 0.06333179026842117 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.4686],\n",
      "        [0.0826]], device='mps:0')\n",
      "Iteration 16790 Training loss 0.0553719699382782 Validation loss 0.06331536918878555 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.4769],\n",
      "        [0.1734]], device='mps:0')\n",
      "Iteration 16800 Training loss 0.06463921070098877 Validation loss 0.06334304064512253 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.9367],\n",
      "        [0.0285]], device='mps:0')\n",
      "Iteration 16810 Training loss 0.06005973368883133 Validation loss 0.06338055431842804 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.2380],\n",
      "        [0.3051]], device='mps:0')\n",
      "Iteration 16820 Training loss 0.06195765361189842 Validation loss 0.06331387907266617 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.6880],\n",
      "        [0.9905]], device='mps:0')\n",
      "Iteration 16830 Training loss 0.06592622399330139 Validation loss 0.06336802244186401 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.6479],\n",
      "        [0.9932]], device='mps:0')\n",
      "Iteration 16840 Training loss 0.060042914003133774 Validation loss 0.06331627815961838 Accuracy 0.8252500295639038\n",
      "Output tensor([[0.0971],\n",
      "        [0.2619]], device='mps:0')\n",
      "Iteration 16850 Training loss 0.062174152582883835 Validation loss 0.0632975846529007 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.1648],\n",
      "        [0.4341]], device='mps:0')\n",
      "Iteration 16860 Training loss 0.0708414614200592 Validation loss 0.06332237273454666 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.1739],\n",
      "        [0.1174]], device='mps:0')\n",
      "Iteration 16870 Training loss 0.06157829985022545 Validation loss 0.06340466439723969 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.4061],\n",
      "        [0.8569]], device='mps:0')\n",
      "Iteration 16880 Training loss 0.06949404627084732 Validation loss 0.06358020007610321 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.3740],\n",
      "        [0.0421]], device='mps:0')\n",
      "Iteration 16890 Training loss 0.06541506946086884 Validation loss 0.06333471834659576 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.4900],\n",
      "        [0.6267]], device='mps:0')\n",
      "Iteration 16900 Training loss 0.06263703107833862 Validation loss 0.06334813684225082 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.0378],\n",
      "        [0.7056]], device='mps:0')\n",
      "Iteration 16910 Training loss 0.05937843769788742 Validation loss 0.06330377608537674 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.0484],\n",
      "        [0.6063]], device='mps:0')\n",
      "Iteration 16920 Training loss 0.05309829115867615 Validation loss 0.06331159919500351 Accuracy 0.8253750205039978\n",
      "Output tensor([[0.4754],\n",
      "        [0.0753]], device='mps:0')\n",
      "Iteration 16930 Training loss 0.07540545612573624 Validation loss 0.06330206245183945 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.8801],\n",
      "        [0.0106]], device='mps:0')\n",
      "Iteration 16940 Training loss 0.06605604290962219 Validation loss 0.06333349645137787 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.4733],\n",
      "        [0.1523]], device='mps:0')\n",
      "Iteration 16950 Training loss 0.06372711807489395 Validation loss 0.06330720335245132 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.9600],\n",
      "        [0.9355]], device='mps:0')\n",
      "Iteration 16960 Training loss 0.06109226867556572 Validation loss 0.06329023838043213 Accuracy 0.827250063419342\n",
      "Output tensor([[0.4425],\n",
      "        [0.9460]], device='mps:0')\n",
      "Iteration 16970 Training loss 0.05962567776441574 Validation loss 0.06332507729530334 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.9468],\n",
      "        [0.6504]], device='mps:0')\n",
      "Iteration 16980 Training loss 0.07063081860542297 Validation loss 0.06335338205099106 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.9296],\n",
      "        [0.9790]], device='mps:0')\n",
      "Iteration 16990 Training loss 0.05883479863405228 Validation loss 0.06356002390384674 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.6644],\n",
      "        [0.9898]], device='mps:0')\n",
      "Iteration 17000 Training loss 0.06192486733198166 Validation loss 0.06332249194383621 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.1354],\n",
      "        [0.2719]], device='mps:0')\n",
      "Iteration 17010 Training loss 0.06755886226892471 Validation loss 0.0634724497795105 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.0445],\n",
      "        [0.0849]], device='mps:0')\n",
      "Iteration 17020 Training loss 0.05592796579003334 Validation loss 0.06332403421401978 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.4508],\n",
      "        [0.9295]], device='mps:0')\n",
      "Iteration 17030 Training loss 0.05131835490465164 Validation loss 0.06328335404396057 Accuracy 0.8251250386238098\n",
      "Output tensor([[0.2908],\n",
      "        [0.1986]], device='mps:0')\n",
      "Iteration 17040 Training loss 0.05681759864091873 Validation loss 0.06324969977140427 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.2900],\n",
      "        [0.6882]], device='mps:0')\n",
      "Iteration 17050 Training loss 0.06050880253314972 Validation loss 0.06364190578460693 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.7115],\n",
      "        [0.1840]], device='mps:0')\n",
      "Iteration 17060 Training loss 0.062380556017160416 Validation loss 0.06324580311775208 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.1660],\n",
      "        [0.8007]], device='mps:0')\n",
      "Iteration 17070 Training loss 0.062042236328125 Validation loss 0.06333934515714645 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.9516],\n",
      "        [0.2427]], device='mps:0')\n",
      "Iteration 17080 Training loss 0.07556220144033432 Validation loss 0.0632370263338089 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.0636],\n",
      "        [0.5835]], device='mps:0')\n",
      "Iteration 17090 Training loss 0.06736552715301514 Validation loss 0.06323797255754471 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.8530],\n",
      "        [0.0603]], device='mps:0')\n",
      "Iteration 17100 Training loss 0.059313610196113586 Validation loss 0.06323163211345673 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.0117],\n",
      "        [0.8911]], device='mps:0')\n",
      "Iteration 17110 Training loss 0.05884835124015808 Validation loss 0.06326813995838165 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.0724],\n",
      "        [0.9521]], device='mps:0')\n",
      "Iteration 17120 Training loss 0.06930910050868988 Validation loss 0.06327493488788605 Accuracy 0.8258750438690186\n",
      "Output tensor([[0.9882],\n",
      "        [0.9388]], device='mps:0')\n",
      "Iteration 17130 Training loss 0.06340115517377853 Validation loss 0.0633634626865387 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.1648],\n",
      "        [0.6076]], device='mps:0')\n",
      "Iteration 17140 Training loss 0.0665288046002388 Validation loss 0.06325355172157288 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.2204],\n",
      "        [0.8222]], device='mps:0')\n",
      "Iteration 17150 Training loss 0.05684084817767143 Validation loss 0.06322813034057617 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.5956],\n",
      "        [0.2640]], device='mps:0')\n",
      "Iteration 17160 Training loss 0.056442465633153915 Validation loss 0.06326615810394287 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.8647],\n",
      "        [0.3315]], device='mps:0')\n",
      "Iteration 17170 Training loss 0.06889566034078598 Validation loss 0.0632471814751625 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.9905],\n",
      "        [0.0149]], device='mps:0')\n",
      "Iteration 17180 Training loss 0.055531810969114304 Validation loss 0.0632266104221344 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.9618],\n",
      "        [0.2822]], device='mps:0')\n",
      "Iteration 17190 Training loss 0.0589844174683094 Validation loss 0.0632258877158165 Accuracy 0.8255000114440918\n",
      "Output tensor([[0.2556],\n",
      "        [0.8788]], device='mps:0')\n",
      "Iteration 17200 Training loss 0.04886753857135773 Validation loss 0.06321912258863449 Accuracy 0.8257500529289246\n",
      "Output tensor([[0.2424],\n",
      "        [0.3971]], device='mps:0')\n",
      "Iteration 17210 Training loss 0.05583290383219719 Validation loss 0.06336463242769241 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.9692],\n",
      "        [0.1877]], device='mps:0')\n",
      "Iteration 17220 Training loss 0.05641786381602287 Validation loss 0.0632522776722908 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.3741],\n",
      "        [0.9782]], device='mps:0')\n",
      "Iteration 17230 Training loss 0.06958060711622238 Validation loss 0.06321649253368378 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.1209],\n",
      "        [0.9660]], device='mps:0')\n",
      "Iteration 17240 Training loss 0.060409463942050934 Validation loss 0.06320285052061081 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.0122],\n",
      "        [0.0910]], device='mps:0')\n",
      "Iteration 17250 Training loss 0.061293408274650574 Validation loss 0.06324291974306107 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.6140],\n",
      "        [0.9840]], device='mps:0')\n",
      "Iteration 17260 Training loss 0.06950556486845016 Validation loss 0.06339273601770401 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.0351],\n",
      "        [0.1965]], device='mps:0')\n",
      "Iteration 17270 Training loss 0.06041501462459564 Validation loss 0.06320834904909134 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9267],\n",
      "        [0.9944]], device='mps:0')\n",
      "Iteration 17280 Training loss 0.0640704333782196 Validation loss 0.06320132315158844 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.9300],\n",
      "        [0.4617]], device='mps:0')\n",
      "Iteration 17290 Training loss 0.058427128940820694 Validation loss 0.06329818814992905 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.4269],\n",
      "        [0.4466]], device='mps:0')\n",
      "Iteration 17300 Training loss 0.0633464828133583 Validation loss 0.06321413069963455 Accuracy 0.827375054359436\n",
      "Output tensor([[0.9735],\n",
      "        [0.9880]], device='mps:0')\n",
      "Iteration 17310 Training loss 0.06664229184389114 Validation loss 0.06329725682735443 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.7614],\n",
      "        [0.9230]], device='mps:0')\n",
      "Iteration 17320 Training loss 0.05978457257151604 Validation loss 0.06320386379957199 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.9855],\n",
      "        [0.0180]], device='mps:0')\n",
      "Iteration 17330 Training loss 0.05357219651341438 Validation loss 0.06324341148138046 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.5625],\n",
      "        [0.5759]], device='mps:0')\n",
      "Iteration 17340 Training loss 0.06275343894958496 Validation loss 0.0631893053650856 Accuracy 0.827375054359436\n",
      "Output tensor([[0.7050],\n",
      "        [0.4754]], device='mps:0')\n",
      "Iteration 17350 Training loss 0.05531702935695648 Validation loss 0.06327100843191147 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.0963],\n",
      "        [0.0476]], device='mps:0')\n",
      "Iteration 17360 Training loss 0.06791456043720245 Validation loss 0.06324296444654465 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.0848],\n",
      "        [0.2361]], device='mps:0')\n",
      "Iteration 17370 Training loss 0.06071935594081879 Validation loss 0.06318145990371704 Accuracy 0.827375054359436\n",
      "Output tensor([[0.7614],\n",
      "        [0.8194]], device='mps:0')\n",
      "Iteration 17380 Training loss 0.06429383903741837 Validation loss 0.06318915635347366 Accuracy 0.827375054359436\n",
      "Output tensor([[0.6022],\n",
      "        [0.9229]], device='mps:0')\n",
      "Iteration 17390 Training loss 0.05806075409054756 Validation loss 0.06317561119794846 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.6700],\n",
      "        [0.9679]], device='mps:0')\n",
      "Iteration 17400 Training loss 0.06488198786973953 Validation loss 0.06321275979280472 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.2861],\n",
      "        [0.7196]], device='mps:0')\n",
      "Iteration 17410 Training loss 0.05846570059657097 Validation loss 0.06330734491348267 Accuracy 0.827250063419342\n",
      "Output tensor([[0.9125],\n",
      "        [0.8847]], device='mps:0')\n",
      "Iteration 17420 Training loss 0.06244591251015663 Validation loss 0.06319597363471985 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.5765],\n",
      "        [0.1072]], device='mps:0')\n",
      "Iteration 17430 Training loss 0.053708791732788086 Validation loss 0.06342220306396484 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.1386],\n",
      "        [0.6084]], device='mps:0')\n",
      "Iteration 17440 Training loss 0.054641589522361755 Validation loss 0.06321211904287338 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.3260],\n",
      "        [0.7037]], device='mps:0')\n",
      "Iteration 17450 Training loss 0.06657370179891586 Validation loss 0.06316248327493668 Accuracy 0.827625036239624\n",
      "Output tensor([[0.9563],\n",
      "        [0.4791]], device='mps:0')\n",
      "Iteration 17460 Training loss 0.060365013778209686 Validation loss 0.06313861906528473 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.7180],\n",
      "        [0.9863]], device='mps:0')\n",
      "Iteration 17470 Training loss 0.056046098470687866 Validation loss 0.06314706057310104 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.7380],\n",
      "        [0.0936]], device='mps:0')\n",
      "Iteration 17480 Training loss 0.055869925767183304 Validation loss 0.06333275884389877 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.1296],\n",
      "        [0.2250]], device='mps:0')\n",
      "Iteration 17490 Training loss 0.06616430729627609 Validation loss 0.06324992328882217 Accuracy 0.827375054359436\n",
      "Output tensor([[0.0917],\n",
      "        [0.7172]], device='mps:0')\n",
      "Iteration 17500 Training loss 0.0569617934525013 Validation loss 0.06313186883926392 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.7919],\n",
      "        [0.7787]], device='mps:0')\n",
      "Iteration 17510 Training loss 0.05648021027445793 Validation loss 0.06316720694303513 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.6141],\n",
      "        [0.4262]], device='mps:0')\n",
      "Iteration 17520 Training loss 0.06337942183017731 Validation loss 0.06316576153039932 Accuracy 0.82750004529953\n",
      "Output tensor([[0.6151],\n",
      "        [0.9672]], device='mps:0')\n",
      "Iteration 17530 Training loss 0.0585695244371891 Validation loss 0.06351207196712494 Accuracy 0.82750004529953\n",
      "Output tensor([[0.5460],\n",
      "        [0.5071]], device='mps:0')\n",
      "Iteration 17540 Training loss 0.0561533160507679 Validation loss 0.06316178292036057 Accuracy 0.82750004529953\n",
      "Output tensor([[0.7315],\n",
      "        [0.6861]], device='mps:0')\n",
      "Iteration 17550 Training loss 0.06042031571269035 Validation loss 0.06354979425668716 Accuracy 0.82750004529953\n",
      "Output tensor([[0.1966],\n",
      "        [0.0447]], device='mps:0')\n",
      "Iteration 17560 Training loss 0.061506908386945724 Validation loss 0.06319767236709595 Accuracy 0.827375054359436\n",
      "Output tensor([[0.7450],\n",
      "        [0.4687]], device='mps:0')\n",
      "Iteration 17570 Training loss 0.06247411668300629 Validation loss 0.06320557743310928 Accuracy 0.827250063419342\n",
      "Output tensor([[0.4903],\n",
      "        [0.4001]], device='mps:0')\n",
      "Iteration 17580 Training loss 0.06609754264354706 Validation loss 0.06335393339395523 Accuracy 0.827250063419342\n",
      "Output tensor([[0.9203],\n",
      "        [0.9213]], device='mps:0')\n",
      "Iteration 17590 Training loss 0.05580684915184975 Validation loss 0.06311580538749695 Accuracy 0.827250063419342\n",
      "Output tensor([[0.2193],\n",
      "        [0.2246]], device='mps:0')\n",
      "Iteration 17600 Training loss 0.0642506405711174 Validation loss 0.06320463865995407 Accuracy 0.827250063419342\n",
      "Output tensor([[0.9398],\n",
      "        [0.0950]], device='mps:0')\n",
      "Iteration 17610 Training loss 0.06648775935173035 Validation loss 0.06320785731077194 Accuracy 0.827250063419342\n",
      "Output tensor([[0.8147],\n",
      "        [0.9110]], device='mps:0')\n",
      "Iteration 17620 Training loss 0.06454694271087646 Validation loss 0.06320962309837341 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.9836],\n",
      "        [0.2907]], device='mps:0')\n",
      "Iteration 17630 Training loss 0.055378761142492294 Validation loss 0.06319743394851685 Accuracy 0.827375054359436\n",
      "Output tensor([[0.8666],\n",
      "        [0.9716]], device='mps:0')\n",
      "Iteration 17640 Training loss 0.05739209055900574 Validation loss 0.06311113387346268 Accuracy 0.827250063419342\n",
      "Output tensor([[0.1651],\n",
      "        [0.9369]], device='mps:0')\n",
      "Iteration 17650 Training loss 0.05657805874943733 Validation loss 0.0631507933139801 Accuracy 0.827250063419342\n",
      "Output tensor([[0.1365],\n",
      "        [0.4516]], device='mps:0')\n",
      "Iteration 17660 Training loss 0.0518103651702404 Validation loss 0.06318356096744537 Accuracy 0.82750004529953\n",
      "Output tensor([[0.0173],\n",
      "        [0.1928]], device='mps:0')\n",
      "Iteration 17670 Training loss 0.05929099768400192 Validation loss 0.06334096193313599 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.9023],\n",
      "        [0.9758]], device='mps:0')\n",
      "Iteration 17680 Training loss 0.0611821785569191 Validation loss 0.06312974542379379 Accuracy 0.827875018119812\n",
      "Output tensor([[0.2973],\n",
      "        [0.9743]], device='mps:0')\n",
      "Iteration 17690 Training loss 0.059135496616363525 Validation loss 0.06308979541063309 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.3323],\n",
      "        [0.0241]], device='mps:0')\n",
      "Iteration 17700 Training loss 0.07740477472543716 Validation loss 0.06310896575450897 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.2373],\n",
      "        [0.2055]], device='mps:0')\n",
      "Iteration 17710 Training loss 0.0684392899274826 Validation loss 0.06308238953351974 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.9692],\n",
      "        [0.1472]], device='mps:0')\n",
      "Iteration 17720 Training loss 0.058125562965869904 Validation loss 0.06307973712682724 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.2237],\n",
      "        [0.7063]], device='mps:0')\n",
      "Iteration 17730 Training loss 0.060347460210323334 Validation loss 0.0633474588394165 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.3015],\n",
      "        [0.4382]], device='mps:0')\n",
      "Iteration 17740 Training loss 0.07304225862026215 Validation loss 0.06308313459157944 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.4415],\n",
      "        [0.2567]], device='mps:0')\n",
      "Iteration 17750 Training loss 0.06570620089769363 Validation loss 0.06325379759073257 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.7862],\n",
      "        [0.8019]], device='mps:0')\n",
      "Iteration 17760 Training loss 0.0604751855134964 Validation loss 0.0631154328584671 Accuracy 0.827375054359436\n",
      "Output tensor([[0.2972],\n",
      "        [0.2628]], device='mps:0')\n",
      "Iteration 17770 Training loss 0.062083691358566284 Validation loss 0.06329493224620819 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.0871],\n",
      "        [0.1228]], device='mps:0')\n",
      "Iteration 17780 Training loss 0.06821195036172867 Validation loss 0.06308713555335999 Accuracy 0.827375054359436\n",
      "Output tensor([[0.0897],\n",
      "        [0.9889]], device='mps:0')\n",
      "Iteration 17790 Training loss 0.06450606882572174 Validation loss 0.06310345977544785 Accuracy 0.827625036239624\n",
      "Output tensor([[0.4585],\n",
      "        [0.7010]], device='mps:0')\n",
      "Iteration 17800 Training loss 0.06409205496311188 Validation loss 0.0631718561053276 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.2812],\n",
      "        [0.9285]], device='mps:0')\n",
      "Iteration 17810 Training loss 0.06054631620645523 Validation loss 0.06307698041200638 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.9943],\n",
      "        [0.0818]], device='mps:0')\n",
      "Iteration 17820 Training loss 0.06626953184604645 Validation loss 0.063087098300457 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.0664],\n",
      "        [0.7334]], device='mps:0')\n",
      "Iteration 17830 Training loss 0.06401059776544571 Validation loss 0.06307931989431381 Accuracy 0.827375054359436\n",
      "Output tensor([[0.6204],\n",
      "        [0.3972]], device='mps:0')\n",
      "Iteration 17840 Training loss 0.05588354542851448 Validation loss 0.06308367848396301 Accuracy 0.827750027179718\n",
      "Output tensor([[0.3082],\n",
      "        [0.9887]], device='mps:0')\n",
      "Iteration 17850 Training loss 0.06920599192380905 Validation loss 0.06306134164333344 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.7962],\n",
      "        [0.5829]], device='mps:0')\n",
      "Iteration 17860 Training loss 0.06417601555585861 Validation loss 0.06314502656459808 Accuracy 0.827250063419342\n",
      "Output tensor([[0.8601],\n",
      "        [0.3002]], device='mps:0')\n",
      "Iteration 17870 Training loss 0.056097954511642456 Validation loss 0.06333072483539581 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.0283],\n",
      "        [0.9467]], device='mps:0')\n",
      "Iteration 17880 Training loss 0.0563722588121891 Validation loss 0.0630795955657959 Accuracy 0.82750004529953\n",
      "Output tensor([[0.9458],\n",
      "        [0.8576]], device='mps:0')\n",
      "Iteration 17890 Training loss 0.06195748224854469 Validation loss 0.06308205425739288 Accuracy 0.8260000348091125\n",
      "Output tensor([[0.9722],\n",
      "        [0.3300]], device='mps:0')\n",
      "Iteration 17900 Training loss 0.057447317987680435 Validation loss 0.06327793747186661 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.8011],\n",
      "        [0.8802]], device='mps:0')\n",
      "Iteration 17910 Training loss 0.06938259303569794 Validation loss 0.06307384371757507 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.3017],\n",
      "        [0.5395]], device='mps:0')\n",
      "Iteration 17920 Training loss 0.06294240802526474 Validation loss 0.06327797472476959 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.3390],\n",
      "        [0.6971]], device='mps:0')\n",
      "Iteration 17930 Training loss 0.06803515553474426 Validation loss 0.06309458613395691 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.5677],\n",
      "        [0.8266]], device='mps:0')\n",
      "Iteration 17940 Training loss 0.06701909750699997 Validation loss 0.06306423991918564 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.1087],\n",
      "        [0.0853]], device='mps:0')\n",
      "Iteration 17950 Training loss 0.06695614010095596 Validation loss 0.06343412399291992 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.7159],\n",
      "        [0.1693]], device='mps:0')\n",
      "Iteration 17960 Training loss 0.06051798537373543 Validation loss 0.06320679187774658 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.0434],\n",
      "        [0.4384]], device='mps:0')\n",
      "Iteration 17970 Training loss 0.058240074664354324 Validation loss 0.06311196088790894 Accuracy 0.827750027179718\n",
      "Output tensor([[0.0443],\n",
      "        [0.2485]], device='mps:0')\n",
      "Iteration 17980 Training loss 0.05454450473189354 Validation loss 0.06312799453735352 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.8298],\n",
      "        [0.7310]], device='mps:0')\n",
      "Iteration 17990 Training loss 0.05748046562075615 Validation loss 0.06305158138275146 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.1529],\n",
      "        [0.5532]], device='mps:0')\n",
      "Iteration 18000 Training loss 0.05831395834684372 Validation loss 0.06306562572717667 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.9971],\n",
      "        [0.7107]], device='mps:0')\n",
      "Iteration 18010 Training loss 0.055763911455869675 Validation loss 0.06305690854787827 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.8738],\n",
      "        [0.0176]], device='mps:0')\n",
      "Iteration 18020 Training loss 0.06176185607910156 Validation loss 0.06318957358598709 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.8938],\n",
      "        [0.7396]], device='mps:0')\n",
      "Iteration 18030 Training loss 0.06950480490922928 Validation loss 0.06302522122859955 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.1253],\n",
      "        [0.9906]], device='mps:0')\n",
      "Iteration 18040 Training loss 0.06522561609745026 Validation loss 0.06308192014694214 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.3759],\n",
      "        [0.9897]], device='mps:0')\n",
      "Iteration 18050 Training loss 0.05596074089407921 Validation loss 0.06302553415298462 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.2863],\n",
      "        [0.9391]], device='mps:0')\n",
      "Iteration 18060 Training loss 0.0659874901175499 Validation loss 0.06304437667131424 Accuracy 0.8261250257492065\n",
      "Output tensor([[0.9299],\n",
      "        [0.9217]], device='mps:0')\n",
      "Iteration 18070 Training loss 0.06514239311218262 Validation loss 0.06310049444437027 Accuracy 0.827875018119812\n",
      "Output tensor([[0.9184],\n",
      "        [0.1350]], device='mps:0')\n",
      "Iteration 18080 Training loss 0.06178715452551842 Validation loss 0.063015878200531 Accuracy 0.827375054359436\n",
      "Output tensor([[0.8996],\n",
      "        [0.0623]], device='mps:0')\n",
      "Iteration 18090 Training loss 0.06444745510816574 Validation loss 0.06315016001462936 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.6689],\n",
      "        [0.1030]], device='mps:0')\n",
      "Iteration 18100 Training loss 0.062121905386447906 Validation loss 0.06307265162467957 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.0139],\n",
      "        [0.4742]], device='mps:0')\n",
      "Iteration 18110 Training loss 0.05735425651073456 Validation loss 0.06300123780965805 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.0584],\n",
      "        [0.5614]], device='mps:0')\n",
      "Iteration 18120 Training loss 0.06208692491054535 Validation loss 0.06306959688663483 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.8500],\n",
      "        [0.6686]], device='mps:0')\n",
      "Iteration 18130 Training loss 0.06497929245233536 Validation loss 0.06300743669271469 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.9315],\n",
      "        [0.2315]], device='mps:0')\n",
      "Iteration 18140 Training loss 0.05174150690436363 Validation loss 0.06307579576969147 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.8018],\n",
      "        [0.9351]], device='mps:0')\n",
      "Iteration 18150 Training loss 0.06611579656600952 Validation loss 0.06306011974811554 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.0419],\n",
      "        [0.8203]], device='mps:0')\n",
      "Iteration 18160 Training loss 0.061405401676893234 Validation loss 0.06321591883897781 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.9350],\n",
      "        [0.6373]], device='mps:0')\n",
      "Iteration 18170 Training loss 0.05474992096424103 Validation loss 0.06302473694086075 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.9560],\n",
      "        [0.0793]], device='mps:0')\n",
      "Iteration 18180 Training loss 0.05563409626483917 Validation loss 0.0629759281873703 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.1819],\n",
      "        [0.8274]], device='mps:0')\n",
      "Iteration 18190 Training loss 0.05990827456116676 Validation loss 0.06306964159011841 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.0254],\n",
      "        [0.3092]], device='mps:0')\n",
      "Iteration 18200 Training loss 0.05898382142186165 Validation loss 0.06300123780965805 Accuracy 0.827750027179718\n",
      "Output tensor([[0.8498],\n",
      "        [0.9792]], device='mps:0')\n",
      "Iteration 18210 Training loss 0.0624229796230793 Validation loss 0.06303814798593521 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.1813],\n",
      "        [0.0048]], device='mps:0')\n",
      "Iteration 18220 Training loss 0.0505191795527935 Validation loss 0.06305276602506638 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.2344],\n",
      "        [0.9877]], device='mps:0')\n",
      "Iteration 18230 Training loss 0.059116560965776443 Validation loss 0.0630163699388504 Accuracy 0.827875018119812\n",
      "Output tensor([[0.7504],\n",
      "        [0.6727]], device='mps:0')\n",
      "Iteration 18240 Training loss 0.060948695987463 Validation loss 0.06310795992612839 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.3832],\n",
      "        [0.1372]], device='mps:0')\n",
      "Iteration 18250 Training loss 0.05880734324455261 Validation loss 0.06296095252037048 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.1183],\n",
      "        [0.0386]], device='mps:0')\n",
      "Iteration 18260 Training loss 0.05752129480242729 Validation loss 0.0629567950963974 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.0657],\n",
      "        [0.9625]], device='mps:0')\n",
      "Iteration 18270 Training loss 0.06593547761440277 Validation loss 0.06305131316184998 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.0784],\n",
      "        [0.3283]], device='mps:0')\n",
      "Iteration 18280 Training loss 0.060954269021749496 Validation loss 0.0629684329032898 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.2445],\n",
      "        [0.8514]], device='mps:0')\n",
      "Iteration 18290 Training loss 0.0638047456741333 Validation loss 0.0630255863070488 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.6540],\n",
      "        [0.0339]], device='mps:0')\n",
      "Iteration 18300 Training loss 0.05618390813469887 Validation loss 0.06296249479055405 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.9286],\n",
      "        [0.9526]], device='mps:0')\n",
      "Iteration 18310 Training loss 0.06022264435887337 Validation loss 0.0630640909075737 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.3249],\n",
      "        [0.3092]], device='mps:0')\n",
      "Iteration 18320 Training loss 0.06005549430847168 Validation loss 0.06293383240699768 Accuracy 0.82750004529953\n",
      "Output tensor([[0.6045],\n",
      "        [0.1126]], device='mps:0')\n",
      "Iteration 18330 Training loss 0.06209530308842659 Validation loss 0.06295114755630493 Accuracy 0.827250063419342\n",
      "Output tensor([[0.1032],\n",
      "        [0.3713]], device='mps:0')\n",
      "Iteration 18340 Training loss 0.06108575686812401 Validation loss 0.06294283270835876 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.6356],\n",
      "        [0.9930]], device='mps:0')\n",
      "Iteration 18350 Training loss 0.055983807891607285 Validation loss 0.06299018114805222 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.0454],\n",
      "        [0.9418]], device='mps:0')\n",
      "Iteration 18360 Training loss 0.0635261982679367 Validation loss 0.06292495131492615 Accuracy 0.827625036239624\n",
      "Output tensor([[0.2190],\n",
      "        [0.9767]], device='mps:0')\n",
      "Iteration 18370 Training loss 0.05632307380437851 Validation loss 0.06291988492012024 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.9792],\n",
      "        [0.0652]], device='mps:0')\n",
      "Iteration 18380 Training loss 0.0642961859703064 Validation loss 0.06292510032653809 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.1862],\n",
      "        [0.9571]], device='mps:0')\n",
      "Iteration 18390 Training loss 0.05864113196730614 Validation loss 0.06296837329864502 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.1449],\n",
      "        [0.4695]], device='mps:0')\n",
      "Iteration 18400 Training loss 0.06045127660036087 Validation loss 0.06291947513818741 Accuracy 0.827625036239624\n",
      "Output tensor([[0.5756],\n",
      "        [0.1121]], device='mps:0')\n",
      "Iteration 18410 Training loss 0.06175687164068222 Validation loss 0.06299110502004623 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.0398],\n",
      "        [0.9376]], device='mps:0')\n",
      "Iteration 18420 Training loss 0.06820767372846603 Validation loss 0.06300603598356247 Accuracy 0.827750027179718\n",
      "Output tensor([[0.5888],\n",
      "        [0.9658]], device='mps:0')\n",
      "Iteration 18430 Training loss 0.0582856684923172 Validation loss 0.06290333718061447 Accuracy 0.827750027179718\n",
      "Output tensor([[0.7119],\n",
      "        [0.3382]], device='mps:0')\n",
      "Iteration 18440 Training loss 0.06847907602787018 Validation loss 0.063010074198246 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.1315],\n",
      "        [0.6885]], device='mps:0')\n",
      "Iteration 18450 Training loss 0.06678830832242966 Validation loss 0.0632031187415123 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.8415],\n",
      "        [0.9835]], device='mps:0')\n",
      "Iteration 18460 Training loss 0.05699468404054642 Validation loss 0.0630468800663948 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.2455],\n",
      "        [0.0920]], device='mps:0')\n",
      "Iteration 18470 Training loss 0.06402874737977982 Validation loss 0.06295210868120193 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.8592],\n",
      "        [0.1135]], device='mps:0')\n",
      "Iteration 18480 Training loss 0.061135727912187576 Validation loss 0.06298470497131348 Accuracy 0.827750027179718\n",
      "Output tensor([[0.8665],\n",
      "        [0.5790]], device='mps:0')\n",
      "Iteration 18490 Training loss 0.056062743067741394 Validation loss 0.06292415410280228 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.1544],\n",
      "        [0.9309]], device='mps:0')\n",
      "Iteration 18500 Training loss 0.059712838381528854 Validation loss 0.06296718120574951 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.8518],\n",
      "        [0.7899]], device='mps:0')\n",
      "Iteration 18510 Training loss 0.05887481942772865 Validation loss 0.06301064789295197 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.3114],\n",
      "        [0.4040]], device='mps:0')\n",
      "Iteration 18520 Training loss 0.06663713604211807 Validation loss 0.0629788488149643 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.9128],\n",
      "        [0.0290]], device='mps:0')\n",
      "Iteration 18530 Training loss 0.05652358755469322 Validation loss 0.06291443854570389 Accuracy 0.827250063419342\n",
      "Output tensor([[0.9443],\n",
      "        [0.0868]], device='mps:0')\n",
      "Iteration 18540 Training loss 0.06661330163478851 Validation loss 0.06291127949953079 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.8307],\n",
      "        [0.0407]], device='mps:0')\n",
      "Iteration 18550 Training loss 0.05850887671113014 Validation loss 0.06287883222103119 Accuracy 0.82750004529953\n",
      "Output tensor([[0.3797],\n",
      "        [0.8706]], device='mps:0')\n",
      "Iteration 18560 Training loss 0.06377459317445755 Validation loss 0.0630989670753479 Accuracy 0.82750004529953\n",
      "Output tensor([[0.4134],\n",
      "        [0.7321]], device='mps:0')\n",
      "Iteration 18570 Training loss 0.056329697370529175 Validation loss 0.06290712207555771 Accuracy 0.8266250491142273\n",
      "Output tensor([[0.2772],\n",
      "        [0.8106]], device='mps:0')\n",
      "Iteration 18580 Training loss 0.0626286044716835 Validation loss 0.06302004307508469 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.9050],\n",
      "        [0.3956]], device='mps:0')\n",
      "Iteration 18590 Training loss 0.060617636889219284 Validation loss 0.06288064271211624 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.2435],\n",
      "        [0.4877]], device='mps:0')\n",
      "Iteration 18600 Training loss 0.07702399045228958 Validation loss 0.06299421191215515 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.8587],\n",
      "        [0.8191]], device='mps:0')\n",
      "Iteration 18610 Training loss 0.06802286207675934 Validation loss 0.06302128732204437 Accuracy 0.827750027179718\n",
      "Output tensor([[0.0557],\n",
      "        [0.4037]], device='mps:0')\n",
      "Iteration 18620 Training loss 0.05655169486999512 Validation loss 0.0631469339132309 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.5739],\n",
      "        [0.8766]], device='mps:0')\n",
      "Iteration 18630 Training loss 0.051505573093891144 Validation loss 0.06290662288665771 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.4733],\n",
      "        [0.1086]], device='mps:0')\n",
      "Iteration 18640 Training loss 0.05512034147977829 Validation loss 0.06287003308534622 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.3858],\n",
      "        [0.8869]], device='mps:0')\n",
      "Iteration 18650 Training loss 0.06296730041503906 Validation loss 0.06331790238618851 Accuracy 0.827875018119812\n",
      "Output tensor([[0.5519],\n",
      "        [0.0386]], device='mps:0')\n",
      "Iteration 18660 Training loss 0.06068824604153633 Validation loss 0.0628562867641449 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.3396],\n",
      "        [0.2877]], device='mps:0')\n",
      "Iteration 18670 Training loss 0.06200167536735535 Validation loss 0.06292340904474258 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.8245],\n",
      "        [0.0880]], device='mps:0')\n",
      "Iteration 18680 Training loss 0.059890732169151306 Validation loss 0.06298508495092392 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.8307],\n",
      "        [0.2894]], device='mps:0')\n",
      "Iteration 18690 Training loss 0.0723915845155716 Validation loss 0.06292151659727097 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.5455],\n",
      "        [0.0798]], device='mps:0')\n",
      "Iteration 18700 Training loss 0.06379551440477371 Validation loss 0.06296058744192123 Accuracy 0.8270000219345093\n",
      "Output tensor([[0.0165],\n",
      "        [0.9839]], device='mps:0')\n",
      "Iteration 18710 Training loss 0.06733402609825134 Validation loss 0.06290527433156967 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.5854],\n",
      "        [0.9169]], device='mps:0')\n",
      "Iteration 18720 Training loss 0.06851863116025925 Validation loss 0.0632205605506897 Accuracy 0.827875018119812\n",
      "Output tensor([[0.3105],\n",
      "        [0.3203]], device='mps:0')\n",
      "Iteration 18730 Training loss 0.06580865383148193 Validation loss 0.06315542757511139 Accuracy 0.827625036239624\n",
      "Output tensor([[0.0249],\n",
      "        [0.8611]], device='mps:0')\n",
      "Iteration 18740 Training loss 0.06001346558332443 Validation loss 0.06285984069108963 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.4014],\n",
      "        [0.4110]], device='mps:0')\n",
      "Iteration 18750 Training loss 0.06574580073356628 Validation loss 0.06301328539848328 Accuracy 0.827250063419342\n",
      "Output tensor([[0.2239],\n",
      "        [0.1047]], device='mps:0')\n",
      "Iteration 18760 Training loss 0.06001110374927521 Validation loss 0.06290442496538162 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.0808],\n",
      "        [0.0601]], device='mps:0')\n",
      "Iteration 18770 Training loss 0.05657167360186577 Validation loss 0.06284603476524353 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.9834],\n",
      "        [0.1356]], device='mps:0')\n",
      "Iteration 18780 Training loss 0.06374508887529373 Validation loss 0.06283614039421082 Accuracy 0.827875018119812\n",
      "Output tensor([[0.7147],\n",
      "        [0.7899]], device='mps:0')\n",
      "Iteration 18790 Training loss 0.05759741738438606 Validation loss 0.06284892559051514 Accuracy 0.82750004529953\n",
      "Output tensor([[0.1578],\n",
      "        [0.6815]], device='mps:0')\n",
      "Iteration 18800 Training loss 0.06200028583407402 Validation loss 0.06294254958629608 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.5190],\n",
      "        [0.9294]], device='mps:0')\n",
      "Iteration 18810 Training loss 0.06310351192951202 Validation loss 0.06328263133764267 Accuracy 0.827625036239624\n",
      "Output tensor([[0.1703],\n",
      "        [0.1004]], device='mps:0')\n",
      "Iteration 18820 Training loss 0.06581728905439377 Validation loss 0.06283868104219437 Accuracy 0.82750004529953\n",
      "Output tensor([[0.1307],\n",
      "        [0.8444]], device='mps:0')\n",
      "Iteration 18830 Training loss 0.0597010962665081 Validation loss 0.06302516162395477 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.2726],\n",
      "        [0.7595]], device='mps:0')\n",
      "Iteration 18840 Training loss 0.05894068628549576 Validation loss 0.06283222883939743 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.9483],\n",
      "        [0.8768]], device='mps:0')\n",
      "Iteration 18850 Training loss 0.05943736433982849 Validation loss 0.06300602108240128 Accuracy 0.827375054359436\n",
      "Output tensor([[0.1125],\n",
      "        [0.3873]], device='mps:0')\n",
      "Iteration 18860 Training loss 0.06326527148485184 Validation loss 0.06312454491853714 Accuracy 0.827875018119812\n",
      "Output tensor([[0.1011],\n",
      "        [0.7370]], device='mps:0')\n",
      "Iteration 18870 Training loss 0.06342388689517975 Validation loss 0.06282636523246765 Accuracy 0.827625036239624\n",
      "Output tensor([[0.7144],\n",
      "        [0.0287]], device='mps:0')\n",
      "Iteration 18880 Training loss 0.06217235326766968 Validation loss 0.06298620998859406 Accuracy 0.827875018119812\n",
      "Output tensor([[0.0121],\n",
      "        [0.5887]], device='mps:0')\n",
      "Iteration 18890 Training loss 0.05897501856088638 Validation loss 0.06281108409166336 Accuracy 0.82750004529953\n",
      "Output tensor([[0.1539],\n",
      "        [0.5122]], device='mps:0')\n",
      "Iteration 18900 Training loss 0.060241393744945526 Validation loss 0.06281477957963943 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.6124],\n",
      "        [0.8599]], device='mps:0')\n",
      "Iteration 18910 Training loss 0.059748705476522446 Validation loss 0.06287011504173279 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.5519],\n",
      "        [0.9824]], device='mps:0')\n",
      "Iteration 18920 Training loss 0.05958131328225136 Validation loss 0.0628250241279602 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.8542],\n",
      "        [0.3107]], device='mps:0')\n",
      "Iteration 18930 Training loss 0.055954474955797195 Validation loss 0.06283257156610489 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.9015],\n",
      "        [0.0822]], device='mps:0')\n",
      "Iteration 18940 Training loss 0.06231781467795372 Validation loss 0.06294192373752594 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.7463],\n",
      "        [0.9709]], device='mps:0')\n",
      "Iteration 18950 Training loss 0.06312285363674164 Validation loss 0.06300301104784012 Accuracy 0.827750027179718\n",
      "Output tensor([[0.5509],\n",
      "        [0.8377]], device='mps:0')\n",
      "Iteration 18960 Training loss 0.0630897805094719 Validation loss 0.06279289722442627 Accuracy 0.827750027179718\n",
      "Output tensor([[0.3114],\n",
      "        [0.0337]], device='mps:0')\n",
      "Iteration 18970 Training loss 0.06602835655212402 Validation loss 0.06278838962316513 Accuracy 0.827875018119812\n",
      "Output tensor([[0.2284],\n",
      "        [0.0420]], device='mps:0')\n",
      "Iteration 18980 Training loss 0.05951473489403725 Validation loss 0.06278906762599945 Accuracy 0.827875018119812\n",
      "Output tensor([[0.9471],\n",
      "        [0.1412]], device='mps:0')\n",
      "Iteration 18990 Training loss 0.06148217245936394 Validation loss 0.06287281960248947 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.9458],\n",
      "        [0.1795]], device='mps:0')\n",
      "Iteration 19000 Training loss 0.05666448548436165 Validation loss 0.06278679519891739 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.0624],\n",
      "        [0.4732]], device='mps:0')\n",
      "Iteration 19010 Training loss 0.06016690656542778 Validation loss 0.0629158765077591 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.2609],\n",
      "        [0.6466]], device='mps:0')\n",
      "Iteration 19020 Training loss 0.06350889801979065 Validation loss 0.062903992831707 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.6716],\n",
      "        [0.0510]], device='mps:0')\n",
      "Iteration 19030 Training loss 0.06482622027397156 Validation loss 0.06281813979148865 Accuracy 0.82750004529953\n",
      "Output tensor([[0.6427],\n",
      "        [0.1171]], device='mps:0')\n",
      "Iteration 19040 Training loss 0.05699336528778076 Validation loss 0.06280224025249481 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.8202],\n",
      "        [0.3747]], device='mps:0')\n",
      "Iteration 19050 Training loss 0.06276724487543106 Validation loss 0.06285621225833893 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.6047],\n",
      "        [0.4554]], device='mps:0')\n",
      "Iteration 19060 Training loss 0.0644778460264206 Validation loss 0.06280558556318283 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.7550],\n",
      "        [0.0184]], device='mps:0')\n",
      "Iteration 19070 Training loss 0.05415647476911545 Validation loss 0.06277130544185638 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.2400],\n",
      "        [0.1357]], device='mps:0')\n",
      "Iteration 19080 Training loss 0.057639311999082565 Validation loss 0.06299764662981033 Accuracy 0.827250063419342\n",
      "Output tensor([[0.3092],\n",
      "        [0.3801]], device='mps:0')\n",
      "Iteration 19090 Training loss 0.06298743188381195 Validation loss 0.06275703012943268 Accuracy 0.827750027179718\n",
      "Output tensor([[0.0469],\n",
      "        [0.1768]], device='mps:0')\n",
      "Iteration 19100 Training loss 0.06385120004415512 Validation loss 0.06276615709066391 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.7948],\n",
      "        [0.9980]], device='mps:0')\n",
      "Iteration 19110 Training loss 0.0656457245349884 Validation loss 0.06275089085102081 Accuracy 0.827750027179718\n",
      "Output tensor([[0.0405],\n",
      "        [0.1286]], device='mps:0')\n",
      "Iteration 19120 Training loss 0.06421550363302231 Validation loss 0.06290312856435776 Accuracy 0.827250063419342\n",
      "Output tensor([[0.9575],\n",
      "        [0.1223]], device='mps:0')\n",
      "Iteration 19130 Training loss 0.0632539615035057 Validation loss 0.06275686621665955 Accuracy 0.827625036239624\n",
      "Output tensor([[0.9883],\n",
      "        [0.8314]], device='mps:0')\n",
      "Iteration 19140 Training loss 0.0601428858935833 Validation loss 0.06275555491447449 Accuracy 0.827875018119812\n",
      "Output tensor([[0.0586],\n",
      "        [0.0155]], device='mps:0')\n",
      "Iteration 19150 Training loss 0.05926864594221115 Validation loss 0.06275128573179245 Accuracy 0.827875018119812\n",
      "Output tensor([[0.9110],\n",
      "        [0.7648]], device='mps:0')\n",
      "Iteration 19160 Training loss 0.07209763675928116 Validation loss 0.06275255233049393 Accuracy 0.827875018119812\n",
      "Output tensor([[0.2650],\n",
      "        [0.9616]], device='mps:0')\n",
      "Iteration 19170 Training loss 0.05845438688993454 Validation loss 0.0627487450838089 Accuracy 0.827750027179718\n",
      "Output tensor([[0.6393],\n",
      "        [0.5985]], device='mps:0')\n",
      "Iteration 19180 Training loss 0.06365139782428741 Validation loss 0.06285596638917923 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.2462],\n",
      "        [0.3269]], device='mps:0')\n",
      "Iteration 19190 Training loss 0.060696519911289215 Validation loss 0.06280377507209778 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.0258],\n",
      "        [0.9790]], device='mps:0')\n",
      "Iteration 19200 Training loss 0.0586794838309288 Validation loss 0.06274396926164627 Accuracy 0.827875018119812\n",
      "Output tensor([[0.9533],\n",
      "        [0.7320]], device='mps:0')\n",
      "Iteration 19210 Training loss 0.05222387984395027 Validation loss 0.06291339546442032 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.2873],\n",
      "        [0.5206]], device='mps:0')\n",
      "Iteration 19220 Training loss 0.06238788366317749 Validation loss 0.06292356550693512 Accuracy 0.827625036239624\n",
      "Output tensor([[0.7333],\n",
      "        [0.7145]], device='mps:0')\n",
      "Iteration 19230 Training loss 0.059224240481853485 Validation loss 0.0627552717924118 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.2582],\n",
      "        [0.5401]], device='mps:0')\n",
      "Iteration 19240 Training loss 0.05257789045572281 Validation loss 0.06277245283126831 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.0912],\n",
      "        [0.9872]], device='mps:0')\n",
      "Iteration 19250 Training loss 0.05774007365107536 Validation loss 0.06273689866065979 Accuracy 0.827875018119812\n",
      "Output tensor([[0.1567],\n",
      "        [0.8426]], device='mps:0')\n",
      "Iteration 19260 Training loss 0.06139037385582924 Validation loss 0.06279637664556503 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.7996],\n",
      "        [0.9351]], device='mps:0')\n",
      "Iteration 19270 Training loss 0.05777846276760101 Validation loss 0.0630347803235054 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.3749],\n",
      "        [0.8640]], device='mps:0')\n",
      "Iteration 19280 Training loss 0.059134386479854584 Validation loss 0.06275428831577301 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.9889],\n",
      "        [0.3004]], device='mps:0')\n",
      "Iteration 19290 Training loss 0.052834492176771164 Validation loss 0.06276106089353561 Accuracy 0.827750027179718\n",
      "Output tensor([[0.2835],\n",
      "        [0.1519]], device='mps:0')\n",
      "Iteration 19300 Training loss 0.05972380191087723 Validation loss 0.06274916976690292 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.7954],\n",
      "        [0.6334]], device='mps:0')\n",
      "Iteration 19310 Training loss 0.06575638800859451 Validation loss 0.06273187696933746 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.0410],\n",
      "        [0.3037]], device='mps:0')\n",
      "Iteration 19320 Training loss 0.049415674060583115 Validation loss 0.06275106966495514 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.9876],\n",
      "        [0.8506]], device='mps:0')\n",
      "Iteration 19330 Training loss 0.052303023636341095 Validation loss 0.06276530772447586 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.0741],\n",
      "        [0.0554]], device='mps:0')\n",
      "Iteration 19340 Training loss 0.06407003849744797 Validation loss 0.06271550059318542 Accuracy 0.827875018119812\n",
      "Output tensor([[0.8476],\n",
      "        [0.2943]], device='mps:0')\n",
      "Iteration 19350 Training loss 0.0689905509352684 Validation loss 0.06273670494556427 Accuracy 0.827875018119812\n",
      "Output tensor([[0.4688],\n",
      "        [0.9767]], device='mps:0')\n",
      "Iteration 19360 Training loss 0.06561653316020966 Validation loss 0.06271560490131378 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.1632],\n",
      "        [0.7905]], device='mps:0')\n",
      "Iteration 19370 Training loss 0.06218266487121582 Validation loss 0.0627438947558403 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.6104],\n",
      "        [0.5416]], device='mps:0')\n",
      "Iteration 19380 Training loss 0.06588482856750488 Validation loss 0.0627087652683258 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.5097],\n",
      "        [0.1348]], device='mps:0')\n",
      "Iteration 19390 Training loss 0.0649682804942131 Validation loss 0.06270939856767654 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.3892],\n",
      "        [0.3367]], device='mps:0')\n",
      "Iteration 19400 Training loss 0.058426860719919205 Validation loss 0.06269852817058563 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.9509],\n",
      "        [0.2319]], device='mps:0')\n",
      "Iteration 19410 Training loss 0.06944477558135986 Validation loss 0.0632476955652237 Accuracy 0.8265000581741333\n",
      "Output tensor([[0.9679],\n",
      "        [0.9855]], device='mps:0')\n",
      "Iteration 19420 Training loss 0.06367062777280807 Validation loss 0.062759630382061 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.4228],\n",
      "        [0.3651]], device='mps:0')\n",
      "Iteration 19430 Training loss 0.06658817082643509 Validation loss 0.06276259571313858 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.1468],\n",
      "        [0.3924]], device='mps:0')\n",
      "Iteration 19440 Training loss 0.06872888654470444 Validation loss 0.06270993500947952 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.8116],\n",
      "        [0.2649]], device='mps:0')\n",
      "Iteration 19450 Training loss 0.06646985560655594 Validation loss 0.06277455389499664 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.9567],\n",
      "        [0.7346]], device='mps:0')\n",
      "Iteration 19460 Training loss 0.058506209403276443 Validation loss 0.06269437074661255 Accuracy 0.827750027179718\n",
      "Output tensor([[0.3591],\n",
      "        [0.8278]], device='mps:0')\n",
      "Iteration 19470 Training loss 0.06654047220945358 Validation loss 0.06275404244661331 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.7250],\n",
      "        [0.3455]], device='mps:0')\n",
      "Iteration 19480 Training loss 0.05728277191519737 Validation loss 0.06269281357526779 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.0831],\n",
      "        [0.1039]], device='mps:0')\n",
      "Iteration 19490 Training loss 0.06627394258975983 Validation loss 0.0627129077911377 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.7685],\n",
      "        [0.3993]], device='mps:0')\n",
      "Iteration 19500 Training loss 0.0666627511382103 Validation loss 0.06272247433662415 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.2461],\n",
      "        [0.2064]], device='mps:0')\n",
      "Iteration 19510 Training loss 0.06094495579600334 Validation loss 0.06268660724163055 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.3585],\n",
      "        [0.9615]], device='mps:0')\n",
      "Iteration 19520 Training loss 0.058171242475509644 Validation loss 0.0627610832452774 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.3890],\n",
      "        [0.8013]], device='mps:0')\n",
      "Iteration 19530 Training loss 0.05796929821372032 Validation loss 0.06268136203289032 Accuracy 0.827875018119812\n",
      "Output tensor([[0.3707],\n",
      "        [0.8058]], device='mps:0')\n",
      "Iteration 19540 Training loss 0.05353633686900139 Validation loss 0.06269186735153198 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.9334],\n",
      "        [0.9310]], device='mps:0')\n",
      "Iteration 19550 Training loss 0.06468728929758072 Validation loss 0.06267978996038437 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.9523],\n",
      "        [0.9910]], device='mps:0')\n",
      "Iteration 19560 Training loss 0.06085636094212532 Validation loss 0.06267540156841278 Accuracy 0.827750027179718\n",
      "Output tensor([[0.6620],\n",
      "        [0.9674]], device='mps:0')\n",
      "Iteration 19570 Training loss 0.06798135489225388 Validation loss 0.062694251537323 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.5710],\n",
      "        [0.0599]], device='mps:0')\n",
      "Iteration 19580 Training loss 0.06634198129177094 Validation loss 0.06271229684352875 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7633],\n",
      "        [0.9181]], device='mps:0')\n",
      "Iteration 19590 Training loss 0.056438446044921875 Validation loss 0.06269030272960663 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.3458],\n",
      "        [0.4085]], device='mps:0')\n",
      "Iteration 19600 Training loss 0.0612524151802063 Validation loss 0.0626542791724205 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.7377],\n",
      "        [0.5190]], device='mps:0')\n",
      "Iteration 19610 Training loss 0.05668209120631218 Validation loss 0.06266128271818161 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.6563],\n",
      "        [0.2969]], device='mps:0')\n",
      "Iteration 19620 Training loss 0.06488250941038132 Validation loss 0.06275776773691177 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.6576],\n",
      "        [0.3757]], device='mps:0')\n",
      "Iteration 19630 Training loss 0.06697222590446472 Validation loss 0.06267525255680084 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.6387],\n",
      "        [0.0409]], device='mps:0')\n",
      "Iteration 19640 Training loss 0.06454939395189285 Validation loss 0.06268355995416641 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.6488],\n",
      "        [0.2917]], device='mps:0')\n",
      "Iteration 19650 Training loss 0.05309216305613518 Validation loss 0.06271908432245255 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.9054],\n",
      "        [0.3063]], device='mps:0')\n",
      "Iteration 19660 Training loss 0.06258245557546616 Validation loss 0.06323655694723129 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.8271],\n",
      "        [0.6891]], device='mps:0')\n",
      "Iteration 19670 Training loss 0.052705854177474976 Validation loss 0.06265061348676682 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.4347],\n",
      "        [0.8465]], device='mps:0')\n",
      "Iteration 19680 Training loss 0.05999673157930374 Validation loss 0.06269149482250214 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.5530],\n",
      "        [0.6083]], device='mps:0')\n",
      "Iteration 19690 Training loss 0.0686153769493103 Validation loss 0.0626525804400444 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.9479],\n",
      "        [0.1197]], device='mps:0')\n",
      "Iteration 19700 Training loss 0.05876132845878601 Validation loss 0.06264840811491013 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.4458],\n",
      "        [0.4841]], device='mps:0')\n",
      "Iteration 19710 Training loss 0.06583116203546524 Validation loss 0.06267361342906952 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.5707],\n",
      "        [0.9523]], device='mps:0')\n",
      "Iteration 19720 Training loss 0.06904474645853043 Validation loss 0.06270825862884521 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.3189],\n",
      "        [0.1311]], device='mps:0')\n",
      "Iteration 19730 Training loss 0.062204454094171524 Validation loss 0.06272588670253754 Accuracy 0.8267500400543213\n",
      "Output tensor([[0.2252],\n",
      "        [0.7240]], device='mps:0')\n",
      "Iteration 19740 Training loss 0.06075511872768402 Validation loss 0.06299053132534027 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.9334],\n",
      "        [0.0936]], device='mps:0')\n",
      "Iteration 19750 Training loss 0.06483564525842667 Validation loss 0.0626605749130249 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.8730],\n",
      "        [0.5303]], device='mps:0')\n",
      "Iteration 19760 Training loss 0.06729558110237122 Validation loss 0.0627393126487732 Accuracy 0.827375054359436\n",
      "Output tensor([[0.8101],\n",
      "        [0.7403]], device='mps:0')\n",
      "Iteration 19770 Training loss 0.059492338448762894 Validation loss 0.06267717480659485 Accuracy 0.82750004529953\n",
      "Output tensor([[0.7680],\n",
      "        [0.2520]], device='mps:0')\n",
      "Iteration 19780 Training loss 0.06712227314710617 Validation loss 0.06284670531749725 Accuracy 0.8262500166893005\n",
      "Output tensor([[0.1238],\n",
      "        [0.0714]], device='mps:0')\n",
      "Iteration 19790 Training loss 0.06151054799556732 Validation loss 0.06271180510520935 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.0867],\n",
      "        [0.6972]], device='mps:0')\n",
      "Iteration 19800 Training loss 0.05908317491412163 Validation loss 0.06263846158981323 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.7834],\n",
      "        [0.5131]], device='mps:0')\n",
      "Iteration 19810 Training loss 0.06101171299815178 Validation loss 0.0628625825047493 Accuracy 0.827625036239624\n",
      "Output tensor([[0.7610],\n",
      "        [0.8237]], device='mps:0')\n",
      "Iteration 19820 Training loss 0.055094461888074875 Validation loss 0.06267250329256058 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.0646],\n",
      "        [0.8713]], device='mps:0')\n",
      "Iteration 19830 Training loss 0.05328274518251419 Validation loss 0.06261724978685379 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.9111],\n",
      "        [0.9566]], device='mps:0')\n",
      "Iteration 19840 Training loss 0.06620081514120102 Validation loss 0.06266340613365173 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.4010],\n",
      "        [0.6477]], device='mps:0')\n",
      "Iteration 19850 Training loss 0.06425818055868149 Validation loss 0.06269445270299911 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.0176],\n",
      "        [0.6515]], device='mps:0')\n",
      "Iteration 19860 Training loss 0.054714594036340714 Validation loss 0.06262331455945969 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.5832],\n",
      "        [0.2154]], device='mps:0')\n",
      "Iteration 19870 Training loss 0.05606427416205406 Validation loss 0.06261273473501205 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.3461],\n",
      "        [0.9096]], device='mps:0')\n",
      "Iteration 19880 Training loss 0.062477272003889084 Validation loss 0.06292711943387985 Accuracy 0.8263750672340393\n",
      "Output tensor([[0.5043],\n",
      "        [0.1560]], device='mps:0')\n",
      "Iteration 19890 Training loss 0.06257210671901703 Validation loss 0.06262762099504471 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.7844],\n",
      "        [0.7194]], device='mps:0')\n",
      "Iteration 19900 Training loss 0.06928683817386627 Validation loss 0.06260469555854797 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.7118],\n",
      "        [0.8838]], device='mps:0')\n",
      "Iteration 19910 Training loss 0.07417801022529602 Validation loss 0.06261897087097168 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.7458],\n",
      "        [0.4510]], device='mps:0')\n",
      "Iteration 19920 Training loss 0.06445351988077164 Validation loss 0.06261210888624191 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.0236],\n",
      "        [0.4222]], device='mps:0')\n",
      "Iteration 19930 Training loss 0.05542851239442825 Validation loss 0.06270516663789749 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.4632],\n",
      "        [0.8893]], device='mps:0')\n",
      "Iteration 19940 Training loss 0.06515264511108398 Validation loss 0.06268540769815445 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.3710],\n",
      "        [0.0706]], device='mps:0')\n",
      "Iteration 19950 Training loss 0.06516426056623459 Validation loss 0.06262475252151489 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.1917],\n",
      "        [0.2001]], device='mps:0')\n",
      "Iteration 19960 Training loss 0.061545949429273605 Validation loss 0.06263130903244019 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.7625],\n",
      "        [0.7951]], device='mps:0')\n",
      "Iteration 19970 Training loss 0.07020696252584457 Validation loss 0.06267435848712921 Accuracy 0.82750004529953\n",
      "Output tensor([[0.0889],\n",
      "        [0.3686]], device='mps:0')\n",
      "Iteration 19980 Training loss 0.06666391342878342 Validation loss 0.06260574609041214 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.9875],\n",
      "        [0.9250]], device='mps:0')\n",
      "Iteration 19990 Training loss 0.06476826220750809 Validation loss 0.06279835850000381 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.6133],\n",
      "        [0.9950]], device='mps:0')\n",
      "Iteration 20000 Training loss 0.06461396813392639 Validation loss 0.06260909885168076 Accuracy 0.827875018119812\n",
      "Output tensor([[0.5406],\n",
      "        [0.8538]], device='mps:0')\n",
      "Iteration 20010 Training loss 0.05657130107283592 Validation loss 0.06258174777030945 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.7247],\n",
      "        [0.7602]], device='mps:0')\n",
      "Iteration 20020 Training loss 0.06874848902225494 Validation loss 0.06267999112606049 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.8767],\n",
      "        [0.6032]], device='mps:0')\n",
      "Iteration 20030 Training loss 0.062019869685173035 Validation loss 0.0627443939447403 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.2780],\n",
      "        [0.1668]], device='mps:0')\n",
      "Iteration 20040 Training loss 0.06474379450082779 Validation loss 0.06268388777971268 Accuracy 0.827375054359436\n",
      "Output tensor([[0.8061],\n",
      "        [0.6299]], device='mps:0')\n",
      "Iteration 20050 Training loss 0.059331368654966354 Validation loss 0.0625801533460617 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.0866],\n",
      "        [0.1594]], device='mps:0')\n",
      "Iteration 20060 Training loss 0.07047612220048904 Validation loss 0.06265190243721008 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.9886],\n",
      "        [0.0220]], device='mps:0')\n",
      "Iteration 20070 Training loss 0.05571021884679794 Validation loss 0.06290605664253235 Accuracy 0.827750027179718\n",
      "Output tensor([[0.9165],\n",
      "        [0.0411]], device='mps:0')\n",
      "Iteration 20080 Training loss 0.059286125004291534 Validation loss 0.06262807548046112 Accuracy 0.827250063419342\n",
      "Output tensor([[0.0351],\n",
      "        [0.1240]], device='mps:0')\n",
      "Iteration 20090 Training loss 0.06149608641862869 Validation loss 0.06268184632062912 Accuracy 0.827250063419342\n",
      "Output tensor([[0.3895],\n",
      "        [0.1131]], device='mps:0')\n",
      "Iteration 20100 Training loss 0.06490302085876465 Validation loss 0.06257133930921555 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.8095],\n",
      "        [0.8946]], device='mps:0')\n",
      "Iteration 20110 Training loss 0.06162960082292557 Validation loss 0.06295937299728394 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.1160],\n",
      "        [0.3155]], device='mps:0')\n",
      "Iteration 20120 Training loss 0.0679847002029419 Validation loss 0.062751904129982 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.9646],\n",
      "        [0.0617]], device='mps:0')\n",
      "Iteration 20130 Training loss 0.06508089601993561 Validation loss 0.06256438791751862 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.2453],\n",
      "        [0.4249]], device='mps:0')\n",
      "Iteration 20140 Training loss 0.0637684017419815 Validation loss 0.06264131516218185 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.3786],\n",
      "        [0.6476]], device='mps:0')\n",
      "Iteration 20150 Training loss 0.059235695749521255 Validation loss 0.06262504309415817 Accuracy 0.8268750309944153\n",
      "Output tensor([[0.8897],\n",
      "        [0.0269]], device='mps:0')\n",
      "Iteration 20160 Training loss 0.06751683354377747 Validation loss 0.06257457286119461 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.1276],\n",
      "        [0.8868]], device='mps:0')\n",
      "Iteration 20170 Training loss 0.06369639933109283 Validation loss 0.06255757063627243 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.8095],\n",
      "        [0.7694]], device='mps:0')\n",
      "Iteration 20180 Training loss 0.057777490466833115 Validation loss 0.06257906556129456 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.0181],\n",
      "        [0.9691]], device='mps:0')\n",
      "Iteration 20190 Training loss 0.06326686590909958 Validation loss 0.06258148699998856 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.5062],\n",
      "        [0.4198]], device='mps:0')\n",
      "Iteration 20200 Training loss 0.06032436341047287 Validation loss 0.0626806989312172 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.1915],\n",
      "        [0.4820]], device='mps:0')\n",
      "Iteration 20210 Training loss 0.06508481502532959 Validation loss 0.06253548711538315 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.2714],\n",
      "        [0.2239]], device='mps:0')\n",
      "Iteration 20220 Training loss 0.05938336253166199 Validation loss 0.06256803125143051 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.7783],\n",
      "        [0.0323]], device='mps:0')\n",
      "Iteration 20230 Training loss 0.07103442400693893 Validation loss 0.06264493614435196 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.4262],\n",
      "        [0.8348]], device='mps:0')\n",
      "Iteration 20240 Training loss 0.06077231094241142 Validation loss 0.06276071816682816 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.9627],\n",
      "        [0.7533]], device='mps:0')\n",
      "Iteration 20250 Training loss 0.06520485877990723 Validation loss 0.06260760873556137 Accuracy 0.827750027179718\n",
      "Output tensor([[0.1930],\n",
      "        [0.4398]], device='mps:0')\n",
      "Iteration 20260 Training loss 0.06635897606611252 Validation loss 0.06258740276098251 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9049],\n",
      "        [0.7424]], device='mps:0')\n",
      "Iteration 20270 Training loss 0.06878602504730225 Validation loss 0.0630713477730751 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.8874],\n",
      "        [0.8543]], device='mps:0')\n",
      "Iteration 20280 Training loss 0.05608290061354637 Validation loss 0.06305325031280518 Accuracy 0.827750027179718\n",
      "Output tensor([[0.9421],\n",
      "        [0.2354]], device='mps:0')\n",
      "Iteration 20290 Training loss 0.06357172131538391 Validation loss 0.06254062801599503 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.0740],\n",
      "        [0.2950]], device='mps:0')\n",
      "Iteration 20300 Training loss 0.06521296501159668 Validation loss 0.062527135014534 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.9836],\n",
      "        [0.2311]], device='mps:0')\n",
      "Iteration 20310 Training loss 0.05513868108391762 Validation loss 0.06252948194742203 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.1412],\n",
      "        [0.3540]], device='mps:0')\n",
      "Iteration 20320 Training loss 0.0651974305510521 Validation loss 0.06261339783668518 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8412],\n",
      "        [0.1491]], device='mps:0')\n",
      "Iteration 20330 Training loss 0.05874668061733246 Validation loss 0.06257569789886475 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.1074],\n",
      "        [0.4468]], device='mps:0')\n",
      "Iteration 20340 Training loss 0.05715629458427429 Validation loss 0.06255115568637848 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.7180],\n",
      "        [0.9261]], device='mps:0')\n",
      "Iteration 20350 Training loss 0.061497047543525696 Validation loss 0.06251747906208038 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.8643],\n",
      "        [0.5661]], device='mps:0')\n",
      "Iteration 20360 Training loss 0.05794460326433182 Validation loss 0.06252674013376236 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.4481],\n",
      "        [0.6497]], device='mps:0')\n",
      "Iteration 20370 Training loss 0.06405181437730789 Validation loss 0.06255853921175003 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.5472],\n",
      "        [0.0217]], device='mps:0')\n",
      "Iteration 20380 Training loss 0.06020364537835121 Validation loss 0.0626734122633934 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.2612],\n",
      "        [0.7787]], device='mps:0')\n",
      "Iteration 20390 Training loss 0.06686968356370926 Validation loss 0.06252050399780273 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.4491],\n",
      "        [0.9908]], device='mps:0')\n",
      "Iteration 20400 Training loss 0.05672209709882736 Validation loss 0.06261252611875534 Accuracy 0.827375054359436\n",
      "Output tensor([[0.9442],\n",
      "        [0.9772]], device='mps:0')\n",
      "Iteration 20410 Training loss 0.06998683512210846 Validation loss 0.06269679963588715 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.3154],\n",
      "        [0.7828]], device='mps:0')\n",
      "Iteration 20420 Training loss 0.059711311012506485 Validation loss 0.06256356090307236 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.4474],\n",
      "        [0.8556]], device='mps:0')\n",
      "Iteration 20430 Training loss 0.05757395550608635 Validation loss 0.0626416727900505 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.0543],\n",
      "        [0.5869]], device='mps:0')\n",
      "Iteration 20440 Training loss 0.05922267958521843 Validation loss 0.06259044259786606 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.6603],\n",
      "        [0.5294]], device='mps:0')\n",
      "Iteration 20450 Training loss 0.06352218240499496 Validation loss 0.06249718368053436 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.2295],\n",
      "        [0.4131]], device='mps:0')\n",
      "Iteration 20460 Training loss 0.06641054153442383 Validation loss 0.06249801069498062 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.8565],\n",
      "        [0.0902]], device='mps:0')\n",
      "Iteration 20470 Training loss 0.06138593703508377 Validation loss 0.0625956654548645 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.8151],\n",
      "        [0.2213]], device='mps:0')\n",
      "Iteration 20480 Training loss 0.05576948821544647 Validation loss 0.06258965283632278 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.3456],\n",
      "        [0.5832]], device='mps:0')\n",
      "Iteration 20490 Training loss 0.05928925424814224 Validation loss 0.06256192922592163 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.9733],\n",
      "        [0.9526]], device='mps:0')\n",
      "Iteration 20500 Training loss 0.06153090298175812 Validation loss 0.0625188946723938 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.5096],\n",
      "        [0.5688]], device='mps:0')\n",
      "Iteration 20510 Training loss 0.057590704411268234 Validation loss 0.06257365643978119 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.1579],\n",
      "        [0.3416]], device='mps:0')\n",
      "Iteration 20520 Training loss 0.06261938065290451 Validation loss 0.06250084191560745 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.2835],\n",
      "        [0.4980]], device='mps:0')\n",
      "Iteration 20530 Training loss 0.06991127878427505 Validation loss 0.06254745274782181 Accuracy 0.827375054359436\n",
      "Output tensor([[0.9016],\n",
      "        [0.9123]], device='mps:0')\n",
      "Iteration 20540 Training loss 0.06163198500871658 Validation loss 0.06252314895391464 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.5214],\n",
      "        [0.1112]], device='mps:0')\n",
      "Iteration 20550 Training loss 0.06311730295419693 Validation loss 0.06264825165271759 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.2299],\n",
      "        [0.6685]], device='mps:0')\n",
      "Iteration 20560 Training loss 0.06281305104494095 Validation loss 0.06252267956733704 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.9165],\n",
      "        [0.2192]], device='mps:0')\n",
      "Iteration 20570 Training loss 0.057286981493234634 Validation loss 0.062477804720401764 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.4572],\n",
      "        [0.1693]], device='mps:0')\n",
      "Iteration 20580 Training loss 0.055634792894124985 Validation loss 0.0625358298420906 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.1628],\n",
      "        [0.7861]], device='mps:0')\n",
      "Iteration 20590 Training loss 0.05309663712978363 Validation loss 0.06247645244002342 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.8347],\n",
      "        [0.0988]], device='mps:0')\n",
      "Iteration 20600 Training loss 0.060588546097278595 Validation loss 0.062472473829984665 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.8166],\n",
      "        [0.0600]], device='mps:0')\n",
      "Iteration 20610 Training loss 0.05799311399459839 Validation loss 0.062463074922561646 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.6153],\n",
      "        [0.9702]], device='mps:0')\n",
      "Iteration 20620 Training loss 0.05501215159893036 Validation loss 0.06252133846282959 Accuracy 0.827875018119812\n",
      "Output tensor([[0.4239],\n",
      "        [0.3138]], device='mps:0')\n",
      "Iteration 20630 Training loss 0.06555595993995667 Validation loss 0.062483493238687515 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.0327],\n",
      "        [0.1767]], device='mps:0')\n",
      "Iteration 20640 Training loss 0.05671871826052666 Validation loss 0.06250936537981033 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.6810],\n",
      "        [0.1475]], device='mps:0')\n",
      "Iteration 20650 Training loss 0.05774977430701256 Validation loss 0.062476739287376404 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.4843],\n",
      "        [0.9048]], device='mps:0')\n",
      "Iteration 20660 Training loss 0.06016365438699722 Validation loss 0.062457773834466934 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.1206],\n",
      "        [0.3349]], device='mps:0')\n",
      "Iteration 20670 Training loss 0.05587003007531166 Validation loss 0.062469545751810074 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9219],\n",
      "        [0.0283]], device='mps:0')\n",
      "Iteration 20680 Training loss 0.06098027154803276 Validation loss 0.06247059255838394 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.8428],\n",
      "        [0.0180]], device='mps:0')\n",
      "Iteration 20690 Training loss 0.05597671493887901 Validation loss 0.06257514655590057 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7775],\n",
      "        [0.7875]], device='mps:0')\n",
      "Iteration 20700 Training loss 0.05928099527955055 Validation loss 0.06246159225702286 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.4619],\n",
      "        [0.0398]], device='mps:0')\n",
      "Iteration 20710 Training loss 0.05830145999789238 Validation loss 0.06264401972293854 Accuracy 0.827375054359436\n",
      "Output tensor([[0.4741],\n",
      "        [0.0867]], device='mps:0')\n",
      "Iteration 20720 Training loss 0.055621035397052765 Validation loss 0.062464337795972824 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.2698],\n",
      "        [0.4710]], device='mps:0')\n",
      "Iteration 20730 Training loss 0.05706339702010155 Validation loss 0.06255744397640228 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.3210],\n",
      "        [0.9402]], device='mps:0')\n",
      "Iteration 20740 Training loss 0.06457255035638809 Validation loss 0.06243522837758064 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.8295],\n",
      "        [0.9130]], device='mps:0')\n",
      "Iteration 20750 Training loss 0.0652887299656868 Validation loss 0.06250959634780884 Accuracy 0.8271250128746033\n",
      "Output tensor([[0.0321],\n",
      "        [0.8047]], device='mps:0')\n",
      "Iteration 20760 Training loss 0.05914221704006195 Validation loss 0.062457673251628876 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.8813],\n",
      "        [0.9799]], device='mps:0')\n",
      "Iteration 20770 Training loss 0.058518026024103165 Validation loss 0.062432389706373215 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.0175],\n",
      "        [0.8458]], device='mps:0')\n",
      "Iteration 20780 Training loss 0.05922630801796913 Validation loss 0.06246820092201233 Accuracy 0.827750027179718\n",
      "Output tensor([[0.5536],\n",
      "        [0.0710]], device='mps:0')\n",
      "Iteration 20790 Training loss 0.06003428250551224 Validation loss 0.062436312437057495 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.6423],\n",
      "        [0.1249]], device='mps:0')\n",
      "Iteration 20800 Training loss 0.06689924746751785 Validation loss 0.062428105622529984 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.1851],\n",
      "        [0.9860]], device='mps:0')\n",
      "Iteration 20810 Training loss 0.06796003878116608 Validation loss 0.062472064048051834 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.9005],\n",
      "        [0.0937]], device='mps:0')\n",
      "Iteration 20820 Training loss 0.054929085075855255 Validation loss 0.06247122585773468 Accuracy 0.82750004529953\n",
      "Output tensor([[0.7400],\n",
      "        [0.6600]], device='mps:0')\n",
      "Iteration 20830 Training loss 0.05332806333899498 Validation loss 0.062430426478385925 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.0652],\n",
      "        [0.5168]], device='mps:0')\n",
      "Iteration 20840 Training loss 0.06223487854003906 Validation loss 0.06243868172168732 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.0647],\n",
      "        [0.0407]], device='mps:0')\n",
      "Iteration 20850 Training loss 0.05660418048501015 Validation loss 0.06242476776242256 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.9571],\n",
      "        [0.9366]], device='mps:0')\n",
      "Iteration 20860 Training loss 0.06498749554157257 Validation loss 0.06265170127153397 Accuracy 0.827250063419342\n",
      "Output tensor([[0.0504],\n",
      "        [0.8469]], device='mps:0')\n",
      "Iteration 20870 Training loss 0.0528443418443203 Validation loss 0.062413379549980164 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.2507],\n",
      "        [0.0337]], device='mps:0')\n",
      "Iteration 20880 Training loss 0.0631357803940773 Validation loss 0.06252596527338028 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.8950],\n",
      "        [0.8419]], device='mps:0')\n",
      "Iteration 20890 Training loss 0.07480070739984512 Validation loss 0.06241736188530922 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.7445],\n",
      "        [0.1579]], device='mps:0')\n",
      "Iteration 20900 Training loss 0.06439416855573654 Validation loss 0.06272593140602112 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.9797],\n",
      "        [0.8843]], device='mps:0')\n",
      "Iteration 20910 Training loss 0.06074334308505058 Validation loss 0.062484703958034515 Accuracy 0.827375054359436\n",
      "Output tensor([[0.7281],\n",
      "        [0.0896]], device='mps:0')\n",
      "Iteration 20920 Training loss 0.05889692157506943 Validation loss 0.06241795793175697 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.9145],\n",
      "        [0.8415]], device='mps:0')\n",
      "Iteration 20930 Training loss 0.060043055564165115 Validation loss 0.062410760670900345 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.9547],\n",
      "        [0.3581]], device='mps:0')\n",
      "Iteration 20940 Training loss 0.05818859487771988 Validation loss 0.06239444389939308 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9587],\n",
      "        [0.3906]], device='mps:0')\n",
      "Iteration 20950 Training loss 0.06536399573087692 Validation loss 0.0624137818813324 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.9278],\n",
      "        [0.6224]], device='mps:0')\n",
      "Iteration 20960 Training loss 0.05486464500427246 Validation loss 0.06243305653333664 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.6654],\n",
      "        [0.4732]], device='mps:0')\n",
      "Iteration 20970 Training loss 0.06527343392372131 Validation loss 0.06251296401023865 Accuracy 0.827250063419342\n",
      "Output tensor([[0.7540],\n",
      "        [0.4496]], device='mps:0')\n",
      "Iteration 20980 Training loss 0.05132322758436203 Validation loss 0.06240345165133476 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.8558],\n",
      "        [0.0434]], device='mps:0')\n",
      "Iteration 20990 Training loss 0.0655708760023117 Validation loss 0.062380846589803696 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.5773],\n",
      "        [0.3475]], device='mps:0')\n",
      "Iteration 21000 Training loss 0.05344070866703987 Validation loss 0.06238265708088875 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.7374],\n",
      "        [0.2438]], device='mps:0')\n",
      "Iteration 21010 Training loss 0.06212977319955826 Validation loss 0.0623948760330677 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0127],\n",
      "        [0.9529]], device='mps:0')\n",
      "Iteration 21020 Training loss 0.06846599280834198 Validation loss 0.06240398809313774 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9362],\n",
      "        [0.1592]], device='mps:0')\n",
      "Iteration 21030 Training loss 0.06016136333346367 Validation loss 0.0624002180993557 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0869],\n",
      "        [0.3290]], device='mps:0')\n",
      "Iteration 21040 Training loss 0.0667625293135643 Validation loss 0.06240071356296539 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.1528],\n",
      "        [0.8459]], device='mps:0')\n",
      "Iteration 21050 Training loss 0.05538208782672882 Validation loss 0.06242901086807251 Accuracy 0.830625057220459\n",
      "Output tensor([[0.1194],\n",
      "        [0.9619]], device='mps:0')\n",
      "Iteration 21060 Training loss 0.06759537011384964 Validation loss 0.062376879155635834 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.2530],\n",
      "        [0.0154]], device='mps:0')\n",
      "Iteration 21070 Training loss 0.0678933784365654 Validation loss 0.062412623316049576 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.1371],\n",
      "        [0.7467]], device='mps:0')\n",
      "Iteration 21080 Training loss 0.05255762115120888 Validation loss 0.062452707439661026 Accuracy 0.827250063419342\n",
      "Output tensor([[0.5155],\n",
      "        [0.5850]], device='mps:0')\n",
      "Iteration 21090 Training loss 0.05372463911771774 Validation loss 0.062379367649555206 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.0364],\n",
      "        [0.4285]], device='mps:0')\n",
      "Iteration 21100 Training loss 0.06575323641300201 Validation loss 0.062374163419008255 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.9886],\n",
      "        [0.3224]], device='mps:0')\n",
      "Iteration 21110 Training loss 0.058703575283288956 Validation loss 0.062372397631406784 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.9334],\n",
      "        [0.9754]], device='mps:0')\n",
      "Iteration 21120 Training loss 0.06581602990627289 Validation loss 0.06235601380467415 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.5433],\n",
      "        [0.8152]], device='mps:0')\n",
      "Iteration 21130 Training loss 0.06875010579824448 Validation loss 0.06240047141909599 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.1508],\n",
      "        [0.4766]], device='mps:0')\n",
      "Iteration 21140 Training loss 0.061988867819309235 Validation loss 0.06239430978894234 Accuracy 0.827875018119812\n",
      "Output tensor([[0.2793],\n",
      "        [0.5874]], device='mps:0')\n",
      "Iteration 21150 Training loss 0.0618562288582325 Validation loss 0.06234518811106682 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.1437],\n",
      "        [0.9409]], device='mps:0')\n",
      "Iteration 21160 Training loss 0.058046478778123856 Validation loss 0.062374092638492584 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.4400],\n",
      "        [0.9612]], device='mps:0')\n",
      "Iteration 21170 Training loss 0.06632982194423676 Validation loss 0.062348973006010056 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.9478],\n",
      "        [0.6266]], device='mps:0')\n",
      "Iteration 21180 Training loss 0.06368160247802734 Validation loss 0.06234364211559296 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.2238],\n",
      "        [0.0634]], device='mps:0')\n",
      "Iteration 21190 Training loss 0.06402561068534851 Validation loss 0.062351830303668976 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.3750],\n",
      "        [0.8809]], device='mps:0')\n",
      "Iteration 21200 Training loss 0.06139256805181503 Validation loss 0.062334802001714706 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.0049],\n",
      "        [0.5093]], device='mps:0')\n",
      "Iteration 21210 Training loss 0.062010958790779114 Validation loss 0.062333591282367706 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.7982],\n",
      "        [0.1674]], device='mps:0')\n",
      "Iteration 21220 Training loss 0.06672206521034241 Validation loss 0.06242341548204422 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.1470],\n",
      "        [0.1413]], device='mps:0')\n",
      "Iteration 21230 Training loss 0.06110610440373421 Validation loss 0.06260361522436142 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.9726],\n",
      "        [0.3835]], device='mps:0')\n",
      "Iteration 21240 Training loss 0.057578835636377335 Validation loss 0.06265447288751602 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.1411],\n",
      "        [0.9941]], device='mps:0')\n",
      "Iteration 21250 Training loss 0.06585048884153366 Validation loss 0.062321726232767105 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.8880],\n",
      "        [0.9864]], device='mps:0')\n",
      "Iteration 21260 Training loss 0.05497398227453232 Validation loss 0.06238430365920067 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.9908],\n",
      "        [0.7786]], device='mps:0')\n",
      "Iteration 21270 Training loss 0.06664986163377762 Validation loss 0.062333885580301285 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.5291],\n",
      "        [0.4484]], device='mps:0')\n",
      "Iteration 21280 Training loss 0.06903544068336487 Validation loss 0.062461286783218384 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.1289],\n",
      "        [0.7515]], device='mps:0')\n",
      "Iteration 21290 Training loss 0.05898573622107506 Validation loss 0.06242673844099045 Accuracy 0.827750027179718\n",
      "Output tensor([[0.9844],\n",
      "        [0.9097]], device='mps:0')\n",
      "Iteration 21300 Training loss 0.05052272602915764 Validation loss 0.06245626136660576 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.7520],\n",
      "        [0.8038]], device='mps:0')\n",
      "Iteration 21310 Training loss 0.06759850680828094 Validation loss 0.06233333796262741 Accuracy 0.830500066280365\n",
      "Output tensor([[0.8711],\n",
      "        [0.2950]], device='mps:0')\n",
      "Iteration 21320 Training loss 0.060824573040008545 Validation loss 0.062303684651851654 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.8724],\n",
      "        [0.5602]], device='mps:0')\n",
      "Iteration 21330 Training loss 0.06137874349951744 Validation loss 0.062300700694322586 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.2362],\n",
      "        [0.0769]], device='mps:0')\n",
      "Iteration 21340 Training loss 0.06964731216430664 Validation loss 0.062304094433784485 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.7194],\n",
      "        [0.5876]], device='mps:0')\n",
      "Iteration 21350 Training loss 0.060166873037815094 Validation loss 0.062305379658937454 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.1153],\n",
      "        [0.0176]], device='mps:0')\n",
      "Iteration 21360 Training loss 0.06003441661596298 Validation loss 0.06254367530345917 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.9475],\n",
      "        [0.7543]], device='mps:0')\n",
      "Iteration 21370 Training loss 0.06055723875761032 Validation loss 0.0623396635055542 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.3137],\n",
      "        [0.1089]], device='mps:0')\n",
      "Iteration 21380 Training loss 0.06915171444416046 Validation loss 0.062368638813495636 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.1376],\n",
      "        [0.0290]], device='mps:0')\n",
      "Iteration 21390 Training loss 0.06782052665948868 Validation loss 0.06268548965454102 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.3502],\n",
      "        [0.9665]], device='mps:0')\n",
      "Iteration 21400 Training loss 0.06209534406661987 Validation loss 0.06244248151779175 Accuracy 0.830500066280365\n",
      "Output tensor([[0.1195],\n",
      "        [0.6486]], device='mps:0')\n",
      "Iteration 21410 Training loss 0.054241474717855453 Validation loss 0.062460023909807205 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8335],\n",
      "        [0.1136]], device='mps:0')\n",
      "Iteration 21420 Training loss 0.058381013572216034 Validation loss 0.06229652464389801 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.9718],\n",
      "        [0.5206]], device='mps:0')\n",
      "Iteration 21430 Training loss 0.060899924486875534 Validation loss 0.0623159222304821 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.4357],\n",
      "        [0.9414]], device='mps:0')\n",
      "Iteration 21440 Training loss 0.06316204369068146 Validation loss 0.062392886728048325 Accuracy 0.827375054359436\n",
      "Output tensor([[0.9427],\n",
      "        [0.8817]], device='mps:0')\n",
      "Iteration 21450 Training loss 0.054337404668331146 Validation loss 0.06232603266835213 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.0369],\n",
      "        [0.0486]], device='mps:0')\n",
      "Iteration 21460 Training loss 0.06286866962909698 Validation loss 0.06230120733380318 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.9808],\n",
      "        [0.9850]], device='mps:0')\n",
      "Iteration 21470 Training loss 0.06949732452630997 Validation loss 0.062295474112033844 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.1733],\n",
      "        [0.8972]], device='mps:0')\n",
      "Iteration 21480 Training loss 0.06755796074867249 Validation loss 0.062328241765499115 Accuracy 0.830500066280365\n",
      "Output tensor([[0.2189],\n",
      "        [0.7531]], device='mps:0')\n",
      "Iteration 21490 Training loss 0.06291365623474121 Validation loss 0.062377769500017166 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.7291],\n",
      "        [0.5276]], device='mps:0')\n",
      "Iteration 21500 Training loss 0.05941347777843475 Validation loss 0.062298502773046494 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.0568],\n",
      "        [0.0730]], device='mps:0')\n",
      "Iteration 21510 Training loss 0.060860149562358856 Validation loss 0.06229334697127342 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.0641],\n",
      "        [0.2342]], device='mps:0')\n",
      "Iteration 21520 Training loss 0.06422121077775955 Validation loss 0.06227505952119827 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.7335],\n",
      "        [0.9744]], device='mps:0')\n",
      "Iteration 21530 Training loss 0.05932964012026787 Validation loss 0.062277901917696 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.4667],\n",
      "        [0.0339]], device='mps:0')\n",
      "Iteration 21540 Training loss 0.0702296793460846 Validation loss 0.062588632106781 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.3363],\n",
      "        [0.9655]], device='mps:0')\n",
      "Iteration 21550 Training loss 0.062084272503852844 Validation loss 0.06237689033150673 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.4127],\n",
      "        [0.5138]], device='mps:0')\n",
      "Iteration 21560 Training loss 0.060787975788116455 Validation loss 0.06245444715023041 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.6987],\n",
      "        [0.3966]], device='mps:0')\n",
      "Iteration 21570 Training loss 0.05516122654080391 Validation loss 0.062463726848363876 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.1310],\n",
      "        [0.2694]], device='mps:0')\n",
      "Iteration 21580 Training loss 0.06263817846775055 Validation loss 0.06264305114746094 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.9021],\n",
      "        [0.2193]], device='mps:0')\n",
      "Iteration 21590 Training loss 0.0581485778093338 Validation loss 0.062279216945171356 Accuracy 0.830500066280365\n",
      "Output tensor([[0.0590],\n",
      "        [0.7373]], device='mps:0')\n",
      "Iteration 21600 Training loss 0.055311497300863266 Validation loss 0.062267713248729706 Accuracy 0.830875039100647\n",
      "Output tensor([[0.1075],\n",
      "        [0.5421]], device='mps:0')\n",
      "Iteration 21610 Training loss 0.06279006600379944 Validation loss 0.06225765123963356 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.1114],\n",
      "        [0.9194]], device='mps:0')\n",
      "Iteration 21620 Training loss 0.06114456057548523 Validation loss 0.06226812303066254 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.8149],\n",
      "        [0.9552]], device='mps:0')\n",
      "Iteration 21630 Training loss 0.0615994818508625 Validation loss 0.06232456490397453 Accuracy 0.830750048160553\n",
      "Output tensor([[0.3484],\n",
      "        [0.7244]], device='mps:0')\n",
      "Iteration 21640 Training loss 0.06709448993206024 Validation loss 0.062251027673482895 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.8351],\n",
      "        [0.5284]], device='mps:0')\n",
      "Iteration 21650 Training loss 0.06491272896528244 Validation loss 0.06224827840924263 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.0261],\n",
      "        [0.5852]], device='mps:0')\n",
      "Iteration 21660 Training loss 0.055234022438526154 Validation loss 0.06226661801338196 Accuracy 0.830625057220459\n",
      "Output tensor([[0.0369],\n",
      "        [0.0287]], device='mps:0')\n",
      "Iteration 21670 Training loss 0.060444679111242294 Validation loss 0.062310896813869476 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.2113],\n",
      "        [0.9604]], device='mps:0')\n",
      "Iteration 21680 Training loss 0.060773592442274094 Validation loss 0.06225130707025528 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.1862],\n",
      "        [0.9553]], device='mps:0')\n",
      "Iteration 21690 Training loss 0.06530047208070755 Validation loss 0.06239398196339607 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8680],\n",
      "        [0.5946]], device='mps:0')\n",
      "Iteration 21700 Training loss 0.06331901252269745 Validation loss 0.06224558874964714 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.7592],\n",
      "        [0.7245]], device='mps:0')\n",
      "Iteration 21710 Training loss 0.058857209980487823 Validation loss 0.06225426122546196 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.9002],\n",
      "        [0.0744]], device='mps:0')\n",
      "Iteration 21720 Training loss 0.05886189639568329 Validation loss 0.0625699833035469 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.0040],\n",
      "        [0.1393]], device='mps:0')\n",
      "Iteration 21730 Training loss 0.05403442308306694 Validation loss 0.06231957674026489 Accuracy 0.830625057220459\n",
      "Output tensor([[0.4632],\n",
      "        [0.0986]], device='mps:0')\n",
      "Iteration 21740 Training loss 0.05746905133128166 Validation loss 0.06223659589886665 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.1450],\n",
      "        [0.9626]], device='mps:0')\n",
      "Iteration 21750 Training loss 0.06652478873729706 Validation loss 0.06229265406727791 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.9136],\n",
      "        [0.2843]], device='mps:0')\n",
      "Iteration 21760 Training loss 0.06167910620570183 Validation loss 0.062420863658189774 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.9869],\n",
      "        [0.1962]], device='mps:0')\n",
      "Iteration 21770 Training loss 0.05799192190170288 Validation loss 0.0622592568397522 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.3846],\n",
      "        [0.1824]], device='mps:0')\n",
      "Iteration 21780 Training loss 0.06260020285844803 Validation loss 0.062301527708768845 Accuracy 0.830750048160553\n",
      "Output tensor([[0.1314],\n",
      "        [0.1208]], device='mps:0')\n",
      "Iteration 21790 Training loss 0.057450491935014725 Validation loss 0.06223036348819733 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.0246],\n",
      "        [0.9686]], device='mps:0')\n",
      "Iteration 21800 Training loss 0.07135237753391266 Validation loss 0.06228956580162048 Accuracy 0.830500066280365\n",
      "Output tensor([[0.0975],\n",
      "        [0.1074]], device='mps:0')\n",
      "Iteration 21810 Training loss 0.06835899502038956 Validation loss 0.06225300952792168 Accuracy 0.830625057220459\n",
      "Output tensor([[0.0583],\n",
      "        [0.2400]], device='mps:0')\n",
      "Iteration 21820 Training loss 0.05094210058450699 Validation loss 0.06261337548494339 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.0872],\n",
      "        [0.0216]], device='mps:0')\n",
      "Iteration 21830 Training loss 0.05820826441049576 Validation loss 0.062356531620025635 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.0283],\n",
      "        [0.5648]], device='mps:0')\n",
      "Iteration 21840 Training loss 0.057220544666051865 Validation loss 0.062312860041856766 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.3427],\n",
      "        [0.9577]], device='mps:0')\n",
      "Iteration 21850 Training loss 0.06122914329171181 Validation loss 0.06222495809197426 Accuracy 0.830750048160553\n",
      "Output tensor([[0.2207],\n",
      "        [0.9655]], device='mps:0')\n",
      "Iteration 21860 Training loss 0.057864122092723846 Validation loss 0.06225951761007309 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.0764],\n",
      "        [0.2065]], device='mps:0')\n",
      "Iteration 21870 Training loss 0.0585080049932003 Validation loss 0.062207385897636414 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.8993],\n",
      "        [0.4415]], device='mps:0')\n",
      "Iteration 21880 Training loss 0.05555255711078644 Validation loss 0.062242500483989716 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.2129],\n",
      "        [0.5505]], device='mps:0')\n",
      "Iteration 21890 Training loss 0.06361661851406097 Validation loss 0.06225639581680298 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.0692],\n",
      "        [0.3781]], device='mps:0')\n",
      "Iteration 21900 Training loss 0.0636986494064331 Validation loss 0.062320902943611145 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.7208],\n",
      "        [0.2729]], device='mps:0')\n",
      "Iteration 21910 Training loss 0.06515925377607346 Validation loss 0.0622861422598362 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.8710],\n",
      "        [0.6309]], device='mps:0')\n",
      "Iteration 21920 Training loss 0.061408184468746185 Validation loss 0.062247395515441895 Accuracy 0.830500066280365\n",
      "Output tensor([[0.0785],\n",
      "        [0.5830]], device='mps:0')\n",
      "Iteration 21930 Training loss 0.06243244186043739 Validation loss 0.062197666615247726 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.6549],\n",
      "        [0.5885]], device='mps:0')\n",
      "Iteration 21940 Training loss 0.06303441524505615 Validation loss 0.062211550772190094 Accuracy 0.830750048160553\n",
      "Output tensor([[0.8420],\n",
      "        [0.8387]], device='mps:0')\n",
      "Iteration 21950 Training loss 0.058316148817539215 Validation loss 0.062191225588321686 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.9248],\n",
      "        [0.9486]], device='mps:0')\n",
      "Iteration 21960 Training loss 0.05923508480191231 Validation loss 0.06220931187272072 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.6398],\n",
      "        [0.7670]], device='mps:0')\n",
      "Iteration 21970 Training loss 0.05339822545647621 Validation loss 0.062832772731781 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.4002],\n",
      "        [0.8915]], device='mps:0')\n",
      "Iteration 21980 Training loss 0.05753125622868538 Validation loss 0.06228473782539368 Accuracy 0.830625057220459\n",
      "Output tensor([[0.2012],\n",
      "        [0.9098]], device='mps:0')\n",
      "Iteration 21990 Training loss 0.06943884491920471 Validation loss 0.06238367781043053 Accuracy 0.82750004529953\n",
      "Output tensor([[0.5992],\n",
      "        [0.7943]], device='mps:0')\n",
      "Iteration 22000 Training loss 0.06222250685095787 Validation loss 0.062276989221572876 Accuracy 0.830500066280365\n",
      "Output tensor([[0.0957],\n",
      "        [0.2466]], device='mps:0')\n",
      "Iteration 22010 Training loss 0.06204397231340408 Validation loss 0.06226081773638725 Accuracy 0.830625057220459\n",
      "Output tensor([[0.1807],\n",
      "        [0.6511]], device='mps:0')\n",
      "Iteration 22020 Training loss 0.05726264789700508 Validation loss 0.0625181719660759 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.2901],\n",
      "        [0.9565]], device='mps:0')\n",
      "Iteration 22030 Training loss 0.059528253972530365 Validation loss 0.062293633818626404 Accuracy 0.830500066280365\n",
      "Output tensor([[0.9554],\n",
      "        [0.8875]], device='mps:0')\n",
      "Iteration 22040 Training loss 0.06455730646848679 Validation loss 0.062168825417757034 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.1853],\n",
      "        [0.0476]], device='mps:0')\n",
      "Iteration 22050 Training loss 0.06299439072608948 Validation loss 0.06225379928946495 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.1855],\n",
      "        [0.0714]], device='mps:0')\n",
      "Iteration 22060 Training loss 0.05661439895629883 Validation loss 0.06223791837692261 Accuracy 0.831000030040741\n",
      "Output tensor([[0.6563],\n",
      "        [0.9313]], device='mps:0')\n",
      "Iteration 22070 Training loss 0.06389591842889786 Validation loss 0.06216323375701904 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.3059],\n",
      "        [0.5656]], device='mps:0')\n",
      "Iteration 22080 Training loss 0.06813196837902069 Validation loss 0.06221800670027733 Accuracy 0.830625057220459\n",
      "Output tensor([[0.2029],\n",
      "        [0.0059]], device='mps:0')\n",
      "Iteration 22090 Training loss 0.06396809220314026 Validation loss 0.0621895007789135 Accuracy 0.831000030040741\n",
      "Output tensor([[0.6954],\n",
      "        [0.0469]], device='mps:0')\n",
      "Iteration 22100 Training loss 0.05700532719492912 Validation loss 0.06235973909497261 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.0118],\n",
      "        [0.8330]], device='mps:0')\n",
      "Iteration 22110 Training loss 0.05758480727672577 Validation loss 0.062160562723875046 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.5917],\n",
      "        [0.1318]], device='mps:0')\n",
      "Iteration 22120 Training loss 0.061973121017217636 Validation loss 0.062164440751075745 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.0319],\n",
      "        [0.3124]], device='mps:0')\n",
      "Iteration 22130 Training loss 0.06243998557329178 Validation loss 0.06216521933674812 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.2496],\n",
      "        [0.3555]], device='mps:0')\n",
      "Iteration 22140 Training loss 0.05863086134195328 Validation loss 0.06216544657945633 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.8807],\n",
      "        [0.5183]], device='mps:0')\n",
      "Iteration 22150 Training loss 0.05822783336043358 Validation loss 0.06220533326268196 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.2976],\n",
      "        [0.7904]], device='mps:0')\n",
      "Iteration 22160 Training loss 0.0600934736430645 Validation loss 0.06222354620695114 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.6135],\n",
      "        [0.0525]], device='mps:0')\n",
      "Iteration 22170 Training loss 0.06099867820739746 Validation loss 0.062155138701200485 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.0073],\n",
      "        [0.6454]], device='mps:0')\n",
      "Iteration 22180 Training loss 0.06381187587976456 Validation loss 0.06221828609704971 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.7909],\n",
      "        [0.1586]], device='mps:0')\n",
      "Iteration 22190 Training loss 0.0682298094034195 Validation loss 0.062194060534238815 Accuracy 0.831000030040741\n",
      "Output tensor([[0.6835],\n",
      "        [0.3998]], device='mps:0')\n",
      "Iteration 22200 Training loss 0.06008182466030121 Validation loss 0.06216644495725632 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.5375],\n",
      "        [0.3167]], device='mps:0')\n",
      "Iteration 22210 Training loss 0.056510474532842636 Validation loss 0.06237117201089859 Accuracy 0.827875018119812\n",
      "Output tensor([[0.8075],\n",
      "        [0.0185]], device='mps:0')\n",
      "Iteration 22220 Training loss 0.06212737038731575 Validation loss 0.06215280666947365 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.5227],\n",
      "        [0.1013]], device='mps:0')\n",
      "Iteration 22230 Training loss 0.05555257573723793 Validation loss 0.06232021749019623 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.4864],\n",
      "        [0.9301]], device='mps:0')\n",
      "Iteration 22240 Training loss 0.05286716669797897 Validation loss 0.06230859458446503 Accuracy 0.8280000686645508\n",
      "Output tensor([[0.9221],\n",
      "        [0.6573]], device='mps:0')\n",
      "Iteration 22250 Training loss 0.057004302740097046 Validation loss 0.06218723952770233 Accuracy 0.830625057220459\n",
      "Output tensor([[0.0635],\n",
      "        [0.9451]], device='mps:0')\n",
      "Iteration 22260 Training loss 0.051957033574581146 Validation loss 0.06217335909605026 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.5846],\n",
      "        [0.2381]], device='mps:0')\n",
      "Iteration 22270 Training loss 0.05668044835329056 Validation loss 0.06217486038804054 Accuracy 0.830625057220459\n",
      "Output tensor([[0.0440],\n",
      "        [0.5514]], device='mps:0')\n",
      "Iteration 22280 Training loss 0.056735843420028687 Validation loss 0.062231920659542084 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.9268],\n",
      "        [0.5684]], device='mps:0')\n",
      "Iteration 22290 Training loss 0.06617554277181625 Validation loss 0.06250174343585968 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8419],\n",
      "        [0.9525]], device='mps:0')\n",
      "Iteration 22300 Training loss 0.05963356792926788 Validation loss 0.062235027551651 Accuracy 0.830500066280365\n",
      "Output tensor([[0.7263],\n",
      "        [0.5592]], device='mps:0')\n",
      "Iteration 22310 Training loss 0.05838165059685707 Validation loss 0.062278665602207184 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.9398],\n",
      "        [0.5482]], device='mps:0')\n",
      "Iteration 22320 Training loss 0.057194311171770096 Validation loss 0.0621933788061142 Accuracy 0.831125020980835\n",
      "Output tensor([[0.9845],\n",
      "        [0.9184]], device='mps:0')\n",
      "Iteration 22330 Training loss 0.06042451784014702 Validation loss 0.06214301660656929 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.4954],\n",
      "        [0.6206]], device='mps:0')\n",
      "Iteration 22340 Training loss 0.0638110563158989 Validation loss 0.06213560327887535 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.8896],\n",
      "        [0.5320]], device='mps:0')\n",
      "Iteration 22350 Training loss 0.06079689413309097 Validation loss 0.06228131428360939 Accuracy 0.8281250596046448\n",
      "Output tensor([[0.6100],\n",
      "        [0.6614]], device='mps:0')\n",
      "Iteration 22360 Training loss 0.05792764946818352 Validation loss 0.06230514869093895 Accuracy 0.8282500505447388\n",
      "Output tensor([[0.6225],\n",
      "        [0.7427]], device='mps:0')\n",
      "Iteration 22370 Training loss 0.054770246148109436 Validation loss 0.06217356398701668 Accuracy 0.830875039100647\n",
      "Output tensor([[0.1751],\n",
      "        [0.0521]], device='mps:0')\n",
      "Iteration 22380 Training loss 0.05366264283657074 Validation loss 0.06252994388341904 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.3023],\n",
      "        [0.2326]], device='mps:0')\n",
      "Iteration 22390 Training loss 0.06588274985551834 Validation loss 0.06212092936038971 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.3105],\n",
      "        [0.5117]], device='mps:0')\n",
      "Iteration 22400 Training loss 0.06476926803588867 Validation loss 0.06212213635444641 Accuracy 0.830625057220459\n",
      "Output tensor([[0.9428],\n",
      "        [0.9649]], device='mps:0')\n",
      "Iteration 22410 Training loss 0.05503709614276886 Validation loss 0.062291521579027176 Accuracy 0.830500066280365\n",
      "Output tensor([[0.9947],\n",
      "        [0.2715]], device='mps:0')\n",
      "Iteration 22420 Training loss 0.06931775063276291 Validation loss 0.06211286783218384 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.5354],\n",
      "        [0.8140]], device='mps:0')\n",
      "Iteration 22430 Training loss 0.05917050689458847 Validation loss 0.062139011919498444 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.6090],\n",
      "        [0.9111]], device='mps:0')\n",
      "Iteration 22440 Training loss 0.05542382225394249 Validation loss 0.06215539947152138 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.1901],\n",
      "        [0.8412]], device='mps:0')\n",
      "Iteration 22450 Training loss 0.04994278773665428 Validation loss 0.06217467039823532 Accuracy 0.831125020980835\n",
      "Output tensor([[0.9375],\n",
      "        [0.6292]], device='mps:0')\n",
      "Iteration 22460 Training loss 0.05567426607012749 Validation loss 0.06212582066655159 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.7955],\n",
      "        [0.8412]], device='mps:0')\n",
      "Iteration 22470 Training loss 0.05467818304896355 Validation loss 0.06211739033460617 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.0593],\n",
      "        [0.9715]], device='mps:0')\n",
      "Iteration 22480 Training loss 0.06027507781982422 Validation loss 0.062116339802742004 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.9039],\n",
      "        [0.9924]], device='mps:0')\n",
      "Iteration 22490 Training loss 0.061324868351221085 Validation loss 0.06211300194263458 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.4946],\n",
      "        [0.2105]], device='mps:0')\n",
      "Iteration 22500 Training loss 0.061152175068855286 Validation loss 0.06214527785778046 Accuracy 0.831000030040741\n",
      "Output tensor([[0.0873],\n",
      "        [0.2108]], device='mps:0')\n",
      "Iteration 22510 Training loss 0.06336332857608795 Validation loss 0.06233340874314308 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.6470],\n",
      "        [0.4249]], device='mps:0')\n",
      "Iteration 22520 Training loss 0.05493355169892311 Validation loss 0.062103308737277985 Accuracy 0.830625057220459\n",
      "Output tensor([[0.4675],\n",
      "        [0.3843]], device='mps:0')\n",
      "Iteration 22530 Training loss 0.06416761130094528 Validation loss 0.06209902837872505 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.1848],\n",
      "        [0.8137]], device='mps:0')\n",
      "Iteration 22540 Training loss 0.06030340865254402 Validation loss 0.062119387090206146 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.4850],\n",
      "        [0.5109]], device='mps:0')\n",
      "Iteration 22550 Training loss 0.057612862437963486 Validation loss 0.06220228970050812 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.4576],\n",
      "        [0.0333]], device='mps:0')\n",
      "Iteration 22560 Training loss 0.06334976106882095 Validation loss 0.06209370121359825 Accuracy 0.830500066280365\n",
      "Output tensor([[0.2160],\n",
      "        [0.6937]], device='mps:0')\n",
      "Iteration 22570 Training loss 0.05262884125113487 Validation loss 0.06209900230169296 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.9683],\n",
      "        [0.0558]], device='mps:0')\n",
      "Iteration 22580 Training loss 0.055581796914339066 Validation loss 0.06208513304591179 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.8347],\n",
      "        [0.5038]], device='mps:0')\n",
      "Iteration 22590 Training loss 0.05214475467801094 Validation loss 0.062083639204502106 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.4263],\n",
      "        [0.3271]], device='mps:0')\n",
      "Iteration 22600 Training loss 0.06216869130730629 Validation loss 0.06215452775359154 Accuracy 0.830625057220459\n",
      "Output tensor([[0.2478],\n",
      "        [0.3161]], device='mps:0')\n",
      "Iteration 22610 Training loss 0.04987196996808052 Validation loss 0.062183819711208344 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.8356],\n",
      "        [0.5360]], device='mps:0')\n",
      "Iteration 22620 Training loss 0.06234977766871452 Validation loss 0.06229771301150322 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.0208],\n",
      "        [0.7972]], device='mps:0')\n",
      "Iteration 22630 Training loss 0.05921691656112671 Validation loss 0.0622212179005146 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.9222],\n",
      "        [0.0556]], device='mps:0')\n",
      "Iteration 22640 Training loss 0.06268715113401413 Validation loss 0.06211978569626808 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.9167],\n",
      "        [0.9522]], device='mps:0')\n",
      "Iteration 22650 Training loss 0.06272559612989426 Validation loss 0.062125805765390396 Accuracy 0.831000030040741\n",
      "Output tensor([[0.7506],\n",
      "        [0.9519]], device='mps:0')\n",
      "Iteration 22660 Training loss 0.049132298678159714 Validation loss 0.06207895651459694 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.2152],\n",
      "        [0.1360]], device='mps:0')\n",
      "Iteration 22670 Training loss 0.06433001160621643 Validation loss 0.06211795285344124 Accuracy 0.830750048160553\n",
      "Output tensor([[0.9283],\n",
      "        [0.1952]], device='mps:0')\n",
      "Iteration 22680 Training loss 0.05606245994567871 Validation loss 0.062137946486473083 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.8638],\n",
      "        [0.0939]], device='mps:0')\n",
      "Iteration 22690 Training loss 0.06681077927350998 Validation loss 0.06206214055418968 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.0429],\n",
      "        [0.8555]], device='mps:0')\n",
      "Iteration 22700 Training loss 0.06078372895717621 Validation loss 0.06216214969754219 Accuracy 0.827875018119812\n",
      "Output tensor([[0.0433],\n",
      "        [0.4074]], device='mps:0')\n",
      "Iteration 22710 Training loss 0.06088153272867203 Validation loss 0.06205714866518974 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.3958],\n",
      "        [0.0545]], device='mps:0')\n",
      "Iteration 22720 Training loss 0.057573407888412476 Validation loss 0.06205407902598381 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.9332],\n",
      "        [0.9121]], device='mps:0')\n",
      "Iteration 22730 Training loss 0.06136532500386238 Validation loss 0.062301214784383774 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.9283],\n",
      "        [0.8831]], device='mps:0')\n",
      "Iteration 22740 Training loss 0.06637797504663467 Validation loss 0.06209871172904968 Accuracy 0.830750048160553\n",
      "Output tensor([[0.0769],\n",
      "        [0.6525]], device='mps:0')\n",
      "Iteration 22750 Training loss 0.06357373297214508 Validation loss 0.062090303748846054 Accuracy 0.831125020980835\n",
      "Output tensor([[0.9655],\n",
      "        [0.2978]], device='mps:0')\n",
      "Iteration 22760 Training loss 0.05632873252034187 Validation loss 0.06204770877957344 Accuracy 0.830625057220459\n",
      "Output tensor([[0.1375],\n",
      "        [0.5933]], device='mps:0')\n",
      "Iteration 22770 Training loss 0.056677378714084625 Validation loss 0.062107883393764496 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.0579],\n",
      "        [0.9592]], device='mps:0')\n",
      "Iteration 22780 Training loss 0.06290477514266968 Validation loss 0.06209946796298027 Accuracy 0.830500066280365\n",
      "Output tensor([[0.8944],\n",
      "        [0.7804]], device='mps:0')\n",
      "Iteration 22790 Training loss 0.06233531981706619 Validation loss 0.06222119182348251 Accuracy 0.8286250233650208\n",
      "Output tensor([[0.5265],\n",
      "        [0.0374]], device='mps:0')\n",
      "Iteration 22800 Training loss 0.06130503490567207 Validation loss 0.06207840517163277 Accuracy 0.831125020980835\n",
      "Output tensor([[0.3092],\n",
      "        [0.2696]], device='mps:0')\n",
      "Iteration 22810 Training loss 0.06200104206800461 Validation loss 0.062059659510850906 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.9925],\n",
      "        [0.8196]], device='mps:0')\n",
      "Iteration 22820 Training loss 0.06321143358945847 Validation loss 0.06207476556301117 Accuracy 0.831125020980835\n",
      "Output tensor([[0.7512],\n",
      "        [0.8533]], device='mps:0')\n",
      "Iteration 22830 Training loss 0.060261838138103485 Validation loss 0.062040165066719055 Accuracy 0.830500066280365\n",
      "Output tensor([[0.9760],\n",
      "        [0.8339]], device='mps:0')\n",
      "Iteration 22840 Training loss 0.07064059376716614 Validation loss 0.06223209574818611 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.9056],\n",
      "        [0.0621]], device='mps:0')\n",
      "Iteration 22850 Training loss 0.06672152876853943 Validation loss 0.0620763897895813 Accuracy 0.830875039100647\n",
      "Output tensor([[0.3484],\n",
      "        [0.2043]], device='mps:0')\n",
      "Iteration 22860 Training loss 0.06844528764486313 Validation loss 0.062101706862449646 Accuracy 0.830750048160553\n",
      "Output tensor([[0.2399],\n",
      "        [0.0424]], device='mps:0')\n",
      "Iteration 22870 Training loss 0.058831434696912766 Validation loss 0.06210636720061302 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.2733],\n",
      "        [0.9584]], device='mps:0')\n",
      "Iteration 22880 Training loss 0.06058431416749954 Validation loss 0.062079057097435 Accuracy 0.830750048160553\n",
      "Output tensor([[0.6336],\n",
      "        [0.8874]], device='mps:0')\n",
      "Iteration 22890 Training loss 0.05839692801237106 Validation loss 0.06238068640232086 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.5481],\n",
      "        [0.4459]], device='mps:0')\n",
      "Iteration 22900 Training loss 0.06625575572252274 Validation loss 0.06205250695347786 Accuracy 0.8290000557899475\n",
      "Output tensor([[0.1104],\n",
      "        [0.9948]], device='mps:0')\n",
      "Iteration 22910 Training loss 0.05719912052154541 Validation loss 0.06213627755641937 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.3585],\n",
      "        [0.9037]], device='mps:0')\n",
      "Iteration 22920 Training loss 0.06933343410491943 Validation loss 0.062079258263111115 Accuracy 0.830625057220459\n",
      "Output tensor([[0.0405],\n",
      "        [0.0948]], device='mps:0')\n",
      "Iteration 22930 Training loss 0.0664328783750534 Validation loss 0.06208392232656479 Accuracy 0.830500066280365\n",
      "Output tensor([[0.7506],\n",
      "        [0.6087]], device='mps:0')\n",
      "Iteration 22940 Training loss 0.05887189880013466 Validation loss 0.06203792989253998 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.8250],\n",
      "        [0.0475]], device='mps:0')\n",
      "Iteration 22950 Training loss 0.06805748492479324 Validation loss 0.062204014509916306 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.8544],\n",
      "        [0.4249]], device='mps:0')\n",
      "Iteration 22960 Training loss 0.06114194914698601 Validation loss 0.062111567705869675 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.4846],\n",
      "        [0.5083]], device='mps:0')\n",
      "Iteration 22970 Training loss 0.0546414889395237 Validation loss 0.062177032232284546 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.3940],\n",
      "        [0.9637]], device='mps:0')\n",
      "Iteration 22980 Training loss 0.059077247977256775 Validation loss 0.062025416642427444 Accuracy 0.831250011920929\n",
      "Output tensor([[0.0312],\n",
      "        [0.0275]], device='mps:0')\n",
      "Iteration 22990 Training loss 0.060705170035362244 Validation loss 0.06200841814279556 Accuracy 0.830500066280365\n",
      "Output tensor([[0.6624],\n",
      "        [0.8944]], device='mps:0')\n",
      "Iteration 23000 Training loss 0.057427968829870224 Validation loss 0.06201392784714699 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.7351],\n",
      "        [0.9419]], device='mps:0')\n",
      "Iteration 23010 Training loss 0.05207077041268349 Validation loss 0.062031522393226624 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.9660],\n",
      "        [0.9116]], device='mps:0')\n",
      "Iteration 23020 Training loss 0.055126991122961044 Validation loss 0.06218625605106354 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.5626],\n",
      "        [0.9800]], device='mps:0')\n",
      "Iteration 23030 Training loss 0.060969989746809006 Validation loss 0.06207156181335449 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.8915],\n",
      "        [0.3158]], device='mps:0')\n",
      "Iteration 23040 Training loss 0.058616142719984055 Validation loss 0.06236017495393753 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.1449],\n",
      "        [0.1122]], device='mps:0')\n",
      "Iteration 23050 Training loss 0.049053046852350235 Validation loss 0.06238168478012085 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.3065],\n",
      "        [0.5038]], device='mps:0')\n",
      "Iteration 23060 Training loss 0.050152648240327835 Validation loss 0.06204497814178467 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.6090],\n",
      "        [0.9958]], device='mps:0')\n",
      "Iteration 23070 Training loss 0.0603269562125206 Validation loss 0.06200475990772247 Accuracy 0.831250011920929\n",
      "Output tensor([[0.9904],\n",
      "        [0.8535]], device='mps:0')\n",
      "Iteration 23080 Training loss 0.06332346051931381 Validation loss 0.062012914568185806 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.2422],\n",
      "        [0.1889]], device='mps:0')\n",
      "Iteration 23090 Training loss 0.06264514476060867 Validation loss 0.06199445575475693 Accuracy 0.831250011920929\n",
      "Output tensor([[0.7148],\n",
      "        [0.9798]], device='mps:0')\n",
      "Iteration 23100 Training loss 0.049024056643247604 Validation loss 0.06202469766139984 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.3709],\n",
      "        [0.1647]], device='mps:0')\n",
      "Iteration 23110 Training loss 0.06038033217191696 Validation loss 0.06210210174322128 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.5191],\n",
      "        [0.9679]], device='mps:0')\n",
      "Iteration 23120 Training loss 0.056606683880090714 Validation loss 0.06211838871240616 Accuracy 0.8283750414848328\n",
      "Output tensor([[0.0490],\n",
      "        [0.5588]], device='mps:0')\n",
      "Iteration 23130 Training loss 0.06432696431875229 Validation loss 0.06200139597058296 Accuracy 0.831125020980835\n",
      "Output tensor([[0.3130],\n",
      "        [0.9807]], device='mps:0')\n",
      "Iteration 23140 Training loss 0.058382920920848846 Validation loss 0.061993371695280075 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.8383],\n",
      "        [0.0103]], device='mps:0')\n",
      "Iteration 23150 Training loss 0.048356909304857254 Validation loss 0.06252548098564148 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.0647],\n",
      "        [0.5461]], device='mps:0')\n",
      "Iteration 23160 Training loss 0.055659350007772446 Validation loss 0.06200561672449112 Accuracy 0.830750048160553\n",
      "Output tensor([[0.0586],\n",
      "        [0.9644]], device='mps:0')\n",
      "Iteration 23170 Training loss 0.06649928539991379 Validation loss 0.061981450766325 Accuracy 0.830750048160553\n",
      "Output tensor([[0.7244],\n",
      "        [0.0336]], device='mps:0')\n",
      "Iteration 23180 Training loss 0.06576036661863327 Validation loss 0.062090855091810226 Accuracy 0.831125020980835\n",
      "Output tensor([[0.9847],\n",
      "        [0.8878]], device='mps:0')\n",
      "Iteration 23190 Training loss 0.0553288497030735 Validation loss 0.06197640672326088 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.8463],\n",
      "        [0.8769]], device='mps:0')\n",
      "Iteration 23200 Training loss 0.05634437873959541 Validation loss 0.061973780393600464 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.2050],\n",
      "        [0.0289]], device='mps:0')\n",
      "Iteration 23210 Training loss 0.0560755729675293 Validation loss 0.06207764521241188 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.8918],\n",
      "        [0.9590]], device='mps:0')\n",
      "Iteration 23220 Training loss 0.05739748850464821 Validation loss 0.06196225434541702 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.3415],\n",
      "        [0.7100]], device='mps:0')\n",
      "Iteration 23230 Training loss 0.05385920777916908 Validation loss 0.062024716287851334 Accuracy 0.831000030040741\n",
      "Output tensor([[0.3253],\n",
      "        [0.6147]], device='mps:0')\n",
      "Iteration 23240 Training loss 0.06012675166130066 Validation loss 0.06196034699678421 Accuracy 0.830500066280365\n",
      "Output tensor([[0.1546],\n",
      "        [0.0760]], device='mps:0')\n",
      "Iteration 23250 Training loss 0.06088864058256149 Validation loss 0.06198902428150177 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.5563],\n",
      "        [0.7050]], device='mps:0')\n",
      "Iteration 23260 Training loss 0.05554669350385666 Validation loss 0.06196291372179985 Accuracy 0.830875039100647\n",
      "Output tensor([[0.2622],\n",
      "        [0.0246]], device='mps:0')\n",
      "Iteration 23270 Training loss 0.05433731898665428 Validation loss 0.061957165598869324 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.6279],\n",
      "        [0.2453]], device='mps:0')\n",
      "Iteration 23280 Training loss 0.054690923541784286 Validation loss 0.06205182522535324 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.4820],\n",
      "        [0.7858]], device='mps:0')\n",
      "Iteration 23290 Training loss 0.06180508807301521 Validation loss 0.06202571466565132 Accuracy 0.830875039100647\n",
      "Output tensor([[0.2416],\n",
      "        [0.8771]], device='mps:0')\n",
      "Iteration 23300 Training loss 0.06321984529495239 Validation loss 0.06195135787129402 Accuracy 0.830750048160553\n",
      "Output tensor([[0.0727],\n",
      "        [0.9613]], device='mps:0')\n",
      "Iteration 23310 Training loss 0.06595920771360397 Validation loss 0.06203078851103783 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.1295],\n",
      "        [0.0177]], device='mps:0')\n",
      "Iteration 23320 Training loss 0.061469901353120804 Validation loss 0.0619552843272686 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.1159],\n",
      "        [0.9529]], device='mps:0')\n",
      "Iteration 23330 Training loss 0.054912906140089035 Validation loss 0.06197310611605644 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.9346],\n",
      "        [0.0547]], device='mps:0')\n",
      "Iteration 23340 Training loss 0.06620372831821442 Validation loss 0.06197617948055267 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.9317],\n",
      "        [0.0606]], device='mps:0')\n",
      "Iteration 23350 Training loss 0.05647504702210426 Validation loss 0.06194593757390976 Accuracy 0.830875039100647\n",
      "Output tensor([[0.5926],\n",
      "        [0.9275]], device='mps:0')\n",
      "Iteration 23360 Training loss 0.058163151144981384 Validation loss 0.061943843960762024 Accuracy 0.830750048160553\n",
      "Output tensor([[0.1057],\n",
      "        [0.0412]], device='mps:0')\n",
      "Iteration 23370 Training loss 0.058876827359199524 Validation loss 0.06194036453962326 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.8566],\n",
      "        [0.0190]], device='mps:0')\n",
      "Iteration 23380 Training loss 0.0630086213350296 Validation loss 0.061956167221069336 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.7934],\n",
      "        [0.0343]], device='mps:0')\n",
      "Iteration 23390 Training loss 0.059714145958423615 Validation loss 0.061950553208589554 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.8740],\n",
      "        [0.1033]], device='mps:0')\n",
      "Iteration 23400 Training loss 0.05661306902766228 Validation loss 0.06197582557797432 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.0467],\n",
      "        [0.9106]], device='mps:0')\n",
      "Iteration 23410 Training loss 0.0581536702811718 Validation loss 0.06197653338313103 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.6464],\n",
      "        [0.4491]], device='mps:0')\n",
      "Iteration 23420 Training loss 0.0647122859954834 Validation loss 0.061950746923685074 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.9027],\n",
      "        [0.7326]], device='mps:0')\n",
      "Iteration 23430 Training loss 0.049528591334819794 Validation loss 0.06203041225671768 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.8907],\n",
      "        [0.5743]], device='mps:0')\n",
      "Iteration 23440 Training loss 0.06367628276348114 Validation loss 0.061975229531526566 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.1308],\n",
      "        [0.2188]], device='mps:0')\n",
      "Iteration 23450 Training loss 0.05343090370297432 Validation loss 0.06192879378795624 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.2452],\n",
      "        [0.7750]], device='mps:0')\n",
      "Iteration 23460 Training loss 0.063590407371521 Validation loss 0.0620100200176239 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.0652],\n",
      "        [0.0400]], device='mps:0')\n",
      "Iteration 23470 Training loss 0.05248717963695526 Validation loss 0.06199723109602928 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.1114],\n",
      "        [0.9689]], device='mps:0')\n",
      "Iteration 23480 Training loss 0.07116346806287766 Validation loss 0.06191643327474594 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.9128],\n",
      "        [0.1687]], device='mps:0')\n",
      "Iteration 23490 Training loss 0.0632738322019577 Validation loss 0.061925556510686874 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.8549],\n",
      "        [0.5125]], device='mps:0')\n",
      "Iteration 23500 Training loss 0.06402775645256042 Validation loss 0.061924759298563004 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.9598],\n",
      "        [0.1869]], device='mps:0')\n",
      "Iteration 23510 Training loss 0.06316985189914703 Validation loss 0.06202426925301552 Accuracy 0.830500066280365\n",
      "Output tensor([[0.9900],\n",
      "        [0.9627]], device='mps:0')\n",
      "Iteration 23520 Training loss 0.055952366441488266 Validation loss 0.06226168945431709 Accuracy 0.830625057220459\n",
      "Output tensor([[0.4939],\n",
      "        [0.7391]], device='mps:0')\n",
      "Iteration 23530 Training loss 0.0708918496966362 Validation loss 0.06197262555360794 Accuracy 0.830500066280365\n",
      "Output tensor([[0.9346],\n",
      "        [0.7727]], device='mps:0')\n",
      "Iteration 23540 Training loss 0.06004781648516655 Validation loss 0.061911165714263916 Accuracy 0.831250011920929\n",
      "Output tensor([[0.8484],\n",
      "        [0.2091]], device='mps:0')\n",
      "Iteration 23550 Training loss 0.06491430848836899 Validation loss 0.062292635440826416 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.5999],\n",
      "        [0.7228]], device='mps:0')\n",
      "Iteration 23560 Training loss 0.06646249443292618 Validation loss 0.061926133930683136 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.8548],\n",
      "        [0.3926]], device='mps:0')\n",
      "Iteration 23570 Training loss 0.059200581163167953 Validation loss 0.06199968233704567 Accuracy 0.830500066280365\n",
      "Output tensor([[0.3639],\n",
      "        [0.8408]], device='mps:0')\n",
      "Iteration 23580 Training loss 0.05208389088511467 Validation loss 0.06191129982471466 Accuracy 0.831250011920929\n",
      "Output tensor([[0.8474],\n",
      "        [0.9325]], device='mps:0')\n",
      "Iteration 23590 Training loss 0.06420076638460159 Validation loss 0.06192401424050331 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.6224],\n",
      "        [0.5654]], device='mps:0')\n",
      "Iteration 23600 Training loss 0.056143105030059814 Validation loss 0.061898671090602875 Accuracy 0.831000030040741\n",
      "Output tensor([[0.5939],\n",
      "        [0.2098]], device='mps:0')\n",
      "Iteration 23610 Training loss 0.06713683158159256 Validation loss 0.0619145892560482 Accuracy 0.831250011920929\n",
      "Output tensor([[0.9458],\n",
      "        [0.6945]], device='mps:0')\n",
      "Iteration 23620 Training loss 0.05665159970521927 Validation loss 0.062055978924036026 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.1405],\n",
      "        [0.3849]], device='mps:0')\n",
      "Iteration 23630 Training loss 0.06031491234898567 Validation loss 0.06191154569387436 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.8864],\n",
      "        [0.8698]], device='mps:0')\n",
      "Iteration 23640 Training loss 0.07518070191144943 Validation loss 0.06214526295661926 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.9665],\n",
      "        [0.2235]], device='mps:0')\n",
      "Iteration 23650 Training loss 0.05328778177499771 Validation loss 0.06194949150085449 Accuracy 0.8287500143051147\n",
      "Output tensor([[0.3495],\n",
      "        [0.9165]], device='mps:0')\n",
      "Iteration 23660 Training loss 0.055176541209220886 Validation loss 0.061879757791757584 Accuracy 0.830750048160553\n",
      "Output tensor([[0.0449],\n",
      "        [0.8033]], device='mps:0')\n",
      "Iteration 23670 Training loss 0.060004133731126785 Validation loss 0.06192459538578987 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.4532],\n",
      "        [0.3341]], device='mps:0')\n",
      "Iteration 23680 Training loss 0.057082440704107285 Validation loss 0.061904024332761765 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.7907],\n",
      "        [0.9001]], device='mps:0')\n",
      "Iteration 23690 Training loss 0.054379481822252274 Validation loss 0.06191524118185043 Accuracy 0.830750048160553\n",
      "Output tensor([[0.2597],\n",
      "        [0.9642]], device='mps:0')\n",
      "Iteration 23700 Training loss 0.05087757483124733 Validation loss 0.06187902018427849 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.6759],\n",
      "        [0.0074]], device='mps:0')\n",
      "Iteration 23710 Training loss 0.059751469641923904 Validation loss 0.061988718807697296 Accuracy 0.830875039100647\n",
      "Output tensor([[0.6813],\n",
      "        [0.4820]], device='mps:0')\n",
      "Iteration 23720 Training loss 0.0659336969256401 Validation loss 0.06200186163187027 Accuracy 0.831125020980835\n",
      "Output tensor([[0.5536],\n",
      "        [0.3143]], device='mps:0')\n",
      "Iteration 23730 Training loss 0.05598260462284088 Validation loss 0.06194024905562401 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.3494],\n",
      "        [0.2395]], device='mps:0')\n",
      "Iteration 23740 Training loss 0.06841433048248291 Validation loss 0.06186975538730621 Accuracy 0.830875039100647\n",
      "Output tensor([[0.9637],\n",
      "        [0.9635]], device='mps:0')\n",
      "Iteration 23750 Training loss 0.06176729500293732 Validation loss 0.061905309557914734 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0809],\n",
      "        [0.3870]], device='mps:0')\n",
      "Iteration 23760 Training loss 0.05324225127696991 Validation loss 0.061870455741882324 Accuracy 0.830750048160553\n",
      "Output tensor([[0.8435],\n",
      "        [0.4597]], device='mps:0')\n",
      "Iteration 23770 Training loss 0.0567617230117321 Validation loss 0.06187174469232559 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.9297],\n",
      "        [0.7957]], device='mps:0')\n",
      "Iteration 23780 Training loss 0.05461849272251129 Validation loss 0.061874378472566605 Accuracy 0.830625057220459\n",
      "Output tensor([[0.9808],\n",
      "        [0.9894]], device='mps:0')\n",
      "Iteration 23790 Training loss 0.05133387818932533 Validation loss 0.06201498582959175 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.0171],\n",
      "        [0.9717]], device='mps:0')\n",
      "Iteration 23800 Training loss 0.06417186558246613 Validation loss 0.06243770197033882 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.5480],\n",
      "        [0.7433]], device='mps:0')\n",
      "Iteration 23810 Training loss 0.0629173144698143 Validation loss 0.06205851957201958 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.9488],\n",
      "        [0.7705]], device='mps:0')\n",
      "Iteration 23820 Training loss 0.052844904363155365 Validation loss 0.06189125031232834 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.0902],\n",
      "        [0.9495]], device='mps:0')\n",
      "Iteration 23830 Training loss 0.055387284606695175 Validation loss 0.06203000992536545 Accuracy 0.830625057220459\n",
      "Output tensor([[0.9413],\n",
      "        [0.9881]], device='mps:0')\n",
      "Iteration 23840 Training loss 0.06614303588867188 Validation loss 0.06186296045780182 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.7376],\n",
      "        [0.8115]], device='mps:0')\n",
      "Iteration 23850 Training loss 0.06057676300406456 Validation loss 0.06189865991473198 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.3404],\n",
      "        [0.9519]], device='mps:0')\n",
      "Iteration 23860 Training loss 0.059358227998018265 Validation loss 0.06208061799407005 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.9145],\n",
      "        [0.8625]], device='mps:0')\n",
      "Iteration 23870 Training loss 0.06102415919303894 Validation loss 0.06220780313014984 Accuracy 0.831000030040741\n",
      "Output tensor([[0.0853],\n",
      "        [0.0460]], device='mps:0')\n",
      "Iteration 23880 Training loss 0.054018691182136536 Validation loss 0.06186908483505249 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.2084],\n",
      "        [0.2033]], device='mps:0')\n",
      "Iteration 23890 Training loss 0.060756616294384 Validation loss 0.061881739646196365 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.0375],\n",
      "        [0.7689]], device='mps:0')\n",
      "Iteration 23900 Training loss 0.0588514544069767 Validation loss 0.06187014281749725 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.5264],\n",
      "        [0.9487]], device='mps:0')\n",
      "Iteration 23910 Training loss 0.0592084601521492 Validation loss 0.06193896010518074 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.9162],\n",
      "        [0.0623]], device='mps:0')\n",
      "Iteration 23920 Training loss 0.059004876762628555 Validation loss 0.06207803264260292 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.8626],\n",
      "        [0.0288]], device='mps:0')\n",
      "Iteration 23930 Training loss 0.0580500103533268 Validation loss 0.061864275485277176 Accuracy 0.831250011920929\n",
      "Output tensor([[0.2612],\n",
      "        [0.2829]], device='mps:0')\n",
      "Iteration 23940 Training loss 0.06400160491466522 Validation loss 0.06253348290920258 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.3268],\n",
      "        [0.2529]], device='mps:0')\n",
      "Iteration 23950 Training loss 0.053866852074861526 Validation loss 0.06196475401520729 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.9063],\n",
      "        [0.9677]], device='mps:0')\n",
      "Iteration 23960 Training loss 0.06705734878778458 Validation loss 0.06189369410276413 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.3799],\n",
      "        [0.8928]], device='mps:0')\n",
      "Iteration 23970 Training loss 0.0643550381064415 Validation loss 0.06203760206699371 Accuracy 0.830500066280365\n",
      "Output tensor([[0.9540],\n",
      "        [0.0558]], device='mps:0')\n",
      "Iteration 23980 Training loss 0.06391822546720505 Validation loss 0.06186489760875702 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0797],\n",
      "        [0.9468]], device='mps:0')\n",
      "Iteration 23990 Training loss 0.062379080802202225 Validation loss 0.061844758689403534 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.1926],\n",
      "        [0.5653]], device='mps:0')\n",
      "Iteration 24000 Training loss 0.06708243489265442 Validation loss 0.06184028089046478 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.7499],\n",
      "        [0.6376]], device='mps:0')\n",
      "Iteration 24010 Training loss 0.056816279888153076 Validation loss 0.061968594789505005 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.0443],\n",
      "        [0.1562]], device='mps:0')\n",
      "Iteration 24020 Training loss 0.05788842588663101 Validation loss 0.06184111163020134 Accuracy 0.830750048160553\n",
      "Output tensor([[0.9843],\n",
      "        [0.8944]], device='mps:0')\n",
      "Iteration 24030 Training loss 0.04968015477061272 Validation loss 0.06209880858659744 Accuracy 0.830625057220459\n",
      "Output tensor([[0.8416],\n",
      "        [0.0528]], device='mps:0')\n",
      "Iteration 24040 Training loss 0.05573311820626259 Validation loss 0.061837416142225266 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.8241],\n",
      "        [0.1122]], device='mps:0')\n",
      "Iteration 24050 Training loss 0.06023746356368065 Validation loss 0.06184057891368866 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.0669],\n",
      "        [0.0142]], device='mps:0')\n",
      "Iteration 24060 Training loss 0.06874971091747284 Validation loss 0.061839159578084946 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.8517],\n",
      "        [0.7380]], device='mps:0')\n",
      "Iteration 24070 Training loss 0.05609579011797905 Validation loss 0.062106598168611526 Accuracy 0.831250011920929\n",
      "Output tensor([[0.3993],\n",
      "        [0.4979]], device='mps:0')\n",
      "Iteration 24080 Training loss 0.07452216744422913 Validation loss 0.06184497848153114 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.6166],\n",
      "        [0.8555]], device='mps:0')\n",
      "Iteration 24090 Training loss 0.058406732976436615 Validation loss 0.06203506886959076 Accuracy 0.830625057220459\n",
      "Output tensor([[0.9681],\n",
      "        [0.2567]], device='mps:0')\n",
      "Iteration 24100 Training loss 0.06041959673166275 Validation loss 0.06194012239575386 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.1591],\n",
      "        [0.8371]], device='mps:0')\n",
      "Iteration 24110 Training loss 0.06665638834238052 Validation loss 0.061844829469919205 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.1728],\n",
      "        [0.7051]], device='mps:0')\n",
      "Iteration 24120 Training loss 0.052694473415613174 Validation loss 0.061835259199142456 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.2917],\n",
      "        [0.1004]], device='mps:0')\n",
      "Iteration 24130 Training loss 0.05621087923645973 Validation loss 0.06182514503598213 Accuracy 0.830875039100647\n",
      "Output tensor([[0.6480],\n",
      "        [0.9084]], device='mps:0')\n",
      "Iteration 24140 Training loss 0.05686599761247635 Validation loss 0.06181382015347481 Accuracy 0.830500066280365\n",
      "Output tensor([[0.6061],\n",
      "        [0.7791]], device='mps:0')\n",
      "Iteration 24150 Training loss 0.053521528840065 Validation loss 0.06193161755800247 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.3288],\n",
      "        [0.3206]], device='mps:0')\n",
      "Iteration 24160 Training loss 0.05451278015971184 Validation loss 0.0618017353117466 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.9521],\n",
      "        [0.6755]], device='mps:0')\n",
      "Iteration 24170 Training loss 0.058661896735429764 Validation loss 0.06191270798444748 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.7280],\n",
      "        [0.9775]], device='mps:0')\n",
      "Iteration 24180 Training loss 0.06529601663351059 Validation loss 0.06182943284511566 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.7879],\n",
      "        [0.7206]], device='mps:0')\n",
      "Iteration 24190 Training loss 0.0708656758069992 Validation loss 0.061795879155397415 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.9772],\n",
      "        [0.8648]], device='mps:0')\n",
      "Iteration 24200 Training loss 0.05816289782524109 Validation loss 0.06184640899300575 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.0428],\n",
      "        [0.7560]], device='mps:0')\n",
      "Iteration 24210 Training loss 0.05895516276359558 Validation loss 0.061805009841918945 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.9498],\n",
      "        [0.0629]], device='mps:0')\n",
      "Iteration 24220 Training loss 0.058531906455755234 Validation loss 0.061793070286512375 Accuracy 0.831250011920929\n",
      "Output tensor([[0.5381],\n",
      "        [0.1120]], device='mps:0')\n",
      "Iteration 24230 Training loss 0.06429341435432434 Validation loss 0.06178908050060272 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.9571],\n",
      "        [0.5144]], device='mps:0')\n",
      "Iteration 24240 Training loss 0.055328480899333954 Validation loss 0.06177780032157898 Accuracy 0.830875039100647\n",
      "Output tensor([[0.0570],\n",
      "        [0.9668]], device='mps:0')\n",
      "Iteration 24250 Training loss 0.06061450392007828 Validation loss 0.061785098165273666 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.0921],\n",
      "        [0.0528]], device='mps:0')\n",
      "Iteration 24260 Training loss 0.056994952261447906 Validation loss 0.06191204488277435 Accuracy 0.831125020980835\n",
      "Output tensor([[0.7397],\n",
      "        [0.9395]], device='mps:0')\n",
      "Iteration 24270 Training loss 0.0603359080851078 Validation loss 0.06177790090441704 Accuracy 0.831250011920929\n",
      "Output tensor([[0.5392],\n",
      "        [0.6177]], device='mps:0')\n",
      "Iteration 24280 Training loss 0.06910891085863113 Validation loss 0.06187791749835014 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.1607],\n",
      "        [0.2147]], device='mps:0')\n",
      "Iteration 24290 Training loss 0.06621900200843811 Validation loss 0.061824433505535126 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.7694],\n",
      "        [0.1063]], device='mps:0')\n",
      "Iteration 24300 Training loss 0.06160161271691322 Validation loss 0.061772365123033524 Accuracy 0.830875039100647\n",
      "Output tensor([[0.7646],\n",
      "        [0.9446]], device='mps:0')\n",
      "Iteration 24310 Training loss 0.05844495818018913 Validation loss 0.06177794933319092 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.1892],\n",
      "        [0.5155]], device='mps:0')\n",
      "Iteration 24320 Training loss 0.06043965369462967 Validation loss 0.06183765083551407 Accuracy 0.831000030040741\n",
      "Output tensor([[0.0546],\n",
      "        [0.8767]], device='mps:0')\n",
      "Iteration 24330 Training loss 0.06727094203233719 Validation loss 0.061836712062358856 Accuracy 0.830750048160553\n",
      "Output tensor([[0.1022],\n",
      "        [0.8462]], device='mps:0')\n",
      "Iteration 24340 Training loss 0.05800531432032585 Validation loss 0.06175961717963219 Accuracy 0.831250011920929\n",
      "Output tensor([[0.9246],\n",
      "        [0.1588]], device='mps:0')\n",
      "Iteration 24350 Training loss 0.050209805369377136 Validation loss 0.06175539642572403 Accuracy 0.831125020980835\n",
      "Output tensor([[0.8881],\n",
      "        [0.7896]], device='mps:0')\n",
      "Iteration 24360 Training loss 0.06646806746721268 Validation loss 0.06174978241324425 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.1676],\n",
      "        [0.9566]], device='mps:0')\n",
      "Iteration 24370 Training loss 0.06244390830397606 Validation loss 0.06177988648414612 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.5088],\n",
      "        [0.9770]], device='mps:0')\n",
      "Iteration 24380 Training loss 0.05611937493085861 Validation loss 0.06173796206712723 Accuracy 0.831250011920929\n",
      "Output tensor([[0.9197],\n",
      "        [0.8713]], device='mps:0')\n",
      "Iteration 24390 Training loss 0.065610371530056 Validation loss 0.061743270605802536 Accuracy 0.830875039100647\n",
      "Output tensor([[0.8048],\n",
      "        [0.5493]], device='mps:0')\n",
      "Iteration 24400 Training loss 0.06013444438576698 Validation loss 0.061820875853300095 Accuracy 0.8288750648498535\n",
      "Output tensor([[0.8278],\n",
      "        [0.3491]], device='mps:0')\n",
      "Iteration 24410 Training loss 0.05760396644473076 Validation loss 0.06185245141386986 Accuracy 0.8285000324249268\n",
      "Output tensor([[0.0473],\n",
      "        [0.4843]], device='mps:0')\n",
      "Iteration 24420 Training loss 0.056959547102451324 Validation loss 0.06178903579711914 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.4130],\n",
      "        [0.0906]], device='mps:0')\n",
      "Iteration 24430 Training loss 0.06072719395160675 Validation loss 0.06177997216582298 Accuracy 0.8292500376701355\n",
      "Output tensor([[0.5822],\n",
      "        [0.9322]], device='mps:0')\n",
      "Iteration 24440 Training loss 0.05173172801733017 Validation loss 0.06173793226480484 Accuracy 0.831250011920929\n",
      "Output tensor([[0.4628],\n",
      "        [0.8906]], device='mps:0')\n",
      "Iteration 24450 Training loss 0.060146182775497437 Validation loss 0.06175464764237404 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.9920],\n",
      "        [0.9283]], device='mps:0')\n",
      "Iteration 24460 Training loss 0.05671015754342079 Validation loss 0.06188583746552467 Accuracy 0.8291250467300415\n",
      "Output tensor([[0.9867],\n",
      "        [0.8789]], device='mps:0')\n",
      "Iteration 24470 Training loss 0.05225960165262222 Validation loss 0.06174199655652046 Accuracy 0.830625057220459\n",
      "Output tensor([[0.8118],\n",
      "        [0.7729]], device='mps:0')\n",
      "Iteration 24480 Training loss 0.06299483776092529 Validation loss 0.0617261566221714 Accuracy 0.830500066280365\n",
      "Output tensor([[0.9004],\n",
      "        [0.8232]], device='mps:0')\n",
      "Iteration 24490 Training loss 0.05753140151500702 Validation loss 0.06181161850690842 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.5867],\n",
      "        [0.9084]], device='mps:0')\n",
      "Iteration 24500 Training loss 0.05992322787642479 Validation loss 0.06179038807749748 Accuracy 0.830500066280365\n",
      "Output tensor([[0.6561],\n",
      "        [0.7491]], device='mps:0')\n",
      "Iteration 24510 Training loss 0.06178433820605278 Validation loss 0.06173047795891762 Accuracy 0.830625057220459\n",
      "Output tensor([[0.8998],\n",
      "        [0.2337]], device='mps:0')\n",
      "Iteration 24520 Training loss 0.05762781947851181 Validation loss 0.06175203248858452 Accuracy 0.830750048160553\n",
      "Output tensor([[0.9775],\n",
      "        [0.2567]], device='mps:0')\n",
      "Iteration 24530 Training loss 0.06296706199645996 Validation loss 0.061764176934957504 Accuracy 0.830875039100647\n",
      "Output tensor([[0.7189],\n",
      "        [0.8792]], device='mps:0')\n",
      "Iteration 24540 Training loss 0.055077146738767624 Validation loss 0.06180508807301521 Accuracy 0.830875039100647\n",
      "Output tensor([[0.9693],\n",
      "        [0.1542]], device='mps:0')\n",
      "Iteration 24550 Training loss 0.05825261026620865 Validation loss 0.062055665999650955 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.3515],\n",
      "        [0.1948]], device='mps:0')\n",
      "Iteration 24560 Training loss 0.062003977596759796 Validation loss 0.06170012429356575 Accuracy 0.830750048160553\n",
      "Output tensor([[0.9193],\n",
      "        [0.5459]], device='mps:0')\n",
      "Iteration 24570 Training loss 0.058962829411029816 Validation loss 0.06173677369952202 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.8263],\n",
      "        [0.5330]], device='mps:0')\n",
      "Iteration 24580 Training loss 0.0612013153731823 Validation loss 0.061718180775642395 Accuracy 0.831000030040741\n",
      "Output tensor([[0.9623],\n",
      "        [0.6963]], device='mps:0')\n",
      "Iteration 24590 Training loss 0.06046508625149727 Validation loss 0.06169528514146805 Accuracy 0.831250011920929\n",
      "Output tensor([[0.1335],\n",
      "        [0.9851]], device='mps:0')\n",
      "Iteration 24600 Training loss 0.056754015386104584 Validation loss 0.06168854981660843 Accuracy 0.830875039100647\n",
      "Output tensor([[0.1006],\n",
      "        [0.5325]], device='mps:0')\n",
      "Iteration 24610 Training loss 0.06393519043922424 Validation loss 0.061809200793504715 Accuracy 0.831000030040741\n",
      "Output tensor([[0.8048],\n",
      "        [0.9883]], device='mps:0')\n",
      "Iteration 24620 Training loss 0.05744641274213791 Validation loss 0.06168457493185997 Accuracy 0.830875039100647\n",
      "Output tensor([[0.8858],\n",
      "        [0.8834]], device='mps:0')\n",
      "Iteration 24630 Training loss 0.06616976857185364 Validation loss 0.061718255281448364 Accuracy 0.830500066280365\n",
      "Output tensor([[0.8750],\n",
      "        [0.1982]], device='mps:0')\n",
      "Iteration 24640 Training loss 0.052637021988630295 Validation loss 0.06168998405337334 Accuracy 0.830625057220459\n",
      "Output tensor([[0.9463],\n",
      "        [0.0425]], device='mps:0')\n",
      "Iteration 24650 Training loss 0.05732826888561249 Validation loss 0.06167957931756973 Accuracy 0.830625057220459\n",
      "Output tensor([[0.0341],\n",
      "        [0.8787]], device='mps:0')\n",
      "Iteration 24660 Training loss 0.05352627485990524 Validation loss 0.06168178841471672 Accuracy 0.831000030040741\n",
      "Output tensor([[0.3071],\n",
      "        [0.5789]], device='mps:0')\n",
      "Iteration 24670 Training loss 0.06061995029449463 Validation loss 0.061861876398324966 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.9711],\n",
      "        [0.9840]], device='mps:0')\n",
      "Iteration 24680 Training loss 0.060825638473033905 Validation loss 0.06166527047753334 Accuracy 0.830875039100647\n",
      "Output tensor([[0.1815],\n",
      "        [0.4204]], device='mps:0')\n",
      "Iteration 24690 Training loss 0.05730390548706055 Validation loss 0.06168977543711662 Accuracy 0.830750048160553\n",
      "Output tensor([[0.0314],\n",
      "        [0.6239]], device='mps:0')\n",
      "Iteration 24700 Training loss 0.061782319098711014 Validation loss 0.06167279928922653 Accuracy 0.831125020980835\n",
      "Output tensor([[0.6704],\n",
      "        [0.2448]], device='mps:0')\n",
      "Iteration 24710 Training loss 0.06864083558320999 Validation loss 0.061774395406246185 Accuracy 0.831125020980835\n",
      "Output tensor([[0.7587],\n",
      "        [0.1498]], device='mps:0')\n",
      "Iteration 24720 Training loss 0.05190592631697655 Validation loss 0.06171907112002373 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.2604],\n",
      "        [0.9847]], device='mps:0')\n",
      "Iteration 24730 Training loss 0.05854301154613495 Validation loss 0.061660036444664 Accuracy 0.830500066280365\n",
      "Output tensor([[0.6168],\n",
      "        [0.9377]], device='mps:0')\n",
      "Iteration 24740 Training loss 0.06517230719327927 Validation loss 0.0617365837097168 Accuracy 0.830750048160553\n",
      "Output tensor([[0.9682],\n",
      "        [0.6809]], device='mps:0')\n",
      "Iteration 24750 Training loss 0.06371273845434189 Validation loss 0.061679281294345856 Accuracy 0.831125020980835\n",
      "Output tensor([[0.2008],\n",
      "        [0.7627]], device='mps:0')\n",
      "Iteration 24760 Training loss 0.06479637324810028 Validation loss 0.061683326959609985 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.5442],\n",
      "        [0.8606]], device='mps:0')\n",
      "Iteration 24770 Training loss 0.05800217390060425 Validation loss 0.06170996278524399 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.8639],\n",
      "        [0.6866]], device='mps:0')\n",
      "Iteration 24780 Training loss 0.06283242255449295 Validation loss 0.06178303435444832 Accuracy 0.8295000195503235\n",
      "Output tensor([[0.2264],\n",
      "        [0.0595]], device='mps:0')\n",
      "Iteration 24790 Training loss 0.06993383914232254 Validation loss 0.06169556826353073 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.5262],\n",
      "        [0.8822]], device='mps:0')\n",
      "Iteration 24800 Training loss 0.05088125169277191 Validation loss 0.06169800087809563 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.7821],\n",
      "        [0.9431]], device='mps:0')\n",
      "Iteration 24810 Training loss 0.0641133114695549 Validation loss 0.06173991411924362 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.8965],\n",
      "        [0.9928]], device='mps:0')\n",
      "Iteration 24820 Training loss 0.06012062728404999 Validation loss 0.06166837736964226 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.3488],\n",
      "        [0.0955]], device='mps:0')\n",
      "Iteration 24830 Training loss 0.060921039432287216 Validation loss 0.0616474412381649 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.1356],\n",
      "        [0.9960]], device='mps:0')\n",
      "Iteration 24840 Training loss 0.062075335532426834 Validation loss 0.06166287511587143 Accuracy 0.830875039100647\n",
      "Output tensor([[0.9923],\n",
      "        [0.3243]], device='mps:0')\n",
      "Iteration 24850 Training loss 0.06445558369159698 Validation loss 0.06164182350039482 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.9789],\n",
      "        [0.2016]], device='mps:0')\n",
      "Iteration 24860 Training loss 0.05477871745824814 Validation loss 0.06177079305052757 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.1059],\n",
      "        [0.0701]], device='mps:0')\n",
      "Iteration 24870 Training loss 0.06612168997526169 Validation loss 0.06165904551744461 Accuracy 0.830750048160553\n",
      "Output tensor([[0.9308],\n",
      "        [0.0605]], device='mps:0')\n",
      "Iteration 24880 Training loss 0.06250603497028351 Validation loss 0.061777882277965546 Accuracy 0.831125020980835\n",
      "Output tensor([[0.9638],\n",
      "        [0.2594]], device='mps:0')\n",
      "Iteration 24890 Training loss 0.057970598340034485 Validation loss 0.06166119873523712 Accuracy 0.830625057220459\n",
      "Output tensor([[0.8031],\n",
      "        [0.7081]], device='mps:0')\n",
      "Iteration 24900 Training loss 0.05940208211541176 Validation loss 0.061671044677495956 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.9096],\n",
      "        [0.3450]], device='mps:0')\n",
      "Iteration 24910 Training loss 0.05555020272731781 Validation loss 0.061684634536504745 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.0330],\n",
      "        [0.5424]], device='mps:0')\n",
      "Iteration 24920 Training loss 0.05738740786910057 Validation loss 0.06165037304162979 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.1719],\n",
      "        [0.8861]], device='mps:0')\n",
      "Iteration 24930 Training loss 0.05797493830323219 Validation loss 0.06166011840105057 Accuracy 0.830875039100647\n",
      "Output tensor([[0.2555],\n",
      "        [0.1662]], device='mps:0')\n",
      "Iteration 24940 Training loss 0.061430882662534714 Validation loss 0.06164370849728584 Accuracy 0.831250011920929\n",
      "Output tensor([[0.3984],\n",
      "        [0.0641]], device='mps:0')\n",
      "Iteration 24950 Training loss 0.06810588389635086 Validation loss 0.06164272502064705 Accuracy 0.831125020980835\n",
      "Output tensor([[0.1373],\n",
      "        [0.7190]], device='mps:0')\n",
      "Iteration 24960 Training loss 0.0578056238591671 Validation loss 0.061663899570703506 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.8069],\n",
      "        [0.9604]], device='mps:0')\n",
      "Iteration 24970 Training loss 0.06535540521144867 Validation loss 0.06165293976664543 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0267],\n",
      "        [0.8020]], device='mps:0')\n",
      "Iteration 24980 Training loss 0.06115557998418808 Validation loss 0.06168266758322716 Accuracy 0.830500066280365\n",
      "Output tensor([[0.2935],\n",
      "        [0.0942]], device='mps:0')\n",
      "Iteration 24990 Training loss 0.05304500088095665 Validation loss 0.06206565350294113 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.1479],\n",
      "        [0.6050]], device='mps:0')\n",
      "Iteration 25000 Training loss 0.07358051091432571 Validation loss 0.06164092943072319 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.4457],\n",
      "        [0.3613]], device='mps:0')\n",
      "Iteration 25010 Training loss 0.051191747188568115 Validation loss 0.061675120145082474 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.9225],\n",
      "        [0.1558]], device='mps:0')\n",
      "Iteration 25020 Training loss 0.055565040558576584 Validation loss 0.061642590910196304 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.2074],\n",
      "        [0.4585]], device='mps:0')\n",
      "Iteration 25030 Training loss 0.057182811200618744 Validation loss 0.06165127456188202 Accuracy 0.831250011920929\n",
      "Output tensor([[0.4609],\n",
      "        [0.8977]], device='mps:0')\n",
      "Iteration 25040 Training loss 0.06266432255506516 Validation loss 0.06167291849851608 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.1847],\n",
      "        [0.0687]], device='mps:0')\n",
      "Iteration 25050 Training loss 0.06625044345855713 Validation loss 0.0618668831884861 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.0830],\n",
      "        [0.2968]], device='mps:0')\n",
      "Iteration 25060 Training loss 0.058041419833898544 Validation loss 0.06168743595480919 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1869],\n",
      "        [0.7920]], device='mps:0')\n",
      "Iteration 25070 Training loss 0.0672474354505539 Validation loss 0.06220877170562744 Accuracy 0.830875039100647\n",
      "Output tensor([[0.9848],\n",
      "        [0.6288]], device='mps:0')\n",
      "Iteration 25080 Training loss 0.06368814408779144 Validation loss 0.06164313852787018 Accuracy 0.831000030040741\n",
      "Output tensor([[0.0523],\n",
      "        [0.9934]], device='mps:0')\n",
      "Iteration 25090 Training loss 0.0629652813076973 Validation loss 0.06163546070456505 Accuracy 0.831250011920929\n",
      "Output tensor([[0.0140],\n",
      "        [0.6067]], device='mps:0')\n",
      "Iteration 25100 Training loss 0.0653705894947052 Validation loss 0.061625950038433075 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.0779],\n",
      "        [0.0618]], device='mps:0')\n",
      "Iteration 25110 Training loss 0.0614689439535141 Validation loss 0.061856042593717575 Accuracy 0.831250011920929\n",
      "Output tensor([[0.9811],\n",
      "        [0.0503]], device='mps:0')\n",
      "Iteration 25120 Training loss 0.06499794125556946 Validation loss 0.061670124530792236 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.7721],\n",
      "        [0.8880]], device='mps:0')\n",
      "Iteration 25130 Training loss 0.06872497498989105 Validation loss 0.06161894649267197 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.1050],\n",
      "        [0.9031]], device='mps:0')\n",
      "Iteration 25140 Training loss 0.06273543834686279 Validation loss 0.06164343282580376 Accuracy 0.830875039100647\n",
      "Output tensor([[0.1081],\n",
      "        [0.6648]], device='mps:0')\n",
      "Iteration 25150 Training loss 0.055581703782081604 Validation loss 0.061619099229574203 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.6893],\n",
      "        [0.0559]], device='mps:0')\n",
      "Iteration 25160 Training loss 0.05434909090399742 Validation loss 0.061617203056812286 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.1075],\n",
      "        [0.9858]], device='mps:0')\n",
      "Iteration 25170 Training loss 0.06338962912559509 Validation loss 0.06160971149802208 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.6547],\n",
      "        [0.9036]], device='mps:0')\n",
      "Iteration 25180 Training loss 0.05429460108280182 Validation loss 0.0617983341217041 Accuracy 0.831125020980835\n",
      "Output tensor([[0.0515],\n",
      "        [0.9564]], device='mps:0')\n",
      "Iteration 25190 Training loss 0.05918067321181297 Validation loss 0.0617128349840641 Accuracy 0.831125020980835\n",
      "Output tensor([[0.1843],\n",
      "        [0.1054]], device='mps:0')\n",
      "Iteration 25200 Training loss 0.05295128747820854 Validation loss 0.06161411851644516 Accuracy 0.830875039100647\n",
      "Output tensor([[0.0147],\n",
      "        [0.9759]], device='mps:0')\n",
      "Iteration 25210 Training loss 0.06652192771434784 Validation loss 0.061681121587753296 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.3958],\n",
      "        [0.0963]], device='mps:0')\n",
      "Iteration 25220 Training loss 0.06278498470783234 Validation loss 0.0615919753909111 Accuracy 0.830875039100647\n",
      "Output tensor([[0.5263],\n",
      "        [0.7406]], device='mps:0')\n",
      "Iteration 25230 Training loss 0.05924652889370918 Validation loss 0.061706528067588806 Accuracy 0.8293750286102295\n",
      "Output tensor([[0.1787],\n",
      "        [0.3862]], device='mps:0')\n",
      "Iteration 25240 Training loss 0.05331071838736534 Validation loss 0.06163369119167328 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.2689],\n",
      "        [0.4705]], device='mps:0')\n",
      "Iteration 25250 Training loss 0.06238790974020958 Validation loss 0.061603303998708725 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0119],\n",
      "        [0.1386]], device='mps:0')\n",
      "Iteration 25260 Training loss 0.05960012227296829 Validation loss 0.06159009784460068 Accuracy 0.830625057220459\n",
      "Output tensor([[0.0936],\n",
      "        [0.1516]], device='mps:0')\n",
      "Iteration 25270 Training loss 0.057725973427295685 Validation loss 0.061609674245119095 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.2934],\n",
      "        [0.1533]], device='mps:0')\n",
      "Iteration 25280 Training loss 0.05849973484873772 Validation loss 0.06159670278429985 Accuracy 0.831000030040741\n",
      "Output tensor([[0.9450],\n",
      "        [0.5499]], device='mps:0')\n",
      "Iteration 25290 Training loss 0.062069665640592575 Validation loss 0.06166375055909157 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.1335],\n",
      "        [0.3642]], device='mps:0')\n",
      "Iteration 25300 Training loss 0.0613052174448967 Validation loss 0.06160051375627518 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.4821],\n",
      "        [0.8313]], device='mps:0')\n",
      "Iteration 25310 Training loss 0.0578426718711853 Validation loss 0.06159542500972748 Accuracy 0.831250011920929\n",
      "Output tensor([[0.8737],\n",
      "        [0.4687]], device='mps:0')\n",
      "Iteration 25320 Training loss 0.05301772803068161 Validation loss 0.06158201023936272 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.3585],\n",
      "        [0.9867]], device='mps:0')\n",
      "Iteration 25330 Training loss 0.059295833110809326 Validation loss 0.061582304537296295 Accuracy 0.830625057220459\n",
      "Output tensor([[0.2045],\n",
      "        [0.1523]], device='mps:0')\n",
      "Iteration 25340 Training loss 0.05501433089375496 Validation loss 0.06158287078142166 Accuracy 0.830875039100647\n",
      "Output tensor([[0.9329],\n",
      "        [0.7476]], device='mps:0')\n",
      "Iteration 25350 Training loss 0.05946430191397667 Validation loss 0.06166894733905792 Accuracy 0.830500066280365\n",
      "Output tensor([[0.9287],\n",
      "        [0.8503]], device='mps:0')\n",
      "Iteration 25360 Training loss 0.06116819754242897 Validation loss 0.06250017881393433 Accuracy 0.831000030040741\n",
      "Output tensor([[0.1475],\n",
      "        [0.2739]], device='mps:0')\n",
      "Iteration 25370 Training loss 0.06096068397164345 Validation loss 0.06172771379351616 Accuracy 0.8297500610351562\n",
      "Output tensor([[0.4803],\n",
      "        [0.9374]], device='mps:0')\n",
      "Iteration 25380 Training loss 0.06116030737757683 Validation loss 0.061615731567144394 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.9103],\n",
      "        [0.4894]], device='mps:0')\n",
      "Iteration 25390 Training loss 0.06652608513832092 Validation loss 0.06157887727022171 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.1608],\n",
      "        [0.7391]], device='mps:0')\n",
      "Iteration 25400 Training loss 0.05211429297924042 Validation loss 0.061645641922950745 Accuracy 0.831000030040741\n",
      "Output tensor([[0.2068],\n",
      "        [0.4072]], device='mps:0')\n",
      "Iteration 25410 Training loss 0.06322169303894043 Validation loss 0.06159639731049538 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.9529],\n",
      "        [0.8524]], device='mps:0')\n",
      "Iteration 25420 Training loss 0.057035572826862335 Validation loss 0.06156853958964348 Accuracy 0.830625057220459\n",
      "Output tensor([[0.8871],\n",
      "        [0.4291]], device='mps:0')\n",
      "Iteration 25430 Training loss 0.06932245939970016 Validation loss 0.061580780893564224 Accuracy 0.830625057220459\n",
      "Output tensor([[0.9532],\n",
      "        [0.0286]], device='mps:0')\n",
      "Iteration 25440 Training loss 0.0557241253554821 Validation loss 0.06158849224448204 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0165],\n",
      "        [0.3268]], device='mps:0')\n",
      "Iteration 25450 Training loss 0.06482837349176407 Validation loss 0.061737820506095886 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.8893],\n",
      "        [0.7735]], device='mps:0')\n",
      "Iteration 25460 Training loss 0.05762471631169319 Validation loss 0.06163959577679634 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.9709],\n",
      "        [0.0459]], device='mps:0')\n",
      "Iteration 25470 Training loss 0.059238746762275696 Validation loss 0.06160170957446098 Accuracy 0.830625057220459\n",
      "Output tensor([[0.1659],\n",
      "        [0.9172]], device='mps:0')\n",
      "Iteration 25480 Training loss 0.06570682674646378 Validation loss 0.06169353798031807 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.5578],\n",
      "        [0.9707]], device='mps:0')\n",
      "Iteration 25490 Training loss 0.06414932757616043 Validation loss 0.061658114194869995 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.0012],\n",
      "        [0.0979]], device='mps:0')\n",
      "Iteration 25500 Training loss 0.059198662638664246 Validation loss 0.06157273054122925 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.9967],\n",
      "        [0.9939]], device='mps:0')\n",
      "Iteration 25510 Training loss 0.06063409522175789 Validation loss 0.06158236041665077 Accuracy 0.831000030040741\n",
      "Output tensor([[0.0743],\n",
      "        [0.9797]], device='mps:0')\n",
      "Iteration 25520 Training loss 0.05270509421825409 Validation loss 0.06157052144408226 Accuracy 0.831000030040741\n",
      "Output tensor([[0.8617],\n",
      "        [0.9196]], device='mps:0')\n",
      "Iteration 25530 Training loss 0.06270565092563629 Validation loss 0.0618077889084816 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.1189],\n",
      "        [0.4895]], device='mps:0')\n",
      "Iteration 25540 Training loss 0.07331231981515884 Validation loss 0.06164322420954704 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.3420],\n",
      "        [0.1214]], device='mps:0')\n",
      "Iteration 25550 Training loss 0.05908648669719696 Validation loss 0.06169333681464195 Accuracy 0.8298750519752502\n",
      "Output tensor([[0.0138],\n",
      "        [0.0804]], device='mps:0')\n",
      "Iteration 25560 Training loss 0.05716579779982567 Validation loss 0.061609867960214615 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.3606],\n",
      "        [0.3292]], device='mps:0')\n",
      "Iteration 25570 Training loss 0.0622553750872612 Validation loss 0.06160147115588188 Accuracy 0.830875039100647\n",
      "Output tensor([[0.1698],\n",
      "        [0.2025]], device='mps:0')\n",
      "Iteration 25580 Training loss 0.05505158007144928 Validation loss 0.06163331866264343 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.3693],\n",
      "        [0.0388]], device='mps:0')\n",
      "Iteration 25590 Training loss 0.06323675811290741 Validation loss 0.06160702183842659 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.8616],\n",
      "        [0.7929]], device='mps:0')\n",
      "Iteration 25600 Training loss 0.059662722051143646 Validation loss 0.061582278460264206 Accuracy 0.831125020980835\n",
      "Output tensor([[0.0499],\n",
      "        [0.7075]], device='mps:0')\n",
      "Iteration 25610 Training loss 0.05393032357096672 Validation loss 0.06178628280758858 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.0926],\n",
      "        [0.5492]], device='mps:0')\n",
      "Iteration 25620 Training loss 0.055598124861717224 Validation loss 0.06157899647951126 Accuracy 0.831250011920929\n",
      "Output tensor([[0.0191],\n",
      "        [0.8468]], device='mps:0')\n",
      "Iteration 25630 Training loss 0.05864172801375389 Validation loss 0.06172472983598709 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.9506],\n",
      "        [0.9955]], device='mps:0')\n",
      "Iteration 25640 Training loss 0.061024975031614304 Validation loss 0.061615001410245895 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.8857],\n",
      "        [0.3491]], device='mps:0')\n",
      "Iteration 25650 Training loss 0.056440066546201706 Validation loss 0.0615568645298481 Accuracy 0.831000030040741\n",
      "Output tensor([[0.8846],\n",
      "        [0.8473]], device='mps:0')\n",
      "Iteration 25660 Training loss 0.05522988736629486 Validation loss 0.06155475974082947 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.5585],\n",
      "        [0.9374]], device='mps:0')\n",
      "Iteration 25670 Training loss 0.05816011130809784 Validation loss 0.061634842306375504 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.9008],\n",
      "        [0.2331]], device='mps:0')\n",
      "Iteration 25680 Training loss 0.05810073763132095 Validation loss 0.06156177073717117 Accuracy 0.831000030040741\n",
      "Output tensor([[0.6002],\n",
      "        [0.3401]], device='mps:0')\n",
      "Iteration 25690 Training loss 0.06085611507296562 Validation loss 0.061543058604002 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.8970],\n",
      "        [0.2670]], device='mps:0')\n",
      "Iteration 25700 Training loss 0.059844568371772766 Validation loss 0.06153532490134239 Accuracy 0.830500066280365\n",
      "Output tensor([[0.2952],\n",
      "        [0.8667]], device='mps:0')\n",
      "Iteration 25710 Training loss 0.05576780065894127 Validation loss 0.061547908931970596 Accuracy 0.830750048160553\n",
      "Output tensor([[0.0566],\n",
      "        [0.6558]], device='mps:0')\n",
      "Iteration 25720 Training loss 0.06093288213014603 Validation loss 0.06157461181282997 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.5024],\n",
      "        [0.4629]], device='mps:0')\n",
      "Iteration 25730 Training loss 0.04744040220975876 Validation loss 0.061621829867362976 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.9904],\n",
      "        [0.8748]], device='mps:0')\n",
      "Iteration 25740 Training loss 0.06547382473945618 Validation loss 0.061533041298389435 Accuracy 0.831250011920929\n",
      "Output tensor([[0.5207],\n",
      "        [0.2785]], device='mps:0')\n",
      "Iteration 25750 Training loss 0.059531670063734055 Validation loss 0.06155908480286598 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.4825],\n",
      "        [0.3959]], device='mps:0')\n",
      "Iteration 25760 Training loss 0.05880909413099289 Validation loss 0.06154731661081314 Accuracy 0.831125020980835\n",
      "Output tensor([[0.0279],\n",
      "        [0.9481]], device='mps:0')\n",
      "Iteration 25770 Training loss 0.0585852712392807 Validation loss 0.06152838468551636 Accuracy 0.830750048160553\n",
      "Output tensor([[0.5083],\n",
      "        [0.3807]], device='mps:0')\n",
      "Iteration 25780 Training loss 0.06568942964076996 Validation loss 0.06152297556400299 Accuracy 0.831125020980835\n",
      "Output tensor([[0.9750],\n",
      "        [0.5448]], device='mps:0')\n",
      "Iteration 25790 Training loss 0.058944933116436005 Validation loss 0.061530545353889465 Accuracy 0.830875039100647\n",
      "Output tensor([[0.0885],\n",
      "        [0.8287]], device='mps:0')\n",
      "Iteration 25800 Training loss 0.0558185912668705 Validation loss 0.06152644753456116 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.0819],\n",
      "        [0.0052]], device='mps:0')\n",
      "Iteration 25810 Training loss 0.06425061076879501 Validation loss 0.06167902797460556 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.9829],\n",
      "        [0.1642]], device='mps:0')\n",
      "Iteration 25820 Training loss 0.07226895540952682 Validation loss 0.06151750311255455 Accuracy 0.831125020980835\n",
      "Output tensor([[0.4292],\n",
      "        [0.2433]], device='mps:0')\n",
      "Iteration 25830 Training loss 0.06007891148328781 Validation loss 0.061517901718616486 Accuracy 0.830875039100647\n",
      "Output tensor([[0.8029],\n",
      "        [0.8890]], device='mps:0')\n",
      "Iteration 25840 Training loss 0.05960885062813759 Validation loss 0.06155010312795639 Accuracy 0.831250011920929\n",
      "Output tensor([[0.9341],\n",
      "        [0.8035]], device='mps:0')\n",
      "Iteration 25850 Training loss 0.061577290296554565 Validation loss 0.06151317059993744 Accuracy 0.831125020980835\n",
      "Output tensor([[0.2147],\n",
      "        [0.1297]], device='mps:0')\n",
      "Iteration 25860 Training loss 0.058838650584220886 Validation loss 0.06152544543147087 Accuracy 0.830500066280365\n",
      "Output tensor([[0.2619],\n",
      "        [0.5734]], device='mps:0')\n",
      "Iteration 25870 Training loss 0.0636528879404068 Validation loss 0.06155455484986305 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.1158],\n",
      "        [0.0415]], device='mps:0')\n",
      "Iteration 25880 Training loss 0.049531951546669006 Validation loss 0.06157476082444191 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.8427],\n",
      "        [0.0416]], device='mps:0')\n",
      "Iteration 25890 Training loss 0.06901979446411133 Validation loss 0.06152886897325516 Accuracy 0.831125020980835\n",
      "Output tensor([[0.2003],\n",
      "        [0.4882]], device='mps:0')\n",
      "Iteration 25900 Training loss 0.060374002903699875 Validation loss 0.06191924586892128 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0072],\n",
      "        [0.7787]], device='mps:0')\n",
      "Iteration 25910 Training loss 0.06530370563268661 Validation loss 0.06151164695620537 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.0316],\n",
      "        [0.1264]], device='mps:0')\n",
      "Iteration 25920 Training loss 0.056927118450403214 Validation loss 0.061517003923654556 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0161],\n",
      "        [0.2421]], device='mps:0')\n",
      "Iteration 25930 Training loss 0.06019112467765808 Validation loss 0.061522480100393295 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.2139],\n",
      "        [0.4839]], device='mps:0')\n",
      "Iteration 25940 Training loss 0.05726298317313194 Validation loss 0.06160278618335724 Accuracy 0.830625057220459\n",
      "Output tensor([[0.9850],\n",
      "        [0.1109]], device='mps:0')\n",
      "Iteration 25950 Training loss 0.06179813668131828 Validation loss 0.06153354048728943 Accuracy 0.830875039100647\n",
      "Output tensor([[0.1937],\n",
      "        [0.6683]], device='mps:0')\n",
      "Iteration 25960 Training loss 0.05794769152998924 Validation loss 0.06155484914779663 Accuracy 0.830625057220459\n",
      "Output tensor([[0.7832],\n",
      "        [0.7529]], device='mps:0')\n",
      "Iteration 25970 Training loss 0.05521461367607117 Validation loss 0.061515193432569504 Accuracy 0.831250011920929\n",
      "Output tensor([[0.7259],\n",
      "        [0.6905]], device='mps:0')\n",
      "Iteration 25980 Training loss 0.06390048563480377 Validation loss 0.06150289624929428 Accuracy 0.831000030040741\n",
      "Output tensor([[0.8477],\n",
      "        [0.7011]], device='mps:0')\n",
      "Iteration 25990 Training loss 0.05605151876807213 Validation loss 0.06150420010089874 Accuracy 0.831000030040741\n",
      "Output tensor([[0.1980],\n",
      "        [0.0996]], device='mps:0')\n",
      "Iteration 26000 Training loss 0.05454951152205467 Validation loss 0.061504997313022614 Accuracy 0.830750048160553\n",
      "Output tensor([[0.6150],\n",
      "        [0.2464]], device='mps:0')\n",
      "Iteration 26010 Training loss 0.06059301272034645 Validation loss 0.06149791181087494 Accuracy 0.830875039100647\n",
      "Output tensor([[0.3251],\n",
      "        [0.0275]], device='mps:0')\n",
      "Iteration 26020 Training loss 0.06161978468298912 Validation loss 0.061492353677749634 Accuracy 0.831000030040741\n",
      "Output tensor([[0.9725],\n",
      "        [0.8533]], device='mps:0')\n",
      "Iteration 26030 Training loss 0.06331650912761688 Validation loss 0.061574723571538925 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.9808],\n",
      "        [0.5911]], device='mps:0')\n",
      "Iteration 26040 Training loss 0.05953473970293999 Validation loss 0.06176646798849106 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0887],\n",
      "        [0.7059]], device='mps:0')\n",
      "Iteration 26050 Training loss 0.055810291320085526 Validation loss 0.06160769239068031 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.0392],\n",
      "        [0.9877]], device='mps:0')\n",
      "Iteration 26060 Training loss 0.06337810307741165 Validation loss 0.06154580041766167 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.9561],\n",
      "        [0.6726]], device='mps:0')\n",
      "Iteration 26070 Training loss 0.06609277427196503 Validation loss 0.06148599460721016 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.5234],\n",
      "        [0.1632]], device='mps:0')\n",
      "Iteration 26080 Training loss 0.05884159728884697 Validation loss 0.06148764118552208 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.4519],\n",
      "        [0.3340]], device='mps:0')\n",
      "Iteration 26090 Training loss 0.053053345531225204 Validation loss 0.06150565668940544 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.2631],\n",
      "        [0.9740]], device='mps:0')\n",
      "Iteration 26100 Training loss 0.06151818111538887 Validation loss 0.06147981435060501 Accuracy 0.831250011920929\n",
      "Output tensor([[0.0541],\n",
      "        [0.8676]], device='mps:0')\n",
      "Iteration 26110 Training loss 0.055956337600946426 Validation loss 0.06168471649289131 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.9297],\n",
      "        [0.9217]], device='mps:0')\n",
      "Iteration 26120 Training loss 0.05838565528392792 Validation loss 0.06150081008672714 Accuracy 0.831000030040741\n",
      "Output tensor([[0.1529],\n",
      "        [0.5943]], device='mps:0')\n",
      "Iteration 26130 Training loss 0.05877338722348213 Validation loss 0.06149529293179512 Accuracy 0.831125020980835\n",
      "Output tensor([[0.7440],\n",
      "        [0.9730]], device='mps:0')\n",
      "Iteration 26140 Training loss 0.06304333359003067 Validation loss 0.06148064136505127 Accuracy 0.831125020980835\n",
      "Output tensor([[0.0997],\n",
      "        [0.2372]], device='mps:0')\n",
      "Iteration 26150 Training loss 0.06165717914700508 Validation loss 0.06152207404375076 Accuracy 0.830875039100647\n",
      "Output tensor([[0.0794],\n",
      "        [0.0354]], device='mps:0')\n",
      "Iteration 26160 Training loss 0.05295930430293083 Validation loss 0.061575181782245636 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.3061],\n",
      "        [0.9642]], device='mps:0')\n",
      "Iteration 26170 Training loss 0.05767026171088219 Validation loss 0.061483271420001984 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.0445],\n",
      "        [0.2860]], device='mps:0')\n",
      "Iteration 26180 Training loss 0.056429747492074966 Validation loss 0.06147400289773941 Accuracy 0.830875039100647\n",
      "Output tensor([[0.2677],\n",
      "        [0.0182]], device='mps:0')\n",
      "Iteration 26190 Training loss 0.057687826454639435 Validation loss 0.06151621416211128 Accuracy 0.831125020980835\n",
      "Output tensor([[0.5214],\n",
      "        [0.1068]], device='mps:0')\n",
      "Iteration 26200 Training loss 0.06088143587112427 Validation loss 0.0615246444940567 Accuracy 0.831250011920929\n",
      "Output tensor([[0.1604],\n",
      "        [0.0939]], device='mps:0')\n",
      "Iteration 26210 Training loss 0.05645928159356117 Validation loss 0.061659034341573715 Accuracy 0.831250011920929\n",
      "Output tensor([[0.0720],\n",
      "        [0.0597]], device='mps:0')\n",
      "Iteration 26220 Training loss 0.05725300684571266 Validation loss 0.061485759913921356 Accuracy 0.830875039100647\n",
      "Output tensor([[0.9718],\n",
      "        [0.0883]], device='mps:0')\n",
      "Iteration 26230 Training loss 0.05996774509549141 Validation loss 0.06152113527059555 Accuracy 0.830875039100647\n",
      "Output tensor([[0.8070],\n",
      "        [0.7541]], device='mps:0')\n",
      "Iteration 26240 Training loss 0.06393636763095856 Validation loss 0.061934273689985275 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.7893],\n",
      "        [0.2455]], device='mps:0')\n",
      "Iteration 26250 Training loss 0.05741409584879875 Validation loss 0.06147336959838867 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0827],\n",
      "        [0.8772]], device='mps:0')\n",
      "Iteration 26260 Training loss 0.06380852311849594 Validation loss 0.06163366511464119 Accuracy 0.8300000429153442\n",
      "Output tensor([[0.0457],\n",
      "        [0.7413]], device='mps:0')\n",
      "Iteration 26270 Training loss 0.05758306011557579 Validation loss 0.061461251229047775 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.2287],\n",
      "        [0.1913]], device='mps:0')\n",
      "Iteration 26280 Training loss 0.06128672510385513 Validation loss 0.061461277306079865 Accuracy 0.831000030040741\n",
      "Output tensor([[0.9178],\n",
      "        [0.6651]], device='mps:0')\n",
      "Iteration 26290 Training loss 0.058711279183626175 Validation loss 0.06146940961480141 Accuracy 0.831125020980835\n",
      "Output tensor([[0.6001],\n",
      "        [0.1401]], device='mps:0')\n",
      "Iteration 26300 Training loss 0.06019826605916023 Validation loss 0.06147482991218567 Accuracy 0.831000030040741\n",
      "Output tensor([[0.9580],\n",
      "        [0.5527]], device='mps:0')\n",
      "Iteration 26310 Training loss 0.05989716574549675 Validation loss 0.06152686849236488 Accuracy 0.830625057220459\n",
      "Output tensor([[0.1510],\n",
      "        [0.0486]], device='mps:0')\n",
      "Iteration 26320 Training loss 0.06029150262475014 Validation loss 0.061809640377759933 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.8611],\n",
      "        [0.9637]], device='mps:0')\n",
      "Iteration 26330 Training loss 0.05619810149073601 Validation loss 0.06145993247628212 Accuracy 0.831000030040741\n",
      "Output tensor([[0.0767],\n",
      "        [0.8043]], device='mps:0')\n",
      "Iteration 26340 Training loss 0.05568697676062584 Validation loss 0.061458393931388855 Accuracy 0.831125020980835\n",
      "Output tensor([[0.1409],\n",
      "        [0.3176]], device='mps:0')\n",
      "Iteration 26350 Training loss 0.06060389429330826 Validation loss 0.06144934520125389 Accuracy 0.831125020980835\n",
      "Output tensor([[0.0780],\n",
      "        [0.3340]], device='mps:0')\n",
      "Iteration 26360 Training loss 0.06413432210683823 Validation loss 0.061451829969882965 Accuracy 0.831250011920929\n",
      "Output tensor([[0.9984],\n",
      "        [0.8605]], device='mps:0')\n",
      "Iteration 26370 Training loss 0.05787018686532974 Validation loss 0.06147502362728119 Accuracy 0.830625057220459\n",
      "Output tensor([[0.1526],\n",
      "        [0.8950]], device='mps:0')\n",
      "Iteration 26380 Training loss 0.06601137667894363 Validation loss 0.06144465506076813 Accuracy 0.831125020980835\n",
      "Output tensor([[0.2259],\n",
      "        [0.1042]], device='mps:0')\n",
      "Iteration 26390 Training loss 0.060556162148714066 Validation loss 0.06149290129542351 Accuracy 0.830875039100647\n",
      "Output tensor([[0.8091],\n",
      "        [0.6561]], device='mps:0')\n",
      "Iteration 26400 Training loss 0.06959867477416992 Validation loss 0.06144192814826965 Accuracy 0.831000030040741\n",
      "Output tensor([[0.9836],\n",
      "        [0.1189]], device='mps:0')\n",
      "Iteration 26410 Training loss 0.05799160897731781 Validation loss 0.06143664941191673 Accuracy 0.831125020980835\n",
      "Output tensor([[0.9337],\n",
      "        [0.2841]], device='mps:0')\n",
      "Iteration 26420 Training loss 0.05854205787181854 Validation loss 0.06149153411388397 Accuracy 0.831000030040741\n",
      "Output tensor([[0.9968],\n",
      "        [0.0372]], device='mps:0')\n",
      "Iteration 26430 Training loss 0.05646326020359993 Validation loss 0.06144171580672264 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.9714],\n",
      "        [0.0543]], device='mps:0')\n",
      "Iteration 26440 Training loss 0.06094149872660637 Validation loss 0.06143484264612198 Accuracy 0.831250011920929\n",
      "Output tensor([[0.0122],\n",
      "        [0.7480]], device='mps:0')\n",
      "Iteration 26450 Training loss 0.06509369611740112 Validation loss 0.06150367483496666 Accuracy 0.830750048160553\n",
      "Output tensor([[0.4379],\n",
      "        [0.7792]], device='mps:0')\n",
      "Iteration 26460 Training loss 0.06680707633495331 Validation loss 0.06145114079117775 Accuracy 0.830875039100647\n",
      "Output tensor([[0.0308],\n",
      "        [0.9122]], device='mps:0')\n",
      "Iteration 26470 Training loss 0.06447509676218033 Validation loss 0.06142689660191536 Accuracy 0.831125020980835\n",
      "Output tensor([[0.8897],\n",
      "        [0.8399]], device='mps:0')\n",
      "Iteration 26480 Training loss 0.056843847036361694 Validation loss 0.0614294707775116 Accuracy 0.831250011920929\n",
      "Output tensor([[0.9533],\n",
      "        [0.2787]], device='mps:0')\n",
      "Iteration 26490 Training loss 0.05092316493391991 Validation loss 0.06143663451075554 Accuracy 0.831250011920929\n",
      "Output tensor([[0.9214],\n",
      "        [0.0910]], device='mps:0')\n",
      "Iteration 26500 Training loss 0.06351465731859207 Validation loss 0.06141623482108116 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.5097],\n",
      "        [0.0909]], device='mps:0')\n",
      "Iteration 26510 Training loss 0.058671873062849045 Validation loss 0.06148234009742737 Accuracy 0.830750048160553\n",
      "Output tensor([[0.6393],\n",
      "        [0.1949]], device='mps:0')\n",
      "Iteration 26520 Training loss 0.06813891977071762 Validation loss 0.06156101077795029 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.2170],\n",
      "        [0.2804]], device='mps:0')\n",
      "Iteration 26530 Training loss 0.06039808318018913 Validation loss 0.06151483580470085 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.5599],\n",
      "        [0.0144]], device='mps:0')\n",
      "Iteration 26540 Training loss 0.05584663897752762 Validation loss 0.061570413410663605 Accuracy 0.831250011920929\n",
      "Output tensor([[0.5084],\n",
      "        [0.8105]], device='mps:0')\n",
      "Iteration 26550 Training loss 0.056313540786504745 Validation loss 0.06150401756167412 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.0673],\n",
      "        [0.9326]], device='mps:0')\n",
      "Iteration 26560 Training loss 0.06439828872680664 Validation loss 0.06146799772977829 Accuracy 0.831250011920929\n",
      "Output tensor([[0.1423],\n",
      "        [0.9794]], device='mps:0')\n",
      "Iteration 26570 Training loss 0.060129035264253616 Validation loss 0.06141497194766998 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.8673],\n",
      "        [0.4510]], device='mps:0')\n",
      "Iteration 26580 Training loss 0.05953536555171013 Validation loss 0.06147010996937752 Accuracy 0.830625057220459\n",
      "Output tensor([[0.8300],\n",
      "        [0.0386]], device='mps:0')\n",
      "Iteration 26590 Training loss 0.06404449790716171 Validation loss 0.061419256031513214 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.9489],\n",
      "        [0.2104]], device='mps:0')\n",
      "Iteration 26600 Training loss 0.06980839371681213 Validation loss 0.061404239386320114 Accuracy 0.831250011920929\n",
      "Output tensor([[0.0180],\n",
      "        [0.9283]], device='mps:0')\n",
      "Iteration 26610 Training loss 0.06064874678850174 Validation loss 0.061424657702445984 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.7669],\n",
      "        [0.3349]], device='mps:0')\n",
      "Iteration 26620 Training loss 0.06465636193752289 Validation loss 0.06143689900636673 Accuracy 0.830875039100647\n",
      "Output tensor([[0.8568],\n",
      "        [0.4088]], device='mps:0')\n",
      "Iteration 26630 Training loss 0.05986502394080162 Validation loss 0.06141588091850281 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.8761],\n",
      "        [0.4863]], device='mps:0')\n",
      "Iteration 26640 Training loss 0.056270621716976166 Validation loss 0.06143017113208771 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.1356],\n",
      "        [0.9881]], device='mps:0')\n",
      "Iteration 26650 Training loss 0.06386692076921463 Validation loss 0.061533693224191666 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.0058],\n",
      "        [0.8620]], device='mps:0')\n",
      "Iteration 26660 Training loss 0.06343797594308853 Validation loss 0.06143111735582352 Accuracy 0.831125020980835\n",
      "Output tensor([[0.4225],\n",
      "        [0.2790]], device='mps:0')\n",
      "Iteration 26670 Training loss 0.05514317378401756 Validation loss 0.061428502202034 Accuracy 0.831250011920929\n",
      "Output tensor([[0.1708],\n",
      "        [0.8113]], device='mps:0')\n",
      "Iteration 26680 Training loss 0.0624275729060173 Validation loss 0.06145652011036873 Accuracy 0.831250011920929\n",
      "Output tensor([[0.3052],\n",
      "        [0.1394]], device='mps:0')\n",
      "Iteration 26690 Training loss 0.0530843622982502 Validation loss 0.06140037998557091 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.8468],\n",
      "        [0.9249]], device='mps:0')\n",
      "Iteration 26700 Training loss 0.054390814155340195 Validation loss 0.06153496727347374 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.6486],\n",
      "        [0.0128]], device='mps:0')\n",
      "Iteration 26710 Training loss 0.0689614936709404 Validation loss 0.06139824166893959 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.8797],\n",
      "        [0.3859]], device='mps:0')\n",
      "Iteration 26720 Training loss 0.06264492869377136 Validation loss 0.06138801947236061 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.2496],\n",
      "        [0.6584]], device='mps:0')\n",
      "Iteration 26730 Training loss 0.06572366505861282 Validation loss 0.06203364208340645 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.2232],\n",
      "        [0.8736]], device='mps:0')\n",
      "Iteration 26740 Training loss 0.05361441522836685 Validation loss 0.06138336658477783 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.0724],\n",
      "        [0.3615]], device='mps:0')\n",
      "Iteration 26750 Training loss 0.06274641305208206 Validation loss 0.06143742799758911 Accuracy 0.830875039100647\n",
      "Output tensor([[0.9124],\n",
      "        [0.0384]], device='mps:0')\n",
      "Iteration 26760 Training loss 0.06472322344779968 Validation loss 0.06147186458110809 Accuracy 0.830500066280365\n",
      "Output tensor([[0.2401],\n",
      "        [0.0017]], device='mps:0')\n",
      "Iteration 26770 Training loss 0.06577397882938385 Validation loss 0.061415113508701324 Accuracy 0.830875039100647\n",
      "Output tensor([[0.0717],\n",
      "        [0.1992]], device='mps:0')\n",
      "Iteration 26780 Training loss 0.05717233195900917 Validation loss 0.06141216680407524 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.8699],\n",
      "        [0.9339]], device='mps:0')\n",
      "Iteration 26790 Training loss 0.06916787475347519 Validation loss 0.06169057637453079 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.8039],\n",
      "        [0.9666]], device='mps:0')\n",
      "Iteration 26800 Training loss 0.0592413991689682 Validation loss 0.06146175041794777 Accuracy 0.831125020980835\n",
      "Output tensor([[0.5340],\n",
      "        [0.9050]], device='mps:0')\n",
      "Iteration 26810 Training loss 0.05370638892054558 Validation loss 0.06154552847146988 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.6327],\n",
      "        [0.7838]], device='mps:0')\n",
      "Iteration 26820 Training loss 0.05639050528407097 Validation loss 0.06145363301038742 Accuracy 0.831125020980835\n",
      "Output tensor([[0.2142],\n",
      "        [0.6089]], device='mps:0')\n",
      "Iteration 26830 Training loss 0.05928493291139603 Validation loss 0.06137612462043762 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.7497],\n",
      "        [0.9164]], device='mps:0')\n",
      "Iteration 26840 Training loss 0.0644535943865776 Validation loss 0.06171081215143204 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.1074],\n",
      "        [0.4474]], device='mps:0')\n",
      "Iteration 26850 Training loss 0.06011900678277016 Validation loss 0.061398521065711975 Accuracy 0.831000030040741\n",
      "Output tensor([[0.3128],\n",
      "        [0.0488]], device='mps:0')\n",
      "Iteration 26860 Training loss 0.056552283465862274 Validation loss 0.06146590784192085 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.1831],\n",
      "        [0.8353]], device='mps:0')\n",
      "Iteration 26870 Training loss 0.05742347240447998 Validation loss 0.061370089650154114 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0535],\n",
      "        [0.1803]], device='mps:0')\n",
      "Iteration 26880 Training loss 0.0643264502286911 Validation loss 0.06138775125145912 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.3569],\n",
      "        [0.4143]], device='mps:0')\n",
      "Iteration 26890 Training loss 0.056232087314128876 Validation loss 0.061377283185720444 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.0311],\n",
      "        [0.0568]], device='mps:0')\n",
      "Iteration 26900 Training loss 0.0677405446767807 Validation loss 0.06136419251561165 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0472],\n",
      "        [0.8852]], device='mps:0')\n",
      "Iteration 26910 Training loss 0.06737913936376572 Validation loss 0.061376605182886124 Accuracy 0.831000030040741\n",
      "Output tensor([[0.7705],\n",
      "        [0.9752]], device='mps:0')\n",
      "Iteration 26920 Training loss 0.061778564006090164 Validation loss 0.06137369945645332 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.1064],\n",
      "        [0.8865]], device='mps:0')\n",
      "Iteration 26930 Training loss 0.05147421360015869 Validation loss 0.0613645501434803 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.1537],\n",
      "        [0.0951]], device='mps:0')\n",
      "Iteration 26940 Training loss 0.057943861931562424 Validation loss 0.06151348724961281 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.2628],\n",
      "        [0.2061]], device='mps:0')\n",
      "Iteration 26950 Training loss 0.05404609441757202 Validation loss 0.06135068088769913 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.1982],\n",
      "        [0.0114]], device='mps:0')\n",
      "Iteration 26960 Training loss 0.05874008685350418 Validation loss 0.061344247311353683 Accuracy 0.831250011920929\n",
      "Output tensor([[0.9673],\n",
      "        [0.9102]], device='mps:0')\n",
      "Iteration 26970 Training loss 0.06225807964801788 Validation loss 0.06136244162917137 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.9157],\n",
      "        [0.9513]], device='mps:0')\n",
      "Iteration 26980 Training loss 0.06338776648044586 Validation loss 0.06134478747844696 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.3935],\n",
      "        [0.9569]], device='mps:0')\n",
      "Iteration 26990 Training loss 0.05094658583402634 Validation loss 0.06138022243976593 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.2849],\n",
      "        [0.3985]], device='mps:0')\n",
      "Iteration 27000 Training loss 0.0603318028151989 Validation loss 0.06148316338658333 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.5377],\n",
      "        [0.0065]], device='mps:0')\n",
      "Iteration 27010 Training loss 0.07036542147397995 Validation loss 0.06140998750925064 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.9475],\n",
      "        [0.9134]], device='mps:0')\n",
      "Iteration 27020 Training loss 0.054641738533973694 Validation loss 0.061358753591775894 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.2055],\n",
      "        [0.0698]], device='mps:0')\n",
      "Iteration 27030 Training loss 0.05433905869722366 Validation loss 0.06140851229429245 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.9524],\n",
      "        [0.2910]], device='mps:0')\n",
      "Iteration 27040 Training loss 0.05948028340935707 Validation loss 0.06143500655889511 Accuracy 0.831125020980835\n",
      "Output tensor([[0.8613],\n",
      "        [0.8608]], device='mps:0')\n",
      "Iteration 27050 Training loss 0.05278301611542702 Validation loss 0.06136137247085571 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.9583],\n",
      "        [0.1847]], device='mps:0')\n",
      "Iteration 27060 Training loss 0.06356556713581085 Validation loss 0.061367351561784744 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.0345],\n",
      "        [0.9026]], device='mps:0')\n",
      "Iteration 27070 Training loss 0.05572061240673065 Validation loss 0.061442699283361435 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.0669],\n",
      "        [0.1891]], device='mps:0')\n",
      "Iteration 27080 Training loss 0.06634777039289474 Validation loss 0.06134306639432907 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.5745],\n",
      "        [0.4507]], device='mps:0')\n",
      "Iteration 27090 Training loss 0.05771437659859657 Validation loss 0.06134960427880287 Accuracy 0.831125020980835\n",
      "Output tensor([[0.1910],\n",
      "        [0.8734]], device='mps:0')\n",
      "Iteration 27100 Training loss 0.0528179332613945 Validation loss 0.06137297675013542 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.8478],\n",
      "        [0.1908]], device='mps:0')\n",
      "Iteration 27110 Training loss 0.057486388832330704 Validation loss 0.06133979931473732 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.9694],\n",
      "        [0.8324]], device='mps:0')\n",
      "Iteration 27120 Training loss 0.06760162860155106 Validation loss 0.06142120063304901 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9739],\n",
      "        [0.2441]], device='mps:0')\n",
      "Iteration 27130 Training loss 0.06269678473472595 Validation loss 0.06139269471168518 Accuracy 0.831000030040741\n",
      "Output tensor([[0.0818],\n",
      "        [0.6852]], device='mps:0')\n",
      "Iteration 27140 Training loss 0.06475239247083664 Validation loss 0.06141030788421631 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.2651],\n",
      "        [0.1064]], device='mps:0')\n",
      "Iteration 27150 Training loss 0.04873266443610191 Validation loss 0.06135399267077446 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.6976],\n",
      "        [0.1762]], device='mps:0')\n",
      "Iteration 27160 Training loss 0.0682668685913086 Validation loss 0.06150078400969505 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.6622],\n",
      "        [0.1944]], device='mps:0')\n",
      "Iteration 27170 Training loss 0.05801000818610191 Validation loss 0.061360202729701996 Accuracy 0.831000030040741\n",
      "Output tensor([[0.3943],\n",
      "        [0.7117]], device='mps:0')\n",
      "Iteration 27180 Training loss 0.05944979563355446 Validation loss 0.06153711676597595 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8919],\n",
      "        [0.0044]], device='mps:0')\n",
      "Iteration 27190 Training loss 0.059284508228302 Validation loss 0.06135699898004532 Accuracy 0.831250011920929\n",
      "Output tensor([[0.3010],\n",
      "        [0.4540]], device='mps:0')\n",
      "Iteration 27200 Training loss 0.059697434306144714 Validation loss 0.061559733003377914 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.0655],\n",
      "        [0.8538]], device='mps:0')\n",
      "Iteration 27210 Training loss 0.061031851917505264 Validation loss 0.06133158504962921 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.0218],\n",
      "        [0.0031]], device='mps:0')\n",
      "Iteration 27220 Training loss 0.07003257423639297 Validation loss 0.06133494898676872 Accuracy 0.831250011920929\n",
      "Output tensor([[0.7054],\n",
      "        [0.5344]], device='mps:0')\n",
      "Iteration 27230 Training loss 0.05258960649371147 Validation loss 0.06147642061114311 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.4702],\n",
      "        [0.0195]], device='mps:0')\n",
      "Iteration 27240 Training loss 0.05401236563920975 Validation loss 0.06135006248950958 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.9200],\n",
      "        [0.9670]], device='mps:0')\n",
      "Iteration 27250 Training loss 0.05346139892935753 Validation loss 0.06139471009373665 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.5328],\n",
      "        [0.9736]], device='mps:0')\n",
      "Iteration 27260 Training loss 0.06765888631343842 Validation loss 0.061497095972299576 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.9165],\n",
      "        [0.2510]], device='mps:0')\n",
      "Iteration 27270 Training loss 0.06108063459396362 Validation loss 0.06134756654500961 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.5926],\n",
      "        [0.5810]], device='mps:0')\n",
      "Iteration 27280 Training loss 0.052831973880529404 Validation loss 0.06140805408358574 Accuracy 0.831125020980835\n",
      "Output tensor([[0.4698],\n",
      "        [0.8850]], device='mps:0')\n",
      "Iteration 27290 Training loss 0.062339551746845245 Validation loss 0.0614803172647953 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.9228],\n",
      "        [0.0485]], device='mps:0')\n",
      "Iteration 27300 Training loss 0.06390586495399475 Validation loss 0.061371903866529465 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.6523],\n",
      "        [0.0990]], device='mps:0')\n",
      "Iteration 27310 Training loss 0.05681968107819557 Validation loss 0.06132259592413902 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.4168],\n",
      "        [0.0803]], device='mps:0')\n",
      "Iteration 27320 Training loss 0.06329481303691864 Validation loss 0.061320915818214417 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.6770],\n",
      "        [0.4864]], device='mps:0')\n",
      "Iteration 27330 Training loss 0.054968204349279404 Validation loss 0.061468176543712616 Accuracy 0.830500066280365\n",
      "Output tensor([[0.6393],\n",
      "        [0.9543]], device='mps:0')\n",
      "Iteration 27340 Training loss 0.05891088768839836 Validation loss 0.06139769032597542 Accuracy 0.831125020980835\n",
      "Output tensor([[0.0556],\n",
      "        [0.5048]], device='mps:0')\n",
      "Iteration 27350 Training loss 0.060663431882858276 Validation loss 0.061335865408182144 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.0324],\n",
      "        [0.2546]], device='mps:0')\n",
      "Iteration 27360 Training loss 0.06618162244558334 Validation loss 0.061319950968027115 Accuracy 0.831000030040741\n",
      "Output tensor([[0.8656],\n",
      "        [0.9354]], device='mps:0')\n",
      "Iteration 27370 Training loss 0.05565928295254707 Validation loss 0.06137479096651077 Accuracy 0.831250011920929\n",
      "Output tensor([[0.9476],\n",
      "        [0.3766]], device='mps:0')\n",
      "Iteration 27380 Training loss 0.06661460548639297 Validation loss 0.061330731958150864 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.8474],\n",
      "        [0.1504]], device='mps:0')\n",
      "Iteration 27390 Training loss 0.058085549622774124 Validation loss 0.0614890456199646 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.2396],\n",
      "        [0.8698]], device='mps:0')\n",
      "Iteration 27400 Training loss 0.06213369593024254 Validation loss 0.06133461371064186 Accuracy 0.831125020980835\n",
      "Output tensor([[0.4923],\n",
      "        [0.0949]], device='mps:0')\n",
      "Iteration 27410 Training loss 0.06248978525400162 Validation loss 0.06132495403289795 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.2899],\n",
      "        [0.8547]], device='mps:0')\n",
      "Iteration 27420 Training loss 0.06735269725322723 Validation loss 0.061337463557720184 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.5404],\n",
      "        [0.1437]], device='mps:0')\n",
      "Iteration 27430 Training loss 0.05592520534992218 Validation loss 0.06147168576717377 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.3541],\n",
      "        [0.7959]], device='mps:0')\n",
      "Iteration 27440 Training loss 0.05634531006217003 Validation loss 0.06130606681108475 Accuracy 0.831125020980835\n",
      "Output tensor([[0.3862],\n",
      "        [0.2228]], device='mps:0')\n",
      "Iteration 27450 Training loss 0.06754733622074127 Validation loss 0.06140507757663727 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.5508],\n",
      "        [0.9224]], device='mps:0')\n",
      "Iteration 27460 Training loss 0.06834064424037933 Validation loss 0.061363257467746735 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.9471],\n",
      "        [0.9072]], device='mps:0')\n",
      "Iteration 27470 Training loss 0.048060640692710876 Validation loss 0.06130281463265419 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.3532],\n",
      "        [0.6865]], device='mps:0')\n",
      "Iteration 27480 Training loss 0.05899542197585106 Validation loss 0.061317019164562225 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8615],\n",
      "        [0.3243]], device='mps:0')\n",
      "Iteration 27490 Training loss 0.06007615104317665 Validation loss 0.06130421906709671 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.9698],\n",
      "        [0.1499]], device='mps:0')\n",
      "Iteration 27500 Training loss 0.06058260798454285 Validation loss 0.06129263713955879 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.8964],\n",
      "        [0.5138]], device='mps:0')\n",
      "Iteration 27510 Training loss 0.06448400020599365 Validation loss 0.06133738532662392 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.0759],\n",
      "        [0.4814]], device='mps:0')\n",
      "Iteration 27520 Training loss 0.06308764219284058 Validation loss 0.06131025403738022 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.1030],\n",
      "        [0.0428]], device='mps:0')\n",
      "Iteration 27530 Training loss 0.05535832419991493 Validation loss 0.061337124556303024 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.2180],\n",
      "        [0.0889]], device='mps:0')\n",
      "Iteration 27540 Training loss 0.057366274297237396 Validation loss 0.0612960085272789 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.1023],\n",
      "        [0.1193]], device='mps:0')\n",
      "Iteration 27550 Training loss 0.059871502220630646 Validation loss 0.061296653002500534 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.2485],\n",
      "        [0.9560]], device='mps:0')\n",
      "Iteration 27560 Training loss 0.053208768367767334 Validation loss 0.06142788380384445 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.8009],\n",
      "        [0.8136]], device='mps:0')\n",
      "Iteration 27570 Training loss 0.06027857959270477 Validation loss 0.061393436044454575 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1087],\n",
      "        [0.9294]], device='mps:0')\n",
      "Iteration 27580 Training loss 0.06494026631116867 Validation loss 0.06132149696350098 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.7661],\n",
      "        [0.9754]], device='mps:0')\n",
      "Iteration 27590 Training loss 0.05144603177905083 Validation loss 0.061293620616197586 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.5305],\n",
      "        [0.6061]], device='mps:0')\n",
      "Iteration 27600 Training loss 0.05876489356160164 Validation loss 0.061441149562597275 Accuracy 0.8301250338554382\n",
      "Output tensor([[0.1438],\n",
      "        [0.9457]], device='mps:0')\n",
      "Iteration 27610 Training loss 0.05782308429479599 Validation loss 0.061278875917196274 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0738],\n",
      "        [0.8287]], device='mps:0')\n",
      "Iteration 27620 Training loss 0.06325741857290268 Validation loss 0.06138346716761589 Accuracy 0.831125020980835\n",
      "Output tensor([[0.2783],\n",
      "        [0.2921]], device='mps:0')\n",
      "Iteration 27630 Training loss 0.05906706303358078 Validation loss 0.06132937967777252 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.7702],\n",
      "        [0.1674]], device='mps:0')\n",
      "Iteration 27640 Training loss 0.06368529796600342 Validation loss 0.061332713812589645 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.6131],\n",
      "        [0.3908]], device='mps:0')\n",
      "Iteration 27650 Training loss 0.06114381551742554 Validation loss 0.061299145221710205 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.0984],\n",
      "        [0.0585]], device='mps:0')\n",
      "Iteration 27660 Training loss 0.06396670639514923 Validation loss 0.061579395085573196 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.5209],\n",
      "        [0.6001]], device='mps:0')\n",
      "Iteration 27670 Training loss 0.05588577315211296 Validation loss 0.06127183884382248 Accuracy 0.831250011920929\n",
      "Output tensor([[0.9701],\n",
      "        [0.9765]], device='mps:0')\n",
      "Iteration 27680 Training loss 0.06248920410871506 Validation loss 0.061314813792705536 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.0202],\n",
      "        [0.1802]], device='mps:0')\n",
      "Iteration 27690 Training loss 0.0521242618560791 Validation loss 0.06129303574562073 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.8848],\n",
      "        [0.9770]], device='mps:0')\n",
      "Iteration 27700 Training loss 0.06317088007926941 Validation loss 0.061313457787036896 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1070],\n",
      "        [0.9411]], device='mps:0')\n",
      "Iteration 27710 Training loss 0.056362178176641464 Validation loss 0.061855364590883255 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.1969],\n",
      "        [0.9654]], device='mps:0')\n",
      "Iteration 27720 Training loss 0.06330659985542297 Validation loss 0.06128392368555069 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.7043],\n",
      "        [0.9859]], device='mps:0')\n",
      "Iteration 27730 Training loss 0.06035325303673744 Validation loss 0.06139586120843887 Accuracy 0.8303750157356262\n",
      "Output tensor([[0.0256],\n",
      "        [0.1209]], device='mps:0')\n",
      "Iteration 27740 Training loss 0.058313701301813126 Validation loss 0.061256203800439835 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.2211],\n",
      "        [0.9730]], device='mps:0')\n",
      "Iteration 27750 Training loss 0.05251646041870117 Validation loss 0.061364494264125824 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.4295],\n",
      "        [0.9985]], device='mps:0')\n",
      "Iteration 27760 Training loss 0.06871654093265533 Validation loss 0.06131535768508911 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9258],\n",
      "        [0.9112]], device='mps:0')\n",
      "Iteration 27770 Training loss 0.056355856359004974 Validation loss 0.06128758564591408 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.6074],\n",
      "        [0.2189]], device='mps:0')\n",
      "Iteration 27780 Training loss 0.053569495677948 Validation loss 0.06141076982021332 Accuracy 0.830875039100647\n",
      "Output tensor([[0.1778],\n",
      "        [0.0574]], device='mps:0')\n",
      "Iteration 27790 Training loss 0.05107750743627548 Validation loss 0.0612310916185379 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.8082],\n",
      "        [0.8425]], device='mps:0')\n",
      "Iteration 27800 Training loss 0.0608772449195385 Validation loss 0.06123409420251846 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0614],\n",
      "        [0.2607]], device='mps:0')\n",
      "Iteration 27810 Training loss 0.055442146956920624 Validation loss 0.06126405671238899 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.8609],\n",
      "        [0.9622]], device='mps:0')\n",
      "Iteration 27820 Training loss 0.05804024264216423 Validation loss 0.06124354526400566 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.9086],\n",
      "        [0.0695]], device='mps:0')\n",
      "Iteration 27830 Training loss 0.060807570815086365 Validation loss 0.061425428837537766 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.3586],\n",
      "        [0.4597]], device='mps:0')\n",
      "Iteration 27840 Training loss 0.06090556085109711 Validation loss 0.061429187655448914 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.8880],\n",
      "        [0.3569]], device='mps:0')\n",
      "Iteration 27850 Training loss 0.06034029647707939 Validation loss 0.0612361878156662 Accuracy 0.831250011920929\n",
      "Output tensor([[0.1384],\n",
      "        [0.0936]], device='mps:0')\n",
      "Iteration 27860 Training loss 0.056863926351070404 Validation loss 0.061273958534002304 Accuracy 0.831125020980835\n",
      "Output tensor([[0.8641],\n",
      "        [0.6909]], device='mps:0')\n",
      "Iteration 27870 Training loss 0.05452531576156616 Validation loss 0.06126154214143753 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.5103],\n",
      "        [0.0663]], device='mps:0')\n",
      "Iteration 27880 Training loss 0.05889104679226875 Validation loss 0.06123434007167816 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.2991],\n",
      "        [0.1620]], device='mps:0')\n",
      "Iteration 27890 Training loss 0.06207700073719025 Validation loss 0.061381563544273376 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0873],\n",
      "        [0.6295]], device='mps:0')\n",
      "Iteration 27900 Training loss 0.060113053768873215 Validation loss 0.061226122081279755 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.7215],\n",
      "        [0.7633]], device='mps:0')\n",
      "Iteration 27910 Training loss 0.05664465203881264 Validation loss 0.06123119965195656 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.7914],\n",
      "        [0.4440]], device='mps:0')\n",
      "Iteration 27920 Training loss 0.060262806713581085 Validation loss 0.061364226043224335 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.1692],\n",
      "        [0.9337]], device='mps:0')\n",
      "Iteration 27930 Training loss 0.06157701462507248 Validation loss 0.061228204518556595 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.0330],\n",
      "        [0.1873]], device='mps:0')\n",
      "Iteration 27940 Training loss 0.06298298388719559 Validation loss 0.06123926118016243 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.9632],\n",
      "        [0.0657]], device='mps:0')\n",
      "Iteration 27950 Training loss 0.07001621276140213 Validation loss 0.06141127273440361 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.9278],\n",
      "        [0.8820]], device='mps:0')\n",
      "Iteration 27960 Training loss 0.05568917095661163 Validation loss 0.06130996719002724 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.0295],\n",
      "        [0.8629]], device='mps:0')\n",
      "Iteration 27970 Training loss 0.062355827540159225 Validation loss 0.0612330362200737 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.8704],\n",
      "        [0.2047]], device='mps:0')\n",
      "Iteration 27980 Training loss 0.06087496504187584 Validation loss 0.06134252995252609 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.2617],\n",
      "        [0.7288]], device='mps:0')\n",
      "Iteration 27990 Training loss 0.06401664018630981 Validation loss 0.06134185940027237 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.7480],\n",
      "        [0.9194]], device='mps:0')\n",
      "Iteration 28000 Training loss 0.05905667692422867 Validation loss 0.06124420464038849 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.3287],\n",
      "        [0.7465]], device='mps:0')\n",
      "Iteration 28010 Training loss 0.05816604569554329 Validation loss 0.06121741607785225 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.4112],\n",
      "        [0.1176]], device='mps:0')\n",
      "Iteration 28020 Training loss 0.06822092086076736 Validation loss 0.061239030212163925 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.4241],\n",
      "        [0.7395]], device='mps:0')\n",
      "Iteration 28030 Training loss 0.06167476624250412 Validation loss 0.06132877245545387 Accuracy 0.830875039100647\n",
      "Output tensor([[0.5592],\n",
      "        [0.4245]], device='mps:0')\n",
      "Iteration 28040 Training loss 0.05977845564484596 Validation loss 0.06121569499373436 Accuracy 0.831000030040741\n",
      "Output tensor([[0.0825],\n",
      "        [0.5306]], device='mps:0')\n",
      "Iteration 28050 Training loss 0.05544300004839897 Validation loss 0.06140461936593056 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.2763],\n",
      "        [0.9403]], device='mps:0')\n",
      "Iteration 28060 Training loss 0.06070529669523239 Validation loss 0.06129198148846626 Accuracy 0.830625057220459\n",
      "Output tensor([[0.4268],\n",
      "        [0.0848]], device='mps:0')\n",
      "Iteration 28070 Training loss 0.056856174021959305 Validation loss 0.06120564416050911 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.7862],\n",
      "        [0.9823]], device='mps:0')\n",
      "Iteration 28080 Training loss 0.057210180908441544 Validation loss 0.06121200695633888 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.0494],\n",
      "        [0.0737]], device='mps:0')\n",
      "Iteration 28090 Training loss 0.055522579699754715 Validation loss 0.06135891377925873 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.0768],\n",
      "        [0.1859]], device='mps:0')\n",
      "Iteration 28100 Training loss 0.05838152393698692 Validation loss 0.06131179630756378 Accuracy 0.831000030040741\n",
      "Output tensor([[0.0178],\n",
      "        [0.1820]], device='mps:0')\n",
      "Iteration 28110 Training loss 0.058104269206523895 Validation loss 0.061379678547382355 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.7222],\n",
      "        [0.2029]], device='mps:0')\n",
      "Iteration 28120 Training loss 0.05691513791680336 Validation loss 0.06123494729399681 Accuracy 0.831250011920929\n",
      "Output tensor([[0.8204],\n",
      "        [0.9557]], device='mps:0')\n",
      "Iteration 28130 Training loss 0.05700536444783211 Validation loss 0.06125405430793762 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.9761],\n",
      "        [0.3125]], device='mps:0')\n",
      "Iteration 28140 Training loss 0.054205331951379776 Validation loss 0.06119723618030548 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.3336],\n",
      "        [0.9357]], device='mps:0')\n",
      "Iteration 28150 Training loss 0.06164867803454399 Validation loss 0.06121785566210747 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.6329],\n",
      "        [0.3058]], device='mps:0')\n",
      "Iteration 28160 Training loss 0.05995522439479828 Validation loss 0.061242882162332535 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0461],\n",
      "        [0.4621]], device='mps:0')\n",
      "Iteration 28170 Training loss 0.06541454046964645 Validation loss 0.06128396838903427 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.0830],\n",
      "        [0.6558]], device='mps:0')\n",
      "Iteration 28180 Training loss 0.057965319603681564 Validation loss 0.06151498481631279 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.0600],\n",
      "        [0.0603]], device='mps:0')\n",
      "Iteration 28190 Training loss 0.055212657898664474 Validation loss 0.06122802570462227 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.3209],\n",
      "        [0.4199]], device='mps:0')\n",
      "Iteration 28200 Training loss 0.05757230520248413 Validation loss 0.061261773109436035 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.0622],\n",
      "        [0.9294]], device='mps:0')\n",
      "Iteration 28210 Training loss 0.06024319678544998 Validation loss 0.06118771806359291 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9522],\n",
      "        [0.9863]], device='mps:0')\n",
      "Iteration 28220 Training loss 0.05760377272963524 Validation loss 0.06118195503950119 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.8301],\n",
      "        [0.5362]], device='mps:0')\n",
      "Iteration 28230 Training loss 0.06204845756292343 Validation loss 0.061640918254852295 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.2160],\n",
      "        [0.7642]], device='mps:0')\n",
      "Iteration 28240 Training loss 0.0634675845503807 Validation loss 0.0612429678440094 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.7112],\n",
      "        [0.9418]], device='mps:0')\n",
      "Iteration 28250 Training loss 0.06123882159590721 Validation loss 0.06119675561785698 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.7525],\n",
      "        [0.6886]], device='mps:0')\n",
      "Iteration 28260 Training loss 0.055960044264793396 Validation loss 0.061496552079916 Accuracy 0.830875039100647\n",
      "Output tensor([[0.3569],\n",
      "        [0.9562]], device='mps:0')\n",
      "Iteration 28270 Training loss 0.07171540707349777 Validation loss 0.06117524206638336 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0256],\n",
      "        [0.9373]], device='mps:0')\n",
      "Iteration 28280 Training loss 0.060421962291002274 Validation loss 0.06119922176003456 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.6942],\n",
      "        [0.5989]], device='mps:0')\n",
      "Iteration 28290 Training loss 0.062420327216386795 Validation loss 0.06123631075024605 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.0388],\n",
      "        [0.8561]], device='mps:0')\n",
      "Iteration 28300 Training loss 0.06066722422838211 Validation loss 0.06118231266736984 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.9633],\n",
      "        [0.5419]], device='mps:0')\n",
      "Iteration 28310 Training loss 0.05795056372880936 Validation loss 0.06115357205271721 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.4610],\n",
      "        [0.4572]], device='mps:0')\n",
      "Iteration 28320 Training loss 0.05497321859002113 Validation loss 0.061150409281253815 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.2803],\n",
      "        [0.8825]], device='mps:0')\n",
      "Iteration 28330 Training loss 0.05279600992798805 Validation loss 0.061184536665678024 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.0562],\n",
      "        [0.0103]], device='mps:0')\n",
      "Iteration 28340 Training loss 0.062197890132665634 Validation loss 0.0611945278942585 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.7812],\n",
      "        [0.8584]], device='mps:0')\n",
      "Iteration 28350 Training loss 0.06259710341691971 Validation loss 0.0611538328230381 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.2895],\n",
      "        [0.8609]], device='mps:0')\n",
      "Iteration 28360 Training loss 0.049409303814172745 Validation loss 0.06116538494825363 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0957],\n",
      "        [0.5098]], device='mps:0')\n",
      "Iteration 28370 Training loss 0.06109205260872841 Validation loss 0.061218179762363434 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.9682],\n",
      "        [0.2607]], device='mps:0')\n",
      "Iteration 28380 Training loss 0.05549106374382973 Validation loss 0.06117399036884308 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.8330],\n",
      "        [0.1516]], device='mps:0')\n",
      "Iteration 28390 Training loss 0.05826055258512497 Validation loss 0.06166861578822136 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.9630],\n",
      "        [0.4390]], device='mps:0')\n",
      "Iteration 28400 Training loss 0.058067552745342255 Validation loss 0.06116095930337906 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.2035],\n",
      "        [0.8965]], device='mps:0')\n",
      "Iteration 28410 Training loss 0.056078504770994186 Validation loss 0.061171818524599075 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.9045],\n",
      "        [0.8344]], device='mps:0')\n",
      "Iteration 28420 Training loss 0.05917355418205261 Validation loss 0.06116529554128647 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.8484],\n",
      "        [0.6867]], device='mps:0')\n",
      "Iteration 28430 Training loss 0.051305465400218964 Validation loss 0.06128401681780815 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.0696],\n",
      "        [0.9224]], device='mps:0')\n",
      "Iteration 28440 Training loss 0.05435134842991829 Validation loss 0.061451856046915054 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.9586],\n",
      "        [0.8896]], device='mps:0')\n",
      "Iteration 28450 Training loss 0.05276830494403839 Validation loss 0.06132064387202263 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.2269],\n",
      "        [0.1147]], device='mps:0')\n",
      "Iteration 28460 Training loss 0.06188489869236946 Validation loss 0.06116212159395218 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.8921],\n",
      "        [0.4322]], device='mps:0')\n",
      "Iteration 28470 Training loss 0.06281246244907379 Validation loss 0.06118549406528473 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.7477],\n",
      "        [0.8495]], device='mps:0')\n",
      "Iteration 28480 Training loss 0.05507272481918335 Validation loss 0.061146702617406845 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.1338],\n",
      "        [0.7159]], device='mps:0')\n",
      "Iteration 28490 Training loss 0.06099734827876091 Validation loss 0.06115841865539551 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.8788],\n",
      "        [0.9331]], device='mps:0')\n",
      "Iteration 28500 Training loss 0.0655403584241867 Validation loss 0.061150044202804565 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.0381],\n",
      "        [0.8305]], device='mps:0')\n",
      "Iteration 28510 Training loss 0.0572686642408371 Validation loss 0.06144658848643303 Accuracy 0.8296250104904175\n",
      "Output tensor([[0.1402],\n",
      "        [0.9371]], device='mps:0')\n",
      "Iteration 28520 Training loss 0.06268276274204254 Validation loss 0.061147917062044144 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.4936],\n",
      "        [0.0190]], device='mps:0')\n",
      "Iteration 28530 Training loss 0.05643710121512413 Validation loss 0.061427049338817596 Accuracy 0.830750048160553\n",
      "Output tensor([[0.0723],\n",
      "        [0.1231]], device='mps:0')\n",
      "Iteration 28540 Training loss 0.05130642279982567 Validation loss 0.06113770976662636 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.5129],\n",
      "        [0.3601]], device='mps:0')\n",
      "Iteration 28550 Training loss 0.05260377749800682 Validation loss 0.061157938092947006 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.8785],\n",
      "        [0.7234]], device='mps:0')\n",
      "Iteration 28560 Training loss 0.059105709195137024 Validation loss 0.061271220445632935 Accuracy 0.830500066280365\n",
      "Output tensor([[0.7230],\n",
      "        [0.9137]], device='mps:0')\n",
      "Iteration 28570 Training loss 0.054878320544958115 Validation loss 0.061136335134506226 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.6469],\n",
      "        [0.8616]], device='mps:0')\n",
      "Iteration 28580 Training loss 0.06250578910112381 Validation loss 0.061130739748477936 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.0790],\n",
      "        [0.2045]], device='mps:0')\n",
      "Iteration 28590 Training loss 0.06242996081709862 Validation loss 0.061281152069568634 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.1485],\n",
      "        [0.9117]], device='mps:0')\n",
      "Iteration 28600 Training loss 0.06108153238892555 Validation loss 0.061145342886447906 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.9217],\n",
      "        [0.9150]], device='mps:0')\n",
      "Iteration 28610 Training loss 0.05915894731879234 Validation loss 0.06112626567482948 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.1710],\n",
      "        [0.8865]], device='mps:0')\n",
      "Iteration 28620 Training loss 0.04733862727880478 Validation loss 0.061152320355176926 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.5451],\n",
      "        [0.2355]], device='mps:0')\n",
      "Iteration 28630 Training loss 0.056111324578523636 Validation loss 0.061124879866838455 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.0617],\n",
      "        [0.0230]], device='mps:0')\n",
      "Iteration 28640 Training loss 0.06267303228378296 Validation loss 0.06136853247880936 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.2985],\n",
      "        [0.0297]], device='mps:0')\n",
      "Iteration 28650 Training loss 0.05696975067257881 Validation loss 0.06118811294436455 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.1461],\n",
      "        [0.2407]], device='mps:0')\n",
      "Iteration 28660 Training loss 0.06195077672600746 Validation loss 0.06115294620394707 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1967],\n",
      "        [0.0790]], device='mps:0')\n",
      "Iteration 28670 Training loss 0.06092967838048935 Validation loss 0.06153058633208275 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.7648],\n",
      "        [0.1669]], device='mps:0')\n",
      "Iteration 28680 Training loss 0.0590624138712883 Validation loss 0.06112132593989372 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.7102],\n",
      "        [0.8287]], device='mps:0')\n",
      "Iteration 28690 Training loss 0.060456693172454834 Validation loss 0.06114857643842697 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8578],\n",
      "        [0.1677]], device='mps:0')\n",
      "Iteration 28700 Training loss 0.05645902454853058 Validation loss 0.0611073337495327 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.7398],\n",
      "        [0.0190]], device='mps:0')\n",
      "Iteration 28710 Training loss 0.058680858463048935 Validation loss 0.061315156519412994 Accuracy 0.830625057220459\n",
      "Output tensor([[0.4137],\n",
      "        [0.8076]], device='mps:0')\n",
      "Iteration 28720 Training loss 0.05801668390631676 Validation loss 0.06112014129757881 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.9762],\n",
      "        [0.0956]], device='mps:0')\n",
      "Iteration 28730 Training loss 0.05503477156162262 Validation loss 0.06120206415653229 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.0863],\n",
      "        [0.1497]], device='mps:0')\n",
      "Iteration 28740 Training loss 0.051448553800582886 Validation loss 0.061123188585042953 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.9116],\n",
      "        [0.5380]], device='mps:0')\n",
      "Iteration 28750 Training loss 0.0671079084277153 Validation loss 0.06110052019357681 Accuracy 0.831000030040741\n",
      "Output tensor([[0.0788],\n",
      "        [0.8329]], device='mps:0')\n",
      "Iteration 28760 Training loss 0.051956892013549805 Validation loss 0.06109006702899933 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.2381],\n",
      "        [0.7132]], device='mps:0')\n",
      "Iteration 28770 Training loss 0.06265397369861603 Validation loss 0.06108575686812401 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.4850],\n",
      "        [0.4883]], device='mps:0')\n",
      "Iteration 28780 Training loss 0.06387904286384583 Validation loss 0.061151325702667236 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.9211],\n",
      "        [0.0891]], device='mps:0')\n",
      "Iteration 28790 Training loss 0.056586336344480515 Validation loss 0.061082515865564346 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.9298],\n",
      "        [0.7238]], device='mps:0')\n",
      "Iteration 28800 Training loss 0.05734061449766159 Validation loss 0.06108960881829262 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.4227],\n",
      "        [0.3483]], device='mps:0')\n",
      "Iteration 28810 Training loss 0.059785690158605576 Validation loss 0.06119066849350929 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.8477],\n",
      "        [0.0633]], device='mps:0')\n",
      "Iteration 28820 Training loss 0.056350477039813995 Validation loss 0.06113826856017113 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.1949],\n",
      "        [0.2317]], device='mps:0')\n",
      "Iteration 28830 Training loss 0.06219916045665741 Validation loss 0.06108417361974716 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.1465],\n",
      "        [0.5484]], device='mps:0')\n",
      "Iteration 28840 Training loss 0.06352483481168747 Validation loss 0.061100322753190994 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.1040],\n",
      "        [0.9643]], device='mps:0')\n",
      "Iteration 28850 Training loss 0.05354064702987671 Validation loss 0.06111247092485428 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.0765],\n",
      "        [0.4561]], device='mps:0')\n",
      "Iteration 28860 Training loss 0.06300859153270721 Validation loss 0.061085350811481476 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.7992],\n",
      "        [0.0880]], device='mps:0')\n",
      "Iteration 28870 Training loss 0.061520833522081375 Validation loss 0.06108345463871956 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.0577],\n",
      "        [0.1172]], device='mps:0')\n",
      "Iteration 28880 Training loss 0.05143020674586296 Validation loss 0.061105165630578995 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.7117],\n",
      "        [0.0722]], device='mps:0')\n",
      "Iteration 28890 Training loss 0.05832543596625328 Validation loss 0.0613817423582077 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.3890],\n",
      "        [0.7854]], device='mps:0')\n",
      "Iteration 28900 Training loss 0.0696500688791275 Validation loss 0.061276569962501526 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.8521],\n",
      "        [0.8154]], device='mps:0')\n",
      "Iteration 28910 Training loss 0.05558932200074196 Validation loss 0.06109029799699783 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.1088],\n",
      "        [0.6064]], device='mps:0')\n",
      "Iteration 28920 Training loss 0.0585133358836174 Validation loss 0.06122605502605438 Accuracy 0.831000030040741\n",
      "Output tensor([[0.1017],\n",
      "        [0.5851]], device='mps:0')\n",
      "Iteration 28930 Training loss 0.05903761833906174 Validation loss 0.06109738349914551 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.5648],\n",
      "        [0.5620]], device='mps:0')\n",
      "Iteration 28940 Training loss 0.05182260274887085 Validation loss 0.061084993183612823 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.5708],\n",
      "        [0.8749]], device='mps:0')\n",
      "Iteration 28950 Training loss 0.0552322082221508 Validation loss 0.06107328459620476 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.3908],\n",
      "        [0.3567]], device='mps:0')\n",
      "Iteration 28960 Training loss 0.050296735018491745 Validation loss 0.0610835999250412 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.9600],\n",
      "        [0.2057]], device='mps:0')\n",
      "Iteration 28970 Training loss 0.06356894224882126 Validation loss 0.0615399070084095 Accuracy 0.830625057220459\n",
      "Output tensor([[0.9961],\n",
      "        [0.9945]], device='mps:0')\n",
      "Iteration 28980 Training loss 0.06104692444205284 Validation loss 0.06107025593519211 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.0715],\n",
      "        [0.6693]], device='mps:0')\n",
      "Iteration 28990 Training loss 0.0609707236289978 Validation loss 0.06118011847138405 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.4868],\n",
      "        [0.3418]], device='mps:0')\n",
      "Iteration 29000 Training loss 0.048024602234363556 Validation loss 0.06106676533818245 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.6372],\n",
      "        [0.6634]], device='mps:0')\n",
      "Iteration 29010 Training loss 0.0604984350502491 Validation loss 0.0612490139901638 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.1733],\n",
      "        [0.2320]], device='mps:0')\n",
      "Iteration 29020 Training loss 0.062262654304504395 Validation loss 0.061213169246912 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.8877],\n",
      "        [0.8164]], device='mps:0')\n",
      "Iteration 29030 Training loss 0.06246542930603027 Validation loss 0.06107804551720619 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.6924],\n",
      "        [0.7979]], device='mps:0')\n",
      "Iteration 29040 Training loss 0.06078273802995682 Validation loss 0.06105675920844078 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1605],\n",
      "        [0.2935]], device='mps:0')\n",
      "Iteration 29050 Training loss 0.053667742758989334 Validation loss 0.06106649339199066 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.8454],\n",
      "        [0.0439]], device='mps:0')\n",
      "Iteration 29060 Training loss 0.06381325423717499 Validation loss 0.06107747182250023 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1478],\n",
      "        [0.7040]], device='mps:0')\n",
      "Iteration 29070 Training loss 0.05933813005685806 Validation loss 0.06109960749745369 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.4287],\n",
      "        [0.8462]], device='mps:0')\n",
      "Iteration 29080 Training loss 0.05907142534852028 Validation loss 0.061079613864421844 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.5240],\n",
      "        [0.7660]], device='mps:0')\n",
      "Iteration 29090 Training loss 0.05322279408574104 Validation loss 0.061107199639081955 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.7620],\n",
      "        [0.0541]], device='mps:0')\n",
      "Iteration 29100 Training loss 0.04842832311987877 Validation loss 0.06109002232551575 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.9301],\n",
      "        [0.0058]], device='mps:0')\n",
      "Iteration 29110 Training loss 0.0532650426030159 Validation loss 0.06106603145599365 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.1821],\n",
      "        [0.1424]], device='mps:0')\n",
      "Iteration 29120 Training loss 0.055985093116760254 Validation loss 0.06112552061676979 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.5526],\n",
      "        [0.8949]], device='mps:0')\n",
      "Iteration 29130 Training loss 0.061610277742147446 Validation loss 0.06106250360608101 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.2339],\n",
      "        [0.9139]], device='mps:0')\n",
      "Iteration 29140 Training loss 0.05848229303956032 Validation loss 0.06110928952693939 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.1516],\n",
      "        [0.0295]], device='mps:0')\n",
      "Iteration 29150 Training loss 0.053659047931432724 Validation loss 0.06105973199009895 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.6805],\n",
      "        [0.7644]], device='mps:0')\n",
      "Iteration 29160 Training loss 0.06155939772725105 Validation loss 0.06106152385473251 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.0464],\n",
      "        [0.2657]], device='mps:0')\n",
      "Iteration 29170 Training loss 0.05794167518615723 Validation loss 0.0611930713057518 Accuracy 0.831250011920929\n",
      "Output tensor([[0.5268],\n",
      "        [0.4951]], device='mps:0')\n",
      "Iteration 29180 Training loss 0.05743464455008507 Validation loss 0.06107385456562042 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.1409],\n",
      "        [0.1864]], device='mps:0')\n",
      "Iteration 29190 Training loss 0.055548448115587234 Validation loss 0.061067402362823486 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.5821],\n",
      "        [0.8889]], device='mps:0')\n",
      "Iteration 29200 Training loss 0.058194611221551895 Validation loss 0.06111469492316246 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.0236],\n",
      "        [0.3773]], device='mps:0')\n",
      "Iteration 29210 Training loss 0.053506724536418915 Validation loss 0.061164356768131256 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.3745],\n",
      "        [0.5370]], device='mps:0')\n",
      "Iteration 29220 Training loss 0.06347000598907471 Validation loss 0.06123868376016617 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.9147],\n",
      "        [0.7196]], device='mps:0')\n",
      "Iteration 29230 Training loss 0.05567328259348869 Validation loss 0.06117051839828491 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.9520],\n",
      "        [0.8334]], device='mps:0')\n",
      "Iteration 29240 Training loss 0.05993042513728142 Validation loss 0.061055541038513184 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.2319],\n",
      "        [0.6406]], device='mps:0')\n",
      "Iteration 29250 Training loss 0.06378878653049469 Validation loss 0.061453815549612045 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.2396],\n",
      "        [0.2138]], device='mps:0')\n",
      "Iteration 29260 Training loss 0.06568891555070877 Validation loss 0.06109285354614258 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.8359],\n",
      "        [0.9493]], device='mps:0')\n",
      "Iteration 29270 Training loss 0.05759180709719658 Validation loss 0.06118600443005562 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.9009],\n",
      "        [0.1554]], device='mps:0')\n",
      "Iteration 29280 Training loss 0.06825346499681473 Validation loss 0.0610453225672245 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.3999],\n",
      "        [0.2208]], device='mps:0')\n",
      "Iteration 29290 Training loss 0.06387396901845932 Validation loss 0.06109875068068504 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.8277],\n",
      "        [0.2840]], device='mps:0')\n",
      "Iteration 29300 Training loss 0.058708980679512024 Validation loss 0.06103762239217758 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.8749],\n",
      "        [0.9156]], device='mps:0')\n",
      "Iteration 29310 Training loss 0.054626282304525375 Validation loss 0.06116652861237526 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.7940],\n",
      "        [0.6295]], device='mps:0')\n",
      "Iteration 29320 Training loss 0.0575094111263752 Validation loss 0.061040569096803665 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.1818],\n",
      "        [0.9687]], device='mps:0')\n",
      "Iteration 29330 Training loss 0.06251614540815353 Validation loss 0.06105333939194679 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.3847],\n",
      "        [0.9374]], device='mps:0')\n",
      "Iteration 29340 Training loss 0.05898704752326012 Validation loss 0.061024703085422516 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.5749],\n",
      "        [0.1932]], device='mps:0')\n",
      "Iteration 29350 Training loss 0.06577492505311966 Validation loss 0.061021216213703156 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.4005],\n",
      "        [0.2792]], device='mps:0')\n",
      "Iteration 29360 Training loss 0.05502845719456673 Validation loss 0.06107368692755699 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.0172],\n",
      "        [0.9619]], device='mps:0')\n",
      "Iteration 29370 Training loss 0.06095359846949577 Validation loss 0.0612783245742321 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.3457],\n",
      "        [0.0126]], device='mps:0')\n",
      "Iteration 29380 Training loss 0.05763660743832588 Validation loss 0.06100732833147049 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.5880],\n",
      "        [0.7055]], device='mps:0')\n",
      "Iteration 29390 Training loss 0.06066320836544037 Validation loss 0.06104570999741554 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.0218],\n",
      "        [0.4784]], device='mps:0')\n",
      "Iteration 29400 Training loss 0.04973325505852699 Validation loss 0.06110801920294762 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.3358],\n",
      "        [0.5976]], device='mps:0')\n",
      "Iteration 29410 Training loss 0.047384075820446014 Validation loss 0.06101709231734276 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.0244],\n",
      "        [0.0680]], device='mps:0')\n",
      "Iteration 29420 Training loss 0.057455990463495255 Validation loss 0.061027541756629944 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.8260],\n",
      "        [0.9894]], device='mps:0')\n",
      "Iteration 29430 Training loss 0.06511081755161285 Validation loss 0.06104346737265587 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.8793],\n",
      "        [0.0639]], device='mps:0')\n",
      "Iteration 29440 Training loss 0.06066786125302315 Validation loss 0.06100694462656975 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.9454],\n",
      "        [0.5730]], device='mps:0')\n",
      "Iteration 29450 Training loss 0.06257300078868866 Validation loss 0.06102081760764122 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.0958],\n",
      "        [0.9020]], device='mps:0')\n",
      "Iteration 29460 Training loss 0.07163026183843613 Validation loss 0.06105131283402443 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.1656],\n",
      "        [0.0048]], device='mps:0')\n",
      "Iteration 29470 Training loss 0.05830290541052818 Validation loss 0.0610109344124794 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.2277],\n",
      "        [0.9549]], device='mps:0')\n",
      "Iteration 29480 Training loss 0.06256664544343948 Validation loss 0.06114839017391205 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.7665],\n",
      "        [0.0499]], device='mps:0')\n",
      "Iteration 29490 Training loss 0.06655871123075485 Validation loss 0.06100056692957878 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.9661],\n",
      "        [0.8928]], device='mps:0')\n",
      "Iteration 29500 Training loss 0.05473877862095833 Validation loss 0.061018314212560654 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.6303],\n",
      "        [0.1588]], device='mps:0')\n",
      "Iteration 29510 Training loss 0.06105904281139374 Validation loss 0.06120502948760986 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.1684],\n",
      "        [0.0827]], device='mps:0')\n",
      "Iteration 29520 Training loss 0.05904999002814293 Validation loss 0.06122255697846413 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.8797],\n",
      "        [0.9853]], device='mps:0')\n",
      "Iteration 29530 Training loss 0.05454840511083603 Validation loss 0.061122629791498184 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.2544],\n",
      "        [0.8142]], device='mps:0')\n",
      "Iteration 29540 Training loss 0.06547252833843231 Validation loss 0.06104302033782005 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.6093],\n",
      "        [0.7780]], device='mps:0')\n",
      "Iteration 29550 Training loss 0.060414694249629974 Validation loss 0.061084408313035965 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.8137],\n",
      "        [0.9827]], device='mps:0')\n",
      "Iteration 29560 Training loss 0.060778673738241196 Validation loss 0.06122145056724548 Accuracy 0.8302500247955322\n",
      "Output tensor([[0.5441],\n",
      "        [0.7093]], device='mps:0')\n",
      "Iteration 29570 Training loss 0.05991454795002937 Validation loss 0.06099976599216461 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.8920],\n",
      "        [0.5581]], device='mps:0')\n",
      "Iteration 29580 Training loss 0.058606233447790146 Validation loss 0.061072543263435364 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.0446],\n",
      "        [0.7330]], device='mps:0')\n",
      "Iteration 29590 Training loss 0.06096186488866806 Validation loss 0.061013899743556976 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.8341],\n",
      "        [0.2758]], device='mps:0')\n",
      "Iteration 29600 Training loss 0.05648547038435936 Validation loss 0.06102688983082771 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.6425],\n",
      "        [0.0680]], device='mps:0')\n",
      "Iteration 29610 Training loss 0.06146026775240898 Validation loss 0.061109479516744614 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.0227],\n",
      "        [0.1188]], device='mps:0')\n",
      "Iteration 29620 Training loss 0.06329147517681122 Validation loss 0.061024896800518036 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.0702],\n",
      "        [0.3046]], device='mps:0')\n",
      "Iteration 29630 Training loss 0.05785715579986572 Validation loss 0.06102461367845535 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.1566],\n",
      "        [0.9487]], device='mps:0')\n",
      "Iteration 29640 Training loss 0.06435281783342361 Validation loss 0.060996510088443756 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1370],\n",
      "        [0.2955]], device='mps:0')\n",
      "Iteration 29650 Training loss 0.05895761400461197 Validation loss 0.061012547463178635 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.0199],\n",
      "        [0.1371]], device='mps:0')\n",
      "Iteration 29660 Training loss 0.0635882169008255 Validation loss 0.0611146055161953 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.0178],\n",
      "        [0.8714]], device='mps:0')\n",
      "Iteration 29670 Training loss 0.05410090461373329 Validation loss 0.06097684055566788 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.1218],\n",
      "        [0.1375]], device='mps:0')\n",
      "Iteration 29680 Training loss 0.05852383375167847 Validation loss 0.06097877398133278 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.9743],\n",
      "        [0.7598]], device='mps:0')\n",
      "Iteration 29690 Training loss 0.05733581259846687 Validation loss 0.060980651527643204 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.8503],\n",
      "        [0.0779]], device='mps:0')\n",
      "Iteration 29700 Training loss 0.06446171551942825 Validation loss 0.06112699583172798 Accuracy 0.8315000534057617\n",
      "Output tensor([[0.9321],\n",
      "        [0.4938]], device='mps:0')\n",
      "Iteration 29710 Training loss 0.05839236453175545 Validation loss 0.06114106997847557 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.6740],\n",
      "        [0.7327]], device='mps:0')\n",
      "Iteration 29720 Training loss 0.06315725296735764 Validation loss 0.06099574640393257 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.5215],\n",
      "        [0.8775]], device='mps:0')\n",
      "Iteration 29730 Training loss 0.054853085428476334 Validation loss 0.06096900254487991 Accuracy 0.8313750624656677\n",
      "Output tensor([[0.3243],\n",
      "        [0.2246]], device='mps:0')\n",
      "Iteration 29740 Training loss 0.054785341024398804 Validation loss 0.06097879633307457 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.9247],\n",
      "        [0.0139]], device='mps:0')\n",
      "Iteration 29750 Training loss 0.047602590173482895 Validation loss 0.06108304485678673 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.8636],\n",
      "        [0.1729]], device='mps:0')\n",
      "Iteration 29760 Training loss 0.05844921991229057 Validation loss 0.060958895832300186 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.3243],\n",
      "        [0.0769]], device='mps:0')\n",
      "Iteration 29770 Training loss 0.05711210519075394 Validation loss 0.06097526103258133 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.9328],\n",
      "        [0.8538]], device='mps:0')\n",
      "Iteration 29780 Training loss 0.061753399670124054 Validation loss 0.0609898716211319 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.1443],\n",
      "        [0.2100]], device='mps:0')\n",
      "Iteration 29790 Training loss 0.05731571093201637 Validation loss 0.06096116825938225 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.5488],\n",
      "        [0.5392]], device='mps:0')\n",
      "Iteration 29800 Training loss 0.05648628994822502 Validation loss 0.0611337386071682 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.3132],\n",
      "        [0.0864]], device='mps:0')\n",
      "Iteration 29810 Training loss 0.06403331458568573 Validation loss 0.061122458428144455 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.1894],\n",
      "        [0.8953]], device='mps:0')\n",
      "Iteration 29820 Training loss 0.06156930699944496 Validation loss 0.06104332208633423 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.9740],\n",
      "        [0.2746]], device='mps:0')\n",
      "Iteration 29830 Training loss 0.05292924866080284 Validation loss 0.06097911298274994 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.5391],\n",
      "        [0.0743]], device='mps:0')\n",
      "Iteration 29840 Training loss 0.0576728917658329 Validation loss 0.06096064671874046 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.5753],\n",
      "        [0.9040]], device='mps:0')\n",
      "Iteration 29850 Training loss 0.06016401946544647 Validation loss 0.06095875799655914 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.7386],\n",
      "        [0.2403]], device='mps:0')\n",
      "Iteration 29860 Training loss 0.053286947309970856 Validation loss 0.060978565365076065 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.7020],\n",
      "        [0.6912]], device='mps:0')\n",
      "Iteration 29870 Training loss 0.05757879465818405 Validation loss 0.06105006858706474 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.8733],\n",
      "        [0.5684]], device='mps:0')\n",
      "Iteration 29880 Training loss 0.06426651030778885 Validation loss 0.06102926284074783 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.0677],\n",
      "        [0.9131]], device='mps:0')\n",
      "Iteration 29890 Training loss 0.06090811640024185 Validation loss 0.06102044880390167 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.8732],\n",
      "        [0.8823]], device='mps:0')\n",
      "Iteration 29900 Training loss 0.06036466360092163 Validation loss 0.060972798615694046 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.5499],\n",
      "        [0.1657]], device='mps:0')\n",
      "Iteration 29910 Training loss 0.06555461883544922 Validation loss 0.06097970902919769 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.7147],\n",
      "        [0.0525]], device='mps:0')\n",
      "Iteration 29920 Training loss 0.06330742686986923 Validation loss 0.0610019750893116 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.9953],\n",
      "        [0.6735]], device='mps:0')\n",
      "Iteration 29930 Training loss 0.04830772429704666 Validation loss 0.061115141957998276 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.0178],\n",
      "        [0.9703]], device='mps:0')\n",
      "Iteration 29940 Training loss 0.054594337940216064 Validation loss 0.06096075847744942 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.0670],\n",
      "        [0.2185]], device='mps:0')\n",
      "Iteration 29950 Training loss 0.06502228230237961 Validation loss 0.06113164871931076 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.7851],\n",
      "        [0.2491]], device='mps:0')\n",
      "Iteration 29960 Training loss 0.05796172842383385 Validation loss 0.06094266474246979 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.5230],\n",
      "        [0.9255]], device='mps:0')\n",
      "Iteration 29970 Training loss 0.05943741649389267 Validation loss 0.06093847006559372 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8424],\n",
      "        [0.6698]], device='mps:0')\n",
      "Iteration 29980 Training loss 0.07098749279975891 Validation loss 0.06093239411711693 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.1367],\n",
      "        [0.9249]], device='mps:0')\n",
      "Iteration 29990 Training loss 0.06688375025987625 Validation loss 0.061002686619758606 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.2263],\n",
      "        [0.2925]], device='mps:0')\n",
      "Iteration 30000 Training loss 0.0533958338201046 Validation loss 0.060957830399274826 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.5983],\n",
      "        [0.9967]], device='mps:0')\n",
      "Iteration 30010 Training loss 0.055247534066438675 Validation loss 0.06093575060367584 Accuracy 0.8316250443458557\n",
      "Output tensor([[0.0092],\n",
      "        [0.8235]], device='mps:0')\n",
      "Iteration 30020 Training loss 0.06235220283269882 Validation loss 0.06094464659690857 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.5580],\n",
      "        [0.7381]], device='mps:0')\n",
      "Iteration 30030 Training loss 0.059803713113069534 Validation loss 0.06094330549240112 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.2574],\n",
      "        [0.8607]], device='mps:0')\n",
      "Iteration 30040 Training loss 0.06310579925775528 Validation loss 0.061002250760793686 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.0257],\n",
      "        [0.0254]], device='mps:0')\n",
      "Iteration 30050 Training loss 0.0621749572455883 Validation loss 0.060993634164333344 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.9271],\n",
      "        [0.4924]], device='mps:0')\n",
      "Iteration 30060 Training loss 0.06065445765852928 Validation loss 0.06093207001686096 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.9896],\n",
      "        [0.0770]], device='mps:0')\n",
      "Iteration 30070 Training loss 0.06188178062438965 Validation loss 0.06104493513703346 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.8952],\n",
      "        [0.1732]], device='mps:0')\n",
      "Iteration 30080 Training loss 0.05499058961868286 Validation loss 0.06094995513558388 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.7456],\n",
      "        [0.4101]], device='mps:0')\n",
      "Iteration 30090 Training loss 0.051555465906858444 Validation loss 0.06091179698705673 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.9482],\n",
      "        [0.9023]], device='mps:0')\n",
      "Iteration 30100 Training loss 0.06147915869951248 Validation loss 0.06094205379486084 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.1849],\n",
      "        [0.2036]], device='mps:0')\n",
      "Iteration 30110 Training loss 0.0567457340657711 Validation loss 0.06093044579029083 Accuracy 0.8317500352859497\n",
      "Output tensor([[0.1010],\n",
      "        [0.9196]], device='mps:0')\n",
      "Iteration 30120 Training loss 0.06623589992523193 Validation loss 0.060978326946496964 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.1830],\n",
      "        [0.0351]], device='mps:0')\n",
      "Iteration 30130 Training loss 0.06164726987481117 Validation loss 0.060991305857896805 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.3347],\n",
      "        [0.0017]], device='mps:0')\n",
      "Iteration 30140 Training loss 0.062232647091150284 Validation loss 0.060922153294086456 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.0787],\n",
      "        [0.0158]], device='mps:0')\n",
      "Iteration 30150 Training loss 0.05493597686290741 Validation loss 0.060910679399967194 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.3982],\n",
      "        [0.1387]], device='mps:0')\n",
      "Iteration 30160 Training loss 0.061684440821409225 Validation loss 0.060911741107702255 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.7829],\n",
      "        [0.1055]], device='mps:0')\n",
      "Iteration 30170 Training loss 0.04876042902469635 Validation loss 0.06100406125187874 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.7818],\n",
      "        [0.0385]], device='mps:0')\n",
      "Iteration 30180 Training loss 0.063836008310318 Validation loss 0.06095581874251366 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.6244],\n",
      "        [0.5993]], device='mps:0')\n",
      "Iteration 30190 Training loss 0.06565819680690765 Validation loss 0.06096215173602104 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.9837],\n",
      "        [0.0953]], device='mps:0')\n",
      "Iteration 30200 Training loss 0.06420629471540451 Validation loss 0.06089997664093971 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.1717],\n",
      "        [0.8723]], device='mps:0')\n",
      "Iteration 30210 Training loss 0.06171863526105881 Validation loss 0.06094551458954811 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.2579],\n",
      "        [0.6638]], device='mps:0')\n",
      "Iteration 30220 Training loss 0.05575467646121979 Validation loss 0.0608922578394413 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.7178],\n",
      "        [0.0228]], device='mps:0')\n",
      "Iteration 30230 Training loss 0.06639620661735535 Validation loss 0.06099354103207588 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.1749],\n",
      "        [0.4757]], device='mps:0')\n",
      "Iteration 30240 Training loss 0.06472500413656235 Validation loss 0.06092265993356705 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.7483],\n",
      "        [0.3593]], device='mps:0')\n",
      "Iteration 30250 Training loss 0.058962080627679825 Validation loss 0.06096450239419937 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9595],\n",
      "        [0.1868]], device='mps:0')\n",
      "Iteration 30260 Training loss 0.06077501177787781 Validation loss 0.06088951230049133 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.0916],\n",
      "        [0.4666]], device='mps:0')\n",
      "Iteration 30270 Training loss 0.0728428065776825 Validation loss 0.06105119362473488 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.8560],\n",
      "        [0.3611]], device='mps:0')\n",
      "Iteration 30280 Training loss 0.060071077197790146 Validation loss 0.060896068811416626 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.8380],\n",
      "        [0.2376]], device='mps:0')\n",
      "Iteration 30290 Training loss 0.0633741095662117 Validation loss 0.06089302524924278 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.9582],\n",
      "        [0.9340]], device='mps:0')\n",
      "Iteration 30300 Training loss 0.05868537724018097 Validation loss 0.060899049043655396 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.9118],\n",
      "        [0.9871]], device='mps:0')\n",
      "Iteration 30310 Training loss 0.057269759476184845 Validation loss 0.06102307513356209 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.1552],\n",
      "        [0.5125]], device='mps:0')\n",
      "Iteration 30320 Training loss 0.05133235827088356 Validation loss 0.060880038887262344 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.9467],\n",
      "        [0.6180]], device='mps:0')\n",
      "Iteration 30330 Training loss 0.05519168823957443 Validation loss 0.060874082148075104 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.2898],\n",
      "        [0.3665]], device='mps:0')\n",
      "Iteration 30340 Training loss 0.0654836893081665 Validation loss 0.06092635542154312 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.0739],\n",
      "        [0.0395]], device='mps:0')\n",
      "Iteration 30350 Training loss 0.05863437429070473 Validation loss 0.060952845960855484 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.0727],\n",
      "        [0.6725]], device='mps:0')\n",
      "Iteration 30360 Training loss 0.060352873057127 Validation loss 0.061099324375391006 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.5526],\n",
      "        [0.2856]], device='mps:0')\n",
      "Iteration 30370 Training loss 0.06132105737924576 Validation loss 0.06113690510392189 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.0461],\n",
      "        [0.9351]], device='mps:0')\n",
      "Iteration 30380 Training loss 0.0521906279027462 Validation loss 0.06088337302207947 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.9256],\n",
      "        [0.8718]], device='mps:0')\n",
      "Iteration 30390 Training loss 0.07185834646224976 Validation loss 0.060987845063209534 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.0221],\n",
      "        [0.8732]], device='mps:0')\n",
      "Iteration 30400 Training loss 0.05086733400821686 Validation loss 0.06089065223932266 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.4441],\n",
      "        [0.6635]], device='mps:0')\n",
      "Iteration 30410 Training loss 0.05139009281992912 Validation loss 0.060914840549230576 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.2227],\n",
      "        [0.1694]], device='mps:0')\n",
      "Iteration 30420 Training loss 0.05935028940439224 Validation loss 0.06091669574379921 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.8338],\n",
      "        [0.8604]], device='mps:0')\n",
      "Iteration 30430 Training loss 0.06374341994524002 Validation loss 0.06095770001411438 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.6818],\n",
      "        [0.9945]], device='mps:0')\n",
      "Iteration 30440 Training loss 0.051108360290527344 Validation loss 0.06086227297782898 Accuracy 0.8320000171661377\n",
      "Output tensor([[0.0387],\n",
      "        [0.0911]], device='mps:0')\n",
      "Iteration 30450 Training loss 0.05181083455681801 Validation loss 0.06086993217468262 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.0695],\n",
      "        [0.2768]], device='mps:0')\n",
      "Iteration 30460 Training loss 0.06641463935375214 Validation loss 0.06101040169596672 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.3443],\n",
      "        [0.2788]], device='mps:0')\n",
      "Iteration 30470 Training loss 0.06553419679403305 Validation loss 0.06087736040353775 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.1876],\n",
      "        [0.6963]], device='mps:0')\n",
      "Iteration 30480 Training loss 0.05734885111451149 Validation loss 0.06136596202850342 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.9063],\n",
      "        [0.1373]], device='mps:0')\n",
      "Iteration 30490 Training loss 0.056937411427497864 Validation loss 0.060860298573970795 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.6921],\n",
      "        [0.5532]], device='mps:0')\n",
      "Iteration 30500 Training loss 0.05539705976843834 Validation loss 0.06086976081132889 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.2758],\n",
      "        [0.3169]], device='mps:0')\n",
      "Iteration 30510 Training loss 0.05707552656531334 Validation loss 0.060870155692100525 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.0970],\n",
      "        [0.1544]], device='mps:0')\n",
      "Iteration 30520 Training loss 0.06608027964830399 Validation loss 0.061190102249383926 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.2606],\n",
      "        [0.4512]], device='mps:0')\n",
      "Iteration 30530 Training loss 0.06435813009738922 Validation loss 0.06090497225522995 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.2879],\n",
      "        [0.1588]], device='mps:0')\n",
      "Iteration 30540 Training loss 0.05717070400714874 Validation loss 0.06088970974087715 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.0512],\n",
      "        [0.9283]], device='mps:0')\n",
      "Iteration 30550 Training loss 0.052476949989795685 Validation loss 0.06091131642460823 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.0372],\n",
      "        [0.0183]], device='mps:0')\n",
      "Iteration 30560 Training loss 0.05918782204389572 Validation loss 0.0608733668923378 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.3525],\n",
      "        [0.0294]], device='mps:0')\n",
      "Iteration 30570 Training loss 0.05450127646327019 Validation loss 0.060984451323747635 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.2007],\n",
      "        [0.9702]], device='mps:0')\n",
      "Iteration 30580 Training loss 0.06588220596313477 Validation loss 0.060896631330251694 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.1077],\n",
      "        [0.1264]], device='mps:0')\n",
      "Iteration 30590 Training loss 0.05454825237393379 Validation loss 0.06088333949446678 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.1118],\n",
      "        [0.1512]], device='mps:0')\n",
      "Iteration 30600 Training loss 0.06343991309404373 Validation loss 0.06087161600589752 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.0164],\n",
      "        [0.2206]], device='mps:0')\n",
      "Iteration 30610 Training loss 0.06338439881801605 Validation loss 0.06087679788470268 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.7032],\n",
      "        [0.1033]], device='mps:0')\n",
      "Iteration 30620 Training loss 0.06956720352172852 Validation loss 0.060941532254219055 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.9788],\n",
      "        [0.9848]], device='mps:0')\n",
      "Iteration 30630 Training loss 0.06655088067054749 Validation loss 0.06086580082774162 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.3572],\n",
      "        [0.1482]], device='mps:0')\n",
      "Iteration 30640 Training loss 0.0547945573925972 Validation loss 0.06090481951832771 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.9318],\n",
      "        [0.1935]], device='mps:0')\n",
      "Iteration 30650 Training loss 0.06042783707380295 Validation loss 0.060866646468639374 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.7435],\n",
      "        [0.2928]], device='mps:0')\n",
      "Iteration 30660 Training loss 0.06242477148771286 Validation loss 0.060877107083797455 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.1258],\n",
      "        [0.5402]], device='mps:0')\n",
      "Iteration 30670 Training loss 0.05746639147400856 Validation loss 0.06093038246035576 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.4716],\n",
      "        [0.1187]], device='mps:0')\n",
      "Iteration 30680 Training loss 0.05813107267022133 Validation loss 0.0609748549759388 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.4557],\n",
      "        [0.6513]], device='mps:0')\n",
      "Iteration 30690 Training loss 0.05959555506706238 Validation loss 0.06090710684657097 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.1863],\n",
      "        [0.9014]], device='mps:0')\n",
      "Iteration 30700 Training loss 0.061296723783016205 Validation loss 0.06088769808411598 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.9666],\n",
      "        [0.4002]], device='mps:0')\n",
      "Iteration 30710 Training loss 0.06015356257557869 Validation loss 0.06098751351237297 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.1013],\n",
      "        [0.9452]], device='mps:0')\n",
      "Iteration 30720 Training loss 0.05829484760761261 Validation loss 0.060852378606796265 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.0289],\n",
      "        [0.7574]], device='mps:0')\n",
      "Iteration 30730 Training loss 0.06310154497623444 Validation loss 0.060855936259031296 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.3369],\n",
      "        [0.9896]], device='mps:0')\n",
      "Iteration 30740 Training loss 0.06077791005373001 Validation loss 0.060859259217977524 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.9790],\n",
      "        [0.8489]], device='mps:0')\n",
      "Iteration 30750 Training loss 0.062180086970329285 Validation loss 0.061013113707304 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.3976],\n",
      "        [0.0978]], device='mps:0')\n",
      "Iteration 30760 Training loss 0.06096194311976433 Validation loss 0.0609126016497612 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.9512],\n",
      "        [0.5238]], device='mps:0')\n",
      "Iteration 30770 Training loss 0.06125567480921745 Validation loss 0.060847535729408264 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.8618],\n",
      "        [0.7480]], device='mps:0')\n",
      "Iteration 30780 Training loss 0.061192724853754044 Validation loss 0.06103359907865524 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.6994],\n",
      "        [0.3384]], device='mps:0')\n",
      "Iteration 30790 Training loss 0.05814290791749954 Validation loss 0.06083763390779495 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.7933],\n",
      "        [0.2322]], device='mps:0')\n",
      "Iteration 30800 Training loss 0.06124000996351242 Validation loss 0.060840267688035965 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.4734],\n",
      "        [0.9565]], device='mps:0')\n",
      "Iteration 30810 Training loss 0.061605166643857956 Validation loss 0.06101880967617035 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.2310],\n",
      "        [0.1824]], device='mps:0')\n",
      "Iteration 30820 Training loss 0.07055176794528961 Validation loss 0.061044104397296906 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.2990],\n",
      "        [0.5782]], device='mps:0')\n",
      "Iteration 30830 Training loss 0.06282301992177963 Validation loss 0.060832440853118896 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.9653],\n",
      "        [0.9944]], device='mps:0')\n",
      "Iteration 30840 Training loss 0.061287954449653625 Validation loss 0.06083771213889122 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.9904],\n",
      "        [0.1210]], device='mps:0')\n",
      "Iteration 30850 Training loss 0.05497293174266815 Validation loss 0.06082247570157051 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.3704],\n",
      "        [0.0837]], device='mps:0')\n",
      "Iteration 30860 Training loss 0.05424167588353157 Validation loss 0.060812633484601974 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.7004],\n",
      "        [0.5939]], device='mps:0')\n",
      "Iteration 30870 Training loss 0.06300213187932968 Validation loss 0.060927778482437134 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.9190],\n",
      "        [0.2236]], device='mps:0')\n",
      "Iteration 30880 Training loss 0.06351382285356522 Validation loss 0.06081254780292511 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.5203],\n",
      "        [0.5366]], device='mps:0')\n",
      "Iteration 30890 Training loss 0.07414688169956207 Validation loss 0.06092764437198639 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.9733],\n",
      "        [0.2527]], device='mps:0')\n",
      "Iteration 30900 Training loss 0.053649842739105225 Validation loss 0.06083955243229866 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.2608],\n",
      "        [0.8844]], device='mps:0')\n",
      "Iteration 30910 Training loss 0.06099437549710274 Validation loss 0.06096891686320305 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.0387],\n",
      "        [0.8504]], device='mps:0')\n",
      "Iteration 30920 Training loss 0.0576697513461113 Validation loss 0.06080709397792816 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.3594],\n",
      "        [0.6892]], device='mps:0')\n",
      "Iteration 30930 Training loss 0.06800508499145508 Validation loss 0.06084807589650154 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.0496],\n",
      "        [0.6175]], device='mps:0')\n",
      "Iteration 30940 Training loss 0.05603580176830292 Validation loss 0.06088217720389366 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.2734],\n",
      "        [0.6720]], device='mps:0')\n",
      "Iteration 30950 Training loss 0.05662847310304642 Validation loss 0.060844022780656815 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.7667],\n",
      "        [0.1161]], device='mps:0')\n",
      "Iteration 30960 Training loss 0.055430199950933456 Validation loss 0.06080375239253044 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.6824],\n",
      "        [0.9452]], device='mps:0')\n",
      "Iteration 30970 Training loss 0.06125696003437042 Validation loss 0.060809243470430374 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.8025],\n",
      "        [0.5648]], device='mps:0')\n",
      "Iteration 30980 Training loss 0.0602056123316288 Validation loss 0.06080981716513634 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.1245],\n",
      "        [0.3190]], device='mps:0')\n",
      "Iteration 30990 Training loss 0.06099013239145279 Validation loss 0.061231259256601334 Accuracy 0.8318750262260437\n",
      "Output tensor([[0.8880],\n",
      "        [0.9233]], device='mps:0')\n",
      "Iteration 31000 Training loss 0.06672608107328415 Validation loss 0.060820866376161575 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.0962],\n",
      "        [0.2373]], device='mps:0')\n",
      "Iteration 31010 Training loss 0.06765937805175781 Validation loss 0.06081066280603409 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.0572],\n",
      "        [0.0299]], device='mps:0')\n",
      "Iteration 31020 Training loss 0.05170854926109314 Validation loss 0.06079067289829254 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.3323],\n",
      "        [0.6430]], device='mps:0')\n",
      "Iteration 31030 Training loss 0.05172758921980858 Validation loss 0.060872770845890045 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.6356],\n",
      "        [0.3519]], device='mps:0')\n",
      "Iteration 31040 Training loss 0.053425271064043045 Validation loss 0.060798436403274536 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.8792],\n",
      "        [0.4677]], device='mps:0')\n",
      "Iteration 31050 Training loss 0.050291359424591064 Validation loss 0.06082973629236221 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.3402],\n",
      "        [0.9365]], device='mps:0')\n",
      "Iteration 31060 Training loss 0.059994570910930634 Validation loss 0.06078246608376503 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.7989],\n",
      "        [0.0172]], device='mps:0')\n",
      "Iteration 31070 Training loss 0.06057368218898773 Validation loss 0.06084712594747543 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.0857],\n",
      "        [0.0447]], device='mps:0')\n",
      "Iteration 31080 Training loss 0.054074976593256 Validation loss 0.060901790857315063 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.4243],\n",
      "        [0.9464]], device='mps:0')\n",
      "Iteration 31090 Training loss 0.07099244743585587 Validation loss 0.06077663227915764 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.4998],\n",
      "        [0.5385]], device='mps:0')\n",
      "Iteration 31100 Training loss 0.062228020280599594 Validation loss 0.060855280607938766 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.0657],\n",
      "        [0.9991]], device='mps:0')\n",
      "Iteration 31110 Training loss 0.061799030750989914 Validation loss 0.06085016578435898 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.9830],\n",
      "        [0.0314]], device='mps:0')\n",
      "Iteration 31120 Training loss 0.06939708441495895 Validation loss 0.060775067657232285 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.0698],\n",
      "        [0.9456]], device='mps:0')\n",
      "Iteration 31130 Training loss 0.06374431401491165 Validation loss 0.06091848388314247 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.4323],\n",
      "        [0.3749]], device='mps:0')\n",
      "Iteration 31140 Training loss 0.058285731822252274 Validation loss 0.06077147647738457 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.0983],\n",
      "        [0.9136]], device='mps:0')\n",
      "Iteration 31150 Training loss 0.05925918370485306 Validation loss 0.06078818067908287 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.2855],\n",
      "        [0.0049]], device='mps:0')\n",
      "Iteration 31160 Training loss 0.06080026552081108 Validation loss 0.06078295037150383 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.9912],\n",
      "        [0.7843]], device='mps:0')\n",
      "Iteration 31170 Training loss 0.059970416128635406 Validation loss 0.060909517109394073 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.8929],\n",
      "        [0.0807]], device='mps:0')\n",
      "Iteration 31180 Training loss 0.06948281824588776 Validation loss 0.060827791690826416 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.9081],\n",
      "        [0.7926]], device='mps:0')\n",
      "Iteration 31190 Training loss 0.06699204444885254 Validation loss 0.06077307090163231 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.8349],\n",
      "        [0.7415]], device='mps:0')\n",
      "Iteration 31200 Training loss 0.06332138925790787 Validation loss 0.06078523397445679 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.9562],\n",
      "        [0.5956]], device='mps:0')\n",
      "Iteration 31210 Training loss 0.0580948069691658 Validation loss 0.060766588896512985 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.2376],\n",
      "        [0.5800]], device='mps:0')\n",
      "Iteration 31220 Training loss 0.057595089077949524 Validation loss 0.06077370420098305 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.3828],\n",
      "        [0.3907]], device='mps:0')\n",
      "Iteration 31230 Training loss 0.05886296182870865 Validation loss 0.06093107536435127 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.9326],\n",
      "        [0.9468]], device='mps:0')\n",
      "Iteration 31240 Training loss 0.06407071650028229 Validation loss 0.06080986186861992 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.6519],\n",
      "        [0.5079]], device='mps:0')\n",
      "Iteration 31250 Training loss 0.05473821982741356 Validation loss 0.0607631616294384 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.7040],\n",
      "        [0.3004]], device='mps:0')\n",
      "Iteration 31260 Training loss 0.06201735511422157 Validation loss 0.06078784912824631 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.4787],\n",
      "        [0.5806]], device='mps:0')\n",
      "Iteration 31270 Training loss 0.06576342135667801 Validation loss 0.06133301928639412 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.2187],\n",
      "        [0.9603]], device='mps:0')\n",
      "Iteration 31280 Training loss 0.06366104632616043 Validation loss 0.06099593639373779 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.0221],\n",
      "        [0.9497]], device='mps:0')\n",
      "Iteration 31290 Training loss 0.05620800703763962 Validation loss 0.06096867844462395 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.9399],\n",
      "        [0.1418]], device='mps:0')\n",
      "Iteration 31300 Training loss 0.06014752388000488 Validation loss 0.0607602521777153 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.0176],\n",
      "        [0.9322]], device='mps:0')\n",
      "Iteration 31310 Training loss 0.05610468611121178 Validation loss 0.06075624004006386 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.8129],\n",
      "        [0.3150]], device='mps:0')\n",
      "Iteration 31320 Training loss 0.057187121361494064 Validation loss 0.06091427430510521 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.2729],\n",
      "        [0.9963]], device='mps:0')\n",
      "Iteration 31330 Training loss 0.05682400241494179 Validation loss 0.06083060801029205 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.8863],\n",
      "        [0.8097]], device='mps:0')\n",
      "Iteration 31340 Training loss 0.05787580832839012 Validation loss 0.06077788025140762 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.4192],\n",
      "        [0.2643]], device='mps:0')\n",
      "Iteration 31350 Training loss 0.06407169252634048 Validation loss 0.06121034547686577 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.1395],\n",
      "        [0.9243]], device='mps:0')\n",
      "Iteration 31360 Training loss 0.05666356906294823 Validation loss 0.06075003370642662 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.0346],\n",
      "        [0.8290]], device='mps:0')\n",
      "Iteration 31370 Training loss 0.061292361468076706 Validation loss 0.06079639866948128 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.0265],\n",
      "        [0.4979]], device='mps:0')\n",
      "Iteration 31380 Training loss 0.052456144243478775 Validation loss 0.06078851595520973 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.0135],\n",
      "        [0.7393]], device='mps:0')\n",
      "Iteration 31390 Training loss 0.05646669119596481 Validation loss 0.060776904225349426 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.9455],\n",
      "        [0.8857]], device='mps:0')\n",
      "Iteration 31400 Training loss 0.059151217341423035 Validation loss 0.060784369707107544 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.4568],\n",
      "        [0.7849]], device='mps:0')\n",
      "Iteration 31410 Training loss 0.0581355094909668 Validation loss 0.06073414534330368 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.3572],\n",
      "        [0.2184]], device='mps:0')\n",
      "Iteration 31420 Training loss 0.06415551155805588 Validation loss 0.06074010953307152 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.7612],\n",
      "        [0.4086]], device='mps:0')\n",
      "Iteration 31430 Training loss 0.061349913477897644 Validation loss 0.06073182076215744 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.0818],\n",
      "        [0.1131]], device='mps:0')\n",
      "Iteration 31440 Training loss 0.05575699359178543 Validation loss 0.06079496443271637 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.3002],\n",
      "        [0.0517]], device='mps:0')\n",
      "Iteration 31450 Training loss 0.059113673865795135 Validation loss 0.06076304242014885 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.4129],\n",
      "        [0.2617]], device='mps:0')\n",
      "Iteration 31460 Training loss 0.05473580211400986 Validation loss 0.06098053976893425 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.0680],\n",
      "        [0.6704]], device='mps:0')\n",
      "Iteration 31470 Training loss 0.05958997458219528 Validation loss 0.06077001616358757 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.1043],\n",
      "        [0.1318]], device='mps:0')\n",
      "Iteration 31480 Training loss 0.05543287470936775 Validation loss 0.06072836369276047 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.9345],\n",
      "        [0.9931]], device='mps:0')\n",
      "Iteration 31490 Training loss 0.060363318771123886 Validation loss 0.06079079210758209 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.9436],\n",
      "        [0.0786]], device='mps:0')\n",
      "Iteration 31500 Training loss 0.06438710540533066 Validation loss 0.060722216963768005 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.7778],\n",
      "        [0.9447]], device='mps:0')\n",
      "Iteration 31510 Training loss 0.0547059141099453 Validation loss 0.06077965721487999 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.8666],\n",
      "        [0.1322]], device='mps:0')\n",
      "Iteration 31520 Training loss 0.056130681186914444 Validation loss 0.06101958826184273 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.0359],\n",
      "        [0.0299]], device='mps:0')\n",
      "Iteration 31530 Training loss 0.0618399940431118 Validation loss 0.06072482466697693 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.2690],\n",
      "        [0.3775]], device='mps:0')\n",
      "Iteration 31540 Training loss 0.05572708323597908 Validation loss 0.06085755676031113 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.1035],\n",
      "        [0.8621]], device='mps:0')\n",
      "Iteration 31550 Training loss 0.05203644558787346 Validation loss 0.060757581144571304 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.2262],\n",
      "        [0.8373]], device='mps:0')\n",
      "Iteration 31560 Training loss 0.05012315511703491 Validation loss 0.06081121787428856 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.6538],\n",
      "        [0.7805]], device='mps:0')\n",
      "Iteration 31570 Training loss 0.04690699651837349 Validation loss 0.060733478516340256 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.5474],\n",
      "        [0.2823]], device='mps:0')\n",
      "Iteration 31580 Training loss 0.06661484390497208 Validation loss 0.06071785092353821 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.8952],\n",
      "        [0.0369]], device='mps:0')\n",
      "Iteration 31590 Training loss 0.05722404271364212 Validation loss 0.06082911416888237 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.9902],\n",
      "        [0.0705]], device='mps:0')\n",
      "Iteration 31600 Training loss 0.06381037831306458 Validation loss 0.060713157057762146 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.1729],\n",
      "        [0.0891]], device='mps:0')\n",
      "Iteration 31610 Training loss 0.059145487844944 Validation loss 0.060809649527072906 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.0331],\n",
      "        [0.1509]], device='mps:0')\n",
      "Iteration 31620 Training loss 0.05678356811404228 Validation loss 0.06079797446727753 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.8178],\n",
      "        [0.0074]], device='mps:0')\n",
      "Iteration 31630 Training loss 0.0573732927441597 Validation loss 0.06071935594081879 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.9250],\n",
      "        [0.8307]], device='mps:0')\n",
      "Iteration 31640 Training loss 0.05548180639743805 Validation loss 0.06078006327152252 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.2221],\n",
      "        [0.9778]], device='mps:0')\n",
      "Iteration 31650 Training loss 0.06078506261110306 Validation loss 0.060791015625 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.0206],\n",
      "        [0.6695]], device='mps:0')\n",
      "Iteration 31660 Training loss 0.06374146044254303 Validation loss 0.06071432679891586 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.5253],\n",
      "        [0.1498]], device='mps:0')\n",
      "Iteration 31670 Training loss 0.04956238716840744 Validation loss 0.060776274651288986 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.7142],\n",
      "        [0.1591]], device='mps:0')\n",
      "Iteration 31680 Training loss 0.05394471064209938 Validation loss 0.060741230845451355 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.0592],\n",
      "        [0.1107]], device='mps:0')\n",
      "Iteration 31690 Training loss 0.06097998470067978 Validation loss 0.06073513999581337 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.1491],\n",
      "        [0.3537]], device='mps:0')\n",
      "Iteration 31700 Training loss 0.07104495912790298 Validation loss 0.0606878288090229 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.2686],\n",
      "        [0.6823]], device='mps:0')\n",
      "Iteration 31710 Training loss 0.060965508222579956 Validation loss 0.060811594128608704 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.0454],\n",
      "        [0.1050]], device='mps:0')\n",
      "Iteration 31720 Training loss 0.05484494939446449 Validation loss 0.06068788096308708 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.2268],\n",
      "        [0.9844]], device='mps:0')\n",
      "Iteration 31730 Training loss 0.05035475641489029 Validation loss 0.06070292368531227 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.7385],\n",
      "        [0.1805]], device='mps:0')\n",
      "Iteration 31740 Training loss 0.06477893888950348 Validation loss 0.06068991869688034 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9446],\n",
      "        [0.3666]], device='mps:0')\n",
      "Iteration 31750 Training loss 0.05739555135369301 Validation loss 0.06073327362537384 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.9027],\n",
      "        [0.9520]], device='mps:0')\n",
      "Iteration 31760 Training loss 0.054403021931648254 Validation loss 0.060698412358760834 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.0741],\n",
      "        [0.4203]], device='mps:0')\n",
      "Iteration 31770 Training loss 0.05802083760499954 Validation loss 0.06070740893483162 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.4992],\n",
      "        [0.9043]], device='mps:0')\n",
      "Iteration 31780 Training loss 0.05847223103046417 Validation loss 0.06067493557929993 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.6910],\n",
      "        [0.7075]], device='mps:0')\n",
      "Iteration 31790 Training loss 0.06133676692843437 Validation loss 0.060708895325660706 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.0927],\n",
      "        [0.1111]], device='mps:0')\n",
      "Iteration 31800 Training loss 0.05975775420665741 Validation loss 0.060692187398672104 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.7260],\n",
      "        [0.1880]], device='mps:0')\n",
      "Iteration 31810 Training loss 0.06488721072673798 Validation loss 0.060693155974149704 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.0715],\n",
      "        [0.3776]], device='mps:0')\n",
      "Iteration 31820 Training loss 0.05710196495056152 Validation loss 0.06069769337773323 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.3344],\n",
      "        [0.0807]], device='mps:0')\n",
      "Iteration 31830 Training loss 0.05666469410061836 Validation loss 0.06067456305027008 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.8835],\n",
      "        [0.5757]], device='mps:0')\n",
      "Iteration 31840 Training loss 0.057744402438402176 Validation loss 0.060695480555295944 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.0653],\n",
      "        [0.4060]], device='mps:0')\n",
      "Iteration 31850 Training loss 0.0560094453394413 Validation loss 0.060793809592723846 Accuracy 0.8323750495910645\n",
      "Output tensor([[0.6381],\n",
      "        [0.2899]], device='mps:0')\n",
      "Iteration 31860 Training loss 0.05515705794095993 Validation loss 0.06067410856485367 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.1614],\n",
      "        [0.3273]], device='mps:0')\n",
      "Iteration 31870 Training loss 0.06005054712295532 Validation loss 0.06067691370844841 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.3508],\n",
      "        [0.1234]], device='mps:0')\n",
      "Iteration 31880 Training loss 0.06735595315694809 Validation loss 0.06067963317036629 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.8104],\n",
      "        [0.7711]], device='mps:0')\n",
      "Iteration 31890 Training loss 0.0586225725710392 Validation loss 0.0607164241373539 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.5874],\n",
      "        [0.0035]], device='mps:0')\n",
      "Iteration 31900 Training loss 0.059812020510435104 Validation loss 0.060680605471134186 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.0374],\n",
      "        [0.1985]], device='mps:0')\n",
      "Iteration 31910 Training loss 0.05370757356286049 Validation loss 0.06068660691380501 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.3048],\n",
      "        [0.0894]], device='mps:0')\n",
      "Iteration 31920 Training loss 0.05190935358405113 Validation loss 0.06066577509045601 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.4838],\n",
      "        [0.3300]], device='mps:0')\n",
      "Iteration 31930 Training loss 0.055890195071697235 Validation loss 0.060692738741636276 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.0440],\n",
      "        [0.6621]], device='mps:0')\n",
      "Iteration 31940 Training loss 0.0560603067278862 Validation loss 0.06065196543931961 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.9587],\n",
      "        [0.9883]], device='mps:0')\n",
      "Iteration 31950 Training loss 0.06545212864875793 Validation loss 0.06065443530678749 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.2529],\n",
      "        [0.4621]], device='mps:0')\n",
      "Iteration 31960 Training loss 0.06234390288591385 Validation loss 0.06081217899918556 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.2543],\n",
      "        [0.5169]], device='mps:0')\n",
      "Iteration 31970 Training loss 0.0645241066813469 Validation loss 0.06065921485424042 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.2705],\n",
      "        [0.1110]], device='mps:0')\n",
      "Iteration 31980 Training loss 0.06042850390076637 Validation loss 0.060669656842947006 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.8621],\n",
      "        [0.9552]], device='mps:0')\n",
      "Iteration 31990 Training loss 0.051157254725694656 Validation loss 0.06070559099316597 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.0145],\n",
      "        [0.9877]], device='mps:0')\n",
      "Iteration 32000 Training loss 0.057976409792900085 Validation loss 0.06089380756020546 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.0472],\n",
      "        [0.0783]], device='mps:0')\n",
      "Iteration 32010 Training loss 0.050749700516462326 Validation loss 0.060652706772089005 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.9580],\n",
      "        [0.9149]], device='mps:0')\n",
      "Iteration 32020 Training loss 0.055347565561532974 Validation loss 0.0606570690870285 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.9918],\n",
      "        [0.5181]], device='mps:0')\n",
      "Iteration 32030 Training loss 0.059307318180799484 Validation loss 0.060668934136629105 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.1177],\n",
      "        [0.3708]], device='mps:0')\n",
      "Iteration 32040 Training loss 0.05115573853254318 Validation loss 0.060661982744932175 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.9429],\n",
      "        [0.0758]], device='mps:0')\n",
      "Iteration 32050 Training loss 0.05948825553059578 Validation loss 0.06072884798049927 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.7315],\n",
      "        [0.2852]], device='mps:0')\n",
      "Iteration 32060 Training loss 0.062356289476156235 Validation loss 0.06066516414284706 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.2059],\n",
      "        [0.9037]], device='mps:0')\n",
      "Iteration 32070 Training loss 0.06378962099552155 Validation loss 0.060662221163511276 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.3892],\n",
      "        [0.9804]], device='mps:0')\n",
      "Iteration 32080 Training loss 0.06776131689548492 Validation loss 0.06065058335661888 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.2195],\n",
      "        [0.9592]], device='mps:0')\n",
      "Iteration 32090 Training loss 0.056385334581136703 Validation loss 0.06063578277826309 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.0266],\n",
      "        [0.0594]], device='mps:0')\n",
      "Iteration 32100 Training loss 0.05617740377783775 Validation loss 0.060668669641017914 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.6642],\n",
      "        [0.5159]], device='mps:0')\n",
      "Iteration 32110 Training loss 0.06353188306093216 Validation loss 0.06067933514714241 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.4919],\n",
      "        [0.9940]], device='mps:0')\n",
      "Iteration 32120 Training loss 0.05883084982633591 Validation loss 0.06061930954456329 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.7797],\n",
      "        [0.9709]], device='mps:0')\n",
      "Iteration 32130 Training loss 0.06083701178431511 Validation loss 0.06081947684288025 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.1161],\n",
      "        [0.0810]], device='mps:0')\n",
      "Iteration 32140 Training loss 0.06224176287651062 Validation loss 0.06069115176796913 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.5226],\n",
      "        [0.8349]], device='mps:0')\n",
      "Iteration 32150 Training loss 0.06375547498464584 Validation loss 0.060697078704833984 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.1964],\n",
      "        [0.7679]], device='mps:0')\n",
      "Iteration 32160 Training loss 0.057751547545194626 Validation loss 0.06065204739570618 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.8604],\n",
      "        [0.8658]], device='mps:0')\n",
      "Iteration 32170 Training loss 0.05713731050491333 Validation loss 0.06066849082708359 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.6964],\n",
      "        [0.9443]], device='mps:0')\n",
      "Iteration 32180 Training loss 0.06117340922355652 Validation loss 0.06073599308729172 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.7850],\n",
      "        [0.1035]], device='mps:0')\n",
      "Iteration 32190 Training loss 0.057471297681331635 Validation loss 0.06063064932823181 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.1580],\n",
      "        [0.7400]], device='mps:0')\n",
      "Iteration 32200 Training loss 0.058011312037706375 Validation loss 0.06063690036535263 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.0702],\n",
      "        [0.9016]], device='mps:0')\n",
      "Iteration 32210 Training loss 0.050307925790548325 Validation loss 0.060813404619693756 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.7638],\n",
      "        [0.8443]], device='mps:0')\n",
      "Iteration 32220 Training loss 0.05727069079875946 Validation loss 0.060848843306303024 Accuracy 0.8327500224113464\n",
      "Output tensor([[0.1436],\n",
      "        [0.0135]], device='mps:0')\n",
      "Iteration 32230 Training loss 0.06204828992486 Validation loss 0.06061172857880592 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.7770],\n",
      "        [0.8792]], device='mps:0')\n",
      "Iteration 32240 Training loss 0.05848099663853645 Validation loss 0.0606071799993515 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.7111],\n",
      "        [0.0235]], device='mps:0')\n",
      "Iteration 32250 Training loss 0.0456259623169899 Validation loss 0.060654543340206146 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.3270],\n",
      "        [0.0736]], device='mps:0')\n",
      "Iteration 32260 Training loss 0.06154217571020126 Validation loss 0.060630105435848236 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.0453],\n",
      "        [0.2418]], device='mps:0')\n",
      "Iteration 32270 Training loss 0.062079742550849915 Validation loss 0.060697998851537704 Accuracy 0.8328750133514404\n",
      "Output tensor([[0.2946],\n",
      "        [0.0196]], device='mps:0')\n",
      "Iteration 32280 Training loss 0.05679131671786308 Validation loss 0.060609571635723114 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0084],\n",
      "        [0.9635]], device='mps:0')\n",
      "Iteration 32290 Training loss 0.05873516574501991 Validation loss 0.060667525976896286 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.9033],\n",
      "        [0.2047]], device='mps:0')\n",
      "Iteration 32300 Training loss 0.06279529631137848 Validation loss 0.060610510408878326 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.9294],\n",
      "        [0.3463]], device='mps:0')\n",
      "Iteration 32310 Training loss 0.05170752480626106 Validation loss 0.06061927229166031 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.7415],\n",
      "        [0.1768]], device='mps:0')\n",
      "Iteration 32320 Training loss 0.05610750615596771 Validation loss 0.06064927205443382 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.0143],\n",
      "        [0.8918]], device='mps:0')\n",
      "Iteration 32330 Training loss 0.06080976873636246 Validation loss 0.06059671565890312 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.3011],\n",
      "        [0.7241]], device='mps:0')\n",
      "Iteration 32340 Training loss 0.0678899958729744 Validation loss 0.06060872972011566 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.8127],\n",
      "        [0.0986]], device='mps:0')\n",
      "Iteration 32350 Training loss 0.06222439557313919 Validation loss 0.060710642486810684 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.7408],\n",
      "        [0.4078]], device='mps:0')\n",
      "Iteration 32360 Training loss 0.060546502470970154 Validation loss 0.060609057545661926 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.5765],\n",
      "        [0.0380]], device='mps:0')\n",
      "Iteration 32370 Training loss 0.05708840861916542 Validation loss 0.06063522771000862 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.1384],\n",
      "        [0.6990]], device='mps:0')\n",
      "Iteration 32380 Training loss 0.06022927165031433 Validation loss 0.0606195330619812 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.5682],\n",
      "        [0.8847]], device='mps:0')\n",
      "Iteration 32390 Training loss 0.06415290385484695 Validation loss 0.060603249818086624 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.0524],\n",
      "        [0.9169]], device='mps:0')\n",
      "Iteration 32400 Training loss 0.058616988360881805 Validation loss 0.06060205027461052 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.6140],\n",
      "        [0.8193]], device='mps:0')\n",
      "Iteration 32410 Training loss 0.051622696220874786 Validation loss 0.06080593541264534 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.2554],\n",
      "        [0.3231]], device='mps:0')\n",
      "Iteration 32420 Training loss 0.062489014118909836 Validation loss 0.060593243688344955 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.3463],\n",
      "        [0.9380]], device='mps:0')\n",
      "Iteration 32430 Training loss 0.06496784090995789 Validation loss 0.06058545038104057 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.2403],\n",
      "        [0.8609]], device='mps:0')\n",
      "Iteration 32440 Training loss 0.06190754845738411 Validation loss 0.06057973951101303 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.8987],\n",
      "        [0.9332]], device='mps:0')\n",
      "Iteration 32450 Training loss 0.06492951512336731 Validation loss 0.06068796291947365 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.8943],\n",
      "        [0.8348]], device='mps:0')\n",
      "Iteration 32460 Training loss 0.0575115866959095 Validation loss 0.06057831272482872 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.4826],\n",
      "        [0.0608]], device='mps:0')\n",
      "Iteration 32470 Training loss 0.047728050500154495 Validation loss 0.060635242611169815 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.9694],\n",
      "        [0.3726]], device='mps:0')\n",
      "Iteration 32480 Training loss 0.060007017105817795 Validation loss 0.060579873621463776 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.0209],\n",
      "        [0.2967]], device='mps:0')\n",
      "Iteration 32490 Training loss 0.06041145697236061 Validation loss 0.0606013685464859 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.6593],\n",
      "        [0.9445]], device='mps:0')\n",
      "Iteration 32500 Training loss 0.0521046444773674 Validation loss 0.0605878047645092 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.0898],\n",
      "        [0.0491]], device='mps:0')\n",
      "Iteration 32510 Training loss 0.05771521106362343 Validation loss 0.06056390702724457 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.9720],\n",
      "        [0.9606]], device='mps:0')\n",
      "Iteration 32520 Training loss 0.056989192962646484 Validation loss 0.06059659644961357 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.0719],\n",
      "        [0.6353]], device='mps:0')\n",
      "Iteration 32530 Training loss 0.05512748658657074 Validation loss 0.06058911606669426 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.6444],\n",
      "        [0.0354]], device='mps:0')\n",
      "Iteration 32540 Training loss 0.05969252437353134 Validation loss 0.06064702942967415 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.9840],\n",
      "        [0.0330]], device='mps:0')\n",
      "Iteration 32550 Training loss 0.06209992617368698 Validation loss 0.0607428252696991 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.1553],\n",
      "        [0.0711]], device='mps:0')\n",
      "Iteration 32560 Training loss 0.06425788998603821 Validation loss 0.0605686716735363 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.5769],\n",
      "        [0.8121]], device='mps:0')\n",
      "Iteration 32570 Training loss 0.06629834324121475 Validation loss 0.06063469499349594 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.0463],\n",
      "        [0.0705]], device='mps:0')\n",
      "Iteration 32580 Training loss 0.06662172824144363 Validation loss 0.060580480843782425 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.4597],\n",
      "        [0.8034]], device='mps:0')\n",
      "Iteration 32590 Training loss 0.06065824255347252 Validation loss 0.06063147261738777 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.1743],\n",
      "        [0.8042]], device='mps:0')\n",
      "Iteration 32600 Training loss 0.05959349125623703 Validation loss 0.060699764639139175 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.5830],\n",
      "        [0.6258]], device='mps:0')\n",
      "Iteration 32610 Training loss 0.05345030128955841 Validation loss 0.06057220324873924 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.9923],\n",
      "        [0.1401]], device='mps:0')\n",
      "Iteration 32620 Training loss 0.05933570861816406 Validation loss 0.06058654934167862 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.0707],\n",
      "        [0.9429]], device='mps:0')\n",
      "Iteration 32630 Training loss 0.051366813480854034 Validation loss 0.06060367822647095 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.1608],\n",
      "        [0.9505]], device='mps:0')\n",
      "Iteration 32640 Training loss 0.058994110673666 Validation loss 0.06057637929916382 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.9455],\n",
      "        [0.9621]], device='mps:0')\n",
      "Iteration 32650 Training loss 0.05771858990192413 Validation loss 0.06057458743453026 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.1703],\n",
      "        [0.0994]], device='mps:0')\n",
      "Iteration 32660 Training loss 0.06719093769788742 Validation loss 0.06055724248290062 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.9767],\n",
      "        [0.9197]], device='mps:0')\n",
      "Iteration 32670 Training loss 0.0511203408241272 Validation loss 0.060569725930690765 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.9566],\n",
      "        [0.8000]], device='mps:0')\n",
      "Iteration 32680 Training loss 0.052860062569379807 Validation loss 0.06069428101181984 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.1569],\n",
      "        [0.9810]], device='mps:0')\n",
      "Iteration 32690 Training loss 0.0569717213511467 Validation loss 0.0605613999068737 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.9062],\n",
      "        [0.7357]], device='mps:0')\n",
      "Iteration 32700 Training loss 0.06456098705530167 Validation loss 0.06055396422743797 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.0988],\n",
      "        [0.9144]], device='mps:0')\n",
      "Iteration 32710 Training loss 0.05820109322667122 Validation loss 0.06055981293320656 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.3696],\n",
      "        [0.9674]], device='mps:0')\n",
      "Iteration 32720 Training loss 0.06105852872133255 Validation loss 0.060883570462465286 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.4244],\n",
      "        [0.1445]], device='mps:0')\n",
      "Iteration 32730 Training loss 0.05895935744047165 Validation loss 0.060673199594020844 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.2124],\n",
      "        [0.2234]], device='mps:0')\n",
      "Iteration 32740 Training loss 0.05827021598815918 Validation loss 0.060775887221097946 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.2866],\n",
      "        [0.0016]], device='mps:0')\n",
      "Iteration 32750 Training loss 0.054905764758586884 Validation loss 0.060586243867874146 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.2270],\n",
      "        [0.2834]], device='mps:0')\n",
      "Iteration 32760 Training loss 0.059580497443675995 Validation loss 0.060760777443647385 Accuracy 0.8321250677108765\n",
      "Output tensor([[0.1680],\n",
      "        [0.9692]], device='mps:0')\n",
      "Iteration 32770 Training loss 0.06512493640184402 Validation loss 0.06070815026760101 Accuracy 0.8322500586509705\n",
      "Output tensor([[0.0617],\n",
      "        [0.0288]], device='mps:0')\n",
      "Iteration 32780 Training loss 0.06213609501719475 Validation loss 0.0606449693441391 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.0744],\n",
      "        [0.2370]], device='mps:0')\n",
      "Iteration 32790 Training loss 0.05268732085824013 Validation loss 0.06056147441267967 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.9640],\n",
      "        [0.8866]], device='mps:0')\n",
      "Iteration 32800 Training loss 0.05549701675772667 Validation loss 0.06056144833564758 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.8747],\n",
      "        [0.9796]], device='mps:0')\n",
      "Iteration 32810 Training loss 0.06885810941457748 Validation loss 0.0605606809258461 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.9223],\n",
      "        [0.1831]], device='mps:0')\n",
      "Iteration 32820 Training loss 0.05501960590481758 Validation loss 0.06062355265021324 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.2090],\n",
      "        [0.1725]], device='mps:0')\n",
      "Iteration 32830 Training loss 0.06614530086517334 Validation loss 0.06071778014302254 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.5415],\n",
      "        [0.9773]], device='mps:0')\n",
      "Iteration 32840 Training loss 0.06186981126666069 Validation loss 0.0605660118162632 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.0279],\n",
      "        [0.8171]], device='mps:0')\n",
      "Iteration 32850 Training loss 0.053194817155599594 Validation loss 0.06056636571884155 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.1474],\n",
      "        [0.1822]], device='mps:0')\n",
      "Iteration 32860 Training loss 0.05780172348022461 Validation loss 0.06062141805887222 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.9182],\n",
      "        [0.6881]], device='mps:0')\n",
      "Iteration 32870 Training loss 0.059377413243055344 Validation loss 0.06061394512653351 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.5508],\n",
      "        [0.4402]], device='mps:0')\n",
      "Iteration 32880 Training loss 0.052903372794389725 Validation loss 0.06056288257241249 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.1587],\n",
      "        [0.9006]], device='mps:0')\n",
      "Iteration 32890 Training loss 0.058528218418359756 Validation loss 0.060556408017873764 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.7835],\n",
      "        [0.8857]], device='mps:0')\n",
      "Iteration 32900 Training loss 0.05516156926751137 Validation loss 0.06057209521532059 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.8993],\n",
      "        [0.8625]], device='mps:0')\n",
      "Iteration 32910 Training loss 0.0577794685959816 Validation loss 0.06055070832371712 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.0183],\n",
      "        [0.5489]], device='mps:0')\n",
      "Iteration 32920 Training loss 0.05364782363176346 Validation loss 0.0605519600212574 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.3090],\n",
      "        [0.0292]], device='mps:0')\n",
      "Iteration 32930 Training loss 0.06642358750104904 Validation loss 0.06054726243019104 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.3113],\n",
      "        [0.8615]], device='mps:0')\n",
      "Iteration 32940 Training loss 0.06065650284290314 Validation loss 0.06059446930885315 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.3447],\n",
      "        [0.1761]], device='mps:0')\n",
      "Iteration 32950 Training loss 0.056936681270599365 Validation loss 0.060567714273929596 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.1380],\n",
      "        [0.2019]], device='mps:0')\n",
      "Iteration 32960 Training loss 0.05918777361512184 Validation loss 0.06054370850324631 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.6140],\n",
      "        [0.9528]], device='mps:0')\n",
      "Iteration 32970 Training loss 0.06196131929755211 Validation loss 0.06055080518126488 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.0804],\n",
      "        [0.1343]], device='mps:0')\n",
      "Iteration 32980 Training loss 0.04949125275015831 Validation loss 0.06085974723100662 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.4375],\n",
      "        [0.9470]], device='mps:0')\n",
      "Iteration 32990 Training loss 0.05929647386074066 Validation loss 0.06069094315171242 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.0175],\n",
      "        [0.7324]], device='mps:0')\n",
      "Iteration 33000 Training loss 0.05756412446498871 Validation loss 0.06060553342103958 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.0117],\n",
      "        [0.2999]], device='mps:0')\n",
      "Iteration 33010 Training loss 0.0665154755115509 Validation loss 0.060765087604522705 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.9593],\n",
      "        [0.8241]], device='mps:0')\n",
      "Iteration 33020 Training loss 0.0549798384308815 Validation loss 0.06055580452084541 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.8691],\n",
      "        [0.2800]], device='mps:0')\n",
      "Iteration 33030 Training loss 0.05538678169250488 Validation loss 0.06055862084031105 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.2874],\n",
      "        [0.7624]], device='mps:0')\n",
      "Iteration 33040 Training loss 0.06324277073144913 Validation loss 0.06053388863801956 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.0499],\n",
      "        [0.5192]], device='mps:0')\n",
      "Iteration 33050 Training loss 0.060537394136190414 Validation loss 0.06064452975988388 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.0694],\n",
      "        [0.0887]], device='mps:0')\n",
      "Iteration 33060 Training loss 0.06215623766183853 Validation loss 0.06057615205645561 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.7338],\n",
      "        [0.5498]], device='mps:0')\n",
      "Iteration 33070 Training loss 0.05542343109846115 Validation loss 0.06073234602808952 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.9523],\n",
      "        [0.2341]], device='mps:0')\n",
      "Iteration 33080 Training loss 0.055406272411346436 Validation loss 0.06052883341908455 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.3677],\n",
      "        [0.0898]], device='mps:0')\n",
      "Iteration 33090 Training loss 0.06661040335893631 Validation loss 0.060520321130752563 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.2935],\n",
      "        [0.7377]], device='mps:0')\n",
      "Iteration 33100 Training loss 0.05585034564137459 Validation loss 0.06078541651368141 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.9464],\n",
      "        [0.0628]], device='mps:0')\n",
      "Iteration 33110 Training loss 0.061865657567977905 Validation loss 0.060652539134025574 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.0252],\n",
      "        [0.7942]], device='mps:0')\n",
      "Iteration 33120 Training loss 0.058881085366010666 Validation loss 0.060517940670251846 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.5833],\n",
      "        [0.1099]], device='mps:0')\n",
      "Iteration 33130 Training loss 0.06101347878575325 Validation loss 0.06053638085722923 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.2470],\n",
      "        [0.0533]], device='mps:0')\n",
      "Iteration 33140 Training loss 0.05717312544584274 Validation loss 0.06051775813102722 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.0831],\n",
      "        [0.5942]], device='mps:0')\n",
      "Iteration 33150 Training loss 0.07164321094751358 Validation loss 0.06062182039022446 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.0254],\n",
      "        [0.5840]], device='mps:0')\n",
      "Iteration 33160 Training loss 0.058354005217552185 Validation loss 0.06052910163998604 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.4371],\n",
      "        [0.4336]], device='mps:0')\n",
      "Iteration 33170 Training loss 0.051377590745687485 Validation loss 0.06072218343615532 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.9166],\n",
      "        [0.8116]], device='mps:0')\n",
      "Iteration 33180 Training loss 0.056738778948783875 Validation loss 0.06051268428564072 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.1491],\n",
      "        [0.3669]], device='mps:0')\n",
      "Iteration 33190 Training loss 0.0542965792119503 Validation loss 0.060606665909290314 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.9513],\n",
      "        [0.3326]], device='mps:0')\n",
      "Iteration 33200 Training loss 0.060241106897592545 Validation loss 0.06059115007519722 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.9080],\n",
      "        [0.4268]], device='mps:0')\n",
      "Iteration 33210 Training loss 0.05762908235192299 Validation loss 0.060512032359838486 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.2587],\n",
      "        [0.4599]], device='mps:0')\n",
      "Iteration 33220 Training loss 0.05946515500545502 Validation loss 0.06092319265007973 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.8669],\n",
      "        [0.0198]], device='mps:0')\n",
      "Iteration 33230 Training loss 0.06622008979320526 Validation loss 0.060651104897260666 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.5047],\n",
      "        [0.6204]], device='mps:0')\n",
      "Iteration 33240 Training loss 0.056785933673381805 Validation loss 0.06051795557141304 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.1231],\n",
      "        [0.7251]], device='mps:0')\n",
      "Iteration 33250 Training loss 0.06249367445707321 Validation loss 0.06068045273423195 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.9118],\n",
      "        [0.1447]], device='mps:0')\n",
      "Iteration 33260 Training loss 0.059146981686353683 Validation loss 0.060548920184373856 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.8925],\n",
      "        [0.8014]], device='mps:0')\n",
      "Iteration 33270 Training loss 0.06455094367265701 Validation loss 0.060560617595911026 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.6047],\n",
      "        [0.9068]], device='mps:0')\n",
      "Iteration 33280 Training loss 0.06174488738179207 Validation loss 0.0605028010904789 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.5939],\n",
      "        [0.0895]], device='mps:0')\n",
      "Iteration 33290 Training loss 0.05967237427830696 Validation loss 0.06053878366947174 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.7108],\n",
      "        [0.1694]], device='mps:0')\n",
      "Iteration 33300 Training loss 0.05577465891838074 Validation loss 0.06073892116546631 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.1890],\n",
      "        [0.0825]], device='mps:0')\n",
      "Iteration 33310 Training loss 0.05704226344823837 Validation loss 0.0605267658829689 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.9502],\n",
      "        [0.2169]], device='mps:0')\n",
      "Iteration 33320 Training loss 0.06027032062411308 Validation loss 0.06050628796219826 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.9114],\n",
      "        [0.4376]], device='mps:0')\n",
      "Iteration 33330 Training loss 0.06832444667816162 Validation loss 0.06048712506890297 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.9281],\n",
      "        [0.5184]], device='mps:0')\n",
      "Iteration 33340 Training loss 0.05775613710284233 Validation loss 0.060483165085315704 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.3081],\n",
      "        [0.9987]], device='mps:0')\n",
      "Iteration 33350 Training loss 0.05048808082938194 Validation loss 0.060744378715753555 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.8675],\n",
      "        [0.9222]], device='mps:0')\n",
      "Iteration 33360 Training loss 0.056201960891485214 Validation loss 0.060477741062641144 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.1020],\n",
      "        [0.0168]], device='mps:0')\n",
      "Iteration 33370 Training loss 0.05465837940573692 Validation loss 0.06054259464144707 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.9934],\n",
      "        [0.5719]], device='mps:0')\n",
      "Iteration 33380 Training loss 0.05644147843122482 Validation loss 0.0604945570230484 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.9766],\n",
      "        [0.5082]], device='mps:0')\n",
      "Iteration 33390 Training loss 0.054297931492328644 Validation loss 0.06051994860172272 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.8697],\n",
      "        [0.8054]], device='mps:0')\n",
      "Iteration 33400 Training loss 0.05555136129260063 Validation loss 0.06050960719585419 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.9102],\n",
      "        [0.0970]], device='mps:0')\n",
      "Iteration 33410 Training loss 0.05961082503199577 Validation loss 0.06051280349493027 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.8848],\n",
      "        [0.3160]], device='mps:0')\n",
      "Iteration 33420 Training loss 0.05046921595931053 Validation loss 0.06048131734132767 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.1610],\n",
      "        [0.9684]], device='mps:0')\n",
      "Iteration 33430 Training loss 0.060648661106824875 Validation loss 0.060668375343084335 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.7476],\n",
      "        [0.6463]], device='mps:0')\n",
      "Iteration 33440 Training loss 0.057424090802669525 Validation loss 0.06051948666572571 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.1306],\n",
      "        [0.3357]], device='mps:0')\n",
      "Iteration 33450 Training loss 0.058479081839323044 Validation loss 0.06050983816385269 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.9502],\n",
      "        [0.8981]], device='mps:0')\n",
      "Iteration 33460 Training loss 0.06242494285106659 Validation loss 0.06049075722694397 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.2086],\n",
      "        [0.7541]], device='mps:0')\n",
      "Iteration 33470 Training loss 0.05110928788781166 Validation loss 0.060522064566612244 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.9235],\n",
      "        [0.9621]], device='mps:0')\n",
      "Iteration 33480 Training loss 0.0533028244972229 Validation loss 0.06048787385225296 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.1525],\n",
      "        [0.2644]], device='mps:0')\n",
      "Iteration 33490 Training loss 0.0577780082821846 Validation loss 0.06088007614016533 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.5369],\n",
      "        [0.0484]], device='mps:0')\n",
      "Iteration 33500 Training loss 0.05167072266340256 Validation loss 0.06047666817903519 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.7441],\n",
      "        [0.2862]], device='mps:0')\n",
      "Iteration 33510 Training loss 0.059530723839998245 Validation loss 0.060814037919044495 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.7585],\n",
      "        [0.0316]], device='mps:0')\n",
      "Iteration 33520 Training loss 0.062207430601119995 Validation loss 0.060478709638118744 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.4670],\n",
      "        [0.0469]], device='mps:0')\n",
      "Iteration 33530 Training loss 0.05674675852060318 Validation loss 0.060484133660793304 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.9438],\n",
      "        [0.2558]], device='mps:0')\n",
      "Iteration 33540 Training loss 0.06257504224777222 Validation loss 0.060693249106407166 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.9702],\n",
      "        [0.2611]], device='mps:0')\n",
      "Iteration 33550 Training loss 0.05251263454556465 Validation loss 0.060475654900074005 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.1451],\n",
      "        [0.1878]], device='mps:0')\n",
      "Iteration 33560 Training loss 0.061879150569438934 Validation loss 0.06047024205327034 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.7114],\n",
      "        [0.5289]], device='mps:0')\n",
      "Iteration 33570 Training loss 0.05571888014674187 Validation loss 0.06047099828720093 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.1080],\n",
      "        [0.4519]], device='mps:0')\n",
      "Iteration 33580 Training loss 0.061798982322216034 Validation loss 0.06048043444752693 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.8051],\n",
      "        [0.0478]], device='mps:0')\n",
      "Iteration 33590 Training loss 0.05765008181333542 Validation loss 0.060531146824359894 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.0492],\n",
      "        [0.0771]], device='mps:0')\n",
      "Iteration 33600 Training loss 0.0569428950548172 Validation loss 0.06053685396909714 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.9526],\n",
      "        [0.1706]], device='mps:0')\n",
      "Iteration 33610 Training loss 0.06122901290655136 Validation loss 0.06064153462648392 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.6605],\n",
      "        [0.0709]], device='mps:0')\n",
      "Iteration 33620 Training loss 0.059491116553545 Validation loss 0.06050841882824898 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.2511],\n",
      "        [0.0016]], device='mps:0')\n",
      "Iteration 33630 Training loss 0.06074388697743416 Validation loss 0.060553018003702164 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.0618],\n",
      "        [0.9056]], device='mps:0')\n",
      "Iteration 33640 Training loss 0.056359197944402695 Validation loss 0.060517117381095886 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.8545],\n",
      "        [0.4023]], device='mps:0')\n",
      "Iteration 33650 Training loss 0.059918325394392014 Validation loss 0.06047265604138374 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.7393],\n",
      "        [0.0556]], device='mps:0')\n",
      "Iteration 33660 Training loss 0.062267523258924484 Validation loss 0.06056704372167587 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.7625],\n",
      "        [0.6754]], device='mps:0')\n",
      "Iteration 33670 Training loss 0.06037038192152977 Validation loss 0.06053098663687706 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.9823],\n",
      "        [0.7430]], device='mps:0')\n",
      "Iteration 33680 Training loss 0.052630890160799026 Validation loss 0.06050247326493263 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.9287],\n",
      "        [0.9994]], device='mps:0')\n",
      "Iteration 33690 Training loss 0.06455881893634796 Validation loss 0.06053429841995239 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.9864],\n",
      "        [0.0846]], device='mps:0')\n",
      "Iteration 33700 Training loss 0.060056254267692566 Validation loss 0.06045424938201904 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.7790],\n",
      "        [0.6972]], device='mps:0')\n",
      "Iteration 33710 Training loss 0.06399118155241013 Validation loss 0.06048991158604622 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.3492],\n",
      "        [0.0848]], device='mps:0')\n",
      "Iteration 33720 Training loss 0.04789911210536957 Validation loss 0.0604478195309639 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.4612],\n",
      "        [0.8785]], device='mps:0')\n",
      "Iteration 33730 Training loss 0.06312232464551926 Validation loss 0.06092110276222229 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.3979],\n",
      "        [0.4815]], device='mps:0')\n",
      "Iteration 33740 Training loss 0.06027012690901756 Validation loss 0.06071130558848381 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.8427],\n",
      "        [0.9249]], device='mps:0')\n",
      "Iteration 33750 Training loss 0.05165878310799599 Validation loss 0.060502778738737106 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.0923],\n",
      "        [0.2682]], device='mps:0')\n",
      "Iteration 33760 Training loss 0.059705886989831924 Validation loss 0.06060464680194855 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.9795],\n",
      "        [0.7172]], device='mps:0')\n",
      "Iteration 33770 Training loss 0.059123095124959946 Validation loss 0.060859836637973785 Accuracy 0.8326250314712524\n",
      "Output tensor([[0.3382],\n",
      "        [0.5004]], device='mps:0')\n",
      "Iteration 33780 Training loss 0.053112562745809555 Validation loss 0.06045357137918472 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.0510],\n",
      "        [0.7501]], device='mps:0')\n",
      "Iteration 33790 Training loss 0.06338147819042206 Validation loss 0.06044459342956543 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.1529],\n",
      "        [0.0644]], device='mps:0')\n",
      "Iteration 33800 Training loss 0.06309601664543152 Validation loss 0.06049281731247902 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.1711],\n",
      "        [0.3277]], device='mps:0')\n",
      "Iteration 33810 Training loss 0.055727146565914154 Validation loss 0.0604911707341671 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.9852],\n",
      "        [0.9575]], device='mps:0')\n",
      "Iteration 33820 Training loss 0.05766058713197708 Validation loss 0.06049226596951485 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.9909],\n",
      "        [0.5210]], device='mps:0')\n",
      "Iteration 33830 Training loss 0.05329250544309616 Validation loss 0.060492292046546936 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.3590],\n",
      "        [0.9933]], device='mps:0')\n",
      "Iteration 33840 Training loss 0.06325269490480423 Validation loss 0.060442984104156494 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.0466],\n",
      "        [0.0323]], device='mps:0')\n",
      "Iteration 33850 Training loss 0.04745139554142952 Validation loss 0.06044747680425644 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.1416],\n",
      "        [0.7457]], device='mps:0')\n",
      "Iteration 33860 Training loss 0.04503611475229263 Validation loss 0.060561053454875946 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.9328],\n",
      "        [0.3308]], device='mps:0')\n",
      "Iteration 33870 Training loss 0.05191301181912422 Validation loss 0.060445915907621384 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.8813],\n",
      "        [0.7851]], device='mps:0')\n",
      "Iteration 33880 Training loss 0.05250648781657219 Validation loss 0.06050544232130051 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.5686],\n",
      "        [0.6399]], device='mps:0')\n",
      "Iteration 33890 Training loss 0.0633058175444603 Validation loss 0.06057316064834595 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.5056],\n",
      "        [0.7328]], device='mps:0')\n",
      "Iteration 33900 Training loss 0.06004251912236214 Validation loss 0.060427673161029816 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.1157],\n",
      "        [0.3187]], device='mps:0')\n",
      "Iteration 33910 Training loss 0.05511192977428436 Validation loss 0.060467977076768875 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.0435],\n",
      "        [0.3730]], device='mps:0')\n",
      "Iteration 33920 Training loss 0.065528005361557 Validation loss 0.060431838035583496 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.6438],\n",
      "        [0.0144]], device='mps:0')\n",
      "Iteration 33930 Training loss 0.06460567563772202 Validation loss 0.060421306639909744 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.1031],\n",
      "        [0.6893]], device='mps:0')\n",
      "Iteration 33940 Training loss 0.06525548547506332 Validation loss 0.06041494384407997 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.2478],\n",
      "        [0.5377]], device='mps:0')\n",
      "Iteration 33950 Training loss 0.05175010859966278 Validation loss 0.06041617691516876 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.8806],\n",
      "        [0.8084]], device='mps:0')\n",
      "Iteration 33960 Training loss 0.0702102854847908 Validation loss 0.06044011563062668 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.0920],\n",
      "        [0.0486]], device='mps:0')\n",
      "Iteration 33970 Training loss 0.06092795357108116 Validation loss 0.060490187257528305 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.7746],\n",
      "        [0.0177]], device='mps:0')\n",
      "Iteration 33980 Training loss 0.05498337000608444 Validation loss 0.06072248890995979 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.9637],\n",
      "        [0.9811]], device='mps:0')\n",
      "Iteration 33990 Training loss 0.0602017380297184 Validation loss 0.06045093014836311 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.9781],\n",
      "        [0.1375]], device='mps:0')\n",
      "Iteration 34000 Training loss 0.0652574971318245 Validation loss 0.06052008271217346 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.2311],\n",
      "        [0.6851]], device='mps:0')\n",
      "Iteration 34010 Training loss 0.05319440737366676 Validation loss 0.06060779094696045 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.3981],\n",
      "        [0.9396]], device='mps:0')\n",
      "Iteration 34020 Training loss 0.06119159236550331 Validation loss 0.06043807789683342 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.3358],\n",
      "        [0.1513]], device='mps:0')\n",
      "Iteration 34030 Training loss 0.06084522604942322 Validation loss 0.0605040043592453 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.2741],\n",
      "        [0.3762]], device='mps:0')\n",
      "Iteration 34040 Training loss 0.04944612458348274 Validation loss 0.06043914705514908 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.9426],\n",
      "        [0.1345]], device='mps:0')\n",
      "Iteration 34050 Training loss 0.058480631560087204 Validation loss 0.060939304530620575 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.8398],\n",
      "        [0.6172]], device='mps:0')\n",
      "Iteration 34060 Training loss 0.06185547634959221 Validation loss 0.06041199713945389 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.1819],\n",
      "        [0.0727]], device='mps:0')\n",
      "Iteration 34070 Training loss 0.05928193777799606 Validation loss 0.060398586094379425 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.8536],\n",
      "        [0.0758]], device='mps:0')\n",
      "Iteration 34080 Training loss 0.052754707634449005 Validation loss 0.06039922684431076 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.1071],\n",
      "        [0.9642]], device='mps:0')\n",
      "Iteration 34090 Training loss 0.06853701174259186 Validation loss 0.06044546142220497 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.4067],\n",
      "        [0.2322]], device='mps:0')\n",
      "Iteration 34100 Training loss 0.06214975565671921 Validation loss 0.060431208461523056 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.6852],\n",
      "        [0.0660]], device='mps:0')\n",
      "Iteration 34110 Training loss 0.06074408069252968 Validation loss 0.06069221347570419 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.1168],\n",
      "        [0.9292]], device='mps:0')\n",
      "Iteration 34120 Training loss 0.05996961519122124 Validation loss 0.060536421835422516 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.3863],\n",
      "        [0.8492]], device='mps:0')\n",
      "Iteration 34130 Training loss 0.0682472512125969 Validation loss 0.060415152460336685 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.1282],\n",
      "        [0.2683]], device='mps:0')\n",
      "Iteration 34140 Training loss 0.06970717012882233 Validation loss 0.06041180342435837 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.1179],\n",
      "        [0.0512]], device='mps:0')\n",
      "Iteration 34150 Training loss 0.05391005426645279 Validation loss 0.06041986495256424 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.6752],\n",
      "        [0.6495]], device='mps:0')\n",
      "Iteration 34160 Training loss 0.05549950525164604 Validation loss 0.06040874868631363 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.9248],\n",
      "        [0.7818]], device='mps:0')\n",
      "Iteration 34170 Training loss 0.06251700967550278 Validation loss 0.06043388321995735 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.9830],\n",
      "        [0.9634]], device='mps:0')\n",
      "Iteration 34180 Training loss 0.05875006690621376 Validation loss 0.060481492429971695 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.2448],\n",
      "        [0.1027]], device='mps:0')\n",
      "Iteration 34190 Training loss 0.06068915128707886 Validation loss 0.06059759482741356 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.8997],\n",
      "        [0.1613]], device='mps:0')\n",
      "Iteration 34200 Training loss 0.05519483610987663 Validation loss 0.06039842218160629 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.0540],\n",
      "        [0.0521]], device='mps:0')\n",
      "Iteration 34210 Training loss 0.05996781215071678 Validation loss 0.06040521711111069 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.5326],\n",
      "        [0.1804]], device='mps:0')\n",
      "Iteration 34220 Training loss 0.07030823081731796 Validation loss 0.060434743762016296 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.3735],\n",
      "        [0.1503]], device='mps:0')\n",
      "Iteration 34230 Training loss 0.06284929811954498 Validation loss 0.06048279255628586 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.1404],\n",
      "        [0.1017]], device='mps:0')\n",
      "Iteration 34240 Training loss 0.05674195662140846 Validation loss 0.06045122817158699 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.1915],\n",
      "        [0.2983]], device='mps:0')\n",
      "Iteration 34250 Training loss 0.054610297083854675 Validation loss 0.06043000519275665 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.7472],\n",
      "        [0.7558]], device='mps:0')\n",
      "Iteration 34260 Training loss 0.05976065620779991 Validation loss 0.060440633445978165 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.3234],\n",
      "        [0.1439]], device='mps:0')\n",
      "Iteration 34270 Training loss 0.06749454140663147 Validation loss 0.06047593802213669 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.9703],\n",
      "        [0.1133]], device='mps:0')\n",
      "Iteration 34280 Training loss 0.061766259372234344 Validation loss 0.06038109213113785 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.1458],\n",
      "        [0.9649]], device='mps:0')\n",
      "Iteration 34290 Training loss 0.06069376692175865 Validation loss 0.0604955330491066 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.5651],\n",
      "        [0.8555]], device='mps:0')\n",
      "Iteration 34300 Training loss 0.05241894721984863 Validation loss 0.06038631871342659 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.7282],\n",
      "        [0.9423]], device='mps:0')\n",
      "Iteration 34310 Training loss 0.05465882271528244 Validation loss 0.060446836054325104 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.0601],\n",
      "        [0.8696]], device='mps:0')\n",
      "Iteration 34320 Training loss 0.0604151152074337 Validation loss 0.06038113683462143 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.6110],\n",
      "        [0.0578]], device='mps:0')\n",
      "Iteration 34330 Training loss 0.05131016671657562 Validation loss 0.060406237840652466 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.2769],\n",
      "        [0.2096]], device='mps:0')\n",
      "Iteration 34340 Training loss 0.054499559104442596 Validation loss 0.06037992238998413 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.7873],\n",
      "        [0.2055]], device='mps:0')\n",
      "Iteration 34350 Training loss 0.05922763794660568 Validation loss 0.06053566560149193 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.6904],\n",
      "        [0.9240]], device='mps:0')\n",
      "Iteration 34360 Training loss 0.06338658183813095 Validation loss 0.060394853353500366 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.6525],\n",
      "        [0.7006]], device='mps:0')\n",
      "Iteration 34370 Training loss 0.06284773349761963 Validation loss 0.06039438396692276 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.0126],\n",
      "        [0.5192]], device='mps:0')\n",
      "Iteration 34380 Training loss 0.06976249814033508 Validation loss 0.06037793308496475 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.1480],\n",
      "        [0.2975]], device='mps:0')\n",
      "Iteration 34390 Training loss 0.06593523919582367 Validation loss 0.06039026007056236 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.2867],\n",
      "        [0.0132]], device='mps:0')\n",
      "Iteration 34400 Training loss 0.06292848289012909 Validation loss 0.06037157028913498 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.2093],\n",
      "        [0.1782]], device='mps:0')\n",
      "Iteration 34410 Training loss 0.05853580683469772 Validation loss 0.0604596883058548 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.9468],\n",
      "        [0.9474]], device='mps:0')\n",
      "Iteration 34420 Training loss 0.05092538893222809 Validation loss 0.06054499000310898 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.0442],\n",
      "        [0.9897]], device='mps:0')\n",
      "Iteration 34430 Training loss 0.05856146663427353 Validation loss 0.06042398884892464 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.1894],\n",
      "        [0.2180]], device='mps:0')\n",
      "Iteration 34440 Training loss 0.05610258877277374 Validation loss 0.06043887883424759 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.0054],\n",
      "        [0.8117]], device='mps:0')\n",
      "Iteration 34450 Training loss 0.053194038569927216 Validation loss 0.06038244068622589 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.2064],\n",
      "        [0.9372]], device='mps:0')\n",
      "Iteration 34460 Training loss 0.05458255112171173 Validation loss 0.06041065603494644 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.8648],\n",
      "        [0.2460]], device='mps:0')\n",
      "Iteration 34470 Training loss 0.06091831997036934 Validation loss 0.06046715006232262 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.0111],\n",
      "        [0.9423]], device='mps:0')\n",
      "Iteration 34480 Training loss 0.0552198588848114 Validation loss 0.060361865907907486 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.5310],\n",
      "        [0.9429]], device='mps:0')\n",
      "Iteration 34490 Training loss 0.05919666960835457 Validation loss 0.060395341366529465 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.3785],\n",
      "        [0.2976]], device='mps:0')\n",
      "Iteration 34500 Training loss 0.054768066853284836 Validation loss 0.060358453541994095 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.6354],\n",
      "        [0.0566]], device='mps:0')\n",
      "Iteration 34510 Training loss 0.06080912426114082 Validation loss 0.06036048009991646 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.4904],\n",
      "        [0.5458]], device='mps:0')\n",
      "Iteration 34520 Training loss 0.061372894793748856 Validation loss 0.06036052852869034 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.1020],\n",
      "        [0.9941]], device='mps:0')\n",
      "Iteration 34530 Training loss 0.06495384871959686 Validation loss 0.06036001816391945 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.2368],\n",
      "        [0.9661]], device='mps:0')\n",
      "Iteration 34540 Training loss 0.06427210569381714 Validation loss 0.06038115173578262 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.5594],\n",
      "        [0.9777]], device='mps:0')\n",
      "Iteration 34550 Training loss 0.05618412047624588 Validation loss 0.060368314385414124 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.1799],\n",
      "        [0.0502]], device='mps:0')\n",
      "Iteration 34560 Training loss 0.05642763897776604 Validation loss 0.06036051735281944 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.9585],\n",
      "        [0.2360]], device='mps:0')\n",
      "Iteration 34570 Training loss 0.05375976487994194 Validation loss 0.06055014580488205 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.0508],\n",
      "        [0.6759]], device='mps:0')\n",
      "Iteration 34580 Training loss 0.052493881434202194 Validation loss 0.060347747057676315 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.1183],\n",
      "        [0.0434]], device='mps:0')\n",
      "Iteration 34590 Training loss 0.05771438777446747 Validation loss 0.060440871864557266 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.0606],\n",
      "        [0.0301]], device='mps:0')\n",
      "Iteration 34600 Training loss 0.053785551339387894 Validation loss 0.06040198728442192 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.8523],\n",
      "        [0.6945]], device='mps:0')\n",
      "Iteration 34610 Training loss 0.05377526953816414 Validation loss 0.060347482562065125 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.0175],\n",
      "        [0.0122]], device='mps:0')\n",
      "Iteration 34620 Training loss 0.060863275080919266 Validation loss 0.060350269079208374 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.0251],\n",
      "        [0.2464]], device='mps:0')\n",
      "Iteration 34630 Training loss 0.07124331593513489 Validation loss 0.060389596968889236 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.5883],\n",
      "        [0.9579]], device='mps:0')\n",
      "Iteration 34640 Training loss 0.06057693436741829 Validation loss 0.0603426918387413 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.7260],\n",
      "        [0.9516]], device='mps:0')\n",
      "Iteration 34650 Training loss 0.057261474430561066 Validation loss 0.060333941131830215 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.0738],\n",
      "        [0.9108]], device='mps:0')\n",
      "Iteration 34660 Training loss 0.05198349803686142 Validation loss 0.060380399227142334 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.5779],\n",
      "        [0.7876]], device='mps:0')\n",
      "Iteration 34670 Training loss 0.05494627729058266 Validation loss 0.06040916591882706 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.0267],\n",
      "        [0.9639]], device='mps:0')\n",
      "Iteration 34680 Training loss 0.05534747615456581 Validation loss 0.06032326817512512 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.9636],\n",
      "        [0.7586]], device='mps:0')\n",
      "Iteration 34690 Training loss 0.056110747158527374 Validation loss 0.06042701378464699 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.5901],\n",
      "        [0.1357]], device='mps:0')\n",
      "Iteration 34700 Training loss 0.05588560551404953 Validation loss 0.06032441183924675 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.3998],\n",
      "        [0.9751]], device='mps:0')\n",
      "Iteration 34710 Training loss 0.06269065290689468 Validation loss 0.06039821729063988 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.0864],\n",
      "        [0.2717]], device='mps:0')\n",
      "Iteration 34720 Training loss 0.056378498673439026 Validation loss 0.06034180149435997 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.9175],\n",
      "        [0.4480]], device='mps:0')\n",
      "Iteration 34730 Training loss 0.05732244998216629 Validation loss 0.060342296957969666 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.1617],\n",
      "        [0.8422]], device='mps:0')\n",
      "Iteration 34740 Training loss 0.060610584914684296 Validation loss 0.06033632159233093 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.9618],\n",
      "        [0.6042]], device='mps:0')\n",
      "Iteration 34750 Training loss 0.052452582865953445 Validation loss 0.060456790030002594 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.0042],\n",
      "        [0.5884]], device='mps:0')\n",
      "Iteration 34760 Training loss 0.05915570259094238 Validation loss 0.060408949851989746 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.1902],\n",
      "        [0.7712]], device='mps:0')\n",
      "Iteration 34770 Training loss 0.06552428752183914 Validation loss 0.060348574072122574 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.7087],\n",
      "        [0.5402]], device='mps:0')\n",
      "Iteration 34780 Training loss 0.054602283984422684 Validation loss 0.06041853874921799 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.9376],\n",
      "        [0.1635]], device='mps:0')\n",
      "Iteration 34790 Training loss 0.06171690672636032 Validation loss 0.06055064499378204 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.4302],\n",
      "        [0.6028]], device='mps:0')\n",
      "Iteration 34800 Training loss 0.056860145181417465 Validation loss 0.060570597648620605 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.8601],\n",
      "        [0.3816]], device='mps:0')\n",
      "Iteration 34810 Training loss 0.06352519243955612 Validation loss 0.06036509945988655 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.5795],\n",
      "        [0.8097]], device='mps:0')\n",
      "Iteration 34820 Training loss 0.06473925709724426 Validation loss 0.06031748652458191 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.0719],\n",
      "        [0.3245]], device='mps:0')\n",
      "Iteration 34830 Training loss 0.05476592853665352 Validation loss 0.06033121049404144 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.9908],\n",
      "        [0.8994]], device='mps:0')\n",
      "Iteration 34840 Training loss 0.055340614169836044 Validation loss 0.060503508895635605 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.0128],\n",
      "        [0.6164]], device='mps:0')\n",
      "Iteration 34850 Training loss 0.05673728510737419 Validation loss 0.060318730771541595 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.8151],\n",
      "        [0.1702]], device='mps:0')\n",
      "Iteration 34860 Training loss 0.059138912707567215 Validation loss 0.06047789752483368 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.0515],\n",
      "        [0.1658]], device='mps:0')\n",
      "Iteration 34870 Training loss 0.05134071409702301 Validation loss 0.06031852960586548 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.2266],\n",
      "        [0.5238]], device='mps:0')\n",
      "Iteration 34880 Training loss 0.05020485818386078 Validation loss 0.060319095849990845 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.9872],\n",
      "        [0.4367]], device='mps:0')\n",
      "Iteration 34890 Training loss 0.06503818184137344 Validation loss 0.060317229479551315 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.0136],\n",
      "        [0.1550]], device='mps:0')\n",
      "Iteration 34900 Training loss 0.05504190921783447 Validation loss 0.06033773347735405 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.6594],\n",
      "        [0.8658]], device='mps:0')\n",
      "Iteration 34910 Training loss 0.06456825882196426 Validation loss 0.06033524498343468 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.0693],\n",
      "        [0.3787]], device='mps:0')\n",
      "Iteration 34920 Training loss 0.05018094927072525 Validation loss 0.060358814895153046 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.2554],\n",
      "        [0.0107]], device='mps:0')\n",
      "Iteration 34930 Training loss 0.05113234743475914 Validation loss 0.06035902351140976 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.9267],\n",
      "        [0.3915]], device='mps:0')\n",
      "Iteration 34940 Training loss 0.062211181968450546 Validation loss 0.06035294011235237 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.7458],\n",
      "        [0.9564]], device='mps:0')\n",
      "Iteration 34950 Training loss 0.05308205634355545 Validation loss 0.06035958230495453 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.2593],\n",
      "        [0.1359]], device='mps:0')\n",
      "Iteration 34960 Training loss 0.058435358107089996 Validation loss 0.060625653713941574 Accuracy 0.8336250185966492\n",
      "Output tensor([[0.1407],\n",
      "        [0.8348]], device='mps:0')\n",
      "Iteration 34970 Training loss 0.049608707427978516 Validation loss 0.06035764515399933 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.1900],\n",
      "        [0.8938]], device='mps:0')\n",
      "Iteration 34980 Training loss 0.06324679404497147 Validation loss 0.06031838431954384 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.2298],\n",
      "        [0.2297]], device='mps:0')\n",
      "Iteration 34990 Training loss 0.047468967735767365 Validation loss 0.06032475084066391 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.2670],\n",
      "        [0.8380]], device='mps:0')\n",
      "Iteration 35000 Training loss 0.054063037037849426 Validation loss 0.06031417474150658 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.0024],\n",
      "        [0.8675]], device='mps:0')\n",
      "Iteration 35010 Training loss 0.05560343712568283 Validation loss 0.06031157821416855 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.1539],\n",
      "        [0.0413]], device='mps:0')\n",
      "Iteration 35020 Training loss 0.048951584845781326 Validation loss 0.060344573110342026 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.0117],\n",
      "        [0.4589]], device='mps:0')\n",
      "Iteration 35030 Training loss 0.06231139227747917 Validation loss 0.06031584367156029 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.2748],\n",
      "        [0.9670]], device='mps:0')\n",
      "Iteration 35040 Training loss 0.05916861444711685 Validation loss 0.06030568853020668 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.2105],\n",
      "        [0.8346]], device='mps:0')\n",
      "Iteration 35050 Training loss 0.059384267777204514 Validation loss 0.06031220778822899 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.3815],\n",
      "        [0.1479]], device='mps:0')\n",
      "Iteration 35060 Training loss 0.0556626133620739 Validation loss 0.060310911387205124 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.0563],\n",
      "        [0.4116]], device='mps:0')\n",
      "Iteration 35070 Training loss 0.05666852369904518 Validation loss 0.06041640415787697 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.2773],\n",
      "        [0.1488]], device='mps:0')\n",
      "Iteration 35080 Training loss 0.06598781794309616 Validation loss 0.060331329703330994 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.9686],\n",
      "        [0.1226]], device='mps:0')\n",
      "Iteration 35090 Training loss 0.05947163328528404 Validation loss 0.06040213257074356 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.9355],\n",
      "        [0.1186]], device='mps:0')\n",
      "Iteration 35100 Training loss 0.06496798247098923 Validation loss 0.060302671045064926 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.0129],\n",
      "        [0.0379]], device='mps:0')\n",
      "Iteration 35110 Training loss 0.059932854026556015 Validation loss 0.060376521199941635 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.9724],\n",
      "        [0.9674]], device='mps:0')\n",
      "Iteration 35120 Training loss 0.06013434752821922 Validation loss 0.06032601743936539 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.9729],\n",
      "        [0.0480]], device='mps:0')\n",
      "Iteration 35130 Training loss 0.0604887381196022 Validation loss 0.060301508754491806 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.4988],\n",
      "        [0.0934]], device='mps:0')\n",
      "Iteration 35140 Training loss 0.06352818012237549 Validation loss 0.060733769088983536 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.9461],\n",
      "        [0.2820]], device='mps:0')\n",
      "Iteration 35150 Training loss 0.059189289808273315 Validation loss 0.0603131428360939 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.1877],\n",
      "        [0.6970]], device='mps:0')\n",
      "Iteration 35160 Training loss 0.06795860081911087 Validation loss 0.060291316360235214 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.8599],\n",
      "        [0.7428]], device='mps:0')\n",
      "Iteration 35170 Training loss 0.05787741392850876 Validation loss 0.06057653948664665 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.7151],\n",
      "        [0.1551]], device='mps:0')\n",
      "Iteration 35180 Training loss 0.06353509426116943 Validation loss 0.06029319390654564 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.9529],\n",
      "        [0.7886]], device='mps:0')\n",
      "Iteration 35190 Training loss 0.06004536524415016 Validation loss 0.060451947152614594 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.0964],\n",
      "        [0.9884]], device='mps:0')\n",
      "Iteration 35200 Training loss 0.06214465945959091 Validation loss 0.060945890843868256 Accuracy 0.8325000405311584\n",
      "Output tensor([[0.0163],\n",
      "        [0.9370]], device='mps:0')\n",
      "Iteration 35210 Training loss 0.05873221158981323 Validation loss 0.06033161282539368 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.8526],\n",
      "        [0.0559]], device='mps:0')\n",
      "Iteration 35220 Training loss 0.06336355954408646 Validation loss 0.060336388647556305 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.4251],\n",
      "        [0.9696]], device='mps:0')\n",
      "Iteration 35230 Training loss 0.06195230036973953 Validation loss 0.06031207740306854 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.8053],\n",
      "        [0.1049]], device='mps:0')\n",
      "Iteration 35240 Training loss 0.049271855503320694 Validation loss 0.06050367280840874 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.4035],\n",
      "        [0.5774]], device='mps:0')\n",
      "Iteration 35250 Training loss 0.0556950569152832 Validation loss 0.06028562784194946 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.4059],\n",
      "        [0.4313]], device='mps:0')\n",
      "Iteration 35260 Training loss 0.06048484519124031 Validation loss 0.060286760330200195 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.2055],\n",
      "        [0.5076]], device='mps:0')\n",
      "Iteration 35270 Training loss 0.06409727036952972 Validation loss 0.06037990376353264 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.2030],\n",
      "        [0.3976]], device='mps:0')\n",
      "Iteration 35280 Training loss 0.05981021374464035 Validation loss 0.0602727010846138 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.1223],\n",
      "        [0.5830]], device='mps:0')\n",
      "Iteration 35290 Training loss 0.06215595081448555 Validation loss 0.06026510149240494 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.8439],\n",
      "        [0.3947]], device='mps:0')\n",
      "Iteration 35300 Training loss 0.04798179492354393 Validation loss 0.06033189967274666 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.6198],\n",
      "        [0.6770]], device='mps:0')\n",
      "Iteration 35310 Training loss 0.06123325228691101 Validation loss 0.06028258800506592 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.3332],\n",
      "        [0.4887]], device='mps:0')\n",
      "Iteration 35320 Training loss 0.05987703055143356 Validation loss 0.06027188152074814 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.0423],\n",
      "        [0.9692]], device='mps:0')\n",
      "Iteration 35330 Training loss 0.05877024680376053 Validation loss 0.0603892020881176 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.7950],\n",
      "        [0.5411]], device='mps:0')\n",
      "Iteration 35340 Training loss 0.05745982006192207 Validation loss 0.060285117477178574 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.7180],\n",
      "        [0.1603]], device='mps:0')\n",
      "Iteration 35350 Training loss 0.06286671757698059 Validation loss 0.060257695615291595 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.0551],\n",
      "        [0.7544]], device='mps:0')\n",
      "Iteration 35360 Training loss 0.0605916865170002 Validation loss 0.060284845530986786 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.9961],\n",
      "        [0.1932]], device='mps:0')\n",
      "Iteration 35370 Training loss 0.06572676450014114 Validation loss 0.06030578166246414 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.8921],\n",
      "        [0.5814]], device='mps:0')\n",
      "Iteration 35380 Training loss 0.062384556978940964 Validation loss 0.0605294406414032 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.8103],\n",
      "        [0.0255]], device='mps:0')\n",
      "Iteration 35390 Training loss 0.058066174387931824 Validation loss 0.060282446444034576 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.7676],\n",
      "        [0.9868]], device='mps:0')\n",
      "Iteration 35400 Training loss 0.05433754622936249 Validation loss 0.060244735330343246 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.8085],\n",
      "        [0.1165]], device='mps:0')\n",
      "Iteration 35410 Training loss 0.052320193499326706 Validation loss 0.06037704274058342 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.0210],\n",
      "        [0.8707]], device='mps:0')\n",
      "Iteration 35420 Training loss 0.05656266212463379 Validation loss 0.06027687340974808 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.8240],\n",
      "        [0.9579]], device='mps:0')\n",
      "Iteration 35430 Training loss 0.04956255480647087 Validation loss 0.06025524437427521 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.3760],\n",
      "        [0.7711]], device='mps:0')\n",
      "Iteration 35440 Training loss 0.05413509160280228 Validation loss 0.06027157977223396 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.8372],\n",
      "        [0.9656]], device='mps:0')\n",
      "Iteration 35450 Training loss 0.060088880360126495 Validation loss 0.06025146692991257 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.0259],\n",
      "        [0.8767]], device='mps:0')\n",
      "Iteration 35460 Training loss 0.05318228900432587 Validation loss 0.060249485075473785 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.8657],\n",
      "        [0.9268]], device='mps:0')\n",
      "Iteration 35470 Training loss 0.05388428643345833 Validation loss 0.06023051217198372 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.4379],\n",
      "        [0.7013]], device='mps:0')\n",
      "Iteration 35480 Training loss 0.06738905608654022 Validation loss 0.06027086079120636 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.0267],\n",
      "        [0.1196]], device='mps:0')\n",
      "Iteration 35490 Training loss 0.058713559061288834 Validation loss 0.060249216854572296 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.8834],\n",
      "        [0.1417]], device='mps:0')\n",
      "Iteration 35500 Training loss 0.044357411563396454 Validation loss 0.060352228581905365 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.0935],\n",
      "        [0.0966]], device='mps:0')\n",
      "Iteration 35510 Training loss 0.06110739707946777 Validation loss 0.06022723391652107 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.9720],\n",
      "        [0.7505]], device='mps:0')\n",
      "Iteration 35520 Training loss 0.060194339603185654 Validation loss 0.06023005023598671 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.8182],\n",
      "        [0.1894]], device='mps:0')\n",
      "Iteration 35530 Training loss 0.06007380411028862 Validation loss 0.06053349748253822 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.9515],\n",
      "        [0.9850]], device='mps:0')\n",
      "Iteration 35540 Training loss 0.05750933662056923 Validation loss 0.06023021042346954 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.7947],\n",
      "        [0.8396]], device='mps:0')\n",
      "Iteration 35550 Training loss 0.054737310856580734 Validation loss 0.06023602560162544 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.9089],\n",
      "        [0.0441]], device='mps:0')\n",
      "Iteration 35560 Training loss 0.05420219153165817 Validation loss 0.06022760272026062 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.9748],\n",
      "        [0.0428]], device='mps:0')\n",
      "Iteration 35570 Training loss 0.06097417697310448 Validation loss 0.06030464172363281 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.8917],\n",
      "        [0.8725]], device='mps:0')\n",
      "Iteration 35580 Training loss 0.06328310072422028 Validation loss 0.06023845821619034 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.9451],\n",
      "        [0.0542]], device='mps:0')\n",
      "Iteration 35590 Training loss 0.0557137057185173 Validation loss 0.06022502854466438 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.6726],\n",
      "        [0.9657]], device='mps:0')\n",
      "Iteration 35600 Training loss 0.04986277595162392 Validation loss 0.06024997681379318 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.9546],\n",
      "        [0.1026]], device='mps:0')\n",
      "Iteration 35610 Training loss 0.059488799422979355 Validation loss 0.0602375827729702 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.5606],\n",
      "        [0.2775]], device='mps:0')\n",
      "Iteration 35620 Training loss 0.06030662730336189 Validation loss 0.06021750718355179 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.5944],\n",
      "        [0.0965]], device='mps:0')\n",
      "Iteration 35630 Training loss 0.06543738394975662 Validation loss 0.06023154780268669 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.8266],\n",
      "        [0.9019]], device='mps:0')\n",
      "Iteration 35640 Training loss 0.05705002695322037 Validation loss 0.06032543256878853 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.4313],\n",
      "        [0.8907]], device='mps:0')\n",
      "Iteration 35650 Training loss 0.0491066537797451 Validation loss 0.06022750213742256 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.0555],\n",
      "        [0.7828]], device='mps:0')\n",
      "Iteration 35660 Training loss 0.0574953630566597 Validation loss 0.060360826551914215 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.1092],\n",
      "        [0.9627]], device='mps:0')\n",
      "Iteration 35670 Training loss 0.06489069759845734 Validation loss 0.06025226041674614 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.5142],\n",
      "        [0.7160]], device='mps:0')\n",
      "Iteration 35680 Training loss 0.06368231773376465 Validation loss 0.060213830322027206 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.2928],\n",
      "        [0.8323]], device='mps:0')\n",
      "Iteration 35690 Training loss 0.0638856515288353 Validation loss 0.06029416248202324 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.0747],\n",
      "        [0.0262]], device='mps:0')\n",
      "Iteration 35700 Training loss 0.0522313117980957 Validation loss 0.06021760776638985 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.3560],\n",
      "        [0.7514]], device='mps:0')\n",
      "Iteration 35710 Training loss 0.05568834766745567 Validation loss 0.060256510972976685 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.1494],\n",
      "        [0.1690]], device='mps:0')\n",
      "Iteration 35720 Training loss 0.04621536657214165 Validation loss 0.06025470793247223 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.0820],\n",
      "        [0.1156]], device='mps:0')\n",
      "Iteration 35730 Training loss 0.052347294986248016 Validation loss 0.06035296991467476 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.9766],\n",
      "        [0.9103]], device='mps:0')\n",
      "Iteration 35740 Training loss 0.05554325133562088 Validation loss 0.060224633663892746 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.9828],\n",
      "        [0.6383]], device='mps:0')\n",
      "Iteration 35750 Training loss 0.059102535247802734 Validation loss 0.060223594307899475 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.7185],\n",
      "        [0.0346]], device='mps:0')\n",
      "Iteration 35760 Training loss 0.060979217290878296 Validation loss 0.06025277450680733 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.9433],\n",
      "        [0.8889]], device='mps:0')\n",
      "Iteration 35770 Training loss 0.05856594070792198 Validation loss 0.060262903571128845 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.1986],\n",
      "        [0.0142]], device='mps:0')\n",
      "Iteration 35780 Training loss 0.05539003387093544 Validation loss 0.06022446230053902 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.7758],\n",
      "        [0.9562]], device='mps:0')\n",
      "Iteration 35790 Training loss 0.045905254781246185 Validation loss 0.06022367253899574 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.9547],\n",
      "        [0.9406]], device='mps:0')\n",
      "Iteration 35800 Training loss 0.05313919112086296 Validation loss 0.060254231095314026 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.1709],\n",
      "        [0.1903]], device='mps:0')\n",
      "Iteration 35810 Training loss 0.05785690248012543 Validation loss 0.0602395161986351 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.4621],\n",
      "        [0.1455]], device='mps:0')\n",
      "Iteration 35820 Training loss 0.061390433460474014 Validation loss 0.06026013568043709 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.4178],\n",
      "        [0.1597]], device='mps:0')\n",
      "Iteration 35830 Training loss 0.05389474704861641 Validation loss 0.06023062765598297 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.0394],\n",
      "        [0.9025]], device='mps:0')\n",
      "Iteration 35840 Training loss 0.05579877272248268 Validation loss 0.060229308903217316 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.0868],\n",
      "        [0.9355]], device='mps:0')\n",
      "Iteration 35850 Training loss 0.054417483508586884 Validation loss 0.060237329453229904 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.3446],\n",
      "        [0.1324]], device='mps:0')\n",
      "Iteration 35860 Training loss 0.05521005392074585 Validation loss 0.06035112217068672 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.8488],\n",
      "        [0.1669]], device='mps:0')\n",
      "Iteration 35870 Training loss 0.05184273049235344 Validation loss 0.06023180112242699 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.0065],\n",
      "        [0.8176]], device='mps:0')\n",
      "Iteration 35880 Training loss 0.05716579779982567 Validation loss 0.06030137836933136 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.0825],\n",
      "        [0.3135]], device='mps:0')\n",
      "Iteration 35890 Training loss 0.05274590849876404 Validation loss 0.06022046133875847 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.8972],\n",
      "        [0.2127]], device='mps:0')\n",
      "Iteration 35900 Training loss 0.06087758019566536 Validation loss 0.060349855571985245 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.0400],\n",
      "        [0.9145]], device='mps:0')\n",
      "Iteration 35910 Training loss 0.05913121625781059 Validation loss 0.060524653643369675 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.2484],\n",
      "        [0.1131]], device='mps:0')\n",
      "Iteration 35920 Training loss 0.05881999060511589 Validation loss 0.06024731695652008 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.1910],\n",
      "        [0.9770]], device='mps:0')\n",
      "Iteration 35930 Training loss 0.062356993556022644 Validation loss 0.06021817401051521 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.6107],\n",
      "        [0.6733]], device='mps:0')\n",
      "Iteration 35940 Training loss 0.07278891652822495 Validation loss 0.06033735349774361 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.6191],\n",
      "        [0.9545]], device='mps:0')\n",
      "Iteration 35950 Training loss 0.052995920181274414 Validation loss 0.06025616079568863 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.6679],\n",
      "        [0.5138]], device='mps:0')\n",
      "Iteration 35960 Training loss 0.0642387643456459 Validation loss 0.06020814925432205 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.9319],\n",
      "        [0.8660]], device='mps:0')\n",
      "Iteration 35970 Training loss 0.05762260779738426 Validation loss 0.06020771339535713 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.8588],\n",
      "        [0.8469]], device='mps:0')\n",
      "Iteration 35980 Training loss 0.06756555289030075 Validation loss 0.060248591005802155 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.7958],\n",
      "        [0.3596]], device='mps:0')\n",
      "Iteration 35990 Training loss 0.061359893530607224 Validation loss 0.060207221657037735 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.4440],\n",
      "        [0.0676]], device='mps:0')\n",
      "Iteration 36000 Training loss 0.05934538692235947 Validation loss 0.060305673629045486 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.0905],\n",
      "        [0.0689]], device='mps:0')\n",
      "Iteration 36010 Training loss 0.07100453972816467 Validation loss 0.060202643275260925 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.7439],\n",
      "        [0.3085]], device='mps:0')\n",
      "Iteration 36020 Training loss 0.05388389527797699 Validation loss 0.0602065771818161 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.9730],\n",
      "        [0.4364]], device='mps:0')\n",
      "Iteration 36030 Training loss 0.05752008408308029 Validation loss 0.06066937372088432 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.2312],\n",
      "        [0.8268]], device='mps:0')\n",
      "Iteration 36040 Training loss 0.06205495819449425 Validation loss 0.060212619602680206 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.1941],\n",
      "        [0.0679]], device='mps:0')\n",
      "Iteration 36050 Training loss 0.05379257723689079 Validation loss 0.060205020010471344 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.8785],\n",
      "        [0.4089]], device='mps:0')\n",
      "Iteration 36060 Training loss 0.05664564669132233 Validation loss 0.06020352244377136 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.2585],\n",
      "        [0.7992]], device='mps:0')\n",
      "Iteration 36070 Training loss 0.06231877580285072 Validation loss 0.0602431446313858 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.9577],\n",
      "        [0.7153]], device='mps:0')\n",
      "Iteration 36080 Training loss 0.05687350034713745 Validation loss 0.06020168587565422 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.9526],\n",
      "        [0.0264]], device='mps:0')\n",
      "Iteration 36090 Training loss 0.06194310635328293 Validation loss 0.06025592237710953 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.8490],\n",
      "        [0.3380]], device='mps:0')\n",
      "Iteration 36100 Training loss 0.054988231509923935 Validation loss 0.06023123860359192 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.6255],\n",
      "        [0.9220]], device='mps:0')\n",
      "Iteration 36110 Training loss 0.06295750290155411 Validation loss 0.06034814938902855 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.9491],\n",
      "        [0.0882]], device='mps:0')\n",
      "Iteration 36120 Training loss 0.058261286467313766 Validation loss 0.060352224856615067 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.9364],\n",
      "        [0.2140]], device='mps:0')\n",
      "Iteration 36130 Training loss 0.062317464500665665 Validation loss 0.060218632221221924 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.3701],\n",
      "        [0.1107]], device='mps:0')\n",
      "Iteration 36140 Training loss 0.06562740355730057 Validation loss 0.060215823352336884 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.1766],\n",
      "        [0.0329]], device='mps:0')\n",
      "Iteration 36150 Training loss 0.05524902790784836 Validation loss 0.0602482333779335 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.2585],\n",
      "        [0.8460]], device='mps:0')\n",
      "Iteration 36160 Training loss 0.05682280287146568 Validation loss 0.06023388355970383 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.2896],\n",
      "        [0.1817]], device='mps:0')\n",
      "Iteration 36170 Training loss 0.05841263756155968 Validation loss 0.0602538026869297 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.8769],\n",
      "        [0.5845]], device='mps:0')\n",
      "Iteration 36180 Training loss 0.06592430174350739 Validation loss 0.06019293889403343 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.0813],\n",
      "        [0.3873]], device='mps:0')\n",
      "Iteration 36190 Training loss 0.060485560446977615 Validation loss 0.06018659844994545 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.1715],\n",
      "        [0.7555]], device='mps:0')\n",
      "Iteration 36200 Training loss 0.06048743054270744 Validation loss 0.06021948158740997 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.2727],\n",
      "        [0.4804]], device='mps:0')\n",
      "Iteration 36210 Training loss 0.056804358959198 Validation loss 0.060327790677547455 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.7422],\n",
      "        [0.7997]], device='mps:0')\n",
      "Iteration 36220 Training loss 0.05663127824664116 Validation loss 0.060192979872226715 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.9819],\n",
      "        [0.0749]], device='mps:0')\n",
      "Iteration 36230 Training loss 0.058750275522470474 Validation loss 0.060192011296749115 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.4575],\n",
      "        [0.0399]], device='mps:0')\n",
      "Iteration 36240 Training loss 0.06414467841386795 Validation loss 0.060330700129270554 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.0227],\n",
      "        [0.0158]], device='mps:0')\n",
      "Iteration 36250 Training loss 0.07442682981491089 Validation loss 0.06074773892760277 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.2916],\n",
      "        [0.6858]], device='mps:0')\n",
      "Iteration 36260 Training loss 0.0626022070646286 Validation loss 0.060221146792173386 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.0938],\n",
      "        [0.9972]], device='mps:0')\n",
      "Iteration 36270 Training loss 0.05893368646502495 Validation loss 0.06019109860062599 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.0177],\n",
      "        [0.0461]], device='mps:0')\n",
      "Iteration 36280 Training loss 0.05923423543572426 Validation loss 0.06019102782011032 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.7533],\n",
      "        [0.5918]], device='mps:0')\n",
      "Iteration 36290 Training loss 0.059415023773908615 Validation loss 0.06020667403936386 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.2144],\n",
      "        [0.8334]], device='mps:0')\n",
      "Iteration 36300 Training loss 0.05401588976383209 Validation loss 0.06029488146305084 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.9163],\n",
      "        [0.4419]], device='mps:0')\n",
      "Iteration 36310 Training loss 0.05964556708931923 Validation loss 0.06023882329463959 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.7064],\n",
      "        [0.2864]], device='mps:0')\n",
      "Iteration 36320 Training loss 0.05136071890592575 Validation loss 0.06035462021827698 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.3094],\n",
      "        [0.6467]], device='mps:0')\n",
      "Iteration 36330 Training loss 0.05640853941440582 Validation loss 0.060182176530361176 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.8729],\n",
      "        [0.1550]], device='mps:0')\n",
      "Iteration 36340 Training loss 0.060933180153369904 Validation loss 0.06039073318243027 Accuracy 0.8333750367164612\n",
      "Output tensor([[0.2928],\n",
      "        [0.9664]], device='mps:0')\n",
      "Iteration 36350 Training loss 0.06313882023096085 Validation loss 0.06017863005399704 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.4725],\n",
      "        [0.9575]], device='mps:0')\n",
      "Iteration 36360 Training loss 0.06614804267883301 Validation loss 0.060235727578401566 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.4856],\n",
      "        [0.9341]], device='mps:0')\n",
      "Iteration 36370 Training loss 0.057796552777290344 Validation loss 0.06021939218044281 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.9944],\n",
      "        [0.8841]], device='mps:0')\n",
      "Iteration 36380 Training loss 0.05981726571917534 Validation loss 0.060258980840444565 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.0174],\n",
      "        [0.0116]], device='mps:0')\n",
      "Iteration 36390 Training loss 0.051537949591875076 Validation loss 0.06018104404211044 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.9880],\n",
      "        [0.8943]], device='mps:0')\n",
      "Iteration 36400 Training loss 0.053004950284957886 Validation loss 0.06019348278641701 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.5052],\n",
      "        [0.3981]], device='mps:0')\n",
      "Iteration 36410 Training loss 0.05661835893988609 Validation loss 0.060201726853847504 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.7051],\n",
      "        [0.8800]], device='mps:0')\n",
      "Iteration 36420 Training loss 0.06246238201856613 Validation loss 0.06015804409980774 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.7964],\n",
      "        [0.5303]], device='mps:0')\n",
      "Iteration 36430 Training loss 0.06022645905613899 Validation loss 0.060247402638196945 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.9621],\n",
      "        [0.2344]], device='mps:0')\n",
      "Iteration 36440 Training loss 0.06175823137164116 Validation loss 0.060362309217453 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.7442],\n",
      "        [0.0513]], device='mps:0')\n",
      "Iteration 36450 Training loss 0.06043901666998863 Validation loss 0.060162682086229324 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.9956],\n",
      "        [0.9804]], device='mps:0')\n",
      "Iteration 36460 Training loss 0.06001516059041023 Validation loss 0.06019824370741844 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.7182],\n",
      "        [0.9134]], device='mps:0')\n",
      "Iteration 36470 Training loss 0.06054407358169556 Validation loss 0.060170892626047134 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.7117],\n",
      "        [0.9935]], device='mps:0')\n",
      "Iteration 36480 Training loss 0.05823512002825737 Validation loss 0.06026647612452507 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.2113],\n",
      "        [0.9243]], device='mps:0')\n",
      "Iteration 36490 Training loss 0.05922665819525719 Validation loss 0.060194142162799835 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.0570],\n",
      "        [0.9783]], device='mps:0')\n",
      "Iteration 36500 Training loss 0.0607764795422554 Validation loss 0.06014743819832802 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.4829],\n",
      "        [0.7206]], device='mps:0')\n",
      "Iteration 36510 Training loss 0.05968411639332771 Validation loss 0.06016470491886139 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.1997],\n",
      "        [0.8141]], device='mps:0')\n",
      "Iteration 36520 Training loss 0.061121974140405655 Validation loss 0.06021765246987343 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.4401],\n",
      "        [0.1119]], device='mps:0')\n",
      "Iteration 36530 Training loss 0.050458867102861404 Validation loss 0.06015723571181297 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.9741],\n",
      "        [0.2598]], device='mps:0')\n",
      "Iteration 36540 Training loss 0.0534508042037487 Validation loss 0.06015681102871895 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.1000],\n",
      "        [0.9562]], device='mps:0')\n",
      "Iteration 36550 Training loss 0.06622587889432907 Validation loss 0.06014696881175041 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.1279],\n",
      "        [0.9396]], device='mps:0')\n",
      "Iteration 36560 Training loss 0.05265561863780022 Validation loss 0.060425691306591034 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.2651],\n",
      "        [0.0366]], device='mps:0')\n",
      "Iteration 36570 Training loss 0.057279665023088455 Validation loss 0.06043297424912453 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.0535],\n",
      "        [0.6302]], device='mps:0')\n",
      "Iteration 36580 Training loss 0.061131078749895096 Validation loss 0.06026251241564751 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.4423],\n",
      "        [0.9614]], device='mps:0')\n",
      "Iteration 36590 Training loss 0.062308281660079956 Validation loss 0.06015697121620178 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.4474],\n",
      "        [0.3521]], device='mps:0')\n",
      "Iteration 36600 Training loss 0.048745982348918915 Validation loss 0.060148756951093674 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.8263],\n",
      "        [0.1410]], device='mps:0')\n",
      "Iteration 36610 Training loss 0.061118386685848236 Validation loss 0.060304053127765656 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.0499],\n",
      "        [0.9578]], device='mps:0')\n",
      "Iteration 36620 Training loss 0.05518120154738426 Validation loss 0.060181450098752975 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.6474],\n",
      "        [0.6211]], device='mps:0')\n",
      "Iteration 36630 Training loss 0.05783197283744812 Validation loss 0.0601610392332077 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.8835],\n",
      "        [0.9796]], device='mps:0')\n",
      "Iteration 36640 Training loss 0.05117931216955185 Validation loss 0.06015275418758392 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.9860],\n",
      "        [0.4953]], device='mps:0')\n",
      "Iteration 36650 Training loss 0.05522768199443817 Validation loss 0.060173798352479935 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.2539],\n",
      "        [0.9490]], device='mps:0')\n",
      "Iteration 36660 Training loss 0.06310930848121643 Validation loss 0.06014409288764 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.8792],\n",
      "        [0.3511]], device='mps:0')\n",
      "Iteration 36670 Training loss 0.05516980215907097 Validation loss 0.06014290452003479 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.0165],\n",
      "        [0.1436]], device='mps:0')\n",
      "Iteration 36680 Training loss 0.05625724792480469 Validation loss 0.0601533018052578 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.5309],\n",
      "        [0.0812]], device='mps:0')\n",
      "Iteration 36690 Training loss 0.062119051814079285 Validation loss 0.060173600912094116 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.9869],\n",
      "        [0.7426]], device='mps:0')\n",
      "Iteration 36700 Training loss 0.06042793020606041 Validation loss 0.06014345586299896 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.9525],\n",
      "        [0.8912]], device='mps:0')\n",
      "Iteration 36710 Training loss 0.056317269802093506 Validation loss 0.060268886387348175 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.9385],\n",
      "        [0.9454]], device='mps:0')\n",
      "Iteration 36720 Training loss 0.05638090521097183 Validation loss 0.060171373188495636 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.0996],\n",
      "        [0.4137]], device='mps:0')\n",
      "Iteration 36730 Training loss 0.05574292689561844 Validation loss 0.060141414403915405 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.7578],\n",
      "        [0.2787]], device='mps:0')\n",
      "Iteration 36740 Training loss 0.05950528010725975 Validation loss 0.06014595925807953 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.0565],\n",
      "        [0.8100]], device='mps:0')\n",
      "Iteration 36750 Training loss 0.059128761291503906 Validation loss 0.060139790177345276 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.9402],\n",
      "        [0.0526]], device='mps:0')\n",
      "Iteration 36760 Training loss 0.053152576088905334 Validation loss 0.06025172770023346 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.6092],\n",
      "        [0.1163]], device='mps:0')\n",
      "Iteration 36770 Training loss 0.053723011165857315 Validation loss 0.06032818183302879 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.3646],\n",
      "        [0.6865]], device='mps:0')\n",
      "Iteration 36780 Training loss 0.057297296822071075 Validation loss 0.060535091906785965 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.0759],\n",
      "        [0.9684]], device='mps:0')\n",
      "Iteration 36790 Training loss 0.06206882372498512 Validation loss 0.060517191886901855 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.8288],\n",
      "        [0.0213]], device='mps:0')\n",
      "Iteration 36800 Training loss 0.06140117347240448 Validation loss 0.06017373502254486 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.0295],\n",
      "        [0.1231]], device='mps:0')\n",
      "Iteration 36810 Training loss 0.05137840285897255 Validation loss 0.06017538905143738 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.2767],\n",
      "        [0.6608]], device='mps:0')\n",
      "Iteration 36820 Training loss 0.06310625374317169 Validation loss 0.06014479324221611 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.8623],\n",
      "        [0.4286]], device='mps:0')\n",
      "Iteration 36830 Training loss 0.06322772055864334 Validation loss 0.06018809974193573 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.0812],\n",
      "        [0.4683]], device='mps:0')\n",
      "Iteration 36840 Training loss 0.06455930322408676 Validation loss 0.06016203388571739 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.4348],\n",
      "        [0.7948]], device='mps:0')\n",
      "Iteration 36850 Training loss 0.05637558922171593 Validation loss 0.06042339652776718 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.6979],\n",
      "        [0.9624]], device='mps:0')\n",
      "Iteration 36860 Training loss 0.05829401686787605 Validation loss 0.0605972521007061 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.3323],\n",
      "        [0.2743]], device='mps:0')\n",
      "Iteration 36870 Training loss 0.05109041929244995 Validation loss 0.060190387070178986 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.0402],\n",
      "        [0.1136]], device='mps:0')\n",
      "Iteration 36880 Training loss 0.050965871661901474 Validation loss 0.06012938916683197 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.0648],\n",
      "        [0.8842]], device='mps:0')\n",
      "Iteration 36890 Training loss 0.06096862256526947 Validation loss 0.0602487176656723 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.9415],\n",
      "        [0.9324]], device='mps:0')\n",
      "Iteration 36900 Training loss 0.0641385167837143 Validation loss 0.060488346964120865 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.2203],\n",
      "        [0.9374]], device='mps:0')\n",
      "Iteration 36910 Training loss 0.051760490983724594 Validation loss 0.06015326455235481 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.4103],\n",
      "        [0.5844]], device='mps:0')\n",
      "Iteration 36920 Training loss 0.0670018121600151 Validation loss 0.06034691259264946 Accuracy 0.8330000638961792\n",
      "Output tensor([[0.5368],\n",
      "        [0.0289]], device='mps:0')\n",
      "Iteration 36930 Training loss 0.06549830734729767 Validation loss 0.060125529766082764 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.9686],\n",
      "        [0.0822]], device='mps:0')\n",
      "Iteration 36940 Training loss 0.06512248516082764 Validation loss 0.060237497091293335 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.5962],\n",
      "        [0.1276]], device='mps:0')\n",
      "Iteration 36950 Training loss 0.060108937323093414 Validation loss 0.06011101230978966 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.2998],\n",
      "        [0.1431]], device='mps:0')\n",
      "Iteration 36960 Training loss 0.061029937118291855 Validation loss 0.06035942956805229 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.5831],\n",
      "        [0.9368]], device='mps:0')\n",
      "Iteration 36970 Training loss 0.06066884472966194 Validation loss 0.06010599061846733 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.6683],\n",
      "        [0.8324]], device='mps:0')\n",
      "Iteration 36980 Training loss 0.05122982710599899 Validation loss 0.060320641845464706 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.9830],\n",
      "        [0.0520]], device='mps:0')\n",
      "Iteration 36990 Training loss 0.05251830071210861 Validation loss 0.060115016996860504 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.1972],\n",
      "        [0.8296]], device='mps:0')\n",
      "Iteration 37000 Training loss 0.061993129551410675 Validation loss 0.060238465666770935 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.3819],\n",
      "        [0.2867]], device='mps:0')\n",
      "Iteration 37010 Training loss 0.055869538336992264 Validation loss 0.060105156153440475 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.4018],\n",
      "        [0.0361]], device='mps:0')\n",
      "Iteration 37020 Training loss 0.058154504746198654 Validation loss 0.06010206788778305 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.1757],\n",
      "        [0.6055]], device='mps:0')\n",
      "Iteration 37030 Training loss 0.05719629302620888 Validation loss 0.060305770486593246 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.1662],\n",
      "        [0.9510]], device='mps:0')\n",
      "Iteration 37040 Training loss 0.05289258062839508 Validation loss 0.060146085917949677 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.8690],\n",
      "        [0.3309]], device='mps:0')\n",
      "Iteration 37050 Training loss 0.06026076152920723 Validation loss 0.06015804409980774 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.0619],\n",
      "        [0.1443]], device='mps:0')\n",
      "Iteration 37060 Training loss 0.060854509472846985 Validation loss 0.06010328233242035 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.5251],\n",
      "        [0.8902]], device='mps:0')\n",
      "Iteration 37070 Training loss 0.057612426578998566 Validation loss 0.060118529945611954 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.9840],\n",
      "        [0.9494]], device='mps:0')\n",
      "Iteration 37080 Training loss 0.059388741850852966 Validation loss 0.06017047539353371 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.4098],\n",
      "        [0.0369]], device='mps:0')\n",
      "Iteration 37090 Training loss 0.059345100075006485 Validation loss 0.060104794800281525 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.9696],\n",
      "        [0.5579]], device='mps:0')\n",
      "Iteration 37100 Training loss 0.06079643592238426 Validation loss 0.060117755085229874 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.1769],\n",
      "        [0.6745]], device='mps:0')\n",
      "Iteration 37110 Training loss 0.05404861271381378 Validation loss 0.06015199422836304 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.8016],\n",
      "        [0.1786]], device='mps:0')\n",
      "Iteration 37120 Training loss 0.0595371313393116 Validation loss 0.06013520434498787 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.0628],\n",
      "        [0.6976]], device='mps:0')\n",
      "Iteration 37130 Training loss 0.0602552630007267 Validation loss 0.060243938118219376 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.0076],\n",
      "        [0.6405]], device='mps:0')\n",
      "Iteration 37140 Training loss 0.051943082362413406 Validation loss 0.06008916720747948 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.5276],\n",
      "        [0.0232]], device='mps:0')\n",
      "Iteration 37150 Training loss 0.05446904897689819 Validation loss 0.060086436569690704 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.1678],\n",
      "        [0.2335]], device='mps:0')\n",
      "Iteration 37160 Training loss 0.0575571246445179 Validation loss 0.06013776361942291 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.9816],\n",
      "        [0.9756]], device='mps:0')\n",
      "Iteration 37170 Training loss 0.05918063968420029 Validation loss 0.060088176280260086 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.7277],\n",
      "        [0.6876]], device='mps:0')\n",
      "Iteration 37180 Training loss 0.06308639049530029 Validation loss 0.06015147641301155 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.3853],\n",
      "        [0.7808]], device='mps:0')\n",
      "Iteration 37190 Training loss 0.05712917819619179 Validation loss 0.060272835195064545 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.7961],\n",
      "        [0.1079]], device='mps:0')\n",
      "Iteration 37200 Training loss 0.06348469853401184 Validation loss 0.060097094625234604 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.2967],\n",
      "        [0.0354]], device='mps:0')\n",
      "Iteration 37210 Training loss 0.057557541877031326 Validation loss 0.06021883711218834 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.9191],\n",
      "        [0.6873]], device='mps:0')\n",
      "Iteration 37220 Training loss 0.06160411611199379 Validation loss 0.060191936790943146 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.5972],\n",
      "        [0.9931]], device='mps:0')\n",
      "Iteration 37230 Training loss 0.053143929690122604 Validation loss 0.06008905917406082 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.0101],\n",
      "        [0.9927]], device='mps:0')\n",
      "Iteration 37240 Training loss 0.05903763696551323 Validation loss 0.060092560946941376 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.0711],\n",
      "        [0.9806]], device='mps:0')\n",
      "Iteration 37250 Training loss 0.05876138433814049 Validation loss 0.06011321768164635 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.1823],\n",
      "        [0.2242]], device='mps:0')\n",
      "Iteration 37260 Training loss 0.06014000624418259 Validation loss 0.06011655554175377 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.0564],\n",
      "        [0.2028]], device='mps:0')\n",
      "Iteration 37270 Training loss 0.06005748733878136 Validation loss 0.06012564152479172 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.2021],\n",
      "        [0.0567]], device='mps:0')\n",
      "Iteration 37280 Training loss 0.06150493025779724 Validation loss 0.060080669820308685 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.9258],\n",
      "        [0.1878]], device='mps:0')\n",
      "Iteration 37290 Training loss 0.050305817276239395 Validation loss 0.06011161580681801 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.8734],\n",
      "        [0.4911]], device='mps:0')\n",
      "Iteration 37300 Training loss 0.05862180143594742 Validation loss 0.060131072998046875 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.0991],\n",
      "        [0.6338]], device='mps:0')\n",
      "Iteration 37310 Training loss 0.052457138895988464 Validation loss 0.06018557399511337 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.4008],\n",
      "        [0.9249]], device='mps:0')\n",
      "Iteration 37320 Training loss 0.05695230886340141 Validation loss 0.06027781218290329 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.2939],\n",
      "        [0.1322]], device='mps:0')\n",
      "Iteration 37330 Training loss 0.059058062732219696 Validation loss 0.06007221341133118 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.6811],\n",
      "        [0.1195]], device='mps:0')\n",
      "Iteration 37340 Training loss 0.047987572848796844 Validation loss 0.06008879095315933 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.4142],\n",
      "        [0.8824]], device='mps:0')\n",
      "Iteration 37350 Training loss 0.06261884421110153 Validation loss 0.06014498323202133 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.9362],\n",
      "        [0.9644]], device='mps:0')\n",
      "Iteration 37360 Training loss 0.06339322775602341 Validation loss 0.06008056551218033 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.0409],\n",
      "        [0.8773]], device='mps:0')\n",
      "Iteration 37370 Training loss 0.05862922593951225 Validation loss 0.060165420174598694 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.2174],\n",
      "        [0.0633]], device='mps:0')\n",
      "Iteration 37380 Training loss 0.05719472095370293 Validation loss 0.06008268520236015 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.3067],\n",
      "        [0.2681]], device='mps:0')\n",
      "Iteration 37390 Training loss 0.06384248286485672 Validation loss 0.06015400215983391 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.5949],\n",
      "        [0.3798]], device='mps:0')\n",
      "Iteration 37400 Training loss 0.055613476783037186 Validation loss 0.060125865042209625 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.9463],\n",
      "        [0.1424]], device='mps:0')\n",
      "Iteration 37410 Training loss 0.05809919536113739 Validation loss 0.06007673218846321 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.6495],\n",
      "        [0.8232]], device='mps:0')\n",
      "Iteration 37420 Training loss 0.0567324198782444 Validation loss 0.06011078879237175 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.9993],\n",
      "        [0.0876]], device='mps:0')\n",
      "Iteration 37430 Training loss 0.07134764641523361 Validation loss 0.06025328114628792 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.9635],\n",
      "        [0.1902]], device='mps:0')\n",
      "Iteration 37440 Training loss 0.06392751634120941 Validation loss 0.06023365259170532 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.6483],\n",
      "        [0.7958]], device='mps:0')\n",
      "Iteration 37450 Training loss 0.05686710402369499 Validation loss 0.0601082518696785 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.8252],\n",
      "        [0.8838]], device='mps:0')\n",
      "Iteration 37460 Training loss 0.05657525733113289 Validation loss 0.06007909029722214 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.0253],\n",
      "        [0.7176]], device='mps:0')\n",
      "Iteration 37470 Training loss 0.06151594966650009 Validation loss 0.06011098995804787 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.9547],\n",
      "        [0.9850]], device='mps:0')\n",
      "Iteration 37480 Training loss 0.06516071408987045 Validation loss 0.060100093483924866 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.4561],\n",
      "        [0.9016]], device='mps:0')\n",
      "Iteration 37490 Training loss 0.05335525423288345 Validation loss 0.06015821546316147 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.3430],\n",
      "        [0.4792]], device='mps:0')\n",
      "Iteration 37500 Training loss 0.06841298937797546 Validation loss 0.06012067571282387 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.0501],\n",
      "        [0.2661]], device='mps:0')\n",
      "Iteration 37510 Training loss 0.0649508684873581 Validation loss 0.06005903705954552 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.9586],\n",
      "        [0.1745]], device='mps:0')\n",
      "Iteration 37520 Training loss 0.057724542915821075 Validation loss 0.06024102121591568 Accuracy 0.8332500457763672\n",
      "Output tensor([[0.8700],\n",
      "        [0.0756]], device='mps:0')\n",
      "Iteration 37530 Training loss 0.06558722257614136 Validation loss 0.06015832722187042 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.1482],\n",
      "        [0.1472]], device='mps:0')\n",
      "Iteration 37540 Training loss 0.06404560804367065 Validation loss 0.06008953973650932 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.0151],\n",
      "        [0.2267]], device='mps:0')\n",
      "Iteration 37550 Training loss 0.052812740206718445 Validation loss 0.06005965545773506 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.6681],\n",
      "        [0.8171]], device='mps:0')\n",
      "Iteration 37560 Training loss 0.05307924002408981 Validation loss 0.06021711975336075 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.5271],\n",
      "        [0.4289]], device='mps:0')\n",
      "Iteration 37570 Training loss 0.05300614610314369 Validation loss 0.0601004920899868 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.1010],\n",
      "        [0.3871]], device='mps:0')\n",
      "Iteration 37580 Training loss 0.040869053453207016 Validation loss 0.06005878001451492 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.9125],\n",
      "        [0.9946]], device='mps:0')\n",
      "Iteration 37590 Training loss 0.06273893266916275 Validation loss 0.06005425006151199 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.5300],\n",
      "        [0.9531]], device='mps:0')\n",
      "Iteration 37600 Training loss 0.05679008364677429 Validation loss 0.06036091223359108 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.8409],\n",
      "        [0.0709]], device='mps:0')\n",
      "Iteration 37610 Training loss 0.06263738125562668 Validation loss 0.060045450925827026 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.2748],\n",
      "        [0.1648]], device='mps:0')\n",
      "Iteration 37620 Training loss 0.060757339000701904 Validation loss 0.060043636709451675 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.6332],\n",
      "        [0.8432]], device='mps:0')\n",
      "Iteration 37630 Training loss 0.06440569460391998 Validation loss 0.06003831699490547 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.6958],\n",
      "        [0.0964]], device='mps:0')\n",
      "Iteration 37640 Training loss 0.052975501865148544 Validation loss 0.060055527836084366 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.8879],\n",
      "        [0.5169]], device='mps:0')\n",
      "Iteration 37650 Training loss 0.05536149442195892 Validation loss 0.060046348720788956 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.1556],\n",
      "        [0.3475]], device='mps:0')\n",
      "Iteration 37660 Training loss 0.05664805322885513 Validation loss 0.06003735959529877 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.9816],\n",
      "        [0.0443]], device='mps:0')\n",
      "Iteration 37670 Training loss 0.05196256563067436 Validation loss 0.06003667041659355 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.0089],\n",
      "        [0.3936]], device='mps:0')\n",
      "Iteration 37680 Training loss 0.0574447326362133 Validation loss 0.06004618480801582 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.0393],\n",
      "        [0.8714]], device='mps:0')\n",
      "Iteration 37690 Training loss 0.05479193478822708 Validation loss 0.060167718678712845 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.0506],\n",
      "        [0.3841]], device='mps:0')\n",
      "Iteration 37700 Training loss 0.06095951795578003 Validation loss 0.06004408001899719 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.2149],\n",
      "        [0.0558]], device='mps:0')\n",
      "Iteration 37710 Training loss 0.04702548310160637 Validation loss 0.06014987453818321 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.1192],\n",
      "        [0.9968]], device='mps:0')\n",
      "Iteration 37720 Training loss 0.05728636682033539 Validation loss 0.06004524976015091 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.9621],\n",
      "        [0.8360]], device='mps:0')\n",
      "Iteration 37730 Training loss 0.052706144750118256 Validation loss 0.06004936993122101 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.9389],\n",
      "        [0.1314]], device='mps:0')\n",
      "Iteration 37740 Training loss 0.057236287742853165 Validation loss 0.060066044330596924 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.3636],\n",
      "        [0.1826]], device='mps:0')\n",
      "Iteration 37750 Training loss 0.054584283381700516 Validation loss 0.060067761689424515 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.7659],\n",
      "        [0.0773]], device='mps:0')\n",
      "Iteration 37760 Training loss 0.0567985437810421 Validation loss 0.060086850076913834 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.4956],\n",
      "        [0.6817]], device='mps:0')\n",
      "Iteration 37770 Training loss 0.05541367828845978 Validation loss 0.06007547676563263 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.3040],\n",
      "        [0.8906]], device='mps:0')\n",
      "Iteration 37780 Training loss 0.053472086787223816 Validation loss 0.06009430065751076 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.1445],\n",
      "        [0.9611]], device='mps:0')\n",
      "Iteration 37790 Training loss 0.05446482449769974 Validation loss 0.06004863977432251 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.0320],\n",
      "        [0.8340]], device='mps:0')\n",
      "Iteration 37800 Training loss 0.05141661688685417 Validation loss 0.060124654322862625 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.4508],\n",
      "        [0.4360]], device='mps:0')\n",
      "Iteration 37810 Training loss 0.057867687195539474 Validation loss 0.06015389785170555 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.3653],\n",
      "        [0.7731]], device='mps:0')\n",
      "Iteration 37820 Training loss 0.060227248817682266 Validation loss 0.060194406658411026 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.9939],\n",
      "        [0.7343]], device='mps:0')\n",
      "Iteration 37830 Training loss 0.05999024212360382 Validation loss 0.06004073843359947 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.0738],\n",
      "        [0.9158]], device='mps:0')\n",
      "Iteration 37840 Training loss 0.0630650669336319 Validation loss 0.060071758925914764 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.5849],\n",
      "        [0.0524]], device='mps:0')\n",
      "Iteration 37850 Training loss 0.0573970302939415 Validation loss 0.060192499309778214 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.8050],\n",
      "        [0.5318]], device='mps:0')\n",
      "Iteration 37860 Training loss 0.060683611780405045 Validation loss 0.06002873554825783 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.0666],\n",
      "        [0.3220]], device='mps:0')\n",
      "Iteration 37870 Training loss 0.05773501098155975 Validation loss 0.0602387897670269 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.1253],\n",
      "        [0.7104]], device='mps:0')\n",
      "Iteration 37880 Training loss 0.06465693563222885 Validation loss 0.060107480734586716 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.0307],\n",
      "        [0.2602]], device='mps:0')\n",
      "Iteration 37890 Training loss 0.050733909010887146 Validation loss 0.06002214923501015 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.7485],\n",
      "        [0.0738]], device='mps:0')\n",
      "Iteration 37900 Training loss 0.05793268233537674 Validation loss 0.06055846065282822 Accuracy 0.8331250548362732\n",
      "Output tensor([[0.2252],\n",
      "        [0.8620]], device='mps:0')\n",
      "Iteration 37910 Training loss 0.05287613719701767 Validation loss 0.06002360209822655 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.0196],\n",
      "        [0.0977]], device='mps:0')\n",
      "Iteration 37920 Training loss 0.05857214331626892 Validation loss 0.06003332510590553 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.1130],\n",
      "        [0.9076]], device='mps:0')\n",
      "Iteration 37930 Training loss 0.06592513620853424 Validation loss 0.06001891568303108 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.6436],\n",
      "        [0.9971]], device='mps:0')\n",
      "Iteration 37940 Training loss 0.05199502035975456 Validation loss 0.0600426159799099 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.2998],\n",
      "        [0.4468]], device='mps:0')\n",
      "Iteration 37950 Training loss 0.05805959179997444 Validation loss 0.060144081711769104 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.0842],\n",
      "        [0.7662]], device='mps:0')\n",
      "Iteration 37960 Training loss 0.06012884899973869 Validation loss 0.06000715494155884 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.9853],\n",
      "        [0.0875]], device='mps:0')\n",
      "Iteration 37970 Training loss 0.05523943901062012 Validation loss 0.06019493192434311 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.7337],\n",
      "        [0.2773]], device='mps:0')\n",
      "Iteration 37980 Training loss 0.067528635263443 Validation loss 0.06001746654510498 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.4013],\n",
      "        [0.5474]], device='mps:0')\n",
      "Iteration 37990 Training loss 0.054678745567798615 Validation loss 0.06034330651164055 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.9242],\n",
      "        [0.6767]], device='mps:0')\n",
      "Iteration 38000 Training loss 0.05476435646414757 Validation loss 0.060124922543764114 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.2401],\n",
      "        [0.9323]], device='mps:0')\n",
      "Iteration 38010 Training loss 0.054844848811626434 Validation loss 0.0600123256444931 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.2397],\n",
      "        [0.9940]], device='mps:0')\n",
      "Iteration 38020 Training loss 0.05887005850672722 Validation loss 0.06001552194356918 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.3294],\n",
      "        [0.2330]], device='mps:0')\n",
      "Iteration 38030 Training loss 0.06266970187425613 Validation loss 0.06004438176751137 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.9314],\n",
      "        [0.7131]], device='mps:0')\n",
      "Iteration 38040 Training loss 0.057032715529203415 Validation loss 0.06001768261194229 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.5909],\n",
      "        [0.1932]], device='mps:0')\n",
      "Iteration 38050 Training loss 0.059871282428503036 Validation loss 0.06000451743602753 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.3858],\n",
      "        [0.8937]], device='mps:0')\n",
      "Iteration 38060 Training loss 0.0661156103014946 Validation loss 0.0600605234503746 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.9799],\n",
      "        [0.9640]], device='mps:0')\n",
      "Iteration 38070 Training loss 0.059765156358480453 Validation loss 0.060032740235328674 Accuracy 0.8342500329017639\n",
      "Output tensor([[0.7643],\n",
      "        [0.0711]], device='mps:0')\n",
      "Iteration 38080 Training loss 0.058904845267534256 Validation loss 0.06001538038253784 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.8196],\n",
      "        [0.7881]], device='mps:0')\n",
      "Iteration 38090 Training loss 0.05924279987812042 Validation loss 0.06014883890748024 Accuracy 0.8340000510215759\n",
      "Output tensor([[0.0613],\n",
      "        [0.1645]], device='mps:0')\n",
      "Iteration 38100 Training loss 0.04895719885826111 Validation loss 0.059987880289554596 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.3885],\n",
      "        [0.1812]], device='mps:0')\n",
      "Iteration 38110 Training loss 0.05743793770670891 Validation loss 0.06000847369432449 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.1564],\n",
      "        [0.9633]], device='mps:0')\n",
      "Iteration 38120 Training loss 0.057779014110565186 Validation loss 0.06002407893538475 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.1782],\n",
      "        [0.4971]], device='mps:0')\n",
      "Iteration 38130 Training loss 0.05369251221418381 Validation loss 0.06002264842391014 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.9944],\n",
      "        [0.6322]], device='mps:0')\n",
      "Iteration 38140 Training loss 0.057859115302562714 Validation loss 0.06014835461974144 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.2814],\n",
      "        [0.8703]], device='mps:0')\n",
      "Iteration 38150 Training loss 0.05715060606598854 Validation loss 0.059987060725688934 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.6717],\n",
      "        [0.9590]], device='mps:0')\n",
      "Iteration 38160 Training loss 0.051567915827035904 Validation loss 0.06004215404391289 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.9825],\n",
      "        [0.1099]], device='mps:0')\n",
      "Iteration 38170 Training loss 0.06460218876600266 Validation loss 0.06004095450043678 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.6428],\n",
      "        [0.9614]], device='mps:0')\n",
      "Iteration 38180 Training loss 0.05626390874385834 Validation loss 0.05998186767101288 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.1461],\n",
      "        [0.5793]], device='mps:0')\n",
      "Iteration 38190 Training loss 0.05696709454059601 Validation loss 0.06001066043972969 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.0721],\n",
      "        [0.9534]], device='mps:0')\n",
      "Iteration 38200 Training loss 0.05471491813659668 Validation loss 0.06044546514749527 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.4752],\n",
      "        [0.1334]], device='mps:0')\n",
      "Iteration 38210 Training loss 0.05576504021883011 Validation loss 0.060134340077638626 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.9093],\n",
      "        [0.9638]], device='mps:0')\n",
      "Iteration 38220 Training loss 0.0544724278151989 Validation loss 0.06006932258605957 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.7108],\n",
      "        [0.9458]], device='mps:0')\n",
      "Iteration 38230 Training loss 0.056042421609163284 Validation loss 0.060105517506599426 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.7314],\n",
      "        [0.1633]], device='mps:0')\n",
      "Iteration 38240 Training loss 0.05744294822216034 Validation loss 0.05998280644416809 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.3708],\n",
      "        [0.3732]], device='mps:0')\n",
      "Iteration 38250 Training loss 0.05932551249861717 Validation loss 0.05998783931136131 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.2350],\n",
      "        [0.0522]], device='mps:0')\n",
      "Iteration 38260 Training loss 0.05666947364807129 Validation loss 0.05999347195029259 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.2053],\n",
      "        [0.7965]], device='mps:0')\n",
      "Iteration 38270 Training loss 0.06037501245737076 Validation loss 0.05998009815812111 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.0595],\n",
      "        [0.9010]], device='mps:0')\n",
      "Iteration 38280 Training loss 0.06100308150053024 Validation loss 0.0601106733083725 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.4344],\n",
      "        [0.0324]], device='mps:0')\n",
      "Iteration 38290 Training loss 0.05054309591650963 Validation loss 0.060048907995224 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.2304],\n",
      "        [0.0439]], device='mps:0')\n",
      "Iteration 38300 Training loss 0.05595856532454491 Validation loss 0.06002941355109215 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.9552],\n",
      "        [0.3400]], device='mps:0')\n",
      "Iteration 38310 Training loss 0.05512792617082596 Validation loss 0.05997198447585106 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.0688],\n",
      "        [0.0822]], device='mps:0')\n",
      "Iteration 38320 Training loss 0.057024162262678146 Validation loss 0.06001592054963112 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.0434],\n",
      "        [0.9610]], device='mps:0')\n",
      "Iteration 38330 Training loss 0.06238332763314247 Validation loss 0.06002866476774216 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.3855],\n",
      "        [0.2035]], device='mps:0')\n",
      "Iteration 38340 Training loss 0.057785358279943466 Validation loss 0.05998458340764046 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.9747],\n",
      "        [0.1281]], device='mps:0')\n",
      "Iteration 38350 Training loss 0.049281056970357895 Validation loss 0.059967465698719025 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.7780],\n",
      "        [0.9115]], device='mps:0')\n",
      "Iteration 38360 Training loss 0.061223696917295456 Validation loss 0.05999823287129402 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.8250],\n",
      "        [0.7627]], device='mps:0')\n",
      "Iteration 38370 Training loss 0.06712328642606735 Validation loss 0.06001461669802666 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.4926],\n",
      "        [0.9564]], device='mps:0')\n",
      "Iteration 38380 Training loss 0.05214240029454231 Validation loss 0.05999384820461273 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.0950],\n",
      "        [0.4148]], device='mps:0')\n",
      "Iteration 38390 Training loss 0.06675495207309723 Validation loss 0.060671474784612656 Accuracy 0.8337500691413879\n",
      "Output tensor([[0.9698],\n",
      "        [0.9308]], device='mps:0')\n",
      "Iteration 38400 Training loss 0.05791669711470604 Validation loss 0.06002279371023178 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.9539],\n",
      "        [0.9113]], device='mps:0')\n",
      "Iteration 38410 Training loss 0.051354531198740005 Validation loss 0.0599985234439373 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.2023],\n",
      "        [0.8430]], device='mps:0')\n",
      "Iteration 38420 Training loss 0.05309191718697548 Validation loss 0.06004362925887108 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.8126],\n",
      "        [0.8350]], device='mps:0')\n",
      "Iteration 38430 Training loss 0.0601930133998394 Validation loss 0.06004286929965019 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.8565],\n",
      "        [0.9378]], device='mps:0')\n",
      "Iteration 38440 Training loss 0.05966375395655632 Validation loss 0.060028452426195145 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.3518],\n",
      "        [0.9023]], device='mps:0')\n",
      "Iteration 38450 Training loss 0.05089300870895386 Validation loss 0.05996645987033844 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.0037],\n",
      "        [0.9880]], device='mps:0')\n",
      "Iteration 38460 Training loss 0.05727675184607506 Validation loss 0.05996561795473099 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.7357],\n",
      "        [0.8107]], device='mps:0')\n",
      "Iteration 38470 Training loss 0.06296559423208237 Validation loss 0.060482051223516464 Accuracy 0.8343750238418579\n",
      "Output tensor([[0.6644],\n",
      "        [0.8680]], device='mps:0')\n",
      "Iteration 38480 Training loss 0.051274195313453674 Validation loss 0.060099393129348755 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.4924],\n",
      "        [0.9548]], device='mps:0')\n",
      "Iteration 38490 Training loss 0.05554649233818054 Validation loss 0.0599525012075901 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.8173],\n",
      "        [0.7824]], device='mps:0')\n",
      "Iteration 38500 Training loss 0.06177062913775444 Validation loss 0.060121163725852966 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.0195],\n",
      "        [0.1399]], device='mps:0')\n",
      "Iteration 38510 Training loss 0.04853539541363716 Validation loss 0.05998514965176582 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.9936],\n",
      "        [0.9107]], device='mps:0')\n",
      "Iteration 38520 Training loss 0.06220788508653641 Validation loss 0.0600474514067173 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.9676],\n",
      "        [0.7878]], device='mps:0')\n",
      "Iteration 38530 Training loss 0.060297850519418716 Validation loss 0.06010764464735985 Accuracy 0.8335000276565552\n",
      "Output tensor([[0.0205],\n",
      "        [0.3744]], device='mps:0')\n",
      "Iteration 38540 Training loss 0.0553307943046093 Validation loss 0.05995875969529152 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.9780],\n",
      "        [0.7888]], device='mps:0')\n",
      "Iteration 38550 Training loss 0.056936901062726974 Validation loss 0.05996081978082657 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.2111],\n",
      "        [0.8944]], device='mps:0')\n",
      "Iteration 38560 Training loss 0.05092623457312584 Validation loss 0.05995509773492813 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.8290],\n",
      "        [0.7820]], device='mps:0')\n",
      "Iteration 38570 Training loss 0.053094282746315 Validation loss 0.05998595431447029 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.9596],\n",
      "        [0.3255]], device='mps:0')\n",
      "Iteration 38580 Training loss 0.06118357181549072 Validation loss 0.06001634895801544 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.0866],\n",
      "        [0.5080]], device='mps:0')\n",
      "Iteration 38590 Training loss 0.04998813197016716 Validation loss 0.06032924726605415 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.0193],\n",
      "        [0.1651]], device='mps:0')\n",
      "Iteration 38600 Training loss 0.054636143147945404 Validation loss 0.06015310063958168 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.6385],\n",
      "        [0.9288]], device='mps:0')\n",
      "Iteration 38610 Training loss 0.058557793498039246 Validation loss 0.059983279556035995 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.8461],\n",
      "        [0.6445]], device='mps:0')\n",
      "Iteration 38620 Training loss 0.05468234792351723 Validation loss 0.059944845736026764 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.8627],\n",
      "        [0.0384]], device='mps:0')\n",
      "Iteration 38630 Training loss 0.056690607219934464 Validation loss 0.06000722572207451 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.5429],\n",
      "        [0.4271]], device='mps:0')\n",
      "Iteration 38640 Training loss 0.054008424282073975 Validation loss 0.05994024500250816 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.1193],\n",
      "        [0.4405]], device='mps:0')\n",
      "Iteration 38650 Training loss 0.051091521978378296 Validation loss 0.060169756412506104 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.9040],\n",
      "        [0.6580]], device='mps:0')\n",
      "Iteration 38660 Training loss 0.05379045382142067 Validation loss 0.05994678661227226 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.1070],\n",
      "        [0.6795]], device='mps:0')\n",
      "Iteration 38670 Training loss 0.06017453595995903 Validation loss 0.059986844658851624 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.8512],\n",
      "        [0.2390]], device='mps:0')\n",
      "Iteration 38680 Training loss 0.05239472910761833 Validation loss 0.05992564186453819 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.9926],\n",
      "        [0.3062]], device='mps:0')\n",
      "Iteration 38690 Training loss 0.06382786482572556 Validation loss 0.06003497540950775 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.0420],\n",
      "        [0.8525]], device='mps:0')\n",
      "Iteration 38700 Training loss 0.05990695208311081 Validation loss 0.059937767684459686 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.0426],\n",
      "        [0.7172]], device='mps:0')\n",
      "Iteration 38710 Training loss 0.055481988936662674 Validation loss 0.05997316539287567 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.9769],\n",
      "        [0.6433]], device='mps:0')\n",
      "Iteration 38720 Training loss 0.05828163027763367 Validation loss 0.05993346869945526 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.1513],\n",
      "        [0.1102]], device='mps:0')\n",
      "Iteration 38730 Training loss 0.05617664381861687 Validation loss 0.05992031842470169 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.0514],\n",
      "        [0.9316]], device='mps:0')\n",
      "Iteration 38740 Training loss 0.053040359169244766 Validation loss 0.05997735634446144 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.1967],\n",
      "        [0.5113]], device='mps:0')\n",
      "Iteration 38750 Training loss 0.057739146053791046 Validation loss 0.059916798025369644 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.9904],\n",
      "        [0.1056]], device='mps:0')\n",
      "Iteration 38760 Training loss 0.057781193405389786 Validation loss 0.05998649820685387 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.1546],\n",
      "        [0.1522]], device='mps:0')\n",
      "Iteration 38770 Training loss 0.05751330405473709 Validation loss 0.05994702875614166 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.1088],\n",
      "        [0.2657]], device='mps:0')\n",
      "Iteration 38780 Training loss 0.05927171930670738 Validation loss 0.05996815860271454 Accuracy 0.8350000381469727\n",
      "Output tensor([[0.9809],\n",
      "        [0.5709]], device='mps:0')\n",
      "Iteration 38790 Training loss 0.05822331830859184 Validation loss 0.059943895787000656 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.5818],\n",
      "        [0.9180]], device='mps:0')\n",
      "Iteration 38800 Training loss 0.05449659377336502 Validation loss 0.060124240815639496 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.5044],\n",
      "        [0.5506]], device='mps:0')\n",
      "Iteration 38810 Training loss 0.059185873717069626 Validation loss 0.05993847921490669 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.0582],\n",
      "        [0.8362]], device='mps:0')\n",
      "Iteration 38820 Training loss 0.05160250887274742 Validation loss 0.060080401599407196 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.9180],\n",
      "        [0.0769]], device='mps:0')\n",
      "Iteration 38830 Training loss 0.05792943760752678 Validation loss 0.05993761867284775 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.9552],\n",
      "        [0.0666]], device='mps:0')\n",
      "Iteration 38840 Training loss 0.0663529559969902 Validation loss 0.059927694499492645 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.4290],\n",
      "        [0.1810]], device='mps:0')\n",
      "Iteration 38850 Training loss 0.05104319378733635 Validation loss 0.06007679924368858 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.0719],\n",
      "        [0.9772]], device='mps:0')\n",
      "Iteration 38860 Training loss 0.0633053258061409 Validation loss 0.060002025216817856 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.1105],\n",
      "        [0.8449]], device='mps:0')\n",
      "Iteration 38870 Training loss 0.050737880170345306 Validation loss 0.06005598232150078 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.9954],\n",
      "        [0.5154]], device='mps:0')\n",
      "Iteration 38880 Training loss 0.056066617369651794 Validation loss 0.05991775169968605 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.7136],\n",
      "        [0.2105]], device='mps:0')\n",
      "Iteration 38890 Training loss 0.051224593073129654 Validation loss 0.05991791561245918 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.9491],\n",
      "        [0.0483]], device='mps:0')\n",
      "Iteration 38900 Training loss 0.05088074877858162 Validation loss 0.060041096061468124 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.3568],\n",
      "        [0.8373]], device='mps:0')\n",
      "Iteration 38910 Training loss 0.052036140114068985 Validation loss 0.05995435267686844 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.9008],\n",
      "        [0.9593]], device='mps:0')\n",
      "Iteration 38920 Training loss 0.05264480412006378 Validation loss 0.05994138494133949 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.1055],\n",
      "        [0.3700]], device='mps:0')\n",
      "Iteration 38930 Training loss 0.05723461136221886 Validation loss 0.05994509533047676 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.0452],\n",
      "        [0.2576]], device='mps:0')\n",
      "Iteration 38940 Training loss 0.054631318897008896 Validation loss 0.05994516238570213 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.5410],\n",
      "        [0.5833]], device='mps:0')\n",
      "Iteration 38950 Training loss 0.05990966781973839 Validation loss 0.05993855744600296 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.4241],\n",
      "        [0.9841]], device='mps:0')\n",
      "Iteration 38960 Training loss 0.05765369161963463 Validation loss 0.059952784329652786 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.1091],\n",
      "        [0.6406]], device='mps:0')\n",
      "Iteration 38970 Training loss 0.06141818314790726 Validation loss 0.05998855084180832 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.3218],\n",
      "        [0.8894]], device='mps:0')\n",
      "Iteration 38980 Training loss 0.05677412822842598 Validation loss 0.05991935357451439 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.0879],\n",
      "        [0.8923]], device='mps:0')\n",
      "Iteration 38990 Training loss 0.05658986046910286 Validation loss 0.059933245182037354 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.8960],\n",
      "        [0.3754]], device='mps:0')\n",
      "Iteration 39000 Training loss 0.0616084560751915 Validation loss 0.0599214993417263 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.0648],\n",
      "        [0.5898]], device='mps:0')\n",
      "Iteration 39010 Training loss 0.06215444952249527 Validation loss 0.06001623347401619 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.1847],\n",
      "        [0.2973]], device='mps:0')\n",
      "Iteration 39020 Training loss 0.05496687442064285 Validation loss 0.05991494283080101 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.9837],\n",
      "        [0.0468]], device='mps:0')\n",
      "Iteration 39030 Training loss 0.05029090866446495 Validation loss 0.059946876019239426 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.2785],\n",
      "        [0.8949]], device='mps:0')\n",
      "Iteration 39040 Training loss 0.06396528333425522 Validation loss 0.05997813120484352 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.3457],\n",
      "        [0.9105]], device='mps:0')\n",
      "Iteration 39050 Training loss 0.0591266043484211 Validation loss 0.060118235647678375 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.0851],\n",
      "        [0.2750]], device='mps:0')\n",
      "Iteration 39060 Training loss 0.06122409552335739 Validation loss 0.05995004251599312 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.7122],\n",
      "        [0.2611]], device='mps:0')\n",
      "Iteration 39070 Training loss 0.0625474825501442 Validation loss 0.060047198086977005 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.9879],\n",
      "        [0.9580]], device='mps:0')\n",
      "Iteration 39080 Training loss 0.05320290848612785 Validation loss 0.059930652379989624 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.7143],\n",
      "        [0.0682]], device='mps:0')\n",
      "Iteration 39090 Training loss 0.0664341077208519 Validation loss 0.059928957372903824 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.1308],\n",
      "        [0.9928]], device='mps:0')\n",
      "Iteration 39100 Training loss 0.056871023029088974 Validation loss 0.059929247945547104 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.8986],\n",
      "        [0.9450]], device='mps:0')\n",
      "Iteration 39110 Training loss 0.06308915466070175 Validation loss 0.05992701277136803 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.1068],\n",
      "        [0.9345]], device='mps:0')\n",
      "Iteration 39120 Training loss 0.05033227801322937 Validation loss 0.060005608946084976 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.0232],\n",
      "        [0.1811]], device='mps:0')\n",
      "Iteration 39130 Training loss 0.05570593848824501 Validation loss 0.059931982308626175 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.6389],\n",
      "        [0.8977]], device='mps:0')\n",
      "Iteration 39140 Training loss 0.04989685118198395 Validation loss 0.05992506071925163 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.2845],\n",
      "        [0.2179]], device='mps:0')\n",
      "Iteration 39150 Training loss 0.05888938903808594 Validation loss 0.05992841720581055 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.1835],\n",
      "        [0.5593]], device='mps:0')\n",
      "Iteration 39160 Training loss 0.062311142683029175 Validation loss 0.05999172106385231 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.6476],\n",
      "        [0.6729]], device='mps:0')\n",
      "Iteration 39170 Training loss 0.04995090886950493 Validation loss 0.05990659445524216 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.0183],\n",
      "        [0.2114]], device='mps:0')\n",
      "Iteration 39180 Training loss 0.056269168853759766 Validation loss 0.05990587919950485 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.4485],\n",
      "        [0.7232]], device='mps:0')\n",
      "Iteration 39190 Training loss 0.05240093544125557 Validation loss 0.06004657968878746 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.5112],\n",
      "        [0.9885]], device='mps:0')\n",
      "Iteration 39200 Training loss 0.05707424134016037 Validation loss 0.059927865862846375 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.9803],\n",
      "        [0.6951]], device='mps:0')\n",
      "Iteration 39210 Training loss 0.05944389849901199 Validation loss 0.05990831181406975 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.9872],\n",
      "        [0.1154]], device='mps:0')\n",
      "Iteration 39220 Training loss 0.048232778906822205 Validation loss 0.059930697083473206 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.1116],\n",
      "        [0.3716]], device='mps:0')\n",
      "Iteration 39230 Training loss 0.06418636441230774 Validation loss 0.05990661308169365 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.4591],\n",
      "        [0.8739]], device='mps:0')\n",
      "Iteration 39240 Training loss 0.050452493131160736 Validation loss 0.059903062880039215 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.8205],\n",
      "        [0.6302]], device='mps:0')\n",
      "Iteration 39250 Training loss 0.06345713138580322 Validation loss 0.05991426482796669 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.0867],\n",
      "        [0.3261]], device='mps:0')\n",
      "Iteration 39260 Training loss 0.05776801332831383 Validation loss 0.05996602773666382 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.2658],\n",
      "        [0.1375]], device='mps:0')\n",
      "Iteration 39270 Training loss 0.053244102746248245 Validation loss 0.05997360870242119 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.6926],\n",
      "        [0.2652]], device='mps:0')\n",
      "Iteration 39280 Training loss 0.05224928632378578 Validation loss 0.05992254614830017 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.5666],\n",
      "        [0.0394]], device='mps:0')\n",
      "Iteration 39290 Training loss 0.0642569363117218 Validation loss 0.059904709458351135 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.1399],\n",
      "        [0.3310]], device='mps:0')\n",
      "Iteration 39300 Training loss 0.05584293603897095 Validation loss 0.05990096554160118 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.1240],\n",
      "        [0.0909]], device='mps:0')\n",
      "Iteration 39310 Training loss 0.04879076033830643 Validation loss 0.059914011508226395 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.8437],\n",
      "        [0.4751]], device='mps:0')\n",
      "Iteration 39320 Training loss 0.05073602870106697 Validation loss 0.05998045578598976 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.8981],\n",
      "        [0.1601]], device='mps:0')\n",
      "Iteration 39330 Training loss 0.05888036638498306 Validation loss 0.06014559417963028 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.8304],\n",
      "        [0.9782]], device='mps:0')\n",
      "Iteration 39340 Training loss 0.06156047806143761 Validation loss 0.05995376035571098 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.9410],\n",
      "        [0.9110]], device='mps:0')\n",
      "Iteration 39350 Training loss 0.06582605093717575 Validation loss 0.05998281389474869 Accuracy 0.8341250419616699\n",
      "Output tensor([[0.5045],\n",
      "        [0.9804]], device='mps:0')\n",
      "Iteration 39360 Training loss 0.06160605326294899 Validation loss 0.059916168451309204 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.3775],\n",
      "        [0.9011]], device='mps:0')\n",
      "Iteration 39370 Training loss 0.057244326919317245 Validation loss 0.0598934069275856 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.0513],\n",
      "        [0.7651]], device='mps:0')\n",
      "Iteration 39380 Training loss 0.06672239303588867 Validation loss 0.05990064516663551 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.4850],\n",
      "        [0.0758]], device='mps:0')\n",
      "Iteration 39390 Training loss 0.055381305515766144 Validation loss 0.05989525839686394 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.9754],\n",
      "        [0.8010]], device='mps:0')\n",
      "Iteration 39400 Training loss 0.058544158935546875 Validation loss 0.0599750354886055 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.0369],\n",
      "        [0.0774]], device='mps:0')\n",
      "Iteration 39410 Training loss 0.060882363468408585 Validation loss 0.059904132038354874 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.9698],\n",
      "        [0.9248]], device='mps:0')\n",
      "Iteration 39420 Training loss 0.05773894488811493 Validation loss 0.05988917499780655 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.0971],\n",
      "        [0.0148]], device='mps:0')\n",
      "Iteration 39430 Training loss 0.055386170744895935 Validation loss 0.05988811329007149 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.9878],\n",
      "        [0.8708]], device='mps:0')\n",
      "Iteration 39440 Training loss 0.056710030883550644 Validation loss 0.05989716574549675 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.3755],\n",
      "        [0.1079]], device='mps:0')\n",
      "Iteration 39450 Training loss 0.06599915027618408 Validation loss 0.05990514159202576 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.2660],\n",
      "        [0.9201]], device='mps:0')\n",
      "Iteration 39460 Training loss 0.057821910828351974 Validation loss 0.05988519266247749 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.1438],\n",
      "        [0.1777]], device='mps:0')\n",
      "Iteration 39470 Training loss 0.05814111605286598 Validation loss 0.05989697948098183 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.0692],\n",
      "        [0.8522]], device='mps:0')\n",
      "Iteration 39480 Training loss 0.06600920855998993 Validation loss 0.059969332069158554 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.7774],\n",
      "        [0.9929]], device='mps:0')\n",
      "Iteration 39490 Training loss 0.05551500990986824 Validation loss 0.05989688262343407 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.0511],\n",
      "        [0.2136]], device='mps:0')\n",
      "Iteration 39500 Training loss 0.053760088980197906 Validation loss 0.05990343540906906 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.6922],\n",
      "        [0.0433]], device='mps:0')\n",
      "Iteration 39510 Training loss 0.060595471411943436 Validation loss 0.06018250435590744 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.0571],\n",
      "        [0.8090]], device='mps:0')\n",
      "Iteration 39520 Training loss 0.057211585342884064 Validation loss 0.05995377525687218 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.9635],\n",
      "        [0.7557]], device='mps:0')\n",
      "Iteration 39530 Training loss 0.05909708887338638 Validation loss 0.06001964211463928 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.9197],\n",
      "        [0.5249]], device='mps:0')\n",
      "Iteration 39540 Training loss 0.05766596272587776 Validation loss 0.05988862365484238 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.0195],\n",
      "        [0.8224]], device='mps:0')\n",
      "Iteration 39550 Training loss 0.06321033090353012 Validation loss 0.06013401597738266 Accuracy 0.8345000147819519\n",
      "Output tensor([[0.8363],\n",
      "        [0.7118]], device='mps:0')\n",
      "Iteration 39560 Training loss 0.058374322950839996 Validation loss 0.060123905539512634 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.8446],\n",
      "        [0.9208]], device='mps:0')\n",
      "Iteration 39570 Training loss 0.060033492743968964 Validation loss 0.05988364666700363 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.5876],\n",
      "        [0.6777]], device='mps:0')\n",
      "Iteration 39580 Training loss 0.06495089083909988 Validation loss 0.059898197650909424 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.0246],\n",
      "        [0.1112]], device='mps:0')\n",
      "Iteration 39590 Training loss 0.056887056678533554 Validation loss 0.060234200209379196 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.9298],\n",
      "        [0.8391]], device='mps:0')\n",
      "Iteration 39600 Training loss 0.0626935213804245 Validation loss 0.06002495065331459 Accuracy 0.8346250653266907\n",
      "Output tensor([[0.2259],\n",
      "        [0.5263]], device='mps:0')\n",
      "Iteration 39610 Training loss 0.057734955102205276 Validation loss 0.06012716144323349 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.9503],\n",
      "        [0.0383]], device='mps:0')\n",
      "Iteration 39620 Training loss 0.061073750257492065 Validation loss 0.05993581563234329 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.6903],\n",
      "        [0.8255]], device='mps:0')\n",
      "Iteration 39630 Training loss 0.0628288984298706 Validation loss 0.059870459139347076 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.9613],\n",
      "        [0.6418]], device='mps:0')\n",
      "Iteration 39640 Training loss 0.057335857301950455 Validation loss 0.05997459962964058 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.0474],\n",
      "        [0.2150]], device='mps:0')\n",
      "Iteration 39650 Training loss 0.05822078883647919 Validation loss 0.05988837778568268 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.1053],\n",
      "        [0.3001]], device='mps:0')\n",
      "Iteration 39660 Training loss 0.05477923899888992 Validation loss 0.05987954139709473 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.9552],\n",
      "        [0.2115]], device='mps:0')\n",
      "Iteration 39670 Training loss 0.056459710001945496 Validation loss 0.05995193123817444 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.3870],\n",
      "        [0.8833]], device='mps:0')\n",
      "Iteration 39680 Training loss 0.055639028549194336 Validation loss 0.05988556891679764 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.8377],\n",
      "        [0.8923]], device='mps:0')\n",
      "Iteration 39690 Training loss 0.06581611186265945 Validation loss 0.059908222407102585 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.1734],\n",
      "        [0.0333]], device='mps:0')\n",
      "Iteration 39700 Training loss 0.052404746413230896 Validation loss 0.060041844844818115 Accuracy 0.8347500562667847\n",
      "Output tensor([[0.9770],\n",
      "        [0.4152]], device='mps:0')\n",
      "Iteration 39710 Training loss 0.054481733590364456 Validation loss 0.060067068785429 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.1910],\n",
      "        [0.0607]], device='mps:0')\n",
      "Iteration 39720 Training loss 0.06263899058103561 Validation loss 0.05989691615104675 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.9344],\n",
      "        [0.7796]], device='mps:0')\n",
      "Iteration 39730 Training loss 0.062317535281181335 Validation loss 0.059983521699905396 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.8274],\n",
      "        [0.3906]], device='mps:0')\n",
      "Iteration 39740 Training loss 0.05481041967868805 Validation loss 0.05988367646932602 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.9864],\n",
      "        [0.9454]], device='mps:0')\n",
      "Iteration 39750 Training loss 0.0611734464764595 Validation loss 0.059898652136325836 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.1469],\n",
      "        [0.0987]], device='mps:0')\n",
      "Iteration 39760 Training loss 0.04975704476237297 Validation loss 0.05990978702902794 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.4327],\n",
      "        [0.4168]], device='mps:0')\n",
      "Iteration 39770 Training loss 0.06112777441740036 Validation loss 0.059894077479839325 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.6171],\n",
      "        [0.5870]], device='mps:0')\n",
      "Iteration 39780 Training loss 0.05141341686248779 Validation loss 0.0598861426115036 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.1579],\n",
      "        [0.8119]], device='mps:0')\n",
      "Iteration 39790 Training loss 0.05862722918391228 Validation loss 0.05986517667770386 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.3804],\n",
      "        [0.0523]], device='mps:0')\n",
      "Iteration 39800 Training loss 0.07013349235057831 Validation loss 0.059940267354249954 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.6189],\n",
      "        [0.8792]], device='mps:0')\n",
      "Iteration 39810 Training loss 0.05977657809853554 Validation loss 0.05991588532924652 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.0417],\n",
      "        [0.3013]], device='mps:0')\n",
      "Iteration 39820 Training loss 0.0662817731499672 Validation loss 0.05987202376127243 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.2242],\n",
      "        [0.7584]], device='mps:0')\n",
      "Iteration 39830 Training loss 0.054305315017700195 Validation loss 0.05984408035874367 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.4513],\n",
      "        [0.8715]], device='mps:0')\n",
      "Iteration 39840 Training loss 0.05430780351161957 Validation loss 0.05990399792790413 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.0625],\n",
      "        [0.2267]], device='mps:0')\n",
      "Iteration 39850 Training loss 0.056402675807476044 Validation loss 0.05984528735280037 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.9836],\n",
      "        [0.9692]], device='mps:0')\n",
      "Iteration 39860 Training loss 0.055875033140182495 Validation loss 0.059839364141225815 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.2690],\n",
      "        [0.9960]], device='mps:0')\n",
      "Iteration 39870 Training loss 0.05694640800356865 Validation loss 0.059836529195308685 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.7542],\n",
      "        [0.9716]], device='mps:0')\n",
      "Iteration 39880 Training loss 0.06310481578111649 Validation loss 0.059946298599243164 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.5653],\n",
      "        [0.8211]], device='mps:0')\n",
      "Iteration 39890 Training loss 0.05430930107831955 Validation loss 0.059842344373464584 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.5249],\n",
      "        [0.1398]], device='mps:0')\n",
      "Iteration 39900 Training loss 0.05301736667752266 Validation loss 0.059861164540052414 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.2371],\n",
      "        [0.1250]], device='mps:0')\n",
      "Iteration 39910 Training loss 0.048860929906368256 Validation loss 0.05984233319759369 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.8822],\n",
      "        [0.8519]], device='mps:0')\n",
      "Iteration 39920 Training loss 0.05387910082936287 Validation loss 0.05983338505029678 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.3618],\n",
      "        [0.4105]], device='mps:0')\n",
      "Iteration 39930 Training loss 0.06061619147658348 Validation loss 0.059833258390426636 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.1381],\n",
      "        [0.8623]], device='mps:0')\n",
      "Iteration 39940 Training loss 0.05794554948806763 Validation loss 0.05990598723292351 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.4138],\n",
      "        [0.7550]], device='mps:0')\n",
      "Iteration 39950 Training loss 0.0667404904961586 Validation loss 0.05995246767997742 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.8907],\n",
      "        [0.7873]], device='mps:0')\n",
      "Iteration 39960 Training loss 0.05717989429831505 Validation loss 0.059849802404642105 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.9793],\n",
      "        [0.3672]], device='mps:0')\n",
      "Iteration 39970 Training loss 0.054440487176179886 Validation loss 0.05985786020755768 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.9034],\n",
      "        [0.7177]], device='mps:0')\n",
      "Iteration 39980 Training loss 0.0564458929002285 Validation loss 0.06003745645284653 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.9925],\n",
      "        [0.9417]], device='mps:0')\n",
      "Iteration 39990 Training loss 0.05337660014629364 Validation loss 0.05989320948719978 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.2275],\n",
      "        [0.1716]], device='mps:0')\n",
      "Iteration 40000 Training loss 0.054024484008550644 Validation loss 0.0598619282245636 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.9929],\n",
      "        [0.1406]], device='mps:0')\n",
      "Iteration 40010 Training loss 0.05436012148857117 Validation loss 0.0598430298268795 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.2290],\n",
      "        [0.0321]], device='mps:0')\n",
      "Iteration 40020 Training loss 0.05635581538081169 Validation loss 0.059853531420230865 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.2482],\n",
      "        [0.9143]], device='mps:0')\n",
      "Iteration 40030 Training loss 0.0573282353579998 Validation loss 0.0599382184445858 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.3465],\n",
      "        [0.5460]], device='mps:0')\n",
      "Iteration 40040 Training loss 0.06377856433391571 Validation loss 0.059837572276592255 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.9095],\n",
      "        [0.2340]], device='mps:0')\n",
      "Iteration 40050 Training loss 0.05175664275884628 Validation loss 0.060064833611249924 Accuracy 0.8338750600814819\n",
      "Output tensor([[0.9825],\n",
      "        [0.9126]], device='mps:0')\n",
      "Iteration 40060 Training loss 0.06821766495704651 Validation loss 0.05984370782971382 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.8172],\n",
      "        [0.8923]], device='mps:0')\n",
      "Iteration 40070 Training loss 0.05690367519855499 Validation loss 0.059928178787231445 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.0521],\n",
      "        [0.4157]], device='mps:0')\n",
      "Iteration 40080 Training loss 0.05002109333872795 Validation loss 0.06028277799487114 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.8168],\n",
      "        [0.9060]], device='mps:0')\n",
      "Iteration 40090 Training loss 0.055353887379169464 Validation loss 0.059913519769907 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.0662],\n",
      "        [0.6250]], device='mps:0')\n",
      "Iteration 40100 Training loss 0.06465014070272446 Validation loss 0.05983734503388405 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.0106],\n",
      "        [0.4782]], device='mps:0')\n",
      "Iteration 40110 Training loss 0.06096838042140007 Validation loss 0.05985119938850403 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.3629],\n",
      "        [0.8407]], device='mps:0')\n",
      "Iteration 40120 Training loss 0.05964256078004837 Validation loss 0.0598481260240078 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.1655],\n",
      "        [0.0709]], device='mps:0')\n",
      "Iteration 40130 Training loss 0.06291960924863815 Validation loss 0.05983016639947891 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.7711],\n",
      "        [0.2632]], device='mps:0')\n",
      "Iteration 40140 Training loss 0.052303608506917953 Validation loss 0.060014739632606506 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.0321],\n",
      "        [0.3355]], device='mps:0')\n",
      "Iteration 40150 Training loss 0.06160770356655121 Validation loss 0.059822481125593185 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.4008],\n",
      "        [0.9767]], device='mps:0')\n",
      "Iteration 40160 Training loss 0.05313665792346001 Validation loss 0.05982103943824768 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.8837],\n",
      "        [0.1049]], device='mps:0')\n",
      "Iteration 40170 Training loss 0.053042180836200714 Validation loss 0.059874407947063446 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.4659],\n",
      "        [0.5322]], device='mps:0')\n",
      "Iteration 40180 Training loss 0.06584865599870682 Validation loss 0.059822626411914825 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.1570],\n",
      "        [0.1897]], device='mps:0')\n",
      "Iteration 40190 Training loss 0.06257855147123337 Validation loss 0.059836309403181076 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.1025],\n",
      "        [0.2763]], device='mps:0')\n",
      "Iteration 40200 Training loss 0.06677472591400146 Validation loss 0.0598331056535244 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.0466],\n",
      "        [0.2489]], device='mps:0')\n",
      "Iteration 40210 Training loss 0.06154355779290199 Validation loss 0.059825800359249115 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.0425],\n",
      "        [0.2403]], device='mps:0')\n",
      "Iteration 40220 Training loss 0.05875541642308235 Validation loss 0.0598110593855381 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.8745],\n",
      "        [0.0562]], device='mps:0')\n",
      "Iteration 40230 Training loss 0.05887432396411896 Validation loss 0.059857118874788284 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.1105],\n",
      "        [0.8610]], device='mps:0')\n",
      "Iteration 40240 Training loss 0.053317394107580185 Validation loss 0.05983882397413254 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.3471],\n",
      "        [0.8716]], device='mps:0')\n",
      "Iteration 40250 Training loss 0.05420324206352234 Validation loss 0.05985807999968529 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.9521],\n",
      "        [0.1642]], device='mps:0')\n",
      "Iteration 40260 Training loss 0.04802786558866501 Validation loss 0.05981186777353287 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.8578],\n",
      "        [0.5134]], device='mps:0')\n",
      "Iteration 40270 Training loss 0.056523654609918594 Validation loss 0.059868671000003815 Accuracy 0.8358750343322754\n",
      "Output tensor([[0.4365],\n",
      "        [0.7955]], device='mps:0')\n",
      "Iteration 40280 Training loss 0.059339504688978195 Validation loss 0.05980975180864334 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.9895],\n",
      "        [0.0580]], device='mps:0')\n",
      "Iteration 40290 Training loss 0.06437531113624573 Validation loss 0.05997408926486969 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.2849],\n",
      "        [0.6872]], device='mps:0')\n",
      "Iteration 40300 Training loss 0.06177280470728874 Validation loss 0.059862006455659866 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.7741],\n",
      "        [0.6580]], device='mps:0')\n",
      "Iteration 40310 Training loss 0.06204185262322426 Validation loss 0.05989637225866318 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.1785],\n",
      "        [0.0633]], device='mps:0')\n",
      "Iteration 40320 Training loss 0.06413321942090988 Validation loss 0.05981215834617615 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.7796],\n",
      "        [0.2342]], device='mps:0')\n",
      "Iteration 40330 Training loss 0.06357713788747787 Validation loss 0.060010187327861786 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.4558],\n",
      "        [0.1339]], device='mps:0')\n",
      "Iteration 40340 Training loss 0.05517369508743286 Validation loss 0.05980658903717995 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.9517],\n",
      "        [0.5717]], device='mps:0')\n",
      "Iteration 40350 Training loss 0.05429103225469589 Validation loss 0.05994291231036186 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.0718],\n",
      "        [0.0149]], device='mps:0')\n",
      "Iteration 40360 Training loss 0.061626844108104706 Validation loss 0.059834402054548264 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.8069],\n",
      "        [0.9494]], device='mps:0')\n",
      "Iteration 40370 Training loss 0.05927470698952675 Validation loss 0.05981016531586647 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.1394],\n",
      "        [0.5743]], device='mps:0')\n",
      "Iteration 40380 Training loss 0.05206020548939705 Validation loss 0.059806276112794876 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.1076],\n",
      "        [0.9890]], device='mps:0')\n",
      "Iteration 40390 Training loss 0.060727305710315704 Validation loss 0.060114696621894836 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.5393],\n",
      "        [0.3191]], device='mps:0')\n",
      "Iteration 40400 Training loss 0.057086240500211716 Validation loss 0.059809330850839615 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.3382],\n",
      "        [0.9280]], device='mps:0')\n",
      "Iteration 40410 Training loss 0.05548647791147232 Validation loss 0.05982314795255661 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.9288],\n",
      "        [0.9605]], device='mps:0')\n",
      "Iteration 40420 Training loss 0.0606258250772953 Validation loss 0.05990815535187721 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.3461],\n",
      "        [0.9114]], device='mps:0')\n",
      "Iteration 40430 Training loss 0.05695923790335655 Validation loss 0.05980593338608742 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.1406],\n",
      "        [0.2844]], device='mps:0')\n",
      "Iteration 40440 Training loss 0.053124960511922836 Validation loss 0.05980437994003296 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.0905],\n",
      "        [0.1120]], device='mps:0')\n",
      "Iteration 40450 Training loss 0.05416489392518997 Validation loss 0.06004282087087631 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.2747],\n",
      "        [0.6160]], device='mps:0')\n",
      "Iteration 40460 Training loss 0.04840103164315224 Validation loss 0.05989667773246765 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.3394],\n",
      "        [0.0412]], device='mps:0')\n",
      "Iteration 40470 Training loss 0.0580584891140461 Validation loss 0.059797532856464386 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.9637],\n",
      "        [0.1351]], device='mps:0')\n",
      "Iteration 40480 Training loss 0.055939849466085434 Validation loss 0.05979861319065094 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.4341],\n",
      "        [0.5573]], device='mps:0')\n",
      "Iteration 40490 Training loss 0.050698909908533096 Validation loss 0.05986648052930832 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.0274],\n",
      "        [0.7492]], device='mps:0')\n",
      "Iteration 40500 Training loss 0.0551169328391552 Validation loss 0.059916675090789795 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.9677],\n",
      "        [0.9845]], device='mps:0')\n",
      "Iteration 40510 Training loss 0.05958288908004761 Validation loss 0.0597916916012764 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.6852],\n",
      "        [0.8846]], device='mps:0')\n",
      "Iteration 40520 Training loss 0.062085140496492386 Validation loss 0.059803493320941925 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.0472],\n",
      "        [0.9948]], device='mps:0')\n",
      "Iteration 40530 Training loss 0.05416401848196983 Validation loss 0.05992908030748367 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.9903],\n",
      "        [0.8373]], device='mps:0')\n",
      "Iteration 40540 Training loss 0.056188859045505524 Validation loss 0.0597880557179451 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.9084],\n",
      "        [0.1688]], device='mps:0')\n",
      "Iteration 40550 Training loss 0.05040907859802246 Validation loss 0.05986875668168068 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.4308],\n",
      "        [0.1227]], device='mps:0')\n",
      "Iteration 40560 Training loss 0.0533096119761467 Validation loss 0.059829600155353546 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.5057],\n",
      "        [0.1617]], device='mps:0')\n",
      "Iteration 40570 Training loss 0.05732207000255585 Validation loss 0.059822484850883484 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.0796],\n",
      "        [0.7127]], device='mps:0')\n",
      "Iteration 40580 Training loss 0.04838872700929642 Validation loss 0.05979085713624954 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.0695],\n",
      "        [0.4041]], device='mps:0')\n",
      "Iteration 40590 Training loss 0.0551847442984581 Validation loss 0.05978410691022873 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.9739],\n",
      "        [0.5771]], device='mps:0')\n",
      "Iteration 40600 Training loss 0.05920231714844704 Validation loss 0.05978306755423546 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.9632],\n",
      "        [0.4249]], device='mps:0')\n",
      "Iteration 40610 Training loss 0.06592173874378204 Validation loss 0.05982761085033417 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.0134],\n",
      "        [0.8466]], device='mps:0')\n",
      "Iteration 40620 Training loss 0.06812654435634613 Validation loss 0.05986915901303291 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.8034],\n",
      "        [0.1786]], device='mps:0')\n",
      "Iteration 40630 Training loss 0.0572500117123127 Validation loss 0.0599156990647316 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.1402],\n",
      "        [0.0310]], device='mps:0')\n",
      "Iteration 40640 Training loss 0.0544038750231266 Validation loss 0.059937622398138046 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.0811],\n",
      "        [0.0577]], device='mps:0')\n",
      "Iteration 40650 Training loss 0.060243017971515656 Validation loss 0.059812236577272415 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.9636],\n",
      "        [0.7421]], device='mps:0')\n",
      "Iteration 40660 Training loss 0.06287797540426254 Validation loss 0.0597844198346138 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.0541],\n",
      "        [0.4104]], device='mps:0')\n",
      "Iteration 40670 Training loss 0.057180698961019516 Validation loss 0.05989879369735718 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.4747],\n",
      "        [0.4190]], device='mps:0')\n",
      "Iteration 40680 Training loss 0.05935662239789963 Validation loss 0.05984341353178024 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.9235],\n",
      "        [0.6890]], device='mps:0')\n",
      "Iteration 40690 Training loss 0.052814748138189316 Validation loss 0.05978241562843323 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.9729],\n",
      "        [0.6099]], device='mps:0')\n",
      "Iteration 40700 Training loss 0.06630399823188782 Validation loss 0.05978427082300186 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.9329],\n",
      "        [0.2645]], device='mps:0')\n",
      "Iteration 40710 Training loss 0.05717715248465538 Validation loss 0.05985374003648758 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.8762],\n",
      "        [0.8252]], device='mps:0')\n",
      "Iteration 40720 Training loss 0.05713101848959923 Validation loss 0.05980294942855835 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.2831],\n",
      "        [0.8090]], device='mps:0')\n",
      "Iteration 40730 Training loss 0.06025562062859535 Validation loss 0.06025591865181923 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.5559],\n",
      "        [0.4317]], device='mps:0')\n",
      "Iteration 40740 Training loss 0.05442686006426811 Validation loss 0.05978488549590111 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.9600],\n",
      "        [0.8811]], device='mps:0')\n",
      "Iteration 40750 Training loss 0.06239193677902222 Validation loss 0.05977901443839073 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.0461],\n",
      "        [0.7339]], device='mps:0')\n",
      "Iteration 40760 Training loss 0.054894961416721344 Validation loss 0.05978502333164215 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.8250],\n",
      "        [0.7501]], device='mps:0')\n",
      "Iteration 40770 Training loss 0.061566393822431564 Validation loss 0.059837061911821365 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.3038],\n",
      "        [0.5952]], device='mps:0')\n",
      "Iteration 40780 Training loss 0.05465498939156532 Validation loss 0.0598023496568203 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.8312],\n",
      "        [0.3840]], device='mps:0')\n",
      "Iteration 40790 Training loss 0.06478599458932877 Validation loss 0.05994543805718422 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.7410],\n",
      "        [0.0725]], device='mps:0')\n",
      "Iteration 40800 Training loss 0.056979309767484665 Validation loss 0.05978924408555031 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.4388],\n",
      "        [0.9647]], device='mps:0')\n",
      "Iteration 40810 Training loss 0.055129699409008026 Validation loss 0.05997313931584358 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.0782],\n",
      "        [0.8185]], device='mps:0')\n",
      "Iteration 40820 Training loss 0.058956507593393326 Validation loss 0.0597633495926857 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.0532],\n",
      "        [0.9884]], device='mps:0')\n",
      "Iteration 40830 Training loss 0.05784479156136513 Validation loss 0.05976492539048195 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.9973],\n",
      "        [0.4294]], device='mps:0')\n",
      "Iteration 40840 Training loss 0.05944431945681572 Validation loss 0.05992778390645981 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.0271],\n",
      "        [0.7244]], device='mps:0')\n",
      "Iteration 40850 Training loss 0.06358730047941208 Validation loss 0.05987618491053581 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.9847],\n",
      "        [0.1019]], device='mps:0')\n",
      "Iteration 40860 Training loss 0.0618106909096241 Validation loss 0.059894416481256485 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.0443],\n",
      "        [0.6760]], device='mps:0')\n",
      "Iteration 40870 Training loss 0.06649966537952423 Validation loss 0.059794094413518906 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.7110],\n",
      "        [0.9793]], device='mps:0')\n",
      "Iteration 40880 Training loss 0.06621690839529037 Validation loss 0.05976257845759392 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.5666],\n",
      "        [0.9010]], device='mps:0')\n",
      "Iteration 40890 Training loss 0.06288453936576843 Validation loss 0.059782661497592926 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.3222],\n",
      "        [0.9733]], device='mps:0')\n",
      "Iteration 40900 Training loss 0.06177584081888199 Validation loss 0.059777118265628815 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.2786],\n",
      "        [0.6452]], device='mps:0')\n",
      "Iteration 40910 Training loss 0.0573190376162529 Validation loss 0.05981934443116188 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.0841],\n",
      "        [0.0083]], device='mps:0')\n",
      "Iteration 40920 Training loss 0.05981223285198212 Validation loss 0.05981341004371643 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.3842],\n",
      "        [0.9799]], device='mps:0')\n",
      "Iteration 40930 Training loss 0.06260957568883896 Validation loss 0.05975571274757385 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.8087],\n",
      "        [0.1991]], device='mps:0')\n",
      "Iteration 40940 Training loss 0.05836225673556328 Validation loss 0.05977566912770271 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.9655],\n",
      "        [0.0577]], device='mps:0')\n",
      "Iteration 40950 Training loss 0.05815619230270386 Validation loss 0.05977776646614075 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.0067],\n",
      "        [0.8868]], device='mps:0')\n",
      "Iteration 40960 Training loss 0.06156497448682785 Validation loss 0.05978616699576378 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.7754],\n",
      "        [0.8987]], device='mps:0')\n",
      "Iteration 40970 Training loss 0.06472977250814438 Validation loss 0.059776753187179565 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.9428],\n",
      "        [0.5355]], device='mps:0')\n",
      "Iteration 40980 Training loss 0.0632861778140068 Validation loss 0.059746257960796356 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.8613],\n",
      "        [0.3768]], device='mps:0')\n",
      "Iteration 40990 Training loss 0.060362014919519424 Validation loss 0.05974012613296509 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.0058],\n",
      "        [0.9486]], device='mps:0')\n",
      "Iteration 41000 Training loss 0.05850811302661896 Validation loss 0.06003907695412636 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.0136],\n",
      "        [0.8913]], device='mps:0')\n",
      "Iteration 41010 Training loss 0.05611586570739746 Validation loss 0.05974099040031433 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.9665],\n",
      "        [0.9936]], device='mps:0')\n",
      "Iteration 41020 Training loss 0.05586656555533409 Validation loss 0.05974332615733147 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.1884],\n",
      "        [0.9101]], device='mps:0')\n",
      "Iteration 41030 Training loss 0.055953748524188995 Validation loss 0.059799131006002426 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.0447],\n",
      "        [0.7783]], device='mps:0')\n",
      "Iteration 41040 Training loss 0.054388903081417084 Validation loss 0.05988037958741188 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.9886],\n",
      "        [0.7828]], device='mps:0')\n",
      "Iteration 41050 Training loss 0.05104425922036171 Validation loss 0.059743732213974 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.0337],\n",
      "        [0.0144]], device='mps:0')\n",
      "Iteration 41060 Training loss 0.051884133368730545 Validation loss 0.05974990501999855 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.8587],\n",
      "        [0.9920]], device='mps:0')\n",
      "Iteration 41070 Training loss 0.04979930445551872 Validation loss 0.05973559990525246 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.7457],\n",
      "        [0.0150]], device='mps:0')\n",
      "Iteration 41080 Training loss 0.06073952093720436 Validation loss 0.05973277613520622 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.9421],\n",
      "        [0.5720]], device='mps:0')\n",
      "Iteration 41090 Training loss 0.05762914940714836 Validation loss 0.0598006546497345 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.0091],\n",
      "        [0.9440]], device='mps:0')\n",
      "Iteration 41100 Training loss 0.06478357315063477 Validation loss 0.05974017456173897 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.8825],\n",
      "        [0.5550]], device='mps:0')\n",
      "Iteration 41110 Training loss 0.06510839611291885 Validation loss 0.05976899340748787 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.9435],\n",
      "        [0.1258]], device='mps:0')\n",
      "Iteration 41120 Training loss 0.06742342561483383 Validation loss 0.05972609296441078 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.0558],\n",
      "        [0.0734]], device='mps:0')\n",
      "Iteration 41130 Training loss 0.05478675663471222 Validation loss 0.059741925448179245 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.8964],\n",
      "        [0.3888]], device='mps:0')\n",
      "Iteration 41140 Training loss 0.05102216452360153 Validation loss 0.05974347144365311 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.1348],\n",
      "        [0.9753]], device='mps:0')\n",
      "Iteration 41150 Training loss 0.0539177805185318 Validation loss 0.059783291071653366 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.0289],\n",
      "        [0.4177]], device='mps:0')\n",
      "Iteration 41160 Training loss 0.0574159175157547 Validation loss 0.05978549271821976 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.9267],\n",
      "        [0.2159]], device='mps:0')\n",
      "Iteration 41170 Training loss 0.04916317015886307 Validation loss 0.05973190441727638 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.8019],\n",
      "        [0.4945]], device='mps:0')\n",
      "Iteration 41180 Training loss 0.050567347556352615 Validation loss 0.059730008244514465 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.3403],\n",
      "        [0.6780]], device='mps:0')\n",
      "Iteration 41190 Training loss 0.053313247859478 Validation loss 0.0598023384809494 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.8767],\n",
      "        [0.9986]], device='mps:0')\n",
      "Iteration 41200 Training loss 0.059966932982206345 Validation loss 0.06000189855694771 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.0454],\n",
      "        [0.8236]], device='mps:0')\n",
      "Iteration 41210 Training loss 0.05151771008968353 Validation loss 0.05987800285220146 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.2242],\n",
      "        [0.1256]], device='mps:0')\n",
      "Iteration 41220 Training loss 0.050885360687971115 Validation loss 0.05972757562994957 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.8528],\n",
      "        [0.7659]], device='mps:0')\n",
      "Iteration 41230 Training loss 0.055714089423418045 Validation loss 0.05978913977742195 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.2992],\n",
      "        [0.9946]], device='mps:0')\n",
      "Iteration 41240 Training loss 0.05224725231528282 Validation loss 0.05969744175672531 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.0212],\n",
      "        [0.1708]], device='mps:0')\n",
      "Iteration 41250 Training loss 0.0676395520567894 Validation loss 0.05973048135638237 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.2162],\n",
      "        [0.9846]], device='mps:0')\n",
      "Iteration 41260 Training loss 0.06040161848068237 Validation loss 0.060020674020051956 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.1491],\n",
      "        [0.8810]], device='mps:0')\n",
      "Iteration 41270 Training loss 0.061490535736083984 Validation loss 0.05976030230522156 Accuracy 0.8348750472068787\n",
      "Output tensor([[0.3733],\n",
      "        [0.9854]], device='mps:0')\n",
      "Iteration 41280 Training loss 0.06124056130647659 Validation loss 0.059694018214941025 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.6929],\n",
      "        [0.3837]], device='mps:0')\n",
      "Iteration 41290 Training loss 0.06157948449254036 Validation loss 0.05972019210457802 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.2484],\n",
      "        [0.8901]], device='mps:0')\n",
      "Iteration 41300 Training loss 0.06215282157063484 Validation loss 0.059698279947042465 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.1902],\n",
      "        [0.0791]], device='mps:0')\n",
      "Iteration 41310 Training loss 0.0595012828707695 Validation loss 0.05968170613050461 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.8958],\n",
      "        [0.5445]], device='mps:0')\n",
      "Iteration 41320 Training loss 0.06338285654783249 Validation loss 0.059822384268045425 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.0891],\n",
      "        [0.8710]], device='mps:0')\n",
      "Iteration 41330 Training loss 0.05599815398454666 Validation loss 0.05972300097346306 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.0726],\n",
      "        [0.6749]], device='mps:0')\n",
      "Iteration 41340 Training loss 0.05066953971982002 Validation loss 0.059688303619623184 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.4194],\n",
      "        [0.2227]], device='mps:0')\n",
      "Iteration 41350 Training loss 0.06834646314382553 Validation loss 0.05972956493496895 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.8428],\n",
      "        [0.1858]], device='mps:0')\n",
      "Iteration 41360 Training loss 0.0614413358271122 Validation loss 0.059700679033994675 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.4633],\n",
      "        [0.4524]], device='mps:0')\n",
      "Iteration 41370 Training loss 0.057641882449388504 Validation loss 0.05972710996866226 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.9280],\n",
      "        [0.5408]], device='mps:0')\n",
      "Iteration 41380 Training loss 0.05814637616276741 Validation loss 0.0597897432744503 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.7580],\n",
      "        [0.1125]], device='mps:0')\n",
      "Iteration 41390 Training loss 0.05826965346932411 Validation loss 0.059726204723119736 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.5845],\n",
      "        [0.0736]], device='mps:0')\n",
      "Iteration 41400 Training loss 0.05650261789560318 Validation loss 0.05970575660467148 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.5794],\n",
      "        [0.2462]], device='mps:0')\n",
      "Iteration 41410 Training loss 0.0652274638414383 Validation loss 0.05977100878953934 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.8938],\n",
      "        [0.4535]], device='mps:0')\n",
      "Iteration 41420 Training loss 0.05926451459527016 Validation loss 0.059689637273550034 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.9893],\n",
      "        [0.9736]], device='mps:0')\n",
      "Iteration 41430 Training loss 0.050478171557188034 Validation loss 0.059821758419275284 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.0445],\n",
      "        [0.0150]], device='mps:0')\n",
      "Iteration 41440 Training loss 0.052545204758644104 Validation loss 0.0596981979906559 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.9723],\n",
      "        [0.5627]], device='mps:0')\n",
      "Iteration 41450 Training loss 0.057356156408786774 Validation loss 0.0596914067864418 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.0875],\n",
      "        [0.2426]], device='mps:0')\n",
      "Iteration 41460 Training loss 0.05663527920842171 Validation loss 0.05969160795211792 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.1009],\n",
      "        [0.6354]], device='mps:0')\n",
      "Iteration 41470 Training loss 0.059114713221788406 Validation loss 0.059722352772951126 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.9571],\n",
      "        [0.3027]], device='mps:0')\n",
      "Iteration 41480 Training loss 0.05224793776869774 Validation loss 0.06001719459891319 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.7126],\n",
      "        [0.9762]], device='mps:0')\n",
      "Iteration 41490 Training loss 0.05190132558345795 Validation loss 0.059911929070949554 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.1101],\n",
      "        [0.0176]], device='mps:0')\n",
      "Iteration 41500 Training loss 0.055590443313121796 Validation loss 0.059744302183389664 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.2994],\n",
      "        [0.9609]], device='mps:0')\n",
      "Iteration 41510 Training loss 0.06022460013628006 Validation loss 0.05977638438344002 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.9138],\n",
      "        [0.7309]], device='mps:0')\n",
      "Iteration 41520 Training loss 0.05435590445995331 Validation loss 0.05970161035656929 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.0765],\n",
      "        [0.1792]], device='mps:0')\n",
      "Iteration 41530 Training loss 0.06678904592990875 Validation loss 0.05979684367775917 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.4742],\n",
      "        [0.0569]], device='mps:0')\n",
      "Iteration 41540 Training loss 0.05981576815247536 Validation loss 0.0596964955329895 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.8223],\n",
      "        [0.9517]], device='mps:0')\n",
      "Iteration 41550 Training loss 0.05300236493349075 Validation loss 0.0597144216299057 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.9388],\n",
      "        [0.7430]], device='mps:0')\n",
      "Iteration 41560 Training loss 0.05632660165429115 Validation loss 0.05971701443195343 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.8231],\n",
      "        [0.4347]], device='mps:0')\n",
      "Iteration 41570 Training loss 0.0565396249294281 Validation loss 0.059899408370256424 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.6054],\n",
      "        [0.3252]], device='mps:0')\n",
      "Iteration 41580 Training loss 0.05294632166624069 Validation loss 0.0596834197640419 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.8072],\n",
      "        [0.0156]], device='mps:0')\n",
      "Iteration 41590 Training loss 0.052764568477869034 Validation loss 0.059689439833164215 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.2314],\n",
      "        [0.6064]], device='mps:0')\n",
      "Iteration 41600 Training loss 0.06289977580308914 Validation loss 0.059685904532670975 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.3016],\n",
      "        [0.0836]], device='mps:0')\n",
      "Iteration 41610 Training loss 0.0598219558596611 Validation loss 0.05973881110548973 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.9492],\n",
      "        [0.6514]], device='mps:0')\n",
      "Iteration 41620 Training loss 0.054146889597177505 Validation loss 0.059684235602617264 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.8835],\n",
      "        [0.9167]], device='mps:0')\n",
      "Iteration 41630 Training loss 0.04805893823504448 Validation loss 0.05975865572690964 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.9592],\n",
      "        [0.0905]], device='mps:0')\n",
      "Iteration 41640 Training loss 0.0569794736802578 Validation loss 0.05975569412112236 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.2393],\n",
      "        [0.1595]], device='mps:0')\n",
      "Iteration 41650 Training loss 0.053051967173814774 Validation loss 0.05987041816115379 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.2345],\n",
      "        [0.7288]], device='mps:0')\n",
      "Iteration 41660 Training loss 0.05418414995074272 Validation loss 0.05980443209409714 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.1212],\n",
      "        [0.4360]], device='mps:0')\n",
      "Iteration 41670 Training loss 0.050786349922418594 Validation loss 0.059679899364709854 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.0188],\n",
      "        [0.0919]], device='mps:0')\n",
      "Iteration 41680 Training loss 0.060128193348646164 Validation loss 0.059712186455726624 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.2888],\n",
      "        [0.5643]], device='mps:0')\n",
      "Iteration 41690 Training loss 0.063059963285923 Validation loss 0.05968335270881653 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.7702],\n",
      "        [0.8616]], device='mps:0')\n",
      "Iteration 41700 Training loss 0.048539917916059494 Validation loss 0.05969342216849327 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.4216],\n",
      "        [0.1516]], device='mps:0')\n",
      "Iteration 41710 Training loss 0.06002878397703171 Validation loss 0.059686169028282166 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.0657],\n",
      "        [0.1629]], device='mps:0')\n",
      "Iteration 41720 Training loss 0.05739637091755867 Validation loss 0.05969208851456642 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.8414],\n",
      "        [0.0628]], device='mps:0')\n",
      "Iteration 41730 Training loss 0.06867694854736328 Validation loss 0.05976654216647148 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.9845],\n",
      "        [0.1032]], device='mps:0')\n",
      "Iteration 41740 Training loss 0.06077570840716362 Validation loss 0.059767451137304306 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.0085],\n",
      "        [0.0292]], device='mps:0')\n",
      "Iteration 41750 Training loss 0.05943365767598152 Validation loss 0.05968492478132248 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.0981],\n",
      "        [0.9291]], device='mps:0')\n",
      "Iteration 41760 Training loss 0.054848626255989075 Validation loss 0.059797801077365875 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.8253],\n",
      "        [0.5342]], device='mps:0')\n",
      "Iteration 41770 Training loss 0.05833716318011284 Validation loss 0.05975678935647011 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.2197],\n",
      "        [0.4695]], device='mps:0')\n",
      "Iteration 41780 Training loss 0.055642928928136826 Validation loss 0.05967937782406807 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.3214],\n",
      "        [0.0864]], device='mps:0')\n",
      "Iteration 41790 Training loss 0.05607283115386963 Validation loss 0.059693772345781326 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.2383],\n",
      "        [0.7114]], device='mps:0')\n",
      "Iteration 41800 Training loss 0.06470228731632233 Validation loss 0.05976239591836929 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.1486],\n",
      "        [0.0905]], device='mps:0')\n",
      "Iteration 41810 Training loss 0.058754872530698776 Validation loss 0.05967443436384201 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.1840],\n",
      "        [0.2395]], device='mps:0')\n",
      "Iteration 41820 Training loss 0.0659467950463295 Validation loss 0.05966529995203018 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.0580],\n",
      "        [0.6886]], device='mps:0')\n",
      "Iteration 41830 Training loss 0.05774742364883423 Validation loss 0.059726037085056305 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.7351],\n",
      "        [0.7458]], device='mps:0')\n",
      "Iteration 41840 Training loss 0.0527956560254097 Validation loss 0.05972132831811905 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.6312],\n",
      "        [0.5862]], device='mps:0')\n",
      "Iteration 41850 Training loss 0.05828530341386795 Validation loss 0.059658437967300415 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.9605],\n",
      "        [0.0636]], device='mps:0')\n",
      "Iteration 41860 Training loss 0.052290137857198715 Validation loss 0.05999871715903282 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.8242],\n",
      "        [0.9074]], device='mps:0')\n",
      "Iteration 41870 Training loss 0.06165101006627083 Validation loss 0.05966935306787491 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.2193],\n",
      "        [0.5791]], device='mps:0')\n",
      "Iteration 41880 Training loss 0.07372196763753891 Validation loss 0.059753067791461945 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.2908],\n",
      "        [0.8603]], device='mps:0')\n",
      "Iteration 41890 Training loss 0.06656542420387268 Validation loss 0.05972835049033165 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.8600],\n",
      "        [0.5707]], device='mps:0')\n",
      "Iteration 41900 Training loss 0.06591399013996124 Validation loss 0.059667378664016724 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.7618],\n",
      "        [0.9477]], device='mps:0')\n",
      "Iteration 41910 Training loss 0.05806519463658333 Validation loss 0.05973467975854874 Accuracy 0.8355000615119934\n",
      "Output tensor([[0.9426],\n",
      "        [0.2359]], device='mps:0')\n",
      "Iteration 41920 Training loss 0.05590014159679413 Validation loss 0.059673961251974106 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.2006],\n",
      "        [0.8852]], device='mps:0')\n",
      "Iteration 41930 Training loss 0.056704264134168625 Validation loss 0.05966796725988388 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.3988],\n",
      "        [0.9861]], device='mps:0')\n",
      "Iteration 41940 Training loss 0.05489232763648033 Validation loss 0.059665095061063766 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.0245],\n",
      "        [0.0577]], device='mps:0')\n",
      "Iteration 41950 Training loss 0.05814797431230545 Validation loss 0.05973593890666962 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.0348],\n",
      "        [0.0869]], device='mps:0')\n",
      "Iteration 41960 Training loss 0.06046905368566513 Validation loss 0.05968358740210533 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.6664],\n",
      "        [0.7547]], device='mps:0')\n",
      "Iteration 41970 Training loss 0.062019847333431244 Validation loss 0.05968909338116646 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.0856],\n",
      "        [0.8026]], device='mps:0')\n",
      "Iteration 41980 Training loss 0.06029903143644333 Validation loss 0.05970068648457527 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.9668],\n",
      "        [0.9393]], device='mps:0')\n",
      "Iteration 41990 Training loss 0.056236401200294495 Validation loss 0.05968528985977173 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.0430],\n",
      "        [0.0470]], device='mps:0')\n",
      "Iteration 42000 Training loss 0.06272109597921371 Validation loss 0.059755899012088776 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.4161],\n",
      "        [0.0366]], device='mps:0')\n",
      "Iteration 42010 Training loss 0.05726469308137894 Validation loss 0.059652842581272125 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.2754],\n",
      "        [0.0184]], device='mps:0')\n",
      "Iteration 42020 Training loss 0.04859643056988716 Validation loss 0.059647709131240845 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.9776],\n",
      "        [0.6321]], device='mps:0')\n",
      "Iteration 42030 Training loss 0.060887668281793594 Validation loss 0.05965302139520645 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.0645],\n",
      "        [0.9762]], device='mps:0')\n",
      "Iteration 42040 Training loss 0.05884786322712898 Validation loss 0.05965699255466461 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.1168],\n",
      "        [0.1454]], device='mps:0')\n",
      "Iteration 42050 Training loss 0.06554102897644043 Validation loss 0.05967547744512558 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.5149],\n",
      "        [0.7176]], device='mps:0')\n",
      "Iteration 42060 Training loss 0.05392026901245117 Validation loss 0.05968650430440903 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.0563],\n",
      "        [0.0332]], device='mps:0')\n",
      "Iteration 42070 Training loss 0.05581188574433327 Validation loss 0.05964628979563713 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.8271],\n",
      "        [0.9030]], device='mps:0')\n",
      "Iteration 42080 Training loss 0.059673089534044266 Validation loss 0.05974571034312248 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.8548],\n",
      "        [0.9583]], device='mps:0')\n",
      "Iteration 42090 Training loss 0.05462450906634331 Validation loss 0.059731945395469666 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.9861],\n",
      "        [0.3819]], device='mps:0')\n",
      "Iteration 42100 Training loss 0.06133917346596718 Validation loss 0.059656187891960144 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.0396],\n",
      "        [0.2023]], device='mps:0')\n",
      "Iteration 42110 Training loss 0.0509815439581871 Validation loss 0.059661321341991425 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.8148],\n",
      "        [0.3361]], device='mps:0')\n",
      "Iteration 42120 Training loss 0.048232126981019974 Validation loss 0.059692416340112686 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.1906],\n",
      "        [0.0367]], device='mps:0')\n",
      "Iteration 42130 Training loss 0.054207395762205124 Validation loss 0.06005104258656502 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.8127],\n",
      "        [0.9467]], device='mps:0')\n",
      "Iteration 42140 Training loss 0.05374535918235779 Validation loss 0.05986523628234863 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.9858],\n",
      "        [0.5962]], device='mps:0')\n",
      "Iteration 42150 Training loss 0.05481851473450661 Validation loss 0.05977581441402435 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.9790],\n",
      "        [0.3773]], device='mps:0')\n",
      "Iteration 42160 Training loss 0.06319853663444519 Validation loss 0.05968737229704857 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.3343],\n",
      "        [0.8146]], device='mps:0')\n",
      "Iteration 42170 Training loss 0.05979032814502716 Validation loss 0.059738922864198685 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.4214],\n",
      "        [0.8897]], device='mps:0')\n",
      "Iteration 42180 Training loss 0.06172850355505943 Validation loss 0.05967128276824951 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.6045],\n",
      "        [0.3287]], device='mps:0')\n",
      "Iteration 42190 Training loss 0.05767010524868965 Validation loss 0.059656448662281036 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.0613],\n",
      "        [0.1295]], device='mps:0')\n",
      "Iteration 42200 Training loss 0.054754991084337234 Validation loss 0.060080066323280334 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.8205],\n",
      "        [0.8281]], device='mps:0')\n",
      "Iteration 42210 Training loss 0.05404853820800781 Validation loss 0.059673771262168884 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.0293],\n",
      "        [0.9672]], device='mps:0')\n",
      "Iteration 42220 Training loss 0.05990145727992058 Validation loss 0.059662871062755585 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.1600],\n",
      "        [0.0278]], device='mps:0')\n",
      "Iteration 42230 Training loss 0.05744314193725586 Validation loss 0.05963309109210968 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.3253],\n",
      "        [0.1493]], device='mps:0')\n",
      "Iteration 42240 Training loss 0.07223294675350189 Validation loss 0.05963299795985222 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.6290],\n",
      "        [0.5771]], device='mps:0')\n",
      "Iteration 42250 Training loss 0.05535310134291649 Validation loss 0.05967587232589722 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.0676],\n",
      "        [0.2427]], device='mps:0')\n",
      "Iteration 42260 Training loss 0.061251845210790634 Validation loss 0.05963001400232315 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.3806],\n",
      "        [0.0048]], device='mps:0')\n",
      "Iteration 42270 Training loss 0.06333605945110321 Validation loss 0.05962599813938141 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.9161],\n",
      "        [0.9470]], device='mps:0')\n",
      "Iteration 42280 Training loss 0.05365029349923134 Validation loss 0.059831731021404266 Accuracy 0.8353750109672546\n",
      "Output tensor([[0.2014],\n",
      "        [0.8475]], device='mps:0')\n",
      "Iteration 42290 Training loss 0.057361774146556854 Validation loss 0.05989375337958336 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.7879],\n",
      "        [0.3054]], device='mps:0')\n",
      "Iteration 42300 Training loss 0.05653217062354088 Validation loss 0.05975024029612541 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0074],\n",
      "        [0.1162]], device='mps:0')\n",
      "Iteration 42310 Training loss 0.05870182812213898 Validation loss 0.05963350459933281 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.3277],\n",
      "        [0.4966]], device='mps:0')\n",
      "Iteration 42320 Training loss 0.05404241383075714 Validation loss 0.05962664261460304 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.6443],\n",
      "        [0.9292]], device='mps:0')\n",
      "Iteration 42330 Training loss 0.056647464632987976 Validation loss 0.05962107330560684 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.4153],\n",
      "        [0.1545]], device='mps:0')\n",
      "Iteration 42340 Training loss 0.06330017000436783 Validation loss 0.05976678058505058 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.0583],\n",
      "        [0.0860]], device='mps:0')\n",
      "Iteration 42350 Training loss 0.051918838173151016 Validation loss 0.05964108556509018 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.6203],\n",
      "        [0.0308]], device='mps:0')\n",
      "Iteration 42360 Training loss 0.05932088568806648 Validation loss 0.05963022634387016 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.0286],\n",
      "        [0.5049]], device='mps:0')\n",
      "Iteration 42370 Training loss 0.06157032772898674 Validation loss 0.05965903028845787 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.9560],\n",
      "        [0.0518]], device='mps:0')\n",
      "Iteration 42380 Training loss 0.057079169899225235 Validation loss 0.059758350253105164 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.0747],\n",
      "        [0.5264]], device='mps:0')\n",
      "Iteration 42390 Training loss 0.046516302973032 Validation loss 0.05964398384094238 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.6398],\n",
      "        [0.0192]], device='mps:0')\n",
      "Iteration 42400 Training loss 0.06541897356510162 Validation loss 0.05962556228041649 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.8209],\n",
      "        [0.6998]], device='mps:0')\n",
      "Iteration 42410 Training loss 0.055183734744787216 Validation loss 0.05969060957431793 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.6578],\n",
      "        [0.0027]], device='mps:0')\n",
      "Iteration 42420 Training loss 0.06349866837263107 Validation loss 0.059659820050001144 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.9776],\n",
      "        [0.1670]], device='mps:0')\n",
      "Iteration 42430 Training loss 0.055394843220710754 Validation loss 0.05993010103702545 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.5686],\n",
      "        [0.0683]], device='mps:0')\n",
      "Iteration 42440 Training loss 0.051759690046310425 Validation loss 0.05960521474480629 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.6588],\n",
      "        [0.8495]], device='mps:0')\n",
      "Iteration 42450 Training loss 0.06434954702854156 Validation loss 0.059698618948459625 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.9397],\n",
      "        [0.6628]], device='mps:0')\n",
      "Iteration 42460 Training loss 0.057668525725603104 Validation loss 0.059625089168548584 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.6254],\n",
      "        [0.9927]], device='mps:0')\n",
      "Iteration 42470 Training loss 0.05946040526032448 Validation loss 0.05960201844573021 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.8103],\n",
      "        [0.7594]], device='mps:0')\n",
      "Iteration 42480 Training loss 0.054270461201667786 Validation loss 0.05967766046524048 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.8534],\n",
      "        [0.2822]], device='mps:0')\n",
      "Iteration 42490 Training loss 0.05447826907038689 Validation loss 0.05964936316013336 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.8780],\n",
      "        [0.0489]], device='mps:0')\n",
      "Iteration 42500 Training loss 0.05719415843486786 Validation loss 0.05972263216972351 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.9058],\n",
      "        [0.9751]], device='mps:0')\n",
      "Iteration 42510 Training loss 0.06082744523882866 Validation loss 0.05959991738200188 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.4806],\n",
      "        [0.7077]], device='mps:0')\n",
      "Iteration 42520 Training loss 0.056368064135313034 Validation loss 0.05959851294755936 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.0181],\n",
      "        [0.1218]], device='mps:0')\n",
      "Iteration 42530 Training loss 0.04711903631687164 Validation loss 0.059593845158815384 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.8588],\n",
      "        [0.0482]], device='mps:0')\n",
      "Iteration 42540 Training loss 0.0683136135339737 Validation loss 0.05962001532316208 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.1484],\n",
      "        [0.3419]], device='mps:0')\n",
      "Iteration 42550 Training loss 0.059970468282699585 Validation loss 0.059682756662368774 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.9567],\n",
      "        [0.2334]], device='mps:0')\n",
      "Iteration 42560 Training loss 0.0528835766017437 Validation loss 0.059620097279548645 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.1490],\n",
      "        [0.7294]], device='mps:0')\n",
      "Iteration 42570 Training loss 0.051929276436567307 Validation loss 0.059597816318273544 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.7138],\n",
      "        [0.1963]], device='mps:0')\n",
      "Iteration 42580 Training loss 0.057889923453330994 Validation loss 0.05959181487560272 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.8134],\n",
      "        [0.1534]], device='mps:0')\n",
      "Iteration 42590 Training loss 0.0532914437353611 Validation loss 0.0596587210893631 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.1759],\n",
      "        [0.8349]], device='mps:0')\n",
      "Iteration 42600 Training loss 0.054009560495615005 Validation loss 0.05983256548643112 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.0236],\n",
      "        [0.9247]], device='mps:0')\n",
      "Iteration 42610 Training loss 0.051886700093746185 Validation loss 0.05970793217420578 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.3995],\n",
      "        [0.1580]], device='mps:0')\n",
      "Iteration 42620 Training loss 0.04773850366473198 Validation loss 0.05968179181218147 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.9451],\n",
      "        [0.0473]], device='mps:0')\n",
      "Iteration 42630 Training loss 0.06114276126027107 Validation loss 0.05959950387477875 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.0887],\n",
      "        [0.9752]], device='mps:0')\n",
      "Iteration 42640 Training loss 0.05455564707517624 Validation loss 0.05958566814661026 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.2453],\n",
      "        [0.1515]], device='mps:0')\n",
      "Iteration 42650 Training loss 0.06200949475169182 Validation loss 0.0596272237598896 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.9673],\n",
      "        [0.0061]], device='mps:0')\n",
      "Iteration 42660 Training loss 0.056150808930397034 Validation loss 0.0595954991877079 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.0031],\n",
      "        [0.9632]], device='mps:0')\n",
      "Iteration 42670 Training loss 0.0644952729344368 Validation loss 0.05958332493901253 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.9407],\n",
      "        [0.1941]], device='mps:0')\n",
      "Iteration 42680 Training loss 0.06213972344994545 Validation loss 0.0596005953848362 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.9525],\n",
      "        [0.9382]], device='mps:0')\n",
      "Iteration 42690 Training loss 0.05531378462910652 Validation loss 0.059801213443279266 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.5565],\n",
      "        [0.4987]], device='mps:0')\n",
      "Iteration 42700 Training loss 0.060186948627233505 Validation loss 0.059778276830911636 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.9585],\n",
      "        [0.1173]], device='mps:0')\n",
      "Iteration 42710 Training loss 0.05653594806790352 Validation loss 0.05973658338189125 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0418],\n",
      "        [0.0947]], device='mps:0')\n",
      "Iteration 42720 Training loss 0.055234167724847794 Validation loss 0.05959262326359749 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.0539],\n",
      "        [0.7470]], device='mps:0')\n",
      "Iteration 42730 Training loss 0.05761377513408661 Validation loss 0.0596470832824707 Accuracy 0.8357500433921814\n",
      "Output tensor([[0.0936],\n",
      "        [0.7636]], device='mps:0')\n",
      "Iteration 42740 Training loss 0.05785951390862465 Validation loss 0.059668004512786865 Accuracy 0.8351250290870667\n",
      "Output tensor([[0.0118],\n",
      "        [0.9427]], device='mps:0')\n",
      "Iteration 42750 Training loss 0.05262104049324989 Validation loss 0.05963878706097603 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.6315],\n",
      "        [0.5442]], device='mps:0')\n",
      "Iteration 42760 Training loss 0.06169489398598671 Validation loss 0.0595836415886879 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.0877],\n",
      "        [0.7958]], device='mps:0')\n",
      "Iteration 42770 Training loss 0.06053629890084267 Validation loss 0.05981914699077606 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.5486],\n",
      "        [0.2524]], device='mps:0')\n",
      "Iteration 42780 Training loss 0.05870378762483597 Validation loss 0.059573374688625336 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.0861],\n",
      "        [0.7933]], device='mps:0')\n",
      "Iteration 42790 Training loss 0.057156410068273544 Validation loss 0.0595778152346611 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.4738],\n",
      "        [0.0617]], device='mps:0')\n",
      "Iteration 42800 Training loss 0.05833079293370247 Validation loss 0.05957531929016113 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.9415],\n",
      "        [0.7457]], device='mps:0')\n",
      "Iteration 42810 Training loss 0.05986221134662628 Validation loss 0.05973616987466812 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.9833],\n",
      "        [0.0438]], device='mps:0')\n",
      "Iteration 42820 Training loss 0.05564551055431366 Validation loss 0.05957808345556259 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.1858],\n",
      "        [0.5635]], device='mps:0')\n",
      "Iteration 42830 Training loss 0.04799097403883934 Validation loss 0.059565022587776184 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.1097],\n",
      "        [0.0154]], device='mps:0')\n",
      "Iteration 42840 Training loss 0.06720437109470367 Validation loss 0.05957309156656265 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9407],\n",
      "        [0.9811]], device='mps:0')\n",
      "Iteration 42850 Training loss 0.06007571518421173 Validation loss 0.05958465114235878 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.8325],\n",
      "        [0.0571]], device='mps:0')\n",
      "Iteration 42860 Training loss 0.05689788609743118 Validation loss 0.05976317450404167 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.0549],\n",
      "        [0.4242]], device='mps:0')\n",
      "Iteration 42870 Training loss 0.0614265538752079 Validation loss 0.05959362909197807 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.1920],\n",
      "        [0.1572]], device='mps:0')\n",
      "Iteration 42880 Training loss 0.05686026066541672 Validation loss 0.05959713086485863 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.9687],\n",
      "        [0.9424]], device='mps:0')\n",
      "Iteration 42890 Training loss 0.0562242791056633 Validation loss 0.06006331741809845 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.1487],\n",
      "        [0.6501]], device='mps:0')\n",
      "Iteration 42900 Training loss 0.05903978273272514 Validation loss 0.059551652520895004 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.7564],\n",
      "        [0.9479]], device='mps:0')\n",
      "Iteration 42910 Training loss 0.058459069579839706 Validation loss 0.0596671886742115 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.9267],\n",
      "        [0.9431]], device='mps:0')\n",
      "Iteration 42920 Training loss 0.05072580277919769 Validation loss 0.05956325680017471 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.7764],\n",
      "        [0.2621]], device='mps:0')\n",
      "Iteration 42930 Training loss 0.06372686475515366 Validation loss 0.05960572510957718 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.2417],\n",
      "        [0.1008]], device='mps:0')\n",
      "Iteration 42940 Training loss 0.06116493046283722 Validation loss 0.059784743934869766 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.2454],\n",
      "        [0.4213]], device='mps:0')\n",
      "Iteration 42950 Training loss 0.053692977875471115 Validation loss 0.05955127999186516 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.0184],\n",
      "        [0.1032]], device='mps:0')\n",
      "Iteration 42960 Training loss 0.05806782469153404 Validation loss 0.05968962982296944 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.9153],\n",
      "        [0.7121]], device='mps:0')\n",
      "Iteration 42970 Training loss 0.053363531827926636 Validation loss 0.05954732000827789 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.8502],\n",
      "        [0.0290]], device='mps:0')\n",
      "Iteration 42980 Training loss 0.05823079124093056 Validation loss 0.059533532708883286 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.4199],\n",
      "        [0.9873]], device='mps:0')\n",
      "Iteration 42990 Training loss 0.05726826936006546 Validation loss 0.059534166008234024 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.0376],\n",
      "        [0.9097]], device='mps:0')\n",
      "Iteration 43000 Training loss 0.05772664025425911 Validation loss 0.059538185596466064 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.7401],\n",
      "        [0.1469]], device='mps:0')\n",
      "Iteration 43010 Training loss 0.05544842779636383 Validation loss 0.05953362211585045 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.9688],\n",
      "        [0.1299]], device='mps:0')\n",
      "Iteration 43020 Training loss 0.0607789009809494 Validation loss 0.059535276144742966 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.8639],\n",
      "        [0.9241]], device='mps:0')\n",
      "Iteration 43030 Training loss 0.06907034665346146 Validation loss 0.059616703540086746 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.1575],\n",
      "        [0.1011]], device='mps:0')\n",
      "Iteration 43040 Training loss 0.05351247638463974 Validation loss 0.05953311175107956 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.2623],\n",
      "        [0.1588]], device='mps:0')\n",
      "Iteration 43050 Training loss 0.056381598114967346 Validation loss 0.059532105922698975 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.1776],\n",
      "        [0.8596]], device='mps:0')\n",
      "Iteration 43060 Training loss 0.04715351015329361 Validation loss 0.05960577726364136 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.5965],\n",
      "        [0.9407]], device='mps:0')\n",
      "Iteration 43070 Training loss 0.058281686156988144 Validation loss 0.05963268503546715 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.1313],\n",
      "        [0.4488]], device='mps:0')\n",
      "Iteration 43080 Training loss 0.06121265888214111 Validation loss 0.059543997049331665 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.4650],\n",
      "        [0.0482]], device='mps:0')\n",
      "Iteration 43090 Training loss 0.05507959425449371 Validation loss 0.059560637921094894 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.3318],\n",
      "        [0.7911]], device='mps:0')\n",
      "Iteration 43100 Training loss 0.057967398315668106 Validation loss 0.05974065512418747 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.1795],\n",
      "        [0.8680]], device='mps:0')\n",
      "Iteration 43110 Training loss 0.05456477403640747 Validation loss 0.059651538729667664 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.9211],\n",
      "        [0.5269]], device='mps:0')\n",
      "Iteration 43120 Training loss 0.06518334895372391 Validation loss 0.059556618332862854 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.3243],\n",
      "        [0.6422]], device='mps:0')\n",
      "Iteration 43130 Training loss 0.0505123995244503 Validation loss 0.05953396111726761 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.3688],\n",
      "        [0.8832]], device='mps:0')\n",
      "Iteration 43140 Training loss 0.06029575318098068 Validation loss 0.0595240592956543 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.9816],\n",
      "        [0.2481]], device='mps:0')\n",
      "Iteration 43150 Training loss 0.061040084809064865 Validation loss 0.05957978218793869 Accuracy 0.8356250524520874\n",
      "Output tensor([[0.9810],\n",
      "        [0.1312]], device='mps:0')\n",
      "Iteration 43160 Training loss 0.05542054399847984 Validation loss 0.05958496779203415 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.8121],\n",
      "        [0.9084]], device='mps:0')\n",
      "Iteration 43170 Training loss 0.06000587344169617 Validation loss 0.05953250452876091 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.8095],\n",
      "        [0.6692]], device='mps:0')\n",
      "Iteration 43180 Training loss 0.055168308317661285 Validation loss 0.05953745171427727 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.8350],\n",
      "        [0.6114]], device='mps:0')\n",
      "Iteration 43190 Training loss 0.05482703074812889 Validation loss 0.05953744426369667 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.3619],\n",
      "        [0.8027]], device='mps:0')\n",
      "Iteration 43200 Training loss 0.06329565495252609 Validation loss 0.05953580141067505 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.9318],\n",
      "        [0.0470]], device='mps:0')\n",
      "Iteration 43210 Training loss 0.0499054454267025 Validation loss 0.05956999957561493 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.0700],\n",
      "        [0.5319]], device='mps:0')\n",
      "Iteration 43220 Training loss 0.06812498718500137 Validation loss 0.05958590283989906 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.8344],\n",
      "        [0.8228]], device='mps:0')\n",
      "Iteration 43230 Training loss 0.0625871941447258 Validation loss 0.05963825434446335 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.8762],\n",
      "        [0.8869]], device='mps:0')\n",
      "Iteration 43240 Training loss 0.049169547855854034 Validation loss 0.05953504517674446 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.0814],\n",
      "        [0.9686]], device='mps:0')\n",
      "Iteration 43250 Training loss 0.05689449608325958 Validation loss 0.059539295732975006 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.7122],\n",
      "        [0.0544]], device='mps:0')\n",
      "Iteration 43260 Training loss 0.051991451531648636 Validation loss 0.059617556631565094 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.7420],\n",
      "        [0.9931]], device='mps:0')\n",
      "Iteration 43270 Training loss 0.04762394726276398 Validation loss 0.05962499603629112 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.6882],\n",
      "        [0.9928]], device='mps:0')\n",
      "Iteration 43280 Training loss 0.05663396790623665 Validation loss 0.05954824760556221 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.4955],\n",
      "        [0.4182]], device='mps:0')\n",
      "Iteration 43290 Training loss 0.06373357027769089 Validation loss 0.0595419704914093 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.9516],\n",
      "        [0.1011]], device='mps:0')\n",
      "Iteration 43300 Training loss 0.0517946295440197 Validation loss 0.059592802077531815 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.3825],\n",
      "        [0.2466]], device='mps:0')\n",
      "Iteration 43310 Training loss 0.05855870991945267 Validation loss 0.05952482298016548 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.1600],\n",
      "        [0.1598]], device='mps:0')\n",
      "Iteration 43320 Training loss 0.058728259056806564 Validation loss 0.05956323817372322 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.6554],\n",
      "        [0.0285]], device='mps:0')\n",
      "Iteration 43330 Training loss 0.05634523183107376 Validation loss 0.05956098064780235 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.6845],\n",
      "        [0.8603]], device='mps:0')\n",
      "Iteration 43340 Training loss 0.05059757083654404 Validation loss 0.05976118519902229 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.3101],\n",
      "        [0.0482]], device='mps:0')\n",
      "Iteration 43350 Training loss 0.04729657992720604 Validation loss 0.05951264873147011 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.9480],\n",
      "        [0.2966]], device='mps:0')\n",
      "Iteration 43360 Training loss 0.04934109002351761 Validation loss 0.059595655649900436 Accuracy 0.8361250162124634\n",
      "Output tensor([[0.0425],\n",
      "        [0.9615]], device='mps:0')\n",
      "Iteration 43370 Training loss 0.06588515639305115 Validation loss 0.059627581387758255 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.6997],\n",
      "        [0.0885]], device='mps:0')\n",
      "Iteration 43380 Training loss 0.057135555893182755 Validation loss 0.059532783925533295 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.0451],\n",
      "        [0.4325]], device='mps:0')\n",
      "Iteration 43390 Training loss 0.05970119684934616 Validation loss 0.05951680988073349 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.7650],\n",
      "        [0.0958]], device='mps:0')\n",
      "Iteration 43400 Training loss 0.05513044819235802 Validation loss 0.05951140075922012 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.6608],\n",
      "        [0.9139]], device='mps:0')\n",
      "Iteration 43410 Training loss 0.054472796618938446 Validation loss 0.0596042200922966 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.5978],\n",
      "        [0.2085]], device='mps:0')\n",
      "Iteration 43420 Training loss 0.0493566170334816 Validation loss 0.05958172306418419 Accuracy 0.8360000252723694\n",
      "Output tensor([[0.0173],\n",
      "        [0.1167]], device='mps:0')\n",
      "Iteration 43430 Training loss 0.05341264605522156 Validation loss 0.059508319944143295 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.7938],\n",
      "        [0.9544]], device='mps:0')\n",
      "Iteration 43440 Training loss 0.05679631233215332 Validation loss 0.059508007019758224 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.5579],\n",
      "        [0.3195]], device='mps:0')\n",
      "Iteration 43450 Training loss 0.05485575646162033 Validation loss 0.059534527361392975 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.8108],\n",
      "        [0.0836]], device='mps:0')\n",
      "Iteration 43460 Training loss 0.05726845562458038 Validation loss 0.05950590968132019 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.9541],\n",
      "        [0.8274]], device='mps:0')\n",
      "Iteration 43470 Training loss 0.05829169973731041 Validation loss 0.05966297537088394 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.9378],\n",
      "        [0.9406]], device='mps:0')\n",
      "Iteration 43480 Training loss 0.055329859256744385 Validation loss 0.05958501249551773 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.9831],\n",
      "        [0.9073]], device='mps:0')\n",
      "Iteration 43490 Training loss 0.06163029372692108 Validation loss 0.05984135717153549 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.7650],\n",
      "        [0.1330]], device='mps:0')\n",
      "Iteration 43500 Training loss 0.05528310686349869 Validation loss 0.0596272349357605 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.9655],\n",
      "        [0.8466]], device='mps:0')\n",
      "Iteration 43510 Training loss 0.06322620064020157 Validation loss 0.05956359580159187 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.6050],\n",
      "        [0.9691]], device='mps:0')\n",
      "Iteration 43520 Training loss 0.06457649171352386 Validation loss 0.05950969085097313 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.4514],\n",
      "        [0.8963]], device='mps:0')\n",
      "Iteration 43530 Training loss 0.052677497267723083 Validation loss 0.05954725295305252 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.2270],\n",
      "        [0.0800]], device='mps:0')\n",
      "Iteration 43540 Training loss 0.056379470974206924 Validation loss 0.05949282646179199 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.0955],\n",
      "        [0.0775]], device='mps:0')\n",
      "Iteration 43550 Training loss 0.05497518926858902 Validation loss 0.059487104415893555 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.2414],\n",
      "        [0.1227]], device='mps:0')\n",
      "Iteration 43560 Training loss 0.0731775239109993 Validation loss 0.05950438976287842 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.3970],\n",
      "        [0.0529]], device='mps:0')\n",
      "Iteration 43570 Training loss 0.07090820372104645 Validation loss 0.05949975922703743 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.7118],\n",
      "        [0.9757]], device='mps:0')\n",
      "Iteration 43580 Training loss 0.05555136129260063 Validation loss 0.05953098088502884 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.5345],\n",
      "        [0.0367]], device='mps:0')\n",
      "Iteration 43590 Training loss 0.059702642261981964 Validation loss 0.059646300971508026 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.8936],\n",
      "        [0.6817]], device='mps:0')\n",
      "Iteration 43600 Training loss 0.058756694197654724 Validation loss 0.059492334723472595 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9806],\n",
      "        [0.0242]], device='mps:0')\n",
      "Iteration 43610 Training loss 0.058880843222141266 Validation loss 0.05949633941054344 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.8349],\n",
      "        [0.9640]], device='mps:0')\n",
      "Iteration 43620 Training loss 0.05370332673192024 Validation loss 0.05952788516879082 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.0127],\n",
      "        [0.3512]], device='mps:0')\n",
      "Iteration 43630 Training loss 0.05991853028535843 Validation loss 0.05960243567824364 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.9472],\n",
      "        [0.1572]], device='mps:0')\n",
      "Iteration 43640 Training loss 0.06274083256721497 Validation loss 0.059655461460351944 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.1037],\n",
      "        [0.8303]], device='mps:0')\n",
      "Iteration 43650 Training loss 0.056406419724226 Validation loss 0.059477973729372025 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.2031],\n",
      "        [0.9115]], device='mps:0')\n",
      "Iteration 43660 Training loss 0.060282591730356216 Validation loss 0.059478510171175 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.9147],\n",
      "        [0.1353]], device='mps:0')\n",
      "Iteration 43670 Training loss 0.05422248691320419 Validation loss 0.05954738333821297 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.9426],\n",
      "        [0.0279]], device='mps:0')\n",
      "Iteration 43680 Training loss 0.06024737283587456 Validation loss 0.05964800342917442 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.0263],\n",
      "        [0.8795]], device='mps:0')\n",
      "Iteration 43690 Training loss 0.06620924174785614 Validation loss 0.059486016631126404 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.9949],\n",
      "        [0.9187]], device='mps:0')\n",
      "Iteration 43700 Training loss 0.055802568793296814 Validation loss 0.05954287201166153 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.9302],\n",
      "        [0.0122]], device='mps:0')\n",
      "Iteration 43710 Training loss 0.05474080145359039 Validation loss 0.0594799667596817 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.0139],\n",
      "        [0.9559]], device='mps:0')\n",
      "Iteration 43720 Training loss 0.05558197945356369 Validation loss 0.05951010063290596 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.3331],\n",
      "        [0.6316]], device='mps:0')\n",
      "Iteration 43730 Training loss 0.061596110463142395 Validation loss 0.05963393300771713 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.7142],\n",
      "        [0.0612]], device='mps:0')\n",
      "Iteration 43740 Training loss 0.05865898355841637 Validation loss 0.05948134511709213 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.7412],\n",
      "        [0.1201]], device='mps:0')\n",
      "Iteration 43750 Training loss 0.05874096602201462 Validation loss 0.05948740243911743 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0890],\n",
      "        [0.9212]], device='mps:0')\n",
      "Iteration 43760 Training loss 0.059039048850536346 Validation loss 0.059597667306661606 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0435],\n",
      "        [0.0811]], device='mps:0')\n",
      "Iteration 43770 Training loss 0.06375216692686081 Validation loss 0.05947771295905113 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.9575],\n",
      "        [0.2075]], device='mps:0')\n",
      "Iteration 43780 Training loss 0.06107305362820625 Validation loss 0.05955486744642258 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.9897],\n",
      "        [0.8203]], device='mps:0')\n",
      "Iteration 43790 Training loss 0.05541093647480011 Validation loss 0.05968007072806358 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.9645],\n",
      "        [0.0019]], device='mps:0')\n",
      "Iteration 43800 Training loss 0.05789215862751007 Validation loss 0.059504177421331406 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.2780],\n",
      "        [0.9770]], device='mps:0')\n",
      "Iteration 43810 Training loss 0.0524020716547966 Validation loss 0.0594656839966774 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0093],\n",
      "        [0.0761]], device='mps:0')\n",
      "Iteration 43820 Training loss 0.06904538720846176 Validation loss 0.05947813764214516 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.6310],\n",
      "        [0.1720]], device='mps:0')\n",
      "Iteration 43830 Training loss 0.05618331953883171 Validation loss 0.0595625638961792 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.1872],\n",
      "        [0.6709]], device='mps:0')\n",
      "Iteration 43840 Training loss 0.06588414311408997 Validation loss 0.05955301970243454 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.9507],\n",
      "        [0.0696]], device='mps:0')\n",
      "Iteration 43850 Training loss 0.059937696903944016 Validation loss 0.05978576838970184 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.8527],\n",
      "        [0.0947]], device='mps:0')\n",
      "Iteration 43860 Training loss 0.05774411931633949 Validation loss 0.05947057902812958 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.1531],\n",
      "        [0.7749]], device='mps:0')\n",
      "Iteration 43870 Training loss 0.052542198449373245 Validation loss 0.05946268141269684 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.6202],\n",
      "        [0.9535]], device='mps:0')\n",
      "Iteration 43880 Training loss 0.05687462538480759 Validation loss 0.05946031212806702 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.9899],\n",
      "        [0.5117]], device='mps:0')\n",
      "Iteration 43890 Training loss 0.06132431700825691 Validation loss 0.05965764820575714 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.2301],\n",
      "        [0.6769]], device='mps:0')\n",
      "Iteration 43900 Training loss 0.0543898344039917 Validation loss 0.059462837874889374 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.9646],\n",
      "        [0.7939]], device='mps:0')\n",
      "Iteration 43910 Training loss 0.06907443702220917 Validation loss 0.05949817970395088 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.1433],\n",
      "        [0.8840]], device='mps:0')\n",
      "Iteration 43920 Training loss 0.05719338729977608 Validation loss 0.059492506086826324 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0827],\n",
      "        [0.7982]], device='mps:0')\n",
      "Iteration 43930 Training loss 0.05226494371891022 Validation loss 0.05946074798703194 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.7590],\n",
      "        [0.8642]], device='mps:0')\n",
      "Iteration 43940 Training loss 0.05843151733279228 Validation loss 0.05946881324052811 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.8754],\n",
      "        [0.2547]], device='mps:0')\n",
      "Iteration 43950 Training loss 0.05556431785225868 Validation loss 0.05962779372930527 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.2448],\n",
      "        [0.0055]], device='mps:0')\n",
      "Iteration 43960 Training loss 0.05109574273228645 Validation loss 0.05945291742682457 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.2357],\n",
      "        [0.0439]], device='mps:0')\n",
      "Iteration 43970 Training loss 0.06222592666745186 Validation loss 0.060333527624607086 Accuracy 0.8352500200271606\n",
      "Output tensor([[0.9367],\n",
      "        [0.8526]], device='mps:0')\n",
      "Iteration 43980 Training loss 0.057355135679244995 Validation loss 0.05947529152035713 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.0789],\n",
      "        [0.3443]], device='mps:0')\n",
      "Iteration 43990 Training loss 0.05975281074643135 Validation loss 0.05945388972759247 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.6121],\n",
      "        [0.1587]], device='mps:0')\n",
      "Iteration 44000 Training loss 0.054036181420087814 Validation loss 0.05944770947098732 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0953],\n",
      "        [0.6697]], device='mps:0')\n",
      "Iteration 44010 Training loss 0.04634172469377518 Validation loss 0.05945298820734024 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.6163],\n",
      "        [0.3765]], device='mps:0')\n",
      "Iteration 44020 Training loss 0.057800207287073135 Validation loss 0.059460803866386414 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.5126],\n",
      "        [0.2361]], device='mps:0')\n",
      "Iteration 44030 Training loss 0.058982525020837784 Validation loss 0.05949585884809494 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.3693],\n",
      "        [0.9832]], device='mps:0')\n",
      "Iteration 44040 Training loss 0.058965861797332764 Validation loss 0.059452272951602936 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.4359],\n",
      "        [0.9866]], device='mps:0')\n",
      "Iteration 44050 Training loss 0.05195441097021103 Validation loss 0.05948042869567871 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.0199],\n",
      "        [0.7221]], device='mps:0')\n",
      "Iteration 44060 Training loss 0.055889979004859924 Validation loss 0.05949437990784645 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.5435],\n",
      "        [0.0898]], device='mps:0')\n",
      "Iteration 44070 Training loss 0.05683770030736923 Validation loss 0.05946030840277672 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.2642],\n",
      "        [0.5255]], device='mps:0')\n",
      "Iteration 44080 Training loss 0.0526944138109684 Validation loss 0.05946573615074158 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.0150],\n",
      "        [0.0180]], device='mps:0')\n",
      "Iteration 44090 Training loss 0.05529982969164848 Validation loss 0.059451960027217865 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0191],\n",
      "        [0.7275]], device='mps:0')\n",
      "Iteration 44100 Training loss 0.05897725373506546 Validation loss 0.05949518084526062 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.2090],\n",
      "        [0.1025]], device='mps:0')\n",
      "Iteration 44110 Training loss 0.053864944726228714 Validation loss 0.059470564126968384 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.9069],\n",
      "        [0.8990]], device='mps:0')\n",
      "Iteration 44120 Training loss 0.05709699168801308 Validation loss 0.059495799243450165 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.8942],\n",
      "        [0.2480]], device='mps:0')\n",
      "Iteration 44130 Training loss 0.0594058595597744 Validation loss 0.05946074798703194 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.6217],\n",
      "        [0.2000]], device='mps:0')\n",
      "Iteration 44140 Training loss 0.062109775841236115 Validation loss 0.05989246815443039 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.9705],\n",
      "        [0.5833]], device='mps:0')\n",
      "Iteration 44150 Training loss 0.06053146719932556 Validation loss 0.059453852474689484 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.9819],\n",
      "        [0.4597]], device='mps:0')\n",
      "Iteration 44160 Training loss 0.05172552913427353 Validation loss 0.059459879994392395 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.0111],\n",
      "        [0.5399]], device='mps:0')\n",
      "Iteration 44170 Training loss 0.05484181270003319 Validation loss 0.05946164205670357 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.7891],\n",
      "        [0.9335]], device='mps:0')\n",
      "Iteration 44180 Training loss 0.059688031673431396 Validation loss 0.059656981378793716 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.9205],\n",
      "        [0.2790]], device='mps:0')\n",
      "Iteration 44190 Training loss 0.0631275549530983 Validation loss 0.05947037413716316 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.9634],\n",
      "        [0.1137]], device='mps:0')\n",
      "Iteration 44200 Training loss 0.05379660800099373 Validation loss 0.059444572776556015 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.7158],\n",
      "        [0.9647]], device='mps:0')\n",
      "Iteration 44210 Training loss 0.056565940380096436 Validation loss 0.059447672218084335 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9908],\n",
      "        [0.9756]], device='mps:0')\n",
      "Iteration 44220 Training loss 0.05703231319785118 Validation loss 0.059450969099998474 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0227],\n",
      "        [0.7272]], device='mps:0')\n",
      "Iteration 44230 Training loss 0.048031628131866455 Validation loss 0.05948704108595848 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.0581],\n",
      "        [0.8550]], device='mps:0')\n",
      "Iteration 44240 Training loss 0.0488731749355793 Validation loss 0.05957493931055069 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.9506],\n",
      "        [0.3518]], device='mps:0')\n",
      "Iteration 44250 Training loss 0.05265626683831215 Validation loss 0.059478480368852615 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.2574],\n",
      "        [0.1498]], device='mps:0')\n",
      "Iteration 44260 Training loss 0.05844176933169365 Validation loss 0.05948486551642418 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.1068],\n",
      "        [0.4931]], device='mps:0')\n",
      "Iteration 44270 Training loss 0.054967835545539856 Validation loss 0.05962483584880829 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.8667],\n",
      "        [0.2921]], device='mps:0')\n",
      "Iteration 44280 Training loss 0.06177586317062378 Validation loss 0.059441644698381424 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.7693],\n",
      "        [0.0970]], device='mps:0')\n",
      "Iteration 44290 Training loss 0.05414629727602005 Validation loss 0.05944488197565079 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.6729],\n",
      "        [0.9736]], device='mps:0')\n",
      "Iteration 44300 Training loss 0.05713171511888504 Validation loss 0.05969758331775665 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.4775],\n",
      "        [0.1429]], device='mps:0')\n",
      "Iteration 44310 Training loss 0.0575016587972641 Validation loss 0.05944270268082619 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.9842],\n",
      "        [0.8349]], device='mps:0')\n",
      "Iteration 44320 Training loss 0.05672908574342728 Validation loss 0.05943822115659714 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.9915],\n",
      "        [0.0289]], device='mps:0')\n",
      "Iteration 44330 Training loss 0.049011848866939545 Validation loss 0.05950019136071205 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.0823],\n",
      "        [0.9016]], device='mps:0')\n",
      "Iteration 44340 Training loss 0.0571058914065361 Validation loss 0.059445999562740326 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.1676],\n",
      "        [0.9549]], device='mps:0')\n",
      "Iteration 44350 Training loss 0.05698959156870842 Validation loss 0.05944598838686943 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.6090],\n",
      "        [0.8855]], device='mps:0')\n",
      "Iteration 44360 Training loss 0.051390618085861206 Validation loss 0.059447962790727615 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.1578],\n",
      "        [0.0607]], device='mps:0')\n",
      "Iteration 44370 Training loss 0.06175868958234787 Validation loss 0.05944867059588432 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9339],\n",
      "        [0.4817]], device='mps:0')\n",
      "Iteration 44380 Training loss 0.06345026940107346 Validation loss 0.059484098106622696 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.8545],\n",
      "        [0.5682]], device='mps:0')\n",
      "Iteration 44390 Training loss 0.057489145547151566 Validation loss 0.05950497090816498 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.9655],\n",
      "        [0.2894]], device='mps:0')\n",
      "Iteration 44400 Training loss 0.0622289702296257 Validation loss 0.05944935977458954 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0862],\n",
      "        [0.8681]], device='mps:0')\n",
      "Iteration 44410 Training loss 0.058516986668109894 Validation loss 0.0594625249505043 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.3110],\n",
      "        [0.9316]], device='mps:0')\n",
      "Iteration 44420 Training loss 0.05794583633542061 Validation loss 0.05953798443078995 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.8534],\n",
      "        [0.1674]], device='mps:0')\n",
      "Iteration 44430 Training loss 0.062024105340242386 Validation loss 0.05942762270569801 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.0334],\n",
      "        [0.1693]], device='mps:0')\n",
      "Iteration 44440 Training loss 0.0546354241669178 Validation loss 0.059491194784641266 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.4141],\n",
      "        [0.1534]], device='mps:0')\n",
      "Iteration 44450 Training loss 0.05953123793005943 Validation loss 0.0594460554420948 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.0490],\n",
      "        [0.7499]], device='mps:0')\n",
      "Iteration 44460 Training loss 0.06080905720591545 Validation loss 0.05942264199256897 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.6583],\n",
      "        [0.0834]], device='mps:0')\n",
      "Iteration 44470 Training loss 0.05899464339017868 Validation loss 0.0594119057059288 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.9826],\n",
      "        [0.1905]], device='mps:0')\n",
      "Iteration 44480 Training loss 0.05386003479361534 Validation loss 0.05946006998419762 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.5936],\n",
      "        [0.9375]], device='mps:0')\n",
      "Iteration 44490 Training loss 0.05180953070521355 Validation loss 0.059467338025569916 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.0422],\n",
      "        [0.0213]], device='mps:0')\n",
      "Iteration 44500 Training loss 0.061825208365917206 Validation loss 0.059453561902046204 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.9714],\n",
      "        [0.1631]], device='mps:0')\n",
      "Iteration 44510 Training loss 0.056363530457019806 Validation loss 0.05959717929363251 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.9045],\n",
      "        [0.7302]], device='mps:0')\n",
      "Iteration 44520 Training loss 0.05727783963084221 Validation loss 0.0594133622944355 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.1843],\n",
      "        [0.8935]], device='mps:0')\n",
      "Iteration 44530 Training loss 0.05939102917909622 Validation loss 0.05942161753773689 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.6905],\n",
      "        [0.1856]], device='mps:0')\n",
      "Iteration 44540 Training loss 0.06430871039628983 Validation loss 0.05942155048251152 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.9827],\n",
      "        [0.5903]], device='mps:0')\n",
      "Iteration 44550 Training loss 0.05768438056111336 Validation loss 0.059451427310705185 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.3204],\n",
      "        [0.2889]], device='mps:0')\n",
      "Iteration 44560 Training loss 0.05512075871229172 Validation loss 0.0594109371304512 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.6658],\n",
      "        [0.1543]], device='mps:0')\n",
      "Iteration 44570 Training loss 0.04956810548901558 Validation loss 0.05940599739551544 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.9603],\n",
      "        [0.4708]], device='mps:0')\n",
      "Iteration 44580 Training loss 0.06173885986208916 Validation loss 0.0594031922519207 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.0159],\n",
      "        [0.7328]], device='mps:0')\n",
      "Iteration 44590 Training loss 0.05040755122900009 Validation loss 0.059408459812402725 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.5126],\n",
      "        [0.9807]], device='mps:0')\n",
      "Iteration 44600 Training loss 0.052688926458358765 Validation loss 0.059408556669950485 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.2382],\n",
      "        [0.9631]], device='mps:0')\n",
      "Iteration 44610 Training loss 0.048887357115745544 Validation loss 0.05941582843661308 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.2822],\n",
      "        [0.5842]], device='mps:0')\n",
      "Iteration 44620 Training loss 0.04881544038653374 Validation loss 0.05979268252849579 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.8970],\n",
      "        [0.9666]], device='mps:0')\n",
      "Iteration 44630 Training loss 0.056808214634656906 Validation loss 0.05941876024007797 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0061],\n",
      "        [0.9731]], device='mps:0')\n",
      "Iteration 44640 Training loss 0.05502689257264137 Validation loss 0.05945673957467079 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.4191],\n",
      "        [0.9755]], device='mps:0')\n",
      "Iteration 44650 Training loss 0.05924989655613899 Validation loss 0.05944279581308365 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.0851],\n",
      "        [0.1887]], device='mps:0')\n",
      "Iteration 44660 Training loss 0.05992725118994713 Validation loss 0.059504322707653046 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.9973],\n",
      "        [0.6483]], device='mps:0')\n",
      "Iteration 44670 Training loss 0.0619545504450798 Validation loss 0.059419091790914536 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.0477],\n",
      "        [0.3046]], device='mps:0')\n",
      "Iteration 44680 Training loss 0.05907103791832924 Validation loss 0.05939997360110283 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.4521],\n",
      "        [0.2445]], device='mps:0')\n",
      "Iteration 44690 Training loss 0.06511358171701431 Validation loss 0.05945487320423126 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.2902],\n",
      "        [0.0921]], device='mps:0')\n",
      "Iteration 44700 Training loss 0.05467798188328743 Validation loss 0.05941271409392357 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.1220],\n",
      "        [0.0576]], device='mps:0')\n",
      "Iteration 44710 Training loss 0.06317905336618423 Validation loss 0.05946207046508789 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.1195],\n",
      "        [0.0607]], device='mps:0')\n",
      "Iteration 44720 Training loss 0.055256374180316925 Validation loss 0.05939088016748428 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.8769],\n",
      "        [0.9852]], device='mps:0')\n",
      "Iteration 44730 Training loss 0.05903775244951248 Validation loss 0.05947960540652275 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.0507],\n",
      "        [0.1422]], device='mps:0')\n",
      "Iteration 44740 Training loss 0.05437757819890976 Validation loss 0.05944187566637993 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.6653],\n",
      "        [0.9624]], device='mps:0')\n",
      "Iteration 44750 Training loss 0.05339290574193001 Validation loss 0.05940455198287964 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.8360],\n",
      "        [0.3041]], device='mps:0')\n",
      "Iteration 44760 Training loss 0.058053143322467804 Validation loss 0.05945504456758499 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.2264],\n",
      "        [0.2210]], device='mps:0')\n",
      "Iteration 44770 Training loss 0.0595845989882946 Validation loss 0.059390366077423096 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.7750],\n",
      "        [0.9426]], device='mps:0')\n",
      "Iteration 44780 Training loss 0.05083338916301727 Validation loss 0.05940282717347145 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.2096],\n",
      "        [0.5478]], device='mps:0')\n",
      "Iteration 44790 Training loss 0.05038272589445114 Validation loss 0.05939248949289322 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.3827],\n",
      "        [0.7625]], device='mps:0')\n",
      "Iteration 44800 Training loss 0.06328597664833069 Validation loss 0.059394847601652145 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9052],\n",
      "        [0.5282]], device='mps:0')\n",
      "Iteration 44810 Training loss 0.05666234716773033 Validation loss 0.059397365897893906 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.0176],\n",
      "        [0.8131]], device='mps:0')\n",
      "Iteration 44820 Training loss 0.056674525141716 Validation loss 0.05939920246601105 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.0564],\n",
      "        [0.1581]], device='mps:0')\n",
      "Iteration 44830 Training loss 0.04936603456735611 Validation loss 0.05949126556515694 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.7074],\n",
      "        [0.7033]], device='mps:0')\n",
      "Iteration 44840 Training loss 0.06006826087832451 Validation loss 0.05938738211989403 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.4626],\n",
      "        [0.1534]], device='mps:0')\n",
      "Iteration 44850 Training loss 0.060190681368112564 Validation loss 0.05943884328007698 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.0421],\n",
      "        [0.4493]], device='mps:0')\n",
      "Iteration 44860 Training loss 0.06716861575841904 Validation loss 0.059447549283504486 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.3390],\n",
      "        [0.8935]], device='mps:0')\n",
      "Iteration 44870 Training loss 0.05713049694895744 Validation loss 0.05945732071995735 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.0671],\n",
      "        [0.9524]], device='mps:0')\n",
      "Iteration 44880 Training loss 0.0564955472946167 Validation loss 0.05944428592920303 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.7796],\n",
      "        [0.0546]], device='mps:0')\n",
      "Iteration 44890 Training loss 0.052949029952287674 Validation loss 0.05950236693024635 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.0147],\n",
      "        [0.0285]], device='mps:0')\n",
      "Iteration 44900 Training loss 0.04897496476769447 Validation loss 0.05947650596499443 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.0391],\n",
      "        [0.4693]], device='mps:0')\n",
      "Iteration 44910 Training loss 0.055665645748376846 Validation loss 0.05939088016748428 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.8827],\n",
      "        [0.5508]], device='mps:0')\n",
      "Iteration 44920 Training loss 0.054932236671447754 Validation loss 0.05940937623381615 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.1229],\n",
      "        [0.4498]], device='mps:0')\n",
      "Iteration 44930 Training loss 0.06111666560173035 Validation loss 0.05938642844557762 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.4388],\n",
      "        [0.9715]], device='mps:0')\n",
      "Iteration 44940 Training loss 0.053427837789058685 Validation loss 0.05938689783215523 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.7823],\n",
      "        [0.9258]], device='mps:0')\n",
      "Iteration 44950 Training loss 0.058414362370967865 Validation loss 0.05939200147986412 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9727],\n",
      "        [0.2752]], device='mps:0')\n",
      "Iteration 44960 Training loss 0.05713829770684242 Validation loss 0.05943635106086731 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.9867],\n",
      "        [0.8779]], device='mps:0')\n",
      "Iteration 44970 Training loss 0.055169571191072464 Validation loss 0.05938396975398064 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.1080],\n",
      "        [0.8641]], device='mps:0')\n",
      "Iteration 44980 Training loss 0.05641535669565201 Validation loss 0.059419695287942886 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.0468],\n",
      "        [0.0392]], device='mps:0')\n",
      "Iteration 44990 Training loss 0.05674762278795242 Validation loss 0.0595516636967659 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.1035],\n",
      "        [0.1379]], device='mps:0')\n",
      "Iteration 45000 Training loss 0.05581195652484894 Validation loss 0.05945366993546486 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.9927],\n",
      "        [0.0495]], device='mps:0')\n",
      "Iteration 45010 Training loss 0.05770737677812576 Validation loss 0.05941266939043999 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.3038],\n",
      "        [0.8996]], device='mps:0')\n",
      "Iteration 45020 Training loss 0.06392116099596024 Validation loss 0.0595785491168499 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0255],\n",
      "        [0.3009]], device='mps:0')\n",
      "Iteration 45030 Training loss 0.056356143206357956 Validation loss 0.05945625528693199 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.9242],\n",
      "        [0.7714]], device='mps:0')\n",
      "Iteration 45040 Training loss 0.05643470585346222 Validation loss 0.05943017825484276 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.8948],\n",
      "        [0.7470]], device='mps:0')\n",
      "Iteration 45050 Training loss 0.06184525787830353 Validation loss 0.05937735363841057 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.8560],\n",
      "        [0.3963]], device='mps:0')\n",
      "Iteration 45060 Training loss 0.05015600100159645 Validation loss 0.05936482921242714 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.0699],\n",
      "        [0.1736]], device='mps:0')\n",
      "Iteration 45070 Training loss 0.05428657308220863 Validation loss 0.05936508625745773 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.0849],\n",
      "        [0.6722]], device='mps:0')\n",
      "Iteration 45080 Training loss 0.05625564604997635 Validation loss 0.05940403416752815 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.4944],\n",
      "        [0.1205]], device='mps:0')\n",
      "Iteration 45090 Training loss 0.054345302283763885 Validation loss 0.059381987899541855 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.6342],\n",
      "        [0.7126]], device='mps:0')\n",
      "Iteration 45100 Training loss 0.05437329784035683 Validation loss 0.05937577411532402 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.5143],\n",
      "        [0.8183]], device='mps:0')\n",
      "Iteration 45110 Training loss 0.055200185626745224 Validation loss 0.059380728751420975 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.3684],\n",
      "        [0.9904]], device='mps:0')\n",
      "Iteration 45120 Training loss 0.05120712146162987 Validation loss 0.05962949991226196 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.8742],\n",
      "        [0.1738]], device='mps:0')\n",
      "Iteration 45130 Training loss 0.058898091316223145 Validation loss 0.05943841114640236 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.7987],\n",
      "        [0.3463]], device='mps:0')\n",
      "Iteration 45140 Training loss 0.052313897758722305 Validation loss 0.059398408979177475 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.0443],\n",
      "        [0.7968]], device='mps:0')\n",
      "Iteration 45150 Training loss 0.0588100329041481 Validation loss 0.05943309888243675 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.3675],\n",
      "        [0.3815]], device='mps:0')\n",
      "Iteration 45160 Training loss 0.057753510773181915 Validation loss 0.059353671967983246 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.1160],\n",
      "        [0.8766]], device='mps:0')\n",
      "Iteration 45170 Training loss 0.0592610202729702 Validation loss 0.05937089025974274 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.4381],\n",
      "        [0.0584]], device='mps:0')\n",
      "Iteration 45180 Training loss 0.0549812987446785 Validation loss 0.059344980865716934 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9925],\n",
      "        [0.9683]], device='mps:0')\n",
      "Iteration 45190 Training loss 0.06263550370931625 Validation loss 0.0593436062335968 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.1474],\n",
      "        [0.9574]], device='mps:0')\n",
      "Iteration 45200 Training loss 0.059214066714048386 Validation loss 0.059372104704380035 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.7858],\n",
      "        [0.0180]], device='mps:0')\n",
      "Iteration 45210 Training loss 0.05597900226712227 Validation loss 0.059357669204473495 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.0996],\n",
      "        [0.7306]], device='mps:0')\n",
      "Iteration 45220 Training loss 0.05819370225071907 Validation loss 0.059375062584877014 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.0939],\n",
      "        [0.0288]], device='mps:0')\n",
      "Iteration 45230 Training loss 0.05413290858268738 Validation loss 0.05936858803033829 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.1894],\n",
      "        [0.5319]], device='mps:0')\n",
      "Iteration 45240 Training loss 0.05649038031697273 Validation loss 0.05938019976019859 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.8643],\n",
      "        [0.8992]], device='mps:0')\n",
      "Iteration 45250 Training loss 0.05686872452497482 Validation loss 0.05934717878699303 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.7529],\n",
      "        [0.0057]], device='mps:0')\n",
      "Iteration 45260 Training loss 0.06486478447914124 Validation loss 0.0593906044960022 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.2058],\n",
      "        [0.9889]], device='mps:0')\n",
      "Iteration 45270 Training loss 0.06042499095201492 Validation loss 0.059881310909986496 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.3715],\n",
      "        [0.4057]], device='mps:0')\n",
      "Iteration 45280 Training loss 0.05907760560512543 Validation loss 0.05934169143438339 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.2572],\n",
      "        [0.9235]], device='mps:0')\n",
      "Iteration 45290 Training loss 0.04827643185853958 Validation loss 0.05941198021173477 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.1796],\n",
      "        [0.1406]], device='mps:0')\n",
      "Iteration 45300 Training loss 0.050358906388282776 Validation loss 0.05936306715011597 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.2766],\n",
      "        [0.9429]], device='mps:0')\n",
      "Iteration 45310 Training loss 0.05401363596320152 Validation loss 0.059380825608968735 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.9878],\n",
      "        [0.2586]], device='mps:0')\n",
      "Iteration 45320 Training loss 0.0612468458712101 Validation loss 0.05936950817704201 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.8065],\n",
      "        [0.6925]], device='mps:0')\n",
      "Iteration 45330 Training loss 0.05542450770735741 Validation loss 0.059437233954668045 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.0381],\n",
      "        [0.1567]], device='mps:0')\n",
      "Iteration 45340 Training loss 0.05772409215569496 Validation loss 0.05934971198439598 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.2868],\n",
      "        [0.6585]], device='mps:0')\n",
      "Iteration 45350 Training loss 0.05694730207324028 Validation loss 0.05935467407107353 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.3077],\n",
      "        [0.9988]], device='mps:0')\n",
      "Iteration 45360 Training loss 0.05345277115702629 Validation loss 0.05955839529633522 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.4301],\n",
      "        [0.8615]], device='mps:0')\n",
      "Iteration 45370 Training loss 0.06501052528619766 Validation loss 0.059360794723033905 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.4414],\n",
      "        [0.1953]], device='mps:0')\n",
      "Iteration 45380 Training loss 0.05597388744354248 Validation loss 0.059367839246988297 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9296],\n",
      "        [0.1142]], device='mps:0')\n",
      "Iteration 45390 Training loss 0.05300549417734146 Validation loss 0.05935106799006462 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.9775],\n",
      "        [0.0910]], device='mps:0')\n",
      "Iteration 45400 Training loss 0.05383438616991043 Validation loss 0.059408921748399734 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.6586],\n",
      "        [0.2718]], device='mps:0')\n",
      "Iteration 45410 Training loss 0.06396080553531647 Validation loss 0.059442661702632904 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.8858],\n",
      "        [0.9683]], device='mps:0')\n",
      "Iteration 45420 Training loss 0.05460912734270096 Validation loss 0.05934102088212967 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.9821],\n",
      "        [0.9574]], device='mps:0')\n",
      "Iteration 45430 Training loss 0.05863186717033386 Validation loss 0.05933448299765587 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.5131],\n",
      "        [0.0564]], device='mps:0')\n",
      "Iteration 45440 Training loss 0.051434338092803955 Validation loss 0.059417519718408585 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.7349],\n",
      "        [0.3235]], device='mps:0')\n",
      "Iteration 45450 Training loss 0.05789191275835037 Validation loss 0.059346143156290054 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.9541],\n",
      "        [0.8893]], device='mps:0')\n",
      "Iteration 45460 Training loss 0.05959086865186691 Validation loss 0.059338536113500595 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.9773],\n",
      "        [0.9137]], device='mps:0')\n",
      "Iteration 45470 Training loss 0.05434611067175865 Validation loss 0.059364400804042816 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.8853],\n",
      "        [0.6548]], device='mps:0')\n",
      "Iteration 45480 Training loss 0.060871776193380356 Validation loss 0.05955047905445099 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0873],\n",
      "        [0.0361]], device='mps:0')\n",
      "Iteration 45490 Training loss 0.060677312314510345 Validation loss 0.05938705801963806 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.1372],\n",
      "        [0.3443]], device='mps:0')\n",
      "Iteration 45500 Training loss 0.054939448833465576 Validation loss 0.059360966086387634 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.0485],\n",
      "        [0.5101]], device='mps:0')\n",
      "Iteration 45510 Training loss 0.053789809346199036 Validation loss 0.05942213162779808 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.0436],\n",
      "        [0.9537]], device='mps:0')\n",
      "Iteration 45520 Training loss 0.05146303027868271 Validation loss 0.05933341383934021 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.9109],\n",
      "        [0.9828]], device='mps:0')\n",
      "Iteration 45530 Training loss 0.05995955318212509 Validation loss 0.05933837965130806 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.1715],\n",
      "        [0.9139]], device='mps:0')\n",
      "Iteration 45540 Training loss 0.052116308361291885 Validation loss 0.05932790786027908 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.7999],\n",
      "        [0.0965]], device='mps:0')\n",
      "Iteration 45550 Training loss 0.05792245641350746 Validation loss 0.05932224541902542 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.8033],\n",
      "        [0.0999]], device='mps:0')\n",
      "Iteration 45560 Training loss 0.05836851894855499 Validation loss 0.05934293568134308 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.0652],\n",
      "        [0.2920]], device='mps:0')\n",
      "Iteration 45570 Training loss 0.054278817027807236 Validation loss 0.05932582914829254 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.5447],\n",
      "        [0.4952]], device='mps:0')\n",
      "Iteration 45580 Training loss 0.0512322261929512 Validation loss 0.05933487042784691 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.8389],\n",
      "        [0.9715]], device='mps:0')\n",
      "Iteration 45590 Training loss 0.058358483016490936 Validation loss 0.05940747261047363 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9317],\n",
      "        [0.9670]], device='mps:0')\n",
      "Iteration 45600 Training loss 0.055417682975530624 Validation loss 0.05933251604437828 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.1213],\n",
      "        [0.1696]], device='mps:0')\n",
      "Iteration 45610 Training loss 0.05953962355852127 Validation loss 0.05935235694050789 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.0711],\n",
      "        [0.2307]], device='mps:0')\n",
      "Iteration 45620 Training loss 0.06302175670862198 Validation loss 0.059330619871616364 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.8864],\n",
      "        [0.1494]], device='mps:0')\n",
      "Iteration 45630 Training loss 0.05369797721505165 Validation loss 0.0594487190246582 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9559],\n",
      "        [0.2037]], device='mps:0')\n",
      "Iteration 45640 Training loss 0.059594761580228806 Validation loss 0.05950836092233658 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.7347],\n",
      "        [0.7214]], device='mps:0')\n",
      "Iteration 45650 Training loss 0.05391940474510193 Validation loss 0.05947307124733925 Accuracy 0.8362500667572021\n",
      "Output tensor([[0.8671],\n",
      "        [0.9466]], device='mps:0')\n",
      "Iteration 45660 Training loss 0.06409256905317307 Validation loss 0.059469256550073624 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.1258],\n",
      "        [0.1492]], device='mps:0')\n",
      "Iteration 45670 Training loss 0.05680183321237564 Validation loss 0.05935826897621155 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.9463],\n",
      "        [0.8850]], device='mps:0')\n",
      "Iteration 45680 Training loss 0.057239703834056854 Validation loss 0.05942763015627861 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.6029],\n",
      "        [0.2503]], device='mps:0')\n",
      "Iteration 45690 Training loss 0.05795077234506607 Validation loss 0.059359628707170486 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.3937],\n",
      "        [0.5231]], device='mps:0')\n",
      "Iteration 45700 Training loss 0.05880172923207283 Validation loss 0.05938152223825455 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.9935],\n",
      "        [0.2194]], device='mps:0')\n",
      "Iteration 45710 Training loss 0.07035204768180847 Validation loss 0.059406302869319916 Accuracy 0.8370000123977661\n",
      "Output tensor([[0.5099],\n",
      "        [0.0563]], device='mps:0')\n",
      "Iteration 45720 Training loss 0.06650430709123611 Validation loss 0.059474460780620575 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.0091],\n",
      "        [0.9494]], device='mps:0')\n",
      "Iteration 45730 Training loss 0.062364187091588974 Validation loss 0.059386659413576126 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.5101],\n",
      "        [0.0102]], device='mps:0')\n",
      "Iteration 45740 Training loss 0.06263384968042374 Validation loss 0.059326205402612686 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.1287],\n",
      "        [0.8003]], device='mps:0')\n",
      "Iteration 45750 Training loss 0.05626147240400314 Validation loss 0.05936582386493683 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.9572],\n",
      "        [0.8296]], device='mps:0')\n",
      "Iteration 45760 Training loss 0.0470716655254364 Validation loss 0.05933593958616257 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.9997],\n",
      "        [0.3814]], device='mps:0')\n",
      "Iteration 45770 Training loss 0.05649301037192345 Validation loss 0.05934877321124077 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.7238],\n",
      "        [0.6413]], device='mps:0')\n",
      "Iteration 45780 Training loss 0.05275621637701988 Validation loss 0.059320613741874695 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.0521],\n",
      "        [0.9672]], device='mps:0')\n",
      "Iteration 45790 Training loss 0.05228843539953232 Validation loss 0.05931709334254265 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.5310],\n",
      "        [0.4950]], device='mps:0')\n",
      "Iteration 45800 Training loss 0.058476485311985016 Validation loss 0.05937263369560242 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.9108],\n",
      "        [0.9912]], device='mps:0')\n",
      "Iteration 45810 Training loss 0.05671146139502525 Validation loss 0.059475868940353394 Accuracy 0.8368750214576721\n",
      "Output tensor([[0.2085],\n",
      "        [0.9906]], device='mps:0')\n",
      "Iteration 45820 Training loss 0.0570698007941246 Validation loss 0.05935412272810936 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.7549],\n",
      "        [0.0728]], device='mps:0')\n",
      "Iteration 45830 Training loss 0.05234633386135101 Validation loss 0.059517476707696915 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.1082],\n",
      "        [0.6494]], device='mps:0')\n",
      "Iteration 45840 Training loss 0.058890778571367264 Validation loss 0.0594228059053421 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.8926],\n",
      "        [0.0845]], device='mps:0')\n",
      "Iteration 45850 Training loss 0.05548888072371483 Validation loss 0.05970282852649689 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.9369],\n",
      "        [0.8088]], device='mps:0')\n",
      "Iteration 45860 Training loss 0.056474070996046066 Validation loss 0.05931894853711128 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.0308],\n",
      "        [0.1311]], device='mps:0')\n",
      "Iteration 45870 Training loss 0.05192644149065018 Validation loss 0.05931754410266876 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.1301],\n",
      "        [0.1137]], device='mps:0')\n",
      "Iteration 45880 Training loss 0.06111666187644005 Validation loss 0.05931016430258751 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.1550],\n",
      "        [0.9336]], device='mps:0')\n",
      "Iteration 45890 Training loss 0.06845281273126602 Validation loss 0.05937286838889122 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.7937],\n",
      "        [0.9213]], device='mps:0')\n",
      "Iteration 45900 Training loss 0.04407940432429314 Validation loss 0.05939611420035362 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.8968],\n",
      "        [0.1271]], device='mps:0')\n",
      "Iteration 45910 Training loss 0.05986243113875389 Validation loss 0.059312883764505386 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.7317],\n",
      "        [0.3906]], device='mps:0')\n",
      "Iteration 45920 Training loss 0.05669788643717766 Validation loss 0.05937565863132477 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.8902],\n",
      "        [0.1256]], device='mps:0')\n",
      "Iteration 45930 Training loss 0.05304771289229393 Validation loss 0.059359826147556305 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0949],\n",
      "        [0.9673]], device='mps:0')\n",
      "Iteration 45940 Training loss 0.054631900042295456 Validation loss 0.05935855209827423 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.0105],\n",
      "        [0.9667]], device='mps:0')\n",
      "Iteration 45950 Training loss 0.053024642169475555 Validation loss 0.05931839719414711 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9523],\n",
      "        [0.8994]], device='mps:0')\n",
      "Iteration 45960 Training loss 0.05856718868017197 Validation loss 0.05931936576962471 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.5025],\n",
      "        [0.0564]], device='mps:0')\n",
      "Iteration 45970 Training loss 0.05703502520918846 Validation loss 0.059322018176317215 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.0459],\n",
      "        [0.4075]], device='mps:0')\n",
      "Iteration 45980 Training loss 0.05229722708463669 Validation loss 0.059537410736083984 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9745],\n",
      "        [0.0917]], device='mps:0')\n",
      "Iteration 45990 Training loss 0.05906171351671219 Validation loss 0.059331346303224564 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.0841],\n",
      "        [0.4940]], device='mps:0')\n",
      "Iteration 46000 Training loss 0.05810525640845299 Validation loss 0.05951574817299843 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.5871],\n",
      "        [0.9654]], device='mps:0')\n",
      "Iteration 46010 Training loss 0.0579681470990181 Validation loss 0.05940692499279976 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.7881],\n",
      "        [0.3320]], device='mps:0')\n",
      "Iteration 46020 Training loss 0.06415334343910217 Validation loss 0.0594884529709816 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.2216],\n",
      "        [0.5399]], device='mps:0')\n",
      "Iteration 46030 Training loss 0.05939469859004021 Validation loss 0.05936991423368454 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.9671],\n",
      "        [0.8532]], device='mps:0')\n",
      "Iteration 46040 Training loss 0.05949976295232773 Validation loss 0.05933598428964615 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.7635],\n",
      "        [0.1167]], device='mps:0')\n",
      "Iteration 46050 Training loss 0.06110883876681328 Validation loss 0.05932684987783432 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.2096],\n",
      "        [0.0261]], device='mps:0')\n",
      "Iteration 46060 Training loss 0.05153397098183632 Validation loss 0.059316232800483704 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.8107],\n",
      "        [0.2501]], device='mps:0')\n",
      "Iteration 46070 Training loss 0.06323365867137909 Validation loss 0.059313397854566574 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0525],\n",
      "        [0.3224]], device='mps:0')\n",
      "Iteration 46080 Training loss 0.054747845977544785 Validation loss 0.05933360755443573 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.0645],\n",
      "        [0.0258]], device='mps:0')\n",
      "Iteration 46090 Training loss 0.06310462206602097 Validation loss 0.059339504688978195 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.9507],\n",
      "        [0.3039]], device='mps:0')\n",
      "Iteration 46100 Training loss 0.05052883177995682 Validation loss 0.059440359473228455 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.0110],\n",
      "        [0.7636]], device='mps:0')\n",
      "Iteration 46110 Training loss 0.06311541795730591 Validation loss 0.05930011719465256 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.2006],\n",
      "        [0.0052]], device='mps:0')\n",
      "Iteration 46120 Training loss 0.05565968528389931 Validation loss 0.05943150445818901 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.5844],\n",
      "        [0.5152]], device='mps:0')\n",
      "Iteration 46130 Training loss 0.061056505888700485 Validation loss 0.05930223688483238 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.0420],\n",
      "        [0.1514]], device='mps:0')\n",
      "Iteration 46140 Training loss 0.04965224117040634 Validation loss 0.059421855956315994 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.4527],\n",
      "        [0.8077]], device='mps:0')\n",
      "Iteration 46150 Training loss 0.05018467456102371 Validation loss 0.059309933334589005 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.9473],\n",
      "        [0.3059]], device='mps:0')\n",
      "Iteration 46160 Training loss 0.05847981199622154 Validation loss 0.05932505801320076 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.2375],\n",
      "        [0.6747]], device='mps:0')\n",
      "Iteration 46170 Training loss 0.06583566963672638 Validation loss 0.05931434407830238 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.2207],\n",
      "        [0.8057]], device='mps:0')\n",
      "Iteration 46180 Training loss 0.06388700753450394 Validation loss 0.05929603800177574 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9225],\n",
      "        [0.6875]], device='mps:0')\n",
      "Iteration 46190 Training loss 0.05320734903216362 Validation loss 0.05931598320603371 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.0364],\n",
      "        [0.1493]], device='mps:0')\n",
      "Iteration 46200 Training loss 0.050556667149066925 Validation loss 0.05928964540362358 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.4300],\n",
      "        [0.1456]], device='mps:0')\n",
      "Iteration 46210 Training loss 0.05829181522130966 Validation loss 0.05931825190782547 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.8818],\n",
      "        [0.0960]], device='mps:0')\n",
      "Iteration 46220 Training loss 0.055577799677848816 Validation loss 0.059316881000995636 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.1924],\n",
      "        [0.0143]], device='mps:0')\n",
      "Iteration 46230 Training loss 0.06284364312887192 Validation loss 0.059279751032590866 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.5703],\n",
      "        [0.9084]], device='mps:0')\n",
      "Iteration 46240 Training loss 0.0586620569229126 Validation loss 0.05928647145628929 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.5055],\n",
      "        [0.9531]], device='mps:0')\n",
      "Iteration 46250 Training loss 0.05148075148463249 Validation loss 0.05927865207195282 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.6163],\n",
      "        [0.3424]], device='mps:0')\n",
      "Iteration 46260 Training loss 0.05580902844667435 Validation loss 0.05938142538070679 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.4097],\n",
      "        [0.0807]], device='mps:0')\n",
      "Iteration 46270 Training loss 0.054623499512672424 Validation loss 0.05928487703204155 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.0358],\n",
      "        [0.0441]], device='mps:0')\n",
      "Iteration 46280 Training loss 0.054927147924900055 Validation loss 0.05940606817603111 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.5424],\n",
      "        [0.9863]], device='mps:0')\n",
      "Iteration 46290 Training loss 0.060417935252189636 Validation loss 0.05932844802737236 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.1387],\n",
      "        [0.9487]], device='mps:0')\n",
      "Iteration 46300 Training loss 0.06151581555604935 Validation loss 0.05929812416434288 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.0299],\n",
      "        [0.0761]], device='mps:0')\n",
      "Iteration 46310 Training loss 0.05361447110772133 Validation loss 0.05937366187572479 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.5528],\n",
      "        [0.9050]], device='mps:0')\n",
      "Iteration 46320 Training loss 0.058706190437078476 Validation loss 0.059266939759254456 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.0182],\n",
      "        [0.9658]], device='mps:0')\n",
      "Iteration 46330 Training loss 0.05405515059828758 Validation loss 0.059290509670972824 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.1020],\n",
      "        [0.1281]], device='mps:0')\n",
      "Iteration 46340 Training loss 0.058250997215509415 Validation loss 0.05925922095775604 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.9582],\n",
      "        [0.0690]], device='mps:0')\n",
      "Iteration 46350 Training loss 0.06432407349348068 Validation loss 0.05944769084453583 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.0736],\n",
      "        [0.1467]], device='mps:0')\n",
      "Iteration 46360 Training loss 0.05676371604204178 Validation loss 0.05932631343603134 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.7154],\n",
      "        [0.4914]], device='mps:0')\n",
      "Iteration 46370 Training loss 0.04728620871901512 Validation loss 0.059547778218984604 Accuracy 0.8366250395774841\n",
      "Output tensor([[0.0048],\n",
      "        [0.7208]], device='mps:0')\n",
      "Iteration 46380 Training loss 0.06125114485621452 Validation loss 0.05931227281689644 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.9937],\n",
      "        [0.3429]], device='mps:0')\n",
      "Iteration 46390 Training loss 0.06079810857772827 Validation loss 0.059291139245033264 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.6767],\n",
      "        [0.9964]], device='mps:0')\n",
      "Iteration 46400 Training loss 0.06862461566925049 Validation loss 0.0593397319316864 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.9691],\n",
      "        [0.5241]], device='mps:0')\n",
      "Iteration 46410 Training loss 0.05670779570937157 Validation loss 0.059339579194784164 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.6716],\n",
      "        [0.5400]], device='mps:0')\n",
      "Iteration 46420 Training loss 0.05502821505069733 Validation loss 0.05938240513205528 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.9382],\n",
      "        [0.1059]], device='mps:0')\n",
      "Iteration 46430 Training loss 0.05550023913383484 Validation loss 0.05951980873942375 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.7947],\n",
      "        [0.8954]], device='mps:0')\n",
      "Iteration 46440 Training loss 0.0579896904528141 Validation loss 0.05932266637682915 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.9498],\n",
      "        [0.2193]], device='mps:0')\n",
      "Iteration 46450 Training loss 0.055834539234638214 Validation loss 0.059263743460178375 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.8657],\n",
      "        [0.1598]], device='mps:0')\n",
      "Iteration 46460 Training loss 0.05101434513926506 Validation loss 0.05932529643177986 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.9885],\n",
      "        [0.9348]], device='mps:0')\n",
      "Iteration 46470 Training loss 0.058906733989715576 Validation loss 0.059259116649627686 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.7694],\n",
      "        [0.7568]], device='mps:0')\n",
      "Iteration 46480 Training loss 0.05985097214579582 Validation loss 0.0592525452375412 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.8016],\n",
      "        [0.4710]], device='mps:0')\n",
      "Iteration 46490 Training loss 0.05990851670503616 Validation loss 0.05925323814153671 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.5789],\n",
      "        [0.4516]], device='mps:0')\n",
      "Iteration 46500 Training loss 0.058733146637678146 Validation loss 0.05925267934799194 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.0681],\n",
      "        [0.7561]], device='mps:0')\n",
      "Iteration 46510 Training loss 0.062386102974414825 Validation loss 0.05924508348107338 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.9012],\n",
      "        [0.5357]], device='mps:0')\n",
      "Iteration 46520 Training loss 0.051620882004499435 Validation loss 0.05971883609890938 Accuracy 0.8363750576972961\n",
      "Output tensor([[0.5879],\n",
      "        [0.3679]], device='mps:0')\n",
      "Iteration 46530 Training loss 0.06585928797721863 Validation loss 0.05925777554512024 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.5426],\n",
      "        [0.8309]], device='mps:0')\n",
      "Iteration 46540 Training loss 0.06131063401699066 Validation loss 0.05945261940360069 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.9921],\n",
      "        [0.4615]], device='mps:0')\n",
      "Iteration 46550 Training loss 0.05265806242823601 Validation loss 0.05943651869893074 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9974],\n",
      "        [0.9585]], device='mps:0')\n",
      "Iteration 46560 Training loss 0.06174773722887039 Validation loss 0.05926930904388428 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.1861],\n",
      "        [0.4552]], device='mps:0')\n",
      "Iteration 46570 Training loss 0.061309002339839935 Validation loss 0.0593050979077816 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.0298],\n",
      "        [0.7800]], device='mps:0')\n",
      "Iteration 46580 Training loss 0.06559651345014572 Validation loss 0.05924121290445328 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.4497],\n",
      "        [0.0733]], device='mps:0')\n",
      "Iteration 46590 Training loss 0.061561476439237595 Validation loss 0.05933413654565811 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.1803],\n",
      "        [0.9590]], device='mps:0')\n",
      "Iteration 46600 Training loss 0.05681179091334343 Validation loss 0.05922840163111687 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.0587],\n",
      "        [0.2104]], device='mps:0')\n",
      "Iteration 46610 Training loss 0.04642981290817261 Validation loss 0.05929054692387581 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.2908],\n",
      "        [0.2035]], device='mps:0')\n",
      "Iteration 46620 Training loss 0.056681063026189804 Validation loss 0.05935773625969887 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9502],\n",
      "        [0.6209]], device='mps:0')\n",
      "Iteration 46630 Training loss 0.06021919101476669 Validation loss 0.059245266020298004 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.0484],\n",
      "        [0.0077]], device='mps:0')\n",
      "Iteration 46640 Training loss 0.062409304082393646 Validation loss 0.05935834348201752 Accuracy 0.8367500305175781\n",
      "Output tensor([[0.0041],\n",
      "        [0.6970]], device='mps:0')\n",
      "Iteration 46650 Training loss 0.046225909143686295 Validation loss 0.05931783840060234 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.4126],\n",
      "        [0.0254]], device='mps:0')\n",
      "Iteration 46660 Training loss 0.05921567603945732 Validation loss 0.0593155100941658 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.0249],\n",
      "        [0.1083]], device='mps:0')\n",
      "Iteration 46670 Training loss 0.06088211014866829 Validation loss 0.059512991458177567 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.0071],\n",
      "        [0.3982]], device='mps:0')\n",
      "Iteration 46680 Training loss 0.05862239748239517 Validation loss 0.05926301330327988 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.0810],\n",
      "        [0.0499]], device='mps:0')\n",
      "Iteration 46690 Training loss 0.05656765028834343 Validation loss 0.0594438835978508 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.9987],\n",
      "        [0.0545]], device='mps:0')\n",
      "Iteration 46700 Training loss 0.06092117354273796 Validation loss 0.05925809592008591 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9722],\n",
      "        [0.0891]], device='mps:0')\n",
      "Iteration 46710 Training loss 0.05711442977190018 Validation loss 0.05931824818253517 Accuracy 0.8372500538825989\n",
      "Output tensor([[0.4890],\n",
      "        [0.1992]], device='mps:0')\n",
      "Iteration 46720 Training loss 0.06171650439500809 Validation loss 0.059340979903936386 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.0201],\n",
      "        [0.0073]], device='mps:0')\n",
      "Iteration 46730 Training loss 0.06423730403184891 Validation loss 0.05927332490682602 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.5403],\n",
      "        [0.9072]], device='mps:0')\n",
      "Iteration 46740 Training loss 0.059294506907463074 Validation loss 0.05928965285420418 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.0227],\n",
      "        [0.6696]], device='mps:0')\n",
      "Iteration 46750 Training loss 0.061884403228759766 Validation loss 0.05924259498715401 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.8507],\n",
      "        [0.4159]], device='mps:0')\n",
      "Iteration 46760 Training loss 0.06184510886669159 Validation loss 0.05925474688410759 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.0767],\n",
      "        [0.6545]], device='mps:0')\n",
      "Iteration 46770 Training loss 0.05806867405772209 Validation loss 0.05924205482006073 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.7779],\n",
      "        [0.0277]], device='mps:0')\n",
      "Iteration 46780 Training loss 0.05305101349949837 Validation loss 0.05922314152121544 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.9048],\n",
      "        [0.0777]], device='mps:0')\n",
      "Iteration 46790 Training loss 0.05406235158443451 Validation loss 0.059355925768613815 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.4450],\n",
      "        [0.8478]], device='mps:0')\n",
      "Iteration 46800 Training loss 0.05738264322280884 Validation loss 0.05924617126584053 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.2147],\n",
      "        [0.0598]], device='mps:0')\n",
      "Iteration 46810 Training loss 0.05597331002354622 Validation loss 0.05921906977891922 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.9125],\n",
      "        [0.8345]], device='mps:0')\n",
      "Iteration 46820 Training loss 0.06296125799417496 Validation loss 0.05924332141876221 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.2703],\n",
      "        [0.4851]], device='mps:0')\n",
      "Iteration 46830 Training loss 0.052854061126708984 Validation loss 0.059289246797561646 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.0123],\n",
      "        [0.0138]], device='mps:0')\n",
      "Iteration 46840 Training loss 0.056062180548906326 Validation loss 0.05925522744655609 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0895],\n",
      "        [0.8750]], device='mps:0')\n",
      "Iteration 46850 Training loss 0.05358829349279404 Validation loss 0.059207797050476074 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.6995],\n",
      "        [0.0848]], device='mps:0')\n",
      "Iteration 46860 Training loss 0.05234755948185921 Validation loss 0.059257056564092636 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.0478],\n",
      "        [0.0228]], device='mps:0')\n",
      "Iteration 46870 Training loss 0.06918389350175858 Validation loss 0.059442684054374695 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.0454],\n",
      "        [0.1371]], device='mps:0')\n",
      "Iteration 46880 Training loss 0.06000813841819763 Validation loss 0.05923602357506752 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.7969],\n",
      "        [0.6634]], device='mps:0')\n",
      "Iteration 46890 Training loss 0.054901786148548126 Validation loss 0.05924514681100845 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9823],\n",
      "        [0.6634]], device='mps:0')\n",
      "Iteration 46900 Training loss 0.06617934256792068 Validation loss 0.05921348184347153 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.7041],\n",
      "        [0.1670]], device='mps:0')\n",
      "Iteration 46910 Training loss 0.05608946830034256 Validation loss 0.059258799999952316 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.8456],\n",
      "        [0.9779]], device='mps:0')\n",
      "Iteration 46920 Training loss 0.05990375950932503 Validation loss 0.05933620035648346 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.8110],\n",
      "        [0.1449]], device='mps:0')\n",
      "Iteration 46930 Training loss 0.056103795766830444 Validation loss 0.05923215672373772 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.9915],\n",
      "        [0.8673]], device='mps:0')\n",
      "Iteration 46940 Training loss 0.05505947768688202 Validation loss 0.05925898253917694 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.0893],\n",
      "        [0.8250]], device='mps:0')\n",
      "Iteration 46950 Training loss 0.0498318076133728 Validation loss 0.05922120809555054 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.6879],\n",
      "        [0.8281]], device='mps:0')\n",
      "Iteration 46960 Training loss 0.05839437246322632 Validation loss 0.05922553688287735 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.0485],\n",
      "        [0.9422]], device='mps:0')\n",
      "Iteration 46970 Training loss 0.061370447278022766 Validation loss 0.05956389755010605 Accuracy 0.8365000486373901\n",
      "Output tensor([[0.3634],\n",
      "        [0.3372]], device='mps:0')\n",
      "Iteration 46980 Training loss 0.05771482735872269 Validation loss 0.05935107544064522 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.3437],\n",
      "        [0.0633]], device='mps:0')\n",
      "Iteration 46990 Training loss 0.050982169806957245 Validation loss 0.05923286825418472 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.4822],\n",
      "        [0.8379]], device='mps:0')\n",
      "Iteration 47000 Training loss 0.056756872683763504 Validation loss 0.05920933187007904 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.2711],\n",
      "        [0.8386]], device='mps:0')\n",
      "Iteration 47010 Training loss 0.06021362170577049 Validation loss 0.05922399461269379 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.9531],\n",
      "        [0.1245]], device='mps:0')\n",
      "Iteration 47020 Training loss 0.052111901342868805 Validation loss 0.059198856353759766 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.0123],\n",
      "        [0.6393]], device='mps:0')\n",
      "Iteration 47030 Training loss 0.051476214081048965 Validation loss 0.05922229588031769 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.1171],\n",
      "        [0.9925]], device='mps:0')\n",
      "Iteration 47040 Training loss 0.05688825622200966 Validation loss 0.059363625943660736 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.7920],\n",
      "        [0.9476]], device='mps:0')\n",
      "Iteration 47050 Training loss 0.05418776348233223 Validation loss 0.05919887125492096 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.5424],\n",
      "        [0.1634]], device='mps:0')\n",
      "Iteration 47060 Training loss 0.06136661395430565 Validation loss 0.05919603630900383 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.1270],\n",
      "        [0.2125]], device='mps:0')\n",
      "Iteration 47070 Training loss 0.054932594299316406 Validation loss 0.05920586735010147 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.7429],\n",
      "        [0.7642]], device='mps:0')\n",
      "Iteration 47080 Training loss 0.053714752197265625 Validation loss 0.059261858463287354 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.2240],\n",
      "        [0.1849]], device='mps:0')\n",
      "Iteration 47090 Training loss 0.04931463301181793 Validation loss 0.05920886620879173 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.3634],\n",
      "        [0.0144]], device='mps:0')\n",
      "Iteration 47100 Training loss 0.05490943789482117 Validation loss 0.059204746037721634 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.9915],\n",
      "        [0.0480]], device='mps:0')\n",
      "Iteration 47110 Training loss 0.05913671851158142 Validation loss 0.05922751873731613 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.4492],\n",
      "        [0.8755]], device='mps:0')\n",
      "Iteration 47120 Training loss 0.060355596244335175 Validation loss 0.059211790561676025 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.8571],\n",
      "        [0.5752]], device='mps:0')\n",
      "Iteration 47130 Training loss 0.06443658471107483 Validation loss 0.05932592228055 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.8909],\n",
      "        [0.0608]], device='mps:0')\n",
      "Iteration 47140 Training loss 0.055790770798921585 Validation loss 0.059228815138339996 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.0363],\n",
      "        [0.2034]], device='mps:0')\n",
      "Iteration 47150 Training loss 0.057693690061569214 Validation loss 0.059452760964632034 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.3119],\n",
      "        [0.1180]], device='mps:0')\n",
      "Iteration 47160 Training loss 0.0573272779583931 Validation loss 0.059260692447423935 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.1464],\n",
      "        [0.6547]], device='mps:0')\n",
      "Iteration 47170 Training loss 0.04847986251115799 Validation loss 0.059201113879680634 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.6392],\n",
      "        [0.2061]], device='mps:0')\n",
      "Iteration 47180 Training loss 0.05474625527858734 Validation loss 0.059222444891929626 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.2261],\n",
      "        [0.2188]], device='mps:0')\n",
      "Iteration 47190 Training loss 0.057273559272289276 Validation loss 0.059188224375247955 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.9185],\n",
      "        [0.6026]], device='mps:0')\n",
      "Iteration 47200 Training loss 0.04847397655248642 Validation loss 0.0592065267264843 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.0109],\n",
      "        [0.9496]], device='mps:0')\n",
      "Iteration 47210 Training loss 0.06155513599514961 Validation loss 0.059360384941101074 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.9986],\n",
      "        [0.0079]], device='mps:0')\n",
      "Iteration 47220 Training loss 0.05498082935810089 Validation loss 0.05919237062335014 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.6091],\n",
      "        [0.2496]], device='mps:0')\n",
      "Iteration 47230 Training loss 0.05313679203391075 Validation loss 0.0592879056930542 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.0953],\n",
      "        [0.9156]], device='mps:0')\n",
      "Iteration 47240 Training loss 0.055076684802770615 Validation loss 0.05919603630900383 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.0080],\n",
      "        [0.9488]], device='mps:0')\n",
      "Iteration 47250 Training loss 0.06078505888581276 Validation loss 0.05945046618580818 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.6822],\n",
      "        [0.0219]], device='mps:0')\n",
      "Iteration 47260 Training loss 0.06174365058541298 Validation loss 0.05916836857795715 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.9971],\n",
      "        [0.2959]], device='mps:0')\n",
      "Iteration 47270 Training loss 0.05019561946392059 Validation loss 0.059172648936510086 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.3175],\n",
      "        [0.1121]], device='mps:0')\n",
      "Iteration 47280 Training loss 0.057003628462553024 Validation loss 0.05936792120337486 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.7771],\n",
      "        [0.8891]], device='mps:0')\n",
      "Iteration 47290 Training loss 0.0628088191151619 Validation loss 0.05925997346639633 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.0079],\n",
      "        [0.5912]], device='mps:0')\n",
      "Iteration 47300 Training loss 0.05108362436294556 Validation loss 0.059308312833309174 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0795],\n",
      "        [0.8894]], device='mps:0')\n",
      "Iteration 47310 Training loss 0.053161147981882095 Validation loss 0.059187598526477814 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.3427],\n",
      "        [0.9680]], device='mps:0')\n",
      "Iteration 47320 Training loss 0.051186274737119675 Validation loss 0.059187255799770355 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.1987],\n",
      "        [0.9704]], device='mps:0')\n",
      "Iteration 47330 Training loss 0.05340218544006348 Validation loss 0.05917859822511673 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.1824],\n",
      "        [0.5900]], device='mps:0')\n",
      "Iteration 47340 Training loss 0.05594104528427124 Validation loss 0.05932099372148514 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.4685],\n",
      "        [0.5401]], device='mps:0')\n",
      "Iteration 47350 Training loss 0.05311332270503044 Validation loss 0.05917031690478325 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.8525],\n",
      "        [0.6407]], device='mps:0')\n",
      "Iteration 47360 Training loss 0.05314047634601593 Validation loss 0.05923311039805412 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.9978],\n",
      "        [0.9404]], device='mps:0')\n",
      "Iteration 47370 Training loss 0.04750201478600502 Validation loss 0.059216178953647614 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.0962],\n",
      "        [0.7573]], device='mps:0')\n",
      "Iteration 47380 Training loss 0.06017953157424927 Validation loss 0.05918606370687485 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9164],\n",
      "        [0.7829]], device='mps:0')\n",
      "Iteration 47390 Training loss 0.05401565507054329 Validation loss 0.0591806098818779 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.8429],\n",
      "        [0.3771]], device='mps:0')\n",
      "Iteration 47400 Training loss 0.05964774638414383 Validation loss 0.05937366560101509 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.6289],\n",
      "        [0.9310]], device='mps:0')\n",
      "Iteration 47410 Training loss 0.053500451147556305 Validation loss 0.0591743178665638 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.8379],\n",
      "        [0.8915]], device='mps:0')\n",
      "Iteration 47420 Training loss 0.05378606542944908 Validation loss 0.05919117107987404 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.2540],\n",
      "        [0.1267]], device='mps:0')\n",
      "Iteration 47430 Training loss 0.06312769651412964 Validation loss 0.05923013389110565 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.8807],\n",
      "        [0.9633]], device='mps:0')\n",
      "Iteration 47440 Training loss 0.050704240798950195 Validation loss 0.05918191000819206 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.2966],\n",
      "        [0.8344]], device='mps:0')\n",
      "Iteration 47450 Training loss 0.059274572879076004 Validation loss 0.0591806098818779 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.0202],\n",
      "        [0.8458]], device='mps:0')\n",
      "Iteration 47460 Training loss 0.05651669204235077 Validation loss 0.05929739400744438 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.7470],\n",
      "        [0.9888]], device='mps:0')\n",
      "Iteration 47470 Training loss 0.05183960497379303 Validation loss 0.05927308276295662 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.9338],\n",
      "        [0.5314]], device='mps:0')\n",
      "Iteration 47480 Training loss 0.06061995401978493 Validation loss 0.059193626046180725 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.0886],\n",
      "        [0.8031]], device='mps:0')\n",
      "Iteration 47490 Training loss 0.06022462621331215 Validation loss 0.05917484685778618 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.9903],\n",
      "        [0.0417]], device='mps:0')\n",
      "Iteration 47500 Training loss 0.06161665916442871 Validation loss 0.059167128056287766 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0311],\n",
      "        [0.4520]], device='mps:0')\n",
      "Iteration 47510 Training loss 0.05257439613342285 Validation loss 0.059230074286460876 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.9230],\n",
      "        [0.1653]], device='mps:0')\n",
      "Iteration 47520 Training loss 0.06983625143766403 Validation loss 0.05919412896037102 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.2476],\n",
      "        [0.8531]], device='mps:0')\n",
      "Iteration 47530 Training loss 0.05527016520500183 Validation loss 0.05917209014296532 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.2560],\n",
      "        [0.2120]], device='mps:0')\n",
      "Iteration 47540 Training loss 0.056427039206027985 Validation loss 0.059209104627370834 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.7887],\n",
      "        [0.1981]], device='mps:0')\n",
      "Iteration 47550 Training loss 0.05333448201417923 Validation loss 0.059242911636829376 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.9439],\n",
      "        [0.2738]], device='mps:0')\n",
      "Iteration 47560 Training loss 0.05282725393772125 Validation loss 0.059306103736162186 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.0528],\n",
      "        [0.3423]], device='mps:0')\n",
      "Iteration 47570 Training loss 0.05613195151090622 Validation loss 0.059401486068964005 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.9108],\n",
      "        [0.1743]], device='mps:0')\n",
      "Iteration 47580 Training loss 0.05080286040902138 Validation loss 0.05922644957900047 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.1937],\n",
      "        [0.9434]], device='mps:0')\n",
      "Iteration 47590 Training loss 0.057679202407598495 Validation loss 0.059170566499233246 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.8274],\n",
      "        [0.1936]], device='mps:0')\n",
      "Iteration 47600 Training loss 0.057721879333257675 Validation loss 0.05919916182756424 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.5689],\n",
      "        [0.4152]], device='mps:0')\n",
      "Iteration 47610 Training loss 0.04511040449142456 Validation loss 0.05933273211121559 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.1130],\n",
      "        [0.6620]], device='mps:0')\n",
      "Iteration 47620 Training loss 0.06212856248021126 Validation loss 0.059242334216833115 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.3740],\n",
      "        [0.7894]], device='mps:0')\n",
      "Iteration 47630 Training loss 0.05257077142596245 Validation loss 0.059157948940992355 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0189],\n",
      "        [0.8768]], device='mps:0')\n",
      "Iteration 47640 Training loss 0.04996689781546593 Validation loss 0.05915747955441475 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.1604],\n",
      "        [0.0652]], device='mps:0')\n",
      "Iteration 47650 Training loss 0.05805031582713127 Validation loss 0.059171490371227264 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.2754],\n",
      "        [0.8877]], device='mps:0')\n",
      "Iteration 47660 Training loss 0.058893706649541855 Validation loss 0.05924171954393387 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.1393],\n",
      "        [0.1610]], device='mps:0')\n",
      "Iteration 47670 Training loss 0.05846405401825905 Validation loss 0.05931119620800018 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.9921],\n",
      "        [0.8450]], device='mps:0')\n",
      "Iteration 47680 Training loss 0.07114341109991074 Validation loss 0.0591677725315094 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.5697],\n",
      "        [0.0260]], device='mps:0')\n",
      "Iteration 47690 Training loss 0.06435325741767883 Validation loss 0.059157680720090866 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.6098],\n",
      "        [0.5929]], device='mps:0')\n",
      "Iteration 47700 Training loss 0.05364246293902397 Validation loss 0.05917336046695709 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.3027],\n",
      "        [0.6061]], device='mps:0')\n",
      "Iteration 47710 Training loss 0.06116441637277603 Validation loss 0.05921665579080582 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.0522],\n",
      "        [0.0819]], device='mps:0')\n",
      "Iteration 47720 Training loss 0.06518208235502243 Validation loss 0.059168506413698196 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.4049],\n",
      "        [0.9687]], device='mps:0')\n",
      "Iteration 47730 Training loss 0.06529666483402252 Validation loss 0.05916519835591316 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.3283],\n",
      "        [0.8830]], device='mps:0')\n",
      "Iteration 47740 Training loss 0.05271851271390915 Validation loss 0.059163112193346024 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.3287],\n",
      "        [0.9275]], device='mps:0')\n",
      "Iteration 47750 Training loss 0.056905996054410934 Validation loss 0.05930265039205551 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.6569],\n",
      "        [0.9722]], device='mps:0')\n",
      "Iteration 47760 Training loss 0.058228734880685806 Validation loss 0.05930433049798012 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.8651],\n",
      "        [0.1208]], device='mps:0')\n",
      "Iteration 47770 Training loss 0.05599742755293846 Validation loss 0.059159670025110245 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.9850],\n",
      "        [0.1443]], device='mps:0')\n",
      "Iteration 47780 Training loss 0.0660746619105339 Validation loss 0.05916889011859894 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0272],\n",
      "        [0.8487]], device='mps:0')\n",
      "Iteration 47790 Training loss 0.058575257658958435 Validation loss 0.059159405529499054 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.8793],\n",
      "        [0.6537]], device='mps:0')\n",
      "Iteration 47800 Training loss 0.05890907719731331 Validation loss 0.05916453152894974 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.9921],\n",
      "        [0.3039]], device='mps:0')\n",
      "Iteration 47810 Training loss 0.053344544023275375 Validation loss 0.05921007692813873 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.1796],\n",
      "        [0.7355]], device='mps:0')\n",
      "Iteration 47820 Training loss 0.058421581983566284 Validation loss 0.05966472625732422 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.4747],\n",
      "        [0.3118]], device='mps:0')\n",
      "Iteration 47830 Training loss 0.056583061814308167 Validation loss 0.05921969190239906 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.9706],\n",
      "        [0.1044]], device='mps:0')\n",
      "Iteration 47840 Training loss 0.0665256604552269 Validation loss 0.05918063968420029 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.1507],\n",
      "        [0.2477]], device='mps:0')\n",
      "Iteration 47850 Training loss 0.0571124404668808 Validation loss 0.05916237831115723 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.6171],\n",
      "        [0.2661]], device='mps:0')\n",
      "Iteration 47860 Training loss 0.05319847911596298 Validation loss 0.05916198715567589 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.7804],\n",
      "        [0.1870]], device='mps:0')\n",
      "Iteration 47870 Training loss 0.06457454711198807 Validation loss 0.059303753077983856 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.5658],\n",
      "        [0.9039]], device='mps:0')\n",
      "Iteration 47880 Training loss 0.060918763279914856 Validation loss 0.05919741466641426 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.8861],\n",
      "        [0.9241]], device='mps:0')\n",
      "Iteration 47890 Training loss 0.051274728029966354 Validation loss 0.059187259525060654 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.8208],\n",
      "        [0.2628]], device='mps:0')\n",
      "Iteration 47900 Training loss 0.06753529608249664 Validation loss 0.059216707944869995 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.7841],\n",
      "        [0.8184]], device='mps:0')\n",
      "Iteration 47910 Training loss 0.061555929481983185 Validation loss 0.05917033553123474 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0524],\n",
      "        [0.8332]], device='mps:0')\n",
      "Iteration 47920 Training loss 0.055137842893600464 Validation loss 0.05927435681223869 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.2425],\n",
      "        [0.8070]], device='mps:0')\n",
      "Iteration 47930 Training loss 0.05837486311793327 Validation loss 0.05915794521570206 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.8641],\n",
      "        [0.8938]], device='mps:0')\n",
      "Iteration 47940 Training loss 0.051132671535015106 Validation loss 0.05920762941241264 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.8326],\n",
      "        [0.5934]], device='mps:0')\n",
      "Iteration 47950 Training loss 0.052323803305625916 Validation loss 0.059155791997909546 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.4322],\n",
      "        [0.9990]], device='mps:0')\n",
      "Iteration 47960 Training loss 0.05984993651509285 Validation loss 0.05924869701266289 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.0505],\n",
      "        [0.4935]], device='mps:0')\n",
      "Iteration 47970 Training loss 0.05449092760682106 Validation loss 0.05915789306163788 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9222],\n",
      "        [0.0261]], device='mps:0')\n",
      "Iteration 47980 Training loss 0.05382067710161209 Validation loss 0.0591529943048954 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.3789],\n",
      "        [0.0878]], device='mps:0')\n",
      "Iteration 47990 Training loss 0.06546571850776672 Validation loss 0.059172388166189194 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.0482],\n",
      "        [0.0904]], device='mps:0')\n",
      "Iteration 48000 Training loss 0.06397814303636551 Validation loss 0.0591353140771389 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.1061],\n",
      "        [0.4426]], device='mps:0')\n",
      "Iteration 48010 Training loss 0.06261970102787018 Validation loss 0.05914071574807167 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.6917],\n",
      "        [0.0230]], device='mps:0')\n",
      "Iteration 48020 Training loss 0.052253760397434235 Validation loss 0.05926738306879997 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.4547],\n",
      "        [0.5854]], device='mps:0')\n",
      "Iteration 48030 Training loss 0.055041078478097916 Validation loss 0.0591425746679306 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.8333],\n",
      "        [0.6363]], device='mps:0')\n",
      "Iteration 48040 Training loss 0.0538572296500206 Validation loss 0.059300921857357025 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.9795],\n",
      "        [0.9472]], device='mps:0')\n",
      "Iteration 48050 Training loss 0.057286154478788376 Validation loss 0.05931869149208069 Accuracy 0.8376250267028809\n",
      "Output tensor([[0.9844],\n",
      "        [0.1084]], device='mps:0')\n",
      "Iteration 48060 Training loss 0.049822814762592316 Validation loss 0.05916598066687584 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.5355],\n",
      "        [0.9577]], device='mps:0')\n",
      "Iteration 48070 Training loss 0.057033710181713104 Validation loss 0.059315282851457596 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.4554],\n",
      "        [0.2951]], device='mps:0')\n",
      "Iteration 48080 Training loss 0.053570907562971115 Validation loss 0.05921415239572525 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.9599],\n",
      "        [0.1968]], device='mps:0')\n",
      "Iteration 48090 Training loss 0.05659158527851105 Validation loss 0.059153541922569275 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.3896],\n",
      "        [0.9860]], device='mps:0')\n",
      "Iteration 48100 Training loss 0.05158289149403572 Validation loss 0.05914013832807541 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.1543],\n",
      "        [0.5296]], device='mps:0')\n",
      "Iteration 48110 Training loss 0.05246877670288086 Validation loss 0.05917177349328995 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.2470],\n",
      "        [0.7552]], device='mps:0')\n",
      "Iteration 48120 Training loss 0.05468069016933441 Validation loss 0.0591411329805851 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.1652],\n",
      "        [0.9702]], device='mps:0')\n",
      "Iteration 48130 Training loss 0.05923395976424217 Validation loss 0.059137698262929916 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0529],\n",
      "        [0.0165]], device='mps:0')\n",
      "Iteration 48140 Training loss 0.05597427859902382 Validation loss 0.059148550033569336 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.5491],\n",
      "        [0.3997]], device='mps:0')\n",
      "Iteration 48150 Training loss 0.05840097367763519 Validation loss 0.05914293974637985 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9892],\n",
      "        [0.8564]], device='mps:0')\n",
      "Iteration 48160 Training loss 0.05341838672757149 Validation loss 0.059142619371414185 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.7253],\n",
      "        [0.0140]], device='mps:0')\n",
      "Iteration 48170 Training loss 0.050282228738069534 Validation loss 0.05920921266078949 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.6964],\n",
      "        [0.0778]], device='mps:0')\n",
      "Iteration 48180 Training loss 0.05615997686982155 Validation loss 0.05915587767958641 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.8039],\n",
      "        [0.0186]], device='mps:0')\n",
      "Iteration 48190 Training loss 0.05910740792751312 Validation loss 0.059141166508197784 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.4323],\n",
      "        [0.7400]], device='mps:0')\n",
      "Iteration 48200 Training loss 0.05549371987581253 Validation loss 0.059206072241067886 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9565],\n",
      "        [0.4047]], device='mps:0')\n",
      "Iteration 48210 Training loss 0.0536087267100811 Validation loss 0.059140294790267944 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.0554],\n",
      "        [0.7839]], device='mps:0')\n",
      "Iteration 48220 Training loss 0.05559909716248512 Validation loss 0.05921536684036255 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.5662],\n",
      "        [0.5471]], device='mps:0')\n",
      "Iteration 48230 Training loss 0.0620092935860157 Validation loss 0.05914834886789322 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.8740],\n",
      "        [0.4696]], device='mps:0')\n",
      "Iteration 48240 Training loss 0.05449799448251724 Validation loss 0.05917910486459732 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.6795],\n",
      "        [0.3205]], device='mps:0')\n",
      "Iteration 48250 Training loss 0.055548060685396194 Validation loss 0.059150781482458115 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.6229],\n",
      "        [0.1772]], device='mps:0')\n",
      "Iteration 48260 Training loss 0.05998922139406204 Validation loss 0.05916469916701317 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.8634],\n",
      "        [0.9811]], device='mps:0')\n",
      "Iteration 48270 Training loss 0.05156273767352104 Validation loss 0.05933063104748726 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.8414],\n",
      "        [0.0867]], device='mps:0')\n",
      "Iteration 48280 Training loss 0.05799533799290657 Validation loss 0.05914480239152908 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.4188],\n",
      "        [0.1371]], device='mps:0')\n",
      "Iteration 48290 Training loss 0.056016530841588974 Validation loss 0.05917157605290413 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.7192],\n",
      "        [0.9479]], device='mps:0')\n",
      "Iteration 48300 Training loss 0.05680949240922928 Validation loss 0.05915858969092369 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9186],\n",
      "        [0.5196]], device='mps:0')\n",
      "Iteration 48310 Training loss 0.06505490094423294 Validation loss 0.05913642793893814 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.2700],\n",
      "        [0.0042]], device='mps:0')\n",
      "Iteration 48320 Training loss 0.0537351593375206 Validation loss 0.05915240943431854 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.1454],\n",
      "        [0.6920]], device='mps:0')\n",
      "Iteration 48330 Training loss 0.0629386156797409 Validation loss 0.05924663692712784 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.0639],\n",
      "        [0.9688]], device='mps:0')\n",
      "Iteration 48340 Training loss 0.06071920320391655 Validation loss 0.059128955006599426 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.9527],\n",
      "        [0.9036]], device='mps:0')\n",
      "Iteration 48350 Training loss 0.057473037391901016 Validation loss 0.059167180210351944 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.9226],\n",
      "        [0.9555]], device='mps:0')\n",
      "Iteration 48360 Training loss 0.04561632499098778 Validation loss 0.05912133306264877 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.1291],\n",
      "        [0.8556]], device='mps:0')\n",
      "Iteration 48370 Training loss 0.05963955819606781 Validation loss 0.05912020429968834 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.6382],\n",
      "        [0.2663]], device='mps:0')\n",
      "Iteration 48380 Training loss 0.0644584372639656 Validation loss 0.059178758412599564 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.6018],\n",
      "        [0.9185]], device='mps:0')\n",
      "Iteration 48390 Training loss 0.054970867931842804 Validation loss 0.05912156403064728 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.4268],\n",
      "        [0.2384]], device='mps:0')\n",
      "Iteration 48400 Training loss 0.053865715861320496 Validation loss 0.059138040989637375 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.1790],\n",
      "        [0.0481]], device='mps:0')\n",
      "Iteration 48410 Training loss 0.064700186252594 Validation loss 0.05912383645772934 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0058],\n",
      "        [0.8267]], device='mps:0')\n",
      "Iteration 48420 Training loss 0.056775983422994614 Validation loss 0.05919506028294563 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.0194],\n",
      "        [0.6910]], device='mps:0')\n",
      "Iteration 48430 Training loss 0.053823087364435196 Validation loss 0.05912266671657562 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.9728],\n",
      "        [0.0265]], device='mps:0')\n",
      "Iteration 48440 Training loss 0.05332314968109131 Validation loss 0.05911709740757942 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.9654],\n",
      "        [0.9855]], device='mps:0')\n",
      "Iteration 48450 Training loss 0.057904500514268875 Validation loss 0.05952126532793045 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.2564],\n",
      "        [0.9505]], device='mps:0')\n",
      "Iteration 48460 Training loss 0.05302063748240471 Validation loss 0.059119805693626404 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.3395],\n",
      "        [0.1062]], device='mps:0')\n",
      "Iteration 48470 Training loss 0.060145214200019836 Validation loss 0.05911899730563164 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0129],\n",
      "        [0.0140]], device='mps:0')\n",
      "Iteration 48480 Training loss 0.05775998905301094 Validation loss 0.05916742980480194 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.5548],\n",
      "        [0.0559]], device='mps:0')\n",
      "Iteration 48490 Training loss 0.06005550175905228 Validation loss 0.05911751091480255 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9576],\n",
      "        [0.8562]], device='mps:0')\n",
      "Iteration 48500 Training loss 0.06044352427124977 Validation loss 0.0591312050819397 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0429],\n",
      "        [0.1810]], device='mps:0')\n",
      "Iteration 48510 Training loss 0.06114622950553894 Validation loss 0.059127021580934525 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.2105],\n",
      "        [0.8958]], device='mps:0')\n",
      "Iteration 48520 Training loss 0.05459676682949066 Validation loss 0.05920616537332535 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.0204],\n",
      "        [0.6815]], device='mps:0')\n",
      "Iteration 48530 Training loss 0.05725066363811493 Validation loss 0.05911971628665924 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.7198],\n",
      "        [0.7622]], device='mps:0')\n",
      "Iteration 48540 Training loss 0.05811883136630058 Validation loss 0.05909936502575874 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.9095],\n",
      "        [0.9813]], device='mps:0')\n",
      "Iteration 48550 Training loss 0.057131242007017136 Validation loss 0.05911538377404213 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.1217],\n",
      "        [0.0249]], device='mps:0')\n",
      "Iteration 48560 Training loss 0.053989071398973465 Validation loss 0.05915934592485428 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.4318],\n",
      "        [0.0264]], device='mps:0')\n",
      "Iteration 48570 Training loss 0.0573187917470932 Validation loss 0.05911645665764809 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.2373],\n",
      "        [0.0058]], device='mps:0')\n",
      "Iteration 48580 Training loss 0.06063339114189148 Validation loss 0.05911172181367874 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.5346],\n",
      "        [0.0565]], device='mps:0')\n",
      "Iteration 48590 Training loss 0.05114373937249184 Validation loss 0.059096239507198334 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.9375],\n",
      "        [0.0150]], device='mps:0')\n",
      "Iteration 48600 Training loss 0.057260673493146896 Validation loss 0.059254761785268784 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.4781],\n",
      "        [0.6776]], device='mps:0')\n",
      "Iteration 48610 Training loss 0.060920845717191696 Validation loss 0.059138257056474686 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.3038],\n",
      "        [0.8739]], device='mps:0')\n",
      "Iteration 48620 Training loss 0.06333703547716141 Validation loss 0.05934343859553337 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.0054],\n",
      "        [0.1734]], device='mps:0')\n",
      "Iteration 48630 Training loss 0.057917870581150055 Validation loss 0.05910094454884529 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.0101],\n",
      "        [0.2375]], device='mps:0')\n",
      "Iteration 48640 Training loss 0.05698632448911667 Validation loss 0.059137340635061264 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.6765],\n",
      "        [0.0310]], device='mps:0')\n",
      "Iteration 48650 Training loss 0.06591858714818954 Validation loss 0.05908220261335373 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.5484],\n",
      "        [0.0178]], device='mps:0')\n",
      "Iteration 48660 Training loss 0.0608852356672287 Validation loss 0.059084005653858185 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.2516],\n",
      "        [0.4280]], device='mps:0')\n",
      "Iteration 48670 Training loss 0.06144096329808235 Validation loss 0.05910573899745941 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.6139],\n",
      "        [0.9585]], device='mps:0')\n",
      "Iteration 48680 Training loss 0.05315642058849335 Validation loss 0.0590839721262455 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.1580],\n",
      "        [0.6576]], device='mps:0')\n",
      "Iteration 48690 Training loss 0.0522172711789608 Validation loss 0.05911937728524208 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.1045],\n",
      "        [0.1201]], device='mps:0')\n",
      "Iteration 48700 Training loss 0.05283614993095398 Validation loss 0.05911726504564285 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.1740],\n",
      "        [0.1906]], device='mps:0')\n",
      "Iteration 48710 Training loss 0.05927169695496559 Validation loss 0.05957503989338875 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.9948],\n",
      "        [0.0292]], device='mps:0')\n",
      "Iteration 48720 Training loss 0.0523994006216526 Validation loss 0.059107501059770584 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9438],\n",
      "        [0.9210]], device='mps:0')\n",
      "Iteration 48730 Training loss 0.06831512600183487 Validation loss 0.0591476634144783 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.1240],\n",
      "        [0.9866]], device='mps:0')\n",
      "Iteration 48740 Training loss 0.0530182383954525 Validation loss 0.05929814651608467 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0663],\n",
      "        [0.2306]], device='mps:0')\n",
      "Iteration 48750 Training loss 0.05551739037036896 Validation loss 0.059093814343214035 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.1877],\n",
      "        [0.0814]], device='mps:0')\n",
      "Iteration 48760 Training loss 0.06600859761238098 Validation loss 0.05908305570483208 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.7622],\n",
      "        [0.9060]], device='mps:0')\n",
      "Iteration 48770 Training loss 0.05377129092812538 Validation loss 0.05908222123980522 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9415],\n",
      "        [0.4523]], device='mps:0')\n",
      "Iteration 48780 Training loss 0.06078596040606499 Validation loss 0.05908047780394554 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.4826],\n",
      "        [0.9974]], device='mps:0')\n",
      "Iteration 48790 Training loss 0.049954913556575775 Validation loss 0.05909263715147972 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.8869],\n",
      "        [0.0720]], device='mps:0')\n",
      "Iteration 48800 Training loss 0.06404651701450348 Validation loss 0.059103868901729584 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.6341],\n",
      "        [0.9867]], device='mps:0')\n",
      "Iteration 48810 Training loss 0.056796811521053314 Validation loss 0.059165049344301224 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.1210],\n",
      "        [0.5329]], device='mps:0')\n",
      "Iteration 48820 Training loss 0.06025834009051323 Validation loss 0.059112682938575745 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.0503],\n",
      "        [0.9278]], device='mps:0')\n",
      "Iteration 48830 Training loss 0.0494229793548584 Validation loss 0.05909712612628937 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.9775],\n",
      "        [0.4111]], device='mps:0')\n",
      "Iteration 48840 Training loss 0.05089932307600975 Validation loss 0.059171538800001144 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.1342],\n",
      "        [0.9263]], device='mps:0')\n",
      "Iteration 48850 Training loss 0.06162094324827194 Validation loss 0.059104837477207184 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.0506],\n",
      "        [0.3383]], device='mps:0')\n",
      "Iteration 48860 Training loss 0.052023038268089294 Validation loss 0.059074416756629944 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.3813],\n",
      "        [0.9093]], device='mps:0')\n",
      "Iteration 48870 Training loss 0.058837778866291046 Validation loss 0.059175148606300354 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.3296],\n",
      "        [0.8387]], device='mps:0')\n",
      "Iteration 48880 Training loss 0.06066974624991417 Validation loss 0.05907757207751274 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.1161],\n",
      "        [0.8191]], device='mps:0')\n",
      "Iteration 48890 Training loss 0.05247494578361511 Validation loss 0.05910550802946091 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.6008],\n",
      "        [0.9381]], device='mps:0')\n",
      "Iteration 48900 Training loss 0.05418321490287781 Validation loss 0.05907783284783363 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.8840],\n",
      "        [0.9519]], device='mps:0')\n",
      "Iteration 48910 Training loss 0.05498544126749039 Validation loss 0.059127628803253174 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.1672],\n",
      "        [0.6054]], device='mps:0')\n",
      "Iteration 48920 Training loss 0.04949900135397911 Validation loss 0.059072259813547134 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.6254],\n",
      "        [0.0472]], device='mps:0')\n",
      "Iteration 48930 Training loss 0.05756672844290733 Validation loss 0.05910365656018257 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.4406],\n",
      "        [0.6191]], device='mps:0')\n",
      "Iteration 48940 Training loss 0.06394404917955399 Validation loss 0.05912010371685028 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.9673],\n",
      "        [0.9674]], device='mps:0')\n",
      "Iteration 48950 Training loss 0.0561339445412159 Validation loss 0.0592157244682312 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.0876],\n",
      "        [0.0097]], device='mps:0')\n",
      "Iteration 48960 Training loss 0.047128744423389435 Validation loss 0.059096649289131165 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.7816],\n",
      "        [0.8214]], device='mps:0')\n",
      "Iteration 48970 Training loss 0.06436984241008759 Validation loss 0.0590963177382946 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.1113],\n",
      "        [0.3012]], device='mps:0')\n",
      "Iteration 48980 Training loss 0.0532163605093956 Validation loss 0.05919092893600464 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.4253],\n",
      "        [0.3439]], device='mps:0')\n",
      "Iteration 48990 Training loss 0.05338939651846886 Validation loss 0.05928090959787369 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.2118],\n",
      "        [0.7649]], device='mps:0')\n",
      "Iteration 49000 Training loss 0.05975420027971268 Validation loss 0.059103891253471375 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.8847],\n",
      "        [0.8346]], device='mps:0')\n",
      "Iteration 49010 Training loss 0.05961685627698898 Validation loss 0.05908221751451492 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.3847],\n",
      "        [0.0326]], device='mps:0')\n",
      "Iteration 49020 Training loss 0.05057245120406151 Validation loss 0.05908247455954552 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.8745],\n",
      "        [0.6365]], device='mps:0')\n",
      "Iteration 49030 Training loss 0.0533226877450943 Validation loss 0.0591171458363533 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9170],\n",
      "        [0.3824]], device='mps:0')\n",
      "Iteration 49040 Training loss 0.053849704563617706 Validation loss 0.059106409549713135 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.9590],\n",
      "        [0.1825]], device='mps:0')\n",
      "Iteration 49050 Training loss 0.05873629450798035 Validation loss 0.059169184416532516 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.6040],\n",
      "        [0.7829]], device='mps:0')\n",
      "Iteration 49060 Training loss 0.06385082751512527 Validation loss 0.05911054462194443 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.8510],\n",
      "        [0.4396]], device='mps:0')\n",
      "Iteration 49070 Training loss 0.05360240861773491 Validation loss 0.059182308614254 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.8943],\n",
      "        [0.4777]], device='mps:0')\n",
      "Iteration 49080 Training loss 0.04937189817428589 Validation loss 0.05914407595992088 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.0180],\n",
      "        [0.9343]], device='mps:0')\n",
      "Iteration 49090 Training loss 0.05735998600721359 Validation loss 0.05916673690080643 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.9181],\n",
      "        [0.2313]], device='mps:0')\n",
      "Iteration 49100 Training loss 0.05106477439403534 Validation loss 0.059090353548526764 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.0605],\n",
      "        [0.3483]], device='mps:0')\n",
      "Iteration 49110 Training loss 0.05745616927742958 Validation loss 0.059085920453071594 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.4449],\n",
      "        [0.9573]], device='mps:0')\n",
      "Iteration 49120 Training loss 0.050153616815805435 Validation loss 0.05924864485859871 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.9727],\n",
      "        [0.1325]], device='mps:0')\n",
      "Iteration 49130 Training loss 0.05841415375471115 Validation loss 0.05929785594344139 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0053],\n",
      "        [0.0469]], device='mps:0')\n",
      "Iteration 49140 Training loss 0.052188482135534286 Validation loss 0.059303559362888336 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.7737],\n",
      "        [0.7224]], device='mps:0')\n",
      "Iteration 49150 Training loss 0.05946163460612297 Validation loss 0.05910370871424675 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.6775],\n",
      "        [0.8531]], device='mps:0')\n",
      "Iteration 49160 Training loss 0.050506312400102615 Validation loss 0.05911665037274361 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.2797],\n",
      "        [0.0104]], device='mps:0')\n",
      "Iteration 49170 Training loss 0.05823703482747078 Validation loss 0.05950911343097687 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.0830],\n",
      "        [0.9384]], device='mps:0')\n",
      "Iteration 49180 Training loss 0.057003770023584366 Validation loss 0.05908431112766266 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.0222],\n",
      "        [0.0830]], device='mps:0')\n",
      "Iteration 49190 Training loss 0.05113915354013443 Validation loss 0.0590708963572979 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.4597],\n",
      "        [0.5924]], device='mps:0')\n",
      "Iteration 49200 Training loss 0.06008339673280716 Validation loss 0.059117335826158524 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.0508],\n",
      "        [0.0660]], device='mps:0')\n",
      "Iteration 49210 Training loss 0.05438866466283798 Validation loss 0.05908060073852539 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.7494],\n",
      "        [0.2105]], device='mps:0')\n",
      "Iteration 49220 Training loss 0.05203673616051674 Validation loss 0.059088703244924545 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.0814],\n",
      "        [0.9564]], device='mps:0')\n",
      "Iteration 49230 Training loss 0.043130338191986084 Validation loss 0.05924473702907562 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.1122],\n",
      "        [0.9528]], device='mps:0')\n",
      "Iteration 49240 Training loss 0.0541817843914032 Validation loss 0.05907583236694336 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.1989],\n",
      "        [0.0575]], device='mps:0')\n",
      "Iteration 49250 Training loss 0.057254400104284286 Validation loss 0.05907171964645386 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.8534],\n",
      "        [0.2732]], device='mps:0')\n",
      "Iteration 49260 Training loss 0.06008516997098923 Validation loss 0.05918879061937332 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.0738],\n",
      "        [0.0276]], device='mps:0')\n",
      "Iteration 49270 Training loss 0.05741836503148079 Validation loss 0.05907560512423515 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0032],\n",
      "        [0.9907]], device='mps:0')\n",
      "Iteration 49280 Training loss 0.062456388026475906 Validation loss 0.0590653270483017 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.7899],\n",
      "        [0.0799]], device='mps:0')\n",
      "Iteration 49290 Training loss 0.052670761942863464 Validation loss 0.05926481634378433 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.9785],\n",
      "        [0.1254]], device='mps:0')\n",
      "Iteration 49300 Training loss 0.0625944510102272 Validation loss 0.0590658113360405 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.8960],\n",
      "        [0.8935]], device='mps:0')\n",
      "Iteration 49310 Training loss 0.05832456052303314 Validation loss 0.0590786449611187 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.9887],\n",
      "        [0.4191]], device='mps:0')\n",
      "Iteration 49320 Training loss 0.05247799679636955 Validation loss 0.059088077396154404 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.9095],\n",
      "        [0.8969]], device='mps:0')\n",
      "Iteration 49330 Training loss 0.05543842166662216 Validation loss 0.05910174921154976 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0513],\n",
      "        [0.9215]], device='mps:0')\n",
      "Iteration 49340 Training loss 0.05221179500222206 Validation loss 0.05908622965216637 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.8641],\n",
      "        [0.0174]], device='mps:0')\n",
      "Iteration 49350 Training loss 0.06556432694196701 Validation loss 0.05906397104263306 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.4968],\n",
      "        [0.3524]], device='mps:0')\n",
      "Iteration 49360 Training loss 0.06185804307460785 Validation loss 0.05921926349401474 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.8988],\n",
      "        [0.0466]], device='mps:0')\n",
      "Iteration 49370 Training loss 0.05896048992872238 Validation loss 0.05912300571799278 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.4553],\n",
      "        [0.0075]], device='mps:0')\n",
      "Iteration 49380 Training loss 0.06361375004053116 Validation loss 0.0591999888420105 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.9073],\n",
      "        [0.5968]], device='mps:0')\n",
      "Iteration 49390 Training loss 0.06442893296480179 Validation loss 0.05905530974268913 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0603],\n",
      "        [0.3254]], device='mps:0')\n",
      "Iteration 49400 Training loss 0.059296078979969025 Validation loss 0.05909411236643791 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.7793],\n",
      "        [0.9702]], device='mps:0')\n",
      "Iteration 49410 Training loss 0.06188157945871353 Validation loss 0.059061694890260696 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.0920],\n",
      "        [0.0602]], device='mps:0')\n",
      "Iteration 49420 Training loss 0.05922773852944374 Validation loss 0.059204746037721634 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.0773],\n",
      "        [0.9728]], device='mps:0')\n",
      "Iteration 49430 Training loss 0.05991274118423462 Validation loss 0.059056978672742844 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0107],\n",
      "        [0.7723]], device='mps:0')\n",
      "Iteration 49440 Training loss 0.05886279046535492 Validation loss 0.05910575017333031 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.9465],\n",
      "        [0.9740]], device='mps:0')\n",
      "Iteration 49450 Training loss 0.045823972672224045 Validation loss 0.05915503576397896 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.3209],\n",
      "        [0.0724]], device='mps:0')\n",
      "Iteration 49460 Training loss 0.0584942027926445 Validation loss 0.059052202850580215 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.0120],\n",
      "        [0.9599]], device='mps:0')\n",
      "Iteration 49470 Training loss 0.054548997431993484 Validation loss 0.05914746969938278 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.1712],\n",
      "        [0.7006]], device='mps:0')\n",
      "Iteration 49480 Training loss 0.048325762152671814 Validation loss 0.05913228541612625 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.8227],\n",
      "        [0.9555]], device='mps:0')\n",
      "Iteration 49490 Training loss 0.053703680634498596 Validation loss 0.05907312035560608 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.3094],\n",
      "        [0.4640]], device='mps:0')\n",
      "Iteration 49500 Training loss 0.05978618562221527 Validation loss 0.05905190110206604 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0347],\n",
      "        [0.7814]], device='mps:0')\n",
      "Iteration 49510 Training loss 0.05030493438243866 Validation loss 0.05904911458492279 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0783],\n",
      "        [0.0233]], device='mps:0')\n",
      "Iteration 49520 Training loss 0.05693807080388069 Validation loss 0.0591159462928772 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.0338],\n",
      "        [0.6417]], device='mps:0')\n",
      "Iteration 49530 Training loss 0.05137408524751663 Validation loss 0.05919325351715088 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0220],\n",
      "        [0.0968]], device='mps:0')\n",
      "Iteration 49540 Training loss 0.05015486478805542 Validation loss 0.05905982851982117 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.0116],\n",
      "        [0.9527]], device='mps:0')\n",
      "Iteration 49550 Training loss 0.05219586193561554 Validation loss 0.05907084047794342 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.6152],\n",
      "        [0.0172]], device='mps:0')\n",
      "Iteration 49560 Training loss 0.050887878984212875 Validation loss 0.05904511362314224 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0211],\n",
      "        [0.9930]], device='mps:0')\n",
      "Iteration 49570 Training loss 0.06430211663246155 Validation loss 0.05941947549581528 Accuracy 0.8371250629425049\n",
      "Output tensor([[0.7073],\n",
      "        [0.4878]], device='mps:0')\n",
      "Iteration 49580 Training loss 0.06037335842847824 Validation loss 0.05904430150985718 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9270],\n",
      "        [0.3199]], device='mps:0')\n",
      "Iteration 49590 Training loss 0.06195810064673424 Validation loss 0.05908449739217758 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.9803],\n",
      "        [0.9704]], device='mps:0')\n",
      "Iteration 49600 Training loss 0.04602278769016266 Validation loss 0.05913098528981209 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.6638],\n",
      "        [0.0847]], device='mps:0')\n",
      "Iteration 49610 Training loss 0.06175489351153374 Validation loss 0.05905207246541977 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.3286],\n",
      "        [0.2743]], device='mps:0')\n",
      "Iteration 49620 Training loss 0.05553373321890831 Validation loss 0.05903321132063866 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.1789],\n",
      "        [0.3417]], device='mps:0')\n",
      "Iteration 49630 Training loss 0.05855213850736618 Validation loss 0.05903168022632599 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.5486],\n",
      "        [0.4518]], device='mps:0')\n",
      "Iteration 49640 Training loss 0.05554533377289772 Validation loss 0.05904146656394005 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.9239],\n",
      "        [0.9405]], device='mps:0')\n",
      "Iteration 49650 Training loss 0.04887605458498001 Validation loss 0.0590694323182106 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.0037],\n",
      "        [0.2045]], device='mps:0')\n",
      "Iteration 49660 Training loss 0.06111166998744011 Validation loss 0.059037111699581146 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.6235],\n",
      "        [0.2415]], device='mps:0')\n",
      "Iteration 49670 Training loss 0.05391174554824829 Validation loss 0.059115421026945114 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0238],\n",
      "        [0.9646]], device='mps:0')\n",
      "Iteration 49680 Training loss 0.05778127536177635 Validation loss 0.0590323731303215 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0096],\n",
      "        [0.7279]], device='mps:0')\n",
      "Iteration 49690 Training loss 0.04847297444939613 Validation loss 0.05908260494470596 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.2422],\n",
      "        [0.3986]], device='mps:0')\n",
      "Iteration 49700 Training loss 0.05576256662607193 Validation loss 0.05914213880896568 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.6150],\n",
      "        [0.2693]], device='mps:0')\n",
      "Iteration 49710 Training loss 0.06040569767355919 Validation loss 0.0592142716050148 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.6484],\n",
      "        [0.5395]], device='mps:0')\n",
      "Iteration 49720 Training loss 0.050513215363025665 Validation loss 0.05905834585428238 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0095],\n",
      "        [0.9974]], device='mps:0')\n",
      "Iteration 49730 Training loss 0.049632880836725235 Validation loss 0.05908593162894249 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.0935],\n",
      "        [0.0146]], device='mps:0')\n",
      "Iteration 49740 Training loss 0.05694514140486717 Validation loss 0.059024982154369354 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.5535],\n",
      "        [0.1039]], device='mps:0')\n",
      "Iteration 49750 Training loss 0.0650710016489029 Validation loss 0.05902715399861336 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.8958],\n",
      "        [0.9126]], device='mps:0')\n",
      "Iteration 49760 Training loss 0.04757459834218025 Validation loss 0.05902298912405968 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.9393],\n",
      "        [0.2729]], device='mps:0')\n",
      "Iteration 49770 Training loss 0.05780499428510666 Validation loss 0.059089016169309616 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.4288],\n",
      "        [0.0560]], device='mps:0')\n",
      "Iteration 49780 Training loss 0.05830574035644531 Validation loss 0.05902784690260887 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0839],\n",
      "        [0.3229]], device='mps:0')\n",
      "Iteration 49790 Training loss 0.05869157612323761 Validation loss 0.059050753712654114 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.3987],\n",
      "        [0.1881]], device='mps:0')\n",
      "Iteration 49800 Training loss 0.056312426924705505 Validation loss 0.05912883207201958 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.1413],\n",
      "        [0.3012]], device='mps:0')\n",
      "Iteration 49810 Training loss 0.05440593138337135 Validation loss 0.059053368866443634 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.4250],\n",
      "        [0.0510]], device='mps:0')\n",
      "Iteration 49820 Training loss 0.05054371803998947 Validation loss 0.059065237641334534 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.5343],\n",
      "        [0.9803]], device='mps:0')\n",
      "Iteration 49830 Training loss 0.05658172443509102 Validation loss 0.05902596190571785 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9796],\n",
      "        [0.0054]], device='mps:0')\n",
      "Iteration 49840 Training loss 0.05038520321249962 Validation loss 0.0590413361787796 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.0167],\n",
      "        [0.9808]], device='mps:0')\n",
      "Iteration 49850 Training loss 0.04865334928035736 Validation loss 0.059054724872112274 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.0396],\n",
      "        [0.2275]], device='mps:0')\n",
      "Iteration 49860 Training loss 0.05480732023715973 Validation loss 0.05917126685380936 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.3758],\n",
      "        [0.1899]], device='mps:0')\n",
      "Iteration 49870 Training loss 0.05807526409626007 Validation loss 0.05901917815208435 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.0188],\n",
      "        [0.6474]], device='mps:0')\n",
      "Iteration 49880 Training loss 0.05696450173854828 Validation loss 0.05904374271631241 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.4958],\n",
      "        [0.1625]], device='mps:0')\n",
      "Iteration 49890 Training loss 0.06018130108714104 Validation loss 0.059108637273311615 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.7299],\n",
      "        [0.0216]], device='mps:0')\n",
      "Iteration 49900 Training loss 0.05932915583252907 Validation loss 0.05910665914416313 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.6524],\n",
      "        [0.0755]], device='mps:0')\n",
      "Iteration 49910 Training loss 0.051394399255514145 Validation loss 0.05905374512076378 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.1601],\n",
      "        [0.1193]], device='mps:0')\n",
      "Iteration 49920 Training loss 0.06460490077733994 Validation loss 0.05902145802974701 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.9167],\n",
      "        [0.9202]], device='mps:0')\n",
      "Iteration 49930 Training loss 0.05806410685181618 Validation loss 0.0590476356446743 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.7774],\n",
      "        [0.7955]], device='mps:0')\n",
      "Iteration 49940 Training loss 0.06103820726275444 Validation loss 0.05900914967060089 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.9816],\n",
      "        [0.5279]], device='mps:0')\n",
      "Iteration 49950 Training loss 0.05705185607075691 Validation loss 0.0590851791203022 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.7533],\n",
      "        [0.1207]], device='mps:0')\n",
      "Iteration 49960 Training loss 0.05492701753973961 Validation loss 0.05909842997789383 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9933],\n",
      "        [0.9994]], device='mps:0')\n",
      "Iteration 49970 Training loss 0.06508193910121918 Validation loss 0.05902507156133652 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.1105],\n",
      "        [0.9389]], device='mps:0')\n",
      "Iteration 49980 Training loss 0.05953167751431465 Validation loss 0.05905095115303993 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.1649],\n",
      "        [0.3135]], device='mps:0')\n",
      "Iteration 49990 Training loss 0.0599689744412899 Validation loss 0.059067659080028534 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.7125],\n",
      "        [0.8681]], device='mps:0')\n",
      "Iteration 50000 Training loss 0.056080155074596405 Validation loss 0.05901017412543297 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.9951],\n",
      "        [0.8079]], device='mps:0')\n",
      "Iteration 50010 Training loss 0.062110595405101776 Validation loss 0.05901624634861946 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.1206],\n",
      "        [0.3642]], device='mps:0')\n",
      "Iteration 50020 Training loss 0.0497080460190773 Validation loss 0.059132613241672516 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.3719],\n",
      "        [0.1674]], device='mps:0')\n",
      "Iteration 50030 Training loss 0.05503523349761963 Validation loss 0.05904011055827141 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.1738],\n",
      "        [0.8679]], device='mps:0')\n",
      "Iteration 50040 Training loss 0.06206495314836502 Validation loss 0.059012383222579956 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.7565],\n",
      "        [0.0916]], device='mps:0')\n",
      "Iteration 50050 Training loss 0.057365674525499344 Validation loss 0.05902663990855217 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.1186],\n",
      "        [0.2160]], device='mps:0')\n",
      "Iteration 50060 Training loss 0.052415065467357635 Validation loss 0.05914987623691559 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.8222],\n",
      "        [0.0480]], device='mps:0')\n",
      "Iteration 50070 Training loss 0.057943567633628845 Validation loss 0.05902538448572159 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.1063],\n",
      "        [0.2840]], device='mps:0')\n",
      "Iteration 50080 Training loss 0.06038570776581764 Validation loss 0.05906099081039429 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.0375],\n",
      "        [0.7512]], device='mps:0')\n",
      "Iteration 50090 Training loss 0.04689189791679382 Validation loss 0.059076275676488876 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.6860],\n",
      "        [0.0678]], device='mps:0')\n",
      "Iteration 50100 Training loss 0.05899132415652275 Validation loss 0.059004779905080795 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.9512],\n",
      "        [0.0716]], device='mps:0')\n",
      "Iteration 50110 Training loss 0.058153435587882996 Validation loss 0.059091921895742416 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.0481],\n",
      "        [0.0205]], device='mps:0')\n",
      "Iteration 50120 Training loss 0.05798579007387161 Validation loss 0.059000421315431595 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9782],\n",
      "        [0.9370]], device='mps:0')\n",
      "Iteration 50130 Training loss 0.051207467913627625 Validation loss 0.05900095775723457 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.0340],\n",
      "        [0.5931]], device='mps:0')\n",
      "Iteration 50140 Training loss 0.054082706570625305 Validation loss 0.058996424078941345 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9645],\n",
      "        [0.0334]], device='mps:0')\n",
      "Iteration 50150 Training loss 0.05268441140651703 Validation loss 0.05923454463481903 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.2676],\n",
      "        [0.9080]], device='mps:0')\n",
      "Iteration 50160 Training loss 0.05744973570108414 Validation loss 0.05908334627747536 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.3234],\n",
      "        [0.0483]], device='mps:0')\n",
      "Iteration 50170 Training loss 0.057652682065963745 Validation loss 0.05899376794695854 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.4530],\n",
      "        [0.7383]], device='mps:0')\n",
      "Iteration 50180 Training loss 0.05735914036631584 Validation loss 0.05900847166776657 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.8211],\n",
      "        [0.9423]], device='mps:0')\n",
      "Iteration 50190 Training loss 0.05642029643058777 Validation loss 0.05901068449020386 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.1320],\n",
      "        [0.8722]], device='mps:0')\n",
      "Iteration 50200 Training loss 0.05825632065534592 Validation loss 0.0590149387717247 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.9267],\n",
      "        [0.0152]], device='mps:0')\n",
      "Iteration 50210 Training loss 0.05120779946446419 Validation loss 0.05900746211409569 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.9915],\n",
      "        [0.6595]], device='mps:0')\n",
      "Iteration 50220 Training loss 0.0519387312233448 Validation loss 0.05904315412044525 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.8645],\n",
      "        [0.0607]], device='mps:0')\n",
      "Iteration 50230 Training loss 0.04779214784502983 Validation loss 0.05901755020022392 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.0005],\n",
      "        [0.1213]], device='mps:0')\n",
      "Iteration 50240 Training loss 0.06104520708322525 Validation loss 0.059096697717905045 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.8425],\n",
      "        [0.1473]], device='mps:0')\n",
      "Iteration 50250 Training loss 0.06235429644584656 Validation loss 0.05901011824607849 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.9249],\n",
      "        [0.0952]], device='mps:0')\n",
      "Iteration 50260 Training loss 0.057983506470918655 Validation loss 0.0591021291911602 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.9210],\n",
      "        [0.8456]], device='mps:0')\n",
      "Iteration 50270 Training loss 0.05702083557844162 Validation loss 0.05905117467045784 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.5246],\n",
      "        [0.9711]], device='mps:0')\n",
      "Iteration 50280 Training loss 0.057344187051057816 Validation loss 0.05901993066072464 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0185],\n",
      "        [0.7376]], device='mps:0')\n",
      "Iteration 50290 Training loss 0.049902647733688354 Validation loss 0.059384144842624664 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.0639],\n",
      "        [0.9811]], device='mps:0')\n",
      "Iteration 50300 Training loss 0.05449764057993889 Validation loss 0.0589895062148571 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.8130],\n",
      "        [0.0222]], device='mps:0')\n",
      "Iteration 50310 Training loss 0.05288606509566307 Validation loss 0.059106335043907166 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.9800],\n",
      "        [0.1795]], device='mps:0')\n",
      "Iteration 50320 Training loss 0.05823846161365509 Validation loss 0.05903854966163635 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.9263],\n",
      "        [0.1611]], device='mps:0')\n",
      "Iteration 50330 Training loss 0.0614037960767746 Validation loss 0.05904396250844002 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.3302],\n",
      "        [0.7863]], device='mps:0')\n",
      "Iteration 50340 Training loss 0.05366908013820648 Validation loss 0.05911603569984436 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.8946],\n",
      "        [0.0836]], device='mps:0')\n",
      "Iteration 50350 Training loss 0.05160505324602127 Validation loss 0.05899792164564133 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.5485],\n",
      "        [0.0750]], device='mps:0')\n",
      "Iteration 50360 Training loss 0.05070861801505089 Validation loss 0.05898308381438255 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.4045],\n",
      "        [0.9899]], device='mps:0')\n",
      "Iteration 50370 Training loss 0.0582461878657341 Validation loss 0.05903833359479904 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9595],\n",
      "        [0.6275]], device='mps:0')\n",
      "Iteration 50380 Training loss 0.05398120358586311 Validation loss 0.059072673320770264 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.6677],\n",
      "        [0.0248]], device='mps:0')\n",
      "Iteration 50390 Training loss 0.05444744974374771 Validation loss 0.05907673016190529 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0994],\n",
      "        [0.2905]], device='mps:0')\n",
      "Iteration 50400 Training loss 0.05942347273230553 Validation loss 0.058994900435209274 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.7582],\n",
      "        [0.2828]], device='mps:0')\n",
      "Iteration 50410 Training loss 0.05691492557525635 Validation loss 0.059053514152765274 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9789],\n",
      "        [0.5024]], device='mps:0')\n",
      "Iteration 50420 Training loss 0.05697140470147133 Validation loss 0.059018149971961975 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.0420],\n",
      "        [0.0207]], device='mps:0')\n",
      "Iteration 50430 Training loss 0.05129330977797508 Validation loss 0.05903322249650955 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9546],\n",
      "        [0.9373]], device='mps:0')\n",
      "Iteration 50440 Training loss 0.05930258706212044 Validation loss 0.05902097001671791 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.0784],\n",
      "        [0.8292]], device='mps:0')\n",
      "Iteration 50450 Training loss 0.05874662101268768 Validation loss 0.05949578434228897 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.8836],\n",
      "        [0.9023]], device='mps:0')\n",
      "Iteration 50460 Training loss 0.053108032792806625 Validation loss 0.058988675475120544 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.9471],\n",
      "        [0.2001]], device='mps:0')\n",
      "Iteration 50470 Training loss 0.0557115338742733 Validation loss 0.058980103582143784 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.4007],\n",
      "        [0.1410]], device='mps:0')\n",
      "Iteration 50480 Training loss 0.05278968811035156 Validation loss 0.058972835540771484 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9645],\n",
      "        [0.9188]], device='mps:0')\n",
      "Iteration 50490 Training loss 0.054726164788007736 Validation loss 0.05901867896318436 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.4247],\n",
      "        [0.2745]], device='mps:0')\n",
      "Iteration 50500 Training loss 0.0590747594833374 Validation loss 0.05899043381214142 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.8982],\n",
      "        [0.4575]], device='mps:0')\n",
      "Iteration 50510 Training loss 0.06173396110534668 Validation loss 0.05913516506552696 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.2699],\n",
      "        [0.5274]], device='mps:0')\n",
      "Iteration 50520 Training loss 0.059170763939619064 Validation loss 0.05920211970806122 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.7499],\n",
      "        [0.3172]], device='mps:0')\n",
      "Iteration 50530 Training loss 0.06120920926332474 Validation loss 0.05897325649857521 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.2529],\n",
      "        [0.0157]], device='mps:0')\n",
      "Iteration 50540 Training loss 0.06259430944919586 Validation loss 0.058971505612134933 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.9323],\n",
      "        [0.8142]], device='mps:0')\n",
      "Iteration 50550 Training loss 0.06098050996661186 Validation loss 0.05917453393340111 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0386],\n",
      "        [0.1652]], device='mps:0')\n",
      "Iteration 50560 Training loss 0.05961908772587776 Validation loss 0.05899709835648537 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.0048],\n",
      "        [0.8313]], device='mps:0')\n",
      "Iteration 50570 Training loss 0.05259736254811287 Validation loss 0.059148725122213364 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.3332],\n",
      "        [0.5504]], device='mps:0')\n",
      "Iteration 50580 Training loss 0.05260540917515755 Validation loss 0.05897778272628784 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.2216],\n",
      "        [0.0023]], device='mps:0')\n",
      "Iteration 50590 Training loss 0.051878709346055984 Validation loss 0.05916841700673103 Accuracy 0.8373750448226929\n",
      "Output tensor([[0.4607],\n",
      "        [0.4437]], device='mps:0')\n",
      "Iteration 50600 Training loss 0.05449240654706955 Validation loss 0.05903350189328194 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0833],\n",
      "        [0.8821]], device='mps:0')\n",
      "Iteration 50610 Training loss 0.05414857715368271 Validation loss 0.05910058319568634 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.0613],\n",
      "        [0.8545]], device='mps:0')\n",
      "Iteration 50620 Training loss 0.051325082778930664 Validation loss 0.05896918475627899 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.2032],\n",
      "        [0.7462]], device='mps:0')\n",
      "Iteration 50630 Training loss 0.06073306128382683 Validation loss 0.05897426977753639 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.8315],\n",
      "        [0.1252]], device='mps:0')\n",
      "Iteration 50640 Training loss 0.05244721472263336 Validation loss 0.05899148806929588 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.8904],\n",
      "        [0.2892]], device='mps:0')\n",
      "Iteration 50650 Training loss 0.056370481848716736 Validation loss 0.059146907180547714 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.6999],\n",
      "        [0.9565]], device='mps:0')\n",
      "Iteration 50660 Training loss 0.059430450201034546 Validation loss 0.059033527970314026 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.0104],\n",
      "        [0.2554]], device='mps:0')\n",
      "Iteration 50670 Training loss 0.05917426943778992 Validation loss 0.05908592417836189 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.0084],\n",
      "        [0.9767]], device='mps:0')\n",
      "Iteration 50680 Training loss 0.0472550205886364 Validation loss 0.058993496000766754 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.9826],\n",
      "        [0.7394]], device='mps:0')\n",
      "Iteration 50690 Training loss 0.05303934961557388 Validation loss 0.058981310576200485 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.8981],\n",
      "        [0.9873]], device='mps:0')\n",
      "Iteration 50700 Training loss 0.061663396656513214 Validation loss 0.05901991203427315 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.2988],\n",
      "        [0.8838]], device='mps:0')\n",
      "Iteration 50710 Training loss 0.05701169744133949 Validation loss 0.058990322053432465 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.9938],\n",
      "        [0.3807]], device='mps:0')\n",
      "Iteration 50720 Training loss 0.06331729143857956 Validation loss 0.05899622663855553 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.0127],\n",
      "        [0.8876]], device='mps:0')\n",
      "Iteration 50730 Training loss 0.061570387333631516 Validation loss 0.05901337042450905 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.7673],\n",
      "        [0.8698]], device='mps:0')\n",
      "Iteration 50740 Training loss 0.054150864481925964 Validation loss 0.059059903025627136 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.0025],\n",
      "        [0.0532]], device='mps:0')\n",
      "Iteration 50750 Training loss 0.05498191714286804 Validation loss 0.05899021029472351 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.4527],\n",
      "        [0.1416]], device='mps:0')\n",
      "Iteration 50760 Training loss 0.052675049751996994 Validation loss 0.0590050145983696 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.6628],\n",
      "        [0.8749]], device='mps:0')\n",
      "Iteration 50770 Training loss 0.053653374314308167 Validation loss 0.05900311470031738 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.8598],\n",
      "        [0.9783]], device='mps:0')\n",
      "Iteration 50780 Training loss 0.05620415508747101 Validation loss 0.05897144600749016 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.5779],\n",
      "        [0.7798]], device='mps:0')\n",
      "Iteration 50790 Training loss 0.05532774329185486 Validation loss 0.058988314121961594 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9905],\n",
      "        [0.1338]], device='mps:0')\n",
      "Iteration 50800 Training loss 0.04985775053501129 Validation loss 0.058955006301403046 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.3321],\n",
      "        [0.9796]], device='mps:0')\n",
      "Iteration 50810 Training loss 0.05830923840403557 Validation loss 0.059047263115644455 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.9919],\n",
      "        [0.9097]], device='mps:0')\n",
      "Iteration 50820 Training loss 0.06406887620687485 Validation loss 0.059173304587602615 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.3511],\n",
      "        [0.3737]], device='mps:0')\n",
      "Iteration 50830 Training loss 0.051381178200244904 Validation loss 0.058953337371349335 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.6420],\n",
      "        [0.9860]], device='mps:0')\n",
      "Iteration 50840 Training loss 0.058596171438694 Validation loss 0.05895667150616646 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9763],\n",
      "        [0.2176]], device='mps:0')\n",
      "Iteration 50850 Training loss 0.062311649322509766 Validation loss 0.0590084046125412 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.2135],\n",
      "        [0.8937]], device='mps:0')\n",
      "Iteration 50860 Training loss 0.05412330478429794 Validation loss 0.058946821838617325 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.9360],\n",
      "        [0.0147]], device='mps:0')\n",
      "Iteration 50870 Training loss 0.05386495590209961 Validation loss 0.05901794880628586 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.0573],\n",
      "        [0.0956]], device='mps:0')\n",
      "Iteration 50880 Training loss 0.055242545902729034 Validation loss 0.058949992060661316 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.4479],\n",
      "        [0.9990]], device='mps:0')\n",
      "Iteration 50890 Training loss 0.04994279891252518 Validation loss 0.05901576206088066 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.8604],\n",
      "        [0.9133]], device='mps:0')\n",
      "Iteration 50900 Training loss 0.05482234060764313 Validation loss 0.05893568694591522 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.8657],\n",
      "        [0.7540]], device='mps:0')\n",
      "Iteration 50910 Training loss 0.06069759652018547 Validation loss 0.0590888187289238 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.5761],\n",
      "        [0.0202]], device='mps:0')\n",
      "Iteration 50920 Training loss 0.04469606652855873 Validation loss 0.05893540382385254 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.3506],\n",
      "        [0.0820]], device='mps:0')\n",
      "Iteration 50930 Training loss 0.05420663580298424 Validation loss 0.05896468460559845 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.9578],\n",
      "        [0.7640]], device='mps:0')\n",
      "Iteration 50940 Training loss 0.05333319678902626 Validation loss 0.05902180075645447 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9122],\n",
      "        [0.8765]], device='mps:0')\n",
      "Iteration 50950 Training loss 0.06028654798865318 Validation loss 0.05900980532169342 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.7875],\n",
      "        [0.9441]], device='mps:0')\n",
      "Iteration 50960 Training loss 0.054695889353752136 Validation loss 0.0591617077589035 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.1065],\n",
      "        [0.4604]], device='mps:0')\n",
      "Iteration 50970 Training loss 0.053223952651023865 Validation loss 0.05894242972135544 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.1124],\n",
      "        [0.0757]], device='mps:0')\n",
      "Iteration 50980 Training loss 0.056543197482824326 Validation loss 0.05897838994860649 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.7519],\n",
      "        [0.0282]], device='mps:0')\n",
      "Iteration 50990 Training loss 0.05457816645503044 Validation loss 0.059114206582307816 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.2541],\n",
      "        [0.9726]], device='mps:0')\n",
      "Iteration 51000 Training loss 0.05716102942824364 Validation loss 0.05917100980877876 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0623],\n",
      "        [0.2927]], device='mps:0')\n",
      "Iteration 51010 Training loss 0.05164917930960655 Validation loss 0.05914800614118576 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.8945],\n",
      "        [0.1086]], device='mps:0')\n",
      "Iteration 51020 Training loss 0.061713747680187225 Validation loss 0.058918293565511703 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.8687],\n",
      "        [0.9701]], device='mps:0')\n",
      "Iteration 51030 Training loss 0.0448322668671608 Validation loss 0.05893946439027786 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.1440],\n",
      "        [0.3603]], device='mps:0')\n",
      "Iteration 51040 Training loss 0.052495598793029785 Validation loss 0.058950792998075485 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.6635],\n",
      "        [0.7825]], device='mps:0')\n",
      "Iteration 51050 Training loss 0.05236915498971939 Validation loss 0.05890814587473869 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.0376],\n",
      "        [0.1093]], device='mps:0')\n",
      "Iteration 51060 Training loss 0.04620625823736191 Validation loss 0.05891232192516327 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.6557],\n",
      "        [0.1946]], device='mps:0')\n",
      "Iteration 51070 Training loss 0.060802582651376724 Validation loss 0.058941759169101715 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.0107],\n",
      "        [0.4140]], device='mps:0')\n",
      "Iteration 51080 Training loss 0.059361498802900314 Validation loss 0.059059709310531616 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.1692],\n",
      "        [0.4060]], device='mps:0')\n",
      "Iteration 51090 Training loss 0.056708142161369324 Validation loss 0.05889930948615074 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.0722],\n",
      "        [0.7971]], device='mps:0')\n",
      "Iteration 51100 Training loss 0.05817731097340584 Validation loss 0.05901362746953964 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.9861],\n",
      "        [0.7723]], device='mps:0')\n",
      "Iteration 51110 Training loss 0.06566250324249268 Validation loss 0.05905213579535484 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0181],\n",
      "        [0.9880]], device='mps:0')\n",
      "Iteration 51120 Training loss 0.05573073402047157 Validation loss 0.05890151485800743 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9888],\n",
      "        [0.1030]], device='mps:0')\n",
      "Iteration 51130 Training loss 0.05753837525844574 Validation loss 0.0589391365647316 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.1539],\n",
      "        [0.2595]], device='mps:0')\n",
      "Iteration 51140 Training loss 0.05636921897530556 Validation loss 0.058898456394672394 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0336],\n",
      "        [0.9493]], device='mps:0')\n",
      "Iteration 51150 Training loss 0.05179614946246147 Validation loss 0.05889409780502319 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.2542],\n",
      "        [0.5191]], device='mps:0')\n",
      "Iteration 51160 Training loss 0.05902431532740593 Validation loss 0.05896936357021332 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.0367],\n",
      "        [0.4803]], device='mps:0')\n",
      "Iteration 51170 Training loss 0.06441321969032288 Validation loss 0.058900751173496246 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0594],\n",
      "        [0.9700]], device='mps:0')\n",
      "Iteration 51180 Training loss 0.054834093898534775 Validation loss 0.05890708416700363 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.6465],\n",
      "        [0.6848]], device='mps:0')\n",
      "Iteration 51190 Training loss 0.06389635801315308 Validation loss 0.05888659879565239 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9550],\n",
      "        [0.8662]], device='mps:0')\n",
      "Iteration 51200 Training loss 0.055626265704631805 Validation loss 0.05899520590901375 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.5292],\n",
      "        [0.8488]], device='mps:0')\n",
      "Iteration 51210 Training loss 0.05436296388506889 Validation loss 0.058883532881736755 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.5998],\n",
      "        [0.0100]], device='mps:0')\n",
      "Iteration 51220 Training loss 0.059754256159067154 Validation loss 0.058889009058475494 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.3187],\n",
      "        [0.1923]], device='mps:0')\n",
      "Iteration 51230 Training loss 0.060062654316425323 Validation loss 0.05891735106706619 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.8146],\n",
      "        [0.1327]], device='mps:0')\n",
      "Iteration 51240 Training loss 0.06076057255268097 Validation loss 0.058975495398044586 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9605],\n",
      "        [0.1381]], device='mps:0')\n",
      "Iteration 51250 Training loss 0.06018517538905144 Validation loss 0.058966558426618576 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.0131],\n",
      "        [0.7818]], device='mps:0')\n",
      "Iteration 51260 Training loss 0.05287976935505867 Validation loss 0.058903876692056656 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.4703],\n",
      "        [0.1039]], device='mps:0')\n",
      "Iteration 51270 Training loss 0.05873927101492882 Validation loss 0.05895569175481796 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.9979],\n",
      "        [0.7829]], device='mps:0')\n",
      "Iteration 51280 Training loss 0.05595031753182411 Validation loss 0.05890387296676636 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.7719],\n",
      "        [0.3051]], device='mps:0')\n",
      "Iteration 51290 Training loss 0.0578361414372921 Validation loss 0.059030380100011826 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.1320],\n",
      "        [0.1823]], device='mps:0')\n",
      "Iteration 51300 Training loss 0.05542578548192978 Validation loss 0.05888722464442253 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.4951],\n",
      "        [0.9487]], device='mps:0')\n",
      "Iteration 51310 Training loss 0.050731465220451355 Validation loss 0.05887983366847038 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.5131],\n",
      "        [0.1543]], device='mps:0')\n",
      "Iteration 51320 Training loss 0.06391293555498123 Validation loss 0.0588851198554039 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.8046],\n",
      "        [0.8031]], device='mps:0')\n",
      "Iteration 51330 Training loss 0.054788988083601 Validation loss 0.05894487723708153 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.1298],\n",
      "        [0.9494]], device='mps:0')\n",
      "Iteration 51340 Training loss 0.0655151903629303 Validation loss 0.05893424525856972 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.8012],\n",
      "        [0.9685]], device='mps:0')\n",
      "Iteration 51350 Training loss 0.06317155808210373 Validation loss 0.05895159766077995 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.9833],\n",
      "        [0.1411]], device='mps:0')\n",
      "Iteration 51360 Training loss 0.053501758724451065 Validation loss 0.058876071125268936 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.8682],\n",
      "        [0.0761]], device='mps:0')\n",
      "Iteration 51370 Training loss 0.06256288290023804 Validation loss 0.05893890559673309 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.1414],\n",
      "        [0.0375]], device='mps:0')\n",
      "Iteration 51380 Training loss 0.049115292727947235 Validation loss 0.05888524278998375 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0286],\n",
      "        [0.4746]], device='mps:0')\n",
      "Iteration 51390 Training loss 0.06568153947591782 Validation loss 0.0590529665350914 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.8084],\n",
      "        [0.1023]], device='mps:0')\n",
      "Iteration 51400 Training loss 0.05060236155986786 Validation loss 0.058870624750852585 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.2579],\n",
      "        [0.3337]], device='mps:0')\n",
      "Iteration 51410 Training loss 0.05683460086584091 Validation loss 0.05893339961767197 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9774],\n",
      "        [0.5910]], device='mps:0')\n",
      "Iteration 51420 Training loss 0.05220187082886696 Validation loss 0.05887364596128464 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.7474],\n",
      "        [0.8428]], device='mps:0')\n",
      "Iteration 51430 Training loss 0.053933773189783096 Validation loss 0.058875247836112976 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0905],\n",
      "        [0.7167]], device='mps:0')\n",
      "Iteration 51440 Training loss 0.0566977933049202 Validation loss 0.05887014791369438 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.4381],\n",
      "        [0.2459]], device='mps:0')\n",
      "Iteration 51450 Training loss 0.05653400346636772 Validation loss 0.05887240543961525 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0185],\n",
      "        [0.5973]], device='mps:0')\n",
      "Iteration 51460 Training loss 0.051575593650341034 Validation loss 0.05887104943394661 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.1047],\n",
      "        [0.3830]], device='mps:0')\n",
      "Iteration 51470 Training loss 0.05313638225197792 Validation loss 0.05892283841967583 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.0616],\n",
      "        [0.2187]], device='mps:0')\n",
      "Iteration 51480 Training loss 0.05722722411155701 Validation loss 0.05887320637702942 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.4969],\n",
      "        [0.0044]], device='mps:0')\n",
      "Iteration 51490 Training loss 0.06291436403989792 Validation loss 0.05887260660529137 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0984],\n",
      "        [0.9408]], device='mps:0')\n",
      "Iteration 51500 Training loss 0.05453905090689659 Validation loss 0.059363529086112976 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.6154],\n",
      "        [0.8994]], device='mps:0')\n",
      "Iteration 51510 Training loss 0.05940145254135132 Validation loss 0.05890140309929848 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.3110],\n",
      "        [0.4217]], device='mps:0')\n",
      "Iteration 51520 Training loss 0.04963602125644684 Validation loss 0.0588875412940979 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.8032],\n",
      "        [0.7038]], device='mps:0')\n",
      "Iteration 51530 Training loss 0.0653485357761383 Validation loss 0.05900650471448898 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.9416],\n",
      "        [0.0559]], device='mps:0')\n",
      "Iteration 51540 Training loss 0.05624640733003616 Validation loss 0.058871470391750336 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.1543],\n",
      "        [0.4425]], device='mps:0')\n",
      "Iteration 51550 Training loss 0.05669836327433586 Validation loss 0.05886126682162285 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0782],\n",
      "        [0.4345]], device='mps:0')\n",
      "Iteration 51560 Training loss 0.054258398711681366 Validation loss 0.0588972233235836 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0849],\n",
      "        [0.1408]], device='mps:0')\n",
      "Iteration 51570 Training loss 0.04784366860985756 Validation loss 0.05913695693016052 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.6949],\n",
      "        [0.1667]], device='mps:0')\n",
      "Iteration 51580 Training loss 0.05902281031012535 Validation loss 0.05906325578689575 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.2586],\n",
      "        [0.9194]], device='mps:0')\n",
      "Iteration 51590 Training loss 0.05001705139875412 Validation loss 0.058858584612607956 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.8013],\n",
      "        [0.9827]], device='mps:0')\n",
      "Iteration 51600 Training loss 0.057627636939287186 Validation loss 0.05896959826350212 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.9747],\n",
      "        [0.0789]], device='mps:0')\n",
      "Iteration 51610 Training loss 0.05002594739198685 Validation loss 0.058972544968128204 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.7629],\n",
      "        [0.0164]], device='mps:0')\n",
      "Iteration 51620 Training loss 0.055558815598487854 Validation loss 0.05889533460140228 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.5597],\n",
      "        [0.3159]], device='mps:0')\n",
      "Iteration 51630 Training loss 0.06606518477201462 Validation loss 0.058861080557107925 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.4659],\n",
      "        [0.9477]], device='mps:0')\n",
      "Iteration 51640 Training loss 0.04870918020606041 Validation loss 0.05897575616836548 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.7291],\n",
      "        [0.2303]], device='mps:0')\n",
      "Iteration 51650 Training loss 0.06173788383603096 Validation loss 0.05895160883665085 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.8994],\n",
      "        [0.5463]], device='mps:0')\n",
      "Iteration 51660 Training loss 0.056557245552539825 Validation loss 0.05894193425774574 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.3359],\n",
      "        [0.9922]], device='mps:0')\n",
      "Iteration 51670 Training loss 0.054729919880628586 Validation loss 0.05886013060808182 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.8069],\n",
      "        [0.8576]], device='mps:0')\n",
      "Iteration 51680 Training loss 0.04459173604846001 Validation loss 0.05901394039392471 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.0027],\n",
      "        [0.1132]], device='mps:0')\n",
      "Iteration 51690 Training loss 0.05560765042901039 Validation loss 0.058861926198005676 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.0083],\n",
      "        [0.7136]], device='mps:0')\n",
      "Iteration 51700 Training loss 0.059278834611177444 Validation loss 0.05887404829263687 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.3955],\n",
      "        [0.4087]], device='mps:0')\n",
      "Iteration 51710 Training loss 0.059729140251874924 Validation loss 0.05885067954659462 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.3078],\n",
      "        [0.6308]], device='mps:0')\n",
      "Iteration 51720 Training loss 0.05984204262495041 Validation loss 0.059106387197971344 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.7249],\n",
      "        [0.1599]], device='mps:0')\n",
      "Iteration 51730 Training loss 0.05628637969493866 Validation loss 0.05916552618145943 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9705],\n",
      "        [0.9795]], device='mps:0')\n",
      "Iteration 51740 Training loss 0.05714702606201172 Validation loss 0.05884633958339691 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.3843],\n",
      "        [0.4168]], device='mps:0')\n",
      "Iteration 51750 Training loss 0.06033393368124962 Validation loss 0.05887386575341225 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.7109],\n",
      "        [0.0812]], device='mps:0')\n",
      "Iteration 51760 Training loss 0.055600766092538834 Validation loss 0.0588727742433548 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.4172],\n",
      "        [0.9212]], device='mps:0')\n",
      "Iteration 51770 Training loss 0.054905977100133896 Validation loss 0.05898112803697586 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.8245],\n",
      "        [0.0521]], device='mps:0')\n",
      "Iteration 51780 Training loss 0.058480504900217056 Validation loss 0.05896163359284401 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.0082],\n",
      "        [0.3076]], device='mps:0')\n",
      "Iteration 51790 Training loss 0.04412596672773361 Validation loss 0.05884543061256409 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.1194],\n",
      "        [0.0390]], device='mps:0')\n",
      "Iteration 51800 Training loss 0.05048476904630661 Validation loss 0.05886773020029068 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0821],\n",
      "        [0.9907]], device='mps:0')\n",
      "Iteration 51810 Training loss 0.06283305585384369 Validation loss 0.05885061249136925 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.1859],\n",
      "        [0.9805]], device='mps:0')\n",
      "Iteration 51820 Training loss 0.0641670897603035 Validation loss 0.05889425799250603 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.8959],\n",
      "        [0.0972]], device='mps:0')\n",
      "Iteration 51830 Training loss 0.0485704205930233 Validation loss 0.05884905159473419 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.7496],\n",
      "        [0.1657]], device='mps:0')\n",
      "Iteration 51840 Training loss 0.048720598220825195 Validation loss 0.05884639918804169 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.6842],\n",
      "        [0.6365]], device='mps:0')\n",
      "Iteration 51850 Training loss 0.06424809992313385 Validation loss 0.05885130539536476 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9678],\n",
      "        [0.2374]], device='mps:0')\n",
      "Iteration 51860 Training loss 0.061665624380111694 Validation loss 0.05892118439078331 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.1081],\n",
      "        [0.2623]], device='mps:0')\n",
      "Iteration 51870 Training loss 0.05195529758930206 Validation loss 0.05885307490825653 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.8138],\n",
      "        [0.8053]], device='mps:0')\n",
      "Iteration 51880 Training loss 0.059463366866111755 Validation loss 0.058971554040908813 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.3147],\n",
      "        [0.9134]], device='mps:0')\n",
      "Iteration 51890 Training loss 0.0616874098777771 Validation loss 0.05894087627530098 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.1932],\n",
      "        [0.7659]], device='mps:0')\n",
      "Iteration 51900 Training loss 0.06371790915727615 Validation loss 0.059101931750774384 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.2580],\n",
      "        [0.3543]], device='mps:0')\n",
      "Iteration 51910 Training loss 0.0499270036816597 Validation loss 0.05899544805288315 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.8843],\n",
      "        [0.9197]], device='mps:0')\n",
      "Iteration 51920 Training loss 0.05710991844534874 Validation loss 0.05895855277776718 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.8134],\n",
      "        [0.1880]], device='mps:0')\n",
      "Iteration 51930 Training loss 0.0566256120800972 Validation loss 0.058995384722948074 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.0693],\n",
      "        [0.0395]], device='mps:0')\n",
      "Iteration 51940 Training loss 0.05290096253156662 Validation loss 0.05885089561343193 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.8956],\n",
      "        [0.5516]], device='mps:0')\n",
      "Iteration 51950 Training loss 0.06359786540269852 Validation loss 0.0588764064013958 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0111],\n",
      "        [0.1209]], device='mps:0')\n",
      "Iteration 51960 Training loss 0.05906862020492554 Validation loss 0.059101372957229614 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.7568],\n",
      "        [0.9931]], device='mps:0')\n",
      "Iteration 51970 Training loss 0.06177676096558571 Validation loss 0.05905332416296005 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.2691],\n",
      "        [0.8377]], device='mps:0')\n",
      "Iteration 51980 Training loss 0.05967340245842934 Validation loss 0.0588739812374115 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.5070],\n",
      "        [0.0875]], device='mps:0')\n",
      "Iteration 51990 Training loss 0.049629077315330505 Validation loss 0.05887671932578087 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9555],\n",
      "        [0.0181]], device='mps:0')\n",
      "Iteration 52000 Training loss 0.061066579073667526 Validation loss 0.05889821797609329 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.8343],\n",
      "        [0.0832]], device='mps:0')\n",
      "Iteration 52010 Training loss 0.052472420036792755 Validation loss 0.058858830481767654 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.2003],\n",
      "        [0.9179]], device='mps:0')\n",
      "Iteration 52020 Training loss 0.062179870903491974 Validation loss 0.05929124727845192 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.6411],\n",
      "        [0.1041]], device='mps:0')\n",
      "Iteration 52030 Training loss 0.05591495707631111 Validation loss 0.05884496495127678 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.3982],\n",
      "        [0.3330]], device='mps:0')\n",
      "Iteration 52040 Training loss 0.061423804610967636 Validation loss 0.058885447680950165 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.8470],\n",
      "        [0.4635]], device='mps:0')\n",
      "Iteration 52050 Training loss 0.054244231432676315 Validation loss 0.059069857001304626 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.4392],\n",
      "        [0.0012]], device='mps:0')\n",
      "Iteration 52060 Training loss 0.05514631047844887 Validation loss 0.05885665863752365 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.2780],\n",
      "        [0.9629]], device='mps:0')\n",
      "Iteration 52070 Training loss 0.054877862334251404 Validation loss 0.058843307197093964 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.8797],\n",
      "        [0.3922]], device='mps:0')\n",
      "Iteration 52080 Training loss 0.05443384870886803 Validation loss 0.05909338593482971 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.2619],\n",
      "        [0.3990]], device='mps:0')\n",
      "Iteration 52090 Training loss 0.05343206599354744 Validation loss 0.058839790523052216 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.1115],\n",
      "        [0.2596]], device='mps:0')\n",
      "Iteration 52100 Training loss 0.048356227576732635 Validation loss 0.05883536860346794 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.1098],\n",
      "        [0.1064]], device='mps:0')\n",
      "Iteration 52110 Training loss 0.053179387003183365 Validation loss 0.05883404240012169 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.7931],\n",
      "        [0.9554]], device='mps:0')\n",
      "Iteration 52120 Training loss 0.0617809034883976 Validation loss 0.05884566158056259 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.3995],\n",
      "        [0.0480]], device='mps:0')\n",
      "Iteration 52130 Training loss 0.05623506009578705 Validation loss 0.05886051803827286 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.7130],\n",
      "        [0.3310]], device='mps:0')\n",
      "Iteration 52140 Training loss 0.05532420054078102 Validation loss 0.05883314833045006 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.0376],\n",
      "        [0.9913]], device='mps:0')\n",
      "Iteration 52150 Training loss 0.048775266855955124 Validation loss 0.058978572487831116 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.1626],\n",
      "        [0.4845]], device='mps:0')\n",
      "Iteration 52160 Training loss 0.055002376437187195 Validation loss 0.05922693759202957 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.3645],\n",
      "        [0.9951]], device='mps:0')\n",
      "Iteration 52170 Training loss 0.05760478973388672 Validation loss 0.059379491955041885 Accuracy 0.8375000357627869\n",
      "Output tensor([[0.9915],\n",
      "        [0.9403]], device='mps:0')\n",
      "Iteration 52180 Training loss 0.05477939546108246 Validation loss 0.059021834284067154 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.7872],\n",
      "        [0.2621]], device='mps:0')\n",
      "Iteration 52190 Training loss 0.06377619504928589 Validation loss 0.05889754742383957 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.1907],\n",
      "        [0.1170]], device='mps:0')\n",
      "Iteration 52200 Training loss 0.05565820261836052 Validation loss 0.05892143398523331 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.2718],\n",
      "        [0.0041]], device='mps:0')\n",
      "Iteration 52210 Training loss 0.06411980837583542 Validation loss 0.058856017887592316 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.8639],\n",
      "        [0.6375]], device='mps:0')\n",
      "Iteration 52220 Training loss 0.05424853041768074 Validation loss 0.05885061249136925 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.9315],\n",
      "        [0.9775]], device='mps:0')\n",
      "Iteration 52230 Training loss 0.05199085921049118 Validation loss 0.0589732863008976 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.3935],\n",
      "        [0.8606]], device='mps:0')\n",
      "Iteration 52240 Training loss 0.0636148452758789 Validation loss 0.059220798313617706 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.2859],\n",
      "        [0.5916]], device='mps:0')\n",
      "Iteration 52250 Training loss 0.057086024433374405 Validation loss 0.05885113403201103 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9274],\n",
      "        [0.9735]], device='mps:0')\n",
      "Iteration 52260 Training loss 0.06594615429639816 Validation loss 0.05893470719456673 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.0623],\n",
      "        [0.9594]], device='mps:0')\n",
      "Iteration 52270 Training loss 0.05901002883911133 Validation loss 0.058855120092630386 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.3705],\n",
      "        [0.5897]], device='mps:0')\n",
      "Iteration 52280 Training loss 0.05443967878818512 Validation loss 0.05882919952273369 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.1116],\n",
      "        [0.0925]], device='mps:0')\n",
      "Iteration 52290 Training loss 0.05698055028915405 Validation loss 0.05887007340788841 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0494],\n",
      "        [0.2004]], device='mps:0')\n",
      "Iteration 52300 Training loss 0.05430462583899498 Validation loss 0.05882957577705383 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.2145],\n",
      "        [0.6374]], device='mps:0')\n",
      "Iteration 52310 Training loss 0.05114077404141426 Validation loss 0.05882255360484123 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0154],\n",
      "        [0.1466]], device='mps:0')\n",
      "Iteration 52320 Training loss 0.05203966423869133 Validation loss 0.05882888287305832 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0293],\n",
      "        [0.1602]], device='mps:0')\n",
      "Iteration 52330 Training loss 0.053885262459516525 Validation loss 0.05884875729680061 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0206],\n",
      "        [0.1886]], device='mps:0')\n",
      "Iteration 52340 Training loss 0.05950029194355011 Validation loss 0.058834828436374664 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.7918],\n",
      "        [0.1772]], device='mps:0')\n",
      "Iteration 52350 Training loss 0.06462543457746506 Validation loss 0.05883227661252022 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.2689],\n",
      "        [0.7727]], device='mps:0')\n",
      "Iteration 52360 Training loss 0.05697377398610115 Validation loss 0.058824848383665085 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0411],\n",
      "        [0.4294]], device='mps:0')\n",
      "Iteration 52370 Training loss 0.05371205136179924 Validation loss 0.058818716555833817 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9651],\n",
      "        [0.9775]], device='mps:0')\n",
      "Iteration 52380 Training loss 0.055590685456991196 Validation loss 0.05881093814969063 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.2914],\n",
      "        [0.0217]], device='mps:0')\n",
      "Iteration 52390 Training loss 0.05360451713204384 Validation loss 0.05890052393078804 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9208],\n",
      "        [0.8184]], device='mps:0')\n",
      "Iteration 52400 Training loss 0.06061026453971863 Validation loss 0.05883732810616493 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.8997],\n",
      "        [0.9227]], device='mps:0')\n",
      "Iteration 52410 Training loss 0.06327679753303528 Validation loss 0.05880580097436905 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.0175],\n",
      "        [0.6063]], device='mps:0')\n",
      "Iteration 52420 Training loss 0.055762793868780136 Validation loss 0.05882178619503975 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.5402],\n",
      "        [0.2904]], device='mps:0')\n",
      "Iteration 52430 Training loss 0.0533762127161026 Validation loss 0.0588553212583065 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.5454],\n",
      "        [0.9540]], device='mps:0')\n",
      "Iteration 52440 Training loss 0.06131653115153313 Validation loss 0.058843161910772324 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0900],\n",
      "        [0.5077]], device='mps:0')\n",
      "Iteration 52450 Training loss 0.05230775848031044 Validation loss 0.05887395143508911 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.8563],\n",
      "        [0.9281]], device='mps:0')\n",
      "Iteration 52460 Training loss 0.053689394146203995 Validation loss 0.05880551412701607 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.1941],\n",
      "        [0.9781]], device='mps:0')\n",
      "Iteration 52470 Training loss 0.051670435816049576 Validation loss 0.05883669853210449 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.9397],\n",
      "        [0.7369]], device='mps:0')\n",
      "Iteration 52480 Training loss 0.053998127579689026 Validation loss 0.059030868113040924 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.0732],\n",
      "        [0.2342]], device='mps:0')\n",
      "Iteration 52490 Training loss 0.054151084274053574 Validation loss 0.058842983096838 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0309],\n",
      "        [0.0553]], device='mps:0')\n",
      "Iteration 52500 Training loss 0.05219277739524841 Validation loss 0.05892535671591759 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.5506],\n",
      "        [0.0300]], device='mps:0')\n",
      "Iteration 52510 Training loss 0.0555742122232914 Validation loss 0.05884706974029541 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.6987],\n",
      "        [0.8097]], device='mps:0')\n",
      "Iteration 52520 Training loss 0.05192123353481293 Validation loss 0.058928560465574265 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.9900],\n",
      "        [0.9213]], device='mps:0')\n",
      "Iteration 52530 Training loss 0.07125571370124817 Validation loss 0.05909380689263344 Accuracy 0.8377500176429749\n",
      "Output tensor([[0.1729],\n",
      "        [0.0151]], device='mps:0')\n",
      "Iteration 52540 Training loss 0.04800340533256531 Validation loss 0.058888256549835205 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.1566],\n",
      "        [0.0599]], device='mps:0')\n",
      "Iteration 52550 Training loss 0.05168746039271355 Validation loss 0.058824360370635986 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.1576],\n",
      "        [0.0905]], device='mps:0')\n",
      "Iteration 52560 Training loss 0.05318264290690422 Validation loss 0.058984968811273575 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.1540],\n",
      "        [0.9680]], device='mps:0')\n",
      "Iteration 52570 Training loss 0.06651943176984787 Validation loss 0.05884911119937897 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.8462],\n",
      "        [0.7806]], device='mps:0')\n",
      "Iteration 52580 Training loss 0.04847061634063721 Validation loss 0.05887880176305771 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.0334],\n",
      "        [0.3216]], device='mps:0')\n",
      "Iteration 52590 Training loss 0.0533255860209465 Validation loss 0.05881243571639061 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9165],\n",
      "        [0.6574]], device='mps:0')\n",
      "Iteration 52600 Training loss 0.05873361974954605 Validation loss 0.0590006448328495 Accuracy 0.8382500410079956\n",
      "Output tensor([[0.1087],\n",
      "        [0.9404]], device='mps:0')\n",
      "Iteration 52610 Training loss 0.05188397690653801 Validation loss 0.05883193016052246 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.9806],\n",
      "        [0.9516]], device='mps:0')\n",
      "Iteration 52620 Training loss 0.05682746320962906 Validation loss 0.05883660539984703 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.2196],\n",
      "        [0.9926]], device='mps:0')\n",
      "Iteration 52630 Training loss 0.062147025018930435 Validation loss 0.05880662798881531 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.5924],\n",
      "        [0.2909]], device='mps:0')\n",
      "Iteration 52640 Training loss 0.06113426014780998 Validation loss 0.05883513391017914 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.0623],\n",
      "        [0.5392]], device='mps:0')\n",
      "Iteration 52650 Training loss 0.05375150218605995 Validation loss 0.05884968861937523 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9215],\n",
      "        [0.9466]], device='mps:0')\n",
      "Iteration 52660 Training loss 0.05531250685453415 Validation loss 0.05885620042681694 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.7184],\n",
      "        [0.7023]], device='mps:0')\n",
      "Iteration 52670 Training loss 0.06152389943599701 Validation loss 0.058814637362957 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.5254],\n",
      "        [0.2818]], device='mps:0')\n",
      "Iteration 52680 Training loss 0.05898018553853035 Validation loss 0.05887354910373688 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.0875],\n",
      "        [0.6962]], device='mps:0')\n",
      "Iteration 52690 Training loss 0.05745163932442665 Validation loss 0.05895271897315979 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9365],\n",
      "        [0.2126]], device='mps:0')\n",
      "Iteration 52700 Training loss 0.06237107142806053 Validation loss 0.05889749899506569 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.5958],\n",
      "        [0.2311]], device='mps:0')\n",
      "Iteration 52710 Training loss 0.06271050125360489 Validation loss 0.058865077793598175 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.8571],\n",
      "        [0.2990]], device='mps:0')\n",
      "Iteration 52720 Training loss 0.050432976335287094 Validation loss 0.05882986634969711 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.2871],\n",
      "        [0.2841]], device='mps:0')\n",
      "Iteration 52730 Training loss 0.049884431064128876 Validation loss 0.058800652623176575 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.7525],\n",
      "        [0.7898]], device='mps:0')\n",
      "Iteration 52740 Training loss 0.06277728825807571 Validation loss 0.058798011392354965 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.3326],\n",
      "        [0.1493]], device='mps:0')\n",
      "Iteration 52750 Training loss 0.05815787985920906 Validation loss 0.05891091004014015 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.6214],\n",
      "        [0.5154]], device='mps:0')\n",
      "Iteration 52760 Training loss 0.046593811362981796 Validation loss 0.059008609503507614 Accuracy 0.8380000591278076\n",
      "Output tensor([[0.9587],\n",
      "        [0.1927]], device='mps:0')\n",
      "Iteration 52770 Training loss 0.05228856950998306 Validation loss 0.05880793556571007 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9624],\n",
      "        [0.3227]], device='mps:0')\n",
      "Iteration 52780 Training loss 0.0565543957054615 Validation loss 0.05900358036160469 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.9931],\n",
      "        [0.1802]], device='mps:0')\n",
      "Iteration 52790 Training loss 0.05194621905684471 Validation loss 0.05880146473646164 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9037],\n",
      "        [0.2083]], device='mps:0')\n",
      "Iteration 52800 Training loss 0.06165657937526703 Validation loss 0.05880451202392578 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0782],\n",
      "        [0.3557]], device='mps:0')\n",
      "Iteration 52810 Training loss 0.05349653586745262 Validation loss 0.05884162336587906 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.1103],\n",
      "        [0.8867]], device='mps:0')\n",
      "Iteration 52820 Training loss 0.05721069127321243 Validation loss 0.05880145728588104 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.8182],\n",
      "        [0.0973]], device='mps:0')\n",
      "Iteration 52830 Training loss 0.0560101680457592 Validation loss 0.05883390083909035 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.7252],\n",
      "        [0.0258]], device='mps:0')\n",
      "Iteration 52840 Training loss 0.06125751882791519 Validation loss 0.05879930406808853 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.1351],\n",
      "        [0.8134]], device='mps:0')\n",
      "Iteration 52850 Training loss 0.05646058917045593 Validation loss 0.058852724730968475 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.9352],\n",
      "        [0.9550]], device='mps:0')\n",
      "Iteration 52860 Training loss 0.05117162689566612 Validation loss 0.058917369693517685 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.9515],\n",
      "        [0.0527]], device='mps:0')\n",
      "Iteration 52870 Training loss 0.05647716298699379 Validation loss 0.05879174917936325 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9553],\n",
      "        [0.1555]], device='mps:0')\n",
      "Iteration 52880 Training loss 0.05181758850812912 Validation loss 0.05883566290140152 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9567],\n",
      "        [0.9879]], device='mps:0')\n",
      "Iteration 52890 Training loss 0.0502789281308651 Validation loss 0.05879442021250725 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9083],\n",
      "        [0.6679]], device='mps:0')\n",
      "Iteration 52900 Training loss 0.045650552958250046 Validation loss 0.05905823037028313 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9298],\n",
      "        [0.7372]], device='mps:0')\n",
      "Iteration 52910 Training loss 0.05467711016535759 Validation loss 0.058918245136737823 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0209],\n",
      "        [0.8243]], device='mps:0')\n",
      "Iteration 52920 Training loss 0.04783920571208 Validation loss 0.05907374992966652 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.8296],\n",
      "        [0.7893]], device='mps:0')\n",
      "Iteration 52930 Training loss 0.04800141602754593 Validation loss 0.05893643572926521 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.2915],\n",
      "        [0.1357]], device='mps:0')\n",
      "Iteration 52940 Training loss 0.05305371806025505 Validation loss 0.0588124580681324 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9758],\n",
      "        [0.6546]], device='mps:0')\n",
      "Iteration 52950 Training loss 0.05538320541381836 Validation loss 0.05884450674057007 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9658],\n",
      "        [0.1075]], device='mps:0')\n",
      "Iteration 52960 Training loss 0.04886100813746452 Validation loss 0.05884578078985214 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.2151],\n",
      "        [0.9913]], device='mps:0')\n",
      "Iteration 52970 Training loss 0.053788021206855774 Validation loss 0.058791324496269226 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.8820],\n",
      "        [0.0432]], device='mps:0')\n",
      "Iteration 52980 Training loss 0.055578917264938354 Validation loss 0.05880354717373848 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.0961],\n",
      "        [0.5949]], device='mps:0')\n",
      "Iteration 52990 Training loss 0.06094019114971161 Validation loss 0.05879582464694977 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.7696],\n",
      "        [0.1427]], device='mps:0')\n",
      "Iteration 53000 Training loss 0.056948550045490265 Validation loss 0.05879249423742294 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.9674],\n",
      "        [0.1205]], device='mps:0')\n",
      "Iteration 53010 Training loss 0.05907553434371948 Validation loss 0.05887935683131218 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.0800],\n",
      "        [0.2790]], device='mps:0')\n",
      "Iteration 53020 Training loss 0.06069483608007431 Validation loss 0.058842241764068604 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.9903],\n",
      "        [0.1123]], device='mps:0')\n",
      "Iteration 53030 Training loss 0.05117462947964668 Validation loss 0.05877519026398659 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.0455],\n",
      "        [0.9848]], device='mps:0')\n",
      "Iteration 53040 Training loss 0.05439343303442001 Validation loss 0.05882800370454788 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.1826],\n",
      "        [0.8354]], device='mps:0')\n",
      "Iteration 53050 Training loss 0.05829506739974022 Validation loss 0.05878564715385437 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.9676],\n",
      "        [0.8483]], device='mps:0')\n",
      "Iteration 53060 Training loss 0.05614149942994118 Validation loss 0.05884414166212082 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.7863],\n",
      "        [0.6902]], device='mps:0')\n",
      "Iteration 53070 Training loss 0.062567338347435 Validation loss 0.05878512188792229 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9284],\n",
      "        [0.8797]], device='mps:0')\n",
      "Iteration 53080 Training loss 0.055525749921798706 Validation loss 0.05883822962641716 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.1344],\n",
      "        [0.9900]], device='mps:0')\n",
      "Iteration 53090 Training loss 0.05954022333025932 Validation loss 0.05881725624203682 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.8629],\n",
      "        [0.4511]], device='mps:0')\n",
      "Iteration 53100 Training loss 0.06755147129297256 Validation loss 0.058862484991550446 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.9419],\n",
      "        [0.5719]], device='mps:0')\n",
      "Iteration 53110 Training loss 0.05961605906486511 Validation loss 0.05880012363195419 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.0234],\n",
      "        [0.0417]], device='mps:0')\n",
      "Iteration 53120 Training loss 0.06000958010554314 Validation loss 0.05882371962070465 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.7885],\n",
      "        [0.2328]], device='mps:0')\n",
      "Iteration 53130 Training loss 0.05377217009663582 Validation loss 0.05878074839711189 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0503],\n",
      "        [0.7867]], device='mps:0')\n",
      "Iteration 53140 Training loss 0.05130235105752945 Validation loss 0.05876563861966133 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.6938],\n",
      "        [0.4100]], device='mps:0')\n",
      "Iteration 53150 Training loss 0.053311366587877274 Validation loss 0.05876738950610161 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.9153],\n",
      "        [0.1338]], device='mps:0')\n",
      "Iteration 53160 Training loss 0.06742405146360397 Validation loss 0.05882173404097557 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.2007],\n",
      "        [0.7396]], device='mps:0')\n",
      "Iteration 53170 Training loss 0.05955711379647255 Validation loss 0.05875809118151665 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0242],\n",
      "        [0.7653]], device='mps:0')\n",
      "Iteration 53180 Training loss 0.060786597430706024 Validation loss 0.05890704691410065 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.1126],\n",
      "        [0.5067]], device='mps:0')\n",
      "Iteration 53190 Training loss 0.064796082675457 Validation loss 0.05904107168316841 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.5051],\n",
      "        [0.1645]], device='mps:0')\n",
      "Iteration 53200 Training loss 0.052558135241270065 Validation loss 0.059186212718486786 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.3624],\n",
      "        [0.9402]], device='mps:0')\n",
      "Iteration 53210 Training loss 0.06116180121898651 Validation loss 0.05875592306256294 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.1885],\n",
      "        [0.4812]], device='mps:0')\n",
      "Iteration 53220 Training loss 0.05740607902407646 Validation loss 0.05876946449279785 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.9746],\n",
      "        [0.7328]], device='mps:0')\n",
      "Iteration 53230 Training loss 0.05634935200214386 Validation loss 0.05875290185213089 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9986],\n",
      "        [0.7395]], device='mps:0')\n",
      "Iteration 53240 Training loss 0.06143113970756531 Validation loss 0.05877234786748886 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.4476],\n",
      "        [0.9303]], device='mps:0')\n",
      "Iteration 53250 Training loss 0.057460710406303406 Validation loss 0.05875851586461067 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.7244],\n",
      "        [0.2805]], device='mps:0')\n",
      "Iteration 53260 Training loss 0.06156347692012787 Validation loss 0.058811381459236145 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.7322],\n",
      "        [0.9970]], device='mps:0')\n",
      "Iteration 53270 Training loss 0.05732642114162445 Validation loss 0.05875755101442337 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.1557],\n",
      "        [0.2828]], device='mps:0')\n",
      "Iteration 53280 Training loss 0.06258785724639893 Validation loss 0.058753978461027145 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.0918],\n",
      "        [0.7071]], device='mps:0')\n",
      "Iteration 53290 Training loss 0.05321534350514412 Validation loss 0.05875444412231445 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.7399],\n",
      "        [0.2983]], device='mps:0')\n",
      "Iteration 53300 Training loss 0.059216082096099854 Validation loss 0.05876161530613899 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9307],\n",
      "        [0.0217]], device='mps:0')\n",
      "Iteration 53310 Training loss 0.05518418177962303 Validation loss 0.05875576287508011 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.5118],\n",
      "        [0.9002]], device='mps:0')\n",
      "Iteration 53320 Training loss 0.06013511121273041 Validation loss 0.05887678265571594 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.1601],\n",
      "        [0.4819]], device='mps:0')\n",
      "Iteration 53330 Training loss 0.05380811542272568 Validation loss 0.05886145681142807 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.4673],\n",
      "        [0.8438]], device='mps:0')\n",
      "Iteration 53340 Training loss 0.0561957024037838 Validation loss 0.05878231301903725 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.8557],\n",
      "        [0.9217]], device='mps:0')\n",
      "Iteration 53350 Training loss 0.05394406244158745 Validation loss 0.058860961347818375 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9971],\n",
      "        [0.6418]], device='mps:0')\n",
      "Iteration 53360 Training loss 0.060704078525304794 Validation loss 0.058782145380973816 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.0548],\n",
      "        [0.0353]], device='mps:0')\n",
      "Iteration 53370 Training loss 0.052093151956796646 Validation loss 0.05874708294868469 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.7629],\n",
      "        [0.6467]], device='mps:0')\n",
      "Iteration 53380 Training loss 0.048304516822099686 Validation loss 0.0588228702545166 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.8450],\n",
      "        [0.6756]], device='mps:0')\n",
      "Iteration 53390 Training loss 0.05065684765577316 Validation loss 0.0587516613304615 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.8139],\n",
      "        [0.2000]], device='mps:0')\n",
      "Iteration 53400 Training loss 0.0525900274515152 Validation loss 0.05874797701835632 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.9771],\n",
      "        [0.4051]], device='mps:0')\n",
      "Iteration 53410 Training loss 0.05812744423747063 Validation loss 0.058760784566402435 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0045],\n",
      "        [0.0894]], device='mps:0')\n",
      "Iteration 53420 Training loss 0.04663284122943878 Validation loss 0.058917075395584106 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9970],\n",
      "        [0.9734]], device='mps:0')\n",
      "Iteration 53430 Training loss 0.060170672833919525 Validation loss 0.058763537555933 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.8792],\n",
      "        [0.8882]], device='mps:0')\n",
      "Iteration 53440 Training loss 0.05525904893875122 Validation loss 0.05883582681417465 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.0740],\n",
      "        [0.2935]], device='mps:0')\n",
      "Iteration 53450 Training loss 0.053043946623802185 Validation loss 0.05912874639034271 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.8897],\n",
      "        [0.4009]], device='mps:0')\n",
      "Iteration 53460 Training loss 0.05562787503004074 Validation loss 0.05879981070756912 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.8065],\n",
      "        [0.8842]], device='mps:0')\n",
      "Iteration 53470 Training loss 0.05065111070871353 Validation loss 0.05876016616821289 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.3903],\n",
      "        [0.0163]], device='mps:0')\n",
      "Iteration 53480 Training loss 0.05887969210743904 Validation loss 0.05875802040100098 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9296],\n",
      "        [0.0308]], device='mps:0')\n",
      "Iteration 53490 Training loss 0.06336355954408646 Validation loss 0.05876656249165535 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.3067],\n",
      "        [0.3717]], device='mps:0')\n",
      "Iteration 53500 Training loss 0.07016880065202713 Validation loss 0.058908890932798386 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.2022],\n",
      "        [0.0986]], device='mps:0')\n",
      "Iteration 53510 Training loss 0.05564146488904953 Validation loss 0.058759674429893494 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0175],\n",
      "        [0.0753]], device='mps:0')\n",
      "Iteration 53520 Training loss 0.05743330344557762 Validation loss 0.05877019837498665 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.0224],\n",
      "        [0.2635]], device='mps:0')\n",
      "Iteration 53530 Training loss 0.062462013214826584 Validation loss 0.0587429478764534 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9790],\n",
      "        [0.5757]], device='mps:0')\n",
      "Iteration 53540 Training loss 0.0525013767182827 Validation loss 0.05874219164252281 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.2217],\n",
      "        [0.0009]], device='mps:0')\n",
      "Iteration 53550 Training loss 0.05830119922757149 Validation loss 0.058888912200927734 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.9337],\n",
      "        [0.1964]], device='mps:0')\n",
      "Iteration 53560 Training loss 0.05836610123515129 Validation loss 0.05874156206846237 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.9967],\n",
      "        [0.8878]], device='mps:0')\n",
      "Iteration 53570 Training loss 0.049144916236400604 Validation loss 0.058740612119436264 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.8052],\n",
      "        [0.9652]], device='mps:0')\n",
      "Iteration 53580 Training loss 0.061435163021087646 Validation loss 0.058836229145526886 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.7583],\n",
      "        [0.7382]], device='mps:0')\n",
      "Iteration 53590 Training loss 0.059534549713134766 Validation loss 0.05875134468078613 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.9607],\n",
      "        [0.0186]], device='mps:0')\n",
      "Iteration 53600 Training loss 0.06002995744347572 Validation loss 0.05884510278701782 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.8745],\n",
      "        [0.7045]], device='mps:0')\n",
      "Iteration 53610 Training loss 0.05203467234969139 Validation loss 0.058744508773088455 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.6640],\n",
      "        [0.2153]], device='mps:0')\n",
      "Iteration 53620 Training loss 0.057002175599336624 Validation loss 0.05875119939446449 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.6711],\n",
      "        [0.3761]], device='mps:0')\n",
      "Iteration 53630 Training loss 0.05581426993012428 Validation loss 0.05888050049543381 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.2361],\n",
      "        [0.1244]], device='mps:0')\n",
      "Iteration 53640 Training loss 0.050400108098983765 Validation loss 0.058737862855196 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.8568],\n",
      "        [0.1038]], device='mps:0')\n",
      "Iteration 53650 Training loss 0.04861113429069519 Validation loss 0.058783549815416336 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.9757],\n",
      "        [0.7166]], device='mps:0')\n",
      "Iteration 53660 Training loss 0.05628509819507599 Validation loss 0.058774854987859726 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.1706],\n",
      "        [0.2084]], device='mps:0')\n",
      "Iteration 53670 Training loss 0.061892569065093994 Validation loss 0.05886448919773102 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9239],\n",
      "        [0.0035]], device='mps:0')\n",
      "Iteration 53680 Training loss 0.058553073555231094 Validation loss 0.058729387819767 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.0655],\n",
      "        [0.0043]], device='mps:0')\n",
      "Iteration 53690 Training loss 0.05384008213877678 Validation loss 0.05873924866318703 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.2420],\n",
      "        [0.8427]], device='mps:0')\n",
      "Iteration 53700 Training loss 0.06219609081745148 Validation loss 0.05873730033636093 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.1686],\n",
      "        [0.0208]], device='mps:0')\n",
      "Iteration 53710 Training loss 0.05660947784781456 Validation loss 0.058781445026397705 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.7422],\n",
      "        [0.1582]], device='mps:0')\n",
      "Iteration 53720 Training loss 0.05212772265076637 Validation loss 0.05874257907271385 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9845],\n",
      "        [0.8811]], device='mps:0')\n",
      "Iteration 53730 Training loss 0.056934792548418045 Validation loss 0.05874382331967354 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.3909],\n",
      "        [0.6535]], device='mps:0')\n",
      "Iteration 53740 Training loss 0.06201370060443878 Validation loss 0.05873870477080345 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.3999],\n",
      "        [0.0457]], device='mps:0')\n",
      "Iteration 53750 Training loss 0.05660860985517502 Validation loss 0.058722659945487976 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9572],\n",
      "        [0.3128]], device='mps:0')\n",
      "Iteration 53760 Training loss 0.05500473827123642 Validation loss 0.05929788574576378 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.0235],\n",
      "        [0.9558]], device='mps:0')\n",
      "Iteration 53770 Training loss 0.057169701904058456 Validation loss 0.05873897671699524 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.6946],\n",
      "        [0.0291]], device='mps:0')\n",
      "Iteration 53780 Training loss 0.05365647375583649 Validation loss 0.05876206234097481 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.1237],\n",
      "        [0.7283]], device='mps:0')\n",
      "Iteration 53790 Training loss 0.05150040611624718 Validation loss 0.05872850492596626 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.1873],\n",
      "        [0.6255]], device='mps:0')\n",
      "Iteration 53800 Training loss 0.05071739852428436 Validation loss 0.058723244816064835 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.0782],\n",
      "        [0.0401]], device='mps:0')\n",
      "Iteration 53810 Training loss 0.052147768437862396 Validation loss 0.05870848521590233 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.8493],\n",
      "        [0.1071]], device='mps:0')\n",
      "Iteration 53820 Training loss 0.05919728800654411 Validation loss 0.05882936343550682 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9717],\n",
      "        [0.0233]], device='mps:0')\n",
      "Iteration 53830 Training loss 0.05466655641794205 Validation loss 0.05870986357331276 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0170],\n",
      "        [0.9930]], device='mps:0')\n",
      "Iteration 53840 Training loss 0.056903235614299774 Validation loss 0.058754224330186844 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.5026],\n",
      "        [0.0154]], device='mps:0')\n",
      "Iteration 53850 Training loss 0.05660048499703407 Validation loss 0.05874371528625488 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.1449],\n",
      "        [0.9833]], device='mps:0')\n",
      "Iteration 53860 Training loss 0.059973783791065216 Validation loss 0.058702148497104645 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.1287],\n",
      "        [0.5099]], device='mps:0')\n",
      "Iteration 53870 Training loss 0.05905473604798317 Validation loss 0.058691129088401794 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.7203],\n",
      "        [0.0220]], device='mps:0')\n",
      "Iteration 53880 Training loss 0.05530354380607605 Validation loss 0.058853719383478165 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.2287],\n",
      "        [0.0133]], device='mps:0')\n",
      "Iteration 53890 Training loss 0.0670831948518753 Validation loss 0.058807726949453354 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.9766],\n",
      "        [0.7983]], device='mps:0')\n",
      "Iteration 53900 Training loss 0.05629216134548187 Validation loss 0.058715932071208954 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0122],\n",
      "        [0.8569]], device='mps:0')\n",
      "Iteration 53910 Training loss 0.05515550449490547 Validation loss 0.05873255431652069 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.3202],\n",
      "        [0.0524]], device='mps:0')\n",
      "Iteration 53920 Training loss 0.06123977527022362 Validation loss 0.058757320046424866 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.4131],\n",
      "        [0.7340]], device='mps:0')\n",
      "Iteration 53930 Training loss 0.05864613503217697 Validation loss 0.058696676045656204 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9461],\n",
      "        [0.0195]], device='mps:0')\n",
      "Iteration 53940 Training loss 0.05951131507754326 Validation loss 0.058782730251550674 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.0424],\n",
      "        [0.9416]], device='mps:0')\n",
      "Iteration 53950 Training loss 0.048332929611206055 Validation loss 0.05869824066758156 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.1784],\n",
      "        [0.4675]], device='mps:0')\n",
      "Iteration 53960 Training loss 0.06072781980037689 Validation loss 0.0587167851626873 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.7322],\n",
      "        [0.8281]], device='mps:0')\n",
      "Iteration 53970 Training loss 0.05729779973626137 Validation loss 0.05870300531387329 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.0766],\n",
      "        [0.8160]], device='mps:0')\n",
      "Iteration 53980 Training loss 0.05449124053120613 Validation loss 0.0587158165872097 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.3167],\n",
      "        [0.9709]], device='mps:0')\n",
      "Iteration 53990 Training loss 0.05413168668746948 Validation loss 0.058746859431266785 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9203],\n",
      "        [0.3689]], device='mps:0')\n",
      "Iteration 54000 Training loss 0.055733632296323776 Validation loss 0.058700334280729294 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9687],\n",
      "        [0.0755]], device='mps:0')\n",
      "Iteration 54010 Training loss 0.0678153708577156 Validation loss 0.05884621664881706 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.8727],\n",
      "        [0.5376]], device='mps:0')\n",
      "Iteration 54020 Training loss 0.05877414718270302 Validation loss 0.058800164610147476 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.6438],\n",
      "        [0.0711]], device='mps:0')\n",
      "Iteration 54030 Training loss 0.05594441294670105 Validation loss 0.05871685594320297 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.8905],\n",
      "        [0.9689]], device='mps:0')\n",
      "Iteration 54040 Training loss 0.04552798345685005 Validation loss 0.05871419608592987 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.7775],\n",
      "        [0.1241]], device='mps:0')\n",
      "Iteration 54050 Training loss 0.060955960303545 Validation loss 0.05874917656183243 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9881],\n",
      "        [0.9623]], device='mps:0')\n",
      "Iteration 54060 Training loss 0.054803136736154556 Validation loss 0.058696772903203964 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.4082],\n",
      "        [0.1758]], device='mps:0')\n",
      "Iteration 54070 Training loss 0.0579025074839592 Validation loss 0.058826275169849396 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.0885],\n",
      "        [0.6625]], device='mps:0')\n",
      "Iteration 54080 Training loss 0.059697043150663376 Validation loss 0.05870957300066948 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.9873],\n",
      "        [0.0223]], device='mps:0')\n",
      "Iteration 54090 Training loss 0.05406110733747482 Validation loss 0.05869700759649277 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.1451],\n",
      "        [0.1363]], device='mps:0')\n",
      "Iteration 54100 Training loss 0.05517568811774254 Validation loss 0.058734286576509476 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.6918],\n",
      "        [0.7906]], device='mps:0')\n",
      "Iteration 54110 Training loss 0.06137702986598015 Validation loss 0.05875275656580925 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.1634],\n",
      "        [0.9849]], device='mps:0')\n",
      "Iteration 54120 Training loss 0.05278107523918152 Validation loss 0.05875461921095848 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.4239],\n",
      "        [0.8431]], device='mps:0')\n",
      "Iteration 54130 Training loss 0.05425138399004936 Validation loss 0.058699943125247955 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.1537],\n",
      "        [0.9641]], device='mps:0')\n",
      "Iteration 54140 Training loss 0.060460831969976425 Validation loss 0.05874871090054512 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.8141],\n",
      "        [0.0532]], device='mps:0')\n",
      "Iteration 54150 Training loss 0.061663608998060226 Validation loss 0.05869228392839432 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0320],\n",
      "        [0.7535]], device='mps:0')\n",
      "Iteration 54160 Training loss 0.04989711567759514 Validation loss 0.05872739851474762 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.0372],\n",
      "        [0.0817]], device='mps:0')\n",
      "Iteration 54170 Training loss 0.05401882156729698 Validation loss 0.05886193737387657 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.1256],\n",
      "        [0.9808]], device='mps:0')\n",
      "Iteration 54180 Training loss 0.055390652269124985 Validation loss 0.0587027408182621 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9583],\n",
      "        [0.0626]], device='mps:0')\n",
      "Iteration 54190 Training loss 0.058011941611766815 Validation loss 0.05869809165596962 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0281],\n",
      "        [0.5536]], device='mps:0')\n",
      "Iteration 54200 Training loss 0.05233039706945419 Validation loss 0.05884646996855736 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.1583],\n",
      "        [0.0763]], device='mps:0')\n",
      "Iteration 54210 Training loss 0.06118592619895935 Validation loss 0.05880635231733322 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.8765],\n",
      "        [0.1541]], device='mps:0')\n",
      "Iteration 54220 Training loss 0.05372661352157593 Validation loss 0.058688968420028687 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.6671],\n",
      "        [0.5746]], device='mps:0')\n",
      "Iteration 54230 Training loss 0.06300188601016998 Validation loss 0.05876130983233452 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.1896],\n",
      "        [0.3239]], device='mps:0')\n",
      "Iteration 54240 Training loss 0.0609988309442997 Validation loss 0.05868497118353844 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.3982],\n",
      "        [0.8899]], device='mps:0')\n",
      "Iteration 54250 Training loss 0.053366173058748245 Validation loss 0.05873986333608627 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.8542],\n",
      "        [0.5413]], device='mps:0')\n",
      "Iteration 54260 Training loss 0.05656377598643303 Validation loss 0.05868104100227356 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.5002],\n",
      "        [0.9848]], device='mps:0')\n",
      "Iteration 54270 Training loss 0.05695842579007149 Validation loss 0.05869049206376076 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9673],\n",
      "        [0.8683]], device='mps:0')\n",
      "Iteration 54280 Training loss 0.052327558398246765 Validation loss 0.05903494358062744 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.4938],\n",
      "        [0.9765]], device='mps:0')\n",
      "Iteration 54290 Training loss 0.04373611509799957 Validation loss 0.05868551507592201 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.1168],\n",
      "        [0.0202]], device='mps:0')\n",
      "Iteration 54300 Training loss 0.04656965658068657 Validation loss 0.05875368043780327 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.2303],\n",
      "        [0.0376]], device='mps:0')\n",
      "Iteration 54310 Training loss 0.06177392974495888 Validation loss 0.05885712429881096 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.5169],\n",
      "        [0.0532]], device='mps:0')\n",
      "Iteration 54320 Training loss 0.0534660629928112 Validation loss 0.058696962893009186 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0870],\n",
      "        [0.5891]], device='mps:0')\n",
      "Iteration 54330 Training loss 0.06012347713112831 Validation loss 0.05869371071457863 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.7689],\n",
      "        [0.9940]], device='mps:0')\n",
      "Iteration 54340 Training loss 0.05471375212073326 Validation loss 0.058777470141649246 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9402],\n",
      "        [0.9663]], device='mps:0')\n",
      "Iteration 54350 Training loss 0.053022850304841995 Validation loss 0.0587199442088604 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9842],\n",
      "        [0.9961]], device='mps:0')\n",
      "Iteration 54360 Training loss 0.06342882663011551 Validation loss 0.058757442981004715 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.8643],\n",
      "        [0.0427]], device='mps:0')\n",
      "Iteration 54370 Training loss 0.05262643098831177 Validation loss 0.05874006077647209 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.0212],\n",
      "        [0.3228]], device='mps:0')\n",
      "Iteration 54380 Training loss 0.0599466972053051 Validation loss 0.05868751183152199 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.5273],\n",
      "        [0.2383]], device='mps:0')\n",
      "Iteration 54390 Training loss 0.06328313797712326 Validation loss 0.058695513755083084 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.3770],\n",
      "        [0.0062]], device='mps:0')\n",
      "Iteration 54400 Training loss 0.060425519943237305 Validation loss 0.058911118656396866 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.7617],\n",
      "        [0.7457]], device='mps:0')\n",
      "Iteration 54410 Training loss 0.0527290441095829 Validation loss 0.058689478784799576 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9860],\n",
      "        [0.7824]], device='mps:0')\n",
      "Iteration 54420 Training loss 0.05768650025129318 Validation loss 0.05876127630472183 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.1608],\n",
      "        [0.2144]], device='mps:0')\n",
      "Iteration 54430 Training loss 0.05626218765974045 Validation loss 0.05870559439063072 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.0262],\n",
      "        [0.1427]], device='mps:0')\n",
      "Iteration 54440 Training loss 0.0657050758600235 Validation loss 0.058744028210639954 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.5546],\n",
      "        [0.6238]], device='mps:0')\n",
      "Iteration 54450 Training loss 0.06083451956510544 Validation loss 0.058698803186416626 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0164],\n",
      "        [0.6330]], device='mps:0')\n",
      "Iteration 54460 Training loss 0.06123979389667511 Validation loss 0.05874504894018173 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9194],\n",
      "        [0.9601]], device='mps:0')\n",
      "Iteration 54470 Training loss 0.05794833227992058 Validation loss 0.05873149260878563 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.1489],\n",
      "        [0.0762]], device='mps:0')\n",
      "Iteration 54480 Training loss 0.05763944610953331 Validation loss 0.058794695883989334 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.0451],\n",
      "        [0.1380]], device='mps:0')\n",
      "Iteration 54490 Training loss 0.05178007856011391 Validation loss 0.05891120806336403 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.3911],\n",
      "        [0.9804]], device='mps:0')\n",
      "Iteration 54500 Training loss 0.054704029113054276 Validation loss 0.0587080717086792 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9925],\n",
      "        [0.7268]], device='mps:0')\n",
      "Iteration 54510 Training loss 0.05053941160440445 Validation loss 0.05872534587979317 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.0436],\n",
      "        [0.9803]], device='mps:0')\n",
      "Iteration 54520 Training loss 0.04997248575091362 Validation loss 0.05881139636039734 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.4306],\n",
      "        [0.2150]], device='mps:0')\n",
      "Iteration 54530 Training loss 0.05343027785420418 Validation loss 0.05867703631520271 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.2125],\n",
      "        [0.0191]], device='mps:0')\n",
      "Iteration 54540 Training loss 0.06087876483798027 Validation loss 0.05881578475236893 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.5921],\n",
      "        [0.6942]], device='mps:0')\n",
      "Iteration 54550 Training loss 0.05515037849545479 Validation loss 0.05878879874944687 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.0458],\n",
      "        [0.9507]], device='mps:0')\n",
      "Iteration 54560 Training loss 0.05109447240829468 Validation loss 0.05866507068276405 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.2081],\n",
      "        [0.0225]], device='mps:0')\n",
      "Iteration 54570 Training loss 0.05003713071346283 Validation loss 0.058757882565259933 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.7581],\n",
      "        [0.8471]], device='mps:0')\n",
      "Iteration 54580 Training loss 0.055766113102436066 Validation loss 0.05870993807911873 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.3450],\n",
      "        [0.9572]], device='mps:0')\n",
      "Iteration 54590 Training loss 0.051750991493463516 Validation loss 0.05869515612721443 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9780],\n",
      "        [0.3129]], device='mps:0')\n",
      "Iteration 54600 Training loss 0.050568122416734695 Validation loss 0.058725178241729736 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0605],\n",
      "        [0.9514]], device='mps:0')\n",
      "Iteration 54610 Training loss 0.05907440185546875 Validation loss 0.05873304232954979 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.8380],\n",
      "        [0.9817]], device='mps:0')\n",
      "Iteration 54620 Training loss 0.054465364664793015 Validation loss 0.05879676714539528 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.8822],\n",
      "        [0.0178]], device='mps:0')\n",
      "Iteration 54630 Training loss 0.058119963854551315 Validation loss 0.058667514473199844 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.2409],\n",
      "        [0.6958]], device='mps:0')\n",
      "Iteration 54640 Training loss 0.06093478575348854 Validation loss 0.058672938495874405 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9362],\n",
      "        [0.3776]], device='mps:0')\n",
      "Iteration 54650 Training loss 0.060107916593551636 Validation loss 0.05872741714119911 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0488],\n",
      "        [0.0245]], device='mps:0')\n",
      "Iteration 54660 Training loss 0.05540933832526207 Validation loss 0.05866292119026184 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9901],\n",
      "        [0.0242]], device='mps:0')\n",
      "Iteration 54670 Training loss 0.05212731286883354 Validation loss 0.058934807777404785 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0060],\n",
      "        [0.9456]], device='mps:0')\n",
      "Iteration 54680 Training loss 0.061464108526706696 Validation loss 0.05865422263741493 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.9497],\n",
      "        [0.7461]], device='mps:0')\n",
      "Iteration 54690 Training loss 0.051687080413103104 Validation loss 0.058667298406362534 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.9599],\n",
      "        [0.5776]], device='mps:0')\n",
      "Iteration 54700 Training loss 0.044587068259716034 Validation loss 0.05866868048906326 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.3955],\n",
      "        [0.0108]], device='mps:0')\n",
      "Iteration 54710 Training loss 0.055540770292282104 Validation loss 0.05867138132452965 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9082],\n",
      "        [0.3483]], device='mps:0')\n",
      "Iteration 54720 Training loss 0.05188136547803879 Validation loss 0.058698657900094986 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0280],\n",
      "        [0.0493]], device='mps:0')\n",
      "Iteration 54730 Training loss 0.04978181794285774 Validation loss 0.058655474334955215 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9949],\n",
      "        [0.9968]], device='mps:0')\n",
      "Iteration 54740 Training loss 0.05360717698931694 Validation loss 0.058690302073955536 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9330],\n",
      "        [0.0155]], device='mps:0')\n",
      "Iteration 54750 Training loss 0.053629517555236816 Validation loss 0.05866359919309616 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.1679],\n",
      "        [0.3833]], device='mps:0')\n",
      "Iteration 54760 Training loss 0.05041947215795517 Validation loss 0.058648981153964996 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.4294],\n",
      "        [0.9436]], device='mps:0')\n",
      "Iteration 54770 Training loss 0.05752715840935707 Validation loss 0.05895095691084862 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0372],\n",
      "        [0.9281]], device='mps:0')\n",
      "Iteration 54780 Training loss 0.06107423081994057 Validation loss 0.05865325406193733 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9006],\n",
      "        [0.0394]], device='mps:0')\n",
      "Iteration 54790 Training loss 0.05704427510499954 Validation loss 0.05864900350570679 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9714],\n",
      "        [0.3906]], device='mps:0')\n",
      "Iteration 54800 Training loss 0.06084775924682617 Validation loss 0.05866051837801933 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.9934],\n",
      "        [0.3913]], device='mps:0')\n",
      "Iteration 54810 Training loss 0.05525611713528633 Validation loss 0.05896648392081261 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0199],\n",
      "        [0.9948]], device='mps:0')\n",
      "Iteration 54820 Training loss 0.05398796871304512 Validation loss 0.05868309363722801 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0253],\n",
      "        [0.9247]], device='mps:0')\n",
      "Iteration 54830 Training loss 0.06584125012159348 Validation loss 0.05866405740380287 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9450],\n",
      "        [0.4759]], device='mps:0')\n",
      "Iteration 54840 Training loss 0.05429818108677864 Validation loss 0.058650195598602295 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.7775],\n",
      "        [0.3898]], device='mps:0')\n",
      "Iteration 54850 Training loss 0.055692918598651886 Validation loss 0.058858539909124374 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.7441],\n",
      "        [0.9154]], device='mps:0')\n",
      "Iteration 54860 Training loss 0.05972282588481903 Validation loss 0.05864555016160011 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0258],\n",
      "        [0.0970]], device='mps:0')\n",
      "Iteration 54870 Training loss 0.05673911049962044 Validation loss 0.058640554547309875 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.8653],\n",
      "        [0.9401]], device='mps:0')\n",
      "Iteration 54880 Training loss 0.055345695465803146 Validation loss 0.058634236454963684 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9849],\n",
      "        [0.8680]], device='mps:0')\n",
      "Iteration 54890 Training loss 0.05102331191301346 Validation loss 0.05880524963140488 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.0589],\n",
      "        [0.4304]], device='mps:0')\n",
      "Iteration 54900 Training loss 0.05336330831050873 Validation loss 0.05865425989031792 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.5438],\n",
      "        [0.6479]], device='mps:0')\n",
      "Iteration 54910 Training loss 0.05866079032421112 Validation loss 0.05864568427205086 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0496],\n",
      "        [0.1995]], device='mps:0')\n",
      "Iteration 54920 Training loss 0.06536590307950974 Validation loss 0.058651141822338104 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.0490],\n",
      "        [0.8818]], device='mps:0')\n",
      "Iteration 54930 Training loss 0.06253936141729355 Validation loss 0.05877169966697693 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.0867],\n",
      "        [0.0129]], device='mps:0')\n",
      "Iteration 54940 Training loss 0.0602726973593235 Validation loss 0.058668725192546844 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.9652],\n",
      "        [0.8007]], device='mps:0')\n",
      "Iteration 54950 Training loss 0.051305100321769714 Validation loss 0.058663636445999146 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9242],\n",
      "        [0.0773]], device='mps:0')\n",
      "Iteration 54960 Training loss 0.06176051124930382 Validation loss 0.05864429846405983 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9499],\n",
      "        [0.9183]], device='mps:0')\n",
      "Iteration 54970 Training loss 0.0674152672290802 Validation loss 0.05873473361134529 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.8272],\n",
      "        [0.1727]], device='mps:0')\n",
      "Iteration 54980 Training loss 0.05762936547398567 Validation loss 0.05875600129365921 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.5704],\n",
      "        [0.9495]], device='mps:0')\n",
      "Iteration 54990 Training loss 0.055931489914655685 Validation loss 0.05864042788743973 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.6157],\n",
      "        [0.5439]], device='mps:0')\n",
      "Iteration 55000 Training loss 0.06142367050051689 Validation loss 0.058635957539081573 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.2050],\n",
      "        [0.9257]], device='mps:0')\n",
      "Iteration 55010 Training loss 0.06412703543901443 Validation loss 0.058723580092191696 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.9870],\n",
      "        [0.3910]], device='mps:0')\n",
      "Iteration 55020 Training loss 0.060900814831256866 Validation loss 0.05863259732723236 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.7777],\n",
      "        [0.0382]], device='mps:0')\n",
      "Iteration 55030 Training loss 0.054206617176532745 Validation loss 0.058643847703933716 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.4481],\n",
      "        [0.2203]], device='mps:0')\n",
      "Iteration 55040 Training loss 0.06917621940374374 Validation loss 0.05863574519753456 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.9731],\n",
      "        [0.1026]], device='mps:0')\n",
      "Iteration 55050 Training loss 0.05514460429549217 Validation loss 0.05864742770791054 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.6637],\n",
      "        [0.9346]], device='mps:0')\n",
      "Iteration 55060 Training loss 0.062099389731884 Validation loss 0.05862808600068092 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9585],\n",
      "        [0.1746]], device='mps:0')\n",
      "Iteration 55070 Training loss 0.0578506663441658 Validation loss 0.05863979458808899 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.2954],\n",
      "        [0.9941]], device='mps:0')\n",
      "Iteration 55080 Training loss 0.050922345370054245 Validation loss 0.058632224798202515 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.1164],\n",
      "        [0.3127]], device='mps:0')\n",
      "Iteration 55090 Training loss 0.054567884653806686 Validation loss 0.0586431622505188 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9406],\n",
      "        [0.9772]], device='mps:0')\n",
      "Iteration 55100 Training loss 0.059133272618055344 Validation loss 0.05862930417060852 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.8443],\n",
      "        [0.1080]], device='mps:0')\n",
      "Iteration 55110 Training loss 0.05954502150416374 Validation loss 0.05911519005894661 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.2087],\n",
      "        [0.8492]], device='mps:0')\n",
      "Iteration 55120 Training loss 0.0575229674577713 Validation loss 0.058828212320804596 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.9706],\n",
      "        [0.0532]], device='mps:0')\n",
      "Iteration 55130 Training loss 0.05810372158885002 Validation loss 0.05861758440732956 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.9471],\n",
      "        [0.3786]], device='mps:0')\n",
      "Iteration 55140 Training loss 0.05410265922546387 Validation loss 0.05869869515299797 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0764],\n",
      "        [0.0196]], device='mps:0')\n",
      "Iteration 55150 Training loss 0.06340299546718597 Validation loss 0.05863673985004425 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.9652],\n",
      "        [0.6189]], device='mps:0')\n",
      "Iteration 55160 Training loss 0.046125929802656174 Validation loss 0.058789417147636414 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.9823],\n",
      "        [0.0922]], device='mps:0')\n",
      "Iteration 55170 Training loss 0.05019179731607437 Validation loss 0.058694954961538315 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.7714],\n",
      "        [0.8156]], device='mps:0')\n",
      "Iteration 55180 Training loss 0.057462405413389206 Validation loss 0.058666568249464035 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.4633],\n",
      "        [0.9544]], device='mps:0')\n",
      "Iteration 55190 Training loss 0.05859290435910225 Validation loss 0.058711327612400055 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.7607],\n",
      "        [0.9516]], device='mps:0')\n",
      "Iteration 55200 Training loss 0.06136906519532204 Validation loss 0.0589226670563221 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9472],\n",
      "        [0.7085]], device='mps:0')\n",
      "Iteration 55210 Training loss 0.05532267317175865 Validation loss 0.05865645781159401 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.8216],\n",
      "        [0.9820]], device='mps:0')\n",
      "Iteration 55220 Training loss 0.053001053631305695 Validation loss 0.05861455202102661 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0561],\n",
      "        [0.9852]], device='mps:0')\n",
      "Iteration 55230 Training loss 0.0480092316865921 Validation loss 0.05862177535891533 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.1562],\n",
      "        [0.1842]], device='mps:0')\n",
      "Iteration 55240 Training loss 0.05333064869046211 Validation loss 0.058613359928131104 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.5154],\n",
      "        [0.9026]], device='mps:0')\n",
      "Iteration 55250 Training loss 0.041038282215595245 Validation loss 0.05864046514034271 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.0614],\n",
      "        [0.8764]], device='mps:0')\n",
      "Iteration 55260 Training loss 0.05265351012349129 Validation loss 0.05866532772779465 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.2981],\n",
      "        [0.9439]], device='mps:0')\n",
      "Iteration 55270 Training loss 0.052416782826185226 Validation loss 0.05869558826088905 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.7676],\n",
      "        [0.8856]], device='mps:0')\n",
      "Iteration 55280 Training loss 0.06204390525817871 Validation loss 0.05859803408384323 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.0833],\n",
      "        [0.1298]], device='mps:0')\n",
      "Iteration 55290 Training loss 0.0627426728606224 Validation loss 0.058594778180122375 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.0726],\n",
      "        [0.1319]], device='mps:0')\n",
      "Iteration 55300 Training loss 0.05334165319800377 Validation loss 0.05859268084168434 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.3683],\n",
      "        [0.4348]], device='mps:0')\n",
      "Iteration 55310 Training loss 0.05708562582731247 Validation loss 0.058604590594768524 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.7785],\n",
      "        [0.0356]], device='mps:0')\n",
      "Iteration 55320 Training loss 0.05908391997218132 Validation loss 0.05859578400850296 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9073],\n",
      "        [0.0101]], device='mps:0')\n",
      "Iteration 55330 Training loss 0.061933696269989014 Validation loss 0.0586056262254715 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.9872],\n",
      "        [0.9918]], device='mps:0')\n",
      "Iteration 55340 Training loss 0.06114452704787254 Validation loss 0.05875812843441963 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9504],\n",
      "        [0.9055]], device='mps:0')\n",
      "Iteration 55350 Training loss 0.06035813316702843 Validation loss 0.05865400284528732 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.4555],\n",
      "        [0.3259]], device='mps:0')\n",
      "Iteration 55360 Training loss 0.05312328785657883 Validation loss 0.05860567465424538 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.6892],\n",
      "        [0.5808]], device='mps:0')\n",
      "Iteration 55370 Training loss 0.04794290289282799 Validation loss 0.05867806077003479 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.2928],\n",
      "        [0.0415]], device='mps:0')\n",
      "Iteration 55380 Training loss 0.0526275709271431 Validation loss 0.0587049201130867 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.0048],\n",
      "        [0.0848]], device='mps:0')\n",
      "Iteration 55390 Training loss 0.057531628757715225 Validation loss 0.058593329042196274 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.1188],\n",
      "        [0.0471]], device='mps:0')\n",
      "Iteration 55400 Training loss 0.05182262137532234 Validation loss 0.058620989322662354 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0563],\n",
      "        [0.8414]], device='mps:0')\n",
      "Iteration 55410 Training loss 0.062418870627880096 Validation loss 0.05862611532211304 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.6741],\n",
      "        [0.9846]], device='mps:0')\n",
      "Iteration 55420 Training loss 0.059080276638269424 Validation loss 0.058621108531951904 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.3860],\n",
      "        [0.3782]], device='mps:0')\n",
      "Iteration 55430 Training loss 0.0582333505153656 Validation loss 0.05859539285302162 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.2005],\n",
      "        [0.3546]], device='mps:0')\n",
      "Iteration 55440 Training loss 0.05171181634068489 Validation loss 0.05860185995697975 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.8925],\n",
      "        [0.9101]], device='mps:0')\n",
      "Iteration 55450 Training loss 0.061274051666259766 Validation loss 0.05866507440805435 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0447],\n",
      "        [0.0765]], device='mps:0')\n",
      "Iteration 55460 Training loss 0.05767204239964485 Validation loss 0.058620963245630264 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.5423],\n",
      "        [0.8325]], device='mps:0')\n",
      "Iteration 55470 Training loss 0.06330548971891403 Validation loss 0.058603085577487946 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.9342],\n",
      "        [0.9261]], device='mps:0')\n",
      "Iteration 55480 Training loss 0.04851832240819931 Validation loss 0.05859320983290672 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.9590],\n",
      "        [0.6788]], device='mps:0')\n",
      "Iteration 55490 Training loss 0.05111309885978699 Validation loss 0.05858446657657623 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.8329],\n",
      "        [0.0157]], device='mps:0')\n",
      "Iteration 55500 Training loss 0.05494826287031174 Validation loss 0.05860038101673126 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.2671],\n",
      "        [0.0301]], device='mps:0')\n",
      "Iteration 55510 Training loss 0.057896241545677185 Validation loss 0.05868403613567352 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0554],\n",
      "        [0.9667]], device='mps:0')\n",
      "Iteration 55520 Training loss 0.059959568083286285 Validation loss 0.05858742818236351 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0562],\n",
      "        [0.0988]], device='mps:0')\n",
      "Iteration 55530 Training loss 0.06422768533229828 Validation loss 0.0585922971367836 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.1016],\n",
      "        [0.8475]], device='mps:0')\n",
      "Iteration 55540 Training loss 0.05473605915904045 Validation loss 0.058582600206136703 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9919],\n",
      "        [0.0222]], device='mps:0')\n",
      "Iteration 55550 Training loss 0.057433973997831345 Validation loss 0.05860080569982529 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.5639],\n",
      "        [0.1130]], device='mps:0')\n",
      "Iteration 55560 Training loss 0.048737142235040665 Validation loss 0.058736033737659454 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.9852],\n",
      "        [0.2416]], device='mps:0')\n",
      "Iteration 55570 Training loss 0.055397577583789825 Validation loss 0.05867216736078262 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.4911],\n",
      "        [0.4702]], device='mps:0')\n",
      "Iteration 55580 Training loss 0.05579352751374245 Validation loss 0.058572519570589066 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.0532],\n",
      "        [0.0826]], device='mps:0')\n",
      "Iteration 55590 Training loss 0.048961687833070755 Validation loss 0.05857183784246445 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.1837],\n",
      "        [0.6823]], device='mps:0')\n",
      "Iteration 55600 Training loss 0.05228126794099808 Validation loss 0.058593325316905975 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.9155],\n",
      "        [0.7855]], device='mps:0')\n",
      "Iteration 55610 Training loss 0.04783429950475693 Validation loss 0.05883576348423958 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.6205],\n",
      "        [0.4801]], device='mps:0')\n",
      "Iteration 55620 Training loss 0.05150510370731354 Validation loss 0.05857045203447342 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.2668],\n",
      "        [0.0050]], device='mps:0')\n",
      "Iteration 55630 Training loss 0.05253086984157562 Validation loss 0.05856596678495407 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.7657],\n",
      "        [0.0756]], device='mps:0')\n",
      "Iteration 55640 Training loss 0.053598079830408096 Validation loss 0.05897429585456848 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.3944],\n",
      "        [0.8794]], device='mps:0')\n",
      "Iteration 55650 Training loss 0.06804727762937546 Validation loss 0.058565814048051834 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.3593],\n",
      "        [0.6466]], device='mps:0')\n",
      "Iteration 55660 Training loss 0.061524610966444016 Validation loss 0.058608729392290115 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.2801],\n",
      "        [0.8346]], device='mps:0')\n",
      "Iteration 55670 Training loss 0.055867135524749756 Validation loss 0.05874406173825264 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9794],\n",
      "        [0.0698]], device='mps:0')\n",
      "Iteration 55680 Training loss 0.06403091549873352 Validation loss 0.05882280692458153 Accuracy 0.8378750681877136\n",
      "Output tensor([[0.7816],\n",
      "        [0.9076]], device='mps:0')\n",
      "Iteration 55690 Training loss 0.059886783361434937 Validation loss 0.05860741063952446 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.2829],\n",
      "        [0.4725]], device='mps:0')\n",
      "Iteration 55700 Training loss 0.06621358543634415 Validation loss 0.05858976021409035 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.9794],\n",
      "        [0.9080]], device='mps:0')\n",
      "Iteration 55710 Training loss 0.056389741599559784 Validation loss 0.058727916330099106 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.8800],\n",
      "        [0.8177]], device='mps:0')\n",
      "Iteration 55720 Training loss 0.05492231249809265 Validation loss 0.05856742337346077 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.8503],\n",
      "        [0.2888]], device='mps:0')\n",
      "Iteration 55730 Training loss 0.06089485064148903 Validation loss 0.05861708149313927 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.0283],\n",
      "        [0.1298]], device='mps:0')\n",
      "Iteration 55740 Training loss 0.052487440407276154 Validation loss 0.05871821194887161 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.3052],\n",
      "        [0.7702]], device='mps:0')\n",
      "Iteration 55750 Training loss 0.05645088851451874 Validation loss 0.05859376862645149 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.0919],\n",
      "        [0.1460]], device='mps:0')\n",
      "Iteration 55760 Training loss 0.05220603942871094 Validation loss 0.05868523195385933 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.3016],\n",
      "        [0.9707]], device='mps:0')\n",
      "Iteration 55770 Training loss 0.05962527170777321 Validation loss 0.058634500950574875 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.4703],\n",
      "        [0.8886]], device='mps:0')\n",
      "Iteration 55780 Training loss 0.06304986774921417 Validation loss 0.058595217764377594 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0324],\n",
      "        [0.5343]], device='mps:0')\n",
      "Iteration 55790 Training loss 0.05831264704465866 Validation loss 0.05880006030201912 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.7121],\n",
      "        [0.0415]], device='mps:0')\n",
      "Iteration 55800 Training loss 0.06196770444512367 Validation loss 0.0586184598505497 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.4879],\n",
      "        [0.8854]], device='mps:0')\n",
      "Iteration 55810 Training loss 0.058796074241399765 Validation loss 0.05856461450457573 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.8119],\n",
      "        [0.5816]], device='mps:0')\n",
      "Iteration 55820 Training loss 0.05759546160697937 Validation loss 0.05856863781809807 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.4945],\n",
      "        [0.9657]], device='mps:0')\n",
      "Iteration 55830 Training loss 0.05430683121085167 Validation loss 0.05855807662010193 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.1492],\n",
      "        [0.1370]], device='mps:0')\n",
      "Iteration 55840 Training loss 0.05660943686962128 Validation loss 0.05856286361813545 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9836],\n",
      "        [0.0599]], device='mps:0')\n",
      "Iteration 55850 Training loss 0.057708580046892166 Validation loss 0.05856001004576683 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.3447],\n",
      "        [0.7931]], device='mps:0')\n",
      "Iteration 55860 Training loss 0.06485838443040848 Validation loss 0.05879759415984154 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.7507],\n",
      "        [0.9167]], device='mps:0')\n",
      "Iteration 55870 Training loss 0.054086752235889435 Validation loss 0.05856197327375412 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.3735],\n",
      "        [0.0199]], device='mps:0')\n",
      "Iteration 55880 Training loss 0.0483999066054821 Validation loss 0.05855254456400871 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9564],\n",
      "        [0.2432]], device='mps:0')\n",
      "Iteration 55890 Training loss 0.06330307573080063 Validation loss 0.05857644975185394 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.0472],\n",
      "        [0.9660]], device='mps:0')\n",
      "Iteration 55900 Training loss 0.06882663816213608 Validation loss 0.05872320756316185 Accuracy 0.8386250138282776\n",
      "Output tensor([[0.9871],\n",
      "        [0.7786]], device='mps:0')\n",
      "Iteration 55910 Training loss 0.05937649682164192 Validation loss 0.058574166148900986 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0981],\n",
      "        [0.2801]], device='mps:0')\n",
      "Iteration 55920 Training loss 0.05690746009349823 Validation loss 0.058547280728816986 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9646],\n",
      "        [0.4483]], device='mps:0')\n",
      "Iteration 55930 Training loss 0.05540526285767555 Validation loss 0.05854174122214317 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.2067],\n",
      "        [0.6276]], device='mps:0')\n",
      "Iteration 55940 Training loss 0.059909638017416 Validation loss 0.05853593349456787 Accuracy 0.843000054359436\n",
      "Output tensor([[0.2386],\n",
      "        [0.8924]], device='mps:0')\n",
      "Iteration 55950 Training loss 0.054592445492744446 Validation loss 0.05852999538183212 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.8773],\n",
      "        [0.0015]], device='mps:0')\n",
      "Iteration 55960 Training loss 0.05416034534573555 Validation loss 0.058633316308259964 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.6856],\n",
      "        [0.0395]], device='mps:0')\n",
      "Iteration 55970 Training loss 0.048400647938251495 Validation loss 0.05866045877337456 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0223],\n",
      "        [0.4122]], device='mps:0')\n",
      "Iteration 55980 Training loss 0.05727003514766693 Validation loss 0.05853618308901787 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.5012],\n",
      "        [0.1594]], device='mps:0')\n",
      "Iteration 55990 Training loss 0.05407712981104851 Validation loss 0.0587804801762104 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.9151],\n",
      "        [0.1866]], device='mps:0')\n",
      "Iteration 56000 Training loss 0.06070015951991081 Validation loss 0.05869092792272568 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.2077],\n",
      "        [0.7644]], device='mps:0')\n",
      "Iteration 56010 Training loss 0.05581509321928024 Validation loss 0.05870211124420166 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.7644],\n",
      "        [0.7355]], device='mps:0')\n",
      "Iteration 56020 Training loss 0.05345537140965462 Validation loss 0.05869913846254349 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.8339],\n",
      "        [0.9308]], device='mps:0')\n",
      "Iteration 56030 Training loss 0.06136414408683777 Validation loss 0.05871789902448654 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.6423],\n",
      "        [0.1337]], device='mps:0')\n",
      "Iteration 56040 Training loss 0.05599573999643326 Validation loss 0.05864483863115311 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.7951],\n",
      "        [0.8170]], device='mps:0')\n",
      "Iteration 56050 Training loss 0.051029425114393234 Validation loss 0.05853136256337166 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.1632],\n",
      "        [0.0728]], device='mps:0')\n",
      "Iteration 56060 Training loss 0.06468431651592255 Validation loss 0.058572422713041306 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.4304],\n",
      "        [0.8593]], device='mps:0')\n",
      "Iteration 56070 Training loss 0.054279401898384094 Validation loss 0.0589124970138073 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.1430],\n",
      "        [0.8541]], device='mps:0')\n",
      "Iteration 56080 Training loss 0.05273929610848427 Validation loss 0.058598924428224564 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.3567],\n",
      "        [0.5778]], device='mps:0')\n",
      "Iteration 56090 Training loss 0.04406499117612839 Validation loss 0.058551859110593796 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.1727],\n",
      "        [0.6098]], device='mps:0')\n",
      "Iteration 56100 Training loss 0.061194371432065964 Validation loss 0.058544766157865524 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.0823],\n",
      "        [0.9634]], device='mps:0')\n",
      "Iteration 56110 Training loss 0.057806722819805145 Validation loss 0.05873214825987816 Accuracy 0.8388750553131104\n",
      "Output tensor([[0.2253],\n",
      "        [0.9472]], device='mps:0')\n",
      "Iteration 56120 Training loss 0.04860735684633255 Validation loss 0.05873900651931763 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.1293],\n",
      "        [0.0637]], device='mps:0')\n",
      "Iteration 56130 Training loss 0.051884975284338 Validation loss 0.05855245515704155 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.5879],\n",
      "        [0.9462]], device='mps:0')\n",
      "Iteration 56140 Training loss 0.05173768103122711 Validation loss 0.058544956147670746 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0104],\n",
      "        [0.4782]], device='mps:0')\n",
      "Iteration 56150 Training loss 0.05189641937613487 Validation loss 0.058538369834423065 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.8928],\n",
      "        [0.3147]], device='mps:0')\n",
      "Iteration 56160 Training loss 0.04589764028787613 Validation loss 0.058541569858789444 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.3222],\n",
      "        [0.3755]], device='mps:0')\n",
      "Iteration 56170 Training loss 0.05418090522289276 Validation loss 0.058543238788843155 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0115],\n",
      "        [0.0712]], device='mps:0')\n",
      "Iteration 56180 Training loss 0.05156496539711952 Validation loss 0.058942005038261414 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.7634],\n",
      "        [0.8865]], device='mps:0')\n",
      "Iteration 56190 Training loss 0.05643070489168167 Validation loss 0.058757707476615906 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.2673],\n",
      "        [0.2483]], device='mps:0')\n",
      "Iteration 56200 Training loss 0.047559745609760284 Validation loss 0.05858396738767624 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.7589],\n",
      "        [0.1314]], device='mps:0')\n",
      "Iteration 56210 Training loss 0.05830864608287811 Validation loss 0.05855080857872963 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9775],\n",
      "        [0.7631]], device='mps:0')\n",
      "Iteration 56220 Training loss 0.05155932158231735 Validation loss 0.05859978869557381 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0633],\n",
      "        [0.7145]], device='mps:0')\n",
      "Iteration 56230 Training loss 0.05962832272052765 Validation loss 0.0590079128742218 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.6345],\n",
      "        [0.9222]], device='mps:0')\n",
      "Iteration 56240 Training loss 0.06318268924951553 Validation loss 0.05874348804354668 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.3998],\n",
      "        [0.3397]], device='mps:0')\n",
      "Iteration 56250 Training loss 0.05615580826997757 Validation loss 0.0586700476706028 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.1466],\n",
      "        [0.0547]], device='mps:0')\n",
      "Iteration 56260 Training loss 0.0585688091814518 Validation loss 0.05867024511098862 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.4862],\n",
      "        [0.1214]], device='mps:0')\n",
      "Iteration 56270 Training loss 0.0569368451833725 Validation loss 0.058550409972667694 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.9892],\n",
      "        [0.4530]], device='mps:0')\n",
      "Iteration 56280 Training loss 0.05245616286993027 Validation loss 0.058687832206487656 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.2261],\n",
      "        [0.3929]], device='mps:0')\n",
      "Iteration 56290 Training loss 0.05489202216267586 Validation loss 0.05854583904147148 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9202],\n",
      "        [0.9818]], device='mps:0')\n",
      "Iteration 56300 Training loss 0.05699113756418228 Validation loss 0.05856224521994591 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.8772],\n",
      "        [0.1392]], device='mps:0')\n",
      "Iteration 56310 Training loss 0.05569244176149368 Validation loss 0.05854979157447815 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.8871],\n",
      "        [0.6419]], device='mps:0')\n",
      "Iteration 56320 Training loss 0.0610867403447628 Validation loss 0.058899667114019394 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9438],\n",
      "        [0.1095]], device='mps:0')\n",
      "Iteration 56330 Training loss 0.060008030384778976 Validation loss 0.05863954499363899 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9605],\n",
      "        [0.3409]], device='mps:0')\n",
      "Iteration 56340 Training loss 0.05599595606327057 Validation loss 0.0585424080491066 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.8542],\n",
      "        [0.0179]], device='mps:0')\n",
      "Iteration 56350 Training loss 0.06031786650419235 Validation loss 0.05858645960688591 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.7508],\n",
      "        [0.6887]], device='mps:0')\n",
      "Iteration 56360 Training loss 0.05894281715154648 Validation loss 0.05863930657505989 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.1670],\n",
      "        [0.7588]], device='mps:0')\n",
      "Iteration 56370 Training loss 0.06280133128166199 Validation loss 0.05855246260762215 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.1097],\n",
      "        [0.2651]], device='mps:0')\n",
      "Iteration 56380 Training loss 0.04886731132864952 Validation loss 0.05854669585824013 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.6694],\n",
      "        [0.9237]], device='mps:0')\n",
      "Iteration 56390 Training loss 0.054291725158691406 Validation loss 0.05854650214314461 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9363],\n",
      "        [0.8803]], device='mps:0')\n",
      "Iteration 56400 Training loss 0.06270048767328262 Validation loss 0.05862443894147873 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.6418],\n",
      "        [0.7788]], device='mps:0')\n",
      "Iteration 56410 Training loss 0.061537180095911026 Validation loss 0.05863746255636215 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.8901],\n",
      "        [0.9489]], device='mps:0')\n",
      "Iteration 56420 Training loss 0.055985916405916214 Validation loss 0.0585763193666935 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.9447],\n",
      "        [0.9083]], device='mps:0')\n",
      "Iteration 56430 Training loss 0.06387898325920105 Validation loss 0.05853521078824997 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.2480],\n",
      "        [0.8821]], device='mps:0')\n",
      "Iteration 56440 Training loss 0.05425504222512245 Validation loss 0.058692678809165955 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.8183],\n",
      "        [0.9944]], device='mps:0')\n",
      "Iteration 56450 Training loss 0.05180371180176735 Validation loss 0.05860648304224014 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.8358],\n",
      "        [0.0027]], device='mps:0')\n",
      "Iteration 56460 Training loss 0.053590286523103714 Validation loss 0.05868886038661003 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9531],\n",
      "        [0.9655]], device='mps:0')\n",
      "Iteration 56470 Training loss 0.06104128807783127 Validation loss 0.05894164741039276 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.6844],\n",
      "        [0.1454]], device='mps:0')\n",
      "Iteration 56480 Training loss 0.05899856612086296 Validation loss 0.058557409793138504 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.7144],\n",
      "        [0.1628]], device='mps:0')\n",
      "Iteration 56490 Training loss 0.048213232308626175 Validation loss 0.05854949355125427 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.8008],\n",
      "        [0.5460]], device='mps:0')\n",
      "Iteration 56500 Training loss 0.05322857201099396 Validation loss 0.058528512716293335 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9668],\n",
      "        [0.9165]], device='mps:0')\n",
      "Iteration 56510 Training loss 0.05823639780282974 Validation loss 0.05852716788649559 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9934],\n",
      "        [0.9612]], device='mps:0')\n",
      "Iteration 56520 Training loss 0.06340555101633072 Validation loss 0.05857386067509651 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9463],\n",
      "        [0.8420]], device='mps:0')\n",
      "Iteration 56530 Training loss 0.052394069731235504 Validation loss 0.05855536833405495 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0634],\n",
      "        [0.9845]], device='mps:0')\n",
      "Iteration 56540 Training loss 0.06235504150390625 Validation loss 0.05853532627224922 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.5761],\n",
      "        [0.0194]], device='mps:0')\n",
      "Iteration 56550 Training loss 0.05114017426967621 Validation loss 0.05854921415448189 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.8951],\n",
      "        [0.9284]], device='mps:0')\n",
      "Iteration 56560 Training loss 0.059352558106184006 Validation loss 0.05893056094646454 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.0244],\n",
      "        [0.3861]], device='mps:0')\n",
      "Iteration 56570 Training loss 0.06371865421533585 Validation loss 0.058529481291770935 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.8977],\n",
      "        [0.0176]], device='mps:0')\n",
      "Iteration 56580 Training loss 0.055176347494125366 Validation loss 0.058532413095235825 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.7162],\n",
      "        [0.9779]], device='mps:0')\n",
      "Iteration 56590 Training loss 0.05553266033530235 Validation loss 0.05858008936047554 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0138],\n",
      "        [0.1804]], device='mps:0')\n",
      "Iteration 56600 Training loss 0.059915315359830856 Validation loss 0.058524470776319504 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0168],\n",
      "        [0.0564]], device='mps:0')\n",
      "Iteration 56610 Training loss 0.05838221311569214 Validation loss 0.05853591859340668 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.0668],\n",
      "        [0.9429]], device='mps:0')\n",
      "Iteration 56620 Training loss 0.053150009363889694 Validation loss 0.058542974293231964 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.0351],\n",
      "        [0.0731]], device='mps:0')\n",
      "Iteration 56630 Training loss 0.05321791395545006 Validation loss 0.05853034555912018 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.8924],\n",
      "        [0.1051]], device='mps:0')\n",
      "Iteration 56640 Training loss 0.05265368893742561 Validation loss 0.058526210486888885 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.9773],\n",
      "        [0.9691]], device='mps:0')\n",
      "Iteration 56650 Training loss 0.052590131759643555 Validation loss 0.05866727977991104 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.0189],\n",
      "        [0.9344]], device='mps:0')\n",
      "Iteration 56660 Training loss 0.0609963983297348 Validation loss 0.05852735415101051 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9757],\n",
      "        [0.3086]], device='mps:0')\n",
      "Iteration 56670 Training loss 0.053871478885412216 Validation loss 0.05857454985380173 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.4655],\n",
      "        [0.8205]], device='mps:0')\n",
      "Iteration 56680 Training loss 0.05585617944598198 Validation loss 0.05853933468461037 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.1701],\n",
      "        [0.9559]], device='mps:0')\n",
      "Iteration 56690 Training loss 0.048193998634815216 Validation loss 0.05859199911355972 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9258],\n",
      "        [0.0948]], device='mps:0')\n",
      "Iteration 56700 Training loss 0.05622909590601921 Validation loss 0.058554757386446 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.7443],\n",
      "        [0.0041]], device='mps:0')\n",
      "Iteration 56710 Training loss 0.0633622333407402 Validation loss 0.05853298306465149 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.2991],\n",
      "        [0.6232]], device='mps:0')\n",
      "Iteration 56720 Training loss 0.05303391441702843 Validation loss 0.05852791666984558 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.5369],\n",
      "        [0.0401]], device='mps:0')\n",
      "Iteration 56730 Training loss 0.054252319037914276 Validation loss 0.05853212997317314 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.5123],\n",
      "        [0.0382]], device='mps:0')\n",
      "Iteration 56740 Training loss 0.06038008630275726 Validation loss 0.05852953717112541 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9528],\n",
      "        [0.9415]], device='mps:0')\n",
      "Iteration 56750 Training loss 0.05293012037873268 Validation loss 0.058694180101156235 Accuracy 0.8391250371932983\n",
      "Output tensor([[0.2280],\n",
      "        [0.9296]], device='mps:0')\n",
      "Iteration 56760 Training loss 0.06096101552248001 Validation loss 0.05857808515429497 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.3032],\n",
      "        [0.9806]], device='mps:0')\n",
      "Iteration 56770 Training loss 0.053313035517930984 Validation loss 0.058621007949113846 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.2702],\n",
      "        [0.9848]], device='mps:0')\n",
      "Iteration 56780 Training loss 0.055113065987825394 Validation loss 0.058538768440485 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.7653],\n",
      "        [0.4302]], device='mps:0')\n",
      "Iteration 56790 Training loss 0.05673345550894737 Validation loss 0.058516744524240494 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.3842],\n",
      "        [0.0775]], device='mps:0')\n",
      "Iteration 56800 Training loss 0.057972367852926254 Validation loss 0.05860038101673126 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.0706],\n",
      "        [0.0623]], device='mps:0')\n",
      "Iteration 56810 Training loss 0.05037488788366318 Validation loss 0.05856750160455704 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.2407],\n",
      "        [0.0343]], device='mps:0')\n",
      "Iteration 56820 Training loss 0.047169607132673264 Validation loss 0.05852029100060463 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.4194],\n",
      "        [0.1738]], device='mps:0')\n",
      "Iteration 56830 Training loss 0.05087011680006981 Validation loss 0.058582864701747894 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.7169],\n",
      "        [0.0793]], device='mps:0')\n",
      "Iteration 56840 Training loss 0.05670047923922539 Validation loss 0.05852273106575012 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.0928],\n",
      "        [0.3367]], device='mps:0')\n",
      "Iteration 56850 Training loss 0.062373436987400055 Validation loss 0.05852106586098671 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.5007],\n",
      "        [0.0369]], device='mps:0')\n",
      "Iteration 56860 Training loss 0.05661860480904579 Validation loss 0.05864919722080231 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0931],\n",
      "        [0.9770]], device='mps:0')\n",
      "Iteration 56870 Training loss 0.05822189152240753 Validation loss 0.05865788459777832 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.0020],\n",
      "        [0.4358]], device='mps:0')\n",
      "Iteration 56880 Training loss 0.04979454725980759 Validation loss 0.05856024846434593 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0841],\n",
      "        [0.1760]], device='mps:0')\n",
      "Iteration 56890 Training loss 0.06470765918493271 Validation loss 0.05855695530772209 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.2140],\n",
      "        [0.0363]], device='mps:0')\n",
      "Iteration 56900 Training loss 0.05493444204330444 Validation loss 0.05852294713258743 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.1115],\n",
      "        [0.7421]], device='mps:0')\n",
      "Iteration 56910 Training loss 0.05594790354371071 Validation loss 0.05852651968598366 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.0301],\n",
      "        [0.7266]], device='mps:0')\n",
      "Iteration 56920 Training loss 0.06279929727315903 Validation loss 0.058655764907598495 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.4968],\n",
      "        [0.6932]], device='mps:0')\n",
      "Iteration 56930 Training loss 0.05310625955462456 Validation loss 0.058602213859558105 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0834],\n",
      "        [0.2032]], device='mps:0')\n",
      "Iteration 56940 Training loss 0.042512331157922745 Validation loss 0.058594003319740295 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.5325],\n",
      "        [0.2370]], device='mps:0')\n",
      "Iteration 56950 Training loss 0.058582935482263565 Validation loss 0.05851360782980919 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.7294],\n",
      "        [0.7093]], device='mps:0')\n",
      "Iteration 56960 Training loss 0.0491987019777298 Validation loss 0.05854637548327446 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.9639],\n",
      "        [0.9980]], device='mps:0')\n",
      "Iteration 56970 Training loss 0.053056687116622925 Validation loss 0.05850900709629059 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.0224],\n",
      "        [0.8596]], device='mps:0')\n",
      "Iteration 56980 Training loss 0.05181156471371651 Validation loss 0.05853461101651192 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.0019],\n",
      "        [0.6241]], device='mps:0')\n",
      "Iteration 56990 Training loss 0.055084228515625 Validation loss 0.058781154453754425 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.2664],\n",
      "        [0.9730]], device='mps:0')\n",
      "Iteration 57000 Training loss 0.05274708941578865 Validation loss 0.05854339152574539 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.1740],\n",
      "        [0.9439]], device='mps:0')\n",
      "Iteration 57010 Training loss 0.05123897269368172 Validation loss 0.05868636816740036 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.3563],\n",
      "        [0.0255]], device='mps:0')\n",
      "Iteration 57020 Training loss 0.048687439411878586 Validation loss 0.058561716228723526 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.2301],\n",
      "        [0.1547]], device='mps:0')\n",
      "Iteration 57030 Training loss 0.047415152192115784 Validation loss 0.05862705409526825 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.9262],\n",
      "        [0.7758]], device='mps:0')\n",
      "Iteration 57040 Training loss 0.05074475333094597 Validation loss 0.058526381850242615 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9727],\n",
      "        [0.0157]], device='mps:0')\n",
      "Iteration 57050 Training loss 0.05248937010765076 Validation loss 0.05866795778274536 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.0881],\n",
      "        [0.9117]], device='mps:0')\n",
      "Iteration 57060 Training loss 0.04887811467051506 Validation loss 0.058500949293375015 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9967],\n",
      "        [0.2303]], device='mps:0')\n",
      "Iteration 57070 Training loss 0.05355771258473396 Validation loss 0.058697473257780075 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.4845],\n",
      "        [0.9892]], device='mps:0')\n",
      "Iteration 57080 Training loss 0.056186482310295105 Validation loss 0.05888919532299042 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0940],\n",
      "        [0.9759]], device='mps:0')\n",
      "Iteration 57090 Training loss 0.04753429442644119 Validation loss 0.05852849781513214 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.2821],\n",
      "        [0.0646]], device='mps:0')\n",
      "Iteration 57100 Training loss 0.05705881863832474 Validation loss 0.058502282947301865 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.8525],\n",
      "        [0.9208]], device='mps:0')\n",
      "Iteration 57110 Training loss 0.054422348737716675 Validation loss 0.058488450944423676 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.8029],\n",
      "        [0.8020]], device='mps:0')\n",
      "Iteration 57120 Training loss 0.05628377944231033 Validation loss 0.05861523002386093 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.0199],\n",
      "        [0.0055]], device='mps:0')\n",
      "Iteration 57130 Training loss 0.05930633842945099 Validation loss 0.05849481374025345 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.2367],\n",
      "        [0.9688]], device='mps:0')\n",
      "Iteration 57140 Training loss 0.061407268047332764 Validation loss 0.05851822718977928 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.3051],\n",
      "        [0.2972]], device='mps:0')\n",
      "Iteration 57150 Training loss 0.06113434582948685 Validation loss 0.058493468910455704 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0869],\n",
      "        [0.9634]], device='mps:0')\n",
      "Iteration 57160 Training loss 0.05411943420767784 Validation loss 0.05864604562520981 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.2660],\n",
      "        [0.3549]], device='mps:0')\n",
      "Iteration 57170 Training loss 0.059418030083179474 Validation loss 0.05853626877069473 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.2511],\n",
      "        [0.8721]], device='mps:0')\n",
      "Iteration 57180 Training loss 0.057126544415950775 Validation loss 0.05858507379889488 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0101],\n",
      "        [0.0804]], device='mps:0')\n",
      "Iteration 57190 Training loss 0.05900253728032112 Validation loss 0.0586567260324955 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9300],\n",
      "        [0.9813]], device='mps:0')\n",
      "Iteration 57200 Training loss 0.054882898926734924 Validation loss 0.05858379602432251 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.0675],\n",
      "        [0.3800]], device='mps:0')\n",
      "Iteration 57210 Training loss 0.057594697922468185 Validation loss 0.058496806770563126 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0920],\n",
      "        [0.7113]], device='mps:0')\n",
      "Iteration 57220 Training loss 0.054697878658771515 Validation loss 0.05849645659327507 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0611],\n",
      "        [0.9272]], device='mps:0')\n",
      "Iteration 57230 Training loss 0.05398040637373924 Validation loss 0.058505214750766754 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.3841],\n",
      "        [0.0635]], device='mps:0')\n",
      "Iteration 57240 Training loss 0.05911976844072342 Validation loss 0.05849163979291916 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.3917],\n",
      "        [0.2651]], device='mps:0')\n",
      "Iteration 57250 Training loss 0.05517764016985893 Validation loss 0.058500535786151886 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.3808],\n",
      "        [0.8399]], device='mps:0')\n",
      "Iteration 57260 Training loss 0.04729604721069336 Validation loss 0.05858718231320381 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.4591],\n",
      "        [0.9938]], device='mps:0')\n",
      "Iteration 57270 Training loss 0.060600195080041885 Validation loss 0.058499887585639954 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.0083],\n",
      "        [0.1599]], device='mps:0')\n",
      "Iteration 57280 Training loss 0.06559727340936661 Validation loss 0.058580853044986725 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.9863],\n",
      "        [0.9625]], device='mps:0')\n",
      "Iteration 57290 Training loss 0.06517081707715988 Validation loss 0.05862005054950714 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.9462],\n",
      "        [0.9360]], device='mps:0')\n",
      "Iteration 57300 Training loss 0.0561470203101635 Validation loss 0.05858805775642395 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.1550],\n",
      "        [0.9694]], device='mps:0')\n",
      "Iteration 57310 Training loss 0.051816292107105255 Validation loss 0.05851724371314049 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.2848],\n",
      "        [0.0326]], device='mps:0')\n",
      "Iteration 57320 Training loss 0.04456872493028641 Validation loss 0.058485835790634155 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.7491],\n",
      "        [0.8254]], device='mps:0')\n",
      "Iteration 57330 Training loss 0.05546778067946434 Validation loss 0.058481570333242416 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.6311],\n",
      "        [0.0478]], device='mps:0')\n",
      "Iteration 57340 Training loss 0.05918806791305542 Validation loss 0.058658450841903687 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9921],\n",
      "        [0.0136]], device='mps:0')\n",
      "Iteration 57350 Training loss 0.05723036453127861 Validation loss 0.05847964808344841 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0619],\n",
      "        [0.6394]], device='mps:0')\n",
      "Iteration 57360 Training loss 0.05410079285502434 Validation loss 0.058526672422885895 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.8130],\n",
      "        [0.9828]], device='mps:0')\n",
      "Iteration 57370 Training loss 0.05744807422161102 Validation loss 0.05847664177417755 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.5074],\n",
      "        [0.0357]], device='mps:0')\n",
      "Iteration 57380 Training loss 0.051304206252098083 Validation loss 0.05848175287246704 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.2796],\n",
      "        [0.0283]], device='mps:0')\n",
      "Iteration 57390 Training loss 0.05859576538205147 Validation loss 0.05859017372131348 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.0660],\n",
      "        [0.1171]], device='mps:0')\n",
      "Iteration 57400 Training loss 0.04942389950156212 Validation loss 0.05847647786140442 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.6443],\n",
      "        [0.7575]], device='mps:0')\n",
      "Iteration 57410 Training loss 0.05076257884502411 Validation loss 0.05846920609474182 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0725],\n",
      "        [0.0103]], device='mps:0')\n",
      "Iteration 57420 Training loss 0.05562792345881462 Validation loss 0.0584816113114357 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0223],\n",
      "        [0.1462]], device='mps:0')\n",
      "Iteration 57430 Training loss 0.06192236766219139 Validation loss 0.05847860872745514 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9669],\n",
      "        [0.1256]], device='mps:0')\n",
      "Iteration 57440 Training loss 0.06002776324748993 Validation loss 0.058470021933317184 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.0481],\n",
      "        [0.9688]], device='mps:0')\n",
      "Iteration 57450 Training loss 0.06148970499634743 Validation loss 0.05856716260313988 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9696],\n",
      "        [0.1269]], device='mps:0')\n",
      "Iteration 57460 Training loss 0.0592786967754364 Validation loss 0.058470044285058975 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0688],\n",
      "        [0.9762]], device='mps:0')\n",
      "Iteration 57470 Training loss 0.05576838552951813 Validation loss 0.05860396474599838 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.4159],\n",
      "        [0.0424]], device='mps:0')\n",
      "Iteration 57480 Training loss 0.05002809315919876 Validation loss 0.058470550924539566 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9001],\n",
      "        [0.0989]], device='mps:0')\n",
      "Iteration 57490 Training loss 0.05732686445116997 Validation loss 0.05857349932193756 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.8045],\n",
      "        [0.1615]], device='mps:0')\n",
      "Iteration 57500 Training loss 0.059625934809446335 Validation loss 0.05846690014004707 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.7722],\n",
      "        [0.6187]], device='mps:0')\n",
      "Iteration 57510 Training loss 0.05427868664264679 Validation loss 0.05849334970116615 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.8750],\n",
      "        [0.6716]], device='mps:0')\n",
      "Iteration 57520 Training loss 0.05171190947294235 Validation loss 0.05851508677005768 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.5583],\n",
      "        [0.3383]], device='mps:0')\n",
      "Iteration 57530 Training loss 0.05566848814487457 Validation loss 0.058467354625463486 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.6632],\n",
      "        [0.1513]], device='mps:0')\n",
      "Iteration 57540 Training loss 0.05054477974772453 Validation loss 0.05848808214068413 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.4081],\n",
      "        [0.7224]], device='mps:0')\n",
      "Iteration 57550 Training loss 0.05416247248649597 Validation loss 0.05851905420422554 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.2132],\n",
      "        [0.2520]], device='mps:0')\n",
      "Iteration 57560 Training loss 0.0575188510119915 Validation loss 0.058458950370550156 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.7393],\n",
      "        [0.9652]], device='mps:0')\n",
      "Iteration 57570 Training loss 0.06251221150159836 Validation loss 0.05848824232816696 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.8328],\n",
      "        [0.0651]], device='mps:0')\n",
      "Iteration 57580 Training loss 0.06044147536158562 Validation loss 0.05846187099814415 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.3212],\n",
      "        [0.2189]], device='mps:0')\n",
      "Iteration 57590 Training loss 0.05067978799343109 Validation loss 0.05859524756669998 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.0316],\n",
      "        [0.0338]], device='mps:0')\n",
      "Iteration 57600 Training loss 0.06320928782224655 Validation loss 0.058487579226493835 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.8016],\n",
      "        [0.7153]], device='mps:0')\n",
      "Iteration 57610 Training loss 0.06052132323384285 Validation loss 0.05848320201039314 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.8849],\n",
      "        [0.1907]], device='mps:0')\n",
      "Iteration 57620 Training loss 0.051226262003183365 Validation loss 0.05848677456378937 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.5417],\n",
      "        [0.9167]], device='mps:0')\n",
      "Iteration 57630 Training loss 0.05262303352355957 Validation loss 0.05845000594854355 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.5971],\n",
      "        [0.3826]], device='mps:0')\n",
      "Iteration 57640 Training loss 0.05446884036064148 Validation loss 0.058513741940259933 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.1251],\n",
      "        [0.8949]], device='mps:0')\n",
      "Iteration 57650 Training loss 0.05996893718838692 Validation loss 0.058459095656871796 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9950],\n",
      "        [0.9900]], device='mps:0')\n",
      "Iteration 57660 Training loss 0.06149166077375412 Validation loss 0.05845111235976219 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.5427],\n",
      "        [0.4703]], device='mps:0')\n",
      "Iteration 57670 Training loss 0.05933390557765961 Validation loss 0.05847726762294769 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.8667],\n",
      "        [0.6475]], device='mps:0')\n",
      "Iteration 57680 Training loss 0.059695567935705185 Validation loss 0.05847357213497162 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.8242],\n",
      "        [0.1119]], device='mps:0')\n",
      "Iteration 57690 Training loss 0.053847379982471466 Validation loss 0.05847381800413132 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.1318],\n",
      "        [0.8878]], device='mps:0')\n",
      "Iteration 57700 Training loss 0.054369643330574036 Validation loss 0.058517176657915115 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.8426],\n",
      "        [0.0016]], device='mps:0')\n",
      "Iteration 57710 Training loss 0.052885815501213074 Validation loss 0.058461952954530716 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.1978],\n",
      "        [0.1973]], device='mps:0')\n",
      "Iteration 57720 Training loss 0.05579150468111038 Validation loss 0.05844900757074356 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0322],\n",
      "        [0.9136]], device='mps:0')\n",
      "Iteration 57730 Training loss 0.05428006500005722 Validation loss 0.0584457702934742 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.1327],\n",
      "        [0.2761]], device='mps:0')\n",
      "Iteration 57740 Training loss 0.05954885482788086 Validation loss 0.058479007333517075 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.0647],\n",
      "        [0.6255]], device='mps:0')\n",
      "Iteration 57750 Training loss 0.05495475232601166 Validation loss 0.0585462711751461 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9531],\n",
      "        [0.0317]], device='mps:0')\n",
      "Iteration 57760 Training loss 0.05905243754386902 Validation loss 0.058483436703681946 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.9602],\n",
      "        [0.7304]], device='mps:0')\n",
      "Iteration 57770 Training loss 0.05520050972700119 Validation loss 0.05846431106328964 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9106],\n",
      "        [0.1929]], device='mps:0')\n",
      "Iteration 57780 Training loss 0.06027922034263611 Validation loss 0.058708857744932175 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.8975],\n",
      "        [0.3184]], device='mps:0')\n",
      "Iteration 57790 Training loss 0.05648774653673172 Validation loss 0.058491554111242294 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.1164],\n",
      "        [0.0288]], device='mps:0')\n",
      "Iteration 57800 Training loss 0.05614921450614929 Validation loss 0.05855269730091095 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.3432],\n",
      "        [0.7819]], device='mps:0')\n",
      "Iteration 57810 Training loss 0.05507178232073784 Validation loss 0.05845460668206215 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.8591],\n",
      "        [0.9878]], device='mps:0')\n",
      "Iteration 57820 Training loss 0.0622447207570076 Validation loss 0.05853661522269249 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.4125],\n",
      "        [0.1438]], device='mps:0')\n",
      "Iteration 57830 Training loss 0.05492649972438812 Validation loss 0.05855436250567436 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.4285],\n",
      "        [0.9876]], device='mps:0')\n",
      "Iteration 57840 Training loss 0.05837602540850639 Validation loss 0.05848359689116478 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.2684],\n",
      "        [0.8339]], device='mps:0')\n",
      "Iteration 57850 Training loss 0.05846269428730011 Validation loss 0.05846519023180008 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.7150],\n",
      "        [0.9543]], device='mps:0')\n",
      "Iteration 57860 Training loss 0.05100264027714729 Validation loss 0.058571137487888336 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.3775],\n",
      "        [0.6514]], device='mps:0')\n",
      "Iteration 57870 Training loss 0.04244164004921913 Validation loss 0.05844081938266754 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9789],\n",
      "        [0.6995]], device='mps:0')\n",
      "Iteration 57880 Training loss 0.05120333656668663 Validation loss 0.05848093703389168 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0546],\n",
      "        [0.8728]], device='mps:0')\n",
      "Iteration 57890 Training loss 0.05422190576791763 Validation loss 0.05849582329392433 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.0500],\n",
      "        [0.9767]], device='mps:0')\n",
      "Iteration 57900 Training loss 0.058180999010801315 Validation loss 0.058438319712877274 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.3423],\n",
      "        [0.2557]], device='mps:0')\n",
      "Iteration 57910 Training loss 0.06398545205593109 Validation loss 0.05845712497830391 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9087],\n",
      "        [0.9215]], device='mps:0')\n",
      "Iteration 57920 Training loss 0.06285735219717026 Validation loss 0.05843508243560791 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.4912],\n",
      "        [0.1675]], device='mps:0')\n",
      "Iteration 57930 Training loss 0.05771128460764885 Validation loss 0.058463096618652344 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.8928],\n",
      "        [0.0386]], device='mps:0')\n",
      "Iteration 57940 Training loss 0.05425722151994705 Validation loss 0.058432795107364655 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9675],\n",
      "        [0.8570]], device='mps:0')\n",
      "Iteration 57950 Training loss 0.050838831812143326 Validation loss 0.05842980369925499 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.4395],\n",
      "        [0.0094]], device='mps:0')\n",
      "Iteration 57960 Training loss 0.052573807537555695 Validation loss 0.0584314689040184 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.4662],\n",
      "        [0.7213]], device='mps:0')\n",
      "Iteration 57970 Training loss 0.05833938345313072 Validation loss 0.05844239518046379 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9008],\n",
      "        [0.3548]], device='mps:0')\n",
      "Iteration 57980 Training loss 0.05349743738770485 Validation loss 0.05855784937739372 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.9383],\n",
      "        [0.7999]], device='mps:0')\n",
      "Iteration 57990 Training loss 0.05719786137342453 Validation loss 0.05845075845718384 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.9279],\n",
      "        [0.0219]], device='mps:0')\n",
      "Iteration 58000 Training loss 0.062055230140686035 Validation loss 0.058466944843530655 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.0191],\n",
      "        [0.4315]], device='mps:0')\n",
      "Iteration 58010 Training loss 0.052126310765743256 Validation loss 0.05850648880004883 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.0232],\n",
      "        [0.5614]], device='mps:0')\n",
      "Iteration 58020 Training loss 0.05927910655736923 Validation loss 0.058492716401815414 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.8036],\n",
      "        [0.1956]], device='mps:0')\n",
      "Iteration 58030 Training loss 0.058193836361169815 Validation loss 0.05845944210886955 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.6510],\n",
      "        [0.9891]], device='mps:0')\n",
      "Iteration 58040 Training loss 0.05065527185797691 Validation loss 0.0584757998585701 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.6578],\n",
      "        [0.0493]], device='mps:0')\n",
      "Iteration 58050 Training loss 0.05835545435547829 Validation loss 0.058565568178892136 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.7409],\n",
      "        [0.1132]], device='mps:0')\n",
      "Iteration 58060 Training loss 0.06198936328291893 Validation loss 0.058470699936151505 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.1471],\n",
      "        [0.8837]], device='mps:0')\n",
      "Iteration 58070 Training loss 0.061174046248197556 Validation loss 0.058515604585409164 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0998],\n",
      "        [0.1191]], device='mps:0')\n",
      "Iteration 58080 Training loss 0.05682201683521271 Validation loss 0.05847126245498657 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.1071],\n",
      "        [0.2595]], device='mps:0')\n",
      "Iteration 58090 Training loss 0.05713612586259842 Validation loss 0.058487214148044586 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.6366],\n",
      "        [0.1247]], device='mps:0')\n",
      "Iteration 58100 Training loss 0.062440626323223114 Validation loss 0.05844024941325188 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.7347],\n",
      "        [0.7465]], device='mps:0')\n",
      "Iteration 58110 Training loss 0.059007056057453156 Validation loss 0.058446720242500305 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.2286],\n",
      "        [0.9535]], device='mps:0')\n",
      "Iteration 58120 Training loss 0.051924657076597214 Validation loss 0.05858040973544121 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.9907],\n",
      "        [0.8353]], device='mps:0')\n",
      "Iteration 58130 Training loss 0.049233607947826385 Validation loss 0.05846114456653595 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.2137],\n",
      "        [0.2011]], device='mps:0')\n",
      "Iteration 58140 Training loss 0.05475752800703049 Validation loss 0.05848851427435875 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.8273],\n",
      "        [0.3817]], device='mps:0')\n",
      "Iteration 58150 Training loss 0.05660010874271393 Validation loss 0.058577071875333786 Accuracy 0.8393750190734863\n",
      "Output tensor([[0.1302],\n",
      "        [0.0807]], device='mps:0')\n",
      "Iteration 58160 Training loss 0.05882170423865318 Validation loss 0.05849967524409294 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.6453],\n",
      "        [0.0387]], device='mps:0')\n",
      "Iteration 58170 Training loss 0.057856254279613495 Validation loss 0.05844704434275627 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.6809],\n",
      "        [0.8234]], device='mps:0')\n",
      "Iteration 58180 Training loss 0.05795080214738846 Validation loss 0.05846492201089859 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9867],\n",
      "        [0.0123]], device='mps:0')\n",
      "Iteration 58190 Training loss 0.04809953644871712 Validation loss 0.058558933436870575 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.7201],\n",
      "        [0.0909]], device='mps:0')\n",
      "Iteration 58200 Training loss 0.05436204746365547 Validation loss 0.058456048369407654 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9606],\n",
      "        [0.3074]], device='mps:0')\n",
      "Iteration 58210 Training loss 0.0497334785759449 Validation loss 0.058440759778022766 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0543],\n",
      "        [0.0370]], device='mps:0')\n",
      "Iteration 58220 Training loss 0.06227825954556465 Validation loss 0.05855514481663704 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.7463],\n",
      "        [0.5692]], device='mps:0')\n",
      "Iteration 58230 Training loss 0.07053117454051971 Validation loss 0.058483440428972244 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.8462],\n",
      "        [0.7054]], device='mps:0')\n",
      "Iteration 58240 Training loss 0.0663091391324997 Validation loss 0.058573681861162186 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.9217],\n",
      "        [0.2114]], device='mps:0')\n",
      "Iteration 58250 Training loss 0.04916905239224434 Validation loss 0.058442119508981705 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.2747],\n",
      "        [0.0021]], device='mps:0')\n",
      "Iteration 58260 Training loss 0.06070934981107712 Validation loss 0.05848099663853645 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.1932],\n",
      "        [0.2997]], device='mps:0')\n",
      "Iteration 58270 Training loss 0.05341535434126854 Validation loss 0.05844483897089958 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.7504],\n",
      "        [0.8239]], device='mps:0')\n",
      "Iteration 58280 Training loss 0.055687855929136276 Validation loss 0.05845231935381889 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.4154],\n",
      "        [0.9221]], device='mps:0')\n",
      "Iteration 58290 Training loss 0.04718334227800369 Validation loss 0.05843213573098183 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.1033],\n",
      "        [0.3396]], device='mps:0')\n",
      "Iteration 58300 Training loss 0.05180373415350914 Validation loss 0.05855175852775574 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.0424],\n",
      "        [0.1546]], device='mps:0')\n",
      "Iteration 58310 Training loss 0.05470889061689377 Validation loss 0.05844037979841232 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.1478],\n",
      "        [0.4332]], device='mps:0')\n",
      "Iteration 58320 Training loss 0.06407642364501953 Validation loss 0.05850619450211525 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.6321],\n",
      "        [0.6233]], device='mps:0')\n",
      "Iteration 58330 Training loss 0.05273386463522911 Validation loss 0.05845904350280762 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.0570],\n",
      "        [0.1386]], device='mps:0')\n",
      "Iteration 58340 Training loss 0.064787358045578 Validation loss 0.05843232944607735 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.1063],\n",
      "        [0.9318]], device='mps:0')\n",
      "Iteration 58350 Training loss 0.0551142692565918 Validation loss 0.05843328312039375 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.1066],\n",
      "        [0.1941]], device='mps:0')\n",
      "Iteration 58360 Training loss 0.052099306136369705 Validation loss 0.05843373015522957 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9654],\n",
      "        [0.3386]], device='mps:0')\n",
      "Iteration 58370 Training loss 0.04775296896696091 Validation loss 0.05846205726265907 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.0467],\n",
      "        [0.9530]], device='mps:0')\n",
      "Iteration 58380 Training loss 0.056792113929986954 Validation loss 0.058448877185583115 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.9793],\n",
      "        [0.7607]], device='mps:0')\n",
      "Iteration 58390 Training loss 0.06932514160871506 Validation loss 0.05846444144845009 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9800],\n",
      "        [0.9104]], device='mps:0')\n",
      "Iteration 58400 Training loss 0.058952897787094116 Validation loss 0.05846051499247551 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.9196],\n",
      "        [0.9833]], device='mps:0')\n",
      "Iteration 58410 Training loss 0.05959434434771538 Validation loss 0.05846400931477547 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.2410],\n",
      "        [0.1635]], device='mps:0')\n",
      "Iteration 58420 Training loss 0.05568864569067955 Validation loss 0.05863049626350403 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.2512],\n",
      "        [0.8690]], device='mps:0')\n",
      "Iteration 58430 Training loss 0.06274561583995819 Validation loss 0.05842595547437668 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.0560],\n",
      "        [0.8769]], device='mps:0')\n",
      "Iteration 58440 Training loss 0.05172581598162651 Validation loss 0.05843077227473259 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.0716],\n",
      "        [0.0947]], device='mps:0')\n",
      "Iteration 58450 Training loss 0.054869744926691055 Validation loss 0.05849607661366463 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.1356],\n",
      "        [0.1819]], device='mps:0')\n",
      "Iteration 58460 Training loss 0.05274094641208649 Validation loss 0.05849626287817955 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.5364],\n",
      "        [0.0992]], device='mps:0')\n",
      "Iteration 58470 Training loss 0.05731458216905594 Validation loss 0.05847087502479553 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.4640],\n",
      "        [0.6022]], device='mps:0')\n",
      "Iteration 58480 Training loss 0.0489589124917984 Validation loss 0.058444008231163025 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.6884],\n",
      "        [0.0211]], device='mps:0')\n",
      "Iteration 58490 Training loss 0.05979036167263985 Validation loss 0.05843697488307953 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.8345],\n",
      "        [0.3030]], device='mps:0')\n",
      "Iteration 58500 Training loss 0.05772770568728447 Validation loss 0.058460261672735214 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.9784],\n",
      "        [0.7610]], device='mps:0')\n",
      "Iteration 58510 Training loss 0.052283015102148056 Validation loss 0.05845427140593529 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.9693],\n",
      "        [0.1476]], device='mps:0')\n",
      "Iteration 58520 Training loss 0.05352022126317024 Validation loss 0.05844037979841232 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0769],\n",
      "        [0.0640]], device='mps:0')\n",
      "Iteration 58530 Training loss 0.058852795511484146 Validation loss 0.05845431610941887 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.1910],\n",
      "        [0.8922]], device='mps:0')\n",
      "Iteration 58540 Training loss 0.059682101011276245 Validation loss 0.05844377353787422 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.6628],\n",
      "        [0.9359]], device='mps:0')\n",
      "Iteration 58550 Training loss 0.05902814865112305 Validation loss 0.05849134176969528 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9656],\n",
      "        [0.5192]], device='mps:0')\n",
      "Iteration 58560 Training loss 0.05866868421435356 Validation loss 0.05843571200966835 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.5639],\n",
      "        [0.1547]], device='mps:0')\n",
      "Iteration 58570 Training loss 0.06525001674890518 Validation loss 0.05845518037676811 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.9880],\n",
      "        [0.5625]], device='mps:0')\n",
      "Iteration 58580 Training loss 0.05896264314651489 Validation loss 0.058518435806035995 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.1235],\n",
      "        [0.7642]], device='mps:0')\n",
      "Iteration 58590 Training loss 0.06519417464733124 Validation loss 0.05844200402498245 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.8358],\n",
      "        [0.0119]], device='mps:0')\n",
      "Iteration 58600 Training loss 0.050171785056591034 Validation loss 0.058520495891571045 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0878],\n",
      "        [0.2236]], device='mps:0')\n",
      "Iteration 58610 Training loss 0.060149502009153366 Validation loss 0.058436281979084015 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.9490],\n",
      "        [0.0898]], device='mps:0')\n",
      "Iteration 58620 Training loss 0.05845269188284874 Validation loss 0.058508288115262985 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.8665],\n",
      "        [0.6890]], device='mps:0')\n",
      "Iteration 58630 Training loss 0.05256552994251251 Validation loss 0.058612655848264694 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.1191],\n",
      "        [0.8870]], device='mps:0')\n",
      "Iteration 58640 Training loss 0.06002805009484291 Validation loss 0.05857497826218605 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.6645],\n",
      "        [0.0435]], device='mps:0')\n",
      "Iteration 58650 Training loss 0.06199178472161293 Validation loss 0.058551087975502014 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.5748],\n",
      "        [0.2339]], device='mps:0')\n",
      "Iteration 58660 Training loss 0.04768811911344528 Validation loss 0.058438099920749664 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.2099],\n",
      "        [0.9153]], device='mps:0')\n",
      "Iteration 58670 Training loss 0.04966702312231064 Validation loss 0.05860688164830208 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.0377],\n",
      "        [0.0975]], device='mps:0')\n",
      "Iteration 58680 Training loss 0.057418014854192734 Validation loss 0.058461688458919525 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.4814],\n",
      "        [0.2229]], device='mps:0')\n",
      "Iteration 58690 Training loss 0.05900004133582115 Validation loss 0.05845881626009941 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.5978],\n",
      "        [0.7653]], device='mps:0')\n",
      "Iteration 58700 Training loss 0.059189148247241974 Validation loss 0.05845194682478905 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.0319],\n",
      "        [0.1778]], device='mps:0')\n",
      "Iteration 58710 Training loss 0.051509179174900055 Validation loss 0.05856775864958763 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.8748],\n",
      "        [0.6462]], device='mps:0')\n",
      "Iteration 58720 Training loss 0.05313687026500702 Validation loss 0.05863271653652191 Accuracy 0.8381250500679016\n",
      "Output tensor([[0.1885],\n",
      "        [0.4623]], device='mps:0')\n",
      "Iteration 58730 Training loss 0.05276118218898773 Validation loss 0.05842050537467003 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9698],\n",
      "        [0.4605]], device='mps:0')\n",
      "Iteration 58740 Training loss 0.04978271946310997 Validation loss 0.058419130742549896 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.3958],\n",
      "        [0.8385]], device='mps:0')\n",
      "Iteration 58750 Training loss 0.056178487837314606 Validation loss 0.05844251438975334 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0926],\n",
      "        [0.9294]], device='mps:0')\n",
      "Iteration 58760 Training loss 0.058224644511938095 Validation loss 0.058460917323827744 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.2889],\n",
      "        [0.0702]], device='mps:0')\n",
      "Iteration 58770 Training loss 0.04894823953509331 Validation loss 0.05882894620299339 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.0204],\n",
      "        [0.8359]], device='mps:0')\n",
      "Iteration 58780 Training loss 0.057157985866069794 Validation loss 0.05857669562101364 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.8730],\n",
      "        [0.9182]], device='mps:0')\n",
      "Iteration 58790 Training loss 0.058345623314380646 Validation loss 0.0586618073284626 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.8995],\n",
      "        [0.9942]], device='mps:0')\n",
      "Iteration 58800 Training loss 0.05437573418021202 Validation loss 0.05852629989385605 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.3140],\n",
      "        [0.1305]], device='mps:0')\n",
      "Iteration 58810 Training loss 0.05097826197743416 Validation loss 0.058518365025520325 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0849],\n",
      "        [0.9380]], device='mps:0')\n",
      "Iteration 58820 Training loss 0.0617792010307312 Validation loss 0.05841859430074692 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.8713],\n",
      "        [0.3655]], device='mps:0')\n",
      "Iteration 58830 Training loss 0.049307312816381454 Validation loss 0.05846905708312988 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.9923],\n",
      "        [0.9840]], device='mps:0')\n",
      "Iteration 58840 Training loss 0.07028727233409882 Validation loss 0.058483753353357315 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.3354],\n",
      "        [0.1422]], device='mps:0')\n",
      "Iteration 58850 Training loss 0.061447903513908386 Validation loss 0.05843096971511841 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.0825],\n",
      "        [0.4242]], device='mps:0')\n",
      "Iteration 58860 Training loss 0.05370515584945679 Validation loss 0.05843086540699005 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.2389],\n",
      "        [0.9954]], device='mps:0')\n",
      "Iteration 58870 Training loss 0.06074238568544388 Validation loss 0.058448292315006256 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.6045],\n",
      "        [0.2124]], device='mps:0')\n",
      "Iteration 58880 Training loss 0.05648509040474892 Validation loss 0.05852159485220909 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.9760],\n",
      "        [0.4154]], device='mps:0')\n",
      "Iteration 58890 Training loss 0.05095567926764488 Validation loss 0.058457307517528534 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.9007],\n",
      "        [0.9324]], device='mps:0')\n",
      "Iteration 58900 Training loss 0.05032835528254509 Validation loss 0.05848044902086258 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.7855],\n",
      "        [0.0886]], device='mps:0')\n",
      "Iteration 58910 Training loss 0.06358503550291061 Validation loss 0.05847221985459328 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.1066],\n",
      "        [0.7602]], device='mps:0')\n",
      "Iteration 58920 Training loss 0.057344950735569 Validation loss 0.05842793732881546 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9099],\n",
      "        [0.9818]], device='mps:0')\n",
      "Iteration 58930 Training loss 0.05395893007516861 Validation loss 0.0584203265607357 Accuracy 0.842875063419342\n",
      "Output tensor([[0.6770],\n",
      "        [0.8580]], device='mps:0')\n",
      "Iteration 58940 Training loss 0.051547516137361526 Validation loss 0.05842960625886917 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.0566],\n",
      "        [0.0982]], device='mps:0')\n",
      "Iteration 58950 Training loss 0.05506370589137077 Validation loss 0.058513715863227844 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.5191],\n",
      "        [0.4394]], device='mps:0')\n",
      "Iteration 58960 Training loss 0.06106483191251755 Validation loss 0.0584443137049675 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.0560],\n",
      "        [0.0022]], device='mps:0')\n",
      "Iteration 58970 Training loss 0.05138071998953819 Validation loss 0.058450039476156235 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.1008],\n",
      "        [0.0102]], device='mps:0')\n",
      "Iteration 58980 Training loss 0.05330974981188774 Validation loss 0.058418307453393936 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.5040],\n",
      "        [0.0242]], device='mps:0')\n",
      "Iteration 58990 Training loss 0.04805021733045578 Validation loss 0.05842394381761551 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.8590],\n",
      "        [0.0466]], device='mps:0')\n",
      "Iteration 59000 Training loss 0.05411507561802864 Validation loss 0.0584254115819931 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.0433],\n",
      "        [0.1014]], device='mps:0')\n",
      "Iteration 59010 Training loss 0.0556233786046505 Validation loss 0.05843885987997055 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.1765],\n",
      "        [0.2452]], device='mps:0')\n",
      "Iteration 59020 Training loss 0.052955422550439835 Validation loss 0.05842074751853943 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.8945],\n",
      "        [0.7636]], device='mps:0')\n",
      "Iteration 59030 Training loss 0.053463637828826904 Validation loss 0.05842093750834465 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.2331],\n",
      "        [0.9245]], device='mps:0')\n",
      "Iteration 59040 Training loss 0.05589338764548302 Validation loss 0.058442242443561554 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.3350],\n",
      "        [0.5024]], device='mps:0')\n",
      "Iteration 59050 Training loss 0.04968559741973877 Validation loss 0.05841314047574997 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9305],\n",
      "        [0.9734]], device='mps:0')\n",
      "Iteration 59060 Training loss 0.05521130934357643 Validation loss 0.058711033314466476 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.8668],\n",
      "        [0.2388]], device='mps:0')\n",
      "Iteration 59070 Training loss 0.05430944636464119 Validation loss 0.05851045995950699 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.9495],\n",
      "        [0.0425]], device='mps:0')\n",
      "Iteration 59080 Training loss 0.04887274652719498 Validation loss 0.05851598456501961 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.4038],\n",
      "        [0.8284]], device='mps:0')\n",
      "Iteration 59090 Training loss 0.05589728057384491 Validation loss 0.058480314910411835 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.6058],\n",
      "        [0.2961]], device='mps:0')\n",
      "Iteration 59100 Training loss 0.05513971671462059 Validation loss 0.05841895937919617 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0425],\n",
      "        [0.3315]], device='mps:0')\n",
      "Iteration 59110 Training loss 0.052183669060468674 Validation loss 0.05842325836420059 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1095],\n",
      "        [0.9396]], device='mps:0')\n",
      "Iteration 59120 Training loss 0.05820192024111748 Validation loss 0.05845862254500389 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.7693],\n",
      "        [0.9243]], device='mps:0')\n",
      "Iteration 59130 Training loss 0.0631856769323349 Validation loss 0.058423299342393875 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.6702],\n",
      "        [0.8373]], device='mps:0')\n",
      "Iteration 59140 Training loss 0.05346699059009552 Validation loss 0.05874035134911537 Accuracy 0.8385000228881836\n",
      "Output tensor([[0.2051],\n",
      "        [0.0679]], device='mps:0')\n",
      "Iteration 59150 Training loss 0.0515885166823864 Validation loss 0.058444324880838394 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.8379],\n",
      "        [0.9146]], device='mps:0')\n",
      "Iteration 59160 Training loss 0.05992647632956505 Validation loss 0.05851667746901512 Accuracy 0.8390000462532043\n",
      "Output tensor([[0.0457],\n",
      "        [0.1537]], device='mps:0')\n",
      "Iteration 59170 Training loss 0.06731667369604111 Validation loss 0.058401111513376236 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.7935],\n",
      "        [0.8222]], device='mps:0')\n",
      "Iteration 59180 Training loss 0.04721694812178612 Validation loss 0.05840720608830452 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0470],\n",
      "        [0.7589]], device='mps:0')\n",
      "Iteration 59190 Training loss 0.056379303336143494 Validation loss 0.05844851955771446 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9640],\n",
      "        [0.3939]], device='mps:0')\n",
      "Iteration 59200 Training loss 0.060266587883234024 Validation loss 0.05840092897415161 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.5299],\n",
      "        [0.8003]], device='mps:0')\n",
      "Iteration 59210 Training loss 0.05608998239040375 Validation loss 0.058539979159832 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.6170],\n",
      "        [0.3332]], device='mps:0')\n",
      "Iteration 59220 Training loss 0.05437518656253815 Validation loss 0.058389950543642044 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.1516],\n",
      "        [0.1223]], device='mps:0')\n",
      "Iteration 59230 Training loss 0.048146944493055344 Validation loss 0.058389194309711456 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9578],\n",
      "        [0.0120]], device='mps:0')\n",
      "Iteration 59240 Training loss 0.04664214327931404 Validation loss 0.05840256065130234 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.2194],\n",
      "        [0.0359]], device='mps:0')\n",
      "Iteration 59250 Training loss 0.06198785826563835 Validation loss 0.05844928324222565 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1748],\n",
      "        [0.4783]], device='mps:0')\n",
      "Iteration 59260 Training loss 0.05348437279462814 Validation loss 0.05839536711573601 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0501],\n",
      "        [0.7351]], device='mps:0')\n",
      "Iteration 59270 Training loss 0.05345518887042999 Validation loss 0.058389805257320404 Accuracy 0.842875063419342\n",
      "Output tensor([[0.6938],\n",
      "        [0.9205]], device='mps:0')\n",
      "Iteration 59280 Training loss 0.061026714742183685 Validation loss 0.05848968029022217 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.6950],\n",
      "        [0.7381]], device='mps:0')\n",
      "Iteration 59290 Training loss 0.04835790395736694 Validation loss 0.05849139764904976 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.8922],\n",
      "        [0.9888]], device='mps:0')\n",
      "Iteration 59300 Training loss 0.06251617521047592 Validation loss 0.05839578062295914 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.2763],\n",
      "        [0.7387]], device='mps:0')\n",
      "Iteration 59310 Training loss 0.055274780839681625 Validation loss 0.05839385837316513 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1116],\n",
      "        [0.9183]], device='mps:0')\n",
      "Iteration 59320 Training loss 0.05183969438076019 Validation loss 0.05850110203027725 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.5452],\n",
      "        [0.8815]], device='mps:0')\n",
      "Iteration 59330 Training loss 0.05117488652467728 Validation loss 0.05842697620391846 Accuracy 0.842875063419342\n",
      "Output tensor([[0.2535],\n",
      "        [0.6869]], device='mps:0')\n",
      "Iteration 59340 Training loss 0.0506792850792408 Validation loss 0.05840194970369339 Accuracy 0.843000054359436\n",
      "Output tensor([[0.8358],\n",
      "        [0.9811]], device='mps:0')\n",
      "Iteration 59350 Training loss 0.05428524687886238 Validation loss 0.05841903015971184 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9711],\n",
      "        [0.9157]], device='mps:0')\n",
      "Iteration 59360 Training loss 0.061723411083221436 Validation loss 0.05839531123638153 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0267],\n",
      "        [0.7346]], device='mps:0')\n",
      "Iteration 59370 Training loss 0.05780891329050064 Validation loss 0.058462195098400116 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.5461],\n",
      "        [0.7290]], device='mps:0')\n",
      "Iteration 59380 Training loss 0.06109216436743736 Validation loss 0.05844377353787422 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.9712],\n",
      "        [0.6069]], device='mps:0')\n",
      "Iteration 59390 Training loss 0.062469471246004105 Validation loss 0.05840921401977539 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0486],\n",
      "        [0.6308]], device='mps:0')\n",
      "Iteration 59400 Training loss 0.060776833444833755 Validation loss 0.0588146336376667 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.6623],\n",
      "        [0.0748]], device='mps:0')\n",
      "Iteration 59410 Training loss 0.056361936032772064 Validation loss 0.05838323011994362 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.3278],\n",
      "        [0.4219]], device='mps:0')\n",
      "Iteration 59420 Training loss 0.0588817335665226 Validation loss 0.05847810581326485 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.3696],\n",
      "        [0.1693]], device='mps:0')\n",
      "Iteration 59430 Training loss 0.053215015679597855 Validation loss 0.058378756046295166 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.0309],\n",
      "        [0.0247]], device='mps:0')\n",
      "Iteration 59440 Training loss 0.05236893519759178 Validation loss 0.058377839624881744 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9365],\n",
      "        [0.5086]], device='mps:0')\n",
      "Iteration 59450 Training loss 0.05777070298790932 Validation loss 0.058404818177223206 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.1009],\n",
      "        [0.3767]], device='mps:0')\n",
      "Iteration 59460 Training loss 0.05649625509977341 Validation loss 0.058375272899866104 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9200],\n",
      "        [0.2541]], device='mps:0')\n",
      "Iteration 59470 Training loss 0.05305688828229904 Validation loss 0.058484338223934174 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.6702],\n",
      "        [0.3538]], device='mps:0')\n",
      "Iteration 59480 Training loss 0.04992955923080444 Validation loss 0.05838631093502045 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1276],\n",
      "        [0.4428]], device='mps:0')\n",
      "Iteration 59490 Training loss 0.057383522391319275 Validation loss 0.058375854045152664 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.5318],\n",
      "        [0.9664]], device='mps:0')\n",
      "Iteration 59500 Training loss 0.05646514892578125 Validation loss 0.05840486288070679 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.9203],\n",
      "        [0.9326]], device='mps:0')\n",
      "Iteration 59510 Training loss 0.05512794852256775 Validation loss 0.05836588889360428 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.5682],\n",
      "        [0.9784]], device='mps:0')\n",
      "Iteration 59520 Training loss 0.06329674273729324 Validation loss 0.058404162526130676 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9883],\n",
      "        [0.9882]], device='mps:0')\n",
      "Iteration 59530 Training loss 0.046850670129060745 Validation loss 0.05843198671936989 Accuracy 0.842875063419342\n",
      "Output tensor([[0.2491],\n",
      "        [0.0773]], device='mps:0')\n",
      "Iteration 59540 Training loss 0.05116439238190651 Validation loss 0.05842334404587746 Accuracy 0.842875063419342\n",
      "Output tensor([[0.4663],\n",
      "        [0.0618]], device='mps:0')\n",
      "Iteration 59550 Training loss 0.05766283720731735 Validation loss 0.05835989490151405 Accuracy 0.843000054359436\n",
      "Output tensor([[0.1487],\n",
      "        [0.2021]], device='mps:0')\n",
      "Iteration 59560 Training loss 0.048691775649785995 Validation loss 0.058372002094984055 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.2399],\n",
      "        [0.5033]], device='mps:0')\n",
      "Iteration 59570 Training loss 0.05604403838515282 Validation loss 0.058387186378240585 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.0464],\n",
      "        [0.9704]], device='mps:0')\n",
      "Iteration 59580 Training loss 0.050387993454933167 Validation loss 0.0583687461912632 Accuracy 0.842875063419342\n",
      "Output tensor([[0.2900],\n",
      "        [0.0054]], device='mps:0')\n",
      "Iteration 59590 Training loss 0.04971905052661896 Validation loss 0.058401040732860565 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.8928],\n",
      "        [0.9210]], device='mps:0')\n",
      "Iteration 59600 Training loss 0.05447377264499664 Validation loss 0.058560051023960114 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.6846],\n",
      "        [0.4236]], device='mps:0')\n",
      "Iteration 59610 Training loss 0.056198205798864365 Validation loss 0.05850914493203163 Accuracy 0.842875063419342\n",
      "Output tensor([[0.3116],\n",
      "        [0.0368]], device='mps:0')\n",
      "Iteration 59620 Training loss 0.05456092581152916 Validation loss 0.05837931111454964 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0896],\n",
      "        [0.0995]], device='mps:0')\n",
      "Iteration 59630 Training loss 0.05977759137749672 Validation loss 0.058388084173202515 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.4455],\n",
      "        [0.9659]], device='mps:0')\n",
      "Iteration 59640 Training loss 0.04767482727766037 Validation loss 0.05838393047451973 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.3239],\n",
      "        [0.0284]], device='mps:0')\n",
      "Iteration 59650 Training loss 0.06243237480521202 Validation loss 0.05835500359535217 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9822],\n",
      "        [0.7215]], device='mps:0')\n",
      "Iteration 59660 Training loss 0.05334610864520073 Validation loss 0.05835413560271263 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9677],\n",
      "        [0.0774]], device='mps:0')\n",
      "Iteration 59670 Training loss 0.05639294162392616 Validation loss 0.05837976932525635 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.1116],\n",
      "        [0.9450]], device='mps:0')\n",
      "Iteration 59680 Training loss 0.05485697463154793 Validation loss 0.058356087654829025 Accuracy 0.842875063419342\n",
      "Output tensor([[0.8444],\n",
      "        [0.6324]], device='mps:0')\n",
      "Iteration 59690 Training loss 0.05617072805762291 Validation loss 0.05835670977830887 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.6893],\n",
      "        [0.7690]], device='mps:0')\n",
      "Iteration 59700 Training loss 0.053627319633960724 Validation loss 0.0583789087831974 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.8063],\n",
      "        [0.7823]], device='mps:0')\n",
      "Iteration 59710 Training loss 0.050073765218257904 Validation loss 0.05840469151735306 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.4687],\n",
      "        [0.7397]], device='mps:0')\n",
      "Iteration 59720 Training loss 0.0564168281853199 Validation loss 0.058383576571941376 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.2009],\n",
      "        [0.6675]], device='mps:0')\n",
      "Iteration 59730 Training loss 0.055803317576646805 Validation loss 0.05834517255425453 Accuracy 0.84312504529953\n",
      "Output tensor([[0.5427],\n",
      "        [0.0430]], device='mps:0')\n",
      "Iteration 59740 Training loss 0.051932696253061295 Validation loss 0.05833571404218674 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0102],\n",
      "        [0.9325]], device='mps:0')\n",
      "Iteration 59750 Training loss 0.057860374450683594 Validation loss 0.05838713422417641 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.1972],\n",
      "        [0.9671]], device='mps:0')\n",
      "Iteration 59760 Training loss 0.05737768113613129 Validation loss 0.05846216529607773 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.6280],\n",
      "        [0.1607]], device='mps:0')\n",
      "Iteration 59770 Training loss 0.052815865725278854 Validation loss 0.058376844972372055 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.1408],\n",
      "        [0.8579]], device='mps:0')\n",
      "Iteration 59780 Training loss 0.06529620289802551 Validation loss 0.058371491730213165 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.2349],\n",
      "        [0.0631]], device='mps:0')\n",
      "Iteration 59790 Training loss 0.051464080810546875 Validation loss 0.058624789118766785 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.8462],\n",
      "        [0.9599]], device='mps:0')\n",
      "Iteration 59800 Training loss 0.04919707030057907 Validation loss 0.05837184935808182 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9871],\n",
      "        [0.1447]], device='mps:0')\n",
      "Iteration 59810 Training loss 0.054216474294662476 Validation loss 0.05863402038812637 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.4417],\n",
      "        [0.7376]], device='mps:0')\n",
      "Iteration 59820 Training loss 0.055317677557468414 Validation loss 0.058327775448560715 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9278],\n",
      "        [0.8077]], device='mps:0')\n",
      "Iteration 59830 Training loss 0.053520187735557556 Validation loss 0.058326274156570435 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0136],\n",
      "        [0.8194]], device='mps:0')\n",
      "Iteration 59840 Training loss 0.052916716784238815 Validation loss 0.05861625075340271 Accuracy 0.8387500643730164\n",
      "Output tensor([[0.8940],\n",
      "        [0.0942]], device='mps:0')\n",
      "Iteration 59850 Training loss 0.052232615649700165 Validation loss 0.0583217479288578 Accuracy 0.842875063419342\n",
      "Output tensor([[0.1902],\n",
      "        [0.0142]], device='mps:0')\n",
      "Iteration 59860 Training loss 0.058701302856206894 Validation loss 0.05833104997873306 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.7659],\n",
      "        [0.2250]], device='mps:0')\n",
      "Iteration 59870 Training loss 0.06545288115739822 Validation loss 0.05832284316420555 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.6596],\n",
      "        [0.2856]], device='mps:0')\n",
      "Iteration 59880 Training loss 0.053111542016267776 Validation loss 0.0586545430123806 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.1215],\n",
      "        [0.9461]], device='mps:0')\n",
      "Iteration 59890 Training loss 0.06700590252876282 Validation loss 0.05840661749243736 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.2061],\n",
      "        [0.9303]], device='mps:0')\n",
      "Iteration 59900 Training loss 0.05180782079696655 Validation loss 0.058339692652225494 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.0614],\n",
      "        [0.3081]], device='mps:0')\n",
      "Iteration 59910 Training loss 0.05425889417529106 Validation loss 0.05842437222599983 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.1476],\n",
      "        [0.4031]], device='mps:0')\n",
      "Iteration 59920 Training loss 0.057807933539152145 Validation loss 0.05843324586749077 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.1213],\n",
      "        [0.0161]], device='mps:0')\n",
      "Iteration 59930 Training loss 0.05166386440396309 Validation loss 0.058642495423555374 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.8920],\n",
      "        [0.6097]], device='mps:0')\n",
      "Iteration 59940 Training loss 0.05756150931119919 Validation loss 0.05832831934094429 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.1751],\n",
      "        [0.7402]], device='mps:0')\n",
      "Iteration 59950 Training loss 0.0561048798263073 Validation loss 0.05837874859571457 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.1240],\n",
      "        [0.9918]], device='mps:0')\n",
      "Iteration 59960 Training loss 0.04521823674440384 Validation loss 0.05831844359636307 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0922],\n",
      "        [0.1996]], device='mps:0')\n",
      "Iteration 59970 Training loss 0.057318322360515594 Validation loss 0.05834579840302467 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.2523],\n",
      "        [0.7491]], device='mps:0')\n",
      "Iteration 59980 Training loss 0.053799550980329514 Validation loss 0.05831291899085045 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.8070],\n",
      "        [0.1197]], device='mps:0')\n",
      "Iteration 59990 Training loss 0.0618702694773674 Validation loss 0.05833625793457031 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.8997],\n",
      "        [0.9551]], device='mps:0')\n",
      "Iteration 60000 Training loss 0.059802282601594925 Validation loss 0.058424096554517746 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0723],\n",
      "        [0.5514]], device='mps:0')\n",
      "Iteration 60010 Training loss 0.05155987665057182 Validation loss 0.0583418607711792 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.7341],\n",
      "        [0.9387]], device='mps:0')\n",
      "Iteration 60020 Training loss 0.059370748698711395 Validation loss 0.05843917280435562 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0696],\n",
      "        [0.0533]], device='mps:0')\n",
      "Iteration 60030 Training loss 0.05857117474079132 Validation loss 0.05833658203482628 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.2306],\n",
      "        [0.7803]], device='mps:0')\n",
      "Iteration 60040 Training loss 0.05272277817130089 Validation loss 0.05841415748000145 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9785],\n",
      "        [0.0830]], device='mps:0')\n",
      "Iteration 60050 Training loss 0.05760197341442108 Validation loss 0.05833043158054352 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0660],\n",
      "        [0.4556]], device='mps:0')\n",
      "Iteration 60060 Training loss 0.05247413367033005 Validation loss 0.05832608416676521 Accuracy 0.843000054359436\n",
      "Output tensor([[0.3936],\n",
      "        [0.0902]], device='mps:0')\n",
      "Iteration 60070 Training loss 0.05055664852261543 Validation loss 0.05834155157208443 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.0086],\n",
      "        [0.0763]], device='mps:0')\n",
      "Iteration 60080 Training loss 0.06004271283745766 Validation loss 0.05832965672016144 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.6907],\n",
      "        [0.6009]], device='mps:0')\n",
      "Iteration 60090 Training loss 0.05161811783909798 Validation loss 0.05831968039274216 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9885],\n",
      "        [0.8266]], device='mps:0')\n",
      "Iteration 60100 Training loss 0.05056704580783844 Validation loss 0.05831209197640419 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9878],\n",
      "        [0.1045]], device='mps:0')\n",
      "Iteration 60110 Training loss 0.058517348021268845 Validation loss 0.05837529897689819 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.4305],\n",
      "        [0.7859]], device='mps:0')\n",
      "Iteration 60120 Training loss 0.055780477821826935 Validation loss 0.05831288546323776 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7962],\n",
      "        [0.3996]], device='mps:0')\n",
      "Iteration 60130 Training loss 0.06597921997308731 Validation loss 0.05841245502233505 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.9401],\n",
      "        [0.3393]], device='mps:0')\n",
      "Iteration 60140 Training loss 0.044015273451805115 Validation loss 0.05833027884364128 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1584],\n",
      "        [0.0964]], device='mps:0')\n",
      "Iteration 60150 Training loss 0.05793256685137749 Validation loss 0.05831807851791382 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.8044],\n",
      "        [0.2609]], device='mps:0')\n",
      "Iteration 60160 Training loss 0.05564451962709427 Validation loss 0.058312252163887024 Accuracy 0.843250036239624\n",
      "Output tensor([[0.1146],\n",
      "        [0.3695]], device='mps:0')\n",
      "Iteration 60170 Training loss 0.052693527191877365 Validation loss 0.058318812400102615 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.4256],\n",
      "        [0.1552]], device='mps:0')\n",
      "Iteration 60180 Training loss 0.055150799453258514 Validation loss 0.058311060070991516 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.2252],\n",
      "        [0.2415]], device='mps:0')\n",
      "Iteration 60190 Training loss 0.05699854716658592 Validation loss 0.05833715200424194 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.3673],\n",
      "        [0.8167]], device='mps:0')\n",
      "Iteration 60200 Training loss 0.04727153852581978 Validation loss 0.058320362120866776 Accuracy 0.843375027179718\n",
      "Output tensor([[0.6731],\n",
      "        [0.0763]], device='mps:0')\n",
      "Iteration 60210 Training loss 0.04617282748222351 Validation loss 0.05836472287774086 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0074],\n",
      "        [0.6918]], device='mps:0')\n",
      "Iteration 60220 Training loss 0.05625788867473602 Validation loss 0.058328282088041306 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0717],\n",
      "        [0.7836]], device='mps:0')\n",
      "Iteration 60230 Training loss 0.05914071947336197 Validation loss 0.05831597372889519 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.0633],\n",
      "        [0.2697]], device='mps:0')\n",
      "Iteration 60240 Training loss 0.056754253804683685 Validation loss 0.05832553282380104 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9717],\n",
      "        [0.1871]], device='mps:0')\n",
      "Iteration 60250 Training loss 0.060909803956747055 Validation loss 0.058317918330430984 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0776],\n",
      "        [0.9894]], device='mps:0')\n",
      "Iteration 60260 Training loss 0.0550079345703125 Validation loss 0.05834976211190224 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.2342],\n",
      "        [0.8984]], device='mps:0')\n",
      "Iteration 60270 Training loss 0.05680243670940399 Validation loss 0.05843111872673035 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.6600],\n",
      "        [0.7770]], device='mps:0')\n",
      "Iteration 60280 Training loss 0.05566250532865524 Validation loss 0.05846022441983223 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0902],\n",
      "        [0.2567]], device='mps:0')\n",
      "Iteration 60290 Training loss 0.05126268416643143 Validation loss 0.058311112225055695 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.6721],\n",
      "        [0.8175]], device='mps:0')\n",
      "Iteration 60300 Training loss 0.06358027458190918 Validation loss 0.05832602083683014 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.1355],\n",
      "        [0.9727]], device='mps:0')\n",
      "Iteration 60310 Training loss 0.05261163040995598 Validation loss 0.058304086327552795 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0144],\n",
      "        [0.2449]], device='mps:0')\n",
      "Iteration 60320 Training loss 0.05948622524738312 Validation loss 0.058418795466423035 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9618],\n",
      "        [0.9972]], device='mps:0')\n",
      "Iteration 60330 Training loss 0.05036553367972374 Validation loss 0.05831388756632805 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9528],\n",
      "        [0.2205]], device='mps:0')\n",
      "Iteration 60340 Training loss 0.05888064205646515 Validation loss 0.058304574340581894 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1141],\n",
      "        [0.0393]], device='mps:0')\n",
      "Iteration 60350 Training loss 0.06234772503376007 Validation loss 0.05833989381790161 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.0988],\n",
      "        [0.8132]], device='mps:0')\n",
      "Iteration 60360 Training loss 0.0545756071805954 Validation loss 0.05833395570516586 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.3621],\n",
      "        [0.6200]], device='mps:0')\n",
      "Iteration 60370 Training loss 0.049914274364709854 Validation loss 0.05856097489595413 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.6235],\n",
      "        [0.9150]], device='mps:0')\n",
      "Iteration 60380 Training loss 0.06050131842494011 Validation loss 0.05830027163028717 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9288],\n",
      "        [0.7428]], device='mps:0')\n",
      "Iteration 60390 Training loss 0.05029112845659256 Validation loss 0.05830675736069679 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9300],\n",
      "        [0.0532]], device='mps:0')\n",
      "Iteration 60400 Training loss 0.05523442476987839 Validation loss 0.05843072384595871 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9691],\n",
      "        [0.0070]], device='mps:0')\n",
      "Iteration 60410 Training loss 0.052310794591903687 Validation loss 0.058340054005384445 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9350],\n",
      "        [0.2643]], device='mps:0')\n",
      "Iteration 60420 Training loss 0.0655898004770279 Validation loss 0.05830015242099762 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0591],\n",
      "        [0.5067]], device='mps:0')\n",
      "Iteration 60430 Training loss 0.05351850017905235 Validation loss 0.058479808270931244 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.5470],\n",
      "        [0.9319]], device='mps:0')\n",
      "Iteration 60440 Training loss 0.0527225099503994 Validation loss 0.05836986005306244 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.0168],\n",
      "        [0.6149]], device='mps:0')\n",
      "Iteration 60450 Training loss 0.052760083228349686 Validation loss 0.05829722434282303 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.2915],\n",
      "        [0.8464]], device='mps:0')\n",
      "Iteration 60460 Training loss 0.05405404046177864 Validation loss 0.058287352323532104 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.5934],\n",
      "        [0.0102]], device='mps:0')\n",
      "Iteration 60470 Training loss 0.05099169537425041 Validation loss 0.058367036283016205 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.8902],\n",
      "        [0.9438]], device='mps:0')\n",
      "Iteration 60480 Training loss 0.04970358684659004 Validation loss 0.05838660150766373 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.0926],\n",
      "        [0.0622]], device='mps:0')\n",
      "Iteration 60490 Training loss 0.049932125955820084 Validation loss 0.058330852538347244 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.2030],\n",
      "        [0.8827]], device='mps:0')\n",
      "Iteration 60500 Training loss 0.04927845299243927 Validation loss 0.058401674032211304 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.6146],\n",
      "        [0.0348]], device='mps:0')\n",
      "Iteration 60510 Training loss 0.06765230000019073 Validation loss 0.058327242732048035 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.7232],\n",
      "        [0.2245]], device='mps:0')\n",
      "Iteration 60520 Training loss 0.05445137247443199 Validation loss 0.05832194164395332 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.3445],\n",
      "        [0.9513]], device='mps:0')\n",
      "Iteration 60530 Training loss 0.05356258898973465 Validation loss 0.05834696441888809 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.1636],\n",
      "        [0.6840]], device='mps:0')\n",
      "Iteration 60540 Training loss 0.06296882778406143 Validation loss 0.05831455811858177 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.7618],\n",
      "        [0.0114]], device='mps:0')\n",
      "Iteration 60550 Training loss 0.051873426884412766 Validation loss 0.05829908326268196 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9615],\n",
      "        [0.6222]], device='mps:0')\n",
      "Iteration 60560 Training loss 0.05247906595468521 Validation loss 0.05830108001828194 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9515],\n",
      "        [0.1971]], device='mps:0')\n",
      "Iteration 60570 Training loss 0.05129353329539299 Validation loss 0.05828580632805824 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9318],\n",
      "        [0.7043]], device='mps:0')\n",
      "Iteration 60580 Training loss 0.0653400793671608 Validation loss 0.05837643891572952 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.4961],\n",
      "        [0.9726]], device='mps:0')\n",
      "Iteration 60590 Training loss 0.055192071944475174 Validation loss 0.05830798298120499 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9853],\n",
      "        [0.0124]], device='mps:0')\n",
      "Iteration 60600 Training loss 0.05528763681650162 Validation loss 0.058307383209466934 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9072],\n",
      "        [0.5928]], device='mps:0')\n",
      "Iteration 60610 Training loss 0.0560389906167984 Validation loss 0.05835363268852234 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.2526],\n",
      "        [0.8634]], device='mps:0')\n",
      "Iteration 60620 Training loss 0.054819218814373016 Validation loss 0.05838986486196518 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.6031],\n",
      "        [0.3285]], device='mps:0')\n",
      "Iteration 60630 Training loss 0.051123037934303284 Validation loss 0.058294422924518585 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.0087],\n",
      "        [0.4798]], device='mps:0')\n",
      "Iteration 60640 Training loss 0.04669415205717087 Validation loss 0.058282237499952316 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.3047],\n",
      "        [0.9327]], device='mps:0')\n",
      "Iteration 60650 Training loss 0.05728002265095711 Validation loss 0.05827262997627258 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9592],\n",
      "        [0.8362]], device='mps:0')\n",
      "Iteration 60660 Training loss 0.05510573089122772 Validation loss 0.05826680362224579 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9638],\n",
      "        [0.1441]], device='mps:0')\n",
      "Iteration 60670 Training loss 0.056151196360588074 Validation loss 0.058300089091062546 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.0074],\n",
      "        [0.0706]], device='mps:0')\n",
      "Iteration 60680 Training loss 0.05547238141298294 Validation loss 0.058377984911203384 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.6426],\n",
      "        [0.0285]], device='mps:0')\n",
      "Iteration 60690 Training loss 0.05754224583506584 Validation loss 0.0585390105843544 Accuracy 0.84312504529953\n",
      "Output tensor([[0.1585],\n",
      "        [0.6361]], device='mps:0')\n",
      "Iteration 60700 Training loss 0.051490552723407745 Validation loss 0.058301664888858795 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.1121],\n",
      "        [0.8776]], device='mps:0')\n",
      "Iteration 60710 Training loss 0.04778142273426056 Validation loss 0.05837529897689819 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.3470],\n",
      "        [0.0193]], device='mps:0')\n",
      "Iteration 60720 Training loss 0.053828585892915726 Validation loss 0.05826929956674576 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9975],\n",
      "        [0.9723]], device='mps:0')\n",
      "Iteration 60730 Training loss 0.047778911888599396 Validation loss 0.05833788588643074 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9720],\n",
      "        [0.5983]], device='mps:0')\n",
      "Iteration 60740 Training loss 0.060456931591033936 Validation loss 0.058275483548641205 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9303],\n",
      "        [0.7073]], device='mps:0')\n",
      "Iteration 60750 Training loss 0.05036713555455208 Validation loss 0.05839219316840172 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0128],\n",
      "        [0.0137]], device='mps:0')\n",
      "Iteration 60760 Training loss 0.055173516273498535 Validation loss 0.05826430767774582 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.0753],\n",
      "        [0.3312]], device='mps:0')\n",
      "Iteration 60770 Training loss 0.05850301682949066 Validation loss 0.05833053961396217 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.8877],\n",
      "        [0.6599]], device='mps:0')\n",
      "Iteration 60780 Training loss 0.06209796294569969 Validation loss 0.058274444192647934 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.1547],\n",
      "        [0.1441]], device='mps:0')\n",
      "Iteration 60790 Training loss 0.05643150210380554 Validation loss 0.05834660679101944 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.0899],\n",
      "        [0.9555]], device='mps:0')\n",
      "Iteration 60800 Training loss 0.061015136539936066 Validation loss 0.05828244239091873 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0257],\n",
      "        [0.6593]], device='mps:0')\n",
      "Iteration 60810 Training loss 0.0595974326133728 Validation loss 0.058260221034288406 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.2042],\n",
      "        [0.7501]], device='mps:0')\n",
      "Iteration 60820 Training loss 0.050796717405319214 Validation loss 0.058266300708055496 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9935],\n",
      "        [0.7640]], device='mps:0')\n",
      "Iteration 60830 Training loss 0.05892162024974823 Validation loss 0.05847737565636635 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.6254],\n",
      "        [0.8913]], device='mps:0')\n",
      "Iteration 60840 Training loss 0.046167466789484024 Validation loss 0.0583052784204483 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.7098],\n",
      "        [0.9396]], device='mps:0')\n",
      "Iteration 60850 Training loss 0.058258771896362305 Validation loss 0.05825848504900932 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.3867],\n",
      "        [0.9678]], device='mps:0')\n",
      "Iteration 60860 Training loss 0.05681852251291275 Validation loss 0.058256182819604874 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.2117],\n",
      "        [0.0297]], device='mps:0')\n",
      "Iteration 60870 Training loss 0.04777282476425171 Validation loss 0.05828524008393288 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.0162],\n",
      "        [0.0514]], device='mps:0')\n",
      "Iteration 60880 Training loss 0.058645233511924744 Validation loss 0.05824987217783928 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.2774],\n",
      "        [0.0885]], device='mps:0')\n",
      "Iteration 60890 Training loss 0.04878988489508629 Validation loss 0.05855635926127434 Accuracy 0.8395000696182251\n",
      "Output tensor([[0.1146],\n",
      "        [0.0751]], device='mps:0')\n",
      "Iteration 60900 Training loss 0.055340927094221115 Validation loss 0.05868072435259819 Accuracy 0.8383750319480896\n",
      "Output tensor([[0.8542],\n",
      "        [0.9572]], device='mps:0')\n",
      "Iteration 60910 Training loss 0.06408586353063583 Validation loss 0.05848133563995361 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.7177],\n",
      "        [0.4322]], device='mps:0')\n",
      "Iteration 60920 Training loss 0.05828551948070526 Validation loss 0.0582440160214901 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.7082],\n",
      "        [0.9238]], device='mps:0')\n",
      "Iteration 60930 Training loss 0.05933372303843498 Validation loss 0.05824483558535576 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.2335],\n",
      "        [0.5465]], device='mps:0')\n",
      "Iteration 60940 Training loss 0.051220282912254333 Validation loss 0.05824209377169609 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.8675],\n",
      "        [0.1984]], device='mps:0')\n",
      "Iteration 60950 Training loss 0.048547226935625076 Validation loss 0.05824391543865204 Accuracy 0.842875063419342\n",
      "Output tensor([[0.6161],\n",
      "        [0.9073]], device='mps:0')\n",
      "Iteration 60960 Training loss 0.05710422247648239 Validation loss 0.05825095251202583 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.5868],\n",
      "        [0.8740]], device='mps:0')\n",
      "Iteration 60970 Training loss 0.05708880349993706 Validation loss 0.05828532576560974 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.1923],\n",
      "        [0.1768]], device='mps:0')\n",
      "Iteration 60980 Training loss 0.04883015528321266 Validation loss 0.058327045291662216 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7205],\n",
      "        [0.9963]], device='mps:0')\n",
      "Iteration 60990 Training loss 0.05460824444890022 Validation loss 0.05824200063943863 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.1261],\n",
      "        [0.2305]], device='mps:0')\n",
      "Iteration 61000 Training loss 0.04712012782692909 Validation loss 0.058245301246643066 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9454],\n",
      "        [0.1720]], device='mps:0')\n",
      "Iteration 61010 Training loss 0.055728692561388016 Validation loss 0.05825447291135788 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.3941],\n",
      "        [0.1615]], device='mps:0')\n",
      "Iteration 61020 Training loss 0.05455275997519493 Validation loss 0.058254968374967575 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.1173],\n",
      "        [0.6038]], device='mps:0')\n",
      "Iteration 61030 Training loss 0.059894390404224396 Validation loss 0.05825062096118927 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.7979],\n",
      "        [0.0037]], device='mps:0')\n",
      "Iteration 61040 Training loss 0.05863521620631218 Validation loss 0.058241866528987885 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0531],\n",
      "        [0.9444]], device='mps:0')\n",
      "Iteration 61050 Training loss 0.05881790071725845 Validation loss 0.058619555085897446 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9775],\n",
      "        [0.0490]], device='mps:0')\n",
      "Iteration 61060 Training loss 0.05108801648020744 Validation loss 0.05823364853858948 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0816],\n",
      "        [0.9255]], device='mps:0')\n",
      "Iteration 61070 Training loss 0.055663757026195526 Validation loss 0.05825353413820267 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.1121],\n",
      "        [0.8778]], device='mps:0')\n",
      "Iteration 61080 Training loss 0.05250042676925659 Validation loss 0.058232489973306656 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.3281],\n",
      "        [0.7826]], device='mps:0')\n",
      "Iteration 61090 Training loss 0.049556389451026917 Validation loss 0.05842641741037369 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.1011],\n",
      "        [0.4377]], device='mps:0')\n",
      "Iteration 61100 Training loss 0.057613860815763474 Validation loss 0.05822697654366493 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1032],\n",
      "        [0.9936]], device='mps:0')\n",
      "Iteration 61110 Training loss 0.060444049537181854 Validation loss 0.058244142681360245 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.5118],\n",
      "        [0.3081]], device='mps:0')\n",
      "Iteration 61120 Training loss 0.045191340148448944 Validation loss 0.058667927980422974 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.9485],\n",
      "        [0.6720]], device='mps:0')\n",
      "Iteration 61130 Training loss 0.052154541015625 Validation loss 0.058227941393852234 Accuracy 0.843000054359436\n",
      "Output tensor([[0.8796],\n",
      "        [0.6885]], device='mps:0')\n",
      "Iteration 61140 Training loss 0.058639831840991974 Validation loss 0.05833124369382858 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.5429],\n",
      "        [0.0369]], device='mps:0')\n",
      "Iteration 61150 Training loss 0.055952705442905426 Validation loss 0.05821936950087547 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9828],\n",
      "        [0.6350]], device='mps:0')\n",
      "Iteration 61160 Training loss 0.04876287281513214 Validation loss 0.05821918323636055 Accuracy 0.84312504529953\n",
      "Output tensor([[0.8765],\n",
      "        [0.7644]], device='mps:0')\n",
      "Iteration 61170 Training loss 0.053701598197221756 Validation loss 0.05826551467180252 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.0105],\n",
      "        [0.9079]], device='mps:0')\n",
      "Iteration 61180 Training loss 0.05518340319395065 Validation loss 0.05822445824742317 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.8642],\n",
      "        [0.0080]], device='mps:0')\n",
      "Iteration 61190 Training loss 0.05724385008215904 Validation loss 0.058238472789525986 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.2222],\n",
      "        [0.2963]], device='mps:0')\n",
      "Iteration 61200 Training loss 0.06294135749340057 Validation loss 0.058228977024555206 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.6142],\n",
      "        [0.4567]], device='mps:0')\n",
      "Iteration 61210 Training loss 0.06172463297843933 Validation loss 0.0583832822740078 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.3321],\n",
      "        [0.3986]], device='mps:0')\n",
      "Iteration 61220 Training loss 0.05909428000450134 Validation loss 0.05832992121577263 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0527],\n",
      "        [0.9581]], device='mps:0')\n",
      "Iteration 61230 Training loss 0.055240992456674576 Validation loss 0.058222245424985886 Accuracy 0.842875063419342\n",
      "Output tensor([[0.7895],\n",
      "        [0.7684]], device='mps:0')\n",
      "Iteration 61240 Training loss 0.0570022277534008 Validation loss 0.058453451842069626 Accuracy 0.842875063419342\n",
      "Output tensor([[0.3735],\n",
      "        [0.0847]], device='mps:0')\n",
      "Iteration 61250 Training loss 0.05704203620553017 Validation loss 0.05823270231485367 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9949],\n",
      "        [0.2807]], device='mps:0')\n",
      "Iteration 61260 Training loss 0.056447163224220276 Validation loss 0.05824100971221924 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.2071],\n",
      "        [0.9542]], device='mps:0')\n",
      "Iteration 61270 Training loss 0.04982693865895271 Validation loss 0.05822902172803879 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9938],\n",
      "        [0.9289]], device='mps:0')\n",
      "Iteration 61280 Training loss 0.062008898705244064 Validation loss 0.05822685360908508 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9937],\n",
      "        [0.0548]], device='mps:0')\n",
      "Iteration 61290 Training loss 0.058915331959724426 Validation loss 0.058224305510520935 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9776],\n",
      "        [0.6092]], device='mps:0')\n",
      "Iteration 61300 Training loss 0.05178055539727211 Validation loss 0.058222293853759766 Accuracy 0.84312504529953\n",
      "Output tensor([[0.4703],\n",
      "        [0.0254]], device='mps:0')\n",
      "Iteration 61310 Training loss 0.054510802030563354 Validation loss 0.058231599628925323 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.5360],\n",
      "        [0.9487]], device='mps:0')\n",
      "Iteration 61320 Training loss 0.061170242726802826 Validation loss 0.058252375572919846 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0415],\n",
      "        [0.8238]], device='mps:0')\n",
      "Iteration 61330 Training loss 0.0579986572265625 Validation loss 0.058220211416482925 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0559],\n",
      "        [0.9166]], device='mps:0')\n",
      "Iteration 61340 Training loss 0.05074126273393631 Validation loss 0.05826713889837265 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.7054],\n",
      "        [0.8662]], device='mps:0')\n",
      "Iteration 61350 Training loss 0.05502687394618988 Validation loss 0.058210648596286774 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.8668],\n",
      "        [0.5066]], device='mps:0')\n",
      "Iteration 61360 Training loss 0.05639485642313957 Validation loss 0.05821799114346504 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9967],\n",
      "        [0.2180]], device='mps:0')\n",
      "Iteration 61370 Training loss 0.05082515999674797 Validation loss 0.058643076568841934 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.1373],\n",
      "        [0.8827]], device='mps:0')\n",
      "Iteration 61380 Training loss 0.05673111602663994 Validation loss 0.058209631592035294 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.3445],\n",
      "        [0.2369]], device='mps:0')\n",
      "Iteration 61390 Training loss 0.05727168917655945 Validation loss 0.058304429054260254 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.0610],\n",
      "        [0.5763]], device='mps:0')\n",
      "Iteration 61400 Training loss 0.047877777367830276 Validation loss 0.05821403115987778 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9592],\n",
      "        [0.9544]], device='mps:0')\n",
      "Iteration 61410 Training loss 0.05692112818360329 Validation loss 0.05821263790130615 Accuracy 0.843000054359436\n",
      "Output tensor([[0.2829],\n",
      "        [0.9955]], device='mps:0')\n",
      "Iteration 61420 Training loss 0.058163899928331375 Validation loss 0.05822422355413437 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.2365],\n",
      "        [0.7306]], device='mps:0')\n",
      "Iteration 61430 Training loss 0.056712012737989426 Validation loss 0.058275531977415085 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.8702],\n",
      "        [0.0351]], device='mps:0')\n",
      "Iteration 61440 Training loss 0.06765881180763245 Validation loss 0.05821266397833824 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.3228],\n",
      "        [0.0618]], device='mps:0')\n",
      "Iteration 61450 Training loss 0.053653016686439514 Validation loss 0.05825886130332947 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.1157],\n",
      "        [0.8256]], device='mps:0')\n",
      "Iteration 61460 Training loss 0.05416158214211464 Validation loss 0.05824324116110802 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.9984],\n",
      "        [0.2618]], device='mps:0')\n",
      "Iteration 61470 Training loss 0.047492388635873795 Validation loss 0.058254797011613846 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0107],\n",
      "        [0.1840]], device='mps:0')\n",
      "Iteration 61480 Training loss 0.05461680889129639 Validation loss 0.05821266397833824 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.3072],\n",
      "        [0.9516]], device='mps:0')\n",
      "Iteration 61490 Training loss 0.05586908385157585 Validation loss 0.0582246333360672 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9353],\n",
      "        [0.5339]], device='mps:0')\n",
      "Iteration 61500 Training loss 0.05069143325090408 Validation loss 0.05822296068072319 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.2864],\n",
      "        [0.9612]], device='mps:0')\n",
      "Iteration 61510 Training loss 0.054211243987083435 Validation loss 0.058240313082933426 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.3571],\n",
      "        [0.9558]], device='mps:0')\n",
      "Iteration 61520 Training loss 0.058729637414216995 Validation loss 0.058238811790943146 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0166],\n",
      "        [0.0380]], device='mps:0')\n",
      "Iteration 61530 Training loss 0.0589987114071846 Validation loss 0.05821312218904495 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0200],\n",
      "        [0.9988]], device='mps:0')\n",
      "Iteration 61540 Training loss 0.06142337992787361 Validation loss 0.05821148306131363 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9988],\n",
      "        [0.1610]], device='mps:0')\n",
      "Iteration 61550 Training loss 0.055236924439668655 Validation loss 0.05824308842420578 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.8769],\n",
      "        [0.0569]], device='mps:0')\n",
      "Iteration 61560 Training loss 0.05319748446345329 Validation loss 0.05823869630694389 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.0380],\n",
      "        [0.9925]], device='mps:0')\n",
      "Iteration 61570 Training loss 0.05568818002939224 Validation loss 0.05822758749127388 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0997],\n",
      "        [0.0399]], device='mps:0')\n",
      "Iteration 61580 Training loss 0.0554451160132885 Validation loss 0.058233216404914856 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.8859],\n",
      "        [0.0807]], device='mps:0')\n",
      "Iteration 61590 Training loss 0.05891609564423561 Validation loss 0.058220040053129196 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.7405],\n",
      "        [0.4114]], device='mps:0')\n",
      "Iteration 61600 Training loss 0.05663299560546875 Validation loss 0.058226577937603 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.6352],\n",
      "        [0.9851]], device='mps:0')\n",
      "Iteration 61610 Training loss 0.0620742067694664 Validation loss 0.05841919407248497 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9773],\n",
      "        [0.0340]], device='mps:0')\n",
      "Iteration 61620 Training loss 0.06142977625131607 Validation loss 0.058213092386722565 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.7060],\n",
      "        [0.0123]], device='mps:0')\n",
      "Iteration 61630 Training loss 0.049258649349212646 Validation loss 0.058301810175180435 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.7624],\n",
      "        [0.0542]], device='mps:0')\n",
      "Iteration 61640 Training loss 0.05036307871341705 Validation loss 0.05853644013404846 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.6165],\n",
      "        [0.7359]], device='mps:0')\n",
      "Iteration 61650 Training loss 0.05282150208950043 Validation loss 0.05818388611078262 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.0383],\n",
      "        [0.0386]], device='mps:0')\n",
      "Iteration 61660 Training loss 0.06643038243055344 Validation loss 0.05819437652826309 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0218],\n",
      "        [0.4301]], device='mps:0')\n",
      "Iteration 61670 Training loss 0.06839189678430557 Validation loss 0.05820108950138092 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.6634],\n",
      "        [0.9368]], device='mps:0')\n",
      "Iteration 61680 Training loss 0.0596543587744236 Validation loss 0.058221787214279175 Accuracy 0.843250036239624\n",
      "Output tensor([[0.1215],\n",
      "        [0.1926]], device='mps:0')\n",
      "Iteration 61690 Training loss 0.05933300778269768 Validation loss 0.058221932500600815 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.8430],\n",
      "        [0.0402]], device='mps:0')\n",
      "Iteration 61700 Training loss 0.052976787090301514 Validation loss 0.05830901488661766 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.6722],\n",
      "        [0.0454]], device='mps:0')\n",
      "Iteration 61710 Training loss 0.049445588141679764 Validation loss 0.058244507759809494 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9433],\n",
      "        [0.9845]], device='mps:0')\n",
      "Iteration 61720 Training loss 0.048340074717998505 Validation loss 0.05821600183844566 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.1622],\n",
      "        [0.2418]], device='mps:0')\n",
      "Iteration 61730 Training loss 0.059230174869298935 Validation loss 0.05825077369809151 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.1569],\n",
      "        [0.1241]], device='mps:0')\n",
      "Iteration 61740 Training loss 0.054957177489995956 Validation loss 0.058192674070596695 Accuracy 0.843250036239624\n",
      "Output tensor([[0.8106],\n",
      "        [0.4214]], device='mps:0')\n",
      "Iteration 61750 Training loss 0.04937766119837761 Validation loss 0.05819825083017349 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9569],\n",
      "        [0.9144]], device='mps:0')\n",
      "Iteration 61760 Training loss 0.055141907185316086 Validation loss 0.05829552188515663 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1409],\n",
      "        [0.1895]], device='mps:0')\n",
      "Iteration 61770 Training loss 0.05450828745961189 Validation loss 0.058372240513563156 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.6779],\n",
      "        [0.4774]], device='mps:0')\n",
      "Iteration 61780 Training loss 0.059647586196660995 Validation loss 0.058205392211675644 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9764],\n",
      "        [0.0058]], device='mps:0')\n",
      "Iteration 61790 Training loss 0.056256864219903946 Validation loss 0.05818934738636017 Accuracy 0.843000054359436\n",
      "Output tensor([[0.8339],\n",
      "        [0.0087]], device='mps:0')\n",
      "Iteration 61800 Training loss 0.05403609201312065 Validation loss 0.05821705237030983 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.1183],\n",
      "        [0.2251]], device='mps:0')\n",
      "Iteration 61810 Training loss 0.05978550761938095 Validation loss 0.05819329991936684 Accuracy 0.843500018119812\n",
      "Output tensor([[0.8594],\n",
      "        [0.9114]], device='mps:0')\n",
      "Iteration 61820 Training loss 0.052979473024606705 Validation loss 0.05834043398499489 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.0648],\n",
      "        [0.9593]], device='mps:0')\n",
      "Iteration 61830 Training loss 0.05548252537846565 Validation loss 0.05818929523229599 Accuracy 0.843000054359436\n",
      "Output tensor([[0.3650],\n",
      "        [0.4858]], device='mps:0')\n",
      "Iteration 61840 Training loss 0.05681800842285156 Validation loss 0.05820092558860779 Accuracy 0.843000054359436\n",
      "Output tensor([[0.5313],\n",
      "        [0.8789]], device='mps:0')\n",
      "Iteration 61850 Training loss 0.050201814621686935 Validation loss 0.05827751383185387 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.7771],\n",
      "        [0.1788]], device='mps:0')\n",
      "Iteration 61860 Training loss 0.0546683631837368 Validation loss 0.058191318064928055 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.8434],\n",
      "        [0.9725]], device='mps:0')\n",
      "Iteration 61870 Training loss 0.05605873838067055 Validation loss 0.058217357844114304 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.7561],\n",
      "        [0.9878]], device='mps:0')\n",
      "Iteration 61880 Training loss 0.053380902856588364 Validation loss 0.05820537358522415 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0138],\n",
      "        [0.0454]], device='mps:0')\n",
      "Iteration 61890 Training loss 0.048446524888277054 Validation loss 0.058176446706056595 Accuracy 0.843500018119812\n",
      "Output tensor([[0.1035],\n",
      "        [0.2735]], device='mps:0')\n",
      "Iteration 61900 Training loss 0.04765237867832184 Validation loss 0.058178652077913284 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0297],\n",
      "        [0.1889]], device='mps:0')\n",
      "Iteration 61910 Training loss 0.0537683442234993 Validation loss 0.058186259120702744 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.6197],\n",
      "        [0.9399]], device='mps:0')\n",
      "Iteration 61920 Training loss 0.058984268456697464 Validation loss 0.058174051344394684 Accuracy 0.84312504529953\n",
      "Output tensor([[0.1746],\n",
      "        [0.1810]], device='mps:0')\n",
      "Iteration 61930 Training loss 0.050390370190143585 Validation loss 0.05819022282958031 Accuracy 0.843250036239624\n",
      "Output tensor([[0.2262],\n",
      "        [0.0304]], device='mps:0')\n",
      "Iteration 61940 Training loss 0.05768485739827156 Validation loss 0.0583014078438282 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9618],\n",
      "        [0.2158]], device='mps:0')\n",
      "Iteration 61950 Training loss 0.05681737884879112 Validation loss 0.05817459896206856 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.8369],\n",
      "        [0.8620]], device='mps:0')\n",
      "Iteration 61960 Training loss 0.05374862253665924 Validation loss 0.05825216323137283 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.4251],\n",
      "        [0.9236]], device='mps:0')\n",
      "Iteration 61970 Training loss 0.04439585655927658 Validation loss 0.058273978531360626 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.2935],\n",
      "        [0.5407]], device='mps:0')\n",
      "Iteration 61980 Training loss 0.054834265261888504 Validation loss 0.058267589658498764 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.1932],\n",
      "        [0.9815]], device='mps:0')\n",
      "Iteration 61990 Training loss 0.05389844626188278 Validation loss 0.058180488646030426 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9582],\n",
      "        [0.0750]], device='mps:0')\n",
      "Iteration 62000 Training loss 0.052157625555992126 Validation loss 0.05820894241333008 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.0535],\n",
      "        [0.9636]], device='mps:0')\n",
      "Iteration 62010 Training loss 0.06134381145238876 Validation loss 0.05819472298026085 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.5445],\n",
      "        [0.0634]], device='mps:0')\n",
      "Iteration 62020 Training loss 0.05920988321304321 Validation loss 0.058175183832645416 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7144],\n",
      "        [0.9601]], device='mps:0')\n",
      "Iteration 62030 Training loss 0.06731666624546051 Validation loss 0.05819711461663246 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9084],\n",
      "        [0.5480]], device='mps:0')\n",
      "Iteration 62040 Training loss 0.05839395523071289 Validation loss 0.05816660448908806 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.8947],\n",
      "        [0.1076]], device='mps:0')\n",
      "Iteration 62050 Training loss 0.05510042980313301 Validation loss 0.05819716304540634 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.6210],\n",
      "        [0.0352]], device='mps:0')\n",
      "Iteration 62060 Training loss 0.0472174733877182 Validation loss 0.058162838220596313 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9836],\n",
      "        [0.1464]], device='mps:0')\n",
      "Iteration 62070 Training loss 0.052629657089710236 Validation loss 0.05821838974952698 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0004],\n",
      "        [0.0534]], device='mps:0')\n",
      "Iteration 62080 Training loss 0.05669794976711273 Validation loss 0.05816476792097092 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0926],\n",
      "        [0.0774]], device='mps:0')\n",
      "Iteration 62090 Training loss 0.05804095044732094 Validation loss 0.05852287635207176 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.2551],\n",
      "        [0.0676]], device='mps:0')\n",
      "Iteration 62100 Training loss 0.057190656661987305 Validation loss 0.05816194415092468 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.6649],\n",
      "        [0.3717]], device='mps:0')\n",
      "Iteration 62110 Training loss 0.06670726835727692 Validation loss 0.058186884969472885 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9939],\n",
      "        [0.9839]], device='mps:0')\n",
      "Iteration 62120 Training loss 0.05123845487833023 Validation loss 0.058153703808784485 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9468],\n",
      "        [0.1593]], device='mps:0')\n",
      "Iteration 62130 Training loss 0.04722566530108452 Validation loss 0.05817210301756859 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.1625],\n",
      "        [0.7804]], device='mps:0')\n",
      "Iteration 62140 Training loss 0.06063758209347725 Validation loss 0.05816810205578804 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.6802],\n",
      "        [0.0991]], device='mps:0')\n",
      "Iteration 62150 Training loss 0.04895565286278725 Validation loss 0.058160167187452316 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0380],\n",
      "        [0.0484]], device='mps:0')\n",
      "Iteration 62160 Training loss 0.05538315698504448 Validation loss 0.058149706572294235 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.5618],\n",
      "        [0.9928]], device='mps:0')\n",
      "Iteration 62170 Training loss 0.05570463091135025 Validation loss 0.058148011565208435 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9027],\n",
      "        [0.9322]], device='mps:0')\n",
      "Iteration 62180 Training loss 0.051899708807468414 Validation loss 0.05819123983383179 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.8726],\n",
      "        [0.6704]], device='mps:0')\n",
      "Iteration 62190 Training loss 0.05483761057257652 Validation loss 0.058151040226221085 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9784],\n",
      "        [0.7245]], device='mps:0')\n",
      "Iteration 62200 Training loss 0.059755366295576096 Validation loss 0.0583605132997036 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.6419],\n",
      "        [0.0276]], device='mps:0')\n",
      "Iteration 62210 Training loss 0.05494822561740875 Validation loss 0.05815393105149269 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.3574],\n",
      "        [0.9092]], device='mps:0')\n",
      "Iteration 62220 Training loss 0.05792062729597092 Validation loss 0.05816265568137169 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9077],\n",
      "        [0.8059]], device='mps:0')\n",
      "Iteration 62230 Training loss 0.05429992452263832 Validation loss 0.058297786861658096 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.4504],\n",
      "        [0.8253]], device='mps:0')\n",
      "Iteration 62240 Training loss 0.05390191450715065 Validation loss 0.05819748342037201 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9697],\n",
      "        [0.4602]], device='mps:0')\n",
      "Iteration 62250 Training loss 0.06046001613140106 Validation loss 0.05814746394753456 Accuracy 0.842875063419342\n",
      "Output tensor([[0.1274],\n",
      "        [0.0719]], device='mps:0')\n",
      "Iteration 62260 Training loss 0.06808094680309296 Validation loss 0.058153823018074036 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0027],\n",
      "        [0.0677]], device='mps:0')\n",
      "Iteration 62270 Training loss 0.050090525299310684 Validation loss 0.05814766138792038 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.3250],\n",
      "        [0.0676]], device='mps:0')\n",
      "Iteration 62280 Training loss 0.05785346403717995 Validation loss 0.058312106877565384 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9079],\n",
      "        [0.3786]], device='mps:0')\n",
      "Iteration 62290 Training loss 0.05763377994298935 Validation loss 0.05814781412482262 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0364],\n",
      "        [0.0181]], device='mps:0')\n",
      "Iteration 62300 Training loss 0.05202166363596916 Validation loss 0.058215994387865067 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9505],\n",
      "        [0.5254]], device='mps:0')\n",
      "Iteration 62310 Training loss 0.05674241483211517 Validation loss 0.05827207490801811 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9516],\n",
      "        [0.0144]], device='mps:0')\n",
      "Iteration 62320 Training loss 0.0512312687933445 Validation loss 0.058165475726127625 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9756],\n",
      "        [0.1456]], device='mps:0')\n",
      "Iteration 62330 Training loss 0.053060565143823624 Validation loss 0.05817856267094612 Accuracy 0.84312504529953\n",
      "Output tensor([[0.7745],\n",
      "        [0.4937]], device='mps:0')\n",
      "Iteration 62340 Training loss 0.04802902042865753 Validation loss 0.05815548449754715 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.2747],\n",
      "        [0.1927]], device='mps:0')\n",
      "Iteration 62350 Training loss 0.057743292301893234 Validation loss 0.05819881707429886 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1653],\n",
      "        [0.0978]], device='mps:0')\n",
      "Iteration 62360 Training loss 0.06223227456212044 Validation loss 0.05822780728340149 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.4148],\n",
      "        [0.6893]], device='mps:0')\n",
      "Iteration 62370 Training loss 0.05703140050172806 Validation loss 0.058162249624729156 Accuracy 0.842875063419342\n",
      "Output tensor([[0.2839],\n",
      "        [0.6223]], device='mps:0')\n",
      "Iteration 62380 Training loss 0.053476229310035706 Validation loss 0.05831313878297806 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9269],\n",
      "        [0.4601]], device='mps:0')\n",
      "Iteration 62390 Training loss 0.0473279170691967 Validation loss 0.058184076100587845 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.7898],\n",
      "        [0.0228]], device='mps:0')\n",
      "Iteration 62400 Training loss 0.05211585387587547 Validation loss 0.05816233158111572 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0683],\n",
      "        [0.0835]], device='mps:0')\n",
      "Iteration 62410 Training loss 0.05298661068081856 Validation loss 0.058164071291685104 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0074],\n",
      "        [0.2202]], device='mps:0')\n",
      "Iteration 62420 Training loss 0.05734426528215408 Validation loss 0.058217890560626984 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.2455],\n",
      "        [0.9648]], device='mps:0')\n",
      "Iteration 62430 Training loss 0.05069765821099281 Validation loss 0.058156222105026245 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.0414],\n",
      "        [0.0584]], device='mps:0')\n",
      "Iteration 62440 Training loss 0.05477529764175415 Validation loss 0.05828123539686203 Accuracy 0.842875063419342\n",
      "Output tensor([[0.1157],\n",
      "        [0.2529]], device='mps:0')\n",
      "Iteration 62450 Training loss 0.057670172303915024 Validation loss 0.05814839527010918 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.2241],\n",
      "        [0.5172]], device='mps:0')\n",
      "Iteration 62460 Training loss 0.05385715514421463 Validation loss 0.05814457684755325 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.8389],\n",
      "        [0.2313]], device='mps:0')\n",
      "Iteration 62470 Training loss 0.050032977014780045 Validation loss 0.058272868394851685 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.1241],\n",
      "        [0.4727]], device='mps:0')\n",
      "Iteration 62480 Training loss 0.0624762661755085 Validation loss 0.058145374059677124 Accuracy 0.843000054359436\n",
      "Output tensor([[0.3624],\n",
      "        [0.1497]], device='mps:0')\n",
      "Iteration 62490 Training loss 0.05844498798251152 Validation loss 0.05815337598323822 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9570],\n",
      "        [0.9789]], device='mps:0')\n",
      "Iteration 62500 Training loss 0.061260007321834564 Validation loss 0.058138661086559296 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9151],\n",
      "        [0.3159]], device='mps:0')\n",
      "Iteration 62510 Training loss 0.05593506991863251 Validation loss 0.05825578421354294 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0030],\n",
      "        [0.2619]], device='mps:0')\n",
      "Iteration 62520 Training loss 0.05146603286266327 Validation loss 0.058181457221508026 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9059],\n",
      "        [0.0354]], device='mps:0')\n",
      "Iteration 62530 Training loss 0.0519431047141552 Validation loss 0.058290962129831314 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.0567],\n",
      "        [0.4495]], device='mps:0')\n",
      "Iteration 62540 Training loss 0.05210099741816521 Validation loss 0.05815780162811279 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.1905],\n",
      "        [0.9547]], device='mps:0')\n",
      "Iteration 62550 Training loss 0.0490216501057148 Validation loss 0.05816882848739624 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0245],\n",
      "        [0.0856]], device='mps:0')\n",
      "Iteration 62560 Training loss 0.05657771974802017 Validation loss 0.05816284939646721 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9605],\n",
      "        [0.1561]], device='mps:0')\n",
      "Iteration 62570 Training loss 0.06055141240358353 Validation loss 0.05823659896850586 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.8387],\n",
      "        [0.5960]], device='mps:0')\n",
      "Iteration 62580 Training loss 0.05118052288889885 Validation loss 0.05815257504582405 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.8627],\n",
      "        [0.8524]], device='mps:0')\n",
      "Iteration 62590 Training loss 0.061025045812129974 Validation loss 0.058148112148046494 Accuracy 0.843000054359436\n",
      "Output tensor([[0.5131],\n",
      "        [0.7893]], device='mps:0')\n",
      "Iteration 62600 Training loss 0.056268539279699326 Validation loss 0.05822177231311798 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0390],\n",
      "        [0.9756]], device='mps:0')\n",
      "Iteration 62610 Training loss 0.05914890021085739 Validation loss 0.05829422175884247 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.8100],\n",
      "        [0.0465]], device='mps:0')\n",
      "Iteration 62620 Training loss 0.05651271715760231 Validation loss 0.05814959853887558 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.3642],\n",
      "        [0.0544]], device='mps:0')\n",
      "Iteration 62630 Training loss 0.060489244759082794 Validation loss 0.05814626067876816 Accuracy 0.843000054359436\n",
      "Output tensor([[0.1057],\n",
      "        [0.9910]], device='mps:0')\n",
      "Iteration 62640 Training loss 0.05505446344614029 Validation loss 0.0582006499171257 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9176],\n",
      "        [0.0227]], device='mps:0')\n",
      "Iteration 62650 Training loss 0.059278037399053574 Validation loss 0.058289721608161926 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9510],\n",
      "        [0.9877]], device='mps:0')\n",
      "Iteration 62660 Training loss 0.05176034942269325 Validation loss 0.05817351117730141 Accuracy 0.843000054359436\n",
      "Output tensor([[0.2633],\n",
      "        [0.9780]], device='mps:0')\n",
      "Iteration 62670 Training loss 0.05432166904211044 Validation loss 0.05814523622393608 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.1099],\n",
      "        [0.0664]], device='mps:0')\n",
      "Iteration 62680 Training loss 0.053794343024492264 Validation loss 0.05818522721529007 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0938],\n",
      "        [0.9639]], device='mps:0')\n",
      "Iteration 62690 Training loss 0.05586393177509308 Validation loss 0.05823613703250885 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.0654],\n",
      "        [0.9919]], device='mps:0')\n",
      "Iteration 62700 Training loss 0.047269370406866074 Validation loss 0.058167506009340286 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.0071],\n",
      "        [0.9974]], device='mps:0')\n",
      "Iteration 62710 Training loss 0.053834494203329086 Validation loss 0.05815119296312332 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0226],\n",
      "        [0.4913]], device='mps:0')\n",
      "Iteration 62720 Training loss 0.05253450572490692 Validation loss 0.058139655739068985 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9756],\n",
      "        [0.0013]], device='mps:0')\n",
      "Iteration 62730 Training loss 0.05960683152079582 Validation loss 0.058175891637802124 Accuracy 0.84312504529953\n",
      "Output tensor([[0.3240],\n",
      "        [0.5480]], device='mps:0')\n",
      "Iteration 62740 Training loss 0.053085263818502426 Validation loss 0.05820015072822571 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.7268],\n",
      "        [0.1179]], device='mps:0')\n",
      "Iteration 62750 Training loss 0.052747175097465515 Validation loss 0.05820693448185921 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.6252],\n",
      "        [0.1069]], device='mps:0')\n",
      "Iteration 62760 Training loss 0.05421807989478111 Validation loss 0.058135051280260086 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0502],\n",
      "        [0.1701]], device='mps:0')\n",
      "Iteration 62770 Training loss 0.05797076225280762 Validation loss 0.05813342705368996 Accuracy 0.842875063419342\n",
      "Output tensor([[0.2531],\n",
      "        [0.2032]], device='mps:0')\n",
      "Iteration 62780 Training loss 0.05817137286067009 Validation loss 0.05816643685102463 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.1069],\n",
      "        [0.0226]], device='mps:0')\n",
      "Iteration 62790 Training loss 0.05220196768641472 Validation loss 0.058169666677713394 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.7617],\n",
      "        [0.1150]], device='mps:0')\n",
      "Iteration 62800 Training loss 0.058669015765190125 Validation loss 0.058177437633275986 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.6442],\n",
      "        [0.9886]], device='mps:0')\n",
      "Iteration 62810 Training loss 0.0521414652466774 Validation loss 0.0581817664206028 Accuracy 0.843375027179718\n",
      "Output tensor([[0.3934],\n",
      "        [0.1301]], device='mps:0')\n",
      "Iteration 62820 Training loss 0.053851086646318436 Validation loss 0.058271266520023346 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.8211],\n",
      "        [0.0134]], device='mps:0')\n",
      "Iteration 62830 Training loss 0.05809647589921951 Validation loss 0.05815684050321579 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.1387],\n",
      "        [0.9938]], device='mps:0')\n",
      "Iteration 62840 Training loss 0.046742524951696396 Validation loss 0.05821790546178818 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.6527],\n",
      "        [0.8318]], device='mps:0')\n",
      "Iteration 62850 Training loss 0.05044205114245415 Validation loss 0.05821451544761658 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.0565],\n",
      "        [0.4341]], device='mps:0')\n",
      "Iteration 62860 Training loss 0.058757808059453964 Validation loss 0.05813410133123398 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9376],\n",
      "        [0.3954]], device='mps:0')\n",
      "Iteration 62870 Training loss 0.0577596090734005 Validation loss 0.058135729283094406 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0732],\n",
      "        [0.0836]], device='mps:0')\n",
      "Iteration 62880 Training loss 0.059020355343818665 Validation loss 0.05850042402744293 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.1655],\n",
      "        [0.3755]], device='mps:0')\n",
      "Iteration 62890 Training loss 0.06402765959501266 Validation loss 0.058135293424129486 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.1065],\n",
      "        [0.3314]], device='mps:0')\n",
      "Iteration 62900 Training loss 0.054569270461797714 Validation loss 0.05812301114201546 Accuracy 0.842875063419342\n",
      "Output tensor([[0.7630],\n",
      "        [0.6360]], device='mps:0')\n",
      "Iteration 62910 Training loss 0.06156189367175102 Validation loss 0.05812199041247368 Accuracy 0.84312504529953\n",
      "Output tensor([[0.4714],\n",
      "        [0.5544]], device='mps:0')\n",
      "Iteration 62920 Training loss 0.05032221972942352 Validation loss 0.058187779039144516 Accuracy 0.84312504529953\n",
      "Output tensor([[0.1039],\n",
      "        [0.1178]], device='mps:0')\n",
      "Iteration 62930 Training loss 0.05524485558271408 Validation loss 0.05813063308596611 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9245],\n",
      "        [0.2837]], device='mps:0')\n",
      "Iteration 62940 Training loss 0.04485325887799263 Validation loss 0.058120064437389374 Accuracy 0.843375027179718\n",
      "Output tensor([[0.1324],\n",
      "        [0.0105]], device='mps:0')\n",
      "Iteration 62950 Training loss 0.0583219900727272 Validation loss 0.05812106654047966 Accuracy 0.843250036239624\n",
      "Output tensor([[0.6133],\n",
      "        [0.9532]], device='mps:0')\n",
      "Iteration 62960 Training loss 0.05990312993526459 Validation loss 0.058119937777519226 Accuracy 0.84312504529953\n",
      "Output tensor([[0.1301],\n",
      "        [0.1258]], device='mps:0')\n",
      "Iteration 62970 Training loss 0.05317908525466919 Validation loss 0.05814286693930626 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.8945],\n",
      "        [0.9807]], device='mps:0')\n",
      "Iteration 62980 Training loss 0.04469757899641991 Validation loss 0.05813509225845337 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.3784],\n",
      "        [0.9535]], device='mps:0')\n",
      "Iteration 62990 Training loss 0.0661981999874115 Validation loss 0.058168210089206696 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9359],\n",
      "        [0.0397]], device='mps:0')\n",
      "Iteration 63000 Training loss 0.05391065403819084 Validation loss 0.058162394911050797 Accuracy 0.843250036239624\n",
      "Output tensor([[0.1104],\n",
      "        [0.9895]], device='mps:0')\n",
      "Iteration 63010 Training loss 0.05035693943500519 Validation loss 0.05829090252518654 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.1640],\n",
      "        [0.9033]], device='mps:0')\n",
      "Iteration 63020 Training loss 0.04928619787096977 Validation loss 0.05821160599589348 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0768],\n",
      "        [0.5680]], device='mps:0')\n",
      "Iteration 63030 Training loss 0.05592953413724899 Validation loss 0.05811774358153343 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.4315],\n",
      "        [0.9550]], device='mps:0')\n",
      "Iteration 63040 Training loss 0.052181754261255264 Validation loss 0.058119066059589386 Accuracy 0.842875063419342\n",
      "Output tensor([[0.3861],\n",
      "        [0.9767]], device='mps:0')\n",
      "Iteration 63050 Training loss 0.050005462020635605 Validation loss 0.058115702122449875 Accuracy 0.843500018119812\n",
      "Output tensor([[0.1455],\n",
      "        [0.9471]], device='mps:0')\n",
      "Iteration 63060 Training loss 0.044047173112630844 Validation loss 0.05822671949863434 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.3685],\n",
      "        [0.6595]], device='mps:0')\n",
      "Iteration 63070 Training loss 0.05781794339418411 Validation loss 0.05814443528652191 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.1341],\n",
      "        [0.0188]], device='mps:0')\n",
      "Iteration 63080 Training loss 0.0506419837474823 Validation loss 0.05813920125365257 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0956],\n",
      "        [0.1127]], device='mps:0')\n",
      "Iteration 63090 Training loss 0.04719754680991173 Validation loss 0.05810577794909477 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0506],\n",
      "        [0.0855]], device='mps:0')\n",
      "Iteration 63100 Training loss 0.05899031460285187 Validation loss 0.05811184644699097 Accuracy 0.843375027179718\n",
      "Output tensor([[0.8688],\n",
      "        [0.7553]], device='mps:0')\n",
      "Iteration 63110 Training loss 0.04877437651157379 Validation loss 0.05820833891630173 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.0199],\n",
      "        [0.4539]], device='mps:0')\n",
      "Iteration 63120 Training loss 0.05209606885910034 Validation loss 0.058109819889068604 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9216],\n",
      "        [0.2366]], device='mps:0')\n",
      "Iteration 63130 Training loss 0.04527502879500389 Validation loss 0.058109354227781296 Accuracy 0.84312504529953\n",
      "Output tensor([[0.5520],\n",
      "        [0.7099]], device='mps:0')\n",
      "Iteration 63140 Training loss 0.050145845860242844 Validation loss 0.05810921639204025 Accuracy 0.843000054359436\n",
      "Output tensor([[0.7614],\n",
      "        [0.0898]], device='mps:0')\n",
      "Iteration 63150 Training loss 0.059890907257795334 Validation loss 0.05819462239742279 Accuracy 0.84312504529953\n",
      "Output tensor([[0.3420],\n",
      "        [0.7730]], device='mps:0')\n",
      "Iteration 63160 Training loss 0.0576479472219944 Validation loss 0.05833611637353897 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.1125],\n",
      "        [0.5141]], device='mps:0')\n",
      "Iteration 63170 Training loss 0.047735802829265594 Validation loss 0.05847226828336716 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9198],\n",
      "        [0.3502]], device='mps:0')\n",
      "Iteration 63180 Training loss 0.056815896183252335 Validation loss 0.05810801312327385 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9844],\n",
      "        [0.4688]], device='mps:0')\n",
      "Iteration 63190 Training loss 0.053424172103405 Validation loss 0.058121878653764725 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.3124],\n",
      "        [0.3577]], device='mps:0')\n",
      "Iteration 63200 Training loss 0.057949818670749664 Validation loss 0.05811857804656029 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.2277],\n",
      "        [0.2868]], device='mps:0')\n",
      "Iteration 63210 Training loss 0.04890873655676842 Validation loss 0.058119114488363266 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0741],\n",
      "        [0.6649]], device='mps:0')\n",
      "Iteration 63220 Training loss 0.05362022668123245 Validation loss 0.05828653275966644 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9725],\n",
      "        [0.7057]], device='mps:0')\n",
      "Iteration 63230 Training loss 0.061474449932575226 Validation loss 0.05811900272965431 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.7279],\n",
      "        [0.0782]], device='mps:0')\n",
      "Iteration 63240 Training loss 0.06783046573400497 Validation loss 0.05814120173454285 Accuracy 0.843375027179718\n",
      "Output tensor([[0.1581],\n",
      "        [0.0433]], device='mps:0')\n",
      "Iteration 63250 Training loss 0.055748701095581055 Validation loss 0.05823039636015892 Accuracy 0.8400000333786011\n",
      "Output tensor([[0.2370],\n",
      "        [0.8656]], device='mps:0')\n",
      "Iteration 63260 Training loss 0.06802589446306229 Validation loss 0.05815305560827255 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9560],\n",
      "        [0.7774]], device='mps:0')\n",
      "Iteration 63270 Training loss 0.05698583647608757 Validation loss 0.05821353197097778 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.7990],\n",
      "        [0.8993]], device='mps:0')\n",
      "Iteration 63280 Training loss 0.04889494180679321 Validation loss 0.05817442759871483 Accuracy 0.843250036239624\n",
      "Output tensor([[0.8891],\n",
      "        [0.1866]], device='mps:0')\n",
      "Iteration 63290 Training loss 0.05209971219301224 Validation loss 0.058166779577732086 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.3827],\n",
      "        [0.0332]], device='mps:0')\n",
      "Iteration 63300 Training loss 0.055073242634534836 Validation loss 0.05813615396618843 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.2597],\n",
      "        [0.6159]], device='mps:0')\n",
      "Iteration 63310 Training loss 0.04822022467851639 Validation loss 0.058127131313085556 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.8811],\n",
      "        [0.9561]], device='mps:0')\n",
      "Iteration 63320 Training loss 0.052123334258794785 Validation loss 0.05824563279747963 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.5145],\n",
      "        [0.0665]], device='mps:0')\n",
      "Iteration 63330 Training loss 0.05787288397550583 Validation loss 0.0585426464676857 Accuracy 0.8392500281333923\n",
      "Output tensor([[0.2095],\n",
      "        [0.9831]], device='mps:0')\n",
      "Iteration 63340 Training loss 0.04884057119488716 Validation loss 0.05811799317598343 Accuracy 0.843375027179718\n",
      "Output tensor([[0.1350],\n",
      "        [0.9833]], device='mps:0')\n",
      "Iteration 63350 Training loss 0.05015117675065994 Validation loss 0.0581112802028656 Accuracy 0.843000054359436\n",
      "Output tensor([[0.1261],\n",
      "        [0.3409]], device='mps:0')\n",
      "Iteration 63360 Training loss 0.05199546739459038 Validation loss 0.05811886861920357 Accuracy 0.843500018119812\n",
      "Output tensor([[0.8665],\n",
      "        [0.7388]], device='mps:0')\n",
      "Iteration 63370 Training loss 0.05690213665366173 Validation loss 0.05811348184943199 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0384],\n",
      "        [0.5008]], device='mps:0')\n",
      "Iteration 63380 Training loss 0.05247795954346657 Validation loss 0.05816669017076492 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0444],\n",
      "        [0.8955]], device='mps:0')\n",
      "Iteration 63390 Training loss 0.053661949932575226 Validation loss 0.05815054103732109 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.6259],\n",
      "        [0.4150]], device='mps:0')\n",
      "Iteration 63400 Training loss 0.06291911751031876 Validation loss 0.05810510739684105 Accuracy 0.84312504529953\n",
      "Output tensor([[0.3225],\n",
      "        [0.9745]], device='mps:0')\n",
      "Iteration 63410 Training loss 0.05264824628829956 Validation loss 0.05813808739185333 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.7660],\n",
      "        [0.1623]], device='mps:0')\n",
      "Iteration 63420 Training loss 0.06590203940868378 Validation loss 0.05818450450897217 Accuracy 0.843000054359436\n",
      "Output tensor([[0.3101],\n",
      "        [0.5025]], device='mps:0')\n",
      "Iteration 63430 Training loss 0.06602407991886139 Validation loss 0.05810132995247841 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9444],\n",
      "        [0.6419]], device='mps:0')\n",
      "Iteration 63440 Training loss 0.058692652732133865 Validation loss 0.0581035315990448 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9680],\n",
      "        [0.6814]], device='mps:0')\n",
      "Iteration 63450 Training loss 0.06202106177806854 Validation loss 0.058197446167469025 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9824],\n",
      "        [0.9430]], device='mps:0')\n",
      "Iteration 63460 Training loss 0.057533156126737595 Validation loss 0.05814627930521965 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.9140],\n",
      "        [0.9641]], device='mps:0')\n",
      "Iteration 63470 Training loss 0.05802742764353752 Validation loss 0.058093294501304626 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0035],\n",
      "        [0.3279]], device='mps:0')\n",
      "Iteration 63480 Training loss 0.052974652498960495 Validation loss 0.058246444910764694 Accuracy 0.8396250605583191\n",
      "Output tensor([[0.8730],\n",
      "        [0.1837]], device='mps:0')\n",
      "Iteration 63490 Training loss 0.049749620258808136 Validation loss 0.058241236954927444 Accuracy 0.8397500514984131\n",
      "Output tensor([[0.0427],\n",
      "        [0.3393]], device='mps:0')\n",
      "Iteration 63500 Training loss 0.05735275149345398 Validation loss 0.058113716542720795 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.0462],\n",
      "        [0.2167]], device='mps:0')\n",
      "Iteration 63510 Training loss 0.05477701127529144 Validation loss 0.05808299407362938 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.6807],\n",
      "        [0.9142]], device='mps:0')\n",
      "Iteration 63520 Training loss 0.04815354570746422 Validation loss 0.058103300631046295 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9294],\n",
      "        [0.0940]], device='mps:0')\n",
      "Iteration 63530 Training loss 0.05946490168571472 Validation loss 0.058077845722436905 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.6214],\n",
      "        [0.0412]], device='mps:0')\n",
      "Iteration 63540 Training loss 0.05382722616195679 Validation loss 0.05811057612299919 Accuracy 0.843375027179718\n",
      "Output tensor([[0.1379],\n",
      "        [0.9098]], device='mps:0')\n",
      "Iteration 63550 Training loss 0.04687166213989258 Validation loss 0.05808599293231964 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.0167],\n",
      "        [0.1504]], device='mps:0')\n",
      "Iteration 63560 Training loss 0.060629233717918396 Validation loss 0.05808513984084129 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9324],\n",
      "        [0.9659]], device='mps:0')\n",
      "Iteration 63570 Training loss 0.058689672499895096 Validation loss 0.05809204280376434 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.7961],\n",
      "        [0.1766]], device='mps:0')\n",
      "Iteration 63580 Training loss 0.057205747812986374 Validation loss 0.058102384209632874 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0404],\n",
      "        [0.8588]], device='mps:0')\n",
      "Iteration 63590 Training loss 0.04990003630518913 Validation loss 0.05807770416140556 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9789],\n",
      "        [0.4882]], device='mps:0')\n",
      "Iteration 63600 Training loss 0.05350002273917198 Validation loss 0.05810744687914848 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0873],\n",
      "        [0.9586]], device='mps:0')\n",
      "Iteration 63610 Training loss 0.06028682738542557 Validation loss 0.058091290295124054 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.5442],\n",
      "        [0.9479]], device='mps:0')\n",
      "Iteration 63620 Training loss 0.04518943652510643 Validation loss 0.05814184620976448 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7958],\n",
      "        [0.2884]], device='mps:0')\n",
      "Iteration 63630 Training loss 0.058503732085227966 Validation loss 0.058264415711164474 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9243],\n",
      "        [0.9565]], device='mps:0')\n",
      "Iteration 63640 Training loss 0.05078288912773132 Validation loss 0.05822298675775528 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9754],\n",
      "        [0.5309]], device='mps:0')\n",
      "Iteration 63650 Training loss 0.048491377383470535 Validation loss 0.05811937153339386 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.7417],\n",
      "        [0.9729]], device='mps:0')\n",
      "Iteration 63660 Training loss 0.05687486380338669 Validation loss 0.05819259583950043 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.1867],\n",
      "        [0.6854]], device='mps:0')\n",
      "Iteration 63670 Training loss 0.05752452835440636 Validation loss 0.05814579874277115 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9037],\n",
      "        [0.8636]], device='mps:0')\n",
      "Iteration 63680 Training loss 0.04918785020709038 Validation loss 0.05810568109154701 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.3291],\n",
      "        [0.0084]], device='mps:0')\n",
      "Iteration 63690 Training loss 0.05448475107550621 Validation loss 0.058077067136764526 Accuracy 0.843000054359436\n",
      "Output tensor([[0.8524],\n",
      "        [0.7566]], device='mps:0')\n",
      "Iteration 63700 Training loss 0.05282089114189148 Validation loss 0.05812917649745941 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0619],\n",
      "        [0.0836]], device='mps:0')\n",
      "Iteration 63710 Training loss 0.056955765932798386 Validation loss 0.058095481246709824 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.2438],\n",
      "        [0.1185]], device='mps:0')\n",
      "Iteration 63720 Training loss 0.05655280128121376 Validation loss 0.05809273570775986 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.7140],\n",
      "        [0.9814]], device='mps:0')\n",
      "Iteration 63730 Training loss 0.05319954827427864 Validation loss 0.05808259919285774 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.1837],\n",
      "        [0.9720]], device='mps:0')\n",
      "Iteration 63740 Training loss 0.05116582289338112 Validation loss 0.0580780990421772 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.2252],\n",
      "        [0.1717]], device='mps:0')\n",
      "Iteration 63750 Training loss 0.04633331298828125 Validation loss 0.05805959552526474 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9854],\n",
      "        [0.2562]], device='mps:0')\n",
      "Iteration 63760 Training loss 0.055050771683454514 Validation loss 0.058060549199581146 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9958],\n",
      "        [0.9649]], device='mps:0')\n",
      "Iteration 63770 Training loss 0.054222140461206436 Validation loss 0.05813053995370865 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.9149],\n",
      "        [0.9315]], device='mps:0')\n",
      "Iteration 63780 Training loss 0.04979100450873375 Validation loss 0.05810200050473213 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.1977],\n",
      "        [0.3308]], device='mps:0')\n",
      "Iteration 63790 Training loss 0.05669568106532097 Validation loss 0.05807623639702797 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.2481],\n",
      "        [0.8292]], device='mps:0')\n",
      "Iteration 63800 Training loss 0.046976685523986816 Validation loss 0.058101773262023926 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.9274],\n",
      "        [0.7217]], device='mps:0')\n",
      "Iteration 63810 Training loss 0.04945361986756325 Validation loss 0.05813494697213173 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.9316],\n",
      "        [0.1609]], device='mps:0')\n",
      "Iteration 63820 Training loss 0.05667203292250633 Validation loss 0.05817854404449463 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.9691],\n",
      "        [0.5050]], device='mps:0')\n",
      "Iteration 63830 Training loss 0.05846991389989853 Validation loss 0.058075644075870514 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.5127],\n",
      "        [0.0994]], device='mps:0')\n",
      "Iteration 63840 Training loss 0.058234043419361115 Validation loss 0.05809773504734039 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9017],\n",
      "        [0.9404]], device='mps:0')\n",
      "Iteration 63850 Training loss 0.053671903908252716 Validation loss 0.0580718033015728 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9705],\n",
      "        [0.1517]], device='mps:0')\n",
      "Iteration 63860 Training loss 0.05996416136622429 Validation loss 0.058288268744945526 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9244],\n",
      "        [0.2019]], device='mps:0')\n",
      "Iteration 63870 Training loss 0.05721139907836914 Validation loss 0.058079104870557785 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.1425],\n",
      "        [0.1295]], device='mps:0')\n",
      "Iteration 63880 Training loss 0.05489322543144226 Validation loss 0.05807216092944145 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9356],\n",
      "        [0.5536]], device='mps:0')\n",
      "Iteration 63890 Training loss 0.054384250193834305 Validation loss 0.05818706750869751 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.2392],\n",
      "        [0.7897]], device='mps:0')\n",
      "Iteration 63900 Training loss 0.058648694306612015 Validation loss 0.05807017162442207 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0379],\n",
      "        [0.3799]], device='mps:0')\n",
      "Iteration 63910 Training loss 0.055403631180524826 Validation loss 0.05812324956059456 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.5452],\n",
      "        [0.7878]], device='mps:0')\n",
      "Iteration 63920 Training loss 0.05604095384478569 Validation loss 0.05820877104997635 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0094],\n",
      "        [0.9401]], device='mps:0')\n",
      "Iteration 63930 Training loss 0.04561271890997887 Validation loss 0.05820811539888382 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.3686],\n",
      "        [0.3845]], device='mps:0')\n",
      "Iteration 63940 Training loss 0.053036585450172424 Validation loss 0.0580550916492939 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.7616],\n",
      "        [0.6782]], device='mps:0')\n",
      "Iteration 63950 Training loss 0.056138962507247925 Validation loss 0.05813249200582504 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9636],\n",
      "        [0.6887]], device='mps:0')\n",
      "Iteration 63960 Training loss 0.05465861037373543 Validation loss 0.058076076209545135 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9324],\n",
      "        [0.9645]], device='mps:0')\n",
      "Iteration 63970 Training loss 0.049821726977825165 Validation loss 0.058183178305625916 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.8963],\n",
      "        [0.9692]], device='mps:0')\n",
      "Iteration 63980 Training loss 0.053524427115917206 Validation loss 0.05804741010069847 Accuracy 0.843000054359436\n",
      "Output tensor([[0.3854],\n",
      "        [0.9139]], device='mps:0')\n",
      "Iteration 63990 Training loss 0.050270289182662964 Validation loss 0.05806485936045647 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9399],\n",
      "        [0.9701]], device='mps:0')\n",
      "Iteration 64000 Training loss 0.05573733150959015 Validation loss 0.05806693434715271 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0631],\n",
      "        [0.0864]], device='mps:0')\n",
      "Iteration 64010 Training loss 0.055808208882808685 Validation loss 0.05809236690402031 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.4111],\n",
      "        [0.9416]], device='mps:0')\n",
      "Iteration 64020 Training loss 0.05764181166887283 Validation loss 0.058132104575634 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9238],\n",
      "        [0.4404]], device='mps:0')\n",
      "Iteration 64030 Training loss 0.05528521537780762 Validation loss 0.05804728344082832 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9554],\n",
      "        [0.4976]], device='mps:0')\n",
      "Iteration 64040 Training loss 0.05717037618160248 Validation loss 0.05808144435286522 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9275],\n",
      "        [0.1436]], device='mps:0')\n",
      "Iteration 64050 Training loss 0.055647868663072586 Validation loss 0.058115437626838684 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.8239],\n",
      "        [0.6375]], device='mps:0')\n",
      "Iteration 64060 Training loss 0.048230819404125214 Validation loss 0.05804731696844101 Accuracy 0.84312504529953\n",
      "Output tensor([[0.4590],\n",
      "        [0.0407]], device='mps:0')\n",
      "Iteration 64070 Training loss 0.0524766705930233 Validation loss 0.05805877968668938 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.9190],\n",
      "        [0.0607]], device='mps:0')\n",
      "Iteration 64080 Training loss 0.06197568401694298 Validation loss 0.05823351442813873 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.5693],\n",
      "        [0.0190]], device='mps:0')\n",
      "Iteration 64090 Training loss 0.0648782029747963 Validation loss 0.05810005962848663 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.4210],\n",
      "        [0.0857]], device='mps:0')\n",
      "Iteration 64100 Training loss 0.05879492685198784 Validation loss 0.058199480175971985 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.5525],\n",
      "        [0.9109]], device='mps:0')\n",
      "Iteration 64110 Training loss 0.06387274712324142 Validation loss 0.058113742619752884 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.1749],\n",
      "        [0.9329]], device='mps:0')\n",
      "Iteration 64120 Training loss 0.05381601303815842 Validation loss 0.058178309351205826 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.0402],\n",
      "        [0.8693]], device='mps:0')\n",
      "Iteration 64130 Training loss 0.06348393857479095 Validation loss 0.05812709778547287 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.6744],\n",
      "        [0.9042]], device='mps:0')\n",
      "Iteration 64140 Training loss 0.06203032284975052 Validation loss 0.05812026187777519 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.5109],\n",
      "        [0.0957]], device='mps:0')\n",
      "Iteration 64150 Training loss 0.053055744618177414 Validation loss 0.05808350071310997 Accuracy 0.842875063419342\n",
      "Output tensor([[0.1280],\n",
      "        [0.3350]], device='mps:0')\n",
      "Iteration 64160 Training loss 0.06076059117913246 Validation loss 0.05806639790534973 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0973],\n",
      "        [0.4440]], device='mps:0')\n",
      "Iteration 64170 Training loss 0.06065582111477852 Validation loss 0.05807187408208847 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.8869],\n",
      "        [0.3570]], device='mps:0')\n",
      "Iteration 64180 Training loss 0.050726402550935745 Validation loss 0.05809517577290535 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0516],\n",
      "        [0.9690]], device='mps:0')\n",
      "Iteration 64190 Training loss 0.05575333163142204 Validation loss 0.05815298855304718 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9772],\n",
      "        [0.8452]], device='mps:0')\n",
      "Iteration 64200 Training loss 0.05573016405105591 Validation loss 0.058071184903383255 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.2092],\n",
      "        [0.0934]], device='mps:0')\n",
      "Iteration 64210 Training loss 0.056805629283189774 Validation loss 0.05806799978017807 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9650],\n",
      "        [0.8988]], device='mps:0')\n",
      "Iteration 64220 Training loss 0.05281118303537369 Validation loss 0.05807749181985855 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0175],\n",
      "        [0.9472]], device='mps:0')\n",
      "Iteration 64230 Training loss 0.05734340474009514 Validation loss 0.05808404088020325 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1475],\n",
      "        [0.0050]], device='mps:0')\n",
      "Iteration 64240 Training loss 0.05787254124879837 Validation loss 0.058067820966243744 Accuracy 0.843250036239624\n",
      "Output tensor([[0.5158],\n",
      "        [0.0745]], device='mps:0')\n",
      "Iteration 64250 Training loss 0.045518554747104645 Validation loss 0.05810943990945816 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9703],\n",
      "        [0.4949]], device='mps:0')\n",
      "Iteration 64260 Training loss 0.05229930579662323 Validation loss 0.05808192864060402 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.8889],\n",
      "        [0.8423]], device='mps:0')\n",
      "Iteration 64270 Training loss 0.05143064260482788 Validation loss 0.05811062082648277 Accuracy 0.843375027179718\n",
      "Output tensor([[0.7473],\n",
      "        [0.6500]], device='mps:0')\n",
      "Iteration 64280 Training loss 0.054724182933568954 Validation loss 0.05849950760602951 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.2316],\n",
      "        [0.2416]], device='mps:0')\n",
      "Iteration 64290 Training loss 0.05295471101999283 Validation loss 0.05807849392294884 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.0630],\n",
      "        [0.6950]], device='mps:0')\n",
      "Iteration 64300 Training loss 0.0543118454515934 Validation loss 0.05856217443943024 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9510],\n",
      "        [0.1116]], device='mps:0')\n",
      "Iteration 64310 Training loss 0.05786409229040146 Validation loss 0.058073658496141434 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9002],\n",
      "        [0.4185]], device='mps:0')\n",
      "Iteration 64320 Training loss 0.06397578865289688 Validation loss 0.058190494775772095 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9420],\n",
      "        [0.8458]], device='mps:0')\n",
      "Iteration 64330 Training loss 0.05717618018388748 Validation loss 0.058069225400686264 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9681],\n",
      "        [0.8778]], device='mps:0')\n",
      "Iteration 64340 Training loss 0.05372537672519684 Validation loss 0.05817641690373421 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.1727],\n",
      "        [0.6679]], device='mps:0')\n",
      "Iteration 64350 Training loss 0.051468949764966965 Validation loss 0.05810139700770378 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0333],\n",
      "        [0.3455]], device='mps:0')\n",
      "Iteration 64360 Training loss 0.05338055640459061 Validation loss 0.058081287890672684 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.5929],\n",
      "        [0.3810]], device='mps:0')\n",
      "Iteration 64370 Training loss 0.05614809691905975 Validation loss 0.05812098830938339 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9413],\n",
      "        [0.0799]], device='mps:0')\n",
      "Iteration 64380 Training loss 0.050800587981939316 Validation loss 0.058091096580028534 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0112],\n",
      "        [0.0242]], device='mps:0')\n",
      "Iteration 64390 Training loss 0.05194922164082527 Validation loss 0.058092981576919556 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.3804],\n",
      "        [0.7150]], device='mps:0')\n",
      "Iteration 64400 Training loss 0.052432067692279816 Validation loss 0.05807593837380409 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.8035],\n",
      "        [0.3965]], device='mps:0')\n",
      "Iteration 64410 Training loss 0.04242987558245659 Validation loss 0.058070916682481766 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9897],\n",
      "        [0.2266]], device='mps:0')\n",
      "Iteration 64420 Training loss 0.05387164652347565 Validation loss 0.058061741292476654 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9996],\n",
      "        [0.0140]], device='mps:0')\n",
      "Iteration 64430 Training loss 0.04880830645561218 Validation loss 0.058066342025995255 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.0978],\n",
      "        [0.8940]], device='mps:0')\n",
      "Iteration 64440 Training loss 0.058648426085710526 Validation loss 0.05806341767311096 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.9362],\n",
      "        [0.9154]], device='mps:0')\n",
      "Iteration 64450 Training loss 0.05708250030875206 Validation loss 0.058059271425008774 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.2303],\n",
      "        [0.3053]], device='mps:0')\n",
      "Iteration 64460 Training loss 0.062002867460250854 Validation loss 0.05811149999499321 Accuracy 0.843500018119812\n",
      "Output tensor([[0.4841],\n",
      "        [0.0351]], device='mps:0')\n",
      "Iteration 64470 Training loss 0.05107705295085907 Validation loss 0.058108896017074585 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.8595],\n",
      "        [0.1148]], device='mps:0')\n",
      "Iteration 64480 Training loss 0.06659962981939316 Validation loss 0.058045223355293274 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.8825],\n",
      "        [0.3888]], device='mps:0')\n",
      "Iteration 64490 Training loss 0.05575515329837799 Validation loss 0.05804230645298958 Accuracy 0.842875063419342\n",
      "Output tensor([[0.1811],\n",
      "        [0.9866]], device='mps:0')\n",
      "Iteration 64500 Training loss 0.06182848662137985 Validation loss 0.05804246664047241 Accuracy 0.84312504529953\n",
      "Output tensor([[0.4000],\n",
      "        [0.8909]], device='mps:0')\n",
      "Iteration 64510 Training loss 0.062403906136751175 Validation loss 0.05804716795682907 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.1255],\n",
      "        [0.8955]], device='mps:0')\n",
      "Iteration 64520 Training loss 0.05534565821290016 Validation loss 0.058242689818143845 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0754],\n",
      "        [0.9407]], device='mps:0')\n",
      "Iteration 64530 Training loss 0.05428096279501915 Validation loss 0.058038029819726944 Accuracy 0.843000054359436\n",
      "Output tensor([[0.3340],\n",
      "        [0.9899]], device='mps:0')\n",
      "Iteration 64540 Training loss 0.05010353401303291 Validation loss 0.058051351457834244 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9282],\n",
      "        [0.9558]], device='mps:0')\n",
      "Iteration 64550 Training loss 0.053922709077596664 Validation loss 0.05815216526389122 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.0132],\n",
      "        [0.9197]], device='mps:0')\n",
      "Iteration 64560 Training loss 0.05055088922381401 Validation loss 0.05804047733545303 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9815],\n",
      "        [0.8818]], device='mps:0')\n",
      "Iteration 64570 Training loss 0.05241543799638748 Validation loss 0.05805625393986702 Accuracy 0.84312504529953\n",
      "Output tensor([[0.6278],\n",
      "        [0.2512]], device='mps:0')\n",
      "Iteration 64580 Training loss 0.05209202319383621 Validation loss 0.058054905384778976 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.6729],\n",
      "        [0.0443]], device='mps:0')\n",
      "Iteration 64590 Training loss 0.0540115162730217 Validation loss 0.0580415315926075 Accuracy 0.843000054359436\n",
      "Output tensor([[0.3191],\n",
      "        [0.1856]], device='mps:0')\n",
      "Iteration 64600 Training loss 0.0605180449783802 Validation loss 0.058105356991291046 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0353],\n",
      "        [0.9305]], device='mps:0')\n",
      "Iteration 64610 Training loss 0.0645814910531044 Validation loss 0.05803639441728592 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.0149],\n",
      "        [0.3608]], device='mps:0')\n",
      "Iteration 64620 Training loss 0.053002793341875076 Validation loss 0.0580398365855217 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0235],\n",
      "        [0.8953]], device='mps:0')\n",
      "Iteration 64630 Training loss 0.05370286852121353 Validation loss 0.058102771639823914 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.6997],\n",
      "        [0.8960]], device='mps:0')\n",
      "Iteration 64640 Training loss 0.0556013248860836 Validation loss 0.05802885815501213 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.6916],\n",
      "        [0.9824]], device='mps:0')\n",
      "Iteration 64650 Training loss 0.05132095143198967 Validation loss 0.05804470181465149 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9564],\n",
      "        [0.4017]], device='mps:0')\n",
      "Iteration 64660 Training loss 0.062237177044153214 Validation loss 0.05803808942437172 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9188],\n",
      "        [0.7016]], device='mps:0')\n",
      "Iteration 64670 Training loss 0.05717988684773445 Validation loss 0.058027200400829315 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.1490],\n",
      "        [0.6010]], device='mps:0')\n",
      "Iteration 64680 Training loss 0.05460146442055702 Validation loss 0.05802895501255989 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.1132],\n",
      "        [0.8407]], device='mps:0')\n",
      "Iteration 64690 Training loss 0.05732324346899986 Validation loss 0.058042947202920914 Accuracy 0.843375027179718\n",
      "Output tensor([[0.7253],\n",
      "        [0.6119]], device='mps:0')\n",
      "Iteration 64700 Training loss 0.05727453902363777 Validation loss 0.058088596910238266 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0221],\n",
      "        [0.0117]], device='mps:0')\n",
      "Iteration 64710 Training loss 0.060677748173475266 Validation loss 0.05810945853590965 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0486],\n",
      "        [0.0441]], device='mps:0')\n",
      "Iteration 64720 Training loss 0.059523094445466995 Validation loss 0.058019090443849564 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0997],\n",
      "        [0.9059]], device='mps:0')\n",
      "Iteration 64730 Training loss 0.05094355344772339 Validation loss 0.05803133547306061 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9460],\n",
      "        [0.9485]], device='mps:0')\n",
      "Iteration 64740 Training loss 0.06038130074739456 Validation loss 0.05815436691045761 Accuracy 0.843250036239624\n",
      "Output tensor([[0.6594],\n",
      "        [0.0536]], device='mps:0')\n",
      "Iteration 64750 Training loss 0.05185805261135101 Validation loss 0.058023739606142044 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9920],\n",
      "        [0.1536]], device='mps:0')\n",
      "Iteration 64760 Training loss 0.04841664060950279 Validation loss 0.05803058296442032 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9346],\n",
      "        [0.5159]], device='mps:0')\n",
      "Iteration 64770 Training loss 0.04592536762356758 Validation loss 0.0580286830663681 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9337],\n",
      "        [0.8076]], device='mps:0')\n",
      "Iteration 64780 Training loss 0.048655763268470764 Validation loss 0.0580563023686409 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.8684],\n",
      "        [0.9799]], device='mps:0')\n",
      "Iteration 64790 Training loss 0.05166906118392944 Validation loss 0.058022163808345795 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.3267],\n",
      "        [0.0066]], device='mps:0')\n",
      "Iteration 64800 Training loss 0.0643363818526268 Validation loss 0.05803439021110535 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.4812],\n",
      "        [0.0378]], device='mps:0')\n",
      "Iteration 64810 Training loss 0.05141960084438324 Validation loss 0.058168359100818634 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.3630],\n",
      "        [0.4681]], device='mps:0')\n",
      "Iteration 64820 Training loss 0.05055772513151169 Validation loss 0.0580282025039196 Accuracy 0.843250036239624\n",
      "Output tensor([[0.4904],\n",
      "        [0.8672]], device='mps:0')\n",
      "Iteration 64830 Training loss 0.059898894280195236 Validation loss 0.05808112025260925 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0914],\n",
      "        [0.7699]], device='mps:0')\n",
      "Iteration 64840 Training loss 0.050233736634254456 Validation loss 0.05802622810006142 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9878],\n",
      "        [0.9565]], device='mps:0')\n",
      "Iteration 64850 Training loss 0.055045727640390396 Validation loss 0.058050815016031265 Accuracy 0.84312504529953\n",
      "Output tensor([[0.5917],\n",
      "        [0.0261]], device='mps:0')\n",
      "Iteration 64860 Training loss 0.059095438569784164 Validation loss 0.05816366896033287 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.9646],\n",
      "        [0.1984]], device='mps:0')\n",
      "Iteration 64870 Training loss 0.051929645240306854 Validation loss 0.05801723152399063 Accuracy 0.843500018119812\n",
      "Output tensor([[0.6489],\n",
      "        [0.0625]], device='mps:0')\n",
      "Iteration 64880 Training loss 0.047833628952503204 Validation loss 0.0580223947763443 Accuracy 0.843000054359436\n",
      "Output tensor([[0.2767],\n",
      "        [0.2985]], device='mps:0')\n",
      "Iteration 64890 Training loss 0.05399974808096886 Validation loss 0.058019693940877914 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0847],\n",
      "        [0.9220]], device='mps:0')\n",
      "Iteration 64900 Training loss 0.055287983268499374 Validation loss 0.05830014869570732 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.8294],\n",
      "        [0.1527]], device='mps:0')\n",
      "Iteration 64910 Training loss 0.05282846465706825 Validation loss 0.05805135518312454 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.9956],\n",
      "        [0.9414]], device='mps:0')\n",
      "Iteration 64920 Training loss 0.05721808224916458 Validation loss 0.05803033336997032 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9064],\n",
      "        [0.0267]], device='mps:0')\n",
      "Iteration 64930 Training loss 0.04815512150526047 Validation loss 0.058198388665914536 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9819],\n",
      "        [0.8510]], device='mps:0')\n",
      "Iteration 64940 Training loss 0.059904564172029495 Validation loss 0.05801524221897125 Accuracy 0.843250036239624\n",
      "Output tensor([[0.8102],\n",
      "        [0.9684]], device='mps:0')\n",
      "Iteration 64950 Training loss 0.057113390415906906 Validation loss 0.058023832738399506 Accuracy 0.842875063419342\n",
      "Output tensor([[0.8787],\n",
      "        [0.9672]], device='mps:0')\n",
      "Iteration 64960 Training loss 0.05555380880832672 Validation loss 0.058082688599824905 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.7120],\n",
      "        [0.9549]], device='mps:0')\n",
      "Iteration 64970 Training loss 0.06115002557635307 Validation loss 0.058078184723854065 Accuracy 0.843375027179718\n",
      "Output tensor([[0.6763],\n",
      "        [0.0296]], device='mps:0')\n",
      "Iteration 64980 Training loss 0.05486240237951279 Validation loss 0.05802979692816734 Accuracy 0.843250036239624\n",
      "Output tensor([[0.1691],\n",
      "        [0.0064]], device='mps:0')\n",
      "Iteration 64990 Training loss 0.04568737745285034 Validation loss 0.058029655367136 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9527],\n",
      "        [0.5115]], device='mps:0')\n",
      "Iteration 65000 Training loss 0.05673777312040329 Validation loss 0.05802483111619949 Accuracy 0.842875063419342\n",
      "Output tensor([[0.2998],\n",
      "        [0.0512]], device='mps:0')\n",
      "Iteration 65010 Training loss 0.05616185441613197 Validation loss 0.05803985148668289 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.8575],\n",
      "        [0.1510]], device='mps:0')\n",
      "Iteration 65020 Training loss 0.04912933334708214 Validation loss 0.058036256581544876 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.1399],\n",
      "        [0.2622]], device='mps:0')\n",
      "Iteration 65030 Training loss 0.0588049441576004 Validation loss 0.058054469525814056 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.2784],\n",
      "        [0.0416]], device='mps:0')\n",
      "Iteration 65040 Training loss 0.05390750616788864 Validation loss 0.058048684149980545 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9080],\n",
      "        [0.9765]], device='mps:0')\n",
      "Iteration 65050 Training loss 0.054214682430028915 Validation loss 0.05806340277194977 Accuracy 0.843000054359436\n",
      "Output tensor([[0.6543],\n",
      "        [0.9912]], device='mps:0')\n",
      "Iteration 65060 Training loss 0.053647756576538086 Validation loss 0.05804122984409332 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.8857],\n",
      "        [0.9674]], device='mps:0')\n",
      "Iteration 65070 Training loss 0.04670598730444908 Validation loss 0.05818891525268555 Accuracy 0.842875063419342\n",
      "Output tensor([[0.3188],\n",
      "        [0.3998]], device='mps:0')\n",
      "Iteration 65080 Training loss 0.05874690040946007 Validation loss 0.05812675505876541 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9541],\n",
      "        [0.0957]], device='mps:0')\n",
      "Iteration 65090 Training loss 0.057886332273483276 Validation loss 0.05807791277766228 Accuracy 0.8398750424385071\n",
      "Output tensor([[0.0253],\n",
      "        [0.4128]], device='mps:0')\n",
      "Iteration 65100 Training loss 0.05662975087761879 Validation loss 0.05801818147301674 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9722],\n",
      "        [0.9183]], device='mps:0')\n",
      "Iteration 65110 Training loss 0.06015501916408539 Validation loss 0.05810698866844177 Accuracy 0.8401250243186951\n",
      "Output tensor([[0.2037],\n",
      "        [0.2252]], device='mps:0')\n",
      "Iteration 65120 Training loss 0.061850063502788544 Validation loss 0.058015063405036926 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0399],\n",
      "        [0.2447]], device='mps:0')\n",
      "Iteration 65130 Training loss 0.050754744559526443 Validation loss 0.058046530932188034 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.3671],\n",
      "        [0.1363]], device='mps:0')\n",
      "Iteration 65140 Training loss 0.05035487189888954 Validation loss 0.0580906942486763 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9334],\n",
      "        [0.9838]], device='mps:0')\n",
      "Iteration 65150 Training loss 0.05464567989110947 Validation loss 0.05801667273044586 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0186],\n",
      "        [0.3647]], device='mps:0')\n",
      "Iteration 65160 Training loss 0.053928766399621964 Validation loss 0.05822879448533058 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.8150],\n",
      "        [0.5455]], device='mps:0')\n",
      "Iteration 65170 Training loss 0.05616823211312294 Validation loss 0.05834674835205078 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9500],\n",
      "        [0.4411]], device='mps:0')\n",
      "Iteration 65180 Training loss 0.050794608891010284 Validation loss 0.05825069546699524 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.8278],\n",
      "        [0.9788]], device='mps:0')\n",
      "Iteration 65190 Training loss 0.058110665529966354 Validation loss 0.05813463777303696 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0904],\n",
      "        [0.1026]], device='mps:0')\n",
      "Iteration 65200 Training loss 0.044805899262428284 Validation loss 0.057994991540908813 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.0655],\n",
      "        [0.6156]], device='mps:0')\n",
      "Iteration 65210 Training loss 0.054633207619190216 Validation loss 0.05799674242734909 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7334],\n",
      "        [0.1246]], device='mps:0')\n",
      "Iteration 65220 Training loss 0.05140995234251022 Validation loss 0.058004092425107956 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.5562],\n",
      "        [0.8515]], device='mps:0')\n",
      "Iteration 65230 Training loss 0.05964166671037674 Validation loss 0.057994868606328964 Accuracy 0.84312504529953\n",
      "Output tensor([[0.8744],\n",
      "        [0.6634]], device='mps:0')\n",
      "Iteration 65240 Training loss 0.059120893478393555 Validation loss 0.057994380593299866 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9623],\n",
      "        [0.1108]], device='mps:0')\n",
      "Iteration 65250 Training loss 0.05929092317819595 Validation loss 0.058030351996421814 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9915],\n",
      "        [0.1598]], device='mps:0')\n",
      "Iteration 65260 Training loss 0.05559703707695007 Validation loss 0.05802461877465248 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9934],\n",
      "        [0.9636]], device='mps:0')\n",
      "Iteration 65270 Training loss 0.044839296489953995 Validation loss 0.0580238401889801 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0313],\n",
      "        [0.3941]], device='mps:0')\n",
      "Iteration 65280 Training loss 0.05116679146885872 Validation loss 0.05800621956586838 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0410],\n",
      "        [0.0847]], device='mps:0')\n",
      "Iteration 65290 Training loss 0.048866767436265945 Validation loss 0.05801531672477722 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.1018],\n",
      "        [0.6722]], device='mps:0')\n",
      "Iteration 65300 Training loss 0.054105520248413086 Validation loss 0.05798160657286644 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9799],\n",
      "        [0.9586]], device='mps:0')\n",
      "Iteration 65310 Training loss 0.05114605650305748 Validation loss 0.05800918862223625 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.3705],\n",
      "        [0.9937]], device='mps:0')\n",
      "Iteration 65320 Training loss 0.05031846463680267 Validation loss 0.05809641629457474 Accuracy 0.8402500152587891\n",
      "Output tensor([[0.3159],\n",
      "        [0.2782]], device='mps:0')\n",
      "Iteration 65330 Training loss 0.060711733996868134 Validation loss 0.05798137187957764 Accuracy 0.842875063419342\n",
      "Output tensor([[0.1199],\n",
      "        [0.2651]], device='mps:0')\n",
      "Iteration 65340 Training loss 0.049968574196100235 Validation loss 0.05805741250514984 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9943],\n",
      "        [0.4116]], device='mps:0')\n",
      "Iteration 65350 Training loss 0.05869420990347862 Validation loss 0.05799653381109238 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9723],\n",
      "        [0.1432]], device='mps:0')\n",
      "Iteration 65360 Training loss 0.051063764840364456 Validation loss 0.05797901749610901 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.4451],\n",
      "        [0.6122]], device='mps:0')\n",
      "Iteration 65370 Training loss 0.05570264533162117 Validation loss 0.057981740683317184 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9874],\n",
      "        [0.7008]], device='mps:0')\n",
      "Iteration 65380 Training loss 0.05236905440688133 Validation loss 0.0580655075609684 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.9575],\n",
      "        [0.4818]], device='mps:0')\n",
      "Iteration 65390 Training loss 0.06554494798183441 Validation loss 0.0579756498336792 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9944],\n",
      "        [0.9386]], device='mps:0')\n",
      "Iteration 65400 Training loss 0.060961008071899414 Validation loss 0.05797354131937027 Accuracy 0.843000054359436\n",
      "Output tensor([[0.6875],\n",
      "        [0.7605]], device='mps:0')\n",
      "Iteration 65410 Training loss 0.06118439510464668 Validation loss 0.057998012751340866 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0090],\n",
      "        [0.9164]], device='mps:0')\n",
      "Iteration 65420 Training loss 0.05241575092077255 Validation loss 0.057999156415462494 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9930],\n",
      "        [0.2386]], device='mps:0')\n",
      "Iteration 65430 Training loss 0.05200396105647087 Validation loss 0.05814184248447418 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0640],\n",
      "        [0.1544]], device='mps:0')\n",
      "Iteration 65440 Training loss 0.058503977954387665 Validation loss 0.05797870084643364 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0012],\n",
      "        [0.5842]], device='mps:0')\n",
      "Iteration 65450 Training loss 0.04548404738306999 Validation loss 0.057975269854068756 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9316],\n",
      "        [0.2032]], device='mps:0')\n",
      "Iteration 65460 Training loss 0.05527377873659134 Validation loss 0.05797823518514633 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.1993],\n",
      "        [0.7283]], device='mps:0')\n",
      "Iteration 65470 Training loss 0.059401124715805054 Validation loss 0.05796528235077858 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0994],\n",
      "        [0.0863]], device='mps:0')\n",
      "Iteration 65480 Training loss 0.05288675054907799 Validation loss 0.058094270527362823 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.7581],\n",
      "        [0.9102]], device='mps:0')\n",
      "Iteration 65490 Training loss 0.056277502328157425 Validation loss 0.05805492773652077 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9832],\n",
      "        [0.3026]], device='mps:0')\n",
      "Iteration 65500 Training loss 0.04749417304992676 Validation loss 0.057979945093393326 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9603],\n",
      "        [0.2556]], device='mps:0')\n",
      "Iteration 65510 Training loss 0.05170934647321701 Validation loss 0.05797595530748367 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9770],\n",
      "        [0.0938]], device='mps:0')\n",
      "Iteration 65520 Training loss 0.05251716822385788 Validation loss 0.058096956461668015 Accuracy 0.842875063419342\n",
      "Output tensor([[0.6876],\n",
      "        [0.9963]], device='mps:0')\n",
      "Iteration 65530 Training loss 0.054163359105587006 Validation loss 0.058005549013614655 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.8664],\n",
      "        [0.6470]], device='mps:0')\n",
      "Iteration 65540 Training loss 0.0551459975540638 Validation loss 0.05811986327171326 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.4907],\n",
      "        [0.0685]], device='mps:0')\n",
      "Iteration 65550 Training loss 0.0551070012152195 Validation loss 0.057963572442531586 Accuracy 0.843000054359436\n",
      "Output tensor([[0.2589],\n",
      "        [0.1298]], device='mps:0')\n",
      "Iteration 65560 Training loss 0.052639808505773544 Validation loss 0.058124564588069916 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.6260],\n",
      "        [0.9484]], device='mps:0')\n",
      "Iteration 65570 Training loss 0.0498022735118866 Validation loss 0.05796357989311218 Accuracy 0.842875063419342\n",
      "Output tensor([[0.7869],\n",
      "        [0.4266]], device='mps:0')\n",
      "Iteration 65580 Training loss 0.05634649470448494 Validation loss 0.05799261853098869 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.6867],\n",
      "        [0.8891]], device='mps:0')\n",
      "Iteration 65590 Training loss 0.06003290042281151 Validation loss 0.05799783021211624 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0984],\n",
      "        [0.8833]], device='mps:0')\n",
      "Iteration 65600 Training loss 0.06001174822449684 Validation loss 0.057975295931100845 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.6087],\n",
      "        [0.7991]], device='mps:0')\n",
      "Iteration 65610 Training loss 0.06546624004840851 Validation loss 0.05796446278691292 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.1267],\n",
      "        [0.2116]], device='mps:0')\n",
      "Iteration 65620 Training loss 0.04953083023428917 Validation loss 0.05803178623318672 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.8023],\n",
      "        [0.9259]], device='mps:0')\n",
      "Iteration 65630 Training loss 0.06456290185451508 Validation loss 0.057972878217697144 Accuracy 0.843000054359436\n",
      "Output tensor([[0.1050],\n",
      "        [0.8322]], device='mps:0')\n",
      "Iteration 65640 Training loss 0.05131997913122177 Validation loss 0.058014050126075745 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.1662],\n",
      "        [0.3773]], device='mps:0')\n",
      "Iteration 65650 Training loss 0.058167342096567154 Validation loss 0.05809934809803963 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9633],\n",
      "        [0.0576]], device='mps:0')\n",
      "Iteration 65660 Training loss 0.05305430293083191 Validation loss 0.0579800046980381 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.1947],\n",
      "        [0.9456]], device='mps:0')\n",
      "Iteration 65670 Training loss 0.06392230093479156 Validation loss 0.05798419192433357 Accuracy 0.843000054359436\n",
      "Output tensor([[0.1601],\n",
      "        [0.7833]], device='mps:0')\n",
      "Iteration 65680 Training loss 0.05319080129265785 Validation loss 0.057955704629421234 Accuracy 0.842875063419342\n",
      "Output tensor([[0.5910],\n",
      "        [0.8378]], device='mps:0')\n",
      "Iteration 65690 Training loss 0.06584092974662781 Validation loss 0.05795597657561302 Accuracy 0.842875063419342\n",
      "Output tensor([[0.4664],\n",
      "        [0.2933]], device='mps:0')\n",
      "Iteration 65700 Training loss 0.051869526505470276 Validation loss 0.05804627388715744 Accuracy 0.8405000567436218\n",
      "Output tensor([[0.9613],\n",
      "        [0.8103]], device='mps:0')\n",
      "Iteration 65710 Training loss 0.05295919254422188 Validation loss 0.05816002935171127 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.2521],\n",
      "        [0.0202]], device='mps:0')\n",
      "Iteration 65720 Training loss 0.04808920621871948 Validation loss 0.05803795903921127 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1714],\n",
      "        [0.7758]], device='mps:0')\n",
      "Iteration 65730 Training loss 0.06028503179550171 Validation loss 0.05796433985233307 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.1270],\n",
      "        [0.0838]], device='mps:0')\n",
      "Iteration 65740 Training loss 0.05324392020702362 Validation loss 0.05795019865036011 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9863],\n",
      "        [0.2989]], device='mps:0')\n",
      "Iteration 65750 Training loss 0.05530835688114166 Validation loss 0.0579475536942482 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9701],\n",
      "        [0.6840]], device='mps:0')\n",
      "Iteration 65760 Training loss 0.05617525056004524 Validation loss 0.057947009801864624 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9881],\n",
      "        [0.9479]], device='mps:0')\n",
      "Iteration 65770 Training loss 0.05485619604587555 Validation loss 0.05795785412192345 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9821],\n",
      "        [0.9730]], device='mps:0')\n",
      "Iteration 65780 Training loss 0.0508364662528038 Validation loss 0.05798537656664848 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0896],\n",
      "        [0.3149]], device='mps:0')\n",
      "Iteration 65790 Training loss 0.051173657178878784 Validation loss 0.05795399099588394 Accuracy 0.843500018119812\n",
      "Output tensor([[0.7181],\n",
      "        [0.0674]], device='mps:0')\n",
      "Iteration 65800 Training loss 0.05598960444331169 Validation loss 0.058141183108091354 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.1879],\n",
      "        [0.3574]], device='mps:0')\n",
      "Iteration 65810 Training loss 0.05244223773479462 Validation loss 0.05795241892337799 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.1295],\n",
      "        [0.1303]], device='mps:0')\n",
      "Iteration 65820 Training loss 0.057190027087926865 Validation loss 0.05801394581794739 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.0532],\n",
      "        [0.2155]], device='mps:0')\n",
      "Iteration 65830 Training loss 0.04566652327775955 Validation loss 0.05793123319745064 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0211],\n",
      "        [0.0713]], device='mps:0')\n",
      "Iteration 65840 Training loss 0.05877207964658737 Validation loss 0.057964831590652466 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.8727],\n",
      "        [0.9686]], device='mps:0')\n",
      "Iteration 65850 Training loss 0.04624707251787186 Validation loss 0.0581355057656765 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.9070],\n",
      "        [0.3003]], device='mps:0')\n",
      "Iteration 65860 Training loss 0.051634013652801514 Validation loss 0.057929955422878265 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9555],\n",
      "        [0.9922]], device='mps:0')\n",
      "Iteration 65870 Training loss 0.05328977480530739 Validation loss 0.05792040750384331 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0481],\n",
      "        [0.9643]], device='mps:0')\n",
      "Iteration 65880 Training loss 0.0539846234023571 Validation loss 0.05792807415127754 Accuracy 0.843000054359436\n",
      "Output tensor([[0.8956],\n",
      "        [0.9253]], device='mps:0')\n",
      "Iteration 65890 Training loss 0.05709538236260414 Validation loss 0.05792481079697609 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9891],\n",
      "        [0.4047]], device='mps:0')\n",
      "Iteration 65900 Training loss 0.04976801946759224 Validation loss 0.05792221054434776 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.4219],\n",
      "        [0.0642]], device='mps:0')\n",
      "Iteration 65910 Training loss 0.05643795058131218 Validation loss 0.05798342451453209 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.8841],\n",
      "        [0.9317]], device='mps:0')\n",
      "Iteration 65920 Training loss 0.05457806587219238 Validation loss 0.05794409289956093 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.2733],\n",
      "        [0.9834]], device='mps:0')\n",
      "Iteration 65930 Training loss 0.056780893355607986 Validation loss 0.05793263018131256 Accuracy 0.843000054359436\n",
      "Output tensor([[0.1428],\n",
      "        [0.8573]], device='mps:0')\n",
      "Iteration 65940 Training loss 0.04879242926836014 Validation loss 0.058007724583148956 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9214],\n",
      "        [0.0377]], device='mps:0')\n",
      "Iteration 65950 Training loss 0.04428158327937126 Validation loss 0.057924944907426834 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.1666],\n",
      "        [0.8231]], device='mps:0')\n",
      "Iteration 65960 Training loss 0.05516902357339859 Validation loss 0.05792911723256111 Accuracy 0.843250036239624\n",
      "Output tensor([[0.1867],\n",
      "        [0.3819]], device='mps:0')\n",
      "Iteration 65970 Training loss 0.059915702790021896 Validation loss 0.05795877426862717 Accuracy 0.843500018119812\n",
      "Output tensor([[0.1245],\n",
      "        [0.1084]], device='mps:0')\n",
      "Iteration 65980 Training loss 0.05767589434981346 Validation loss 0.05802128091454506 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.0194],\n",
      "        [0.0387]], device='mps:0')\n",
      "Iteration 65990 Training loss 0.05503823608160019 Validation loss 0.05798137187957764 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.0317],\n",
      "        [0.9046]], device='mps:0')\n",
      "Iteration 66000 Training loss 0.05061047896742821 Validation loss 0.05794011801481247 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0418],\n",
      "        [0.9478]], device='mps:0')\n",
      "Iteration 66010 Training loss 0.05973922461271286 Validation loss 0.0579657107591629 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9720],\n",
      "        [0.2426]], device='mps:0')\n",
      "Iteration 66020 Training loss 0.05890721082687378 Validation loss 0.057963717728853226 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.4057],\n",
      "        [0.9206]], device='mps:0')\n",
      "Iteration 66030 Training loss 0.05399784818291664 Validation loss 0.057932429015636444 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.2485],\n",
      "        [0.2084]], device='mps:0')\n",
      "Iteration 66040 Training loss 0.055802278220653534 Validation loss 0.05807789787650108 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.8619],\n",
      "        [0.0184]], device='mps:0')\n",
      "Iteration 66050 Training loss 0.051510196179151535 Validation loss 0.05793841928243637 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9863],\n",
      "        [0.0830]], device='mps:0')\n",
      "Iteration 66060 Training loss 0.05814482644200325 Validation loss 0.05798087641596794 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.1246],\n",
      "        [0.7772]], device='mps:0')\n",
      "Iteration 66070 Training loss 0.053952064365148544 Validation loss 0.05793410912156105 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.2830],\n",
      "        [0.8233]], device='mps:0')\n",
      "Iteration 66080 Training loss 0.05909867212176323 Validation loss 0.05805524066090584 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.6497],\n",
      "        [0.0594]], device='mps:0')\n",
      "Iteration 66090 Training loss 0.05845585837960243 Validation loss 0.05793151631951332 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.4695],\n",
      "        [0.1863]], device='mps:0')\n",
      "Iteration 66100 Training loss 0.0472993366420269 Validation loss 0.05812937021255493 Accuracy 0.8403750658035278\n",
      "Output tensor([[0.0533],\n",
      "        [0.9456]], device='mps:0')\n",
      "Iteration 66110 Training loss 0.04568528011441231 Validation loss 0.05792679637670517 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.6486],\n",
      "        [0.8017]], device='mps:0')\n",
      "Iteration 66120 Training loss 0.047445524483919144 Validation loss 0.05793911591172218 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.1134],\n",
      "        [0.0928]], device='mps:0')\n",
      "Iteration 66130 Training loss 0.0607992522418499 Validation loss 0.058038219809532166 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9549],\n",
      "        [0.8167]], device='mps:0')\n",
      "Iteration 66140 Training loss 0.06284617632627487 Validation loss 0.0579657182097435 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0459],\n",
      "        [0.9314]], device='mps:0')\n",
      "Iteration 66150 Training loss 0.056739553809165955 Validation loss 0.05803301930427551 Accuracy 0.84312504529953\n",
      "Output tensor([[0.7455],\n",
      "        [0.1129]], device='mps:0')\n",
      "Iteration 66160 Training loss 0.04817803204059601 Validation loss 0.05796494707465172 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0651],\n",
      "        [0.0670]], device='mps:0')\n",
      "Iteration 66170 Training loss 0.062321946024894714 Validation loss 0.05794838070869446 Accuracy 0.843500018119812\n",
      "Output tensor([[0.2296],\n",
      "        [0.9455]], device='mps:0')\n",
      "Iteration 66180 Training loss 0.05602122098207474 Validation loss 0.05800492316484451 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9628],\n",
      "        [0.0353]], device='mps:0')\n",
      "Iteration 66190 Training loss 0.051963042467832565 Validation loss 0.0579327717423439 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.0660],\n",
      "        [0.8208]], device='mps:0')\n",
      "Iteration 66200 Training loss 0.05317063629627228 Validation loss 0.05800306051969528 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9826],\n",
      "        [0.0630]], device='mps:0')\n",
      "Iteration 66210 Training loss 0.06360385566949844 Validation loss 0.05795079469680786 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.0132],\n",
      "        [0.9737]], device='mps:0')\n",
      "Iteration 66220 Training loss 0.057787712663412094 Validation loss 0.057967107743024826 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9804],\n",
      "        [0.8455]], device='mps:0')\n",
      "Iteration 66230 Training loss 0.05065145716071129 Validation loss 0.058361537754535675 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.0528],\n",
      "        [0.3092]], device='mps:0')\n",
      "Iteration 66240 Training loss 0.056672751903533936 Validation loss 0.05813407525420189 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.2061],\n",
      "        [0.1192]], device='mps:0')\n",
      "Iteration 66250 Training loss 0.05656881257891655 Validation loss 0.05797021463513374 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.1413],\n",
      "        [0.0443]], device='mps:0')\n",
      "Iteration 66260 Training loss 0.050809700042009354 Validation loss 0.05791807919740677 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9348],\n",
      "        [0.7868]], device='mps:0')\n",
      "Iteration 66270 Training loss 0.05691458657383919 Validation loss 0.05795181542634964 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9182],\n",
      "        [0.8462]], device='mps:0')\n",
      "Iteration 66280 Training loss 0.052918851375579834 Validation loss 0.05791537091135979 Accuracy 0.842875063419342\n",
      "Output tensor([[0.8069],\n",
      "        [0.9699]], device='mps:0')\n",
      "Iteration 66290 Training loss 0.06426966190338135 Validation loss 0.057922374457120895 Accuracy 0.843000054359436\n",
      "Output tensor([[0.6992],\n",
      "        [0.9915]], device='mps:0')\n",
      "Iteration 66300 Training loss 0.05604953318834305 Validation loss 0.057978127151727676 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.7713],\n",
      "        [0.9676]], device='mps:0')\n",
      "Iteration 66310 Training loss 0.050302933901548386 Validation loss 0.05805893987417221 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9436],\n",
      "        [0.9511]], device='mps:0')\n",
      "Iteration 66320 Training loss 0.05793105810880661 Validation loss 0.058051303029060364 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0807],\n",
      "        [0.0251]], device='mps:0')\n",
      "Iteration 66330 Training loss 0.053278848528862 Validation loss 0.05794227868318558 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.1357],\n",
      "        [0.9635]], device='mps:0')\n",
      "Iteration 66340 Training loss 0.05372167378664017 Validation loss 0.057948749512434006 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.7090],\n",
      "        [0.2047]], device='mps:0')\n",
      "Iteration 66350 Training loss 0.05698889493942261 Validation loss 0.05798810347914696 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7082],\n",
      "        [0.3628]], device='mps:0')\n",
      "Iteration 66360 Training loss 0.04720580577850342 Validation loss 0.05802690237760544 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9865],\n",
      "        [0.2182]], device='mps:0')\n",
      "Iteration 66370 Training loss 0.04571330547332764 Validation loss 0.05792153999209404 Accuracy 0.843375027179718\n",
      "Output tensor([[0.7859],\n",
      "        [0.2711]], device='mps:0')\n",
      "Iteration 66380 Training loss 0.05380428582429886 Validation loss 0.05791720002889633 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0716],\n",
      "        [0.5763]], device='mps:0')\n",
      "Iteration 66390 Training loss 0.05441662669181824 Validation loss 0.05799827352166176 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9959],\n",
      "        [0.2712]], device='mps:0')\n",
      "Iteration 66400 Training loss 0.05801621451973915 Validation loss 0.057966846972703934 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.7916],\n",
      "        [0.0750]], device='mps:0')\n",
      "Iteration 66410 Training loss 0.05307312682271004 Validation loss 0.0579831637442112 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9401],\n",
      "        [0.9974]], device='mps:0')\n",
      "Iteration 66420 Training loss 0.057561397552490234 Validation loss 0.058026716113090515 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9918],\n",
      "        [0.1242]], device='mps:0')\n",
      "Iteration 66430 Training loss 0.052473220974206924 Validation loss 0.05791578069329262 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0100],\n",
      "        [0.0458]], device='mps:0')\n",
      "Iteration 66440 Training loss 0.053729843348264694 Validation loss 0.057917553931474686 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1964],\n",
      "        [0.4916]], device='mps:0')\n",
      "Iteration 66450 Training loss 0.06154102832078934 Validation loss 0.05791559815406799 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.1099],\n",
      "        [0.9330]], device='mps:0')\n",
      "Iteration 66460 Training loss 0.04990001022815704 Validation loss 0.057951297610998154 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.4262],\n",
      "        [0.1338]], device='mps:0')\n",
      "Iteration 66470 Training loss 0.06284652650356293 Validation loss 0.057920560240745544 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0106],\n",
      "        [0.9646]], device='mps:0')\n",
      "Iteration 66480 Training loss 0.0521744042634964 Validation loss 0.058042071759700775 Accuracy 0.84312504529953\n",
      "Output tensor([[0.3008],\n",
      "        [0.3397]], device='mps:0')\n",
      "Iteration 66490 Training loss 0.045371659100055695 Validation loss 0.05795694887638092 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9478],\n",
      "        [0.0833]], device='mps:0')\n",
      "Iteration 66500 Training loss 0.0485934242606163 Validation loss 0.05793343484401703 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.6798],\n",
      "        [0.9707]], device='mps:0')\n",
      "Iteration 66510 Training loss 0.06013242527842522 Validation loss 0.05790834501385689 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.8732],\n",
      "        [0.1324]], device='mps:0')\n",
      "Iteration 66520 Training loss 0.056704871356487274 Validation loss 0.057912081480026245 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9226],\n",
      "        [0.0828]], device='mps:0')\n",
      "Iteration 66530 Training loss 0.04922536760568619 Validation loss 0.05819173529744148 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.1104],\n",
      "        [0.6137]], device='mps:0')\n",
      "Iteration 66540 Training loss 0.056992996484041214 Validation loss 0.0579201877117157 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.6126],\n",
      "        [0.9936]], device='mps:0')\n",
      "Iteration 66550 Training loss 0.056514181196689606 Validation loss 0.05793212354183197 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.5590],\n",
      "        [0.0871]], device='mps:0')\n",
      "Iteration 66560 Training loss 0.05462353676557541 Validation loss 0.05791022628545761 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0871],\n",
      "        [0.0942]], device='mps:0')\n",
      "Iteration 66570 Training loss 0.05428522825241089 Validation loss 0.058027852326631546 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9367],\n",
      "        [0.6125]], device='mps:0')\n",
      "Iteration 66580 Training loss 0.057647425681352615 Validation loss 0.058202069252729416 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.5052],\n",
      "        [0.6839]], device='mps:0')\n",
      "Iteration 66590 Training loss 0.051580242812633514 Validation loss 0.05820297822356224 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.9209],\n",
      "        [0.5321]], device='mps:0')\n",
      "Iteration 66600 Training loss 0.053457338362932205 Validation loss 0.05818026140332222 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0816],\n",
      "        [0.7636]], device='mps:0')\n",
      "Iteration 66610 Training loss 0.06206652522087097 Validation loss 0.05792712792754173 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0493],\n",
      "        [0.3756]], device='mps:0')\n",
      "Iteration 66620 Training loss 0.052522361278533936 Validation loss 0.057942867279052734 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0342],\n",
      "        [0.9769]], device='mps:0')\n",
      "Iteration 66630 Training loss 0.05310303717851639 Validation loss 0.05797422304749489 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.1944],\n",
      "        [0.0253]], device='mps:0')\n",
      "Iteration 66640 Training loss 0.06377553939819336 Validation loss 0.05791177973151207 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9945],\n",
      "        [0.9674]], device='mps:0')\n",
      "Iteration 66650 Training loss 0.054758470505476 Validation loss 0.057913850992918015 Accuracy 0.843000054359436\n",
      "Output tensor([[0.7444],\n",
      "        [0.0416]], device='mps:0')\n",
      "Iteration 66660 Training loss 0.06186849623918533 Validation loss 0.05790986120700836 Accuracy 0.842875063419342\n",
      "Output tensor([[0.2870],\n",
      "        [0.1525]], device='mps:0')\n",
      "Iteration 66670 Training loss 0.05105052888393402 Validation loss 0.057910531759262085 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0103],\n",
      "        [0.0688]], device='mps:0')\n",
      "Iteration 66680 Training loss 0.050395019352436066 Validation loss 0.05790435150265694 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.1675],\n",
      "        [0.9935]], device='mps:0')\n",
      "Iteration 66690 Training loss 0.05708348751068115 Validation loss 0.05802326649427414 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9280],\n",
      "        [0.9281]], device='mps:0')\n",
      "Iteration 66700 Training loss 0.049681633710861206 Validation loss 0.05798668786883354 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.8364],\n",
      "        [0.8945]], device='mps:0')\n",
      "Iteration 66710 Training loss 0.05101988837122917 Validation loss 0.05822145193815231 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.3529],\n",
      "        [0.9048]], device='mps:0')\n",
      "Iteration 66720 Training loss 0.05720321834087372 Validation loss 0.058108750730752945 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.8871],\n",
      "        [0.1035]], device='mps:0')\n",
      "Iteration 66730 Training loss 0.06012387201189995 Validation loss 0.05798057094216347 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9608],\n",
      "        [0.7576]], device='mps:0')\n",
      "Iteration 66740 Training loss 0.050818633288145065 Validation loss 0.05793272331357002 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.5212],\n",
      "        [0.9582]], device='mps:0')\n",
      "Iteration 66750 Training loss 0.06163328140974045 Validation loss 0.05790809169411659 Accuracy 0.84312504529953\n",
      "Output tensor([[0.8208],\n",
      "        [0.1547]], device='mps:0')\n",
      "Iteration 66760 Training loss 0.053372230380773544 Validation loss 0.05816030502319336 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.8812],\n",
      "        [0.1022]], device='mps:0')\n",
      "Iteration 66770 Training loss 0.05345437303185463 Validation loss 0.0579269677400589 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.8607],\n",
      "        [0.1062]], device='mps:0')\n",
      "Iteration 66780 Training loss 0.0579141229391098 Validation loss 0.05824074521660805 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.0108],\n",
      "        [0.4462]], device='mps:0')\n",
      "Iteration 66790 Training loss 0.05777176842093468 Validation loss 0.058057285845279694 Accuracy 0.8406250476837158\n",
      "Output tensor([[0.0602],\n",
      "        [0.6749]], device='mps:0')\n",
      "Iteration 66800 Training loss 0.056450728327035904 Validation loss 0.05795688554644585 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.7379],\n",
      "        [0.4875]], device='mps:0')\n",
      "Iteration 66810 Training loss 0.06335064023733139 Validation loss 0.05790586397051811 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0189],\n",
      "        [0.3882]], device='mps:0')\n",
      "Iteration 66820 Training loss 0.05725666880607605 Validation loss 0.05789679288864136 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.7258],\n",
      "        [0.0620]], device='mps:0')\n",
      "Iteration 66830 Training loss 0.0608963780105114 Validation loss 0.057892173528671265 Accuracy 0.843500018119812\n",
      "Output tensor([[0.2050],\n",
      "        [0.6568]], device='mps:0')\n",
      "Iteration 66840 Training loss 0.04837583750486374 Validation loss 0.057972658425569534 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9992],\n",
      "        [0.2105]], device='mps:0')\n",
      "Iteration 66850 Training loss 0.05086621269583702 Validation loss 0.05790020897984505 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9273],\n",
      "        [0.5150]], device='mps:0')\n",
      "Iteration 66860 Training loss 0.05076555907726288 Validation loss 0.057907771319150925 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.8969],\n",
      "        [0.8225]], device='mps:0')\n",
      "Iteration 66870 Training loss 0.06235890090465546 Validation loss 0.05791641026735306 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.4398],\n",
      "        [0.6562]], device='mps:0')\n",
      "Iteration 66880 Training loss 0.05867277830839157 Validation loss 0.057909730821847916 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9398],\n",
      "        [0.1225]], device='mps:0')\n",
      "Iteration 66890 Training loss 0.0545637346804142 Validation loss 0.05791272222995758 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.4470],\n",
      "        [0.9333]], device='mps:0')\n",
      "Iteration 66900 Training loss 0.0490831695497036 Validation loss 0.05793919041752815 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.0161],\n",
      "        [0.6116]], device='mps:0')\n",
      "Iteration 66910 Training loss 0.04911913722753525 Validation loss 0.05790838599205017 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9143],\n",
      "        [0.1195]], device='mps:0')\n",
      "Iteration 66920 Training loss 0.054959896951913834 Validation loss 0.05792262405157089 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.8047],\n",
      "        [0.1221]], device='mps:0')\n",
      "Iteration 66930 Training loss 0.044396400451660156 Validation loss 0.057912494987249374 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.3253],\n",
      "        [0.2825]], device='mps:0')\n",
      "Iteration 66940 Training loss 0.058948416262865067 Validation loss 0.0581054650247097 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0061],\n",
      "        [0.9706]], device='mps:0')\n",
      "Iteration 66950 Training loss 0.058541376143693924 Validation loss 0.05796460062265396 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9861],\n",
      "        [0.6722]], device='mps:0')\n",
      "Iteration 66960 Training loss 0.05571262538433075 Validation loss 0.05791427940130234 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.5715],\n",
      "        [0.0065]], device='mps:0')\n",
      "Iteration 66970 Training loss 0.053453244268894196 Validation loss 0.05799645557999611 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0186],\n",
      "        [0.9178]], device='mps:0')\n",
      "Iteration 66980 Training loss 0.05368700996041298 Validation loss 0.05804867297410965 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0121],\n",
      "        [0.0213]], device='mps:0')\n",
      "Iteration 66990 Training loss 0.06186103820800781 Validation loss 0.058201611042022705 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.8659],\n",
      "        [0.6652]], device='mps:0')\n",
      "Iteration 67000 Training loss 0.05508596450090408 Validation loss 0.057916685938835144 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.6384],\n",
      "        [0.4999]], device='mps:0')\n",
      "Iteration 67010 Training loss 0.05121026933193207 Validation loss 0.05790473520755768 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0126],\n",
      "        [0.3617]], device='mps:0')\n",
      "Iteration 67020 Training loss 0.05661337077617645 Validation loss 0.05793197825551033 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.3373],\n",
      "        [0.9936]], device='mps:0')\n",
      "Iteration 67030 Training loss 0.05705394223332405 Validation loss 0.05790874361991882 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9903],\n",
      "        [0.0547]], device='mps:0')\n",
      "Iteration 67040 Training loss 0.045979589223861694 Validation loss 0.05789337307214737 Accuracy 0.84312504529953\n",
      "Output tensor([[0.7473],\n",
      "        [0.5919]], device='mps:0')\n",
      "Iteration 67050 Training loss 0.057053092867136 Validation loss 0.057902757078409195 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.2875],\n",
      "        [0.0080]], device='mps:0')\n",
      "Iteration 67060 Training loss 0.06351428478956223 Validation loss 0.057991717010736465 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9681],\n",
      "        [0.9302]], device='mps:0')\n",
      "Iteration 67070 Training loss 0.05907658487558365 Validation loss 0.05789951607584953 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.2098],\n",
      "        [0.1529]], device='mps:0')\n",
      "Iteration 67080 Training loss 0.05857827141880989 Validation loss 0.05798286572098732 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.3344],\n",
      "        [0.0706]], device='mps:0')\n",
      "Iteration 67090 Training loss 0.05572238937020302 Validation loss 0.05801839753985405 Accuracy 0.843250036239624\n",
      "Output tensor([[0.1518],\n",
      "        [0.1514]], device='mps:0')\n",
      "Iteration 67100 Training loss 0.05369196832180023 Validation loss 0.05810662731528282 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.6819],\n",
      "        [0.0728]], device='mps:0')\n",
      "Iteration 67110 Training loss 0.06578630208969116 Validation loss 0.05789326876401901 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.1487],\n",
      "        [0.0255]], device='mps:0')\n",
      "Iteration 67120 Training loss 0.05993465334177017 Validation loss 0.05814746022224426 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.1272],\n",
      "        [0.4859]], device='mps:0')\n",
      "Iteration 67130 Training loss 0.04806624352931976 Validation loss 0.05789089947938919 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9569],\n",
      "        [0.0040]], device='mps:0')\n",
      "Iteration 67140 Training loss 0.058164697140455246 Validation loss 0.05802271142601967 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0118],\n",
      "        [0.2250]], device='mps:0')\n",
      "Iteration 67150 Training loss 0.055290672928094864 Validation loss 0.05793475732207298 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.8982],\n",
      "        [0.0248]], device='mps:0')\n",
      "Iteration 67160 Training loss 0.0635349377989769 Validation loss 0.05790582299232483 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0656],\n",
      "        [0.8115]], device='mps:0')\n",
      "Iteration 67170 Training loss 0.05144403502345085 Validation loss 0.057885363698005676 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.7647],\n",
      "        [0.4770]], device='mps:0')\n",
      "Iteration 67180 Training loss 0.05541924387216568 Validation loss 0.057890817523002625 Accuracy 0.843000054359436\n",
      "Output tensor([[0.7759],\n",
      "        [0.4497]], device='mps:0')\n",
      "Iteration 67190 Training loss 0.05299392715096474 Validation loss 0.05811450630426407 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9766],\n",
      "        [0.9248]], device='mps:0')\n",
      "Iteration 67200 Training loss 0.06271716952323914 Validation loss 0.057897984981536865 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.2936],\n",
      "        [0.9899]], device='mps:0')\n",
      "Iteration 67210 Training loss 0.06304414570331573 Validation loss 0.05791187658905983 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.8578],\n",
      "        [0.9808]], device='mps:0')\n",
      "Iteration 67220 Training loss 0.04877568036317825 Validation loss 0.05790665000677109 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.3499],\n",
      "        [0.9924]], device='mps:0')\n",
      "Iteration 67230 Training loss 0.05263961851596832 Validation loss 0.05791214108467102 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.2289],\n",
      "        [0.9951]], device='mps:0')\n",
      "Iteration 67240 Training loss 0.05881905555725098 Validation loss 0.05787837505340576 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0673],\n",
      "        [0.2982]], device='mps:0')\n",
      "Iteration 67250 Training loss 0.056786343455314636 Validation loss 0.05802956223487854 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.2269],\n",
      "        [0.9327]], device='mps:0')\n",
      "Iteration 67260 Training loss 0.05561009421944618 Validation loss 0.057898979634046555 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.4305],\n",
      "        [0.2019]], device='mps:0')\n",
      "Iteration 67270 Training loss 0.05149013549089432 Validation loss 0.057879723608493805 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.3391],\n",
      "        [0.9496]], device='mps:0')\n",
      "Iteration 67280 Training loss 0.05958939716219902 Validation loss 0.057880278676748276 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.8554],\n",
      "        [0.9678]], device='mps:0')\n",
      "Iteration 67290 Training loss 0.048941973596811295 Validation loss 0.05791513994336128 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.8451],\n",
      "        [0.9168]], device='mps:0')\n",
      "Iteration 67300 Training loss 0.04546411707997322 Validation loss 0.05789206922054291 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0433],\n",
      "        [0.0520]], device='mps:0')\n",
      "Iteration 67310 Training loss 0.05673283338546753 Validation loss 0.05791999772191048 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.6153],\n",
      "        [0.1091]], device='mps:0')\n",
      "Iteration 67320 Training loss 0.048468831926584244 Validation loss 0.05794684216380119 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.8319],\n",
      "        [0.9147]], device='mps:0')\n",
      "Iteration 67330 Training loss 0.05651002377271652 Validation loss 0.05790335312485695 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.6169],\n",
      "        [0.7697]], device='mps:0')\n",
      "Iteration 67340 Training loss 0.05721932649612427 Validation loss 0.05789193883538246 Accuracy 0.843500018119812\n",
      "Output tensor([[0.8683],\n",
      "        [0.9319]], device='mps:0')\n",
      "Iteration 67350 Training loss 0.06278868764638901 Validation loss 0.0580185204744339 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.3602],\n",
      "        [0.6744]], device='mps:0')\n",
      "Iteration 67360 Training loss 0.0510779544711113 Validation loss 0.057879697531461716 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9838],\n",
      "        [0.8881]], device='mps:0')\n",
      "Iteration 67370 Training loss 0.05671009048819542 Validation loss 0.058115195482969284 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.0394],\n",
      "        [0.1038]], device='mps:0')\n",
      "Iteration 67380 Training loss 0.050566453486680984 Validation loss 0.057876281440258026 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9807],\n",
      "        [0.7866]], device='mps:0')\n",
      "Iteration 67390 Training loss 0.056694649159908295 Validation loss 0.05787273496389389 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9972],\n",
      "        [0.1447]], device='mps:0')\n",
      "Iteration 67400 Training loss 0.05470062047243118 Validation loss 0.057880330830812454 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.4389],\n",
      "        [0.8953]], device='mps:0')\n",
      "Iteration 67410 Training loss 0.054796524345874786 Validation loss 0.05787114053964615 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.6000],\n",
      "        [0.1549]], device='mps:0')\n",
      "Iteration 67420 Training loss 0.05789773911237717 Validation loss 0.05788564682006836 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.1990],\n",
      "        [0.5539]], device='mps:0')\n",
      "Iteration 67430 Training loss 0.05725594237446785 Validation loss 0.05790427699685097 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.1355],\n",
      "        [0.0138]], device='mps:0')\n",
      "Iteration 67440 Training loss 0.0475304052233696 Validation loss 0.05797104164958 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.4892],\n",
      "        [0.9499]], device='mps:0')\n",
      "Iteration 67450 Training loss 0.06173640117049217 Validation loss 0.05788993462920189 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.1937],\n",
      "        [0.5328]], device='mps:0')\n",
      "Iteration 67460 Training loss 0.04978154972195625 Validation loss 0.05787042900919914 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9382],\n",
      "        [0.2733]], device='mps:0')\n",
      "Iteration 67470 Training loss 0.04811973497271538 Validation loss 0.05807122215628624 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.8970],\n",
      "        [0.9047]], device='mps:0')\n",
      "Iteration 67480 Training loss 0.05291115492582321 Validation loss 0.057860422879457474 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.6615],\n",
      "        [0.6463]], device='mps:0')\n",
      "Iteration 67490 Training loss 0.0549631267786026 Validation loss 0.05785899609327316 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7467],\n",
      "        [0.0402]], device='mps:0')\n",
      "Iteration 67500 Training loss 0.04990777000784874 Validation loss 0.05786069110035896 Accuracy 0.843500018119812\n",
      "Output tensor([[0.5278],\n",
      "        [0.4157]], device='mps:0')\n",
      "Iteration 67510 Training loss 0.04898602515459061 Validation loss 0.057994294911623 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.7575],\n",
      "        [0.2714]], device='mps:0')\n",
      "Iteration 67520 Training loss 0.05974379554390907 Validation loss 0.05808497220277786 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.8664],\n",
      "        [0.1691]], device='mps:0')\n",
      "Iteration 67530 Training loss 0.06016610190272331 Validation loss 0.057907938957214355 Accuracy 0.843000054359436\n",
      "Output tensor([[0.8279],\n",
      "        [0.9347]], device='mps:0')\n",
      "Iteration 67540 Training loss 0.05900821089744568 Validation loss 0.05787484347820282 Accuracy 0.842875063419342\n",
      "Output tensor([[0.7620],\n",
      "        [0.8152]], device='mps:0')\n",
      "Iteration 67550 Training loss 0.056104857474565506 Validation loss 0.05789034441113472 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.1831],\n",
      "        [0.9667]], device='mps:0')\n",
      "Iteration 67560 Training loss 0.06324999034404755 Validation loss 0.05789336562156677 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.5936],\n",
      "        [0.0330]], device='mps:0')\n",
      "Iteration 67570 Training loss 0.056393079459667206 Validation loss 0.05787675827741623 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9923],\n",
      "        [0.9339]], device='mps:0')\n",
      "Iteration 67580 Training loss 0.05527675151824951 Validation loss 0.057893212884664536 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.3080],\n",
      "        [0.9465]], device='mps:0')\n",
      "Iteration 67590 Training loss 0.05098305270075798 Validation loss 0.05786886438727379 Accuracy 0.843500018119812\n",
      "Output tensor([[0.5746],\n",
      "        [0.3268]], device='mps:0')\n",
      "Iteration 67600 Training loss 0.054650742560625076 Validation loss 0.057909198105335236 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.3702],\n",
      "        [0.0459]], device='mps:0')\n",
      "Iteration 67610 Training loss 0.05459374934434891 Validation loss 0.057858292013406754 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9189],\n",
      "        [0.9360]], device='mps:0')\n",
      "Iteration 67620 Training loss 0.058435481041669846 Validation loss 0.057967495173215866 Accuracy 0.84312504529953\n",
      "Output tensor([[0.2376],\n",
      "        [0.7777]], device='mps:0')\n",
      "Iteration 67630 Training loss 0.06791903078556061 Validation loss 0.05785751715302467 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9685],\n",
      "        [0.9770]], device='mps:0')\n",
      "Iteration 67640 Training loss 0.05117771029472351 Validation loss 0.057924844324588776 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.4567],\n",
      "        [0.3309]], device='mps:0')\n",
      "Iteration 67650 Training loss 0.05925802141427994 Validation loss 0.057884830981492996 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0458],\n",
      "        [0.2068]], device='mps:0')\n",
      "Iteration 67660 Training loss 0.05020583048462868 Validation loss 0.05800359696149826 Accuracy 0.843000054359436\n",
      "Output tensor([[0.2020],\n",
      "        [0.4759]], device='mps:0')\n",
      "Iteration 67670 Training loss 0.048193998634815216 Validation loss 0.057902734726667404 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.8377],\n",
      "        [0.0285]], device='mps:0')\n",
      "Iteration 67680 Training loss 0.05742761492729187 Validation loss 0.057907458394765854 Accuracy 0.843375027179718\n",
      "Output tensor([[0.2326],\n",
      "        [0.9371]], device='mps:0')\n",
      "Iteration 67690 Training loss 0.05578970909118652 Validation loss 0.05786244943737984 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9965],\n",
      "        [0.9843]], device='mps:0')\n",
      "Iteration 67700 Training loss 0.05152716860175133 Validation loss 0.05798369273543358 Accuracy 0.842875063419342\n",
      "Output tensor([[0.8659],\n",
      "        [0.5142]], device='mps:0')\n",
      "Iteration 67710 Training loss 0.04700224846601486 Validation loss 0.05786183476448059 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.0600],\n",
      "        [0.9625]], device='mps:0')\n",
      "Iteration 67720 Training loss 0.05778765678405762 Validation loss 0.05785156413912773 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9244],\n",
      "        [0.0815]], device='mps:0')\n",
      "Iteration 67730 Training loss 0.05156087130308151 Validation loss 0.057901687920093536 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.4061],\n",
      "        [0.6147]], device='mps:0')\n",
      "Iteration 67740 Training loss 0.06354527920484543 Validation loss 0.05791577324271202 Accuracy 0.84312504529953\n",
      "Output tensor([[0.2978],\n",
      "        [0.9413]], device='mps:0')\n",
      "Iteration 67750 Training loss 0.05677928775548935 Validation loss 0.057893235236406326 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0486],\n",
      "        [0.5321]], device='mps:0')\n",
      "Iteration 67760 Training loss 0.05581759288907051 Validation loss 0.05785540118813515 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0550],\n",
      "        [0.1818]], device='mps:0')\n",
      "Iteration 67770 Training loss 0.0546349436044693 Validation loss 0.05788158252835274 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.3988],\n",
      "        [0.0649]], device='mps:0')\n",
      "Iteration 67780 Training loss 0.06348306685686111 Validation loss 0.057847484946250916 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.2344],\n",
      "        [0.0117]], device='mps:0')\n",
      "Iteration 67790 Training loss 0.048749856650829315 Validation loss 0.05786693096160889 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0811],\n",
      "        [0.6336]], device='mps:0')\n",
      "Iteration 67800 Training loss 0.052008941769599915 Validation loss 0.057877857238054276 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9876],\n",
      "        [0.1340]], device='mps:0')\n",
      "Iteration 67810 Training loss 0.05174383893609047 Validation loss 0.05785062536597252 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.6382],\n",
      "        [0.7840]], device='mps:0')\n",
      "Iteration 67820 Training loss 0.058001305907964706 Validation loss 0.05786296725273132 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.4453],\n",
      "        [0.9528]], device='mps:0')\n",
      "Iteration 67830 Training loss 0.06190715730190277 Validation loss 0.05785627290606499 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0651],\n",
      "        [0.0087]], device='mps:0')\n",
      "Iteration 67840 Training loss 0.04317432641983032 Validation loss 0.05806875228881836 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0958],\n",
      "        [0.0849]], device='mps:0')\n",
      "Iteration 67850 Training loss 0.052546367049217224 Validation loss 0.05790472775697708 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9678],\n",
      "        [0.2145]], device='mps:0')\n",
      "Iteration 67860 Training loss 0.061442602425813675 Validation loss 0.05784143507480621 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0989],\n",
      "        [0.8086]], device='mps:0')\n",
      "Iteration 67870 Training loss 0.05530381202697754 Validation loss 0.057885296642780304 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.0834],\n",
      "        [0.9801]], device='mps:0')\n",
      "Iteration 67880 Training loss 0.053880684077739716 Validation loss 0.05783950537443161 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.1333],\n",
      "        [0.9279]], device='mps:0')\n",
      "Iteration 67890 Training loss 0.054969944059848785 Validation loss 0.05783490091562271 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7266],\n",
      "        [0.1344]], device='mps:0')\n",
      "Iteration 67900 Training loss 0.052601948380470276 Validation loss 0.057839296758174896 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.6911],\n",
      "        [0.4073]], device='mps:0')\n",
      "Iteration 67910 Training loss 0.052498310804367065 Validation loss 0.057857491075992584 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.8394],\n",
      "        [0.7953]], device='mps:0')\n",
      "Iteration 67920 Training loss 0.05559169128537178 Validation loss 0.057835016399621964 Accuracy 0.843000054359436\n",
      "Output tensor([[0.3312],\n",
      "        [0.1022]], device='mps:0')\n",
      "Iteration 67930 Training loss 0.058285344392061234 Validation loss 0.05784047394990921 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.7535],\n",
      "        [0.1806]], device='mps:0')\n",
      "Iteration 67940 Training loss 0.05463434010744095 Validation loss 0.05782943218946457 Accuracy 0.843500018119812\n",
      "Output tensor([[0.2220],\n",
      "        [0.8703]], device='mps:0')\n",
      "Iteration 67950 Training loss 0.055797357112169266 Validation loss 0.05782972648739815 Accuracy 0.843375027179718\n",
      "Output tensor([[0.2052],\n",
      "        [0.7715]], device='mps:0')\n",
      "Iteration 67960 Training loss 0.050452470779418945 Validation loss 0.05801474303007126 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0035],\n",
      "        [0.0340]], device='mps:0')\n",
      "Iteration 67970 Training loss 0.04936230555176735 Validation loss 0.058056216686964035 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.7878],\n",
      "        [0.6231]], device='mps:0')\n",
      "Iteration 67980 Training loss 0.05168358236551285 Validation loss 0.057905733585357666 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9914],\n",
      "        [0.0420]], device='mps:0')\n",
      "Iteration 67990 Training loss 0.043974120169878006 Validation loss 0.05790991336107254 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.7194],\n",
      "        [0.2772]], device='mps:0')\n",
      "Iteration 68000 Training loss 0.053961895406246185 Validation loss 0.057841941714286804 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9150],\n",
      "        [0.0289]], device='mps:0')\n",
      "Iteration 68010 Training loss 0.048661068081855774 Validation loss 0.057829562574625015 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.1331],\n",
      "        [0.9755]], device='mps:0')\n",
      "Iteration 68020 Training loss 0.06223372742533684 Validation loss 0.057841669768095016 Accuracy 0.842875063419342\n",
      "Output tensor([[0.8507],\n",
      "        [0.8739]], device='mps:0')\n",
      "Iteration 68030 Training loss 0.05228697508573532 Validation loss 0.05784744396805763 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0307],\n",
      "        [0.8716]], device='mps:0')\n",
      "Iteration 68040 Training loss 0.056677378714084625 Validation loss 0.05783912539482117 Accuracy 0.843250036239624\n",
      "Output tensor([[0.2833],\n",
      "        [0.1418]], device='mps:0')\n",
      "Iteration 68050 Training loss 0.049359481781721115 Validation loss 0.05782917141914368 Accuracy 0.843250036239624\n",
      "Output tensor([[0.2578],\n",
      "        [0.9404]], device='mps:0')\n",
      "Iteration 68060 Training loss 0.0589778870344162 Validation loss 0.057826463133096695 Accuracy 0.843375027179718\n",
      "Output tensor([[0.4994],\n",
      "        [0.3317]], device='mps:0')\n",
      "Iteration 68070 Training loss 0.052503082901239395 Validation loss 0.05783165246248245 Accuracy 0.843250036239624\n",
      "Output tensor([[0.1003],\n",
      "        [0.6388]], device='mps:0')\n",
      "Iteration 68080 Training loss 0.05273183807730675 Validation loss 0.05787740647792816 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9138],\n",
      "        [0.9476]], device='mps:0')\n",
      "Iteration 68090 Training loss 0.056962527334690094 Validation loss 0.05805156007409096 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9352],\n",
      "        [0.3063]], device='mps:0')\n",
      "Iteration 68100 Training loss 0.05692962929606438 Validation loss 0.0578778013586998 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0224],\n",
      "        [0.0098]], device='mps:0')\n",
      "Iteration 68110 Training loss 0.05297181382775307 Validation loss 0.05785521864891052 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.1228],\n",
      "        [0.8043]], device='mps:0')\n",
      "Iteration 68120 Training loss 0.04906442016363144 Validation loss 0.05788714438676834 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.7147],\n",
      "        [0.9637]], device='mps:0')\n",
      "Iteration 68130 Training loss 0.05240890383720398 Validation loss 0.05790074169635773 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.0428],\n",
      "        [0.9632]], device='mps:0')\n",
      "Iteration 68140 Training loss 0.06356435269117355 Validation loss 0.05792956054210663 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.1635],\n",
      "        [0.9667]], device='mps:0')\n",
      "Iteration 68150 Training loss 0.05189676210284233 Validation loss 0.057834282517433167 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0272],\n",
      "        [0.8006]], device='mps:0')\n",
      "Iteration 68160 Training loss 0.05494788661599159 Validation loss 0.057842619717121124 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9427],\n",
      "        [0.5332]], device='mps:0')\n",
      "Iteration 68170 Training loss 0.05505545437335968 Validation loss 0.05797090753912926 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0195],\n",
      "        [0.6524]], device='mps:0')\n",
      "Iteration 68180 Training loss 0.056531235575675964 Validation loss 0.05782889947295189 Accuracy 0.843375027179718\n",
      "Output tensor([[0.6083],\n",
      "        [0.5484]], device='mps:0')\n",
      "Iteration 68190 Training loss 0.04547716677188873 Validation loss 0.05784611776471138 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0366],\n",
      "        [0.3737]], device='mps:0')\n",
      "Iteration 68200 Training loss 0.05591568350791931 Validation loss 0.05782943218946457 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9504],\n",
      "        [0.9914]], device='mps:0')\n",
      "Iteration 68210 Training loss 0.05539223924279213 Validation loss 0.05782870575785637 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.5326],\n",
      "        [0.6692]], device='mps:0')\n",
      "Iteration 68220 Training loss 0.04703230783343315 Validation loss 0.05782530829310417 Accuracy 0.843250036239624\n",
      "Output tensor([[0.8262],\n",
      "        [0.9750]], device='mps:0')\n",
      "Iteration 68230 Training loss 0.05943651869893074 Validation loss 0.057845477014780045 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.7362],\n",
      "        [0.0456]], device='mps:0')\n",
      "Iteration 68240 Training loss 0.05061887577176094 Validation loss 0.05782751739025116 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0053],\n",
      "        [0.1349]], device='mps:0')\n",
      "Iteration 68250 Training loss 0.055333688855171204 Validation loss 0.05782673507928848 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.1226],\n",
      "        [0.7943]], device='mps:0')\n",
      "Iteration 68260 Training loss 0.05333231762051582 Validation loss 0.058094896376132965 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9679],\n",
      "        [0.8252]], device='mps:0')\n",
      "Iteration 68270 Training loss 0.05040491372346878 Validation loss 0.05781429633498192 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0373],\n",
      "        [0.9258]], device='mps:0')\n",
      "Iteration 68280 Training loss 0.05809222161769867 Validation loss 0.05781623721122742 Accuracy 0.843375027179718\n",
      "Output tensor([[0.4256],\n",
      "        [0.4614]], device='mps:0')\n",
      "Iteration 68290 Training loss 0.0542275495827198 Validation loss 0.05781673640012741 Accuracy 0.843500018119812\n",
      "Output tensor([[0.6170],\n",
      "        [0.9361]], device='mps:0')\n",
      "Iteration 68300 Training loss 0.05745404213666916 Validation loss 0.05781261622905731 Accuracy 0.843375027179718\n",
      "Output tensor([[0.8792],\n",
      "        [0.3834]], device='mps:0')\n",
      "Iteration 68310 Training loss 0.05215460807085037 Validation loss 0.057845525443553925 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.4955],\n",
      "        [0.4670]], device='mps:0')\n",
      "Iteration 68320 Training loss 0.05968613550066948 Validation loss 0.057883091270923615 Accuracy 0.843000054359436\n",
      "Output tensor([[0.8554],\n",
      "        [0.0144]], device='mps:0')\n",
      "Iteration 68330 Training loss 0.05035693943500519 Validation loss 0.05782699957489967 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.7832],\n",
      "        [0.4208]], device='mps:0')\n",
      "Iteration 68340 Training loss 0.06264050304889679 Validation loss 0.057831231504678726 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.5956],\n",
      "        [0.9767]], device='mps:0')\n",
      "Iteration 68350 Training loss 0.042603034526109695 Validation loss 0.05784379690885544 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.7979],\n",
      "        [0.7418]], device='mps:0')\n",
      "Iteration 68360 Training loss 0.050804395228624344 Validation loss 0.057814083993434906 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9582],\n",
      "        [0.9450]], device='mps:0')\n",
      "Iteration 68370 Training loss 0.05326082929968834 Validation loss 0.05799242854118347 Accuracy 0.843000054359436\n",
      "Output tensor([[0.1225],\n",
      "        [0.2314]], device='mps:0')\n",
      "Iteration 68380 Training loss 0.05158793181180954 Validation loss 0.057808857411146164 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9602],\n",
      "        [0.9927]], device='mps:0')\n",
      "Iteration 68390 Training loss 0.057608671486377716 Validation loss 0.05780864134430885 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0373],\n",
      "        [0.9885]], device='mps:0')\n",
      "Iteration 68400 Training loss 0.05836755782365799 Validation loss 0.0579180009663105 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0877],\n",
      "        [0.9921]], device='mps:0')\n",
      "Iteration 68410 Training loss 0.06137390062212944 Validation loss 0.057800907641649246 Accuracy 0.843375027179718\n",
      "Output tensor([[0.6088],\n",
      "        [0.5459]], device='mps:0')\n",
      "Iteration 68420 Training loss 0.06207019090652466 Validation loss 0.0578242763876915 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.3099],\n",
      "        [0.3401]], device='mps:0')\n",
      "Iteration 68430 Training loss 0.049869801849126816 Validation loss 0.05792390555143356 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0560],\n",
      "        [0.8509]], device='mps:0')\n",
      "Iteration 68440 Training loss 0.04966078698635101 Validation loss 0.05781719088554382 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.8630],\n",
      "        [0.9904]], device='mps:0')\n",
      "Iteration 68450 Training loss 0.054597124457359314 Validation loss 0.05781972035765648 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.4762],\n",
      "        [0.7948]], device='mps:0')\n",
      "Iteration 68460 Training loss 0.05811024457216263 Validation loss 0.05783731862902641 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.1186],\n",
      "        [0.2334]], device='mps:0')\n",
      "Iteration 68470 Training loss 0.05799955129623413 Validation loss 0.05788661539554596 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9566],\n",
      "        [0.9928]], device='mps:0')\n",
      "Iteration 68480 Training loss 0.05584368482232094 Validation loss 0.05786088854074478 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0055],\n",
      "        [0.9571]], device='mps:0')\n",
      "Iteration 68490 Training loss 0.055646855384111404 Validation loss 0.05798579007387161 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9634],\n",
      "        [0.9629]], device='mps:0')\n",
      "Iteration 68500 Training loss 0.05768340080976486 Validation loss 0.057888466864824295 Accuracy 0.84312504529953\n",
      "Output tensor([[0.2893],\n",
      "        [0.8899]], device='mps:0')\n",
      "Iteration 68510 Training loss 0.043308403342962265 Validation loss 0.057786840945482254 Accuracy 0.843500018119812\n",
      "Output tensor([[0.2183],\n",
      "        [0.6943]], device='mps:0')\n",
      "Iteration 68520 Training loss 0.0499751903116703 Validation loss 0.05781006067991257 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9021],\n",
      "        [0.8934]], device='mps:0')\n",
      "Iteration 68530 Training loss 0.05548498034477234 Validation loss 0.05784770846366882 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.9841],\n",
      "        [0.6926]], device='mps:0')\n",
      "Iteration 68540 Training loss 0.05260844901204109 Validation loss 0.05786396935582161 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0531],\n",
      "        [0.7075]], device='mps:0')\n",
      "Iteration 68550 Training loss 0.06503542512655258 Validation loss 0.05779494717717171 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9737],\n",
      "        [0.9409]], device='mps:0')\n",
      "Iteration 68560 Training loss 0.05631588399410248 Validation loss 0.05778910964727402 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.4889],\n",
      "        [0.1881]], device='mps:0')\n",
      "Iteration 68570 Training loss 0.05700897425413132 Validation loss 0.05779994651675224 Accuracy 0.843000054359436\n",
      "Output tensor([[0.7872],\n",
      "        [0.8516]], device='mps:0')\n",
      "Iteration 68580 Training loss 0.048436786979436874 Validation loss 0.05784391239285469 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.6193],\n",
      "        [0.4679]], device='mps:0')\n",
      "Iteration 68590 Training loss 0.05967452749609947 Validation loss 0.05778198689222336 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9827],\n",
      "        [0.0065]], device='mps:0')\n",
      "Iteration 68600 Training loss 0.04993138089776039 Validation loss 0.05779033154249191 Accuracy 0.843250036239624\n",
      "Output tensor([[0.2161],\n",
      "        [0.4749]], device='mps:0')\n",
      "Iteration 68610 Training loss 0.05975601449608803 Validation loss 0.05787349492311478 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0460],\n",
      "        [0.0017]], device='mps:0')\n",
      "Iteration 68620 Training loss 0.05394692346453667 Validation loss 0.057784829288721085 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.8408],\n",
      "        [0.9891]], device='mps:0')\n",
      "Iteration 68630 Training loss 0.05450412631034851 Validation loss 0.057792507112026215 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.5315],\n",
      "        [0.8600]], device='mps:0')\n",
      "Iteration 68640 Training loss 0.04886700585484505 Validation loss 0.05778627470135689 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0544],\n",
      "        [0.1547]], device='mps:0')\n",
      "Iteration 68650 Training loss 0.06571158021688461 Validation loss 0.057805582880973816 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9239],\n",
      "        [0.1004]], device='mps:0')\n",
      "Iteration 68660 Training loss 0.05154024437069893 Validation loss 0.057840172201395035 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0884],\n",
      "        [0.9578]], device='mps:0')\n",
      "Iteration 68670 Training loss 0.05097803846001625 Validation loss 0.0577889010310173 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9643],\n",
      "        [0.1191]], device='mps:0')\n",
      "Iteration 68680 Training loss 0.04851783066987991 Validation loss 0.05779428407549858 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9673],\n",
      "        [0.9178]], device='mps:0')\n",
      "Iteration 68690 Training loss 0.051153991371393204 Validation loss 0.05781008303165436 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.7379],\n",
      "        [0.8936]], device='mps:0')\n",
      "Iteration 68700 Training loss 0.054431602358818054 Validation loss 0.057790569961071014 Accuracy 0.843500018119812\n",
      "Output tensor([[0.1020],\n",
      "        [0.9819]], device='mps:0')\n",
      "Iteration 68710 Training loss 0.051176633685827255 Validation loss 0.05779116973280907 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9343],\n",
      "        [0.4523]], device='mps:0')\n",
      "Iteration 68720 Training loss 0.04857524856925011 Validation loss 0.05782295763492584 Accuracy 0.843375027179718\n",
      "Output tensor([[0.5986],\n",
      "        [0.0579]], device='mps:0')\n",
      "Iteration 68730 Training loss 0.05318879336118698 Validation loss 0.057800401002168655 Accuracy 0.84312504529953\n",
      "Output tensor([[0.8875],\n",
      "        [0.5534]], device='mps:0')\n",
      "Iteration 68740 Training loss 0.05364890769124031 Validation loss 0.05781259387731552 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.5520],\n",
      "        [0.5749]], device='mps:0')\n",
      "Iteration 68750 Training loss 0.06020541116595268 Validation loss 0.057808198034763336 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0168],\n",
      "        [0.7338]], device='mps:0')\n",
      "Iteration 68760 Training loss 0.0456191711127758 Validation loss 0.05779070034623146 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.1297],\n",
      "        [0.9623]], device='mps:0')\n",
      "Iteration 68770 Training loss 0.05471734330058098 Validation loss 0.0579354465007782 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0501],\n",
      "        [0.0463]], device='mps:0')\n",
      "Iteration 68780 Training loss 0.04753878340125084 Validation loss 0.057784561067819595 Accuracy 0.843375027179718\n",
      "Output tensor([[0.1499],\n",
      "        [0.0421]], device='mps:0')\n",
      "Iteration 68790 Training loss 0.05508789047598839 Validation loss 0.057806581258773804 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0555],\n",
      "        [0.9945]], device='mps:0')\n",
      "Iteration 68800 Training loss 0.04038155451416969 Validation loss 0.057866983115673065 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7690],\n",
      "        [0.5798]], device='mps:0')\n",
      "Iteration 68810 Training loss 0.05031248927116394 Validation loss 0.05779477208852768 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9001],\n",
      "        [0.2341]], device='mps:0')\n",
      "Iteration 68820 Training loss 0.06008229777216911 Validation loss 0.05780002102255821 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0690],\n",
      "        [0.0459]], device='mps:0')\n",
      "Iteration 68830 Training loss 0.06460036337375641 Validation loss 0.0578017495572567 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9417],\n",
      "        [0.7222]], device='mps:0')\n",
      "Iteration 68840 Training loss 0.05493190512061119 Validation loss 0.057796038687229156 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9969],\n",
      "        [0.0500]], device='mps:0')\n",
      "Iteration 68850 Training loss 0.05201835557818413 Validation loss 0.05781370773911476 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.8851],\n",
      "        [0.4473]], device='mps:0')\n",
      "Iteration 68860 Training loss 0.056465033441782 Validation loss 0.05778461694717407 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0671],\n",
      "        [0.0113]], device='mps:0')\n",
      "Iteration 68870 Training loss 0.053749971091747284 Validation loss 0.05779435113072395 Accuracy 0.843000054359436\n",
      "Output tensor([[0.8335],\n",
      "        [0.0119]], device='mps:0')\n",
      "Iteration 68880 Training loss 0.057342249900102615 Validation loss 0.05778840184211731 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9794],\n",
      "        [0.9636]], device='mps:0')\n",
      "Iteration 68890 Training loss 0.047575969249010086 Validation loss 0.05796901136636734 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.7473],\n",
      "        [0.1561]], device='mps:0')\n",
      "Iteration 68900 Training loss 0.05071839317679405 Validation loss 0.05778801441192627 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.2849],\n",
      "        [0.3626]], device='mps:0')\n",
      "Iteration 68910 Training loss 0.06406988948583603 Validation loss 0.05777168646454811 Accuracy 0.843250036239624\n",
      "Output tensor([[0.2601],\n",
      "        [0.9871]], device='mps:0')\n",
      "Iteration 68920 Training loss 0.05236106365919113 Validation loss 0.05785704404115677 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9799],\n",
      "        [0.4978]], device='mps:0')\n",
      "Iteration 68930 Training loss 0.05207724869251251 Validation loss 0.05790288746356964 Accuracy 0.843000054359436\n",
      "Output tensor([[0.3460],\n",
      "        [0.0016]], device='mps:0')\n",
      "Iteration 68940 Training loss 0.0628231093287468 Validation loss 0.05781269446015358 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9458],\n",
      "        [0.1851]], device='mps:0')\n",
      "Iteration 68950 Training loss 0.057204537093639374 Validation loss 0.05796271562576294 Accuracy 0.8408750295639038\n",
      "Output tensor([[0.0128],\n",
      "        [0.8208]], device='mps:0')\n",
      "Iteration 68960 Training loss 0.05438798666000366 Validation loss 0.05777667462825775 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.8825],\n",
      "        [0.9411]], device='mps:0')\n",
      "Iteration 68970 Training loss 0.0565643310546875 Validation loss 0.05780934542417526 Accuracy 0.843500018119812\n",
      "Output tensor([[0.6814],\n",
      "        [0.9552]], device='mps:0')\n",
      "Iteration 68980 Training loss 0.04828937351703644 Validation loss 0.057774074375629425 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.4538],\n",
      "        [0.9860]], device='mps:0')\n",
      "Iteration 68990 Training loss 0.05356822907924652 Validation loss 0.05783364549279213 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.1611],\n",
      "        [0.0579]], device='mps:0')\n",
      "Iteration 69000 Training loss 0.05053040385246277 Validation loss 0.05792977660894394 Accuracy 0.843500018119812\n",
      "Output tensor([[0.1616],\n",
      "        [0.9161]], device='mps:0')\n",
      "Iteration 69010 Training loss 0.05486566573381424 Validation loss 0.05781562998890877 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9958],\n",
      "        [0.5947]], device='mps:0')\n",
      "Iteration 69020 Training loss 0.057261209934949875 Validation loss 0.05776865780353546 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.2853],\n",
      "        [0.9678]], device='mps:0')\n",
      "Iteration 69030 Training loss 0.05518938601016998 Validation loss 0.05783216282725334 Accuracy 0.843375027179718\n",
      "Output tensor([[0.6414],\n",
      "        [0.3104]], device='mps:0')\n",
      "Iteration 69040 Training loss 0.05664702132344246 Validation loss 0.057839132845401764 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9259],\n",
      "        [0.3580]], device='mps:0')\n",
      "Iteration 69050 Training loss 0.055075064301490784 Validation loss 0.057770997285842896 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0654],\n",
      "        [0.9414]], device='mps:0')\n",
      "Iteration 69060 Training loss 0.05019959434866905 Validation loss 0.057778581976890564 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.3323],\n",
      "        [0.4478]], device='mps:0')\n",
      "Iteration 69070 Training loss 0.053977567702531815 Validation loss 0.057838160544633865 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9558],\n",
      "        [0.9909]], device='mps:0')\n",
      "Iteration 69080 Training loss 0.053651198744773865 Validation loss 0.0577608123421669 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9509],\n",
      "        [0.0448]], device='mps:0')\n",
      "Iteration 69090 Training loss 0.05879145488142967 Validation loss 0.05778580531477928 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9153],\n",
      "        [0.8397]], device='mps:0')\n",
      "Iteration 69100 Training loss 0.04986930266022682 Validation loss 0.057836756110191345 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.5330],\n",
      "        [0.1190]], device='mps:0')\n",
      "Iteration 69110 Training loss 0.05475650727748871 Validation loss 0.05775494500994682 Accuracy 0.843375027179718\n",
      "Output tensor([[0.5950],\n",
      "        [0.9834]], device='mps:0')\n",
      "Iteration 69120 Training loss 0.04867880791425705 Validation loss 0.057781822979450226 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9114],\n",
      "        [0.7942]], device='mps:0')\n",
      "Iteration 69130 Training loss 0.06412161886692047 Validation loss 0.057970188558101654 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.0487],\n",
      "        [0.5162]], device='mps:0')\n",
      "Iteration 69140 Training loss 0.06075442582368851 Validation loss 0.05776170641183853 Accuracy 0.843250036239624\n",
      "Output tensor([[0.2218],\n",
      "        [0.9191]], device='mps:0')\n",
      "Iteration 69150 Training loss 0.04993591830134392 Validation loss 0.05780837684869766 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.6831],\n",
      "        [0.9678]], device='mps:0')\n",
      "Iteration 69160 Training loss 0.05722011253237724 Validation loss 0.05775025486946106 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9375],\n",
      "        [0.2760]], device='mps:0')\n",
      "Iteration 69170 Training loss 0.05666442587971687 Validation loss 0.057785797864198685 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9321],\n",
      "        [0.6160]], device='mps:0')\n",
      "Iteration 69180 Training loss 0.05702594667673111 Validation loss 0.05776694044470787 Accuracy 0.842875063419342\n",
      "Output tensor([[0.4662],\n",
      "        [0.9315]], device='mps:0')\n",
      "Iteration 69190 Training loss 0.04642980545759201 Validation loss 0.0577603243291378 Accuracy 0.843375027179718\n",
      "Output tensor([[0.1167],\n",
      "        [0.0076]], device='mps:0')\n",
      "Iteration 69200 Training loss 0.060001127421855927 Validation loss 0.05774515122175217 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0607],\n",
      "        [0.0433]], device='mps:0')\n",
      "Iteration 69210 Training loss 0.05547173321247101 Validation loss 0.05774534121155739 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.1137],\n",
      "        [0.4766]], device='mps:0')\n",
      "Iteration 69220 Training loss 0.05300384387373924 Validation loss 0.05778449401259422 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0280],\n",
      "        [0.7538]], device='mps:0')\n",
      "Iteration 69230 Training loss 0.05172570422291756 Validation loss 0.057756513357162476 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0844],\n",
      "        [0.5058]], device='mps:0')\n",
      "Iteration 69240 Training loss 0.055768534541130066 Validation loss 0.05776406079530716 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.8082],\n",
      "        [0.6279]], device='mps:0')\n",
      "Iteration 69250 Training loss 0.051046524196863174 Validation loss 0.05773643031716347 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9127],\n",
      "        [0.9708]], device='mps:0')\n",
      "Iteration 69260 Training loss 0.05958826094865799 Validation loss 0.057789042592048645 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.5447],\n",
      "        [0.1473]], device='mps:0')\n",
      "Iteration 69270 Training loss 0.05921443924307823 Validation loss 0.05774398893117905 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.2499],\n",
      "        [0.6202]], device='mps:0')\n",
      "Iteration 69280 Training loss 0.05820853263139725 Validation loss 0.05778127908706665 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.1697],\n",
      "        [0.6581]], device='mps:0')\n",
      "Iteration 69290 Training loss 0.058155011385679245 Validation loss 0.057738032191991806 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0244],\n",
      "        [0.9804]], device='mps:0')\n",
      "Iteration 69300 Training loss 0.05875598266720772 Validation loss 0.05775051191449165 Accuracy 0.843000054359436\n",
      "Output tensor([[0.7757],\n",
      "        [0.0644]], device='mps:0')\n",
      "Iteration 69310 Training loss 0.057661015540361404 Validation loss 0.05776364356279373 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.2945],\n",
      "        [0.7188]], device='mps:0')\n",
      "Iteration 69320 Training loss 0.05125662684440613 Validation loss 0.0578119121491909 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.4857],\n",
      "        [0.9219]], device='mps:0')\n",
      "Iteration 69330 Training loss 0.04616686329245567 Validation loss 0.0577898733317852 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.1349],\n",
      "        [0.0936]], device='mps:0')\n",
      "Iteration 69340 Training loss 0.061182934790849686 Validation loss 0.057730093598365784 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.6597],\n",
      "        [0.2031]], device='mps:0')\n",
      "Iteration 69350 Training loss 0.05507805570960045 Validation loss 0.057733241468667984 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.1248],\n",
      "        [0.2642]], device='mps:0')\n",
      "Iteration 69360 Training loss 0.05316092446446419 Validation loss 0.057734064757823944 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.2600],\n",
      "        [0.9262]], device='mps:0')\n",
      "Iteration 69370 Training loss 0.0529889278113842 Validation loss 0.05775638297200203 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0785],\n",
      "        [0.1296]], device='mps:0')\n",
      "Iteration 69380 Training loss 0.053708646446466446 Validation loss 0.05775425583124161 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.4083],\n",
      "        [0.2348]], device='mps:0')\n",
      "Iteration 69390 Training loss 0.05576908960938454 Validation loss 0.05772993341088295 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.8865],\n",
      "        [0.9809]], device='mps:0')\n",
      "Iteration 69400 Training loss 0.05416103079915047 Validation loss 0.057958681136369705 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.0380],\n",
      "        [0.1384]], device='mps:0')\n",
      "Iteration 69410 Training loss 0.058917902410030365 Validation loss 0.05775732174515724 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.2098],\n",
      "        [0.6871]], device='mps:0')\n",
      "Iteration 69420 Training loss 0.05065277963876724 Validation loss 0.05792279168963432 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.6228],\n",
      "        [0.0723]], device='mps:0')\n",
      "Iteration 69430 Training loss 0.05418171361088753 Validation loss 0.05780237168073654 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.5729],\n",
      "        [0.0550]], device='mps:0')\n",
      "Iteration 69440 Training loss 0.050603799521923065 Validation loss 0.057738352566957474 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9398],\n",
      "        [0.2809]], device='mps:0')\n",
      "Iteration 69450 Training loss 0.059459373354911804 Validation loss 0.057902220636606216 Accuracy 0.843000054359436\n",
      "Output tensor([[0.2725],\n",
      "        [0.0554]], device='mps:0')\n",
      "Iteration 69460 Training loss 0.05803748965263367 Validation loss 0.05775932967662811 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.3007],\n",
      "        [0.0810]], device='mps:0')\n",
      "Iteration 69470 Training loss 0.05326380580663681 Validation loss 0.05772106349468231 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.2736],\n",
      "        [0.9805]], device='mps:0')\n",
      "Iteration 69480 Training loss 0.044075220823287964 Validation loss 0.057722173631191254 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.7719],\n",
      "        [0.7556]], device='mps:0')\n",
      "Iteration 69490 Training loss 0.056240785866975784 Validation loss 0.057735759764909744 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.2112],\n",
      "        [0.4540]], device='mps:0')\n",
      "Iteration 69500 Training loss 0.054634351283311844 Validation loss 0.05785087123513222 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0053],\n",
      "        [0.0758]], device='mps:0')\n",
      "Iteration 69510 Training loss 0.053346291184425354 Validation loss 0.05772082880139351 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.3721],\n",
      "        [0.9928]], device='mps:0')\n",
      "Iteration 69520 Training loss 0.054788731038570404 Validation loss 0.05774293094873428 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.0320],\n",
      "        [0.0405]], device='mps:0')\n",
      "Iteration 69530 Training loss 0.06301961094141006 Validation loss 0.05773352459073067 Accuracy 0.842875063419342\n",
      "Output tensor([[0.7558],\n",
      "        [0.8435]], device='mps:0')\n",
      "Iteration 69540 Training loss 0.05593424662947655 Validation loss 0.057745311409235 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9348],\n",
      "        [0.2277]], device='mps:0')\n",
      "Iteration 69550 Training loss 0.05325022712349892 Validation loss 0.05807675048708916 Accuracy 0.8407500386238098\n",
      "Output tensor([[0.9176],\n",
      "        [0.9081]], device='mps:0')\n",
      "Iteration 69560 Training loss 0.04966346547007561 Validation loss 0.05783214047551155 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0847],\n",
      "        [0.9245]], device='mps:0')\n",
      "Iteration 69570 Training loss 0.05061449855566025 Validation loss 0.05782391503453255 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.7380],\n",
      "        [0.0855]], device='mps:0')\n",
      "Iteration 69580 Training loss 0.0562492199242115 Validation loss 0.057740870863199234 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.1759],\n",
      "        [0.1606]], device='mps:0')\n",
      "Iteration 69590 Training loss 0.05867379531264305 Validation loss 0.05773395672440529 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.2575],\n",
      "        [0.9977]], device='mps:0')\n",
      "Iteration 69600 Training loss 0.05829969793558121 Validation loss 0.05773286893963814 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.1030],\n",
      "        [0.9903]], device='mps:0')\n",
      "Iteration 69610 Training loss 0.05586434155702591 Validation loss 0.05773231014609337 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9853],\n",
      "        [0.0297]], device='mps:0')\n",
      "Iteration 69620 Training loss 0.04320361092686653 Validation loss 0.05773515626788139 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7870],\n",
      "        [0.7587]], device='mps:0')\n",
      "Iteration 69630 Training loss 0.047248199582099915 Validation loss 0.05774373933672905 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.1018],\n",
      "        [0.0662]], device='mps:0')\n",
      "Iteration 69640 Training loss 0.04804087430238724 Validation loss 0.057792432606220245 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9569],\n",
      "        [0.9413]], device='mps:0')\n",
      "Iteration 69650 Training loss 0.05266483128070831 Validation loss 0.057753417640924454 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9795],\n",
      "        [0.8811]], device='mps:0')\n",
      "Iteration 69660 Training loss 0.05168558657169342 Validation loss 0.05774809047579765 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9112],\n",
      "        [0.9558]], device='mps:0')\n",
      "Iteration 69670 Training loss 0.04803412780165672 Validation loss 0.057797614485025406 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.2533],\n",
      "        [0.0412]], device='mps:0')\n",
      "Iteration 69680 Training loss 0.05062684789299965 Validation loss 0.05778177082538605 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.3379],\n",
      "        [0.0457]], device='mps:0')\n",
      "Iteration 69690 Training loss 0.06653138250112534 Validation loss 0.057838816195726395 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9593],\n",
      "        [0.5402]], device='mps:0')\n",
      "Iteration 69700 Training loss 0.05480446666479111 Validation loss 0.057969577610492706 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.0234],\n",
      "        [0.5624]], device='mps:0')\n",
      "Iteration 69710 Training loss 0.054773636162281036 Validation loss 0.05777063965797424 Accuracy 0.843250036239624\n",
      "Output tensor([[0.5472],\n",
      "        [0.6138]], device='mps:0')\n",
      "Iteration 69720 Training loss 0.05367011949419975 Validation loss 0.05786661058664322 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.8403],\n",
      "        [0.1083]], device='mps:0')\n",
      "Iteration 69730 Training loss 0.05506180599331856 Validation loss 0.05773577466607094 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0977],\n",
      "        [0.0781]], device='mps:0')\n",
      "Iteration 69740 Training loss 0.049740664660930634 Validation loss 0.057801783084869385 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.9394],\n",
      "        [0.0848]], device='mps:0')\n",
      "Iteration 69750 Training loss 0.05439705774188042 Validation loss 0.057739078998565674 Accuracy 0.843000054359436\n",
      "Output tensor([[0.1261],\n",
      "        [0.8319]], device='mps:0')\n",
      "Iteration 69760 Training loss 0.04973979294300079 Validation loss 0.057737309485673904 Accuracy 0.843500018119812\n",
      "Output tensor([[0.1498],\n",
      "        [0.9869]], device='mps:0')\n",
      "Iteration 69770 Training loss 0.05228746309876442 Validation loss 0.0577479749917984 Accuracy 0.842875063419342\n",
      "Output tensor([[0.2805],\n",
      "        [0.5269]], device='mps:0')\n",
      "Iteration 69780 Training loss 0.05590926855802536 Validation loss 0.057746440172195435 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.1846],\n",
      "        [0.0611]], device='mps:0')\n",
      "Iteration 69790 Training loss 0.047861915081739426 Validation loss 0.05789200961589813 Accuracy 0.842875063419342\n",
      "Output tensor([[0.2295],\n",
      "        [0.0357]], device='mps:0')\n",
      "Iteration 69800 Training loss 0.0549292154610157 Validation loss 0.057750847190618515 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9305],\n",
      "        [0.1891]], device='mps:0')\n",
      "Iteration 69810 Training loss 0.05213891342282295 Validation loss 0.05776935815811157 Accuracy 0.842875063419342\n",
      "Output tensor([[0.1738],\n",
      "        [0.0703]], device='mps:0')\n",
      "Iteration 69820 Training loss 0.05786588415503502 Validation loss 0.05776631087064743 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9888],\n",
      "        [0.7232]], device='mps:0')\n",
      "Iteration 69830 Training loss 0.05875165015459061 Validation loss 0.057746462523937225 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.0220],\n",
      "        [0.2155]], device='mps:0')\n",
      "Iteration 69840 Training loss 0.05591229349374771 Validation loss 0.05784071981906891 Accuracy 0.843250036239624\n",
      "Output tensor([[0.8335],\n",
      "        [0.9133]], device='mps:0')\n",
      "Iteration 69850 Training loss 0.05421295017004013 Validation loss 0.05784593150019646 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.3754],\n",
      "        [0.0169]], device='mps:0')\n",
      "Iteration 69860 Training loss 0.05377553403377533 Validation loss 0.05799511820077896 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.3596],\n",
      "        [0.8820]], device='mps:0')\n",
      "Iteration 69870 Training loss 0.05085700750350952 Validation loss 0.05782940238714218 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.2999],\n",
      "        [0.8781]], device='mps:0')\n",
      "Iteration 69880 Training loss 0.058421503752470016 Validation loss 0.057761505246162415 Accuracy 0.84312504529953\n",
      "Output tensor([[0.2608],\n",
      "        [0.9533]], device='mps:0')\n",
      "Iteration 69890 Training loss 0.05162458121776581 Validation loss 0.05776478722691536 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7694],\n",
      "        [0.9661]], device='mps:0')\n",
      "Iteration 69900 Training loss 0.05277107283473015 Validation loss 0.057731639593839645 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0626],\n",
      "        [0.9241]], device='mps:0')\n",
      "Iteration 69910 Training loss 0.06140433996915817 Validation loss 0.0578196682035923 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9670],\n",
      "        [0.3216]], device='mps:0')\n",
      "Iteration 69920 Training loss 0.0554288774728775 Validation loss 0.05784335732460022 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.0985],\n",
      "        [0.0651]], device='mps:0')\n",
      "Iteration 69930 Training loss 0.053976863622665405 Validation loss 0.0577264204621315 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0429],\n",
      "        [0.2433]], device='mps:0')\n",
      "Iteration 69940 Training loss 0.04544929414987564 Validation loss 0.057759400457143784 Accuracy 0.843000054359436\n",
      "Output tensor([[0.7085],\n",
      "        [0.9255]], device='mps:0')\n",
      "Iteration 69950 Training loss 0.04289158433675766 Validation loss 0.057744454592466354 Accuracy 0.842875063419342\n",
      "Output tensor([[0.5190],\n",
      "        [0.9092]], device='mps:0')\n",
      "Iteration 69960 Training loss 0.05272891744971275 Validation loss 0.057731207460165024 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.4754],\n",
      "        [0.1868]], device='mps:0')\n",
      "Iteration 69970 Training loss 0.0621635764837265 Validation loss 0.05780037119984627 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9430],\n",
      "        [0.7730]], device='mps:0')\n",
      "Iteration 69980 Training loss 0.05779356509447098 Validation loss 0.057738855481147766 Accuracy 0.843500018119812\n",
      "Output tensor([[0.5507],\n",
      "        [0.5752]], device='mps:0')\n",
      "Iteration 69990 Training loss 0.04408525303006172 Validation loss 0.05772290378808975 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.5100],\n",
      "        [0.7436]], device='mps:0')\n",
      "Iteration 70000 Training loss 0.052428338676691055 Validation loss 0.05780816078186035 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.7211],\n",
      "        [0.8965]], device='mps:0')\n",
      "Iteration 70010 Training loss 0.05461256206035614 Validation loss 0.05773460492491722 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.1588],\n",
      "        [0.8946]], device='mps:0')\n",
      "Iteration 70020 Training loss 0.04973185434937477 Validation loss 0.05796543508768082 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.3814],\n",
      "        [0.1420]], device='mps:0')\n",
      "Iteration 70030 Training loss 0.047076910734176636 Validation loss 0.05783277377486229 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9919],\n",
      "        [0.9958]], device='mps:0')\n",
      "Iteration 70040 Training loss 0.0589192658662796 Validation loss 0.05771385878324509 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.4670],\n",
      "        [0.5435]], device='mps:0')\n",
      "Iteration 70050 Training loss 0.053772542625665665 Validation loss 0.0577615424990654 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0114],\n",
      "        [0.5884]], device='mps:0')\n",
      "Iteration 70060 Training loss 0.05657507851719856 Validation loss 0.05771865323185921 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.7736],\n",
      "        [0.2344]], device='mps:0')\n",
      "Iteration 70070 Training loss 0.04706050455570221 Validation loss 0.057715993374586105 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1467],\n",
      "        [0.0490]], device='mps:0')\n",
      "Iteration 70080 Training loss 0.05514054000377655 Validation loss 0.05773720145225525 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0102],\n",
      "        [0.4276]], device='mps:0')\n",
      "Iteration 70090 Training loss 0.04838838800787926 Validation loss 0.057773176580667496 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0731],\n",
      "        [0.8386]], device='mps:0')\n",
      "Iteration 70100 Training loss 0.056386690586805344 Validation loss 0.05785268545150757 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9747],\n",
      "        [0.0140]], device='mps:0')\n",
      "Iteration 70110 Training loss 0.048855654895305634 Validation loss 0.05783716216683388 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.9559],\n",
      "        [0.9208]], device='mps:0')\n",
      "Iteration 70120 Training loss 0.06438050419092178 Validation loss 0.05786815285682678 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.2331],\n",
      "        [0.8180]], device='mps:0')\n",
      "Iteration 70130 Training loss 0.05685380846261978 Validation loss 0.057723503559827805 Accuracy 0.843500018119812\n",
      "Output tensor([[0.8547],\n",
      "        [0.1743]], device='mps:0')\n",
      "Iteration 70140 Training loss 0.060363929718732834 Validation loss 0.05781811103224754 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9137],\n",
      "        [0.4487]], device='mps:0')\n",
      "Iteration 70150 Training loss 0.059923380613327026 Validation loss 0.05770894140005112 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9990],\n",
      "        [0.9063]], device='mps:0')\n",
      "Iteration 70160 Training loss 0.047195471823215485 Validation loss 0.05772334709763527 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.3015],\n",
      "        [0.9887]], device='mps:0')\n",
      "Iteration 70170 Training loss 0.05145081505179405 Validation loss 0.057712167501449585 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.8942],\n",
      "        [0.0956]], device='mps:0')\n",
      "Iteration 70180 Training loss 0.05168259143829346 Validation loss 0.05774307623505592 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9113],\n",
      "        [0.4578]], device='mps:0')\n",
      "Iteration 70190 Training loss 0.06809394061565399 Validation loss 0.057729437947273254 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7408],\n",
      "        [0.0108]], device='mps:0')\n",
      "Iteration 70200 Training loss 0.048306774348020554 Validation loss 0.05773123353719711 Accuracy 0.84312504529953\n",
      "Output tensor([[0.8986],\n",
      "        [0.2458]], device='mps:0')\n",
      "Iteration 70210 Training loss 0.056345388293266296 Validation loss 0.05771410092711449 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.8814],\n",
      "        [0.9923]], device='mps:0')\n",
      "Iteration 70220 Training loss 0.05699725076556206 Validation loss 0.057703737169504166 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9306],\n",
      "        [0.8642]], device='mps:0')\n",
      "Iteration 70230 Training loss 0.0557321235537529 Validation loss 0.05769665166735649 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.1708],\n",
      "        [0.9118]], device='mps:0')\n",
      "Iteration 70240 Training loss 0.05865605175495148 Validation loss 0.05770347639918327 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1760],\n",
      "        [0.0295]], device='mps:0')\n",
      "Iteration 70250 Training loss 0.06115499883890152 Validation loss 0.05770449340343475 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.6563],\n",
      "        [0.9875]], device='mps:0')\n",
      "Iteration 70260 Training loss 0.04330393671989441 Validation loss 0.05772118642926216 Accuracy 0.843250036239624\n",
      "Output tensor([[0.6298],\n",
      "        [0.0858]], device='mps:0')\n",
      "Iteration 70270 Training loss 0.05736175924539566 Validation loss 0.057696420699357986 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.5570],\n",
      "        [0.3270]], device='mps:0')\n",
      "Iteration 70280 Training loss 0.05294349417090416 Validation loss 0.05780560150742531 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9679],\n",
      "        [0.2231]], device='mps:0')\n",
      "Iteration 70290 Training loss 0.05924715846776962 Validation loss 0.057759493589401245 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.3811],\n",
      "        [0.9474]], device='mps:0')\n",
      "Iteration 70300 Training loss 0.06245177984237671 Validation loss 0.05776521563529968 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.4703],\n",
      "        [0.0216]], device='mps:0')\n",
      "Iteration 70310 Training loss 0.053763262927532196 Validation loss 0.05769606679677963 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0661],\n",
      "        [0.9090]], device='mps:0')\n",
      "Iteration 70320 Training loss 0.053452957421541214 Validation loss 0.05771761015057564 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9838],\n",
      "        [0.0541]], device='mps:0')\n",
      "Iteration 70330 Training loss 0.047486960887908936 Validation loss 0.05785311385989189 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.9159],\n",
      "        [0.7453]], device='mps:0')\n",
      "Iteration 70340 Training loss 0.06220753490924835 Validation loss 0.057697173207998276 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.3708],\n",
      "        [0.6880]], device='mps:0')\n",
      "Iteration 70350 Training loss 0.052483007311820984 Validation loss 0.057699959725141525 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.7487],\n",
      "        [0.5706]], device='mps:0')\n",
      "Iteration 70360 Training loss 0.05876202508807182 Validation loss 0.05769915506243706 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9112],\n",
      "        [0.8635]], device='mps:0')\n",
      "Iteration 70370 Training loss 0.05544836074113846 Validation loss 0.057790860533714294 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9878],\n",
      "        [0.3071]], device='mps:0')\n",
      "Iteration 70380 Training loss 0.05654258280992508 Validation loss 0.05769452825188637 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.8360],\n",
      "        [0.1002]], device='mps:0')\n",
      "Iteration 70390 Training loss 0.05177251994609833 Validation loss 0.0577615350484848 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0123],\n",
      "        [0.9287]], device='mps:0')\n",
      "Iteration 70400 Training loss 0.051674000918865204 Validation loss 0.057725220918655396 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.1683],\n",
      "        [0.9977]], device='mps:0')\n",
      "Iteration 70410 Training loss 0.06457121670246124 Validation loss 0.057705074548721313 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9764],\n",
      "        [0.9776]], device='mps:0')\n",
      "Iteration 70420 Training loss 0.06801307946443558 Validation loss 0.057693544775247574 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.7036],\n",
      "        [0.8419]], device='mps:0')\n",
      "Iteration 70430 Training loss 0.05174083635210991 Validation loss 0.0577087327837944 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0350],\n",
      "        [0.0574]], device='mps:0')\n",
      "Iteration 70440 Training loss 0.059623703360557556 Validation loss 0.05769400671124458 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.8457],\n",
      "        [0.8472]], device='mps:0')\n",
      "Iteration 70450 Training loss 0.04733977094292641 Validation loss 0.05773385241627693 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9218],\n",
      "        [0.2823]], device='mps:0')\n",
      "Iteration 70460 Training loss 0.050230834633111954 Validation loss 0.057689644396305084 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.2546],\n",
      "        [0.4131]], device='mps:0')\n",
      "Iteration 70470 Training loss 0.045364703983068466 Validation loss 0.05782360956072807 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.1556],\n",
      "        [0.9516]], device='mps:0')\n",
      "Iteration 70480 Training loss 0.05201466754078865 Validation loss 0.057736821472644806 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0748],\n",
      "        [0.7972]], device='mps:0')\n",
      "Iteration 70490 Training loss 0.05537489429116249 Validation loss 0.0577019602060318 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.5406],\n",
      "        [0.7017]], device='mps:0')\n",
      "Iteration 70500 Training loss 0.06370607018470764 Validation loss 0.05770863592624664 Accuracy 0.843500018119812\n",
      "Output tensor([[0.6200],\n",
      "        [0.3652]], device='mps:0')\n",
      "Iteration 70510 Training loss 0.056070711463689804 Validation loss 0.057720884680747986 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.3583],\n",
      "        [0.1918]], device='mps:0')\n",
      "Iteration 70520 Training loss 0.05553770065307617 Validation loss 0.05771491676568985 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9193],\n",
      "        [0.1182]], device='mps:0')\n",
      "Iteration 70530 Training loss 0.062360771000385284 Validation loss 0.05773426592350006 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.5557],\n",
      "        [0.6735]], device='mps:0')\n",
      "Iteration 70540 Training loss 0.05517266318202019 Validation loss 0.05769848823547363 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.4377],\n",
      "        [0.0088]], device='mps:0')\n",
      "Iteration 70550 Training loss 0.05283725634217262 Validation loss 0.05780995637178421 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.2027],\n",
      "        [0.0615]], device='mps:0')\n",
      "Iteration 70560 Training loss 0.056230492889881134 Validation loss 0.05776410549879074 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.9830],\n",
      "        [0.9944]], device='mps:0')\n",
      "Iteration 70570 Training loss 0.06252233684062958 Validation loss 0.05776570364832878 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.2250],\n",
      "        [0.8792]], device='mps:0')\n",
      "Iteration 70580 Training loss 0.05845417454838753 Validation loss 0.05770597234368324 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.8420],\n",
      "        [0.7054]], device='mps:0')\n",
      "Iteration 70590 Training loss 0.050035860389471054 Validation loss 0.05769537016749382 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.4234],\n",
      "        [0.8642]], device='mps:0')\n",
      "Iteration 70600 Training loss 0.04475625976920128 Validation loss 0.05768783017992973 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0415],\n",
      "        [0.0466]], device='mps:0')\n",
      "Iteration 70610 Training loss 0.05055966600775719 Validation loss 0.05777878314256668 Accuracy 0.8412500619888306\n",
      "Output tensor([[0.9673],\n",
      "        [0.7142]], device='mps:0')\n",
      "Iteration 70620 Training loss 0.05231992527842522 Validation loss 0.057749949395656586 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0463],\n",
      "        [0.8352]], device='mps:0')\n",
      "Iteration 70630 Training loss 0.05298996716737747 Validation loss 0.057694945484399796 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0323],\n",
      "        [0.8437]], device='mps:0')\n",
      "Iteration 70640 Training loss 0.049633271992206573 Validation loss 0.05776650458574295 Accuracy 0.843500018119812\n",
      "Output tensor([[0.8252],\n",
      "        [0.1376]], device='mps:0')\n",
      "Iteration 70650 Training loss 0.04967256262898445 Validation loss 0.05770687013864517 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.7623],\n",
      "        [0.4014]], device='mps:0')\n",
      "Iteration 70660 Training loss 0.053332794457674026 Validation loss 0.05769352614879608 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.6556],\n",
      "        [0.0445]], device='mps:0')\n",
      "Iteration 70670 Training loss 0.06637035310268402 Validation loss 0.05768686160445213 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0525],\n",
      "        [0.4308]], device='mps:0')\n",
      "Iteration 70680 Training loss 0.054048679769039154 Validation loss 0.05780981853604317 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.3886],\n",
      "        [0.9314]], device='mps:0')\n",
      "Iteration 70690 Training loss 0.05580856278538704 Validation loss 0.05768408998847008 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.9775],\n",
      "        [0.0288]], device='mps:0')\n",
      "Iteration 70700 Training loss 0.0616714172065258 Validation loss 0.057683300226926804 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0346],\n",
      "        [0.1668]], device='mps:0')\n",
      "Iteration 70710 Training loss 0.05194574594497681 Validation loss 0.05767446011304855 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.2406],\n",
      "        [0.9553]], device='mps:0')\n",
      "Iteration 70720 Training loss 0.04935714229941368 Validation loss 0.0576978474855423 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.5247],\n",
      "        [0.8556]], device='mps:0')\n",
      "Iteration 70730 Training loss 0.05863645300269127 Validation loss 0.057703644037246704 Accuracy 0.84312504529953\n",
      "Output tensor([[0.1494],\n",
      "        [0.0778]], device='mps:0')\n",
      "Iteration 70740 Training loss 0.05824310705065727 Validation loss 0.0577060841023922 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.3135],\n",
      "        [0.7297]], device='mps:0')\n",
      "Iteration 70750 Training loss 0.056666068732738495 Validation loss 0.05773765221238136 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0875],\n",
      "        [0.4919]], device='mps:0')\n",
      "Iteration 70760 Training loss 0.06189235672354698 Validation loss 0.05790821835398674 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.6669],\n",
      "        [0.5946]], device='mps:0')\n",
      "Iteration 70770 Training loss 0.053703680634498596 Validation loss 0.057804446667432785 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.8183],\n",
      "        [0.0244]], device='mps:0')\n",
      "Iteration 70780 Training loss 0.04871426522731781 Validation loss 0.05769672989845276 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0329],\n",
      "        [0.9980]], device='mps:0')\n",
      "Iteration 70790 Training loss 0.056352775543928146 Validation loss 0.05766615644097328 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.3740],\n",
      "        [0.9354]], device='mps:0')\n",
      "Iteration 70800 Training loss 0.046382512897253036 Validation loss 0.05785956606268883 Accuracy 0.843375027179718\n",
      "Output tensor([[0.5886],\n",
      "        [0.5521]], device='mps:0')\n",
      "Iteration 70810 Training loss 0.0588233657181263 Validation loss 0.057673580944538116 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.5214],\n",
      "        [0.5548]], device='mps:0')\n",
      "Iteration 70820 Training loss 0.06253207474946976 Validation loss 0.05765774846076965 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9570],\n",
      "        [0.0048]], device='mps:0')\n",
      "Iteration 70830 Training loss 0.05744078382849693 Validation loss 0.05771218240261078 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0199],\n",
      "        [0.0184]], device='mps:0')\n",
      "Iteration 70840 Training loss 0.05851803347468376 Validation loss 0.057708658277988434 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.2169],\n",
      "        [0.2429]], device='mps:0')\n",
      "Iteration 70850 Training loss 0.055953338742256165 Validation loss 0.057669587433338165 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.5892],\n",
      "        [0.1438]], device='mps:0')\n",
      "Iteration 70860 Training loss 0.048701778054237366 Validation loss 0.057705529034137726 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.5936],\n",
      "        [0.8149]], device='mps:0')\n",
      "Iteration 70870 Training loss 0.05814201384782791 Validation loss 0.05765826627612114 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.0213],\n",
      "        [0.0043]], device='mps:0')\n",
      "Iteration 70880 Training loss 0.053337931632995605 Validation loss 0.05766984075307846 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.1247],\n",
      "        [0.9569]], device='mps:0')\n",
      "Iteration 70890 Training loss 0.053168971091508865 Validation loss 0.05768171325325966 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9181],\n",
      "        [0.0042]], device='mps:0')\n",
      "Iteration 70900 Training loss 0.05115611478686333 Validation loss 0.057729728519916534 Accuracy 0.843500018119812\n",
      "Output tensor([[0.3011],\n",
      "        [0.9723]], device='mps:0')\n",
      "Iteration 70910 Training loss 0.050556838512420654 Validation loss 0.0577460341155529 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9398],\n",
      "        [0.4655]], device='mps:0')\n",
      "Iteration 70920 Training loss 0.057153258472681046 Validation loss 0.057721320539712906 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.8669],\n",
      "        [0.0106]], device='mps:0')\n",
      "Iteration 70930 Training loss 0.05839751288294792 Validation loss 0.05766608566045761 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.8988],\n",
      "        [0.7728]], device='mps:0')\n",
      "Iteration 70940 Training loss 0.05467329919338226 Validation loss 0.057671912014484406 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9708],\n",
      "        [0.7681]], device='mps:0')\n",
      "Iteration 70950 Training loss 0.05438681319355965 Validation loss 0.057812418788671494 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.1785],\n",
      "        [0.0559]], device='mps:0')\n",
      "Iteration 70960 Training loss 0.05072172358632088 Validation loss 0.05765967816114426 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.7867],\n",
      "        [0.3072]], device='mps:0')\n",
      "Iteration 70970 Training loss 0.05656231939792633 Validation loss 0.05766725540161133 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9191],\n",
      "        [0.0277]], device='mps:0')\n",
      "Iteration 70980 Training loss 0.054186973720788956 Validation loss 0.05786612257361412 Accuracy 0.843000054359436\n",
      "Output tensor([[0.6501],\n",
      "        [0.9093]], device='mps:0')\n",
      "Iteration 70990 Training loss 0.048982348293066025 Validation loss 0.05766584724187851 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9373],\n",
      "        [0.2503]], device='mps:0')\n",
      "Iteration 71000 Training loss 0.048541273921728134 Validation loss 0.057792048901319504 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7960],\n",
      "        [0.9793]], device='mps:0')\n",
      "Iteration 71010 Training loss 0.04487554356455803 Validation loss 0.05766942352056503 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9109],\n",
      "        [0.0386]], device='mps:0')\n",
      "Iteration 71020 Training loss 0.05409408360719681 Validation loss 0.05768314003944397 Accuracy 0.843000054359436\n",
      "Output tensor([[0.1436],\n",
      "        [0.9927]], device='mps:0')\n",
      "Iteration 71030 Training loss 0.055058181285858154 Validation loss 0.05766206979751587 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.6033],\n",
      "        [0.4618]], device='mps:0')\n",
      "Iteration 71040 Training loss 0.05269377678632736 Validation loss 0.05773836746811867 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.2129],\n",
      "        [0.0032]], device='mps:0')\n",
      "Iteration 71050 Training loss 0.05170103535056114 Validation loss 0.05766180902719498 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9498],\n",
      "        [0.5984]], device='mps:0')\n",
      "Iteration 71060 Training loss 0.04863674193620682 Validation loss 0.05765889585018158 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.8985],\n",
      "        [0.6211]], device='mps:0')\n",
      "Iteration 71070 Training loss 0.052244093269109726 Validation loss 0.05767730996012688 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.8980],\n",
      "        [0.1699]], device='mps:0')\n",
      "Iteration 71080 Training loss 0.05187375843524933 Validation loss 0.057657428085803986 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.1594],\n",
      "        [0.3647]], device='mps:0')\n",
      "Iteration 71090 Training loss 0.04753030091524124 Validation loss 0.05766373872756958 Accuracy 0.843375027179718\n",
      "Output tensor([[0.2957],\n",
      "        [0.0472]], device='mps:0')\n",
      "Iteration 71100 Training loss 0.055650290101766586 Validation loss 0.05774962529540062 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.1159],\n",
      "        [0.8756]], device='mps:0')\n",
      "Iteration 71110 Training loss 0.055092327296733856 Validation loss 0.05765853077173233 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.8860],\n",
      "        [0.9114]], device='mps:0')\n",
      "Iteration 71120 Training loss 0.059038836508989334 Validation loss 0.05776112154126167 Accuracy 0.843375027179718\n",
      "Output tensor([[0.8849],\n",
      "        [0.0280]], device='mps:0')\n",
      "Iteration 71130 Training loss 0.04975641146302223 Validation loss 0.05765928328037262 Accuracy 0.843500018119812\n",
      "Output tensor([[0.2961],\n",
      "        [0.9794]], device='mps:0')\n",
      "Iteration 71140 Training loss 0.0646972581744194 Validation loss 0.05770866200327873 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.3705],\n",
      "        [0.0668]], device='mps:0')\n",
      "Iteration 71150 Training loss 0.056477345526218414 Validation loss 0.0576573982834816 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.4587],\n",
      "        [0.1497]], device='mps:0')\n",
      "Iteration 71160 Training loss 0.05348682403564453 Validation loss 0.057797741144895554 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.6888],\n",
      "        [0.9427]], device='mps:0')\n",
      "Iteration 71170 Training loss 0.06227041035890579 Validation loss 0.05767979845404625 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.1254],\n",
      "        [0.0095]], device='mps:0')\n",
      "Iteration 71180 Training loss 0.05694688484072685 Validation loss 0.05765015259385109 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9524],\n",
      "        [0.8688]], device='mps:0')\n",
      "Iteration 71190 Training loss 0.0550333634018898 Validation loss 0.057748936116695404 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0576],\n",
      "        [0.1657]], device='mps:0')\n",
      "Iteration 71200 Training loss 0.048735275864601135 Validation loss 0.05767401307821274 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0955],\n",
      "        [0.1413]], device='mps:0')\n",
      "Iteration 71210 Training loss 0.05311392992734909 Validation loss 0.05765267461538315 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9612],\n",
      "        [0.0129]], device='mps:0')\n",
      "Iteration 71220 Training loss 0.06134582310914993 Validation loss 0.05775545910000801 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.4067],\n",
      "        [0.9218]], device='mps:0')\n",
      "Iteration 71230 Training loss 0.059624165296554565 Validation loss 0.057677965611219406 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0137],\n",
      "        [0.9968]], device='mps:0')\n",
      "Iteration 71240 Training loss 0.05430028960108757 Validation loss 0.0576443187892437 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9453],\n",
      "        [0.0683]], device='mps:0')\n",
      "Iteration 71250 Training loss 0.049450911581516266 Validation loss 0.057717859745025635 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9616],\n",
      "        [0.6929]], device='mps:0')\n",
      "Iteration 71260 Training loss 0.047841377556324005 Validation loss 0.05773795768618584 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.6717],\n",
      "        [0.2243]], device='mps:0')\n",
      "Iteration 71270 Training loss 0.04884156957268715 Validation loss 0.05774340778589249 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0412],\n",
      "        [0.9700]], device='mps:0')\n",
      "Iteration 71280 Training loss 0.04985235258936882 Validation loss 0.05765127018094063 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0194],\n",
      "        [0.0041]], device='mps:0')\n",
      "Iteration 71290 Training loss 0.04551849141716957 Validation loss 0.05785529688000679 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9815],\n",
      "        [0.8818]], device='mps:0')\n",
      "Iteration 71300 Training loss 0.05104278400540352 Validation loss 0.057759642601013184 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0979],\n",
      "        [0.8712]], device='mps:0')\n",
      "Iteration 71310 Training loss 0.05455872789025307 Validation loss 0.05767703801393509 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0489],\n",
      "        [0.0427]], device='mps:0')\n",
      "Iteration 71320 Training loss 0.050232380628585815 Validation loss 0.057639408856630325 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.7847],\n",
      "        [0.8506]], device='mps:0')\n",
      "Iteration 71330 Training loss 0.05469881743192673 Validation loss 0.05763933062553406 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9368],\n",
      "        [0.5741]], device='mps:0')\n",
      "Iteration 71340 Training loss 0.04895279183983803 Validation loss 0.05770033970475197 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.5341],\n",
      "        [0.5622]], device='mps:0')\n",
      "Iteration 71350 Training loss 0.04968979209661484 Validation loss 0.057664431631565094 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9958],\n",
      "        [0.1398]], device='mps:0')\n",
      "Iteration 71360 Training loss 0.054281748831272125 Validation loss 0.057643335312604904 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.5595],\n",
      "        [0.5569]], device='mps:0')\n",
      "Iteration 71370 Training loss 0.06202847883105278 Validation loss 0.057862263172864914 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.9156],\n",
      "        [0.6841]], device='mps:0')\n",
      "Iteration 71380 Training loss 0.04619983583688736 Validation loss 0.05765671283006668 Accuracy 0.843250036239624\n",
      "Output tensor([[0.1067],\n",
      "        [0.8551]], device='mps:0')\n",
      "Iteration 71390 Training loss 0.05052761733531952 Validation loss 0.057627953588962555 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0309],\n",
      "        [0.9154]], device='mps:0')\n",
      "Iteration 71400 Training loss 0.05004796013236046 Validation loss 0.057725321501493454 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.3772],\n",
      "        [0.9771]], device='mps:0')\n",
      "Iteration 71410 Training loss 0.052400246262550354 Validation loss 0.05762789398431778 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9947],\n",
      "        [0.1589]], device='mps:0')\n",
      "Iteration 71420 Training loss 0.04721427336335182 Validation loss 0.05774581432342529 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0701],\n",
      "        [0.2867]], device='mps:0')\n",
      "Iteration 71430 Training loss 0.049918968230485916 Validation loss 0.05763779580593109 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0730],\n",
      "        [0.9386]], device='mps:0')\n",
      "Iteration 71440 Training loss 0.05200326442718506 Validation loss 0.057738322764635086 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.1207],\n",
      "        [0.1026]], device='mps:0')\n",
      "Iteration 71450 Training loss 0.05302150547504425 Validation loss 0.05764487385749817 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.7024],\n",
      "        [0.1219]], device='mps:0')\n",
      "Iteration 71460 Training loss 0.05919007956981659 Validation loss 0.05765341594815254 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.6525],\n",
      "        [0.6589]], device='mps:0')\n",
      "Iteration 71470 Training loss 0.04862609878182411 Validation loss 0.05762549489736557 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9601],\n",
      "        [0.1920]], device='mps:0')\n",
      "Iteration 71480 Training loss 0.04915212094783783 Validation loss 0.0577009953558445 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.6400],\n",
      "        [0.9182]], device='mps:0')\n",
      "Iteration 71490 Training loss 0.04254797473549843 Validation loss 0.05769134312868118 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0315],\n",
      "        [0.6574]], device='mps:0')\n",
      "Iteration 71500 Training loss 0.06155405938625336 Validation loss 0.05762118101119995 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.7498],\n",
      "        [0.0554]], device='mps:0')\n",
      "Iteration 71510 Training loss 0.053707022219896317 Validation loss 0.057618580758571625 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.6673],\n",
      "        [0.5896]], device='mps:0')\n",
      "Iteration 71520 Training loss 0.041975997388362885 Validation loss 0.057648226618766785 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9047],\n",
      "        [0.9935]], device='mps:0')\n",
      "Iteration 71530 Training loss 0.0581810362637043 Validation loss 0.05763966962695122 Accuracy 0.843375027179718\n",
      "Output tensor([[0.8215],\n",
      "        [0.1458]], device='mps:0')\n",
      "Iteration 71540 Training loss 0.05090459808707237 Validation loss 0.058042723685503006 Accuracy 0.8410000205039978\n",
      "Output tensor([[0.3342],\n",
      "        [0.8093]], device='mps:0')\n",
      "Iteration 71550 Training loss 0.05454978719353676 Validation loss 0.0578678660094738 Accuracy 0.843250036239624\n",
      "Output tensor([[0.8589],\n",
      "        [0.5344]], device='mps:0')\n",
      "Iteration 71560 Training loss 0.05448172613978386 Validation loss 0.057631295174360275 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.7346],\n",
      "        [0.9350]], device='mps:0')\n",
      "Iteration 71570 Training loss 0.05199605971574783 Validation loss 0.05782423913478851 Accuracy 0.8415000438690186\n",
      "Output tensor([[0.2903],\n",
      "        [0.9466]], device='mps:0')\n",
      "Iteration 71580 Training loss 0.04806583374738693 Validation loss 0.057622745633125305 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9023],\n",
      "        [0.9496]], device='mps:0')\n",
      "Iteration 71590 Training loss 0.050942763686180115 Validation loss 0.05768595263361931 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9599],\n",
      "        [0.9895]], device='mps:0')\n",
      "Iteration 71600 Training loss 0.056109096854925156 Validation loss 0.057614557445049286 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0127],\n",
      "        [0.6736]], device='mps:0')\n",
      "Iteration 71610 Training loss 0.061975717544555664 Validation loss 0.05769611895084381 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0502],\n",
      "        [0.1639]], device='mps:0')\n",
      "Iteration 71620 Training loss 0.05780507251620293 Validation loss 0.05777604505419731 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.4449],\n",
      "        [0.2547]], device='mps:0')\n",
      "Iteration 71630 Training loss 0.05310680344700813 Validation loss 0.057677093893289566 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.8094],\n",
      "        [0.0232]], device='mps:0')\n",
      "Iteration 71640 Training loss 0.06312371045351028 Validation loss 0.05765940248966217 Accuracy 0.842875063419342\n",
      "Output tensor([[0.3198],\n",
      "        [0.7779]], device='mps:0')\n",
      "Iteration 71650 Training loss 0.052408430725336075 Validation loss 0.05762568116188049 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.2400],\n",
      "        [0.5784]], device='mps:0')\n",
      "Iteration 71660 Training loss 0.0544247031211853 Validation loss 0.057637911289930344 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.4611],\n",
      "        [0.9852]], device='mps:0')\n",
      "Iteration 71670 Training loss 0.04965154081583023 Validation loss 0.05761386454105377 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.5759],\n",
      "        [0.2712]], device='mps:0')\n",
      "Iteration 71680 Training loss 0.06176295131444931 Validation loss 0.057611893862485886 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.4587],\n",
      "        [0.0611]], device='mps:0')\n",
      "Iteration 71690 Training loss 0.04909906163811684 Validation loss 0.05791840702295303 Accuracy 0.84312504529953\n",
      "Output tensor([[0.1358],\n",
      "        [0.3374]], device='mps:0')\n",
      "Iteration 71700 Training loss 0.05400853976607323 Validation loss 0.05768125504255295 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.1026],\n",
      "        [0.2081]], device='mps:0')\n",
      "Iteration 71710 Training loss 0.04683390632271767 Validation loss 0.057709623128175735 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.7597],\n",
      "        [0.1834]], device='mps:0')\n",
      "Iteration 71720 Training loss 0.04700397327542305 Validation loss 0.05781792849302292 Accuracy 0.8413750529289246\n",
      "Output tensor([[0.0098],\n",
      "        [0.9200]], device='mps:0')\n",
      "Iteration 71730 Training loss 0.05410565435886383 Validation loss 0.05769168958067894 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7575],\n",
      "        [0.0159]], device='mps:0')\n",
      "Iteration 71740 Training loss 0.04680612310767174 Validation loss 0.05773423984646797 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.6893],\n",
      "        [0.1729]], device='mps:0')\n",
      "Iteration 71750 Training loss 0.053738515824079514 Validation loss 0.05768779292702675 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0053],\n",
      "        [0.9074]], device='mps:0')\n",
      "Iteration 71760 Training loss 0.04487506300210953 Validation loss 0.05761757493019104 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9360],\n",
      "        [0.3979]], device='mps:0')\n",
      "Iteration 71770 Training loss 0.0597510039806366 Validation loss 0.057646654546260834 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1900],\n",
      "        [0.0947]], device='mps:0')\n",
      "Iteration 71780 Training loss 0.04929094389081001 Validation loss 0.05766000598669052 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9798],\n",
      "        [0.9891]], device='mps:0')\n",
      "Iteration 71790 Training loss 0.047892630100250244 Validation loss 0.05762699246406555 Accuracy 0.843000054359436\n",
      "Output tensor([[0.8180],\n",
      "        [0.9510]], device='mps:0')\n",
      "Iteration 71800 Training loss 0.05680817365646362 Validation loss 0.057656869292259216 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9432],\n",
      "        [0.9851]], device='mps:0')\n",
      "Iteration 71810 Training loss 0.05476566031575203 Validation loss 0.05763319507241249 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.6590],\n",
      "        [0.9612]], device='mps:0')\n",
      "Iteration 71820 Training loss 0.04562774673104286 Validation loss 0.057717692106962204 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9763],\n",
      "        [0.0065]], device='mps:0')\n",
      "Iteration 71830 Training loss 0.053293418139219284 Validation loss 0.05762067809700966 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.8668],\n",
      "        [0.6837]], device='mps:0')\n",
      "Iteration 71840 Training loss 0.05714597925543785 Validation loss 0.057759761810302734 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.7552],\n",
      "        [0.6030]], device='mps:0')\n",
      "Iteration 71850 Training loss 0.05075092241168022 Validation loss 0.05763803422451019 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9007],\n",
      "        [0.6306]], device='mps:0')\n",
      "Iteration 71860 Training loss 0.05718318000435829 Validation loss 0.057636599987745285 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0367],\n",
      "        [0.1857]], device='mps:0')\n",
      "Iteration 71870 Training loss 0.05531903728842735 Validation loss 0.05762305110692978 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0958],\n",
      "        [0.9496]], device='mps:0')\n",
      "Iteration 71880 Training loss 0.05453409254550934 Validation loss 0.0576239638030529 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0328],\n",
      "        [0.5496]], device='mps:0')\n",
      "Iteration 71890 Training loss 0.05042487010359764 Validation loss 0.057677049189805984 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9854],\n",
      "        [0.0371]], device='mps:0')\n",
      "Iteration 71900 Training loss 0.07224481552839279 Validation loss 0.05762108042836189 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9618],\n",
      "        [0.9884]], device='mps:0')\n",
      "Iteration 71910 Training loss 0.05255701020359993 Validation loss 0.05812480300664902 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.3146],\n",
      "        [0.7763]], device='mps:0')\n",
      "Iteration 71920 Training loss 0.05619347468018532 Validation loss 0.05772591382265091 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9720],\n",
      "        [0.0772]], device='mps:0')\n",
      "Iteration 71930 Training loss 0.050682563334703445 Validation loss 0.057662758976221085 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9595],\n",
      "        [0.2293]], device='mps:0')\n",
      "Iteration 71940 Training loss 0.05163426324725151 Validation loss 0.05765539035201073 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.4007],\n",
      "        [0.9802]], device='mps:0')\n",
      "Iteration 71950 Training loss 0.046500176191329956 Validation loss 0.05761779472231865 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9932],\n",
      "        [0.9700]], device='mps:0')\n",
      "Iteration 71960 Training loss 0.048218078911304474 Validation loss 0.05766898766160011 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0357],\n",
      "        [0.0352]], device='mps:0')\n",
      "Iteration 71970 Training loss 0.05165417492389679 Validation loss 0.05780705437064171 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.2973],\n",
      "        [0.9742]], device='mps:0')\n",
      "Iteration 71980 Training loss 0.05691183730959892 Validation loss 0.05761927366256714 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9908],\n",
      "        [0.8856]], device='mps:0')\n",
      "Iteration 71990 Training loss 0.05780986696481705 Validation loss 0.05766492336988449 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.0205],\n",
      "        [0.2481]], device='mps:0')\n",
      "Iteration 72000 Training loss 0.050155892968177795 Validation loss 0.05768394097685814 Accuracy 0.843500018119812\n",
      "Output tensor([[0.8334],\n",
      "        [0.9922]], device='mps:0')\n",
      "Iteration 72010 Training loss 0.05170528590679169 Validation loss 0.057658601552248 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0384],\n",
      "        [0.6226]], device='mps:0')\n",
      "Iteration 72020 Training loss 0.0482884980738163 Validation loss 0.057617612183094025 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.4245],\n",
      "        [0.0231]], device='mps:0')\n",
      "Iteration 72030 Training loss 0.06082816794514656 Validation loss 0.057612087577581406 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1071],\n",
      "        [0.1100]], device='mps:0')\n",
      "Iteration 72040 Training loss 0.05954906716942787 Validation loss 0.0576125867664814 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9010],\n",
      "        [0.9506]], device='mps:0')\n",
      "Iteration 72050 Training loss 0.04774068668484688 Validation loss 0.05760832875967026 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.2799],\n",
      "        [0.9593]], device='mps:0')\n",
      "Iteration 72060 Training loss 0.058200445026159286 Validation loss 0.05766313523054123 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.8990],\n",
      "        [0.9577]], device='mps:0')\n",
      "Iteration 72070 Training loss 0.05175788700580597 Validation loss 0.05759498104453087 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.8176],\n",
      "        [0.9388]], device='mps:0')\n",
      "Iteration 72080 Training loss 0.05179983004927635 Validation loss 0.05761634185910225 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9889],\n",
      "        [0.2025]], device='mps:0')\n",
      "Iteration 72090 Training loss 0.04787677153944969 Validation loss 0.057598598301410675 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.5782],\n",
      "        [0.3751]], device='mps:0')\n",
      "Iteration 72100 Training loss 0.053803037852048874 Validation loss 0.05788399651646614 Accuracy 0.8416250348091125\n",
      "Output tensor([[0.6046],\n",
      "        [0.8402]], device='mps:0')\n",
      "Iteration 72110 Training loss 0.04901672154664993 Validation loss 0.05777183920145035 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9028],\n",
      "        [0.1221]], device='mps:0')\n",
      "Iteration 72120 Training loss 0.052610572427511215 Validation loss 0.05763259157538414 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0491],\n",
      "        [0.5875]], device='mps:0')\n",
      "Iteration 72130 Training loss 0.054706115275621414 Validation loss 0.05765584111213684 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1105],\n",
      "        [0.0530]], device='mps:0')\n",
      "Iteration 72140 Training loss 0.05399254709482193 Validation loss 0.05765806883573532 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.8186],\n",
      "        [0.7871]], device='mps:0')\n",
      "Iteration 72150 Training loss 0.056196678429841995 Validation loss 0.05760762467980385 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9757],\n",
      "        [0.8904]], device='mps:0')\n",
      "Iteration 72160 Training loss 0.054124753922224045 Validation loss 0.05761788785457611 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9634],\n",
      "        [0.0652]], device='mps:0')\n",
      "Iteration 72170 Training loss 0.05029073730111122 Validation loss 0.05759875476360321 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9887],\n",
      "        [0.9778]], device='mps:0')\n",
      "Iteration 72180 Training loss 0.05393201857805252 Validation loss 0.05759216099977493 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0103],\n",
      "        [0.8690]], device='mps:0')\n",
      "Iteration 72190 Training loss 0.05714884027838707 Validation loss 0.0577259287238121 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9528],\n",
      "        [0.8544]], device='mps:0')\n",
      "Iteration 72200 Training loss 0.055707354098558426 Validation loss 0.05762491747736931 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0102],\n",
      "        [0.8480]], device='mps:0')\n",
      "Iteration 72210 Training loss 0.053090933710336685 Validation loss 0.05768294259905815 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9540],\n",
      "        [0.9115]], device='mps:0')\n",
      "Iteration 72220 Training loss 0.05105796828866005 Validation loss 0.057664982974529266 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.4570],\n",
      "        [0.1093]], device='mps:0')\n",
      "Iteration 72230 Training loss 0.0597543939948082 Validation loss 0.05764925479888916 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.5832],\n",
      "        [0.0057]], device='mps:0')\n",
      "Iteration 72240 Training loss 0.059291109442710876 Validation loss 0.05759961158037186 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.2340],\n",
      "        [0.8192]], device='mps:0')\n",
      "Iteration 72250 Training loss 0.06638612598180771 Validation loss 0.05761876329779625 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9894],\n",
      "        [0.2945]], device='mps:0')\n",
      "Iteration 72260 Training loss 0.04824848100543022 Validation loss 0.05767614394426346 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.1491],\n",
      "        [0.8155]], device='mps:0')\n",
      "Iteration 72270 Training loss 0.052095770835876465 Validation loss 0.057701319456100464 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.2594],\n",
      "        [0.4458]], device='mps:0')\n",
      "Iteration 72280 Training loss 0.05833366885781288 Validation loss 0.05760163441300392 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0508],\n",
      "        [0.9647]], device='mps:0')\n",
      "Iteration 72290 Training loss 0.06191810593008995 Validation loss 0.057633787393569946 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0430],\n",
      "        [0.8546]], device='mps:0')\n",
      "Iteration 72300 Training loss 0.044917285442352295 Validation loss 0.05759979411959648 Accuracy 0.843250036239624\n",
      "Output tensor([[0.8834],\n",
      "        [0.6883]], device='mps:0')\n",
      "Iteration 72310 Training loss 0.058199141174554825 Validation loss 0.0576048269867897 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9956],\n",
      "        [0.4936]], device='mps:0')\n",
      "Iteration 72320 Training loss 0.05226922035217285 Validation loss 0.057665884494781494 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.0325],\n",
      "        [0.4768]], device='mps:0')\n",
      "Iteration 72330 Training loss 0.04714911803603172 Validation loss 0.057614993304014206 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.6552],\n",
      "        [0.9222]], device='mps:0')\n",
      "Iteration 72340 Training loss 0.05408158153295517 Validation loss 0.05762343853712082 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.4911],\n",
      "        [0.9626]], device='mps:0')\n",
      "Iteration 72350 Training loss 0.05875186249613762 Validation loss 0.05773576349020004 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.1820],\n",
      "        [0.8367]], device='mps:0')\n",
      "Iteration 72360 Training loss 0.05732981488108635 Validation loss 0.057597219944000244 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0970],\n",
      "        [0.0352]], device='mps:0')\n",
      "Iteration 72370 Training loss 0.05203283578157425 Validation loss 0.05763566121459007 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0160],\n",
      "        [0.9258]], device='mps:0')\n",
      "Iteration 72380 Training loss 0.054790813475847244 Validation loss 0.057589516043663025 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9552],\n",
      "        [0.0408]], device='mps:0')\n",
      "Iteration 72390 Training loss 0.059710998088121414 Validation loss 0.057606372982263565 Accuracy 0.843500018119812\n",
      "Output tensor([[0.8855],\n",
      "        [0.8741]], device='mps:0')\n",
      "Iteration 72400 Training loss 0.05288417264819145 Validation loss 0.057680416852235794 Accuracy 0.843500018119812\n",
      "Output tensor([[0.7064],\n",
      "        [0.0039]], device='mps:0')\n",
      "Iteration 72410 Training loss 0.06035235896706581 Validation loss 0.05759244039654732 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.8599],\n",
      "        [0.3446]], device='mps:0')\n",
      "Iteration 72420 Training loss 0.05313907563686371 Validation loss 0.05761735141277313 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.8990],\n",
      "        [0.2162]], device='mps:0')\n",
      "Iteration 72430 Training loss 0.05038601905107498 Validation loss 0.05770302191376686 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.1003],\n",
      "        [0.8824]], device='mps:0')\n",
      "Iteration 72440 Training loss 0.055683281272649765 Validation loss 0.05781593546271324 Accuracy 0.843500018119812\n",
      "Output tensor([[0.2283],\n",
      "        [0.9322]], device='mps:0')\n",
      "Iteration 72450 Training loss 0.04780832305550575 Validation loss 0.057591814547777176 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0586],\n",
      "        [0.9296]], device='mps:0')\n",
      "Iteration 72460 Training loss 0.05945022776722908 Validation loss 0.05761611461639404 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9153],\n",
      "        [0.0978]], device='mps:0')\n",
      "Iteration 72470 Training loss 0.05967435613274574 Validation loss 0.05766252800822258 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0493],\n",
      "        [0.2610]], device='mps:0')\n",
      "Iteration 72480 Training loss 0.05370559170842171 Validation loss 0.05761456489562988 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1380],\n",
      "        [0.0846]], device='mps:0')\n",
      "Iteration 72490 Training loss 0.055368222296237946 Validation loss 0.05798066779971123 Accuracy 0.84312504529953\n",
      "Output tensor([[0.3039],\n",
      "        [0.6787]], device='mps:0')\n",
      "Iteration 72500 Training loss 0.046381525695323944 Validation loss 0.05762406438589096 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9616],\n",
      "        [0.0272]], device='mps:0')\n",
      "Iteration 72510 Training loss 0.05256076529622078 Validation loss 0.05760328471660614 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9043],\n",
      "        [0.2434]], device='mps:0')\n",
      "Iteration 72520 Training loss 0.05287698656320572 Validation loss 0.057730477303266525 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.6366],\n",
      "        [0.8724]], device='mps:0')\n",
      "Iteration 72530 Training loss 0.05425424501299858 Validation loss 0.05762186646461487 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.4939],\n",
      "        [0.9649]], device='mps:0')\n",
      "Iteration 72540 Training loss 0.04430374130606651 Validation loss 0.05759025365114212 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.5996],\n",
      "        [0.0096]], device='mps:0')\n",
      "Iteration 72550 Training loss 0.05187954753637314 Validation loss 0.05758371204137802 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9672],\n",
      "        [0.9456]], device='mps:0')\n",
      "Iteration 72560 Training loss 0.051295969635248184 Validation loss 0.05759450048208237 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.8304],\n",
      "        [0.9934]], device='mps:0')\n",
      "Iteration 72570 Training loss 0.05723338946700096 Validation loss 0.05757162347435951 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.0814],\n",
      "        [0.0740]], device='mps:0')\n",
      "Iteration 72580 Training loss 0.057941120117902756 Validation loss 0.05759669095277786 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9339],\n",
      "        [0.6140]], device='mps:0')\n",
      "Iteration 72590 Training loss 0.057619743049144745 Validation loss 0.05757136270403862 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.1374],\n",
      "        [0.0376]], device='mps:0')\n",
      "Iteration 72600 Training loss 0.05452386662364006 Validation loss 0.05765140801668167 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0760],\n",
      "        [0.2296]], device='mps:0')\n",
      "Iteration 72610 Training loss 0.060441236943006516 Validation loss 0.057595834136009216 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.7133],\n",
      "        [0.9303]], device='mps:0')\n",
      "Iteration 72620 Training loss 0.05921156704425812 Validation loss 0.057571254670619965 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.8660],\n",
      "        [0.9699]], device='mps:0')\n",
      "Iteration 72630 Training loss 0.05310887098312378 Validation loss 0.05769359692931175 Accuracy 0.843500018119812\n",
      "Output tensor([[0.7445],\n",
      "        [0.9471]], device='mps:0')\n",
      "Iteration 72640 Training loss 0.04942726343870163 Validation loss 0.05769555643200874 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0158],\n",
      "        [0.0146]], device='mps:0')\n",
      "Iteration 72650 Training loss 0.047357816249132156 Validation loss 0.057957179844379425 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9336],\n",
      "        [0.6252]], device='mps:0')\n",
      "Iteration 72660 Training loss 0.052546098828315735 Validation loss 0.057669155299663544 Accuracy 0.842875063419342\n",
      "Output tensor([[0.3508],\n",
      "        [0.0069]], device='mps:0')\n",
      "Iteration 72670 Training loss 0.05351618677377701 Validation loss 0.057570163160562515 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.7483],\n",
      "        [0.3274]], device='mps:0')\n",
      "Iteration 72680 Training loss 0.05822816863656044 Validation loss 0.05757438391447067 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.5033],\n",
      "        [0.8753]], device='mps:0')\n",
      "Iteration 72690 Training loss 0.05962499603629112 Validation loss 0.05758080258965492 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.7207],\n",
      "        [0.5076]], device='mps:0')\n",
      "Iteration 72700 Training loss 0.04868176206946373 Validation loss 0.05758020654320717 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.4550],\n",
      "        [0.0628]], device='mps:0')\n",
      "Iteration 72710 Training loss 0.051687754690647125 Validation loss 0.057586297392845154 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.7834],\n",
      "        [0.0122]], device='mps:0')\n",
      "Iteration 72720 Training loss 0.0601947158575058 Validation loss 0.057587217539548874 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0852],\n",
      "        [0.8389]], device='mps:0')\n",
      "Iteration 72730 Training loss 0.05070074275135994 Validation loss 0.05768963694572449 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.6751],\n",
      "        [0.9351]], device='mps:0')\n",
      "Iteration 72740 Training loss 0.042849186807870865 Validation loss 0.057665545493364334 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.6734],\n",
      "        [0.9843]], device='mps:0')\n",
      "Iteration 72750 Training loss 0.058484334498643875 Validation loss 0.057570796459913254 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.7704],\n",
      "        [0.0711]], device='mps:0')\n",
      "Iteration 72760 Training loss 0.051959361881017685 Validation loss 0.05756070464849472 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.7589],\n",
      "        [0.0389]], device='mps:0')\n",
      "Iteration 72770 Training loss 0.05104859545826912 Validation loss 0.057578783482313156 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0383],\n",
      "        [0.8083]], device='mps:0')\n",
      "Iteration 72780 Training loss 0.0512714721262455 Validation loss 0.05757874995470047 Accuracy 0.84312504529953\n",
      "Output tensor([[0.2709],\n",
      "        [0.8382]], device='mps:0')\n",
      "Iteration 72790 Training loss 0.06103551760315895 Validation loss 0.05763540044426918 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0425],\n",
      "        [0.0026]], device='mps:0')\n",
      "Iteration 72800 Training loss 0.05059611052274704 Validation loss 0.05758277326822281 Accuracy 0.84312504529953\n",
      "Output tensor([[0.8627],\n",
      "        [0.9627]], device='mps:0')\n",
      "Iteration 72810 Training loss 0.05644391477108002 Validation loss 0.05755036324262619 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0342],\n",
      "        [0.1007]], device='mps:0')\n",
      "Iteration 72820 Training loss 0.053500350564718246 Validation loss 0.05772049352526665 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.3698],\n",
      "        [0.1131]], device='mps:0')\n",
      "Iteration 72830 Training loss 0.04874509572982788 Validation loss 0.057583507150411606 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9354],\n",
      "        [0.1890]], device='mps:0')\n",
      "Iteration 72840 Training loss 0.0548500157892704 Validation loss 0.05770868435502052 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9758],\n",
      "        [0.2567]], device='mps:0')\n",
      "Iteration 72850 Training loss 0.06372775882482529 Validation loss 0.05759580805897713 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1839],\n",
      "        [0.0664]], device='mps:0')\n",
      "Iteration 72860 Training loss 0.05156108736991882 Validation loss 0.057549141347408295 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.7523],\n",
      "        [0.0637]], device='mps:0')\n",
      "Iteration 72870 Training loss 0.048400696367025375 Validation loss 0.05808233842253685 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.8659],\n",
      "        [0.9436]], device='mps:0')\n",
      "Iteration 72880 Training loss 0.04968378320336342 Validation loss 0.05760617181658745 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.9203],\n",
      "        [0.6613]], device='mps:0')\n",
      "Iteration 72890 Training loss 0.05720032379031181 Validation loss 0.057549625635147095 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0695],\n",
      "        [0.1136]], device='mps:0')\n",
      "Iteration 72900 Training loss 0.060172975063323975 Validation loss 0.057549942284822464 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.5224],\n",
      "        [0.8661]], device='mps:0')\n",
      "Iteration 72910 Training loss 0.05449170991778374 Validation loss 0.05755108967423439 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.8786],\n",
      "        [0.8880]], device='mps:0')\n",
      "Iteration 72920 Training loss 0.05966649577021599 Validation loss 0.05756356194615364 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.5101],\n",
      "        [0.5498]], device='mps:0')\n",
      "Iteration 72930 Training loss 0.0478336364030838 Validation loss 0.0575779527425766 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0531],\n",
      "        [0.6940]], device='mps:0')\n",
      "Iteration 72940 Training loss 0.0620974637567997 Validation loss 0.05758114904165268 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.4309],\n",
      "        [0.4302]], device='mps:0')\n",
      "Iteration 72950 Training loss 0.05789906531572342 Validation loss 0.057550281286239624 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.8749],\n",
      "        [0.9864]], device='mps:0')\n",
      "Iteration 72960 Training loss 0.0526450052857399 Validation loss 0.057583604007959366 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9118],\n",
      "        [0.1469]], device='mps:0')\n",
      "Iteration 72970 Training loss 0.05487777665257454 Validation loss 0.057544443756341934 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.7885],\n",
      "        [0.9370]], device='mps:0')\n",
      "Iteration 72980 Training loss 0.05662604048848152 Validation loss 0.057543329894542694 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0597],\n",
      "        [0.9891]], device='mps:0')\n",
      "Iteration 72990 Training loss 0.04714580252766609 Validation loss 0.057575494050979614 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9709],\n",
      "        [0.6588]], device='mps:0')\n",
      "Iteration 73000 Training loss 0.049688950181007385 Validation loss 0.057590335607528687 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9059],\n",
      "        [0.1860]], device='mps:0')\n",
      "Iteration 73010 Training loss 0.047566499561071396 Validation loss 0.057639505714178085 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.1175],\n",
      "        [0.7838]], device='mps:0')\n",
      "Iteration 73020 Training loss 0.05473984777927399 Validation loss 0.05760818347334862 Accuracy 0.843500018119812\n",
      "Output tensor([[0.3319],\n",
      "        [0.9064]], device='mps:0')\n",
      "Iteration 73030 Training loss 0.05438859015703201 Validation loss 0.05758330598473549 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0093],\n",
      "        [0.9041]], device='mps:0')\n",
      "Iteration 73040 Training loss 0.04919067397713661 Validation loss 0.05753408372402191 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9330],\n",
      "        [0.0143]], device='mps:0')\n",
      "Iteration 73050 Training loss 0.059467922896146774 Validation loss 0.05755264684557915 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9131],\n",
      "        [0.9007]], device='mps:0')\n",
      "Iteration 73060 Training loss 0.053493622690439224 Validation loss 0.05757273733615875 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1646],\n",
      "        [0.9921]], device='mps:0')\n",
      "Iteration 73070 Training loss 0.054906971752643585 Validation loss 0.057543374598026276 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.5362],\n",
      "        [0.4182]], device='mps:0')\n",
      "Iteration 73080 Training loss 0.05278904363512993 Validation loss 0.05754547193646431 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0491],\n",
      "        [0.6810]], device='mps:0')\n",
      "Iteration 73090 Training loss 0.06019028648734093 Validation loss 0.05752715840935707 Accuracy 0.843375027179718\n",
      "Output tensor([[0.8593],\n",
      "        [0.0199]], device='mps:0')\n",
      "Iteration 73100 Training loss 0.05663278326392174 Validation loss 0.05761372670531273 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9218],\n",
      "        [0.0576]], device='mps:0')\n",
      "Iteration 73110 Training loss 0.04853421077132225 Validation loss 0.057541538029909134 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.1461],\n",
      "        [0.0431]], device='mps:0')\n",
      "Iteration 73120 Training loss 0.059864796698093414 Validation loss 0.057597946375608444 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.7410],\n",
      "        [0.0118]], device='mps:0')\n",
      "Iteration 73130 Training loss 0.049337901175022125 Validation loss 0.057538386434316635 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.8081],\n",
      "        [0.3693]], device='mps:0')\n",
      "Iteration 73140 Training loss 0.05726158991456032 Validation loss 0.057612910866737366 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.1110],\n",
      "        [0.8678]], device='mps:0')\n",
      "Iteration 73150 Training loss 0.0538930781185627 Validation loss 0.05752192437648773 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9879],\n",
      "        [0.1380]], device='mps:0')\n",
      "Iteration 73160 Training loss 0.055098287761211395 Validation loss 0.05766918510198593 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.7843],\n",
      "        [0.0527]], device='mps:0')\n",
      "Iteration 73170 Training loss 0.053839948028326035 Validation loss 0.057523369789123535 Accuracy 0.843250036239624\n",
      "Output tensor([[0.3328],\n",
      "        [0.0214]], device='mps:0')\n",
      "Iteration 73180 Training loss 0.06071674823760986 Validation loss 0.057521719485521317 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.4981],\n",
      "        [0.9621]], device='mps:0')\n",
      "Iteration 73190 Training loss 0.048026178032159805 Validation loss 0.05770519748330116 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.7712],\n",
      "        [0.2249]], device='mps:0')\n",
      "Iteration 73200 Training loss 0.06059711426496506 Validation loss 0.057682424783706665 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.3536],\n",
      "        [0.8262]], device='mps:0')\n",
      "Iteration 73210 Training loss 0.05921148136258125 Validation loss 0.057523973286151886 Accuracy 0.843500018119812\n",
      "Output tensor([[0.4170],\n",
      "        [0.8990]], device='mps:0')\n",
      "Iteration 73220 Training loss 0.04954720288515091 Validation loss 0.0576239638030529 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0195],\n",
      "        [0.7244]], device='mps:0')\n",
      "Iteration 73230 Training loss 0.049686089158058167 Validation loss 0.05753539502620697 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.6335],\n",
      "        [0.1029]], device='mps:0')\n",
      "Iteration 73240 Training loss 0.05922723934054375 Validation loss 0.05752596631646156 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0232],\n",
      "        [0.4991]], device='mps:0')\n",
      "Iteration 73250 Training loss 0.05509364604949951 Validation loss 0.05758029595017433 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0660],\n",
      "        [0.7934]], device='mps:0')\n",
      "Iteration 73260 Training loss 0.05136151239275932 Validation loss 0.05767147243022919 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.5423],\n",
      "        [0.1071]], device='mps:0')\n",
      "Iteration 73270 Training loss 0.04610249027609825 Validation loss 0.05766363441944122 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9644],\n",
      "        [0.9161]], device='mps:0')\n",
      "Iteration 73280 Training loss 0.05950997769832611 Validation loss 0.057546306401491165 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9142],\n",
      "        [0.0666]], device='mps:0')\n",
      "Iteration 73290 Training loss 0.050234731286764145 Validation loss 0.05762752145528793 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.7866],\n",
      "        [0.9294]], device='mps:0')\n",
      "Iteration 73300 Training loss 0.05253370478749275 Validation loss 0.05751528590917587 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0387],\n",
      "        [0.0019]], device='mps:0')\n",
      "Iteration 73310 Training loss 0.060953617095947266 Validation loss 0.057595640420913696 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.0321],\n",
      "        [0.0992]], device='mps:0')\n",
      "Iteration 73320 Training loss 0.05364752560853958 Validation loss 0.05751359090209007 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0928],\n",
      "        [0.9875]], device='mps:0')\n",
      "Iteration 73330 Training loss 0.04839034751057625 Validation loss 0.057523950934410095 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.1796],\n",
      "        [0.1063]], device='mps:0')\n",
      "Iteration 73340 Training loss 0.05419829115271568 Validation loss 0.057539213448762894 Accuracy 0.84312504529953\n",
      "Output tensor([[0.2944],\n",
      "        [0.7185]], device='mps:0')\n",
      "Iteration 73350 Training loss 0.05149930343031883 Validation loss 0.057534441351890564 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9327],\n",
      "        [0.7831]], device='mps:0')\n",
      "Iteration 73360 Training loss 0.053192056715488434 Validation loss 0.05766391381621361 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9829],\n",
      "        [0.8937]], device='mps:0')\n",
      "Iteration 73370 Training loss 0.04893810302019119 Validation loss 0.057511720806360245 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9147],\n",
      "        [0.8980]], device='mps:0')\n",
      "Iteration 73380 Training loss 0.05418519675731659 Validation loss 0.058215465396642685 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.9708],\n",
      "        [0.5967]], device='mps:0')\n",
      "Iteration 73390 Training loss 0.05503533408045769 Validation loss 0.057759661227464676 Accuracy 0.843375027179718\n",
      "Output tensor([[0.4650],\n",
      "        [0.8892]], device='mps:0')\n",
      "Iteration 73400 Training loss 0.050215449184179306 Validation loss 0.05750846117734909 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.5352],\n",
      "        [0.1017]], device='mps:0')\n",
      "Iteration 73410 Training loss 0.043038032948970795 Validation loss 0.05761590972542763 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.8507],\n",
      "        [0.1713]], device='mps:0')\n",
      "Iteration 73420 Training loss 0.05303724482655525 Validation loss 0.05762480944395065 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9734],\n",
      "        [0.0220]], device='mps:0')\n",
      "Iteration 73430 Training loss 0.05828508734703064 Validation loss 0.05754980072379112 Accuracy 0.84312504529953\n",
      "Output tensor([[0.5567],\n",
      "        [0.1140]], device='mps:0')\n",
      "Iteration 73440 Training loss 0.05194190889596939 Validation loss 0.05751492455601692 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.3611],\n",
      "        [0.9197]], device='mps:0')\n",
      "Iteration 73450 Training loss 0.05853527784347534 Validation loss 0.05754687264561653 Accuracy 0.843250036239624\n",
      "Output tensor([[0.6998],\n",
      "        [0.5552]], device='mps:0')\n",
      "Iteration 73460 Training loss 0.05610779672861099 Validation loss 0.057562604546546936 Accuracy 0.842875063419342\n",
      "Output tensor([[0.8805],\n",
      "        [0.2234]], device='mps:0')\n",
      "Iteration 73470 Training loss 0.048802878707647324 Validation loss 0.057705074548721313 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0679],\n",
      "        [0.1600]], device='mps:0')\n",
      "Iteration 73480 Training loss 0.0531642809510231 Validation loss 0.05751235783100128 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1310],\n",
      "        [0.6685]], device='mps:0')\n",
      "Iteration 73490 Training loss 0.047256339341402054 Validation loss 0.05752378702163696 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9168],\n",
      "        [0.3054]], device='mps:0')\n",
      "Iteration 73500 Training loss 0.053911302238702774 Validation loss 0.05762752145528793 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9423],\n",
      "        [0.0889]], device='mps:0')\n",
      "Iteration 73510 Training loss 0.05383599177002907 Validation loss 0.05751681327819824 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.6859],\n",
      "        [0.7990]], device='mps:0')\n",
      "Iteration 73520 Training loss 0.06263705343008041 Validation loss 0.05767693743109703 Accuracy 0.8411250114440918\n",
      "Output tensor([[0.0649],\n",
      "        [0.1037]], device='mps:0')\n",
      "Iteration 73530 Training loss 0.0546148419380188 Validation loss 0.0575532466173172 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0682],\n",
      "        [0.8786]], device='mps:0')\n",
      "Iteration 73540 Training loss 0.056133270263671875 Validation loss 0.05753006786108017 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.4864],\n",
      "        [0.9886]], device='mps:0')\n",
      "Iteration 73550 Training loss 0.0529043935239315 Validation loss 0.05756544694304466 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9387],\n",
      "        [0.2360]], device='mps:0')\n",
      "Iteration 73560 Training loss 0.05246764048933983 Validation loss 0.05757192149758339 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0297],\n",
      "        [0.1597]], device='mps:0')\n",
      "Iteration 73570 Training loss 0.044638875871896744 Validation loss 0.05757087841629982 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.1714],\n",
      "        [0.9278]], device='mps:0')\n",
      "Iteration 73580 Training loss 0.06057865172624588 Validation loss 0.057580456137657166 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9444],\n",
      "        [0.8191]], device='mps:0')\n",
      "Iteration 73590 Training loss 0.05077024921774864 Validation loss 0.057565174996852875 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9749],\n",
      "        [0.9351]], device='mps:0')\n",
      "Iteration 73600 Training loss 0.04927878454327583 Validation loss 0.05753999575972557 Accuracy 0.843500018119812\n",
      "Output tensor([[0.4436],\n",
      "        [0.8371]], device='mps:0')\n",
      "Iteration 73610 Training loss 0.055674828588962555 Validation loss 0.05755576491355896 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0107],\n",
      "        [0.8999]], device='mps:0')\n",
      "Iteration 73620 Training loss 0.057606663554906845 Validation loss 0.05752864480018616 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.4013],\n",
      "        [0.0412]], device='mps:0')\n",
      "Iteration 73630 Training loss 0.06141282990574837 Validation loss 0.057532016187906265 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.9564],\n",
      "        [0.5570]], device='mps:0')\n",
      "Iteration 73640 Training loss 0.05220381170511246 Validation loss 0.057621199637651443 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9839],\n",
      "        [0.8216]], device='mps:0')\n",
      "Iteration 73650 Training loss 0.04932606965303421 Validation loss 0.05754736438393593 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.2651],\n",
      "        [0.0935]], device='mps:0')\n",
      "Iteration 73660 Training loss 0.057264864444732666 Validation loss 0.057581666857004166 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9698],\n",
      "        [0.7219]], device='mps:0')\n",
      "Iteration 73670 Training loss 0.05018393322825432 Validation loss 0.05767381563782692 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.8101],\n",
      "        [0.0521]], device='mps:0')\n",
      "Iteration 73680 Training loss 0.05211416259407997 Validation loss 0.05761713907122612 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.3746],\n",
      "        [0.2473]], device='mps:0')\n",
      "Iteration 73690 Training loss 0.04980015009641647 Validation loss 0.05754362419247627 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.1719],\n",
      "        [0.1783]], device='mps:0')\n",
      "Iteration 73700 Training loss 0.04437391087412834 Validation loss 0.05761805176734924 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.5458],\n",
      "        [0.9043]], device='mps:0')\n",
      "Iteration 73710 Training loss 0.04905760660767555 Validation loss 0.057539451867341995 Accuracy 0.843375027179718\n",
      "Output tensor([[0.1080],\n",
      "        [0.4155]], device='mps:0')\n",
      "Iteration 73720 Training loss 0.06094227731227875 Validation loss 0.05759559944272041 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.1903],\n",
      "        [0.2478]], device='mps:0')\n",
      "Iteration 73730 Training loss 0.0516081228852272 Validation loss 0.057552121579647064 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.6248],\n",
      "        [0.0305]], device='mps:0')\n",
      "Iteration 73740 Training loss 0.04917920008301735 Validation loss 0.05753891170024872 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0603],\n",
      "        [0.9460]], device='mps:0')\n",
      "Iteration 73750 Training loss 0.05729939416050911 Validation loss 0.0578702874481678 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9825],\n",
      "        [0.9416]], device='mps:0')\n",
      "Iteration 73760 Training loss 0.053195513784885406 Validation loss 0.057552263140678406 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9584],\n",
      "        [0.0941]], device='mps:0')\n",
      "Iteration 73770 Training loss 0.05503617227077484 Validation loss 0.05759481340646744 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9363],\n",
      "        [0.0380]], device='mps:0')\n",
      "Iteration 73780 Training loss 0.05210096761584282 Validation loss 0.05769151821732521 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.1051],\n",
      "        [0.2765]], device='mps:0')\n",
      "Iteration 73790 Training loss 0.045784033834934235 Validation loss 0.05751912668347359 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.3967],\n",
      "        [0.8013]], device='mps:0')\n",
      "Iteration 73800 Training loss 0.05192703753709793 Validation loss 0.05752541497349739 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9253],\n",
      "        [0.9605]], device='mps:0')\n",
      "Iteration 73810 Training loss 0.05172916129231453 Validation loss 0.05753175541758537 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.6298],\n",
      "        [0.1483]], device='mps:0')\n",
      "Iteration 73820 Training loss 0.05079396069049835 Validation loss 0.05756116658449173 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9204],\n",
      "        [0.2930]], device='mps:0')\n",
      "Iteration 73830 Training loss 0.04634697735309601 Validation loss 0.057547781616449356 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.8341],\n",
      "        [0.6345]], device='mps:0')\n",
      "Iteration 73840 Training loss 0.05356893315911293 Validation loss 0.05752881243824959 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0231],\n",
      "        [0.7493]], device='mps:0')\n",
      "Iteration 73850 Training loss 0.05306439846754074 Validation loss 0.05752125009894371 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.9573],\n",
      "        [0.0346]], device='mps:0')\n",
      "Iteration 73860 Training loss 0.05650275945663452 Validation loss 0.05754947289824486 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.6399],\n",
      "        [0.0884]], device='mps:0')\n",
      "Iteration 73870 Training loss 0.05356130376458168 Validation loss 0.05770277604460716 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.1197],\n",
      "        [0.0045]], device='mps:0')\n",
      "Iteration 73880 Training loss 0.051323600113391876 Validation loss 0.057516250759363174 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9062],\n",
      "        [0.5629]], device='mps:0')\n",
      "Iteration 73890 Training loss 0.04745260253548622 Validation loss 0.0576225146651268 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.2490],\n",
      "        [0.1755]], device='mps:0')\n",
      "Iteration 73900 Training loss 0.058030251413583755 Validation loss 0.05754244327545166 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.8842],\n",
      "        [0.5499]], device='mps:0')\n",
      "Iteration 73910 Training loss 0.048994794487953186 Validation loss 0.05767985060811043 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.2629],\n",
      "        [0.9431]], device='mps:0')\n",
      "Iteration 73920 Training loss 0.051939718425273895 Validation loss 0.05752856284379959 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.6144],\n",
      "        [0.8427]], device='mps:0')\n",
      "Iteration 73930 Training loss 0.05233542248606682 Validation loss 0.057568080723285675 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.8934],\n",
      "        [0.5307]], device='mps:0')\n",
      "Iteration 73940 Training loss 0.06439407914876938 Validation loss 0.05753026902675629 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.8984],\n",
      "        [0.2184]], device='mps:0')\n",
      "Iteration 73950 Training loss 0.05588308721780777 Validation loss 0.05753045901656151 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.5417],\n",
      "        [0.3081]], device='mps:0')\n",
      "Iteration 73960 Training loss 0.04721054434776306 Validation loss 0.0575297586619854 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.4710],\n",
      "        [0.0488]], device='mps:0')\n",
      "Iteration 73970 Training loss 0.04685905948281288 Validation loss 0.057611774653196335 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9688],\n",
      "        [0.0459]], device='mps:0')\n",
      "Iteration 73980 Training loss 0.048428211361169815 Validation loss 0.0575864352285862 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.1164],\n",
      "        [0.0073]], device='mps:0')\n",
      "Iteration 73990 Training loss 0.05070299655199051 Validation loss 0.057540081441402435 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1057],\n",
      "        [0.7490]], device='mps:0')\n",
      "Iteration 74000 Training loss 0.05041823163628578 Validation loss 0.05753158777952194 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.2204],\n",
      "        [0.9637]], device='mps:0')\n",
      "Iteration 74010 Training loss 0.05418279394507408 Validation loss 0.057551875710487366 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0679],\n",
      "        [0.3465]], device='mps:0')\n",
      "Iteration 74020 Training loss 0.053640976548194885 Validation loss 0.05753452703356743 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0236],\n",
      "        [0.1065]], device='mps:0')\n",
      "Iteration 74030 Training loss 0.05412866175174713 Validation loss 0.05755096673965454 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.2824],\n",
      "        [0.9861]], device='mps:0')\n",
      "Iteration 74040 Training loss 0.062471043318510056 Validation loss 0.057631801813840866 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.3647],\n",
      "        [0.0402]], device='mps:0')\n",
      "Iteration 74050 Training loss 0.05640886723995209 Validation loss 0.057745616883039474 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1931],\n",
      "        [0.9203]], device='mps:0')\n",
      "Iteration 74060 Training loss 0.05370500311255455 Validation loss 0.057548560202121735 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9748],\n",
      "        [0.9321]], device='mps:0')\n",
      "Iteration 74070 Training loss 0.05545184016227722 Validation loss 0.057559527456760406 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9078],\n",
      "        [0.9531]], device='mps:0')\n",
      "Iteration 74080 Training loss 0.05422980338335037 Validation loss 0.05759153142571449 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.2225],\n",
      "        [0.2089]], device='mps:0')\n",
      "Iteration 74090 Training loss 0.0540461428463459 Validation loss 0.057697843760252 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9906],\n",
      "        [0.9566]], device='mps:0')\n",
      "Iteration 74100 Training loss 0.059333447366952896 Validation loss 0.057633791118860245 Accuracy 0.843375027179718\n",
      "Output tensor([[0.7391],\n",
      "        [0.8673]], device='mps:0')\n",
      "Iteration 74110 Training loss 0.049019306898117065 Validation loss 0.05753488466143608 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.2461],\n",
      "        [0.8703]], device='mps:0')\n",
      "Iteration 74120 Training loss 0.059396933764219284 Validation loss 0.05752474442124367 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0511],\n",
      "        [0.0970]], device='mps:0')\n",
      "Iteration 74130 Training loss 0.05315791815519333 Validation loss 0.0575406514108181 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.2907],\n",
      "        [0.2379]], device='mps:0')\n",
      "Iteration 74140 Training loss 0.06069864705204964 Validation loss 0.05752384662628174 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9339],\n",
      "        [0.9598]], device='mps:0')\n",
      "Iteration 74150 Training loss 0.05134378746151924 Validation loss 0.05754736438393593 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9317],\n",
      "        [0.3917]], device='mps:0')\n",
      "Iteration 74160 Training loss 0.061886657029390335 Validation loss 0.05755714327096939 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.8963],\n",
      "        [0.0325]], device='mps:0')\n",
      "Iteration 74170 Training loss 0.06194145977497101 Validation loss 0.057526279240846634 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.2878],\n",
      "        [0.7933]], device='mps:0')\n",
      "Iteration 74180 Training loss 0.05591660365462303 Validation loss 0.0575237013399601 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0959],\n",
      "        [0.8396]], device='mps:0')\n",
      "Iteration 74190 Training loss 0.0639844760298729 Validation loss 0.05752454325556755 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0263],\n",
      "        [0.9256]], device='mps:0')\n",
      "Iteration 74200 Training loss 0.05559248849749565 Validation loss 0.05752912536263466 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9515],\n",
      "        [0.6869]], device='mps:0')\n",
      "Iteration 74210 Training loss 0.058235228061676025 Validation loss 0.057567279785871506 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0506],\n",
      "        [0.2178]], device='mps:0')\n",
      "Iteration 74220 Training loss 0.05403663590550423 Validation loss 0.057599231600761414 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9453],\n",
      "        [0.0407]], device='mps:0')\n",
      "Iteration 74230 Training loss 0.05258914828300476 Validation loss 0.05753866210579872 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.4996],\n",
      "        [0.0304]], device='mps:0')\n",
      "Iteration 74240 Training loss 0.06353895366191864 Validation loss 0.05752493441104889 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1821],\n",
      "        [0.1843]], device='mps:0')\n",
      "Iteration 74250 Training loss 0.0590815544128418 Validation loss 0.05754150077700615 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.6796],\n",
      "        [0.0057]], device='mps:0')\n",
      "Iteration 74260 Training loss 0.052915770560503006 Validation loss 0.05764719098806381 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.9786],\n",
      "        [0.9654]], device='mps:0')\n",
      "Iteration 74270 Training loss 0.053953930735588074 Validation loss 0.05752129480242729 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1274],\n",
      "        [0.9069]], device='mps:0')\n",
      "Iteration 74280 Training loss 0.056267641484737396 Validation loss 0.05752037838101387 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9552],\n",
      "        [0.9516]], device='mps:0')\n",
      "Iteration 74290 Training loss 0.053729549050331116 Validation loss 0.05755404010415077 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.7672],\n",
      "        [0.6448]], device='mps:0')\n",
      "Iteration 74300 Training loss 0.05892807990312576 Validation loss 0.057527266442775726 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0816],\n",
      "        [0.1871]], device='mps:0')\n",
      "Iteration 74310 Training loss 0.05407280847430229 Validation loss 0.057564813643693924 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.8347],\n",
      "        [0.1475]], device='mps:0')\n",
      "Iteration 74320 Training loss 0.057900264859199524 Validation loss 0.05753403156995773 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.6206],\n",
      "        [0.1043]], device='mps:0')\n",
      "Iteration 74330 Training loss 0.05649604648351669 Validation loss 0.057564448565244675 Accuracy 0.84312504529953\n",
      "Output tensor([[0.8276],\n",
      "        [0.9157]], device='mps:0')\n",
      "Iteration 74340 Training loss 0.0542757585644722 Validation loss 0.05752060189843178 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.3229],\n",
      "        [0.9670]], device='mps:0')\n",
      "Iteration 74350 Training loss 0.05192147567868233 Validation loss 0.05756429582834244 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.1002],\n",
      "        [0.6429]], device='mps:0')\n",
      "Iteration 74360 Training loss 0.05014381930232048 Validation loss 0.05751907825469971 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.8626],\n",
      "        [0.9557]], device='mps:0')\n",
      "Iteration 74370 Training loss 0.05405089631676674 Validation loss 0.057562150061130524 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.7994],\n",
      "        [0.1580]], device='mps:0')\n",
      "Iteration 74380 Training loss 0.05857570469379425 Validation loss 0.057514436542987823 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.1169],\n",
      "        [0.7810]], device='mps:0')\n",
      "Iteration 74390 Training loss 0.046585917472839355 Validation loss 0.057534344494342804 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0169],\n",
      "        [0.2227]], device='mps:0')\n",
      "Iteration 74400 Training loss 0.06106242164969444 Validation loss 0.057514455169439316 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.1542],\n",
      "        [0.9351]], device='mps:0')\n",
      "Iteration 74410 Training loss 0.05130703002214432 Validation loss 0.057521119713783264 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0458],\n",
      "        [0.6255]], device='mps:0')\n",
      "Iteration 74420 Training loss 0.057826537638902664 Validation loss 0.05775555595755577 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9594],\n",
      "        [0.2216]], device='mps:0')\n",
      "Iteration 74430 Training loss 0.06928479671478271 Validation loss 0.05773309990763664 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9888],\n",
      "        [0.0369]], device='mps:0')\n",
      "Iteration 74440 Training loss 0.061136215925216675 Validation loss 0.057512734085321426 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.6273],\n",
      "        [0.9548]], device='mps:0')\n",
      "Iteration 74450 Training loss 0.055696018040180206 Validation loss 0.057517144829034805 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9324],\n",
      "        [0.0773]], device='mps:0')\n",
      "Iteration 74460 Training loss 0.05087164789438248 Validation loss 0.057576753199100494 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.8710],\n",
      "        [0.7487]], device='mps:0')\n",
      "Iteration 74470 Training loss 0.06166331097483635 Validation loss 0.057581931352615356 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0496],\n",
      "        [0.7611]], device='mps:0')\n",
      "Iteration 74480 Training loss 0.06065346673130989 Validation loss 0.0575532391667366 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.2324],\n",
      "        [0.1482]], device='mps:0')\n",
      "Iteration 74490 Training loss 0.062230441719293594 Validation loss 0.05752616748213768 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.3485],\n",
      "        [0.8981]], device='mps:0')\n",
      "Iteration 74500 Training loss 0.055555444210767746 Validation loss 0.057522546499967575 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.7268],\n",
      "        [0.3123]], device='mps:0')\n",
      "Iteration 74510 Training loss 0.05290946364402771 Validation loss 0.05780061334371567 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9579],\n",
      "        [0.0201]], device='mps:0')\n",
      "Iteration 74520 Training loss 0.056902214884757996 Validation loss 0.05754419043660164 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.6580],\n",
      "        [0.8361]], device='mps:0')\n",
      "Iteration 74530 Training loss 0.05020161345601082 Validation loss 0.05757741257548332 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0977],\n",
      "        [0.1167]], device='mps:0')\n",
      "Iteration 74540 Training loss 0.05396649241447449 Validation loss 0.05765878036618233 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.7975],\n",
      "        [0.8919]], device='mps:0')\n",
      "Iteration 74550 Training loss 0.06010110676288605 Validation loss 0.05748335272073746 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0457],\n",
      "        [0.9917]], device='mps:0')\n",
      "Iteration 74560 Training loss 0.05361993610858917 Validation loss 0.057501379400491714 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.2748],\n",
      "        [0.1622]], device='mps:0')\n",
      "Iteration 74570 Training loss 0.051318082958459854 Validation loss 0.05748872831463814 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.1076],\n",
      "        [0.7932]], device='mps:0')\n",
      "Iteration 74580 Training loss 0.057283591479063034 Validation loss 0.05749518796801567 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9124],\n",
      "        [0.9137]], device='mps:0')\n",
      "Iteration 74590 Training loss 0.05632593855261803 Validation loss 0.05749579891562462 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.3328],\n",
      "        [0.3463]], device='mps:0')\n",
      "Iteration 74600 Training loss 0.04939216375350952 Validation loss 0.05756819248199463 Accuracy 0.843000054359436\n",
      "Output tensor([[0.3602],\n",
      "        [0.9296]], device='mps:0')\n",
      "Iteration 74610 Training loss 0.05189067870378494 Validation loss 0.05757823586463928 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.5591],\n",
      "        [0.0227]], device='mps:0')\n",
      "Iteration 74620 Training loss 0.05240233615040779 Validation loss 0.057525135576725006 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0937],\n",
      "        [0.3813]], device='mps:0')\n",
      "Iteration 74630 Training loss 0.05787516385316849 Validation loss 0.05751768872141838 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0649],\n",
      "        [0.7372]], device='mps:0')\n",
      "Iteration 74640 Training loss 0.051627133041620255 Validation loss 0.05752497911453247 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.8478],\n",
      "        [0.4935]], device='mps:0')\n",
      "Iteration 74650 Training loss 0.06163882091641426 Validation loss 0.05759924650192261 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.6050],\n",
      "        [0.0449]], device='mps:0')\n",
      "Iteration 74660 Training loss 0.05435400456190109 Validation loss 0.05749785155057907 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9839],\n",
      "        [0.6299]], device='mps:0')\n",
      "Iteration 74670 Training loss 0.05045546963810921 Validation loss 0.057584889233112335 Accuracy 0.843000054359436\n",
      "Output tensor([[0.2273],\n",
      "        [0.0556]], device='mps:0')\n",
      "Iteration 74680 Training loss 0.05751330405473709 Validation loss 0.05750679224729538 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.6303],\n",
      "        [0.9203]], device='mps:0')\n",
      "Iteration 74690 Training loss 0.050724372267723083 Validation loss 0.05748840048909187 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.8421],\n",
      "        [0.0431]], device='mps:0')\n",
      "Iteration 74700 Training loss 0.05450773239135742 Validation loss 0.05750799924135208 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9704],\n",
      "        [0.9938]], device='mps:0')\n",
      "Iteration 74710 Training loss 0.05241509899497032 Validation loss 0.057479847222566605 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0533],\n",
      "        [0.7920]], device='mps:0')\n",
      "Iteration 74720 Training loss 0.049178846180438995 Validation loss 0.057754628360271454 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.0447],\n",
      "        [0.1777]], device='mps:0')\n",
      "Iteration 74730 Training loss 0.05672759935259819 Validation loss 0.057488199323415756 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.6294],\n",
      "        [0.3089]], device='mps:0')\n",
      "Iteration 74740 Training loss 0.044989317655563354 Validation loss 0.057863034307956696 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9519],\n",
      "        [0.0245]], device='mps:0')\n",
      "Iteration 74750 Training loss 0.05468599498271942 Validation loss 0.05755997821688652 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.2816],\n",
      "        [0.0958]], device='mps:0')\n",
      "Iteration 74760 Training loss 0.05566997453570366 Validation loss 0.057482827454805374 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9631],\n",
      "        [0.1248]], device='mps:0')\n",
      "Iteration 74770 Training loss 0.05424387753009796 Validation loss 0.05748515576124191 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.5728],\n",
      "        [0.9764]], device='mps:0')\n",
      "Iteration 74780 Training loss 0.056006912142038345 Validation loss 0.05747765302658081 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.8026],\n",
      "        [0.9551]], device='mps:0')\n",
      "Iteration 74790 Training loss 0.05361008644104004 Validation loss 0.057637955993413925 Accuracy 0.8418750166893005\n",
      "Output tensor([[0.7581],\n",
      "        [0.2258]], device='mps:0')\n",
      "Iteration 74800 Training loss 0.059958260506391525 Validation loss 0.057517874985933304 Accuracy 0.843500018119812\n",
      "Output tensor([[0.8820],\n",
      "        [0.9660]], device='mps:0')\n",
      "Iteration 74810 Training loss 0.061097245663404465 Validation loss 0.0574740469455719 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.6754],\n",
      "        [0.7727]], device='mps:0')\n",
      "Iteration 74820 Training loss 0.05298162251710892 Validation loss 0.05746627226471901 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0779],\n",
      "        [0.2981]], device='mps:0')\n",
      "Iteration 74830 Training loss 0.05745242163538933 Validation loss 0.057646073400974274 Accuracy 0.843250036239624\n",
      "Output tensor([[0.1111],\n",
      "        [0.0674]], device='mps:0')\n",
      "Iteration 74840 Training loss 0.050744589418172836 Validation loss 0.057586222887039185 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9332],\n",
      "        [0.5041]], device='mps:0')\n",
      "Iteration 74850 Training loss 0.05393264815211296 Validation loss 0.05746148154139519 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9103],\n",
      "        [0.1888]], device='mps:0')\n",
      "Iteration 74860 Training loss 0.05198940634727478 Validation loss 0.05753801390528679 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0126],\n",
      "        [0.1239]], device='mps:0')\n",
      "Iteration 74870 Training loss 0.05148128420114517 Validation loss 0.057460322976112366 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.2460],\n",
      "        [0.4306]], device='mps:0')\n",
      "Iteration 74880 Training loss 0.05698920041322708 Validation loss 0.05762389674782753 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9477],\n",
      "        [0.1591]], device='mps:0')\n",
      "Iteration 74890 Training loss 0.053698014467954636 Validation loss 0.05746287852525711 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.1782],\n",
      "        [0.5937]], device='mps:0')\n",
      "Iteration 74900 Training loss 0.04652184247970581 Validation loss 0.057608895003795624 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.9878],\n",
      "        [0.2983]], device='mps:0')\n",
      "Iteration 74910 Training loss 0.05381854996085167 Validation loss 0.05759366974234581 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.2170],\n",
      "        [0.9899]], device='mps:0')\n",
      "Iteration 74920 Training loss 0.06288570165634155 Validation loss 0.05751785635948181 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0401],\n",
      "        [0.0114]], device='mps:0')\n",
      "Iteration 74930 Training loss 0.04554910585284233 Validation loss 0.057944562286138535 Accuracy 0.8417500257492065\n",
      "Output tensor([[0.5566],\n",
      "        [0.0786]], device='mps:0')\n",
      "Iteration 74940 Training loss 0.062310799956321716 Validation loss 0.057564351707696915 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.3870],\n",
      "        [0.0342]], device='mps:0')\n",
      "Iteration 74950 Training loss 0.0512031652033329 Validation loss 0.05747919902205467 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9629],\n",
      "        [0.4426]], device='mps:0')\n",
      "Iteration 74960 Training loss 0.05170542746782303 Validation loss 0.05748874321579933 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0299],\n",
      "        [0.9808]], device='mps:0')\n",
      "Iteration 74970 Training loss 0.049379777163267136 Validation loss 0.05760934203863144 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.7970],\n",
      "        [0.1906]], device='mps:0')\n",
      "Iteration 74980 Training loss 0.05161033570766449 Validation loss 0.05747482553124428 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.3250],\n",
      "        [0.7291]], device='mps:0')\n",
      "Iteration 74990 Training loss 0.05083242431282997 Validation loss 0.05746221914887428 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.2436],\n",
      "        [0.9883]], device='mps:0')\n",
      "Iteration 75000 Training loss 0.05628900229930878 Validation loss 0.057473331689834595 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.0515],\n",
      "        [0.9984]], device='mps:0')\n",
      "Iteration 75010 Training loss 0.0499197393655777 Validation loss 0.05751890689134598 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0513],\n",
      "        [0.8641]], device='mps:0')\n",
      "Iteration 75020 Training loss 0.052443310618400574 Validation loss 0.05748719349503517 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.1063],\n",
      "        [0.9270]], device='mps:0')\n",
      "Iteration 75030 Training loss 0.05118178576231003 Validation loss 0.05757463350892067 Accuracy 0.8421250581741333\n",
      "Output tensor([[0.8456],\n",
      "        [0.9296]], device='mps:0')\n",
      "Iteration 75040 Training loss 0.05747281759977341 Validation loss 0.05746181309223175 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0217],\n",
      "        [0.9988]], device='mps:0')\n",
      "Iteration 75050 Training loss 0.05784423276782036 Validation loss 0.05748923867940903 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.1700],\n",
      "        [0.3937]], device='mps:0')\n",
      "Iteration 75060 Training loss 0.0643240213394165 Validation loss 0.05771483853459358 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9886],\n",
      "        [0.0198]], device='mps:0')\n",
      "Iteration 75070 Training loss 0.06265164911746979 Validation loss 0.057566411793231964 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.0246],\n",
      "        [0.1656]], device='mps:0')\n",
      "Iteration 75080 Training loss 0.0534769631922245 Validation loss 0.057449739426374435 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0349],\n",
      "        [0.7749]], device='mps:0')\n",
      "Iteration 75090 Training loss 0.06539261341094971 Validation loss 0.057443372905254364 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9789],\n",
      "        [0.0419]], device='mps:0')\n",
      "Iteration 75100 Training loss 0.0627741664648056 Validation loss 0.05746779218316078 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.7084],\n",
      "        [0.0494]], device='mps:0')\n",
      "Iteration 75110 Training loss 0.060562074184417725 Validation loss 0.05759691819548607 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.1453],\n",
      "        [0.9884]], device='mps:0')\n",
      "Iteration 75120 Training loss 0.05536814406514168 Validation loss 0.05750496685504913 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.6190],\n",
      "        [0.9904]], device='mps:0')\n",
      "Iteration 75130 Training loss 0.05637460574507713 Validation loss 0.05744661018252373 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.6223],\n",
      "        [0.8628]], device='mps:0')\n",
      "Iteration 75140 Training loss 0.054791536182165146 Validation loss 0.05754934996366501 Accuracy 0.843500018119812\n",
      "Output tensor([[0.5329],\n",
      "        [0.8326]], device='mps:0')\n",
      "Iteration 75150 Training loss 0.057693347334861755 Validation loss 0.05752599611878395 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0494],\n",
      "        [0.1947]], device='mps:0')\n",
      "Iteration 75160 Training loss 0.05081739276647568 Validation loss 0.057583168148994446 Accuracy 0.843375027179718\n",
      "Output tensor([[0.5915],\n",
      "        [0.9778]], device='mps:0')\n",
      "Iteration 75170 Training loss 0.05371859297156334 Validation loss 0.05758928135037422 Accuracy 0.842875063419342\n",
      "Output tensor([[0.6106],\n",
      "        [0.3510]], device='mps:0')\n",
      "Iteration 75180 Training loss 0.05569250136613846 Validation loss 0.057563189417123795 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9400],\n",
      "        [0.9739]], device='mps:0')\n",
      "Iteration 75190 Training loss 0.052568469196558 Validation loss 0.05744249373674393 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.6934],\n",
      "        [0.0302]], device='mps:0')\n",
      "Iteration 75200 Training loss 0.04939631372690201 Validation loss 0.05759167671203613 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0817],\n",
      "        [0.9552]], device='mps:0')\n",
      "Iteration 75210 Training loss 0.0557265542447567 Validation loss 0.0574372373521328 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.5720],\n",
      "        [0.2030]], device='mps:0')\n",
      "Iteration 75220 Training loss 0.049510855227708817 Validation loss 0.05747785419225693 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9108],\n",
      "        [0.8708]], device='mps:0')\n",
      "Iteration 75230 Training loss 0.05562639981508255 Validation loss 0.05745439603924751 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.3532],\n",
      "        [0.0375]], device='mps:0')\n",
      "Iteration 75240 Training loss 0.05052834749221802 Validation loss 0.05743201822042465 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0150],\n",
      "        [0.3477]], device='mps:0')\n",
      "Iteration 75250 Training loss 0.05295449122786522 Validation loss 0.057540182024240494 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.1719],\n",
      "        [0.4728]], device='mps:0')\n",
      "Iteration 75260 Training loss 0.055322930216789246 Validation loss 0.057573944330215454 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0485],\n",
      "        [0.8107]], device='mps:0')\n",
      "Iteration 75270 Training loss 0.05328706279397011 Validation loss 0.05749798193573952 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.8003],\n",
      "        [0.4361]], device='mps:0')\n",
      "Iteration 75280 Training loss 0.0579862967133522 Validation loss 0.05746222659945488 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0034],\n",
      "        [0.2770]], device='mps:0')\n",
      "Iteration 75290 Training loss 0.05718572810292244 Validation loss 0.05742647498846054 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0229],\n",
      "        [0.9475]], device='mps:0')\n",
      "Iteration 75300 Training loss 0.06085725501179695 Validation loss 0.05743643641471863 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.4150],\n",
      "        [0.8500]], device='mps:0')\n",
      "Iteration 75310 Training loss 0.05853820592164993 Validation loss 0.05742853879928589 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.5881],\n",
      "        [0.9786]], device='mps:0')\n",
      "Iteration 75320 Training loss 0.04844333231449127 Validation loss 0.05753656104207039 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.0588],\n",
      "        [0.0298]], device='mps:0')\n",
      "Iteration 75330 Training loss 0.054335661232471466 Validation loss 0.05743839219212532 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9506],\n",
      "        [0.8428]], device='mps:0')\n",
      "Iteration 75340 Training loss 0.05660036951303482 Validation loss 0.05751556158065796 Accuracy 0.843375027179718\n",
      "Output tensor([[0.4325],\n",
      "        [0.5234]], device='mps:0')\n",
      "Iteration 75350 Training loss 0.05902347341179848 Validation loss 0.05745727941393852 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0636],\n",
      "        [0.0798]], device='mps:0')\n",
      "Iteration 75360 Training loss 0.059170302003622055 Validation loss 0.05770668014883995 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0690],\n",
      "        [0.4841]], device='mps:0')\n",
      "Iteration 75370 Training loss 0.048158783465623856 Validation loss 0.05781400948762894 Accuracy 0.842875063419342\n",
      "Output tensor([[0.1116],\n",
      "        [0.0040]], device='mps:0')\n",
      "Iteration 75380 Training loss 0.04916746914386749 Validation loss 0.05741007626056671 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0947],\n",
      "        [0.9284]], device='mps:0')\n",
      "Iteration 75390 Training loss 0.04499132186174393 Validation loss 0.05741485208272934 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.8206],\n",
      "        [0.5155]], device='mps:0')\n",
      "Iteration 75400 Training loss 0.04621778428554535 Validation loss 0.05745525658130646 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9756],\n",
      "        [0.2280]], device='mps:0')\n",
      "Iteration 75410 Training loss 0.057954661548137665 Validation loss 0.0574205219745636 Accuracy 0.843500018119812\n",
      "Output tensor([[0.1840],\n",
      "        [0.2437]], device='mps:0')\n",
      "Iteration 75420 Training loss 0.054931215941905975 Validation loss 0.05740600451827049 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.9968],\n",
      "        [0.7615]], device='mps:0')\n",
      "Iteration 75430 Training loss 0.06312002241611481 Validation loss 0.05739978328347206 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.8291],\n",
      "        [0.0402]], device='mps:0')\n",
      "Iteration 75440 Training loss 0.052741050720214844 Validation loss 0.05741743743419647 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9777],\n",
      "        [0.0065]], device='mps:0')\n",
      "Iteration 75450 Training loss 0.06284255534410477 Validation loss 0.05755012854933739 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0326],\n",
      "        [0.1218]], device='mps:0')\n",
      "Iteration 75460 Training loss 0.05567006766796112 Validation loss 0.05768909677863121 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.5807],\n",
      "        [0.9629]], device='mps:0')\n",
      "Iteration 75470 Training loss 0.06092990189790726 Validation loss 0.05740857869386673 Accuracy 0.843375027179718\n",
      "Output tensor([[0.5355],\n",
      "        [0.0556]], device='mps:0')\n",
      "Iteration 75480 Training loss 0.05953173339366913 Validation loss 0.05740184336900711 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.2279],\n",
      "        [0.4924]], device='mps:0')\n",
      "Iteration 75490 Training loss 0.05749140679836273 Validation loss 0.05747656151652336 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0408],\n",
      "        [0.9740]], device='mps:0')\n",
      "Iteration 75500 Training loss 0.042133573442697525 Validation loss 0.05758500099182129 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.4903],\n",
      "        [0.3406]], device='mps:0')\n",
      "Iteration 75510 Training loss 0.041602667421102524 Validation loss 0.05739915370941162 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9920],\n",
      "        [0.6294]], device='mps:0')\n",
      "Iteration 75520 Training loss 0.05339431017637253 Validation loss 0.05740144103765488 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.4103],\n",
      "        [0.0327]], device='mps:0')\n",
      "Iteration 75530 Training loss 0.06102351099252701 Validation loss 0.05748586729168892 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.2117],\n",
      "        [0.3928]], device='mps:0')\n",
      "Iteration 75540 Training loss 0.05614014342427254 Validation loss 0.057654961943626404 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0178],\n",
      "        [0.0668]], device='mps:0')\n",
      "Iteration 75550 Training loss 0.05618596822023392 Validation loss 0.05742944777011871 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9088],\n",
      "        [0.8684]], device='mps:0')\n",
      "Iteration 75560 Training loss 0.054547857493162155 Validation loss 0.05744970217347145 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.8801],\n",
      "        [0.9656]], device='mps:0')\n",
      "Iteration 75570 Training loss 0.05726637691259384 Validation loss 0.05739883705973625 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.3668],\n",
      "        [0.8208]], device='mps:0')\n",
      "Iteration 75580 Training loss 0.05674257129430771 Validation loss 0.057405032217502594 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0223],\n",
      "        [0.5493]], device='mps:0')\n",
      "Iteration 75590 Training loss 0.05548859387636185 Validation loss 0.05740481987595558 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.2139],\n",
      "        [0.7091]], device='mps:0')\n",
      "Iteration 75600 Training loss 0.06548842787742615 Validation loss 0.057432372123003006 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9506],\n",
      "        [0.0381]], device='mps:0')\n",
      "Iteration 75610 Training loss 0.055034536868333817 Validation loss 0.05740579962730408 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0121],\n",
      "        [0.3453]], device='mps:0')\n",
      "Iteration 75620 Training loss 0.04916680231690407 Validation loss 0.057412102818489075 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0618],\n",
      "        [0.9524]], device='mps:0')\n",
      "Iteration 75630 Training loss 0.06232042983174324 Validation loss 0.05742067098617554 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.8882],\n",
      "        [0.7179]], device='mps:0')\n",
      "Iteration 75640 Training loss 0.05542492866516113 Validation loss 0.0574040561914444 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9599],\n",
      "        [0.3138]], device='mps:0')\n",
      "Iteration 75650 Training loss 0.049978453665971756 Validation loss 0.057489242404699326 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0261],\n",
      "        [0.0838]], device='mps:0')\n",
      "Iteration 75660 Training loss 0.05208361893892288 Validation loss 0.05740361660718918 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.3919],\n",
      "        [0.7200]], device='mps:0')\n",
      "Iteration 75670 Training loss 0.05058343708515167 Validation loss 0.05740141123533249 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0149],\n",
      "        [0.1186]], device='mps:0')\n",
      "Iteration 75680 Training loss 0.056264981627464294 Validation loss 0.05742032825946808 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9490],\n",
      "        [0.0342]], device='mps:0')\n",
      "Iteration 75690 Training loss 0.05822664126753807 Validation loss 0.05749562382698059 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9720],\n",
      "        [0.0020]], device='mps:0')\n",
      "Iteration 75700 Training loss 0.04466962814331055 Validation loss 0.05744341015815735 Accuracy 0.843375027179718\n",
      "Output tensor([[0.3830],\n",
      "        [0.9733]], device='mps:0')\n",
      "Iteration 75710 Training loss 0.05451485514640808 Validation loss 0.05742799490690231 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9646],\n",
      "        [0.9871]], device='mps:0')\n",
      "Iteration 75720 Training loss 0.06205511465668678 Validation loss 0.057594068348407745 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9773],\n",
      "        [0.5611]], device='mps:0')\n",
      "Iteration 75730 Training loss 0.048413071781396866 Validation loss 0.05742679908871651 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9883],\n",
      "        [0.3300]], device='mps:0')\n",
      "Iteration 75740 Training loss 0.05254519730806351 Validation loss 0.05738808587193489 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0072],\n",
      "        [0.9806]], device='mps:0')\n",
      "Iteration 75750 Training loss 0.06136033311486244 Validation loss 0.05740848556160927 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0162],\n",
      "        [0.1505]], device='mps:0')\n",
      "Iteration 75760 Training loss 0.04860056936740875 Validation loss 0.0573938749730587 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.3283],\n",
      "        [0.7537]], device='mps:0')\n",
      "Iteration 75770 Training loss 0.05254945531487465 Validation loss 0.05752298980951309 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9893],\n",
      "        [0.0068]], device='mps:0')\n",
      "Iteration 75780 Training loss 0.05530325695872307 Validation loss 0.05740012601017952 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9159],\n",
      "        [0.2492]], device='mps:0')\n",
      "Iteration 75790 Training loss 0.04917075112462044 Validation loss 0.05738121643662453 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.8890],\n",
      "        [0.1688]], device='mps:0')\n",
      "Iteration 75800 Training loss 0.05435514822602272 Validation loss 0.057401254773139954 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.1069],\n",
      "        [0.0884]], device='mps:0')\n",
      "Iteration 75810 Training loss 0.05654090270400047 Validation loss 0.05738089233636856 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.1892],\n",
      "        [0.0840]], device='mps:0')\n",
      "Iteration 75820 Training loss 0.05106913298368454 Validation loss 0.05743137374520302 Accuracy 0.843250036239624\n",
      "Output tensor([[0.6104],\n",
      "        [0.2908]], device='mps:0')\n",
      "Iteration 75830 Training loss 0.04952249303460121 Validation loss 0.05745018273591995 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.1039],\n",
      "        [0.0967]], device='mps:0')\n",
      "Iteration 75840 Training loss 0.06076565757393837 Validation loss 0.057381510734558105 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.8229],\n",
      "        [0.0882]], device='mps:0')\n",
      "Iteration 75850 Training loss 0.05509514734148979 Validation loss 0.05738933011889458 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.2250],\n",
      "        [0.2057]], device='mps:0')\n",
      "Iteration 75860 Training loss 0.05243087559938431 Validation loss 0.05753844231367111 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.6907],\n",
      "        [0.0528]], device='mps:0')\n",
      "Iteration 75870 Training loss 0.06152505800127983 Validation loss 0.05772075057029724 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9885],\n",
      "        [0.8744]], device='mps:0')\n",
      "Iteration 75880 Training loss 0.05104899778962135 Validation loss 0.05739736929535866 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9834],\n",
      "        [0.0102]], device='mps:0')\n",
      "Iteration 75890 Training loss 0.05767243728041649 Validation loss 0.057380419224500656 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.2226],\n",
      "        [0.3724]], device='mps:0')\n",
      "Iteration 75900 Training loss 0.044749487191438675 Validation loss 0.05743315443396568 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.7692],\n",
      "        [0.7043]], device='mps:0')\n",
      "Iteration 75910 Training loss 0.058159589767456055 Validation loss 0.057541146874427795 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.2582],\n",
      "        [0.0150]], device='mps:0')\n",
      "Iteration 75920 Training loss 0.053416840732097626 Validation loss 0.05741238594055176 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0165],\n",
      "        [0.4317]], device='mps:0')\n",
      "Iteration 75930 Training loss 0.046331632882356644 Validation loss 0.05741707608103752 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.1534],\n",
      "        [0.0250]], device='mps:0')\n",
      "Iteration 75940 Training loss 0.06133560463786125 Validation loss 0.05739878490567207 Accuracy 0.843500018119812\n",
      "Output tensor([[0.1839],\n",
      "        [0.9728]], device='mps:0')\n",
      "Iteration 75950 Training loss 0.057188499718904495 Validation loss 0.057373542338609695 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.1793],\n",
      "        [0.8091]], device='mps:0')\n",
      "Iteration 75960 Training loss 0.05016808956861496 Validation loss 0.0573861263692379 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.2146],\n",
      "        [0.9472]], device='mps:0')\n",
      "Iteration 75970 Training loss 0.05362094193696976 Validation loss 0.057379405945539474 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.4183],\n",
      "        [0.0306]], device='mps:0')\n",
      "Iteration 75980 Training loss 0.05807870626449585 Validation loss 0.05741928517818451 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0747],\n",
      "        [0.9682]], device='mps:0')\n",
      "Iteration 75990 Training loss 0.04598654806613922 Validation loss 0.05738803744316101 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.2275],\n",
      "        [0.9703]], device='mps:0')\n",
      "Iteration 76000 Training loss 0.058070000261068344 Validation loss 0.057557612657547 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0068],\n",
      "        [0.3833]], device='mps:0')\n",
      "Iteration 76010 Training loss 0.05330297350883484 Validation loss 0.05737878009676933 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0162],\n",
      "        [0.6949]], device='mps:0')\n",
      "Iteration 76020 Training loss 0.05322621390223503 Validation loss 0.05743115395307541 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.1612],\n",
      "        [0.3962]], device='mps:0')\n",
      "Iteration 76030 Training loss 0.055730704218149185 Validation loss 0.05743098631501198 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0312],\n",
      "        [0.9863]], device='mps:0')\n",
      "Iteration 76040 Training loss 0.05890027806162834 Validation loss 0.05738005414605141 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.0825],\n",
      "        [0.0501]], device='mps:0')\n",
      "Iteration 76050 Training loss 0.055160149931907654 Validation loss 0.05754709988832474 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9574],\n",
      "        [0.9266]], device='mps:0')\n",
      "Iteration 76060 Training loss 0.04921442270278931 Validation loss 0.05743683502078056 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.8335],\n",
      "        [0.3230]], device='mps:0')\n",
      "Iteration 76070 Training loss 0.054010480642318726 Validation loss 0.05743872746825218 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.8324],\n",
      "        [0.0719]], device='mps:0')\n",
      "Iteration 76080 Training loss 0.0516577884554863 Validation loss 0.057378146797418594 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.9502],\n",
      "        [0.1000]], device='mps:0')\n",
      "Iteration 76090 Training loss 0.04601180925965309 Validation loss 0.057555604726076126 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9905],\n",
      "        [0.1487]], device='mps:0')\n",
      "Iteration 76100 Training loss 0.0409967303276062 Validation loss 0.057389937341213226 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9590],\n",
      "        [0.7242]], device='mps:0')\n",
      "Iteration 76110 Training loss 0.05160645395517349 Validation loss 0.05738149955868721 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.0641],\n",
      "        [0.7046]], device='mps:0')\n",
      "Iteration 76120 Training loss 0.05063898488879204 Validation loss 0.057388775050640106 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.5315],\n",
      "        [0.0396]], device='mps:0')\n",
      "Iteration 76130 Training loss 0.05367988348007202 Validation loss 0.05747828260064125 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0489],\n",
      "        [0.8836]], device='mps:0')\n",
      "Iteration 76140 Training loss 0.05189961940050125 Validation loss 0.05745018273591995 Accuracy 0.843500018119812\n",
      "Output tensor([[0.4199],\n",
      "        [0.0422]], device='mps:0')\n",
      "Iteration 76150 Training loss 0.05474042892456055 Validation loss 0.057437971234321594 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9344],\n",
      "        [0.2951]], device='mps:0')\n",
      "Iteration 76160 Training loss 0.05080264061689377 Validation loss 0.05739520490169525 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.7581],\n",
      "        [0.0589]], device='mps:0')\n",
      "Iteration 76170 Training loss 0.05941532179713249 Validation loss 0.05741577222943306 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9819],\n",
      "        [0.0545]], device='mps:0')\n",
      "Iteration 76180 Training loss 0.04784207418560982 Validation loss 0.057410698384046555 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0155],\n",
      "        [0.4844]], device='mps:0')\n",
      "Iteration 76190 Training loss 0.05285746604204178 Validation loss 0.057394832372665405 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.3827],\n",
      "        [0.8153]], device='mps:0')\n",
      "Iteration 76200 Training loss 0.049160368740558624 Validation loss 0.057417843490839005 Accuracy 0.843000054359436\n",
      "Output tensor([[0.3966],\n",
      "        [0.5183]], device='mps:0')\n",
      "Iteration 76210 Training loss 0.05422364920377731 Validation loss 0.05739039182662964 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.0459],\n",
      "        [0.9679]], device='mps:0')\n",
      "Iteration 76220 Training loss 0.057124264538288116 Validation loss 0.0574943870306015 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9774],\n",
      "        [0.1589]], device='mps:0')\n",
      "Iteration 76230 Training loss 0.05268925428390503 Validation loss 0.05743919685482979 Accuracy 0.843250036239624\n",
      "Output tensor([[0.1998],\n",
      "        [0.5045]], device='mps:0')\n",
      "Iteration 76240 Training loss 0.05220809951424599 Validation loss 0.057385530322790146 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.7811],\n",
      "        [0.3092]], device='mps:0')\n",
      "Iteration 76250 Training loss 0.04407922551035881 Validation loss 0.05739858001470566 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9002],\n",
      "        [0.3885]], device='mps:0')\n",
      "Iteration 76260 Training loss 0.05973409488797188 Validation loss 0.05744163319468498 Accuracy 0.843250036239624\n",
      "Output tensor([[0.1060],\n",
      "        [0.9182]], device='mps:0')\n",
      "Iteration 76270 Training loss 0.05828222632408142 Validation loss 0.05754133313894272 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.6574],\n",
      "        [0.6154]], device='mps:0')\n",
      "Iteration 76280 Training loss 0.05522332713007927 Validation loss 0.05738677456974983 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.1863],\n",
      "        [0.3764]], device='mps:0')\n",
      "Iteration 76290 Training loss 0.05822112783789635 Validation loss 0.05741402879357338 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.9182],\n",
      "        [0.6930]], device='mps:0')\n",
      "Iteration 76300 Training loss 0.0535878986120224 Validation loss 0.057483598589897156 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.2697],\n",
      "        [0.0798]], device='mps:0')\n",
      "Iteration 76310 Training loss 0.04444952681660652 Validation loss 0.05738041177392006 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.0502],\n",
      "        [0.0369]], device='mps:0')\n",
      "Iteration 76320 Training loss 0.05211391672492027 Validation loss 0.05748165398836136 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.7802],\n",
      "        [0.2152]], device='mps:0')\n",
      "Iteration 76330 Training loss 0.05000283196568489 Validation loss 0.057432468980550766 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0452],\n",
      "        [0.7023]], device='mps:0')\n",
      "Iteration 76340 Training loss 0.0565749928355217 Validation loss 0.057369414716959 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.6022],\n",
      "        [0.0332]], device='mps:0')\n",
      "Iteration 76350 Training loss 0.05121712386608124 Validation loss 0.05742323398590088 Accuracy 0.843250036239624\n",
      "Output tensor([[0.8656],\n",
      "        [0.9361]], device='mps:0')\n",
      "Iteration 76360 Training loss 0.0498938150703907 Validation loss 0.05738849192857742 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9706],\n",
      "        [0.8413]], device='mps:0')\n",
      "Iteration 76370 Training loss 0.04966941848397255 Validation loss 0.057476840913295746 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9896],\n",
      "        [0.9864]], device='mps:0')\n",
      "Iteration 76380 Training loss 0.05314699187874794 Validation loss 0.05748648941516876 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.9318],\n",
      "        [0.0724]], device='mps:0')\n",
      "Iteration 76390 Training loss 0.05428291857242584 Validation loss 0.05737929046154022 Accuracy 0.843500018119812\n",
      "Output tensor([[0.6644],\n",
      "        [0.9694]], device='mps:0')\n",
      "Iteration 76400 Training loss 0.054064538329839706 Validation loss 0.057445451617240906 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.8802],\n",
      "        [0.0293]], device='mps:0')\n",
      "Iteration 76410 Training loss 0.06170985475182533 Validation loss 0.057734787464141846 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1155],\n",
      "        [0.4449]], device='mps:0')\n",
      "Iteration 76420 Training loss 0.050300233066082 Validation loss 0.05737613886594772 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.1623],\n",
      "        [0.9616]], device='mps:0')\n",
      "Iteration 76430 Training loss 0.05415230616927147 Validation loss 0.05741475895047188 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0813],\n",
      "        [0.7421]], device='mps:0')\n",
      "Iteration 76440 Training loss 0.06442617624998093 Validation loss 0.0574512779712677 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.2522],\n",
      "        [0.7794]], device='mps:0')\n",
      "Iteration 76450 Training loss 0.06306742876768112 Validation loss 0.057377319782972336 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9219],\n",
      "        [0.9455]], device='mps:0')\n",
      "Iteration 76460 Training loss 0.050938740372657776 Validation loss 0.05738632380962372 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.3535],\n",
      "        [0.6597]], device='mps:0')\n",
      "Iteration 76470 Training loss 0.06613703072071075 Validation loss 0.05738798901438713 Accuracy 0.843500018119812\n",
      "Output tensor([[0.7699],\n",
      "        [0.2440]], device='mps:0')\n",
      "Iteration 76480 Training loss 0.05623516067862511 Validation loss 0.05743050202727318 Accuracy 0.843500018119812\n",
      "Output tensor([[0.8836],\n",
      "        [0.0782]], device='mps:0')\n",
      "Iteration 76490 Training loss 0.05586998164653778 Validation loss 0.05740809813141823 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9966],\n",
      "        [0.0482]], device='mps:0')\n",
      "Iteration 76500 Training loss 0.05633702129125595 Validation loss 0.057396963238716125 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.5126],\n",
      "        [0.9405]], device='mps:0')\n",
      "Iteration 76510 Training loss 0.0629120022058487 Validation loss 0.05737282708287239 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9923],\n",
      "        [0.6278]], device='mps:0')\n",
      "Iteration 76520 Training loss 0.05118599906563759 Validation loss 0.05744658038020134 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9406],\n",
      "        [0.9944]], device='mps:0')\n",
      "Iteration 76530 Training loss 0.059913329780101776 Validation loss 0.05739407613873482 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9566],\n",
      "        [0.0894]], device='mps:0')\n",
      "Iteration 76540 Training loss 0.05337010324001312 Validation loss 0.05740896984934807 Accuracy 0.843250036239624\n",
      "Output tensor([[0.3129],\n",
      "        [0.0072]], device='mps:0')\n",
      "Iteration 76550 Training loss 0.04936462268233299 Validation loss 0.05736413970589638 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.2446],\n",
      "        [0.0473]], device='mps:0')\n",
      "Iteration 76560 Training loss 0.05689414218068123 Validation loss 0.057381078600883484 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.8617],\n",
      "        [0.9524]], device='mps:0')\n",
      "Iteration 76570 Training loss 0.05140336975455284 Validation loss 0.05741573125123978 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.2260],\n",
      "        [0.5129]], device='mps:0')\n",
      "Iteration 76580 Training loss 0.05260956287384033 Validation loss 0.057374678552150726 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.2845],\n",
      "        [0.9292]], device='mps:0')\n",
      "Iteration 76590 Training loss 0.055609166622161865 Validation loss 0.057374075055122375 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0926],\n",
      "        [0.0537]], device='mps:0')\n",
      "Iteration 76600 Training loss 0.06198373809456825 Validation loss 0.057389888912439346 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0409],\n",
      "        [0.9837]], device='mps:0')\n",
      "Iteration 76610 Training loss 0.05788550153374672 Validation loss 0.0575808584690094 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.3177],\n",
      "        [0.9667]], device='mps:0')\n",
      "Iteration 76620 Training loss 0.059280283749103546 Validation loss 0.05738101527094841 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.9467],\n",
      "        [0.1041]], device='mps:0')\n",
      "Iteration 76630 Training loss 0.04965082183480263 Validation loss 0.05741380527615547 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.8491],\n",
      "        [0.5436]], device='mps:0')\n",
      "Iteration 76640 Training loss 0.05318824201822281 Validation loss 0.05748991668224335 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.6754],\n",
      "        [0.0363]], device='mps:0')\n",
      "Iteration 76650 Training loss 0.05269740894436836 Validation loss 0.05737513676285744 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9496],\n",
      "        [0.0263]], device='mps:0')\n",
      "Iteration 76660 Training loss 0.055344607681035995 Validation loss 0.05739564821124077 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.5309],\n",
      "        [0.2695]], device='mps:0')\n",
      "Iteration 76670 Training loss 0.05986276641488075 Validation loss 0.057364970445632935 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.7273],\n",
      "        [0.6226]], device='mps:0')\n",
      "Iteration 76680 Training loss 0.05322814732789993 Validation loss 0.05736679956316948 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9396],\n",
      "        [0.5250]], device='mps:0')\n",
      "Iteration 76690 Training loss 0.05585472285747528 Validation loss 0.0574040450155735 Accuracy 0.843250036239624\n",
      "Output tensor([[0.8610],\n",
      "        [0.6973]], device='mps:0')\n",
      "Iteration 76700 Training loss 0.05117137357592583 Validation loss 0.05736089497804642 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.3780],\n",
      "        [0.2112]], device='mps:0')\n",
      "Iteration 76710 Training loss 0.0674406960606575 Validation loss 0.05740049108862877 Accuracy 0.842875063419342\n",
      "Output tensor([[0.7875],\n",
      "        [0.2391]], device='mps:0')\n",
      "Iteration 76720 Training loss 0.05536951869726181 Validation loss 0.05771767720580101 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.9533],\n",
      "        [0.9750]], device='mps:0')\n",
      "Iteration 76730 Training loss 0.05133699253201485 Validation loss 0.05753765627741814 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.2467],\n",
      "        [0.1524]], device='mps:0')\n",
      "Iteration 76740 Training loss 0.05869537591934204 Validation loss 0.057405613362789154 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0236],\n",
      "        [0.8993]], device='mps:0')\n",
      "Iteration 76750 Training loss 0.06261546164751053 Validation loss 0.05747605115175247 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0322],\n",
      "        [0.9434]], device='mps:0')\n",
      "Iteration 76760 Training loss 0.05598592013120651 Validation loss 0.05742998048663139 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0027],\n",
      "        [0.8969]], device='mps:0')\n",
      "Iteration 76770 Training loss 0.060914669185876846 Validation loss 0.05747409909963608 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.1521],\n",
      "        [0.0036]], device='mps:0')\n",
      "Iteration 76780 Training loss 0.05610207840800285 Validation loss 0.057374272495508194 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9608],\n",
      "        [0.0261]], device='mps:0')\n",
      "Iteration 76790 Training loss 0.052361030131578445 Validation loss 0.05735628679394722 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9534],\n",
      "        [0.9408]], device='mps:0')\n",
      "Iteration 76800 Training loss 0.05449739843606949 Validation loss 0.05735467001795769 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.0767],\n",
      "        [0.0277]], device='mps:0')\n",
      "Iteration 76810 Training loss 0.06071116775274277 Validation loss 0.05748651921749115 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.3679],\n",
      "        [0.0578]], device='mps:0')\n",
      "Iteration 76820 Training loss 0.04745843634009361 Validation loss 0.057370636612176895 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9483],\n",
      "        [0.3177]], device='mps:0')\n",
      "Iteration 76830 Training loss 0.047968510538339615 Validation loss 0.057452213019132614 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.5723],\n",
      "        [0.8866]], device='mps:0')\n",
      "Iteration 76840 Training loss 0.05085105821490288 Validation loss 0.05753738805651665 Accuracy 0.8422500491142273\n",
      "Output tensor([[0.1178],\n",
      "        [0.9990]], device='mps:0')\n",
      "Iteration 76850 Training loss 0.05026812106370926 Validation loss 0.05745043605566025 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.8198],\n",
      "        [0.8672]], device='mps:0')\n",
      "Iteration 76860 Training loss 0.05224769562482834 Validation loss 0.05735379457473755 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9847],\n",
      "        [0.1162]], device='mps:0')\n",
      "Iteration 76870 Training loss 0.062234725803136826 Validation loss 0.057352375239133835 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0256],\n",
      "        [0.3199]], device='mps:0')\n",
      "Iteration 76880 Training loss 0.05442923307418823 Validation loss 0.05747116729617119 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.9310],\n",
      "        [0.1679]], device='mps:0')\n",
      "Iteration 76890 Training loss 0.04502950236201286 Validation loss 0.05736822262406349 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.4447],\n",
      "        [0.9906]], device='mps:0')\n",
      "Iteration 76900 Training loss 0.05277334898710251 Validation loss 0.05738208442926407 Accuracy 0.843375027179718\n",
      "Output tensor([[0.9220],\n",
      "        [0.5845]], device='mps:0')\n",
      "Iteration 76910 Training loss 0.05470085144042969 Validation loss 0.05759654566645622 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.3194],\n",
      "        [0.1681]], device='mps:0')\n",
      "Iteration 76920 Training loss 0.05649028718471527 Validation loss 0.05767370015382767 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.8706],\n",
      "        [0.7794]], device='mps:0')\n",
      "Iteration 76930 Training loss 0.055314578115940094 Validation loss 0.05736741051077843 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9310],\n",
      "        [0.9984]], device='mps:0')\n",
      "Iteration 76940 Training loss 0.054019082337617874 Validation loss 0.05735577270388603 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.7806],\n",
      "        [0.4324]], device='mps:0')\n",
      "Iteration 76950 Training loss 0.05262896418571472 Validation loss 0.05738857015967369 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9612],\n",
      "        [0.0156]], device='mps:0')\n",
      "Iteration 76960 Training loss 0.057713478803634644 Validation loss 0.05735611915588379 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.4481],\n",
      "        [0.1495]], device='mps:0')\n",
      "Iteration 76970 Training loss 0.05236472934484482 Validation loss 0.057359177619218826 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0213],\n",
      "        [0.4302]], device='mps:0')\n",
      "Iteration 76980 Training loss 0.05313466861844063 Validation loss 0.05740894749760628 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0758],\n",
      "        [0.6781]], device='mps:0')\n",
      "Iteration 76990 Training loss 0.06011113151907921 Validation loss 0.05735631659626961 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.7102],\n",
      "        [0.8610]], device='mps:0')\n",
      "Iteration 77000 Training loss 0.05286083742976189 Validation loss 0.057351481169462204 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0469],\n",
      "        [0.1615]], device='mps:0')\n",
      "Iteration 77010 Training loss 0.04896287992596626 Validation loss 0.057436179369688034 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0723],\n",
      "        [0.0042]], device='mps:0')\n",
      "Iteration 77020 Training loss 0.05448327586054802 Validation loss 0.057359013706445694 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0887],\n",
      "        [0.3159]], device='mps:0')\n",
      "Iteration 77030 Training loss 0.05223569646477699 Validation loss 0.05735870450735092 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.9763],\n",
      "        [0.7285]], device='mps:0')\n",
      "Iteration 77040 Training loss 0.04992252215743065 Validation loss 0.057381462305784225 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9884],\n",
      "        [0.0882]], device='mps:0')\n",
      "Iteration 77050 Training loss 0.05165640637278557 Validation loss 0.05749732255935669 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1822],\n",
      "        [0.3461]], device='mps:0')\n",
      "Iteration 77060 Training loss 0.04948887228965759 Validation loss 0.057364534586668015 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0995],\n",
      "        [0.9594]], device='mps:0')\n",
      "Iteration 77070 Training loss 0.0499444417655468 Validation loss 0.05739789083600044 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.2376],\n",
      "        [0.9839]], device='mps:0')\n",
      "Iteration 77080 Training loss 0.05350213870406151 Validation loss 0.05736164376139641 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9044],\n",
      "        [0.1473]], device='mps:0')\n",
      "Iteration 77090 Training loss 0.061078332364559174 Validation loss 0.057550325989723206 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.4037],\n",
      "        [0.6851]], device='mps:0')\n",
      "Iteration 77100 Training loss 0.05360168218612671 Validation loss 0.05740603804588318 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9554],\n",
      "        [0.2168]], device='mps:0')\n",
      "Iteration 77110 Training loss 0.05575685203075409 Validation loss 0.057360827922821045 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9813],\n",
      "        [0.0492]], device='mps:0')\n",
      "Iteration 77120 Training loss 0.05152835696935654 Validation loss 0.05736124515533447 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.2979],\n",
      "        [0.8613]], device='mps:0')\n",
      "Iteration 77130 Training loss 0.05670411139726639 Validation loss 0.05736217275261879 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.1149],\n",
      "        [0.9527]], device='mps:0')\n",
      "Iteration 77140 Training loss 0.048322875052690506 Validation loss 0.057356685400009155 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.7671],\n",
      "        [0.0324]], device='mps:0')\n",
      "Iteration 77150 Training loss 0.06012166664004326 Validation loss 0.057355888187885284 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.8867],\n",
      "        [0.9398]], device='mps:0')\n",
      "Iteration 77160 Training loss 0.05289473757147789 Validation loss 0.05735626444220543 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.9754],\n",
      "        [0.3484]], device='mps:0')\n",
      "Iteration 77170 Training loss 0.056634753942489624 Validation loss 0.05738978460431099 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9656],\n",
      "        [0.7226]], device='mps:0')\n",
      "Iteration 77180 Training loss 0.04865824058651924 Validation loss 0.05736878141760826 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0988],\n",
      "        [0.7774]], device='mps:0')\n",
      "Iteration 77190 Training loss 0.04627140983939171 Validation loss 0.057401400059461594 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0854],\n",
      "        [0.1846]], device='mps:0')\n",
      "Iteration 77200 Training loss 0.05233102664351463 Validation loss 0.0573958121240139 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9932],\n",
      "        [0.9868]], device='mps:0')\n",
      "Iteration 77210 Training loss 0.04689548537135124 Validation loss 0.05734987184405327 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0486],\n",
      "        [0.0995]], device='mps:0')\n",
      "Iteration 77220 Training loss 0.05255183205008507 Validation loss 0.0573887825012207 Accuracy 0.843375027179718\n",
      "Output tensor([[0.1382],\n",
      "        [0.3283]], device='mps:0')\n",
      "Iteration 77230 Training loss 0.052370548248291016 Validation loss 0.05753388628363609 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.5257],\n",
      "        [0.0827]], device='mps:0')\n",
      "Iteration 77240 Training loss 0.05404036492109299 Validation loss 0.05734235793352127 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9793],\n",
      "        [0.9259]], device='mps:0')\n",
      "Iteration 77250 Training loss 0.06138264387845993 Validation loss 0.05733951926231384 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.3148],\n",
      "        [0.4093]], device='mps:0')\n",
      "Iteration 77260 Training loss 0.05488444119691849 Validation loss 0.0573514960706234 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.1808],\n",
      "        [0.8943]], device='mps:0')\n",
      "Iteration 77270 Training loss 0.04812852293252945 Validation loss 0.05734693631529808 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0315],\n",
      "        [0.2899]], device='mps:0')\n",
      "Iteration 77280 Training loss 0.046504344791173935 Validation loss 0.05734177678823471 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.1884],\n",
      "        [0.0158]], device='mps:0')\n",
      "Iteration 77290 Training loss 0.05222177878022194 Validation loss 0.05733565241098404 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9480],\n",
      "        [0.9951]], device='mps:0')\n",
      "Iteration 77300 Training loss 0.047433674335479736 Validation loss 0.05737094208598137 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.4558],\n",
      "        [0.9147]], device='mps:0')\n",
      "Iteration 77310 Training loss 0.05402912199497223 Validation loss 0.057502176612615585 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9185],\n",
      "        [0.2202]], device='mps:0')\n",
      "Iteration 77320 Training loss 0.05870247632265091 Validation loss 0.05743151530623436 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0763],\n",
      "        [0.1394]], device='mps:0')\n",
      "Iteration 77330 Training loss 0.05508017912507057 Validation loss 0.05736136808991432 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7481],\n",
      "        [0.6039]], device='mps:0')\n",
      "Iteration 77340 Training loss 0.05438153073191643 Validation loss 0.05733852833509445 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9601],\n",
      "        [0.0183]], device='mps:0')\n",
      "Iteration 77350 Training loss 0.05205211415886879 Validation loss 0.05734002962708473 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9504],\n",
      "        [0.6542]], device='mps:0')\n",
      "Iteration 77360 Training loss 0.04705443233251572 Validation loss 0.05743257701396942 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0961],\n",
      "        [0.8137]], device='mps:0')\n",
      "Iteration 77370 Training loss 0.055657777935266495 Validation loss 0.057365935295820236 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0600],\n",
      "        [0.3049]], device='mps:0')\n",
      "Iteration 77380 Training loss 0.05181631073355675 Validation loss 0.057380929589271545 Accuracy 0.84312504529953\n",
      "Output tensor([[0.7123],\n",
      "        [0.8675]], device='mps:0')\n",
      "Iteration 77390 Training loss 0.045713797211647034 Validation loss 0.057395998388528824 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.9504],\n",
      "        [0.9282]], device='mps:0')\n",
      "Iteration 77400 Training loss 0.04882282018661499 Validation loss 0.057324040681123734 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.4818],\n",
      "        [0.5221]], device='mps:0')\n",
      "Iteration 77410 Training loss 0.05148281529545784 Validation loss 0.05736963078379631 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.3460],\n",
      "        [0.4145]], device='mps:0')\n",
      "Iteration 77420 Training loss 0.06149161979556084 Validation loss 0.05734412372112274 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0224],\n",
      "        [0.9017]], device='mps:0')\n",
      "Iteration 77430 Training loss 0.04938674718141556 Validation loss 0.05740302801132202 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9433],\n",
      "        [0.8968]], device='mps:0')\n",
      "Iteration 77440 Training loss 0.05230731889605522 Validation loss 0.0573534294962883 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0105],\n",
      "        [0.3200]], device='mps:0')\n",
      "Iteration 77450 Training loss 0.04623527452349663 Validation loss 0.057324234396219254 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.8711],\n",
      "        [0.0806]], device='mps:0')\n",
      "Iteration 77460 Training loss 0.05397610366344452 Validation loss 0.05732205510139465 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.1189],\n",
      "        [0.1431]], device='mps:0')\n",
      "Iteration 77470 Training loss 0.05337539687752724 Validation loss 0.05732869356870651 Accuracy 0.843500018119812\n",
      "Output tensor([[0.7014],\n",
      "        [0.9618]], device='mps:0')\n",
      "Iteration 77480 Training loss 0.05107433721423149 Validation loss 0.05731842294335365 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9822],\n",
      "        [0.5380]], device='mps:0')\n",
      "Iteration 77490 Training loss 0.05910282954573631 Validation loss 0.05731732398271561 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0300],\n",
      "        [0.2122]], device='mps:0')\n",
      "Iteration 77500 Training loss 0.05969768390059471 Validation loss 0.05743132904171944 Accuracy 0.843000054359436\n",
      "Output tensor([[0.4396],\n",
      "        [0.3675]], device='mps:0')\n",
      "Iteration 77510 Training loss 0.05645619332790375 Validation loss 0.057339075952768326 Accuracy 0.843375027179718\n",
      "Output tensor([[0.8267],\n",
      "        [0.3241]], device='mps:0')\n",
      "Iteration 77520 Training loss 0.050604112446308136 Validation loss 0.05731679126620293 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.2017],\n",
      "        [0.2895]], device='mps:0')\n",
      "Iteration 77530 Training loss 0.05505306273698807 Validation loss 0.05731097236275673 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0111],\n",
      "        [0.9499]], device='mps:0')\n",
      "Iteration 77540 Training loss 0.07208142429590225 Validation loss 0.05746450275182724 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.7236],\n",
      "        [0.9419]], device='mps:0')\n",
      "Iteration 77550 Training loss 0.048007186502218246 Validation loss 0.05733862891793251 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.8615],\n",
      "        [0.0647]], device='mps:0')\n",
      "Iteration 77560 Training loss 0.05337097868323326 Validation loss 0.05731495842337608 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0907],\n",
      "        [0.0913]], device='mps:0')\n",
      "Iteration 77570 Training loss 0.058047350496053696 Validation loss 0.05731828510761261 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0436],\n",
      "        [0.1885]], device='mps:0')\n",
      "Iteration 77580 Training loss 0.056788891553878784 Validation loss 0.05732792243361473 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0195],\n",
      "        [0.7643]], device='mps:0')\n",
      "Iteration 77590 Training loss 0.05225472152233124 Validation loss 0.05737467110157013 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9904],\n",
      "        [0.9660]], device='mps:0')\n",
      "Iteration 77600 Training loss 0.05847065895795822 Validation loss 0.05733168125152588 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0195],\n",
      "        [0.2263]], device='mps:0')\n",
      "Iteration 77610 Training loss 0.05734531581401825 Validation loss 0.0575321801006794 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9718],\n",
      "        [0.8159]], device='mps:0')\n",
      "Iteration 77620 Training loss 0.05791943520307541 Validation loss 0.057378243654966354 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9800],\n",
      "        [0.0531]], device='mps:0')\n",
      "Iteration 77630 Training loss 0.057114582508802414 Validation loss 0.057341136038303375 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.3503],\n",
      "        [0.1032]], device='mps:0')\n",
      "Iteration 77640 Training loss 0.04933873936533928 Validation loss 0.057338349521160126 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.1398],\n",
      "        [0.5423]], device='mps:0')\n",
      "Iteration 77650 Training loss 0.05589306727051735 Validation loss 0.05737398937344551 Accuracy 0.84312504529953\n",
      "Output tensor([[0.7275],\n",
      "        [0.0627]], device='mps:0')\n",
      "Iteration 77660 Training loss 0.0571470633149147 Validation loss 0.05738086998462677 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.7600],\n",
      "        [0.9588]], device='mps:0')\n",
      "Iteration 77670 Training loss 0.060503337532281876 Validation loss 0.0574193000793457 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.3442],\n",
      "        [0.1577]], device='mps:0')\n",
      "Iteration 77680 Training loss 0.0589974969625473 Validation loss 0.057339031249284744 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.6996],\n",
      "        [0.2632]], device='mps:0')\n",
      "Iteration 77690 Training loss 0.05237041413784027 Validation loss 0.057317931205034256 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.1811],\n",
      "        [0.9916]], device='mps:0')\n",
      "Iteration 77700 Training loss 0.05611805617809296 Validation loss 0.05747587978839874 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.3266],\n",
      "        [0.9484]], device='mps:0')\n",
      "Iteration 77710 Training loss 0.05702538415789604 Validation loss 0.057532742619514465 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.8334],\n",
      "        [0.6769]], device='mps:0')\n",
      "Iteration 77720 Training loss 0.04828312620520592 Validation loss 0.057337068021297455 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0576],\n",
      "        [0.9720]], device='mps:0')\n",
      "Iteration 77730 Training loss 0.05474385991692543 Validation loss 0.057550616562366486 Accuracy 0.843000054359436\n",
      "Output tensor([[0.3417],\n",
      "        [0.4242]], device='mps:0')\n",
      "Iteration 77740 Training loss 0.06402454525232315 Validation loss 0.057360246777534485 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9808],\n",
      "        [0.0822]], device='mps:0')\n",
      "Iteration 77750 Training loss 0.05852728709578514 Validation loss 0.05733150616288185 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0051],\n",
      "        [0.2471]], device='mps:0')\n",
      "Iteration 77760 Training loss 0.061163462698459625 Validation loss 0.05733314901590347 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.3807],\n",
      "        [0.8059]], device='mps:0')\n",
      "Iteration 77770 Training loss 0.04895450174808502 Validation loss 0.05732296407222748 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.4401],\n",
      "        [0.2601]], device='mps:0')\n",
      "Iteration 77780 Training loss 0.05366646498441696 Validation loss 0.05732220783829689 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.2179],\n",
      "        [0.9459]], device='mps:0')\n",
      "Iteration 77790 Training loss 0.050969455391168594 Validation loss 0.05743696168065071 Accuracy 0.843000054359436\n",
      "Output tensor([[0.0513],\n",
      "        [0.1481]], device='mps:0')\n",
      "Iteration 77800 Training loss 0.053894832730293274 Validation loss 0.057352568954229355 Accuracy 0.843500018119812\n",
      "Output tensor([[0.8957],\n",
      "        [0.3043]], device='mps:0')\n",
      "Iteration 77810 Training loss 0.052949223667383194 Validation loss 0.05737360566854477 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1833],\n",
      "        [0.0501]], device='mps:0')\n",
      "Iteration 77820 Training loss 0.05077069252729416 Validation loss 0.057317834347486496 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0991],\n",
      "        [0.9907]], device='mps:0')\n",
      "Iteration 77830 Training loss 0.05521976947784424 Validation loss 0.05732361972332001 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.1406],\n",
      "        [0.1393]], device='mps:0')\n",
      "Iteration 77840 Training loss 0.0517754927277565 Validation loss 0.057331256568431854 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9384],\n",
      "        [0.8196]], device='mps:0')\n",
      "Iteration 77850 Training loss 0.058032240718603134 Validation loss 0.05730469897389412 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0262],\n",
      "        [0.0142]], device='mps:0')\n",
      "Iteration 77860 Training loss 0.04713648557662964 Validation loss 0.057472772896289825 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.4752],\n",
      "        [0.9716]], device='mps:0')\n",
      "Iteration 77870 Training loss 0.051583144813776016 Validation loss 0.05736231431365013 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9510],\n",
      "        [0.6451]], device='mps:0')\n",
      "Iteration 77880 Training loss 0.06069967523217201 Validation loss 0.057332370430231094 Accuracy 0.843500018119812\n",
      "Output tensor([[0.9325],\n",
      "        [0.0465]], device='mps:0')\n",
      "Iteration 77890 Training loss 0.05992579832673073 Validation loss 0.05730292946100235 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.6792],\n",
      "        [0.0336]], device='mps:0')\n",
      "Iteration 77900 Training loss 0.04796665534377098 Validation loss 0.057307641953229904 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.7018],\n",
      "        [0.7789]], device='mps:0')\n",
      "Iteration 77910 Training loss 0.055710237473249435 Validation loss 0.057539161294698715 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9700],\n",
      "        [0.1277]], device='mps:0')\n",
      "Iteration 77920 Training loss 0.05506075918674469 Validation loss 0.05733887106180191 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.7947],\n",
      "        [0.4971]], device='mps:0')\n",
      "Iteration 77930 Training loss 0.05838673934340477 Validation loss 0.05730469524860382 Accuracy 0.8458750247955322\n",
      "Output tensor([[0.7489],\n",
      "        [0.7568]], device='mps:0')\n",
      "Iteration 77940 Training loss 0.04979616031050682 Validation loss 0.057311274111270905 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9004],\n",
      "        [0.9512]], device='mps:0')\n",
      "Iteration 77950 Training loss 0.0487668402493 Validation loss 0.057310570031404495 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.7582],\n",
      "        [0.9433]], device='mps:0')\n",
      "Iteration 77960 Training loss 0.05519146844744682 Validation loss 0.05735344812273979 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.8769],\n",
      "        [0.9923]], device='mps:0')\n",
      "Iteration 77970 Training loss 0.05581638216972351 Validation loss 0.05729752406477928 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.0251],\n",
      "        [0.2144]], device='mps:0')\n",
      "Iteration 77980 Training loss 0.045778095722198486 Validation loss 0.0573323629796505 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.1332],\n",
      "        [0.2505]], device='mps:0')\n",
      "Iteration 77990 Training loss 0.05450572818517685 Validation loss 0.05734335631132126 Accuracy 0.8426250219345093\n",
      "Output tensor([[0.6716],\n",
      "        [0.3933]], device='mps:0')\n",
      "Iteration 78000 Training loss 0.05250122770667076 Validation loss 0.05731460452079773 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0466],\n",
      "        [0.0263]], device='mps:0')\n",
      "Iteration 78010 Training loss 0.05029870942234993 Validation loss 0.05728969722986221 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0471],\n",
      "        [0.8879]], device='mps:0')\n",
      "Iteration 78020 Training loss 0.05156697332859039 Validation loss 0.05731004476547241 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9596],\n",
      "        [0.9382]], device='mps:0')\n",
      "Iteration 78030 Training loss 0.05154135450720787 Validation loss 0.05732277035713196 Accuracy 0.843000054359436\n",
      "Output tensor([[0.9577],\n",
      "        [0.6786]], device='mps:0')\n",
      "Iteration 78040 Training loss 0.05853831768035889 Validation loss 0.05728618800640106 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.7993],\n",
      "        [0.1066]], device='mps:0')\n",
      "Iteration 78050 Training loss 0.050529640167951584 Validation loss 0.05739050731062889 Accuracy 0.84312504529953\n",
      "Output tensor([[0.3967],\n",
      "        [0.0728]], device='mps:0')\n",
      "Iteration 78060 Training loss 0.048342108726501465 Validation loss 0.05728301778435707 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.0612],\n",
      "        [0.4587]], device='mps:0')\n",
      "Iteration 78070 Training loss 0.04985981434583664 Validation loss 0.05728766322135925 Accuracy 0.8457500338554382\n",
      "Output tensor([[0.7768],\n",
      "        [0.7748]], device='mps:0')\n",
      "Iteration 78080 Training loss 0.047293949872255325 Validation loss 0.057304076850414276 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.8253],\n",
      "        [0.1110]], device='mps:0')\n",
      "Iteration 78090 Training loss 0.054448604583740234 Validation loss 0.05741063877940178 Accuracy 0.843250036239624\n",
      "Output tensor([[0.1481],\n",
      "        [0.2453]], device='mps:0')\n",
      "Iteration 78100 Training loss 0.046884048730134964 Validation loss 0.05731779709458351 Accuracy 0.842875063419342\n",
      "Output tensor([[0.0417],\n",
      "        [0.7282]], device='mps:0')\n",
      "Iteration 78110 Training loss 0.04858190566301346 Validation loss 0.05727704241871834 Accuracy 0.8455000519752502\n",
      "Output tensor([[0.2215],\n",
      "        [0.9598]], device='mps:0')\n",
      "Iteration 78120 Training loss 0.060779035091400146 Validation loss 0.05729619041085243 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.4593],\n",
      "        [0.1053]], device='mps:0')\n",
      "Iteration 78130 Training loss 0.057594165205955505 Validation loss 0.0573170967400074 Accuracy 0.842875063419342\n",
      "Output tensor([[0.9329],\n",
      "        [0.0082]], device='mps:0')\n",
      "Iteration 78140 Training loss 0.056062549352645874 Validation loss 0.05727905407547951 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.6759],\n",
      "        [0.0079]], device='mps:0')\n",
      "Iteration 78150 Training loss 0.046776074916124344 Validation loss 0.057282883673906326 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9153],\n",
      "        [0.9237]], device='mps:0')\n",
      "Iteration 78160 Training loss 0.050314683467149734 Validation loss 0.05728873610496521 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.0275],\n",
      "        [0.6937]], device='mps:0')\n",
      "Iteration 78170 Training loss 0.049730557948350906 Validation loss 0.057320695370435715 Accuracy 0.84312504529953\n",
      "Output tensor([[0.0286],\n",
      "        [0.0356]], device='mps:0')\n",
      "Iteration 78180 Training loss 0.05406055971980095 Validation loss 0.0573267862200737 Accuracy 0.843375027179718\n",
      "Output tensor([[0.6716],\n",
      "        [0.9975]], device='mps:0')\n",
      "Iteration 78190 Training loss 0.05216090381145477 Validation loss 0.05728919431567192 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.3400],\n",
      "        [0.1822]], device='mps:0')\n",
      "Iteration 78200 Training loss 0.05090438202023506 Validation loss 0.05757341533899307 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.6096],\n",
      "        [0.0895]], device='mps:0')\n",
      "Iteration 78210 Training loss 0.052340686321258545 Validation loss 0.057443421334028244 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9977],\n",
      "        [0.5383]], device='mps:0')\n",
      "Iteration 78220 Training loss 0.04977666586637497 Validation loss 0.057282883673906326 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.9438],\n",
      "        [0.9972]], device='mps:0')\n",
      "Iteration 78230 Training loss 0.05266426503658295 Validation loss 0.057342708110809326 Accuracy 0.843375027179718\n",
      "Output tensor([[0.0554],\n",
      "        [0.0257]], device='mps:0')\n",
      "Iteration 78240 Training loss 0.05055089667439461 Validation loss 0.05729173123836517 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.8896],\n",
      "        [0.5789]], device='mps:0')\n",
      "Iteration 78250 Training loss 0.0566476434469223 Validation loss 0.057315606623888016 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.5506],\n",
      "        [0.3283]], device='mps:0')\n",
      "Iteration 78260 Training loss 0.05609085038304329 Validation loss 0.057299643754959106 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9789],\n",
      "        [0.7031]], device='mps:0')\n",
      "Iteration 78270 Training loss 0.048600949347019196 Validation loss 0.05730990692973137 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9431],\n",
      "        [0.8776]], device='mps:0')\n",
      "Iteration 78280 Training loss 0.04475166276097298 Validation loss 0.057428788393735886 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1389],\n",
      "        [0.4499]], device='mps:0')\n",
      "Iteration 78290 Training loss 0.05733153223991394 Validation loss 0.05739007517695427 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.8516],\n",
      "        [0.0276]], device='mps:0')\n",
      "Iteration 78300 Training loss 0.05910293012857437 Validation loss 0.057278428226709366 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.7218],\n",
      "        [0.0751]], device='mps:0')\n",
      "Iteration 78310 Training loss 0.05974160507321358 Validation loss 0.057292256504297256 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.7636],\n",
      "        [0.9076]], device='mps:0')\n",
      "Iteration 78320 Training loss 0.04938529059290886 Validation loss 0.05727396160364151 Accuracy 0.8455000519752502\n",
      "Output tensor([[0.5104],\n",
      "        [0.8184]], device='mps:0')\n",
      "Iteration 78330 Training loss 0.04741555452346802 Validation loss 0.05747038498520851 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.0976],\n",
      "        [0.0440]], device='mps:0')\n",
      "Iteration 78340 Training loss 0.04975565895438194 Validation loss 0.057270754128694534 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.0153],\n",
      "        [0.6376]], device='mps:0')\n",
      "Iteration 78350 Training loss 0.043145641684532166 Validation loss 0.057293277233839035 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0391],\n",
      "        [0.8798]], device='mps:0')\n",
      "Iteration 78360 Training loss 0.04304927587509155 Validation loss 0.057281456887722015 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0941],\n",
      "        [0.9162]], device='mps:0')\n",
      "Iteration 78370 Training loss 0.05790510028600693 Validation loss 0.057331833988428116 Accuracy 0.843375027179718\n",
      "Output tensor([[0.5335],\n",
      "        [0.9607]], device='mps:0')\n",
      "Iteration 78380 Training loss 0.05864258110523224 Validation loss 0.05729571729898453 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.7139],\n",
      "        [0.6479]], device='mps:0')\n",
      "Iteration 78390 Training loss 0.057101067155599594 Validation loss 0.05730943754315376 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.8401],\n",
      "        [0.6538]], device='mps:0')\n",
      "Iteration 78400 Training loss 0.05029384419322014 Validation loss 0.057255085557699203 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.3706],\n",
      "        [0.5689]], device='mps:0')\n",
      "Iteration 78410 Training loss 0.05484916269779205 Validation loss 0.057252444326877594 Accuracy 0.8455000519752502\n",
      "Output tensor([[0.0718],\n",
      "        [0.0599]], device='mps:0')\n",
      "Iteration 78420 Training loss 0.0516950897872448 Validation loss 0.05727757513523102 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0104],\n",
      "        [0.6528]], device='mps:0')\n",
      "Iteration 78430 Training loss 0.06093800440430641 Validation loss 0.057250987738370895 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.9699],\n",
      "        [0.9127]], device='mps:0')\n",
      "Iteration 78440 Training loss 0.05680026486515999 Validation loss 0.05729273334145546 Accuracy 0.843500018119812\n",
      "Output tensor([[0.2467],\n",
      "        [0.0387]], device='mps:0')\n",
      "Iteration 78450 Training loss 0.05404931306838989 Validation loss 0.05731522664427757 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0389],\n",
      "        [0.4677]], device='mps:0')\n",
      "Iteration 78460 Training loss 0.06074574962258339 Validation loss 0.05738065764307976 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0163],\n",
      "        [0.0725]], device='mps:0')\n",
      "Iteration 78470 Training loss 0.06211269274353981 Validation loss 0.05725886672735214 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9362],\n",
      "        [0.9722]], device='mps:0')\n",
      "Iteration 78480 Training loss 0.05459179729223251 Validation loss 0.0572662390768528 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.5305],\n",
      "        [0.0715]], device='mps:0')\n",
      "Iteration 78490 Training loss 0.06251264363527298 Validation loss 0.057260215282440186 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.5331],\n",
      "        [0.1412]], device='mps:0')\n",
      "Iteration 78500 Training loss 0.0537501722574234 Validation loss 0.05724724009633064 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.9745],\n",
      "        [0.8304]], device='mps:0')\n",
      "Iteration 78510 Training loss 0.056660037487745285 Validation loss 0.057309120893478394 Accuracy 0.843375027179718\n",
      "Output tensor([[0.1066],\n",
      "        [0.0087]], device='mps:0')\n",
      "Iteration 78520 Training loss 0.052255161106586456 Validation loss 0.05731986090540886 Accuracy 0.84312504529953\n",
      "Output tensor([[0.5926],\n",
      "        [0.0558]], device='mps:0')\n",
      "Iteration 78530 Training loss 0.06110001355409622 Validation loss 0.05739668756723404 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0140],\n",
      "        [0.8531]], device='mps:0')\n",
      "Iteration 78540 Training loss 0.045638952404260635 Validation loss 0.05726385861635208 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.7918],\n",
      "        [0.2050]], device='mps:0')\n",
      "Iteration 78550 Training loss 0.05577994883060455 Validation loss 0.05726352706551552 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.8402],\n",
      "        [0.2574]], device='mps:0')\n",
      "Iteration 78560 Training loss 0.04914318770170212 Validation loss 0.05724211782217026 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.2099],\n",
      "        [0.9753]], device='mps:0')\n",
      "Iteration 78570 Training loss 0.045700959861278534 Validation loss 0.057252202183008194 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.1361],\n",
      "        [0.7515]], device='mps:0')\n",
      "Iteration 78580 Training loss 0.05399556830525398 Validation loss 0.05724288150668144 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.3331],\n",
      "        [0.8938]], device='mps:0')\n",
      "Iteration 78590 Training loss 0.057608116418123245 Validation loss 0.05727037414908409 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.8338],\n",
      "        [0.8970]], device='mps:0')\n",
      "Iteration 78600 Training loss 0.05290435627102852 Validation loss 0.057265207171440125 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.7760],\n",
      "        [0.5421]], device='mps:0')\n",
      "Iteration 78610 Training loss 0.049925774335861206 Validation loss 0.057241130620241165 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.7903],\n",
      "        [0.1051]], device='mps:0')\n",
      "Iteration 78620 Training loss 0.05623233690857887 Validation loss 0.05723945051431656 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.0953],\n",
      "        [0.9847]], device='mps:0')\n",
      "Iteration 78630 Training loss 0.04555942863225937 Validation loss 0.057247813791036606 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.2526],\n",
      "        [0.2567]], device='mps:0')\n",
      "Iteration 78640 Training loss 0.04924701526761055 Validation loss 0.05724981799721718 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9636],\n",
      "        [0.0229]], device='mps:0')\n",
      "Iteration 78650 Training loss 0.05312129855155945 Validation loss 0.05723700299859047 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.8850],\n",
      "        [0.1307]], device='mps:0')\n",
      "Iteration 78660 Training loss 0.0490855872631073 Validation loss 0.057339295744895935 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7866],\n",
      "        [0.2468]], device='mps:0')\n",
      "Iteration 78670 Training loss 0.053619708865880966 Validation loss 0.057253994047641754 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9685],\n",
      "        [0.9026]], device='mps:0')\n",
      "Iteration 78680 Training loss 0.04987978935241699 Validation loss 0.05723016336560249 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.0363],\n",
      "        [0.2486]], device='mps:0')\n",
      "Iteration 78690 Training loss 0.0543675571680069 Validation loss 0.05725373700261116 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.6957],\n",
      "        [0.8339]], device='mps:0')\n",
      "Iteration 78700 Training loss 0.049139756709337234 Validation loss 0.057258784770965576 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9157],\n",
      "        [0.0021]], device='mps:0')\n",
      "Iteration 78710 Training loss 0.05522916838526726 Validation loss 0.05724103003740311 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9657],\n",
      "        [0.6317]], device='mps:0')\n",
      "Iteration 78720 Training loss 0.05255259573459625 Validation loss 0.057406410574913025 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0299],\n",
      "        [0.4851]], device='mps:0')\n",
      "Iteration 78730 Training loss 0.04825836047530174 Validation loss 0.057315438985824585 Accuracy 0.84312504529953\n",
      "Output tensor([[0.8033],\n",
      "        [0.0872]], device='mps:0')\n",
      "Iteration 78740 Training loss 0.049384955316782 Validation loss 0.05727267637848854 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.8230],\n",
      "        [0.3765]], device='mps:0')\n",
      "Iteration 78750 Training loss 0.051066216081380844 Validation loss 0.05723953992128372 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0812],\n",
      "        [0.1532]], device='mps:0')\n",
      "Iteration 78760 Training loss 0.05347475782036781 Validation loss 0.05729277804493904 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9711],\n",
      "        [0.8073]], device='mps:0')\n",
      "Iteration 78770 Training loss 0.05794506520032883 Validation loss 0.05730847269296646 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.5879],\n",
      "        [0.9440]], device='mps:0')\n",
      "Iteration 78780 Training loss 0.053635016083717346 Validation loss 0.05733285844326019 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0886],\n",
      "        [0.2009]], device='mps:0')\n",
      "Iteration 78790 Training loss 0.046543776988983154 Validation loss 0.05722568929195404 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.0913],\n",
      "        [0.5868]], device='mps:0')\n",
      "Iteration 78800 Training loss 0.05462367460131645 Validation loss 0.0573037751019001 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1393],\n",
      "        [0.2352]], device='mps:0')\n",
      "Iteration 78810 Training loss 0.05428757518529892 Validation loss 0.05722174420952797 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.0383],\n",
      "        [0.4863]], device='mps:0')\n",
      "Iteration 78820 Training loss 0.05576612427830696 Validation loss 0.057280298322439194 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.7134],\n",
      "        [0.9781]], device='mps:0')\n",
      "Iteration 78830 Training loss 0.05413779243826866 Validation loss 0.05722662806510925 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9296],\n",
      "        [0.9707]], device='mps:0')\n",
      "Iteration 78840 Training loss 0.05571872740983963 Validation loss 0.05732100084424019 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.7157],\n",
      "        [0.6580]], device='mps:0')\n",
      "Iteration 78850 Training loss 0.060877781361341476 Validation loss 0.05726413056254387 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0865],\n",
      "        [0.9569]], device='mps:0')\n",
      "Iteration 78860 Training loss 0.05326678976416588 Validation loss 0.05726242437958717 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9368],\n",
      "        [0.0302]], device='mps:0')\n",
      "Iteration 78870 Training loss 0.06219354644417763 Validation loss 0.05722741037607193 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.1728],\n",
      "        [0.2117]], device='mps:0')\n",
      "Iteration 78880 Training loss 0.053609322756528854 Validation loss 0.057219866663217545 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.8542],\n",
      "        [0.1424]], device='mps:0')\n",
      "Iteration 78890 Training loss 0.049289949238300323 Validation loss 0.05721299722790718 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.9554],\n",
      "        [0.9645]], device='mps:0')\n",
      "Iteration 78900 Training loss 0.057336874306201935 Validation loss 0.057226456701755524 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9639],\n",
      "        [0.3013]], device='mps:0')\n",
      "Iteration 78910 Training loss 0.05712142214179039 Validation loss 0.05721204727888107 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.8835],\n",
      "        [0.2741]], device='mps:0')\n",
      "Iteration 78920 Training loss 0.05265488848090172 Validation loss 0.0576702319085598 Accuracy 0.843500018119812\n",
      "Output tensor([[0.1616],\n",
      "        [0.3801]], device='mps:0')\n",
      "Iteration 78930 Training loss 0.05395479127764702 Validation loss 0.05720781162381172 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.8591],\n",
      "        [0.6601]], device='mps:0')\n",
      "Iteration 78940 Training loss 0.05189625173807144 Validation loss 0.05724141374230385 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.6663],\n",
      "        [0.3321]], device='mps:0')\n",
      "Iteration 78950 Training loss 0.054296139627695084 Validation loss 0.05721615254878998 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9500],\n",
      "        [0.9869]], device='mps:0')\n",
      "Iteration 78960 Training loss 0.0512082464993 Validation loss 0.05737053602933884 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.0158],\n",
      "        [0.9924]], device='mps:0')\n",
      "Iteration 78970 Training loss 0.04967104643583298 Validation loss 0.05730992555618286 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9634],\n",
      "        [0.0423]], device='mps:0')\n",
      "Iteration 78980 Training loss 0.05462852492928505 Validation loss 0.057213302701711655 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.1673],\n",
      "        [0.9747]], device='mps:0')\n",
      "Iteration 78990 Training loss 0.049536626785993576 Validation loss 0.05720340833067894 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.2848],\n",
      "        [0.1537]], device='mps:0')\n",
      "Iteration 79000 Training loss 0.05270516127347946 Validation loss 0.057207293808460236 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.8103],\n",
      "        [0.3255]], device='mps:0')\n",
      "Iteration 79010 Training loss 0.05228504166007042 Validation loss 0.057277508080005646 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1500],\n",
      "        [0.2692]], device='mps:0')\n",
      "Iteration 79020 Training loss 0.05688988417387009 Validation loss 0.057275354862213135 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0513],\n",
      "        [0.0285]], device='mps:0')\n",
      "Iteration 79030 Training loss 0.04792017862200737 Validation loss 0.057363856583833694 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.2568],\n",
      "        [0.0498]], device='mps:0')\n",
      "Iteration 79040 Training loss 0.04920772835612297 Validation loss 0.05723531171679497 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.8971],\n",
      "        [0.8930]], device='mps:0')\n",
      "Iteration 79050 Training loss 0.0474836528301239 Validation loss 0.05724409222602844 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.6176],\n",
      "        [0.2396]], device='mps:0')\n",
      "Iteration 79060 Training loss 0.052565354853868484 Validation loss 0.05720978230237961 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9726],\n",
      "        [0.0577]], device='mps:0')\n",
      "Iteration 79070 Training loss 0.05796007812023163 Validation loss 0.05719512328505516 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.7117],\n",
      "        [0.0809]], device='mps:0')\n",
      "Iteration 79080 Training loss 0.05356427654623985 Validation loss 0.057207152247428894 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.4622],\n",
      "        [0.7678]], device='mps:0')\n",
      "Iteration 79090 Training loss 0.048318490386009216 Validation loss 0.05721527710556984 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.1740],\n",
      "        [0.3395]], device='mps:0')\n",
      "Iteration 79100 Training loss 0.05685485154390335 Validation loss 0.05719612538814545 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1005],\n",
      "        [0.3633]], device='mps:0')\n",
      "Iteration 79110 Training loss 0.05164019390940666 Validation loss 0.0572349838912487 Accuracy 0.84312504529953\n",
      "Output tensor([[0.7887],\n",
      "        [0.2078]], device='mps:0')\n",
      "Iteration 79120 Training loss 0.06063447892665863 Validation loss 0.057223375886678696 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.0989],\n",
      "        [0.2618]], device='mps:0')\n",
      "Iteration 79130 Training loss 0.05552862584590912 Validation loss 0.05724884197115898 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.8679],\n",
      "        [0.1457]], device='mps:0')\n",
      "Iteration 79140 Training loss 0.052687160670757294 Validation loss 0.05722188577055931 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9872],\n",
      "        [0.5148]], device='mps:0')\n",
      "Iteration 79150 Training loss 0.05839359015226364 Validation loss 0.05722048506140709 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1423],\n",
      "        [0.0657]], device='mps:0')\n",
      "Iteration 79160 Training loss 0.05202249810099602 Validation loss 0.05719931796193123 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.8937],\n",
      "        [0.7323]], device='mps:0')\n",
      "Iteration 79170 Training loss 0.04558439552783966 Validation loss 0.05733172222971916 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.3769],\n",
      "        [0.9661]], device='mps:0')\n",
      "Iteration 79180 Training loss 0.0557742677628994 Validation loss 0.05721805617213249 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0196],\n",
      "        [0.9560]], device='mps:0')\n",
      "Iteration 79190 Training loss 0.05153370648622513 Validation loss 0.05724039301276207 Accuracy 0.843500018119812\n",
      "Output tensor([[0.1116],\n",
      "        [0.2497]], device='mps:0')\n",
      "Iteration 79200 Training loss 0.06289517134428024 Validation loss 0.057301100343465805 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.2003],\n",
      "        [0.7686]], device='mps:0')\n",
      "Iteration 79210 Training loss 0.04686032608151436 Validation loss 0.05723203718662262 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.6529],\n",
      "        [0.1877]], device='mps:0')\n",
      "Iteration 79220 Training loss 0.04781343415379524 Validation loss 0.05720284953713417 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.9093],\n",
      "        [0.0171]], device='mps:0')\n",
      "Iteration 79230 Training loss 0.047372400760650635 Validation loss 0.0572555735707283 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9599],\n",
      "        [0.9058]], device='mps:0')\n",
      "Iteration 79240 Training loss 0.052426569163799286 Validation loss 0.05720077082514763 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.9959],\n",
      "        [0.0152]], device='mps:0')\n",
      "Iteration 79250 Training loss 0.057953447103500366 Validation loss 0.05720582976937294 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.1109],\n",
      "        [0.7817]], device='mps:0')\n",
      "Iteration 79260 Training loss 0.0479184128344059 Validation loss 0.05724970996379852 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0625],\n",
      "        [0.0480]], device='mps:0')\n",
      "Iteration 79270 Training loss 0.05569051578640938 Validation loss 0.05722830444574356 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1167],\n",
      "        [0.4624]], device='mps:0')\n",
      "Iteration 79280 Training loss 0.05533621832728386 Validation loss 0.057192590087652206 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.0705],\n",
      "        [0.4172]], device='mps:0')\n",
      "Iteration 79290 Training loss 0.057722434401512146 Validation loss 0.05732638016343117 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.8721],\n",
      "        [0.0225]], device='mps:0')\n",
      "Iteration 79300 Training loss 0.05841788649559021 Validation loss 0.057379815727472305 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9836],\n",
      "        [0.3546]], device='mps:0')\n",
      "Iteration 79310 Training loss 0.05184761807322502 Validation loss 0.05747700855135918 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9442],\n",
      "        [0.7723]], device='mps:0')\n",
      "Iteration 79320 Training loss 0.061290573328733444 Validation loss 0.05724450573325157 Accuracy 0.843500018119812\n",
      "Output tensor([[0.7525],\n",
      "        [0.0216]], device='mps:0')\n",
      "Iteration 79330 Training loss 0.05497667193412781 Validation loss 0.057230621576309204 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0528],\n",
      "        [0.1098]], device='mps:0')\n",
      "Iteration 79340 Training loss 0.047198016196489334 Validation loss 0.05725337564945221 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0514],\n",
      "        [0.7694]], device='mps:0')\n",
      "Iteration 79350 Training loss 0.05642729997634888 Validation loss 0.05720004811882973 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9911],\n",
      "        [0.2173]], device='mps:0')\n",
      "Iteration 79360 Training loss 0.06251685321331024 Validation loss 0.05725839361548424 Accuracy 0.843375027179718\n",
      "Output tensor([[0.8701],\n",
      "        [0.9597]], device='mps:0')\n",
      "Iteration 79370 Training loss 0.05437829717993736 Validation loss 0.05721458047628403 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9496],\n",
      "        [0.6162]], device='mps:0')\n",
      "Iteration 79380 Training loss 0.05446793884038925 Validation loss 0.05742102488875389 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9232],\n",
      "        [0.7158]], device='mps:0')\n",
      "Iteration 79390 Training loss 0.056696806102991104 Validation loss 0.05754253640770912 Accuracy 0.8425000309944153\n",
      "Output tensor([[0.1854],\n",
      "        [0.9332]], device='mps:0')\n",
      "Iteration 79400 Training loss 0.05700838565826416 Validation loss 0.05722185969352722 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.8724],\n",
      "        [0.7304]], device='mps:0')\n",
      "Iteration 79410 Training loss 0.05422534793615341 Validation loss 0.05721649155020714 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0477],\n",
      "        [0.6593]], device='mps:0')\n",
      "Iteration 79420 Training loss 0.05742589011788368 Validation loss 0.0574517548084259 Accuracy 0.843250036239624\n",
      "Output tensor([[0.7874],\n",
      "        [0.0307]], device='mps:0')\n",
      "Iteration 79430 Training loss 0.04995245859026909 Validation loss 0.05719988793134689 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.3421],\n",
      "        [0.8715]], device='mps:0')\n",
      "Iteration 79440 Training loss 0.057957570999860764 Validation loss 0.057224340736866 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9747],\n",
      "        [0.6086]], device='mps:0')\n",
      "Iteration 79450 Training loss 0.06061822175979614 Validation loss 0.05720756947994232 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.8033],\n",
      "        [0.9864]], device='mps:0')\n",
      "Iteration 79460 Training loss 0.06020813807845116 Validation loss 0.05718859285116196 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.8574],\n",
      "        [0.4550]], device='mps:0')\n",
      "Iteration 79470 Training loss 0.04714329540729523 Validation loss 0.05719635263085365 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.7710],\n",
      "        [0.7697]], device='mps:0')\n",
      "Iteration 79480 Training loss 0.05250660330057144 Validation loss 0.05718115717172623 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.2281],\n",
      "        [0.8466]], device='mps:0')\n",
      "Iteration 79490 Training loss 0.06119716167449951 Validation loss 0.05717987194657326 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.1556],\n",
      "        [0.6908]], device='mps:0')\n",
      "Iteration 79500 Training loss 0.05775168165564537 Validation loss 0.057286981493234634 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.8660],\n",
      "        [0.1858]], device='mps:0')\n",
      "Iteration 79510 Training loss 0.057248786091804504 Validation loss 0.057259462773799896 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9864],\n",
      "        [0.9351]], device='mps:0')\n",
      "Iteration 79520 Training loss 0.0509134903550148 Validation loss 0.057290926575660706 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.1680],\n",
      "        [0.2117]], device='mps:0')\n",
      "Iteration 79530 Training loss 0.047216493636369705 Validation loss 0.05734190717339516 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0887],\n",
      "        [0.8092]], device='mps:0')\n",
      "Iteration 79540 Training loss 0.050743721425533295 Validation loss 0.057189349085092545 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.1907],\n",
      "        [0.6096]], device='mps:0')\n",
      "Iteration 79550 Training loss 0.057398200035095215 Validation loss 0.05718062072992325 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.1051],\n",
      "        [0.0820]], device='mps:0')\n",
      "Iteration 79560 Training loss 0.04868953302502632 Validation loss 0.057237811386585236 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.3599],\n",
      "        [0.9333]], device='mps:0')\n",
      "Iteration 79570 Training loss 0.05671688914299011 Validation loss 0.05722300335764885 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0509],\n",
      "        [0.4895]], device='mps:0')\n",
      "Iteration 79580 Training loss 0.046204760670661926 Validation loss 0.057193491607904434 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0675],\n",
      "        [0.9882]], device='mps:0')\n",
      "Iteration 79590 Training loss 0.041778143495321274 Validation loss 0.057232875376939774 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.6808],\n",
      "        [0.0720]], device='mps:0')\n",
      "Iteration 79600 Training loss 0.05445418134331703 Validation loss 0.05720330402255058 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.7229],\n",
      "        [0.2244]], device='mps:0')\n",
      "Iteration 79610 Training loss 0.06216375157237053 Validation loss 0.05718301981687546 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.8255],\n",
      "        [0.0968]], device='mps:0')\n",
      "Iteration 79620 Training loss 0.0470939464867115 Validation loss 0.057286955416202545 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.9706],\n",
      "        [0.2618]], device='mps:0')\n",
      "Iteration 79630 Training loss 0.05310608819127083 Validation loss 0.0572003498673439 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9469],\n",
      "        [0.1523]], device='mps:0')\n",
      "Iteration 79640 Training loss 0.04926019906997681 Validation loss 0.05723195523023605 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9368],\n",
      "        [0.9886]], device='mps:0')\n",
      "Iteration 79650 Training loss 0.04801967367529869 Validation loss 0.057206060737371445 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9314],\n",
      "        [0.0596]], device='mps:0')\n",
      "Iteration 79660 Training loss 0.045991890132427216 Validation loss 0.057205237448215485 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0793],\n",
      "        [0.7228]], device='mps:0')\n",
      "Iteration 79670 Training loss 0.05001001060009003 Validation loss 0.057259369641542435 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.0287],\n",
      "        [0.9579]], device='mps:0')\n",
      "Iteration 79680 Training loss 0.052586879581213 Validation loss 0.057196054607629776 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.0817],\n",
      "        [0.7317]], device='mps:0')\n",
      "Iteration 79690 Training loss 0.05718456953763962 Validation loss 0.05718999728560448 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.4517],\n",
      "        [0.8097]], device='mps:0')\n",
      "Iteration 79700 Training loss 0.053263891488313675 Validation loss 0.05718803033232689 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.9932],\n",
      "        [0.0334]], device='mps:0')\n",
      "Iteration 79710 Training loss 0.05478609353303909 Validation loss 0.05719310790300369 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.8991],\n",
      "        [0.2129]], device='mps:0')\n",
      "Iteration 79720 Training loss 0.05440370365977287 Validation loss 0.05718991532921791 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.7538],\n",
      "        [0.6320]], device='mps:0')\n",
      "Iteration 79730 Training loss 0.052848171442747116 Validation loss 0.05722096562385559 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.2253],\n",
      "        [0.4090]], device='mps:0')\n",
      "Iteration 79740 Training loss 0.057677287608385086 Validation loss 0.05721794068813324 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9656],\n",
      "        [0.0462]], device='mps:0')\n",
      "Iteration 79750 Training loss 0.05162813887000084 Validation loss 0.057189881801605225 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.4245],\n",
      "        [0.4805]], device='mps:0')\n",
      "Iteration 79760 Training loss 0.05247332528233528 Validation loss 0.05719338357448578 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.4077],\n",
      "        [0.6488]], device='mps:0')\n",
      "Iteration 79770 Training loss 0.04390450939536095 Validation loss 0.05722888931632042 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0814],\n",
      "        [0.0771]], device='mps:0')\n",
      "Iteration 79780 Training loss 0.058014675974845886 Validation loss 0.05718803033232689 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.1565],\n",
      "        [0.9216]], device='mps:0')\n",
      "Iteration 79790 Training loss 0.05414411425590515 Validation loss 0.057517506182193756 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0503],\n",
      "        [0.4792]], device='mps:0')\n",
      "Iteration 79800 Training loss 0.05141008272767067 Validation loss 0.05721653252840042 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.6977],\n",
      "        [0.2361]], device='mps:0')\n",
      "Iteration 79810 Training loss 0.05199022218585014 Validation loss 0.05776842311024666 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0031],\n",
      "        [0.0245]], device='mps:0')\n",
      "Iteration 79820 Training loss 0.05515482276678085 Validation loss 0.057192910462617874 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.1289],\n",
      "        [0.7889]], device='mps:0')\n",
      "Iteration 79830 Training loss 0.05717900022864342 Validation loss 0.05720609799027443 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9854],\n",
      "        [0.5250]], device='mps:0')\n",
      "Iteration 79840 Training loss 0.05094387009739876 Validation loss 0.05721257999539375 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9193],\n",
      "        [0.9716]], device='mps:0')\n",
      "Iteration 79850 Training loss 0.04891354590654373 Validation loss 0.05719459801912308 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.8968],\n",
      "        [0.5482]], device='mps:0')\n",
      "Iteration 79860 Training loss 0.04969114810228348 Validation loss 0.05719536170363426 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.3295],\n",
      "        [0.4367]], device='mps:0')\n",
      "Iteration 79870 Training loss 0.058031115680933 Validation loss 0.057227786630392075 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0222],\n",
      "        [0.4927]], device='mps:0')\n",
      "Iteration 79880 Training loss 0.06446223706007004 Validation loss 0.057390276342630386 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0061],\n",
      "        [0.0563]], device='mps:0')\n",
      "Iteration 79890 Training loss 0.0585843063890934 Validation loss 0.05719418451189995 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.1084],\n",
      "        [0.8493]], device='mps:0')\n",
      "Iteration 79900 Training loss 0.049773700535297394 Validation loss 0.05719296634197235 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.9641],\n",
      "        [0.9352]], device='mps:0')\n",
      "Iteration 79910 Training loss 0.060754984617233276 Validation loss 0.05734466016292572 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.2665],\n",
      "        [0.9793]], device='mps:0')\n",
      "Iteration 79920 Training loss 0.04692736640572548 Validation loss 0.05726556107401848 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0553],\n",
      "        [0.1353]], device='mps:0')\n",
      "Iteration 79930 Training loss 0.05151402950286865 Validation loss 0.057292282581329346 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.1604],\n",
      "        [0.0845]], device='mps:0')\n",
      "Iteration 79940 Training loss 0.048495158553123474 Validation loss 0.05719418078660965 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9886],\n",
      "        [0.1087]], device='mps:0')\n",
      "Iteration 79950 Training loss 0.04717542231082916 Validation loss 0.05723610520362854 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9156],\n",
      "        [0.0320]], device='mps:0')\n",
      "Iteration 79960 Training loss 0.05394555628299713 Validation loss 0.057192303240299225 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9385],\n",
      "        [0.0083]], device='mps:0')\n",
      "Iteration 79970 Training loss 0.05708034336566925 Validation loss 0.057372406125068665 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.1676],\n",
      "        [0.0206]], device='mps:0')\n",
      "Iteration 79980 Training loss 0.04537656530737877 Validation loss 0.05720032751560211 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.6049],\n",
      "        [0.0425]], device='mps:0')\n",
      "Iteration 79990 Training loss 0.05233538895845413 Validation loss 0.05723492056131363 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9854],\n",
      "        [0.2964]], device='mps:0')\n",
      "Iteration 80000 Training loss 0.05527976527810097 Validation loss 0.05719970166683197 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0255],\n",
      "        [0.0331]], device='mps:0')\n",
      "Iteration 80010 Training loss 0.05600323528051376 Validation loss 0.05721909552812576 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9742],\n",
      "        [0.6047]], device='mps:0')\n",
      "Iteration 80020 Training loss 0.05101441219449043 Validation loss 0.057188481092453 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.8059],\n",
      "        [0.0972]], device='mps:0')\n",
      "Iteration 80030 Training loss 0.05827523022890091 Validation loss 0.05736949294805527 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9740],\n",
      "        [0.7797]], device='mps:0')\n",
      "Iteration 80040 Training loss 0.06151912733912468 Validation loss 0.057188402861356735 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0403],\n",
      "        [0.1330]], device='mps:0')\n",
      "Iteration 80050 Training loss 0.05406506359577179 Validation loss 0.05718373879790306 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.9106],\n",
      "        [0.9395]], device='mps:0')\n",
      "Iteration 80060 Training loss 0.05503617227077484 Validation loss 0.05721442401409149 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.8413],\n",
      "        [0.8165]], device='mps:0')\n",
      "Iteration 80070 Training loss 0.05697127804160118 Validation loss 0.0572134368121624 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.8878],\n",
      "        [0.9644]], device='mps:0')\n",
      "Iteration 80080 Training loss 0.05802349001169205 Validation loss 0.057383161038160324 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9741],\n",
      "        [0.8929]], device='mps:0')\n",
      "Iteration 80090 Training loss 0.05141156166791916 Validation loss 0.057278040796518326 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0377],\n",
      "        [0.7376]], device='mps:0')\n",
      "Iteration 80100 Training loss 0.0553823746740818 Validation loss 0.057179033756256104 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.5801],\n",
      "        [0.7420]], device='mps:0')\n",
      "Iteration 80110 Training loss 0.06110665202140808 Validation loss 0.057174935936927795 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.2639],\n",
      "        [0.4219]], device='mps:0')\n",
      "Iteration 80120 Training loss 0.06572747230529785 Validation loss 0.05718204751610756 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.3382],\n",
      "        [0.6938]], device='mps:0')\n",
      "Iteration 80130 Training loss 0.054500482976436615 Validation loss 0.05717414990067482 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0992],\n",
      "        [0.9016]], device='mps:0')\n",
      "Iteration 80140 Training loss 0.042173851281404495 Validation loss 0.05717737600207329 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.5791],\n",
      "        [0.9967]], device='mps:0')\n",
      "Iteration 80150 Training loss 0.05223361402750015 Validation loss 0.05723120644688606 Accuracy 0.843375027179718\n",
      "Output tensor([[0.3859],\n",
      "        [0.1871]], device='mps:0')\n",
      "Iteration 80160 Training loss 0.05765001103281975 Validation loss 0.05718924105167389 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.8909],\n",
      "        [0.0397]], device='mps:0')\n",
      "Iteration 80170 Training loss 0.05216487869620323 Validation loss 0.05724480748176575 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.4096],\n",
      "        [0.0865]], device='mps:0')\n",
      "Iteration 80180 Training loss 0.050578758120536804 Validation loss 0.057215616106987 Accuracy 0.843500018119812\n",
      "Output tensor([[0.4244],\n",
      "        [0.2565]], device='mps:0')\n",
      "Iteration 80190 Training loss 0.05210879072546959 Validation loss 0.0572679378092289 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.8139],\n",
      "        [0.0232]], device='mps:0')\n",
      "Iteration 80200 Training loss 0.05166624113917351 Validation loss 0.05720418691635132 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.5438],\n",
      "        [0.2703]], device='mps:0')\n",
      "Iteration 80210 Training loss 0.04778938367962837 Validation loss 0.05726571008563042 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.4473],\n",
      "        [0.3662]], device='mps:0')\n",
      "Iteration 80220 Training loss 0.04446376860141754 Validation loss 0.05718660727143288 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0532],\n",
      "        [0.9381]], device='mps:0')\n",
      "Iteration 80230 Training loss 0.05350742116570473 Validation loss 0.05719206482172012 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9573],\n",
      "        [0.4423]], device='mps:0')\n",
      "Iteration 80240 Training loss 0.05290190503001213 Validation loss 0.05716662481427193 Accuracy 0.8455000519752502\n",
      "Output tensor([[0.9255],\n",
      "        [0.7300]], device='mps:0')\n",
      "Iteration 80250 Training loss 0.05689013749361038 Validation loss 0.05719422921538353 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0201],\n",
      "        [0.1501]], device='mps:0')\n",
      "Iteration 80260 Training loss 0.06342526525259018 Validation loss 0.05733096972107887 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9191],\n",
      "        [0.7654]], device='mps:0')\n",
      "Iteration 80270 Training loss 0.04863356053829193 Validation loss 0.057221878319978714 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.8521],\n",
      "        [0.9744]], device='mps:0')\n",
      "Iteration 80280 Training loss 0.05364978685975075 Validation loss 0.05718211457133293 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9907],\n",
      "        [0.5543]], device='mps:0')\n",
      "Iteration 80290 Training loss 0.05243873968720436 Validation loss 0.05721111223101616 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.5453],\n",
      "        [0.2507]], device='mps:0')\n",
      "Iteration 80300 Training loss 0.04611247405409813 Validation loss 0.057188134640455246 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0131],\n",
      "        [0.0480]], device='mps:0')\n",
      "Iteration 80310 Training loss 0.05653872340917587 Validation loss 0.05737341195344925 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.3718],\n",
      "        [0.4430]], device='mps:0')\n",
      "Iteration 80320 Training loss 0.0568808987736702 Validation loss 0.05716510862112045 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.1337],\n",
      "        [0.4559]], device='mps:0')\n",
      "Iteration 80330 Training loss 0.05309109389781952 Validation loss 0.057171229273080826 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0399],\n",
      "        [0.0535]], device='mps:0')\n",
      "Iteration 80340 Training loss 0.05179561674594879 Validation loss 0.0571611262857914 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9663],\n",
      "        [0.3705]], device='mps:0')\n",
      "Iteration 80350 Training loss 0.054639413952827454 Validation loss 0.05719403177499771 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.9664],\n",
      "        [0.6541]], device='mps:0')\n",
      "Iteration 80360 Training loss 0.057572364807128906 Validation loss 0.05751758813858032 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9561],\n",
      "        [0.0528]], device='mps:0')\n",
      "Iteration 80370 Training loss 0.06045537814497948 Validation loss 0.05716356262564659 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.9745],\n",
      "        [0.0102]], device='mps:0')\n",
      "Iteration 80380 Training loss 0.04765304923057556 Validation loss 0.05718576908111572 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.3509],\n",
      "        [0.4793]], device='mps:0')\n",
      "Iteration 80390 Training loss 0.04942046478390694 Validation loss 0.05723337456583977 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.3327],\n",
      "        [0.2639]], device='mps:0')\n",
      "Iteration 80400 Training loss 0.04709671437740326 Validation loss 0.05716073140501976 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.0908],\n",
      "        [0.1690]], device='mps:0')\n",
      "Iteration 80410 Training loss 0.05454535409808159 Validation loss 0.057266008108854294 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1019],\n",
      "        [0.4066]], device='mps:0')\n",
      "Iteration 80420 Training loss 0.048710115253925323 Validation loss 0.05717557668685913 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.3032],\n",
      "        [0.1333]], device='mps:0')\n",
      "Iteration 80430 Training loss 0.05520142614841461 Validation loss 0.05719256401062012 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9850],\n",
      "        [0.5368]], device='mps:0')\n",
      "Iteration 80440 Training loss 0.04393450543284416 Validation loss 0.05719536915421486 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.7088],\n",
      "        [0.2703]], device='mps:0')\n",
      "Iteration 80450 Training loss 0.057339753955602646 Validation loss 0.05726160481572151 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.2936],\n",
      "        [0.6200]], device='mps:0')\n",
      "Iteration 80460 Training loss 0.05892670527100563 Validation loss 0.057237863540649414 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9075],\n",
      "        [0.4417]], device='mps:0')\n",
      "Iteration 80470 Training loss 0.058343056589365005 Validation loss 0.057268816977739334 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.6180],\n",
      "        [0.5488]], device='mps:0')\n",
      "Iteration 80480 Training loss 0.049858663231134415 Validation loss 0.057141177356243134 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.6792],\n",
      "        [0.0022]], device='mps:0')\n",
      "Iteration 80490 Training loss 0.05693165212869644 Validation loss 0.05715160816907883 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.0575],\n",
      "        [0.4683]], device='mps:0')\n",
      "Iteration 80500 Training loss 0.047358188778162 Validation loss 0.057153716683387756 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9154],\n",
      "        [0.0816]], device='mps:0')\n",
      "Iteration 80510 Training loss 0.05144850164651871 Validation loss 0.05715097859501839 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.0065],\n",
      "        [0.1855]], device='mps:0')\n",
      "Iteration 80520 Training loss 0.05806318297982216 Validation loss 0.057188618928194046 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.5176],\n",
      "        [0.9926]], device='mps:0')\n",
      "Iteration 80530 Training loss 0.05552416294813156 Validation loss 0.05716518685221672 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.8238],\n",
      "        [0.1019]], device='mps:0')\n",
      "Iteration 80540 Training loss 0.04812421277165413 Validation loss 0.057198960334062576 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.1371],\n",
      "        [0.2318]], device='mps:0')\n",
      "Iteration 80550 Training loss 0.06087559461593628 Validation loss 0.057517871260643005 Accuracy 0.843000054359436\n",
      "Output tensor([[0.8194],\n",
      "        [0.0890]], device='mps:0')\n",
      "Iteration 80560 Training loss 0.05540858209133148 Validation loss 0.05724795535206795 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0362],\n",
      "        [0.7561]], device='mps:0')\n",
      "Iteration 80570 Training loss 0.04458126053214073 Validation loss 0.05715743452310562 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.8770],\n",
      "        [0.9667]], device='mps:0')\n",
      "Iteration 80580 Training loss 0.05244354158639908 Validation loss 0.05713148042559624 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.9530],\n",
      "        [0.8815]], device='mps:0')\n",
      "Iteration 80590 Training loss 0.06202433630824089 Validation loss 0.057129792869091034 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.1372],\n",
      "        [0.4147]], device='mps:0')\n",
      "Iteration 80600 Training loss 0.06475701928138733 Validation loss 0.05732600763440132 Accuracy 0.8427500128746033\n",
      "Output tensor([[0.8157],\n",
      "        [0.7447]], device='mps:0')\n",
      "Iteration 80610 Training loss 0.04772554337978363 Validation loss 0.05719975009560585 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0557],\n",
      "        [0.9842]], device='mps:0')\n",
      "Iteration 80620 Training loss 0.050694532692432404 Validation loss 0.057190172374248505 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1399],\n",
      "        [0.9636]], device='mps:0')\n",
      "Iteration 80630 Training loss 0.058899249881505966 Validation loss 0.057127077132463455 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.3624],\n",
      "        [0.5011]], device='mps:0')\n",
      "Iteration 80640 Training loss 0.052514176815748215 Validation loss 0.057134900242090225 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9181],\n",
      "        [0.5443]], device='mps:0')\n",
      "Iteration 80650 Training loss 0.052339982241392136 Validation loss 0.05717207491397858 Accuracy 0.843250036239624\n",
      "Output tensor([[0.8296],\n",
      "        [0.2634]], device='mps:0')\n",
      "Iteration 80660 Training loss 0.050984375178813934 Validation loss 0.05717591196298599 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.7015],\n",
      "        [0.6991]], device='mps:0')\n",
      "Iteration 80670 Training loss 0.05648751184344292 Validation loss 0.057318948209285736 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9053],\n",
      "        [0.8746]], device='mps:0')\n",
      "Iteration 80680 Training loss 0.049983102828264236 Validation loss 0.05718202888965607 Accuracy 0.843250036239624\n",
      "Output tensor([[0.0374],\n",
      "        [0.9072]], device='mps:0')\n",
      "Iteration 80690 Training loss 0.058264680206775665 Validation loss 0.057230137288570404 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.1373],\n",
      "        [0.0516]], device='mps:0')\n",
      "Iteration 80700 Training loss 0.04079611226916313 Validation loss 0.05738573148846626 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.6959],\n",
      "        [0.9228]], device='mps:0')\n",
      "Iteration 80710 Training loss 0.058018382638692856 Validation loss 0.05722460523247719 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9957],\n",
      "        [0.0568]], device='mps:0')\n",
      "Iteration 80720 Training loss 0.05717635154724121 Validation loss 0.05712701007723808 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9190],\n",
      "        [0.1153]], device='mps:0')\n",
      "Iteration 80730 Training loss 0.053786762058734894 Validation loss 0.05714369937777519 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9981],\n",
      "        [0.9334]], device='mps:0')\n",
      "Iteration 80740 Training loss 0.05984124541282654 Validation loss 0.057244256138801575 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.1296],\n",
      "        [0.8794]], device='mps:0')\n",
      "Iteration 80750 Training loss 0.058690574020147324 Validation loss 0.05711827427148819 Accuracy 0.8456250429153442\n",
      "Output tensor([[0.5303],\n",
      "        [0.9963]], device='mps:0')\n",
      "Iteration 80760 Training loss 0.04983087256550789 Validation loss 0.05711941421031952 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.4862],\n",
      "        [0.7005]], device='mps:0')\n",
      "Iteration 80770 Training loss 0.058030541986227036 Validation loss 0.05715680867433548 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.0213],\n",
      "        [0.2435]], device='mps:0')\n",
      "Iteration 80780 Training loss 0.04746423661708832 Validation loss 0.05712900683283806 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.0989],\n",
      "        [0.0981]], device='mps:0')\n",
      "Iteration 80790 Training loss 0.04981546476483345 Validation loss 0.05714445933699608 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1965],\n",
      "        [0.9479]], device='mps:0')\n",
      "Iteration 80800 Training loss 0.05520083010196686 Validation loss 0.0571197010576725 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.0455],\n",
      "        [0.0712]], device='mps:0')\n",
      "Iteration 80810 Training loss 0.04494291543960571 Validation loss 0.0571674108505249 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1446],\n",
      "        [0.2799]], device='mps:0')\n",
      "Iteration 80820 Training loss 0.05174797400832176 Validation loss 0.05714224651455879 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0790],\n",
      "        [0.0702]], device='mps:0')\n",
      "Iteration 80830 Training loss 0.05133127048611641 Validation loss 0.05712958797812462 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.8703],\n",
      "        [0.5721]], device='mps:0')\n",
      "Iteration 80840 Training loss 0.06888026744127274 Validation loss 0.057143330574035645 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.1598],\n",
      "        [0.1137]], device='mps:0')\n",
      "Iteration 80850 Training loss 0.05492451414465904 Validation loss 0.0571313351392746 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.7996],\n",
      "        [0.0042]], device='mps:0')\n",
      "Iteration 80860 Training loss 0.05274326354265213 Validation loss 0.057284340262413025 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9266],\n",
      "        [0.0029]], device='mps:0')\n",
      "Iteration 80870 Training loss 0.04227987676858902 Validation loss 0.05716147646307945 Accuracy 0.843500018119812\n",
      "Output tensor([[0.0178],\n",
      "        [0.3910]], device='mps:0')\n",
      "Iteration 80880 Training loss 0.05249514430761337 Validation loss 0.05712936073541641 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.6928],\n",
      "        [0.5074]], device='mps:0')\n",
      "Iteration 80890 Training loss 0.053357839584350586 Validation loss 0.05716510862112045 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0742],\n",
      "        [0.2245]], device='mps:0')\n",
      "Iteration 80900 Training loss 0.057319797575473785 Validation loss 0.057271622121334076 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0997],\n",
      "        [0.9518]], device='mps:0')\n",
      "Iteration 80910 Training loss 0.053768813610076904 Validation loss 0.05712044611573219 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0336],\n",
      "        [0.0097]], device='mps:0')\n",
      "Iteration 80920 Training loss 0.0593968965113163 Validation loss 0.05723975598812103 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.2402],\n",
      "        [0.9927]], device='mps:0')\n",
      "Iteration 80930 Training loss 0.05492233484983444 Validation loss 0.057151295244693756 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.8984],\n",
      "        [0.4173]], device='mps:0')\n",
      "Iteration 80940 Training loss 0.05232460796833038 Validation loss 0.057128921151161194 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.5562],\n",
      "        [0.0813]], device='mps:0')\n",
      "Iteration 80950 Training loss 0.05373822897672653 Validation loss 0.05725966766476631 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.3132],\n",
      "        [0.0698]], device='mps:0')\n",
      "Iteration 80960 Training loss 0.055893685668706894 Validation loss 0.05712873861193657 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0340],\n",
      "        [0.2440]], device='mps:0')\n",
      "Iteration 80970 Training loss 0.07034282386302948 Validation loss 0.05714472383260727 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.2326],\n",
      "        [0.7276]], device='mps:0')\n",
      "Iteration 80980 Training loss 0.05018627643585205 Validation loss 0.05716123431921005 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0506],\n",
      "        [0.1158]], device='mps:0')\n",
      "Iteration 80990 Training loss 0.0523129440844059 Validation loss 0.05722528323531151 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.1752],\n",
      "        [0.0538]], device='mps:0')\n",
      "Iteration 81000 Training loss 0.05569559335708618 Validation loss 0.05720905587077141 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9931],\n",
      "        [0.9598]], device='mps:0')\n",
      "Iteration 81010 Training loss 0.05181504786014557 Validation loss 0.057213716208934784 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0700],\n",
      "        [0.9011]], device='mps:0')\n",
      "Iteration 81020 Training loss 0.04869882017374039 Validation loss 0.05714176595211029 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.3330],\n",
      "        [0.7781]], device='mps:0')\n",
      "Iteration 81030 Training loss 0.0587373822927475 Validation loss 0.05712175741791725 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.3224],\n",
      "        [0.9203]], device='mps:0')\n",
      "Iteration 81040 Training loss 0.05747789144515991 Validation loss 0.05712801590561867 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.0737],\n",
      "        [0.8229]], device='mps:0')\n",
      "Iteration 81050 Training loss 0.05428115651011467 Validation loss 0.05713622644543648 Accuracy 0.8455000519752502\n",
      "Output tensor([[0.0837],\n",
      "        [0.0473]], device='mps:0')\n",
      "Iteration 81060 Training loss 0.05496561899781227 Validation loss 0.05714474618434906 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.0506],\n",
      "        [0.8140]], device='mps:0')\n",
      "Iteration 81070 Training loss 0.05185417830944061 Validation loss 0.05716020613908768 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.6252],\n",
      "        [0.1811]], device='mps:0')\n",
      "Iteration 81080 Training loss 0.04675285890698433 Validation loss 0.05713465437293053 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.0393],\n",
      "        [0.6834]], device='mps:0')\n",
      "Iteration 81090 Training loss 0.058367449790239334 Validation loss 0.05713309720158577 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.9251],\n",
      "        [0.1634]], device='mps:0')\n",
      "Iteration 81100 Training loss 0.05641492083668709 Validation loss 0.057139359414577484 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.1042],\n",
      "        [0.0508]], device='mps:0')\n",
      "Iteration 81110 Training loss 0.057368457317352295 Validation loss 0.057194918394088745 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.1871],\n",
      "        [0.0969]], device='mps:0')\n",
      "Iteration 81120 Training loss 0.05058811977505684 Validation loss 0.057607751339673996 Accuracy 0.8420000672340393\n",
      "Output tensor([[0.1577],\n",
      "        [0.9843]], device='mps:0')\n",
      "Iteration 81130 Training loss 0.056087952107191086 Validation loss 0.05712136998772621 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.4679],\n",
      "        [0.9517]], device='mps:0')\n",
      "Iteration 81140 Training loss 0.051404789090156555 Validation loss 0.057161856442689896 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.7057],\n",
      "        [0.3643]], device='mps:0')\n",
      "Iteration 81150 Training loss 0.060071296989917755 Validation loss 0.0571906678378582 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9009],\n",
      "        [0.2202]], device='mps:0')\n",
      "Iteration 81160 Training loss 0.04565948247909546 Validation loss 0.057302702218294144 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.1161],\n",
      "        [0.9760]], device='mps:0')\n",
      "Iteration 81170 Training loss 0.056753627955913544 Validation loss 0.05712343007326126 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.9887],\n",
      "        [0.6527]], device='mps:0')\n",
      "Iteration 81180 Training loss 0.0428897961974144 Validation loss 0.05744713172316551 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.6838],\n",
      "        [0.2615]], device='mps:0')\n",
      "Iteration 81190 Training loss 0.05790712311863899 Validation loss 0.057125356048345566 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.0816],\n",
      "        [0.9325]], device='mps:0')\n",
      "Iteration 81200 Training loss 0.04908338561654091 Validation loss 0.05712374672293663 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.6401],\n",
      "        [0.0781]], device='mps:0')\n",
      "Iteration 81210 Training loss 0.055387429893016815 Validation loss 0.057129114866256714 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.4810],\n",
      "        [0.0665]], device='mps:0')\n",
      "Iteration 81220 Training loss 0.053145602345466614 Validation loss 0.057176340371370316 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.7287],\n",
      "        [0.3535]], device='mps:0')\n",
      "Iteration 81230 Training loss 0.05728534609079361 Validation loss 0.05717705190181732 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.6785],\n",
      "        [0.7528]], device='mps:0')\n",
      "Iteration 81240 Training loss 0.055693618953228 Validation loss 0.057226765900850296 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1475],\n",
      "        [0.3322]], device='mps:0')\n",
      "Iteration 81250 Training loss 0.055377550423145294 Validation loss 0.05711904168128967 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.9514],\n",
      "        [0.6778]], device='mps:0')\n",
      "Iteration 81260 Training loss 0.057811226695775986 Validation loss 0.057202067226171494 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.0615],\n",
      "        [0.9844]], device='mps:0')\n",
      "Iteration 81270 Training loss 0.05335858091711998 Validation loss 0.05717175081372261 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0610],\n",
      "        [0.0427]], device='mps:0')\n",
      "Iteration 81280 Training loss 0.04642660543322563 Validation loss 0.0572953000664711 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.6151],\n",
      "        [0.0336]], device='mps:0')\n",
      "Iteration 81290 Training loss 0.05698547884821892 Validation loss 0.05711947754025459 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.3731],\n",
      "        [0.8532]], device='mps:0')\n",
      "Iteration 81300 Training loss 0.05371086299419403 Validation loss 0.05714428052306175 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.8541],\n",
      "        [0.9534]], device='mps:0')\n",
      "Iteration 81310 Training loss 0.05628122389316559 Validation loss 0.057177864015102386 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.3974],\n",
      "        [0.9748]], device='mps:0')\n",
      "Iteration 81320 Training loss 0.05879584327340126 Validation loss 0.05710937827825546 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1083],\n",
      "        [0.3904]], device='mps:0')\n",
      "Iteration 81330 Training loss 0.059211041778326035 Validation loss 0.05714023485779762 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.7022],\n",
      "        [0.4279]], device='mps:0')\n",
      "Iteration 81340 Training loss 0.05567198619246483 Validation loss 0.05712055042386055 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.3821],\n",
      "        [0.0991]], device='mps:0')\n",
      "Iteration 81350 Training loss 0.0533694252371788 Validation loss 0.05713214352726936 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9068],\n",
      "        [0.9239]], device='mps:0')\n",
      "Iteration 81360 Training loss 0.04967879131436348 Validation loss 0.05719178915023804 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.3800],\n",
      "        [0.6614]], device='mps:0')\n",
      "Iteration 81370 Training loss 0.057894621044397354 Validation loss 0.05714956298470497 Accuracy 0.84312504529953\n",
      "Output tensor([[0.9351],\n",
      "        [0.8833]], device='mps:0')\n",
      "Iteration 81380 Training loss 0.053279079496860504 Validation loss 0.057162895798683167 Accuracy 0.843250036239624\n",
      "Output tensor([[0.9029],\n",
      "        [0.0991]], device='mps:0')\n",
      "Iteration 81390 Training loss 0.046645186841487885 Validation loss 0.057108324021101 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.6773],\n",
      "        [0.9353]], device='mps:0')\n",
      "Iteration 81400 Training loss 0.04712911322712898 Validation loss 0.05729103460907936 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.4869],\n",
      "        [0.7030]], device='mps:0')\n",
      "Iteration 81410 Training loss 0.05840316787362099 Validation loss 0.0571918711066246 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.9313],\n",
      "        [0.0705]], device='mps:0')\n",
      "Iteration 81420 Training loss 0.0499933697283268 Validation loss 0.057101357728242874 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.0346],\n",
      "        [0.0308]], device='mps:0')\n",
      "Iteration 81430 Training loss 0.06233508512377739 Validation loss 0.05710624158382416 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.2890],\n",
      "        [0.9658]], device='mps:0')\n",
      "Iteration 81440 Training loss 0.05240839719772339 Validation loss 0.0571010522544384 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.8319],\n",
      "        [0.0100]], device='mps:0')\n",
      "Iteration 81450 Training loss 0.06305902451276779 Validation loss 0.05716588720679283 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0275],\n",
      "        [0.0173]], device='mps:0')\n",
      "Iteration 81460 Training loss 0.05372985824942589 Validation loss 0.057137973606586456 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.2802],\n",
      "        [0.1325]], device='mps:0')\n",
      "Iteration 81470 Training loss 0.051254890859127045 Validation loss 0.057110413908958435 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9342],\n",
      "        [0.9073]], device='mps:0')\n",
      "Iteration 81480 Training loss 0.05185201019048691 Validation loss 0.057112712413072586 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.1789],\n",
      "        [0.1410]], device='mps:0')\n",
      "Iteration 81490 Training loss 0.05014830455183983 Validation loss 0.05709638446569443 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0610],\n",
      "        [0.7466]], device='mps:0')\n",
      "Iteration 81500 Training loss 0.04611453041434288 Validation loss 0.05716673657298088 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.2330],\n",
      "        [0.0351]], device='mps:0')\n",
      "Iteration 81510 Training loss 0.05327935144305229 Validation loss 0.05709904804825783 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.4488],\n",
      "        [0.9401]], device='mps:0')\n",
      "Iteration 81520 Training loss 0.05793728679418564 Validation loss 0.057105425745248795 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.7740],\n",
      "        [0.3465]], device='mps:0')\n",
      "Iteration 81530 Training loss 0.05473334714770317 Validation loss 0.057119112461805344 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.4706],\n",
      "        [0.3327]], device='mps:0')\n",
      "Iteration 81540 Training loss 0.04662428796291351 Validation loss 0.05711479112505913 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.4339],\n",
      "        [0.9219]], device='mps:0')\n",
      "Iteration 81550 Training loss 0.06075162813067436 Validation loss 0.05721377208828926 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.8943],\n",
      "        [0.0827]], device='mps:0')\n",
      "Iteration 81560 Training loss 0.044056542217731476 Validation loss 0.05711699277162552 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9739],\n",
      "        [0.6296]], device='mps:0')\n",
      "Iteration 81570 Training loss 0.06718842685222626 Validation loss 0.05711399018764496 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9133],\n",
      "        [0.8411]], device='mps:0')\n",
      "Iteration 81580 Training loss 0.04884866625070572 Validation loss 0.05722978338599205 Accuracy 0.843375027179718\n",
      "Output tensor([[0.7079],\n",
      "        [0.4953]], device='mps:0')\n",
      "Iteration 81590 Training loss 0.058075860142707825 Validation loss 0.057110052555799484 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.7438],\n",
      "        [0.6044]], device='mps:0')\n",
      "Iteration 81600 Training loss 0.04729693382978439 Validation loss 0.05710515379905701 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.8312],\n",
      "        [0.9818]], device='mps:0')\n",
      "Iteration 81610 Training loss 0.054673537611961365 Validation loss 0.05711200833320618 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.6635],\n",
      "        [0.8905]], device='mps:0')\n",
      "Iteration 81620 Training loss 0.04897824302315712 Validation loss 0.05720481276512146 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0940],\n",
      "        [0.8909]], device='mps:0')\n",
      "Iteration 81630 Training loss 0.05631707236170769 Validation loss 0.05719618499279022 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0097],\n",
      "        [0.9943]], device='mps:0')\n",
      "Iteration 81640 Training loss 0.061067625880241394 Validation loss 0.05711030215024948 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.1140],\n",
      "        [0.5090]], device='mps:0')\n",
      "Iteration 81650 Training loss 0.05493711307644844 Validation loss 0.05710994079709053 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.3006],\n",
      "        [0.9666]], device='mps:0')\n",
      "Iteration 81660 Training loss 0.052472036331892014 Validation loss 0.05723031237721443 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.1130],\n",
      "        [0.1456]], device='mps:0')\n",
      "Iteration 81670 Training loss 0.060292914509773254 Validation loss 0.0571068674325943 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.6026],\n",
      "        [0.2399]], device='mps:0')\n",
      "Iteration 81680 Training loss 0.0542222298681736 Validation loss 0.05716751515865326 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9010],\n",
      "        [0.8612]], device='mps:0')\n",
      "Iteration 81690 Training loss 0.05771184712648392 Validation loss 0.057196035981178284 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.8867],\n",
      "        [0.7567]], device='mps:0')\n",
      "Iteration 81700 Training loss 0.059442996978759766 Validation loss 0.05711202695965767 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9431],\n",
      "        [0.2351]], device='mps:0')\n",
      "Iteration 81710 Training loss 0.0556035116314888 Validation loss 0.05725996568799019 Accuracy 0.8436250686645508\n",
      "Output tensor([[0.0420],\n",
      "        [0.6479]], device='mps:0')\n",
      "Iteration 81720 Training loss 0.051085393875837326 Validation loss 0.05713282525539398 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0732],\n",
      "        [0.5750]], device='mps:0')\n",
      "Iteration 81730 Training loss 0.046523503959178925 Validation loss 0.05714220926165581 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.6693],\n",
      "        [0.7510]], device='mps:0')\n",
      "Iteration 81740 Training loss 0.04788215458393097 Validation loss 0.05714482441544533 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.7902],\n",
      "        [0.6914]], device='mps:0')\n",
      "Iteration 81750 Training loss 0.05353153124451637 Validation loss 0.05722346156835556 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0478],\n",
      "        [0.9950]], device='mps:0')\n",
      "Iteration 81760 Training loss 0.048690494149923325 Validation loss 0.05715540051460266 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.3426],\n",
      "        [0.1143]], device='mps:0')\n",
      "Iteration 81770 Training loss 0.050979211926460266 Validation loss 0.057111985981464386 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9766],\n",
      "        [0.8292]], device='mps:0')\n",
      "Iteration 81780 Training loss 0.057680871337652206 Validation loss 0.05712534114718437 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.8572],\n",
      "        [0.1225]], device='mps:0')\n",
      "Iteration 81790 Training loss 0.05919492617249489 Validation loss 0.05726652964949608 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.8610],\n",
      "        [0.5904]], device='mps:0')\n",
      "Iteration 81800 Training loss 0.04913255199790001 Validation loss 0.05718758702278137 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.6898],\n",
      "        [0.7931]], device='mps:0')\n",
      "Iteration 81810 Training loss 0.05618783459067345 Validation loss 0.05719846487045288 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.2077],\n",
      "        [0.0291]], device='mps:0')\n",
      "Iteration 81820 Training loss 0.05332895740866661 Validation loss 0.057569194585084915 Accuracy 0.8423750400543213\n",
      "Output tensor([[0.0050],\n",
      "        [0.1931]], device='mps:0')\n",
      "Iteration 81830 Training loss 0.04766664281487465 Validation loss 0.057150665670633316 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.1409],\n",
      "        [0.7599]], device='mps:0')\n",
      "Iteration 81840 Training loss 0.04713882505893707 Validation loss 0.05711168795824051 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9785],\n",
      "        [0.0141]], device='mps:0')\n",
      "Iteration 81850 Training loss 0.055518291890621185 Validation loss 0.05720042064785957 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.9138],\n",
      "        [0.9499]], device='mps:0')\n",
      "Iteration 81860 Training loss 0.0650518387556076 Validation loss 0.05711398273706436 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.9947],\n",
      "        [0.3496]], device='mps:0')\n",
      "Iteration 81870 Training loss 0.06177011504769325 Validation loss 0.057218246161937714 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.1348],\n",
      "        [0.0660]], device='mps:0')\n",
      "Iteration 81880 Training loss 0.0570666678249836 Validation loss 0.05709916725754738 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0334],\n",
      "        [0.4597]], device='mps:0')\n",
      "Iteration 81890 Training loss 0.0547192320227623 Validation loss 0.05710311979055405 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.2420],\n",
      "        [0.5302]], device='mps:0')\n",
      "Iteration 81900 Training loss 0.0565953329205513 Validation loss 0.05710224062204361 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.7461],\n",
      "        [0.1073]], device='mps:0')\n",
      "Iteration 81910 Training loss 0.06145556643605232 Validation loss 0.05714685097336769 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.1930],\n",
      "        [0.5916]], device='mps:0')\n",
      "Iteration 81920 Training loss 0.05298730731010437 Validation loss 0.05709851160645485 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.8594],\n",
      "        [0.9833]], device='mps:0')\n",
      "Iteration 81930 Training loss 0.054169148206710815 Validation loss 0.05710017681121826 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.8750],\n",
      "        [0.4715]], device='mps:0')\n",
      "Iteration 81940 Training loss 0.05980142578482628 Validation loss 0.057367075234651566 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0769],\n",
      "        [0.3223]], device='mps:0')\n",
      "Iteration 81950 Training loss 0.05706540867686272 Validation loss 0.057229183614254 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0831],\n",
      "        [0.3730]], device='mps:0')\n",
      "Iteration 81960 Training loss 0.060647185891866684 Validation loss 0.05711520090699196 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.2475],\n",
      "        [0.2427]], device='mps:0')\n",
      "Iteration 81970 Training loss 0.050273340195417404 Validation loss 0.057222314178943634 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9152],\n",
      "        [0.0482]], device='mps:0')\n",
      "Iteration 81980 Training loss 0.06458871811628342 Validation loss 0.05715404823422432 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1190],\n",
      "        [0.5339]], device='mps:0')\n",
      "Iteration 81990 Training loss 0.057092007249593735 Validation loss 0.05715068057179451 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.8529],\n",
      "        [0.9809]], device='mps:0')\n",
      "Iteration 82000 Training loss 0.05115407705307007 Validation loss 0.05710824206471443 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.0127],\n",
      "        [0.0103]], device='mps:0')\n",
      "Iteration 82010 Training loss 0.05365535989403725 Validation loss 0.05725403130054474 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.2170],\n",
      "        [0.1759]], device='mps:0')\n",
      "Iteration 82020 Training loss 0.044228695333004 Validation loss 0.05713193118572235 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.2638],\n",
      "        [0.0714]], device='mps:0')\n",
      "Iteration 82030 Training loss 0.058719128370285034 Validation loss 0.057097453624010086 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.0276],\n",
      "        [0.1397]], device='mps:0')\n",
      "Iteration 82040 Training loss 0.0500175841152668 Validation loss 0.05711173638701439 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0940],\n",
      "        [0.2988]], device='mps:0')\n",
      "Iteration 82050 Training loss 0.05492822453379631 Validation loss 0.05720212683081627 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.0835],\n",
      "        [0.9639]], device='mps:0')\n",
      "Iteration 82060 Training loss 0.04934508353471756 Validation loss 0.057097312062978745 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9846],\n",
      "        [0.1750]], device='mps:0')\n",
      "Iteration 82070 Training loss 0.056920651346445084 Validation loss 0.05711151659488678 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.9612],\n",
      "        [0.9377]], device='mps:0')\n",
      "Iteration 82080 Training loss 0.05861986428499222 Validation loss 0.057101063430309296 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.7088],\n",
      "        [0.7737]], device='mps:0')\n",
      "Iteration 82090 Training loss 0.05592726916074753 Validation loss 0.05710161104798317 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.9018],\n",
      "        [0.6983]], device='mps:0')\n",
      "Iteration 82100 Training loss 0.05675023794174194 Validation loss 0.057105351239442825 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.2512],\n",
      "        [0.2157]], device='mps:0')\n",
      "Iteration 82110 Training loss 0.056229934096336365 Validation loss 0.05713099613785744 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.2836],\n",
      "        [0.3785]], device='mps:0')\n",
      "Iteration 82120 Training loss 0.05844089016318321 Validation loss 0.057198408991098404 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.0016],\n",
      "        [0.0986]], device='mps:0')\n",
      "Iteration 82130 Training loss 0.05717892199754715 Validation loss 0.057097069919109344 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.7807],\n",
      "        [0.7038]], device='mps:0')\n",
      "Iteration 82140 Training loss 0.050223030149936676 Validation loss 0.05710269510746002 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.8656],\n",
      "        [0.7181]], device='mps:0')\n",
      "Iteration 82150 Training loss 0.052571456879377365 Validation loss 0.05737990140914917 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9783],\n",
      "        [0.9246]], device='mps:0')\n",
      "Iteration 82160 Training loss 0.05299321562051773 Validation loss 0.05737221613526344 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.6640],\n",
      "        [0.1261]], device='mps:0')\n",
      "Iteration 82170 Training loss 0.04757513850927353 Validation loss 0.05726287141442299 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.9736],\n",
      "        [0.9799]], device='mps:0')\n",
      "Iteration 82180 Training loss 0.06306403130292892 Validation loss 0.05713403597474098 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.9681],\n",
      "        [0.6153]], device='mps:0')\n",
      "Iteration 82190 Training loss 0.050422295928001404 Validation loss 0.05710446089506149 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.1093],\n",
      "        [0.8704]], device='mps:0')\n",
      "Iteration 82200 Training loss 0.05653621628880501 Validation loss 0.05711160600185394 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.9563],\n",
      "        [0.9497]], device='mps:0')\n",
      "Iteration 82210 Training loss 0.050910551100969315 Validation loss 0.05731557309627533 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9708],\n",
      "        [0.8928]], device='mps:0')\n",
      "Iteration 82220 Training loss 0.047626156359910965 Validation loss 0.057114388793706894 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.9588],\n",
      "        [0.0071]], device='mps:0')\n",
      "Iteration 82230 Training loss 0.057396285235881805 Validation loss 0.057127200067043304 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.9871],\n",
      "        [0.0526]], device='mps:0')\n",
      "Iteration 82240 Training loss 0.051259879022836685 Validation loss 0.05725272372364998 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.9260],\n",
      "        [0.8870]], device='mps:0')\n",
      "Iteration 82250 Training loss 0.05172976478934288 Validation loss 0.05725117027759552 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.2605],\n",
      "        [0.3966]], device='mps:0')\n",
      "Iteration 82260 Training loss 0.05305620655417442 Validation loss 0.057440370321273804 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.4077],\n",
      "        [0.7876]], device='mps:0')\n",
      "Iteration 82270 Training loss 0.0586567260324955 Validation loss 0.05718175694346428 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9927],\n",
      "        [0.3488]], device='mps:0')\n",
      "Iteration 82280 Training loss 0.048999715596437454 Validation loss 0.05714701861143112 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1668],\n",
      "        [0.7955]], device='mps:0')\n",
      "Iteration 82290 Training loss 0.055439919233322144 Validation loss 0.05710708722472191 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1303],\n",
      "        [0.7311]], device='mps:0')\n",
      "Iteration 82300 Training loss 0.05376534163951874 Validation loss 0.05722460895776749 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.1822],\n",
      "        [0.1776]], device='mps:0')\n",
      "Iteration 82310 Training loss 0.05021355673670769 Validation loss 0.057096224278211594 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.8699],\n",
      "        [0.0265]], device='mps:0')\n",
      "Iteration 82320 Training loss 0.05605410039424896 Validation loss 0.05710655450820923 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.2489],\n",
      "        [0.5530]], device='mps:0')\n",
      "Iteration 82330 Training loss 0.046577222645282745 Validation loss 0.057102467864751816 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.9112],\n",
      "        [0.2542]], device='mps:0')\n",
      "Iteration 82340 Training loss 0.05419743433594704 Validation loss 0.057096872478723526 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.4281],\n",
      "        [0.2345]], device='mps:0')\n",
      "Iteration 82350 Training loss 0.0556802824139595 Validation loss 0.05714062601327896 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.2275],\n",
      "        [0.0069]], device='mps:0')\n",
      "Iteration 82360 Training loss 0.05403198301792145 Validation loss 0.057099513709545135 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.2834],\n",
      "        [0.3105]], device='mps:0')\n",
      "Iteration 82370 Training loss 0.0542626827955246 Validation loss 0.05731761455535889 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.0079],\n",
      "        [0.0326]], device='mps:0')\n",
      "Iteration 82380 Training loss 0.04627244919538498 Validation loss 0.057190265506505966 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.6064],\n",
      "        [0.3685]], device='mps:0')\n",
      "Iteration 82390 Training loss 0.05766170471906662 Validation loss 0.05710672214627266 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.1746],\n",
      "        [0.3469]], device='mps:0')\n",
      "Iteration 82400 Training loss 0.05170902982354164 Validation loss 0.0571257509291172 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0283],\n",
      "        [0.0033]], device='mps:0')\n",
      "Iteration 82410 Training loss 0.053079117089509964 Validation loss 0.05709093064069748 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.5593],\n",
      "        [0.9735]], device='mps:0')\n",
      "Iteration 82420 Training loss 0.04759841784834862 Validation loss 0.057095982134342194 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0384],\n",
      "        [0.8075]], device='mps:0')\n",
      "Iteration 82430 Training loss 0.049670275300741196 Validation loss 0.057118069380521774 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.3599],\n",
      "        [0.0198]], device='mps:0')\n",
      "Iteration 82440 Training loss 0.06080267205834389 Validation loss 0.0571722611784935 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.5101],\n",
      "        [0.0077]], device='mps:0')\n",
      "Iteration 82450 Training loss 0.05219103768467903 Validation loss 0.05708319693803787 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.3526],\n",
      "        [0.0181]], device='mps:0')\n",
      "Iteration 82460 Training loss 0.05567270144820213 Validation loss 0.0570908822119236 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9251],\n",
      "        [0.9459]], device='mps:0')\n",
      "Iteration 82470 Training loss 0.04904300719499588 Validation loss 0.05708957836031914 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.0885],\n",
      "        [0.9119]], device='mps:0')\n",
      "Iteration 82480 Training loss 0.04499097540974617 Validation loss 0.057129375636577606 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.4902],\n",
      "        [0.3225]], device='mps:0')\n",
      "Iteration 82490 Training loss 0.05849657952785492 Validation loss 0.057146478444337845 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.8620],\n",
      "        [0.4599]], device='mps:0')\n",
      "Iteration 82500 Training loss 0.055551499128341675 Validation loss 0.057135261595249176 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.6134],\n",
      "        [0.1731]], device='mps:0')\n",
      "Iteration 82510 Training loss 0.0545836016535759 Validation loss 0.05708931386470795 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0813],\n",
      "        [0.6984]], device='mps:0')\n",
      "Iteration 82520 Training loss 0.04732766002416611 Validation loss 0.057124391198158264 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.1203],\n",
      "        [0.1398]], device='mps:0')\n",
      "Iteration 82530 Training loss 0.04657178744673729 Validation loss 0.057405732572078705 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.7967],\n",
      "        [0.9083]], device='mps:0')\n",
      "Iteration 82540 Training loss 0.04579024389386177 Validation loss 0.057099491357803345 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.7001],\n",
      "        [0.7093]], device='mps:0')\n",
      "Iteration 82550 Training loss 0.050167445093393326 Validation loss 0.057229526340961456 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.5009],\n",
      "        [0.4544]], device='mps:0')\n",
      "Iteration 82560 Training loss 0.05236406996846199 Validation loss 0.057084400206804276 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.3243],\n",
      "        [0.0911]], device='mps:0')\n",
      "Iteration 82570 Training loss 0.05672833323478699 Validation loss 0.05709775909781456 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.3574],\n",
      "        [0.2965]], device='mps:0')\n",
      "Iteration 82580 Training loss 0.04396902397274971 Validation loss 0.057063065469264984 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.4080],\n",
      "        [0.0895]], device='mps:0')\n",
      "Iteration 82590 Training loss 0.05615217238664627 Validation loss 0.05706051364541054 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.1056],\n",
      "        [0.6759]], device='mps:0')\n",
      "Iteration 82600 Training loss 0.05015794187784195 Validation loss 0.05706128478050232 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.8492],\n",
      "        [0.0266]], device='mps:0')\n",
      "Iteration 82610 Training loss 0.051746826618909836 Validation loss 0.057071030139923096 Accuracy 0.8455000519752502\n",
      "Output tensor([[0.9065],\n",
      "        [0.1814]], device='mps:0')\n",
      "Iteration 82620 Training loss 0.0539008229970932 Validation loss 0.05719999969005585 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0024],\n",
      "        [0.9851]], device='mps:0')\n",
      "Iteration 82630 Training loss 0.0577545203268528 Validation loss 0.05707463622093201 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.1184],\n",
      "        [0.7922]], device='mps:0')\n",
      "Iteration 82640 Training loss 0.05272078514099121 Validation loss 0.05709507316350937 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.8214],\n",
      "        [0.9132]], device='mps:0')\n",
      "Iteration 82650 Training loss 0.046496886759996414 Validation loss 0.05707580968737602 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.4380],\n",
      "        [0.1029]], device='mps:0')\n",
      "Iteration 82660 Training loss 0.05888737365603447 Validation loss 0.05719700828194618 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.8511],\n",
      "        [0.8885]], device='mps:0')\n",
      "Iteration 82670 Training loss 0.053078100085258484 Validation loss 0.05706920102238655 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.8176],\n",
      "        [0.9635]], device='mps:0')\n",
      "Iteration 82680 Training loss 0.05125253647565842 Validation loss 0.05707337334752083 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9234],\n",
      "        [0.6734]], device='mps:0')\n",
      "Iteration 82690 Training loss 0.054947927594184875 Validation loss 0.05720125138759613 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.6603],\n",
      "        [0.2666]], device='mps:0')\n",
      "Iteration 82700 Training loss 0.04900401458144188 Validation loss 0.05735335499048233 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.6660],\n",
      "        [0.3087]], device='mps:0')\n",
      "Iteration 82710 Training loss 0.04955518990755081 Validation loss 0.05706329643726349 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.9436],\n",
      "        [0.2927]], device='mps:0')\n",
      "Iteration 82720 Training loss 0.05364600941538811 Validation loss 0.057236868888139725 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.3352],\n",
      "        [0.4909]], device='mps:0')\n",
      "Iteration 82730 Training loss 0.05063127353787422 Validation loss 0.05713186413049698 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0232],\n",
      "        [0.9351]], device='mps:0')\n",
      "Iteration 82740 Training loss 0.052563659846782684 Validation loss 0.05710954591631889 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0506],\n",
      "        [0.0587]], device='mps:0')\n",
      "Iteration 82750 Training loss 0.04894645884633064 Validation loss 0.057062242180109024 Accuracy 0.8453750610351562\n",
      "Output tensor([[0.4174],\n",
      "        [0.0657]], device='mps:0')\n",
      "Iteration 82760 Training loss 0.05708114430308342 Validation loss 0.05710332840681076 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.6856],\n",
      "        [0.1125]], device='mps:0')\n",
      "Iteration 82770 Training loss 0.0621083527803421 Validation loss 0.05721425637602806 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0259],\n",
      "        [0.7135]], device='mps:0')\n",
      "Iteration 82780 Training loss 0.052792634814977646 Validation loss 0.05711420997977257 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.1200],\n",
      "        [0.0174]], device='mps:0')\n",
      "Iteration 82790 Training loss 0.05933152884244919 Validation loss 0.057124510407447815 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9252],\n",
      "        [0.7522]], device='mps:0')\n",
      "Iteration 82800 Training loss 0.05251653492450714 Validation loss 0.05705733597278595 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.0155],\n",
      "        [0.0682]], device='mps:0')\n",
      "Iteration 82810 Training loss 0.0550571009516716 Validation loss 0.05706080049276352 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0106],\n",
      "        [0.8886]], device='mps:0')\n",
      "Iteration 82820 Training loss 0.04949679225683212 Validation loss 0.057088784873485565 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.0763],\n",
      "        [0.0841]], device='mps:0')\n",
      "Iteration 82830 Training loss 0.05342923104763031 Validation loss 0.0571882538497448 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.2788],\n",
      "        [0.5811]], device='mps:0')\n",
      "Iteration 82840 Training loss 0.054685719311237335 Validation loss 0.05708411708474159 Accuracy 0.843375027179718\n",
      "Output tensor([[0.7680],\n",
      "        [0.0881]], device='mps:0')\n",
      "Iteration 82850 Training loss 0.052103251218795776 Validation loss 0.057126887142658234 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.6562],\n",
      "        [0.9553]], device='mps:0')\n",
      "Iteration 82860 Training loss 0.05067744106054306 Validation loss 0.05706717073917389 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.0294],\n",
      "        [0.6906]], device='mps:0')\n",
      "Iteration 82870 Training loss 0.05681166052818298 Validation loss 0.057071007788181305 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.7844],\n",
      "        [0.9828]], device='mps:0')\n",
      "Iteration 82880 Training loss 0.048668745905160904 Validation loss 0.057055383920669556 Accuracy 0.8452500104904175\n",
      "Output tensor([[0.9432],\n",
      "        [0.4313]], device='mps:0')\n",
      "Iteration 82890 Training loss 0.05455109477043152 Validation loss 0.057060204446315765 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.9739],\n",
      "        [0.1286]], device='mps:0')\n",
      "Iteration 82900 Training loss 0.04959811270236969 Validation loss 0.0570724718272686 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0930],\n",
      "        [0.6456]], device='mps:0')\n",
      "Iteration 82910 Training loss 0.0610249899327755 Validation loss 0.05706239119172096 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9357],\n",
      "        [0.8250]], device='mps:0')\n",
      "Iteration 82920 Training loss 0.05220761150121689 Validation loss 0.05706082284450531 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.0896],\n",
      "        [0.0429]], device='mps:0')\n",
      "Iteration 82930 Training loss 0.057311661541461945 Validation loss 0.057064399123191833 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.7410],\n",
      "        [0.7813]], device='mps:0')\n",
      "Iteration 82940 Training loss 0.05638141930103302 Validation loss 0.05706905573606491 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.3693],\n",
      "        [0.8567]], device='mps:0')\n",
      "Iteration 82950 Training loss 0.046418435871601105 Validation loss 0.0571378692984581 Accuracy 0.8440000414848328\n",
      "Output tensor([[0.4332],\n",
      "        [0.8620]], device='mps:0')\n",
      "Iteration 82960 Training loss 0.055040959268808365 Validation loss 0.05706770718097687 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.1174],\n",
      "        [0.8616]], device='mps:0')\n",
      "Iteration 82970 Training loss 0.04565097764134407 Validation loss 0.05706984922289848 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9931],\n",
      "        [0.0992]], device='mps:0')\n",
      "Iteration 82980 Training loss 0.05517200753092766 Validation loss 0.05708607658743858 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.6885],\n",
      "        [0.9356]], device='mps:0')\n",
      "Iteration 82990 Training loss 0.055194418877363205 Validation loss 0.05714140832424164 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.7363],\n",
      "        [0.0457]], device='mps:0')\n",
      "Iteration 83000 Training loss 0.05348769575357437 Validation loss 0.057058945298194885 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.2314],\n",
      "        [0.0281]], device='mps:0')\n",
      "Iteration 83010 Training loss 0.04898817092180252 Validation loss 0.05712165683507919 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.7776],\n",
      "        [0.9358]], device='mps:0')\n",
      "Iteration 83020 Training loss 0.05105426535010338 Validation loss 0.05710573494434357 Accuracy 0.8445000648498535\n",
      "Output tensor([[0.1774],\n",
      "        [0.6758]], device='mps:0')\n",
      "Iteration 83030 Training loss 0.055541981011629105 Validation loss 0.0572439543902874 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.0454],\n",
      "        [0.9290]], device='mps:0')\n",
      "Iteration 83040 Training loss 0.052973803132772446 Validation loss 0.057118356227874756 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.8072],\n",
      "        [0.0091]], device='mps:0')\n",
      "Iteration 83050 Training loss 0.045732129365205765 Validation loss 0.057069674134254456 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.8433],\n",
      "        [0.7351]], device='mps:0')\n",
      "Iteration 83060 Training loss 0.05338510498404503 Validation loss 0.05719812959432602 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9902],\n",
      "        [0.4068]], device='mps:0')\n",
      "Iteration 83070 Training loss 0.05007261037826538 Validation loss 0.05715615302324295 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.2316],\n",
      "        [0.0695]], device='mps:0')\n",
      "Iteration 83080 Training loss 0.05357683077454567 Validation loss 0.0572110079228878 Accuracy 0.8450000286102295\n",
      "Output tensor([[0.0436],\n",
      "        [0.8246]], device='mps:0')\n",
      "Iteration 83090 Training loss 0.057357292622327805 Validation loss 0.05715468153357506 Accuracy 0.8438750505447388\n",
      "Output tensor([[0.9425],\n",
      "        [0.6539]], device='mps:0')\n",
      "Iteration 83100 Training loss 0.05287322774529457 Validation loss 0.0571366548538208 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.0497],\n",
      "        [0.9367]], device='mps:0')\n",
      "Iteration 83110 Training loss 0.060526806861162186 Validation loss 0.05707026273012161 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.2947],\n",
      "        [0.2982]], device='mps:0')\n",
      "Iteration 83120 Training loss 0.049664776772260666 Validation loss 0.05706769973039627 Accuracy 0.8448750376701355\n",
      "Output tensor([[0.4415],\n",
      "        [0.0522]], device='mps:0')\n",
      "Iteration 83130 Training loss 0.0555780790746212 Validation loss 0.05706348642706871 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.9116],\n",
      "        [0.7188]], device='mps:0')\n",
      "Iteration 83140 Training loss 0.0506686307489872 Validation loss 0.0571652315557003 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.9636],\n",
      "        [0.2116]], device='mps:0')\n",
      "Iteration 83150 Training loss 0.05133181810379028 Validation loss 0.057110495865345 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.8031],\n",
      "        [0.5921]], device='mps:0')\n",
      "Iteration 83160 Training loss 0.05515348166227341 Validation loss 0.05708189681172371 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9154],\n",
      "        [0.7101]], device='mps:0')\n",
      "Iteration 83170 Training loss 0.04800919443368912 Validation loss 0.05706549063324928 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.9850],\n",
      "        [0.2734]], device='mps:0')\n",
      "Iteration 83180 Training loss 0.05891216918826103 Validation loss 0.057117633521556854 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.1557],\n",
      "        [0.1302]], device='mps:0')\n",
      "Iteration 83190 Training loss 0.0567619614303112 Validation loss 0.05723140761256218 Accuracy 0.8437500596046448\n",
      "Output tensor([[0.1415],\n",
      "        [0.2896]], device='mps:0')\n",
      "Iteration 83200 Training loss 0.051381487399339676 Validation loss 0.05706533044576645 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.7762],\n",
      "        [0.8028]], device='mps:0')\n",
      "Iteration 83210 Training loss 0.0595363974571228 Validation loss 0.05705832690000534 Accuracy 0.8455000519752502\n",
      "Output tensor([[0.8780],\n",
      "        [0.0896]], device='mps:0')\n",
      "Iteration 83220 Training loss 0.055417049676179886 Validation loss 0.05745181068778038 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.1564],\n",
      "        [0.6886]], device='mps:0')\n",
      "Iteration 83230 Training loss 0.048026297241449356 Validation loss 0.05707180127501488 Accuracy 0.8446250557899475\n",
      "Output tensor([[0.3487],\n",
      "        [0.1387]], device='mps:0')\n",
      "Iteration 83240 Training loss 0.04913662001490593 Validation loss 0.05711430683732033 Accuracy 0.8443750143051147\n",
      "Output tensor([[0.8068],\n",
      "        [0.6959]], device='mps:0')\n",
      "Iteration 83250 Training loss 0.05052288994193077 Validation loss 0.05708446726202965 Accuracy 0.8442500233650208\n",
      "Output tensor([[0.8666],\n",
      "        [0.5596]], device='mps:0')\n",
      "Iteration 83260 Training loss 0.053743671625852585 Validation loss 0.05708543583750725 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.9441],\n",
      "        [0.9942]], device='mps:0')\n",
      "Iteration 83270 Training loss 0.0568917915225029 Validation loss 0.057153165340423584 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.8009],\n",
      "        [0.2537]], device='mps:0')\n",
      "Iteration 83280 Training loss 0.0540243536233902 Validation loss 0.057141996920108795 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.8173],\n",
      "        [0.2300]], device='mps:0')\n",
      "Iteration 83290 Training loss 0.05143813416361809 Validation loss 0.05721548944711685 Accuracy 0.8451250195503235\n",
      "Output tensor([[0.0604],\n",
      "        [0.6050]], device='mps:0')\n",
      "Iteration 83300 Training loss 0.05558687448501587 Validation loss 0.05706821754574776 Accuracy 0.8447500467300415\n",
      "Output tensor([[0.9612],\n",
      "        [0.9758]], device='mps:0')\n",
      "Iteration 83310 Training loss 0.062437865883111954 Validation loss 0.057138532400131226 Accuracy 0.8441250324249268\n",
      "Output tensor([[0.0406],\n",
      "        [0.4749]], device='mps:0')\n",
      "Iteration 83320 Training loss 0.05562270060181618 Validation loss 0.05710630863904953 Accuracy 0.8437500596046448\n"
     ]
    }
   ],
   "source": [
    "binary_model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 1.6, 1e-8, 0, 0, 1e-1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08bfb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer = binary_classification_three_layer_NN(3072, 2048, 2048, eps_init = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c674f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 1.6, the number of datas used for the training is 38007905.54899443 and the number of iterations is 95019.\n",
      "Iteration 0 Training loss 0.12500080466270447 Validation loss 0.12499957531690598 Accuracy 0.5\n",
      "Iteration 10 Training loss 0.12093019485473633 Validation loss 0.12107241898775101 Accuracy 0.5211250185966492\n",
      "Iteration 20 Training loss 0.11532209813594818 Validation loss 0.11973229050636292 Accuracy 0.5237500071525574\n",
      "Iteration 30 Training loss 0.11548647284507751 Validation loss 0.11222895979881287 Accuracy 0.562000036239624\n",
      "Iteration 40 Training loss 0.10447465628385544 Validation loss 0.10420231521129608 Accuracy 0.7387500405311584\n",
      "Iteration 50 Training loss 0.12713125348091125 Validation loss 0.1436787247657776 Accuracy 0.5355000495910645\n",
      "Iteration 60 Training loss 0.10485385358333588 Validation loss 0.10228648781776428 Accuracy 0.7306250333786011\n",
      "Iteration 70 Training loss 0.16319262981414795 Validation loss 0.16209183633327484 Accuracy 0.5238749980926514\n",
      "Iteration 80 Training loss 0.08206546306610107 Validation loss 0.08365879207849503 Accuracy 0.7575000524520874\n",
      "Iteration 90 Training loss 0.09579849243164062 Validation loss 0.09069200605154037 Accuracy 0.7360000610351562\n",
      "Iteration 100 Training loss 0.10100392997264862 Validation loss 0.10087146610021591 Accuracy 0.7055000066757202\n",
      "Iteration 110 Training loss 0.08363410830497742 Validation loss 0.09186702221632004 Accuracy 0.736875057220459\n",
      "Iteration 120 Training loss 0.09346391260623932 Validation loss 0.09645848721265793 Accuracy 0.7193750143051147\n",
      "Iteration 130 Training loss 0.08328728377819061 Validation loss 0.07777994871139526 Accuracy 0.7772500514984131\n",
      "Iteration 140 Training loss 0.10004065930843353 Validation loss 0.09459950774908066 Accuracy 0.7022500038146973\n",
      "Iteration 150 Training loss 0.06730031222105026 Validation loss 0.07655439525842667 Accuracy 0.781125009059906\n",
      "Iteration 160 Training loss 0.08334832638502121 Validation loss 0.08122371882200241 Accuracy 0.7680000066757202\n",
      "Iteration 170 Training loss 0.07984478026628494 Validation loss 0.07479782402515411 Accuracy 0.7897500395774841\n",
      "Iteration 180 Training loss 0.08265849947929382 Validation loss 0.07533727586269379 Accuracy 0.7856250405311584\n",
      "Iteration 190 Training loss 0.07334596663713455 Validation loss 0.07883699238300323 Accuracy 0.7753750085830688\n",
      "Iteration 200 Training loss 0.06663984060287476 Validation loss 0.07428842037916183 Accuracy 0.7882500290870667\n",
      "Iteration 210 Training loss 0.0851057767868042 Validation loss 0.0737488716840744 Accuracy 0.7926250100135803\n",
      "Iteration 220 Training loss 0.07661794126033783 Validation loss 0.07659267634153366 Accuracy 0.7800000309944153\n",
      "Iteration 230 Training loss 0.07358391582965851 Validation loss 0.07680657505989075 Accuracy 0.78062504529953\n",
      "Iteration 240 Training loss 0.07637401670217514 Validation loss 0.07916656881570816 Accuracy 0.7716250419616699\n",
      "Iteration 250 Training loss 0.06749668717384338 Validation loss 0.07263519614934921 Accuracy 0.799750030040741\n",
      "Iteration 260 Training loss 0.07309260219335556 Validation loss 0.07197153568267822 Accuracy 0.799250066280365\n",
      "Iteration 270 Training loss 0.08025006949901581 Validation loss 0.07182324677705765 Accuracy 0.8005000352859497\n",
      "Iteration 280 Training loss 0.07102220505475998 Validation loss 0.07194454967975616 Accuracy 0.79625004529953\n",
      "Iteration 290 Training loss 0.07890938967466354 Validation loss 0.08447235822677612 Accuracy 0.7542500495910645\n",
      "Iteration 300 Training loss 0.07248841971158981 Validation loss 0.0748334527015686 Accuracy 0.7913750410079956\n",
      "Iteration 310 Training loss 0.072250597178936 Validation loss 0.07365310937166214 Accuracy 0.7891250252723694\n",
      "Iteration 320 Training loss 0.08407705277204514 Validation loss 0.07408926635980606 Accuracy 0.7880000472068787\n",
      "Iteration 330 Training loss 0.0756174847483635 Validation loss 0.07467766106128693 Accuracy 0.8002500534057617\n",
      "Iteration 340 Training loss 0.08764753490686417 Validation loss 0.08620268851518631 Accuracy 0.7420000433921814\n",
      "Iteration 350 Training loss 0.0681229829788208 Validation loss 0.06878741830587387 Accuracy 0.8092500567436218\n",
      "Iteration 360 Training loss 0.0667998194694519 Validation loss 0.07098647952079773 Accuracy 0.8075000643730164\n",
      "Iteration 370 Training loss 0.0710035189986229 Validation loss 0.07181373983621597 Accuracy 0.8002500534057617\n",
      "Iteration 380 Training loss 0.06339236348867416 Validation loss 0.06878437101840973 Accuracy 0.8101250529289246\n",
      "Iteration 390 Training loss 0.07714448124170303 Validation loss 0.07519879192113876 Accuracy 0.7898750305175781\n",
      "Iteration 400 Training loss 0.07388637214899063 Validation loss 0.07695264369249344 Accuracy 0.784375011920929\n",
      "Iteration 410 Training loss 0.07525958120822906 Validation loss 0.0721137598156929 Accuracy 0.7952500581741333\n",
      "Iteration 420 Training loss 0.07250278443098068 Validation loss 0.06689686328172684 Accuracy 0.8137500286102295\n",
      "Iteration 430 Training loss 0.06285234540700912 Validation loss 0.06638310849666595 Accuracy 0.8157500624656677\n",
      "Iteration 440 Training loss 0.06849786639213562 Validation loss 0.06706076115369797 Accuracy 0.8163750171661377\n",
      "Iteration 450 Training loss 0.06327392160892487 Validation loss 0.06575679033994675 Accuracy 0.8158750534057617\n",
      "Iteration 460 Training loss 0.059702519327402115 Validation loss 0.06563926488161087 Accuracy 0.8210000395774841\n",
      "Iteration 470 Training loss 0.07343573123216629 Validation loss 0.06857319921255112 Accuracy 0.8143750429153442\n",
      "Iteration 480 Training loss 0.06383059173822403 Validation loss 0.06593309342861176 Accuracy 0.8176250457763672\n",
      "Iteration 490 Training loss 0.07289297878742218 Validation loss 0.06404183059930801 Accuracy 0.8225000500679016\n",
      "Iteration 500 Training loss 0.06784181296825409 Validation loss 0.0674310028553009 Accuracy 0.8147500157356262\n",
      "Iteration 510 Training loss 0.07902680337429047 Validation loss 0.06662684679031372 Accuracy 0.8136250376701355\n",
      "Iteration 520 Training loss 0.060538142919540405 Validation loss 0.06513507664203644 Accuracy 0.8231250643730164\n",
      "Iteration 530 Training loss 0.07320176810026169 Validation loss 0.0728171244263649 Accuracy 0.799625039100647\n",
      "Iteration 540 Training loss 0.07143981009721756 Validation loss 0.0644359439611435 Accuracy 0.8180000185966492\n",
      "Iteration 550 Training loss 0.06517767161130905 Validation loss 0.06849300116300583 Accuracy 0.8101250529289246\n",
      "Iteration 560 Training loss 0.06795207411050797 Validation loss 0.06964469701051712 Accuracy 0.8045000433921814\n",
      "Iteration 570 Training loss 0.059741225093603134 Validation loss 0.06301795691251755 Accuracy 0.8266250491142273\n",
      "Iteration 580 Training loss 0.06451252847909927 Validation loss 0.06213347241282463 Accuracy 0.8291250467300415\n",
      "Iteration 590 Training loss 0.05439189821481705 Validation loss 0.06337552517652512 Accuracy 0.8295000195503235\n",
      "Iteration 600 Training loss 0.06919015198945999 Validation loss 0.06841740012168884 Accuracy 0.8087500333786011\n",
      "Iteration 610 Training loss 0.08198413997888565 Validation loss 0.06894876062870026 Accuracy 0.8105000257492065\n",
      "Iteration 620 Training loss 0.066145159304142 Validation loss 0.06374838203191757 Accuracy 0.8247500658035278\n",
      "Iteration 630 Training loss 0.06848779320716858 Validation loss 0.06099828705191612 Accuracy 0.8347500562667847\n",
      "Iteration 640 Training loss 0.05386229231953621 Validation loss 0.0654233768582344 Accuracy 0.8142500519752502\n",
      "Iteration 650 Training loss 0.07131049782037735 Validation loss 0.07168098539113998 Accuracy 0.7921250462532043\n",
      "Iteration 660 Training loss 0.07217054069042206 Validation loss 0.06699129194021225 Accuracy 0.8085000514984131\n",
      "Iteration 670 Training loss 0.05539073050022125 Validation loss 0.06140242516994476 Accuracy 0.8313750624656677\n",
      "Iteration 680 Training loss 0.06508508324623108 Validation loss 0.0606820173561573 Accuracy 0.8361250162124634\n",
      "Iteration 690 Training loss 0.07055498659610748 Validation loss 0.06893794238567352 Accuracy 0.8033750653266907\n",
      "Iteration 700 Training loss 0.055313125252723694 Validation loss 0.06419409066438675 Accuracy 0.8243750333786011\n",
      "Iteration 710 Training loss 0.06250422447919846 Validation loss 0.06144464388489723 Accuracy 0.8297500610351562\n",
      "Iteration 720 Training loss 0.061080202460289 Validation loss 0.06032039225101471 Accuracy 0.8343750238418579\n",
      "Iteration 730 Training loss 0.058103445917367935 Validation loss 0.05986320599913597 Accuracy 0.8356250524520874\n",
      "Iteration 740 Training loss 0.05039813742041588 Validation loss 0.05969414487481117 Accuracy 0.8367500305175781\n",
      "Iteration 750 Training loss 0.06505807489156723 Validation loss 0.06376373767852783 Accuracy 0.8206250667572021\n",
      "Iteration 760 Training loss 0.06697395443916321 Validation loss 0.05902554467320442 Accuracy 0.8396250605583191\n",
      "Iteration 770 Training loss 0.06139546260237694 Validation loss 0.06344631314277649 Accuracy 0.8212500214576721\n",
      "Iteration 780 Training loss 0.05438441038131714 Validation loss 0.05969958007335663 Accuracy 0.8377500176429749\n",
      "Iteration 790 Training loss 0.05779355764389038 Validation loss 0.05997234955430031 Accuracy 0.8365000486373901\n",
      "Iteration 800 Training loss 0.05962277203798294 Validation loss 0.06733480095863342 Accuracy 0.811625063419342\n",
      "Iteration 810 Training loss 0.059043996036052704 Validation loss 0.058844249695539474 Accuracy 0.8406250476837158\n",
      "Iteration 820 Training loss 0.06361304968595505 Validation loss 0.06659375131130219 Accuracy 0.8106250166893005\n",
      "Iteration 830 Training loss 0.05644950643181801 Validation loss 0.061049629002809525 Accuracy 0.8280000686645508\n",
      "Iteration 840 Training loss 0.04745794087648392 Validation loss 0.05812191590666771 Accuracy 0.8407500386238098\n",
      "Iteration 850 Training loss 0.05573190748691559 Validation loss 0.06273090094327927 Accuracy 0.8286250233650208\n",
      "Iteration 860 Training loss 0.04644245654344559 Validation loss 0.05795932188630104 Accuracy 0.8401250243186951\n",
      "Iteration 870 Training loss 0.07492628693580627 Validation loss 0.07088347524404526 Accuracy 0.8025000095367432\n",
      "Iteration 880 Training loss 0.060803525149822235 Validation loss 0.057542502880096436 Accuracy 0.8422500491142273\n",
      "Iteration 890 Training loss 0.06159505620598793 Validation loss 0.06877245008945465 Accuracy 0.8096250295639038\n",
      "Iteration 900 Training loss 0.06171863526105881 Validation loss 0.05708557367324829 Accuracy 0.8445000648498535\n",
      "Iteration 910 Training loss 0.05885428190231323 Validation loss 0.056845370680093765 Accuracy 0.8455000519752502\n",
      "Iteration 920 Training loss 0.050597939640283585 Validation loss 0.057721447199583054 Accuracy 0.8407500386238098\n",
      "Iteration 930 Training loss 0.05972234159708023 Validation loss 0.06725499033927917 Accuracy 0.8140000104904175\n",
      "Iteration 940 Training loss 0.05034651607275009 Validation loss 0.061884887516498566 Accuracy 0.8315000534057617\n",
      "Iteration 950 Training loss 0.0587521456182003 Validation loss 0.05778220295906067 Accuracy 0.843250036239624\n",
      "Iteration 960 Training loss 0.05701574683189392 Validation loss 0.05654371902346611 Accuracy 0.846750020980835\n",
      "Iteration 970 Training loss 0.04952310770750046 Validation loss 0.059109579771757126 Accuracy 0.8367500305175781\n",
      "Iteration 980 Training loss 0.05034797638654709 Validation loss 0.056178756058216095 Accuracy 0.8471250534057617\n",
      "Iteration 990 Training loss 0.0640583336353302 Validation loss 0.05974569171667099 Accuracy 0.8365000486373901\n",
      "Iteration 1000 Training loss 0.06061885878443718 Validation loss 0.0607738122344017 Accuracy 0.8326250314712524\n",
      "Iteration 1010 Training loss 0.05460188165307045 Validation loss 0.055214885622262955 Accuracy 0.8517500162124634\n",
      "Iteration 1020 Training loss 0.062412671744823456 Validation loss 0.06941375136375427 Accuracy 0.8012500405311584\n",
      "Iteration 1030 Training loss 0.05137436091899872 Validation loss 0.05824337899684906 Accuracy 0.8372500538825989\n",
      "Iteration 1040 Training loss 0.055752430111169815 Validation loss 0.05753195285797119 Accuracy 0.843000054359436\n",
      "Iteration 1050 Training loss 0.053289320319890976 Validation loss 0.05502372980117798 Accuracy 0.8496250510215759\n",
      "Iteration 1060 Training loss 0.046143610030412674 Validation loss 0.05614502355456352 Accuracy 0.8437500596046448\n",
      "Iteration 1070 Training loss 0.06064974144101143 Validation loss 0.060357630252838135 Accuracy 0.8270000219345093\n",
      "Iteration 1080 Training loss 0.06052893400192261 Validation loss 0.05863886699080467 Accuracy 0.8360000252723694\n",
      "Iteration 1090 Training loss 0.05112450569868088 Validation loss 0.054556794464588165 Accuracy 0.8510000109672546\n",
      "Iteration 1100 Training loss 0.053580544888973236 Validation loss 0.05862335488200188 Accuracy 0.8402500152587891\n",
      "Iteration 1110 Training loss 0.05678151547908783 Validation loss 0.054322995245456696 Accuracy 0.8506250381469727\n",
      "Iteration 1120 Training loss 0.06044706329703331 Validation loss 0.0586443729698658 Accuracy 0.8337500691413879\n",
      "Iteration 1130 Training loss 0.05575114116072655 Validation loss 0.05525100603699684 Accuracy 0.8472500443458557\n",
      "Iteration 1140 Training loss 0.057882875204086304 Validation loss 0.0621466338634491 Accuracy 0.8242500424385071\n",
      "Iteration 1150 Training loss 0.05939646065235138 Validation loss 0.05503806471824646 Accuracy 0.8476250171661377\n",
      "Iteration 1160 Training loss 0.05441269651055336 Validation loss 0.05447610095143318 Accuracy 0.8511250615119934\n",
      "Iteration 1170 Training loss 0.048535849899053574 Validation loss 0.054662205278873444 Accuracy 0.8497500419616699\n",
      "Iteration 1180 Training loss 0.06040507182478905 Validation loss 0.05452859029173851 Accuracy 0.8510000109672546\n",
      "Iteration 1190 Training loss 0.053411636501550674 Validation loss 0.05379282683134079 Accuracy 0.8543750643730164\n",
      "Iteration 1200 Training loss 0.05868978053331375 Validation loss 0.05393960699439049 Accuracy 0.8515000343322754\n",
      "Iteration 1210 Training loss 0.05955164134502411 Validation loss 0.05541061609983444 Accuracy 0.8450000286102295\n",
      "Iteration 1220 Training loss 0.05102125555276871 Validation loss 0.05357300117611885 Accuracy 0.8543750643730164\n",
      "Iteration 1230 Training loss 0.05562447011470795 Validation loss 0.056765589863061905 Accuracy 0.8388750553131104\n",
      "Iteration 1240 Training loss 0.04946506768465042 Validation loss 0.05497206002473831 Accuracy 0.8496250510215759\n",
      "Iteration 1250 Training loss 0.05353853106498718 Validation loss 0.05357588455080986 Accuracy 0.8528750538825989\n",
      "Iteration 1260 Training loss 0.06007692217826843 Validation loss 0.05583443492650986 Accuracy 0.8457500338554382\n",
      "Iteration 1270 Training loss 0.06019387021660805 Validation loss 0.053334809839725494 Accuracy 0.8550000190734863\n",
      "Iteration 1280 Training loss 0.05870766192674637 Validation loss 0.058179933577775955 Accuracy 0.8421250581741333\n",
      "Iteration 1290 Training loss 0.04994426667690277 Validation loss 0.05314527824521065 Accuracy 0.8540000319480896\n",
      "Iteration 1300 Training loss 0.04818454757332802 Validation loss 0.05395829305052757 Accuracy 0.8536250591278076\n",
      "Iteration 1310 Training loss 0.05390431359410286 Validation loss 0.052464358508586884 Accuracy 0.8556250333786011\n",
      "Iteration 1320 Training loss 0.07257048785686493 Validation loss 0.07556859403848648 Accuracy 0.7916250228881836\n",
      "Iteration 1330 Training loss 0.0605994388461113 Validation loss 0.06056923791766167 Accuracy 0.8255000114440918\n",
      "Iteration 1340 Training loss 0.0553387813270092 Validation loss 0.05401124060153961 Accuracy 0.8510000109672546\n",
      "Iteration 1350 Training loss 0.05143142491579056 Validation loss 0.05250784382224083 Accuracy 0.8547500371932983\n",
      "Iteration 1360 Training loss 0.04472043737769127 Validation loss 0.058000799268484116 Accuracy 0.8421250581741333\n",
      "Iteration 1370 Training loss 0.06732005625963211 Validation loss 0.05922321602702141 Accuracy 0.8377500176429749\n",
      "Iteration 1380 Training loss 0.04810713604092598 Validation loss 0.05466499924659729 Accuracy 0.8493750691413879\n",
      "Iteration 1390 Training loss 0.0445440337061882 Validation loss 0.05150274559855461 Accuracy 0.8581250309944153\n",
      "Iteration 1400 Training loss 0.05023092031478882 Validation loss 0.05212875455617905 Accuracy 0.8560000658035278\n",
      "Iteration 1410 Training loss 0.05323577672243118 Validation loss 0.051981501281261444 Accuracy 0.8546250462532043\n",
      "Iteration 1420 Training loss 0.05837790295481682 Validation loss 0.05962207168340683 Accuracy 0.8321250677108765\n",
      "Iteration 1430 Training loss 0.0442676916718483 Validation loss 0.056124597787857056 Accuracy 0.8478750586509705\n",
      "Iteration 1440 Training loss 0.0478411503136158 Validation loss 0.05137909576296806 Accuracy 0.8575000166893005\n",
      "Iteration 1450 Training loss 0.060652121901512146 Validation loss 0.05137323960661888 Accuracy 0.8575000166893005\n",
      "Iteration 1460 Training loss 0.044108789414167404 Validation loss 0.051550302654504776 Accuracy 0.85875004529953\n",
      "Iteration 1470 Training loss 0.05809326469898224 Validation loss 0.056244730949401855 Accuracy 0.8425000309944153\n",
      "Iteration 1480 Training loss 0.04738723486661911 Validation loss 0.05232585221529007 Accuracy 0.8520000576972961\n",
      "Iteration 1490 Training loss 0.048303715884685516 Validation loss 0.05100296810269356 Accuracy 0.8597500324249268\n",
      "Iteration 1500 Training loss 0.05604775249958038 Validation loss 0.05671388655900955 Accuracy 0.8471250534057617\n",
      "Iteration 1510 Training loss 0.054424818605184555 Validation loss 0.052056171000003815 Accuracy 0.8601250648498535\n",
      "Iteration 1520 Training loss 0.05083636939525604 Validation loss 0.05282969027757645 Accuracy 0.8533750176429749\n",
      "Iteration 1530 Training loss 0.054685208946466446 Validation loss 0.061269838362932205 Accuracy 0.8286250233650208\n",
      "Iteration 1540 Training loss 0.046122532337903976 Validation loss 0.053046345710754395 Accuracy 0.8575000166893005\n",
      "Iteration 1550 Training loss 0.05376540124416351 Validation loss 0.051466334611177444 Accuracy 0.8555000424385071\n",
      "Iteration 1560 Training loss 0.06042475625872612 Validation loss 0.057212505489587784 Accuracy 0.8382500410079956\n",
      "Iteration 1570 Training loss 0.05421441048383713 Validation loss 0.05414813756942749 Accuracy 0.8542500138282776\n",
      "Iteration 1580 Training loss 0.043498843908309937 Validation loss 0.05003844201564789 Accuracy 0.8626250624656677\n",
      "Iteration 1590 Training loss 0.056864481419324875 Validation loss 0.06001431494951248 Accuracy 0.82750004529953\n",
      "Iteration 1600 Training loss 0.0553462915122509 Validation loss 0.0582641176879406 Accuracy 0.8417500257492065\n",
      "Iteration 1610 Training loss 0.048936501145362854 Validation loss 0.049975015223026276 Accuracy 0.8637500405311584\n",
      "Iteration 1620 Training loss 0.053601596504449844 Validation loss 0.05602859705686569 Accuracy 0.8506250381469727\n",
      "Iteration 1630 Training loss 0.04448222368955612 Validation loss 0.05041045695543289 Accuracy 0.862000048160553\n",
      "Iteration 1640 Training loss 0.04989837482571602 Validation loss 0.05211196094751358 Accuracy 0.8571250438690186\n",
      "Iteration 1650 Training loss 0.039993029087781906 Validation loss 0.0503363274037838 Accuracy 0.8643750548362732\n",
      "Iteration 1660 Training loss 0.0537203811109066 Validation loss 0.0504276305437088 Accuracy 0.858875036239624\n",
      "Iteration 1670 Training loss 0.05977243930101395 Validation loss 0.05836319923400879 Accuracy 0.8416250348091125\n",
      "Iteration 1680 Training loss 0.0664321780204773 Validation loss 0.0608668178319931 Accuracy 0.8372500538825989\n",
      "Iteration 1690 Training loss 0.04710344225168228 Validation loss 0.049737244844436646 Accuracy 0.8642500638961792\n",
      "Iteration 1700 Training loss 0.0489933080971241 Validation loss 0.04932909458875656 Accuracy 0.8637500405311584\n",
      "Iteration 1710 Training loss 0.04198860377073288 Validation loss 0.049888111650943756 Accuracy 0.8656250238418579\n",
      "Iteration 1720 Training loss 0.059240784496068954 Validation loss 0.054311759769916534 Accuracy 0.8478750586509705\n",
      "Iteration 1730 Training loss 0.0548807755112648 Validation loss 0.05168716609477997 Accuracy 0.8563750386238098\n",
      "Iteration 1740 Training loss 0.05083925276994705 Validation loss 0.04973677918314934 Accuracy 0.8593750596046448\n",
      "Iteration 1750 Training loss 0.05148157477378845 Validation loss 0.05072030797600746 Accuracy 0.8627500534057617\n",
      "Iteration 1760 Training loss 0.03850226476788521 Validation loss 0.04957869276404381 Accuracy 0.862125039100647\n",
      "Iteration 1770 Training loss 0.05670815333724022 Validation loss 0.05681420490145683 Accuracy 0.8400000333786011\n",
      "Iteration 1780 Training loss 0.040180254727602005 Validation loss 0.04880516231060028 Accuracy 0.862375020980835\n",
      "Iteration 1790 Training loss 0.050046470016241074 Validation loss 0.053488634526729584 Accuracy 0.8492500185966492\n",
      "Iteration 1800 Training loss 0.052817974239587784 Validation loss 0.05604521185159683 Accuracy 0.842875063419342\n",
      "Iteration 1810 Training loss 0.045172203332185745 Validation loss 0.05093775689601898 Accuracy 0.8572500348091125\n",
      "Iteration 1820 Training loss 0.05226544663310051 Validation loss 0.05324211344122887 Accuracy 0.8526250123977661\n",
      "Iteration 1830 Training loss 0.035028133541345596 Validation loss 0.04908062517642975 Accuracy 0.8616250157356262\n",
      "Iteration 1840 Training loss 0.05443684756755829 Validation loss 0.0573309101164341 Accuracy 0.8398750424385071\n",
      "Iteration 1850 Training loss 0.06123204901814461 Validation loss 0.06120217591524124 Accuracy 0.8265000581741333\n",
      "Iteration 1860 Training loss 0.03906606510281563 Validation loss 0.048566997051239014 Accuracy 0.8667500615119934\n",
      "Iteration 1870 Training loss 0.0637713298201561 Validation loss 0.06090866029262543 Accuracy 0.8257500529289246\n",
      "Iteration 1880 Training loss 0.053899459540843964 Validation loss 0.05064648017287254 Accuracy 0.8573750257492065\n",
      "Iteration 1890 Training loss 0.04547884687781334 Validation loss 0.04803107678890228 Accuracy 0.8682500123977661\n",
      "Iteration 1900 Training loss 0.0583009272813797 Validation loss 0.060826968401670456 Accuracy 0.8283750414848328\n",
      "Iteration 1910 Training loss 0.03990636393427849 Validation loss 0.04849017411470413 Accuracy 0.8658750653266907\n",
      "Iteration 1920 Training loss 0.04466000571846962 Validation loss 0.051576822996139526 Accuracy 0.862000048160553\n",
      "Iteration 1930 Training loss 0.048347920179367065 Validation loss 0.0476544089615345 Accuracy 0.8691250681877136\n",
      "Iteration 1940 Training loss 0.04454006254673004 Validation loss 0.04975897818803787 Accuracy 0.8652500510215759\n",
      "Iteration 1950 Training loss 0.04315175861120224 Validation loss 0.04806144908070564 Accuracy 0.8680000305175781\n",
      "Iteration 1960 Training loss 0.04206883907318115 Validation loss 0.04738732427358627 Accuracy 0.8688750267028809\n",
      "Iteration 1970 Training loss 0.06876502186059952 Validation loss 0.06042620539665222 Accuracy 0.8298750519752502\n",
      "Iteration 1980 Training loss 0.051883529871702194 Validation loss 0.047841280698776245 Accuracy 0.8690000176429749\n",
      "Iteration 1990 Training loss 0.03853955119848251 Validation loss 0.04826021566987038 Accuracy 0.8658750653266907\n",
      "Iteration 2000 Training loss 0.051204707473516464 Validation loss 0.054372936487197876 Accuracy 0.8443750143051147\n",
      "Iteration 2010 Training loss 0.0430649034678936 Validation loss 0.04791479557752609 Accuracy 0.8668750524520874\n",
      "Iteration 2020 Training loss 0.04835556820034981 Validation loss 0.055108096450567245 Accuracy 0.8521250486373901\n",
      "Iteration 2030 Training loss 0.04256428778171539 Validation loss 0.0473780557513237 Accuracy 0.8705000281333923\n",
      "Iteration 2040 Training loss 0.03735323250293732 Validation loss 0.04719120264053345 Accuracy 0.8690000176429749\n",
      "Iteration 2050 Training loss 0.04896091669797897 Validation loss 0.04975196346640587 Accuracy 0.8650000691413879\n",
      "Iteration 2060 Training loss 0.046471141278743744 Validation loss 0.049234721809625626 Accuracy 0.8633750677108765\n",
      "Iteration 2070 Training loss 0.04356369748711586 Validation loss 0.04719509929418564 Accuracy 0.8707500696182251\n",
      "Iteration 2080 Training loss 0.043450728058815 Validation loss 0.04743890464305878 Accuracy 0.8676250576972961\n",
      "Iteration 2090 Training loss 0.04493282362818718 Validation loss 0.04712490364909172 Accuracy 0.8698750138282776\n",
      "Iteration 2100 Training loss 0.04484066367149353 Validation loss 0.04946335032582283 Accuracy 0.8645000457763672\n",
      "Iteration 2110 Training loss 0.045270927250385284 Validation loss 0.05307473987340927 Accuracy 0.8562500476837158\n",
      "Iteration 2120 Training loss 0.04576921463012695 Validation loss 0.047611597925424576 Accuracy 0.8658750653266907\n",
      "Iteration 2130 Training loss 0.045514754951000214 Validation loss 0.04690378159284592 Accuracy 0.8727500438690186\n",
      "Iteration 2140 Training loss 0.04311148077249527 Validation loss 0.05090612545609474 Accuracy 0.8572500348091125\n",
      "Iteration 2150 Training loss 0.04618699848651886 Validation loss 0.05073443800210953 Accuracy 0.8562500476837158\n",
      "Iteration 2160 Training loss 0.04601920396089554 Validation loss 0.04684262350201607 Accuracy 0.8712500333786011\n",
      "Iteration 2170 Training loss 0.040956512093544006 Validation loss 0.04750985652208328 Accuracy 0.8688750267028809\n",
      "Iteration 2180 Training loss 0.04080861806869507 Validation loss 0.04725077375769615 Accuracy 0.8686250448226929\n",
      "Iteration 2190 Training loss 0.04390290006995201 Validation loss 0.0551641508936882 Accuracy 0.8522500395774841\n",
      "Iteration 2200 Training loss 0.03526010364294052 Validation loss 0.046539306640625 Accuracy 0.8696250319480896\n",
      "Iteration 2210 Training loss 0.06367901712656021 Validation loss 0.06462094932794571 Accuracy 0.8180000185966492\n",
      "Iteration 2220 Training loss 0.037442684173583984 Validation loss 0.04680701717734337 Accuracy 0.8716250658035278\n",
      "Iteration 2230 Training loss 0.03852566331624985 Validation loss 0.0462597981095314 Accuracy 0.8712500333786011\n",
      "Iteration 2240 Training loss 0.037920959293842316 Validation loss 0.04620876535773277 Accuracy 0.8736250400543213\n",
      "Iteration 2250 Training loss 0.04382815584540367 Validation loss 0.046127911657094955 Accuracy 0.8722500205039978\n",
      "Iteration 2260 Training loss 0.0406448133289814 Validation loss 0.04744519665837288 Accuracy 0.8712500333786011\n",
      "Iteration 2270 Training loss 0.038353681564331055 Validation loss 0.04653657600283623 Accuracy 0.8702500462532043\n",
      "Iteration 2280 Training loss 0.051628388464450836 Validation loss 0.05253445729613304 Accuracy 0.8583750128746033\n",
      "Iteration 2290 Training loss 0.038266122341156006 Validation loss 0.04858143627643585 Accuracy 0.8702500462532043\n",
      "Iteration 2300 Training loss 0.03583153337240219 Validation loss 0.04659078270196915 Accuracy 0.8722500205039978\n",
      "Iteration 2310 Training loss 0.03905337676405907 Validation loss 0.05122418329119682 Accuracy 0.8637500405311584\n",
      "Iteration 2320 Training loss 0.04261272773146629 Validation loss 0.047098804265260696 Accuracy 0.8711250424385071\n",
      "Iteration 2330 Training loss 0.04794614389538765 Validation loss 0.049764275550842285 Accuracy 0.8608750700950623\n",
      "Iteration 2340 Training loss 0.05127045139670372 Validation loss 0.05520056560635567 Accuracy 0.8440000414848328\n",
      "Iteration 2350 Training loss 0.042073171585798264 Validation loss 0.04647788032889366 Accuracy 0.8706250190734863\n",
      "Iteration 2360 Training loss 0.04333934187889099 Validation loss 0.04635738953948021 Accuracy 0.8707500696182251\n",
      "Iteration 2370 Training loss 0.05092449113726616 Validation loss 0.053162191063165665 Accuracy 0.8508750200271606\n",
      "Iteration 2380 Training loss 0.03372975066304207 Validation loss 0.04753636196255684 Accuracy 0.8683750629425049\n",
      "Iteration 2390 Training loss 0.05180675908923149 Validation loss 0.0509382039308548 Accuracy 0.8582500219345093\n",
      "Iteration 2400 Training loss 0.03100908361375332 Validation loss 0.048110231757164 Accuracy 0.8720000386238098\n",
      "Iteration 2410 Training loss 0.035183001309633255 Validation loss 0.04632829874753952 Accuracy 0.8707500696182251\n",
      "Iteration 2420 Training loss 0.039873477071523666 Validation loss 0.0473332554101944 Accuracy 0.8655000329017639\n",
      "Iteration 2430 Training loss 0.03717740625143051 Validation loss 0.04641430452466011 Accuracy 0.8731250166893005\n",
      "Iteration 2440 Training loss 0.03757248818874359 Validation loss 0.050491198897361755 Accuracy 0.8632500171661377\n",
      "Iteration 2450 Training loss 0.03741488233208656 Validation loss 0.04597359523177147 Accuracy 0.8718750476837158\n",
      "Iteration 2460 Training loss 0.04130661487579346 Validation loss 0.049510709941387177 Accuracy 0.862125039100647\n",
      "Iteration 2470 Training loss 0.042135611176490784 Validation loss 0.04641128331422806 Accuracy 0.8721250295639038\n",
      "Iteration 2480 Training loss 0.04093638435006142 Validation loss 0.048488836735486984 Accuracy 0.8703750371932983\n",
      "Iteration 2490 Training loss 0.04633234441280365 Validation loss 0.04607274755835533 Accuracy 0.8731250166893005\n",
      "Iteration 2500 Training loss 0.03266771882772446 Validation loss 0.04618915170431137 Accuracy 0.8702500462532043\n",
      "Iteration 2510 Training loss 0.046580638736486435 Validation loss 0.04828253388404846 Accuracy 0.8702500462532043\n",
      "Iteration 2520 Training loss 0.03193264827132225 Validation loss 0.046439796686172485 Accuracy 0.8740000128746033\n",
      "Iteration 2530 Training loss 0.03666968643665314 Validation loss 0.04794298857450485 Accuracy 0.8646250367164612\n",
      "Iteration 2540 Training loss 0.034231580793857574 Validation loss 0.04637986049056053 Accuracy 0.8725000619888306\n",
      "Iteration 2550 Training loss 0.04880145937204361 Validation loss 0.05457833409309387 Accuracy 0.8480000495910645\n",
      "Iteration 2560 Training loss 0.04151129722595215 Validation loss 0.04737710952758789 Accuracy 0.8677500486373901\n",
      "Iteration 2570 Training loss 0.032215915620326996 Validation loss 0.04618042707443237 Accuracy 0.8701250553131104\n",
      "Iteration 2580 Training loss 0.042767394334077835 Validation loss 0.04929116368293762 Accuracy 0.8627500534057617\n",
      "Iteration 2590 Training loss 0.04878048971295357 Validation loss 0.04894495755434036 Accuracy 0.8671250343322754\n",
      "Iteration 2600 Training loss 0.030483640730381012 Validation loss 0.04607081040740013 Accuracy 0.874250054359436\n",
      "Iteration 2610 Training loss 0.04006809741258621 Validation loss 0.04638541489839554 Accuracy 0.8705000281333923\n",
      "Iteration 2620 Training loss 0.043273162096738815 Validation loss 0.04577409476041794 Accuracy 0.8750000596046448\n",
      "Iteration 2630 Training loss 0.046043701469898224 Validation loss 0.05171927809715271 Accuracy 0.8595000505447388\n",
      "Iteration 2640 Training loss 0.04030377417802811 Validation loss 0.04688718914985657 Accuracy 0.8715000152587891\n",
      "Iteration 2650 Training loss 0.03281494975090027 Validation loss 0.0473187193274498 Accuracy 0.8661250472068787\n",
      "Iteration 2660 Training loss 0.041816215962171555 Validation loss 0.046314485371112823 Accuracy 0.8712500333786011\n",
      "Iteration 2670 Training loss 0.04274041950702667 Validation loss 0.04661998525261879 Accuracy 0.8707500696182251\n",
      "Iteration 2680 Training loss 0.03264416754245758 Validation loss 0.04595448076725006 Accuracy 0.8727500438690186\n",
      "Iteration 2690 Training loss 0.0462300106883049 Validation loss 0.04951561614871025 Accuracy 0.8642500638961792\n",
      "Iteration 2700 Training loss 0.04946524649858475 Validation loss 0.05914919450879097 Accuracy 0.8423750400543213\n",
      "Iteration 2710 Training loss 0.03903644531965256 Validation loss 0.04541584849357605 Accuracy 0.8761250376701355\n",
      "Iteration 2720 Training loss 0.03593294695019722 Validation loss 0.046718355268239975 Accuracy 0.8710000514984131\n",
      "Iteration 2730 Training loss 0.047390539199113846 Validation loss 0.050489865243434906 Accuracy 0.85875004529953\n",
      "Iteration 2740 Training loss 0.03832167387008667 Validation loss 0.04652567580342293 Accuracy 0.8692500591278076\n",
      "Iteration 2750 Training loss 0.041512832045555115 Validation loss 0.0491134375333786 Accuracy 0.8673750162124634\n",
      "Iteration 2760 Training loss 0.040630731731653214 Validation loss 0.04541058838367462 Accuracy 0.8763750195503235\n",
      "Iteration 2770 Training loss 0.03965369984507561 Validation loss 0.04504550248384476 Accuracy 0.8762500286102295\n",
      "Iteration 2780 Training loss 0.036669254302978516 Validation loss 0.047888241708278656 Accuracy 0.8697500228881836\n",
      "Iteration 2790 Training loss 0.03655974194407463 Validation loss 0.049056168645620346 Accuracy 0.8667500615119934\n",
      "Iteration 2800 Training loss 0.0316634476184845 Validation loss 0.04555811360478401 Accuracy 0.8722500205039978\n",
      "Iteration 2810 Training loss 0.03670921176671982 Validation loss 0.04434659704566002 Accuracy 0.8787500262260437\n",
      "Iteration 2820 Training loss 0.02785712666809559 Validation loss 0.04703424125909805 Accuracy 0.8713750243186951\n",
      "Iteration 2830 Training loss 0.04105566814541817 Validation loss 0.04450035095214844 Accuracy 0.8788750171661377\n",
      "Iteration 2840 Training loss 0.04419462010264397 Validation loss 0.050235286355018616 Accuracy 0.8571250438690186\n",
      "Iteration 2850 Training loss 0.0287905503064394 Validation loss 0.044645097106695175 Accuracy 0.8772500157356262\n",
      "Iteration 2860 Training loss 0.06762570887804031 Validation loss 0.06748753786087036 Accuracy 0.8206250667572021\n",
      "Iteration 2870 Training loss 0.03539541736245155 Validation loss 0.0445023737847805 Accuracy 0.8762500286102295\n",
      "Iteration 2880 Training loss 0.03600170463323593 Validation loss 0.050917670130729675 Accuracy 0.8541250228881836\n",
      "Iteration 2890 Training loss 0.03600292280316353 Validation loss 0.044316936284303665 Accuracy 0.877375066280365\n",
      "Iteration 2900 Training loss 0.04624844342470169 Validation loss 0.04938783496618271 Accuracy 0.8658750653266907\n",
      "Iteration 2910 Training loss 0.04401331767439842 Validation loss 0.04833338409662247 Accuracy 0.8682500123977661\n",
      "Iteration 2920 Training loss 0.037814054638147354 Validation loss 0.048774752765893936 Accuracy 0.8692500591278076\n",
      "Iteration 2930 Training loss 0.04047278314828873 Validation loss 0.04907142370939255 Accuracy 0.8693750500679016\n",
      "Iteration 2940 Training loss 0.04027310386300087 Validation loss 0.04655461385846138 Accuracy 0.8723750710487366\n",
      "Iteration 2950 Training loss 0.035722535103559494 Validation loss 0.04441588744521141 Accuracy 0.878000020980835\n",
      "Iteration 2960 Training loss 0.03508591279387474 Validation loss 0.04624902829527855 Accuracy 0.8725000619888306\n",
      "Iteration 2970 Training loss 0.057048872113227844 Validation loss 0.05635795742273331 Accuracy 0.8480000495910645\n",
      "Iteration 2980 Training loss 0.0374014712870121 Validation loss 0.04638177156448364 Accuracy 0.8676250576972961\n",
      "Iteration 2990 Training loss 0.03085881844162941 Validation loss 0.04410414770245552 Accuracy 0.8785000443458557\n",
      "Iteration 3000 Training loss 0.03721591457724571 Validation loss 0.04596393182873726 Accuracy 0.8733750581741333\n",
      "Iteration 3010 Training loss 0.03226688131690025 Validation loss 0.04477636143565178 Accuracy 0.8761250376701355\n",
      "Iteration 3020 Training loss 0.04035106673836708 Validation loss 0.047877874225378036 Accuracy 0.8693750500679016\n",
      "Iteration 3030 Training loss 0.04899220913648605 Validation loss 0.05153357982635498 Accuracy 0.861750066280365\n",
      "Iteration 3040 Training loss 0.04115596041083336 Validation loss 0.04479498788714409 Accuracy 0.874500036239624\n",
      "Iteration 3050 Training loss 0.028538018465042114 Validation loss 0.044709984213113785 Accuracy 0.8765000700950623\n",
      "Iteration 3060 Training loss 0.032015834003686905 Validation loss 0.04478371515870094 Accuracy 0.877625048160553\n",
      "Iteration 3070 Training loss 0.03795137628912926 Validation loss 0.047957200556993484 Accuracy 0.8717500567436218\n",
      "Iteration 3080 Training loss 0.037523649632930756 Validation loss 0.04765738174319267 Accuracy 0.8716250658035278\n",
      "Iteration 3090 Training loss 0.026979101821780205 Validation loss 0.04409833624958992 Accuracy 0.8792500495910645\n",
      "Iteration 3100 Training loss 0.03090020641684532 Validation loss 0.044554032385349274 Accuracy 0.877750039100647\n",
      "Iteration 3110 Training loss 0.034684233367443085 Validation loss 0.044733066111803055 Accuracy 0.8756250143051147\n",
      "Iteration 3120 Training loss 0.03567218780517578 Validation loss 0.044790808111429214 Accuracy 0.8772500157356262\n",
      "Iteration 3130 Training loss 0.0346558578312397 Validation loss 0.04405287280678749 Accuracy 0.8765000700950623\n",
      "Iteration 3140 Training loss 0.039079807698726654 Validation loss 0.05297568067908287 Accuracy 0.8497500419616699\n",
      "Iteration 3150 Training loss 0.033709488809108734 Validation loss 0.04469968006014824 Accuracy 0.8761250376701355\n",
      "Iteration 3160 Training loss 0.05588236078619957 Validation loss 0.06673581153154373 Accuracy 0.8140000104904175\n",
      "Iteration 3170 Training loss 0.027606291696429253 Validation loss 0.0440429262816906 Accuracy 0.877750039100647\n",
      "Iteration 3180 Training loss 0.041157420724630356 Validation loss 0.043574701994657516 Accuracy 0.8797500133514404\n",
      "Iteration 3190 Training loss 0.031457699835300446 Validation loss 0.04579157009720802 Accuracy 0.877875030040741\n",
      "Iteration 3200 Training loss 0.03212100267410278 Validation loss 0.045184314250946045 Accuracy 0.8772500157356262\n",
      "Iteration 3210 Training loss 0.02567409910261631 Validation loss 0.04590459540486336 Accuracy 0.8757500648498535\n",
      "Iteration 3220 Training loss 0.06949169188737869 Validation loss 0.05924636498093605 Accuracy 0.8425000309944153\n",
      "Iteration 3230 Training loss 0.03242135047912598 Validation loss 0.0445791557431221 Accuracy 0.877875030040741\n",
      "Iteration 3240 Training loss 0.028156189247965813 Validation loss 0.04425022751092911 Accuracy 0.8772500157356262\n",
      "Iteration 3250 Training loss 0.03228827938437462 Validation loss 0.043919291347265244 Accuracy 0.8808750510215759\n",
      "Iteration 3260 Training loss 0.03624226525425911 Validation loss 0.04432664066553116 Accuracy 0.8755000233650208\n",
      "Iteration 3270 Training loss 0.06482703238725662 Validation loss 0.06540896743535995 Accuracy 0.8195000290870667\n",
      "Iteration 3280 Training loss 0.03104517236351967 Validation loss 0.04411603510379791 Accuracy 0.8772500157356262\n",
      "Iteration 3290 Training loss 0.03556262329220772 Validation loss 0.046130381524562836 Accuracy 0.8768750429153442\n",
      "Iteration 3300 Training loss 0.03656592220067978 Validation loss 0.0445210225880146 Accuracy 0.877875030040741\n",
      "Iteration 3310 Training loss 0.03784036263823509 Validation loss 0.048177480697631836 Accuracy 0.8722500205039978\n",
      "Iteration 3320 Training loss 0.032737694680690765 Validation loss 0.04393310099840164 Accuracy 0.8808750510215759\n",
      "Iteration 3330 Training loss 0.028026530519127846 Validation loss 0.044026877731084824 Accuracy 0.8801250457763672\n",
      "Iteration 3340 Training loss 0.049057912081480026 Validation loss 0.05471451207995415 Accuracy 0.8470000624656677\n",
      "Iteration 3350 Training loss 0.037396352738142014 Validation loss 0.0441877581179142 Accuracy 0.8811250329017639\n",
      "Iteration 3360 Training loss 0.03152371570467949 Validation loss 0.0434446707367897 Accuracy 0.8817500472068787\n",
      "Iteration 3370 Training loss 0.03275711461901665 Validation loss 0.043593019247055054 Accuracy 0.8808750510215759\n",
      "Iteration 3380 Training loss 0.035032857209444046 Validation loss 0.04270004481077194 Accuracy 0.8820000290870667\n",
      "Iteration 3390 Training loss 0.04099857062101364 Validation loss 0.05098748952150345 Accuracy 0.8553750514984131\n",
      "Iteration 3400 Training loss 0.03518883138895035 Validation loss 0.044363368302583694 Accuracy 0.877500057220459\n",
      "Iteration 3410 Training loss 0.033738527446985245 Validation loss 0.04738229513168335 Accuracy 0.8708750605583191\n",
      "Iteration 3420 Training loss 0.031109046190977097 Validation loss 0.04474290832877159 Accuracy 0.877750039100647\n",
      "Iteration 3430 Training loss 0.0312089454382658 Validation loss 0.04354431480169296 Accuracy 0.8795000314712524\n",
      "Iteration 3440 Training loss 0.05803461745381355 Validation loss 0.062337808310985565 Accuracy 0.8346250653266907\n",
      "Iteration 3450 Training loss 0.03456629812717438 Validation loss 0.04402907192707062 Accuracy 0.8786250352859497\n",
      "Iteration 3460 Training loss 0.036047790199518204 Validation loss 0.04924263432621956 Accuracy 0.8661250472068787\n",
      "Iteration 3470 Training loss 0.03163270279765129 Validation loss 0.04435480758547783 Accuracy 0.878000020980835\n",
      "Iteration 3480 Training loss 0.03538566455245018 Validation loss 0.044833552092313766 Accuracy 0.8757500648498535\n",
      "Iteration 3490 Training loss 0.03070646896958351 Validation loss 0.04425499215722084 Accuracy 0.8760000467300415\n",
      "Iteration 3500 Training loss 0.04385538771748543 Validation loss 0.045404352247714996 Accuracy 0.8751250505447388\n",
      "Iteration 3510 Training loss 0.030209116637706757 Validation loss 0.04482656344771385 Accuracy 0.8791250586509705\n",
      "Iteration 3520 Training loss 0.03967956826090813 Validation loss 0.0517209991812706 Accuracy 0.8616250157356262\n",
      "Iteration 3530 Training loss 0.0301120076328516 Validation loss 0.044611405581235886 Accuracy 0.8790000677108765\n",
      "Iteration 3540 Training loss 0.043778933584690094 Validation loss 0.05041482672095299 Accuracy 0.8645000457763672\n",
      "Iteration 3550 Training loss 0.03368266671895981 Validation loss 0.0458628311753273 Accuracy 0.8758750557899475\n",
      "Iteration 3560 Training loss 0.030492085963487625 Validation loss 0.04632887244224548 Accuracy 0.874625027179718\n",
      "Iteration 3570 Training loss 0.04064447432756424 Validation loss 0.04971540719270706 Accuracy 0.8572500348091125\n",
      "Iteration 3580 Training loss 0.03878173232078552 Validation loss 0.051028214395046234 Accuracy 0.8578750491142273\n",
      "Iteration 3590 Training loss 0.056219395250082016 Validation loss 0.0536881722509861 Accuracy 0.8511250615119934\n",
      "Iteration 3600 Training loss 0.030887002125382423 Validation loss 0.0431470088660717 Accuracy 0.8832500576972961\n",
      "Iteration 3610 Training loss 0.033460043370723724 Validation loss 0.04514642804861069 Accuracy 0.8760000467300415\n",
      "Iteration 3620 Training loss 0.03582121431827545 Validation loss 0.043026361614465714 Accuracy 0.8835000395774841\n",
      "Iteration 3630 Training loss 0.03835352510213852 Validation loss 0.044232700020074844 Accuracy 0.8788750171661377\n",
      "Iteration 3640 Training loss 0.045721620321273804 Validation loss 0.05826443061232567 Accuracy 0.8412500619888306\n",
      "Iteration 3650 Training loss 0.03264755755662918 Validation loss 0.046317677944898605 Accuracy 0.8756250143051147\n",
      "Iteration 3660 Training loss 0.030388521030545235 Validation loss 0.04375777393579483 Accuracy 0.8792500495910645\n",
      "Iteration 3670 Training loss 0.04669461026787758 Validation loss 0.055578477680683136 Accuracy 0.8511250615119934\n",
      "Iteration 3680 Training loss 0.027044525370001793 Validation loss 0.042388662695884705 Accuracy 0.8847500681877136\n",
      "Iteration 3690 Training loss 0.03171068802475929 Validation loss 0.04354270547628403 Accuracy 0.8830000162124634\n",
      "Iteration 3700 Training loss 0.06048130989074707 Validation loss 0.06317947059869766 Accuracy 0.8248750567436218\n",
      "Iteration 3710 Training loss 0.029719602316617966 Validation loss 0.04420752078294754 Accuracy 0.8792500495910645\n",
      "Iteration 3720 Training loss 0.03696036711335182 Validation loss 0.048631712794303894 Accuracy 0.862125039100647\n",
      "Iteration 3730 Training loss 0.028939606621861458 Validation loss 0.04357074201107025 Accuracy 0.8791250586509705\n",
      "Iteration 3740 Training loss 0.02928299643099308 Validation loss 0.04296984151005745 Accuracy 0.8831250667572021\n",
      "Iteration 3750 Training loss 0.02673901990056038 Validation loss 0.04344681650400162 Accuracy 0.8818750381469727\n",
      "Iteration 3760 Training loss 0.035232048481702805 Validation loss 0.04677174612879753 Accuracy 0.8730000257492065\n",
      "Iteration 3770 Training loss 0.05501518025994301 Validation loss 0.056557685136795044 Accuracy 0.8481250405311584\n",
      "Iteration 3780 Training loss 0.025296863168478012 Validation loss 0.043217983096838 Accuracy 0.8817500472068787\n",
      "Iteration 3790 Training loss 0.033881135284900665 Validation loss 0.04534482955932617 Accuracy 0.8765000700950623\n",
      "Iteration 3800 Training loss 0.030407780781388283 Validation loss 0.04501955956220627 Accuracy 0.8782500624656677\n",
      "Iteration 3810 Training loss 0.025448443368077278 Validation loss 0.04256080090999603 Accuracy 0.8818750381469727\n",
      "Iteration 3820 Training loss 0.042132727801799774 Validation loss 0.058194950222969055 Accuracy 0.8451250195503235\n",
      "Iteration 3830 Training loss 0.029116110876202583 Validation loss 0.0434897281229496 Accuracy 0.8816250562667847\n",
      "Iteration 3840 Training loss 0.02909001335501671 Validation loss 0.04583117365837097 Accuracy 0.8755000233650208\n",
      "Iteration 3850 Training loss 0.03603462129831314 Validation loss 0.04361836612224579 Accuracy 0.8795000314712524\n",
      "Iteration 3860 Training loss 0.03336010500788689 Validation loss 0.04426630586385727 Accuracy 0.8786250352859497\n",
      "Iteration 3870 Training loss 0.026971101760864258 Validation loss 0.044953975826501846 Accuracy 0.8765000700950623\n",
      "Iteration 3880 Training loss 0.025448421016335487 Validation loss 0.043553948402404785 Accuracy 0.8810000419616699\n",
      "Iteration 3890 Training loss 0.03550530970096588 Validation loss 0.04361135512590408 Accuracy 0.8815000653266907\n",
      "Iteration 3900 Training loss 0.029103044420480728 Validation loss 0.04321587458252907 Accuracy 0.8812500238418579\n",
      "Iteration 3910 Training loss 0.037171315401792526 Validation loss 0.04559923708438873 Accuracy 0.8763750195503235\n",
      "Iteration 3920 Training loss 0.03400073200464249 Validation loss 0.0451226532459259 Accuracy 0.8748750686645508\n",
      "Iteration 3930 Training loss 0.03885072469711304 Validation loss 0.04957115277647972 Accuracy 0.8641250133514404\n",
      "Iteration 3940 Training loss 0.055025383830070496 Validation loss 0.058862850069999695 Accuracy 0.8418750166893005\n",
      "Iteration 3950 Training loss 0.03339116647839546 Validation loss 0.04298153147101402 Accuracy 0.8820000290870667\n",
      "Iteration 3960 Training loss 0.050415389239788055 Validation loss 0.05725729465484619 Accuracy 0.8415000438690186\n",
      "Iteration 3970 Training loss 0.027187971398234367 Validation loss 0.04264897480607033 Accuracy 0.8818750381469727\n",
      "Iteration 3980 Training loss 0.029303845018148422 Validation loss 0.04761818051338196 Accuracy 0.8697500228881836\n",
      "Iteration 3990 Training loss 0.03515603765845299 Validation loss 0.04939332976937294 Accuracy 0.8673750162124634\n",
      "Iteration 4000 Training loss 0.03171961382031441 Validation loss 0.045256294310092926 Accuracy 0.8760000467300415\n",
      "Iteration 4010 Training loss 0.021091165021061897 Validation loss 0.04354032501578331 Accuracy 0.8832500576972961\n",
      "Iteration 4020 Training loss 0.0249587744474411 Validation loss 0.04227174445986748 Accuracy 0.8835000395774841\n",
      "Iteration 4030 Training loss 0.02814767323434353 Validation loss 0.046494126319885254 Accuracy 0.8738750219345093\n",
      "Iteration 4040 Training loss 0.0352814607322216 Validation loss 0.04541723057627678 Accuracy 0.8756250143051147\n",
      "Iteration 4050 Training loss 0.030641209334135056 Validation loss 0.04224969819188118 Accuracy 0.8847500681877136\n",
      "Iteration 4060 Training loss 0.02764345146715641 Validation loss 0.04641013965010643 Accuracy 0.877500057220459\n",
      "Iteration 4070 Training loss 0.02495400793850422 Validation loss 0.045337874442338943 Accuracy 0.8766250610351562\n",
      "Iteration 4080 Training loss 0.02909713052213192 Validation loss 0.043051887303590775 Accuracy 0.8808750510215759\n",
      "Iteration 4090 Training loss 0.02948317863047123 Validation loss 0.044013362377882004 Accuracy 0.8803750276565552\n",
      "Iteration 4100 Training loss 0.027910960838198662 Validation loss 0.04387439787387848 Accuracy 0.877875030040741\n",
      "Iteration 4110 Training loss 0.029895640909671783 Validation loss 0.042489636689424515 Accuracy 0.8822500705718994\n",
      "Iteration 4120 Training loss 0.025394843891263008 Validation loss 0.04287199676036835 Accuracy 0.8827500343322754\n",
      "Iteration 4130 Training loss 0.04872751235961914 Validation loss 0.06028970703482628 Accuracy 0.8338750600814819\n",
      "Iteration 4140 Training loss 0.02935481071472168 Validation loss 0.04346952214837074 Accuracy 0.878125011920929\n",
      "Iteration 4150 Training loss 0.03058118000626564 Validation loss 0.045136887580156326 Accuracy 0.8730000257492065\n",
      "Iteration 4160 Training loss 0.023811081424355507 Validation loss 0.04530249536037445 Accuracy 0.877750039100647\n",
      "Iteration 4170 Training loss 0.026911361142992973 Validation loss 0.045626793056726456 Accuracy 0.874250054359436\n",
      "Iteration 4180 Training loss 0.030833721160888672 Validation loss 0.04816795513033867 Accuracy 0.8687500357627869\n",
      "Iteration 4190 Training loss 0.023675676435232162 Validation loss 0.041803862899541855 Accuracy 0.8845000267028809\n",
      "Iteration 4200 Training loss 0.04112790897488594 Validation loss 0.051288194954395294 Accuracy 0.8567500114440918\n",
      "Iteration 4210 Training loss 0.02750335820019245 Validation loss 0.04733459651470184 Accuracy 0.8691250681877136\n",
      "Iteration 4220 Training loss 0.026762263849377632 Validation loss 0.04327290505170822 Accuracy 0.8803750276565552\n",
      "Iteration 4230 Training loss 0.021703561767935753 Validation loss 0.04301685467362404 Accuracy 0.8825000524520874\n",
      "Iteration 4240 Training loss 0.03430991247296333 Validation loss 0.05013393983244896 Accuracy 0.8655000329017639\n",
      "Iteration 4250 Training loss 0.028583835810422897 Validation loss 0.04249304533004761 Accuracy 0.8826250433921814\n",
      "Iteration 4260 Training loss 0.02381168119609356 Validation loss 0.04354649409651756 Accuracy 0.8803750276565552\n",
      "Iteration 4270 Training loss 0.037809714674949646 Validation loss 0.050943583250045776 Accuracy 0.858500063419342\n",
      "Iteration 4280 Training loss 0.02874048240482807 Validation loss 0.042591020464897156 Accuracy 0.8857500553131104\n",
      "Iteration 4290 Training loss 0.025467080995440483 Validation loss 0.04546970874071121 Accuracy 0.8782500624656677\n",
      "Iteration 4300 Training loss 0.0296434685587883 Validation loss 0.04206939786672592 Accuracy 0.8850000500679016\n",
      "Iteration 4310 Training loss 0.04445643350481987 Validation loss 0.056330304592847824 Accuracy 0.8485000133514404\n",
      "Iteration 4320 Training loss 0.02317000739276409 Validation loss 0.04261885955929756 Accuracy 0.8845000267028809\n",
      "Iteration 4330 Training loss 0.027239713817834854 Validation loss 0.043999675661325455 Accuracy 0.8801250457763672\n",
      "Iteration 4340 Training loss 0.022181734442710876 Validation loss 0.042804211378097534 Accuracy 0.8832500576972961\n",
      "Iteration 4350 Training loss 0.024130934849381447 Validation loss 0.04404088854789734 Accuracy 0.8803750276565552\n",
      "Iteration 4360 Training loss 0.03738466650247574 Validation loss 0.048616789281368256 Accuracy 0.8655000329017639\n",
      "Iteration 4370 Training loss 0.0277297031134367 Validation loss 0.04284653440117836 Accuracy 0.8836250305175781\n",
      "Iteration 4380 Training loss 0.027719220146536827 Validation loss 0.04283178225159645 Accuracy 0.8812500238418579\n",
      "Iteration 4390 Training loss 0.028166236355900764 Validation loss 0.04277008771896362 Accuracy 0.8821250200271606\n",
      "Iteration 4400 Training loss 0.020829634740948677 Validation loss 0.04266727343201637 Accuracy 0.8806250691413879\n",
      "Iteration 4410 Training loss 0.0297992005944252 Validation loss 0.044431619346141815 Accuracy 0.8793750405311584\n",
      "Iteration 4420 Training loss 0.02660480886697769 Validation loss 0.045238934457302094 Accuracy 0.874625027179718\n",
      "Iteration 4430 Training loss 0.03314638137817383 Validation loss 0.04355517029762268 Accuracy 0.8806250691413879\n",
      "Iteration 4440 Training loss 0.021900467574596405 Validation loss 0.043021202087402344 Accuracy 0.8803750276565552\n",
      "Iteration 4450 Training loss 0.02335774339735508 Validation loss 0.04309127852320671 Accuracy 0.8821250200271606\n",
      "Iteration 4460 Training loss 0.0331244133412838 Validation loss 0.04843560978770256 Accuracy 0.8706250190734863\n",
      "Iteration 4470 Training loss 0.03359739109873772 Validation loss 0.04252425581216812 Accuracy 0.8845000267028809\n",
      "Iteration 4480 Training loss 0.02169530838727951 Validation loss 0.042736802250146866 Accuracy 0.8836250305175781\n",
      "Iteration 4490 Training loss 0.031120948493480682 Validation loss 0.04645000025629997 Accuracy 0.8751250505447388\n",
      "Iteration 4500 Training loss 0.025360656902194023 Validation loss 0.04423249885439873 Accuracy 0.8808750510215759\n",
      "Iteration 4510 Training loss 0.022357773035764694 Validation loss 0.04706640914082527 Accuracy 0.874750018119812\n",
      "Iteration 4520 Training loss 0.02758537232875824 Validation loss 0.04257672280073166 Accuracy 0.8860000371932983\n",
      "Iteration 4530 Training loss 0.03489888459444046 Validation loss 0.05479345843195915 Accuracy 0.8550000190734863\n",
      "Iteration 4540 Training loss 0.03179401159286499 Validation loss 0.04270010441541672 Accuracy 0.8823750615119934\n",
      "Iteration 4550 Training loss 0.03165076673030853 Validation loss 0.04580559581518173 Accuracy 0.8735000491142273\n",
      "Iteration 4560 Training loss 0.031785547733306885 Validation loss 0.049626730382442474 Accuracy 0.858875036239624\n",
      "Iteration 4570 Training loss 0.02806623838841915 Validation loss 0.042140159755945206 Accuracy 0.8846250176429749\n",
      "Iteration 4580 Training loss 0.03294313699007034 Validation loss 0.04908391088247299 Accuracy 0.8671250343322754\n",
      "Iteration 4590 Training loss 0.029691951349377632 Validation loss 0.044502172619104385 Accuracy 0.8810000419616699\n",
      "Iteration 4600 Training loss 0.03583504259586334 Validation loss 0.050489604473114014 Accuracy 0.862125039100647\n",
      "Iteration 4610 Training loss 0.025029979646205902 Validation loss 0.04247160255908966 Accuracy 0.8843750357627869\n",
      "Iteration 4620 Training loss 0.02722344733774662 Validation loss 0.044364579021930695 Accuracy 0.8800000548362732\n",
      "Iteration 4630 Training loss 0.027256149798631668 Validation loss 0.04748702421784401 Accuracy 0.8700000643730164\n",
      "Iteration 4640 Training loss 0.022535324096679688 Validation loss 0.043200839310884476 Accuracy 0.8833750486373901\n",
      "Iteration 4650 Training loss 0.03088041953742504 Validation loss 0.045793648809194565 Accuracy 0.8728750348091125\n",
      "Iteration 4660 Training loss 0.0224674791097641 Validation loss 0.044359978288412094 Accuracy 0.8812500238418579\n",
      "Iteration 4670 Training loss 0.01878643035888672 Validation loss 0.041821688413619995 Accuracy 0.8880000710487366\n",
      "Iteration 4680 Training loss 0.022852415218949318 Validation loss 0.04293806478381157 Accuracy 0.8846250176429749\n",
      "Iteration 4690 Training loss 0.024384960532188416 Validation loss 0.04245645925402641 Accuracy 0.8852500319480896\n",
      "Iteration 4700 Training loss 0.018253372982144356 Validation loss 0.041428595781326294 Accuracy 0.8871250152587891\n",
      "Iteration 4710 Training loss 0.04943164810538292 Validation loss 0.06447479873895645 Accuracy 0.8247500658035278\n",
      "Iteration 4720 Training loss 0.026987232267856598 Validation loss 0.04236330837011337 Accuracy 0.8828750252723694\n",
      "Iteration 4730 Training loss 0.037072766572237015 Validation loss 0.05476193502545357 Accuracy 0.8452500104904175\n",
      "Iteration 4740 Training loss 0.020850781351327896 Validation loss 0.04188072308897972 Accuracy 0.8851250410079956\n",
      "Iteration 4750 Training loss 0.019252097234129906 Validation loss 0.04137178137898445 Accuracy 0.8870000243186951\n",
      "Iteration 4760 Training loss 0.0230867862701416 Validation loss 0.044506367295980453 Accuracy 0.874750018119812\n",
      "Iteration 4770 Training loss 0.022683095186948776 Validation loss 0.043501704931259155 Accuracy 0.8816250562667847\n",
      "Iteration 4780 Training loss 0.03926395624876022 Validation loss 0.058163564652204514 Accuracy 0.8413750529289246\n",
      "Iteration 4790 Training loss 0.030572090297937393 Validation loss 0.04554596543312073 Accuracy 0.8791250586509705\n",
      "Iteration 4800 Training loss 0.040053434669971466 Validation loss 0.05516048148274422 Accuracy 0.8540000319480896\n",
      "Iteration 4810 Training loss 0.018809901550412178 Validation loss 0.04301934689283371 Accuracy 0.8831250667572021\n",
      "Iteration 4820 Training loss 0.02562331221997738 Validation loss 0.042650315910577774 Accuracy 0.8862500190734863\n",
      "Iteration 4830 Training loss 0.021198172122240067 Validation loss 0.044719550758600235 Accuracy 0.877625048160553\n",
      "Iteration 4840 Training loss 0.024528643116354942 Validation loss 0.050277527421712875 Accuracy 0.8646250367164612\n",
      "Iteration 4850 Training loss 0.03457881882786751 Validation loss 0.05078183859586716 Accuracy 0.8650000691413879\n",
      "Iteration 4860 Training loss 0.026812413707375526 Validation loss 0.04735969752073288 Accuracy 0.8708750605583191\n",
      "Iteration 4870 Training loss 0.030513552948832512 Validation loss 0.04486444219946861 Accuracy 0.8750000596046448\n",
      "Iteration 4880 Training loss 0.033909495919942856 Validation loss 0.05217067524790764 Accuracy 0.8563750386238098\n",
      "Iteration 4890 Training loss 0.025033030658960342 Validation loss 0.041422296315431595 Accuracy 0.8858750462532043\n",
      "Iteration 4900 Training loss 0.021515855565667152 Validation loss 0.043535470962524414 Accuracy 0.8830000162124634\n",
      "Iteration 4910 Training loss 0.019473634660243988 Validation loss 0.041566941887140274 Accuracy 0.8861250281333923\n",
      "Iteration 4920 Training loss 0.0215090773999691 Validation loss 0.042240288108587265 Accuracy 0.8855000138282776\n",
      "Iteration 4930 Training loss 0.020278243348002434 Validation loss 0.042732615023851395 Accuracy 0.8832500576972961\n",
      "Iteration 4940 Training loss 0.02085544541478157 Validation loss 0.04277470335364342 Accuracy 0.8820000290870667\n",
      "Iteration 4950 Training loss 0.024131281301379204 Validation loss 0.04176545888185501 Accuracy 0.8868750333786011\n",
      "Iteration 4960 Training loss 0.023243077099323273 Validation loss 0.04207484796643257 Accuracy 0.8855000138282776\n",
      "Iteration 4970 Training loss 0.035760585218667984 Validation loss 0.05032730847597122 Accuracy 0.8592500686645508\n",
      "Iteration 4980 Training loss 0.024029307067394257 Validation loss 0.041932620108127594 Accuracy 0.8867500424385071\n",
      "Iteration 4990 Training loss 0.024141015484929085 Validation loss 0.04338318854570389 Accuracy 0.8820000290870667\n",
      "Iteration 5000 Training loss 0.032475896179676056 Validation loss 0.046939387917518616 Accuracy 0.8737500309944153\n",
      "Iteration 5010 Training loss 0.017009669914841652 Validation loss 0.042380139231681824 Accuracy 0.8848750591278076\n",
      "Iteration 5020 Training loss 0.02314942516386509 Validation loss 0.04129485413432121 Accuracy 0.8882500529289246\n",
      "Iteration 5030 Training loss 0.022738732397556305 Validation loss 0.045459650456905365 Accuracy 0.8760000467300415\n",
      "Iteration 5040 Training loss 0.03411082550883293 Validation loss 0.057142473757267 Accuracy 0.8407500386238098\n",
      "Iteration 5050 Training loss 0.01903601549565792 Validation loss 0.04309163987636566 Accuracy 0.8850000500679016\n",
      "Iteration 5060 Training loss 0.028993288055062294 Validation loss 0.04373873397707939 Accuracy 0.8791250586509705\n",
      "Iteration 5070 Training loss 0.020212573930621147 Validation loss 0.0436638668179512 Accuracy 0.877875030040741\n",
      "Iteration 5080 Training loss 0.01856696978211403 Validation loss 0.04255059361457825 Accuracy 0.8842500448226929\n",
      "Iteration 5090 Training loss 0.027716871351003647 Validation loss 0.04389908164739609 Accuracy 0.8825000524520874\n",
      "Iteration 5100 Training loss 0.027287278324365616 Validation loss 0.04270336776971817 Accuracy 0.8857500553131104\n",
      "Iteration 5110 Training loss 0.02696852572262287 Validation loss 0.04124582186341286 Accuracy 0.8853750228881836\n",
      "Iteration 5120 Training loss 0.050673410296440125 Validation loss 0.06687948852777481 Accuracy 0.8170000314712524\n",
      "Iteration 5130 Training loss 0.02198236621916294 Validation loss 0.044116612523794174 Accuracy 0.8768750429153442\n",
      "Iteration 5140 Training loss 0.020016362890601158 Validation loss 0.042672574520111084 Accuracy 0.8827500343322754\n",
      "Iteration 5150 Training loss 0.017562517896294594 Validation loss 0.04158902168273926 Accuracy 0.8877500295639038\n",
      "Iteration 5160 Training loss 0.020404426380991936 Validation loss 0.04302390292286873 Accuracy 0.8810000419616699\n",
      "Iteration 5170 Training loss 0.0457240492105484 Validation loss 0.0567726269364357 Accuracy 0.8506250381469727\n",
      "Iteration 5180 Training loss 0.019723471254110336 Validation loss 0.04162047058343887 Accuracy 0.8863750696182251\n",
      "Iteration 5190 Training loss 0.019168540835380554 Validation loss 0.041906632483005524 Accuracy 0.8855000138282776\n",
      "Iteration 5200 Training loss 0.022863319143652916 Validation loss 0.05041494220495224 Accuracy 0.8630000352859497\n",
      "Iteration 5210 Training loss 0.018939698114991188 Validation loss 0.04183175787329674 Accuracy 0.8880000710487366\n",
      "Iteration 5220 Training loss 0.022351941093802452 Validation loss 0.04164135083556175 Accuracy 0.8887500166893005\n",
      "Iteration 5230 Training loss 0.029544943943619728 Validation loss 0.05273084342479706 Accuracy 0.8523750305175781\n",
      "Iteration 5240 Training loss 0.026891831308603287 Validation loss 0.051838092505931854 Accuracy 0.859000027179718\n",
      "Iteration 5250 Training loss 0.02513951249420643 Validation loss 0.044395480304956436 Accuracy 0.8807500600814819\n",
      "Iteration 5260 Training loss 0.019282007589936256 Validation loss 0.04177306219935417 Accuracy 0.8861250281333923\n",
      "Iteration 5270 Training loss 0.03892090171575546 Validation loss 0.054612208157777786 Accuracy 0.8492500185966492\n",
      "Iteration 5280 Training loss 0.021291030570864677 Validation loss 0.04295894876122475 Accuracy 0.8813750147819519\n",
      "Iteration 5290 Training loss 0.027202704921364784 Validation loss 0.04721175506711006 Accuracy 0.8703750371932983\n",
      "Iteration 5300 Training loss 0.016379205510020256 Validation loss 0.04150378331542015 Accuracy 0.8871250152587891\n",
      "Iteration 5310 Training loss 0.019211452454328537 Validation loss 0.04136189818382263 Accuracy 0.8861250281333923\n",
      "Iteration 5320 Training loss 0.02288917638361454 Validation loss 0.04418332874774933 Accuracy 0.8783750534057617\n",
      "Iteration 5330 Training loss 0.02653135173022747 Validation loss 0.04554881528019905 Accuracy 0.8737500309944153\n",
      "Iteration 5340 Training loss 0.015678290277719498 Validation loss 0.04233740270137787 Accuracy 0.8842500448226929\n",
      "Iteration 5350 Training loss 0.025293532758951187 Validation loss 0.042260751128196716 Accuracy 0.8836250305175781\n",
      "Iteration 5360 Training loss 0.02164851315319538 Validation loss 0.0433347262442112 Accuracy 0.8783750534057617\n",
      "Iteration 5370 Training loss 0.028968321159482002 Validation loss 0.04781534895300865 Accuracy 0.8707500696182251\n",
      "Iteration 5380 Training loss 0.01996389776468277 Validation loss 0.04216170683503151 Accuracy 0.8862500190734863\n",
      "Iteration 5390 Training loss 0.02306428551673889 Validation loss 0.04203442484140396 Accuracy 0.8832500576972961\n",
      "Iteration 5400 Training loss 0.021568235009908676 Validation loss 0.04187258332967758 Accuracy 0.8857500553131104\n",
      "Iteration 5410 Training loss 0.04372768849134445 Validation loss 0.05922617018222809 Accuracy 0.8443750143051147\n",
      "Iteration 5420 Training loss 0.016575729474425316 Validation loss 0.041021279990673065 Accuracy 0.8875000476837158\n",
      "Iteration 5430 Training loss 0.02053208276629448 Validation loss 0.0423654280602932 Accuracy 0.8838750123977661\n",
      "Iteration 5440 Training loss 0.020384840667247772 Validation loss 0.04578981176018715 Accuracy 0.8772500157356262\n",
      "Iteration 5450 Training loss 0.021738843992352486 Validation loss 0.04569211229681969 Accuracy 0.8766250610351562\n",
      "Iteration 5460 Training loss 0.0248821210116148 Validation loss 0.04452336207032204 Accuracy 0.877750039100647\n",
      "Iteration 5470 Training loss 0.020208463072776794 Validation loss 0.04208486154675484 Accuracy 0.8850000500679016\n",
      "Iteration 5480 Training loss 0.016000384464859962 Validation loss 0.0416581816971302 Accuracy 0.8881250619888306\n",
      "Iteration 5490 Training loss 0.01917877420783043 Validation loss 0.04157722368836403 Accuracy 0.8876250386238098\n",
      "Iteration 5500 Training loss 0.014718780294060707 Validation loss 0.04454251378774643 Accuracy 0.877625048160553\n",
      "Iteration 5510 Training loss 0.014737628400325775 Validation loss 0.04403802007436752 Accuracy 0.8815000653266907\n",
      "Iteration 5520 Training loss 0.019160320982336998 Validation loss 0.04148361459374428 Accuracy 0.89000004529953\n",
      "Iteration 5530 Training loss 0.01505016814917326 Validation loss 0.04125553369522095 Accuracy 0.8893750309944153\n",
      "Iteration 5540 Training loss 0.03156914561986923 Validation loss 0.053460534662008286 Accuracy 0.8508750200271606\n",
      "Iteration 5550 Training loss 0.029807917773723602 Validation loss 0.041307542473077774 Accuracy 0.890375018119812\n",
      "Iteration 5560 Training loss 0.01585359498858452 Validation loss 0.041650235652923584 Accuracy 0.8878750205039978\n",
      "Iteration 5570 Training loss 0.02606596238911152 Validation loss 0.04500425234436989 Accuracy 0.877375066280365\n",
      "Iteration 5580 Training loss 0.016554702073335648 Validation loss 0.04202592000365257 Accuracy 0.8847500681877136\n",
      "Iteration 5590 Training loss 0.01640279032289982 Validation loss 0.04142823442816734 Accuracy 0.8875000476837158\n",
      "Iteration 5600 Training loss 0.023901671171188354 Validation loss 0.05443006381392479 Accuracy 0.8536250591278076\n",
      "Iteration 5610 Training loss 0.022398481145501137 Validation loss 0.042270686477422714 Accuracy 0.8857500553131104\n",
      "Iteration 5620 Training loss 0.026780031621456146 Validation loss 0.05636157467961311 Accuracy 0.8497500419616699\n",
      "Iteration 5630 Training loss 0.016486767679452896 Validation loss 0.041621722280979156 Accuracy 0.8865000605583191\n",
      "Iteration 5640 Training loss 0.022375578060746193 Validation loss 0.04204804450273514 Accuracy 0.8845000267028809\n",
      "Iteration 5650 Training loss 0.0308622308075428 Validation loss 0.050921518355607986 Accuracy 0.8603750467300415\n",
      "Iteration 5660 Training loss 0.020784635096788406 Validation loss 0.04129244387149811 Accuracy 0.8848750591278076\n",
      "Iteration 5670 Training loss 0.01765676774084568 Validation loss 0.041037485003471375 Accuracy 0.8880000710487366\n",
      "Iteration 5680 Training loss 0.021880963817238808 Validation loss 0.042144957929849625 Accuracy 0.8846250176429749\n",
      "Iteration 5690 Training loss 0.015061742626130581 Validation loss 0.042425546795129776 Accuracy 0.8842500448226929\n",
      "Iteration 5700 Training loss 0.015617192722856998 Validation loss 0.04192453995347023 Accuracy 0.8853750228881836\n",
      "Iteration 5710 Training loss 0.0198400616645813 Validation loss 0.04595789685845375 Accuracy 0.8752500414848328\n",
      "Iteration 5720 Training loss 0.019128739833831787 Validation loss 0.04231729730963707 Accuracy 0.8870000243186951\n",
      "Iteration 5730 Training loss 0.023273026570677757 Validation loss 0.04445767402648926 Accuracy 0.8803750276565552\n",
      "Iteration 5740 Training loss 0.01997038535773754 Validation loss 0.041173867881298065 Accuracy 0.8890000581741333\n",
      "Iteration 5750 Training loss 0.022685805335640907 Validation loss 0.04530468210577965 Accuracy 0.8762500286102295\n",
      "Iteration 5760 Training loss 0.020002368837594986 Validation loss 0.04123126342892647 Accuracy 0.8890000581741333\n",
      "Iteration 5770 Training loss 0.04086661338806152 Validation loss 0.06375323981046677 Accuracy 0.827250063419342\n",
      "Iteration 5780 Training loss 0.029896698892116547 Validation loss 0.04701151326298714 Accuracy 0.8697500228881836\n",
      "Iteration 5790 Training loss 0.012719745747745037 Validation loss 0.041131142526865005 Accuracy 0.8905000686645508\n",
      "Iteration 5800 Training loss 0.012788258492946625 Validation loss 0.04238466918468475 Accuracy 0.8866250514984131\n",
      "Iteration 5810 Training loss 0.017410848289728165 Validation loss 0.041632890701293945 Accuracy 0.8868750333786011\n",
      "Iteration 5820 Training loss 0.013977715745568275 Validation loss 0.041410181671381 Accuracy 0.8873750567436218\n",
      "Iteration 5830 Training loss 0.02861202135682106 Validation loss 0.051093850284814835 Accuracy 0.8651250600814819\n",
      "Iteration 5840 Training loss 0.01337172370404005 Validation loss 0.04148144647479057 Accuracy 0.8865000605583191\n",
      "Iteration 5850 Training loss 0.022462472319602966 Validation loss 0.043849315494298935 Accuracy 0.8806250691413879\n",
      "Iteration 5860 Training loss 0.028110327199101448 Validation loss 0.05517524853348732 Accuracy 0.8487500548362732\n",
      "Iteration 5870 Training loss 0.020903324708342552 Validation loss 0.04197978228330612 Accuracy 0.8876250386238098\n",
      "Iteration 5880 Training loss 0.023431595414876938 Validation loss 0.04145629331469536 Accuracy 0.8858750462532043\n",
      "Iteration 5890 Training loss 0.01681661419570446 Validation loss 0.044467005878686905 Accuracy 0.8817500472068787\n",
      "Iteration 5900 Training loss 0.023902203887701035 Validation loss 0.049498263746500015 Accuracy 0.8680000305175781\n",
      "Iteration 5910 Training loss 0.01367819495499134 Validation loss 0.041155245155096054 Accuracy 0.8885000348091125\n",
      "Iteration 5920 Training loss 0.017098836600780487 Validation loss 0.04136653244495392 Accuracy 0.8885000348091125\n",
      "Iteration 5930 Training loss 0.032137833535671234 Validation loss 0.052709925919771194 Accuracy 0.8632500171661377\n",
      "Iteration 5940 Training loss 0.01547551155090332 Validation loss 0.04173465073108673 Accuracy 0.8886250257492065\n",
      "Iteration 5950 Training loss 0.015016419813036919 Validation loss 0.04153286665678024 Accuracy 0.8878750205039978\n",
      "Iteration 5960 Training loss 0.019660301506519318 Validation loss 0.04298500344157219 Accuracy 0.8818750381469727\n",
      "Iteration 5970 Training loss 0.057327259331941605 Validation loss 0.0610487163066864 Accuracy 0.8440000414848328\n",
      "Iteration 5980 Training loss 0.01553870365023613 Validation loss 0.0417151115834713 Accuracy 0.8870000243186951\n",
      "Iteration 5990 Training loss 0.01922755129635334 Validation loss 0.04243256896734238 Accuracy 0.8857500553131104\n",
      "Iteration 6000 Training loss 0.017266876995563507 Validation loss 0.042680107057094574 Accuracy 0.8850000500679016\n",
      "Iteration 6010 Training loss 0.024474449455738068 Validation loss 0.04258061572909355 Accuracy 0.8836250305175781\n",
      "Iteration 6020 Training loss 0.01780194789171219 Validation loss 0.04111649841070175 Accuracy 0.8886250257492065\n",
      "Iteration 6030 Training loss 0.05751413106918335 Validation loss 0.07441768050193787 Accuracy 0.8138750195503235\n",
      "Iteration 6040 Training loss 0.021292319521307945 Validation loss 0.041677363216876984 Accuracy 0.8856250643730164\n",
      "Iteration 6050 Training loss 0.01604619435966015 Validation loss 0.04167966544628143 Accuracy 0.8890000581741333\n",
      "Iteration 6060 Training loss 0.01895144209265709 Validation loss 0.0428943857550621 Accuracy 0.8843750357627869\n",
      "Iteration 6070 Training loss 0.044517505913972855 Validation loss 0.06155260652303696 Accuracy 0.8357500433921814\n",
      "Iteration 6080 Training loss 0.02671712078154087 Validation loss 0.04335761442780495 Accuracy 0.8820000290870667\n",
      "Iteration 6090 Training loss 0.01190276350826025 Validation loss 0.04156285151839256 Accuracy 0.8873750567436218\n",
      "Iteration 6100 Training loss 0.017196284607052803 Validation loss 0.04236710071563721 Accuracy 0.8856250643730164\n",
      "Iteration 6110 Training loss 0.019997451454401016 Validation loss 0.04320117458701134 Accuracy 0.8825000524520874\n",
      "Iteration 6120 Training loss 0.01642773672938347 Validation loss 0.0413687638938427 Accuracy 0.8887500166893005\n",
      "Iteration 6130 Training loss 0.023227453231811523 Validation loss 0.044123660773038864 Accuracy 0.8808750510215759\n",
      "Iteration 6140 Training loss 0.014360267668962479 Validation loss 0.042172010987997055 Accuracy 0.8868750333786011\n",
      "Iteration 6150 Training loss 0.01779937744140625 Validation loss 0.043594345450401306 Accuracy 0.8806250691413879\n",
      "Iteration 6160 Training loss 0.032402947545051575 Validation loss 0.053560394793748856 Accuracy 0.8561250567436218\n",
      "Iteration 6170 Training loss 0.019021784886717796 Validation loss 0.04211743548512459 Accuracy 0.8882500529289246\n",
      "Iteration 6180 Training loss 0.010051224380731583 Validation loss 0.04263189062476158 Accuracy 0.8840000629425049\n",
      "Iteration 6190 Training loss 0.013405527919530869 Validation loss 0.04180262237787247 Accuracy 0.8851250410079956\n",
      "Iteration 6200 Training loss 0.0159121323376894 Validation loss 0.043997567147016525 Accuracy 0.8825000524520874\n",
      "Iteration 6210 Training loss 0.01735924929380417 Validation loss 0.04186461865901947 Accuracy 0.8878750205039978\n",
      "Iteration 6220 Training loss 0.022046837955713272 Validation loss 0.04308179020881653 Accuracy 0.8818750381469727\n",
      "Iteration 6230 Training loss 0.010854559019207954 Validation loss 0.04151030257344246 Accuracy 0.8860000371932983\n",
      "Iteration 6240 Training loss 0.010936185717582703 Validation loss 0.042542893439531326 Accuracy 0.8836250305175781\n",
      "Iteration 6250 Training loss 0.07244601845741272 Validation loss 0.0786166563630104 Accuracy 0.8052500486373901\n",
      "Iteration 6260 Training loss 0.01589561253786087 Validation loss 0.0416090302169323 Accuracy 0.8868750333786011\n",
      "Iteration 6270 Training loss 0.01987980492413044 Validation loss 0.0456107035279274 Accuracy 0.8783750534057617\n",
      "Iteration 6280 Training loss 0.021309129893779755 Validation loss 0.0418633297085762 Accuracy 0.8890000581741333\n",
      "Iteration 6290 Training loss 0.03242680802941322 Validation loss 0.05444684997200966 Accuracy 0.8562500476837158\n",
      "Iteration 6300 Training loss 0.014130474999547005 Validation loss 0.041339579969644547 Accuracy 0.8883750438690186\n",
      "Iteration 6310 Training loss 0.018126146867871284 Validation loss 0.04400097206234932 Accuracy 0.8813750147819519\n",
      "Iteration 6320 Training loss 0.021016474813222885 Validation loss 0.04199470952153206 Accuracy 0.8850000500679016\n",
      "Iteration 6330 Training loss 0.020160559564828873 Validation loss 0.04331520199775696 Accuracy 0.8825000524520874\n",
      "Iteration 6340 Training loss 0.018466578796505928 Validation loss 0.04468245059251785 Accuracy 0.877625048160553\n",
      "Iteration 6350 Training loss 0.011872357688844204 Validation loss 0.04221244528889656 Accuracy 0.8852500319480896\n",
      "Iteration 6360 Training loss 0.03510268032550812 Validation loss 0.05690794810652733 Accuracy 0.846625030040741\n",
      "Iteration 6370 Training loss 0.014364955015480518 Validation loss 0.04202057793736458 Accuracy 0.8851250410079956\n",
      "Iteration 6380 Training loss 0.016728131100535393 Validation loss 0.04473205655813217 Accuracy 0.877750039100647\n",
      "Iteration 6390 Training loss 0.018228506669402122 Validation loss 0.041693031787872314 Accuracy 0.8885000348091125\n",
      "Iteration 6400 Training loss 0.01699371263384819 Validation loss 0.0428185760974884 Accuracy 0.8853750228881836\n",
      "Iteration 6410 Training loss 0.03225652500987053 Validation loss 0.050084833055734634 Accuracy 0.8691250681877136\n",
      "Iteration 6420 Training loss 0.01441908534616232 Validation loss 0.04148869588971138 Accuracy 0.8863750696182251\n",
      "Iteration 6430 Training loss 0.017268981784582138 Validation loss 0.0444457046687603 Accuracy 0.8816250562667847\n",
      "Iteration 6440 Training loss 0.015669815242290497 Validation loss 0.04337824508547783 Accuracy 0.8825000524520874\n",
      "Iteration 6450 Training loss 0.027978666126728058 Validation loss 0.05363733321428299 Accuracy 0.8577500581741333\n",
      "Iteration 6460 Training loss 0.011781034991145134 Validation loss 0.0411502905189991 Accuracy 0.8890000581741333\n",
      "Iteration 6470 Training loss 0.01835026778280735 Validation loss 0.044588811695575714 Accuracy 0.8756250143051147\n",
      "Iteration 6480 Training loss 0.02693242020905018 Validation loss 0.0475497767329216 Accuracy 0.874250054359436\n",
      "Iteration 6490 Training loss 0.012392186559736729 Validation loss 0.04244685173034668 Accuracy 0.8851250410079956\n",
      "Iteration 6500 Training loss 0.02538919821381569 Validation loss 0.05078217014670372 Accuracy 0.8611250519752502\n",
      "Iteration 6510 Training loss 0.01618409901857376 Validation loss 0.04202800244092941 Accuracy 0.8822500705718994\n",
      "Iteration 6520 Training loss 0.014059651643037796 Validation loss 0.042527422308921814 Accuracy 0.8845000267028809\n",
      "Iteration 6530 Training loss 0.012205151841044426 Validation loss 0.04308722913265228 Accuracy 0.8823750615119934\n",
      "Iteration 6540 Training loss 0.035879284143447876 Validation loss 0.058884717524051666 Accuracy 0.8455000519752502\n",
      "Iteration 6550 Training loss 0.015235703438520432 Validation loss 0.04252878949046135 Accuracy 0.8852500319480896\n",
      "Iteration 6560 Training loss 0.02288403920829296 Validation loss 0.04422609135508537 Accuracy 0.8816250562667847\n",
      "Iteration 6570 Training loss 0.02232491411268711 Validation loss 0.04430943354964256 Accuracy 0.8822500705718994\n",
      "Iteration 6580 Training loss 0.017012156546115875 Validation loss 0.04149893298745155 Accuracy 0.8896250128746033\n",
      "Iteration 6590 Training loss 0.0161567535251379 Validation loss 0.04146889969706535 Accuracy 0.8891250491142273\n",
      "Iteration 6600 Training loss 0.04258900135755539 Validation loss 0.06637126207351685 Accuracy 0.831000030040741\n",
      "Iteration 6610 Training loss 0.015258057042956352 Validation loss 0.041465312242507935 Accuracy 0.8872500658035278\n",
      "Iteration 6620 Training loss 0.011384449899196625 Validation loss 0.04191087558865547 Accuracy 0.8878750205039978\n",
      "Iteration 6630 Training loss 0.01544811949133873 Validation loss 0.04257098212838173 Accuracy 0.8870000243186951\n",
      "Iteration 6640 Training loss 0.030426735058426857 Validation loss 0.05688346549868584 Accuracy 0.8422500491142273\n",
      "Iteration 6650 Training loss 0.013109592720866203 Validation loss 0.04127278923988342 Accuracy 0.889875054359436\n",
      "Iteration 6660 Training loss 0.011282258667051792 Validation loss 0.0412997230887413 Accuracy 0.889875054359436\n",
      "Iteration 6670 Training loss 0.014069127850234509 Validation loss 0.04167591780424118 Accuracy 0.8882500529289246\n",
      "Iteration 6680 Training loss 0.02934461459517479 Validation loss 0.058148644864559174 Accuracy 0.8448750376701355\n",
      "Iteration 6690 Training loss 0.020943202078342438 Validation loss 0.04574025794863701 Accuracy 0.8761250376701355\n",
      "Iteration 6700 Training loss 0.01634134352207184 Validation loss 0.042579200118780136 Accuracy 0.8848750591278076\n",
      "Iteration 6710 Training loss 0.018688896670937538 Validation loss 0.044214632362127304 Accuracy 0.8813750147819519\n",
      "Iteration 6720 Training loss 0.01623949408531189 Validation loss 0.041865210980176926 Accuracy 0.8887500166893005\n",
      "Iteration 6730 Training loss 0.013239010237157345 Validation loss 0.042369719594717026 Accuracy 0.8870000243186951\n",
      "Iteration 6740 Training loss 0.013069425709545612 Validation loss 0.04295748472213745 Accuracy 0.8852500319480896\n",
      "Iteration 6750 Training loss 0.024330075830221176 Validation loss 0.046883899718523026 Accuracy 0.8758750557899475\n",
      "Iteration 6760 Training loss 0.01163415890187025 Validation loss 0.042719218879938126 Accuracy 0.8832500576972961\n",
      "Iteration 6770 Training loss 0.01089476514607668 Validation loss 0.04148174449801445 Accuracy 0.8893750309944153\n",
      "Iteration 6780 Training loss 0.013903249986469746 Validation loss 0.04471215978264809 Accuracy 0.8788750171661377\n",
      "Iteration 6790 Training loss 0.04027038440108299 Validation loss 0.05680488795042038 Accuracy 0.8541250228881836\n",
      "Iteration 6800 Training loss 0.009381596930325031 Validation loss 0.04138626158237457 Accuracy 0.890250027179718\n",
      "Iteration 6810 Training loss 0.011022309772670269 Validation loss 0.04125581309199333 Accuracy 0.8896250128746033\n",
      "Iteration 6820 Training loss 0.014481612481176853 Validation loss 0.04325934872031212 Accuracy 0.8845000267028809\n",
      "Iteration 6830 Training loss 0.011921376921236515 Validation loss 0.041250161826610565 Accuracy 0.890375018119812\n",
      "Iteration 6840 Training loss 0.014836742542684078 Validation loss 0.04423738643527031 Accuracy 0.8805000185966492\n",
      "Iteration 6850 Training loss 0.03198697790503502 Validation loss 0.05724486708641052 Accuracy 0.8477500677108765\n",
      "Iteration 6860 Training loss 0.013341504149138927 Validation loss 0.04155926778912544 Accuracy 0.8873750567436218\n",
      "Iteration 6870 Training loss 0.010824799537658691 Validation loss 0.04224228113889694 Accuracy 0.8877500295639038\n",
      "Iteration 6880 Training loss 0.011783947236835957 Validation loss 0.04202478006482124 Accuracy 0.8863750696182251\n",
      "Iteration 6890 Training loss 0.01859613135457039 Validation loss 0.04949694126844406 Accuracy 0.8713750243186951\n",
      "Iteration 6900 Training loss 0.04387867450714111 Validation loss 0.06449652463197708 Accuracy 0.8336250185966492\n",
      "Iteration 6910 Training loss 0.014900901354849339 Validation loss 0.04336593672633171 Accuracy 0.8828750252723694\n",
      "Iteration 6920 Training loss 0.01653267629444599 Validation loss 0.04160502552986145 Accuracy 0.8876250386238098\n",
      "Iteration 6930 Training loss 0.014574592001736164 Validation loss 0.041926026344299316 Accuracy 0.8867500424385071\n",
      "Iteration 6940 Training loss 0.020875772461295128 Validation loss 0.04814858362078667 Accuracy 0.8730000257492065\n",
      "Iteration 6950 Training loss 0.015056075528264046 Validation loss 0.046438202261924744 Accuracy 0.8767500519752502\n",
      "Iteration 6960 Training loss 0.012542394921183586 Validation loss 0.041849490255117416 Accuracy 0.8876250386238098\n",
      "Iteration 6970 Training loss 0.012598643079400063 Validation loss 0.04122720658779144 Accuracy 0.8896250128746033\n",
      "Iteration 6980 Training loss 0.010429637506604195 Validation loss 0.04188369959592819 Accuracy 0.8868750333786011\n",
      "Iteration 6990 Training loss 0.03931738808751106 Validation loss 0.055744074285030365 Accuracy 0.8525000214576721\n",
      "Iteration 7000 Training loss 0.01084940042346716 Validation loss 0.04203307628631592 Accuracy 0.8856250643730164\n",
      "Iteration 7010 Training loss 0.011061469092965126 Validation loss 0.04192343354225159 Accuracy 0.8855000138282776\n",
      "Iteration 7020 Training loss 0.008418207988142967 Validation loss 0.041603945195674896 Accuracy 0.8883750438690186\n",
      "Iteration 7030 Training loss 0.01134470198303461 Validation loss 0.04227977991104126 Accuracy 0.8858750462532043\n",
      "Iteration 7040 Training loss 0.014410225674510002 Validation loss 0.04745916277170181 Accuracy 0.874750018119812\n",
      "Iteration 7050 Training loss 0.011856958270072937 Validation loss 0.04370732232928276 Accuracy 0.8837500214576721\n",
      "Iteration 7060 Training loss 0.011026916094124317 Validation loss 0.041252702474594116 Accuracy 0.8891250491142273\n",
      "Iteration 7070 Training loss 0.013922042213380337 Validation loss 0.041319672018289566 Accuracy 0.8887500166893005\n",
      "Iteration 7080 Training loss 0.011739697307348251 Validation loss 0.041610293090343475 Accuracy 0.889750063419342\n",
      "Iteration 7090 Training loss 0.056284330785274506 Validation loss 0.07560117542743683 Accuracy 0.8046250343322754\n",
      "Iteration 7100 Training loss 0.017705220729112625 Validation loss 0.042254917323589325 Accuracy 0.8881250619888306\n",
      "Iteration 7110 Training loss 0.014173335395753384 Validation loss 0.04094822332262993 Accuracy 0.8906250596046448\n",
      "Iteration 7120 Training loss 0.011548405513167381 Validation loss 0.0413670688867569 Accuracy 0.890375018119812\n",
      "Iteration 7130 Training loss 0.014404269866645336 Validation loss 0.0412672758102417 Accuracy 0.8906250596046448\n",
      "Iteration 7140 Training loss 0.011310088448226452 Validation loss 0.04161302372813225 Accuracy 0.89000004529953\n",
      "Iteration 7150 Training loss 0.011391257867217064 Validation loss 0.042908746749162674 Accuracy 0.8845000267028809\n",
      "Iteration 7160 Training loss 0.007062900345772505 Validation loss 0.04197702184319496 Accuracy 0.8883750438690186\n",
      "Iteration 7170 Training loss 0.008311042562127113 Validation loss 0.0411517433822155 Accuracy 0.890375018119812\n",
      "Iteration 7180 Training loss 0.01894194819033146 Validation loss 0.04701129347085953 Accuracy 0.8733750581741333\n",
      "Iteration 7190 Training loss 0.031036991626024246 Validation loss 0.052409764379262924 Accuracy 0.8593750596046448\n",
      "Iteration 7200 Training loss 0.009922686964273453 Validation loss 0.0417790412902832 Accuracy 0.8850000500679016\n",
      "Iteration 7210 Training loss 0.010105078108608723 Validation loss 0.04108906537294388 Accuracy 0.889875054359436\n",
      "Iteration 7220 Training loss 0.016202354803681374 Validation loss 0.04637230187654495 Accuracy 0.8756250143051147\n",
      "Iteration 7230 Training loss 0.01699577271938324 Validation loss 0.04190821573138237 Accuracy 0.8863750696182251\n",
      "Iteration 7240 Training loss 0.011835499666631222 Validation loss 0.041454773396253586 Accuracy 0.8907500505447388\n",
      "Iteration 7250 Training loss 0.049124620854854584 Validation loss 0.06168249621987343 Accuracy 0.8436250686645508\n",
      "Iteration 7260 Training loss 0.013825573958456516 Validation loss 0.04319459944963455 Accuracy 0.8846250176429749\n",
      "Iteration 7270 Training loss 0.00927998311817646 Validation loss 0.04163578897714615 Accuracy 0.8871250152587891\n",
      "Iteration 7280 Training loss 0.01310260221362114 Validation loss 0.04206056520342827 Accuracy 0.8863750696182251\n",
      "Iteration 7290 Training loss 0.011051476001739502 Validation loss 0.04404127597808838 Accuracy 0.8832500576972961\n",
      "Iteration 7300 Training loss 0.0183097030967474 Validation loss 0.04972948133945465 Accuracy 0.8700000643730164\n",
      "Iteration 7310 Training loss 0.010924207977950573 Validation loss 0.041387755423784256 Accuracy 0.8873750567436218\n",
      "Iteration 7320 Training loss 0.0120476633310318 Validation loss 0.04193124920129776 Accuracy 0.8857500553131104\n",
      "Iteration 7330 Training loss 0.01218775287270546 Validation loss 0.041646845638751984 Accuracy 0.8863750696182251\n",
      "Iteration 7340 Training loss 0.008205553516745567 Validation loss 0.041890308260917664 Accuracy 0.8867500424385071\n",
      "Iteration 7350 Training loss 0.02926045097410679 Validation loss 0.0610908642411232 Accuracy 0.8361250162124634\n",
      "Iteration 7360 Training loss 0.01667110249400139 Validation loss 0.04351836442947388 Accuracy 0.8817500472068787\n",
      "Iteration 7370 Training loss 0.014960202388465405 Validation loss 0.041886866092681885 Accuracy 0.8886250257492065\n",
      "Iteration 7380 Training loss 0.010746832937002182 Validation loss 0.042715076357126236 Accuracy 0.8858750462532043\n",
      "Iteration 7390 Training loss 0.021288733929395676 Validation loss 0.044954776763916016 Accuracy 0.8810000419616699\n",
      "Iteration 7400 Training loss 0.00814065895974636 Validation loss 0.042096514254808426 Accuracy 0.8861250281333923\n",
      "Iteration 7410 Training loss 0.013444547541439533 Validation loss 0.04203695058822632 Accuracy 0.8878750205039978\n",
      "Iteration 7420 Training loss 0.020636823028326035 Validation loss 0.04624967277050018 Accuracy 0.8760000467300415\n",
      "Iteration 7430 Training loss 0.009895209223031998 Validation loss 0.042174119502305984 Accuracy 0.8836250305175781\n",
      "Iteration 7440 Training loss 0.012857453897595406 Validation loss 0.0415346696972847 Accuracy 0.8865000605583191\n",
      "Iteration 7450 Training loss 0.009705915115773678 Validation loss 0.04178399592638016 Accuracy 0.8881250619888306\n",
      "Iteration 7460 Training loss 0.013677935115993023 Validation loss 0.041660126298666 Accuracy 0.8881250619888306\n",
      "Iteration 7470 Training loss 0.01810586452484131 Validation loss 0.04573805630207062 Accuracy 0.8800000548362732\n",
      "Iteration 7480 Training loss 0.007055140100419521 Validation loss 0.041540347039699554 Accuracy 0.8907500505447388\n",
      "Iteration 7490 Training loss 0.01452613528817892 Validation loss 0.04158942401409149 Accuracy 0.8880000710487366\n",
      "Iteration 7500 Training loss 0.009404723532497883 Validation loss 0.04311928153038025 Accuracy 0.8805000185966492\n",
      "Iteration 7510 Training loss 0.03266812488436699 Validation loss 0.060192592442035675 Accuracy 0.8421250581741333\n",
      "Iteration 7520 Training loss 0.014703207649290562 Validation loss 0.04274212196469307 Accuracy 0.8865000605583191\n",
      "Iteration 7530 Training loss 0.007209464907646179 Validation loss 0.04214558005332947 Accuracy 0.8876250386238098\n",
      "Iteration 7540 Training loss 0.004646512679755688 Validation loss 0.041283365339040756 Accuracy 0.8905000686645508\n",
      "Iteration 7550 Training loss 0.01204174105077982 Validation loss 0.041355568915605545 Accuracy 0.8908750414848328\n",
      "Iteration 7560 Training loss 0.008260311558842659 Validation loss 0.043492354452610016 Accuracy 0.8842500448226929\n",
      "Iteration 7570 Training loss 0.01596018858253956 Validation loss 0.046612996608018875 Accuracy 0.8767500519752502\n",
      "Iteration 7580 Training loss 0.013701117597520351 Validation loss 0.04138163477182388 Accuracy 0.8888750672340393\n",
      "Iteration 7590 Training loss 0.013163678348064423 Validation loss 0.0420922115445137 Accuracy 0.8872500658035278\n",
      "Iteration 7600 Training loss 0.013934154994785786 Validation loss 0.04575970023870468 Accuracy 0.8783750534057617\n",
      "Iteration 7610 Training loss 0.012629002332687378 Validation loss 0.04323180764913559 Accuracy 0.8865000605583191\n",
      "Iteration 7620 Training loss 0.007435563486069441 Validation loss 0.0455193966627121 Accuracy 0.877750039100647\n",
      "Iteration 7630 Training loss 0.01111691165715456 Validation loss 0.04273698478937149 Accuracy 0.8876250386238098\n",
      "Iteration 7640 Training loss 0.011114067398011684 Validation loss 0.04461800679564476 Accuracy 0.8802500367164612\n",
      "Iteration 7650 Training loss 0.021654615178704262 Validation loss 0.049324046820402145 Accuracy 0.8691250681877136\n",
      "Iteration 7660 Training loss 0.008666804060339928 Validation loss 0.042524393647909164 Accuracy 0.8871250152587891\n",
      "Iteration 7670 Training loss 0.013133838772773743 Validation loss 0.044640764594078064 Accuracy 0.8792500495910645\n",
      "Iteration 7680 Training loss 0.01696379855275154 Validation loss 0.044152166694402695 Accuracy 0.8816250562667847\n",
      "Iteration 7690 Training loss 0.04002867639064789 Validation loss 0.06389845907688141 Accuracy 0.8382500410079956\n",
      "Iteration 7700 Training loss 0.016072837635874748 Validation loss 0.04189889132976532 Accuracy 0.8873750567436218\n",
      "Iteration 7710 Training loss 0.008608187548816204 Validation loss 0.04204268753528595 Accuracy 0.8862500190734863\n",
      "Iteration 7720 Training loss 0.009671926498413086 Validation loss 0.04173104837536812 Accuracy 0.8883750438690186\n",
      "Iteration 7730 Training loss 0.014809791930019855 Validation loss 0.04284539818763733 Accuracy 0.8861250281333923\n",
      "Iteration 7740 Training loss 0.01319179031997919 Validation loss 0.04296088218688965 Accuracy 0.8838750123977661\n",
      "Iteration 7750 Training loss 0.0068361638113856316 Validation loss 0.04150700569152832 Accuracy 0.8877500295639038\n",
      "Iteration 7760 Training loss 0.013907439075410366 Validation loss 0.04174869880080223 Accuracy 0.8888750672340393\n",
      "Iteration 7770 Training loss 0.009635155089199543 Validation loss 0.04185918718576431 Accuracy 0.890125036239624\n",
      "Iteration 7780 Training loss 0.017270566895604134 Validation loss 0.048091791570186615 Accuracy 0.8755000233650208\n",
      "Iteration 7790 Training loss 0.011319033801555634 Validation loss 0.04275207966566086 Accuracy 0.8852500319480896\n",
      "Iteration 7800 Training loss 0.013162651099264622 Validation loss 0.042683038860559464 Accuracy 0.8871250152587891\n",
      "Iteration 7810 Training loss 0.011465962044894695 Validation loss 0.04576445370912552 Accuracy 0.8782500624656677\n",
      "Iteration 7820 Training loss 0.010876421816647053 Validation loss 0.041338685899972916 Accuracy 0.8922500610351562\n",
      "Iteration 7830 Training loss 0.007944398559629917 Validation loss 0.04108938202261925 Accuracy 0.8896250128746033\n",
      "Iteration 7840 Training loss 0.013285411521792412 Validation loss 0.04267766699194908 Accuracy 0.8852500319480896\n",
      "Iteration 7850 Training loss 0.00884939730167389 Validation loss 0.041616521775722504 Accuracy 0.8896250128746033\n",
      "Iteration 7860 Training loss 0.008941562846302986 Validation loss 0.042719803750514984 Accuracy 0.8896250128746033\n",
      "Iteration 7870 Training loss 0.007458293344825506 Validation loss 0.041936684399843216 Accuracy 0.8883750438690186\n",
      "Iteration 7880 Training loss 0.015639206394553185 Validation loss 0.0466451458632946 Accuracy 0.8763750195503235\n",
      "Iteration 7890 Training loss 0.009858816862106323 Validation loss 0.04139937087893486 Accuracy 0.8875000476837158\n",
      "Iteration 7900 Training loss 0.016484107822179794 Validation loss 0.04233347997069359 Accuracy 0.8868750333786011\n",
      "Iteration 7910 Training loss 0.010594641789793968 Validation loss 0.04140231013298035 Accuracy 0.89000004529953\n",
      "Iteration 7920 Training loss 0.00690949521958828 Validation loss 0.042726658284664154 Accuracy 0.8860000371932983\n",
      "Iteration 7930 Training loss 0.00674277963116765 Validation loss 0.041705772280693054 Accuracy 0.8896250128746033\n",
      "Iteration 7940 Training loss 0.01125425286591053 Validation loss 0.04517243802547455 Accuracy 0.8783750534057617\n",
      "Iteration 7950 Training loss 0.011598963290452957 Validation loss 0.04160863906145096 Accuracy 0.889875054359436\n",
      "Iteration 7960 Training loss 0.006695725955069065 Validation loss 0.04148339107632637 Accuracy 0.889750063419342\n",
      "Iteration 7970 Training loss 0.007287268526852131 Validation loss 0.041807420551776886 Accuracy 0.889750063419342\n",
      "Iteration 7980 Training loss 0.05607199668884277 Validation loss 0.07364147901535034 Accuracy 0.8192500472068787\n",
      "Iteration 7990 Training loss 0.012097214348614216 Validation loss 0.04242200404405594 Accuracy 0.8882500529289246\n",
      "Iteration 8000 Training loss 0.007562937680631876 Validation loss 0.04235134273767471 Accuracy 0.8872500658035278\n",
      "Iteration 8010 Training loss 0.009272373281419277 Validation loss 0.04257380589842796 Accuracy 0.8890000581741333\n",
      "Iteration 8020 Training loss 0.009753765538334846 Validation loss 0.042420677840709686 Accuracy 0.8878750205039978\n",
      "Iteration 8030 Training loss 0.009973151609301567 Validation loss 0.04230551794171333 Accuracy 0.8852500319480896\n",
      "Iteration 8040 Training loss 0.03990921750664711 Validation loss 0.06606552749872208 Accuracy 0.8290000557899475\n",
      "Iteration 8050 Training loss 0.011888210661709309 Validation loss 0.04318173602223396 Accuracy 0.8863750696182251\n",
      "Iteration 8060 Training loss 0.006990569643676281 Validation loss 0.0415116623044014 Accuracy 0.8910000324249268\n",
      "Iteration 8070 Training loss 0.017052466049790382 Validation loss 0.042648281902074814 Accuracy 0.8857500553131104\n",
      "Iteration 8080 Training loss 0.004071441944688559 Validation loss 0.04149048775434494 Accuracy 0.890375018119812\n",
      "Iteration 8090 Training loss 0.011352449655532837 Validation loss 0.041433073580265045 Accuracy 0.8920000195503235\n",
      "Iteration 8100 Training loss 0.011875959113240242 Validation loss 0.043339356780052185 Accuracy 0.8851250410079956\n",
      "Iteration 8110 Training loss 0.007199110928922892 Validation loss 0.04170059412717819 Accuracy 0.8885000348091125\n",
      "Iteration 8120 Training loss 0.007684092503041029 Validation loss 0.04385194554924965 Accuracy 0.8816250562667847\n",
      "Iteration 8130 Training loss 0.004339748062193394 Validation loss 0.04207051172852516 Accuracy 0.8858750462532043\n",
      "Iteration 8140 Training loss 0.006128333043307066 Validation loss 0.04246433079242706 Accuracy 0.8880000710487366\n",
      "Iteration 8150 Training loss 0.014618422836065292 Validation loss 0.04163063317537308 Accuracy 0.889750063419342\n",
      "Iteration 8160 Training loss 0.022977998480200768 Validation loss 0.05328590050339699 Accuracy 0.8650000691413879\n",
      "Iteration 8170 Training loss 0.010212864726781845 Validation loss 0.042351894080638885 Accuracy 0.8873750567436218\n",
      "Iteration 8180 Training loss 0.008349105715751648 Validation loss 0.042014963924884796 Accuracy 0.8867500424385071\n",
      "Iteration 8190 Training loss 0.008154806680977345 Validation loss 0.042124148458242416 Accuracy 0.8865000605583191\n",
      "Iteration 8200 Training loss 0.006567138247191906 Validation loss 0.042017169296741486 Accuracy 0.8886250257492065\n",
      "Iteration 8210 Training loss 0.012734310701489449 Validation loss 0.050334230065345764 Accuracy 0.8640000224113464\n",
      "Iteration 8220 Training loss 0.01272939145565033 Validation loss 0.04437697306275368 Accuracy 0.8802500367164612\n",
      "Iteration 8230 Training loss 0.011054441332817078 Validation loss 0.04234069585800171 Accuracy 0.8863750696182251\n",
      "Iteration 8240 Training loss 0.011430946178734303 Validation loss 0.04252054914832115 Accuracy 0.8883750438690186\n",
      "Iteration 8250 Training loss 0.014701875858008862 Validation loss 0.04575904458761215 Accuracy 0.877875030040741\n",
      "Iteration 8260 Training loss 0.006615770515054464 Validation loss 0.042037032544612885 Accuracy 0.8888750672340393\n",
      "Iteration 8270 Training loss 0.007720337714999914 Validation loss 0.04229142516851425 Accuracy 0.8867500424385071\n",
      "Iteration 8280 Training loss 0.013754427433013916 Validation loss 0.045144222676754 Accuracy 0.8803750276565552\n",
      "Iteration 8290 Training loss 0.017851607874035835 Validation loss 0.04659384489059448 Accuracy 0.8761250376701355\n",
      "Iteration 8300 Training loss 0.01129604410380125 Validation loss 0.042146891355514526 Accuracy 0.8873750567436218\n",
      "Iteration 8310 Training loss 0.01735425367951393 Validation loss 0.04538286104798317 Accuracy 0.8798750638961792\n",
      "Iteration 8320 Training loss 0.008998879231512547 Validation loss 0.04472789540886879 Accuracy 0.8803750276565552\n",
      "Iteration 8330 Training loss 0.005447175819426775 Validation loss 0.04212162271142006 Accuracy 0.8870000243186951\n",
      "Iteration 8340 Training loss 0.007251173723489046 Validation loss 0.04153120517730713 Accuracy 0.8895000219345093\n",
      "Iteration 8350 Training loss 0.01560970488935709 Validation loss 0.045813512057065964 Accuracy 0.8787500262260437\n",
      "Iteration 8360 Training loss 0.009400403127074242 Validation loss 0.042138632386922836 Accuracy 0.8881250619888306\n",
      "Iteration 8370 Training loss 0.007979188114404678 Validation loss 0.04210038110613823 Accuracy 0.890125036239624\n",
      "Iteration 8380 Training loss 0.005909962113946676 Validation loss 0.04183278977870941 Accuracy 0.8883750438690186\n",
      "Iteration 8390 Training loss 0.005400494206696749 Validation loss 0.04161957651376724 Accuracy 0.8882500529289246\n",
      "Iteration 8400 Training loss 0.005332804750651121 Validation loss 0.042123567312955856 Accuracy 0.8880000710487366\n",
      "Iteration 8410 Training loss 0.009071805514395237 Validation loss 0.041397154331207275 Accuracy 0.8911250233650208\n",
      "Iteration 8420 Training loss 0.008832999505102634 Validation loss 0.04224567487835884 Accuracy 0.8885000348091125\n",
      "Iteration 8430 Training loss 0.012925693765282631 Validation loss 0.048469457775354385 Accuracy 0.8732500672340393\n",
      "Iteration 8440 Training loss 0.0712655708193779 Validation loss 0.07627913355827332 Accuracy 0.8115000128746033\n",
      "Iteration 8450 Training loss 0.01231714803725481 Validation loss 0.04201560094952583 Accuracy 0.8855000138282776\n",
      "Iteration 8460 Training loss 0.008405155502259731 Validation loss 0.041745997965335846 Accuracy 0.8893750309944153\n",
      "Iteration 8470 Training loss 0.009493476711213589 Validation loss 0.041413117200136185 Accuracy 0.8911250233650208\n",
      "Iteration 8480 Training loss 0.006184296682476997 Validation loss 0.042637452483177185 Accuracy 0.8872500658035278\n",
      "Iteration 8490 Training loss 0.010766474530100822 Validation loss 0.04418640583753586 Accuracy 0.8830000162124634\n",
      "Iteration 8500 Training loss 0.010730581358075142 Validation loss 0.042985666543245316 Accuracy 0.8868750333786011\n",
      "Iteration 8510 Training loss 0.01124646421521902 Validation loss 0.04210308939218521 Accuracy 0.8881250619888306\n",
      "Iteration 8520 Training loss 0.005044517572969198 Validation loss 0.04236791282892227 Accuracy 0.8853750228881836\n",
      "Iteration 8530 Training loss 0.01034542080014944 Validation loss 0.04169728234410286 Accuracy 0.8907500505447388\n",
      "Iteration 8540 Training loss 0.00592694990336895 Validation loss 0.042706239968538284 Accuracy 0.8862500190734863\n",
      "Iteration 8550 Training loss 0.012046482414007187 Validation loss 0.0421968474984169 Accuracy 0.8880000710487366\n",
      "Iteration 8560 Training loss 0.0056676724925637245 Validation loss 0.04232712462544441 Accuracy 0.8885000348091125\n",
      "Iteration 8570 Training loss 0.041223276406526566 Validation loss 0.06581728160381317 Accuracy 0.8345000147819519\n",
      "Iteration 8580 Training loss 0.014078651554882526 Validation loss 0.04428335651755333 Accuracy 0.8831250667572021\n",
      "Iteration 8590 Training loss 0.008711283095180988 Validation loss 0.04133929684758186 Accuracy 0.8920000195503235\n",
      "Iteration 8600 Training loss 0.014026536606252193 Validation loss 0.04301632568240166 Accuracy 0.8876250386238098\n",
      "Iteration 8610 Training loss 0.0075472136959433556 Validation loss 0.04232132062315941 Accuracy 0.8885000348091125\n",
      "Iteration 8620 Training loss 0.006560715846717358 Validation loss 0.041901037096977234 Accuracy 0.8885000348091125\n",
      "Iteration 8630 Training loss 0.006509039085358381 Validation loss 0.04179031029343605 Accuracy 0.8892500400543213\n",
      "Iteration 8640 Training loss 0.003957136068493128 Validation loss 0.04188255965709686 Accuracy 0.889875054359436\n",
      "Iteration 8650 Training loss 0.006202206015586853 Validation loss 0.041689954698085785 Accuracy 0.8908750414848328\n",
      "Iteration 8660 Training loss 0.008904548361897469 Validation loss 0.042529430240392685 Accuracy 0.8885000348091125\n",
      "Iteration 8670 Training loss 0.005600101314485073 Validation loss 0.04276968911290169 Accuracy 0.8885000348091125\n",
      "Iteration 8680 Training loss 0.007998104207217693 Validation loss 0.042476702481508255 Accuracy 0.8877500295639038\n",
      "Iteration 8690 Training loss 0.0068955812603235245 Validation loss 0.044319845736026764 Accuracy 0.8840000629425049\n",
      "Iteration 8700 Training loss 0.010446889325976372 Validation loss 0.042987920343875885 Accuracy 0.8828750252723694\n",
      "Iteration 8710 Training loss 0.004960921127349138 Validation loss 0.041768647730350494 Accuracy 0.8892500400543213\n",
      "Iteration 8720 Training loss 0.0052215526811778545 Validation loss 0.042343784123659134 Accuracy 0.8882500529289246\n",
      "Iteration 8730 Training loss 0.012526518665254116 Validation loss 0.04323933646082878 Accuracy 0.8861250281333923\n",
      "Iteration 8740 Training loss 0.007255276199430227 Validation loss 0.042330551892519 Accuracy 0.8878750205039978\n",
      "Iteration 8750 Training loss 0.004418030381202698 Validation loss 0.04252501577138901 Accuracy 0.8876250386238098\n",
      "Iteration 8760 Training loss 0.015153598040342331 Validation loss 0.049679264426231384 Accuracy 0.8708750605583191\n",
      "Iteration 8770 Training loss 0.00525303091853857 Validation loss 0.04186565801501274 Accuracy 0.8905000686645508\n",
      "Iteration 8780 Training loss 0.010769065469503403 Validation loss 0.04238111898303032 Accuracy 0.8886250257492065\n",
      "Iteration 8790 Training loss 0.007373491767793894 Validation loss 0.0415034182369709 Accuracy 0.8906250596046448\n",
      "Iteration 8800 Training loss 0.003978523891419172 Validation loss 0.04230034723877907 Accuracy 0.8870000243186951\n",
      "Iteration 8810 Training loss 0.006770191714167595 Validation loss 0.042111270129680634 Accuracy 0.8882500529289246\n",
      "Iteration 8820 Training loss 0.008522236719727516 Validation loss 0.04297527298331261 Accuracy 0.8858750462532043\n",
      "Iteration 8830 Training loss 0.006499903276562691 Validation loss 0.04257465526461601 Accuracy 0.8870000243186951\n",
      "Iteration 8840 Training loss 0.04699632152915001 Validation loss 0.06025053560733795 Accuracy 0.8516250252723694\n",
      "Iteration 8850 Training loss 0.015032288618385792 Validation loss 0.04379119351506233 Accuracy 0.8830000162124634\n",
      "Iteration 8860 Training loss 0.005024499259889126 Validation loss 0.04379426687955856 Accuracy 0.8842500448226929\n",
      "Iteration 8870 Training loss 0.007341197691857815 Validation loss 0.04228612780570984 Accuracy 0.8880000710487366\n",
      "Iteration 8880 Training loss 0.006512143649160862 Validation loss 0.042458415031433105 Accuracy 0.8886250257492065\n",
      "Iteration 8890 Training loss 0.005660989321768284 Validation loss 0.042143020778894424 Accuracy 0.8881250619888306\n",
      "Iteration 8900 Training loss 0.0076764593832194805 Validation loss 0.041701365262269974 Accuracy 0.8910000324249268\n",
      "Iteration 8910 Training loss 0.009490493685007095 Validation loss 0.042062513530254364 Accuracy 0.8887500166893005\n",
      "Iteration 8920 Training loss 0.006858408451080322 Validation loss 0.04214991256594658 Accuracy 0.889875054359436\n",
      "Iteration 8930 Training loss 0.011899876408278942 Validation loss 0.041750963777303696 Accuracy 0.890375018119812\n",
      "Iteration 8940 Training loss 0.007930743508040905 Validation loss 0.04480986297130585 Accuracy 0.8800000548362732\n",
      "Iteration 8950 Training loss 0.0066727506928145885 Validation loss 0.04194861277937889 Accuracy 0.89000004529953\n",
      "Iteration 8960 Training loss 0.007627889979630709 Validation loss 0.0426030233502388 Accuracy 0.8893750309944153\n",
      "Iteration 8970 Training loss 0.01020065788179636 Validation loss 0.04246646538376808 Accuracy 0.8876250386238098\n",
      "Iteration 8980 Training loss 0.010419079102575779 Validation loss 0.041909780353307724 Accuracy 0.8912500143051147\n",
      "Iteration 8990 Training loss 0.005397049244493246 Validation loss 0.045290231704711914 Accuracy 0.8813750147819519\n",
      "Iteration 9000 Training loss 0.005323402117937803 Validation loss 0.04175819829106331 Accuracy 0.8911250233650208\n",
      "Iteration 9010 Training loss 0.004472672939300537 Validation loss 0.04188385233283043 Accuracy 0.8881250619888306\n",
      "Iteration 9020 Training loss 0.08070868998765945 Validation loss 0.08721702545881271 Accuracy 0.7975000143051147\n",
      "Iteration 9030 Training loss 0.012786086648702621 Validation loss 0.043546587228775024 Accuracy 0.8827500343322754\n",
      "Iteration 9040 Training loss 0.006617925129830837 Validation loss 0.04557490348815918 Accuracy 0.8772500157356262\n",
      "Iteration 9050 Training loss 0.007059323601424694 Validation loss 0.04168711230158806 Accuracy 0.889875054359436\n",
      "Iteration 9060 Training loss 0.007960454560816288 Validation loss 0.042016319930553436 Accuracy 0.8893750309944153\n",
      "Iteration 9070 Training loss 0.008617928251624107 Validation loss 0.04258467257022858 Accuracy 0.8875000476837158\n",
      "Iteration 9080 Training loss 0.005332616623491049 Validation loss 0.04223989322781563 Accuracy 0.8896250128746033\n",
      "Iteration 9090 Training loss 0.010671076364815235 Validation loss 0.042678721249103546 Accuracy 0.8861250281333923\n",
      "Iteration 9100 Training loss 0.004663906991481781 Validation loss 0.04155061021447182 Accuracy 0.893000066280365\n",
      "Iteration 9110 Training loss 0.011661683209240437 Validation loss 0.0527857281267643 Accuracy 0.8581250309944153\n",
      "Iteration 9120 Training loss 0.020261825993657112 Validation loss 0.057450372725725174 Accuracy 0.8528750538825989\n",
      "Iteration 9130 Training loss 0.013347934000194073 Validation loss 0.043448470532894135 Accuracy 0.8842500448226929\n",
      "Iteration 9140 Training loss 0.00904223695397377 Validation loss 0.04373778775334358 Accuracy 0.8838750123977661\n",
      "Iteration 9150 Training loss 0.006683180574327707 Validation loss 0.04209187626838684 Accuracy 0.8888750672340393\n",
      "Iteration 9160 Training loss 0.006740963086485863 Validation loss 0.0422300286591053 Accuracy 0.8887500166893005\n",
      "Iteration 9170 Training loss 0.009220478124916553 Validation loss 0.043452225625514984 Accuracy 0.8843750357627869\n",
      "Iteration 9180 Training loss 0.006057026796042919 Validation loss 0.04239990562200546 Accuracy 0.8875000476837158\n",
      "Iteration 9190 Training loss 0.0039610485546290874 Validation loss 0.042105115950107574 Accuracy 0.8896250128746033\n",
      "Iteration 9200 Training loss 0.006575232371687889 Validation loss 0.044528182595968246 Accuracy 0.8837500214576721\n",
      "Iteration 9210 Training loss 0.004696915857493877 Validation loss 0.04202321171760559 Accuracy 0.8917500376701355\n",
      "Iteration 9220 Training loss 0.004335652571171522 Validation loss 0.04227209463715553 Accuracy 0.8867500424385071\n",
      "Iteration 9230 Training loss 0.008020422421395779 Validation loss 0.04169989377260208 Accuracy 0.889875054359436\n",
      "Iteration 9240 Training loss 0.009389717131853104 Validation loss 0.04203209653496742 Accuracy 0.8912500143051147\n",
      "Iteration 9250 Training loss 0.004369822330772877 Validation loss 0.04167873412370682 Accuracy 0.8920000195503235\n",
      "Iteration 9260 Training loss 0.01285849791020155 Validation loss 0.04343933239579201 Accuracy 0.8858750462532043\n",
      "Iteration 9270 Training loss 0.037607330828905106 Validation loss 0.06250403076410294 Accuracy 0.8413750529289246\n",
      "Iteration 9280 Training loss 0.011736063286662102 Validation loss 0.04290911555290222 Accuracy 0.8865000605583191\n",
      "Iteration 9290 Training loss 0.009650273248553276 Validation loss 0.04221495985984802 Accuracy 0.8887500166893005\n",
      "Iteration 9300 Training loss 0.011136863380670547 Validation loss 0.04239627346396446 Accuracy 0.8877500295639038\n",
      "Iteration 9310 Training loss 0.007324857637286186 Validation loss 0.04175511375069618 Accuracy 0.8905000686645508\n",
      "Iteration 9320 Training loss 0.008948351256549358 Validation loss 0.041616879403591156 Accuracy 0.8927500247955322\n",
      "Iteration 9330 Training loss 0.0072399466298520565 Validation loss 0.04182819649577141 Accuracy 0.8912500143051147\n",
      "Iteration 9340 Training loss 0.006778772920370102 Validation loss 0.0426144078373909 Accuracy 0.8857500553131104\n",
      "Iteration 9350 Training loss 0.007760603912174702 Validation loss 0.04218561574816704 Accuracy 0.890375018119812\n",
      "Iteration 9360 Training loss 0.010071597062051296 Validation loss 0.04664313420653343 Accuracy 0.8757500648498535\n",
      "Iteration 9370 Training loss 0.00363902747631073 Validation loss 0.04215763136744499 Accuracy 0.8890000581741333\n",
      "Iteration 9380 Training loss 0.0034229920711368322 Validation loss 0.04229331016540527 Accuracy 0.8876250386238098\n",
      "Iteration 9390 Training loss 0.003163291374221444 Validation loss 0.04202858731150627 Accuracy 0.8913750648498535\n",
      "Iteration 9400 Training loss 0.006499265320599079 Validation loss 0.041568223387002945 Accuracy 0.8925000429153442\n",
      "Iteration 9410 Training loss 0.0749322846531868 Validation loss 0.0795799046754837 Accuracy 0.8141250610351562\n",
      "Iteration 9420 Training loss 0.01093295682221651 Validation loss 0.04243450239300728 Accuracy 0.8881250619888306\n",
      "Iteration 9430 Training loss 0.00845229160040617 Validation loss 0.04255127161741257 Accuracy 0.8875000476837158\n",
      "Iteration 9440 Training loss 0.0046784477308392525 Validation loss 0.04131127521395683 Accuracy 0.8912500143051147\n",
      "Iteration 9450 Training loss 0.005741423927247524 Validation loss 0.041560374200344086 Accuracy 0.893250048160553\n",
      "Iteration 9460 Training loss 0.004846230149269104 Validation loss 0.04180499538779259 Accuracy 0.890250027179718\n",
      "Iteration 9470 Training loss 0.008413691073656082 Validation loss 0.04186408594250679 Accuracy 0.8928750157356262\n",
      "Iteration 9480 Training loss 0.008801007643342018 Validation loss 0.04278557747602463 Accuracy 0.8873750567436218\n",
      "Iteration 9490 Training loss 0.0019367339555174112 Validation loss 0.041753802448511124 Accuracy 0.8911250233650208\n",
      "Iteration 9500 Training loss 0.00619485741481185 Validation loss 0.04165249317884445 Accuracy 0.8906250596046448\n",
      "Iteration 9510 Training loss 0.010435144416987896 Validation loss 0.04240359738469124 Accuracy 0.8890000581741333\n",
      "Iteration 9520 Training loss 0.005816122051328421 Validation loss 0.041698914021253586 Accuracy 0.893000066280365\n",
      "Iteration 9530 Training loss 0.007971308194100857 Validation loss 0.044017184525728226 Accuracy 0.8848750591278076\n",
      "Iteration 9540 Training loss 0.015693876892328262 Validation loss 0.04982561245560646 Accuracy 0.8712500333786011\n",
      "Iteration 9550 Training loss 0.015654828399419785 Validation loss 0.04545719176530838 Accuracy 0.8797500133514404\n",
      "Iteration 9560 Training loss 0.006615931633859873 Validation loss 0.04169914498925209 Accuracy 0.8911250233650208\n",
      "Iteration 9570 Training loss 0.005864724982529879 Validation loss 0.041604913771152496 Accuracy 0.8927500247955322\n",
      "Iteration 9580 Training loss 0.006094150245189667 Validation loss 0.042105723172426224 Accuracy 0.8890000581741333\n",
      "Iteration 9590 Training loss 0.008873812854290009 Validation loss 0.04307939484715462 Accuracy 0.8853750228881836\n",
      "Iteration 9600 Training loss 0.00578939588740468 Validation loss 0.04182691499590874 Accuracy 0.8916250467300415\n",
      "Iteration 9610 Training loss 0.006164657883346081 Validation loss 0.04181361943483353 Accuracy 0.8922500610351562\n",
      "Iteration 9620 Training loss 0.00397584680467844 Validation loss 0.04189454764127731 Accuracy 0.890250027179718\n",
      "Iteration 9630 Training loss 0.035712599754333496 Validation loss 0.06360378116369247 Accuracy 0.8447500467300415\n",
      "Iteration 9640 Training loss 0.010791873559355736 Validation loss 0.043628305196762085 Accuracy 0.8835000395774841\n",
      "Iteration 9650 Training loss 0.009598729200661182 Validation loss 0.04239727556705475 Accuracy 0.8881250619888306\n",
      "Iteration 9660 Training loss 0.007422716822475195 Validation loss 0.041835419833660126 Accuracy 0.8890000581741333\n",
      "Iteration 9670 Training loss 0.009530299343168736 Validation loss 0.04171450436115265 Accuracy 0.8906250596046448\n",
      "Iteration 9680 Training loss 0.006398701574653387 Validation loss 0.041805293411016464 Accuracy 0.890250027179718\n",
      "Iteration 9690 Training loss 0.0065573337487876415 Validation loss 0.04169325530529022 Accuracy 0.8920000195503235\n",
      "Iteration 9700 Training loss 0.006084837019443512 Validation loss 0.04236217588186264 Accuracy 0.8872500658035278\n",
      "Iteration 9710 Training loss 0.004972676746547222 Validation loss 0.04192987084388733 Accuracy 0.8896250128746033\n",
      "Iteration 9720 Training loss 0.009386775083839893 Validation loss 0.04212159663438797 Accuracy 0.8895000219345093\n",
      "Iteration 9730 Training loss 0.004459379706531763 Validation loss 0.0420968271791935 Accuracy 0.8913750648498535\n",
      "Iteration 9740 Training loss 0.006646457593888044 Validation loss 0.04249437898397446 Accuracy 0.8891250491142273\n",
      "Iteration 9750 Training loss 0.029963558539748192 Validation loss 0.0626915916800499 Accuracy 0.8442500233650208\n",
      "Iteration 9760 Training loss 0.01667180471122265 Validation loss 0.05095890909433365 Accuracy 0.8685000538825989\n",
      "Iteration 9770 Training loss 0.00694544380530715 Validation loss 0.04233569651842117 Accuracy 0.8868750333786011\n",
      "Iteration 9780 Training loss 0.012244923040270805 Validation loss 0.04449094459414482 Accuracy 0.8803750276565552\n",
      "Iteration 9790 Training loss 0.006233446300029755 Validation loss 0.04207611456513405 Accuracy 0.8892500400543213\n",
      "Iteration 9800 Training loss 0.009560218080878258 Validation loss 0.04212892800569534 Accuracy 0.8866250514984131\n",
      "Iteration 9810 Training loss 0.008937803097069263 Validation loss 0.042444586753845215 Accuracy 0.8886250257492065\n",
      "Iteration 9820 Training loss 0.004082422703504562 Validation loss 0.041990842670202255 Accuracy 0.889750063419342\n",
      "Iteration 9830 Training loss 0.009130499325692654 Validation loss 0.04260954260826111 Accuracy 0.8883750438690186\n",
      "Iteration 9840 Training loss 0.005889208987355232 Validation loss 0.042122285813093185 Accuracy 0.8883750438690186\n",
      "Iteration 9850 Training loss 0.008168315514922142 Validation loss 0.04184011369943619 Accuracy 0.8906250596046448\n",
      "Iteration 9860 Training loss 0.0045637027360498905 Validation loss 0.04224073141813278 Accuracy 0.8905000686645508\n",
      "Iteration 9870 Training loss 0.004890859127044678 Validation loss 0.04187609255313873 Accuracy 0.893125057220459\n",
      "Iteration 9880 Training loss 0.00781319197267294 Validation loss 0.043111108243465424 Accuracy 0.8870000243186951\n",
      "Iteration 9890 Training loss 0.013190612196922302 Validation loss 0.04998323321342468 Accuracy 0.8707500696182251\n",
      "Iteration 9900 Training loss 0.012080320157110691 Validation loss 0.04227772727608681 Accuracy 0.8878750205039978\n",
      "Iteration 9910 Training loss 0.004300182685256004 Validation loss 0.042513031512498856 Accuracy 0.8867500424385071\n",
      "Iteration 9920 Training loss 0.005431137513369322 Validation loss 0.042710158973932266 Accuracy 0.8873750567436218\n",
      "Iteration 9930 Training loss 0.00399026507511735 Validation loss 0.04233797639608383 Accuracy 0.8877500295639038\n",
      "Iteration 9940 Training loss 0.004361895378679037 Validation loss 0.04227543994784355 Accuracy 0.8893750309944153\n",
      "Iteration 9950 Training loss 0.010969358496367931 Validation loss 0.04191761836409569 Accuracy 0.89000004529953\n",
      "Iteration 9960 Training loss 0.006554363761097193 Validation loss 0.041873782873153687 Accuracy 0.8915000557899475\n",
      "Iteration 9970 Training loss 0.004984687082469463 Validation loss 0.04464572295546532 Accuracy 0.8830000162124634\n",
      "Iteration 9980 Training loss 0.004614302422851324 Validation loss 0.04209655150771141 Accuracy 0.8915000557899475\n",
      "Iteration 9990 Training loss 0.00414918502792716 Validation loss 0.043062154203653336 Accuracy 0.8872500658035278\n",
      "Iteration 10000 Training loss 0.005772141274064779 Validation loss 0.04217540845274925 Accuracy 0.8905000686645508\n",
      "Iteration 10010 Training loss 0.003806876949965954 Validation loss 0.04183921962976456 Accuracy 0.8920000195503235\n",
      "Iteration 10020 Training loss 0.002667995635420084 Validation loss 0.04184597358107567 Accuracy 0.8922500610351562\n",
      "Iteration 10030 Training loss 0.0034303246065974236 Validation loss 0.041975148022174835 Accuracy 0.8918750286102295\n",
      "Iteration 10040 Training loss 0.002830843674018979 Validation loss 0.042207229882478714 Accuracy 0.8913750648498535\n",
      "Iteration 10050 Training loss 0.0032764843199402094 Validation loss 0.04325979948043823 Accuracy 0.8870000243186951\n",
      "Iteration 10060 Training loss 0.009302821010351181 Validation loss 0.04234422370791435 Accuracy 0.8905000686645508\n",
      "Iteration 10070 Training loss 0.00397782027721405 Validation loss 0.042070936411619186 Accuracy 0.893625020980835\n",
      "Iteration 10080 Training loss 0.008152518421411514 Validation loss 0.04566717892885208 Accuracy 0.8821250200271606\n",
      "Iteration 10090 Training loss 0.004918799735605717 Validation loss 0.042712196707725525 Accuracy 0.8893750309944153\n",
      "Iteration 10100 Training loss 0.005107548553496599 Validation loss 0.04198598861694336 Accuracy 0.8916250467300415\n",
      "Iteration 10110 Training loss 0.004225183743983507 Validation loss 0.04232257604598999 Accuracy 0.890375018119812\n",
      "Iteration 10120 Training loss 0.00656680203974247 Validation loss 0.04245978966355324 Accuracy 0.890375018119812\n",
      "Iteration 10130 Training loss 0.008308508433401585 Validation loss 0.04205174371600151 Accuracy 0.889875054359436\n",
      "Iteration 10140 Training loss 0.004089323803782463 Validation loss 0.04199282452464104 Accuracy 0.8908750414848328\n",
      "Iteration 10150 Training loss 0.011035563424229622 Validation loss 0.0462123304605484 Accuracy 0.8813750147819519\n",
      "Iteration 10160 Training loss 0.02375214546918869 Validation loss 0.051952555775642395 Accuracy 0.8693750500679016\n",
      "Iteration 10170 Training loss 0.004623699001967907 Validation loss 0.04209538549184799 Accuracy 0.890125036239624\n",
      "Iteration 10180 Training loss 0.004220034461468458 Validation loss 0.04237323999404907 Accuracy 0.889750063419342\n",
      "Iteration 10190 Training loss 0.0053226822055876255 Validation loss 0.041487958282232285 Accuracy 0.8928750157356262\n",
      "Iteration 10200 Training loss 0.007956448011100292 Validation loss 0.04166153818368912 Accuracy 0.8925000429153442\n",
      "Iteration 10210 Training loss 0.006119922269135714 Validation loss 0.04274881258606911 Accuracy 0.8888750672340393\n",
      "Iteration 10220 Training loss 0.004888220224529505 Validation loss 0.04157225415110588 Accuracy 0.8913750648498535\n",
      "Iteration 10230 Training loss 0.007721120025962591 Validation loss 0.04213552549481392 Accuracy 0.89000004529953\n",
      "Iteration 10240 Training loss 0.005613246932625771 Validation loss 0.0421069897711277 Accuracy 0.8905000686645508\n",
      "Iteration 10250 Training loss 0.008009389042854309 Validation loss 0.04243622347712517 Accuracy 0.89000004529953\n",
      "Iteration 10260 Training loss 0.008733107708394527 Validation loss 0.04222598299384117 Accuracy 0.8916250467300415\n",
      "Iteration 10270 Training loss 0.00871820654720068 Validation loss 0.04223296791315079 Accuracy 0.890250027179718\n",
      "Iteration 10280 Training loss 0.004363748710602522 Validation loss 0.04262162372469902 Accuracy 0.8865000605583191\n",
      "Iteration 10290 Training loss 0.0074866944923996925 Validation loss 0.042866211384534836 Accuracy 0.8920000195503235\n",
      "Iteration 10300 Training loss 0.0071675959043204784 Validation loss 0.04257555305957794 Accuracy 0.8882500529289246\n",
      "Iteration 10310 Training loss 0.006278741639107466 Validation loss 0.042169515043497086 Accuracy 0.8908750414848328\n",
      "Iteration 10320 Training loss 0.008071978576481342 Validation loss 0.04273505508899689 Accuracy 0.8893750309944153\n",
      "Iteration 10330 Training loss 0.005875862669199705 Validation loss 0.04189828038215637 Accuracy 0.893500030040741\n",
      "Iteration 10340 Training loss 0.0034121882636100054 Validation loss 0.04317781329154968 Accuracy 0.8855000138282776\n",
      "Iteration 10350 Training loss 0.006764623336493969 Validation loss 0.044277604669332504 Accuracy 0.8808750510215759\n",
      "Iteration 10360 Training loss 0.005778077058494091 Validation loss 0.04252089187502861 Accuracy 0.8907500505447388\n",
      "Iteration 10370 Training loss 0.021163994446396828 Validation loss 0.054826173931360245 Accuracy 0.8653750419616699\n",
      "Iteration 10380 Training loss 0.01661895401775837 Validation loss 0.04795880615711212 Accuracy 0.8733750581741333\n",
      "Iteration 10390 Training loss 0.0064995549619197845 Validation loss 0.04228823259472847 Accuracy 0.890375018119812\n",
      "Iteration 10400 Training loss 0.004667395260185003 Validation loss 0.04258383437991142 Accuracy 0.8868750333786011\n",
      "Iteration 10410 Training loss 0.004430115222930908 Validation loss 0.042435526847839355 Accuracy 0.8905000686645508\n",
      "Iteration 10420 Training loss 0.006644433364272118 Validation loss 0.04203768074512482 Accuracy 0.889750063419342\n",
      "Iteration 10430 Training loss 0.007357934024184942 Validation loss 0.04182100296020508 Accuracy 0.8921250700950623\n",
      "Iteration 10440 Training loss 0.0029194909147918224 Validation loss 0.04267379641532898 Accuracy 0.8892500400543213\n",
      "Iteration 10450 Training loss 0.008757278323173523 Validation loss 0.04313277825713158 Accuracy 0.8892500400543213\n",
      "Iteration 10460 Training loss 0.006738792639225721 Validation loss 0.04214101657271385 Accuracy 0.8907500505447388\n",
      "Iteration 10470 Training loss 0.0019096522592008114 Validation loss 0.04259467497467995 Accuracy 0.8860000371932983\n",
      "Iteration 10480 Training loss 0.004574212711304426 Validation loss 0.042063042521476746 Accuracy 0.8892500400543213\n",
      "Iteration 10490 Training loss 0.005694395396858454 Validation loss 0.042632754892110825 Accuracy 0.8867500424385071\n",
      "Iteration 10500 Training loss 0.006367711815983057 Validation loss 0.04197222366929054 Accuracy 0.8910000324249268\n",
      "Iteration 10510 Training loss 0.002878617960959673 Validation loss 0.04284776374697685 Accuracy 0.8863750696182251\n",
      "Iteration 10520 Training loss 0.0028729706536978483 Validation loss 0.04264197126030922 Accuracy 0.8925000429153442\n",
      "Iteration 10530 Training loss 0.0033605939242988825 Validation loss 0.04256303235888481 Accuracy 0.8882500529289246\n",
      "Iteration 10540 Training loss 0.0062848953530192375 Validation loss 0.04204642400145531 Accuracy 0.8925000429153442\n",
      "Iteration 10550 Training loss 0.005463815294206142 Validation loss 0.04213881120085716 Accuracy 0.890250027179718\n",
      "Iteration 10560 Training loss 0.0018738353392109275 Validation loss 0.04302535578608513 Accuracy 0.8858750462532043\n",
      "Iteration 10570 Training loss 0.01574753038585186 Validation loss 0.05122034251689911 Accuracy 0.8695000410079956\n",
      "Iteration 10580 Training loss 0.006280812434852123 Validation loss 0.04427291825413704 Accuracy 0.8862500190734863\n",
      "Iteration 10590 Training loss 0.006208923179656267 Validation loss 0.04237062856554985 Accuracy 0.8895000219345093\n",
      "Iteration 10600 Training loss 0.007835235446691513 Validation loss 0.042805131524801254 Accuracy 0.8871250152587891\n",
      "Iteration 10610 Training loss 0.005312933586537838 Validation loss 0.04192129150032997 Accuracy 0.8912500143051147\n",
      "Iteration 10620 Training loss 0.008206942118704319 Validation loss 0.042043671011924744 Accuracy 0.8908750414848328\n",
      "Iteration 10630 Training loss 0.00636318838223815 Validation loss 0.042605433613061905 Accuracy 0.8882500529289246\n",
      "Iteration 10640 Training loss 0.006375251803547144 Validation loss 0.0420837439596653 Accuracy 0.893250048160553\n",
      "Iteration 10650 Training loss 0.0033408147282898426 Validation loss 0.042276591062545776 Accuracy 0.8893750309944153\n",
      "Iteration 10660 Training loss 0.006766017526388168 Validation loss 0.041896361857652664 Accuracy 0.8923750519752502\n",
      "Iteration 10670 Training loss 0.007738916669040918 Validation loss 0.042887430638074875 Accuracy 0.8885000348091125\n",
      "Iteration 10680 Training loss 0.0074849664233624935 Validation loss 0.042861755937337875 Accuracy 0.8858750462532043\n",
      "Iteration 10690 Training loss 0.008717282675206661 Validation loss 0.04221709817647934 Accuracy 0.890125036239624\n",
      "Iteration 10700 Training loss 0.00727485865354538 Validation loss 0.0426550917327404 Accuracy 0.8892500400543213\n",
      "Iteration 10710 Training loss 0.0040887510403990746 Validation loss 0.04206401854753494 Accuracy 0.893000066280365\n",
      "Iteration 10720 Training loss 0.0029007282573729753 Validation loss 0.0420478917658329 Accuracy 0.8916250467300415\n",
      "Iteration 10730 Training loss 0.001409120624884963 Validation loss 0.042173612862825394 Accuracy 0.893375039100647\n",
      "Iteration 10740 Training loss 0.0033737742342054844 Validation loss 0.04226264730095863 Accuracy 0.890125036239624\n",
      "Iteration 10750 Training loss 0.0027912810910493135 Validation loss 0.04220016673207283 Accuracy 0.8912500143051147\n",
      "Iteration 10760 Training loss 0.006267961580306292 Validation loss 0.042428698390722275 Accuracy 0.890250027179718\n",
      "Iteration 10770 Training loss 0.006641747895628214 Validation loss 0.04325900599360466 Accuracy 0.8886250257492065\n",
      "Iteration 10780 Training loss 0.003052725689485669 Validation loss 0.043214861303567886 Accuracy 0.8882500529289246\n",
      "Iteration 10790 Training loss 0.0057259597815573215 Validation loss 0.04223334789276123 Accuracy 0.8921250700950623\n",
      "Iteration 10800 Training loss 0.005749215371906757 Validation loss 0.04355274885892868 Accuracy 0.8877500295639038\n",
      "Iteration 10810 Training loss 0.006710960529744625 Validation loss 0.04253009334206581 Accuracy 0.8906250596046448\n",
      "Iteration 10820 Training loss 0.003605784149840474 Validation loss 0.043766189366579056 Accuracy 0.8848750591278076\n",
      "Iteration 10830 Training loss 0.004896108992397785 Validation loss 0.04232582822442055 Accuracy 0.8888750672340393\n",
      "Iteration 10840 Training loss 0.003008848987519741 Validation loss 0.04318666458129883 Accuracy 0.8886250257492065\n",
      "Iteration 10850 Training loss 0.004248912911862135 Validation loss 0.043165985494852066 Accuracy 0.8855000138282776\n",
      "Iteration 10860 Training loss 0.0040759132243692875 Validation loss 0.042729150503873825 Accuracy 0.8868750333786011\n",
      "Iteration 10870 Training loss 0.0011684849159792066 Validation loss 0.042450323700904846 Accuracy 0.8905000686645508\n",
      "Iteration 10880 Training loss 0.005027553532272577 Validation loss 0.04215649515390396 Accuracy 0.8918750286102295\n",
      "Iteration 10890 Training loss 0.007213345728814602 Validation loss 0.042958956211805344 Accuracy 0.8885000348091125\n",
      "Iteration 10900 Training loss 0.004404209554195404 Validation loss 0.04270017519593239 Accuracy 0.890125036239624\n",
      "Iteration 10910 Training loss 0.0024627072270959616 Validation loss 0.04317520931363106 Accuracy 0.8877500295639038\n",
      "Iteration 10920 Training loss 0.008076288737356663 Validation loss 0.0421614870429039 Accuracy 0.8916250467300415\n",
      "Iteration 10930 Training loss 0.00446003582328558 Validation loss 0.0422564260661602 Accuracy 0.8907500505447388\n",
      "Iteration 10940 Training loss 0.0023443519603461027 Validation loss 0.042482465505599976 Accuracy 0.890250027179718\n",
      "Iteration 10950 Training loss 0.007569385226815939 Validation loss 0.04217008873820305 Accuracy 0.89000004529953\n",
      "Iteration 10960 Training loss 0.0037651651073247194 Validation loss 0.04248935729265213 Accuracy 0.8892500400543213\n",
      "Iteration 10970 Training loss 0.0011300850892439485 Validation loss 0.04209919273853302 Accuracy 0.8921250700950623\n",
      "Iteration 10980 Training loss 0.0034119407646358013 Validation loss 0.04299836605787277 Accuracy 0.8885000348091125\n",
      "Iteration 10990 Training loss 0.0030572323594242334 Validation loss 0.04262058809399605 Accuracy 0.8907500505447388\n",
      "Iteration 11000 Training loss 0.004677972290664911 Validation loss 0.04254889860749245 Accuracy 0.8906250596046448\n",
      "Iteration 11010 Training loss 0.0029067706782370806 Validation loss 0.04225956276059151 Accuracy 0.89000004529953\n",
      "Iteration 11020 Training loss 0.003944991156458855 Validation loss 0.042724765837192535 Accuracy 0.8896250128746033\n",
      "Iteration 11030 Training loss 0.005358345340937376 Validation loss 0.04257746785879135 Accuracy 0.8891250491142273\n",
      "Iteration 11040 Training loss 0.0032463143579661846 Validation loss 0.04229934513568878 Accuracy 0.889750063419342\n",
      "Iteration 11050 Training loss 0.0030051367357373238 Validation loss 0.04273337870836258 Accuracy 0.8882500529289246\n",
      "Iteration 11060 Training loss 0.0043635256588459015 Validation loss 0.0445227287709713 Accuracy 0.8852500319480896\n",
      "Iteration 11070 Training loss 0.0021052209194749594 Validation loss 0.04268587380647659 Accuracy 0.8906250596046448\n",
      "Iteration 11080 Training loss 0.005561269354075193 Validation loss 0.04203925281763077 Accuracy 0.8923750519752502\n",
      "Iteration 11090 Training loss 0.007916096597909927 Validation loss 0.045181091874837875 Accuracy 0.8812500238418579\n",
      "Iteration 11100 Training loss 0.006717870011925697 Validation loss 0.042714305222034454 Accuracy 0.8887500166893005\n",
      "Iteration 11110 Training loss 0.0027596622239798307 Validation loss 0.04290667921304703 Accuracy 0.8892500400543213\n",
      "Iteration 11120 Training loss 0.008125984109938145 Validation loss 0.04276556521654129 Accuracy 0.890250027179718\n",
      "Iteration 11130 Training loss 0.007635279092937708 Validation loss 0.04248347878456116 Accuracy 0.8915000557899475\n",
      "Iteration 11140 Training loss 0.0038395889569073915 Validation loss 0.042546480894088745 Accuracy 0.8888750672340393\n",
      "Iteration 11150 Training loss 0.011342122219502926 Validation loss 0.046726569533348083 Accuracy 0.8811250329017639\n",
      "Iteration 11160 Training loss 0.08722376823425293 Validation loss 0.10591285675764084 Accuracy 0.7542500495910645\n",
      "Iteration 11170 Training loss 0.0330919548869133 Validation loss 0.05765635892748833 Accuracy 0.8556250333786011\n",
      "Iteration 11180 Training loss 0.006857333239167929 Validation loss 0.04219416528940201 Accuracy 0.8891250491142273\n",
      "Iteration 11190 Training loss 0.006855177693068981 Validation loss 0.041918862611055374 Accuracy 0.8921250700950623\n",
      "Iteration 11200 Training loss 0.005408364348113537 Validation loss 0.04281647875905037 Accuracy 0.8871250152587891\n",
      "Iteration 11210 Training loss 0.006493426859378815 Validation loss 0.04328105226159096 Accuracy 0.8881250619888306\n",
      "Iteration 11220 Training loss 0.004883561749011278 Validation loss 0.042858123779296875 Accuracy 0.8885000348091125\n",
      "Iteration 11230 Training loss 0.004337620921432972 Validation loss 0.042257159948349 Accuracy 0.8905000686645508\n",
      "Iteration 11240 Training loss 0.008906838484108448 Validation loss 0.042429063469171524 Accuracy 0.89000004529953\n",
      "Iteration 11250 Training loss 0.007606387138366699 Validation loss 0.043099142611026764 Accuracy 0.8888750672340393\n",
      "Iteration 11260 Training loss 0.006041135638952255 Validation loss 0.04233206808567047 Accuracy 0.8908750414848328\n",
      "Iteration 11270 Training loss 0.0017313805874437094 Validation loss 0.042887087911367416 Accuracy 0.8870000243186951\n",
      "Iteration 11280 Training loss 0.004933570045977831 Validation loss 0.04227037355303764 Accuracy 0.8907500505447388\n",
      "Iteration 11290 Training loss 0.003064468502998352 Validation loss 0.04253789037466049 Accuracy 0.8911250233650208\n",
      "Iteration 11300 Training loss 0.005829425994306803 Validation loss 0.04241204261779785 Accuracy 0.890125036239624\n",
      "Iteration 11310 Training loss 0.008151516318321228 Validation loss 0.04530061408877373 Accuracy 0.8823750615119934\n",
      "Iteration 11320 Training loss 0.0027731640730053186 Validation loss 0.0424664132297039 Accuracy 0.8912500143051147\n",
      "Iteration 11330 Training loss 0.006297084502875805 Validation loss 0.04260445013642311 Accuracy 0.8878750205039978\n",
      "Iteration 11340 Training loss 0.004141090903431177 Validation loss 0.04267304390668869 Accuracy 0.8878750205039978\n",
      "Iteration 11350 Training loss 0.0022595671471208334 Validation loss 0.042348358780145645 Accuracy 0.890125036239624\n",
      "Iteration 11360 Training loss 0.003980094101279974 Validation loss 0.042313188314437866 Accuracy 0.8912500143051147\n",
      "Iteration 11370 Training loss 0.002243859227746725 Validation loss 0.042694468051195145 Accuracy 0.8877500295639038\n",
      "Iteration 11380 Training loss 0.005603031255304813 Validation loss 0.04323064908385277 Accuracy 0.8868750333786011\n",
      "Iteration 11390 Training loss 0.004544767551124096 Validation loss 0.04313252493739128 Accuracy 0.8880000710487366\n",
      "Iteration 11400 Training loss 0.004558369517326355 Validation loss 0.043219782412052155 Accuracy 0.8906250596046448\n",
      "Iteration 11410 Training loss 0.00536852004006505 Validation loss 0.04294872283935547 Accuracy 0.8885000348091125\n",
      "Iteration 11420 Training loss 0.00532956700772047 Validation loss 0.04253038391470909 Accuracy 0.8908750414848328\n",
      "Iteration 11430 Training loss 0.0048030647449195385 Validation loss 0.042635831981897354 Accuracy 0.890125036239624\n",
      "Iteration 11440 Training loss 0.004929233342409134 Validation loss 0.043042056262493134 Accuracy 0.8887500166893005\n",
      "Iteration 11450 Training loss 0.0009166551171801984 Validation loss 0.04319960996508598 Accuracy 0.8890000581741333\n",
      "Iteration 11460 Training loss 0.004048981238156557 Validation loss 0.04232030734419823 Accuracy 0.8927500247955322\n",
      "Iteration 11470 Training loss 0.00534838018938899 Validation loss 0.04353424534201622 Accuracy 0.889875054359436\n",
      "Iteration 11480 Training loss 0.0058502512983977795 Validation loss 0.04225385934114456 Accuracy 0.8917500376701355\n",
      "Iteration 11490 Training loss 0.0019618261139839888 Validation loss 0.04290379583835602 Accuracy 0.8885000348091125\n",
      "Iteration 11500 Training loss 0.009686612524092197 Validation loss 0.044081877917051315 Accuracy 0.8880000710487366\n",
      "Iteration 11510 Training loss 0.0028653829358518124 Validation loss 0.042285241186618805 Accuracy 0.8921250700950623\n",
      "Iteration 11520 Training loss 0.006723596248775721 Validation loss 0.04517281427979469 Accuracy 0.8826250433921814\n",
      "Iteration 11530 Training loss 0.004627038259059191 Validation loss 0.042636942118406296 Accuracy 0.8880000710487366\n",
      "Iteration 11540 Training loss 0.010003332979977131 Validation loss 0.04872607812285423 Accuracy 0.8750000596046448\n",
      "Iteration 11550 Training loss 0.03611711785197258 Validation loss 0.06472312659025192 Accuracy 0.8361250162124634\n",
      "Iteration 11560 Training loss 0.004066314548254013 Validation loss 0.04340006411075592 Accuracy 0.8868750333786011\n",
      "Iteration 11570 Training loss 0.003170702140778303 Validation loss 0.04219227284193039 Accuracy 0.8906250596046448\n",
      "Iteration 11580 Training loss 0.002006567781791091 Validation loss 0.04174882546067238 Accuracy 0.8927500247955322\n",
      "Iteration 11590 Training loss 0.002319708000868559 Validation loss 0.04323239251971245 Accuracy 0.8876250386238098\n",
      "Iteration 11600 Training loss 0.00791094359010458 Validation loss 0.04292566701769829 Accuracy 0.8885000348091125\n",
      "Iteration 11610 Training loss 0.0027418648824095726 Validation loss 0.04211694747209549 Accuracy 0.8927500247955322\n",
      "Iteration 11620 Training loss 0.004580131731927395 Validation loss 0.04367681220173836 Accuracy 0.8865000605583191\n",
      "Iteration 11630 Training loss 0.004369611386209726 Validation loss 0.042604438960552216 Accuracy 0.8896250128746033\n",
      "Iteration 11640 Training loss 0.0032666956540197134 Validation loss 0.044601891189813614 Accuracy 0.8807500600814819\n",
      "Iteration 11650 Training loss 0.001004586461931467 Validation loss 0.042470019310712814 Accuracy 0.8915000557899475\n",
      "Iteration 11660 Training loss 0.0020716225262731314 Validation loss 0.04234146326780319 Accuracy 0.8917500376701355\n",
      "Iteration 11670 Training loss 0.00554380938410759 Validation loss 0.042221300303936005 Accuracy 0.8907500505447388\n",
      "Iteration 11680 Training loss 0.006576025392860174 Validation loss 0.04341093450784683 Accuracy 0.8861250281333923\n",
      "Iteration 11690 Training loss 0.007492147386074066 Validation loss 0.04318681359291077 Accuracy 0.8870000243186951\n",
      "Iteration 11700 Training loss 0.0039759306237101555 Validation loss 0.04287422448396683 Accuracy 0.8890000581741333\n",
      "Iteration 11710 Training loss 0.005235347431153059 Validation loss 0.042903587222099304 Accuracy 0.8892500400543213\n",
      "Iteration 11720 Training loss 0.006360833067446947 Validation loss 0.04278431087732315 Accuracy 0.8907500505447388\n",
      "Iteration 11730 Training loss 0.007253024261444807 Validation loss 0.043303925544023514 Accuracy 0.8877500295639038\n",
      "Iteration 11740 Training loss 0.005551726091653109 Validation loss 0.04278615862131119 Accuracy 0.8915000557899475\n",
      "Iteration 11750 Training loss 0.002734804293140769 Validation loss 0.04290301725268364 Accuracy 0.8890000581741333\n",
      "Iteration 11760 Training loss 0.004252062179148197 Validation loss 0.04463937506079674 Accuracy 0.8838750123977661\n",
      "Iteration 11770 Training loss 0.005064915865659714 Validation loss 0.044063959270715714 Accuracy 0.8860000371932983\n",
      "Iteration 11780 Training loss 0.0018531793029978871 Validation loss 0.042796727269887924 Accuracy 0.890250027179718\n",
      "Iteration 11790 Training loss 0.007274304982274771 Validation loss 0.042705655097961426 Accuracy 0.8895000219345093\n",
      "Iteration 11800 Training loss 0.00371491233818233 Validation loss 0.04259592294692993 Accuracy 0.8895000219345093\n",
      "Iteration 11810 Training loss 0.005695234518498182 Validation loss 0.043001316487789154 Accuracy 0.8887500166893005\n",
      "Iteration 11820 Training loss 0.002125575440004468 Validation loss 0.04269653558731079 Accuracy 0.890250027179718\n",
      "Iteration 11830 Training loss 0.004782304633408785 Validation loss 0.042868129909038544 Accuracy 0.8895000219345093\n",
      "Iteration 11840 Training loss 0.004718954674899578 Validation loss 0.042764101177453995 Accuracy 0.8907500505447388\n",
      "Iteration 11850 Training loss 0.003321354975923896 Validation loss 0.0429002083837986 Accuracy 0.8888750672340393\n",
      "Iteration 11860 Training loss 0.003831185633316636 Validation loss 0.04242350906133652 Accuracy 0.8911250233650208\n",
      "Iteration 11870 Training loss 0.002693175571039319 Validation loss 0.0425974540412426 Accuracy 0.8928750157356262\n",
      "Iteration 11880 Training loss 0.0033131535165011883 Validation loss 0.042734451591968536 Accuracy 0.8883750438690186\n",
      "Iteration 11890 Training loss 0.003462834982201457 Validation loss 0.04229126125574112 Accuracy 0.893375039100647\n",
      "Iteration 11900 Training loss 0.008436847478151321 Validation loss 0.04685680568218231 Accuracy 0.878000020980835\n",
      "Iteration 11910 Training loss 0.016956869512796402 Validation loss 0.0529920756816864 Accuracy 0.8692500591278076\n",
      "Iteration 11920 Training loss 0.007564431056380272 Validation loss 0.046181391924619675 Accuracy 0.8791250586509705\n",
      "Iteration 11930 Training loss 0.006303749978542328 Validation loss 0.04296766221523285 Accuracy 0.889750063419342\n",
      "Iteration 11940 Training loss 0.0020804554224014282 Validation loss 0.0429118350148201 Accuracy 0.89000004529953\n",
      "Iteration 11950 Training loss 0.00652779545634985 Validation loss 0.042498040944337845 Accuracy 0.8893750309944153\n",
      "Iteration 11960 Training loss 0.00698121590539813 Validation loss 0.042477816343307495 Accuracy 0.890375018119812\n",
      "Iteration 11970 Training loss 0.002026173286139965 Validation loss 0.042707156389951706 Accuracy 0.8893750309944153\n",
      "Iteration 11980 Training loss 0.0038893793243914843 Validation loss 0.042535774409770966 Accuracy 0.8908750414848328\n",
      "Iteration 11990 Training loss 0.0006985164945945144 Validation loss 0.042571138590574265 Accuracy 0.889750063419342\n",
      "Iteration 12000 Training loss 0.00553837651386857 Validation loss 0.04359035566449165 Accuracy 0.8876250386238098\n",
      "Iteration 12010 Training loss 0.00244542327709496 Validation loss 0.04290153831243515 Accuracy 0.8883750438690186\n",
      "Iteration 12020 Training loss 0.006161292549222708 Validation loss 0.04275468736886978 Accuracy 0.8892500400543213\n",
      "Iteration 12030 Training loss 0.0039814733900129795 Validation loss 0.04269063100218773 Accuracy 0.8905000686645508\n",
      "Iteration 12040 Training loss 0.007279162295162678 Validation loss 0.046144723892211914 Accuracy 0.8788750171661377\n",
      "Iteration 12050 Training loss 0.0033365637063980103 Validation loss 0.04269447550177574 Accuracy 0.889750063419342\n",
      "Iteration 12060 Training loss 0.002990287495777011 Validation loss 0.04418933019042015 Accuracy 0.8827500343322754\n",
      "Iteration 12070 Training loss 0.0020372243598103523 Validation loss 0.04275582358241081 Accuracy 0.889750063419342\n",
      "Iteration 12080 Training loss 0.005133156664669514 Validation loss 0.04283813759684563 Accuracy 0.8910000324249268\n",
      "Iteration 12090 Training loss 0.0029535628855228424 Validation loss 0.043268267065286636 Accuracy 0.889750063419342\n",
      "Iteration 12100 Training loss 0.00221425318159163 Validation loss 0.04270423576235771 Accuracy 0.889750063419342\n",
      "Iteration 12110 Training loss 0.0019201085669919848 Validation loss 0.04276111721992493 Accuracy 0.8905000686645508\n",
      "Iteration 12120 Training loss 0.0031212312169373035 Validation loss 0.04362223297357559 Accuracy 0.8856250643730164\n",
      "Iteration 12130 Training loss 0.003464852459728718 Validation loss 0.04291441664099693 Accuracy 0.8918750286102295\n",
      "Iteration 12140 Training loss 0.00642929645255208 Validation loss 0.04271403327584267 Accuracy 0.8905000686645508\n",
      "Iteration 12150 Training loss 0.0032490710727870464 Validation loss 0.042707815766334534 Accuracy 0.8890000581741333\n",
      "Iteration 12160 Training loss 0.0008374033495783806 Validation loss 0.04274137318134308 Accuracy 0.890250027179718\n",
      "Iteration 12170 Training loss 0.0024428777396678925 Validation loss 0.04282683879137039 Accuracy 0.8895000219345093\n",
      "Iteration 12180 Training loss 0.004145433660596609 Validation loss 0.04287880286574364 Accuracy 0.8895000219345093\n",
      "Iteration 12190 Training loss 0.0036108004860579967 Validation loss 0.04338979348540306 Accuracy 0.889875054359436\n",
      "Iteration 12200 Training loss 0.004374147392809391 Validation loss 0.043474823236465454 Accuracy 0.8882500529289246\n",
      "Iteration 12210 Training loss 0.002822952112182975 Validation loss 0.04314213991165161 Accuracy 0.8890000581741333\n",
      "Iteration 12220 Training loss 0.0015139501774683595 Validation loss 0.04338895156979561 Accuracy 0.8886250257492065\n",
      "Iteration 12230 Training loss 0.0037747337482869625 Validation loss 0.04278363287448883 Accuracy 0.889875054359436\n",
      "Iteration 12240 Training loss 0.002654311014339328 Validation loss 0.042637526988983154 Accuracy 0.8923750519752502\n",
      "Iteration 12250 Training loss 0.0037415693514049053 Validation loss 0.04322802275419235 Accuracy 0.8882500529289246\n",
      "Iteration 12260 Training loss 0.004987736698240042 Validation loss 0.042683906853199005 Accuracy 0.890375018119812\n",
      "Iteration 12270 Training loss 0.0023842023219913244 Validation loss 0.04275244474411011 Accuracy 0.8917500376701355\n",
      "Iteration 12280 Training loss 0.0034409654326736927 Validation loss 0.04274914413690567 Accuracy 0.8905000686645508\n",
      "Iteration 12290 Training loss 0.004031008575111628 Validation loss 0.042901620268821716 Accuracy 0.8892500400543213\n",
      "Iteration 12300 Training loss 0.0016011707484722137 Validation loss 0.0429355762898922 Accuracy 0.889875054359436\n",
      "Iteration 12310 Training loss 0.0022232953924685717 Validation loss 0.04322309046983719 Accuracy 0.8888750672340393\n",
      "Iteration 12320 Training loss 0.002366455504670739 Validation loss 0.04299089312553406 Accuracy 0.890375018119812\n",
      "Iteration 12330 Training loss 0.004124595317989588 Validation loss 0.042851969599723816 Accuracy 0.893250048160553\n",
      "Iteration 12340 Training loss 0.004525937605649233 Validation loss 0.04282788559794426 Accuracy 0.8906250596046448\n",
      "Iteration 12350 Training loss 0.0056316363625228405 Validation loss 0.042735472321510315 Accuracy 0.8907500505447388\n",
      "Iteration 12360 Training loss 0.04749833419919014 Validation loss 0.06372743099927902 Accuracy 0.8565000295639038\n",
      "Iteration 12370 Training loss 0.045669544488191605 Validation loss 0.0695788785815239 Accuracy 0.8302500247955322\n",
      "Iteration 12380 Training loss 0.0066987513564527035 Validation loss 0.04552736133337021 Accuracy 0.8811250329017639\n",
      "Iteration 12390 Training loss 0.0025195078924298286 Validation loss 0.04222282022237778 Accuracy 0.8916250467300415\n",
      "Iteration 12400 Training loss 0.002342418534681201 Validation loss 0.042464304715394974 Accuracy 0.8907500505447388\n",
      "Iteration 12410 Training loss 0.006972485687583685 Validation loss 0.042252473533153534 Accuracy 0.8913750648498535\n",
      "Iteration 12420 Training loss 0.004041655454784632 Validation loss 0.042904146015644073 Accuracy 0.8908750414848328\n",
      "Iteration 12430 Training loss 0.006092050578445196 Validation loss 0.042228877544403076 Accuracy 0.8920000195503235\n",
      "Iteration 12440 Training loss 0.005788111127912998 Validation loss 0.04249674826860428 Accuracy 0.8912500143051147\n",
      "Iteration 12450 Training loss 0.0038567478768527508 Validation loss 0.042412713170051575 Accuracy 0.8923750519752502\n",
      "Iteration 12460 Training loss 0.0056254081428050995 Validation loss 0.04388454556465149 Accuracy 0.8852500319480896\n",
      "Iteration 12470 Training loss 0.0033594192937016487 Validation loss 0.04268718883395195 Accuracy 0.8905000686645508\n",
      "Iteration 12480 Training loss 0.0031198204960674047 Validation loss 0.042430032044649124 Accuracy 0.8910000324249268\n",
      "Iteration 12490 Training loss 0.003057279856875539 Validation loss 0.042425043880939484 Accuracy 0.8908750414848328\n",
      "Iteration 12500 Training loss 0.0022592172026634216 Validation loss 0.042542777955532074 Accuracy 0.8926250338554382\n",
      "Iteration 12510 Training loss 0.004192482680082321 Validation loss 0.043197374790906906 Accuracy 0.890250027179718\n",
      "Iteration 12520 Training loss 0.0012858600821346045 Validation loss 0.04263436421751976 Accuracy 0.8917500376701355\n",
      "Iteration 12530 Training loss 0.0036581535823643208 Validation loss 0.04214993491768837 Accuracy 0.8923750519752502\n",
      "Iteration 12540 Training loss 0.004953192546963692 Validation loss 0.04282424598932266 Accuracy 0.8913750648498535\n",
      "Iteration 12550 Training loss 0.004432530142366886 Validation loss 0.04296064376831055 Accuracy 0.8888750672340393\n",
      "Iteration 12560 Training loss 0.003393273800611496 Validation loss 0.04292863979935646 Accuracy 0.889750063419342\n",
      "Iteration 12570 Training loss 0.001720994827337563 Validation loss 0.0426185242831707 Accuracy 0.8905000686645508\n",
      "Iteration 12580 Training loss 0.0032629852648824453 Validation loss 0.04237549379467964 Accuracy 0.89000004529953\n",
      "Iteration 12590 Training loss 0.004928446840494871 Validation loss 0.042347781360149384 Accuracy 0.893125057220459\n",
      "Iteration 12600 Training loss 0.0023845266550779343 Validation loss 0.04295777156949043 Accuracy 0.889875054359436\n",
      "Iteration 12610 Training loss 0.005162211135029793 Validation loss 0.04278559982776642 Accuracy 0.8927500247955322\n",
      "Iteration 12620 Training loss 0.006733802612870932 Validation loss 0.042577944695949554 Accuracy 0.8911250233650208\n",
      "Iteration 12630 Training loss 0.0018075787229463458 Validation loss 0.04337971657514572 Accuracy 0.8905000686645508\n",
      "Iteration 12640 Training loss 0.0019717414397746325 Validation loss 0.04281275346875191 Accuracy 0.8908750414848328\n",
      "Iteration 12650 Training loss 0.006161454599350691 Validation loss 0.04258836433291435 Accuracy 0.8922500610351562\n",
      "Iteration 12660 Training loss 0.004882378503680229 Validation loss 0.04281279072165489 Accuracy 0.889750063419342\n",
      "Iteration 12670 Training loss 0.003744562389329076 Validation loss 0.04297791048884392 Accuracy 0.890125036239624\n",
      "Iteration 12680 Training loss 0.004844261799007654 Validation loss 0.04333829507231712 Accuracy 0.8873750567436218\n",
      "Iteration 12690 Training loss 0.001812906819395721 Validation loss 0.042883746325969696 Accuracy 0.890250027179718\n",
      "Iteration 12700 Training loss 0.0034082112833857536 Validation loss 0.0425676666200161 Accuracy 0.8916250467300415\n",
      "Iteration 12710 Training loss 0.004322669003158808 Validation loss 0.04281572252511978 Accuracy 0.8886250257492065\n",
      "Iteration 12720 Training loss 0.004380738828331232 Validation loss 0.04251325502991676 Accuracy 0.8922500610351562\n",
      "Iteration 12730 Training loss 0.0057533965446054935 Validation loss 0.04280420392751694 Accuracy 0.8896250128746033\n",
      "Iteration 12740 Training loss 0.0025888492818921804 Validation loss 0.04312059283256531 Accuracy 0.8875000476837158\n",
      "Iteration 12750 Training loss 0.001980897504836321 Validation loss 0.04322773963212967 Accuracy 0.889875054359436\n",
      "Iteration 12760 Training loss 0.0005932957283221185 Validation loss 0.042864564806222916 Accuracy 0.890125036239624\n",
      "Iteration 12770 Training loss 0.002167669590562582 Validation loss 0.04316916689276695 Accuracy 0.890375018119812\n",
      "Iteration 12780 Training loss 0.002899502171203494 Validation loss 0.043050579726696014 Accuracy 0.8893750309944153\n",
      "Iteration 12790 Training loss 0.0019310011994093657 Validation loss 0.04343980550765991 Accuracy 0.8893750309944153\n",
      "Iteration 12800 Training loss 0.003033351618796587 Validation loss 0.04343520104885101 Accuracy 0.8887500166893005\n",
      "Iteration 12810 Training loss 0.004243429284542799 Validation loss 0.04268152266740799 Accuracy 0.8925000429153442\n",
      "Iteration 12820 Training loss 0.004791809245944023 Validation loss 0.044616732746362686 Accuracy 0.8873750567436218\n",
      "Iteration 12830 Training loss 0.0008465807186439633 Validation loss 0.0428181029856205 Accuracy 0.8911250233650208\n",
      "Iteration 12840 Training loss 0.001990314107388258 Validation loss 0.04307332634925842 Accuracy 0.8912500143051147\n",
      "Iteration 12850 Training loss 0.002246206859126687 Validation loss 0.043113213032484055 Accuracy 0.8882500529289246\n",
      "Iteration 12860 Training loss 0.001501248450949788 Validation loss 0.043728746473789215 Accuracy 0.89000004529953\n",
      "Iteration 12870 Training loss 0.006623306777328253 Validation loss 0.04430210217833519 Accuracy 0.8861250281333923\n",
      "Iteration 12880 Training loss 0.0012883966555818915 Validation loss 0.042729396373033524 Accuracy 0.893000066280365\n",
      "Iteration 12890 Training loss 0.0018799924291670322 Validation loss 0.04317257180809975 Accuracy 0.8896250128746033\n",
      "Iteration 12900 Training loss 0.004255220293998718 Validation loss 0.043096184730529785 Accuracy 0.8890000581741333\n",
      "Iteration 12910 Training loss 0.008723151870071888 Validation loss 0.04389284551143646 Accuracy 0.8865000605583191\n",
      "Iteration 12920 Training loss 0.004038561601191759 Validation loss 0.04299139976501465 Accuracy 0.8915000557899475\n",
      "Iteration 12930 Training loss 0.0007583114202134311 Validation loss 0.04284287989139557 Accuracy 0.89000004529953\n",
      "Iteration 12940 Training loss 0.0008917347877286375 Validation loss 0.042621150612831116 Accuracy 0.8926250338554382\n",
      "Iteration 12950 Training loss 0.003130932105705142 Validation loss 0.04256027564406395 Accuracy 0.8923750519752502\n",
      "Iteration 12960 Training loss 0.00327126937918365 Validation loss 0.043031755834817886 Accuracy 0.890250027179718\n",
      "Iteration 12970 Training loss 0.0022824436891824007 Validation loss 0.04317310452461243 Accuracy 0.8896250128746033\n",
      "Iteration 12980 Training loss 0.0023518139496445656 Validation loss 0.04350469633936882 Accuracy 0.8887500166893005\n",
      "Iteration 12990 Training loss 0.005800686310976744 Validation loss 0.04279187321662903 Accuracy 0.8910000324249268\n",
      "Iteration 13000 Training loss 0.0017049126327037811 Validation loss 0.04256431758403778 Accuracy 0.8908750414848328\n",
      "Iteration 13010 Training loss 0.0033139747101813555 Validation loss 0.04309854283928871 Accuracy 0.8911250233650208\n",
      "Iteration 13020 Training loss 0.003233472118154168 Validation loss 0.04285215958952904 Accuracy 0.8918750286102295\n",
      "Iteration 13030 Training loss 0.0005240498576313257 Validation loss 0.04367431625723839 Accuracy 0.8880000710487366\n",
      "Iteration 13040 Training loss 0.0034530514385551214 Validation loss 0.042682625353336334 Accuracy 0.8913750648498535\n",
      "Iteration 13050 Training loss 0.00267133885063231 Validation loss 0.0438191294670105 Accuracy 0.8848750591278076\n",
      "Iteration 13060 Training loss 0.002588882576674223 Validation loss 0.04301547631621361 Accuracy 0.8905000686645508\n",
      "Iteration 13070 Training loss 0.004175493028014898 Validation loss 0.04306161776185036 Accuracy 0.8891250491142273\n",
      "Iteration 13080 Training loss 0.0012974438723176718 Validation loss 0.042587220668792725 Accuracy 0.8911250233650208\n",
      "Iteration 13090 Training loss 0.0017352871363982558 Validation loss 0.042776189744472504 Accuracy 0.8913750648498535\n",
      "Iteration 13100 Training loss 0.004121959675103426 Validation loss 0.04263880103826523 Accuracy 0.8916250467300415\n",
      "Iteration 13110 Training loss 0.0017631230875849724 Validation loss 0.04280080273747444 Accuracy 0.8905000686645508\n",
      "Iteration 13120 Training loss 0.002894439036026597 Validation loss 0.04289904609322548 Accuracy 0.889750063419342\n",
      "Iteration 13130 Training loss 0.0010992599418386817 Validation loss 0.042536672204732895 Accuracy 0.8922500610351562\n",
      "Iteration 13140 Training loss 0.0010054500307887793 Validation loss 0.04322307929396629 Accuracy 0.8891250491142273\n",
      "Iteration 13150 Training loss 0.002022295957431197 Validation loss 0.04274045675992966 Accuracy 0.8905000686645508\n",
      "Iteration 13160 Training loss 0.006421568803489208 Validation loss 0.042801398783922195 Accuracy 0.8910000324249268\n",
      "Iteration 13170 Training loss 0.0007488554692827165 Validation loss 0.04264630004763603 Accuracy 0.8921250700950623\n",
      "Iteration 13180 Training loss 0.0029714833945035934 Validation loss 0.042955439537763596 Accuracy 0.8917500376701355\n",
      "Iteration 13190 Training loss 0.0035231211222708225 Validation loss 0.043842919170856476 Accuracy 0.8865000605583191\n",
      "Iteration 13200 Training loss 0.0019450398394837976 Validation loss 0.04285633936524391 Accuracy 0.8917500376701355\n",
      "Iteration 13210 Training loss 0.0059936498291790485 Validation loss 0.04350078105926514 Accuracy 0.8888750672340393\n",
      "Iteration 13220 Training loss 0.0007732912781648338 Validation loss 0.04271211475133896 Accuracy 0.8913750648498535\n",
      "Iteration 13230 Training loss 0.0043462300673127174 Validation loss 0.043240174651145935 Accuracy 0.890250027179718\n",
      "Iteration 13240 Training loss 0.0043512568809092045 Validation loss 0.042975377291440964 Accuracy 0.890375018119812\n",
      "Iteration 13250 Training loss 0.005861865356564522 Validation loss 0.04285335913300514 Accuracy 0.8912500143051147\n",
      "Iteration 13260 Training loss 0.0021998716983944178 Validation loss 0.04279967024922371 Accuracy 0.8915000557899475\n",
      "Iteration 13270 Training loss 0.003475388279184699 Validation loss 0.04308614507317543 Accuracy 0.8913750648498535\n",
      "Iteration 13280 Training loss 0.004773517604917288 Validation loss 0.04275455325841904 Accuracy 0.8927500247955322\n",
      "Iteration 13290 Training loss 0.0006832262733951211 Validation loss 0.042812373489141464 Accuracy 0.8912500143051147\n",
      "Iteration 13300 Training loss 0.0017858530627563596 Validation loss 0.043859925121068954 Accuracy 0.8876250386238098\n",
      "Iteration 13310 Training loss 0.000821626977995038 Validation loss 0.04300820454955101 Accuracy 0.8910000324249268\n",
      "Iteration 13320 Training loss 0.0029021205846220255 Validation loss 0.042883340269327164 Accuracy 0.8923750519752502\n",
      "Iteration 13330 Training loss 0.003321256022900343 Validation loss 0.042783915996551514 Accuracy 0.8920000195503235\n",
      "Iteration 13340 Training loss 0.0016137787606567144 Validation loss 0.04314403235912323 Accuracy 0.8921250700950623\n",
      "Iteration 13350 Training loss 0.005592206493020058 Validation loss 0.04405207931995392 Accuracy 0.8863750696182251\n",
      "Iteration 13360 Training loss 0.002910421695560217 Validation loss 0.04283563420176506 Accuracy 0.8923750519752502\n",
      "Iteration 13370 Training loss 0.001444840687327087 Validation loss 0.04347953945398331 Accuracy 0.8888750672340393\n",
      "Iteration 13380 Training loss 0.003017088398337364 Validation loss 0.042755305767059326 Accuracy 0.8928750157356262\n",
      "Iteration 13390 Training loss 0.0038417361211031675 Validation loss 0.04295331612229347 Accuracy 0.8895000219345093\n",
      "Iteration 13400 Training loss 0.0015344622079283 Validation loss 0.04370434954762459 Accuracy 0.8885000348091125\n",
      "Iteration 13410 Training loss 0.05672777071595192 Validation loss 0.0869971215724945 Accuracy 0.7848750352859497\n",
      "Iteration 13420 Training loss 0.042630959302186966 Validation loss 0.06519558280706406 Accuracy 0.8483750224113464\n",
      "Iteration 13430 Training loss 0.004789278842508793 Validation loss 0.043548334389925 Accuracy 0.8868750333786011\n",
      "Iteration 13440 Training loss 0.0043297428637743 Validation loss 0.0442131944000721 Accuracy 0.8873750567436218\n",
      "Iteration 13450 Training loss 0.0034372799564152956 Validation loss 0.04355907067656517 Accuracy 0.8873750567436218\n",
      "Iteration 13460 Training loss 0.0012611328857019544 Validation loss 0.0429728627204895 Accuracy 0.8896250128746033\n",
      "Iteration 13470 Training loss 0.0050574252381920815 Validation loss 0.04311489313840866 Accuracy 0.8887500166893005\n",
      "Iteration 13480 Training loss 0.0029536180663853884 Validation loss 0.043165892362594604 Accuracy 0.8881250619888306\n",
      "Iteration 13490 Training loss 0.003995796665549278 Validation loss 0.04404386505484581 Accuracy 0.8833750486373901\n",
      "Iteration 13500 Training loss 0.0031198158394545317 Validation loss 0.04291318729519844 Accuracy 0.8895000219345093\n",
      "Iteration 13510 Training loss 0.000635794538538903 Validation loss 0.04336705803871155 Accuracy 0.8886250257492065\n",
      "Iteration 13520 Training loss 0.001981884241104126 Validation loss 0.04309207201004028 Accuracy 0.889875054359436\n",
      "Iteration 13530 Training loss 0.0018183037173002958 Validation loss 0.04312187433242798 Accuracy 0.8892500400543213\n",
      "Iteration 13540 Training loss 0.0018435759702697396 Validation loss 0.04277865216135979 Accuracy 0.8891250491142273\n",
      "Iteration 13550 Training loss 0.004125842824578285 Validation loss 0.042920492589473724 Accuracy 0.8895000219345093\n",
      "Iteration 13560 Training loss 0.00389482663013041 Validation loss 0.04360179975628853 Accuracy 0.8860000371932983\n",
      "Iteration 13570 Training loss 0.0030694042798131704 Validation loss 0.04330712556838989 Accuracy 0.8895000219345093\n",
      "Iteration 13580 Training loss 0.0018594611901789904 Validation loss 0.043021272867918015 Accuracy 0.890250027179718\n",
      "Iteration 13590 Training loss 0.0006505853380076587 Validation loss 0.04342265427112579 Accuracy 0.8870000243186951\n",
      "Iteration 13600 Training loss 0.0016707825707271695 Validation loss 0.04386613517999649 Accuracy 0.8856250643730164\n",
      "Iteration 13610 Training loss 0.0017127044266089797 Validation loss 0.04290059208869934 Accuracy 0.8907500505447388\n",
      "Iteration 13620 Training loss 0.0030168325174599886 Validation loss 0.042952749878168106 Accuracy 0.889750063419342\n",
      "Iteration 13630 Training loss 0.0022125057876110077 Validation loss 0.04303658381104469 Accuracy 0.8887500166893005\n",
      "Iteration 13640 Training loss 0.0033404752612113953 Validation loss 0.042898815125226974 Accuracy 0.8895000219345093\n",
      "Iteration 13650 Training loss 0.0018112853867933154 Validation loss 0.04363148286938667 Accuracy 0.8868750333786011\n",
      "Iteration 13660 Training loss 0.0022641499526798725 Validation loss 0.043788712471723557 Accuracy 0.8872500658035278\n",
      "Iteration 13670 Training loss 0.0021440002601593733 Validation loss 0.04342322796583176 Accuracy 0.8885000348091125\n",
      "Iteration 13680 Training loss 0.004237901885062456 Validation loss 0.04293804243206978 Accuracy 0.8910000324249268\n",
      "Iteration 13690 Training loss 0.0067038582637906075 Validation loss 0.04300843924283981 Accuracy 0.890375018119812\n",
      "Iteration 13700 Training loss 0.0023857764899730682 Validation loss 0.04326269030570984 Accuracy 0.889750063419342\n",
      "Iteration 13710 Training loss 0.006440493743866682 Validation loss 0.0430690199136734 Accuracy 0.8910000324249268\n",
      "Iteration 13720 Training loss 0.003153993980959058 Validation loss 0.04326576367020607 Accuracy 0.890125036239624\n",
      "Iteration 13730 Training loss 0.0025190466549247503 Validation loss 0.043393030762672424 Accuracy 0.8886250257492065\n",
      "Iteration 13740 Training loss 0.0021134563721716404 Validation loss 0.043545592576265335 Accuracy 0.8860000371932983\n",
      "Iteration 13750 Training loss 0.003709666430950165 Validation loss 0.043280526995658875 Accuracy 0.8892500400543213\n",
      "Iteration 13760 Training loss 0.0019799969159066677 Validation loss 0.04335359111428261 Accuracy 0.8893750309944153\n",
      "Iteration 13770 Training loss 0.0041745384223759174 Validation loss 0.043500106781721115 Accuracy 0.8871250152587891\n",
      "Iteration 13780 Training loss 0.0009290606249123812 Validation loss 0.043172746896743774 Accuracy 0.8910000324249268\n",
      "Iteration 13790 Training loss 0.0032202773727476597 Validation loss 0.04329618439078331 Accuracy 0.8908750414848328\n",
      "Iteration 13800 Training loss 0.0016689961776137352 Validation loss 0.04322332143783569 Accuracy 0.8896250128746033\n",
      "Iteration 13810 Training loss 0.006687130779027939 Validation loss 0.04326048493385315 Accuracy 0.890250027179718\n",
      "Iteration 13820 Training loss 0.0021291624289005995 Validation loss 0.044108226895332336 Accuracy 0.8862500190734863\n",
      "Iteration 13830 Training loss 0.0058523439802229404 Validation loss 0.044252000749111176 Accuracy 0.8871250152587891\n",
      "Iteration 13840 Training loss 0.002489987062290311 Validation loss 0.043051883578300476 Accuracy 0.890250027179718\n",
      "Iteration 13850 Training loss 0.005491355434060097 Validation loss 0.04356864094734192 Accuracy 0.8880000710487366\n",
      "Iteration 13860 Training loss 0.0037134108133614063 Validation loss 0.04350188001990318 Accuracy 0.8880000710487366\n",
      "Iteration 13870 Training loss 0.004699873272329569 Validation loss 0.04317865148186684 Accuracy 0.8913750648498535\n",
      "Iteration 13880 Training loss 0.030743751674890518 Validation loss 0.06046532467007637 Accuracy 0.8517500162124634\n",
      "Iteration 13890 Training loss 0.019573379307985306 Validation loss 0.05737893283367157 Accuracy 0.859125018119812\n",
      "Iteration 13900 Training loss 0.0019400590099394321 Validation loss 0.04333005100488663 Accuracy 0.8877500295639038\n",
      "Iteration 13910 Training loss 0.0014791544526815414 Validation loss 0.04310046508908272 Accuracy 0.8908750414848328\n",
      "Iteration 13920 Training loss 0.004453384783118963 Validation loss 0.04258173704147339 Accuracy 0.8911250233650208\n",
      "Iteration 13930 Training loss 0.0021674742456525564 Validation loss 0.0430506095290184 Accuracy 0.8905000686645508\n",
      "Iteration 13940 Training loss 0.008602732792496681 Validation loss 0.043047137558460236 Accuracy 0.890250027179718\n",
      "Iteration 13950 Training loss 0.0008178054122254252 Validation loss 0.04294441267848015 Accuracy 0.890250027179718\n",
      "Iteration 13960 Training loss 0.003321937285363674 Validation loss 0.042811792343854904 Accuracy 0.89000004529953\n",
      "Iteration 13970 Training loss 0.004216314293444157 Validation loss 0.042739998549222946 Accuracy 0.8907500505447388\n",
      "Iteration 13980 Training loss 0.0022438194137066603 Validation loss 0.044418931007385254 Accuracy 0.8851250410079956\n",
      "Iteration 13990 Training loss 0.0008023199043236673 Validation loss 0.042971834540367126 Accuracy 0.890375018119812\n",
      "Iteration 14000 Training loss 0.004956061951816082 Validation loss 0.043105028569698334 Accuracy 0.8905000686645508\n",
      "Iteration 14010 Training loss 0.0018325503915548325 Validation loss 0.043001528829336166 Accuracy 0.8915000557899475\n",
      "Iteration 14020 Training loss 0.0018315109191462398 Validation loss 0.04308496415615082 Accuracy 0.889750063419342\n",
      "Iteration 14030 Training loss 0.0027297765482217073 Validation loss 0.042792003601789474 Accuracy 0.8911250233650208\n",
      "Iteration 14040 Training loss 0.0015725961420685053 Validation loss 0.042808160185813904 Accuracy 0.8916250467300415\n",
      "Iteration 14050 Training loss 0.002963562263175845 Validation loss 0.042559701949357986 Accuracy 0.8915000557899475\n",
      "Iteration 14060 Training loss 0.0020044397097080946 Validation loss 0.0430792011320591 Accuracy 0.8886250257492065\n",
      "Iteration 14070 Training loss 0.0066714040003716946 Validation loss 0.042633458971977234 Accuracy 0.8913750648498535\n",
      "Iteration 14080 Training loss 0.004378240089863539 Validation loss 0.04280541464686394 Accuracy 0.890375018119812\n",
      "Iteration 14090 Training loss 0.002148265717551112 Validation loss 0.04280330240726471 Accuracy 0.893125057220459\n",
      "Iteration 14100 Training loss 0.0007996378699317575 Validation loss 0.04299057275056839 Accuracy 0.8906250596046448\n",
      "Iteration 14110 Training loss 0.0005144838360138237 Validation loss 0.043002650141716 Accuracy 0.889875054359436\n",
      "Iteration 14120 Training loss 0.003074791980907321 Validation loss 0.04272393137216568 Accuracy 0.8918750286102295\n",
      "Iteration 14130 Training loss 0.0038252542726695538 Validation loss 0.04272895306348801 Accuracy 0.8917500376701355\n",
      "Iteration 14140 Training loss 0.00080019241431728 Validation loss 0.042820315808057785 Accuracy 0.8920000195503235\n",
      "Iteration 14150 Training loss 0.0033397418446838856 Validation loss 0.04279657453298569 Accuracy 0.8922500610351562\n",
      "Iteration 14160 Training loss 0.002159704454243183 Validation loss 0.04338568076491356 Accuracy 0.8886250257492065\n",
      "Iteration 14170 Training loss 0.005501303356140852 Validation loss 0.04286002367734909 Accuracy 0.8927500247955322\n",
      "Iteration 14180 Training loss 0.0018446946050971746 Validation loss 0.04294478893280029 Accuracy 0.8912500143051147\n",
      "Iteration 14190 Training loss 0.004049147013574839 Validation loss 0.04276876151561737 Accuracy 0.8911250233650208\n",
      "Iteration 14200 Training loss 0.0053614042699337006 Validation loss 0.04287172853946686 Accuracy 0.890375018119812\n",
      "Iteration 14210 Training loss 0.0016997572965919971 Validation loss 0.042673029005527496 Accuracy 0.8918750286102295\n",
      "Iteration 14220 Training loss 0.006847660522907972 Validation loss 0.04315807670354843 Accuracy 0.8921250700950623\n",
      "Iteration 14230 Training loss 0.002956161741167307 Validation loss 0.042840104550123215 Accuracy 0.8923750519752502\n",
      "Iteration 14240 Training loss 0.002231311984360218 Validation loss 0.04320539906620979 Accuracy 0.8913750648498535\n",
      "Iteration 14250 Training loss 0.003235987853258848 Validation loss 0.043923743069171906 Accuracy 0.8856250643730164\n",
      "Iteration 14260 Training loss 0.003916254732757807 Validation loss 0.044129613786935806 Accuracy 0.8868750333786011\n",
      "Iteration 14270 Training loss 0.0032547854352742434 Validation loss 0.04297087341547012 Accuracy 0.8910000324249268\n",
      "Iteration 14280 Training loss 0.0019091642461717129 Validation loss 0.042632073163986206 Accuracy 0.8925000429153442\n",
      "Iteration 14290 Training loss 0.0010670723859220743 Validation loss 0.043261121958494186 Accuracy 0.8923750519752502\n",
      "Iteration 14300 Training loss 0.00443430757150054 Validation loss 0.04287068545818329 Accuracy 0.8920000195503235\n",
      "Iteration 14310 Training loss 0.004080233629792929 Validation loss 0.042882513254880905 Accuracy 0.8918750286102295\n",
      "Iteration 14320 Training loss 0.005261570680886507 Validation loss 0.042962901294231415 Accuracy 0.893125057220459\n",
      "Iteration 14330 Training loss 0.0017674629343673587 Validation loss 0.042849101126194 Accuracy 0.890250027179718\n",
      "Iteration 14340 Training loss 0.004418067634105682 Validation loss 0.04286230728030205 Accuracy 0.8910000324249268\n",
      "Iteration 14350 Training loss 0.0017561566783115268 Validation loss 0.042723022401332855 Accuracy 0.8920000195503235\n",
      "Iteration 14360 Training loss 0.004503331612795591 Validation loss 0.04280928149819374 Accuracy 0.8926250338554382\n",
      "Iteration 14370 Training loss 0.002945570508018136 Validation loss 0.04296634718775749 Accuracy 0.8927500247955322\n",
      "Iteration 14380 Training loss 0.004189716186374426 Validation loss 0.042883019894361496 Accuracy 0.890250027179718\n",
      "Iteration 14390 Training loss 0.00428925221785903 Validation loss 0.042896874248981476 Accuracy 0.8910000324249268\n",
      "Iteration 14400 Training loss 0.004000723361968994 Validation loss 0.04319993406534195 Accuracy 0.8893750309944153\n",
      "Iteration 14410 Training loss 0.0028004914056509733 Validation loss 0.04305882379412651 Accuracy 0.8907500505447388\n",
      "Iteration 14420 Training loss 0.0008844957919791341 Validation loss 0.04314599931240082 Accuracy 0.8911250233650208\n",
      "Iteration 14430 Training loss 0.0028855830896645784 Validation loss 0.04305233806371689 Accuracy 0.890250027179718\n",
      "Iteration 14440 Training loss 0.004245314281433821 Validation loss 0.04295799508690834 Accuracy 0.8915000557899475\n",
      "Iteration 14450 Training loss 0.004284391179680824 Validation loss 0.043188754469156265 Accuracy 0.889875054359436\n",
      "Iteration 14460 Training loss 0.0052718305960297585 Validation loss 0.043023932725191116 Accuracy 0.8906250596046448\n",
      "Iteration 14470 Training loss 0.007004837971180677 Validation loss 0.04314956068992615 Accuracy 0.8920000195503235\n",
      "Iteration 14480 Training loss 0.0018653083825483918 Validation loss 0.0431169830262661 Accuracy 0.8916250467300415\n",
      "Iteration 14490 Training loss 0.002064504660665989 Validation loss 0.04297570511698723 Accuracy 0.8910000324249268\n",
      "Iteration 14500 Training loss 0.0006802133866585791 Validation loss 0.0432719886302948 Accuracy 0.8907500505447388\n",
      "Iteration 14510 Training loss 0.002908433089032769 Validation loss 0.04327171668410301 Accuracy 0.8895000219345093\n",
      "Iteration 14520 Training loss 0.0031202691607177258 Validation loss 0.04318596422672272 Accuracy 0.890125036239624\n",
      "Iteration 14530 Training loss 0.0030831594485789537 Validation loss 0.04389803484082222 Accuracy 0.8887500166893005\n",
      "Iteration 14540 Training loss 0.002463477198034525 Validation loss 0.04314817115664482 Accuracy 0.8915000557899475\n",
      "Iteration 14550 Training loss 0.003584148595109582 Validation loss 0.04458489641547203 Accuracy 0.8842500448226929\n",
      "Iteration 14560 Training loss 0.0018759823869913816 Validation loss 0.043068550527095795 Accuracy 0.8916250467300415\n",
      "Iteration 14570 Training loss 0.002840843517333269 Validation loss 0.04317765310406685 Accuracy 0.8913750648498535\n",
      "Iteration 14580 Training loss 0.009188630618155003 Validation loss 0.04340561106801033 Accuracy 0.890125036239624\n",
      "Iteration 14590 Training loss 0.0027173515409231186 Validation loss 0.04325844347476959 Accuracy 0.8910000324249268\n",
      "Iteration 14600 Training loss 0.002125002909451723 Validation loss 0.0429915115237236 Accuracy 0.8905000686645508\n",
      "Iteration 14610 Training loss 0.0018155849538743496 Validation loss 0.04314528405666351 Accuracy 0.8915000557899475\n",
      "Iteration 14620 Training loss 0.005260883364826441 Validation loss 0.04303457960486412 Accuracy 0.8913750648498535\n",
      "Iteration 14630 Training loss 0.0018043925520032644 Validation loss 0.04280510172247887 Accuracy 0.8916250467300415\n",
      "Iteration 14640 Training loss 0.0053765675984323025 Validation loss 0.04304749146103859 Accuracy 0.8913750648498535\n",
      "Iteration 14650 Training loss 0.0006596010643988848 Validation loss 0.042950183153152466 Accuracy 0.8913750648498535\n",
      "Iteration 14660 Training loss 0.0005229706293903291 Validation loss 0.04305196553468704 Accuracy 0.8895000219345093\n",
      "Iteration 14670 Training loss 0.002892240881919861 Validation loss 0.042888931930065155 Accuracy 0.8910000324249268\n",
      "Iteration 14680 Training loss 0.006797499489039183 Validation loss 0.04342633858323097 Accuracy 0.8907500505447388\n",
      "Iteration 14690 Training loss 0.00037589858402498066 Validation loss 0.04307353496551514 Accuracy 0.8911250233650208\n",
      "Iteration 14700 Training loss 0.0030056475661695004 Validation loss 0.043480854481458664 Accuracy 0.8892500400543213\n",
      "Iteration 14710 Training loss 0.0015853687655180693 Validation loss 0.04317094385623932 Accuracy 0.890250027179718\n",
      "Iteration 14720 Training loss 0.0005126837641000748 Validation loss 0.04292033612728119 Accuracy 0.8915000557899475\n",
      "Iteration 14730 Training loss 0.0028401133604347706 Validation loss 0.04337148368358612 Accuracy 0.8893750309944153\n",
      "Iteration 14740 Training loss 0.0028165678959339857 Validation loss 0.04327649623155594 Accuracy 0.890250027179718\n",
      "Iteration 14750 Training loss 0.0015962767647579312 Validation loss 0.044218387454748154 Accuracy 0.8857500553131104\n",
      "Iteration 14760 Training loss 0.00291788880713284 Validation loss 0.04273791238665581 Accuracy 0.8917500376701355\n",
      "Iteration 14770 Training loss 0.002220281632617116 Validation loss 0.0429934523999691 Accuracy 0.8925000429153442\n",
      "Iteration 14780 Training loss 0.0017091856570914388 Validation loss 0.043184228241443634 Accuracy 0.889750063419342\n",
      "Iteration 14790 Training loss 0.0015484802424907684 Validation loss 0.04315778240561485 Accuracy 0.8913750648498535\n",
      "Iteration 14800 Training loss 0.0022482904605567455 Validation loss 0.04310553893446922 Accuracy 0.8916250467300415\n",
      "Iteration 14810 Training loss 0.0003614391607698053 Validation loss 0.042895082384347916 Accuracy 0.8912500143051147\n",
      "Iteration 14820 Training loss 0.002912757219746709 Validation loss 0.04292284697294235 Accuracy 0.8912500143051147\n",
      "Iteration 14830 Training loss 0.005415908992290497 Validation loss 0.043007031083106995 Accuracy 0.8908750414848328\n",
      "Iteration 14840 Training loss 0.004355511162430048 Validation loss 0.04315555468201637 Accuracy 0.8908750414848328\n",
      "Iteration 14850 Training loss 0.0015193776926025748 Validation loss 0.04276696965098381 Accuracy 0.8921250700950623\n",
      "Iteration 14860 Training loss 0.004550881218165159 Validation loss 0.04288274422287941 Accuracy 0.8923750519752502\n",
      "Iteration 14870 Training loss 0.0019796525593847036 Validation loss 0.04284285008907318 Accuracy 0.893125057220459\n",
      "Iteration 14880 Training loss 0.0028349878266453743 Validation loss 0.04298675060272217 Accuracy 0.8910000324249268\n",
      "Iteration 14890 Training loss 0.004566661547869444 Validation loss 0.04331714287400246 Accuracy 0.8886250257492065\n",
      "Iteration 14900 Training loss 0.0016478359466418624 Validation loss 0.0431140698492527 Accuracy 0.8892500400543213\n",
      "Iteration 14910 Training loss 0.0017549494514241815 Validation loss 0.04309210553765297 Accuracy 0.889750063419342\n",
      "Iteration 14920 Training loss 0.00036980558070354164 Validation loss 0.04319164529442787 Accuracy 0.8913750648498535\n",
      "Iteration 14930 Training loss 0.0010990819428116083 Validation loss 0.043123360723257065 Accuracy 0.8916250467300415\n",
      "Iteration 14940 Training loss 0.0005629778606817126 Validation loss 0.04321461543440819 Accuracy 0.8905000686645508\n",
      "Iteration 14950 Training loss 0.00155573186930269 Validation loss 0.043078936636447906 Accuracy 0.8908750414848328\n",
      "Iteration 14960 Training loss 0.0017016492784023285 Validation loss 0.04315544292330742 Accuracy 0.89000004529953\n",
      "Iteration 14970 Training loss 0.005875735078006983 Validation loss 0.04292421415448189 Accuracy 0.890375018119812\n",
      "Iteration 14980 Training loss 0.001639493857510388 Validation loss 0.04295661300420761 Accuracy 0.890375018119812\n",
      "Iteration 14990 Training loss 0.0005835043266415596 Validation loss 0.042919009923934937 Accuracy 0.8907500505447388\n",
      "Iteration 15000 Training loss 0.0028454677667468786 Validation loss 0.04292432591319084 Accuracy 0.89000004529953\n",
      "Iteration 15010 Training loss 0.00038841005880385637 Validation loss 0.0431484580039978 Accuracy 0.8906250596046448\n",
      "Iteration 15020 Training loss 0.0006630626157857478 Validation loss 0.042892854660749435 Accuracy 0.8905000686645508\n",
      "Iteration 15030 Training loss 0.0016150337178260088 Validation loss 0.043118637055158615 Accuracy 0.8917500376701355\n",
      "Iteration 15040 Training loss 0.0008838405483402312 Validation loss 0.04417388513684273 Accuracy 0.8910000324249268\n",
      "Iteration 15050 Training loss 0.003261895151808858 Validation loss 0.0427754707634449 Accuracy 0.8923750519752502\n",
      "Iteration 15060 Training loss 0.0015834965743124485 Validation loss 0.04315118491649628 Accuracy 0.8912500143051147\n",
      "Iteration 15070 Training loss 0.005160380620509386 Validation loss 0.0430799201130867 Accuracy 0.8912500143051147\n",
      "Iteration 15080 Training loss 0.004287250339984894 Validation loss 0.04361272603273392 Accuracy 0.8888750672340393\n",
      "Iteration 15090 Training loss 0.005359607748687267 Validation loss 0.042899034917354584 Accuracy 0.8917500376701355\n",
      "Iteration 15100 Training loss 0.0028579235076904297 Validation loss 0.04331629350781441 Accuracy 0.890375018119812\n",
      "Iteration 15110 Training loss 0.0002165755140595138 Validation loss 0.04308680444955826 Accuracy 0.8910000324249268\n",
      "Iteration 15120 Training loss 0.002045501722022891 Validation loss 0.04329986125230789 Accuracy 0.89000004529953\n",
      "Iteration 15130 Training loss 0.004117253702133894 Validation loss 0.04322793707251549 Accuracy 0.8893750309944153\n",
      "Iteration 15140 Training loss 0.001505899359472096 Validation loss 0.04314151033759117 Accuracy 0.8911250233650208\n",
      "Iteration 15150 Training loss 0.0015468125930055976 Validation loss 0.04307958483695984 Accuracy 0.8891250491142273\n",
      "Iteration 15160 Training loss 0.00033310087746940553 Validation loss 0.043045248836278915 Accuracy 0.8910000324249268\n",
      "Iteration 15170 Training loss 0.00024202799249906093 Validation loss 0.043230753391981125 Accuracy 0.8926250338554382\n",
      "Iteration 15180 Training loss 0.005396251566708088 Validation loss 0.0433456152677536 Accuracy 0.8908750414848328\n",
      "Iteration 15190 Training loss 0.0006099893362261355 Validation loss 0.04290706664323807 Accuracy 0.8916250467300415\n",
      "Iteration 15200 Training loss 0.0016640233807265759 Validation loss 0.04312805458903313 Accuracy 0.8915000557899475\n",
      "Iteration 15210 Training loss 0.04824911803007126 Validation loss 0.06852275133132935 Accuracy 0.8480000495910645\n",
      "Iteration 15220 Training loss 0.04253552854061127 Validation loss 0.0753927230834961 Accuracy 0.8211250305175781\n",
      "Iteration 15230 Training loss 0.04092506319284439 Validation loss 0.0648559108376503 Accuracy 0.8403750658035278\n",
      "Iteration 15240 Training loss 0.05242341756820679 Validation loss 0.0703037902712822 Accuracy 0.8352500200271606\n",
      "Iteration 15250 Training loss 0.0069615659303963184 Validation loss 0.048022061586380005 Accuracy 0.8766250610351562\n",
      "Iteration 15260 Training loss 0.00321399443782866 Validation loss 0.04583767428994179 Accuracy 0.877500057220459\n",
      "Iteration 15270 Training loss 0.004376397468149662 Validation loss 0.04375069588422775 Accuracy 0.8881250619888306\n",
      "Iteration 15280 Training loss 0.0016065709060057998 Validation loss 0.04345005378127098 Accuracy 0.8876250386238098\n",
      "Iteration 15290 Training loss 0.005436981096863747 Validation loss 0.042972397059202194 Accuracy 0.889875054359436\n",
      "Iteration 15300 Training loss 0.0047828517854213715 Validation loss 0.04273807629942894 Accuracy 0.893125057220459\n",
      "Iteration 15310 Training loss 0.0009571588598191738 Validation loss 0.043090105056762695 Accuracy 0.8911250233650208\n",
      "Iteration 15320 Training loss 0.0029123106505721807 Validation loss 0.04300548881292343 Accuracy 0.8911250233650208\n",
      "Iteration 15330 Training loss 0.0008859114022925496 Validation loss 0.043040286749601364 Accuracy 0.89000004529953\n",
      "Iteration 15340 Training loss 0.0009264376712962985 Validation loss 0.042965635657310486 Accuracy 0.8905000686645508\n",
      "Iteration 15350 Training loss 0.003915245644748211 Validation loss 0.04256286099553108 Accuracy 0.8940000534057617\n",
      "Iteration 15360 Training loss 0.004484732169657946 Validation loss 0.04272809624671936 Accuracy 0.8926250338554382\n",
      "Iteration 15370 Training loss 0.0036736130714416504 Validation loss 0.04280678927898407 Accuracy 0.8911250233650208\n",
      "Iteration 15380 Training loss 0.0004051171417813748 Validation loss 0.04321013018488884 Accuracy 0.8891250491142273\n",
      "Iteration 15390 Training loss 0.003309764200821519 Validation loss 0.04302137717604637 Accuracy 0.8908750414848328\n",
      "Iteration 15400 Training loss 0.0020033835899084806 Validation loss 0.04318603500723839 Accuracy 0.8911250233650208\n",
      "Iteration 15410 Training loss 0.0038380443584173918 Validation loss 0.044163499027490616 Accuracy 0.8857500553131104\n",
      "Iteration 15420 Training loss 0.0023073304910212755 Validation loss 0.043289270251989365 Accuracy 0.8888750672340393\n",
      "Iteration 15430 Training loss 0.0019459477625787258 Validation loss 0.0429212749004364 Accuracy 0.8920000195503235\n",
      "Iteration 15440 Training loss 0.003115219995379448 Validation loss 0.042838629335165024 Accuracy 0.8920000195503235\n",
      "Iteration 15450 Training loss 0.0004382977494969964 Validation loss 0.043024834245443344 Accuracy 0.8918750286102295\n",
      "Iteration 15460 Training loss 0.0005560321733355522 Validation loss 0.042931750416755676 Accuracy 0.8922500610351562\n",
      "Iteration 15470 Training loss 0.0017370538553223014 Validation loss 0.043117374181747437 Accuracy 0.8911250233650208\n",
      "Iteration 15480 Training loss 0.001896524103358388 Validation loss 0.04357830435037613 Accuracy 0.8893750309944153\n",
      "Iteration 15490 Training loss 0.003022673074156046 Validation loss 0.04296870529651642 Accuracy 0.8906250596046448\n",
      "Iteration 15500 Training loss 0.0016179783269762993 Validation loss 0.043145857751369476 Accuracy 0.8926250338554382\n",
      "Iteration 15510 Training loss 0.0005941635463386774 Validation loss 0.04340169578790665 Accuracy 0.8895000219345093\n",
      "Iteration 15520 Training loss 0.0028141725342720747 Validation loss 0.04323244094848633 Accuracy 0.8890000581741333\n",
      "Iteration 15530 Training loss 0.0016646088333800435 Validation loss 0.043253205716609955 Accuracy 0.8905000686645508\n",
      "Iteration 15540 Training loss 0.0004884087247774005 Validation loss 0.042963769286870956 Accuracy 0.8913750648498535\n",
      "Iteration 15550 Training loss 0.002893553813919425 Validation loss 0.043089307844638824 Accuracy 0.8907500505447388\n",
      "Iteration 15560 Training loss 0.005878003779798746 Validation loss 0.043051619082689285 Accuracy 0.8911250233650208\n",
      "Iteration 15570 Training loss 0.002957044169306755 Validation loss 0.04326518997550011 Accuracy 0.8905000686645508\n",
      "Iteration 15580 Training loss 0.0020908319856971502 Validation loss 0.04307550936937332 Accuracy 0.893125057220459\n",
      "Iteration 15590 Training loss 0.0033235603477805853 Validation loss 0.043207865208387375 Accuracy 0.8911250233650208\n",
      "Iteration 15600 Training loss 0.0029963727574795485 Validation loss 0.043371282517910004 Accuracy 0.890375018119812\n",
      "Iteration 15610 Training loss 0.0016256120288744569 Validation loss 0.043336182832717896 Accuracy 0.890250027179718\n",
      "Iteration 15620 Training loss 0.0018094152910634875 Validation loss 0.04304413124918938 Accuracy 0.8910000324249268\n",
      "Iteration 15630 Training loss 0.003077005036175251 Validation loss 0.043381739407777786 Accuracy 0.8905000686645508\n",
      "Iteration 15640 Training loss 0.00040661334060132504 Validation loss 0.04335689917206764 Accuracy 0.8917500376701355\n",
      "Iteration 15650 Training loss 0.0003230256261304021 Validation loss 0.043126121163368225 Accuracy 0.8916250467300415\n",
      "Iteration 15660 Training loss 0.00160696217790246 Validation loss 0.043129969388246536 Accuracy 0.893000066280365\n",
      "Iteration 15670 Training loss 0.0028640057425945997 Validation loss 0.04299746826291084 Accuracy 0.8921250700950623\n",
      "Iteration 15680 Training loss 0.0042569455690681934 Validation loss 0.04339689388871193 Accuracy 0.8906250596046448\n",
      "Iteration 15690 Training loss 0.0019197752699255943 Validation loss 0.04302920773625374 Accuracy 0.893250048160553\n",
      "Iteration 15700 Training loss 0.00032471492886543274 Validation loss 0.04317505285143852 Accuracy 0.8907500505447388\n",
      "Iteration 15710 Training loss 0.0031543844379484653 Validation loss 0.04303089529275894 Accuracy 0.8908750414848328\n",
      "Iteration 15720 Training loss 0.0029036574997007847 Validation loss 0.04293793439865112 Accuracy 0.8918750286102295\n",
      "Iteration 15730 Training loss 0.00041243419400416315 Validation loss 0.04310045763850212 Accuracy 0.8915000557899475\n",
      "Iteration 15740 Training loss 0.004589037969708443 Validation loss 0.042923372238874435 Accuracy 0.8917500376701355\n",
      "Iteration 15750 Training loss 0.0015993114793673158 Validation loss 0.04297872260212898 Accuracy 0.8916250467300415\n",
      "Iteration 15760 Training loss 0.0030827312730252743 Validation loss 0.04554268717765808 Accuracy 0.8851250410079956\n",
      "Iteration 15770 Training loss 0.00032085514976643026 Validation loss 0.0432242751121521 Accuracy 0.8916250467300415\n",
      "Iteration 15780 Training loss 0.00302120135165751 Validation loss 0.0435042679309845 Accuracy 0.8887500166893005\n",
      "Iteration 15790 Training loss 0.0015954435802996159 Validation loss 0.043111652135849 Accuracy 0.8920000195503235\n",
      "Iteration 15800 Training loss 0.0015199790941551328 Validation loss 0.04307528957724571 Accuracy 0.8895000219345093\n",
      "Iteration 15810 Training loss 0.11001947522163391 Validation loss 0.1103515625 Accuracy 0.7588750123977661\n",
      "Iteration 15820 Training loss 0.013975664973258972 Validation loss 0.050709713250398636 Accuracy 0.8725000619888306\n",
      "Iteration 15830 Training loss 0.004748963750898838 Validation loss 0.04450584575533867 Accuracy 0.8846250176429749\n",
      "Iteration 15840 Training loss 0.0027963730972260237 Validation loss 0.04409969970583916 Accuracy 0.8857500553131104\n",
      "Iteration 15850 Training loss 0.006133162882179022 Validation loss 0.043446335941553116 Accuracy 0.8878750205039978\n",
      "Iteration 15860 Training loss 0.0020805876702070236 Validation loss 0.04358359053730965 Accuracy 0.8873750567436218\n",
      "Iteration 15870 Training loss 0.0029466424603015184 Validation loss 0.04343371093273163 Accuracy 0.8888750672340393\n",
      "Iteration 15880 Training loss 0.0021213272120803595 Validation loss 0.0434013195335865 Accuracy 0.8876250386238098\n",
      "Iteration 15890 Training loss 0.002983447164297104 Validation loss 0.04326014220714569 Accuracy 0.8891250491142273\n",
      "Iteration 15900 Training loss 0.003383386880159378 Validation loss 0.04319284111261368 Accuracy 0.89000004529953\n",
      "Iteration 15910 Training loss 0.001731428666971624 Validation loss 0.04348320513963699 Accuracy 0.89000004529953\n",
      "Iteration 15920 Training loss 0.003080677008256316 Validation loss 0.043293777853250504 Accuracy 0.890125036239624\n",
      "Iteration 15930 Training loss 0.0019934403244405985 Validation loss 0.04482080414891243 Accuracy 0.8873750567436218\n",
      "Iteration 15940 Training loss 0.0015821590786799788 Validation loss 0.04309331253170967 Accuracy 0.8916250467300415\n",
      "Iteration 15950 Training loss 0.0016549325082451105 Validation loss 0.043102458119392395 Accuracy 0.8910000324249268\n",
      "Iteration 15960 Training loss 0.0025888835079967976 Validation loss 0.04305480048060417 Accuracy 0.8923750519752502\n",
      "Iteration 15970 Training loss 0.004252503626048565 Validation loss 0.04299861565232277 Accuracy 0.8916250467300415\n",
      "Iteration 15980 Training loss 0.0003092784609179944 Validation loss 0.043301861733198166 Accuracy 0.8911250233650208\n",
      "Iteration 15990 Training loss 0.0003838398843072355 Validation loss 0.04313753545284271 Accuracy 0.8911250233650208\n",
      "Iteration 16000 Training loss 0.0029798077885061502 Validation loss 0.04344703257083893 Accuracy 0.8910000324249268\n",
      "Iteration 16010 Training loss 0.0009429848869331181 Validation loss 0.043302759528160095 Accuracy 0.8910000324249268\n",
      "Iteration 16020 Training loss 0.0003741937398444861 Validation loss 0.04323513060808182 Accuracy 0.8913750648498535\n",
      "Iteration 16030 Training loss 0.0020368611440062523 Validation loss 0.04369351267814636 Accuracy 0.8893750309944153\n",
      "Iteration 16040 Training loss 0.0035341717302799225 Validation loss 0.04315604642033577 Accuracy 0.8920000195503235\n",
      "Iteration 16050 Training loss 0.001634281245060265 Validation loss 0.0432589128613472 Accuracy 0.8912500143051147\n",
      "Iteration 16060 Training loss 0.0033720810897648335 Validation loss 0.04424833878874779 Accuracy 0.8873750567436218\n",
      "Iteration 16070 Training loss 0.0028101031202822924 Validation loss 0.04327160865068436 Accuracy 0.8926250338554382\n",
      "Iteration 16080 Training loss 0.0003521295147947967 Validation loss 0.04302842915058136 Accuracy 0.8920000195503235\n",
      "Iteration 16090 Training loss 0.005332942120730877 Validation loss 0.04331986606121063 Accuracy 0.8910000324249268\n",
      "Iteration 16100 Training loss 0.002891330048441887 Validation loss 0.04311757907271385 Accuracy 0.8918750286102295\n",
      "Iteration 16110 Training loss 0.0028024481143802404 Validation loss 0.043212298303842545 Accuracy 0.8910000324249268\n",
      "Iteration 16120 Training loss 0.0015554312849417329 Validation loss 0.04302215203642845 Accuracy 0.8923750519752502\n",
      "Iteration 16130 Training loss 0.0016781871672719717 Validation loss 0.04321644455194473 Accuracy 0.8915000557899475\n",
      "Iteration 16140 Training loss 0.0030672657303512096 Validation loss 0.04335831478238106 Accuracy 0.8920000195503235\n",
      "Iteration 16150 Training loss 0.00037214483018033206 Validation loss 0.043303973972797394 Accuracy 0.8912500143051147\n",
      "Iteration 16160 Training loss 0.001641607959754765 Validation loss 0.04335213825106621 Accuracy 0.8910000324249268\n",
      "Iteration 16170 Training loss 0.0028924185317009687 Validation loss 0.043281253427267075 Accuracy 0.8906250596046448\n",
      "Iteration 16180 Training loss 0.0032321836333721876 Validation loss 0.04333578795194626 Accuracy 0.8916250467300415\n",
      "Iteration 16190 Training loss 0.00293903611600399 Validation loss 0.04324416443705559 Accuracy 0.890375018119812\n",
      "Iteration 16200 Training loss 0.0015830191550776362 Validation loss 0.04322121664881706 Accuracy 0.8906250596046448\n",
      "Iteration 16210 Training loss 0.00291119865141809 Validation loss 0.04318632557988167 Accuracy 0.8923750519752502\n",
      "Iteration 16220 Training loss 0.0002757966867648065 Validation loss 0.04349571093916893 Accuracy 0.8892500400543213\n",
      "Iteration 16230 Training loss 0.0003202187945134938 Validation loss 0.04349995777010918 Accuracy 0.890250027179718\n",
      "Iteration 16240 Training loss 0.005280714016407728 Validation loss 0.04323268309235573 Accuracy 0.8921250700950623\n",
      "Iteration 16250 Training loss 0.002073461888357997 Validation loss 0.043271906673908234 Accuracy 0.8917500376701355\n",
      "Iteration 16260 Training loss 0.004139263648539782 Validation loss 0.04313886538147926 Accuracy 0.8925000429153442\n",
      "Iteration 16270 Training loss 0.0014954502694308758 Validation loss 0.04323574900627136 Accuracy 0.8940000534057617\n",
      "Iteration 16280 Training loss 0.002929544309154153 Validation loss 0.04338957741856575 Accuracy 0.8922500610351562\n",
      "Iteration 16290 Training loss 0.004039101302623749 Validation loss 0.04316231608390808 Accuracy 0.8925000429153442\n",
      "Iteration 16300 Training loss 0.002805150579661131 Validation loss 0.04317518696188927 Accuracy 0.8920000195503235\n",
      "Iteration 16310 Training loss 0.001786884036846459 Validation loss 0.04347476363182068 Accuracy 0.8911250233650208\n",
      "Iteration 16320 Training loss 0.0019683220889419317 Validation loss 0.04349540174007416 Accuracy 0.89000004529953\n",
      "Iteration 16330 Training loss 0.00034585906541906297 Validation loss 0.04318804293870926 Accuracy 0.8906250596046448\n",
      "Iteration 16340 Training loss 0.0017359977355226874 Validation loss 0.043739017099142075 Accuracy 0.8912500143051147\n",
      "Iteration 16350 Training loss 0.0024135976564139128 Validation loss 0.04315803945064545 Accuracy 0.8913750648498535\n",
      "Iteration 16360 Training loss 0.001988906180486083 Validation loss 0.0433708131313324 Accuracy 0.8916250467300415\n",
      "Iteration 16370 Training loss 0.0005352360894903541 Validation loss 0.04312313348054886 Accuracy 0.8912500143051147\n",
      "Iteration 16380 Training loss 0.0028963261283934116 Validation loss 0.043040093034505844 Accuracy 0.893250048160553\n",
      "Iteration 16390 Training loss 0.0014934297651052475 Validation loss 0.04324791207909584 Accuracy 0.8918750286102295\n",
      "Iteration 16400 Training loss 0.00032981621916405857 Validation loss 0.04326983168721199 Accuracy 0.8927500247955322\n",
      "Iteration 16410 Training loss 0.0024371084291487932 Validation loss 0.043078530579805374 Accuracy 0.8926250338554382\n",
      "Iteration 16420 Training loss 0.0006377917598001659 Validation loss 0.0432375967502594 Accuracy 0.8915000557899475\n",
      "Iteration 16430 Training loss 0.0028939752373844385 Validation loss 0.04316428676247597 Accuracy 0.8927500247955322\n",
      "Iteration 16440 Training loss 0.0028824666514992714 Validation loss 0.0430724062025547 Accuracy 0.8942500352859497\n",
      "Iteration 16450 Training loss 0.0005036580841988325 Validation loss 0.04329117760062218 Accuracy 0.890375018119812\n",
      "Iteration 16460 Training loss 0.004147768020629883 Validation loss 0.04339483007788658 Accuracy 0.8910000324249268\n",
      "Iteration 16470 Training loss 0.00040675653144717216 Validation loss 0.04333531856536865 Accuracy 0.8912500143051147\n",
      "Iteration 16480 Training loss 0.002965024672448635 Validation loss 0.04333934187889099 Accuracy 0.8906250596046448\n",
      "Iteration 16490 Training loss 0.0016444793436676264 Validation loss 0.04367657005786896 Accuracy 0.889750063419342\n",
      "Iteration 16500 Training loss 0.0015789719764143229 Validation loss 0.04330695420503616 Accuracy 0.8923750519752502\n",
      "Iteration 16510 Training loss 0.0001683029840933159 Validation loss 0.0433979295194149 Accuracy 0.89000004529953\n",
      "Iteration 16520 Training loss 0.00033460211125202477 Validation loss 0.04321512207388878 Accuracy 0.8916250467300415\n",
      "Iteration 16530 Training loss 0.0002880401734728366 Validation loss 0.04322357475757599 Accuracy 0.8918750286102295\n",
      "Iteration 16540 Training loss 0.00022907301899977028 Validation loss 0.04334703087806702 Accuracy 0.8910000324249268\n",
      "Iteration 16550 Training loss 0.0004628640308510512 Validation loss 0.043323542922735214 Accuracy 0.8917500376701355\n",
      "Iteration 16560 Training loss 0.000379165168851614 Validation loss 0.04323365166783333 Accuracy 0.8916250467300415\n",
      "Iteration 16570 Training loss 0.0014863016549497843 Validation loss 0.043201103806495667 Accuracy 0.8918750286102295\n",
      "Iteration 16580 Training loss 0.0015758770750835538 Validation loss 0.04316901043057442 Accuracy 0.8917500376701355\n",
      "Iteration 16590 Training loss 0.0015993897104635835 Validation loss 0.04335184767842293 Accuracy 0.8908750414848328\n",
      "Iteration 16600 Training loss 0.001993712969124317 Validation loss 0.04357297718524933 Accuracy 0.8912500143051147\n",
      "Iteration 16610 Training loss 0.0002444035199005157 Validation loss 0.043348170816898346 Accuracy 0.8908750414848328\n",
      "Iteration 16620 Training loss 0.004379947669804096 Validation loss 0.043350137770175934 Accuracy 0.8915000557899475\n",
      "Iteration 16630 Training loss 0.0016173466574400663 Validation loss 0.04317711666226387 Accuracy 0.8922500610351562\n",
      "Iteration 16640 Training loss 0.004014034755527973 Validation loss 0.04321117699146271 Accuracy 0.893250048160553\n",
      "Iteration 16650 Training loss 0.006523946765810251 Validation loss 0.043168727308511734 Accuracy 0.8921250700950623\n",
      "Iteration 16660 Training loss 0.0028332239016890526 Validation loss 0.04346410557627678 Accuracy 0.8912500143051147\n",
      "Iteration 16670 Training loss 0.0027002182323485613 Validation loss 0.04320903494954109 Accuracy 0.8916250467300415\n",
      "Iteration 16680 Training loss 0.0013859692262485623 Validation loss 0.04339827969670296 Accuracy 0.8910000324249268\n",
      "Iteration 16690 Training loss 0.0025546783581376076 Validation loss 0.043471526354551315 Accuracy 0.8908750414848328\n",
      "Iteration 16700 Training loss 0.002785953227430582 Validation loss 0.043371956795454025 Accuracy 0.8923750519752502\n",
      "Iteration 16710 Training loss 0.0015700326766818762 Validation loss 0.043297410011291504 Accuracy 0.8916250467300415\n",
      "Iteration 16720 Training loss 0.005477464757859707 Validation loss 0.043444085866212845 Accuracy 0.8907500505447388\n",
      "Iteration 16730 Training loss 0.005377267487347126 Validation loss 0.043885353952646255 Accuracy 0.8895000219345093\n",
      "Iteration 16740 Training loss 0.0016419088933616877 Validation loss 0.04400103911757469 Accuracy 0.889750063419342\n",
      "Iteration 16750 Training loss 0.0015532826073467731 Validation loss 0.04334971308708191 Accuracy 0.8910000324249268\n",
      "Iteration 16760 Training loss 0.0030705605167895555 Validation loss 0.043299078941345215 Accuracy 0.8906250596046448\n",
      "Iteration 16770 Training loss 0.0013766753254458308 Validation loss 0.043657682836055756 Accuracy 0.889875054359436\n",
      "Iteration 16780 Training loss 0.00018719426589086652 Validation loss 0.043256502598524094 Accuracy 0.8923750519752502\n",
      "Iteration 16790 Training loss 0.0004714774258900434 Validation loss 0.044819049537181854 Accuracy 0.8868750333786011\n",
      "Iteration 16800 Training loss 0.0027566575445234776 Validation loss 0.04361781105399132 Accuracy 0.890250027179718\n",
      "Iteration 16810 Training loss 0.0032896820921450853 Validation loss 0.04339442029595375 Accuracy 0.889875054359436\n",
      "Iteration 16820 Training loss 0.0027688592672348022 Validation loss 0.043517883867025375 Accuracy 0.8907500505447388\n",
      "Iteration 16830 Training loss 0.0002591522934380919 Validation loss 0.04345235601067543 Accuracy 0.8907500505447388\n",
      "Iteration 16840 Training loss 0.00150026916526258 Validation loss 0.043551743030548096 Accuracy 0.8895000219345093\n",
      "Iteration 16850 Training loss 0.005309475585818291 Validation loss 0.04410463199019432 Accuracy 0.8890000581741333\n",
      "Iteration 16860 Training loss 0.0014066626317799091 Validation loss 0.043359022587537766 Accuracy 0.8916250467300415\n",
      "Iteration 16870 Training loss 0.00022294123482424766 Validation loss 0.04346896708011627 Accuracy 0.8906250596046448\n",
      "Iteration 16880 Training loss 0.0003241920785512775 Validation loss 0.043645091354846954 Accuracy 0.889750063419342\n",
      "Iteration 16890 Training loss 0.0027546966448426247 Validation loss 0.04341503232717514 Accuracy 0.8906250596046448\n",
      "Iteration 16900 Training loss 0.0007128228317014873 Validation loss 0.0434485599398613 Accuracy 0.8928750157356262\n",
      "Iteration 16910 Training loss 0.002227583434432745 Validation loss 0.04362867772579193 Accuracy 0.8895000219345093\n",
      "Iteration 16920 Training loss 0.0006587809766642749 Validation loss 0.043919917196035385 Accuracy 0.8896250128746033\n",
      "Iteration 16930 Training loss 0.0002614959666971117 Validation loss 0.04330577701330185 Accuracy 0.8911250233650208\n",
      "Iteration 16940 Training loss 0.0018456256948411465 Validation loss 0.04452299699187279 Accuracy 0.8877500295639038\n",
      "Iteration 16950 Training loss 0.0016060438938438892 Validation loss 0.04331484064459801 Accuracy 0.8906250596046448\n",
      "Iteration 16960 Training loss 0.0015542643377557397 Validation loss 0.043439168483018875 Accuracy 0.889875054359436\n",
      "Iteration 16970 Training loss 0.0016501498175784945 Validation loss 0.043569862842559814 Accuracy 0.889750063419342\n",
      "Iteration 16980 Training loss 0.0026820367202162743 Validation loss 0.043418917804956436 Accuracy 0.8908750414848328\n",
      "Iteration 16990 Training loss 0.0015355488285422325 Validation loss 0.043717481195926666 Accuracy 0.889750063419342\n",
      "Iteration 17000 Training loss 0.0014508073218166828 Validation loss 0.04342332482337952 Accuracy 0.8908750414848328\n",
      "Iteration 17010 Training loss 0.00019734003581106663 Validation loss 0.04334188252687454 Accuracy 0.8908750414848328\n",
      "Iteration 17020 Training loss 0.0002788153651636094 Validation loss 0.043310023844242096 Accuracy 0.8918750286102295\n",
      "Iteration 17030 Training loss 0.0039011328481137753 Validation loss 0.045931845903396606 Accuracy 0.8865000605583191\n",
      "Iteration 17040 Training loss 0.0027245490346103907 Validation loss 0.0433276891708374 Accuracy 0.8908750414848328\n",
      "Iteration 17050 Training loss 0.0026832467410713434 Validation loss 0.043234650045633316 Accuracy 0.8917500376701355\n",
      "Iteration 17060 Training loss 0.002821909496560693 Validation loss 0.0433603897690773 Accuracy 0.8906250596046448\n",
      "Iteration 17070 Training loss 0.001892673666588962 Validation loss 0.04342823475599289 Accuracy 0.8915000557899475\n",
      "Iteration 17080 Training loss 0.001570201013237238 Validation loss 0.0434814915060997 Accuracy 0.8915000557899475\n",
      "Iteration 17090 Training loss 0.0003596817550715059 Validation loss 0.0434197299182415 Accuracy 0.8913750648498535\n",
      "Iteration 17100 Training loss 0.002836993196979165 Validation loss 0.04352947324514389 Accuracy 0.8922500610351562\n",
      "Iteration 17110 Training loss 0.0065805958583951 Validation loss 0.043790772557258606 Accuracy 0.8890000581741333\n",
      "Iteration 17120 Training loss 0.0027401575352996588 Validation loss 0.04347529634833336 Accuracy 0.8927500247955322\n",
      "Iteration 17130 Training loss 0.0018184815999120474 Validation loss 0.0434064082801342 Accuracy 0.8915000557899475\n",
      "Iteration 17140 Training loss 0.0019902156200259924 Validation loss 0.04369908571243286 Accuracy 0.8895000219345093\n",
      "Iteration 17150 Training loss 0.0003755690122488886 Validation loss 0.04346105456352234 Accuracy 0.8908750414848328\n",
      "Iteration 17160 Training loss 0.002697467105463147 Validation loss 0.0435602143406868 Accuracy 0.8910000324249268\n",
      "Iteration 17170 Training loss 0.003984429873526096 Validation loss 0.043650079518556595 Accuracy 0.889750063419342\n",
      "Iteration 17180 Training loss 0.00019136990886181593 Validation loss 0.04342162609100342 Accuracy 0.8916250467300415\n",
      "Iteration 17190 Training loss 0.0027117605786770582 Validation loss 0.04354770481586456 Accuracy 0.8907500505447388\n",
      "Iteration 17200 Training loss 0.0026693399995565414 Validation loss 0.04356597363948822 Accuracy 0.890375018119812\n",
      "Iteration 17210 Training loss 0.0026956135407090187 Validation loss 0.043501149863004684 Accuracy 0.8906250596046448\n",
      "Iteration 17220 Training loss 0.0014596445253118873 Validation loss 0.04395601153373718 Accuracy 0.8886250257492065\n",
      "Iteration 17230 Training loss 0.00016487120592501014 Validation loss 0.04348788782954216 Accuracy 0.8908750414848328\n",
      "Iteration 17240 Training loss 0.0014630339574068785 Validation loss 0.04344332963228226 Accuracy 0.8916250467300415\n",
      "Iteration 17250 Training loss 0.0014066752046346664 Validation loss 0.043480318039655685 Accuracy 0.890250027179718\n",
      "Iteration 17260 Training loss 0.001471396186389029 Validation loss 0.04341224953532219 Accuracy 0.8910000324249268\n",
      "Iteration 17270 Training loss 0.0002853429759852588 Validation loss 0.04358002915978432 Accuracy 0.8907500505447388\n",
      "Iteration 17280 Training loss 0.00015696592163294554 Validation loss 0.04365307092666626 Accuracy 0.890125036239624\n",
      "Iteration 17290 Training loss 0.0003618437622208148 Validation loss 0.04354729503393173 Accuracy 0.89000004529953\n",
      "Iteration 17300 Training loss 0.0027217771857976913 Validation loss 0.04372890666127205 Accuracy 0.890125036239624\n",
      "Iteration 17310 Training loss 0.0015509481308981776 Validation loss 0.04339146614074707 Accuracy 0.8915000557899475\n",
      "Iteration 17320 Training loss 0.0001612127962289378 Validation loss 0.04344916716217995 Accuracy 0.8910000324249268\n",
      "Iteration 17330 Training loss 0.000427854509325698 Validation loss 0.04354983568191528 Accuracy 0.889750063419342\n",
      "Iteration 17340 Training loss 0.002428078558295965 Validation loss 0.04589208588004112 Accuracy 0.8815000653266907\n",
      "Iteration 17350 Training loss 0.001651055528782308 Validation loss 0.04351538419723511 Accuracy 0.890375018119812\n",
      "Iteration 17360 Training loss 0.0028278708923608065 Validation loss 0.04327136278152466 Accuracy 0.8923750519752502\n",
      "Iteration 17370 Training loss 0.00031509078689850867 Validation loss 0.04362039268016815 Accuracy 0.8905000686645508\n",
      "Iteration 17380 Training loss 0.0027124732732772827 Validation loss 0.04335664585232735 Accuracy 0.8913750648498535\n",
      "Iteration 17390 Training loss 0.0014488545712083578 Validation loss 0.04332972317934036 Accuracy 0.8913750648498535\n",
      "Iteration 17400 Training loss 0.001547149964608252 Validation loss 0.043539468199014664 Accuracy 0.8908750414848328\n",
      "Iteration 17410 Training loss 0.0021322318352758884 Validation loss 0.04400366544723511 Accuracy 0.8907500505447388\n",
      "Iteration 17420 Training loss 0.001877556904219091 Validation loss 0.04358650743961334 Accuracy 0.8906250596046448\n",
      "Iteration 17430 Training loss 0.001525110681541264 Validation loss 0.04351622238755226 Accuracy 0.8896250128746033\n",
      "Iteration 17440 Training loss 0.00031002212199382484 Validation loss 0.04336002469062805 Accuracy 0.8910000324249268\n",
      "Iteration 17450 Training loss 0.0003117532469332218 Validation loss 0.043430671095848083 Accuracy 0.8917500376701355\n",
      "Iteration 17460 Training loss 0.002693618880584836 Validation loss 0.04359511286020279 Accuracy 0.8908750414848328\n",
      "Iteration 17470 Training loss 0.001390844234265387 Validation loss 0.043830402195453644 Accuracy 0.8895000219345093\n",
      "Iteration 17480 Training loss 0.002758104121312499 Validation loss 0.04351354390382767 Accuracy 0.8906250596046448\n",
      "Iteration 17490 Training loss 0.00023579275875817984 Validation loss 0.04362177103757858 Accuracy 0.8895000219345093\n",
      "Iteration 17500 Training loss 0.005216363351792097 Validation loss 0.043414805084466934 Accuracy 0.8905000686645508\n",
      "Iteration 17510 Training loss 0.0005392794846557081 Validation loss 0.04347200319170952 Accuracy 0.890375018119812\n",
      "Iteration 17520 Training loss 0.005191258154809475 Validation loss 0.04348735511302948 Accuracy 0.8912500143051147\n",
      "Iteration 17530 Training loss 0.0014521891716867685 Validation loss 0.04340313747525215 Accuracy 0.890250027179718\n",
      "Iteration 17540 Training loss 0.0026751277036964893 Validation loss 0.04362097382545471 Accuracy 0.889750063419342\n",
      "Iteration 17550 Training loss 0.003133130492642522 Validation loss 0.04399924725294113 Accuracy 0.8878750205039978\n",
      "Iteration 17560 Training loss 0.0027884207665920258 Validation loss 0.043480392545461655 Accuracy 0.8906250596046448\n",
      "Iteration 17570 Training loss 0.0006182980141602457 Validation loss 0.043740175664424896 Accuracy 0.890125036239624\n",
      "Iteration 17580 Training loss 0.0028852939140051603 Validation loss 0.04358075186610222 Accuracy 0.8910000324249268\n",
      "Iteration 17590 Training loss 0.002755815861746669 Validation loss 0.043507855385541916 Accuracy 0.8922500610351562\n",
      "Iteration 17600 Training loss 0.003921564668416977 Validation loss 0.0433625765144825 Accuracy 0.8921250700950623\n",
      "Iteration 17610 Training loss 0.0026854833122342825 Validation loss 0.043510157614946365 Accuracy 0.8910000324249268\n",
      "Iteration 17620 Training loss 0.00015611617709510028 Validation loss 0.04346337914466858 Accuracy 0.8906250596046448\n",
      "Iteration 17630 Training loss 0.0027563958428800106 Validation loss 0.04352109134197235 Accuracy 0.8910000324249268\n",
      "Iteration 17640 Training loss 0.0003276902425568551 Validation loss 0.043552737683057785 Accuracy 0.8910000324249268\n",
      "Iteration 17650 Training loss 0.0015270912554115057 Validation loss 0.043528154492378235 Accuracy 0.8927500247955322\n",
      "Iteration 17660 Training loss 0.002779343631118536 Validation loss 0.04347790405154228 Accuracy 0.8906250596046448\n",
      "Iteration 17670 Training loss 0.001478303223848343 Validation loss 0.04371434822678566 Accuracy 0.889875054359436\n",
      "Iteration 17680 Training loss 0.001488430774770677 Validation loss 0.043449729681015015 Accuracy 0.8915000557899475\n",
      "Iteration 17690 Training loss 0.0015457059489563107 Validation loss 0.043735504150390625 Accuracy 0.8906250596046448\n",
      "Iteration 17700 Training loss 0.00021765357814729214 Validation loss 0.04331755265593529 Accuracy 0.8906250596046448\n",
      "Iteration 17710 Training loss 0.0014979448169469833 Validation loss 0.043469011783599854 Accuracy 0.8908750414848328\n",
      "Iteration 17720 Training loss 0.002655588323250413 Validation loss 0.04338394105434418 Accuracy 0.8910000324249268\n",
      "Iteration 17730 Training loss 0.005211898125708103 Validation loss 0.04454413801431656 Accuracy 0.8852500319480896\n",
      "Iteration 17740 Training loss 0.0022362847812473774 Validation loss 0.04368024691939354 Accuracy 0.8916250467300415\n",
      "Iteration 17750 Training loss 0.0003988739335909486 Validation loss 0.04349343106150627 Accuracy 0.8913750648498535\n",
      "Iteration 17760 Training loss 0.0014740375336259604 Validation loss 0.04345782846212387 Accuracy 0.8917500376701355\n",
      "Iteration 17770 Training loss 0.005229185800999403 Validation loss 0.04365675151348114 Accuracy 0.8915000557899475\n",
      "Iteration 17780 Training loss 0.00393606536090374 Validation loss 0.043497905135154724 Accuracy 0.8916250467300415\n",
      "Iteration 17790 Training loss 0.0015433848602697253 Validation loss 0.04358680918812752 Accuracy 0.889875054359436\n",
      "Iteration 17800 Training loss 0.002613221062347293 Validation loss 0.043474048376083374 Accuracy 0.8912500143051147\n",
      "Iteration 17810 Training loss 0.0015619639307260513 Validation loss 0.0440916046500206 Accuracy 0.8890000581741333\n",
      "Iteration 17820 Training loss 0.00021569017553701997 Validation loss 0.04355378448963165 Accuracy 0.8907500505447388\n",
      "Iteration 17830 Training loss 0.0004798232694156468 Validation loss 0.04357492923736572 Accuracy 0.889875054359436\n",
      "Iteration 17840 Training loss 0.0002706182422116399 Validation loss 0.04363587498664856 Accuracy 0.890250027179718\n",
      "Iteration 17850 Training loss 0.00035662163281813264 Validation loss 0.04412522912025452 Accuracy 0.8873750567436218\n",
      "Iteration 17860 Training loss 0.00022112774604465812 Validation loss 0.04350927844643593 Accuracy 0.8916250467300415\n",
      "Iteration 17870 Training loss 0.0003472102398518473 Validation loss 0.04360227286815643 Accuracy 0.889875054359436\n",
      "Iteration 17880 Training loss 0.0027884591836482286 Validation loss 0.04381488263607025 Accuracy 0.8907500505447388\n",
      "Iteration 17890 Training loss 0.002063651103526354 Validation loss 0.04348413646221161 Accuracy 0.8915000557899475\n",
      "Iteration 17900 Training loss 0.00147028430365026 Validation loss 0.043552469462156296 Accuracy 0.890125036239624\n",
      "Iteration 17910 Training loss 0.0014190779766067863 Validation loss 0.04361267015337944 Accuracy 0.890375018119812\n",
      "Iteration 17920 Training loss 0.004063711501657963 Validation loss 0.04358764737844467 Accuracy 0.8911250233650208\n",
      "Iteration 17930 Training loss 0.0014603949384763837 Validation loss 0.043644167482852936 Accuracy 0.8912500143051147\n",
      "Iteration 17940 Training loss 0.0014055592473596334 Validation loss 0.04362683370709419 Accuracy 0.8905000686645508\n",
      "Iteration 17950 Training loss 0.00023575073282700032 Validation loss 0.04360552132129669 Accuracy 0.890250027179718\n",
      "Iteration 17960 Training loss 0.0015844973968341947 Validation loss 0.044057756662368774 Accuracy 0.8892500400543213\n",
      "Iteration 17970 Training loss 0.0029001706279814243 Validation loss 0.04356863722205162 Accuracy 0.890375018119812\n",
      "Iteration 17980 Training loss 0.0026999444235116243 Validation loss 0.04419328644871712 Accuracy 0.8875000476837158\n",
      "Iteration 17990 Training loss 0.004031050018966198 Validation loss 0.04412446171045303 Accuracy 0.8885000348091125\n",
      "Iteration 18000 Training loss 0.0003350435581523925 Validation loss 0.04359392449259758 Accuracy 0.8905000686645508\n",
      "Iteration 18010 Training loss 0.0027807122096419334 Validation loss 0.04351459816098213 Accuracy 0.8910000324249268\n",
      "Iteration 18020 Training loss 0.002676977775990963 Validation loss 0.0433603972196579 Accuracy 0.8927500247955322\n",
      "Iteration 18030 Training loss 0.0027410336770117283 Validation loss 0.043676480650901794 Accuracy 0.890125036239624\n",
      "Iteration 18040 Training loss 0.00020190728537272662 Validation loss 0.04342586174607277 Accuracy 0.8912500143051147\n",
      "Iteration 18050 Training loss 0.0015534339472651482 Validation loss 0.04351600259542465 Accuracy 0.8912500143051147\n",
      "Iteration 18060 Training loss 0.00022623683616984636 Validation loss 0.043449610471725464 Accuracy 0.8913750648498535\n",
      "Iteration 18070 Training loss 0.001428190153092146 Validation loss 0.04369323328137398 Accuracy 0.89000004529953\n",
      "Iteration 18080 Training loss 0.0028065273072570562 Validation loss 0.04430823028087616 Accuracy 0.8868750333786011\n",
      "Iteration 18090 Training loss 0.0015162365743890405 Validation loss 0.04357340931892395 Accuracy 0.8905000686645508\n",
      "Iteration 18100 Training loss 0.0014225855702534318 Validation loss 0.043663445860147476 Accuracy 0.8916250467300415\n",
      "Iteration 18110 Training loss 0.001645494718104601 Validation loss 0.04362769052386284 Accuracy 0.8905000686645508\n",
      "Iteration 18120 Training loss 0.0026899571530520916 Validation loss 0.04363657161593437 Accuracy 0.890125036239624\n",
      "Iteration 18130 Training loss 0.0014699575258418918 Validation loss 0.043371040374040604 Accuracy 0.8912500143051147\n",
      "Iteration 18140 Training loss 0.00020572944777086377 Validation loss 0.04342888295650482 Accuracy 0.8912500143051147\n",
      "Iteration 18150 Training loss 0.00147658446803689 Validation loss 0.04357708990573883 Accuracy 0.8908750414848328\n",
      "Iteration 18160 Training loss 0.001720605418086052 Validation loss 0.04355070739984512 Accuracy 0.8917500376701355\n",
      "Iteration 18170 Training loss 0.00015887092740740627 Validation loss 0.043464403599500656 Accuracy 0.8912500143051147\n",
      "Iteration 18180 Training loss 0.00024765447597019374 Validation loss 0.0434938482940197 Accuracy 0.8927500247955322\n",
      "Iteration 18190 Training loss 0.0027187971863895655 Validation loss 0.04358752816915512 Accuracy 0.889750063419342\n",
      "Iteration 18200 Training loss 0.003966315183788538 Validation loss 0.04338798671960831 Accuracy 0.8912500143051147\n",
      "Iteration 18210 Training loss 0.0015263521345332265 Validation loss 0.0433068610727787 Accuracy 0.8912500143051147\n",
      "Iteration 18220 Training loss 0.0003240179503336549 Validation loss 0.04348871856927872 Accuracy 0.8910000324249268\n",
      "Iteration 18230 Training loss 0.00142419443000108 Validation loss 0.04337499290704727 Accuracy 0.8913750648498535\n",
      "Iteration 18240 Training loss 0.003945755772292614 Validation loss 0.04373641684651375 Accuracy 0.890250027179718\n",
      "Iteration 18250 Training loss 0.0015380055410787463 Validation loss 0.04361622780561447 Accuracy 0.8912500143051147\n",
      "Iteration 18260 Training loss 0.0015913820825517178 Validation loss 0.04381173104047775 Accuracy 0.890250027179718\n",
      "Iteration 18270 Training loss 0.003273960202932358 Validation loss 0.04345831647515297 Accuracy 0.8922500610351562\n",
      "Iteration 18280 Training loss 0.0002757789916358888 Validation loss 0.04343816637992859 Accuracy 0.8907500505447388\n",
      "Iteration 18290 Training loss 0.00025374608230777085 Validation loss 0.043506402522325516 Accuracy 0.889750063419342\n",
      "Iteration 18300 Training loss 0.000167000966030173 Validation loss 0.043497662991285324 Accuracy 0.8913750648498535\n",
      "Iteration 18310 Training loss 0.0027063172310590744 Validation loss 0.043337833136320114 Accuracy 0.8926250338554382\n",
      "Iteration 18320 Training loss 0.003960492089390755 Validation loss 0.04336829483509064 Accuracy 0.8918750286102295\n",
      "Iteration 18330 Training loss 0.0014937331434339285 Validation loss 0.043540239334106445 Accuracy 0.8908750414848328\n",
      "Iteration 18340 Training loss 0.0017328221583738923 Validation loss 0.04367125779390335 Accuracy 0.8915000557899475\n",
      "Iteration 18350 Training loss 0.002745643723756075 Validation loss 0.0434553436934948 Accuracy 0.8918750286102295\n",
      "Iteration 18360 Training loss 0.0029568581376224756 Validation loss 0.04473927617073059 Accuracy 0.8853750228881836\n",
      "Iteration 18370 Training loss 0.0028354839887470007 Validation loss 0.0436808243393898 Accuracy 0.8906250596046448\n",
      "Iteration 18380 Training loss 0.0017715363064780831 Validation loss 0.043728020042181015 Accuracy 0.8907500505447388\n",
      "Iteration 18390 Training loss 0.0036044882144778967 Validation loss 0.045813847333192825 Accuracy 0.8838750123977661\n",
      "Iteration 18400 Training loss 0.0014934635255485773 Validation loss 0.043630968779325485 Accuracy 0.8910000324249268\n",
      "Iteration 18410 Training loss 0.0014931486221030354 Validation loss 0.043519869446754456 Accuracy 0.8908750414848328\n",
      "Iteration 18420 Training loss 0.0027362932451069355 Validation loss 0.04365282878279686 Accuracy 0.8908750414848328\n",
      "Iteration 18430 Training loss 0.0027609076350927353 Validation loss 0.04369674250483513 Accuracy 0.890250027179718\n",
      "Iteration 18440 Training loss 0.0028204049449414015 Validation loss 0.043739404529333115 Accuracy 0.890250027179718\n",
      "Iteration 18450 Training loss 0.0027165859937667847 Validation loss 0.04348120465874672 Accuracy 0.890375018119812\n",
      "Iteration 18460 Training loss 0.00021989837114233524 Validation loss 0.04369622841477394 Accuracy 0.8911250233650208\n",
      "Iteration 18470 Training loss 0.0014339805347844958 Validation loss 0.04342102259397507 Accuracy 0.8915000557899475\n",
      "Iteration 18480 Training loss 0.005201088264584541 Validation loss 0.04338785633444786 Accuracy 0.8915000557899475\n",
      "Iteration 18490 Training loss 0.0027751619927585125 Validation loss 0.04350923001766205 Accuracy 0.8915000557899475\n",
      "Iteration 18500 Training loss 0.0027360294479876757 Validation loss 0.043534792959690094 Accuracy 0.8907500505447388\n",
      "Iteration 18510 Training loss 0.00010730463691288605 Validation loss 0.04356791824102402 Accuracy 0.8905000686645508\n",
      "Iteration 18520 Training loss 0.0014342789072543383 Validation loss 0.04352983832359314 Accuracy 0.8918750286102295\n",
      "Iteration 18530 Training loss 0.003967976663261652 Validation loss 0.04358530417084694 Accuracy 0.8917500376701355\n",
      "Iteration 18540 Training loss 0.004114038776606321 Validation loss 0.043482523411512375 Accuracy 0.8913750648498535\n",
      "Iteration 18550 Training loss 0.003969041630625725 Validation loss 0.043471559882164 Accuracy 0.8915000557899475\n",
      "Iteration 18560 Training loss 0.00019694975344464183 Validation loss 0.043594103306531906 Accuracy 0.8908750414848328\n",
      "Iteration 18570 Training loss 0.0015922589227557182 Validation loss 0.043570198118686676 Accuracy 0.8917500376701355\n",
      "Iteration 18580 Training loss 0.0001627813617233187 Validation loss 0.04351230710744858 Accuracy 0.890375018119812\n",
      "Iteration 18590 Training loss 0.003910774365067482 Validation loss 0.043463945388793945 Accuracy 0.8911250233650208\n",
      "Iteration 18600 Training loss 0.0039154659025371075 Validation loss 0.043437570333480835 Accuracy 0.8927500247955322\n",
      "Iteration 18610 Training loss 0.0014767999527975917 Validation loss 0.043687425553798676 Accuracy 0.890375018119812\n",
      "Iteration 18620 Training loss 0.0028547816909849644 Validation loss 0.04371332749724388 Accuracy 0.8910000324249268\n",
      "Iteration 18630 Training loss 0.0026988633908331394 Validation loss 0.04352943226695061 Accuracy 0.8893750309944153\n",
      "Iteration 18640 Training loss 0.002628545742481947 Validation loss 0.043493401259183884 Accuracy 0.8915000557899475\n",
      "Iteration 18650 Training loss 0.0004416067386046052 Validation loss 0.043794114142656326 Accuracy 0.8908750414848328\n",
      "Iteration 18660 Training loss 0.003921558149158955 Validation loss 0.04352002218365669 Accuracy 0.8912500143051147\n",
      "Iteration 18670 Training loss 0.0014694785932078958 Validation loss 0.043736811727285385 Accuracy 0.8905000686645508\n",
      "Iteration 18680 Training loss 0.0001775710261426866 Validation loss 0.04355284944176674 Accuracy 0.890375018119812\n",
      "Iteration 18690 Training loss 0.001411378849297762 Validation loss 0.04350926727056503 Accuracy 0.8915000557899475\n",
      "Iteration 18700 Training loss 0.0017410899745300412 Validation loss 0.043721042573451996 Accuracy 0.8908750414848328\n",
      "Iteration 18710 Training loss 0.0038977854419499636 Validation loss 0.043633103370666504 Accuracy 0.8911250233650208\n",
      "Iteration 18720 Training loss 0.003893636167049408 Validation loss 0.04359382763504982 Accuracy 0.8911250233650208\n",
      "Iteration 18730 Training loss 0.0026644598692655563 Validation loss 0.04347818344831467 Accuracy 0.890375018119812\n",
      "Iteration 18740 Training loss 0.00017596308316569775 Validation loss 0.0437014065682888 Accuracy 0.889875054359436\n",
      "Iteration 18750 Training loss 0.0002872916229534894 Validation loss 0.043630048632621765 Accuracy 0.890250027179718\n",
      "Iteration 18760 Training loss 0.00012554596469271928 Validation loss 0.04363588988780975 Accuracy 0.8905000686645508\n",
      "Iteration 18770 Training loss 0.0027180633042007685 Validation loss 0.04343269392848015 Accuracy 0.8906250596046448\n",
      "Iteration 18780 Training loss 0.004620822612196207 Validation loss 0.04483954980969429 Accuracy 0.8860000371932983\n",
      "Iteration 18790 Training loss 0.0016666838200762868 Validation loss 0.04501917213201523 Accuracy 0.8857500553131104\n",
      "Iteration 18800 Training loss 0.002648210385814309 Validation loss 0.04335926100611687 Accuracy 0.8918750286102295\n",
      "Iteration 18810 Training loss 0.003870720276609063 Validation loss 0.04348215088248253 Accuracy 0.8913750648498535\n",
      "Iteration 18820 Training loss 0.0014663884649053216 Validation loss 0.04346843808889389 Accuracy 0.8915000557899475\n",
      "Iteration 18830 Training loss 0.0042242854833602905 Validation loss 0.04343506693840027 Accuracy 0.8911250233650208\n",
      "Iteration 18840 Training loss 0.005200546700507402 Validation loss 0.043513644486665726 Accuracy 0.8907500505447388\n",
      "Iteration 18850 Training loss 0.0026985995937138796 Validation loss 0.04354628175497055 Accuracy 0.8913750648498535\n",
      "Iteration 18860 Training loss 0.002688874490559101 Validation loss 0.04349676892161369 Accuracy 0.8908750414848328\n",
      "Iteration 18870 Training loss 0.002612248295918107 Validation loss 0.04348801448941231 Accuracy 0.8908750414848328\n",
      "Iteration 18880 Training loss 0.0001104629845940508 Validation loss 0.04351099580526352 Accuracy 0.8913750648498535\n",
      "Iteration 18890 Training loss 0.002873076358810067 Validation loss 0.04450790956616402 Accuracy 0.8860000371932983\n",
      "Iteration 18900 Training loss 0.0015155147993937135 Validation loss 0.04352022334933281 Accuracy 0.8915000557899475\n",
      "Iteration 18910 Training loss 0.0026915755588561296 Validation loss 0.04364153742790222 Accuracy 0.8911250233650208\n",
      "Iteration 18920 Training loss 0.0015132945263758302 Validation loss 0.043596748262643814 Accuracy 0.890125036239624\n",
      "Iteration 18930 Training loss 0.00023968603636603802 Validation loss 0.04399576783180237 Accuracy 0.8906250596046448\n",
      "Iteration 18940 Training loss 0.0015829602489247918 Validation loss 0.04375558719038963 Accuracy 0.8908750414848328\n",
      "Iteration 18950 Training loss 0.0026945658028125763 Validation loss 0.04362103343009949 Accuracy 0.8915000557899475\n",
      "Iteration 18960 Training loss 0.00015953714319039136 Validation loss 0.04361201077699661 Accuracy 0.8908750414848328\n",
      "Iteration 18970 Training loss 0.002666546730324626 Validation loss 0.043527860194444656 Accuracy 0.8922500610351562\n",
      "Iteration 18980 Training loss 0.002645273460075259 Validation loss 0.0435783788561821 Accuracy 0.8913750648498535\n",
      "Iteration 18990 Training loss 0.00015868023911025375 Validation loss 0.04352264478802681 Accuracy 0.8916250467300415\n",
      "Iteration 19000 Training loss 0.0014547972241416574 Validation loss 0.04359443858265877 Accuracy 0.89000004529953\n",
      "Iteration 19010 Training loss 0.002711107023060322 Validation loss 0.0437559075653553 Accuracy 0.890125036239624\n",
      "Iteration 19020 Training loss 0.00028733396902680397 Validation loss 0.04364481940865517 Accuracy 0.8905000686645508\n",
      "Iteration 19030 Training loss 0.003877233248203993 Validation loss 0.043576959520578384 Accuracy 0.8911250233650208\n",
      "Iteration 19040 Training loss 0.00012654767488129437 Validation loss 0.04358821362257004 Accuracy 0.8908750414848328\n",
      "Iteration 19050 Training loss 0.001567241852171719 Validation loss 0.04354557394981384 Accuracy 0.8921250700950623\n",
      "Iteration 19060 Training loss 0.0027364727575331926 Validation loss 0.04361867532134056 Accuracy 0.8916250467300415\n",
      "Iteration 19070 Training loss 0.0014968320028856397 Validation loss 0.043619535863399506 Accuracy 0.8921250700950623\n",
      "Iteration 19080 Training loss 0.0014811259461566806 Validation loss 0.043889712542295456 Accuracy 0.8906250596046448\n",
      "Iteration 19090 Training loss 0.0038969607558101416 Validation loss 0.04371053725481033 Accuracy 0.8907500505447388\n",
      "Iteration 19100 Training loss 0.00016866586520336568 Validation loss 0.0435822531580925 Accuracy 0.8913750648498535\n",
      "Iteration 19110 Training loss 0.002729809610173106 Validation loss 0.04361594095826149 Accuracy 0.8916250467300415\n",
      "Iteration 19120 Training loss 0.003951074555516243 Validation loss 0.04362715408205986 Accuracy 0.8911250233650208\n",
      "Iteration 19130 Training loss 0.0029388153925538063 Validation loss 0.04378407821059227 Accuracy 0.890375018119812\n",
      "Iteration 19140 Training loss 0.00016319233691319823 Validation loss 0.04378544166684151 Accuracy 0.8921250700950623\n",
      "Iteration 19150 Training loss 0.0028747771866619587 Validation loss 0.04369637742638588 Accuracy 0.8915000557899475\n",
      "Iteration 19160 Training loss 0.0014528919709846377 Validation loss 0.04372440651059151 Accuracy 0.8912500143051147\n",
      "Iteration 19170 Training loss 0.0015127243241295218 Validation loss 0.04390999302268028 Accuracy 0.889750063419342\n",
      "Iteration 19180 Training loss 0.0038962119724601507 Validation loss 0.043800000101327896 Accuracy 0.8911250233650208\n",
      "Iteration 19190 Training loss 0.002649704460054636 Validation loss 0.04365617781877518 Accuracy 0.8927500247955322\n",
      "Iteration 19200 Training loss 0.0013918844051659107 Validation loss 0.04378485307097435 Accuracy 0.8910000324249268\n",
      "Iteration 19210 Training loss 0.006571358069777489 Validation loss 0.044043175876140594 Accuracy 0.8888750672340393\n",
      "Iteration 19220 Training loss 0.0007839605095796287 Validation loss 0.04547200724482536 Accuracy 0.8847500681877136\n",
      "Iteration 19230 Training loss 0.0014686365611851215 Validation loss 0.04407929256558418 Accuracy 0.889750063419342\n",
      "Iteration 19240 Training loss 0.001439030165784061 Validation loss 0.04434525594115257 Accuracy 0.8887500166893005\n",
      "Iteration 19250 Training loss 0.0027466588653624058 Validation loss 0.04402751848101616 Accuracy 0.8892500400543213\n",
      "Iteration 19260 Training loss 0.005111841484904289 Validation loss 0.043737344443798065 Accuracy 0.8908750414848328\n",
      "Iteration 19270 Training loss 0.001571413828060031 Validation loss 0.04402858391404152 Accuracy 0.8907500505447388\n",
      "Iteration 19280 Training loss 0.0026166928000748158 Validation loss 0.04378488287329674 Accuracy 0.8911250233650208\n",
      "Iteration 19290 Training loss 0.0013768640346825123 Validation loss 0.043737344443798065 Accuracy 0.8918750286102295\n",
      "Iteration 19300 Training loss 0.00017761833441909403 Validation loss 0.04389645531773567 Accuracy 0.890375018119812\n",
      "Iteration 19310 Training loss 0.00019807930220849812 Validation loss 0.043653033673763275 Accuracy 0.8923750519752502\n",
      "Iteration 19320 Training loss 0.002701297402381897 Validation loss 0.04389863833785057 Accuracy 0.890375018119812\n",
      "Iteration 19330 Training loss 0.002665232867002487 Validation loss 0.043653421103954315 Accuracy 0.8912500143051147\n",
      "Iteration 19340 Training loss 0.001471999567002058 Validation loss 0.04374152794480324 Accuracy 0.8920000195503235\n",
      "Iteration 19350 Training loss 0.0014284562785178423 Validation loss 0.04356100410223007 Accuracy 0.8920000195503235\n",
      "Iteration 19360 Training loss 0.0026385991368442774 Validation loss 0.043587300926446915 Accuracy 0.8916250467300415\n",
      "Iteration 19370 Training loss 0.00016514874005224556 Validation loss 0.043706826865673065 Accuracy 0.8906250596046448\n",
      "Iteration 19380 Training loss 0.001509690540842712 Validation loss 0.04374012351036072 Accuracy 0.8917500376701355\n",
      "Iteration 19390 Training loss 0.002617419697344303 Validation loss 0.043739739805459976 Accuracy 0.8915000557899475\n",
      "Iteration 19400 Training loss 0.001487083500251174 Validation loss 0.045434415340423584 Accuracy 0.8837500214576721\n",
      "Iteration 19410 Training loss 0.00013078235497232527 Validation loss 0.04374926537275314 Accuracy 0.8918750286102295\n",
      "Iteration 19420 Training loss 0.0014868518337607384 Validation loss 0.04372020065784454 Accuracy 0.8918750286102295\n",
      "Iteration 19430 Training loss 0.00013460162153933197 Validation loss 0.04359791800379753 Accuracy 0.8916250467300415\n",
      "Iteration 19440 Training loss 0.00404208293184638 Validation loss 0.04360640048980713 Accuracy 0.8916250467300415\n",
      "Iteration 19450 Training loss 0.0014642974128946662 Validation loss 0.04365029186010361 Accuracy 0.8913750648498535\n",
      "Iteration 19460 Training loss 0.00514875166118145 Validation loss 0.043760061264038086 Accuracy 0.8907500505447388\n",
      "Iteration 19470 Training loss 0.0013641045661643147 Validation loss 0.04372548684477806 Accuracy 0.8911250233650208\n",
      "Iteration 19480 Training loss 0.0013953704619780183 Validation loss 0.04373093694448471 Accuracy 0.8908750414848328\n",
      "Iteration 19490 Training loss 0.002646505832672119 Validation loss 0.04393087327480316 Accuracy 0.889875054359436\n",
      "Iteration 19500 Training loss 0.0007587950676679611 Validation loss 0.043686121702194214 Accuracy 0.8912500143051147\n",
      "Iteration 19510 Training loss 0.0014828973216935992 Validation loss 0.043665528297424316 Accuracy 0.8920000195503235\n",
      "Iteration 19520 Training loss 0.0014208328211680055 Validation loss 0.04365807771682739 Accuracy 0.8920000195503235\n",
      "Iteration 19530 Training loss 0.005562882870435715 Validation loss 0.04596812278032303 Accuracy 0.8836250305175781\n",
      "Iteration 19540 Training loss 0.0015215280000120401 Validation loss 0.04384661465883255 Accuracy 0.8908750414848328\n",
      "Iteration 19550 Training loss 0.0013844674685969949 Validation loss 0.043650805950164795 Accuracy 0.8912500143051147\n",
      "Iteration 19560 Training loss 0.0027180123142898083 Validation loss 0.04356220364570618 Accuracy 0.8926250338554382\n",
      "Iteration 19570 Training loss 0.001540020341053605 Validation loss 0.04421745985746384 Accuracy 0.8907500505447388\n",
      "Iteration 19580 Training loss 0.0027300810907036066 Validation loss 0.043699946254491806 Accuracy 0.8918750286102295\n",
      "Iteration 19590 Training loss 0.0026600609999150038 Validation loss 0.04365513101220131 Accuracy 0.8927500247955322\n",
      "Iteration 19600 Training loss 0.003924638964235783 Validation loss 0.043654825538396835 Accuracy 0.8917500376701355\n",
      "Iteration 19610 Training loss 0.00021555628336500376 Validation loss 0.0438089556992054 Accuracy 0.890250027179718\n",
      "Iteration 19620 Training loss 0.003048226237297058 Validation loss 0.0436517708003521 Accuracy 0.8918750286102295\n",
      "Iteration 19630 Training loss 0.0027186935767531395 Validation loss 0.043790001422166824 Accuracy 0.8905000686645508\n",
      "Iteration 19640 Training loss 0.0026637690607458353 Validation loss 0.0436660498380661 Accuracy 0.8921250700950623\n",
      "Iteration 19650 Training loss 0.005042383447289467 Validation loss 0.04362427070736885 Accuracy 0.8918750286102295\n",
      "Iteration 19660 Training loss 0.002831952180713415 Validation loss 0.04383271932601929 Accuracy 0.8915000557899475\n",
      "Iteration 19670 Training loss 0.0026255615521222353 Validation loss 0.04374103248119354 Accuracy 0.8916250467300415\n",
      "Iteration 19680 Training loss 0.0027450781781226397 Validation loss 0.043587639927864075 Accuracy 0.893125057220459\n",
      "Iteration 19690 Training loss 0.0026469274889677763 Validation loss 0.04370494186878204 Accuracy 0.8916250467300415\n",
      "Iteration 19700 Training loss 0.00021423492580652237 Validation loss 0.043585579842329025 Accuracy 0.8920000195503235\n",
      "Iteration 19710 Training loss 0.00012666379916481674 Validation loss 0.04366917535662651 Accuracy 0.8911250233650208\n",
      "Iteration 19720 Training loss 0.004015331622213125 Validation loss 0.04422364383935928 Accuracy 0.8918750286102295\n",
      "Iteration 19730 Training loss 0.0025315373204648495 Validation loss 0.043677978217601776 Accuracy 0.8915000557899475\n",
      "Iteration 19740 Training loss 0.0015387454768642783 Validation loss 0.04363008961081505 Accuracy 0.8927500247955322\n",
      "Iteration 19750 Training loss 0.003871412482112646 Validation loss 0.04370512440800667 Accuracy 0.8917500376701355\n",
      "Iteration 19760 Training loss 0.0013997916830703616 Validation loss 0.04396265000104904 Accuracy 0.890250027179718\n",
      "Iteration 19770 Training loss 0.002634235192090273 Validation loss 0.04404525086283684 Accuracy 0.8911250233650208\n",
      "Iteration 19780 Training loss 0.0024179709143936634 Validation loss 0.043777644634246826 Accuracy 0.8906250596046448\n",
      "Iteration 19790 Training loss 0.0013420945033431053 Validation loss 0.04380233213305473 Accuracy 0.8912500143051147\n",
      "Iteration 19800 Training loss 0.002625858411192894 Validation loss 0.04375806078314781 Accuracy 0.8913750648498535\n",
      "Iteration 19810 Training loss 0.0027362853288650513 Validation loss 0.04368918761610985 Accuracy 0.8913750648498535\n",
      "Iteration 19820 Training loss 0.0014315411681309342 Validation loss 0.043615005910396576 Accuracy 0.8922500610351562\n",
      "Iteration 19830 Training loss 0.0026851780712604523 Validation loss 0.043658722192049026 Accuracy 0.8920000195503235\n",
      "Iteration 19840 Training loss 0.0014811127912253141 Validation loss 0.043668556958436966 Accuracy 0.8927500247955322\n",
      "Iteration 19850 Training loss 0.0003188462578691542 Validation loss 0.043880969285964966 Accuracy 0.8917500376701355\n",
      "Iteration 19860 Training loss 0.0039632306434214115 Validation loss 0.04382454976439476 Accuracy 0.8913750648498535\n",
      "Iteration 19870 Training loss 0.0039028499741107225 Validation loss 0.043842580169439316 Accuracy 0.8910000324249268\n",
      "Iteration 19880 Training loss 0.005138608627021313 Validation loss 0.04378161206841469 Accuracy 0.8907500505447388\n",
      "Iteration 19890 Training loss 0.00511220283806324 Validation loss 0.04388231411576271 Accuracy 0.890125036239624\n",
      "Iteration 19900 Training loss 0.0014495947398245335 Validation loss 0.04454440250992775 Accuracy 0.8871250152587891\n",
      "Iteration 19910 Training loss 0.004059872590005398 Validation loss 0.04384372755885124 Accuracy 0.890125036239624\n",
      "Iteration 19920 Training loss 0.00017297922750003636 Validation loss 0.04367160424590111 Accuracy 0.890250027179718\n",
      "Iteration 19930 Training loss 0.00012591626727953553 Validation loss 0.0437970831990242 Accuracy 0.8913750648498535\n",
      "Iteration 19940 Training loss 0.0001219536061398685 Validation loss 0.04385299235582352 Accuracy 0.890375018119812\n",
      "Iteration 19950 Training loss 0.0013958306517452002 Validation loss 0.043773602694272995 Accuracy 0.8907500505447388\n",
      "Iteration 19960 Training loss 0.0014459448866546154 Validation loss 0.04392995685338974 Accuracy 0.890250027179718\n",
      "Iteration 19970 Training loss 0.006418074015527964 Validation loss 0.04390818998217583 Accuracy 0.889750063419342\n",
      "Iteration 19980 Training loss 0.0001271543587790802 Validation loss 0.04373524710536003 Accuracy 0.8913750648498535\n",
      "Iteration 19990 Training loss 0.002629695227369666 Validation loss 0.04372623562812805 Accuracy 0.8908750414848328\n",
      "Iteration 20000 Training loss 0.0038924275431782007 Validation loss 0.04367491230368614 Accuracy 0.8911250233650208\n",
      "Iteration 20010 Training loss 0.0026159908156841993 Validation loss 0.04397355765104294 Accuracy 0.890250027179718\n",
      "Iteration 20020 Training loss 0.005126778967678547 Validation loss 0.04383016377687454 Accuracy 0.8910000324249268\n",
      "Iteration 20030 Training loss 0.005059855990111828 Validation loss 0.0438603013753891 Accuracy 0.8908750414848328\n",
      "Iteration 20040 Training loss 0.0013676767703145742 Validation loss 0.043823227286338806 Accuracy 0.8910000324249268\n",
      "Iteration 20050 Training loss 0.005128609947860241 Validation loss 0.04408567398786545 Accuracy 0.889750063419342\n",
      "Iteration 20060 Training loss 0.00012660420907195657 Validation loss 0.04386163502931595 Accuracy 0.8906250596046448\n",
      "Iteration 20070 Training loss 0.0013598963851109147 Validation loss 0.043859608471393585 Accuracy 0.890375018119812\n",
      "Iteration 20080 Training loss 0.00015787935990374535 Validation loss 0.043785691261291504 Accuracy 0.8910000324249268\n",
      "Iteration 20090 Training loss 0.00296272081322968 Validation loss 0.04377606138586998 Accuracy 0.8910000324249268\n",
      "Iteration 20100 Training loss 0.0014230029191821814 Validation loss 0.043489594012498856 Accuracy 0.8923750519752502\n",
      "Iteration 20110 Training loss 0.00015091629757080227 Validation loss 0.04357064142823219 Accuracy 0.8922500610351562\n",
      "Iteration 20120 Training loss 0.0026718240696936846 Validation loss 0.04366679862141609 Accuracy 0.8917500376701355\n",
      "Iteration 20130 Training loss 0.00015353044727817178 Validation loss 0.04364881291985512 Accuracy 0.8928750157356262\n",
      "Iteration 20140 Training loss 0.0011713759740814567 Validation loss 0.04373866692185402 Accuracy 0.8926250338554382\n",
      "Iteration 20150 Training loss 0.001418239320628345 Validation loss 0.04378736764192581 Accuracy 0.8913750648498535\n",
      "Iteration 20160 Training loss 0.0026155540253967047 Validation loss 0.043801844120025635 Accuracy 0.8921250700950623\n",
      "Iteration 20170 Training loss 0.0013624613638967276 Validation loss 0.04371955618262291 Accuracy 0.8917500376701355\n",
      "Iteration 20180 Training loss 0.0013938263291493058 Validation loss 0.043831437826156616 Accuracy 0.8912500143051147\n",
      "Iteration 20190 Training loss 0.003975981846451759 Validation loss 0.04397551342844963 Accuracy 0.8905000686645508\n",
      "Iteration 20200 Training loss 0.0063861976377666 Validation loss 0.043802157044410706 Accuracy 0.8916250467300415\n",
      "Iteration 20210 Training loss 0.00010222473792964593 Validation loss 0.04397488385438919 Accuracy 0.8912500143051147\n",
      "Iteration 20220 Training loss 0.002691674279049039 Validation loss 0.04390547424554825 Accuracy 0.8907500505447388\n",
      "Iteration 20230 Training loss 0.0014248219085857272 Validation loss 0.04382658004760742 Accuracy 0.8921250700950623\n",
      "Iteration 20240 Training loss 9.441316069569439e-05 Validation loss 0.043870553374290466 Accuracy 0.8921250700950623\n",
      "Iteration 20250 Training loss 0.00016401629545725882 Validation loss 0.04385177791118622 Accuracy 0.8906250596046448\n",
      "Iteration 20260 Training loss 0.0026458578649908304 Validation loss 0.044136080890893936 Accuracy 0.890125036239624\n",
      "Iteration 20270 Training loss 0.0013759286375716329 Validation loss 0.043896690011024475 Accuracy 0.8920000195503235\n",
      "Iteration 20280 Training loss 0.00015364240971393883 Validation loss 0.04381930083036423 Accuracy 0.8910000324249268\n",
      "Iteration 20290 Training loss 0.001390970777720213 Validation loss 0.04392145201563835 Accuracy 0.8911250233650208\n",
      "Iteration 20300 Training loss 0.001404479844495654 Validation loss 0.04380013793706894 Accuracy 0.8925000429153442\n",
      "Iteration 20310 Training loss 0.002619657199829817 Validation loss 0.04402346909046173 Accuracy 0.890250027179718\n",
      "Iteration 20320 Training loss 0.0014645596966147423 Validation loss 0.043819934129714966 Accuracy 0.8915000557899475\n",
      "Iteration 20330 Training loss 0.00012024184252368286 Validation loss 0.04384111240506172 Accuracy 0.8920000195503235\n",
      "Iteration 20340 Training loss 0.0015209398698061705 Validation loss 0.04384434595704079 Accuracy 0.8920000195503235\n",
      "Iteration 20350 Training loss 0.002595985773950815 Validation loss 0.04381190240383148 Accuracy 0.8922500610351562\n",
      "Iteration 20360 Training loss 0.0013502816436812282 Validation loss 0.043782610446214676 Accuracy 0.8911250233650208\n",
      "Iteration 20370 Training loss 0.0026546078734099865 Validation loss 0.04389391094446182 Accuracy 0.8921250700950623\n",
      "Iteration 20380 Training loss 0.0038828684482723475 Validation loss 0.04384373500943184 Accuracy 0.8908750414848328\n",
      "Iteration 20390 Training loss 0.00012900462024845183 Validation loss 0.04376748576760292 Accuracy 0.8915000557899475\n",
      "Iteration 20400 Training loss 0.0013561579398810863 Validation loss 0.04374124854803085 Accuracy 0.8917500376701355\n",
      "Iteration 20410 Training loss 0.001368873636238277 Validation loss 0.043720994144678116 Accuracy 0.8915000557899475\n",
      "Iteration 20420 Training loss 0.00510965334251523 Validation loss 0.043698135763406754 Accuracy 0.8921250700950623\n",
      "Iteration 20430 Training loss 0.0014051222242414951 Validation loss 0.04368032142519951 Accuracy 0.8916250467300415\n",
      "Iteration 20440 Training loss 0.0038139079697430134 Validation loss 0.043845389038324356 Accuracy 0.8906250596046448\n",
      "Iteration 20450 Training loss 0.0026234351098537445 Validation loss 0.04366051033139229 Accuracy 0.8926250338554382\n",
      "Iteration 20460 Training loss 0.0027533364482223988 Validation loss 0.043843965977430344 Accuracy 0.8907500505447388\n",
      "Iteration 20470 Training loss 0.0034329837653785944 Validation loss 0.0436825305223465 Accuracy 0.8918750286102295\n",
      "Iteration 20480 Training loss 0.0001389927347190678 Validation loss 0.0438816174864769 Accuracy 0.8910000324249268\n",
      "Iteration 20490 Training loss 0.00011521360283950344 Validation loss 0.044020503759384155 Accuracy 0.8911250233650208\n",
      "Iteration 20500 Training loss 0.005224328022450209 Validation loss 0.04386277124285698 Accuracy 0.8917500376701355\n",
      "Iteration 20510 Training loss 0.0013858836609870195 Validation loss 0.04376900568604469 Accuracy 0.8916250467300415\n",
      "Iteration 20520 Training loss 0.003897053422406316 Validation loss 0.04389418289065361 Accuracy 0.8905000686645508\n",
      "Iteration 20530 Training loss 0.00139827118255198 Validation loss 0.04390856996178627 Accuracy 0.8905000686645508\n",
      "Iteration 20540 Training loss 0.0014180996222421527 Validation loss 0.04378421977162361 Accuracy 0.8907500505447388\n",
      "Iteration 20550 Training loss 0.0025815709959715605 Validation loss 0.04379004240036011 Accuracy 0.8910000324249268\n",
      "Iteration 20560 Training loss 0.0001653973595239222 Validation loss 0.044052060693502426 Accuracy 0.889750063419342\n",
      "Iteration 20570 Training loss 0.0013655659276992083 Validation loss 0.04402213171124458 Accuracy 0.8905000686645508\n",
      "Iteration 20580 Training loss 0.0013912580907344818 Validation loss 0.043964967131614685 Accuracy 0.8906250596046448\n",
      "Iteration 20590 Training loss 0.0013513017911463976 Validation loss 0.04385912045836449 Accuracy 0.8912500143051147\n",
      "Iteration 20600 Training loss 0.0001852991699706763 Validation loss 0.04391653835773468 Accuracy 0.8905000686645508\n",
      "Iteration 20610 Training loss 0.0026513116899877787 Validation loss 0.043909963220357895 Accuracy 0.8912500143051147\n",
      "Iteration 20620 Training loss 0.003934063017368317 Validation loss 0.044814858585596085 Accuracy 0.890250027179718\n",
      "Iteration 20630 Training loss 0.0014399680076166987 Validation loss 0.04444340616464615 Accuracy 0.8907500505447388\n",
      "Iteration 20640 Training loss 0.0013732267543673515 Validation loss 0.043837886303663254 Accuracy 0.8907500505447388\n",
      "Iteration 20650 Training loss 0.00015352011541835964 Validation loss 0.04399348050355911 Accuracy 0.89000004529953\n",
      "Iteration 20660 Training loss 0.005144263617694378 Validation loss 0.04385250434279442 Accuracy 0.8907500505447388\n",
      "Iteration 20670 Training loss 0.0014138860860839486 Validation loss 0.043859660625457764 Accuracy 0.8915000557899475\n",
      "Iteration 20680 Training loss 0.00030537130078300834 Validation loss 0.04402340203523636 Accuracy 0.8911250233650208\n",
      "Iteration 20690 Training loss 0.0014644318725913763 Validation loss 0.043732792139053345 Accuracy 0.8920000195503235\n",
      "Iteration 20700 Training loss 0.0026595627423375845 Validation loss 0.043781571090221405 Accuracy 0.8912500143051147\n",
      "Iteration 20710 Training loss 0.00025560680660419166 Validation loss 0.0443655401468277 Accuracy 0.8891250491142273\n",
      "Iteration 20720 Training loss 0.00025040435139089823 Validation loss 0.04381709545850754 Accuracy 0.8926250338554382\n",
      "Iteration 20730 Training loss 0.0038329290691763163 Validation loss 0.043895769864320755 Accuracy 0.8906250596046448\n",
      "Iteration 20740 Training loss 0.0026427768170833588 Validation loss 0.04396753013134003 Accuracy 0.8908750414848328\n",
      "Iteration 20750 Training loss 0.0014064800925552845 Validation loss 0.0439019575715065 Accuracy 0.8908750414848328\n",
      "Iteration 20760 Training loss 0.0014703540364280343 Validation loss 0.0437781922519207 Accuracy 0.8915000557899475\n",
      "Iteration 20770 Training loss 9.837515244726092e-05 Validation loss 0.044032298028469086 Accuracy 0.8910000324249268\n",
      "Iteration 20780 Training loss 0.0013828836381435394 Validation loss 0.04391549900174141 Accuracy 0.8913750648498535\n",
      "Iteration 20790 Training loss 0.0025889186654239893 Validation loss 0.04394520819187164 Accuracy 0.890125036239624\n",
      "Iteration 20800 Training loss 0.0014038586523383856 Validation loss 0.04394296184182167 Accuracy 0.8911250233650208\n",
      "Iteration 20810 Training loss 0.0026520276442170143 Validation loss 0.04395820572972298 Accuracy 0.8907500505447388\n",
      "Iteration 20820 Training loss 9.899683209368959e-05 Validation loss 0.044030070304870605 Accuracy 0.8912500143051147\n",
      "Iteration 20830 Training loss 0.0015570814721286297 Validation loss 0.044811997562646866 Accuracy 0.8885000348091125\n",
      "Iteration 20840 Training loss 0.001440171618014574 Validation loss 0.04395904019474983 Accuracy 0.889875054359436\n",
      "Iteration 20850 Training loss 0.0013891019625589252 Validation loss 0.04387488216161728 Accuracy 0.8910000324249268\n",
      "Iteration 20860 Training loss 0.0014475794741883874 Validation loss 0.04385159909725189 Accuracy 0.8911250233650208\n",
      "Iteration 20870 Training loss 0.0005471885087899864 Validation loss 0.04448637366294861 Accuracy 0.8882500529289246\n",
      "Iteration 20880 Training loss 0.0026573161594569683 Validation loss 0.04375692084431648 Accuracy 0.8911250233650208\n",
      "Iteration 20890 Training loss 0.0026511012110859156 Validation loss 0.04368222877383232 Accuracy 0.8920000195503235\n",
      "Iteration 20900 Training loss 0.0013754375977441669 Validation loss 0.043699540197849274 Accuracy 0.8920000195503235\n",
      "Iteration 20910 Training loss 0.00017745215154718608 Validation loss 0.04369574785232544 Accuracy 0.8920000195503235\n",
      "Iteration 20920 Training loss 0.002581288805231452 Validation loss 0.04372277855873108 Accuracy 0.8917500376701355\n",
      "Iteration 20930 Training loss 0.0014383288798853755 Validation loss 0.04411880671977997 Accuracy 0.8890000581741333\n",
      "Iteration 20940 Training loss 0.0014748857356607914 Validation loss 0.04383383318781853 Accuracy 0.8917500376701355\n",
      "Iteration 20950 Training loss 0.0014424510300159454 Validation loss 0.04456063732504845 Accuracy 0.8890000581741333\n",
      "Iteration 20960 Training loss 0.00011589813948376104 Validation loss 0.04380885884165764 Accuracy 0.8911250233650208\n",
      "Iteration 20970 Training loss 0.001373809645883739 Validation loss 0.04393581673502922 Accuracy 0.8908750414848328\n",
      "Iteration 20980 Training loss 0.001378202228806913 Validation loss 0.0439249649643898 Accuracy 0.8912500143051147\n",
      "Iteration 20990 Training loss 0.0013697638642042875 Validation loss 0.04406115785241127 Accuracy 0.8911250233650208\n",
      "Iteration 21000 Training loss 0.00015077157877385616 Validation loss 0.04390689358115196 Accuracy 0.8912500143051147\n",
      "Iteration 21010 Training loss 0.0025734873488545418 Validation loss 0.04390312731266022 Accuracy 0.8913750648498535\n",
      "Iteration 21020 Training loss 0.0025748147163540125 Validation loss 0.043837789446115494 Accuracy 0.8911250233650208\n",
      "Iteration 21030 Training loss 0.0014997735852375627 Validation loss 0.043831296265125275 Accuracy 0.8910000324249268\n",
      "Iteration 21040 Training loss 0.0013925159582868218 Validation loss 0.04387756437063217 Accuracy 0.893000066280365\n",
      "Iteration 21050 Training loss 9.469363430980593e-05 Validation loss 0.04387594759464264 Accuracy 0.8916250467300415\n",
      "Iteration 21060 Training loss 0.001487764879129827 Validation loss 0.04392629489302635 Accuracy 0.8912500143051147\n",
      "Iteration 21070 Training loss 0.00015396405069623142 Validation loss 0.043845515698194504 Accuracy 0.8910000324249268\n",
      "Iteration 21080 Training loss 0.005197437945753336 Validation loss 0.04394577443599701 Accuracy 0.8912500143051147\n",
      "Iteration 21090 Training loss 0.00016281555872410536 Validation loss 0.04379190877079964 Accuracy 0.8920000195503235\n",
      "Iteration 21100 Training loss 0.0013653073692694306 Validation loss 0.04387424513697624 Accuracy 0.8918750286102295\n",
      "Iteration 21110 Training loss 0.0014126569731160998 Validation loss 0.043823182582855225 Accuracy 0.8923750519752502\n",
      "Iteration 21120 Training loss 0.0026080720126628876 Validation loss 0.043774474412202835 Accuracy 0.8911250233650208\n",
      "Iteration 21130 Training loss 0.00010370564996264875 Validation loss 0.04376860335469246 Accuracy 0.8916250467300415\n",
      "Iteration 21140 Training loss 0.00011206718045286834 Validation loss 0.04386792331933975 Accuracy 0.8910000324249268\n",
      "Iteration 21150 Training loss 0.00010301515430910513 Validation loss 0.04378964379429817 Accuracy 0.8916250467300415\n",
      "Iteration 21160 Training loss 0.003946359269320965 Validation loss 0.044092874974012375 Accuracy 0.890375018119812\n",
      "Iteration 21170 Training loss 0.0026835522148758173 Validation loss 0.04387227073311806 Accuracy 0.8906250596046448\n",
      "Iteration 21180 Training loss 0.00015881058061495423 Validation loss 0.04388580471277237 Accuracy 0.8910000324249268\n",
      "Iteration 21190 Training loss 0.0013336756965145469 Validation loss 0.043840136379003525 Accuracy 0.8912500143051147\n",
      "Iteration 21200 Training loss 0.0013924395898357034 Validation loss 0.04383525252342224 Accuracy 0.8916250467300415\n",
      "Iteration 21210 Training loss 0.0002097664400935173 Validation loss 0.043835557997226715 Accuracy 0.8910000324249268\n",
      "Iteration 21220 Training loss 0.002635741839185357 Validation loss 0.04386238381266594 Accuracy 0.8911250233650208\n",
      "Iteration 21230 Training loss 0.0003670318692456931 Validation loss 0.044086847454309464 Accuracy 0.8911250233650208\n",
      "Iteration 21240 Training loss 0.0014289803802967072 Validation loss 0.04374871030449867 Accuracy 0.8912500143051147\n",
      "Iteration 21250 Training loss 0.00010080922220367938 Validation loss 0.04376232996582985 Accuracy 0.8912500143051147\n",
      "Iteration 21260 Training loss 0.0038703870959579945 Validation loss 0.04377982020378113 Accuracy 0.8923750519752502\n",
      "Iteration 21270 Training loss 0.00013370871602091938 Validation loss 0.04382344335317612 Accuracy 0.8921250700950623\n",
      "Iteration 21280 Training loss 0.003854358335956931 Validation loss 0.04375038295984268 Accuracy 0.8920000195503235\n",
      "Iteration 21290 Training loss 0.00012339063687250018 Validation loss 0.043799057602882385 Accuracy 0.8915000557899475\n",
      "Iteration 21300 Training loss 0.00010017955355579033 Validation loss 0.043841518461704254 Accuracy 0.8921250700950623\n",
      "Iteration 21310 Training loss 0.0013521478977054358 Validation loss 0.04374294355511665 Accuracy 0.8923750519752502\n",
      "Iteration 21320 Training loss 0.0026881264057010412 Validation loss 0.04419991374015808 Accuracy 0.8913750648498535\n",
      "Iteration 21330 Training loss 0.001364655327051878 Validation loss 0.04411564767360687 Accuracy 0.890125036239624\n",
      "Iteration 21340 Training loss 9.969359234673902e-05 Validation loss 0.044037558138370514 Accuracy 0.8910000324249268\n",
      "Iteration 21350 Training loss 0.003919301554560661 Validation loss 0.04397115483880043 Accuracy 0.8910000324249268\n",
      "Iteration 21360 Training loss 0.0013350393855944276 Validation loss 0.04402359202504158 Accuracy 0.8908750414848328\n",
      "Iteration 21370 Training loss 0.005104889627546072 Validation loss 0.043939705938100815 Accuracy 0.8918750286102295\n",
      "Iteration 21380 Training loss 0.0014038990484550595 Validation loss 0.04399089142680168 Accuracy 0.8908750414848328\n",
      "Iteration 21390 Training loss 0.002713149646297097 Validation loss 0.044767413288354874 Accuracy 0.8882500529289246\n",
      "Iteration 21400 Training loss 0.0022383113391697407 Validation loss 0.04569018632173538 Accuracy 0.8846250176429749\n",
      "Iteration 21410 Training loss 0.000537700136192143 Validation loss 0.04480596259236336 Accuracy 0.8890000581741333\n",
      "Iteration 21420 Training loss 0.00262902257964015 Validation loss 0.0438084751367569 Accuracy 0.890375018119812\n",
      "Iteration 21430 Training loss 0.0013838618760928512 Validation loss 0.043851930648088455 Accuracy 0.8908750414848328\n",
      "Iteration 21440 Training loss 0.002683623693883419 Validation loss 0.043694619089365005 Accuracy 0.8913750648498535\n",
      "Iteration 21450 Training loss 0.0014495478244498372 Validation loss 0.04377234727144241 Accuracy 0.8912500143051147\n",
      "Iteration 21460 Training loss 0.0038787780795246363 Validation loss 0.04381317272782326 Accuracy 0.8917500376701355\n",
      "Iteration 21470 Training loss 0.0026511403266340494 Validation loss 0.04384788125753403 Accuracy 0.8906250596046448\n",
      "Iteration 21480 Training loss 0.0026138778775930405 Validation loss 0.043886635452508926 Accuracy 0.890375018119812\n",
      "Iteration 21490 Training loss 0.002769147278741002 Validation loss 0.044081296771764755 Accuracy 0.8911250233650208\n",
      "Iteration 21500 Training loss 0.0014072476187720895 Validation loss 0.043786484748125076 Accuracy 0.8908750414848328\n",
      "Iteration 21510 Training loss 0.00020736901205964386 Validation loss 0.04379863291978836 Accuracy 0.8915000557899475\n",
      "Iteration 21520 Training loss 0.0014482563128694892 Validation loss 0.04410683736205101 Accuracy 0.8896250128746033\n",
      "Iteration 21530 Training loss 0.00020971096819266677 Validation loss 0.04384308308362961 Accuracy 0.8918750286102295\n",
      "Iteration 21540 Training loss 0.0013468320248648524 Validation loss 0.043833278119564056 Accuracy 0.890375018119812\n",
      "Iteration 21550 Training loss 0.0026038805954158306 Validation loss 0.043889615684747696 Accuracy 0.890375018119812\n",
      "Iteration 21560 Training loss 0.00010255881352350116 Validation loss 0.0438297837972641 Accuracy 0.8910000324249268\n",
      "Iteration 21570 Training loss 0.0018652105936780572 Validation loss 0.04417671635746956 Accuracy 0.8922500610351562\n",
      "Iteration 21580 Training loss 0.0025727658066898584 Validation loss 0.04384274780750275 Accuracy 0.890250027179718\n",
      "Iteration 21590 Training loss 0.002606402849778533 Validation loss 0.04387893155217171 Accuracy 0.89000004529953\n",
      "Iteration 21600 Training loss 0.00012429403432179242 Validation loss 0.043905314058065414 Accuracy 0.8910000324249268\n",
      "Iteration 21610 Training loss 0.0026096045039594173 Validation loss 0.04396195337176323 Accuracy 0.8905000686645508\n",
      "Iteration 21620 Training loss 0.0026253818068653345 Validation loss 0.04400978237390518 Accuracy 0.890125036239624\n",
      "Iteration 21630 Training loss 0.003933921921998262 Validation loss 0.04396441951394081 Accuracy 0.8906250596046448\n",
      "Iteration 21640 Training loss 0.0013265747111290693 Validation loss 0.04396366328001022 Accuracy 0.89000004529953\n",
      "Iteration 21650 Training loss 0.0013378041330724955 Validation loss 0.043824560940265656 Accuracy 0.8917500376701355\n",
      "Iteration 21660 Training loss 0.0002153474051738158 Validation loss 0.044025033712387085 Accuracy 0.8892500400543213\n",
      "Iteration 21670 Training loss 0.00015902254381217062 Validation loss 0.04519035667181015 Accuracy 0.8855000138282776\n",
      "Iteration 21680 Training loss 0.0013460780028253794 Validation loss 0.043925974518060684 Accuracy 0.8911250233650208\n",
      "Iteration 21690 Training loss 0.0026357632596045732 Validation loss 0.04392451047897339 Accuracy 0.8907500505447388\n",
      "Iteration 21700 Training loss 0.0016434263670817018 Validation loss 0.04386824369430542 Accuracy 0.8916250467300415\n",
      "Iteration 21710 Training loss 0.002623207401484251 Validation loss 0.043809954077005386 Accuracy 0.8911250233650208\n",
      "Iteration 21720 Training loss 0.001360800233669579 Validation loss 0.0438198558986187 Accuracy 0.8912500143051147\n",
      "Iteration 21730 Training loss 0.002814054721966386 Validation loss 0.04397287592291832 Accuracy 0.8912500143051147\n",
      "Iteration 21740 Training loss 0.0013829264789819717 Validation loss 0.04377450793981552 Accuracy 0.8923750519752502\n",
      "Iteration 21750 Training loss 0.00012078432337148115 Validation loss 0.043858516961336136 Accuracy 0.8921250700950623\n",
      "Iteration 21760 Training loss 0.0013625586871057749 Validation loss 0.04392720386385918 Accuracy 0.8908750414848328\n",
      "Iteration 21770 Training loss 0.00010740748984972015 Validation loss 0.04394696652889252 Accuracy 0.8912500143051147\n",
      "Iteration 21780 Training loss 0.00011182741582160816 Validation loss 0.04397730529308319 Accuracy 0.8917500376701355\n",
      "Iteration 21790 Training loss 0.0013549468712881207 Validation loss 0.04394054040312767 Accuracy 0.889750063419342\n",
      "Iteration 21800 Training loss 0.0014049469027668238 Validation loss 0.044055406004190445 Accuracy 0.8916250467300415\n",
      "Iteration 21810 Training loss 0.002600722946226597 Validation loss 0.04391530156135559 Accuracy 0.8918750286102295\n",
      "Iteration 21820 Training loss 0.0014248085208237171 Validation loss 0.04390229284763336 Accuracy 0.8915000557899475\n",
      "Iteration 21830 Training loss 0.002610839204862714 Validation loss 0.04383986443281174 Accuracy 0.8916250467300415\n",
      "Iteration 21840 Training loss 0.001330779166892171 Validation loss 0.04386527091264725 Accuracy 0.8917500376701355\n",
      "Iteration 21850 Training loss 0.00012066652561770752 Validation loss 0.04391312226653099 Accuracy 0.8925000429153442\n",
      "Iteration 21860 Training loss 0.0013420588802546263 Validation loss 0.04400542750954628 Accuracy 0.8918750286102295\n",
      "Iteration 21870 Training loss 0.00012320505629759282 Validation loss 0.04396532103419304 Accuracy 0.8916250467300415\n",
      "Iteration 21880 Training loss 0.0013594877673313022 Validation loss 0.044084250926971436 Accuracy 0.890250027179718\n",
      "Iteration 21890 Training loss 0.0038725044578313828 Validation loss 0.044009722769260406 Accuracy 0.8916250467300415\n",
      "Iteration 21900 Training loss 0.00011160906433360651 Validation loss 0.044028639793395996 Accuracy 0.8915000557899475\n",
      "Iteration 21910 Training loss 0.000154214576468803 Validation loss 0.043919168412685394 Accuracy 0.8913750648498535\n",
      "Iteration 21920 Training loss 9.166444215225056e-05 Validation loss 0.044005099684000015 Accuracy 0.8908750414848328\n",
      "Iteration 21930 Training loss 0.003849478904157877 Validation loss 0.04403962939977646 Accuracy 0.8910000324249268\n",
      "Iteration 21940 Training loss 0.003924182150512934 Validation loss 0.04424155503511429 Accuracy 0.890375018119812\n",
      "Iteration 21950 Training loss 0.0013707958860322833 Validation loss 0.044290393590927124 Accuracy 0.89000004529953\n",
      "Iteration 21960 Training loss 0.0013558086939156055 Validation loss 0.043985724449157715 Accuracy 0.8908750414848328\n",
      "Iteration 21970 Training loss 0.0013997243950143456 Validation loss 0.04410412907600403 Accuracy 0.8905000686645508\n",
      "Iteration 21980 Training loss 0.0013508711708709598 Validation loss 0.04406020790338516 Accuracy 0.8907500505447388\n",
      "Iteration 21990 Training loss 0.0014696777798235416 Validation loss 0.0441475510597229 Accuracy 0.8912500143051147\n",
      "Iteration 22000 Training loss 0.0013449550606310368 Validation loss 0.04398326575756073 Accuracy 0.8912500143051147\n",
      "Iteration 22010 Training loss 0.001318823779001832 Validation loss 0.04392869397997856 Accuracy 0.8916250467300415\n",
      "Iteration 22020 Training loss 0.0013222061097621918 Validation loss 0.04399479553103447 Accuracy 0.8911250233650208\n",
      "Iteration 22030 Training loss 0.0013970312429592013 Validation loss 0.04401800036430359 Accuracy 0.8921250700950623\n",
      "Iteration 22040 Training loss 0.00012779916869476438 Validation loss 0.04393870756030083 Accuracy 0.8913750648498535\n",
      "Iteration 22050 Training loss 0.002592873526737094 Validation loss 0.043902527540922165 Accuracy 0.8915000557899475\n",
      "Iteration 22060 Training loss 0.0013778583379462361 Validation loss 0.04393934831023216 Accuracy 0.8916250467300415\n",
      "Iteration 22070 Training loss 0.0026218509301543236 Validation loss 0.04394880682229996 Accuracy 0.8912500143051147\n",
      "Iteration 22080 Training loss 0.0051483456045389175 Validation loss 0.044029880315065384 Accuracy 0.8910000324249268\n",
      "Iteration 22090 Training loss 0.0025735662784427404 Validation loss 0.04393001273274422 Accuracy 0.8915000557899475\n",
      "Iteration 22100 Training loss 0.00011520765110617504 Validation loss 0.04403449967503548 Accuracy 0.890125036239624\n",
      "Iteration 22110 Training loss 0.002573317149654031 Validation loss 0.043939460068941116 Accuracy 0.8910000324249268\n",
      "Iteration 22120 Training loss 0.0051185875199735165 Validation loss 0.044075921177864075 Accuracy 0.890375018119812\n",
      "Iteration 22130 Training loss 0.001359110581688583 Validation loss 0.043785545974969864 Accuracy 0.8921250700950623\n",
      "Iteration 22140 Training loss 0.003878672607243061 Validation loss 0.04390177130699158 Accuracy 0.8920000195503235\n",
      "Iteration 22150 Training loss 0.003839651821181178 Validation loss 0.04392952844500542 Accuracy 0.8915000557899475\n",
      "Iteration 22160 Training loss 0.001327239559032023 Validation loss 0.043984305113554 Accuracy 0.8921250700950623\n",
      "Iteration 22170 Training loss 0.001349643454886973 Validation loss 0.0439048707485199 Accuracy 0.8922500610351562\n",
      "Iteration 22180 Training loss 0.0001717366831144318 Validation loss 0.04395829886198044 Accuracy 0.8920000195503235\n",
      "Iteration 22190 Training loss 0.00011692394036799669 Validation loss 0.043973665684461594 Accuracy 0.8915000557899475\n",
      "Iteration 22200 Training loss 0.003885171376168728 Validation loss 0.04416581243276596 Accuracy 0.8912500143051147\n",
      "Iteration 22210 Training loss 0.0014252495020627975 Validation loss 0.04413206875324249 Accuracy 0.89000004529953\n",
      "Iteration 22220 Training loss 7.434052531607449e-05 Validation loss 0.04398532584309578 Accuracy 0.8906250596046448\n",
      "Iteration 22230 Training loss 0.0018398407846689224 Validation loss 0.04398920759558678 Accuracy 0.8907500505447388\n",
      "Iteration 22240 Training loss 0.002660200232639909 Validation loss 0.04395760968327522 Accuracy 0.8912500143051147\n",
      "Iteration 22250 Training loss 0.0013771143276244402 Validation loss 0.04395494982600212 Accuracy 0.8916250467300415\n",
      "Iteration 22260 Training loss 0.0001457333128200844 Validation loss 0.04394884780049324 Accuracy 0.8911250233650208\n",
      "Iteration 22270 Training loss 0.0026039564982056618 Validation loss 0.04386435076594353 Accuracy 0.8912500143051147\n",
      "Iteration 22280 Training loss 0.0028046316001564264 Validation loss 0.04395739361643791 Accuracy 0.8915000557899475\n",
      "Iteration 22290 Training loss 0.0014843074604868889 Validation loss 0.044209763407707214 Accuracy 0.8913750648498535\n",
      "Iteration 22300 Training loss 0.00013017789751756936 Validation loss 0.043989136815071106 Accuracy 0.8927500247955322\n",
      "Iteration 22310 Training loss 0.00260138395242393 Validation loss 0.04402443394064903 Accuracy 0.8912500143051147\n",
      "Iteration 22320 Training loss 0.00020324651268310845 Validation loss 0.044055353850126266 Accuracy 0.8912500143051147\n",
      "Iteration 22330 Training loss 0.001347496174275875 Validation loss 0.044054094702005386 Accuracy 0.8905000686645508\n",
      "Iteration 22340 Training loss 0.0013572954339906573 Validation loss 0.04428313672542572 Accuracy 0.889750063419342\n",
      "Iteration 22350 Training loss 9.912332461681217e-05 Validation loss 0.04394670948386192 Accuracy 0.8908750414848328\n",
      "Iteration 22360 Training loss 0.00016506269457750022 Validation loss 0.04414014145731926 Accuracy 0.8908750414848328\n",
      "Iteration 22370 Training loss 9.310371387982741e-05 Validation loss 0.04408354312181473 Accuracy 0.8917500376701355\n",
      "Iteration 22380 Training loss 0.001342383911833167 Validation loss 0.044005174189805984 Accuracy 0.890375018119812\n",
      "Iteration 22390 Training loss 0.0026536397635936737 Validation loss 0.044001154601573944 Accuracy 0.8905000686645508\n",
      "Iteration 22400 Training loss 0.0014328235993161798 Validation loss 0.04403400793671608 Accuracy 0.8910000324249268\n",
      "Iteration 22410 Training loss 0.0013512675650417805 Validation loss 0.04412220045924187 Accuracy 0.890125036239624\n",
      "Iteration 22420 Training loss 0.0013890170957893133 Validation loss 0.043982964009046555 Accuracy 0.8906250596046448\n",
      "Iteration 22430 Training loss 0.0025997154880315065 Validation loss 0.043979402631521225 Accuracy 0.8912500143051147\n",
      "Iteration 22440 Training loss 0.0013380302116274834 Validation loss 0.044025544077157974 Accuracy 0.8905000686645508\n",
      "Iteration 22450 Training loss 0.001374885905534029 Validation loss 0.044114984571933746 Accuracy 0.8906250596046448\n",
      "Iteration 22460 Training loss 0.0013751849764958024 Validation loss 0.04411369934678078 Accuracy 0.8905000686645508\n",
      "Iteration 22470 Training loss 0.002688994398340583 Validation loss 0.04416247829794884 Accuracy 0.8906250596046448\n",
      "Iteration 22480 Training loss 0.0013371866662055254 Validation loss 0.04414396733045578 Accuracy 0.8908750414848328\n",
      "Iteration 22490 Training loss 0.0001508772693341598 Validation loss 0.04412923380732536 Accuracy 0.8906250596046448\n",
      "Iteration 22500 Training loss 0.0013469122350215912 Validation loss 0.04409334808588028 Accuracy 0.8905000686645508\n",
      "Iteration 22510 Training loss 0.001367782591842115 Validation loss 0.044009797275066376 Accuracy 0.8907500505447388\n",
      "Iteration 22520 Training loss 0.006414956878870726 Validation loss 0.04412524774670601 Accuracy 0.890375018119812\n",
      "Iteration 22530 Training loss 0.00012849841732531786 Validation loss 0.044041574001312256 Accuracy 0.8905000686645508\n",
      "Iteration 22540 Training loss 0.00011724161595338956 Validation loss 0.044204454869031906 Accuracy 0.890250027179718\n",
      "Iteration 22550 Training loss 0.0026808029506355524 Validation loss 0.044009871780872345 Accuracy 0.8911250233650208\n",
      "Iteration 22560 Training loss 0.0025932129938155413 Validation loss 0.044107913970947266 Accuracy 0.89000004529953\n",
      "Iteration 22570 Training loss 0.002605958841741085 Validation loss 0.04400733485817909 Accuracy 0.8912500143051147\n",
      "Iteration 22580 Training loss 0.0001066273616743274 Validation loss 0.04411944001913071 Accuracy 0.8906250596046448\n",
      "Iteration 22590 Training loss 0.0001015091038425453 Validation loss 0.04412463679909706 Accuracy 0.8906250596046448\n",
      "Iteration 22600 Training loss 0.0013779987348243594 Validation loss 0.04405659809708595 Accuracy 0.8908750414848328\n",
      "Iteration 22610 Training loss 0.0026713416446000338 Validation loss 0.044175151735544205 Accuracy 0.8922500610351562\n",
      "Iteration 22620 Training loss 0.00016291595238726586 Validation loss 0.04399462789297104 Accuracy 0.8921250700950623\n",
      "Iteration 22630 Training loss 0.0013653675559908152 Validation loss 0.043955303728580475 Accuracy 0.8921250700950623\n",
      "Iteration 22640 Training loss 0.001395519240759313 Validation loss 0.044036608189344406 Accuracy 0.8910000324249268\n",
      "Iteration 22650 Training loss 0.0050997561775147915 Validation loss 0.044040605425834656 Accuracy 0.8912500143051147\n",
      "Iteration 22660 Training loss 0.0013825553469359875 Validation loss 0.04412226378917694 Accuracy 0.890125036239624\n",
      "Iteration 22670 Training loss 0.0001485894899815321 Validation loss 0.04401281848549843 Accuracy 0.8906250596046448\n",
      "Iteration 22680 Training loss 0.00017642488819546998 Validation loss 0.04394010826945305 Accuracy 0.8917500376701355\n",
      "Iteration 22690 Training loss 0.002580420346930623 Validation loss 0.043959420174360275 Accuracy 0.8915000557899475\n",
      "Iteration 22700 Training loss 7.172163168434054e-05 Validation loss 0.04394185543060303 Accuracy 0.8913750648498535\n",
      "Iteration 22710 Training loss 0.0013574117328971624 Validation loss 0.044000446796417236 Accuracy 0.8907500505447388\n",
      "Iteration 22720 Training loss 0.003892316250130534 Validation loss 0.04400207847356796 Accuracy 0.8906250596046448\n",
      "Iteration 22730 Training loss 0.001339175272732973 Validation loss 0.04406086355447769 Accuracy 0.8906250596046448\n",
      "Iteration 22740 Training loss 0.0026110762264579535 Validation loss 0.043974827975034714 Accuracy 0.8908750414848328\n",
      "Iteration 22750 Training loss 0.0025882916525006294 Validation loss 0.043987926095724106 Accuracy 0.8918750286102295\n",
      "Iteration 22760 Training loss 0.00021067207853775471 Validation loss 0.04396779090166092 Accuracy 0.8915000557899475\n",
      "Iteration 22770 Training loss 0.0013498722109943628 Validation loss 0.044019199907779694 Accuracy 0.8907500505447388\n",
      "Iteration 22780 Training loss 0.0013701486168429255 Validation loss 0.04413869231939316 Accuracy 0.8906250596046448\n",
      "Iteration 22790 Training loss 0.002628570655360818 Validation loss 0.04410512000322342 Accuracy 0.890375018119812\n",
      "Iteration 22800 Training loss 0.001352984458208084 Validation loss 0.0440896637737751 Accuracy 0.890375018119812\n",
      "Iteration 22810 Training loss 0.0025925408117473125 Validation loss 0.04418454319238663 Accuracy 0.8910000324249268\n",
      "Iteration 22820 Training loss 0.0013263551518321037 Validation loss 0.04408596456050873 Accuracy 0.89000004529953\n",
      "Iteration 22830 Training loss 0.002715636044740677 Validation loss 0.04405298829078674 Accuracy 0.890375018119812\n",
      "Iteration 22840 Training loss 0.0013294764794409275 Validation loss 0.0440288707613945 Accuracy 0.8906250596046448\n",
      "Iteration 22850 Training loss 0.00025760388234630227 Validation loss 0.044280439615249634 Accuracy 0.8925000429153442\n",
      "Iteration 22860 Training loss 0.001366432523354888 Validation loss 0.04416776821017265 Accuracy 0.890375018119812\n",
      "Iteration 22870 Training loss 0.0013269161572679877 Validation loss 0.04418905824422836 Accuracy 0.8905000686645508\n",
      "Iteration 22880 Training loss 0.002588173607364297 Validation loss 0.04416235536336899 Accuracy 0.890375018119812\n",
      "Iteration 22890 Training loss 0.0013632558984681964 Validation loss 0.04407242313027382 Accuracy 0.8911250233650208\n",
      "Iteration 22900 Training loss 0.001363342278636992 Validation loss 0.04402819648385048 Accuracy 0.8905000686645508\n",
      "Iteration 22910 Training loss 0.0027089561335742474 Validation loss 0.045007944107055664 Accuracy 0.8866250514984131\n",
      "Iteration 22920 Training loss 0.001389967044815421 Validation loss 0.04409249499440193 Accuracy 0.8905000686645508\n",
      "Iteration 22930 Training loss 0.0013376462738960981 Validation loss 0.044155552983284 Accuracy 0.89000004529953\n",
      "Iteration 22940 Training loss 0.0013706055469810963 Validation loss 0.0442260317504406 Accuracy 0.8906250596046448\n",
      "Iteration 22950 Training loss 0.0013624833663925529 Validation loss 0.044463906437158585 Accuracy 0.890125036239624\n",
      "Iteration 22960 Training loss 0.002615314442664385 Validation loss 0.04410407692193985 Accuracy 0.8896250128746033\n",
      "Iteration 22970 Training loss 0.0013763144379481673 Validation loss 0.0440450944006443 Accuracy 0.8913750648498535\n",
      "Iteration 22980 Training loss 0.0038454134482890368 Validation loss 0.044129256159067154 Accuracy 0.8911250233650208\n",
      "Iteration 22990 Training loss 0.003845855128020048 Validation loss 0.04416703060269356 Accuracy 0.89000004529953\n",
      "Iteration 23000 Training loss 0.00012326914293225855 Validation loss 0.044081781059503555 Accuracy 0.8910000324249268\n",
      "Iteration 23010 Training loss 0.002671587746590376 Validation loss 0.04411407560110092 Accuracy 0.8908750414848328\n",
      "Iteration 23020 Training loss 0.0013718551490455866 Validation loss 0.04408866539597511 Accuracy 0.8907500505447388\n",
      "Iteration 23030 Training loss 0.0026128238532692194 Validation loss 0.04414787143468857 Accuracy 0.8911250233650208\n",
      "Iteration 23040 Training loss 0.0013005745131522417 Validation loss 0.04425741732120514 Accuracy 0.890375018119812\n",
      "Iteration 23050 Training loss 0.006330560427159071 Validation loss 0.04408466815948486 Accuracy 0.893000066280365\n",
      "Iteration 23060 Training loss 0.0014117801329120994 Validation loss 0.04406880959868431 Accuracy 0.8916250467300415\n",
      "Iteration 23070 Training loss 0.0013424985809251666 Validation loss 0.04411941021680832 Accuracy 0.8905000686645508\n",
      "Iteration 23080 Training loss 0.0013323588063940406 Validation loss 0.04422849044203758 Accuracy 0.8911250233650208\n",
      "Iteration 23090 Training loss 0.001398306107148528 Validation loss 0.04421002045273781 Accuracy 0.8906250596046448\n",
      "Iteration 23100 Training loss 0.0001525349944131449 Validation loss 0.04409463331103325 Accuracy 0.8910000324249268\n",
      "Iteration 23110 Training loss 0.0038392459973692894 Validation loss 0.04406948760151863 Accuracy 0.8911250233650208\n",
      "Iteration 23120 Training loss 0.00012516844435594976 Validation loss 0.04410335794091225 Accuracy 0.890375018119812\n",
      "Iteration 23130 Training loss 0.00259502325206995 Validation loss 0.04412278160452843 Accuracy 0.8907500505447388\n",
      "Iteration 23140 Training loss 0.0013438202440738678 Validation loss 0.04410865157842636 Accuracy 0.8911250233650208\n",
      "Iteration 23150 Training loss 8.936922677094117e-05 Validation loss 0.044243376702070236 Accuracy 0.8895000219345093\n",
      "Iteration 23160 Training loss 0.0025942570064216852 Validation loss 0.04411168023943901 Accuracy 0.8910000324249268\n",
      "Iteration 23170 Training loss 0.0013586742570623755 Validation loss 0.04411142319440842 Accuracy 0.8905000686645508\n",
      "Iteration 23180 Training loss 0.0025973489973694086 Validation loss 0.044248368591070175 Accuracy 0.8893750309944153\n",
      "Iteration 23190 Training loss 0.00010423777712276205 Validation loss 0.044145017862319946 Accuracy 0.8908750414848328\n",
      "Iteration 23200 Training loss 7.139670196920633e-05 Validation loss 0.044063273817300797 Accuracy 0.8907500505447388\n",
      "Iteration 23210 Training loss 0.002632548799738288 Validation loss 0.04426981881260872 Accuracy 0.890375018119812\n",
      "Iteration 23220 Training loss 0.00260438141413033 Validation loss 0.04410131275653839 Accuracy 0.8910000324249268\n",
      "Iteration 23230 Training loss 0.0025971883442252874 Validation loss 0.044146616011857986 Accuracy 0.8908750414848328\n",
      "Iteration 23240 Training loss 0.001315429457463324 Validation loss 0.04408498853445053 Accuracy 0.8915000557899475\n",
      "Iteration 23250 Training loss 0.002573374193161726 Validation loss 0.044126927852630615 Accuracy 0.8912500143051147\n",
      "Iteration 23260 Training loss 8.73990502441302e-05 Validation loss 0.04399733617901802 Accuracy 0.8920000195503235\n",
      "Iteration 23270 Training loss 0.0013189631281420588 Validation loss 0.044221535325050354 Accuracy 0.889875054359436\n",
      "Iteration 23280 Training loss 0.00010894256411120296 Validation loss 0.04407183453440666 Accuracy 0.8911250233650208\n",
      "Iteration 23290 Training loss 0.0013409050879999995 Validation loss 0.044041622430086136 Accuracy 0.8912500143051147\n",
      "Iteration 23300 Training loss 0.0026039043441414833 Validation loss 0.04418087378144264 Accuracy 0.8906250596046448\n",
      "Iteration 23310 Training loss 0.0013956533512100577 Validation loss 0.04487834498286247 Accuracy 0.8883750438690186\n",
      "Iteration 23320 Training loss 0.0038479133509099483 Validation loss 0.04419519379734993 Accuracy 0.890250027179718\n",
      "Iteration 23330 Training loss 9.502846660325304e-05 Validation loss 0.04405708611011505 Accuracy 0.8920000195503235\n",
      "Iteration 23340 Training loss 0.0025805782061070204 Validation loss 0.04403524845838547 Accuracy 0.8907500505447388\n",
      "Iteration 23350 Training loss 0.002587831811979413 Validation loss 0.044020526111125946 Accuracy 0.8917500376701355\n",
      "Iteration 23360 Training loss 0.0025996086187660694 Validation loss 0.044060252606868744 Accuracy 0.8908750414848328\n",
      "Iteration 23370 Training loss 0.0025731006171554327 Validation loss 0.044112689793109894 Accuracy 0.890375018119812\n",
      "Iteration 23380 Training loss 0.0038071726448833942 Validation loss 0.044063761830329895 Accuracy 0.8908750414848328\n",
      "Iteration 23390 Training loss 0.0001191261108033359 Validation loss 0.04410180076956749 Accuracy 0.8896250128746033\n",
      "Iteration 23400 Training loss 8.51453878567554e-05 Validation loss 0.044249679893255234 Accuracy 0.8896250128746033\n",
      "Iteration 23410 Training loss 0.002598137827590108 Validation loss 0.04418613761663437 Accuracy 0.889875054359436\n",
      "Iteration 23420 Training loss 0.0013596812495961785 Validation loss 0.04411311447620392 Accuracy 0.8907500505447388\n",
      "Iteration 23430 Training loss 0.0013159796362742782 Validation loss 0.0440925657749176 Accuracy 0.890250027179718\n",
      "Iteration 23440 Training loss 0.0013369987718760967 Validation loss 0.0440867580473423 Accuracy 0.889875054359436\n",
      "Iteration 23450 Training loss 9.128441888606176e-05 Validation loss 0.044106192886829376 Accuracy 0.8907500505447388\n",
      "Iteration 23460 Training loss 0.00014809094136580825 Validation loss 0.04421639442443848 Accuracy 0.889750063419342\n",
      "Iteration 23470 Training loss 0.0038312494289129972 Validation loss 0.0440368726849556 Accuracy 0.8907500505447388\n",
      "Iteration 23480 Training loss 8.76237972988747e-05 Validation loss 0.044038135558366776 Accuracy 0.8907500505447388\n",
      "Iteration 23490 Training loss 9.369327744934708e-05 Validation loss 0.04408400133252144 Accuracy 0.889875054359436\n",
      "Iteration 23500 Training loss 0.001347402110695839 Validation loss 0.04406079277396202 Accuracy 0.8910000324249268\n",
      "Iteration 23510 Training loss 0.00258903787471354 Validation loss 0.044074442237615585 Accuracy 0.8915000557899475\n",
      "Iteration 23520 Training loss 0.00010500611824681982 Validation loss 0.04402513429522514 Accuracy 0.8911250233650208\n",
      "Iteration 23530 Training loss 0.005137409083545208 Validation loss 0.04413832724094391 Accuracy 0.890125036239624\n",
      "Iteration 23540 Training loss 0.0025598183274269104 Validation loss 0.04414721578359604 Accuracy 0.8906250596046448\n",
      "Iteration 23550 Training loss 7.924465899122879e-05 Validation loss 0.0440964475274086 Accuracy 0.8908750414848328\n",
      "Iteration 23560 Training loss 0.0015510827070102096 Validation loss 0.04409356787800789 Accuracy 0.8911250233650208\n",
      "Iteration 23570 Training loss 0.0026072028558701277 Validation loss 0.04408038407564163 Accuracy 0.890375018119812\n",
      "Iteration 23580 Training loss 0.00515442481264472 Validation loss 0.04400905221700668 Accuracy 0.8912500143051147\n",
      "Iteration 23590 Training loss 0.003845795290544629 Validation loss 0.044018182903528214 Accuracy 0.8908750414848328\n",
      "Iteration 23600 Training loss 0.00018345944408793002 Validation loss 0.044074732810258865 Accuracy 0.889875054359436\n",
      "Iteration 23610 Training loss 0.001365529838949442 Validation loss 0.044177502393722534 Accuracy 0.890375018119812\n",
      "Iteration 23620 Training loss 0.005103566683828831 Validation loss 0.044212456792593 Accuracy 0.89000004529953\n",
      "Iteration 23630 Training loss 0.00386075209826231 Validation loss 0.04405129328370094 Accuracy 0.8911250233650208\n",
      "Iteration 23640 Training loss 0.0013333524111658335 Validation loss 0.044115107506513596 Accuracy 0.8908750414848328\n",
      "Iteration 23650 Training loss 0.003917065914720297 Validation loss 0.04410569369792938 Accuracy 0.8906250596046448\n",
      "Iteration 23660 Training loss 0.00012522305769380182 Validation loss 0.04413911700248718 Accuracy 0.889875054359436\n",
      "Iteration 23670 Training loss 0.0014223068719729781 Validation loss 0.044159211218357086 Accuracy 0.890375018119812\n",
      "Iteration 23680 Training loss 6.566603406099603e-05 Validation loss 0.044225435703992844 Accuracy 0.890375018119812\n",
      "Iteration 23690 Training loss 0.0038599169347435236 Validation loss 0.04416769742965698 Accuracy 0.8906250596046448\n",
      "Iteration 23700 Training loss 0.00011038584489142522 Validation loss 0.044153664261102676 Accuracy 0.8906250596046448\n",
      "Iteration 23710 Training loss 0.0013124607503414154 Validation loss 0.04418400302529335 Accuracy 0.890375018119812\n",
      "Iteration 23720 Training loss 0.0013794744154438376 Validation loss 0.04416579008102417 Accuracy 0.8905000686645508\n",
      "Iteration 23730 Training loss 0.0013326945481821895 Validation loss 0.044167861342430115 Accuracy 0.8907500505447388\n",
      "Iteration 23740 Training loss 0.0013997589703649282 Validation loss 0.04431700333952904 Accuracy 0.890375018119812\n",
      "Iteration 23750 Training loss 0.001308808452449739 Validation loss 0.044264085590839386 Accuracy 0.889750063419342\n",
      "Iteration 23760 Training loss 7.46801815694198e-05 Validation loss 0.04407896474003792 Accuracy 0.8915000557899475\n",
      "Iteration 23770 Training loss 0.00012698426144197583 Validation loss 0.044090189039707184 Accuracy 0.8911250233650208\n",
      "Iteration 23780 Training loss 0.0013389082159847021 Validation loss 0.04417753592133522 Accuracy 0.8908750414848328\n",
      "Iteration 23790 Training loss 0.0013742931187152863 Validation loss 0.04410374537110329 Accuracy 0.8905000686645508\n",
      "Iteration 23800 Training loss 0.0025797488633543253 Validation loss 0.04411241412162781 Accuracy 0.8905000686645508\n",
      "Iteration 23810 Training loss 6.509207014460117e-05 Validation loss 0.04414236918091774 Accuracy 0.89000004529953\n",
      "Iteration 23820 Training loss 0.0013902088394388556 Validation loss 0.04413369297981262 Accuracy 0.8907500505447388\n",
      "Iteration 23830 Training loss 0.0025744535960257053 Validation loss 0.044215571135282516 Accuracy 0.889750063419342\n",
      "Iteration 23840 Training loss 0.00011280768376309425 Validation loss 0.04412709176540375 Accuracy 0.8910000324249268\n",
      "Iteration 23850 Training loss 0.00010467737592989579 Validation loss 0.0442119836807251 Accuracy 0.8905000686645508\n",
      "Iteration 23860 Training loss 0.0001144464040407911 Validation loss 0.04409642145037651 Accuracy 0.890375018119812\n",
      "Iteration 23870 Training loss 0.00511837238445878 Validation loss 0.04415566474199295 Accuracy 0.8908750414848328\n",
      "Iteration 23880 Training loss 0.001349861267954111 Validation loss 0.04407614469528198 Accuracy 0.8908750414848328\n",
      "Iteration 23890 Training loss 0.0013442190829664469 Validation loss 0.04411681741476059 Accuracy 0.8908750414848328\n",
      "Iteration 23900 Training loss 0.0026053639594465494 Validation loss 0.04411976784467697 Accuracy 0.8908750414848328\n",
      "Iteration 23910 Training loss 9.803727880353108e-05 Validation loss 0.04434884339570999 Accuracy 0.8911250233650208\n",
      "Iteration 23920 Training loss 0.001326065743342042 Validation loss 0.04415754973888397 Accuracy 0.8907500505447388\n",
      "Iteration 23930 Training loss 0.002605183981359005 Validation loss 0.04419447109103203 Accuracy 0.8906250596046448\n",
      "Iteration 23940 Training loss 8.789818821242079e-05 Validation loss 0.04422110691666603 Accuracy 0.8905000686645508\n",
      "Iteration 23950 Training loss 6.754934292985126e-05 Validation loss 0.04409031197428703 Accuracy 0.8913750648498535\n",
      "Iteration 23960 Training loss 0.0013336465926840901 Validation loss 0.044104184955358505 Accuracy 0.8905000686645508\n",
      "Iteration 23970 Training loss 0.0013697242829948664 Validation loss 0.04411032423377037 Accuracy 0.8912500143051147\n",
      "Iteration 23980 Training loss 0.00011179599823663011 Validation loss 0.044110339134931564 Accuracy 0.8913750648498535\n",
      "Iteration 23990 Training loss 0.003821347840130329 Validation loss 0.04417639970779419 Accuracy 0.8908750414848328\n",
      "Iteration 24000 Training loss 0.002591228811070323 Validation loss 0.044188205152750015 Accuracy 0.8917500376701355\n",
      "Iteration 24010 Training loss 8.506735321134329e-05 Validation loss 0.0443207286298275 Accuracy 0.889875054359436\n",
      "Iteration 24020 Training loss 0.003857686650007963 Validation loss 0.04423971846699715 Accuracy 0.8911250233650208\n",
      "Iteration 24030 Training loss 0.0026084817945957184 Validation loss 0.044264376163482666 Accuracy 0.8893750309944153\n",
      "Iteration 24040 Training loss 0.0013539341744035482 Validation loss 0.04416844993829727 Accuracy 0.890250027179718\n",
      "Iteration 24050 Training loss 0.003853453090414405 Validation loss 0.04410791024565697 Accuracy 0.8911250233650208\n",
      "Iteration 24060 Training loss 0.0050894273445010185 Validation loss 0.04415537044405937 Accuracy 0.8910000324249268\n",
      "Iteration 24070 Training loss 7.788695074850693e-05 Validation loss 0.04418105259537697 Accuracy 0.8906250596046448\n",
      "Iteration 24080 Training loss 0.001334900502115488 Validation loss 0.04425343871116638 Accuracy 0.890250027179718\n",
      "Iteration 24090 Training loss 0.0013135754270479083 Validation loss 0.044266872107982635 Accuracy 0.8908750414848328\n",
      "Iteration 24100 Training loss 0.0013454864965751767 Validation loss 0.04417763650417328 Accuracy 0.8915000557899475\n",
      "Iteration 24110 Training loss 0.001317005604505539 Validation loss 0.04423525556921959 Accuracy 0.8907500505447388\n",
      "Iteration 24120 Training loss 0.0013246574671939015 Validation loss 0.04429420456290245 Accuracy 0.890250027179718\n",
      "Iteration 24130 Training loss 0.00264794216491282 Validation loss 0.044320061802864075 Accuracy 0.8911250233650208\n",
      "Iteration 24140 Training loss 0.00010696396930143237 Validation loss 0.044260211288928986 Accuracy 0.890250027179718\n",
      "Iteration 24150 Training loss 7.346815982600674e-05 Validation loss 0.04430067539215088 Accuracy 0.890250027179718\n",
      "Iteration 24160 Training loss 8.525196608388796e-05 Validation loss 0.04422636702656746 Accuracy 0.8906250596046448\n",
      "Iteration 24170 Training loss 0.0013612719485536218 Validation loss 0.04421110823750496 Accuracy 0.8906250596046448\n",
      "Iteration 24180 Training loss 0.0013403405901044607 Validation loss 0.04413091391324997 Accuracy 0.8910000324249268\n",
      "Iteration 24190 Training loss 0.0013580627273768187 Validation loss 0.04415740817785263 Accuracy 0.8911250233650208\n",
      "Iteration 24200 Training loss 0.0013253162615001202 Validation loss 0.04418119043111801 Accuracy 0.89000004529953\n",
      "Iteration 24210 Training loss 0.001330527593381703 Validation loss 0.0442107617855072 Accuracy 0.8908750414848328\n",
      "Iteration 24220 Training loss 0.0013303409796208143 Validation loss 0.04418649524450302 Accuracy 0.8912500143051147\n",
      "Iteration 24230 Training loss 7.662335701752454e-05 Validation loss 0.044594358652830124 Accuracy 0.890375018119812\n",
      "Iteration 24240 Training loss 8.635334233986214e-05 Validation loss 0.04439898952841759 Accuracy 0.8906250596046448\n",
      "Iteration 24250 Training loss 0.0013261204585433006 Validation loss 0.04446299746632576 Accuracy 0.8892500400543213\n",
      "Iteration 24260 Training loss 0.001331112696789205 Validation loss 0.04439752548933029 Accuracy 0.8908750414848328\n",
      "Iteration 24270 Training loss 0.0013161612441763282 Validation loss 0.0443839505314827 Accuracy 0.8906250596046448\n",
      "Iteration 24280 Training loss 0.001336797489784658 Validation loss 0.044309377670288086 Accuracy 0.890125036239624\n",
      "Iteration 24290 Training loss 0.0026411395519971848 Validation loss 0.04432046040892601 Accuracy 0.890125036239624\n",
      "Iteration 24300 Training loss 0.00010053641744889319 Validation loss 0.044273313134908676 Accuracy 0.8906250596046448\n",
      "Iteration 24310 Training loss 0.0025917231105268 Validation loss 0.04432664066553116 Accuracy 0.890125036239624\n",
      "Iteration 24320 Training loss 8.552020153729245e-05 Validation loss 0.04439226910471916 Accuracy 0.89000004529953\n",
      "Iteration 24330 Training loss 0.0026084636338055134 Validation loss 0.044349122792482376 Accuracy 0.889750063419342\n",
      "Iteration 24340 Training loss 0.0026208017952740192 Validation loss 0.04433006793260574 Accuracy 0.8890000581741333\n",
      "Iteration 24350 Training loss 0.0013221714179962873 Validation loss 0.04426734521985054 Accuracy 0.8908750414848328\n",
      "Iteration 24360 Training loss 0.0038218374829739332 Validation loss 0.04428382217884064 Accuracy 0.8905000686645508\n",
      "Iteration 24370 Training loss 0.0038920121733099222 Validation loss 0.04439998418092728 Accuracy 0.89000004529953\n",
      "Iteration 24380 Training loss 0.0013276705285534263 Validation loss 0.04444395750761032 Accuracy 0.8891250491142273\n",
      "Iteration 24390 Training loss 0.001314658671617508 Validation loss 0.044381868094205856 Accuracy 0.8890000581741333\n",
      "Iteration 24400 Training loss 0.0013464180519804358 Validation loss 0.044368624687194824 Accuracy 0.890125036239624\n",
      "Iteration 24410 Training loss 0.005097468849271536 Validation loss 0.04430873692035675 Accuracy 0.8895000219345093\n",
      "Iteration 24420 Training loss 0.0013439274625852704 Validation loss 0.04426540434360504 Accuracy 0.8912500143051147\n",
      "Iteration 24430 Training loss 9.146593220066279e-05 Validation loss 0.04426920413970947 Accuracy 0.890250027179718\n",
      "Iteration 24440 Training loss 0.0013734394451603293 Validation loss 0.04438811168074608 Accuracy 0.889875054359436\n",
      "Iteration 24450 Training loss 0.0038581518456339836 Validation loss 0.04423225671052933 Accuracy 0.890125036239624\n",
      "Iteration 24460 Training loss 8.634341793367639e-05 Validation loss 0.04425281658768654 Accuracy 0.8917500376701355\n",
      "Iteration 24470 Training loss 9.653440793044865e-05 Validation loss 0.044271308928728104 Accuracy 0.8910000324249268\n",
      "Iteration 24480 Training loss 0.002599561121314764 Validation loss 0.04429163038730621 Accuracy 0.8905000686645508\n",
      "Iteration 24490 Training loss 0.005085756070911884 Validation loss 0.044400736689567566 Accuracy 0.890250027179718\n",
      "Iteration 24500 Training loss 0.0013377828290686011 Validation loss 0.044297028332948685 Accuracy 0.8907500505447388\n",
      "Iteration 24510 Training loss 6.268262950470671e-05 Validation loss 0.04431356489658356 Accuracy 0.89000004529953\n",
      "Iteration 24520 Training loss 0.0013228481402620673 Validation loss 0.044333040714263916 Accuracy 0.8908750414848328\n",
      "Iteration 24530 Training loss 0.0038302349857985973 Validation loss 0.04435892775654793 Accuracy 0.8908750414848328\n",
      "Iteration 24540 Training loss 0.0026263417676091194 Validation loss 0.04424288496375084 Accuracy 0.890125036239624\n",
      "Iteration 24550 Training loss 0.0013517092447727919 Validation loss 0.044323619455099106 Accuracy 0.8896250128746033\n",
      "Iteration 24560 Training loss 0.0025392340030521154 Validation loss 0.04431602731347084 Accuracy 0.890125036239624\n",
      "Iteration 24570 Training loss 9.156700252788141e-05 Validation loss 0.04430316388607025 Accuracy 0.8906250596046448\n",
      "Iteration 24580 Training loss 0.0038574333302676678 Validation loss 0.044244538992643356 Accuracy 0.890375018119812\n",
      "Iteration 24590 Training loss 0.001356618944555521 Validation loss 0.044347990304231644 Accuracy 0.8908750414848328\n",
      "Iteration 24600 Training loss 0.0013248272007331252 Validation loss 0.0442662388086319 Accuracy 0.89000004529953\n",
      "Iteration 24610 Training loss 0.00011867748253280297 Validation loss 0.044321220368146896 Accuracy 0.890375018119812\n",
      "Iteration 24620 Training loss 0.0013495805906131864 Validation loss 0.04433803632855415 Accuracy 0.8895000219345093\n",
      "Iteration 24630 Training loss 7.475676102330908e-05 Validation loss 0.044361554086208344 Accuracy 0.8888750672340393\n",
      "Iteration 24640 Training loss 0.005073524545878172 Validation loss 0.04427865520119667 Accuracy 0.890250027179718\n",
      "Iteration 24650 Training loss 0.0025618416257202625 Validation loss 0.04425293952226639 Accuracy 0.890375018119812\n",
      "Iteration 24660 Training loss 5.542046346818097e-05 Validation loss 0.04425954818725586 Accuracy 0.889875054359436\n",
      "Iteration 24670 Training loss 0.0013653493952006102 Validation loss 0.04432417452335358 Accuracy 0.890125036239624\n",
      "Iteration 24680 Training loss 0.003818604163825512 Validation loss 0.04430648684501648 Accuracy 0.890250027179718\n",
      "Iteration 24690 Training loss 0.0013429868267849088 Validation loss 0.044267311692237854 Accuracy 0.889875054359436\n",
      "Iteration 24700 Training loss 0.003795513417571783 Validation loss 0.044255342334508896 Accuracy 0.8911250233650208\n",
      "Iteration 24710 Training loss 0.0026024580001831055 Validation loss 0.04422672092914581 Accuracy 0.8906250596046448\n",
      "Iteration 24720 Training loss 0.003817625343799591 Validation loss 0.04437891021370888 Accuracy 0.890125036239624\n",
      "Iteration 24730 Training loss 0.0013505973620340228 Validation loss 0.04429566115140915 Accuracy 0.8905000686645508\n",
      "Iteration 24740 Training loss 0.0025684924330562353 Validation loss 0.04425845667719841 Accuracy 0.890375018119812\n",
      "Iteration 24750 Training loss 0.001315897679887712 Validation loss 0.04424162581562996 Accuracy 0.890125036239624\n",
      "Iteration 24760 Training loss 0.0013410865794867277 Validation loss 0.04418705031275749 Accuracy 0.8907500505447388\n",
      "Iteration 24770 Training loss 7.580205419799313e-05 Validation loss 0.044287629425525665 Accuracy 0.8890000581741333\n",
      "Iteration 24780 Training loss 0.0001758219877956435 Validation loss 0.04420756921172142 Accuracy 0.890125036239624\n",
      "Iteration 24790 Training loss 0.0001271060318686068 Validation loss 0.044282373040914536 Accuracy 0.8910000324249268\n",
      "Iteration 24800 Training loss 0.0013161329552531242 Validation loss 0.04430170729756355 Accuracy 0.8907500505447388\n",
      "Iteration 24810 Training loss 0.0038308827206492424 Validation loss 0.04430408030748367 Accuracy 0.890375018119812\n",
      "Iteration 24820 Training loss 0.0038060187362134457 Validation loss 0.044227369129657745 Accuracy 0.8910000324249268\n",
      "Iteration 24830 Training loss 0.005074450746178627 Validation loss 0.04419899731874466 Accuracy 0.8908750414848328\n",
      "Iteration 24840 Training loss 0.0025547107215970755 Validation loss 0.04439263045787811 Accuracy 0.8908750414848328\n",
      "Iteration 24850 Training loss 0.0013272634241729975 Validation loss 0.04431784152984619 Accuracy 0.890125036239624\n",
      "Iteration 24860 Training loss 8.132631046464667e-05 Validation loss 0.044286563992500305 Accuracy 0.89000004529953\n",
      "Iteration 24870 Training loss 9.596876770956442e-05 Validation loss 0.04442192241549492 Accuracy 0.889750063419342\n",
      "Iteration 24880 Training loss 0.0013456624001264572 Validation loss 0.04422735050320625 Accuracy 0.889750063419342\n",
      "Iteration 24890 Training loss 0.005106490105390549 Validation loss 0.04419475793838501 Accuracy 0.89000004529953\n",
      "Iteration 24900 Training loss 0.0013469329569488764 Validation loss 0.04430490359663963 Accuracy 0.8905000686645508\n",
      "Iteration 24910 Training loss 5.5100812460295856e-05 Validation loss 0.04429211467504501 Accuracy 0.890375018119812\n",
      "Iteration 24920 Training loss 0.0025911324191838503 Validation loss 0.04437198117375374 Accuracy 0.8907500505447388\n",
      "Iteration 24930 Training loss 0.0038136213552206755 Validation loss 0.04427652060985565 Accuracy 0.889875054359436\n",
      "Iteration 24940 Training loss 0.0013183424016460776 Validation loss 0.04426604509353638 Accuracy 0.890375018119812\n",
      "Iteration 24950 Training loss 0.0025942777283489704 Validation loss 0.04463210329413414 Accuracy 0.890125036239624\n",
      "Iteration 24960 Training loss 0.001308379345573485 Validation loss 0.044229090213775635 Accuracy 0.8908750414848328\n",
      "Iteration 24970 Training loss 0.0012958619045093656 Validation loss 0.04446478560566902 Accuracy 0.8915000557899475\n",
      "Iteration 24980 Training loss 6.548139936057851e-05 Validation loss 0.04438060522079468 Accuracy 0.890375018119812\n",
      "Iteration 24990 Training loss 0.0025765355676412582 Validation loss 0.04437801614403725 Accuracy 0.8907500505447388\n",
      "Iteration 25000 Training loss 0.001304417965002358 Validation loss 0.0443318709731102 Accuracy 0.89000004529953\n",
      "Iteration 25010 Training loss 0.0013019803445786238 Validation loss 0.04434557631611824 Accuracy 0.8905000686645508\n",
      "Iteration 25020 Training loss 0.0013911821879446507 Validation loss 0.044250067323446274 Accuracy 0.890375018119812\n",
      "Iteration 25030 Training loss 0.00010114969336427748 Validation loss 0.044238850474357605 Accuracy 0.8905000686645508\n",
      "Iteration 25040 Training loss 9.982872870750725e-05 Validation loss 0.04429301619529724 Accuracy 0.890250027179718\n",
      "Iteration 25050 Training loss 0.0026228991337120533 Validation loss 0.04449694976210594 Accuracy 0.8891250491142273\n",
      "Iteration 25060 Training loss 0.00011459087545517832 Validation loss 0.044378139078617096 Accuracy 0.8892500400543213\n",
      "Iteration 25070 Training loss 0.001332578482106328 Validation loss 0.04432417452335358 Accuracy 0.8905000686645508\n",
      "Iteration 25080 Training loss 0.0013063332298770547 Validation loss 0.04430278018116951 Accuracy 0.8915000557899475\n",
      "Iteration 25090 Training loss 0.0013524956302717328 Validation loss 0.04431089758872986 Accuracy 0.89000004529953\n",
      "Iteration 25100 Training loss 0.0038509683217853308 Validation loss 0.04425671696662903 Accuracy 0.890375018119812\n",
      "Iteration 25110 Training loss 0.002569850068539381 Validation loss 0.04429014027118683 Accuracy 0.890125036239624\n",
      "Iteration 25120 Training loss 0.0026183591689914465 Validation loss 0.04428613558411598 Accuracy 0.890125036239624\n",
      "Iteration 25130 Training loss 0.0013436420122161508 Validation loss 0.04430842399597168 Accuracy 0.8907500505447388\n",
      "Iteration 25140 Training loss 0.00010227370512438938 Validation loss 0.04436071589589119 Accuracy 0.8912500143051147\n",
      "Iteration 25150 Training loss 0.005068225786089897 Validation loss 0.044293444603681564 Accuracy 0.8892500400543213\n",
      "Iteration 25160 Training loss 9.315751231042668e-05 Validation loss 0.044375307857990265 Accuracy 0.890375018119812\n",
      "Iteration 25170 Training loss 0.0013626472791656852 Validation loss 0.04427932947874069 Accuracy 0.890375018119812\n",
      "Iteration 25180 Training loss 0.0013094706228002906 Validation loss 0.044305019080638885 Accuracy 0.8905000686645508\n",
      "Iteration 25190 Training loss 0.001320863957516849 Validation loss 0.04429059848189354 Accuracy 0.890375018119812\n",
      "Iteration 25200 Training loss 0.003833826631307602 Validation loss 0.0443132221698761 Accuracy 0.8895000219345093\n",
      "Iteration 25210 Training loss 0.0026282649487257004 Validation loss 0.04437306150794029 Accuracy 0.89000004529953\n",
      "Iteration 25220 Training loss 0.0013541296357288957 Validation loss 0.04451761767268181 Accuracy 0.8895000219345093\n",
      "Iteration 25230 Training loss 5.760568819823675e-05 Validation loss 0.04434205964207649 Accuracy 0.8893750309944153\n",
      "Iteration 25240 Training loss 0.001347557408735156 Validation loss 0.04426851123571396 Accuracy 0.889875054359436\n",
      "Iteration 25250 Training loss 9.717523789731786e-05 Validation loss 0.04433947801589966 Accuracy 0.8905000686645508\n",
      "Iteration 25260 Training loss 0.0038153144996613264 Validation loss 0.04429274797439575 Accuracy 0.890375018119812\n",
      "Iteration 25270 Training loss 0.0013368007494136691 Validation loss 0.04429294914007187 Accuracy 0.890125036239624\n",
      "Iteration 25280 Training loss 6.887963536428288e-05 Validation loss 0.0443311482667923 Accuracy 0.8910000324249268\n",
      "Iteration 25290 Training loss 0.0025958861224353313 Validation loss 0.044302649796009064 Accuracy 0.89000004529953\n",
      "Iteration 25300 Training loss 0.002584125380963087 Validation loss 0.04423946514725685 Accuracy 0.8905000686645508\n",
      "Iteration 25310 Training loss 0.005095628555864096 Validation loss 0.04431900754570961 Accuracy 0.89000004529953\n",
      "Iteration 25320 Training loss 6.844272866146639e-05 Validation loss 0.04456166923046112 Accuracy 0.8890000581741333\n",
      "Iteration 25330 Training loss 0.00510342326015234 Validation loss 0.0443180650472641 Accuracy 0.8905000686645508\n",
      "Iteration 25340 Training loss 0.002577285747975111 Validation loss 0.04436097294092178 Accuracy 0.8911250233650208\n",
      "Iteration 25350 Training loss 8.931951742852107e-05 Validation loss 0.04433637857437134 Accuracy 0.8907500505447388\n",
      "Iteration 25360 Training loss 0.0013141874223947525 Validation loss 0.0442909337580204 Accuracy 0.890375018119812\n",
      "Iteration 25370 Training loss 0.001321646268479526 Validation loss 0.04425707086920738 Accuracy 0.8911250233650208\n",
      "Iteration 25380 Training loss 0.00012632734433282167 Validation loss 0.04434695094823837 Accuracy 0.890125036239624\n",
      "Iteration 25390 Training loss 0.006307706702500582 Validation loss 0.0442955419421196 Accuracy 0.890125036239624\n",
      "Iteration 25400 Training loss 0.0013087898259982467 Validation loss 0.044434402137994766 Accuracy 0.890375018119812\n",
      "Iteration 25410 Training loss 0.0038296272978186607 Validation loss 0.04428654536604881 Accuracy 0.890125036239624\n",
      "Iteration 25420 Training loss 7.613826164742932e-05 Validation loss 0.04433329403400421 Accuracy 0.8908750414848328\n",
      "Iteration 25430 Training loss 0.0025871547404676676 Validation loss 0.044323574751615524 Accuracy 0.8888750672340393\n",
      "Iteration 25440 Training loss 9.013341332320124e-05 Validation loss 0.04426681995391846 Accuracy 0.890250027179718\n",
      "Iteration 25450 Training loss 0.002621376421302557 Validation loss 0.04432322084903717 Accuracy 0.890375018119812\n",
      "Iteration 25460 Training loss 0.0013546027475968003 Validation loss 0.044298529624938965 Accuracy 0.890125036239624\n",
      "Iteration 25470 Training loss 0.0013080507051199675 Validation loss 0.04427977278828621 Accuracy 0.890250027179718\n",
      "Iteration 25480 Training loss 0.0026263936888426542 Validation loss 0.04426529258489609 Accuracy 0.8918750286102295\n",
      "Iteration 25490 Training loss 6.76084600854665e-05 Validation loss 0.04433118551969528 Accuracy 0.890375018119812\n",
      "Iteration 25500 Training loss 0.002579431515187025 Validation loss 0.044456567615270615 Accuracy 0.889875054359436\n",
      "Iteration 25510 Training loss 6.135171861387789e-05 Validation loss 0.04437650367617607 Accuracy 0.89000004529953\n",
      "Iteration 25520 Training loss 8.882307884050533e-05 Validation loss 0.04424377530813217 Accuracy 0.889750063419342\n",
      "Iteration 25530 Training loss 0.001334490953013301 Validation loss 0.04424327239394188 Accuracy 0.8905000686645508\n",
      "Iteration 25540 Training loss 0.0013242944842204452 Validation loss 0.04428433999419212 Accuracy 0.8911250233650208\n",
      "Iteration 25550 Training loss 6.640825449721888e-05 Validation loss 0.04430661350488663 Accuracy 0.890125036239624\n",
      "Iteration 25560 Training loss 6.313230551313609e-05 Validation loss 0.04423781856894493 Accuracy 0.8905000686645508\n",
      "Iteration 25570 Training loss 0.001402817084454 Validation loss 0.04420197010040283 Accuracy 0.8905000686645508\n",
      "Iteration 25580 Training loss 6.053647302906029e-05 Validation loss 0.04422594979405403 Accuracy 0.8910000324249268\n",
      "Iteration 25590 Training loss 0.003839824115857482 Validation loss 0.04427674412727356 Accuracy 0.8896250128746033\n",
      "Iteration 25600 Training loss 0.0013521082000806928 Validation loss 0.04445408657193184 Accuracy 0.890250027179718\n",
      "Iteration 25610 Training loss 0.0013070737477391958 Validation loss 0.04425833001732826 Accuracy 0.89000004529953\n",
      "Iteration 25620 Training loss 0.0013193066697567701 Validation loss 0.04424161836504936 Accuracy 0.8905000686645508\n",
      "Iteration 25630 Training loss 0.001348029589280486 Validation loss 0.044282156974077225 Accuracy 0.89000004529953\n",
      "Iteration 25640 Training loss 0.002586961956694722 Validation loss 0.04430364444851875 Accuracy 0.890125036239624\n",
      "Iteration 25650 Training loss 0.0038429771084338427 Validation loss 0.04437115788459778 Accuracy 0.8887500166893005\n",
      "Iteration 25660 Training loss 0.0025640639942139387 Validation loss 0.04432258754968643 Accuracy 0.8895000219345093\n",
      "Iteration 25670 Training loss 0.0013433457352221012 Validation loss 0.04430438578128815 Accuracy 0.8895000219345093\n",
      "Iteration 25680 Training loss 0.0038770288228988647 Validation loss 0.04430072382092476 Accuracy 0.889875054359436\n",
      "Iteration 25690 Training loss 0.0013063575606793165 Validation loss 0.04430677741765976 Accuracy 0.8905000686645508\n",
      "Iteration 25700 Training loss 0.0013590186135843396 Validation loss 0.044309377670288086 Accuracy 0.8910000324249268\n",
      "Iteration 25710 Training loss 0.0013266804162412882 Validation loss 0.044320955872535706 Accuracy 0.8907500505447388\n",
      "Iteration 25720 Training loss 8.443809201708063e-05 Validation loss 0.0444079153239727 Accuracy 0.8905000686645508\n",
      "Iteration 25730 Training loss 0.0013111334992572665 Validation loss 0.0443023256957531 Accuracy 0.890125036239624\n",
      "Iteration 25740 Training loss 0.001335549633949995 Validation loss 0.04436635598540306 Accuracy 0.889750063419342\n",
      "Iteration 25750 Training loss 9.544529893901199e-05 Validation loss 0.044293489307165146 Accuracy 0.890250027179718\n",
      "Iteration 25760 Training loss 0.0013903321232646704 Validation loss 0.04431959614157677 Accuracy 0.890250027179718\n",
      "Iteration 25770 Training loss 9.771929035196081e-05 Validation loss 0.04442707076668739 Accuracy 0.8910000324249268\n",
      "Iteration 25780 Training loss 7.795594865456223e-05 Validation loss 0.04432367533445358 Accuracy 0.890375018119812\n",
      "Iteration 25790 Training loss 5.4761170758865774e-05 Validation loss 0.04428809508681297 Accuracy 0.8907500505447388\n",
      "Iteration 25800 Training loss 0.00016383783076889813 Validation loss 0.044289540499448776 Accuracy 0.8912500143051147\n",
      "Iteration 25810 Training loss 0.0013306646142154932 Validation loss 0.044412486255168915 Accuracy 0.8908750414848328\n",
      "Iteration 25820 Training loss 6.207747355801985e-05 Validation loss 0.044356029480695724 Accuracy 0.890125036239624\n",
      "Iteration 25830 Training loss 0.0013153150212019682 Validation loss 0.04439299926161766 Accuracy 0.8913750648498535\n",
      "Iteration 25840 Training loss 8.73560638865456e-05 Validation loss 0.044301170855760574 Accuracy 0.8912500143051147\n",
      "Iteration 25850 Training loss 0.003834618953987956 Validation loss 0.044343993067741394 Accuracy 0.890375018119812\n",
      "Iteration 25860 Training loss 0.0013196782674640417 Validation loss 0.04428916797041893 Accuracy 0.8906250596046448\n",
      "Iteration 25870 Training loss 0.0013254587538540363 Validation loss 0.04438433423638344 Accuracy 0.89000004529953\n",
      "Iteration 25880 Training loss 7.686272147111595e-05 Validation loss 0.04442089796066284 Accuracy 0.8911250233650208\n",
      "Iteration 25890 Training loss 0.0026137817185372114 Validation loss 0.044402312487363815 Accuracy 0.8905000686645508\n",
      "Iteration 25900 Training loss 0.0013582193059846759 Validation loss 0.044464170932769775 Accuracy 0.890125036239624\n",
      "Iteration 25910 Training loss 0.0001240761048393324 Validation loss 0.04443816840648651 Accuracy 0.890250027179718\n",
      "Iteration 25920 Training loss 0.0025694130454212427 Validation loss 0.04439375177025795 Accuracy 0.890375018119812\n",
      "Iteration 25930 Training loss 7.275859388755634e-05 Validation loss 0.04435684159398079 Accuracy 0.8910000324249268\n",
      "Iteration 25940 Training loss 0.0013091500150039792 Validation loss 0.044394027441740036 Accuracy 0.8906250596046448\n",
      "Iteration 25950 Training loss 4.197180169285275e-05 Validation loss 0.04437045753002167 Accuracy 0.8907500505447388\n",
      "Iteration 25960 Training loss 0.0025905841030180454 Validation loss 0.04439254850149155 Accuracy 0.8906250596046448\n",
      "Iteration 25970 Training loss 0.0013110846048220992 Validation loss 0.044301241636276245 Accuracy 0.890125036239624\n",
      "Iteration 25980 Training loss 0.0025623689871281385 Validation loss 0.04430661350488663 Accuracy 0.8905000686645508\n",
      "Iteration 25990 Training loss 0.003840881399810314 Validation loss 0.04435007646679878 Accuracy 0.890375018119812\n",
      "Iteration 26000 Training loss 0.003814942203462124 Validation loss 0.044351812452077866 Accuracy 0.8896250128746033\n",
      "Iteration 26010 Training loss 0.0013042445061728358 Validation loss 0.04423060268163681 Accuracy 0.8906250596046448\n",
      "Iteration 26020 Training loss 0.003833797760307789 Validation loss 0.04440079629421234 Accuracy 0.8905000686645508\n",
      "Iteration 26030 Training loss 7.146839197957888e-05 Validation loss 0.04426133260130882 Accuracy 0.8911250233650208\n",
      "Iteration 26040 Training loss 0.0013394069392234087 Validation loss 0.044305469840765 Accuracy 0.8910000324249268\n",
      "Iteration 26050 Training loss 0.0013128709979355335 Validation loss 0.04431571811437607 Accuracy 0.890125036239624\n",
      "Iteration 26060 Training loss 0.0012827211758121848 Validation loss 0.044328849762678146 Accuracy 0.8908750414848328\n",
      "Iteration 26070 Training loss 0.0025803607422858477 Validation loss 0.044430263340473175 Accuracy 0.8906250596046448\n",
      "Iteration 26080 Training loss 0.0013192598707973957 Validation loss 0.044348351657390594 Accuracy 0.890125036239624\n",
      "Iteration 26090 Training loss 0.0013225020375102758 Validation loss 0.04436689615249634 Accuracy 0.8893750309944153\n",
      "Iteration 26100 Training loss 0.0012916241539642215 Validation loss 0.04441414028406143 Accuracy 0.8911250233650208\n",
      "Iteration 26110 Training loss 0.001340588554739952 Validation loss 0.04432098940014839 Accuracy 0.8910000324249268\n",
      "Iteration 26120 Training loss 8.371073636226356e-05 Validation loss 0.04434352368116379 Accuracy 0.8907500505447388\n",
      "Iteration 26130 Training loss 0.0038297681603580713 Validation loss 0.04428039491176605 Accuracy 0.8913750648498535\n",
      "Iteration 26140 Training loss 0.0013231232296675444 Validation loss 0.04437815770506859 Accuracy 0.890250027179718\n",
      "Iteration 26150 Training loss 0.001304894918575883 Validation loss 0.044307317584753036 Accuracy 0.890250027179718\n",
      "Iteration 26160 Training loss 0.003835329320281744 Validation loss 0.04428819566965103 Accuracy 0.890375018119812\n",
      "Iteration 26170 Training loss 0.0038343830965459347 Validation loss 0.04431308060884476 Accuracy 0.890125036239624\n",
      "Iteration 26180 Training loss 0.002588722389191389 Validation loss 0.044285327196121216 Accuracy 0.8905000686645508\n",
      "Iteration 26190 Training loss 6.800126720918342e-05 Validation loss 0.04429157078266144 Accuracy 0.889875054359436\n",
      "Iteration 26200 Training loss 0.001308112870901823 Validation loss 0.044330183416604996 Accuracy 0.89000004529953\n",
      "Iteration 26210 Training loss 0.0012980606406927109 Validation loss 0.044295839965343475 Accuracy 0.889875054359436\n",
      "Iteration 26220 Training loss 0.0013037564931437373 Validation loss 0.04427728056907654 Accuracy 0.889750063419342\n",
      "Iteration 26230 Training loss 0.002562623005360365 Validation loss 0.04426410421729088 Accuracy 0.8906250596046448\n",
      "Iteration 26240 Training loss 0.0025658344384282827 Validation loss 0.04425806552171707 Accuracy 0.890250027179718\n",
      "Iteration 26250 Training loss 0.0013159762602299452 Validation loss 0.044301483780145645 Accuracy 0.8908750414848328\n",
      "Iteration 26260 Training loss 0.00381819112226367 Validation loss 0.04428602382540703 Accuracy 0.889875054359436\n",
      "Iteration 26270 Training loss 0.001300800358876586 Validation loss 0.04433836042881012 Accuracy 0.890250027179718\n",
      "Iteration 26280 Training loss 0.0013067851541563869 Validation loss 0.044212616980075836 Accuracy 0.89000004529953\n",
      "Iteration 26290 Training loss 0.001320359529927373 Validation loss 0.04425264894962311 Accuracy 0.8907500505447388\n",
      "Iteration 26300 Training loss 0.0013277760008350015 Validation loss 0.04430217295885086 Accuracy 0.890375018119812\n",
      "Iteration 26310 Training loss 0.0025702244602143764 Validation loss 0.04427023604512215 Accuracy 0.890250027179718\n",
      "Iteration 26320 Training loss 0.0013123438693583012 Validation loss 0.044263046234846115 Accuracy 0.8896250128746033\n",
      "Iteration 26330 Training loss 6.973294512135908e-05 Validation loss 0.044367387890815735 Accuracy 0.890125036239624\n",
      "Iteration 26340 Training loss 0.004854058381170034 Validation loss 0.04431065544486046 Accuracy 0.8910000324249268\n",
      "Iteration 26350 Training loss 0.0026109234895557165 Validation loss 0.04437835142016411 Accuracy 0.890375018119812\n",
      "Iteration 26360 Training loss 0.0025502466596663 Validation loss 0.04427182674407959 Accuracy 0.8906250596046448\n",
      "Iteration 26370 Training loss 0.002570187207311392 Validation loss 0.04429123178124428 Accuracy 0.890375018119812\n",
      "Iteration 26380 Training loss 5.5414566304534674e-05 Validation loss 0.04426748305559158 Accuracy 0.8908750414848328\n",
      "Iteration 26390 Training loss 0.005103652831166983 Validation loss 0.04428209736943245 Accuracy 0.8908750414848328\n",
      "Iteration 26400 Training loss 0.001028959290124476 Validation loss 0.044324036687612534 Accuracy 0.89000004529953\n",
      "Iteration 26410 Training loss 5.070772749604657e-05 Validation loss 0.04430496320128441 Accuracy 0.8907500505447388\n",
      "Iteration 26420 Training loss 8.41225846670568e-05 Validation loss 0.04441307857632637 Accuracy 0.8905000686645508\n",
      "Iteration 26430 Training loss 0.0038346080109477043 Validation loss 0.04439801350235939 Accuracy 0.8905000686645508\n",
      "Iteration 26440 Training loss 6.386155291693285e-05 Validation loss 0.04427032172679901 Accuracy 0.889875054359436\n",
      "Iteration 26450 Training loss 8.524475560989231e-05 Validation loss 0.04433969408273697 Accuracy 0.8910000324249268\n",
      "Iteration 26460 Training loss 7.99544868641533e-05 Validation loss 0.044279925525188446 Accuracy 0.890125036239624\n",
      "Iteration 26470 Training loss 0.0013045904925093055 Validation loss 0.04434482008218765 Accuracy 0.89000004529953\n",
      "Iteration 26480 Training loss 0.0013108353596180677 Validation loss 0.044380828738212585 Accuracy 0.889875054359436\n",
      "Iteration 26490 Training loss 0.002559036947786808 Validation loss 0.04427596181631088 Accuracy 0.8907500505447388\n",
      "Iteration 26500 Training loss 7.012826972641051e-05 Validation loss 0.04438535496592522 Accuracy 0.8906250596046448\n",
      "Iteration 26510 Training loss 0.0013380196178331971 Validation loss 0.044381216168403625 Accuracy 0.8905000686645508\n",
      "Iteration 26520 Training loss 8.789826097199693e-05 Validation loss 0.04435446858406067 Accuracy 0.8915000557899475\n",
      "Iteration 26530 Training loss 0.0013527614064514637 Validation loss 0.044343359768390656 Accuracy 0.890125036239624\n",
      "Iteration 26540 Training loss 0.002571693854406476 Validation loss 0.04434764385223389 Accuracy 0.8905000686645508\n",
      "Iteration 26550 Training loss 0.0012959694722667336 Validation loss 0.0443488173186779 Accuracy 0.8905000686645508\n",
      "Iteration 26560 Training loss 9.384845179738477e-05 Validation loss 0.04440517723560333 Accuracy 0.8907500505447388\n",
      "Iteration 26570 Training loss 0.00010070604184875265 Validation loss 0.04447319358587265 Accuracy 0.890250027179718\n",
      "Iteration 26580 Training loss 0.001323128817602992 Validation loss 0.044439200311899185 Accuracy 0.890250027179718\n",
      "Iteration 26590 Training loss 0.0038084920961409807 Validation loss 0.044346265494823456 Accuracy 0.8913750648498535\n",
      "Iteration 26600 Training loss 9.347216837340966e-05 Validation loss 0.04479885846376419 Accuracy 0.8893750309944153\n",
      "Iteration 26610 Training loss 0.0013427026569843292 Validation loss 0.04439738020300865 Accuracy 0.890250027179718\n",
      "Iteration 26620 Training loss 0.002591218799352646 Validation loss 0.044354021549224854 Accuracy 0.890375018119812\n",
      "Iteration 26630 Training loss 6.564650539075956e-05 Validation loss 0.04440300539135933 Accuracy 0.889750063419342\n",
      "Iteration 26640 Training loss 0.0038588200695812702 Validation loss 0.044394999742507935 Accuracy 0.8896250128746033\n",
      "Iteration 26650 Training loss 0.0013160983799025416 Validation loss 0.04441136121749878 Accuracy 0.8905000686645508\n",
      "Iteration 26660 Training loss 0.0025918239261955023 Validation loss 0.04437112808227539 Accuracy 0.8908750414848328\n",
      "Iteration 26670 Training loss 0.0013090100837871432 Validation loss 0.04442580044269562 Accuracy 0.8905000686645508\n",
      "Iteration 26680 Training loss 0.0013227773597463965 Validation loss 0.04434947296977043 Accuracy 0.8910000324249268\n",
      "Iteration 26690 Training loss 6.983256753301248e-05 Validation loss 0.044365473091602325 Accuracy 0.8905000686645508\n",
      "Iteration 26700 Training loss 0.005105580203235149 Validation loss 0.04452022910118103 Accuracy 0.890250027179718\n",
      "Iteration 26710 Training loss 0.0012612546561285853 Validation loss 0.04441389441490173 Accuracy 0.8912500143051147\n",
      "Iteration 26720 Training loss 0.0026684280019253492 Validation loss 0.044328004121780396 Accuracy 0.8907500505447388\n",
      "Iteration 26730 Training loss 9.652061999076977e-05 Validation loss 0.04436545446515083 Accuracy 0.8916250467300415\n",
      "Iteration 26740 Training loss 0.00133879657369107 Validation loss 0.0442839078605175 Accuracy 0.8912500143051147\n",
      "Iteration 26750 Training loss 0.0038257502019405365 Validation loss 0.04426494985818863 Accuracy 0.8915000557899475\n",
      "Iteration 26760 Training loss 0.0025696929078549147 Validation loss 0.044364336878061295 Accuracy 0.889750063419342\n",
      "Iteration 26770 Training loss 0.00015956605784595013 Validation loss 0.04442155361175537 Accuracy 0.8895000219345093\n",
      "Iteration 26780 Training loss 6.629660492762923e-05 Validation loss 0.044418878853321075 Accuracy 0.8907500505447388\n",
      "Iteration 26790 Training loss 0.0013851398834958673 Validation loss 0.04437346011400223 Accuracy 0.8905000686645508\n",
      "Iteration 26800 Training loss 5.59155669179745e-05 Validation loss 0.044355619698762894 Accuracy 0.8907500505447388\n",
      "Iteration 26810 Training loss 0.0013480954803526402 Validation loss 0.044332150369882584 Accuracy 0.8913750648498535\n",
      "Iteration 26820 Training loss 0.0013224872527644038 Validation loss 0.044346727430820465 Accuracy 0.8906250596046448\n",
      "Iteration 26830 Training loss 0.0013296917313709855 Validation loss 0.04434170573949814 Accuracy 0.8905000686645508\n",
      "Iteration 26840 Training loss 0.0025525949895381927 Validation loss 0.04437049850821495 Accuracy 0.890250027179718\n",
      "Iteration 26850 Training loss 0.0013391568791121244 Validation loss 0.044393666088581085 Accuracy 0.889875054359436\n",
      "Iteration 26860 Training loss 0.0025586539413779974 Validation loss 0.04440824314951897 Accuracy 0.890125036239624\n",
      "Iteration 26870 Training loss 0.0013288180343806744 Validation loss 0.044371288269758224 Accuracy 0.890375018119812\n",
      "Iteration 26880 Training loss 0.0013038782635703683 Validation loss 0.04426760971546173 Accuracy 0.8911250233650208\n",
      "Iteration 26890 Training loss 6.811366620240733e-05 Validation loss 0.04427948221564293 Accuracy 0.8912500143051147\n",
      "Iteration 26900 Training loss 0.0038193967193365097 Validation loss 0.04438694193959236 Accuracy 0.890250027179718\n",
      "Iteration 26910 Training loss 7.407624798361212e-05 Validation loss 0.04436291754245758 Accuracy 0.890125036239624\n",
      "Iteration 26920 Training loss 0.0025780999567359686 Validation loss 0.04434271529316902 Accuracy 0.8896250128746033\n",
      "Iteration 26930 Training loss 0.0038052310701459646 Validation loss 0.044356171041727066 Accuracy 0.8906250596046448\n",
      "Iteration 26940 Training loss 0.005112479440867901 Validation loss 0.04435895010828972 Accuracy 0.8895000219345093\n",
      "Iteration 26950 Training loss 0.0038176095113158226 Validation loss 0.044380027800798416 Accuracy 0.890125036239624\n",
      "Iteration 26960 Training loss 0.0038079950027167797 Validation loss 0.04436259716749191 Accuracy 0.89000004529953\n",
      "Iteration 26970 Training loss 5.3476731409318745e-05 Validation loss 0.04434426873922348 Accuracy 0.8895000219345093\n",
      "Iteration 26980 Training loss 0.0026054924819618464 Validation loss 0.04437035694718361 Accuracy 0.890375018119812\n",
      "Iteration 26990 Training loss 0.0025632702745497227 Validation loss 0.04436938837170601 Accuracy 0.8896250128746033\n",
      "Iteration 27000 Training loss 0.0025718598626554012 Validation loss 0.04431375861167908 Accuracy 0.8915000557899475\n",
      "Iteration 27010 Training loss 0.0013223303249105811 Validation loss 0.044390786439180374 Accuracy 0.8907500505447388\n",
      "Iteration 27020 Training loss 0.0038112567272037268 Validation loss 0.04440523684024811 Accuracy 0.890125036239624\n",
      "Iteration 27030 Training loss 0.002573783975094557 Validation loss 0.04438351094722748 Accuracy 0.8905000686645508\n",
      "Iteration 27040 Training loss 0.002569983247667551 Validation loss 0.04432429000735283 Accuracy 0.890125036239624\n",
      "Iteration 27050 Training loss 5.147119736648165e-05 Validation loss 0.04443554952740669 Accuracy 0.8892500400543213\n",
      "Iteration 27060 Training loss 7.312245725188404e-05 Validation loss 0.04444713145494461 Accuracy 0.8895000219345093\n",
      "Iteration 27070 Training loss 0.0013152309693396091 Validation loss 0.044426824897527695 Accuracy 0.8891250491142273\n",
      "Iteration 27080 Training loss 0.0013184911804273725 Validation loss 0.044410280883312225 Accuracy 0.889750063419342\n",
      "Iteration 27090 Training loss 0.00259459693916142 Validation loss 0.044326234608888626 Accuracy 0.890125036239624\n",
      "Iteration 27100 Training loss 0.00131232850253582 Validation loss 0.04433206096291542 Accuracy 0.8906250596046448\n",
      "Iteration 27110 Training loss 0.0013025987427681684 Validation loss 0.044346705079078674 Accuracy 0.8896250128746033\n",
      "Iteration 27120 Training loss 0.00382157857529819 Validation loss 0.04436786100268364 Accuracy 0.8905000686645508\n",
      "Iteration 27130 Training loss 0.001313124317675829 Validation loss 0.04432487115263939 Accuracy 0.8907500505447388\n",
      "Iteration 27140 Training loss 0.002557650674134493 Validation loss 0.044412050396203995 Accuracy 0.89000004529953\n",
      "Iteration 27150 Training loss 0.0012861628783866763 Validation loss 0.04435710236430168 Accuracy 0.889750063419342\n",
      "Iteration 27160 Training loss 0.0025462277699261904 Validation loss 0.04430801421403885 Accuracy 0.8905000686645508\n",
      "Iteration 27170 Training loss 0.0014392761513590813 Validation loss 0.044447384774684906 Accuracy 0.890375018119812\n",
      "Iteration 27180 Training loss 6.149231921881437e-05 Validation loss 0.044418610632419586 Accuracy 0.8895000219345093\n",
      "Iteration 27190 Training loss 6.012681114953011e-05 Validation loss 0.04440145194530487 Accuracy 0.8895000219345093\n",
      "Iteration 27200 Training loss 0.001311905449256301 Validation loss 0.04447382315993309 Accuracy 0.8905000686645508\n",
      "Iteration 27210 Training loss 8.244692435255274e-05 Validation loss 0.044430118054151535 Accuracy 0.890375018119812\n",
      "Iteration 27220 Training loss 0.002544145565479994 Validation loss 0.04441508650779724 Accuracy 0.889750063419342\n",
      "Iteration 27230 Training loss 6.334946374408901e-05 Validation loss 0.04436154291033745 Accuracy 0.8907500505447388\n",
      "Iteration 27240 Training loss 0.002548946999013424 Validation loss 0.044345032423734665 Accuracy 0.8910000324249268\n",
      "Iteration 27250 Training loss 0.0013196708168834448 Validation loss 0.044413838535547256 Accuracy 0.889750063419342\n",
      "Iteration 27260 Training loss 0.003865183098241687 Validation loss 0.044661346822977066 Accuracy 0.89000004529953\n",
      "Iteration 27270 Training loss 5.8154513681074604e-05 Validation loss 0.044421419501304626 Accuracy 0.889875054359436\n",
      "Iteration 27280 Training loss 0.003795960219576955 Validation loss 0.04444141313433647 Accuracy 0.889875054359436\n",
      "Iteration 27290 Training loss 0.001303561613894999 Validation loss 0.044467419385910034 Accuracy 0.8895000219345093\n",
      "Iteration 27300 Training loss 0.0012936604907736182 Validation loss 0.044382575899362564 Accuracy 0.8905000686645508\n",
      "Iteration 27310 Training loss 0.0025758191477507353 Validation loss 0.04443145915865898 Accuracy 0.89000004529953\n",
      "Iteration 27320 Training loss 0.0013189486926421523 Validation loss 0.04442029446363449 Accuracy 0.890375018119812\n",
      "Iteration 27330 Training loss 6.707428838126361e-05 Validation loss 0.044492799788713455 Accuracy 0.8912500143051147\n",
      "Iteration 27340 Training loss 0.007578286342322826 Validation loss 0.044415052980184555 Accuracy 0.8912500143051147\n",
      "Iteration 27350 Training loss 0.001329704187810421 Validation loss 0.04443269595503807 Accuracy 0.8905000686645508\n",
      "Iteration 27360 Training loss 0.002562698908150196 Validation loss 0.044413011521101 Accuracy 0.8905000686645508\n",
      "Iteration 27370 Training loss 4.7344739869004115e-05 Validation loss 0.044444888830184937 Accuracy 0.8907500505447388\n",
      "Iteration 27380 Training loss 4.378031735541299e-05 Validation loss 0.04448825493454933 Accuracy 0.8908750414848328\n",
      "Iteration 27390 Training loss 0.0013276244280859828 Validation loss 0.04456830769777298 Accuracy 0.8925000429153442\n",
      "Iteration 27400 Training loss 0.00507140439003706 Validation loss 0.04464743286371231 Accuracy 0.8907500505447388\n",
      "Iteration 27410 Training loss 0.002594196470454335 Validation loss 0.04460487142205238 Accuracy 0.890125036239624\n",
      "Iteration 27420 Training loss 0.0013219453394412994 Validation loss 0.04461776837706566 Accuracy 0.890375018119812\n",
      "Iteration 27430 Training loss 0.0013308218913152814 Validation loss 0.044516682624816895 Accuracy 0.8913750648498535\n",
      "Iteration 27440 Training loss 5.7139306591125205e-05 Validation loss 0.04450244456529617 Accuracy 0.8913750648498535\n",
      "Iteration 27450 Training loss 0.003771665506064892 Validation loss 0.04450024664402008 Accuracy 0.890250027179718\n",
      "Iteration 27460 Training loss 0.0025665389839559793 Validation loss 0.04453679174184799 Accuracy 0.890125036239624\n",
      "Iteration 27470 Training loss 0.0038026724942028522 Validation loss 0.04460934177041054 Accuracy 0.890375018119812\n",
      "Iteration 27480 Training loss 0.0013268634211272001 Validation loss 0.044516053050756454 Accuracy 0.890125036239624\n",
      "Iteration 27490 Training loss 0.0013105050893500447 Validation loss 0.044431328773498535 Accuracy 0.8908750414848328\n",
      "Iteration 27500 Training loss 6.555106665473431e-05 Validation loss 0.0444665402173996 Accuracy 0.890250027179718\n",
      "Iteration 27510 Training loss 0.0013159459922462702 Validation loss 0.04451732710003853 Accuracy 0.8906250596046448\n",
      "Iteration 27520 Training loss 0.001309241633862257 Validation loss 0.04446303844451904 Accuracy 0.890375018119812\n",
      "Iteration 27530 Training loss 4.5150649384595454e-05 Validation loss 0.04445653781294823 Accuracy 0.89000004529953\n",
      "Iteration 27540 Training loss 0.0013143743854016066 Validation loss 0.04448150843381882 Accuracy 0.890125036239624\n",
      "Iteration 27550 Training loss 0.001309109851717949 Validation loss 0.04450678080320358 Accuracy 0.8890000581741333\n",
      "Iteration 27560 Training loss 0.0013393987901508808 Validation loss 0.04451749473810196 Accuracy 0.890250027179718\n",
      "Iteration 27570 Training loss 0.0013032275019213557 Validation loss 0.0444500707089901 Accuracy 0.8905000686645508\n",
      "Iteration 27580 Training loss 0.00012179428449599072 Validation loss 0.04448379948735237 Accuracy 0.890375018119812\n",
      "Iteration 27590 Training loss 0.003808314213529229 Validation loss 0.04451199248433113 Accuracy 0.890375018119812\n",
      "Iteration 27600 Training loss 0.005061290226876736 Validation loss 0.04449262097477913 Accuracy 0.890125036239624\n",
      "Iteration 27610 Training loss 7.042883953545243e-05 Validation loss 0.0444687083363533 Accuracy 0.890250027179718\n",
      "Iteration 27620 Training loss 4.162353798164986e-05 Validation loss 0.044533707201480865 Accuracy 0.89000004529953\n",
      "Iteration 27630 Training loss 0.0013227665331214666 Validation loss 0.04450347274541855 Accuracy 0.89000004529953\n",
      "Iteration 27640 Training loss 0.0026183580048382282 Validation loss 0.04446326941251755 Accuracy 0.890125036239624\n",
      "Iteration 27650 Training loss 0.0013447145465761423 Validation loss 0.04448704794049263 Accuracy 0.8893750309944153\n",
      "Iteration 27660 Training loss 0.0013140158262103796 Validation loss 0.04445431008934975 Accuracy 0.8896250128746033\n",
      "Iteration 27670 Training loss 0.001326972502283752 Validation loss 0.044381167739629745 Accuracy 0.8906250596046448\n",
      "Iteration 27680 Training loss 0.0013439736794680357 Validation loss 0.04443851858377457 Accuracy 0.89000004529953\n",
      "Iteration 27690 Training loss 0.002563527785241604 Validation loss 0.0445166639983654 Accuracy 0.889875054359436\n",
      "Iteration 27700 Training loss 0.0016657693777233362 Validation loss 0.044523753225803375 Accuracy 0.890375018119812\n",
      "Iteration 27710 Training loss 0.0013512191362679005 Validation loss 0.04442134127020836 Accuracy 0.8905000686645508\n",
      "Iteration 27720 Training loss 0.0013411084655672312 Validation loss 0.044390495866537094 Accuracy 0.889750063419342\n",
      "Iteration 27730 Training loss 0.0025695657823234797 Validation loss 0.04446805268526077 Accuracy 0.890250027179718\n",
      "Iteration 27740 Training loss 0.002594590652734041 Validation loss 0.044431641697883606 Accuracy 0.890250027179718\n",
      "Iteration 27750 Training loss 8.936237281886861e-05 Validation loss 0.04450942948460579 Accuracy 0.890125036239624\n",
      "Iteration 27760 Training loss 0.0025572010781615973 Validation loss 0.04459991678595543 Accuracy 0.890125036239624\n",
      "Iteration 27770 Training loss 0.002556243445724249 Validation loss 0.0445421077311039 Accuracy 0.8895000219345093\n",
      "Iteration 27780 Training loss 0.0025794117245823145 Validation loss 0.044501207768917084 Accuracy 0.889875054359436\n",
      "Iteration 27790 Training loss 0.003827072912827134 Validation loss 0.04451177269220352 Accuracy 0.8906250596046448\n",
      "Iteration 27800 Training loss 0.0025785916950553656 Validation loss 0.044514209032058716 Accuracy 0.8906250596046448\n",
      "Iteration 27810 Training loss 6.71117304591462e-05 Validation loss 0.044547878205776215 Accuracy 0.8896250128746033\n",
      "Iteration 27820 Training loss 0.0013257437385618687 Validation loss 0.04452187567949295 Accuracy 0.89000004529953\n",
      "Iteration 27830 Training loss 0.0037896083667874336 Validation loss 0.04457775130867958 Accuracy 0.889750063419342\n",
      "Iteration 27840 Training loss 0.0014124395092949271 Validation loss 0.045375216752290726 Accuracy 0.8907500505447388\n",
      "Iteration 27850 Training loss 0.0026097269728779793 Validation loss 0.044390253722667694 Accuracy 0.8912500143051147\n",
      "Iteration 27860 Training loss 8.719903416931629e-05 Validation loss 0.0445476695895195 Accuracy 0.890375018119812\n",
      "Iteration 27870 Training loss 7.225633453344926e-05 Validation loss 0.04447828605771065 Accuracy 0.890375018119812\n",
      "Iteration 27880 Training loss 8.575798710808158e-05 Validation loss 0.044487953186035156 Accuracy 0.8907500505447388\n",
      "Iteration 27890 Training loss 0.0013092650333419442 Validation loss 0.04449555277824402 Accuracy 0.890125036239624\n",
      "Iteration 27900 Training loss 0.0013125516707077622 Validation loss 0.04449734464287758 Accuracy 0.890250027179718\n",
      "Iteration 27910 Training loss 0.0013177166692912579 Validation loss 0.044529326260089874 Accuracy 0.889750063419342\n",
      "Iteration 27920 Training loss 0.005081821698695421 Validation loss 0.04458766430616379 Accuracy 0.889875054359436\n",
      "Iteration 27930 Training loss 0.0013220114633440971 Validation loss 0.04456426948308945 Accuracy 0.890125036239624\n",
      "Iteration 27940 Training loss 6.0791073337895796e-05 Validation loss 0.0444783940911293 Accuracy 0.8907500505447388\n",
      "Iteration 27950 Training loss 0.0013120669173076749 Validation loss 0.04448430985212326 Accuracy 0.8908750414848328\n",
      "Iteration 27960 Training loss 0.0013145875418558717 Validation loss 0.044595371931791306 Accuracy 0.89000004529953\n",
      "Iteration 27970 Training loss 5.943380165263079e-05 Validation loss 0.044596116989851 Accuracy 0.8896250128746033\n",
      "Iteration 27980 Training loss 0.002543374430388212 Validation loss 0.04454413801431656 Accuracy 0.8895000219345093\n",
      "Iteration 27990 Training loss 0.002565814182162285 Validation loss 0.04447305202484131 Accuracy 0.8907500505447388\n",
      "Iteration 28000 Training loss 0.0013142731040716171 Validation loss 0.04451555013656616 Accuracy 0.8893750309944153\n",
      "Iteration 28010 Training loss 8.128740591928363e-05 Validation loss 0.044477999210357666 Accuracy 0.8908750414848328\n",
      "Iteration 28020 Training loss 9.162837523035705e-05 Validation loss 0.044520098716020584 Accuracy 0.890125036239624\n",
      "Iteration 28030 Training loss 0.0013520808424800634 Validation loss 0.044606998562812805 Accuracy 0.8891250491142273\n",
      "Iteration 28040 Training loss 0.0051234266720712185 Validation loss 0.04453909769654274 Accuracy 0.8891250491142273\n",
      "Iteration 28050 Training loss 0.0013121635420247912 Validation loss 0.0445006899535656 Accuracy 0.890375018119812\n",
      "Iteration 28060 Training loss 0.0013195323990657926 Validation loss 0.04452776536345482 Accuracy 0.8891250491142273\n",
      "Iteration 28070 Training loss 0.0025337960105389357 Validation loss 0.04454568028450012 Accuracy 0.8891250491142273\n",
      "Iteration 28080 Training loss 0.001339371083304286 Validation loss 0.044445235282182693 Accuracy 0.889750063419342\n",
      "Iteration 28090 Training loss 0.0025794452521950006 Validation loss 0.04444032534956932 Accuracy 0.889875054359436\n",
      "Iteration 28100 Training loss 0.0013012124691158533 Validation loss 0.04444967582821846 Accuracy 0.890250027179718\n",
      "Iteration 28110 Training loss 0.0038181303534656763 Validation loss 0.04447832703590393 Accuracy 0.8895000219345093\n",
      "Iteration 28120 Training loss 0.0012912859674543142 Validation loss 0.04451214149594307 Accuracy 0.89000004529953\n",
      "Iteration 28130 Training loss 5.8630619605537504e-05 Validation loss 0.044432383030653 Accuracy 0.890250027179718\n",
      "Iteration 28140 Training loss 0.003811138216406107 Validation loss 0.04442182555794716 Accuracy 0.8906250596046448\n",
      "Iteration 28150 Training loss 0.00384689774364233 Validation loss 0.04452097415924072 Accuracy 0.8891250491142273\n",
      "Iteration 28160 Training loss 0.0038107389118522406 Validation loss 0.04447966068983078 Accuracy 0.8908750414848328\n",
      "Iteration 28170 Training loss 0.003789562499150634 Validation loss 0.044499125331640244 Accuracy 0.889875054359436\n",
      "Iteration 28180 Training loss 0.0013080640928819776 Validation loss 0.04451126977801323 Accuracy 0.890375018119812\n",
      "Iteration 28190 Training loss 4.167023143963888e-05 Validation loss 0.0445287860929966 Accuracy 0.8892500400543213\n",
      "Iteration 28200 Training loss 6.764347199350595e-05 Validation loss 0.04458064213395119 Accuracy 0.8885000348091125\n",
      "Iteration 28210 Training loss 0.0025948137044906616 Validation loss 0.044540513306856155 Accuracy 0.890250027179718\n",
      "Iteration 28220 Training loss 0.00254229037091136 Validation loss 0.04449816048145294 Accuracy 0.8893750309944153\n",
      "Iteration 28230 Training loss 6.90938177285716e-05 Validation loss 0.04450291767716408 Accuracy 0.890125036239624\n",
      "Iteration 28240 Training loss 0.00259491428732872 Validation loss 0.04449005424976349 Accuracy 0.890375018119812\n",
      "Iteration 28250 Training loss 7.241699495352805e-05 Validation loss 0.044537581503391266 Accuracy 0.889875054359436\n",
      "Iteration 28260 Training loss 8.015710773179308e-05 Validation loss 0.04459099844098091 Accuracy 0.8895000219345093\n",
      "Iteration 28270 Training loss 0.006305414251983166 Validation loss 0.04456262290477753 Accuracy 0.8886250257492065\n",
      "Iteration 28280 Training loss 0.003828429849818349 Validation loss 0.0445341058075428 Accuracy 0.8895000219345093\n",
      "Iteration 28290 Training loss 0.0025482885539531708 Validation loss 0.044460274279117584 Accuracy 0.8905000686645508\n",
      "Iteration 28300 Training loss 0.001327777048572898 Validation loss 0.04449453204870224 Accuracy 0.8891250491142273\n",
      "Iteration 28310 Training loss 0.0038002245128154755 Validation loss 0.04456119239330292 Accuracy 0.8893750309944153\n",
      "Iteration 28320 Training loss 0.002555088372901082 Validation loss 0.044473711401224136 Accuracy 0.8896250128746033\n",
      "Iteration 28330 Training loss 0.0012974534183740616 Validation loss 0.04451863467693329 Accuracy 0.8886250257492065\n",
      "Iteration 28340 Training loss 0.0025492997374385595 Validation loss 0.044607941061258316 Accuracy 0.8896250128746033\n",
      "Iteration 28350 Training loss 4.622691267286427e-05 Validation loss 0.04453637823462486 Accuracy 0.8905000686645508\n",
      "Iteration 28360 Training loss 5.7367684348719195e-05 Validation loss 0.04452621564269066 Accuracy 0.889875054359436\n",
      "Iteration 28370 Training loss 0.0012926970375701785 Validation loss 0.04457279294729233 Accuracy 0.8896250128746033\n",
      "Iteration 28380 Training loss 0.0013295614626258612 Validation loss 0.04457923769950867 Accuracy 0.8892500400543213\n",
      "Iteration 28390 Training loss 0.0012934741098433733 Validation loss 0.04459038004279137 Accuracy 0.889750063419342\n",
      "Iteration 28400 Training loss 0.0013701048446819186 Validation loss 0.04460899159312248 Accuracy 0.8885000348091125\n",
      "Iteration 28410 Training loss 0.0013192904880270362 Validation loss 0.04453682154417038 Accuracy 0.8892500400543213\n",
      "Iteration 28420 Training loss 0.0038323691114783287 Validation loss 0.04450846090912819 Accuracy 0.890375018119812\n",
      "Iteration 28430 Training loss 5.917575253988616e-05 Validation loss 0.04468568414449692 Accuracy 0.8893750309944153\n",
      "Iteration 28440 Training loss 0.00382092222571373 Validation loss 0.04455156996846199 Accuracy 0.8896250128746033\n",
      "Iteration 28450 Training loss 0.0025841055903583765 Validation loss 0.04456780478358269 Accuracy 0.8896250128746033\n",
      "Iteration 28460 Training loss 5.548109038500115e-05 Validation loss 0.044582296162843704 Accuracy 0.8895000219345093\n",
      "Iteration 28470 Training loss 0.001301669399254024 Validation loss 0.04450349137187004 Accuracy 0.889750063419342\n",
      "Iteration 28480 Training loss 0.002538307337090373 Validation loss 0.04457114636898041 Accuracy 0.8888750672340393\n",
      "Iteration 28490 Training loss 0.0025778301060199738 Validation loss 0.04466012865304947 Accuracy 0.8890000581741333\n",
      "Iteration 28500 Training loss 0.001297693233937025 Validation loss 0.04460959509015083 Accuracy 0.8892500400543213\n",
      "Iteration 28510 Training loss 0.0013063198421150446 Validation loss 0.04455716907978058 Accuracy 0.8887500166893005\n",
      "Iteration 28520 Training loss 0.0013013164279982448 Validation loss 0.04463543742895126 Accuracy 0.8896250128746033\n",
      "Iteration 28530 Training loss 0.0013004748616367579 Validation loss 0.044577281922101974 Accuracy 0.8892500400543213\n",
      "Iteration 28540 Training loss 0.001315859379246831 Validation loss 0.044694673269987106 Accuracy 0.8890000581741333\n",
      "Iteration 28550 Training loss 0.0012921078596264124 Validation loss 0.04452493414282799 Accuracy 0.8892500400543213\n",
      "Iteration 28560 Training loss 0.0012972360709682107 Validation loss 0.04451873153448105 Accuracy 0.8892500400543213\n",
      "Iteration 28570 Training loss 0.0012968454975634813 Validation loss 0.044508788734674454 Accuracy 0.8890000581741333\n",
      "Iteration 28580 Training loss 6.910600495757535e-05 Validation loss 0.04447707533836365 Accuracy 0.890250027179718\n",
      "Iteration 28590 Training loss 0.002561308676376939 Validation loss 0.04452984407544136 Accuracy 0.8895000219345093\n",
      "Iteration 28600 Training loss 0.0013127569109201431 Validation loss 0.04457376152276993 Accuracy 0.8896250128746033\n",
      "Iteration 28610 Training loss 7.210858166217804e-05 Validation loss 0.04453973472118378 Accuracy 0.8892500400543213\n",
      "Iteration 28620 Training loss 0.002567167626693845 Validation loss 0.04457005858421326 Accuracy 0.89000004529953\n",
      "Iteration 28630 Training loss 0.003807845525443554 Validation loss 0.04454192519187927 Accuracy 0.8891250491142273\n",
      "Iteration 28640 Training loss 0.0013231370830908418 Validation loss 0.04471750557422638 Accuracy 0.8887500166893005\n",
      "Iteration 28650 Training loss 0.00382150337100029 Validation loss 0.044614527374506 Accuracy 0.8896250128746033\n",
      "Iteration 28660 Training loss 0.0013047948013991117 Validation loss 0.044644150882959366 Accuracy 0.8883750438690186\n",
      "Iteration 28670 Training loss 0.0013034350704401731 Validation loss 0.04462951049208641 Accuracy 0.8883750438690186\n",
      "Iteration 28680 Training loss 0.0013066797982901335 Validation loss 0.044573068618774414 Accuracy 0.8892500400543213\n",
      "Iteration 28690 Training loss 7.100193033693358e-05 Validation loss 0.04456907510757446 Accuracy 0.8892500400543213\n",
      "Iteration 28700 Training loss 0.0025426053907722235 Validation loss 0.044575996696949005 Accuracy 0.8892500400543213\n",
      "Iteration 28710 Training loss 0.0013008007081225514 Validation loss 0.044570524245500565 Accuracy 0.890125036239624\n",
      "Iteration 28720 Training loss 0.0013330647489055991 Validation loss 0.0446324497461319 Accuracy 0.8890000581741333\n",
      "Iteration 28730 Training loss 0.001306841499172151 Validation loss 0.04451700299978256 Accuracy 0.889750063419342\n",
      "Iteration 28740 Training loss 0.005059662740677595 Validation loss 0.04448672756552696 Accuracy 0.8896250128746033\n",
      "Iteration 28750 Training loss 0.0025669133756309748 Validation loss 0.044608112424612045 Accuracy 0.8891250491142273\n",
      "Iteration 28760 Training loss 0.001324571785517037 Validation loss 0.04456903040409088 Accuracy 0.8892500400543213\n",
      "Iteration 28770 Training loss 0.0025590157601982355 Validation loss 0.04455259442329407 Accuracy 0.8893750309944153\n",
      "Iteration 28780 Training loss 0.0038261280860751867 Validation loss 0.04455534741282463 Accuracy 0.8892500400543213\n",
      "Iteration 28790 Training loss 0.0025620097294449806 Validation loss 0.04459075257182121 Accuracy 0.8896250128746033\n",
      "Iteration 28800 Training loss 0.002543395385146141 Validation loss 0.044507112354040146 Accuracy 0.8896250128746033\n",
      "Iteration 28810 Training loss 0.002567858435213566 Validation loss 0.04453461617231369 Accuracy 0.8895000219345093\n",
      "Iteration 28820 Training loss 0.001326306606642902 Validation loss 0.04451201856136322 Accuracy 0.8893750309944153\n",
      "Iteration 28830 Training loss 0.0013007160741835833 Validation loss 0.04453572258353233 Accuracy 0.8891250491142273\n",
      "Iteration 28840 Training loss 0.0025912271812558174 Validation loss 0.044858939945697784 Accuracy 0.889750063419342\n",
      "Iteration 28850 Training loss 8.6374711827375e-05 Validation loss 0.04453856870532036 Accuracy 0.8896250128746033\n",
      "Iteration 28860 Training loss 5.267991582513787e-05 Validation loss 0.044555146247148514 Accuracy 0.890250027179718\n",
      "Iteration 28870 Training loss 0.0013158083893358707 Validation loss 0.04453624039888382 Accuracy 0.8887500166893005\n",
      "Iteration 28880 Training loss 0.0013118126662448049 Validation loss 0.044633347541093826 Accuracy 0.8888750672340393\n",
      "Iteration 28890 Training loss 0.0038031204603612423 Validation loss 0.04445876553654671 Accuracy 0.889875054359436\n",
      "Iteration 28900 Training loss 0.005078399088233709 Validation loss 0.04448114335536957 Accuracy 0.8895000219345093\n",
      "Iteration 28910 Training loss 0.0013123914832249284 Validation loss 0.04452984035015106 Accuracy 0.8891250491142273\n",
      "Iteration 28920 Training loss 0.0012909519718959928 Validation loss 0.044464562088251114 Accuracy 0.89000004529953\n",
      "Iteration 28930 Training loss 0.0013012017589062452 Validation loss 0.04450336843729019 Accuracy 0.889875054359436\n",
      "Iteration 28940 Training loss 0.001323286211118102 Validation loss 0.04459172859787941 Accuracy 0.8890000581741333\n",
      "Iteration 28950 Training loss 0.0013398644514381886 Validation loss 0.04457198083400726 Accuracy 0.8890000581741333\n",
      "Iteration 28960 Training loss 0.0025819619186222553 Validation loss 0.04454634338617325 Accuracy 0.8896250128746033\n",
      "Iteration 28970 Training loss 0.0026189922355115414 Validation loss 0.044559236615896225 Accuracy 0.89000004529953\n",
      "Iteration 28980 Training loss 0.0025697494857013226 Validation loss 0.044585198163986206 Accuracy 0.8891250491142273\n",
      "Iteration 28990 Training loss 4.936407276545651e-05 Validation loss 0.044559601694345474 Accuracy 0.8888750672340393\n",
      "Iteration 29000 Training loss 4.668232941185124e-05 Validation loss 0.04452703893184662 Accuracy 0.8887500166893005\n",
      "Iteration 29010 Training loss 5.195048652240075e-05 Validation loss 0.04450684413313866 Accuracy 0.8886250257492065\n",
      "Iteration 29020 Training loss 0.0012960993917658925 Validation loss 0.044607773423194885 Accuracy 0.8893750309944153\n",
      "Iteration 29030 Training loss 7.038730836939067e-05 Validation loss 0.044597260653972626 Accuracy 0.8887500166893005\n",
      "Iteration 29040 Training loss 6.0805334214819595e-05 Validation loss 0.044473372399806976 Accuracy 0.890125036239624\n",
      "Iteration 29050 Training loss 7.084642129484564e-05 Validation loss 0.044481970369815826 Accuracy 0.8896250128746033\n",
      "Iteration 29060 Training loss 9.6520219813101e-05 Validation loss 0.044454462826251984 Accuracy 0.8893750309944153\n",
      "Iteration 29070 Training loss 0.0025514799635857344 Validation loss 0.044496841728687286 Accuracy 0.890125036239624\n",
      "Iteration 29080 Training loss 0.0013006060617044568 Validation loss 0.04449952766299248 Accuracy 0.889750063419342\n",
      "Iteration 29090 Training loss 0.001311772153712809 Validation loss 0.04450700432062149 Accuracy 0.8890000581741333\n",
      "Iteration 29100 Training loss 0.00010267026664223522 Validation loss 0.04472411423921585 Accuracy 0.8887500166893005\n",
      "Iteration 29110 Training loss 0.0038062508683651686 Validation loss 0.044556908309459686 Accuracy 0.8890000581741333\n",
      "Iteration 29120 Training loss 0.0013157232897356153 Validation loss 0.04454057291150093 Accuracy 0.8892500400543213\n",
      "Iteration 29130 Training loss 0.0025481050834059715 Validation loss 0.04464276507496834 Accuracy 0.8883750438690186\n",
      "Iteration 29140 Training loss 0.0012983010383322835 Validation loss 0.044566720724105835 Accuracy 0.8893750309944153\n",
      "Iteration 29150 Training loss 0.0025733436923474073 Validation loss 0.04459623247385025 Accuracy 0.889875054359436\n",
      "Iteration 29160 Training loss 5.064475772087462e-05 Validation loss 0.044551242142915726 Accuracy 0.889750063419342\n",
      "Iteration 29170 Training loss 0.0013086475664749742 Validation loss 0.044459305703639984 Accuracy 0.8896250128746033\n",
      "Iteration 29180 Training loss 0.0013479357585310936 Validation loss 0.044520240277051926 Accuracy 0.890125036239624\n",
      "Iteration 29190 Training loss 0.0012988699600100517 Validation loss 0.04463152214884758 Accuracy 0.8885000348091125\n",
      "Iteration 29200 Training loss 0.0025408612564206123 Validation loss 0.044598475098609924 Accuracy 0.8886250257492065\n",
      "Iteration 29210 Training loss 0.0025687532033771276 Validation loss 0.04454135522246361 Accuracy 0.8895000219345093\n",
      "Iteration 29220 Training loss 0.0013287962647154927 Validation loss 0.0447293259203434 Accuracy 0.8887500166893005\n",
      "Iteration 29230 Training loss 0.0013173861661925912 Validation loss 0.044543903321027756 Accuracy 0.8895000219345093\n",
      "Iteration 29240 Training loss 0.001327737350948155 Validation loss 0.04470833018422127 Accuracy 0.8892500400543213\n",
      "Iteration 29250 Training loss 9.753170888870955e-05 Validation loss 0.044577836990356445 Accuracy 0.8895000219345093\n",
      "Iteration 29260 Training loss 0.0025779223069548607 Validation loss 0.044555649161338806 Accuracy 0.8890000581741333\n",
      "Iteration 29270 Training loss 0.002559926360845566 Validation loss 0.04454341158270836 Accuracy 0.8905000686645508\n",
      "Iteration 29280 Training loss 5.587518171523698e-05 Validation loss 0.04456392303109169 Accuracy 0.890125036239624\n",
      "Iteration 29290 Training loss 5.4669792007189244e-05 Validation loss 0.04463847726583481 Accuracy 0.8906250596046448\n",
      "Iteration 29300 Training loss 0.002551546087488532 Validation loss 0.04455767199397087 Accuracy 0.889875054359436\n",
      "Iteration 29310 Training loss 0.0013245971640571952 Validation loss 0.04452000558376312 Accuracy 0.89000004529953\n",
      "Iteration 29320 Training loss 7.295831164810807e-05 Validation loss 0.04481809586286545 Accuracy 0.889875054359436\n",
      "Iteration 29330 Training loss 8.512684144079685e-05 Validation loss 0.04453561455011368 Accuracy 0.8892500400543213\n",
      "Iteration 29340 Training loss 6.806965393479913e-05 Validation loss 0.04449930042028427 Accuracy 0.89000004529953\n",
      "Iteration 29350 Training loss 5.62297718715854e-05 Validation loss 0.044575922191143036 Accuracy 0.890250027179718\n",
      "Iteration 29360 Training loss 0.0025468894746154547 Validation loss 0.04455907642841339 Accuracy 0.8891250491142273\n",
      "Iteration 29370 Training loss 0.0013194152852520347 Validation loss 0.0445965901017189 Accuracy 0.889750063419342\n",
      "Iteration 29380 Training loss 7.660392293473706e-05 Validation loss 0.0445544458925724 Accuracy 0.8890000581741333\n",
      "Iteration 29390 Training loss 0.0025625547859817743 Validation loss 0.04474266245961189 Accuracy 0.89000004529953\n",
      "Iteration 29400 Training loss 0.0013313837116584182 Validation loss 0.04461474344134331 Accuracy 0.8892500400543213\n",
      "Iteration 29410 Training loss 0.00381918391212821 Validation loss 0.04454526677727699 Accuracy 0.8888750672340393\n",
      "Iteration 29420 Training loss 6.987625965848565e-05 Validation loss 0.044486016035079956 Accuracy 0.890125036239624\n",
      "Iteration 29430 Training loss 0.0038186353631317616 Validation loss 0.04455682635307312 Accuracy 0.8893750309944153\n",
      "Iteration 29440 Training loss 0.0012973234988749027 Validation loss 0.04453137516975403 Accuracy 0.8896250128746033\n",
      "Iteration 29450 Training loss 0.0038021367508918047 Validation loss 0.044628046452999115 Accuracy 0.8891250491142273\n",
      "Iteration 29460 Training loss 0.0025628991425037384 Validation loss 0.044639430940151215 Accuracy 0.8886250257492065\n",
      "Iteration 29470 Training loss 0.002526714699342847 Validation loss 0.0446326769888401 Accuracy 0.8896250128746033\n",
      "Iteration 29480 Training loss 0.002562990179285407 Validation loss 0.044664669781923294 Accuracy 0.8891250491142273\n",
      "Iteration 29490 Training loss 0.0025663955602794886 Validation loss 0.04454931989312172 Accuracy 0.8896250128746033\n",
      "Iteration 29500 Training loss 0.0038200682029128075 Validation loss 0.044651057571172714 Accuracy 0.89000004529953\n",
      "Iteration 29510 Training loss 0.002549294149503112 Validation loss 0.044625360518693924 Accuracy 0.8893750309944153\n",
      "Iteration 29520 Training loss 6.378931720973924e-05 Validation loss 0.04461048170924187 Accuracy 0.8892500400543213\n",
      "Iteration 29530 Training loss 0.0025601147208362818 Validation loss 0.044541023671627045 Accuracy 0.8892500400543213\n",
      "Iteration 29540 Training loss 0.001280349213629961 Validation loss 0.04462338611483574 Accuracy 0.8892500400543213\n",
      "Iteration 29550 Training loss 0.0013065541861578822 Validation loss 0.044634781777858734 Accuracy 0.8890000581741333\n",
      "Iteration 29560 Training loss 0.0013152704341337085 Validation loss 0.04458629712462425 Accuracy 0.8888750672340393\n",
      "Iteration 29570 Training loss 0.0025673215277493 Validation loss 0.04457822069525719 Accuracy 0.8892500400543213\n",
      "Iteration 29580 Training loss 0.002575078047811985 Validation loss 0.04456419497728348 Accuracy 0.89000004529953\n",
      "Iteration 29590 Training loss 4.993999391444959e-05 Validation loss 0.044573307037353516 Accuracy 0.8893750309944153\n",
      "Iteration 29600 Training loss 0.0013114309404045343 Validation loss 0.04456498846411705 Accuracy 0.8896250128746033\n",
      "Iteration 29610 Training loss 0.0025601412635296583 Validation loss 0.04457171633839607 Accuracy 0.8895000219345093\n",
      "Iteration 29620 Training loss 0.002536536194384098 Validation loss 0.044548895210027695 Accuracy 0.889875054359436\n",
      "Iteration 29630 Training loss 7.667393219890073e-05 Validation loss 0.04456421732902527 Accuracy 0.890125036239624\n",
      "Iteration 29640 Training loss 0.003803959349170327 Validation loss 0.04450549557805061 Accuracy 0.89000004529953\n",
      "Iteration 29650 Training loss 8.226202771766111e-05 Validation loss 0.04454110935330391 Accuracy 0.8890000581741333\n",
      "Iteration 29660 Training loss 3.6954643292119727e-05 Validation loss 0.044538140296936035 Accuracy 0.8891250491142273\n",
      "Iteration 29670 Training loss 0.005070727318525314 Validation loss 0.04455101862549782 Accuracy 0.8891250491142273\n",
      "Iteration 29680 Training loss 5.374933607527055e-05 Validation loss 0.04468601569533348 Accuracy 0.89000004529953\n",
      "Iteration 29690 Training loss 0.0012986821820959449 Validation loss 0.04450049623847008 Accuracy 0.8892500400543213\n",
      "Iteration 29700 Training loss 0.0025453579146414995 Validation loss 0.04449549689888954 Accuracy 0.8893750309944153\n",
      "Iteration 29710 Training loss 0.0013169609010219574 Validation loss 0.044530823826789856 Accuracy 0.8890000581741333\n",
      "Iteration 29720 Training loss 0.0027353449258953333 Validation loss 0.04487313702702522 Accuracy 0.890250027179718\n",
      "Iteration 29730 Training loss 0.002578405197709799 Validation loss 0.044555675238370895 Accuracy 0.8883750438690186\n",
      "Iteration 29740 Training loss 0.0013572913594543934 Validation loss 0.044552989304065704 Accuracy 0.8893750309944153\n",
      "Iteration 29750 Training loss 0.001321967807598412 Validation loss 0.04466843605041504 Accuracy 0.8907500505447388\n",
      "Iteration 29760 Training loss 0.0025600597728043795 Validation loss 0.04457850381731987 Accuracy 0.8891250491142273\n",
      "Iteration 29770 Training loss 0.0025532986037433147 Validation loss 0.04456460103392601 Accuracy 0.8891250491142273\n",
      "Iteration 29780 Training loss 6.587525422219187e-05 Validation loss 0.044579461216926575 Accuracy 0.890125036239624\n",
      "Iteration 29790 Training loss 0.0013128198916092515 Validation loss 0.0445607453584671 Accuracy 0.8887500166893005\n",
      "Iteration 29800 Training loss 5.6154483900172636e-05 Validation loss 0.044674161821603775 Accuracy 0.8883750438690186\n",
      "Iteration 29810 Training loss 6.3557323301211e-05 Validation loss 0.04461792856454849 Accuracy 0.8890000581741333\n",
      "Iteration 29820 Training loss 0.0038009583950042725 Validation loss 0.04470966383814812 Accuracy 0.8887500166893005\n",
      "Iteration 29830 Training loss 5.918712122365832e-05 Validation loss 0.044645436108112335 Accuracy 0.8891250491142273\n",
      "Iteration 29840 Training loss 0.003810044378042221 Validation loss 0.04463209584355354 Accuracy 0.8887500166893005\n",
      "Iteration 29850 Training loss 0.0013376602437347174 Validation loss 0.044595442712306976 Accuracy 0.8891250491142273\n",
      "Iteration 29860 Training loss 0.0013016901211813092 Validation loss 0.04453583061695099 Accuracy 0.890375018119812\n",
      "Iteration 29870 Training loss 0.003824516199529171 Validation loss 0.044570744037628174 Accuracy 0.8891250491142273\n",
      "Iteration 29880 Training loss 8.07976393844001e-05 Validation loss 0.04471324384212494 Accuracy 0.8905000686645508\n",
      "Iteration 29890 Training loss 0.002582817105576396 Validation loss 0.04461067542433739 Accuracy 0.8892500400543213\n",
      "Iteration 29900 Training loss 0.001333432155661285 Validation loss 0.04460061714053154 Accuracy 0.8887500166893005\n",
      "Iteration 29910 Training loss 0.0025638972874730825 Validation loss 0.044615525752305984 Accuracy 0.8893750309944153\n",
      "Iteration 29920 Training loss 6.643286906182766e-05 Validation loss 0.044664546847343445 Accuracy 0.8890000581741333\n",
      "Iteration 29930 Training loss 5.357035479391925e-05 Validation loss 0.04461312294006348 Accuracy 0.8895000219345093\n",
      "Iteration 29940 Training loss 0.003798997960984707 Validation loss 0.0445392020046711 Accuracy 0.8893750309944153\n",
      "Iteration 29950 Training loss 0.0001063122326740995 Validation loss 0.044653236865997314 Accuracy 0.8893750309944153\n",
      "Iteration 29960 Training loss 0.0013202640693634748 Validation loss 0.04459628462791443 Accuracy 0.8882500529289246\n",
      "Iteration 29970 Training loss 0.002558652078732848 Validation loss 0.04472397640347481 Accuracy 0.8892500400543213\n",
      "Iteration 29980 Training loss 0.00629348075017333 Validation loss 0.04460576921701431 Accuracy 0.8892500400543213\n",
      "Iteration 29990 Training loss 0.0013067774707451463 Validation loss 0.04464513063430786 Accuracy 0.8887500166893005\n",
      "Iteration 30000 Training loss 0.0013067014515399933 Validation loss 0.04467714950442314 Accuracy 0.8888750672340393\n",
      "Iteration 30010 Training loss 0.005041280295699835 Validation loss 0.04465252906084061 Accuracy 0.8893750309944153\n",
      "Iteration 30020 Training loss 0.0013061811914667487 Validation loss 0.04459259286522865 Accuracy 0.8887500166893005\n",
      "Iteration 30030 Training loss 0.0025706528685986996 Validation loss 0.04466928541660309 Accuracy 0.8892500400543213\n",
      "Iteration 30040 Training loss 0.0012896896805614233 Validation loss 0.044638071209192276 Accuracy 0.8887500166893005\n",
      "Iteration 30050 Training loss 0.002551563084125519 Validation loss 0.04459242522716522 Accuracy 0.8891250491142273\n",
      "Iteration 30060 Training loss 0.002566583687439561 Validation loss 0.04469672217965126 Accuracy 0.8893750309944153\n",
      "Iteration 30070 Training loss 0.0012892986414954066 Validation loss 0.04465384781360626 Accuracy 0.8891250491142273\n",
      "Iteration 30080 Training loss 0.006313865073025227 Validation loss 0.04475043714046478 Accuracy 0.890375018119812\n",
      "Iteration 30090 Training loss 0.0013004959328100085 Validation loss 0.04459823668003082 Accuracy 0.8887500166893005\n",
      "Iteration 30100 Training loss 0.0012993932468816638 Validation loss 0.04468141868710518 Accuracy 0.889750063419342\n",
      "Iteration 30110 Training loss 6.0958620451856405e-05 Validation loss 0.044612184166908264 Accuracy 0.8895000219345093\n",
      "Iteration 30120 Training loss 0.0025673850905150175 Validation loss 0.0446314811706543 Accuracy 0.8887500166893005\n",
      "Iteration 30130 Training loss 0.0012921631569042802 Validation loss 0.04460460692644119 Accuracy 0.8896250128746033\n",
      "Iteration 30140 Training loss 0.0013043025974184275 Validation loss 0.04454632103443146 Accuracy 0.8890000581741333\n",
      "Iteration 30150 Training loss 4.499023634707555e-05 Validation loss 0.044566962867975235 Accuracy 0.8895000219345093\n",
      "Iteration 30160 Training loss 0.002541633788496256 Validation loss 0.04460088536143303 Accuracy 0.8895000219345093\n",
      "Iteration 30170 Training loss 0.0013235857477411628 Validation loss 0.04458468779921532 Accuracy 0.8890000581741333\n",
      "Iteration 30180 Training loss 7.33366105123423e-05 Validation loss 0.04464814439415932 Accuracy 0.8893750309944153\n",
      "Iteration 30190 Training loss 0.001296758884564042 Validation loss 0.044660281389951706 Accuracy 0.8892500400543213\n",
      "Iteration 30200 Training loss 0.002549519296735525 Validation loss 0.04461723193526268 Accuracy 0.8891250491142273\n",
      "Iteration 30210 Training loss 0.0013294052332639694 Validation loss 0.044665124267339706 Accuracy 0.8886250257492065\n",
      "Iteration 30220 Training loss 6.387281609931961e-05 Validation loss 0.044675905257463455 Accuracy 0.8882500529289246\n",
      "Iteration 30230 Training loss 6.311767356237397e-05 Validation loss 0.044665779918432236 Accuracy 0.8885000348091125\n",
      "Iteration 30240 Training loss 5.00310052302666e-05 Validation loss 0.04462961480021477 Accuracy 0.8890000581741333\n",
      "Iteration 30250 Training loss 6.50892689009197e-05 Validation loss 0.044587358832359314 Accuracy 0.8892500400543213\n",
      "Iteration 30260 Training loss 5.415151099441573e-05 Validation loss 0.04456622898578644 Accuracy 0.8895000219345093\n",
      "Iteration 30270 Training loss 0.0038158257957547903 Validation loss 0.04457712173461914 Accuracy 0.889750063419342\n",
      "Iteration 30280 Training loss 0.0025586974807083607 Validation loss 0.04464803263545036 Accuracy 0.8892500400543213\n",
      "Iteration 30290 Training loss 7.72041967138648e-05 Validation loss 0.04460037499666214 Accuracy 0.8907500505447388\n",
      "Iteration 30300 Training loss 0.0012978831073269248 Validation loss 0.04465348273515701 Accuracy 0.8877500295639038\n",
      "Iteration 30310 Training loss 0.0012909233337268233 Validation loss 0.04458184167742729 Accuracy 0.890125036239624\n",
      "Iteration 30320 Training loss 0.0012940866872668266 Validation loss 0.04462728276848793 Accuracy 0.8890000581741333\n",
      "Iteration 30330 Training loss 0.0025627294089645147 Validation loss 0.04455561935901642 Accuracy 0.8892500400543213\n",
      "Iteration 30340 Training loss 0.005058606620877981 Validation loss 0.04457627609372139 Accuracy 0.8883750438690186\n",
      "Iteration 30350 Training loss 0.0012966005597263575 Validation loss 0.04457980394363403 Accuracy 0.89000004529953\n",
      "Iteration 30360 Training loss 6.782574200769886e-05 Validation loss 0.04484790191054344 Accuracy 0.8896250128746033\n",
      "Iteration 30370 Training loss 0.002551079960539937 Validation loss 0.0445743165910244 Accuracy 0.8891250491142273\n",
      "Iteration 30380 Training loss 0.0025588851422071457 Validation loss 0.04463466256856918 Accuracy 0.8882500529289246\n",
      "Iteration 30390 Training loss 0.0012937670107930899 Validation loss 0.044654522091150284 Accuracy 0.8893750309944153\n",
      "Iteration 30400 Training loss 0.0012925980845466256 Validation loss 0.044663529843091965 Accuracy 0.8888750672340393\n",
      "Iteration 30410 Training loss 0.001298088813200593 Validation loss 0.04470163956284523 Accuracy 0.8893750309944153\n",
      "Iteration 30420 Training loss 0.0013106478145346045 Validation loss 0.04463112726807594 Accuracy 0.8890000581741333\n",
      "Iteration 30430 Training loss 0.005048612132668495 Validation loss 0.044600363820791245 Accuracy 0.8890000581741333\n",
      "Iteration 30440 Training loss 0.0012949957745149732 Validation loss 0.044667672365903854 Accuracy 0.8890000581741333\n",
      "Iteration 30450 Training loss 0.0012952026445418596 Validation loss 0.04464218392968178 Accuracy 0.8886250257492065\n",
      "Iteration 30460 Training loss 0.0025630975142121315 Validation loss 0.04460613429546356 Accuracy 0.8890000581741333\n",
      "Iteration 30470 Training loss 3.8466732803499326e-05 Validation loss 0.04460178688168526 Accuracy 0.8890000581741333\n",
      "Iteration 30480 Training loss 7.901100616436452e-05 Validation loss 0.044724494218826294 Accuracy 0.8893750309944153\n",
      "Iteration 30490 Training loss 0.002549568424001336 Validation loss 0.04466874897480011 Accuracy 0.8887500166893005\n",
      "Iteration 30500 Training loss 0.003606383455917239 Validation loss 0.04465162754058838 Accuracy 0.8891250491142273\n",
      "Iteration 30510 Training loss 6.257378117879853e-05 Validation loss 0.044664256274700165 Accuracy 0.8891250491142273\n",
      "Iteration 30520 Training loss 0.0012913249665871263 Validation loss 0.04456370696425438 Accuracy 0.889875054359436\n",
      "Iteration 30530 Training loss 0.0025423334445804358 Validation loss 0.04463727027177811 Accuracy 0.8896250128746033\n",
      "Iteration 30540 Training loss 0.005080387461930513 Validation loss 0.044679272919893265 Accuracy 0.8887500166893005\n",
      "Iteration 30550 Training loss 0.0012984995264559984 Validation loss 0.04462222382426262 Accuracy 0.8892500400543213\n",
      "Iteration 30560 Training loss 0.002562884008511901 Validation loss 0.044536102563142776 Accuracy 0.8895000219345093\n",
      "Iteration 30570 Training loss 0.001335166860371828 Validation loss 0.04454713314771652 Accuracy 0.8893750309944153\n",
      "Iteration 30580 Training loss 8.615612023277208e-05 Validation loss 0.044588908553123474 Accuracy 0.890250027179718\n",
      "Iteration 30590 Training loss 0.001308069913648069 Validation loss 0.04462173581123352 Accuracy 0.890250027179718\n",
      "Iteration 30600 Training loss 0.0037951963022351265 Validation loss 0.04461991786956787 Accuracy 0.890375018119812\n",
      "Iteration 30610 Training loss 6.951774412300438e-05 Validation loss 0.04466872662305832 Accuracy 0.8908750414848328\n",
      "Iteration 30620 Training loss 7.211411138996482e-05 Validation loss 0.044664423912763596 Accuracy 0.8896250128746033\n",
      "Iteration 30630 Training loss 0.0013199785025790334 Validation loss 0.044608861207962036 Accuracy 0.889750063419342\n",
      "Iteration 30640 Training loss 0.0013187990989536047 Validation loss 0.044636111706495285 Accuracy 0.8896250128746033\n",
      "Iteration 30650 Training loss 4.084025931661017e-05 Validation loss 0.04455133154988289 Accuracy 0.8896250128746033\n",
      "Iteration 30660 Training loss 0.001287568942643702 Validation loss 0.04455524682998657 Accuracy 0.8896250128746033\n",
      "Iteration 30670 Training loss 0.0013040645280852914 Validation loss 0.044643569737672806 Accuracy 0.8893750309944153\n",
      "Iteration 30680 Training loss 4.501677904045209e-05 Validation loss 0.04458126425743103 Accuracy 0.889750063419342\n",
      "Iteration 30690 Training loss 5.508577305590734e-05 Validation loss 0.04462665691971779 Accuracy 0.8895000219345093\n",
      "Iteration 30700 Training loss 0.002590880263596773 Validation loss 0.04459775239229202 Accuracy 0.890250027179718\n",
      "Iteration 30710 Training loss 5.829927249578759e-05 Validation loss 0.044569842517375946 Accuracy 0.889750063419342\n",
      "Iteration 30720 Training loss 5.2595332817872986e-05 Validation loss 0.04453336447477341 Accuracy 0.8896250128746033\n",
      "Iteration 30730 Training loss 0.0013049236731603742 Validation loss 0.04458106681704521 Accuracy 0.8896250128746033\n",
      "Iteration 30740 Training loss 8.560885180486366e-05 Validation loss 0.044801753014326096 Accuracy 0.8893750309944153\n",
      "Iteration 30750 Training loss 0.0013178263325244188 Validation loss 0.04460179805755615 Accuracy 0.889875054359436\n",
      "Iteration 30760 Training loss 3.787437890423462e-05 Validation loss 0.044609420001506805 Accuracy 0.8896250128746033\n",
      "Iteration 30770 Training loss 6.964214117033407e-05 Validation loss 0.044619087129831314 Accuracy 0.89000004529953\n",
      "Iteration 30780 Training loss 7.159064261941239e-05 Validation loss 0.04495633393526077 Accuracy 0.8888750672340393\n",
      "Iteration 30790 Training loss 0.002557436702772975 Validation loss 0.044972389936447144 Accuracy 0.8882500529289246\n",
      "Iteration 30800 Training loss 3.5738594306167215e-05 Validation loss 0.04463624954223633 Accuracy 0.8896250128746033\n",
      "Iteration 30810 Training loss 0.00130621122661978 Validation loss 0.044602297246456146 Accuracy 0.8896250128746033\n",
      "Iteration 30820 Training loss 0.0038173687644302845 Validation loss 0.04479127749800682 Accuracy 0.8893750309944153\n",
      "Iteration 30830 Training loss 0.0013218505773693323 Validation loss 0.04461265727877617 Accuracy 0.890250027179718\n",
      "Iteration 30840 Training loss 0.002552276011556387 Validation loss 0.04460333287715912 Accuracy 0.890125036239624\n",
      "Iteration 30850 Training loss 4.548444485408254e-05 Validation loss 0.04476410895586014 Accuracy 0.8896250128746033\n",
      "Iteration 30860 Training loss 6.947894871700555e-05 Validation loss 0.04456605762243271 Accuracy 0.8905000686645508\n",
      "Iteration 30870 Training loss 0.002559283282607794 Validation loss 0.04460705816745758 Accuracy 0.8893750309944153\n",
      "Iteration 30880 Training loss 0.001385044539347291 Validation loss 0.04472602158784866 Accuracy 0.89000004529953\n",
      "Iteration 30890 Training loss 8.444298873655498e-05 Validation loss 0.04456479847431183 Accuracy 0.8911250233650208\n",
      "Iteration 30900 Training loss 3.3224583603441715e-05 Validation loss 0.04464724287390709 Accuracy 0.8895000219345093\n",
      "Iteration 30910 Training loss 0.0026124739088118076 Validation loss 0.04515029489994049 Accuracy 0.8875000476837158\n",
      "Iteration 30920 Training loss 0.0013247427996248007 Validation loss 0.044650040566921234 Accuracy 0.8881250619888306\n",
      "Iteration 30930 Training loss 0.0012895783875137568 Validation loss 0.044661879539489746 Accuracy 0.8893750309944153\n",
      "Iteration 30940 Training loss 0.0025628595612943172 Validation loss 0.044638924300670624 Accuracy 0.8906250596046448\n",
      "Iteration 30950 Training loss 0.0013375700218603015 Validation loss 0.044648781418800354 Accuracy 0.890125036239624\n",
      "Iteration 30960 Training loss 4.110272493562661e-05 Validation loss 0.04460727050900459 Accuracy 0.8893750309944153\n",
      "Iteration 30970 Training loss 4.9308880988974124e-05 Validation loss 0.04460671544075012 Accuracy 0.8896250128746033\n",
      "Iteration 30980 Training loss 0.002546104835346341 Validation loss 0.04462666064500809 Accuracy 0.889875054359436\n",
      "Iteration 30990 Training loss 5.9098743804497644e-05 Validation loss 0.044620733708143234 Accuracy 0.890125036239624\n",
      "Iteration 31000 Training loss 0.0013108152197673917 Validation loss 0.044552698731422424 Accuracy 0.890250027179718\n",
      "Iteration 31010 Training loss 0.002562248846516013 Validation loss 0.04465341567993164 Accuracy 0.889875054359436\n",
      "Iteration 31020 Training loss 6.276729982346296e-05 Validation loss 0.04472935572266579 Accuracy 0.890250027179718\n",
      "Iteration 31030 Training loss 4.643515421776101e-05 Validation loss 0.04470338672399521 Accuracy 0.8905000686645508\n",
      "Iteration 31040 Training loss 0.0025506627280265093 Validation loss 0.044579844921827316 Accuracy 0.890250027179718\n",
      "Iteration 31050 Training loss 0.0037954836152493954 Validation loss 0.044610634446144104 Accuracy 0.8893750309944153\n",
      "Iteration 31060 Training loss 0.003817710094153881 Validation loss 0.04462658613920212 Accuracy 0.890125036239624\n",
      "Iteration 31070 Training loss 0.0025628781877458096 Validation loss 0.04459600895643234 Accuracy 0.890375018119812\n",
      "Iteration 31080 Training loss 0.00131546042393893 Validation loss 0.044600747525691986 Accuracy 0.8906250596046448\n",
      "Iteration 31090 Training loss 0.0038218670524656773 Validation loss 0.044623978435993195 Accuracy 0.890125036239624\n",
      "Iteration 31100 Training loss 0.0013064382364973426 Validation loss 0.04460747539997101 Accuracy 0.890375018119812\n",
      "Iteration 31110 Training loss 4.6250177547335625e-05 Validation loss 0.044589389115571976 Accuracy 0.890250027179718\n",
      "Iteration 31120 Training loss 0.002548241289332509 Validation loss 0.04467837139964104 Accuracy 0.8906250596046448\n",
      "Iteration 31130 Training loss 0.0038189468905329704 Validation loss 0.04453318193554878 Accuracy 0.8905000686645508\n",
      "Iteration 31140 Training loss 8.101614366751164e-05 Validation loss 0.04453489929437637 Accuracy 0.889750063419342\n",
      "Iteration 31150 Training loss 0.0013004858046770096 Validation loss 0.04458741098642349 Accuracy 0.8896250128746033\n",
      "Iteration 31160 Training loss 5.235548087512143e-05 Validation loss 0.04456876590847969 Accuracy 0.8895000219345093\n",
      "Iteration 31170 Training loss 7.056428876239806e-05 Validation loss 0.044530294835567474 Accuracy 0.8910000324249268\n",
      "Iteration 31180 Training loss 4.908624032395892e-05 Validation loss 0.04457925632596016 Accuracy 0.889875054359436\n",
      "Iteration 31190 Training loss 0.001293396344408393 Validation loss 0.04456465691328049 Accuracy 0.890375018119812\n",
      "Iteration 31200 Training loss 0.005043577868491411 Validation loss 0.04456699639558792 Accuracy 0.890375018119812\n",
      "Iteration 31210 Training loss 0.0013028443790972233 Validation loss 0.04455677419900894 Accuracy 0.8907500505447388\n",
      "Iteration 31220 Training loss 5.713434438803233e-05 Validation loss 0.04458494111895561 Accuracy 0.8895000219345093\n",
      "Iteration 31230 Training loss 7.598432421218604e-05 Validation loss 0.04458707571029663 Accuracy 0.8888750672340393\n",
      "Iteration 31240 Training loss 0.0012795323273167014 Validation loss 0.04455425217747688 Accuracy 0.889750063419342\n",
      "Iteration 31250 Training loss 6.177832983667031e-05 Validation loss 0.044587623327970505 Accuracy 0.8888750672340393\n",
      "Iteration 31260 Training loss 0.0013106417609378695 Validation loss 0.04466104507446289 Accuracy 0.889875054359436\n",
      "Iteration 31270 Training loss 0.0012983472552150488 Validation loss 0.04453907161951065 Accuracy 0.890125036239624\n",
      "Iteration 31280 Training loss 0.0013037080643698573 Validation loss 0.04454458877444267 Accuracy 0.890125036239624\n",
      "Iteration 31290 Training loss 0.0013230203185230494 Validation loss 0.044592972844839096 Accuracy 0.889750063419342\n",
      "Iteration 31300 Training loss 0.0025573812890797853 Validation loss 0.04464886710047722 Accuracy 0.8896250128746033\n",
      "Iteration 31310 Training loss 0.002557388273999095 Validation loss 0.04462221637368202 Accuracy 0.889750063419342\n",
      "Iteration 31320 Training loss 0.0013008656678721309 Validation loss 0.044581230729818344 Accuracy 0.8895000219345093\n",
      "Iteration 31330 Training loss 0.0012927502393722534 Validation loss 0.044527195394039154 Accuracy 0.890375018119812\n",
      "Iteration 31340 Training loss 0.0025595121551305056 Validation loss 0.0445745550096035 Accuracy 0.889750063419342\n",
      "Iteration 31350 Training loss 0.0013256819220259786 Validation loss 0.044542230665683746 Accuracy 0.890250027179718\n",
      "Iteration 31360 Training loss 3.421440487727523e-05 Validation loss 0.04462243244051933 Accuracy 0.890125036239624\n",
      "Iteration 31370 Training loss 0.001311996951699257 Validation loss 0.044801920652389526 Accuracy 0.890125036239624\n",
      "Iteration 31380 Training loss 7.520757935708389e-05 Validation loss 0.04466971755027771 Accuracy 0.890375018119812\n",
      "Iteration 31390 Training loss 5.6517270422773436e-05 Validation loss 0.044609345495700836 Accuracy 0.890250027179718\n",
      "Iteration 31400 Training loss 0.0012929473305121064 Validation loss 0.044566038995981216 Accuracy 0.890250027179718\n",
      "Iteration 31410 Training loss 6.229810242075473e-05 Validation loss 0.04460266977548599 Accuracy 0.8893750309944153\n",
      "Iteration 31420 Training loss 0.0012976012658327818 Validation loss 0.04461941123008728 Accuracy 0.8896250128746033\n",
      "Iteration 31430 Training loss 0.0013039768673479557 Validation loss 0.04459225758910179 Accuracy 0.8895000219345093\n",
      "Iteration 31440 Training loss 0.001305704819969833 Validation loss 0.04457705840468407 Accuracy 0.89000004529953\n",
      "Iteration 31450 Training loss 4.8986123147187755e-05 Validation loss 0.044569145888090134 Accuracy 0.890375018119812\n",
      "Iteration 31460 Training loss 0.0013003689236938953 Validation loss 0.04461846500635147 Accuracy 0.8895000219345093\n",
      "Iteration 31470 Training loss 0.0013096409384161234 Validation loss 0.04467848315834999 Accuracy 0.8896250128746033\n",
      "Iteration 31480 Training loss 5.255639916867949e-05 Validation loss 0.04460667446255684 Accuracy 0.89000004529953\n",
      "Iteration 31490 Training loss 0.0037882288452237844 Validation loss 0.04460320994257927 Accuracy 0.8896250128746033\n",
      "Iteration 31500 Training loss 3.8760346797062084e-05 Validation loss 0.04457486793398857 Accuracy 0.8895000219345093\n",
      "Iteration 31510 Training loss 0.001282274373807013 Validation loss 0.04455769807100296 Accuracy 0.89000004529953\n",
      "Iteration 31520 Training loss 0.0012895967811346054 Validation loss 0.044617388397455215 Accuracy 0.8896250128746033\n",
      "Iteration 31530 Training loss 3.881428710883483e-05 Validation loss 0.04464054852724075 Accuracy 0.8896250128746033\n",
      "Iteration 31540 Training loss 0.0013001060578972101 Validation loss 0.04459148272871971 Accuracy 0.890125036239624\n",
      "Iteration 31550 Training loss 0.002556524006649852 Validation loss 0.0446399487555027 Accuracy 0.8892500400543213\n",
      "Iteration 31560 Training loss 4.3313335481798276e-05 Validation loss 0.04463861882686615 Accuracy 0.8893750309944153\n",
      "Iteration 31570 Training loss 0.0025480000767856836 Validation loss 0.044554706662893295 Accuracy 0.889750063419342\n",
      "Iteration 31580 Training loss 5.627957216347568e-05 Validation loss 0.044554032385349274 Accuracy 0.8888750672340393\n",
      "Iteration 31590 Training loss 0.0012915681581944227 Validation loss 0.04459182918071747 Accuracy 0.89000004529953\n",
      "Iteration 31600 Training loss 0.0012844761367887259 Validation loss 0.044582124799489975 Accuracy 0.889750063419342\n",
      "Iteration 31610 Training loss 7.869373803259805e-05 Validation loss 0.044590823352336884 Accuracy 0.889875054359436\n",
      "Iteration 31620 Training loss 0.0012959411833435297 Validation loss 0.04460560157895088 Accuracy 0.8891250491142273\n",
      "Iteration 31630 Training loss 0.0025821614544838667 Validation loss 0.04452451691031456 Accuracy 0.890250027179718\n",
      "Iteration 31640 Training loss 5.513809446711093e-05 Validation loss 0.044631075114011765 Accuracy 0.8891250491142273\n",
      "Iteration 31650 Training loss 6.935133569641039e-05 Validation loss 0.04467543214559555 Accuracy 0.8891250491142273\n",
      "Iteration 31660 Training loss 0.0013099381467327476 Validation loss 0.044613368809223175 Accuracy 0.8888750672340393\n",
      "Iteration 31670 Training loss 0.0013218324165791273 Validation loss 0.044644374400377274 Accuracy 0.8890000581741333\n",
      "Iteration 31680 Training loss 0.0013099993811920285 Validation loss 0.044580161571502686 Accuracy 0.8892500400543213\n",
      "Iteration 31690 Training loss 5.618081922875717e-05 Validation loss 0.044584859162569046 Accuracy 0.8892500400543213\n",
      "Iteration 31700 Training loss 6.0987178585492074e-05 Validation loss 0.04463580250740051 Accuracy 0.889750063419342\n",
      "Iteration 31710 Training loss 0.0012988848611712456 Validation loss 0.04460659995675087 Accuracy 0.8892500400543213\n",
      "Iteration 31720 Training loss 0.003790048649534583 Validation loss 0.04460655525326729 Accuracy 0.8896250128746033\n",
      "Iteration 31730 Training loss 0.0013031256385147572 Validation loss 0.044620364904403687 Accuracy 0.890125036239624\n",
      "Iteration 31740 Training loss 0.003803678322583437 Validation loss 0.04468834772706032 Accuracy 0.8892500400543213\n",
      "Iteration 31750 Training loss 0.0012929319636896253 Validation loss 0.04467838630080223 Accuracy 0.8893750309944153\n",
      "Iteration 31760 Training loss 0.0025637929793447256 Validation loss 0.04469608515501022 Accuracy 0.8895000219345093\n",
      "Iteration 31770 Training loss 0.002550590317696333 Validation loss 0.04466176778078079 Accuracy 0.8892500400543213\n",
      "Iteration 31780 Training loss 4.633857315639034e-05 Validation loss 0.044621679931879044 Accuracy 0.89000004529953\n",
      "Iteration 31790 Training loss 0.0025530746206641197 Validation loss 0.04461480677127838 Accuracy 0.890375018119812\n",
      "Iteration 31800 Training loss 0.003793648211285472 Validation loss 0.044671688228845596 Accuracy 0.8892500400543213\n",
      "Iteration 31810 Training loss 5.721954585169442e-05 Validation loss 0.04460306465625763 Accuracy 0.890375018119812\n",
      "Iteration 31820 Training loss 4.088381683686748e-05 Validation loss 0.044684961438179016 Accuracy 0.8893750309944153\n",
      "Iteration 31830 Training loss 0.001312444219365716 Validation loss 0.044751349836587906 Accuracy 0.8891250491142273\n",
      "Iteration 31840 Training loss 0.0013158797519281507 Validation loss 0.044624537229537964 Accuracy 0.889750063419342\n",
      "Iteration 31850 Training loss 0.003806530265137553 Validation loss 0.04483184963464737 Accuracy 0.889750063419342\n",
      "Iteration 31860 Training loss 0.0013006787048652768 Validation loss 0.04463982582092285 Accuracy 0.8893750309944153\n",
      "Iteration 31870 Training loss 0.0012965728528797626 Validation loss 0.044638678431510925 Accuracy 0.889750063419342\n",
      "Iteration 31880 Training loss 0.0012930089142173529 Validation loss 0.04468910023570061 Accuracy 0.889875054359436\n",
      "Iteration 31890 Training loss 0.0013246008893474936 Validation loss 0.0446283333003521 Accuracy 0.8893750309944153\n",
      "Iteration 31900 Training loss 0.006322356406599283 Validation loss 0.04460518807172775 Accuracy 0.89000004529953\n",
      "Iteration 31910 Training loss 6.090385795687325e-05 Validation loss 0.04457685723900795 Accuracy 0.8905000686645508\n",
      "Iteration 31920 Training loss 6.022999878041446e-05 Validation loss 0.04462837427854538 Accuracy 0.889875054359436\n",
      "Iteration 31930 Training loss 0.001306941150687635 Validation loss 0.04462888464331627 Accuracy 0.889750063419342\n",
      "Iteration 31940 Training loss 3.70260386262089e-05 Validation loss 0.04465397447347641 Accuracy 0.8895000219345093\n",
      "Iteration 31950 Training loss 4.347831054474227e-05 Validation loss 0.04461701214313507 Accuracy 0.890250027179718\n",
      "Iteration 31960 Training loss 0.0012967869406566024 Validation loss 0.044686105102300644 Accuracy 0.8895000219345093\n",
      "Iteration 31970 Training loss 0.0012894492829218507 Validation loss 0.04463176801800728 Accuracy 0.890375018119812\n",
      "Iteration 31980 Training loss 0.0013034625444561243 Validation loss 0.04462729021906853 Accuracy 0.8905000686645508\n",
      "Iteration 31990 Training loss 0.0013201833935454488 Validation loss 0.0446651317179203 Accuracy 0.889750063419342\n",
      "Iteration 32000 Training loss 0.0025403008330613375 Validation loss 0.04465043172240257 Accuracy 0.8895000219345093\n",
      "Iteration 32010 Training loss 0.0013003208441659808 Validation loss 0.044596295803785324 Accuracy 0.8896250128746033\n",
      "Iteration 32020 Training loss 0.0012899142457172275 Validation loss 0.04460230469703674 Accuracy 0.89000004529953\n",
      "Iteration 32030 Training loss 5.364380558603443e-05 Validation loss 0.044643472880125046 Accuracy 0.889750063419342\n",
      "Iteration 32040 Training loss 0.0013102971715852618 Validation loss 0.04464048892259598 Accuracy 0.8893750309944153\n",
      "Iteration 32050 Training loss 0.0025402673054486513 Validation loss 0.044565945863723755 Accuracy 0.889750063419342\n",
      "Iteration 32060 Training loss 0.0012995317811146379 Validation loss 0.04460099712014198 Accuracy 0.8896250128746033\n",
      "Iteration 32070 Training loss 6.193825538503006e-05 Validation loss 0.044614262878894806 Accuracy 0.8896250128746033\n",
      "Iteration 32080 Training loss 0.0013052603462710977 Validation loss 0.04460941627621651 Accuracy 0.8895000219345093\n",
      "Iteration 32090 Training loss 4.673789953812957e-05 Validation loss 0.0445973165333271 Accuracy 0.889875054359436\n",
      "Iteration 32100 Training loss 0.0025660055689513683 Validation loss 0.04470761492848396 Accuracy 0.8892500400543213\n",
      "Iteration 32110 Training loss 0.0012979332823306322 Validation loss 0.044621553272008896 Accuracy 0.8896250128746033\n",
      "Iteration 32120 Training loss 0.0013331559021025896 Validation loss 0.044774916023015976 Accuracy 0.889750063419342\n",
      "Iteration 32130 Training loss 0.00381010165438056 Validation loss 0.04476432129740715 Accuracy 0.889750063419342\n",
      "Iteration 32140 Training loss 4.443634315975942e-05 Validation loss 0.0446273535490036 Accuracy 0.889875054359436\n",
      "Iteration 32150 Training loss 0.0025557747576385736 Validation loss 0.044649288058280945 Accuracy 0.8891250491142273\n",
      "Iteration 32160 Training loss 0.0013084369711577892 Validation loss 0.0446295291185379 Accuracy 0.8890000581741333\n",
      "Iteration 32170 Training loss 0.0025548257399350405 Validation loss 0.04464522749185562 Accuracy 0.8890000581741333\n",
      "Iteration 32180 Training loss 0.0013105060206726193 Validation loss 0.04464060440659523 Accuracy 0.8893750309944153\n",
      "Iteration 32190 Training loss 0.0013147641438990831 Validation loss 0.04461706057190895 Accuracy 0.890125036239624\n",
      "Iteration 32200 Training loss 0.0025551682338118553 Validation loss 0.04463040083646774 Accuracy 0.8891250491142273\n",
      "Iteration 32210 Training loss 0.005047052167356014 Validation loss 0.044558085501194 Accuracy 0.890375018119812\n",
      "Iteration 32220 Training loss 4.449781772564165e-05 Validation loss 0.044618863612413406 Accuracy 0.890250027179718\n",
      "Iteration 32230 Training loss 0.0013083982048556209 Validation loss 0.04457395523786545 Accuracy 0.890125036239624\n",
      "Iteration 32240 Training loss 0.002569988602772355 Validation loss 0.04459516704082489 Accuracy 0.890125036239624\n",
      "Iteration 32250 Training loss 0.0025605468545109034 Validation loss 0.044640328735113144 Accuracy 0.8892500400543213\n",
      "Iteration 32260 Training loss 0.0037963390350341797 Validation loss 0.044683702290058136 Accuracy 0.8891250491142273\n",
      "Iteration 32270 Training loss 0.002546844771131873 Validation loss 0.044613733887672424 Accuracy 0.889875054359436\n",
      "Iteration 32280 Training loss 0.0025964572560042143 Validation loss 0.04459291324019432 Accuracy 0.890250027179718\n",
      "Iteration 32290 Training loss 0.002541018184274435 Validation loss 0.044639769941568375 Accuracy 0.8895000219345093\n",
      "Iteration 32300 Training loss 5.512218194780871e-05 Validation loss 0.04468303173780441 Accuracy 0.8893750309944153\n",
      "Iteration 32310 Training loss 0.001321682590059936 Validation loss 0.04461337625980377 Accuracy 0.890125036239624\n",
      "Iteration 32320 Training loss 0.0013005963992327452 Validation loss 0.04463491961359978 Accuracy 0.8893750309944153\n",
      "Iteration 32330 Training loss 0.0038087032735347748 Validation loss 0.044671788811683655 Accuracy 0.8892500400543213\n",
      "Iteration 32340 Training loss 0.006284174043685198 Validation loss 0.04461418837308884 Accuracy 0.889875054359436\n",
      "Iteration 32350 Training loss 0.0012979790335521102 Validation loss 0.04462471231818199 Accuracy 0.889750063419342\n",
      "Iteration 32360 Training loss 3.094655039603822e-05 Validation loss 0.04462679475545883 Accuracy 0.8895000219345093\n",
      "Iteration 32370 Training loss 0.005045637954026461 Validation loss 0.044618502259254456 Accuracy 0.89000004529953\n",
      "Iteration 32380 Training loss 0.0012936708517372608 Validation loss 0.044652484357357025 Accuracy 0.89000004529953\n",
      "Iteration 32390 Training loss 3.781871419050731e-05 Validation loss 0.044678010046482086 Accuracy 0.889750063419342\n",
      "Iteration 32400 Training loss 0.0013152218889445066 Validation loss 0.044728923588991165 Accuracy 0.8893750309944153\n",
      "Iteration 32410 Training loss 0.0012917111162096262 Validation loss 0.04471927508711815 Accuracy 0.8895000219345093\n",
      "Iteration 32420 Training loss 0.0013003594940528274 Validation loss 0.0446985587477684 Accuracy 0.8895000219345093\n",
      "Iteration 32430 Training loss 0.0025605526752769947 Validation loss 0.044711560010910034 Accuracy 0.8892500400543213\n",
      "Iteration 32440 Training loss 0.0012933657271787524 Validation loss 0.04469321668148041 Accuracy 0.8893750309944153\n",
      "Iteration 32450 Training loss 0.0025715488009154797 Validation loss 0.04476552829146385 Accuracy 0.8888750672340393\n",
      "Iteration 32460 Training loss 5.1518709369702265e-05 Validation loss 0.04470779001712799 Accuracy 0.8887500166893005\n",
      "Iteration 32470 Training loss 0.003810320980846882 Validation loss 0.044673219323158264 Accuracy 0.8896250128746033\n",
      "Iteration 32480 Training loss 0.0013064208906143904 Validation loss 0.04471983015537262 Accuracy 0.8885000348091125\n",
      "Iteration 32490 Training loss 0.0013146663550287485 Validation loss 0.044690728187561035 Accuracy 0.8893750309944153\n",
      "Iteration 32500 Training loss 0.0013319903519004583 Validation loss 0.04475770145654678 Accuracy 0.8886250257492065\n",
      "Iteration 32510 Training loss 4.477412949199788e-05 Validation loss 0.044835492968559265 Accuracy 0.8895000219345093\n",
      "Iteration 32520 Training loss 0.002552312333136797 Validation loss 0.04467092826962471 Accuracy 0.889875054359436\n",
      "Iteration 32530 Training loss 5.139020140632056e-05 Validation loss 0.04472845420241356 Accuracy 0.8888750672340393\n",
      "Iteration 32540 Training loss 0.0013026611413806677 Validation loss 0.04468008130788803 Accuracy 0.8891250491142273\n",
      "Iteration 32550 Training loss 0.0012939880834892392 Validation loss 0.04476632922887802 Accuracy 0.89000004529953\n",
      "Iteration 32560 Training loss 0.0013233096105977893 Validation loss 0.04468829929828644 Accuracy 0.8890000581741333\n",
      "Iteration 32570 Training loss 0.0025406198110431433 Validation loss 0.04469230771064758 Accuracy 0.8887500166893005\n",
      "Iteration 32580 Training loss 5.6645261793164536e-05 Validation loss 0.0446368046104908 Accuracy 0.8893750309944153\n",
      "Iteration 32590 Training loss 0.0025509349070489407 Validation loss 0.04467890411615372 Accuracy 0.889875054359436\n",
      "Iteration 32600 Training loss 0.0038044380489736795 Validation loss 0.04464118555188179 Accuracy 0.889875054359436\n",
      "Iteration 32610 Training loss 0.001303651719354093 Validation loss 0.04469401761889458 Accuracy 0.8892500400543213\n",
      "Iteration 32620 Training loss 5.35604267497547e-05 Validation loss 0.04474529251456261 Accuracy 0.8888750672340393\n",
      "Iteration 32630 Training loss 3.760569597943686e-05 Validation loss 0.044708043336868286 Accuracy 0.8895000219345093\n",
      "Iteration 32640 Training loss 0.0012995218858122826 Validation loss 0.04470059275627136 Accuracy 0.889750063419342\n",
      "Iteration 32650 Training loss 0.0012884450843557715 Validation loss 0.04471338912844658 Accuracy 0.890250027179718\n",
      "Iteration 32660 Training loss 6.413677328964695e-05 Validation loss 0.04473983868956566 Accuracy 0.8892500400543213\n",
      "Iteration 32670 Training loss 0.0012835371308028698 Validation loss 0.04470033571124077 Accuracy 0.8886250257492065\n",
      "Iteration 32680 Training loss 4.7299796278821304e-05 Validation loss 0.0448332279920578 Accuracy 0.8888750672340393\n",
      "Iteration 32690 Training loss 0.0012881497386842966 Validation loss 0.044760093092918396 Accuracy 0.89000004529953\n",
      "Iteration 32700 Training loss 0.0012835271190851927 Validation loss 0.0447547547519207 Accuracy 0.8890000581741333\n",
      "Iteration 32710 Training loss 0.002543041715398431 Validation loss 0.04469592496752739 Accuracy 0.8895000219345093\n",
      "Iteration 32720 Training loss 0.002543965121731162 Validation loss 0.0447043813765049 Accuracy 0.8895000219345093\n",
      "Iteration 32730 Training loss 5.572852387558669e-05 Validation loss 0.0447566993534565 Accuracy 0.8891250491142273\n",
      "Iteration 32740 Training loss 0.0037989323027431965 Validation loss 0.04476388543844223 Accuracy 0.8891250491142273\n",
      "Iteration 32750 Training loss 3.742162880371325e-05 Validation loss 0.04471466317772865 Accuracy 0.8893750309944153\n",
      "Iteration 32760 Training loss 4.134395203436725e-05 Validation loss 0.04477466642856598 Accuracy 0.8887500166893005\n",
      "Iteration 32770 Training loss 4.1096976929111406e-05 Validation loss 0.04466799274086952 Accuracy 0.889750063419342\n",
      "Iteration 32780 Training loss 5.147565025254153e-05 Validation loss 0.04466452822089195 Accuracy 0.8893750309944153\n",
      "Iteration 32790 Training loss 0.0037940777838230133 Validation loss 0.04468763247132301 Accuracy 0.8892500400543213\n",
      "Iteration 32800 Training loss 3.9893900975584984e-05 Validation loss 0.04468972980976105 Accuracy 0.8893750309944153\n",
      "Iteration 32810 Training loss 0.0025384831242263317 Validation loss 0.044764790683984756 Accuracy 0.8885000348091125\n",
      "Iteration 32820 Training loss 0.0013130118604749441 Validation loss 0.04464105889201164 Accuracy 0.8893750309944153\n",
      "Iteration 32830 Training loss 0.005043577868491411 Validation loss 0.044717058539390564 Accuracy 0.8888750672340393\n",
      "Iteration 32840 Training loss 4.608387826010585e-05 Validation loss 0.044690463691949844 Accuracy 0.8892500400543213\n",
      "Iteration 32850 Training loss 0.002552054123952985 Validation loss 0.04468955472111702 Accuracy 0.890250027179718\n",
      "Iteration 32860 Training loss 0.006288615986704826 Validation loss 0.04475177824497223 Accuracy 0.8885000348091125\n",
      "Iteration 32870 Training loss 0.00254085217602551 Validation loss 0.0446942001581192 Accuracy 0.8892500400543213\n",
      "Iteration 32880 Training loss 0.0038002924993634224 Validation loss 0.04470497742295265 Accuracy 0.8896250128746033\n",
      "Iteration 32890 Training loss 0.002563584130257368 Validation loss 0.044712524861097336 Accuracy 0.8890000581741333\n",
      "Iteration 32900 Training loss 0.003789515933021903 Validation loss 0.04470779746770859 Accuracy 0.8895000219345093\n",
      "Iteration 32910 Training loss 3.602910146582872e-05 Validation loss 0.04472193494439125 Accuracy 0.8895000219345093\n",
      "Iteration 32920 Training loss 0.0012755681527778506 Validation loss 0.04471103101968765 Accuracy 0.89000004529953\n",
      "Iteration 32930 Training loss 0.0012808538740500808 Validation loss 0.0447017140686512 Accuracy 0.8893750309944153\n",
      "Iteration 32940 Training loss 2.6959552997141145e-05 Validation loss 0.04471356049180031 Accuracy 0.8893750309944153\n",
      "Iteration 32950 Training loss 0.003800726495683193 Validation loss 0.044644564390182495 Accuracy 0.89000004529953\n",
      "Iteration 32960 Training loss 0.0012809725012630224 Validation loss 0.04466948285698891 Accuracy 0.889750063419342\n",
      "Iteration 32970 Training loss 0.0012884417083114386 Validation loss 0.04469641298055649 Accuracy 0.889750063419342\n",
      "Iteration 32980 Training loss 5.742989378632046e-05 Validation loss 0.044736526906490326 Accuracy 0.8886250257492065\n",
      "Iteration 32990 Training loss 2.3821945433155634e-05 Validation loss 0.04468474164605141 Accuracy 0.8890000581741333\n",
      "Iteration 33000 Training loss 4.0850653022062033e-05 Validation loss 0.0446857213973999 Accuracy 0.8891250491142273\n",
      "Iteration 33010 Training loss 0.0012900648871436715 Validation loss 0.044655755162239075 Accuracy 0.8895000219345093\n",
      "Iteration 33020 Training loss 0.0012868719641119242 Validation loss 0.04464847594499588 Accuracy 0.8895000219345093\n",
      "Iteration 33030 Training loss 3.5654284147312865e-05 Validation loss 0.04470639303326607 Accuracy 0.8887500166893005\n",
      "Iteration 33040 Training loss 3.784333966905251e-05 Validation loss 0.04467314854264259 Accuracy 0.8892500400543213\n",
      "Iteration 33050 Training loss 0.001291747554205358 Validation loss 0.04471227154135704 Accuracy 0.8886250257492065\n",
      "Iteration 33060 Training loss 0.001308897160924971 Validation loss 0.04470451548695564 Accuracy 0.8887500166893005\n",
      "Iteration 33070 Training loss 6.412083166651428e-05 Validation loss 0.04473504424095154 Accuracy 0.8890000581741333\n",
      "Iteration 33080 Training loss 0.003795303637161851 Validation loss 0.0446648970246315 Accuracy 0.89000004529953\n",
      "Iteration 33090 Training loss 3.489647860988043e-05 Validation loss 0.044723786413669586 Accuracy 0.8890000581741333\n",
      "Iteration 33100 Training loss 0.002551581710577011 Validation loss 0.04469205066561699 Accuracy 0.8891250491142273\n",
      "Iteration 33110 Training loss 0.002568569965660572 Validation loss 0.044682517647743225 Accuracy 0.8892500400543213\n",
      "Iteration 33120 Training loss 3.781207851716317e-05 Validation loss 0.044752899557352066 Accuracy 0.8893750309944153\n",
      "Iteration 33130 Training loss 0.002544515300542116 Validation loss 0.04472722113132477 Accuracy 0.8895000219345093\n",
      "Iteration 33140 Training loss 0.0025524550583213568 Validation loss 0.04470590502023697 Accuracy 0.8896250128746033\n",
      "Iteration 33150 Training loss 0.0013005159562453628 Validation loss 0.04470517858862877 Accuracy 0.8888750672340393\n",
      "Iteration 33160 Training loss 0.0025373667012900114 Validation loss 0.04467425122857094 Accuracy 0.8896250128746033\n",
      "Iteration 33170 Training loss 0.005036113318055868 Validation loss 0.04472116380929947 Accuracy 0.8888750672340393\n",
      "Iteration 33180 Training loss 0.0012968286173418164 Validation loss 0.04469369724392891 Accuracy 0.889750063419342\n",
      "Iteration 33190 Training loss 0.0013085412792861462 Validation loss 0.044679898768663406 Accuracy 0.889750063419342\n",
      "Iteration 33200 Training loss 0.0025366288609802723 Validation loss 0.04470933601260185 Accuracy 0.889875054359436\n",
      "Iteration 33210 Training loss 0.0025677711237221956 Validation loss 0.044643647968769073 Accuracy 0.8907500505447388\n",
      "Iteration 33220 Training loss 0.0012923189206048846 Validation loss 0.04470181092619896 Accuracy 0.8887500166893005\n",
      "Iteration 33230 Training loss 0.0025465572252869606 Validation loss 0.04472219944000244 Accuracy 0.8888750672340393\n",
      "Iteration 33240 Training loss 0.00255190790630877 Validation loss 0.04470395669341087 Accuracy 0.8891250491142273\n",
      "Iteration 33250 Training loss 0.0013001891784369946 Validation loss 0.04474581032991409 Accuracy 0.8892500400543213\n",
      "Iteration 33260 Training loss 0.0025403897743672132 Validation loss 0.0446755476295948 Accuracy 0.889750063419342\n",
      "Iteration 33270 Training loss 0.001292182132601738 Validation loss 0.044701166450977325 Accuracy 0.8895000219345093\n",
      "Iteration 33280 Training loss 0.0037981404457241297 Validation loss 0.044703252613544464 Accuracy 0.889750063419342\n",
      "Iteration 33290 Training loss 7.244434527819976e-05 Validation loss 0.04474271833896637 Accuracy 0.8892500400543213\n",
      "Iteration 33300 Training loss 0.002550916513428092 Validation loss 0.04483357444405556 Accuracy 0.8890000581741333\n",
      "Iteration 33310 Training loss 0.0012877037515863776 Validation loss 0.04473332315683365 Accuracy 0.8896250128746033\n",
      "Iteration 33320 Training loss 0.0012988136149942875 Validation loss 0.04473687708377838 Accuracy 0.8892500400543213\n",
      "Iteration 33330 Training loss 3.200518767698668e-05 Validation loss 0.044782642275094986 Accuracy 0.8888750672340393\n",
      "Iteration 33340 Training loss 3.9028000173857436e-05 Validation loss 0.04480544477701187 Accuracy 0.8887500166893005\n",
      "Iteration 33350 Training loss 0.002552430611103773 Validation loss 0.044781316071748734 Accuracy 0.8890000581741333\n",
      "Iteration 33360 Training loss 0.00254556885920465 Validation loss 0.04474348574876785 Accuracy 0.8896250128746033\n",
      "Iteration 33370 Training loss 0.003797394223511219 Validation loss 0.044715844094753265 Accuracy 0.8892500400543213\n",
      "Iteration 33380 Training loss 5.1152845117030665e-05 Validation loss 0.044743966311216354 Accuracy 0.89000004529953\n",
      "Iteration 33390 Training loss 5.70979063923005e-05 Validation loss 0.04481358453631401 Accuracy 0.8890000581741333\n",
      "Iteration 33400 Training loss 0.0013006628723815084 Validation loss 0.044712845236063004 Accuracy 0.89000004529953\n",
      "Iteration 33410 Training loss 0.0025513824075460434 Validation loss 0.044742465019226074 Accuracy 0.8892500400543213\n",
      "Iteration 33420 Training loss 0.002540954854339361 Validation loss 0.04469430819153786 Accuracy 0.889750063419342\n",
      "Iteration 33430 Training loss 0.007552781142294407 Validation loss 0.044713832437992096 Accuracy 0.889875054359436\n",
      "Iteration 33440 Training loss 0.0012968898518010974 Validation loss 0.044740717858076096 Accuracy 0.889875054359436\n",
      "Iteration 33450 Training loss 0.0013164144475013018 Validation loss 0.044731736183166504 Accuracy 0.889750063419342\n",
      "Iteration 33460 Training loss 0.0012895321706309915 Validation loss 0.04472212493419647 Accuracy 0.8896250128746033\n",
      "Iteration 33470 Training loss 0.0025442002806812525 Validation loss 0.044716380536556244 Accuracy 0.8896250128746033\n",
      "Iteration 33480 Training loss 0.0012915291590616107 Validation loss 0.04472602903842926 Accuracy 0.8892500400543213\n",
      "Iteration 33490 Training loss 0.0025322847068309784 Validation loss 0.04470822587609291 Accuracy 0.8895000219345093\n",
      "Iteration 33500 Training loss 0.002541718538850546 Validation loss 0.044748954474925995 Accuracy 0.8892500400543213\n",
      "Iteration 33510 Training loss 0.0012963248882442713 Validation loss 0.04471924901008606 Accuracy 0.8885000348091125\n",
      "Iteration 33520 Training loss 0.0013127063866704702 Validation loss 0.04475365951657295 Accuracy 0.8892500400543213\n",
      "Iteration 33530 Training loss 0.002536139218136668 Validation loss 0.04475824907422066 Accuracy 0.8895000219345093\n",
      "Iteration 33540 Training loss 0.0013288392219692469 Validation loss 0.04477113112807274 Accuracy 0.8893750309944153\n",
      "Iteration 33550 Training loss 0.0012974884593859315 Validation loss 0.04475916177034378 Accuracy 0.8895000219345093\n",
      "Iteration 33560 Training loss 0.0013050228590145707 Validation loss 0.04474063217639923 Accuracy 0.8893750309944153\n",
      "Iteration 33570 Training loss 0.0025481104385107756 Validation loss 0.04474475234746933 Accuracy 0.8891250491142273\n",
      "Iteration 33580 Training loss 4.3414009269326925e-05 Validation loss 0.0447268933057785 Accuracy 0.8893750309944153\n",
      "Iteration 33590 Training loss 4.716340481536463e-05 Validation loss 0.0447445809841156 Accuracy 0.8891250491142273\n",
      "Iteration 33600 Training loss 0.0012816616799682379 Validation loss 0.044731441885232925 Accuracy 0.8888750672340393\n",
      "Iteration 33610 Training loss 3.724608905031346e-05 Validation loss 0.044727176427841187 Accuracy 0.8887500166893005\n",
      "Iteration 33620 Training loss 0.0025409439112991095 Validation loss 0.04479184001684189 Accuracy 0.8888750672340393\n",
      "Iteration 33630 Training loss 5.288835018291138e-05 Validation loss 0.04475130885839462 Accuracy 0.8895000219345093\n",
      "Iteration 33640 Training loss 0.0025448936503380537 Validation loss 0.044764336198568344 Accuracy 0.8888750672340393\n",
      "Iteration 33650 Training loss 7.18884402886033e-05 Validation loss 0.04473644867539406 Accuracy 0.8890000581741333\n",
      "Iteration 33660 Training loss 0.001295312773436308 Validation loss 0.04472685977816582 Accuracy 0.8890000581741333\n",
      "Iteration 33670 Training loss 0.0025463616475462914 Validation loss 0.04479154571890831 Accuracy 0.8887500166893005\n",
      "Iteration 33680 Training loss 4.7753648686921224e-05 Validation loss 0.04475482925772667 Accuracy 0.8893750309944153\n",
      "Iteration 33690 Training loss 0.0025479018222540617 Validation loss 0.04470491409301758 Accuracy 0.8896250128746033\n",
      "Iteration 33700 Training loss 3.6132027162238955e-05 Validation loss 0.04471176117658615 Accuracy 0.8887500166893005\n",
      "Iteration 33710 Training loss 0.005056239198893309 Validation loss 0.04477499797940254 Accuracy 0.8888750672340393\n",
      "Iteration 33720 Training loss 0.0012835204834118485 Validation loss 0.04470210149884224 Accuracy 0.8890000581741333\n",
      "Iteration 33730 Training loss 0.0025287049356848 Validation loss 0.04468478262424469 Accuracy 0.8891250491142273\n",
      "Iteration 33740 Training loss 0.0012958034640178084 Validation loss 0.04474598169326782 Accuracy 0.8891250491142273\n",
      "Iteration 33750 Training loss 0.00130844593513757 Validation loss 0.044757403433322906 Accuracy 0.8890000581741333\n",
      "Iteration 33760 Training loss 5.0851664127549157e-05 Validation loss 0.04469358175992966 Accuracy 0.8887500166893005\n",
      "Iteration 33770 Training loss 0.001301233540289104 Validation loss 0.04472918063402176 Accuracy 0.8895000219345093\n",
      "Iteration 33780 Training loss 4.220804476062767e-05 Validation loss 0.04474054276943207 Accuracy 0.8893750309944153\n",
      "Iteration 33790 Training loss 0.0012951522367075086 Validation loss 0.0447068028151989 Accuracy 0.8895000219345093\n",
      "Iteration 33800 Training loss 0.0025326209142804146 Validation loss 0.04469816014170647 Accuracy 0.8891250491142273\n",
      "Iteration 33810 Training loss 0.0012840655399486423 Validation loss 0.04478299245238304 Accuracy 0.8890000581741333\n",
      "Iteration 33820 Training loss 5.169564610696398e-05 Validation loss 0.0447377972304821 Accuracy 0.8893750309944153\n",
      "Iteration 33830 Training loss 0.003794437274336815 Validation loss 0.044722940772771835 Accuracy 0.8893750309944153\n",
      "Iteration 33840 Training loss 0.0012864682357758284 Validation loss 0.044746093451976776 Accuracy 0.889875054359436\n",
      "Iteration 33850 Training loss 5.030793909099884e-05 Validation loss 0.04480224847793579 Accuracy 0.8896250128746033\n",
      "Iteration 33860 Training loss 0.0012913858518004417 Validation loss 0.044753290712833405 Accuracy 0.8893750309944153\n",
      "Iteration 33870 Training loss 0.003783739637583494 Validation loss 0.04475950822234154 Accuracy 0.8890000581741333\n",
      "Iteration 33880 Training loss 0.0012837938265874982 Validation loss 0.044704411178827286 Accuracy 0.89000004529953\n",
      "Iteration 33890 Training loss 0.0025510750710964203 Validation loss 0.04476240649819374 Accuracy 0.8895000219345093\n",
      "Iteration 33900 Training loss 0.0025511044077575207 Validation loss 0.044760365039110184 Accuracy 0.8895000219345093\n",
      "Iteration 33910 Training loss 4.34935609519016e-05 Validation loss 0.0447772815823555 Accuracy 0.8895000219345093\n",
      "Iteration 33920 Training loss 3.316714719403535e-05 Validation loss 0.044755060225725174 Accuracy 0.8893750309944153\n",
      "Iteration 33930 Training loss 0.0012823769357055426 Validation loss 0.044716812670230865 Accuracy 0.8893750309944153\n",
      "Iteration 33940 Training loss 0.001298793125897646 Validation loss 0.04470434784889221 Accuracy 0.8896250128746033\n",
      "Iteration 33950 Training loss 0.001296714530326426 Validation loss 0.04470236971974373 Accuracy 0.8893750309944153\n",
      "Iteration 33960 Training loss 0.003788762027397752 Validation loss 0.04475865513086319 Accuracy 0.8890000581741333\n",
      "Iteration 33970 Training loss 0.0012895853724330664 Validation loss 0.04474007338285446 Accuracy 0.889875054359436\n",
      "Iteration 33980 Training loss 0.006271817721426487 Validation loss 0.04479831084609032 Accuracy 0.8891250491142273\n",
      "Iteration 33990 Training loss 3.65363885066472e-05 Validation loss 0.04477325826883316 Accuracy 0.8895000219345093\n",
      "Iteration 34000 Training loss 0.0037870381493121386 Validation loss 0.04479387030005455 Accuracy 0.8892500400543213\n",
      "Iteration 34010 Training loss 0.0037951848935335875 Validation loss 0.04477522522211075 Accuracy 0.8890000581741333\n",
      "Iteration 34020 Training loss 0.001308239414356649 Validation loss 0.04479913413524628 Accuracy 0.8895000219345093\n",
      "Iteration 34030 Training loss 0.001290576416067779 Validation loss 0.04494452103972435 Accuracy 0.8887500166893005\n",
      "Iteration 34040 Training loss 4.197822272544727e-05 Validation loss 0.04488120600581169 Accuracy 0.8891250491142273\n",
      "Iteration 34050 Training loss 4.88143450638745e-05 Validation loss 0.044830575585365295 Accuracy 0.8890000581741333\n",
      "Iteration 34060 Training loss 5.379873255151324e-05 Validation loss 0.044861406087875366 Accuracy 0.8886250257492065\n",
      "Iteration 34070 Training loss 0.001304836361669004 Validation loss 0.04479479044675827 Accuracy 0.8887500166893005\n",
      "Iteration 34080 Training loss 5.7664808991830796e-05 Validation loss 0.04479873180389404 Accuracy 0.8893750309944153\n",
      "Iteration 34090 Training loss 0.0012857686961069703 Validation loss 0.04479629173874855 Accuracy 0.8893750309944153\n",
      "Iteration 34100 Training loss 0.003805493703112006 Validation loss 0.0447748526930809 Accuracy 0.8890000581741333\n",
      "Iteration 34110 Training loss 3.944894706364721e-05 Validation loss 0.044740255922079086 Accuracy 0.889875054359436\n",
      "Iteration 34120 Training loss 0.0025500792544335127 Validation loss 0.04473850131034851 Accuracy 0.889750063419342\n",
      "Iteration 34130 Training loss 0.0012905215844511986 Validation loss 0.04481631517410278 Accuracy 0.8890000581741333\n",
      "Iteration 34140 Training loss 5.3838117310078815e-05 Validation loss 0.04484237730503082 Accuracy 0.8893750309944153\n",
      "Iteration 34150 Training loss 0.0012948712101206183 Validation loss 0.04481660574674606 Accuracy 0.8891250491142273\n",
      "Iteration 34160 Training loss 0.002531236968934536 Validation loss 0.044885918498039246 Accuracy 0.8892500400543213\n",
      "Iteration 34170 Training loss 0.0037932065315544605 Validation loss 0.04482744634151459 Accuracy 0.889750063419342\n",
      "Iteration 34180 Training loss 0.0025419534649699926 Validation loss 0.044784966856241226 Accuracy 0.89000004529953\n",
      "Iteration 34190 Training loss 0.002434264402836561 Validation loss 0.04477416351437569 Accuracy 0.8892500400543213\n",
      "Iteration 34200 Training loss 0.003788098692893982 Validation loss 0.04487938433885574 Accuracy 0.8893750309944153\n",
      "Iteration 34210 Training loss 0.0013055946910753846 Validation loss 0.04477516561746597 Accuracy 0.8896250128746033\n",
      "Iteration 34220 Training loss 0.0012936379062011838 Validation loss 0.04476713761687279 Accuracy 0.8896250128746033\n",
      "Iteration 34230 Training loss 4.996429561288096e-05 Validation loss 0.04474993795156479 Accuracy 0.890125036239624\n",
      "Iteration 34240 Training loss 0.0013030886184424162 Validation loss 0.04473336786031723 Accuracy 0.889750063419342\n",
      "Iteration 34250 Training loss 0.0037933194544166327 Validation loss 0.04477642849087715 Accuracy 0.8892500400543213\n",
      "Iteration 34260 Training loss 0.005029913038015366 Validation loss 0.044803500175476074 Accuracy 0.8891250491142273\n",
      "Iteration 34270 Training loss 0.0025265144649893045 Validation loss 0.0447285920381546 Accuracy 0.889875054359436\n",
      "Iteration 34280 Training loss 4.4854110456071794e-05 Validation loss 0.04478152096271515 Accuracy 0.8895000219345093\n",
      "Iteration 34290 Training loss 0.0013047355460003018 Validation loss 0.04476715624332428 Accuracy 0.889875054359436\n",
      "Iteration 34300 Training loss 3.669986472232267e-05 Validation loss 0.044775690883398056 Accuracy 0.8893750309944153\n",
      "Iteration 34310 Training loss 6.0996022511972114e-05 Validation loss 0.04473700001835823 Accuracy 0.8892500400543213\n",
      "Iteration 34320 Training loss 0.0025404782500118017 Validation loss 0.04473032057285309 Accuracy 0.8896250128746033\n",
      "Iteration 34330 Training loss 3.841355282929726e-05 Validation loss 0.04473842680454254 Accuracy 0.8891250491142273\n",
      "Iteration 34340 Training loss 0.003771818708628416 Validation loss 0.04475171118974686 Accuracy 0.8893750309944153\n",
      "Iteration 34350 Training loss 0.0025370868388563395 Validation loss 0.04481117054820061 Accuracy 0.8885000348091125\n",
      "Iteration 34360 Training loss 0.0013094107853248715 Validation loss 0.04476098343729973 Accuracy 0.8891250491142273\n",
      "Iteration 34370 Training loss 0.00254974327981472 Validation loss 0.04472871124744415 Accuracy 0.889750063419342\n",
      "Iteration 34380 Training loss 0.0038033821620047092 Validation loss 0.04475311562418938 Accuracy 0.8896250128746033\n",
      "Iteration 34390 Training loss 0.0012794510694220662 Validation loss 0.044739823788404465 Accuracy 0.8895000219345093\n",
      "Iteration 34400 Training loss 0.0025472582783550024 Validation loss 0.044806674122810364 Accuracy 0.8891250491142273\n",
      "Iteration 34410 Training loss 0.0025646169669926167 Validation loss 0.04475702717900276 Accuracy 0.8892500400543213\n",
      "Iteration 34420 Training loss 3.3154698030557483e-05 Validation loss 0.04474472254514694 Accuracy 0.89000004529953\n",
      "Iteration 34430 Training loss 0.001299485331401229 Validation loss 0.044923074543476105 Accuracy 0.8885000348091125\n",
      "Iteration 34440 Training loss 0.0018337409710511565 Validation loss 0.04497714340686798 Accuracy 0.8891250491142273\n",
      "Iteration 34450 Training loss 0.06738339364528656 Validation loss 0.08128704130649567 Accuracy 0.8168750405311584\n",
      "Iteration 34460 Training loss 0.024981806054711342 Validation loss 0.051686450839042664 Accuracy 0.8748750686645508\n",
      "Iteration 34470 Training loss 0.05243104696273804 Validation loss 0.09244436770677567 Accuracy 0.7826250195503235\n",
      "Iteration 34480 Training loss 0.04628539830446243 Validation loss 0.07219590991735458 Accuracy 0.8336250185966492\n",
      "Iteration 34490 Training loss 0.022496633231639862 Validation loss 0.05498144030570984 Accuracy 0.8603750467300415\n",
      "Iteration 34500 Training loss 0.005990155041217804 Validation loss 0.04624498263001442 Accuracy 0.8820000290870667\n",
      "Iteration 34510 Training loss 0.004096603486686945 Validation loss 0.04518434777855873 Accuracy 0.8860000371932983\n",
      "Iteration 34520 Training loss 0.0030199687462300062 Validation loss 0.04435604438185692 Accuracy 0.8876250386238098\n",
      "Iteration 34530 Training loss 0.0007597261574119329 Validation loss 0.04421623796224594 Accuracy 0.8878750205039978\n",
      "Iteration 34540 Training loss 0.0011711700353771448 Validation loss 0.04431601241230965 Accuracy 0.8863750696182251\n",
      "Iteration 34550 Training loss 0.004137708805501461 Validation loss 0.044538069516420364 Accuracy 0.8865000605583191\n",
      "Iteration 34560 Training loss 0.0031154167372733355 Validation loss 0.04411574825644493 Accuracy 0.8893750309944153\n",
      "Iteration 34570 Training loss 0.003706116694957018 Validation loss 0.0450136661529541 Accuracy 0.8856250643730164\n",
      "Iteration 34580 Training loss 0.0016878853784874082 Validation loss 0.044441673904657364 Accuracy 0.8855000138282776\n",
      "Iteration 34590 Training loss 0.003050814149901271 Validation loss 0.0446651317179203 Accuracy 0.8855000138282776\n",
      "Iteration 34600 Training loss 0.0014889599988237023 Validation loss 0.044442836195230484 Accuracy 0.890125036239624\n",
      "Iteration 34610 Training loss 0.0002450106549076736 Validation loss 0.04452131688594818 Accuracy 0.8863750696182251\n",
      "Iteration 34620 Training loss 0.001718749525025487 Validation loss 0.04404409974813461 Accuracy 0.8891250491142273\n",
      "Iteration 34630 Training loss 0.002929053734987974 Validation loss 0.044002633541822433 Accuracy 0.8887500166893005\n",
      "Iteration 34640 Training loss 0.00399360153824091 Validation loss 0.044282615184783936 Accuracy 0.8868750333786011\n",
      "Iteration 34650 Training loss 0.0004740720905829221 Validation loss 0.04418419674038887 Accuracy 0.8888750672340393\n",
      "Iteration 34660 Training loss 0.00025578265194781125 Validation loss 0.04421507567167282 Accuracy 0.8888750672340393\n",
      "Iteration 34670 Training loss 0.0002339972706977278 Validation loss 0.04434834420681 Accuracy 0.8877500295639038\n",
      "Iteration 34680 Training loss 0.001362629234790802 Validation loss 0.0443594753742218 Accuracy 0.8878750205039978\n",
      "Iteration 34690 Training loss 0.0014650203520432115 Validation loss 0.04460509121417999 Accuracy 0.8873750567436218\n",
      "Iteration 34700 Training loss 0.0014367912663146853 Validation loss 0.0442177914083004 Accuracy 0.8895000219345093\n",
      "Iteration 34710 Training loss 0.002802714006975293 Validation loss 0.04430480673909187 Accuracy 0.8891250491142273\n",
      "Iteration 34720 Training loss 0.00014124420704320073 Validation loss 0.04424913972616196 Accuracy 0.8893750309944153\n",
      "Iteration 34730 Training loss 0.00019123737001791596 Validation loss 0.044182032346725464 Accuracy 0.8891250491142273\n",
      "Iteration 34740 Training loss 0.0013739620335400105 Validation loss 0.04433991760015488 Accuracy 0.890250027179718\n",
      "Iteration 34750 Training loss 0.0014019679510965943 Validation loss 0.04434279724955559 Accuracy 0.8890000581741333\n",
      "Iteration 34760 Training loss 0.00010764101170934737 Validation loss 0.04437980800867081 Accuracy 0.8893750309944153\n",
      "Iteration 34770 Training loss 0.0013863500207662582 Validation loss 0.04430146887898445 Accuracy 0.8891250491142273\n",
      "Iteration 34780 Training loss 0.00017872959142550826 Validation loss 0.044454485177993774 Accuracy 0.8886250257492065\n",
      "Iteration 34790 Training loss 0.003870239481329918 Validation loss 0.044554781168699265 Accuracy 0.8871250152587891\n",
      "Iteration 34800 Training loss 8.61632070154883e-05 Validation loss 0.044441357254981995 Accuracy 0.8885000348091125\n",
      "Iteration 34810 Training loss 0.00387199386022985 Validation loss 0.04443695396184921 Accuracy 0.8882500529289246\n",
      "Iteration 34820 Training loss 0.00012872619845438749 Validation loss 0.04437864199280739 Accuracy 0.8883750438690186\n",
      "Iteration 34830 Training loss 0.002598320599645376 Validation loss 0.044481392949819565 Accuracy 0.8881250619888306\n",
      "Iteration 34840 Training loss 0.001315475208684802 Validation loss 0.044676996767520905 Accuracy 0.8891250491142273\n",
      "Iteration 34850 Training loss 0.006409635301679373 Validation loss 0.04441208019852638 Accuracy 0.8880000710487366\n",
      "Iteration 34860 Training loss 0.00015212490689009428 Validation loss 0.04438968747854233 Accuracy 0.8886250257492065\n",
      "Iteration 34870 Training loss 0.0013718721456825733 Validation loss 0.04437911882996559 Accuracy 0.8891250491142273\n",
      "Iteration 34880 Training loss 9.881928417598829e-05 Validation loss 0.04433668032288551 Accuracy 0.8883750438690186\n",
      "Iteration 34890 Training loss 0.0013284437591210008 Validation loss 0.04439721256494522 Accuracy 0.8882500529289246\n",
      "Iteration 34900 Training loss 7.074438326526433e-05 Validation loss 0.0443902388215065 Accuracy 0.8883750438690186\n",
      "Iteration 34910 Training loss 9.140728070633486e-05 Validation loss 0.044557616114616394 Accuracy 0.8877500295639038\n",
      "Iteration 34920 Training loss 0.001311318832449615 Validation loss 0.04450454190373421 Accuracy 0.8878750205039978\n",
      "Iteration 34930 Training loss 0.00014210348308552057 Validation loss 0.04436255991458893 Accuracy 0.8888750672340393\n",
      "Iteration 34940 Training loss 0.0026319383177906275 Validation loss 0.04457053542137146 Accuracy 0.8883750438690186\n",
      "Iteration 34950 Training loss 6.0690101236104965e-05 Validation loss 0.04461056739091873 Accuracy 0.890375018119812\n",
      "Iteration 34960 Training loss 7.781497697578743e-05 Validation loss 0.044604942202568054 Accuracy 0.8882500529289246\n",
      "Iteration 34970 Training loss 0.002590719610452652 Validation loss 0.04443640261888504 Accuracy 0.8891250491142273\n",
      "Iteration 34980 Training loss 0.0013344282051548362 Validation loss 0.04436686635017395 Accuracy 0.8891250491142273\n",
      "Iteration 34990 Training loss 0.001342633506283164 Validation loss 0.04431217536330223 Accuracy 0.8896250128746033\n",
      "Iteration 35000 Training loss 0.006336011923849583 Validation loss 0.04429098218679428 Accuracy 0.890250027179718\n",
      "Iteration 35010 Training loss 0.00010367466165916994 Validation loss 0.044385310262441635 Accuracy 0.8896250128746033\n",
      "Iteration 35020 Training loss 6.039788422640413e-05 Validation loss 0.04433063790202141 Accuracy 0.890375018119812\n",
      "Iteration 35030 Training loss 0.0013160377275198698 Validation loss 0.04423019662499428 Accuracy 0.890250027179718\n",
      "Iteration 35040 Training loss 7.581793761346489e-05 Validation loss 0.04429700970649719 Accuracy 0.8910000324249268\n",
      "Iteration 35050 Training loss 6.395974924089387e-05 Validation loss 0.044355787336826324 Accuracy 0.8906250596046448\n",
      "Iteration 35060 Training loss 0.003824107348918915 Validation loss 0.044421661645174026 Accuracy 0.89000004529953\n",
      "Iteration 35070 Training loss 0.0013934836024418473 Validation loss 0.04440535604953766 Accuracy 0.889875054359436\n",
      "Iteration 35080 Training loss 0.004069220740348101 Validation loss 0.044479355216026306 Accuracy 0.890250027179718\n",
      "Iteration 35090 Training loss 0.0013631004840135574 Validation loss 0.04443071782588959 Accuracy 0.89000004529953\n",
      "Iteration 35100 Training loss 0.0038530861493200064 Validation loss 0.044467031955718994 Accuracy 0.8906250596046448\n",
      "Iteration 35110 Training loss 9.120554750552401e-05 Validation loss 0.044429514557123184 Accuracy 0.8896250128746033\n",
      "Iteration 35120 Training loss 0.0013404807541519403 Validation loss 0.04441875219345093 Accuracy 0.8907500505447388\n",
      "Iteration 35130 Training loss 6.078022488509305e-05 Validation loss 0.044638268649578094 Accuracy 0.889875054359436\n",
      "Iteration 35140 Training loss 0.002604136476293206 Validation loss 0.04457072168588638 Accuracy 0.8887500166893005\n",
      "Iteration 35150 Training loss 0.00010326338815502822 Validation loss 0.04432360455393791 Accuracy 0.890375018119812\n",
      "Iteration 35160 Training loss 0.0013382734032347798 Validation loss 0.04438537359237671 Accuracy 0.8888750672340393\n",
      "Iteration 35170 Training loss 0.0013160747475922108 Validation loss 0.044411685317754745 Accuracy 0.8895000219345093\n",
      "Iteration 35180 Training loss 7.369473314611241e-05 Validation loss 0.044366005808115005 Accuracy 0.8908750414848328\n",
      "Iteration 35190 Training loss 0.0013046207604929805 Validation loss 0.044500552117824554 Accuracy 0.8893750309944153\n",
      "Iteration 35200 Training loss 4.4175656512379646e-05 Validation loss 0.04448927193880081 Accuracy 0.8905000686645508\n",
      "Iteration 35210 Training loss 0.0038382273633033037 Validation loss 0.04453102499246597 Accuracy 0.890250027179718\n",
      "Iteration 35220 Training loss 7.799571903888136e-05 Validation loss 0.04444269835948944 Accuracy 0.890250027179718\n",
      "Iteration 35230 Training loss 8.517555397702381e-05 Validation loss 0.0444912426173687 Accuracy 0.8895000219345093\n",
      "Iteration 35240 Training loss 0.0013618842931464314 Validation loss 0.044544290751218796 Accuracy 0.8885000348091125\n",
      "Iteration 35250 Training loss 0.0013035121373832226 Validation loss 0.04439540207386017 Accuracy 0.8908750414848328\n",
      "Iteration 35260 Training loss 9.420718561159447e-05 Validation loss 0.04450182616710663 Accuracy 0.890250027179718\n",
      "Iteration 35270 Training loss 0.0013484135270118713 Validation loss 0.04441199451684952 Accuracy 0.8896250128746033\n",
      "Iteration 35280 Training loss 0.0025688877794891596 Validation loss 0.04437869042158127 Accuracy 0.8905000686645508\n",
      "Iteration 35290 Training loss 6.614113226532936e-05 Validation loss 0.04444672539830208 Accuracy 0.8907500505447388\n",
      "Iteration 35300 Training loss 0.0013247637543827295 Validation loss 0.04437115788459778 Accuracy 0.8912500143051147\n",
      "Iteration 35310 Training loss 0.003836902091279626 Validation loss 0.044423799961805344 Accuracy 0.8905000686645508\n",
      "Iteration 35320 Training loss 0.002626343397423625 Validation loss 0.044715337455272675 Accuracy 0.8893750309944153\n",
      "Iteration 35330 Training loss 5.05362477269955e-05 Validation loss 0.04439637064933777 Accuracy 0.890375018119812\n",
      "Iteration 35340 Training loss 7.011345587670803e-05 Validation loss 0.04430937394499779 Accuracy 0.8917500376701355\n",
      "Iteration 35350 Training loss 9.123250492848456e-05 Validation loss 0.044376272708177567 Accuracy 0.8905000686645508\n",
      "Iteration 35360 Training loss 0.003806900233030319 Validation loss 0.04442356899380684 Accuracy 0.890375018119812\n",
      "Iteration 35370 Training loss 0.0013225191505625844 Validation loss 0.04445377737283707 Accuracy 0.890250027179718\n",
      "Iteration 35380 Training loss 0.0025890159886330366 Validation loss 0.0443682037293911 Accuracy 0.8911250233650208\n",
      "Iteration 35390 Training loss 9.984926873585209e-05 Validation loss 0.04442967474460602 Accuracy 0.890250027179718\n",
      "Iteration 35400 Training loss 0.0025489937979727983 Validation loss 0.04447560012340546 Accuracy 0.89000004529953\n",
      "Iteration 35410 Training loss 0.0013146395795047283 Validation loss 0.04448026791214943 Accuracy 0.890250027179718\n",
      "Iteration 35420 Training loss 0.0013143143150955439 Validation loss 0.044498227536678314 Accuracy 0.89000004529953\n",
      "Iteration 35430 Training loss 8.903866546461359e-05 Validation loss 0.04441791772842407 Accuracy 0.8906250596046448\n",
      "Iteration 35440 Training loss 0.0025689206086099148 Validation loss 0.04443506896495819 Accuracy 0.89000004529953\n",
      "Iteration 35450 Training loss 0.0025717588141560555 Validation loss 0.044397905468940735 Accuracy 0.8908750414848328\n",
      "Iteration 35460 Training loss 0.0012958513107150793 Validation loss 0.04442303627729416 Accuracy 0.8906250596046448\n",
      "Iteration 35470 Training loss 0.0013195129577070475 Validation loss 0.044419124722480774 Accuracy 0.8906250596046448\n",
      "Iteration 35480 Training loss 5.92711367062293e-05 Validation loss 0.04446442797780037 Accuracy 0.890250027179718\n",
      "Iteration 35490 Training loss 0.0026119714602828026 Validation loss 0.04454347863793373 Accuracy 0.889875054359436\n",
      "Iteration 35500 Training loss 0.0025674072094261646 Validation loss 0.04448804259300232 Accuracy 0.890375018119812\n",
      "Iteration 35510 Training loss 8.501599222654477e-05 Validation loss 0.04445905238389969 Accuracy 0.8910000324249268\n",
      "Iteration 35520 Training loss 0.00011007534340023994 Validation loss 0.04446936026215553 Accuracy 0.8905000686645508\n",
      "Iteration 35530 Training loss 6.774481880711392e-05 Validation loss 0.04454667493700981 Accuracy 0.890250027179718\n",
      "Iteration 35540 Training loss 6.630834104726091e-05 Validation loss 0.04493001475930214 Accuracy 0.8873750567436218\n",
      "Iteration 35550 Training loss 0.0013100537471473217 Validation loss 0.04455270618200302 Accuracy 0.8895000219345093\n",
      "Iteration 35560 Training loss 4.8047295422293246e-05 Validation loss 0.04459067061543465 Accuracy 0.8891250491142273\n",
      "Iteration 35570 Training loss 0.0012964888010174036 Validation loss 0.04459156095981598 Accuracy 0.889875054359436\n",
      "Iteration 35580 Training loss 6.785964069422334e-05 Validation loss 0.04449198395013809 Accuracy 0.890375018119812\n",
      "Iteration 35590 Training loss 0.0013223306741565466 Validation loss 0.044603846967220306 Accuracy 0.8888750672340393\n",
      "Iteration 35600 Training loss 0.0012836845126003027 Validation loss 0.04456140473484993 Accuracy 0.89000004529953\n",
      "Iteration 35610 Training loss 0.0025835256092250347 Validation loss 0.044506318867206573 Accuracy 0.8893750309944153\n",
      "Iteration 35620 Training loss 0.0025717609096318483 Validation loss 0.04449698328971863 Accuracy 0.89000004529953\n",
      "Iteration 35630 Training loss 0.0013069084379822016 Validation loss 0.04449984058737755 Accuracy 0.89000004529953\n",
      "Iteration 35640 Training loss 0.0013121793745085597 Validation loss 0.04453893378376961 Accuracy 0.8896250128746033\n",
      "Iteration 35650 Training loss 5.053820859757252e-05 Validation loss 0.04454518482089043 Accuracy 0.889750063419342\n",
      "Iteration 35660 Training loss 3.926446152036078e-05 Validation loss 0.04446512833237648 Accuracy 0.890125036239624\n",
      "Iteration 35670 Training loss 0.0025762680452317 Validation loss 0.044553354382514954 Accuracy 0.8896250128746033\n",
      "Iteration 35680 Training loss 0.0013363080797716975 Validation loss 0.044589657336473465 Accuracy 0.8893750309944153\n",
      "Iteration 35690 Training loss 0.0013060463825240731 Validation loss 0.044628776609897614 Accuracy 0.8891250491142273\n",
      "Iteration 35700 Training loss 0.0012949329102411866 Validation loss 0.04454284906387329 Accuracy 0.8893750309944153\n",
      "Iteration 35710 Training loss 8.282325870823115e-05 Validation loss 0.04454730451107025 Accuracy 0.890250027179718\n",
      "Iteration 35720 Training loss 5.7257515436504036e-05 Validation loss 0.044608213007450104 Accuracy 0.8891250491142273\n",
      "Iteration 35730 Training loss 5.974121086183004e-05 Validation loss 0.044600650668144226 Accuracy 0.8891250491142273\n",
      "Iteration 35740 Training loss 0.0012864194577559829 Validation loss 0.04458298534154892 Accuracy 0.889750063419342\n",
      "Iteration 35750 Training loss 0.0013143570395186543 Validation loss 0.044534213840961456 Accuracy 0.890250027179718\n",
      "Iteration 35760 Training loss 0.0013237150851637125 Validation loss 0.04463513568043709 Accuracy 0.8886250257492065\n",
      "Iteration 35770 Training loss 0.0013107191771268845 Validation loss 0.0444461964070797 Accuracy 0.8905000686645508\n",
      "Iteration 35780 Training loss 0.0013087402330711484 Validation loss 0.04449313506484032 Accuracy 0.89000004529953\n",
      "Iteration 35790 Training loss 0.0013219777029007673 Validation loss 0.044599734246730804 Accuracy 0.8888750672340393\n",
      "Iteration 35800 Training loss 0.0025869435630738735 Validation loss 0.04447626322507858 Accuracy 0.890250027179718\n",
      "Iteration 35810 Training loss 0.0025504084769636393 Validation loss 0.04458256810903549 Accuracy 0.8890000581741333\n",
      "Iteration 35820 Training loss 0.0038030515424907207 Validation loss 0.0447622612118721 Accuracy 0.8885000348091125\n",
      "Iteration 35830 Training loss 0.0025856883730739355 Validation loss 0.04449949041008949 Accuracy 0.889750063419342\n",
      "Iteration 35840 Training loss 0.00254150852560997 Validation loss 0.044548723846673965 Accuracy 0.890375018119812\n",
      "Iteration 35850 Training loss 0.0013185985153540969 Validation loss 0.044557176530361176 Accuracy 0.889875054359436\n",
      "Iteration 35860 Training loss 0.0013049127301201224 Validation loss 0.04444543272256851 Accuracy 0.8908750414848328\n",
      "Iteration 35870 Training loss 0.002570487093180418 Validation loss 0.04457522928714752 Accuracy 0.8887500166893005\n",
      "Iteration 35880 Training loss 0.0012948343064635992 Validation loss 0.04447861388325691 Accuracy 0.8908750414848328\n",
      "Iteration 35890 Training loss 0.0037877969443798065 Validation loss 0.044474758207798004 Accuracy 0.890250027179718\n",
      "Iteration 35900 Training loss 6.431124347727746e-05 Validation loss 0.04449521005153656 Accuracy 0.89000004529953\n",
      "Iteration 35910 Training loss 6.661537190666422e-05 Validation loss 0.0444708913564682 Accuracy 0.890125036239624\n",
      "Iteration 35920 Training loss 0.00130807317327708 Validation loss 0.044473495334386826 Accuracy 0.890125036239624\n",
      "Iteration 35930 Training loss 5.8789446484297514e-05 Validation loss 0.044550005346536636 Accuracy 0.89000004529953\n",
      "Iteration 35940 Training loss 0.0013176759239286184 Validation loss 0.044446829706430435 Accuracy 0.8912500143051147\n",
      "Iteration 35950 Training loss 0.0012932408135384321 Validation loss 0.0444633774459362 Accuracy 0.8907500505447388\n",
      "Iteration 35960 Training loss 0.001318609807640314 Validation loss 0.044442132115364075 Accuracy 0.8910000324249268\n",
      "Iteration 35970 Training loss 0.0012921699089929461 Validation loss 0.04441385716199875 Accuracy 0.8906250596046448\n",
      "Iteration 35980 Training loss 5.026818689657375e-05 Validation loss 0.04446856305003166 Accuracy 0.89000004529953\n",
      "Iteration 35990 Training loss 4.82023497170303e-05 Validation loss 0.04448506236076355 Accuracy 0.8905000686645508\n",
      "Iteration 36000 Training loss 0.001302707358263433 Validation loss 0.04442925751209259 Accuracy 0.890125036239624\n",
      "Iteration 36010 Training loss 0.002546324860304594 Validation loss 0.04435969144105911 Accuracy 0.8910000324249268\n",
      "Iteration 36020 Training loss 0.003829555818811059 Validation loss 0.04443242400884628 Accuracy 0.8910000324249268\n",
      "Iteration 36030 Training loss 0.0013309397036209702 Validation loss 0.04443538933992386 Accuracy 0.8915000557899475\n",
      "Iteration 36040 Training loss 0.0013124466640874743 Validation loss 0.04446405917406082 Accuracy 0.8911250233650208\n",
      "Iteration 36050 Training loss 0.002563983201980591 Validation loss 0.04447226598858833 Accuracy 0.890375018119812\n",
      "Iteration 36060 Training loss 0.002550289034843445 Validation loss 0.044499531388282776 Accuracy 0.8905000686645508\n",
      "Iteration 36070 Training loss 0.0013049572007730603 Validation loss 0.04450356215238571 Accuracy 0.8910000324249268\n",
      "Iteration 36080 Training loss 4.420894038048573e-05 Validation loss 0.04462357982993126 Accuracy 0.8892500400543213\n",
      "Iteration 36090 Training loss 0.0013359843287616968 Validation loss 0.044471681118011475 Accuracy 0.8907500505447388\n",
      "Iteration 36100 Training loss 3.884304533130489e-05 Validation loss 0.04462942108511925 Accuracy 0.8888750672340393\n",
      "Iteration 36110 Training loss 5.383227835409343e-05 Validation loss 0.04460591450333595 Accuracy 0.89000004529953\n",
      "Iteration 36120 Training loss 0.0037891194224357605 Validation loss 0.04450037702918053 Accuracy 0.8907500505447388\n",
      "Iteration 36130 Training loss 0.002553408732637763 Validation loss 0.04450242966413498 Accuracy 0.8905000686645508\n",
      "Iteration 36140 Training loss 3.674591062008403e-05 Validation loss 0.04463543742895126 Accuracy 0.8891250491142273\n",
      "Iteration 36150 Training loss 5.6712913647061214e-05 Validation loss 0.04461164399981499 Accuracy 0.8895000219345093\n",
      "Iteration 36160 Training loss 0.0025624025147408247 Validation loss 0.044570665806531906 Accuracy 0.890375018119812\n",
      "Iteration 36170 Training loss 0.0012902447488158941 Validation loss 0.0445672869682312 Accuracy 0.890125036239624\n",
      "Iteration 36180 Training loss 0.0013046246021986008 Validation loss 0.04455363750457764 Accuracy 0.889875054359436\n",
      "Iteration 36190 Training loss 0.0025547256227582693 Validation loss 0.044535279273986816 Accuracy 0.890375018119812\n",
      "Iteration 36200 Training loss 0.003819681704044342 Validation loss 0.04453528672456741 Accuracy 0.889875054359436\n",
      "Iteration 36210 Training loss 0.0013038348406553268 Validation loss 0.04457055777311325 Accuracy 0.8887500166893005\n",
      "Iteration 36220 Training loss 0.002548418240621686 Validation loss 0.044480469077825546 Accuracy 0.890125036239624\n",
      "Iteration 36230 Training loss 0.0025349229108542204 Validation loss 0.044551532715559006 Accuracy 0.889875054359436\n",
      "Iteration 36240 Training loss 0.0012943667825311422 Validation loss 0.04446827247738838 Accuracy 0.8905000686645508\n",
      "Iteration 36250 Training loss 0.0025630637537688017 Validation loss 0.044542163610458374 Accuracy 0.8895000219345093\n",
      "Iteration 36260 Training loss 0.00257695559412241 Validation loss 0.04455824941396713 Accuracy 0.8892500400543213\n",
      "Iteration 36270 Training loss 0.00254559563472867 Validation loss 0.04454663768410683 Accuracy 0.889750063419342\n",
      "Iteration 36280 Training loss 0.002555009676143527 Validation loss 0.04471705108880997 Accuracy 0.8885000348091125\n",
      "Iteration 36290 Training loss 0.005044165067374706 Validation loss 0.04452992603182793 Accuracy 0.8913750648498535\n",
      "Iteration 36300 Training loss 0.0013152096653357148 Validation loss 0.04452276602387428 Accuracy 0.8907500505447388\n",
      "Iteration 36310 Training loss 0.0012939029838889837 Validation loss 0.044570986181497574 Accuracy 0.889750063419342\n",
      "Iteration 36320 Training loss 0.0012986279325559735 Validation loss 0.044617194682359695 Accuracy 0.889750063419342\n",
      "Iteration 36330 Training loss 0.00130848225671798 Validation loss 0.0446346253156662 Accuracy 0.8893750309944153\n",
      "Iteration 36340 Training loss 3.632208608905785e-05 Validation loss 0.04460309073328972 Accuracy 0.8895000219345093\n",
      "Iteration 36350 Training loss 3.230557558708824e-05 Validation loss 0.04465322196483612 Accuracy 0.8888750672340393\n",
      "Iteration 36360 Training loss 0.0013004231732338667 Validation loss 0.044538699090480804 Accuracy 0.889875054359436\n",
      "Iteration 36370 Training loss 0.0025663706474006176 Validation loss 0.04452996328473091 Accuracy 0.889875054359436\n",
      "Iteration 36380 Training loss 0.003794462187215686 Validation loss 0.04453515633940697 Accuracy 0.8890000581741333\n",
      "Iteration 36390 Training loss 4.702420483226888e-05 Validation loss 0.04460102692246437 Accuracy 0.8893750309944153\n",
      "Iteration 36400 Training loss 0.0012986580841243267 Validation loss 0.0446537509560585 Accuracy 0.8893750309944153\n",
      "Iteration 36410 Training loss 4.5899636461399496e-05 Validation loss 0.044658515602350235 Accuracy 0.8891250491142273\n",
      "Iteration 36420 Training loss 0.0025419050361961126 Validation loss 0.04458627104759216 Accuracy 0.889875054359436\n",
      "Iteration 36430 Training loss 0.002540204208344221 Validation loss 0.0445721372961998 Accuracy 0.889750063419342\n",
      "Iteration 36440 Training loss 0.0013016744051128626 Validation loss 0.04458536580204964 Accuracy 0.8893750309944153\n",
      "Iteration 36450 Training loss 0.0013164952397346497 Validation loss 0.0445130281150341 Accuracy 0.890375018119812\n",
      "Iteration 36460 Training loss 0.002546193776652217 Validation loss 0.044547051191329956 Accuracy 0.890125036239624\n",
      "Iteration 36470 Training loss 0.0013032949063926935 Validation loss 0.04456199333071709 Accuracy 0.8896250128746033\n",
      "Iteration 36480 Training loss 0.0026127928867936134 Validation loss 0.04462197422981262 Accuracy 0.8895000219345093\n",
      "Iteration 36490 Training loss 0.0025373422540724277 Validation loss 0.044616952538490295 Accuracy 0.890375018119812\n",
      "Iteration 36500 Training loss 0.0012932346435263753 Validation loss 0.044706158339977264 Accuracy 0.8885000348091125\n",
      "Iteration 36510 Training loss 0.0012895301915705204 Validation loss 0.04459661990404129 Accuracy 0.890375018119812\n",
      "Iteration 36520 Training loss 0.0025469553656876087 Validation loss 0.04458210617303848 Accuracy 0.890375018119812\n",
      "Iteration 36530 Training loss 0.0012830215273424983 Validation loss 0.04460164159536362 Accuracy 0.890375018119812\n",
      "Iteration 36540 Training loss 3.134278449579142e-05 Validation loss 0.0446561761200428 Accuracy 0.8895000219345093\n",
      "Iteration 36550 Training loss 7.192284829216078e-05 Validation loss 0.044611360877752304 Accuracy 0.8910000324249268\n",
      "Iteration 36560 Training loss 0.0012938632862642407 Validation loss 0.044638536870479584 Accuracy 0.89000004529953\n",
      "Iteration 36570 Training loss 0.00382539676502347 Validation loss 0.044615939259529114 Accuracy 0.890375018119812\n",
      "Iteration 36580 Training loss 0.002571200719103217 Validation loss 0.04462150111794472 Accuracy 0.889750063419342\n",
      "Iteration 36590 Training loss 0.0037992012221366167 Validation loss 0.04465470463037491 Accuracy 0.8893750309944153\n",
      "Iteration 36600 Training loss 0.0025422978214919567 Validation loss 0.04458728805184364 Accuracy 0.889875054359436\n",
      "Iteration 36610 Training loss 6.663404928985983e-05 Validation loss 0.04459016025066376 Accuracy 0.889750063419342\n",
      "Iteration 36620 Training loss 5.737439641961828e-05 Validation loss 0.044652778655290604 Accuracy 0.8885000348091125\n",
      "Iteration 36630 Training loss 4.0785052988212556e-05 Validation loss 0.04462822154164314 Accuracy 0.8896250128746033\n",
      "Iteration 36640 Training loss 0.0013146185083314776 Validation loss 0.04464751109480858 Accuracy 0.8886250257492065\n",
      "Iteration 36650 Training loss 3.953878695028834e-05 Validation loss 0.044702086597681046 Accuracy 0.8892500400543213\n",
      "Iteration 36660 Training loss 0.0013170808088034391 Validation loss 0.044608671218156815 Accuracy 0.8891250491142273\n",
      "Iteration 36670 Training loss 0.0025487756356596947 Validation loss 0.04464394599199295 Accuracy 0.8888750672340393\n",
      "Iteration 36680 Training loss 0.0012971925316378474 Validation loss 0.04454527050256729 Accuracy 0.8907500505447388\n",
      "Iteration 36690 Training loss 4.6227498387452215e-05 Validation loss 0.044542063027620316 Accuracy 0.8905000686645508\n",
      "Iteration 36700 Training loss 0.0012862671865150332 Validation loss 0.04456501081585884 Accuracy 0.890125036239624\n",
      "Iteration 36710 Training loss 0.002552105113863945 Validation loss 0.044609081000089645 Accuracy 0.89000004529953\n",
      "Iteration 36720 Training loss 0.002544374903663993 Validation loss 0.044628970324993134 Accuracy 0.8895000219345093\n",
      "Iteration 36730 Training loss 0.0013222717680037022 Validation loss 0.04464446380734444 Accuracy 0.889875054359436\n",
      "Iteration 36740 Training loss 5.005350976716727e-05 Validation loss 0.04465918242931366 Accuracy 0.8892500400543213\n",
      "Iteration 36750 Training loss 0.0012778814416378736 Validation loss 0.044626519083976746 Accuracy 0.889875054359436\n",
      "Iteration 36760 Training loss 0.003803639905527234 Validation loss 0.04467770457267761 Accuracy 0.8893750309944153\n",
      "Iteration 36770 Training loss 4.593304402078502e-05 Validation loss 0.044672891497612 Accuracy 0.889750063419342\n",
      "Iteration 36780 Training loss 0.0025428393855690956 Validation loss 0.04463109374046326 Accuracy 0.89000004529953\n",
      "Iteration 36790 Training loss 0.002541064051911235 Validation loss 0.04457920417189598 Accuracy 0.890250027179718\n",
      "Iteration 36800 Training loss 0.002544279443100095 Validation loss 0.04469377174973488 Accuracy 0.8886250257492065\n",
      "Iteration 36810 Training loss 0.0012977785663679242 Validation loss 0.044666800647974014 Accuracy 0.8896250128746033\n",
      "Iteration 36820 Training loss 5.2734434575540945e-05 Validation loss 0.044618744403123856 Accuracy 0.8892500400543213\n",
      "Iteration 36830 Training loss 0.0012803585268557072 Validation loss 0.044587213546037674 Accuracy 0.89000004529953\n",
      "Iteration 36840 Training loss 0.001286463113501668 Validation loss 0.04464396834373474 Accuracy 0.8892500400543213\n",
      "Iteration 36850 Training loss 3.9252001442946494e-05 Validation loss 0.04466141387820244 Accuracy 0.8892500400543213\n",
      "Iteration 36860 Training loss 0.0025499719195067883 Validation loss 0.04461628198623657 Accuracy 0.889875054359436\n",
      "Iteration 36870 Training loss 0.0012850225903093815 Validation loss 0.04463569447398186 Accuracy 0.8895000219345093\n",
      "Iteration 36880 Training loss 4.5424381823977455e-05 Validation loss 0.04464060440659523 Accuracy 0.8885000348091125\n",
      "Iteration 36890 Training loss 3.0134924600133672e-05 Validation loss 0.044706620275974274 Accuracy 0.8893750309944153\n",
      "Iteration 36900 Training loss 4.69857914140448e-05 Validation loss 0.04470906779170036 Accuracy 0.8893750309944153\n",
      "Iteration 36910 Training loss 5.12522819917649e-05 Validation loss 0.044695839285850525 Accuracy 0.89000004529953\n",
      "Iteration 36920 Training loss 0.0013035848969593644 Validation loss 0.04467014968395233 Accuracy 0.889750063419342\n",
      "Iteration 36930 Training loss 0.002546110888943076 Validation loss 0.0447564572095871 Accuracy 0.8887500166893005\n",
      "Iteration 36940 Training loss 8.829553553368896e-05 Validation loss 0.044621702283620834 Accuracy 0.890125036239624\n",
      "Iteration 36950 Training loss 6.621004285989329e-05 Validation loss 0.04466474428772926 Accuracy 0.889750063419342\n",
      "Iteration 36960 Training loss 0.0012926629278808832 Validation loss 0.044656701385974884 Accuracy 0.8896250128746033\n",
      "Iteration 36970 Training loss 0.0012988289818167686 Validation loss 0.04470673203468323 Accuracy 0.8892500400543213\n",
      "Iteration 36980 Training loss 0.003797295968979597 Validation loss 0.04464990645647049 Accuracy 0.889750063419342\n",
      "Iteration 36990 Training loss 5.369777136365883e-05 Validation loss 0.04470868036150932 Accuracy 0.8892500400543213\n",
      "Iteration 37000 Training loss 0.0050450339913368225 Validation loss 0.0445760153234005 Accuracy 0.8905000686645508\n",
      "Iteration 37010 Training loss 0.0012926894705742598 Validation loss 0.04463141784071922 Accuracy 0.890250027179718\n",
      "Iteration 37020 Training loss 4.244048977852799e-05 Validation loss 0.04467868432402611 Accuracy 0.8893750309944153\n",
      "Iteration 37030 Training loss 0.0013288514455780387 Validation loss 0.044682856649160385 Accuracy 0.8890000581741333\n",
      "Iteration 37040 Training loss 5.8428599004400894e-05 Validation loss 0.044709961861371994 Accuracy 0.8888750672340393\n",
      "Iteration 37050 Training loss 0.003785745706409216 Validation loss 0.044707465916872025 Accuracy 0.8895000219345093\n",
      "Iteration 37060 Training loss 0.0012841698480769992 Validation loss 0.04470208287239075 Accuracy 0.889750063419342\n",
      "Iteration 37070 Training loss 0.0012750173918902874 Validation loss 0.044678810983896255 Accuracy 0.8895000219345093\n",
      "Iteration 37080 Training loss 0.005038880743086338 Validation loss 0.04465154558420181 Accuracy 0.890250027179718\n",
      "Iteration 37090 Training loss 0.0037871634121984243 Validation loss 0.044642671942710876 Accuracy 0.890375018119812\n",
      "Iteration 37100 Training loss 0.003796827746555209 Validation loss 0.044710464775562286 Accuracy 0.8888750672340393\n",
      "Iteration 37110 Training loss 4.013196303276345e-05 Validation loss 0.04464692994952202 Accuracy 0.890125036239624\n",
      "Iteration 37120 Training loss 0.0012801684206351638 Validation loss 0.04475480690598488 Accuracy 0.8891250491142273\n",
      "Iteration 37130 Training loss 3.133032805635594e-05 Validation loss 0.04472595080733299 Accuracy 0.8888750672340393\n",
      "Iteration 37140 Training loss 0.0025443213526159525 Validation loss 0.044732965528964996 Accuracy 0.8886250257492065\n",
      "Iteration 37150 Training loss 5.156149200047366e-05 Validation loss 0.044688355177640915 Accuracy 0.8893750309944153\n",
      "Iteration 37160 Training loss 0.0038026070687919855 Validation loss 0.04468159005045891 Accuracy 0.8890000581741333\n",
      "Iteration 37170 Training loss 0.00253699510358274 Validation loss 0.04463209584355354 Accuracy 0.8895000219345093\n",
      "Iteration 37180 Training loss 0.0012990015093237162 Validation loss 0.04464733973145485 Accuracy 0.889875054359436\n",
      "Iteration 37190 Training loss 0.0037906926590949297 Validation loss 0.044620394706726074 Accuracy 0.890375018119812\n",
      "Iteration 37200 Training loss 0.002542552538216114 Validation loss 0.04468243569135666 Accuracy 0.8896250128746033\n",
      "Iteration 37210 Training loss 0.0013049686094745994 Validation loss 0.04474615678191185 Accuracy 0.8885000348091125\n",
      "Iteration 37220 Training loss 2.829269942594692e-05 Validation loss 0.044669583439826965 Accuracy 0.889750063419342\n",
      "Iteration 37230 Training loss 0.0012950587552040815 Validation loss 0.044678106904029846 Accuracy 0.89000004529953\n",
      "Iteration 37240 Training loss 0.00255148159340024 Validation loss 0.04471177980303764 Accuracy 0.8895000219345093\n",
      "Iteration 37250 Training loss 0.0025530324783176184 Validation loss 0.044690851122140884 Accuracy 0.89000004529953\n",
      "Iteration 37260 Training loss 0.0012890580110251904 Validation loss 0.04470669850707054 Accuracy 0.8892500400543213\n",
      "Iteration 37270 Training loss 0.002532623941078782 Validation loss 0.04468205198645592 Accuracy 0.889875054359436\n",
      "Iteration 37280 Training loss 0.0012996864970773458 Validation loss 0.04473678022623062 Accuracy 0.8891250491142273\n",
      "Iteration 37290 Training loss 0.0012924785260111094 Validation loss 0.04464853182435036 Accuracy 0.889875054359436\n",
      "Iteration 37300 Training loss 4.6490513341268525e-05 Validation loss 0.04473687335848808 Accuracy 0.890125036239624\n",
      "Iteration 37310 Training loss 0.0037970123812556267 Validation loss 0.04470386728644371 Accuracy 0.8892500400543213\n",
      "Iteration 37320 Training loss 4.2525305616436526e-05 Validation loss 0.04474365711212158 Accuracy 0.8891250491142273\n",
      "Iteration 37330 Training loss 0.0012781454715877771 Validation loss 0.04468681290745735 Accuracy 0.890250027179718\n",
      "Iteration 37340 Training loss 0.0013040321646258235 Validation loss 0.044700801372528076 Accuracy 0.889750063419342\n",
      "Iteration 37350 Training loss 0.002543691312894225 Validation loss 0.04469546675682068 Accuracy 0.8896250128746033\n",
      "Iteration 37360 Training loss 4.8681202315492555e-05 Validation loss 0.04474044218659401 Accuracy 0.8892500400543213\n",
      "Iteration 37370 Training loss 0.00253427610732615 Validation loss 0.04468527063727379 Accuracy 0.8893750309944153\n",
      "Iteration 37380 Training loss 0.0012974397977814078 Validation loss 0.04474832862615585 Accuracy 0.889875054359436\n",
      "Iteration 37390 Training loss 0.002540143672376871 Validation loss 0.04478488489985466 Accuracy 0.8892500400543213\n",
      "Iteration 37400 Training loss 0.002557171741500497 Validation loss 0.04487199708819389 Accuracy 0.8888750672340393\n",
      "Iteration 37410 Training loss 0.0012877269182354212 Validation loss 0.044743455946445465 Accuracy 0.8892500400543213\n",
      "Iteration 37420 Training loss 0.0037835543043911457 Validation loss 0.04472601041197777 Accuracy 0.8896250128746033\n",
      "Iteration 37430 Training loss 0.0013032619608566165 Validation loss 0.04468964412808418 Accuracy 0.889875054359436\n",
      "Iteration 37440 Training loss 0.002570023527368903 Validation loss 0.04478274658322334 Accuracy 0.8890000581741333\n",
      "Iteration 37450 Training loss 4.318754145060666e-05 Validation loss 0.044763121753931046 Accuracy 0.889750063419342\n",
      "Iteration 37460 Training loss 0.002538395579904318 Validation loss 0.04476465284824371 Accuracy 0.8893750309944153\n",
      "Iteration 37470 Training loss 0.0012995254946872592 Validation loss 0.044829729944467545 Accuracy 0.8888750672340393\n",
      "Iteration 37480 Training loss 0.0012858675327152014 Validation loss 0.04478497430682182 Accuracy 0.889750063419342\n",
      "Iteration 37490 Training loss 0.001299044000916183 Validation loss 0.044774677604436874 Accuracy 0.8895000219345093\n",
      "Iteration 37500 Training loss 0.002558234380558133 Validation loss 0.04470871388912201 Accuracy 0.8895000219345093\n",
      "Iteration 37510 Training loss 0.0012806145241484046 Validation loss 0.04474642127752304 Accuracy 0.8887500166893005\n",
      "Iteration 37520 Training loss 5.2599261834984645e-05 Validation loss 0.04475175589323044 Accuracy 0.8891250491142273\n",
      "Iteration 37530 Training loss 0.001315903733484447 Validation loss 0.044738415628671646 Accuracy 0.889750063419342\n",
      "Iteration 37540 Training loss 5.899590178159997e-05 Validation loss 0.044762033969163895 Accuracy 0.8888750672340393\n",
      "Iteration 37550 Training loss 5.460602551465854e-05 Validation loss 0.04475511983036995 Accuracy 0.8890000581741333\n",
      "Iteration 37560 Training loss 4.5365792175289243e-05 Validation loss 0.044715363532304764 Accuracy 0.8893750309944153\n",
      "Iteration 37570 Training loss 3.933253901777789e-05 Validation loss 0.04475802183151245 Accuracy 0.8895000219345093\n",
      "Iteration 37580 Training loss 0.0013002303894609213 Validation loss 0.04480276256799698 Accuracy 0.8892500400543213\n",
      "Iteration 37590 Training loss 0.0012867121258750558 Validation loss 0.04474762827157974 Accuracy 0.8896250128746033\n",
      "Iteration 37600 Training loss 3.0357856303453445e-05 Validation loss 0.044769734144210815 Accuracy 0.8886250257492065\n",
      "Iteration 37610 Training loss 3.7456131394719705e-05 Validation loss 0.044775933027267456 Accuracy 0.8896250128746033\n",
      "Iteration 37620 Training loss 0.0025356870610266924 Validation loss 0.044730741530656815 Accuracy 0.8896250128746033\n",
      "Iteration 37630 Training loss 0.0013183035189285874 Validation loss 0.04471573606133461 Accuracy 0.889750063419342\n",
      "Iteration 37640 Training loss 0.0013309958158060908 Validation loss 0.044718608260154724 Accuracy 0.890125036239624\n",
      "Iteration 37650 Training loss 0.002550475299358368 Validation loss 0.044733744114637375 Accuracy 0.8891250491142273\n",
      "Iteration 37660 Training loss 3.773469870793633e-05 Validation loss 0.044757720082998276 Accuracy 0.8890000581741333\n",
      "Iteration 37670 Training loss 0.0012958907755091786 Validation loss 0.04476163536310196 Accuracy 0.889750063419342\n",
      "Iteration 37680 Training loss 4.988677756045945e-05 Validation loss 0.04473613202571869 Accuracy 0.889750063419342\n",
      "Iteration 37690 Training loss 0.002526490017771721 Validation loss 0.044773198664188385 Accuracy 0.8892500400543213\n",
      "Iteration 37700 Training loss 0.0013070341665297747 Validation loss 0.04477739334106445 Accuracy 0.8895000219345093\n",
      "Iteration 37710 Training loss 0.0025504003278911114 Validation loss 0.04482563957571983 Accuracy 0.8888750672340393\n",
      "Iteration 37720 Training loss 0.0013130158185958862 Validation loss 0.04478353261947632 Accuracy 0.8892500400543213\n",
      "Iteration 37730 Training loss 3.999626278528012e-05 Validation loss 0.04484333097934723 Accuracy 0.8882500529289246\n",
      "Iteration 37740 Training loss 0.002537749707698822 Validation loss 0.04483252763748169 Accuracy 0.8886250257492065\n",
      "Iteration 37750 Training loss 0.0025387483183294535 Validation loss 0.04474521055817604 Accuracy 0.8895000219345093\n",
      "Iteration 37760 Training loss 0.0025469607207924128 Validation loss 0.04481067880988121 Accuracy 0.8887500166893005\n",
      "Iteration 37770 Training loss 0.0037931096740067005 Validation loss 0.044707849621772766 Accuracy 0.8895000219345093\n",
      "Iteration 37780 Training loss 0.0012774791102856398 Validation loss 0.04474066197872162 Accuracy 0.8891250491142273\n",
      "Iteration 37790 Training loss 0.0012966911308467388 Validation loss 0.0447395034134388 Accuracy 0.8896250128746033\n",
      "Iteration 37800 Training loss 4.525495751295239e-05 Validation loss 0.04474518820643425 Accuracy 0.8886250257492065\n",
      "Iteration 37810 Training loss 0.0013085035607218742 Validation loss 0.044768862426280975 Accuracy 0.8883750438690186\n",
      "Iteration 37820 Training loss 0.002532555954530835 Validation loss 0.04476259648799896 Accuracy 0.8886250257492065\n",
      "Iteration 37830 Training loss 4.987447755411267e-05 Validation loss 0.04473011568188667 Accuracy 0.8892500400543213\n",
      "Iteration 37840 Training loss 0.0038200237322598696 Validation loss 0.044774074107408524 Accuracy 0.8887500166893005\n",
      "Iteration 37850 Training loss 0.0025591813027858734 Validation loss 0.04470476508140564 Accuracy 0.8896250128746033\n",
      "Iteration 37860 Training loss 0.0025309035554528236 Validation loss 0.04484372213482857 Accuracy 0.8891250491142273\n",
      "Iteration 37870 Training loss 0.0025281417183578014 Validation loss 0.044798702001571655 Accuracy 0.8893750309944153\n",
      "Iteration 37880 Training loss 0.002537434920668602 Validation loss 0.044783879071474075 Accuracy 0.8891250491142273\n",
      "Iteration 37890 Training loss 3.0419092581723817e-05 Validation loss 0.04483203962445259 Accuracy 0.8888750672340393\n",
      "Iteration 37900 Training loss 0.002536787884309888 Validation loss 0.04476841166615486 Accuracy 0.8895000219345093\n",
      "Iteration 37910 Training loss 0.0025492969434708357 Validation loss 0.04480564966797829 Accuracy 0.8893750309944153\n",
      "Iteration 37920 Training loss 4.3555286538321525e-05 Validation loss 0.04479281231760979 Accuracy 0.8895000219345093\n",
      "Iteration 37930 Training loss 0.001299003604799509 Validation loss 0.04478834569454193 Accuracy 0.8891250491142273\n",
      "Iteration 37940 Training loss 0.001283133402466774 Validation loss 0.04477226361632347 Accuracy 0.8890000581741333\n",
      "Iteration 37950 Training loss 0.0038043330423533916 Validation loss 0.04476024955511093 Accuracy 0.8888750672340393\n",
      "Iteration 37960 Training loss 4.215284570818767e-05 Validation loss 0.04478631541132927 Accuracy 0.8888750672340393\n",
      "Iteration 37970 Training loss 2.7421139748184942e-05 Validation loss 0.044774819165468216 Accuracy 0.8892500400543213\n",
      "Iteration 37980 Training loss 0.0038154348731040955 Validation loss 0.04476621747016907 Accuracy 0.8896250128746033\n",
      "Iteration 37990 Training loss 0.0025406067725270987 Validation loss 0.0447668693959713 Accuracy 0.8895000219345093\n",
      "Iteration 38000 Training loss 0.0037900961469858885 Validation loss 0.04480414837598801 Accuracy 0.8891250491142273\n",
      "Iteration 38010 Training loss 0.0025438263546675444 Validation loss 0.04476822540163994 Accuracy 0.8892500400543213\n",
      "Iteration 38020 Training loss 0.0037921275943517685 Validation loss 0.044804491102695465 Accuracy 0.8888750672340393\n",
      "Iteration 38030 Training loss 0.001294473186135292 Validation loss 0.04474404454231262 Accuracy 0.8891250491142273\n",
      "Iteration 38040 Training loss 0.002539301523938775 Validation loss 0.0447392463684082 Accuracy 0.8893750309944153\n",
      "Iteration 38050 Training loss 4.73825894005131e-05 Validation loss 0.04475666955113411 Accuracy 0.8895000219345093\n",
      "Iteration 38060 Training loss 5.1289469411130995e-05 Validation loss 0.045013345777988434 Accuracy 0.8881250619888306\n",
      "Iteration 38070 Training loss 0.0012937459396198392 Validation loss 0.0447695329785347 Accuracy 0.8891250491142273\n",
      "Iteration 38080 Training loss 0.002546881791204214 Validation loss 0.044746220111846924 Accuracy 0.8892500400543213\n",
      "Iteration 38090 Training loss 0.0025392204988747835 Validation loss 0.044733162969350815 Accuracy 0.889750063419342\n",
      "Iteration 38100 Training loss 0.0025517211761325598 Validation loss 0.04484182596206665 Accuracy 0.8893750309944153\n",
      "Iteration 38110 Training loss 0.002546230098232627 Validation loss 0.044878482818603516 Accuracy 0.8885000348091125\n",
      "Iteration 38120 Training loss 3.112741615041159e-05 Validation loss 0.04482715204358101 Accuracy 0.8891250491142273\n",
      "Iteration 38130 Training loss 0.0012946715578436852 Validation loss 0.04477629065513611 Accuracy 0.8895000219345093\n",
      "Iteration 38140 Training loss 0.0012797042727470398 Validation loss 0.04481701925396919 Accuracy 0.8893750309944153\n",
      "Iteration 38150 Training loss 4.067166446475312e-05 Validation loss 0.044784028083086014 Accuracy 0.8895000219345093\n",
      "Iteration 38160 Training loss 3.001260120072402e-05 Validation loss 0.04480009526014328 Accuracy 0.8890000581741333\n",
      "Iteration 38170 Training loss 0.0025282332208007574 Validation loss 0.044935062527656555 Accuracy 0.8881250619888306\n",
      "Iteration 38180 Training loss 0.0013016951270401478 Validation loss 0.04486275836825371 Accuracy 0.8886250257492065\n",
      "Iteration 38190 Training loss 4.575037382892333e-05 Validation loss 0.04479033872485161 Accuracy 0.890125036239624\n",
      "Iteration 38200 Training loss 0.00130394846200943 Validation loss 0.044815417379140854 Accuracy 0.8890000581741333\n",
      "Iteration 38210 Training loss 4.867757888860069e-05 Validation loss 0.044773995876312256 Accuracy 0.8895000219345093\n",
      "Iteration 38220 Training loss 3.844236198347062e-05 Validation loss 0.04475681483745575 Accuracy 0.889750063419342\n",
      "Iteration 38230 Training loss 0.0013100382639095187 Validation loss 0.044794756919145584 Accuracy 0.8885000348091125\n",
      "Iteration 38240 Training loss 0.0012828607577830553 Validation loss 0.044794633984565735 Accuracy 0.8887500166893005\n",
      "Iteration 38250 Training loss 0.002537710126489401 Validation loss 0.044738445430994034 Accuracy 0.8896250128746033\n",
      "Iteration 38260 Training loss 0.0038277762942016125 Validation loss 0.04479753598570824 Accuracy 0.8887500166893005\n",
      "Iteration 38270 Training loss 0.0013054641894996166 Validation loss 0.0447283498942852 Accuracy 0.8896250128746033\n",
      "Iteration 38280 Training loss 0.00129222241230309 Validation loss 0.04474325478076935 Accuracy 0.889750063419342\n",
      "Iteration 38290 Training loss 0.0013010347029194236 Validation loss 0.04480578005313873 Accuracy 0.8886250257492065\n",
      "Iteration 38300 Training loss 0.0012950253440067172 Validation loss 0.044779591262340546 Accuracy 0.8891250491142273\n",
      "Iteration 38310 Training loss 0.0025486769154667854 Validation loss 0.044745054095983505 Accuracy 0.89000004529953\n",
      "Iteration 38320 Training loss 3.256760464864783e-05 Validation loss 0.044826239347457886 Accuracy 0.8895000219345093\n",
      "Iteration 38330 Training loss 4.896879181615077e-05 Validation loss 0.04475405067205429 Accuracy 0.89000004529953\n",
      "Iteration 38340 Training loss 5.253892595646903e-05 Validation loss 0.044787272810935974 Accuracy 0.8893750309944153\n",
      "Iteration 38350 Training loss 0.0013013046700507402 Validation loss 0.04478415101766586 Accuracy 0.8886250257492065\n",
      "Iteration 38360 Training loss 0.0025342365261167288 Validation loss 0.04476369544863701 Accuracy 0.8893750309944153\n",
      "Iteration 38370 Training loss 4.0033686673268676e-05 Validation loss 0.044818636029958725 Accuracy 0.8887500166893005\n",
      "Iteration 38380 Training loss 0.001285523409023881 Validation loss 0.04478727653622627 Accuracy 0.8891250491142273\n",
      "Iteration 38390 Training loss 0.002558276988565922 Validation loss 0.04480987787246704 Accuracy 0.8888750672340393\n",
      "Iteration 38400 Training loss 3.765934525290504e-05 Validation loss 0.04481249302625656 Accuracy 0.8888750672340393\n",
      "Iteration 38410 Training loss 0.003783073741942644 Validation loss 0.04478815197944641 Accuracy 0.8891250491142273\n",
      "Iteration 38420 Training loss 0.002530477475374937 Validation loss 0.044818487018346786 Accuracy 0.8890000581741333\n",
      "Iteration 38430 Training loss 0.0012874961830675602 Validation loss 0.044779594987630844 Accuracy 0.8887500166893005\n",
      "Iteration 38440 Training loss 0.0012846949975937605 Validation loss 0.044756706804037094 Accuracy 0.8882500529289246\n",
      "Iteration 38450 Training loss 0.0025247239973396063 Validation loss 0.04482511803507805 Accuracy 0.8886250257492065\n",
      "Iteration 38460 Training loss 0.0012935566483065486 Validation loss 0.04476421698927879 Accuracy 0.8893750309944153\n",
      "Iteration 38470 Training loss 0.0012783741112798452 Validation loss 0.04476630687713623 Accuracy 0.889750063419342\n",
      "Iteration 38480 Training loss 0.0012875390239059925 Validation loss 0.044837046414613724 Accuracy 0.8887500166893005\n",
      "Iteration 38490 Training loss 4.005128357675858e-05 Validation loss 0.04483175277709961 Accuracy 0.8892500400543213\n",
      "Iteration 38500 Training loss 0.003800060134381056 Validation loss 0.04477401822805405 Accuracy 0.889875054359436\n",
      "Iteration 38510 Training loss 0.0012884240131825209 Validation loss 0.04484513774514198 Accuracy 0.8883750438690186\n",
      "Iteration 38520 Training loss 0.003787036519497633 Validation loss 0.04478274658322334 Accuracy 0.8893750309944153\n",
      "Iteration 38530 Training loss 0.0025366442278027534 Validation loss 0.044782716780900955 Accuracy 0.8893750309944153\n",
      "Iteration 38540 Training loss 0.002546714385971427 Validation loss 0.044802311807870865 Accuracy 0.8891250491142273\n",
      "Iteration 38550 Training loss 0.002551234094426036 Validation loss 0.04483514279127121 Accuracy 0.8892500400543213\n",
      "Iteration 38560 Training loss 0.0012876433320343494 Validation loss 0.044865984469652176 Accuracy 0.8886250257492065\n",
      "Iteration 38570 Training loss 0.0012785136932507157 Validation loss 0.04484707862138748 Accuracy 0.8888750672340393\n",
      "Iteration 38580 Training loss 0.005038639064878225 Validation loss 0.04474194347858429 Accuracy 0.8893750309944153\n",
      "Iteration 38590 Training loss 0.001284077181480825 Validation loss 0.044795501977205276 Accuracy 0.8893750309944153\n",
      "Iteration 38600 Training loss 0.0012837594840675592 Validation loss 0.04476567357778549 Accuracy 0.8895000219345093\n",
      "Iteration 38610 Training loss 7.240425475174561e-05 Validation loss 0.044753920286893845 Accuracy 0.8890000581741333\n",
      "Iteration 38620 Training loss 0.0012962414184585214 Validation loss 0.04477393999695778 Accuracy 0.8887500166893005\n",
      "Iteration 38630 Training loss 0.002561435103416443 Validation loss 0.04479539021849632 Accuracy 0.8886250257492065\n",
      "Iteration 38640 Training loss 5.157349369255826e-05 Validation loss 0.044826507568359375 Accuracy 0.8886250257492065\n",
      "Iteration 38650 Training loss 3.443563400651328e-05 Validation loss 0.044875189661979675 Accuracy 0.8891250491142273\n",
      "Iteration 38660 Training loss 0.0025460352189838886 Validation loss 0.044916726648807526 Accuracy 0.8886250257492065\n",
      "Iteration 38670 Training loss 0.0013019550824537873 Validation loss 0.044850654900074005 Accuracy 0.8887500166893005\n",
      "Iteration 38680 Training loss 4.427511157700792e-05 Validation loss 0.044875018298625946 Accuracy 0.8886250257492065\n",
      "Iteration 38690 Training loss 0.0038229606579989195 Validation loss 0.044931359589099884 Accuracy 0.8888750672340393\n",
      "Iteration 38700 Training loss 0.0012855763779953122 Validation loss 0.0448608361184597 Accuracy 0.8891250491142273\n",
      "Iteration 38710 Training loss 3.898238719557412e-05 Validation loss 0.04489437863230705 Accuracy 0.8888750672340393\n",
      "Iteration 38720 Training loss 0.0025462997145950794 Validation loss 0.04489988833665848 Accuracy 0.8881250619888306\n",
      "Iteration 38730 Training loss 0.0037959169130772352 Validation loss 0.044867243617773056 Accuracy 0.8888750672340393\n",
      "Iteration 38740 Training loss 0.0012831997592002153 Validation loss 0.04487859457731247 Accuracy 0.8883750438690186\n",
      "Iteration 38750 Training loss 0.001286400482058525 Validation loss 0.04485297575592995 Accuracy 0.8886250257492065\n",
      "Iteration 38760 Training loss 0.0012903905007988214 Validation loss 0.044910039752721786 Accuracy 0.8882500529289246\n",
      "Iteration 38770 Training loss 0.001291610300540924 Validation loss 0.04488641396164894 Accuracy 0.8896250128746033\n",
      "Iteration 38780 Training loss 0.0037959341425448656 Validation loss 0.04484543576836586 Accuracy 0.8888750672340393\n",
      "Iteration 38790 Training loss 4.382896077004261e-05 Validation loss 0.04491075128316879 Accuracy 0.8882500529289246\n",
      "Iteration 38800 Training loss 0.002570999786257744 Validation loss 0.04482199251651764 Accuracy 0.8892500400543213\n",
      "Iteration 38810 Training loss 4.021361382910982e-05 Validation loss 0.04488964378833771 Accuracy 0.8883750438690186\n",
      "Iteration 38820 Training loss 0.002537349471822381 Validation loss 0.04490143060684204 Accuracy 0.8886250257492065\n",
      "Iteration 38830 Training loss 2.2365054974216036e-05 Validation loss 0.044947512447834015 Accuracy 0.8886250257492065\n",
      "Iteration 38840 Training loss 0.0012937613064423203 Validation loss 0.04484153166413307 Accuracy 0.8895000219345093\n",
      "Iteration 38850 Training loss 0.002543251495808363 Validation loss 0.044852714985609055 Accuracy 0.8893750309944153\n",
      "Iteration 38860 Training loss 3.481017120066099e-05 Validation loss 0.04482614994049072 Accuracy 0.8893750309944153\n",
      "Iteration 38870 Training loss 3.411087891436182e-05 Validation loss 0.04485759511590004 Accuracy 0.8891250491142273\n",
      "Iteration 38880 Training loss 0.0025549510028213263 Validation loss 0.044917795807123184 Accuracy 0.8885000348091125\n",
      "Iteration 38890 Training loss 3.267828287789598e-05 Validation loss 0.04490015655755997 Accuracy 0.8890000581741333\n",
      "Iteration 38900 Training loss 3.7039870221633464e-05 Validation loss 0.04488860443234444 Accuracy 0.8882500529289246\n",
      "Iteration 38910 Training loss 0.0013069204287603498 Validation loss 0.044837020337581635 Accuracy 0.8887500166893005\n",
      "Iteration 38920 Training loss 0.001295227324590087 Validation loss 0.0448642261326313 Accuracy 0.8890000581741333\n",
      "Iteration 38930 Training loss 0.0012851436622440815 Validation loss 0.044931430369615555 Accuracy 0.8886250257492065\n",
      "Iteration 38940 Training loss 0.0025444000493735075 Validation loss 0.04482969641685486 Accuracy 0.8891250491142273\n",
      "Iteration 38950 Training loss 4.733959212899208e-05 Validation loss 0.04492376744747162 Accuracy 0.8882500529289246\n",
      "Iteration 38960 Training loss 0.002562521258369088 Validation loss 0.04492141306400299 Accuracy 0.8892500400543213\n",
      "Iteration 38970 Training loss 0.002532655606046319 Validation loss 0.044874608516693115 Accuracy 0.8896250128746033\n",
      "Iteration 38980 Training loss 4.8938363761408255e-05 Validation loss 0.04483982175588608 Accuracy 0.889750063419342\n",
      "Iteration 38990 Training loss 0.0012981322361156344 Validation loss 0.044861551374197006 Accuracy 0.889750063419342\n",
      "Iteration 39000 Training loss 0.0013098272029310465 Validation loss 0.04494622349739075 Accuracy 0.8881250619888306\n",
      "Iteration 39010 Training loss 0.0012926540803164244 Validation loss 0.04489650949835777 Accuracy 0.8891250491142273\n",
      "Iteration 39020 Training loss 0.00129531545098871 Validation loss 0.04492582008242607 Accuracy 0.8883750438690186\n",
      "Iteration 39030 Training loss 3.7576719478238374e-05 Validation loss 0.0449000783264637 Accuracy 0.8882500529289246\n",
      "Iteration 39040 Training loss 0.0025404065381735563 Validation loss 0.04494532197713852 Accuracy 0.8883750438690186\n",
      "Iteration 39050 Training loss 0.0025406978093087673 Validation loss 0.04481951519846916 Accuracy 0.8896250128746033\n",
      "Iteration 39060 Training loss 4.432613786775619e-05 Validation loss 0.044891323894262314 Accuracy 0.8886250257492065\n",
      "Iteration 39070 Training loss 0.0012933778343722224 Validation loss 0.04488100856542587 Accuracy 0.8890000581741333\n",
      "Iteration 39080 Training loss 0.0012862102594226599 Validation loss 0.04486135020852089 Accuracy 0.889750063419342\n",
      "Iteration 39090 Training loss 0.0012769136810675263 Validation loss 0.04488583281636238 Accuracy 0.8887500166893005\n",
      "Iteration 39100 Training loss 0.002541234716773033 Validation loss 0.04485607519745827 Accuracy 0.8895000219345093\n",
      "Iteration 39110 Training loss 0.002543147187680006 Validation loss 0.04487035050988197 Accuracy 0.8891250491142273\n",
      "Iteration 39120 Training loss 6.830226629972458e-05 Validation loss 0.04489987716078758 Accuracy 0.8891250491142273\n",
      "Iteration 39130 Training loss 0.0013052012072876096 Validation loss 0.044906388968229294 Accuracy 0.8882500529289246\n",
      "Iteration 39140 Training loss 0.003792357863858342 Validation loss 0.044871095567941666 Accuracy 0.8887500166893005\n",
      "Iteration 39150 Training loss 0.0012739610392600298 Validation loss 0.044895824044942856 Accuracy 0.8890000581741333\n",
      "Iteration 39160 Training loss 4.456374517758377e-05 Validation loss 0.04492620751261711 Accuracy 0.8883750438690186\n",
      "Iteration 39170 Training loss 3.421595829422586e-05 Validation loss 0.044847145676612854 Accuracy 0.890125036239624\n",
      "Iteration 39180 Training loss 3.4689590393099934e-05 Validation loss 0.044937070459127426 Accuracy 0.8887500166893005\n",
      "Iteration 39190 Training loss 0.0025280171539634466 Validation loss 0.04490279406309128 Accuracy 0.8887500166893005\n",
      "Iteration 39200 Training loss 0.0012821756536141038 Validation loss 0.04488053172826767 Accuracy 0.8888750672340393\n",
      "Iteration 39210 Training loss 5.806448825751431e-05 Validation loss 0.044893402606248856 Accuracy 0.8887500166893005\n",
      "Iteration 39220 Training loss 3.530428512021899e-05 Validation loss 0.04490721970796585 Accuracy 0.8886250257492065\n",
      "Iteration 39230 Training loss 0.001292706816457212 Validation loss 0.044936083257198334 Accuracy 0.8885000348091125\n",
      "Iteration 39240 Training loss 0.002529506105929613 Validation loss 0.04490149766206741 Accuracy 0.8891250491142273\n",
      "Iteration 39250 Training loss 0.0025350700598210096 Validation loss 0.04486989974975586 Accuracy 0.8893750309944153\n",
      "Iteration 39260 Training loss 0.003791258903220296 Validation loss 0.044877056032419205 Accuracy 0.8893750309944153\n",
      "Iteration 39270 Training loss 0.0012866497272625566 Validation loss 0.04488677531480789 Accuracy 0.8887500166893005\n",
      "Iteration 39280 Training loss 3.558929165592417e-05 Validation loss 0.04488692060112953 Accuracy 0.8890000581741333\n",
      "Iteration 39290 Training loss 4.282372538000345e-05 Validation loss 0.04491903632879257 Accuracy 0.8890000581741333\n",
      "Iteration 39300 Training loss 0.0025553801096975803 Validation loss 0.044885966926813126 Accuracy 0.8891250491142273\n",
      "Iteration 39310 Training loss 5.733443686040118e-05 Validation loss 0.044941652566194534 Accuracy 0.8885000348091125\n",
      "Iteration 39320 Training loss 5.352316293283366e-05 Validation loss 0.0448896586894989 Accuracy 0.8892500400543213\n",
      "Iteration 39330 Training loss 3.0816099751973525e-05 Validation loss 0.04489390179514885 Accuracy 0.8893750309944153\n",
      "Iteration 39340 Training loss 0.0025298409163951874 Validation loss 0.04490037262439728 Accuracy 0.8892500400543213\n",
      "Iteration 39350 Training loss 6.717612995998934e-05 Validation loss 0.044844698160886765 Accuracy 0.8896250128746033\n",
      "Iteration 39360 Training loss 0.0012881513684988022 Validation loss 0.04485293850302696 Accuracy 0.89000004529953\n",
      "Iteration 39370 Training loss 0.002540705958381295 Validation loss 0.044867560267448425 Accuracy 0.8890000581741333\n",
      "Iteration 39380 Training loss 3.532808841555379e-05 Validation loss 0.04477103427052498 Accuracy 0.890250027179718\n",
      "Iteration 39390 Training loss 0.002535962499678135 Validation loss 0.04488087072968483 Accuracy 0.8888750672340393\n",
      "Iteration 39400 Training loss 4.1205857996828854e-05 Validation loss 0.04487013444304466 Accuracy 0.8893750309944153\n",
      "Iteration 39410 Training loss 4.0759423427516595e-05 Validation loss 0.044874660670757294 Accuracy 0.8888750672340393\n",
      "Iteration 39420 Training loss 3.864348036586307e-05 Validation loss 0.044903580099344254 Accuracy 0.8886250257492065\n",
      "Iteration 39430 Training loss 3.068489968427457e-05 Validation loss 0.04483436048030853 Accuracy 0.8892500400543213\n",
      "Iteration 39440 Training loss 0.0025371324736624956 Validation loss 0.044902004301548004 Accuracy 0.8886250257492065\n",
      "Iteration 39450 Training loss 0.0025339575950056314 Validation loss 0.04489380866289139 Accuracy 0.8886250257492065\n",
      "Iteration 39460 Training loss 7.066554826451465e-05 Validation loss 0.04491943493485451 Accuracy 0.8887500166893005\n",
      "Iteration 39470 Training loss 4.550932862912305e-05 Validation loss 0.04489504545927048 Accuracy 0.8891250491142273\n",
      "Iteration 39480 Training loss 2.9562468625954352e-05 Validation loss 0.0448307991027832 Accuracy 0.8891250491142273\n",
      "Iteration 39490 Training loss 0.0025366665795445442 Validation loss 0.0448809452354908 Accuracy 0.8895000219345093\n",
      "Iteration 39500 Training loss 3.2318686862709e-05 Validation loss 0.04490058869123459 Accuracy 0.8891250491142273\n",
      "Iteration 39510 Training loss 4.1836359741864726e-05 Validation loss 0.044936951249837875 Accuracy 0.8888750672340393\n",
      "Iteration 39520 Training loss 4.007200550404377e-05 Validation loss 0.04491785168647766 Accuracy 0.8891250491142273\n",
      "Iteration 39530 Training loss 0.00377654773183167 Validation loss 0.04485737159848213 Accuracy 0.889875054359436\n",
      "Iteration 39540 Training loss 3.9703099901089445e-05 Validation loss 0.04481004923582077 Accuracy 0.8905000686645508\n",
      "Iteration 39550 Training loss 0.0012877999106422067 Validation loss 0.04491744562983513 Accuracy 0.8892500400543213\n",
      "Iteration 39560 Training loss 0.0012985134962946177 Validation loss 0.04493388533592224 Accuracy 0.8893750309944153\n",
      "Iteration 39570 Training loss 0.0012765442952513695 Validation loss 0.04489705711603165 Accuracy 0.889750063419342\n",
      "Iteration 39580 Training loss 0.002542105969041586 Validation loss 0.04490773379802704 Accuracy 0.889750063419342\n",
      "Iteration 39590 Training loss 2.757079346338287e-05 Validation loss 0.044878944754600525 Accuracy 0.8892500400543213\n",
      "Iteration 39600 Training loss 0.0012861897703260183 Validation loss 0.04485290125012398 Accuracy 0.889750063419342\n",
      "Iteration 39610 Training loss 0.0013154707849025726 Validation loss 0.04485565423965454 Accuracy 0.8896250128746033\n",
      "Iteration 39620 Training loss 0.0012853204971179366 Validation loss 0.04489675909280777 Accuracy 0.8891250491142273\n",
      "Iteration 39630 Training loss 4.4686177716357633e-05 Validation loss 0.04486186057329178 Accuracy 0.8896250128746033\n",
      "Iteration 39640 Training loss 0.0012908735079690814 Validation loss 0.04487031325697899 Accuracy 0.8887500166893005\n",
      "Iteration 39650 Training loss 0.0037797351833432913 Validation loss 0.04484231770038605 Accuracy 0.890125036239624\n",
      "Iteration 39660 Training loss 0.0025368572678416967 Validation loss 0.04491593316197395 Accuracy 0.8888750672340393\n",
      "Iteration 39670 Training loss 0.0038065484259277582 Validation loss 0.04486628621816635 Accuracy 0.8892500400543213\n",
      "Iteration 39680 Training loss 2.834443330357317e-05 Validation loss 0.04486782103776932 Accuracy 0.8893750309944153\n",
      "Iteration 39690 Training loss 0.005030239000916481 Validation loss 0.044919099658727646 Accuracy 0.8885000348091125\n",
      "Iteration 39700 Training loss 0.002521574031561613 Validation loss 0.04488128796219826 Accuracy 0.8888750672340393\n",
      "Iteration 39710 Training loss 0.001283960766158998 Validation loss 0.044857267290353775 Accuracy 0.8892500400543213\n",
      "Iteration 39720 Training loss 0.0037721258122473955 Validation loss 0.04489613696932793 Accuracy 0.8890000581741333\n",
      "Iteration 39730 Training loss 0.0012755192583426833 Validation loss 0.04486924782395363 Accuracy 0.8888750672340393\n",
      "Iteration 39740 Training loss 5.596683331532404e-05 Validation loss 0.04485921561717987 Accuracy 0.8892500400543213\n",
      "Iteration 39750 Training loss 0.0037785416934639215 Validation loss 0.04490736499428749 Accuracy 0.8887500166893005\n",
      "Iteration 39760 Training loss 0.0013078355696052313 Validation loss 0.044896386563777924 Accuracy 0.8887500166893005\n",
      "Iteration 39770 Training loss 0.0012921405723318458 Validation loss 0.04486868530511856 Accuracy 0.8892500400543213\n",
      "Iteration 39780 Training loss 3.630921128205955e-05 Validation loss 0.04491950571537018 Accuracy 0.8892500400543213\n",
      "Iteration 39790 Training loss 0.0012838036054745317 Validation loss 0.04489579424262047 Accuracy 0.8893750309944153\n",
      "Iteration 39800 Training loss 0.0037881957832723856 Validation loss 0.044892288744449615 Accuracy 0.8887500166893005\n",
      "Iteration 39810 Training loss 0.0012736208736896515 Validation loss 0.044938940554857254 Accuracy 0.8878750205039978\n",
      "Iteration 39820 Training loss 0.0012865631142631173 Validation loss 0.04486856237053871 Accuracy 0.8891250491142273\n",
      "Iteration 39830 Training loss 0.0037747700698673725 Validation loss 0.04483895003795624 Accuracy 0.8893750309944153\n",
      "Iteration 39840 Training loss 3.0488376069115475e-05 Validation loss 0.044886551797389984 Accuracy 0.8893750309944153\n",
      "Iteration 39850 Training loss 0.0012969688978046179 Validation loss 0.044906627386808395 Accuracy 0.8892500400543213\n",
      "Iteration 39860 Training loss 4.5738997869193554e-05 Validation loss 0.04491792619228363 Accuracy 0.8895000219345093\n",
      "Iteration 39870 Training loss 3.471761738182977e-05 Validation loss 0.04492785409092903 Accuracy 0.8896250128746033\n",
      "Iteration 39880 Training loss 3.0158456866047345e-05 Validation loss 0.04493316262960434 Accuracy 0.8887500166893005\n",
      "Iteration 39890 Training loss 0.005033525638282299 Validation loss 0.044926706701517105 Accuracy 0.8891250491142273\n",
      "Iteration 39900 Training loss 0.0037716676015406847 Validation loss 0.044960711151361465 Accuracy 0.8888750672340393\n",
      "Iteration 39910 Training loss 0.0025203258264809847 Validation loss 0.04494661092758179 Accuracy 0.8895000219345093\n",
      "Iteration 39920 Training loss 3.056881905649789e-05 Validation loss 0.04493524506688118 Accuracy 0.8888750672340393\n",
      "Iteration 39930 Training loss 0.0012821576092392206 Validation loss 0.044949281960725784 Accuracy 0.8887500166893005\n",
      "Iteration 39940 Training loss 0.003804879728704691 Validation loss 0.04503905028104782 Accuracy 0.8886250257492065\n",
      "Iteration 39950 Training loss 4.2393923649797216e-05 Validation loss 0.04497085139155388 Accuracy 0.8893750309944153\n",
      "Iteration 39960 Training loss 3.537302472977899e-05 Validation loss 0.04495474323630333 Accuracy 0.8891250491142273\n",
      "Iteration 39970 Training loss 0.0012921224115416408 Validation loss 0.044989168643951416 Accuracy 0.8892500400543213\n",
      "Iteration 39980 Training loss 0.0025260585825890303 Validation loss 0.044965457171201706 Accuracy 0.8886250257492065\n",
      "Iteration 39990 Training loss 3.2620937417959794e-05 Validation loss 0.044984206557273865 Accuracy 0.8890000581741333\n",
      "Iteration 40000 Training loss 0.0012871120125055313 Validation loss 0.044966597110033035 Accuracy 0.8891250491142273\n",
      "Iteration 40010 Training loss 0.0012888521887362003 Validation loss 0.04493189603090286 Accuracy 0.8896250128746033\n",
      "Iteration 40020 Training loss 0.0025468431413173676 Validation loss 0.04498319327831268 Accuracy 0.8890000581741333\n",
      "Iteration 40030 Training loss 0.002538456581532955 Validation loss 0.04500593617558479 Accuracy 0.8888750672340393\n",
      "Iteration 40040 Training loss 0.0012775167124345899 Validation loss 0.04496181011199951 Accuracy 0.8886250257492065\n",
      "Iteration 40050 Training loss 0.0025310134515166283 Validation loss 0.0450144037604332 Accuracy 0.8882500529289246\n",
      "Iteration 40060 Training loss 0.003778124460950494 Validation loss 0.044955797493457794 Accuracy 0.8890000581741333\n",
      "Iteration 40070 Training loss 3.111843761871569e-05 Validation loss 0.04495043307542801 Accuracy 0.8890000581741333\n",
      "Iteration 40080 Training loss 0.002535850740969181 Validation loss 0.04496420547366142 Accuracy 0.8892500400543213\n",
      "Iteration 40090 Training loss 0.0012896445114165545 Validation loss 0.04495523124933243 Accuracy 0.8888750672340393\n",
      "Iteration 40100 Training loss 0.005033425055444241 Validation loss 0.04493636637926102 Accuracy 0.8890000581741333\n",
      "Iteration 40110 Training loss 0.002530490979552269 Validation loss 0.04491369053721428 Accuracy 0.8895000219345093\n",
      "Iteration 40120 Training loss 0.001279271556995809 Validation loss 0.04487075284123421 Accuracy 0.8895000219345093\n",
      "Iteration 40130 Training loss 3.96093528252095e-05 Validation loss 0.0449024923145771 Accuracy 0.8891250491142273\n",
      "Iteration 40140 Training loss 0.0012956107966601849 Validation loss 0.04489019885659218 Accuracy 0.8893750309944153\n",
      "Iteration 40150 Training loss 0.0025480305776000023 Validation loss 0.04491983354091644 Accuracy 0.8891250491142273\n",
      "Iteration 40160 Training loss 0.001295491587370634 Validation loss 0.044982872903347015 Accuracy 0.8881250619888306\n",
      "Iteration 40170 Training loss 3.6115430702921e-05 Validation loss 0.044883694499731064 Accuracy 0.89000004529953\n",
      "Iteration 40180 Training loss 0.0012797022936865687 Validation loss 0.044929977506399155 Accuracy 0.889750063419342\n",
      "Iteration 40190 Training loss 3.819930134341121e-05 Validation loss 0.04495137184858322 Accuracy 0.8895000219345093\n",
      "Iteration 40200 Training loss 2.9514310881495476e-05 Validation loss 0.04497597739100456 Accuracy 0.8888750672340393\n",
      "Iteration 40210 Training loss 0.0012974751880392432 Validation loss 0.04499828442931175 Accuracy 0.8890000581741333\n",
      "Iteration 40220 Training loss 0.0012761717662215233 Validation loss 0.045007869601249695 Accuracy 0.8887500166893005\n",
      "Iteration 40230 Training loss 0.003770500421524048 Validation loss 0.04496266320347786 Accuracy 0.8887500166893005\n",
      "Iteration 40240 Training loss 3.096655564149842e-05 Validation loss 0.0449557825922966 Accuracy 0.8893750309944153\n",
      "Iteration 40250 Training loss 0.0012732131872326136 Validation loss 0.04498399794101715 Accuracy 0.8890000581741333\n",
      "Iteration 40260 Training loss 0.0012966226786375046 Validation loss 0.044999077916145325 Accuracy 0.8891250491142273\n",
      "Iteration 40270 Training loss 3.678752909763716e-05 Validation loss 0.044971708208322525 Accuracy 0.8891250491142273\n",
      "Iteration 40280 Training loss 0.005039143841713667 Validation loss 0.04495398700237274 Accuracy 0.8893750309944153\n",
      "Iteration 40290 Training loss 2.8217706130817533e-05 Validation loss 0.04495054855942726 Accuracy 0.8893750309944153\n",
      "Iteration 40300 Training loss 0.00129688810557127 Validation loss 0.04498625174164772 Accuracy 0.8887500166893005\n",
      "Iteration 40310 Training loss 0.0012830753112211823 Validation loss 0.04495789855718613 Accuracy 0.8896250128746033\n",
      "Iteration 40320 Training loss 0.002546540228649974 Validation loss 0.04499261826276779 Accuracy 0.8886250257492065\n",
      "Iteration 40330 Training loss 0.001278444193303585 Validation loss 0.04495014250278473 Accuracy 0.8893750309944153\n",
      "Iteration 40340 Training loss 2.852920806617476e-05 Validation loss 0.04498027265071869 Accuracy 0.8888750672340393\n",
      "Iteration 40350 Training loss 0.0025414307601749897 Validation loss 0.044954750686883926 Accuracy 0.8891250491142273\n",
      "Iteration 40360 Training loss 0.0037879864685237408 Validation loss 0.044929083436727524 Accuracy 0.8895000219345093\n",
      "Iteration 40370 Training loss 0.002526735421270132 Validation loss 0.04493908956646919 Accuracy 0.889875054359436\n",
      "Iteration 40380 Training loss 3.7025696656201035e-05 Validation loss 0.04491938278079033 Accuracy 0.889750063419342\n",
      "Iteration 40390 Training loss 3.976829248131253e-05 Validation loss 0.044997889548540115 Accuracy 0.8888750672340393\n",
      "Iteration 40400 Training loss 0.0013142635580152273 Validation loss 0.04493040591478348 Accuracy 0.889750063419342\n",
      "Iteration 40410 Training loss 3.887151615344919e-05 Validation loss 0.044944148510694504 Accuracy 0.889750063419342\n",
      "Iteration 40420 Training loss 3.1203930120682344e-05 Validation loss 0.04495731368660927 Accuracy 0.8896250128746033\n",
      "Iteration 40430 Training loss 0.0012802723795175552 Validation loss 0.045023515820503235 Accuracy 0.8888750672340393\n",
      "Iteration 40440 Training loss 4.5845725253457204e-05 Validation loss 0.04499411955475807 Accuracy 0.8887500166893005\n",
      "Iteration 40450 Training loss 0.003774728626012802 Validation loss 0.04500345140695572 Accuracy 0.8886250257492065\n",
      "Iteration 40460 Training loss 4.214874206809327e-05 Validation loss 0.04500620812177658 Accuracy 0.8887500166893005\n",
      "Iteration 40470 Training loss 5.1640297897392884e-05 Validation loss 0.04496742784976959 Accuracy 0.8893750309944153\n",
      "Iteration 40480 Training loss 0.0025277817621827126 Validation loss 0.04495051130652428 Accuracy 0.8893750309944153\n",
      "Iteration 40490 Training loss 0.0025404568295925856 Validation loss 0.04499322175979614 Accuracy 0.8887500166893005\n",
      "Iteration 40500 Training loss 0.0012788429157808423 Validation loss 0.04505448415875435 Accuracy 0.8886250257492065\n",
      "Iteration 40510 Training loss 4.5056054659653455e-05 Validation loss 0.04497692734003067 Accuracy 0.8893750309944153\n",
      "Iteration 40520 Training loss 0.0013071547728031874 Validation loss 0.044946879148483276 Accuracy 0.89000004529953\n",
      "Iteration 40530 Training loss 0.0037802148144692183 Validation loss 0.04501625895500183 Accuracy 0.8881250619888306\n",
      "Iteration 40540 Training loss 0.0012869586935266852 Validation loss 0.0449928417801857 Accuracy 0.8891250491142273\n",
      "Iteration 40550 Training loss 0.0012943055480718613 Validation loss 0.044989198446273804 Accuracy 0.8891250491142273\n",
      "Iteration 40560 Training loss 0.001288160914555192 Validation loss 0.044968921691179276 Accuracy 0.889750063419342\n",
      "Iteration 40570 Training loss 4.855001316173002e-05 Validation loss 0.045020636171102524 Accuracy 0.8886250257492065\n",
      "Iteration 40580 Training loss 0.005044345278292894 Validation loss 0.04500027000904083 Accuracy 0.8891250491142273\n",
      "Iteration 40590 Training loss 0.0012852820800617337 Validation loss 0.045000240206718445 Accuracy 0.8888750672340393\n",
      "Iteration 40600 Training loss 2.8542959626065567e-05 Validation loss 0.04498516395688057 Accuracy 0.8891250491142273\n",
      "Iteration 40610 Training loss 0.0025274932850152254 Validation loss 0.04498356580734253 Accuracy 0.8892500400543213\n",
      "Iteration 40620 Training loss 0.002534241881221533 Validation loss 0.04496133700013161 Accuracy 0.8891250491142273\n",
      "Iteration 40630 Training loss 3.4257751394761726e-05 Validation loss 0.045023318380117416 Accuracy 0.8885000348091125\n",
      "Iteration 40640 Training loss 0.001287186169065535 Validation loss 0.04495455324649811 Accuracy 0.8888750672340393\n",
      "Iteration 40650 Training loss 0.003772579599171877 Validation loss 0.044966280460357666 Accuracy 0.8888750672340393\n",
      "Iteration 40660 Training loss 0.0025332977529615164 Validation loss 0.04498295485973358 Accuracy 0.8890000581741333\n",
      "Iteration 40670 Training loss 0.002532268175855279 Validation loss 0.044926878064870834 Accuracy 0.889750063419342\n",
      "Iteration 40680 Training loss 0.0012827265309169888 Validation loss 0.0449780598282814 Accuracy 0.8888750672340393\n",
      "Iteration 40690 Training loss 2.5696299417177215e-05 Validation loss 0.04499717429280281 Accuracy 0.8883750438690186\n",
      "Iteration 40700 Training loss 0.0012885688338428736 Validation loss 0.04492875188589096 Accuracy 0.8895000219345093\n",
      "Iteration 40710 Training loss 0.0025455751456320286 Validation loss 0.04495925456285477 Accuracy 0.8892500400543213\n",
      "Iteration 40720 Training loss 2.3255601263372228e-05 Validation loss 0.04492425546050072 Accuracy 0.8892500400543213\n",
      "Iteration 40730 Training loss 3.3225620427401736e-05 Validation loss 0.04495267570018768 Accuracy 0.8895000219345093\n",
      "Iteration 40740 Training loss 3.1917617889121175e-05 Validation loss 0.04492688551545143 Accuracy 0.8895000219345093\n",
      "Iteration 40750 Training loss 0.0012810762273147702 Validation loss 0.044960152357816696 Accuracy 0.8892500400543213\n",
      "Iteration 40760 Training loss 0.002536870539188385 Validation loss 0.04496905207633972 Accuracy 0.8887500166893005\n",
      "Iteration 40770 Training loss 0.0012803105637431145 Validation loss 0.044962961226701736 Accuracy 0.8892500400543213\n",
      "Iteration 40780 Training loss 2.2986321710050106e-05 Validation loss 0.04495927691459656 Accuracy 0.8892500400543213\n",
      "Iteration 40790 Training loss 0.003780082333832979 Validation loss 0.04497150704264641 Accuracy 0.8892500400543213\n",
      "Iteration 40800 Training loss 0.0012991902185603976 Validation loss 0.04503301531076431 Accuracy 0.8886250257492065\n",
      "Iteration 40810 Training loss 0.0012742583639919758 Validation loss 0.044972121715545654 Accuracy 0.8893750309944153\n",
      "Iteration 40820 Training loss 0.0025238776579499245 Validation loss 0.04494234919548035 Accuracy 0.8896250128746033\n",
      "Iteration 40830 Training loss 0.001272750785574317 Validation loss 0.0449753999710083 Accuracy 0.8893750309944153\n",
      "Iteration 40840 Training loss 3.074167398153804e-05 Validation loss 0.04493678733706474 Accuracy 0.889875054359436\n",
      "Iteration 40850 Training loss 0.001276038121432066 Validation loss 0.044943444430828094 Accuracy 0.8896250128746033\n",
      "Iteration 40860 Training loss 0.0013128338614478707 Validation loss 0.044968362897634506 Accuracy 0.8893750309944153\n",
      "Iteration 40870 Training loss 0.001290197018533945 Validation loss 0.04492275416851044 Accuracy 0.890125036239624\n",
      "Iteration 40880 Training loss 0.0012844076845794916 Validation loss 0.04492167755961418 Accuracy 0.8896250128746033\n",
      "Iteration 40890 Training loss 0.0025412049144506454 Validation loss 0.044965196400880814 Accuracy 0.8890000581741333\n",
      "Iteration 40900 Training loss 0.0025316765531897545 Validation loss 0.04490971192717552 Accuracy 0.8893750309944153\n",
      "Iteration 40910 Training loss 4.6630524593638256e-05 Validation loss 0.0449422150850296 Accuracy 0.8892500400543213\n",
      "Iteration 40920 Training loss 0.002533175051212311 Validation loss 0.04498438537120819 Accuracy 0.8882500529289246\n",
      "Iteration 40930 Training loss 0.0012763767736032605 Validation loss 0.044948138296604156 Accuracy 0.8891250491142273\n",
      "Iteration 40940 Training loss 0.0012775521026924253 Validation loss 0.04496331140398979 Accuracy 0.8895000219345093\n",
      "Iteration 40950 Training loss 0.0012885764008387923 Validation loss 0.04504767432808876 Accuracy 0.8882500529289246\n",
      "Iteration 40960 Training loss 0.001286828308366239 Validation loss 0.044942088425159454 Accuracy 0.89000004529953\n",
      "Iteration 40970 Training loss 0.0013012723065912724 Validation loss 0.044978830963373184 Accuracy 0.8892500400543213\n",
      "Iteration 40980 Training loss 0.0050394064746797085 Validation loss 0.045027051120996475 Accuracy 0.8891250491142273\n",
      "Iteration 40990 Training loss 0.0012879472924396396 Validation loss 0.04495134949684143 Accuracy 0.8896250128746033\n",
      "Iteration 41000 Training loss 0.0012911410303786397 Validation loss 0.0449800044298172 Accuracy 0.8887500166893005\n",
      "Iteration 41010 Training loss 3.646587720140815e-05 Validation loss 0.044970497488975525 Accuracy 0.8892500400543213\n",
      "Iteration 41020 Training loss 0.002533040940761566 Validation loss 0.044970493763685226 Accuracy 0.8887500166893005\n",
      "Iteration 41030 Training loss 5.103915827930905e-05 Validation loss 0.04504567012190819 Accuracy 0.8877500295639038\n",
      "Iteration 41040 Training loss 0.0012806605082005262 Validation loss 0.04501596838235855 Accuracy 0.8886250257492065\n",
      "Iteration 41050 Training loss 0.001285383477807045 Validation loss 0.044999994337558746 Accuracy 0.8882500529289246\n",
      "Iteration 41060 Training loss 3.7991339922882617e-05 Validation loss 0.04500927776098251 Accuracy 0.8886250257492065\n",
      "Iteration 41070 Training loss 0.0012908893404528499 Validation loss 0.0450623482465744 Accuracy 0.8881250619888306\n",
      "Iteration 41080 Training loss 2.7073201636085287e-05 Validation loss 0.045027848333120346 Accuracy 0.8886250257492065\n",
      "Iteration 41090 Training loss 0.0025299477856606245 Validation loss 0.04503387585282326 Accuracy 0.8881250619888306\n",
      "Iteration 41100 Training loss 3.1789862987352535e-05 Validation loss 0.04495541751384735 Accuracy 0.8891250491142273\n",
      "Iteration 41110 Training loss 0.002533768070861697 Validation loss 0.04503057152032852 Accuracy 0.8887500166893005\n",
      "Iteration 41120 Training loss 3.442605520831421e-05 Validation loss 0.045062534511089325 Accuracy 0.8882500529289246\n",
      "Iteration 41130 Training loss 0.002539729466661811 Validation loss 0.04497252032160759 Accuracy 0.8890000581741333\n",
      "Iteration 41140 Training loss 3.4589695133036e-05 Validation loss 0.045010458678007126 Accuracy 0.8888750672340393\n",
      "Iteration 41150 Training loss 0.0012978677405044436 Validation loss 0.044968895614147186 Accuracy 0.8893750309944153\n",
      "Iteration 41160 Training loss 0.001291449647396803 Validation loss 0.04505153000354767 Accuracy 0.8881250619888306\n",
      "Iteration 41170 Training loss 0.0012815964873880148 Validation loss 0.04500410333275795 Accuracy 0.8888750672340393\n",
      "Iteration 41180 Training loss 0.002529613673686981 Validation loss 0.04500017315149307 Accuracy 0.8887500166893005\n",
      "Iteration 41190 Training loss 0.0012789549073204398 Validation loss 0.044964954257011414 Accuracy 0.8892500400543213\n",
      "Iteration 41200 Training loss 5.080369010102004e-05 Validation loss 0.044994812458753586 Accuracy 0.8890000581741333\n",
      "Iteration 41210 Training loss 0.0012740334495902061 Validation loss 0.045013148337602615 Accuracy 0.8890000581741333\n",
      "Iteration 41220 Training loss 0.0025412633549422026 Validation loss 0.04507805034518242 Accuracy 0.8875000476837158\n",
      "Iteration 41230 Training loss 0.0025329776108264923 Validation loss 0.04502275213599205 Accuracy 0.8886250257492065\n",
      "Iteration 41240 Training loss 0.005027104634791613 Validation loss 0.04503084719181061 Accuracy 0.8878750205039978\n",
      "Iteration 41250 Training loss 0.0037736035883426666 Validation loss 0.04501911252737045 Accuracy 0.8887500166893005\n",
      "Iteration 41260 Training loss 2.8060820113751106e-05 Validation loss 0.04497760161757469 Accuracy 0.8891250491142273\n",
      "Iteration 41270 Training loss 0.0025342588778585196 Validation loss 0.04497475177049637 Accuracy 0.8887500166893005\n",
      "Iteration 41280 Training loss 0.0025526429526507854 Validation loss 0.044995322823524475 Accuracy 0.8893750309944153\n",
      "Iteration 41290 Training loss 3.097389344475232e-05 Validation loss 0.045045915991067886 Accuracy 0.8881250619888306\n",
      "Iteration 41300 Training loss 0.0012815692462027073 Validation loss 0.04499061778187752 Accuracy 0.8891250491142273\n",
      "Iteration 41310 Training loss 0.0012773998314514756 Validation loss 0.045023638755083084 Accuracy 0.8888750672340393\n",
      "Iteration 41320 Training loss 0.002528515411540866 Validation loss 0.04498598724603653 Accuracy 0.8891250491142273\n",
      "Iteration 41330 Training loss 0.00252704881131649 Validation loss 0.04504500329494476 Accuracy 0.8893750309944153\n",
      "Iteration 41340 Training loss 0.002533172955736518 Validation loss 0.0450061671435833 Accuracy 0.8888750672340393\n",
      "Iteration 41350 Training loss 0.0012728075962513685 Validation loss 0.04499262198805809 Accuracy 0.8892500400543213\n",
      "Iteration 41360 Training loss 0.0025398798752576113 Validation loss 0.04501083120703697 Accuracy 0.8892500400543213\n",
      "Iteration 41370 Training loss 2.3815111489966512e-05 Validation loss 0.04497629031538963 Accuracy 0.89000004529953\n",
      "Iteration 41380 Training loss 0.0012847303878515959 Validation loss 0.04501741752028465 Accuracy 0.8887500166893005\n",
      "Iteration 41390 Training loss 0.002525994088500738 Validation loss 0.04499062895774841 Accuracy 0.8895000219345093\n",
      "Iteration 41400 Training loss 0.005037638358771801 Validation loss 0.04495411738753319 Accuracy 0.889750063419342\n",
      "Iteration 41410 Training loss 0.0012759241508319974 Validation loss 0.04500506445765495 Accuracy 0.8893750309944153\n",
      "Iteration 41420 Training loss 2.3177983166533522e-05 Validation loss 0.04501430690288544 Accuracy 0.8892500400543213\n",
      "Iteration 41430 Training loss 5.16313812113367e-05 Validation loss 0.04506785795092583 Accuracy 0.8888750672340393\n",
      "Iteration 41440 Training loss 3.418242704356089e-05 Validation loss 0.04503113031387329 Accuracy 0.8892500400543213\n",
      "Iteration 41450 Training loss 0.0012836046516895294 Validation loss 0.04500534385442734 Accuracy 0.889750063419342\n",
      "Iteration 41460 Training loss 2.5915747755789198e-05 Validation loss 0.04503310099244118 Accuracy 0.8891250491142273\n",
      "Iteration 41470 Training loss 2.187534119002521e-05 Validation loss 0.04502004012465477 Accuracy 0.8893750309944153\n",
      "Iteration 41480 Training loss 3.2372558052884415e-05 Validation loss 0.04502510651946068 Accuracy 0.8888750672340393\n",
      "Iteration 41490 Training loss 2.808350109262392e-05 Validation loss 0.045018669217824936 Accuracy 0.8892500400543213\n",
      "Iteration 41500 Training loss 3.4436045098118484e-05 Validation loss 0.04502556473016739 Accuracy 0.8888750672340393\n",
      "Iteration 41510 Training loss 0.0012851585634052753 Validation loss 0.04503422603011131 Accuracy 0.8892500400543213\n",
      "Iteration 41520 Training loss 0.001282205106690526 Validation loss 0.04501883313059807 Accuracy 0.8896250128746033\n",
      "Iteration 41530 Training loss 0.001269937725737691 Validation loss 0.0449821874499321 Accuracy 0.8896250128746033\n",
      "Iteration 41540 Training loss 0.0037765600718557835 Validation loss 0.044990044087171555 Accuracy 0.8890000581741333\n",
      "Iteration 41550 Training loss 0.0012759066885337234 Validation loss 0.044962700456380844 Accuracy 0.8885000348091125\n",
      "Iteration 41560 Training loss 0.002523597329854965 Validation loss 0.04499758407473564 Accuracy 0.8886250257492065\n",
      "Iteration 41570 Training loss 5.205915294936858e-05 Validation loss 0.04498430714011192 Accuracy 0.8896250128746033\n",
      "Iteration 41580 Training loss 0.0025305168237537146 Validation loss 0.045020539313554764 Accuracy 0.8887500166893005\n",
      "Iteration 41590 Training loss 0.002538929460570216 Validation loss 0.04499984160065651 Accuracy 0.8888750672340393\n",
      "Iteration 41600 Training loss 0.0012973797274753451 Validation loss 0.045048076659440994 Accuracy 0.8886250257492065\n",
      "Iteration 41610 Training loss 0.0012863719603046775 Validation loss 0.0449942871928215 Accuracy 0.8887500166893005\n",
      "Iteration 41620 Training loss 0.005033629946410656 Validation loss 0.04500564560294151 Accuracy 0.8895000219345093\n",
      "Iteration 41630 Training loss 0.0025271489284932613 Validation loss 0.04495568200945854 Accuracy 0.8891250491142273\n",
      "Iteration 41640 Training loss 3.177559847244993e-05 Validation loss 0.045007601380348206 Accuracy 0.8890000581741333\n",
      "Iteration 41650 Training loss 0.0037801144644618034 Validation loss 0.045085359364748 Accuracy 0.8882500529289246\n",
      "Iteration 41660 Training loss 0.002520328387618065 Validation loss 0.04507231339812279 Accuracy 0.8886250257492065\n",
      "Iteration 41670 Training loss 0.0012901456793770194 Validation loss 0.045222293585538864 Accuracy 0.8886250257492065\n",
      "Iteration 41680 Training loss 4.382040788186714e-05 Validation loss 0.04514473304152489 Accuracy 0.8891250491142273\n",
      "Iteration 41690 Training loss 0.002541357884183526 Validation loss 0.045114915817976 Accuracy 0.8887500166893005\n",
      "Iteration 41700 Training loss 3.4522006899351254e-05 Validation loss 0.04510721564292908 Accuracy 0.8888750672340393\n",
      "Iteration 41710 Training loss 0.0025300073903054 Validation loss 0.04527522623538971 Accuracy 0.8878750205039978\n",
      "Iteration 41720 Training loss 0.0037888886872678995 Validation loss 0.04521579295396805 Accuracy 0.8881250619888306\n",
      "Iteration 41730 Training loss 0.0012906115734949708 Validation loss 0.045213889330625534 Accuracy 0.8878750205039978\n",
      "Iteration 41740 Training loss 0.0012790068285539746 Validation loss 0.045131243765354156 Accuracy 0.8891250491142273\n",
      "Iteration 41750 Training loss 0.0012879341375082731 Validation loss 0.04512079805135727 Accuracy 0.8890000581741333\n",
      "Iteration 41760 Training loss 3.6896777601214126e-05 Validation loss 0.04510701075196266 Accuracy 0.8888750672340393\n",
      "Iteration 41770 Training loss 0.0012765572173520923 Validation loss 0.0450814850628376 Accuracy 0.8892500400543213\n",
      "Iteration 41780 Training loss 0.0025286132004112005 Validation loss 0.04506296664476395 Accuracy 0.8896250128746033\n",
      "Iteration 41790 Training loss 0.002535957610234618 Validation loss 0.04506209120154381 Accuracy 0.8892500400543213\n",
      "Iteration 41800 Training loss 3.658077184809372e-05 Validation loss 0.04505410045385361 Accuracy 0.8890000581741333\n",
      "Iteration 41810 Training loss 0.0025374467950314283 Validation loss 0.04506340250372887 Accuracy 0.8890000581741333\n",
      "Iteration 41820 Training loss 0.001290242886170745 Validation loss 0.045091547071933746 Accuracy 0.8891250491142273\n",
      "Iteration 41830 Training loss 0.001274238689802587 Validation loss 0.04506095498800278 Accuracy 0.8893750309944153\n",
      "Iteration 41840 Training loss 0.0037791591603308916 Validation loss 0.045059915632009506 Accuracy 0.8888750672340393\n",
      "Iteration 41850 Training loss 3.45511689374689e-05 Validation loss 0.04508720710873604 Accuracy 0.8888750672340393\n",
      "Iteration 41860 Training loss 0.002532481448724866 Validation loss 0.0450691394507885 Accuracy 0.8888750672340393\n",
      "Iteration 41870 Training loss 0.0025349140632897615 Validation loss 0.04507045820355415 Accuracy 0.8886250257492065\n",
      "Iteration 41880 Training loss 0.0037810017820447683 Validation loss 0.04505567625164986 Accuracy 0.8888750672340393\n",
      "Iteration 41890 Training loss 6.13108350080438e-05 Validation loss 0.04554571956396103 Accuracy 0.8883750438690186\n",
      "Iteration 41900 Training loss 4.221757626510225e-05 Validation loss 0.04519539326429367 Accuracy 0.8892500400543213\n",
      "Iteration 41910 Training loss 0.00379900261759758 Validation loss 0.0451514832675457 Accuracy 0.8890000581741333\n",
      "Iteration 41920 Training loss 0.0012809736654162407 Validation loss 0.045099738985300064 Accuracy 0.8895000219345093\n",
      "Iteration 41930 Training loss 0.0025431655813008547 Validation loss 0.04513367637991905 Accuracy 0.8888750672340393\n",
      "Iteration 41940 Training loss 0.003788280999287963 Validation loss 0.04516099765896797 Accuracy 0.8885000348091125\n",
      "Iteration 41950 Training loss 0.003775310702621937 Validation loss 0.04508912190794945 Accuracy 0.8895000219345093\n",
      "Iteration 41960 Training loss 0.003778770798817277 Validation loss 0.04512319341301918 Accuracy 0.8891250491142273\n",
      "Iteration 41970 Training loss 0.005035966634750366 Validation loss 0.04510512202978134 Accuracy 0.890250027179718\n",
      "Iteration 41980 Training loss 0.002522414783015847 Validation loss 0.04513690620660782 Accuracy 0.8895000219345093\n",
      "Iteration 41990 Training loss 0.0012803771533071995 Validation loss 0.0451609343290329 Accuracy 0.8883750438690186\n",
      "Iteration 42000 Training loss 5.298107862472534e-05 Validation loss 0.045071374624967575 Accuracy 0.8895000219345093\n",
      "Iteration 42010 Training loss 7.858335447963327e-05 Validation loss 0.046188127249479294 Accuracy 0.8852500319480896\n",
      "Iteration 42020 Training loss 3.487297362880781e-05 Validation loss 0.04532433673739433 Accuracy 0.8881250619888306\n",
      "Iteration 42030 Training loss 0.001281679724343121 Validation loss 0.045278239995241165 Accuracy 0.8887500166893005\n",
      "Iteration 42040 Training loss 0.0012849995400756598 Validation loss 0.04522670805454254 Accuracy 0.8886250257492065\n",
      "Iteration 42050 Training loss 0.0012847559992223978 Validation loss 0.045229628682136536 Accuracy 0.8882500529289246\n",
      "Iteration 42060 Training loss 0.0025413602124899626 Validation loss 0.04519851133227348 Accuracy 0.8887500166893005\n",
      "Iteration 42070 Training loss 3.475075573078357e-05 Validation loss 0.045158836990594864 Accuracy 0.8887500166893005\n",
      "Iteration 42080 Training loss 0.0025339736603200436 Validation loss 0.0452364906668663 Accuracy 0.8882500529289246\n",
      "Iteration 42090 Training loss 3.566791565390304e-05 Validation loss 0.04520225152373314 Accuracy 0.8883750438690186\n",
      "Iteration 42100 Training loss 0.0012845875462517142 Validation loss 0.04516443982720375 Accuracy 0.8890000581741333\n",
      "Iteration 42110 Training loss 0.0012883666204288602 Validation loss 0.04513739049434662 Accuracy 0.8895000219345093\n",
      "Iteration 42120 Training loss 4.6391465730266646e-05 Validation loss 0.045196324586868286 Accuracy 0.8886250257492065\n",
      "Iteration 42130 Training loss 0.0012830491177737713 Validation loss 0.045156244188547134 Accuracy 0.8887500166893005\n",
      "Iteration 42140 Training loss 0.0012808857718482614 Validation loss 0.04512741416692734 Accuracy 0.8886250257492065\n",
      "Iteration 42150 Training loss 0.0012730964226648211 Validation loss 0.045112330466508865 Accuracy 0.8891250491142273\n",
      "Iteration 42160 Training loss 2.559983840910718e-05 Validation loss 0.045129407197237015 Accuracy 0.8892500400543213\n",
      "Iteration 42170 Training loss 0.0012847620528191328 Validation loss 0.04513731598854065 Accuracy 0.8885000348091125\n",
      "Iteration 42180 Training loss 0.0012907292693853378 Validation loss 0.045066073536872864 Accuracy 0.889750063419342\n",
      "Iteration 42190 Training loss 0.003768307389691472 Validation loss 0.0451187789440155 Accuracy 0.8892500400543213\n",
      "Iteration 42200 Training loss 0.00010244518489344046 Validation loss 0.04511590674519539 Accuracy 0.8891250491142273\n",
      "Iteration 42210 Training loss 0.0012850853381678462 Validation loss 0.045132480561733246 Accuracy 0.890375018119812\n",
      "Iteration 42220 Training loss 0.0012839139671996236 Validation loss 0.04514611139893532 Accuracy 0.8888750672340393\n",
      "Iteration 42230 Training loss 0.0025312670040875673 Validation loss 0.045117586851119995 Accuracy 0.8896250128746033\n",
      "Iteration 42240 Training loss 0.003778478829190135 Validation loss 0.045117054134607315 Accuracy 0.89000004529953\n",
      "Iteration 42250 Training loss 0.0012821715790778399 Validation loss 0.045127417892217636 Accuracy 0.889875054359436\n",
      "Iteration 42260 Training loss 2.6589890694594942e-05 Validation loss 0.04515564441680908 Accuracy 0.8890000581741333\n",
      "Iteration 42270 Training loss 0.002520565642043948 Validation loss 0.0451904758810997 Accuracy 0.8890000581741333\n",
      "Iteration 42280 Training loss 0.001288783852942288 Validation loss 0.045099467039108276 Accuracy 0.8890000581741333\n",
      "Iteration 42290 Training loss 0.001281802891753614 Validation loss 0.04514320567250252 Accuracy 0.8888750672340393\n",
      "Iteration 42300 Training loss 3.7472105759661645e-05 Validation loss 0.04512349143624306 Accuracy 0.8896250128746033\n",
      "Iteration 42310 Training loss 0.003786807879805565 Validation loss 0.045180052518844604 Accuracy 0.8882500529289246\n",
      "Iteration 42320 Training loss 3.003329038619995e-05 Validation loss 0.045108817517757416 Accuracy 0.8895000219345093\n",
      "Iteration 42330 Training loss 0.0012765399878844619 Validation loss 0.04508655518293381 Accuracy 0.8896250128746033\n",
      "Iteration 42340 Training loss 4.2364517867099494e-05 Validation loss 0.04509749263525009 Accuracy 0.8892500400543213\n",
      "Iteration 42350 Training loss 0.0025389278307557106 Validation loss 0.04513460025191307 Accuracy 0.8895000219345093\n",
      "Iteration 42360 Training loss 0.0038025055546313524 Validation loss 0.04521460831165314 Accuracy 0.8888750672340393\n",
      "Iteration 42370 Training loss 0.0025277817621827126 Validation loss 0.045172516256570816 Accuracy 0.8890000581741333\n",
      "Iteration 42380 Training loss 0.0037730406038463116 Validation loss 0.045168206095695496 Accuracy 0.8890000581741333\n",
      "Iteration 42390 Training loss 4.866588278673589e-05 Validation loss 0.04518313333392143 Accuracy 0.8891250491142273\n",
      "Iteration 42400 Training loss 0.001281087868846953 Validation loss 0.045196350663900375 Accuracy 0.8893750309944153\n",
      "Iteration 42410 Training loss 0.0012739483499899507 Validation loss 0.04517337679862976 Accuracy 0.890125036239624\n",
      "Iteration 42420 Training loss 0.0025397606659680605 Validation loss 0.04518105834722519 Accuracy 0.8888750672340393\n",
      "Iteration 42430 Training loss 3.9895661757327616e-05 Validation loss 0.045160505920648575 Accuracy 0.8891250491142273\n",
      "Iteration 42440 Training loss 0.0012788960011675954 Validation loss 0.04509822279214859 Accuracy 0.8896250128746033\n",
      "Iteration 42450 Training loss 0.0012728450819849968 Validation loss 0.045114487409591675 Accuracy 0.8893750309944153\n",
      "Iteration 42460 Training loss 0.005035796668380499 Validation loss 0.04512229934334755 Accuracy 0.8895000219345093\n",
      "Iteration 42470 Training loss 3.537717930157669e-05 Validation loss 0.045128609985113144 Accuracy 0.8893750309944153\n",
      "Iteration 42480 Training loss 0.002536667278036475 Validation loss 0.04512543976306915 Accuracy 0.8888750672340393\n",
      "Iteration 42490 Training loss 0.0012879041023552418 Validation loss 0.045149460434913635 Accuracy 0.8887500166893005\n",
      "Iteration 42500 Training loss 0.003778386628255248 Validation loss 0.04511791467666626 Accuracy 0.8895000219345093\n",
      "Iteration 42510 Training loss 3.375214146217331e-05 Validation loss 0.04510783404111862 Accuracy 0.889750063419342\n",
      "Iteration 42520 Training loss 0.0012766331201419234 Validation loss 0.04517842456698418 Accuracy 0.8885000348091125\n",
      "Iteration 42530 Training loss 2.7329029762768187e-05 Validation loss 0.04510917887091637 Accuracy 0.889750063419342\n",
      "Iteration 42540 Training loss 0.001268892316147685 Validation loss 0.04512527585029602 Accuracy 0.889875054359436\n",
      "Iteration 42550 Training loss 0.0012766049476340413 Validation loss 0.0451342910528183 Accuracy 0.889750063419342\n",
      "Iteration 42560 Training loss 2.9688364520552568e-05 Validation loss 0.045258451253175735 Accuracy 0.8885000348091125\n",
      "Iteration 42570 Training loss 4.0546907257521525e-05 Validation loss 0.045191165059804916 Accuracy 0.8892500400543213\n",
      "Iteration 42580 Training loss 0.0012840537820011377 Validation loss 0.04522249847650528 Accuracy 0.8883750438690186\n",
      "Iteration 42590 Training loss 5.322634751792066e-05 Validation loss 0.0451250858604908 Accuracy 0.889750063419342\n",
      "Iteration 42600 Training loss 0.0012809354811906815 Validation loss 0.04522429034113884 Accuracy 0.8881250619888306\n",
      "Iteration 42610 Training loss 4.505656397668645e-05 Validation loss 0.04517929255962372 Accuracy 0.889875054359436\n",
      "Iteration 42620 Training loss 0.002532642101868987 Validation loss 0.04518734663724899 Accuracy 0.889875054359436\n",
      "Iteration 42630 Training loss 0.002535930834710598 Validation loss 0.045188918709754944 Accuracy 0.8893750309944153\n",
      "Iteration 42640 Training loss 4.390500907902606e-05 Validation loss 0.045153506100177765 Accuracy 0.889750063419342\n",
      "Iteration 42650 Training loss 0.005040771793574095 Validation loss 0.04524938389658928 Accuracy 0.8881250619888306\n",
      "Iteration 42660 Training loss 0.0013004648499190807 Validation loss 0.04518171772360802 Accuracy 0.8890000581741333\n",
      "Iteration 42670 Training loss 0.0025349378120154142 Validation loss 0.045163366943597794 Accuracy 0.8891250491142273\n",
      "Iteration 42680 Training loss 0.0012869047932326794 Validation loss 0.04515615478157997 Accuracy 0.8888750672340393\n",
      "Iteration 42690 Training loss 0.003765151370316744 Validation loss 0.045155543833971024 Accuracy 0.8896250128746033\n",
      "Iteration 42700 Training loss 0.0012959960149601102 Validation loss 0.04516586661338806 Accuracy 0.8892500400543213\n",
      "Iteration 42710 Training loss 3.201281651854515e-05 Validation loss 0.04515353962779045 Accuracy 0.8895000219345093\n",
      "Iteration 42720 Training loss 3.098212619079277e-05 Validation loss 0.045173950493335724 Accuracy 0.8892500400543213\n",
      "Iteration 42730 Training loss 2.434022280795034e-05 Validation loss 0.04521132633090019 Accuracy 0.8881250619888306\n",
      "Iteration 42740 Training loss 0.0012795161455869675 Validation loss 0.045224592089653015 Accuracy 0.8881250619888306\n",
      "Iteration 42750 Training loss 0.0012859092094004154 Validation loss 0.04519883170723915 Accuracy 0.8888750672340393\n",
      "Iteration 42760 Training loss 0.002533032326027751 Validation loss 0.045134287327528 Accuracy 0.8888750672340393\n",
      "Iteration 42770 Training loss 0.0012784881982952356 Validation loss 0.04511081799864769 Accuracy 0.8891250491142273\n",
      "Iteration 42780 Training loss 0.0012900193687528372 Validation loss 0.045184168964624405 Accuracy 0.8877500295639038\n",
      "Iteration 42790 Training loss 4.3049360101576895e-05 Validation loss 0.04511357471346855 Accuracy 0.8895000219345093\n",
      "Iteration 42800 Training loss 0.002539542270824313 Validation loss 0.04518187791109085 Accuracy 0.8880000710487366\n",
      "Iteration 42810 Training loss 2.8755825042026117e-05 Validation loss 0.045118335634469986 Accuracy 0.8887500166893005\n",
      "Iteration 42820 Training loss 0.001289755804464221 Validation loss 0.04509532451629639 Accuracy 0.8888750672340393\n",
      "Iteration 42830 Training loss 0.0025313750375062227 Validation loss 0.045147497206926346 Accuracy 0.8877500295639038\n",
      "Iteration 42840 Training loss 0.0012791987974196672 Validation loss 0.045122478157281876 Accuracy 0.8890000581741333\n",
      "Iteration 42850 Training loss 0.0012765249703079462 Validation loss 0.04513702914118767 Accuracy 0.8890000581741333\n",
      "Iteration 42860 Training loss 0.0012875419342890382 Validation loss 0.04515974596142769 Accuracy 0.8887500166893005\n",
      "Iteration 42870 Training loss 0.002519087167456746 Validation loss 0.04509829729795456 Accuracy 0.8888750672340393\n",
      "Iteration 42880 Training loss 4.112143142265268e-05 Validation loss 0.045120637863874435 Accuracy 0.8888750672340393\n",
      "Iteration 42890 Training loss 0.0025328684132546186 Validation loss 0.04512831196188927 Accuracy 0.8883750438690186\n",
      "Iteration 42900 Training loss 0.001329757389612496 Validation loss 0.04542003199458122 Accuracy 0.8892500400543213\n",
      "Iteration 42910 Training loss 4.760718729812652e-05 Validation loss 0.04512832686305046 Accuracy 0.89000004529953\n",
      "Iteration 42920 Training loss 3.0440369300777093e-05 Validation loss 0.04514482244849205 Accuracy 0.8896250128746033\n",
      "Iteration 42930 Training loss 3.244783147238195e-05 Validation loss 0.045210741460323334 Accuracy 0.8882500529289246\n",
      "Iteration 42940 Training loss 3.152075078105554e-05 Validation loss 0.04509890824556351 Accuracy 0.8888750672340393\n",
      "Iteration 42950 Training loss 0.0025291305501013994 Validation loss 0.04516301304101944 Accuracy 0.8877500295639038\n",
      "Iteration 42960 Training loss 0.0012815751833841205 Validation loss 0.04514982923865318 Accuracy 0.8880000710487366\n",
      "Iteration 42970 Training loss 0.001269540167413652 Validation loss 0.045160580426454544 Accuracy 0.8882500529289246\n",
      "Iteration 42980 Training loss 0.0025221870746463537 Validation loss 0.045064203441143036 Accuracy 0.8892500400543213\n",
      "Iteration 42990 Training loss 3.883640965796076e-05 Validation loss 0.04506004601716995 Accuracy 0.8891250491142273\n",
      "Iteration 43000 Training loss 0.0012882482260465622 Validation loss 0.04508649557828903 Accuracy 0.8890000581741333\n",
      "Iteration 43010 Training loss 0.0025330164935439825 Validation loss 0.045108433812856674 Accuracy 0.8888750672340393\n",
      "Iteration 43020 Training loss 3.398577246116474e-05 Validation loss 0.04515179619193077 Accuracy 0.8887500166893005\n",
      "Iteration 43030 Training loss 0.00252381619066 Validation loss 0.04511946812272072 Accuracy 0.8888750672340393\n",
      "Iteration 43040 Training loss 0.0025294360239058733 Validation loss 0.04510000720620155 Accuracy 0.8892500400543213\n",
      "Iteration 43050 Training loss 2.9875949621782638e-05 Validation loss 0.04510164633393288 Accuracy 0.8888750672340393\n",
      "Iteration 43060 Training loss 3.8161400880198926e-05 Validation loss 0.04509899765253067 Accuracy 0.8888750672340393\n",
      "Iteration 43070 Training loss 0.001271887798793614 Validation loss 0.04510292410850525 Accuracy 0.8890000581741333\n",
      "Iteration 43080 Training loss 0.0025341606233268976 Validation loss 0.04516374692320824 Accuracy 0.8877500295639038\n",
      "Iteration 43090 Training loss 4.433361755218357e-05 Validation loss 0.045207180082798004 Accuracy 0.8875000476837158\n",
      "Iteration 43100 Training loss 3.848801861749962e-05 Validation loss 0.04532737284898758 Accuracy 0.8888750672340393\n",
      "Iteration 43110 Training loss 0.002528700279071927 Validation loss 0.04526403173804283 Accuracy 0.8886250257492065\n",
      "Iteration 43120 Training loss 3.514831041684374e-05 Validation loss 0.045252200216054916 Accuracy 0.8883750438690186\n",
      "Iteration 43130 Training loss 0.002525958465412259 Validation loss 0.045249879360198975 Accuracy 0.8888750672340393\n",
      "Iteration 43140 Training loss 0.005045365076512098 Validation loss 0.045206986367702484 Accuracy 0.8890000581741333\n",
      "Iteration 43150 Training loss 4.031074058730155e-05 Validation loss 0.04521055147051811 Accuracy 0.8887500166893005\n",
      "Iteration 43160 Training loss 3.523994018905796e-05 Validation loss 0.04517575725913048 Accuracy 0.8887500166893005\n",
      "Iteration 43170 Training loss 0.0012804042780771852 Validation loss 0.045195724815130234 Accuracy 0.8896250128746033\n",
      "Iteration 43180 Training loss 4.4532891479320824e-05 Validation loss 0.045201320201158524 Accuracy 0.8891250491142273\n",
      "Iteration 43190 Training loss 0.0012766559375450015 Validation loss 0.04517660662531853 Accuracy 0.8890000581741333\n",
      "Iteration 43200 Training loss 0.0012839146656915545 Validation loss 0.045219652354717255 Accuracy 0.8885000348091125\n",
      "Iteration 43210 Training loss 2.358164965698961e-05 Validation loss 0.045185450464487076 Accuracy 0.8888750672340393\n",
      "Iteration 43220 Training loss 0.0012764037819579244 Validation loss 0.04520023614168167 Accuracy 0.8888750672340393\n",
      "Iteration 43230 Training loss 3.601315984269604e-05 Validation loss 0.045185498893260956 Accuracy 0.8887500166893005\n",
      "Iteration 43240 Training loss 0.002522183582186699 Validation loss 0.04518071934580803 Accuracy 0.8886250257492065\n",
      "Iteration 43250 Training loss 0.002522141905501485 Validation loss 0.04515019431710243 Accuracy 0.8893750309944153\n",
      "Iteration 43260 Training loss 3.4221488022012636e-05 Validation loss 0.045177336782217026 Accuracy 0.8890000581741333\n",
      "Iteration 43270 Training loss 0.0025399711448699236 Validation loss 0.04515251889824867 Accuracy 0.8886250257492065\n",
      "Iteration 43280 Training loss 0.002534642815589905 Validation loss 0.04513327404856682 Accuracy 0.8892500400543213\n",
      "Iteration 43290 Training loss 0.002536879386752844 Validation loss 0.04515611380338669 Accuracy 0.8886250257492065\n",
      "Iteration 43300 Training loss 0.003781920298933983 Validation loss 0.04513851925730705 Accuracy 0.8887500166893005\n",
      "Iteration 43310 Training loss 1.97939298232086e-05 Validation loss 0.045163676142692566 Accuracy 0.8882500529289246\n",
      "Iteration 43320 Training loss 0.0012734098127111793 Validation loss 0.0451747365295887 Accuracy 0.8878750205039978\n",
      "Iteration 43330 Training loss 0.0012769063469022512 Validation loss 0.04508889839053154 Accuracy 0.8896250128746033\n",
      "Iteration 43340 Training loss 0.0012803415302187204 Validation loss 0.045154981315135956 Accuracy 0.8891250491142273\n",
      "Iteration 43350 Training loss 2.632899304444436e-05 Validation loss 0.04513489082455635 Accuracy 0.8883750438690186\n",
      "Iteration 43360 Training loss 0.0012875259853899479 Validation loss 0.045183777809143066 Accuracy 0.8882500529289246\n",
      "Iteration 43370 Training loss 3.5833025322062895e-05 Validation loss 0.0451614074409008 Accuracy 0.8881250619888306\n",
      "Iteration 43380 Training loss 0.002527395961806178 Validation loss 0.045139431953430176 Accuracy 0.8885000348091125\n",
      "Iteration 43390 Training loss 3.061426104977727e-05 Validation loss 0.0451718308031559 Accuracy 0.8877500295639038\n",
      "Iteration 43400 Training loss 0.0025298171676695347 Validation loss 0.045164063572883606 Accuracy 0.8876250386238098\n",
      "Iteration 43410 Training loss 2.6143799914279953e-05 Validation loss 0.04512765631079674 Accuracy 0.8886250257492065\n",
      "Iteration 43420 Training loss 0.0012836524983868003 Validation loss 0.04513080045580864 Accuracy 0.8895000219345093\n",
      "Iteration 43430 Training loss 2.9014561732765287e-05 Validation loss 0.04516676813364029 Accuracy 0.8890000581741333\n",
      "Iteration 43440 Training loss 5.155866529094055e-05 Validation loss 0.04522138088941574 Accuracy 0.8882500529289246\n",
      "Iteration 43450 Training loss 0.0012800103286281228 Validation loss 0.04517496004700661 Accuracy 0.8890000581741333\n",
      "Iteration 43460 Training loss 0.002531974809244275 Validation loss 0.04517852142453194 Accuracy 0.8887500166893005\n",
      "Iteration 43470 Training loss 0.0025294506922364235 Validation loss 0.045168690383434296 Accuracy 0.8886250257492065\n",
      "Iteration 43480 Training loss 0.001280828146263957 Validation loss 0.045198503881692886 Accuracy 0.8886250257492065\n",
      "Iteration 43490 Training loss 0.0025292367208749056 Validation loss 0.04515114054083824 Accuracy 0.8891250491142273\n",
      "Iteration 43500 Training loss 0.002543136477470398 Validation loss 0.04516339302062988 Accuracy 0.8895000219345093\n",
      "Iteration 43510 Training loss 2.1510648366529495e-05 Validation loss 0.04516911506652832 Accuracy 0.8893750309944153\n",
      "Iteration 43520 Training loss 2.5884397473419085e-05 Validation loss 0.045147065073251724 Accuracy 0.8895000219345093\n",
      "Iteration 43530 Training loss 0.0025442573241889477 Validation loss 0.04514912888407707 Accuracy 0.8891250491142273\n",
      "Iteration 43540 Training loss 0.001290853600949049 Validation loss 0.04517151042819023 Accuracy 0.8886250257492065\n",
      "Iteration 43550 Training loss 0.0012766086729243398 Validation loss 0.045175883919000626 Accuracy 0.8882500529289246\n",
      "Iteration 43560 Training loss 0.0012756915530189872 Validation loss 0.045156948268413544 Accuracy 0.8890000581741333\n",
      "Iteration 43570 Training loss 0.0037765461020171642 Validation loss 0.04514865577220917 Accuracy 0.8891250491142273\n",
      "Iteration 43580 Training loss 3.016312257386744e-05 Validation loss 0.04523755982518196 Accuracy 0.8882500529289246\n",
      "Iteration 43590 Training loss 0.0025309394113719463 Validation loss 0.04515289515256882 Accuracy 0.8888750672340393\n",
      "Iteration 43600 Training loss 0.002533110324293375 Validation loss 0.04517185688018799 Accuracy 0.8888750672340393\n",
      "Iteration 43610 Training loss 0.0025334679521620274 Validation loss 0.045163433998823166 Accuracy 0.8883750438690186\n",
      "Iteration 43620 Training loss 0.0025322833098471165 Validation loss 0.045147381722927094 Accuracy 0.8885000348091125\n",
      "Iteration 43630 Training loss 0.0012807986931875348 Validation loss 0.045177239924669266 Accuracy 0.8886250257492065\n",
      "Iteration 43640 Training loss 2.078923898807261e-05 Validation loss 0.04520706832408905 Accuracy 0.8883750438690186\n",
      "Iteration 43650 Training loss 0.0012839033734053373 Validation loss 0.045220084488391876 Accuracy 0.8880000710487366\n",
      "Iteration 43660 Training loss 0.002536113141104579 Validation loss 0.045156329870224 Accuracy 0.8886250257492065\n",
      "Iteration 43670 Training loss 0.0025265824515372515 Validation loss 0.04515394568443298 Accuracy 0.8878750205039978\n",
      "Iteration 43680 Training loss 0.0012898006243631244 Validation loss 0.04514823481440544 Accuracy 0.8885000348091125\n",
      "Iteration 43690 Training loss 0.0012677253689616919 Validation loss 0.045138515532016754 Accuracy 0.8890000581741333\n",
      "Iteration 43700 Training loss 0.0025408375076949596 Validation loss 0.045178767293691635 Accuracy 0.8885000348091125\n",
      "Iteration 43710 Training loss 2.599783692858182e-05 Validation loss 0.04520244151353836 Accuracy 0.8881250619888306\n",
      "Iteration 43720 Training loss 0.002524325158447027 Validation loss 0.04516484960913658 Accuracy 0.8882500529289246\n",
      "Iteration 43730 Training loss 2.6003601306001656e-05 Validation loss 0.04522784799337387 Accuracy 0.8871250152587891\n",
      "Iteration 43740 Training loss 0.0012723196996375918 Validation loss 0.04518735408782959 Accuracy 0.8876250386238098\n",
      "Iteration 43750 Training loss 0.0037909357342869043 Validation loss 0.04513217881321907 Accuracy 0.8887500166893005\n",
      "Iteration 43760 Training loss 2.273842619615607e-05 Validation loss 0.04514962062239647 Accuracy 0.8885000348091125\n",
      "Iteration 43770 Training loss 0.002521464368328452 Validation loss 0.04514600336551666 Accuracy 0.8886250257492065\n",
      "Iteration 43780 Training loss 0.002530645113438368 Validation loss 0.04513503983616829 Accuracy 0.8887500166893005\n",
      "Iteration 43790 Training loss 3.428678974159993e-05 Validation loss 0.045152582228183746 Accuracy 0.8888750672340393\n",
      "Iteration 43800 Training loss 0.0025200790259987116 Validation loss 0.04516452178359032 Accuracy 0.8890000581741333\n",
      "Iteration 43810 Training loss 3.8862443034304306e-05 Validation loss 0.04519285634160042 Accuracy 0.8888750672340393\n",
      "Iteration 43820 Training loss 4.020550841232762e-05 Validation loss 0.04518791660666466 Accuracy 0.8886250257492065\n",
      "Iteration 43830 Training loss 0.0012795805232599378 Validation loss 0.045186299830675125 Accuracy 0.8883750438690186\n",
      "Iteration 43840 Training loss 0.0012706683482974768 Validation loss 0.04521092399954796 Accuracy 0.8881250619888306\n",
      "Iteration 43850 Training loss 3.3258835173910484e-05 Validation loss 0.045213110744953156 Accuracy 0.8878750205039978\n",
      "Iteration 43860 Training loss 0.001285554259084165 Validation loss 0.04538968205451965 Accuracy 0.8881250619888306\n",
      "Iteration 43870 Training loss 2.993809903273359e-05 Validation loss 0.04531533271074295 Accuracy 0.8878750205039978\n",
      "Iteration 43880 Training loss 5.476373917190358e-05 Validation loss 0.04531644657254219 Accuracy 0.8877500295639038\n",
      "Iteration 43890 Training loss 0.0012796287192031741 Validation loss 0.04523812234401703 Accuracy 0.8887500166893005\n",
      "Iteration 43900 Training loss 0.005017953924834728 Validation loss 0.04522022604942322 Accuracy 0.8882500529289246\n",
      "Iteration 43910 Training loss 3.4347980545135215e-05 Validation loss 0.0452328585088253 Accuracy 0.8887500166893005\n",
      "Iteration 43920 Training loss 2.4815280994516797e-05 Validation loss 0.04524329677224159 Accuracy 0.8890000581741333\n",
      "Iteration 43930 Training loss 3.4722848795354366e-05 Validation loss 0.045225005596876144 Accuracy 0.8893750309944153\n",
      "Iteration 43940 Training loss 0.0012748276349157095 Validation loss 0.045249879360198975 Accuracy 0.8888750672340393\n",
      "Iteration 43950 Training loss 0.0037845021579414606 Validation loss 0.04523955658078194 Accuracy 0.8887500166893005\n",
      "Iteration 43960 Training loss 2.7863072318723425e-05 Validation loss 0.04522639140486717 Accuracy 0.8887500166893005\n",
      "Iteration 43970 Training loss 0.002541732043027878 Validation loss 0.04520167410373688 Accuracy 0.889875054359436\n",
      "Iteration 43980 Training loss 0.0012788495514541864 Validation loss 0.045212212949991226 Accuracy 0.8888750672340393\n",
      "Iteration 43990 Training loss 0.0012886677868664265 Validation loss 0.04520571976900101 Accuracy 0.8886250257492065\n",
      "Iteration 44000 Training loss 0.001271869638003409 Validation loss 0.045210834592580795 Accuracy 0.8883750438690186\n",
      "Iteration 44010 Training loss 3.111672413069755e-05 Validation loss 0.04520466923713684 Accuracy 0.8887500166893005\n",
      "Iteration 44020 Training loss 3.0379700547200628e-05 Validation loss 0.04523933678865433 Accuracy 0.8882500529289246\n",
      "Iteration 44030 Training loss 0.0012723481049761176 Validation loss 0.045264147222042084 Accuracy 0.8887500166893005\n",
      "Iteration 44040 Training loss 0.0012736188946291804 Validation loss 0.045244138687849045 Accuracy 0.8881250619888306\n",
      "Iteration 44050 Training loss 0.0012891803635284305 Validation loss 0.04522140324115753 Accuracy 0.8886250257492065\n",
      "Iteration 44060 Training loss 0.002521822927519679 Validation loss 0.045220911502838135 Accuracy 0.8882500529289246\n",
      "Iteration 44070 Training loss 0.0012789929751306772 Validation loss 0.045196909457445145 Accuracy 0.8887500166893005\n",
      "Iteration 44080 Training loss 5.1620081649161875e-05 Validation loss 0.04523496702313423 Accuracy 0.8883750438690186\n",
      "Iteration 44090 Training loss 0.0050291335210204124 Validation loss 0.04527436941862106 Accuracy 0.8877500295639038\n",
      "Iteration 44100 Training loss 0.005027536768466234 Validation loss 0.04523240774869919 Accuracy 0.8877500295639038\n",
      "Iteration 44110 Training loss 0.0012745625572279096 Validation loss 0.045241259038448334 Accuracy 0.8877500295639038\n",
      "Iteration 44120 Training loss 0.0012791412882506847 Validation loss 0.04521344229578972 Accuracy 0.8893750309944153\n",
      "Iteration 44130 Training loss 0.001279755262658 Validation loss 0.04521354287862778 Accuracy 0.8886250257492065\n",
      "Iteration 44140 Training loss 0.0012815971858799458 Validation loss 0.0452083945274353 Accuracy 0.8890000581741333\n",
      "Iteration 44150 Training loss 0.0012891198275610805 Validation loss 0.04526347666978836 Accuracy 0.8883750438690186\n",
      "Iteration 44160 Training loss 3.0930896173231304e-05 Validation loss 0.04525943100452423 Accuracy 0.8883750438690186\n",
      "Iteration 44170 Training loss 0.0025291889905929565 Validation loss 0.04521694406867027 Accuracy 0.8888750672340393\n",
      "Iteration 44180 Training loss 0.0012745133135467768 Validation loss 0.045244257897138596 Accuracy 0.8881250619888306\n",
      "Iteration 44190 Training loss 0.0012864269083365798 Validation loss 0.04527559131383896 Accuracy 0.8881250619888306\n",
      "Iteration 44200 Training loss 0.001289878855459392 Validation loss 0.04524858295917511 Accuracy 0.8883750438690186\n",
      "Iteration 44210 Training loss 0.0025319689884781837 Validation loss 0.04522791877388954 Accuracy 0.8886250257492065\n",
      "Iteration 44220 Training loss 0.0012724905973300338 Validation loss 0.0452260822057724 Accuracy 0.8877500295639038\n",
      "Iteration 44230 Training loss 0.003793420037254691 Validation loss 0.045219141989946365 Accuracy 0.8881250619888306\n",
      "Iteration 44240 Training loss 2.9498571166186593e-05 Validation loss 0.0451919361948967 Accuracy 0.8888750672340393\n",
      "Iteration 44250 Training loss 2.2455322323367e-05 Validation loss 0.045218273997306824 Accuracy 0.8888750672340393\n",
      "Iteration 44260 Training loss 0.0037727372255176306 Validation loss 0.04519772529602051 Accuracy 0.8885000348091125\n",
      "Iteration 44270 Training loss 0.0012736280914396048 Validation loss 0.04517180845141411 Accuracy 0.889875054359436\n",
      "Iteration 44280 Training loss 0.001279850141145289 Validation loss 0.04522131383419037 Accuracy 0.8888750672340393\n",
      "Iteration 44290 Training loss 3.1940769986249506e-05 Validation loss 0.04522205516695976 Accuracy 0.8885000348091125\n",
      "Iteration 44300 Training loss 0.0012816372327506542 Validation loss 0.04518013820052147 Accuracy 0.8893750309944153\n",
      "Iteration 44310 Training loss 0.0037790441419929266 Validation loss 0.04519214108586311 Accuracy 0.8886250257492065\n",
      "Iteration 44320 Training loss 0.001273645437322557 Validation loss 0.045187950134277344 Accuracy 0.8887500166893005\n",
      "Iteration 44330 Training loss 0.001277073984965682 Validation loss 0.04516948014497757 Accuracy 0.8890000581741333\n",
      "Iteration 44340 Training loss 0.002542070345953107 Validation loss 0.04520765319466591 Accuracy 0.8887500166893005\n",
      "Iteration 44350 Training loss 0.005023941397666931 Validation loss 0.04516587033867836 Accuracy 0.8891250491142273\n",
      "Iteration 44360 Training loss 0.001271691988222301 Validation loss 0.04518911987543106 Accuracy 0.8887500166893005\n",
      "Iteration 44370 Training loss 2.7683181542670354e-05 Validation loss 0.04518352076411247 Accuracy 0.8885000348091125\n",
      "Iteration 44380 Training loss 0.0025285708252340555 Validation loss 0.045220937579870224 Accuracy 0.8881250619888306\n",
      "Iteration 44390 Training loss 0.0012795341899618506 Validation loss 0.04521583020687103 Accuracy 0.8886250257492065\n",
      "Iteration 44400 Training loss 3.1860065064392984e-05 Validation loss 0.04519873112440109 Accuracy 0.8887500166893005\n",
      "Iteration 44410 Training loss 3.563874270184897e-05 Validation loss 0.04521133750677109 Accuracy 0.8891250491142273\n",
      "Iteration 44420 Training loss 2.3973365387064405e-05 Validation loss 0.04519633576273918 Accuracy 0.8893750309944153\n",
      "Iteration 44430 Training loss 2.609391594887711e-05 Validation loss 0.04521758481860161 Accuracy 0.8886250257492065\n",
      "Iteration 44440 Training loss 0.002538522705435753 Validation loss 0.045202769339084625 Accuracy 0.8890000581741333\n",
      "Iteration 44450 Training loss 0.0012737949145957828 Validation loss 0.04521026462316513 Accuracy 0.8891250491142273\n",
      "Iteration 44460 Training loss 0.00254463916644454 Validation loss 0.04521990567445755 Accuracy 0.8890000581741333\n",
      "Iteration 44470 Training loss 0.0025318479165434837 Validation loss 0.04520660266280174 Accuracy 0.8888750672340393\n",
      "Iteration 44480 Training loss 0.0012866343604400754 Validation loss 0.04525904357433319 Accuracy 0.8883750438690186\n",
      "Iteration 44490 Training loss 0.0025239347014576197 Validation loss 0.04522571340203285 Accuracy 0.8883750438690186\n",
      "Iteration 44500 Training loss 0.002529990393668413 Validation loss 0.045216675847768784 Accuracy 0.8886250257492065\n",
      "Iteration 44510 Training loss 0.001286988495849073 Validation loss 0.04521041363477707 Accuracy 0.8881250619888306\n",
      "Iteration 44520 Training loss 0.0037772345822304487 Validation loss 0.04519470036029816 Accuracy 0.8887500166893005\n",
      "Iteration 44530 Training loss 0.0012848633341491222 Validation loss 0.04522348940372467 Accuracy 0.8887500166893005\n",
      "Iteration 44540 Training loss 0.0012760073877871037 Validation loss 0.045218575745821 Accuracy 0.8886250257492065\n",
      "Iteration 44550 Training loss 0.0012966805370524526 Validation loss 0.045225679874420166 Accuracy 0.8878750205039978\n",
      "Iteration 44560 Training loss 3.182242653565481e-05 Validation loss 0.045208681374788284 Accuracy 0.8882500529289246\n",
      "Iteration 44570 Training loss 0.0012799955438822508 Validation loss 0.0451824814081192 Accuracy 0.8895000219345093\n",
      "Iteration 44580 Training loss 0.003774818032979965 Validation loss 0.04520460590720177 Accuracy 0.8887500166893005\n",
      "Iteration 44590 Training loss 0.002526070922613144 Validation loss 0.04517354071140289 Accuracy 0.8893750309944153\n",
      "Iteration 44600 Training loss 1.853204048529733e-05 Validation loss 0.045188989490270615 Accuracy 0.8887500166893005\n",
      "Iteration 44610 Training loss 0.001272867782972753 Validation loss 0.045195430517196655 Accuracy 0.8892500400543213\n",
      "Iteration 44620 Training loss 2.3907608920126222e-05 Validation loss 0.045201197266578674 Accuracy 0.8886250257492065\n",
      "Iteration 44630 Training loss 3.964163624914363e-05 Validation loss 0.04530227556824684 Accuracy 0.8873750567436218\n",
      "Iteration 44640 Training loss 0.001273125410079956 Validation loss 0.045284830033779144 Accuracy 0.8880000710487366\n",
      "Iteration 44650 Training loss 0.0025332157965749502 Validation loss 0.0452139750123024 Accuracy 0.8890000581741333\n",
      "Iteration 44660 Training loss 0.001270017004571855 Validation loss 0.045211061835289 Accuracy 0.8886250257492065\n",
      "Iteration 44670 Training loss 2.111658250214532e-05 Validation loss 0.04518071562051773 Accuracy 0.8896250128746033\n",
      "Iteration 44680 Training loss 0.0037826071493327618 Validation loss 0.04523146152496338 Accuracy 0.8880000710487366\n",
      "Iteration 44690 Training loss 0.0012769799213856459 Validation loss 0.04521477222442627 Accuracy 0.8891250491142273\n",
      "Iteration 44700 Training loss 2.769571256067138e-05 Validation loss 0.04522111266851425 Accuracy 0.8886250257492065\n",
      "Iteration 44710 Training loss 0.0012727066641673446 Validation loss 0.04522731155157089 Accuracy 0.8883750438690186\n",
      "Iteration 44720 Training loss 0.002530458150431514 Validation loss 0.04520619288086891 Accuracy 0.8892500400543213\n",
      "Iteration 44730 Training loss 0.005041927099227905 Validation loss 0.045168694108724594 Accuracy 0.8895000219345093\n",
      "Iteration 44740 Training loss 2.6429299396113493e-05 Validation loss 0.045185867697000504 Accuracy 0.8887500166893005\n",
      "Iteration 44750 Training loss 0.001273085130378604 Validation loss 0.04517892003059387 Accuracy 0.8887500166893005\n",
      "Iteration 44760 Training loss 0.0037715514190495014 Validation loss 0.045170992612838745 Accuracy 0.8890000581741333\n",
      "Iteration 44770 Training loss 3.398524131625891e-05 Validation loss 0.0451955609023571 Accuracy 0.8881250619888306\n",
      "Iteration 44780 Training loss 0.0012809998588636518 Validation loss 0.045197147876024246 Accuracy 0.8885000348091125\n",
      "Iteration 44790 Training loss 0.001272445428185165 Validation loss 0.045200277119874954 Accuracy 0.8892500400543213\n",
      "Iteration 44800 Training loss 0.0012758028460666537 Validation loss 0.045189447700977325 Accuracy 0.889750063419342\n",
      "Iteration 44810 Training loss 0.005025316495448351 Validation loss 0.045194655656814575 Accuracy 0.8886250257492065\n",
      "Iteration 44820 Training loss 0.0025244676508009434 Validation loss 0.04522769898176193 Accuracy 0.8883750438690186\n",
      "Iteration 44830 Training loss 0.001277521369047463 Validation loss 0.045188721269369125 Accuracy 0.8886250257492065\n",
      "Iteration 44840 Training loss 2.906060944951605e-05 Validation loss 0.04515465721487999 Accuracy 0.8895000219345093\n",
      "Iteration 44850 Training loss 3.678523353300989e-05 Validation loss 0.04515191912651062 Accuracy 0.8890000581741333\n",
      "Iteration 44860 Training loss 0.0012720210943371058 Validation loss 0.04524648189544678 Accuracy 0.8883750438690186\n",
      "Iteration 44870 Training loss 0.0012734464835375547 Validation loss 0.04522526264190674 Accuracy 0.8890000581741333\n",
      "Iteration 44880 Training loss 0.005029871128499508 Validation loss 0.04524293914437294 Accuracy 0.8887500166893005\n",
      "Iteration 44890 Training loss 0.0012813896173611283 Validation loss 0.045236483216285706 Accuracy 0.8892500400543213\n",
      "Iteration 44900 Training loss 0.002537407912313938 Validation loss 0.04525655135512352 Accuracy 0.8888750672340393\n",
      "Iteration 44910 Training loss 0.0025520245544612408 Validation loss 0.04526584595441818 Accuracy 0.8880000710487366\n",
      "Iteration 44920 Training loss 0.0025243465788662434 Validation loss 0.04527561739087105 Accuracy 0.8877500295639038\n",
      "Iteration 44930 Training loss 2.4847337044775486e-05 Validation loss 0.04525217413902283 Accuracy 0.8890000581741333\n",
      "Iteration 44940 Training loss 0.002519540721550584 Validation loss 0.045209597796201706 Accuracy 0.8895000219345093\n",
      "Iteration 44950 Training loss 4.81811621284578e-05 Validation loss 0.04526416212320328 Accuracy 0.8878750205039978\n",
      "Iteration 44960 Training loss 0.002531220903620124 Validation loss 0.04518616199493408 Accuracy 0.8888750672340393\n",
      "Iteration 44970 Training loss 0.0025255526416003704 Validation loss 0.04523983597755432 Accuracy 0.8883750438690186\n",
      "Iteration 44980 Training loss 3.8800088077550754e-05 Validation loss 0.04524121806025505 Accuracy 0.8885000348091125\n",
      "Iteration 44990 Training loss 0.001281622564420104 Validation loss 0.04520945996046066 Accuracy 0.8893750309944153\n",
      "Iteration 45000 Training loss 3.0728297133464366e-05 Validation loss 0.045186419039964676 Accuracy 0.8895000219345093\n",
      "Iteration 45010 Training loss 0.0037790280766785145 Validation loss 0.04522382840514183 Accuracy 0.8886250257492065\n",
      "Iteration 45020 Training loss 0.0012838431866839528 Validation loss 0.04520131275057793 Accuracy 0.8896250128746033\n",
      "Iteration 45030 Training loss 1.652785977057647e-05 Validation loss 0.045210763812065125 Accuracy 0.8892500400543213\n",
      "Iteration 45040 Training loss 0.0025149874854832888 Validation loss 0.04523415490984917 Accuracy 0.8887500166893005\n",
      "Iteration 45050 Training loss 0.0012732085306197405 Validation loss 0.04520871862769127 Accuracy 0.8891250491142273\n",
      "Iteration 45060 Training loss 0.0012712476309388876 Validation loss 0.04516987502574921 Accuracy 0.8893750309944153\n",
      "Iteration 45070 Training loss 0.002526966854929924 Validation loss 0.04524603113532066 Accuracy 0.8886250257492065\n",
      "Iteration 45080 Training loss 0.0012772607151418924 Validation loss 0.045204561203718185 Accuracy 0.8886250257492065\n",
      "Iteration 45090 Training loss 2.6328185413149185e-05 Validation loss 0.045205093920230865 Accuracy 0.8892500400543213\n",
      "Iteration 45100 Training loss 3.68253568012733e-05 Validation loss 0.04519372805953026 Accuracy 0.8887500166893005\n",
      "Iteration 45110 Training loss 0.0012804250000044703 Validation loss 0.04519478976726532 Accuracy 0.8890000581741333\n",
      "Iteration 45120 Training loss 0.0012856838293373585 Validation loss 0.04523545876145363 Accuracy 0.8880000710487366\n",
      "Iteration 45130 Training loss 0.001275054644793272 Validation loss 0.045190922915935516 Accuracy 0.8885000348091125\n",
      "Iteration 45140 Training loss 2.1541773094213568e-05 Validation loss 0.04519333690404892 Accuracy 0.8885000348091125\n",
      "Iteration 45150 Training loss 2.2911030100658536e-05 Validation loss 0.045153483748435974 Accuracy 0.8896250128746033\n",
      "Iteration 45160 Training loss 2.0417768610059284e-05 Validation loss 0.04519553482532501 Accuracy 0.8883750438690186\n",
      "Iteration 45170 Training loss 3.049204133276362e-05 Validation loss 0.04519721493124962 Accuracy 0.8886250257492065\n",
      "Iteration 45180 Training loss 0.003785744309425354 Validation loss 0.04516575112938881 Accuracy 0.8895000219345093\n",
      "Iteration 45190 Training loss 2.677888005564455e-05 Validation loss 0.04519379884004593 Accuracy 0.8881250619888306\n",
      "Iteration 45200 Training loss 2.305418274772819e-05 Validation loss 0.045163996517658234 Accuracy 0.8896250128746033\n",
      "Iteration 45210 Training loss 0.0012791167246177793 Validation loss 0.045267924666404724 Accuracy 0.8883750438690186\n",
      "Iteration 45220 Training loss 2.8997756089665927e-05 Validation loss 0.045159608125686646 Accuracy 0.8882500529289246\n",
      "Iteration 45230 Training loss 4.069687565788627e-05 Validation loss 0.04516473785042763 Accuracy 0.8883750438690186\n",
      "Iteration 45240 Training loss 0.001287349034100771 Validation loss 0.045198794454336166 Accuracy 0.8891250491142273\n",
      "Iteration 45250 Training loss 0.0062804254703223705 Validation loss 0.04519897699356079 Accuracy 0.8886250257492065\n",
      "Iteration 45260 Training loss 0.003759783459827304 Validation loss 0.04516401141881943 Accuracy 0.8891250491142273\n",
      "Iteration 45270 Training loss 0.0012726394925266504 Validation loss 0.04521775618195534 Accuracy 0.8882500529289246\n",
      "Iteration 45280 Training loss 0.0012786383740603924 Validation loss 0.04518381133675575 Accuracy 0.8887500166893005\n",
      "Iteration 45290 Training loss 0.0012783135753124952 Validation loss 0.0451626293361187 Accuracy 0.8890000581741333\n",
      "Iteration 45300 Training loss 0.0025310188066214323 Validation loss 0.045171983540058136 Accuracy 0.8893750309944153\n",
      "Iteration 45310 Training loss 0.0012751644244417548 Validation loss 0.04518251121044159 Accuracy 0.8888750672340393\n",
      "Iteration 45320 Training loss 2.23929946514545e-05 Validation loss 0.045182954519987106 Accuracy 0.8887500166893005\n",
      "Iteration 45330 Training loss 2.3861824956838973e-05 Validation loss 0.04526468738913536 Accuracy 0.8883750438690186\n",
      "Iteration 45340 Training loss 0.0012865395983681083 Validation loss 0.04520079866051674 Accuracy 0.8886250257492065\n",
      "Iteration 45350 Training loss 0.0025306916795670986 Validation loss 0.04522412270307541 Accuracy 0.8878750205039978\n",
      "Iteration 45360 Training loss 0.003779359394684434 Validation loss 0.045194488018751144 Accuracy 0.8891250491142273\n",
      "Iteration 45370 Training loss 0.0012790131149813533 Validation loss 0.045226000249385834 Accuracy 0.8887500166893005\n",
      "Iteration 45380 Training loss 0.002526170341297984 Validation loss 0.045223668217659 Accuracy 0.8888750672340393\n",
      "Iteration 45390 Training loss 0.0012818436371162534 Validation loss 0.04521418735384941 Accuracy 0.8890000581741333\n",
      "Iteration 45400 Training loss 0.0025243004783988 Validation loss 0.04523618519306183 Accuracy 0.8891250491142273\n",
      "Iteration 45410 Training loss 2.4815806682454422e-05 Validation loss 0.04524559900164604 Accuracy 0.8887500166893005\n",
      "Iteration 45420 Training loss 3.754996942006983e-05 Validation loss 0.04523948207497597 Accuracy 0.8890000581741333\n",
      "Iteration 45430 Training loss 0.0025242750998586416 Validation loss 0.045230355113744736 Accuracy 0.8888750672340393\n",
      "Iteration 45440 Training loss 4.244040610501543e-05 Validation loss 0.045219238847494125 Accuracy 0.8896250128746033\n",
      "Iteration 45450 Training loss 0.002524124225601554 Validation loss 0.04524196684360504 Accuracy 0.8883750438690186\n",
      "Iteration 45460 Training loss 0.0012641570065170527 Validation loss 0.04522349685430527 Accuracy 0.8890000581741333\n",
      "Iteration 45470 Training loss 0.00502802012488246 Validation loss 0.04520291090011597 Accuracy 0.8885000348091125\n",
      "Iteration 45480 Training loss 2.1862228095415048e-05 Validation loss 0.04521162062883377 Accuracy 0.8882500529289246\n",
      "Iteration 45490 Training loss 1.9580716980271973e-05 Validation loss 0.04522855207324028 Accuracy 0.8878750205039978\n",
      "Iteration 45500 Training loss 0.0012881672009825706 Validation loss 0.045255281031131744 Accuracy 0.8880000710487366\n",
      "Iteration 45510 Training loss 2.5310730052297004e-05 Validation loss 0.04518803954124451 Accuracy 0.8890000581741333\n",
      "Iteration 45520 Training loss 0.001285092206671834 Validation loss 0.045200325548648834 Accuracy 0.8886250257492065\n",
      "Iteration 45530 Training loss 2.350923023186624e-05 Validation loss 0.0451916828751564 Accuracy 0.8885000348091125\n",
      "Iteration 45540 Training loss 0.002525599440559745 Validation loss 0.04525370895862579 Accuracy 0.8881250619888306\n",
      "Iteration 45550 Training loss 0.0025214748457074165 Validation loss 0.04523169994354248 Accuracy 0.8881250619888306\n",
      "Iteration 45560 Training loss 0.001271202345378697 Validation loss 0.04522346705198288 Accuracy 0.8886250257492065\n",
      "Iteration 45570 Training loss 2.590814074210357e-05 Validation loss 0.04525108262896538 Accuracy 0.8880000710487366\n",
      "Iteration 45580 Training loss 0.0012726311106234789 Validation loss 0.04524850845336914 Accuracy 0.8891250491142273\n",
      "Iteration 45590 Training loss 3.0211847843020223e-05 Validation loss 0.045276135206222534 Accuracy 0.8881250619888306\n",
      "Iteration 45600 Training loss 2.9739010642515495e-05 Validation loss 0.04526263475418091 Accuracy 0.8883750438690186\n",
      "Iteration 45610 Training loss 4.892749711871147e-05 Validation loss 0.045308735221624374 Accuracy 0.8877500295639038\n",
      "Iteration 45620 Training loss 0.0012769950553774834 Validation loss 0.04521450400352478 Accuracy 0.8888750672340393\n",
      "Iteration 45630 Training loss 3.44961772498209e-05 Validation loss 0.04521721228957176 Accuracy 0.8886250257492065\n",
      "Iteration 45640 Training loss 0.002523323753848672 Validation loss 0.04528070241212845 Accuracy 0.8877500295639038\n",
      "Iteration 45650 Training loss 3.648665006039664e-05 Validation loss 0.0452226884663105 Accuracy 0.8882500529289246\n",
      "Iteration 45660 Training loss 0.0012939939042553306 Validation loss 0.04522116482257843 Accuracy 0.8890000581741333\n",
      "Iteration 45670 Training loss 0.001280657364986837 Validation loss 0.04524296522140503 Accuracy 0.8882500529289246\n",
      "Iteration 45680 Training loss 0.0025443744380027056 Validation loss 0.04523657262325287 Accuracy 0.8887500166893005\n",
      "Iteration 45690 Training loss 2.5627963623264804e-05 Validation loss 0.04524325951933861 Accuracy 0.8886250257492065\n",
      "Iteration 45700 Training loss 0.002531918929889798 Validation loss 0.045241907238960266 Accuracy 0.8891250491142273\n",
      "Iteration 45710 Training loss 3.0312266972032376e-05 Validation loss 0.04524340480566025 Accuracy 0.8885000348091125\n",
      "Iteration 45720 Training loss 0.0025405590422451496 Validation loss 0.045243170112371445 Accuracy 0.8878750205039978\n",
      "Iteration 45730 Training loss 0.0012816473608836532 Validation loss 0.04521922767162323 Accuracy 0.8883750438690186\n",
      "Iteration 45740 Training loss 0.0012780294055119157 Validation loss 0.045219436287879944 Accuracy 0.8886250257492065\n",
      "Iteration 45750 Training loss 3.060257586184889e-05 Validation loss 0.045241352170705795 Accuracy 0.8878750205039978\n",
      "Iteration 45760 Training loss 0.0050152079202234745 Validation loss 0.04514496773481369 Accuracy 0.8895000219345093\n",
      "Iteration 45770 Training loss 0.0025273901410400867 Validation loss 0.04519520699977875 Accuracy 0.8883750438690186\n",
      "Iteration 45780 Training loss 3.274804839747958e-05 Validation loss 0.04519692063331604 Accuracy 0.8882500529289246\n",
      "Iteration 45790 Training loss 2.640885031723883e-05 Validation loss 0.04518814757466316 Accuracy 0.8891250491142273\n",
      "Iteration 45800 Training loss 2.317580765520688e-05 Validation loss 0.04515817016363144 Accuracy 0.8892500400543213\n",
      "Iteration 45810 Training loss 3.773696516873315e-05 Validation loss 0.04521631821990013 Accuracy 0.8885000348091125\n",
      "Iteration 45820 Training loss 2.1404397557489574e-05 Validation loss 0.0451752170920372 Accuracy 0.8888750672340393\n",
      "Iteration 45830 Training loss 0.0037732848431915045 Validation loss 0.045213550329208374 Accuracy 0.8886250257492065\n",
      "Iteration 45840 Training loss 4.623131826519966e-05 Validation loss 0.04519833251833916 Accuracy 0.8881250619888306\n",
      "Iteration 45850 Training loss 0.001286609098315239 Validation loss 0.04516911506652832 Accuracy 0.889750063419342\n",
      "Iteration 45860 Training loss 2.5845954951364547e-05 Validation loss 0.045257437974214554 Accuracy 0.8877500295639038\n",
      "Iteration 45870 Training loss 0.0012716205092146993 Validation loss 0.0452350452542305 Accuracy 0.8883750438690186\n",
      "Iteration 45880 Training loss 0.0012790539767593145 Validation loss 0.04526007920503616 Accuracy 0.8881250619888306\n",
      "Iteration 45890 Training loss 0.0012852649670094252 Validation loss 0.04522411525249481 Accuracy 0.8883750438690186\n",
      "Iteration 45900 Training loss 0.001291012391448021 Validation loss 0.045206405222415924 Accuracy 0.8882500529289246\n",
      "Iteration 45910 Training loss 0.0012772787595167756 Validation loss 0.04518686234951019 Accuracy 0.8893750309944153\n",
      "Iteration 45920 Training loss 0.0025278108660131693 Validation loss 0.04521595314145088 Accuracy 0.8887500166893005\n",
      "Iteration 45930 Training loss 0.0025235977955162525 Validation loss 0.04522541165351868 Accuracy 0.8885000348091125\n",
      "Iteration 45940 Training loss 0.002534853760153055 Validation loss 0.045219022780656815 Accuracy 0.8883750438690186\n",
      "Iteration 45950 Training loss 3.3875152439577505e-05 Validation loss 0.04522807523608208 Accuracy 0.8891250491142273\n",
      "Iteration 45960 Training loss 2.080936792481225e-05 Validation loss 0.045211922377347946 Accuracy 0.8892500400543213\n",
      "Iteration 45970 Training loss 2.536790634621866e-05 Validation loss 0.04521210864186287 Accuracy 0.89000004529953\n",
      "Iteration 45980 Training loss 3.050467603316065e-05 Validation loss 0.04526741802692413 Accuracy 0.8885000348091125\n",
      "Iteration 45990 Training loss 0.002525918185710907 Validation loss 0.045227862894535065 Accuracy 0.8896250128746033\n",
      "Iteration 46000 Training loss 0.0012730169110000134 Validation loss 0.0452222041785717 Accuracy 0.8895000219345093\n",
      "Iteration 46010 Training loss 0.001290175481699407 Validation loss 0.04524055868387222 Accuracy 0.8883750438690186\n",
      "Iteration 46020 Training loss 2.6156236344831996e-05 Validation loss 0.045248012989759445 Accuracy 0.8886250257492065\n",
      "Iteration 46030 Training loss 2.1350204406189732e-05 Validation loss 0.045255523175001144 Accuracy 0.8888750672340393\n",
      "Iteration 46040 Training loss 1.9395221897866577e-05 Validation loss 0.04522617906332016 Accuracy 0.8891250491142273\n",
      "Iteration 46050 Training loss 0.001281409291550517 Validation loss 0.04524053633213043 Accuracy 0.8885000348091125\n",
      "Iteration 46060 Training loss 2.647019937285222e-05 Validation loss 0.04520297050476074 Accuracy 0.8890000581741333\n",
      "Iteration 46070 Training loss 2.3882761524873786e-05 Validation loss 0.045218002051115036 Accuracy 0.8895000219345093\n",
      "Iteration 46080 Training loss 0.0037743060383945704 Validation loss 0.04523696377873421 Accuracy 0.8890000581741333\n",
      "Iteration 46090 Training loss 0.0012692539021372795 Validation loss 0.045304857194423676 Accuracy 0.8881250619888306\n",
      "Iteration 46100 Training loss 0.0012724752305075526 Validation loss 0.04522259905934334 Accuracy 0.8891250491142273\n",
      "Iteration 46110 Training loss 0.002529576886445284 Validation loss 0.04523000866174698 Accuracy 0.8885000348091125\n",
      "Iteration 46120 Training loss 0.00252357916906476 Validation loss 0.045182134956121445 Accuracy 0.8888750672340393\n",
      "Iteration 46130 Training loss 0.0012778028612956405 Validation loss 0.045216917991638184 Accuracy 0.8881250619888306\n",
      "Iteration 46140 Training loss 3.07811023958493e-05 Validation loss 0.04524930939078331 Accuracy 0.8881250619888306\n",
      "Iteration 46150 Training loss 0.001274374546483159 Validation loss 0.04525000974535942 Accuracy 0.8885000348091125\n",
      "Iteration 46160 Training loss 0.0012724503176286817 Validation loss 0.04524339735507965 Accuracy 0.8881250619888306\n",
      "Iteration 46170 Training loss 2.2743066438124515e-05 Validation loss 0.045249924063682556 Accuracy 0.8880000710487366\n",
      "Iteration 46180 Training loss 2.2275275114225224e-05 Validation loss 0.0452483706176281 Accuracy 0.8882500529289246\n",
      "Iteration 46190 Training loss 0.0025194520130753517 Validation loss 0.04526013508439064 Accuracy 0.8880000710487366\n",
      "Iteration 46200 Training loss 2.3076672732713632e-05 Validation loss 0.04526972398161888 Accuracy 0.8885000348091125\n",
      "Iteration 46210 Training loss 0.0025245510041713715 Validation loss 0.045244719833135605 Accuracy 0.8883750438690186\n",
      "Iteration 46220 Training loss 0.001268122810870409 Validation loss 0.04524049535393715 Accuracy 0.8891250491142273\n",
      "Iteration 46230 Training loss 0.0012806912418454885 Validation loss 0.04527492821216583 Accuracy 0.8885000348091125\n",
      "Iteration 46240 Training loss 0.0012757525546476245 Validation loss 0.04520693048834801 Accuracy 0.8890000581741333\n",
      "Iteration 46250 Training loss 0.0012752735055983067 Validation loss 0.04522445425391197 Accuracy 0.8892500400543213\n",
      "Iteration 46260 Training loss 0.00128456752281636 Validation loss 0.04526504874229431 Accuracy 0.8887500166893005\n",
      "Iteration 46270 Training loss 0.0025269018951803446 Validation loss 0.04522847384214401 Accuracy 0.8885000348091125\n",
      "Iteration 46280 Training loss 0.003771844319999218 Validation loss 0.045240938663482666 Accuracy 0.8886250257492065\n",
      "Iteration 46290 Training loss 0.0012709132861346006 Validation loss 0.04522879421710968 Accuracy 0.8896250128746033\n",
      "Iteration 46300 Training loss 0.0012673847377300262 Validation loss 0.04525838792324066 Accuracy 0.8885000348091125\n",
      "Iteration 46310 Training loss 0.0012811857741326094 Validation loss 0.045207567512989044 Accuracy 0.8895000219345093\n",
      "Iteration 46320 Training loss 0.001282712328247726 Validation loss 0.04523879289627075 Accuracy 0.8892500400543213\n",
      "Iteration 46330 Training loss 1.822645208449103e-05 Validation loss 0.04519222304224968 Accuracy 0.8893750309944153\n",
      "Iteration 46340 Training loss 0.002539923647418618 Validation loss 0.04521888121962547 Accuracy 0.8890000581741333\n",
      "Iteration 46350 Training loss 0.006274917628616095 Validation loss 0.04516996070742607 Accuracy 0.89000004529953\n",
      "Iteration 46360 Training loss 3.737763472599909e-05 Validation loss 0.0452381931245327 Accuracy 0.8885000348091125\n",
      "Iteration 46370 Training loss 3.0491561119561084e-05 Validation loss 0.045278772711753845 Accuracy 0.8883750438690186\n",
      "Iteration 46380 Training loss 0.0012745708227157593 Validation loss 0.04522005468606949 Accuracy 0.8886250257492065\n",
      "Iteration 46390 Training loss 0.0012857335386797786 Validation loss 0.045238886028528214 Accuracy 0.8888750672340393\n",
      "Iteration 46400 Training loss 2.0106148440390825e-05 Validation loss 0.045192424207925797 Accuracy 0.8886250257492065\n",
      "Iteration 46410 Training loss 0.002520743291825056 Validation loss 0.04522862657904625 Accuracy 0.8883750438690186\n",
      "Iteration 46420 Training loss 0.0025225647259503603 Validation loss 0.04522423818707466 Accuracy 0.8883750438690186\n",
      "Iteration 46430 Training loss 0.0037751267664134502 Validation loss 0.045216403901576996 Accuracy 0.8891250491142273\n",
      "Iteration 46440 Training loss 0.0012922539608553052 Validation loss 0.045220717787742615 Accuracy 0.8892500400543213\n",
      "Iteration 46450 Training loss 0.0012641162611544132 Validation loss 0.045270759612321854 Accuracy 0.8887500166893005\n",
      "Iteration 46460 Training loss 2.10334674193291e-05 Validation loss 0.04524950310587883 Accuracy 0.8882500529289246\n",
      "Iteration 46470 Training loss 0.0025305370800197124 Validation loss 0.045243628323078156 Accuracy 0.8881250619888306\n",
      "Iteration 46480 Training loss 0.002530153375118971 Validation loss 0.045248400419950485 Accuracy 0.8885000348091125\n",
      "Iteration 46490 Training loss 3.2800242479424924e-05 Validation loss 0.04524324834346771 Accuracy 0.8882500529289246\n",
      "Iteration 46500 Training loss 0.0037713937927037477 Validation loss 0.04525924101471901 Accuracy 0.8882500529289246\n",
      "Iteration 46510 Training loss 0.0025202797260135412 Validation loss 0.045250192284584045 Accuracy 0.8876250386238098\n",
      "Iteration 46520 Training loss 0.0012708958238363266 Validation loss 0.04522030055522919 Accuracy 0.8887500166893005\n",
      "Iteration 46530 Training loss 0.0012736298376694322 Validation loss 0.04528547823429108 Accuracy 0.8881250619888306\n",
      "Iteration 46540 Training loss 2.2153672034619376e-05 Validation loss 0.04523098096251488 Accuracy 0.8886250257492065\n",
      "Iteration 46550 Training loss 0.0012712536845356226 Validation loss 0.04520603269338608 Accuracy 0.8891250491142273\n",
      "Iteration 46560 Training loss 0.0025271913036704063 Validation loss 0.04523780941963196 Accuracy 0.8886250257492065\n",
      "Iteration 46570 Training loss 0.0012851764913648367 Validation loss 0.04521630331873894 Accuracy 0.8891250491142273\n",
      "Iteration 46580 Training loss 2.401913661742583e-05 Validation loss 0.04527086764574051 Accuracy 0.8882500529289246\n",
      "Iteration 46590 Training loss 0.002526999916881323 Validation loss 0.04526655375957489 Accuracy 0.8883750438690186\n",
      "Iteration 46600 Training loss 0.0012921309098601341 Validation loss 0.04525979980826378 Accuracy 0.8888750672340393\n",
      "Iteration 46610 Training loss 0.0012723037507385015 Validation loss 0.04528099671006203 Accuracy 0.8880000710487366\n",
      "Iteration 46620 Training loss 0.0012811360647901893 Validation loss 0.04527131840586662 Accuracy 0.8885000348091125\n",
      "Iteration 46630 Training loss 0.0012670926516875625 Validation loss 0.045257970690727234 Accuracy 0.8887500166893005\n",
      "Iteration 46640 Training loss 2.794727697619237e-05 Validation loss 0.04521283879876137 Accuracy 0.8890000581741333\n",
      "Iteration 46650 Training loss 2.4750623197178356e-05 Validation loss 0.04522603750228882 Accuracy 0.8888750672340393\n",
      "Iteration 46660 Training loss 0.0012688589049503207 Validation loss 0.04527575522661209 Accuracy 0.8886250257492065\n",
      "Iteration 46670 Training loss 0.001282749930396676 Validation loss 0.045234762132167816 Accuracy 0.8891250491142273\n",
      "Iteration 46680 Training loss 0.005027974955737591 Validation loss 0.04526901617646217 Accuracy 0.8880000710487366\n",
      "Iteration 46690 Training loss 0.0012709021102637053 Validation loss 0.045190904289484024 Accuracy 0.8895000219345093\n",
      "Iteration 46700 Training loss 0.00127224565949291 Validation loss 0.04522615298628807 Accuracy 0.8888750672340393\n",
      "Iteration 46710 Training loss 0.0025351440999656916 Validation loss 0.04521257057785988 Accuracy 0.8895000219345093\n",
      "Iteration 46720 Training loss 0.001289093168452382 Validation loss 0.045240458101034164 Accuracy 0.8890000581741333\n",
      "Iteration 46730 Training loss 2.929300899268128e-05 Validation loss 0.04527905583381653 Accuracy 0.8886250257492065\n",
      "Iteration 46740 Training loss 2.0374181985971518e-05 Validation loss 0.04528480023145676 Accuracy 0.8883750438690186\n",
      "Iteration 46750 Training loss 4.005299342679791e-05 Validation loss 0.04526012763381004 Accuracy 0.8890000581741333\n",
      "Iteration 46760 Training loss 0.001272155437618494 Validation loss 0.045261286199092865 Accuracy 0.8887500166893005\n",
      "Iteration 46770 Training loss 0.0012710047885775566 Validation loss 0.045237328857183456 Accuracy 0.8891250491142273\n",
      "Iteration 46780 Training loss 0.002528394805267453 Validation loss 0.045269113034009933 Accuracy 0.8885000348091125\n",
      "Iteration 46790 Training loss 2.0554200091282837e-05 Validation loss 0.04526504501700401 Accuracy 0.8888750672340393\n",
      "Iteration 46800 Training loss 2.1062358428025618e-05 Validation loss 0.045274220407009125 Accuracy 0.8887500166893005\n",
      "Iteration 46810 Training loss 2.0729261450469494e-05 Validation loss 0.04525486379861832 Accuracy 0.8893750309944153\n",
      "Iteration 46820 Training loss 0.0012683479581028223 Validation loss 0.045263733714818954 Accuracy 0.8881250619888306\n",
      "Iteration 46830 Training loss 0.0012702321400865912 Validation loss 0.045258570462465286 Accuracy 0.8886250257492065\n",
      "Iteration 46840 Training loss 0.0012812161585316062 Validation loss 0.04524599015712738 Accuracy 0.8895000219345093\n",
      "Iteration 46850 Training loss 0.0037780022248625755 Validation loss 0.04532185569405556 Accuracy 0.8882500529289246\n",
      "Iteration 46860 Training loss 0.0025299068074673414 Validation loss 0.045330923050642014 Accuracy 0.8882500529289246\n",
      "Iteration 46870 Training loss 0.0012806325685232878 Validation loss 0.045275479555130005 Accuracy 0.8891250491142273\n",
      "Iteration 46880 Training loss 0.0025194259360432625 Validation loss 0.045273829251527786 Accuracy 0.8893750309944153\n",
      "Iteration 46890 Training loss 0.0012781296391040087 Validation loss 0.045305218547582626 Accuracy 0.8887500166893005\n",
      "Iteration 46900 Training loss 2.8731081329169683e-05 Validation loss 0.04527777433395386 Accuracy 0.8887500166893005\n",
      "Iteration 46910 Training loss 0.0025214299093931913 Validation loss 0.045265913009643555 Accuracy 0.8892500400543213\n",
      "Iteration 46920 Training loss 2.31386766245123e-05 Validation loss 0.04524505138397217 Accuracy 0.8890000581741333\n",
      "Iteration 46930 Training loss 1.7681300960248336e-05 Validation loss 0.045239102095365524 Accuracy 0.8890000581741333\n",
      "Iteration 46940 Training loss 0.003744156565517187 Validation loss 0.0452127605676651 Accuracy 0.8893750309944153\n",
      "Iteration 46950 Training loss 0.001280141994357109 Validation loss 0.04522654414176941 Accuracy 0.8890000581741333\n",
      "Iteration 46960 Training loss 0.0037690340541303158 Validation loss 0.04520720988512039 Accuracy 0.8892500400543213\n",
      "Iteration 46970 Training loss 2.506611963326577e-05 Validation loss 0.045232728123664856 Accuracy 0.8891250491142273\n",
      "Iteration 46980 Training loss 1.950442310771905e-05 Validation loss 0.045254457741975784 Accuracy 0.8887500166893005\n",
      "Iteration 46990 Training loss 1.7779133486328647e-05 Validation loss 0.04519623890519142 Accuracy 0.8892500400543213\n",
      "Iteration 47000 Training loss 0.0025178820360451937 Validation loss 0.04522000253200531 Accuracy 0.8886250257492065\n",
      "Iteration 47010 Training loss 3.076894790865481e-05 Validation loss 0.04522918164730072 Accuracy 0.8887500166893005\n",
      "Iteration 47020 Training loss 0.0012786760926246643 Validation loss 0.045241259038448334 Accuracy 0.8895000219345093\n",
      "Iteration 47030 Training loss 0.0012885381001979113 Validation loss 0.04527653753757477 Accuracy 0.8891250491142273\n",
      "Iteration 47040 Training loss 3.4682707337196916e-05 Validation loss 0.04521998018026352 Accuracy 0.8893750309944153\n",
      "Iteration 47050 Training loss 0.0012700754450634122 Validation loss 0.04522687941789627 Accuracy 0.8893750309944153\n",
      "Iteration 47060 Training loss 0.0025248059537261724 Validation loss 0.04527517780661583 Accuracy 0.8886250257492065\n",
      "Iteration 47070 Training loss 2.43916038016323e-05 Validation loss 0.045275285840034485 Accuracy 0.8887500166893005\n",
      "Iteration 47080 Training loss 0.0012719747610390186 Validation loss 0.04525337368249893 Accuracy 0.8888750672340393\n",
      "Iteration 47090 Training loss 3.595318048610352e-05 Validation loss 0.045252662152051926 Accuracy 0.8891250491142273\n",
      "Iteration 47100 Training loss 0.0012713276082649827 Validation loss 0.045272037386894226 Accuracy 0.8886250257492065\n",
      "Iteration 47110 Training loss 4.1073712054640055e-05 Validation loss 0.04528160020709038 Accuracy 0.8890000581741333\n",
      "Iteration 47120 Training loss 2.5774028472369537e-05 Validation loss 0.04525863006711006 Accuracy 0.8886250257492065\n",
      "Iteration 47130 Training loss 0.0025308108888566494 Validation loss 0.045275088399648666 Accuracy 0.8895000219345093\n",
      "Iteration 47140 Training loss 3.308671875856817e-05 Validation loss 0.045272741466760635 Accuracy 0.8895000219345093\n",
      "Iteration 47150 Training loss 2.39226283156313e-05 Validation loss 0.0452873632311821 Accuracy 0.8888750672340393\n",
      "Iteration 47160 Training loss 2.9415781682473607e-05 Validation loss 0.04527317360043526 Accuracy 0.8887500166893005\n",
      "Iteration 47170 Training loss 0.001274695387110114 Validation loss 0.04524458572268486 Accuracy 0.8888750672340393\n",
      "Iteration 47180 Training loss 3.242514867451973e-05 Validation loss 0.04522361978888512 Accuracy 0.8891250491142273\n",
      "Iteration 47190 Training loss 0.0025010528042912483 Validation loss 0.04527559503912926 Accuracy 0.8882500529289246\n",
      "Iteration 47200 Training loss 0.0012818215182051063 Validation loss 0.04527227208018303 Accuracy 0.8880000710487366\n",
      "Iteration 47210 Training loss 0.0012806874001398683 Validation loss 0.04526534304022789 Accuracy 0.8883750438690186\n",
      "Iteration 47220 Training loss 2.8240472602192312e-05 Validation loss 0.04522562399506569 Accuracy 0.8888750672340393\n",
      "Iteration 47230 Training loss 0.0012852975632995367 Validation loss 0.04526761174201965 Accuracy 0.8886250257492065\n",
      "Iteration 47240 Training loss 2.146744736819528e-05 Validation loss 0.04528150334954262 Accuracy 0.8885000348091125\n",
      "Iteration 47250 Training loss 0.0012746135471388698 Validation loss 0.0452665276825428 Accuracy 0.8888750672340393\n",
      "Iteration 47260 Training loss 0.0012821935815736651 Validation loss 0.045242711901664734 Accuracy 0.8887500166893005\n",
      "Iteration 47270 Training loss 0.003772928612306714 Validation loss 0.045242007821798325 Accuracy 0.8885000348091125\n",
      "Iteration 47280 Training loss 0.001272890716791153 Validation loss 0.04525335133075714 Accuracy 0.8885000348091125\n",
      "Iteration 47290 Training loss 0.0025256520602852106 Validation loss 0.045337025076150894 Accuracy 0.8880000710487366\n",
      "Iteration 47300 Training loss 3.0461604183074087e-05 Validation loss 0.045237261801958084 Accuracy 0.8887500166893005\n",
      "Iteration 47310 Training loss 2.7415246222517453e-05 Validation loss 0.04523048549890518 Accuracy 0.8887500166893005\n",
      "Iteration 47320 Training loss 3.136896339128725e-05 Validation loss 0.045245613902807236 Accuracy 0.8883750438690186\n",
      "Iteration 47330 Training loss 2.5193132387357764e-05 Validation loss 0.04523776099085808 Accuracy 0.8883750438690186\n",
      "Iteration 47340 Training loss 0.0012757864315062761 Validation loss 0.04523611068725586 Accuracy 0.8886250257492065\n",
      "Iteration 47350 Training loss 0.0012817451497539878 Validation loss 0.04527546092867851 Accuracy 0.8887500166893005\n",
      "Iteration 47360 Training loss 0.001273545902222395 Validation loss 0.045270513743162155 Accuracy 0.8888750672340393\n",
      "Iteration 47370 Training loss 0.003773509757593274 Validation loss 0.045231789350509644 Accuracy 0.8895000219345093\n",
      "Iteration 47380 Training loss 0.002517516491934657 Validation loss 0.045224629342556 Accuracy 0.890375018119812\n",
      "Iteration 47390 Training loss 2.171242522308603e-05 Validation loss 0.04529067128896713 Accuracy 0.8891250491142273\n",
      "Iteration 47400 Training loss 0.0025199847295880318 Validation loss 0.04526420682668686 Accuracy 0.8886250257492065\n",
      "Iteration 47410 Training loss 0.0025229244492948055 Validation loss 0.04529745131731033 Accuracy 0.8890000581741333\n",
      "Iteration 47420 Training loss 0.002519944915547967 Validation loss 0.04528775066137314 Accuracy 0.8886250257492065\n",
      "Iteration 47430 Training loss 3.629614002420567e-05 Validation loss 0.04526152089238167 Accuracy 0.8881250619888306\n",
      "Iteration 47440 Training loss 1.7997737813857384e-05 Validation loss 0.04526887089014053 Accuracy 0.8882500529289246\n",
      "Iteration 47450 Training loss 0.002533863764256239 Validation loss 0.04527781158685684 Accuracy 0.8895000219345093\n",
      "Iteration 47460 Training loss 0.001277816598303616 Validation loss 0.04525928571820259 Accuracy 0.8892500400543213\n",
      "Iteration 47470 Training loss 2.355171091039665e-05 Validation loss 0.04526876285672188 Accuracy 0.889875054359436\n",
      "Iteration 47480 Training loss 0.00502106873318553 Validation loss 0.045350853353738785 Accuracy 0.8887500166893005\n",
      "Iteration 47490 Training loss 0.0012790512992069125 Validation loss 0.04526917636394501 Accuracy 0.8886250257492065\n",
      "Iteration 47500 Training loss 0.0012721115490421653 Validation loss 0.04521767050027847 Accuracy 0.8896250128746033\n",
      "Iteration 47510 Training loss 2.4979477529996075e-05 Validation loss 0.045208584517240524 Accuracy 0.8892500400543213\n",
      "Iteration 47520 Training loss 0.0025259791873395443 Validation loss 0.0452580451965332 Accuracy 0.8886250257492065\n",
      "Iteration 47530 Training loss 0.001272964058443904 Validation loss 0.045208949595689774 Accuracy 0.8895000219345093\n",
      "Iteration 47540 Training loss 0.00252267112955451 Validation loss 0.045201126486063004 Accuracy 0.8896250128746033\n",
      "Iteration 47550 Training loss 0.0012859043199568987 Validation loss 0.045277249068021774 Accuracy 0.8890000581741333\n",
      "Iteration 47560 Training loss 0.002524032024666667 Validation loss 0.04527072235941887 Accuracy 0.8892500400543213\n",
      "Iteration 47570 Training loss 0.0012714157346636057 Validation loss 0.045223984867334366 Accuracy 0.889750063419342\n",
      "Iteration 47580 Training loss 0.0012715974589809775 Validation loss 0.045232370495796204 Accuracy 0.8893750309944153\n",
      "Iteration 47590 Training loss 2.2634290871792473e-05 Validation loss 0.04526841640472412 Accuracy 0.8890000581741333\n",
      "Iteration 47600 Training loss 0.0012684756657108665 Validation loss 0.04524070769548416 Accuracy 0.8891250491142273\n",
      "Iteration 47610 Training loss 0.0012695130426436663 Validation loss 0.04521245136857033 Accuracy 0.89000004529953\n",
      "Iteration 47620 Training loss 2.423498335701879e-05 Validation loss 0.04527544602751732 Accuracy 0.8893750309944153\n",
      "Iteration 47630 Training loss 0.005026296712458134 Validation loss 0.04525681585073471 Accuracy 0.8892500400543213\n",
      "Iteration 47640 Training loss 0.0012715477496385574 Validation loss 0.045241571962833405 Accuracy 0.8895000219345093\n",
      "Iteration 47650 Training loss 2.5799286959227175e-05 Validation loss 0.045276470482349396 Accuracy 0.8891250491142273\n",
      "Iteration 47660 Training loss 0.0025233023334294558 Validation loss 0.04525258392095566 Accuracy 0.8892500400543213\n",
      "Iteration 47670 Training loss 0.0037800807040184736 Validation loss 0.04519922286272049 Accuracy 0.889750063419342\n",
      "Iteration 47680 Training loss 2.2063035430619493e-05 Validation loss 0.04525616019964218 Accuracy 0.8888750672340393\n",
      "Iteration 47690 Training loss 0.00253039738163352 Validation loss 0.04521596059203148 Accuracy 0.89000004529953\n",
      "Iteration 47700 Training loss 0.0037725467700511217 Validation loss 0.045353129506111145 Accuracy 0.8880000710487366\n",
      "Iteration 47710 Training loss 3.993161953985691e-05 Validation loss 0.04526430368423462 Accuracy 0.8891250491142273\n",
      "Iteration 47720 Training loss 0.0012767453445121646 Validation loss 0.045206647366285324 Accuracy 0.8895000219345093\n",
      "Iteration 47730 Training loss 0.0025216771755367517 Validation loss 0.04524612054228783 Accuracy 0.8895000219345093\n",
      "Iteration 47740 Training loss 0.003784380154684186 Validation loss 0.045303720980882645 Accuracy 0.8888750672340393\n",
      "Iteration 47750 Training loss 0.0037689325399696827 Validation loss 0.04521680623292923 Accuracy 0.890250027179718\n",
      "Iteration 47760 Training loss 2.8465097784646787e-05 Validation loss 0.04568765312433243 Accuracy 0.8888750672340393\n",
      "Iteration 47770 Training loss 0.002523943781852722 Validation loss 0.04530174285173416 Accuracy 0.8896250128746033\n",
      "Iteration 47780 Training loss 0.0012688891729339957 Validation loss 0.04531068354845047 Accuracy 0.889875054359436\n",
      "Iteration 47790 Training loss 0.0025413453113287687 Validation loss 0.045281749218702316 Accuracy 0.890125036239624\n",
      "Iteration 47800 Training loss 2.3831360522308387e-05 Validation loss 0.04525728523731232 Accuracy 0.889875054359436\n",
      "Iteration 47810 Training loss 0.0012683746172115207 Validation loss 0.04524454101920128 Accuracy 0.89000004529953\n",
      "Iteration 47820 Training loss 2.587854760349728e-05 Validation loss 0.04519173502922058 Accuracy 0.890375018119812\n",
      "Iteration 47830 Training loss 0.0025300015695393085 Validation loss 0.045199204236269 Accuracy 0.8896250128746033\n",
      "Iteration 47840 Training loss 0.0025299075059592724 Validation loss 0.04522301256656647 Accuracy 0.889750063419342\n",
      "Iteration 47850 Training loss 3.1260231480700895e-05 Validation loss 0.0452565997838974 Accuracy 0.8891250491142273\n",
      "Iteration 47860 Training loss 0.003777979640290141 Validation loss 0.045174483209848404 Accuracy 0.8906250596046448\n",
      "Iteration 47870 Training loss 2.0341627532616258e-05 Validation loss 0.045190852135419846 Accuracy 0.890375018119812\n",
      "Iteration 47880 Training loss 2.3540160327684134e-05 Validation loss 0.04524623230099678 Accuracy 0.8888750672340393\n",
      "Iteration 47890 Training loss 2.060135011561215e-05 Validation loss 0.04517451301217079 Accuracy 0.890125036239624\n",
      "Iteration 47900 Training loss 0.003769899718463421 Validation loss 0.04518882930278778 Accuracy 0.8895000219345093\n",
      "Iteration 47910 Training loss 0.0012803450226783752 Validation loss 0.04522166773676872 Accuracy 0.889750063419342\n",
      "Iteration 47920 Training loss 0.002520485781133175 Validation loss 0.04520644247531891 Accuracy 0.8895000219345093\n",
      "Iteration 47930 Training loss 0.002518391003832221 Validation loss 0.045186758041381836 Accuracy 0.8893750309944153\n",
      "Iteration 47940 Training loss 0.002511662198230624 Validation loss 0.04518968239426613 Accuracy 0.889875054359436\n",
      "Iteration 47950 Training loss 0.0012743069091811776 Validation loss 0.045215997844934464 Accuracy 0.8896250128746033\n",
      "Iteration 47960 Training loss 0.0012685379479080439 Validation loss 0.045171547681093216 Accuracy 0.890125036239624\n",
      "Iteration 47970 Training loss 0.0025243686977773905 Validation loss 0.045216288417577744 Accuracy 0.8890000581741333\n",
      "Iteration 47980 Training loss 0.005024388432502747 Validation loss 0.04520504176616669 Accuracy 0.8896250128746033\n",
      "Iteration 47990 Training loss 0.0025303601287305355 Validation loss 0.0451737605035305 Accuracy 0.889875054359436\n",
      "Iteration 48000 Training loss 0.0012722443789243698 Validation loss 0.045202456414699554 Accuracy 0.8892500400543213\n",
      "Iteration 48010 Training loss 1.802908445824869e-05 Validation loss 0.0452071949839592 Accuracy 0.8891250491142273\n",
      "Iteration 48020 Training loss 0.001278250478208065 Validation loss 0.0452447384595871 Accuracy 0.8892500400543213\n",
      "Iteration 48030 Training loss 0.0012723570689558983 Validation loss 0.04520031064748764 Accuracy 0.889750063419342\n",
      "Iteration 48040 Training loss 0.002522822003811598 Validation loss 0.04523060470819473 Accuracy 0.8893750309944153\n",
      "Iteration 48050 Training loss 1.839793549152091e-05 Validation loss 0.0451718233525753 Accuracy 0.890125036239624\n",
      "Iteration 48060 Training loss 2.7008312827092595e-05 Validation loss 0.04522562772035599 Accuracy 0.8892500400543213\n",
      "Iteration 48070 Training loss 0.001268755760975182 Validation loss 0.045242153108119965 Accuracy 0.8892500400543213\n",
      "Iteration 48080 Training loss 0.001270099077373743 Validation loss 0.04523030295968056 Accuracy 0.8895000219345093\n",
      "Iteration 48090 Training loss 0.005024156533181667 Validation loss 0.045254141092300415 Accuracy 0.8890000581741333\n",
      "Iteration 48100 Training loss 0.001295610680244863 Validation loss 0.04519309848546982 Accuracy 0.890250027179718\n",
      "Iteration 48110 Training loss 0.0025380533188581467 Validation loss 0.04522247612476349 Accuracy 0.8893750309944153\n",
      "Iteration 48120 Training loss 0.001278137438930571 Validation loss 0.04524656757712364 Accuracy 0.8891250491142273\n",
      "Iteration 48130 Training loss 2.2568143322132528e-05 Validation loss 0.04525231570005417 Accuracy 0.8891250491142273\n",
      "Iteration 48140 Training loss 0.0025397788267582655 Validation loss 0.04526691511273384 Accuracy 0.8892500400543213\n",
      "Iteration 48150 Training loss 0.0012728228466585279 Validation loss 0.04530901461839676 Accuracy 0.8887500166893005\n",
      "Iteration 48160 Training loss 0.0012791320914402604 Validation loss 0.04528123885393143 Accuracy 0.8893750309944153\n",
      "Iteration 48170 Training loss 0.001271537970751524 Validation loss 0.045394640415906906 Accuracy 0.8881250619888306\n",
      "Iteration 48180 Training loss 1.7912658222485334e-05 Validation loss 0.04531196132302284 Accuracy 0.889750063419342\n",
      "Iteration 48190 Training loss 0.0012689701979979873 Validation loss 0.045285798609256744 Accuracy 0.8895000219345093\n",
      "Iteration 48200 Training loss 0.0037751789204776287 Validation loss 0.04534286633133888 Accuracy 0.8887500166893005\n",
      "Iteration 48210 Training loss 0.0012738677905872464 Validation loss 0.045292794704437256 Accuracy 0.8888750672340393\n",
      "Iteration 48220 Training loss 0.0012729914160445333 Validation loss 0.04527956619858742 Accuracy 0.8896250128746033\n",
      "Iteration 48230 Training loss 0.0012768240412697196 Validation loss 0.0452694408595562 Accuracy 0.8893750309944153\n",
      "Iteration 48240 Training loss 0.003768673399463296 Validation loss 0.04529942572116852 Accuracy 0.8893750309944153\n",
      "Iteration 48250 Training loss 0.002518756315112114 Validation loss 0.04528675228357315 Accuracy 0.8896250128746033\n",
      "Iteration 48260 Training loss 0.0025219633243978024 Validation loss 0.0452427938580513 Accuracy 0.890375018119812\n",
      "Iteration 48270 Training loss 3.133937934762798e-05 Validation loss 0.04526779055595398 Accuracy 0.889750063419342\n",
      "Iteration 48280 Training loss 2.654264244483784e-05 Validation loss 0.045256227254867554 Accuracy 0.889875054359436\n",
      "Iteration 48290 Training loss 1.9982237063231878e-05 Validation loss 0.045268069952726364 Accuracy 0.8892500400543213\n",
      "Iteration 48300 Training loss 0.0012725699925795197 Validation loss 0.04524752497673035 Accuracy 0.8892500400543213\n",
      "Iteration 48310 Training loss 0.0013102806406095624 Validation loss 0.045236531645059586 Accuracy 0.8895000219345093\n",
      "Iteration 48320 Training loss 2.442939148750156e-05 Validation loss 0.045271135866642 Accuracy 0.8895000219345093\n",
      "Iteration 48330 Training loss 0.001266989391297102 Validation loss 0.04525486379861832 Accuracy 0.8895000219345093\n",
      "Iteration 48340 Training loss 1.6213913113460876e-05 Validation loss 0.045242175459861755 Accuracy 0.8896250128746033\n",
      "Iteration 48350 Training loss 0.0025246315635740757 Validation loss 0.0452251210808754 Accuracy 0.8895000219345093\n",
      "Iteration 48360 Training loss 0.0012738087680190802 Validation loss 0.045241136103868484 Accuracy 0.8891250491142273\n",
      "Iteration 48370 Training loss 0.002524616662412882 Validation loss 0.04522419348359108 Accuracy 0.8893750309944153\n",
      "Iteration 48380 Training loss 0.002523984294384718 Validation loss 0.045245423913002014 Accuracy 0.8888750672340393\n",
      "Iteration 48390 Training loss 0.006285996176302433 Validation loss 0.045242782682180405 Accuracy 0.8887500166893005\n",
      "Iteration 48400 Training loss 2.5319446649518795e-05 Validation loss 0.04525100067257881 Accuracy 0.8890000581741333\n",
      "Iteration 48410 Training loss 0.0025286388117820024 Validation loss 0.04523865133523941 Accuracy 0.8888750672340393\n",
      "Iteration 48420 Training loss 1.3901577403885312e-05 Validation loss 0.04523489251732826 Accuracy 0.8888750672340393\n",
      "Iteration 48430 Training loss 0.0012719205114990473 Validation loss 0.045252736657857895 Accuracy 0.8885000348091125\n",
      "Iteration 48440 Training loss 0.0037722736597061157 Validation loss 0.04526340961456299 Accuracy 0.8888750672340393\n",
      "Iteration 48450 Training loss 2.5343797460664064e-05 Validation loss 0.04525213688611984 Accuracy 0.8896250128746033\n",
      "Iteration 48460 Training loss 0.0025199572555720806 Validation loss 0.04524902626872063 Accuracy 0.8895000219345093\n",
      "Iteration 48470 Training loss 2.203417898272164e-05 Validation loss 0.04525510594248772 Accuracy 0.8895000219345093\n",
      "Iteration 48480 Training loss 0.0037696324288845062 Validation loss 0.04525154456496239 Accuracy 0.8895000219345093\n",
      "Iteration 48490 Training loss 0.0012728648725897074 Validation loss 0.04527071863412857 Accuracy 0.8895000219345093\n",
      "Iteration 48500 Training loss 2.331643736397382e-05 Validation loss 0.04524942487478256 Accuracy 0.8892500400543213\n",
      "Iteration 48510 Training loss 0.0025166692212224007 Validation loss 0.045274775475263596 Accuracy 0.8891250491142273\n",
      "Iteration 48520 Training loss 0.0012757823569700122 Validation loss 0.04533502086997032 Accuracy 0.8887500166893005\n",
      "Iteration 48530 Training loss 3.1320349080488086e-05 Validation loss 0.04527606442570686 Accuracy 0.8892500400543213\n",
      "Iteration 48540 Training loss 6.188140832819045e-05 Validation loss 0.04525933414697647 Accuracy 0.8891250491142273\n",
      "Iteration 48550 Training loss 2.8970305720577016e-05 Validation loss 0.045252617448568344 Accuracy 0.8892500400543213\n",
      "Iteration 48560 Training loss 0.0025235360953956842 Validation loss 0.045232515782117844 Accuracy 0.8896250128746033\n",
      "Iteration 48570 Training loss 0.001275508664548397 Validation loss 0.04525233060121536 Accuracy 0.8893750309944153\n",
      "Iteration 48580 Training loss 2.077139106404502e-05 Validation loss 0.0452190637588501 Accuracy 0.8893750309944153\n",
      "Iteration 48590 Training loss 0.002524491399526596 Validation loss 0.0452529639005661 Accuracy 0.8895000219345093\n",
      "Iteration 48600 Training loss 0.0012818952091038227 Validation loss 0.045273151248693466 Accuracy 0.8895000219345093\n",
      "Iteration 48610 Training loss 0.002523252507671714 Validation loss 0.045249514281749725 Accuracy 0.889750063419342\n",
      "Iteration 48620 Training loss 0.0025208175648003817 Validation loss 0.04530853405594826 Accuracy 0.8887500166893005\n",
      "Iteration 48630 Training loss 0.0012868951307609677 Validation loss 0.04523975029587746 Accuracy 0.8893750309944153\n",
      "Iteration 48640 Training loss 0.0025267659220844507 Validation loss 0.045276984572410583 Accuracy 0.8895000219345093\n",
      "Iteration 48650 Training loss 0.0012740466045215726 Validation loss 0.04525198042392731 Accuracy 0.8893750309944153\n",
      "Iteration 48660 Training loss 2.7690892238751985e-05 Validation loss 0.04525912180542946 Accuracy 0.8893750309944153\n",
      "Iteration 48670 Training loss 0.0025331429205834866 Validation loss 0.045308876782655716 Accuracy 0.8890000581741333\n",
      "Iteration 48680 Training loss 0.0012847328325733542 Validation loss 0.04526912048459053 Accuracy 0.889750063419342\n",
      "Iteration 48690 Training loss 2.6585003070067614e-05 Validation loss 0.045252855867147446 Accuracy 0.8891250491142273\n",
      "Iteration 48700 Training loss 0.0012777108931913972 Validation loss 0.04526015743613243 Accuracy 0.8891250491142273\n",
      "Iteration 48710 Training loss 0.0012724072439596057 Validation loss 0.04522470384836197 Accuracy 0.890125036239624\n",
      "Iteration 48720 Training loss 0.001270152279175818 Validation loss 0.04526319354772568 Accuracy 0.889750063419342\n",
      "Iteration 48730 Training loss 2.766886609606445e-05 Validation loss 0.045236967504024506 Accuracy 0.8893750309944153\n",
      "Iteration 48740 Training loss 2.5409715817659162e-05 Validation loss 0.04526782035827637 Accuracy 0.8890000581741333\n",
      "Iteration 48750 Training loss 0.002523889532312751 Validation loss 0.045227356255054474 Accuracy 0.8893750309944153\n",
      "Iteration 48760 Training loss 0.001269545522518456 Validation loss 0.04526086524128914 Accuracy 0.889875054359436\n",
      "Iteration 48770 Training loss 0.0012707733549177647 Validation loss 0.0452730655670166 Accuracy 0.8893750309944153\n",
      "Iteration 48780 Training loss 1.8804665160132572e-05 Validation loss 0.04531276226043701 Accuracy 0.8890000581741333\n",
      "Iteration 48790 Training loss 0.001269488362595439 Validation loss 0.045282311737537384 Accuracy 0.8892500400543213\n",
      "Iteration 48800 Training loss 0.0025213754270225763 Validation loss 0.04529356583952904 Accuracy 0.8895000219345093\n",
      "Iteration 48810 Training loss 0.0025285871233791113 Validation loss 0.045238398015499115 Accuracy 0.889875054359436\n",
      "Iteration 48820 Training loss 0.0037777088582515717 Validation loss 0.04523999243974686 Accuracy 0.8896250128746033\n",
      "Iteration 48830 Training loss 2.699334072531201e-05 Validation loss 0.04524756222963333 Accuracy 0.8896250128746033\n",
      "Iteration 48840 Training loss 1.6357058484572917e-05 Validation loss 0.045283470302820206 Accuracy 0.8886250257492065\n",
      "Iteration 48850 Training loss 0.006274226121604443 Validation loss 0.04526665806770325 Accuracy 0.889875054359436\n",
      "Iteration 48860 Training loss 0.0025295454543083906 Validation loss 0.04527688026428223 Accuracy 0.8896250128746033\n",
      "Iteration 48870 Training loss 0.0012763957493007183 Validation loss 0.04528547078371048 Accuracy 0.889750063419342\n",
      "Iteration 48880 Training loss 0.0012718760408461094 Validation loss 0.04529961571097374 Accuracy 0.8893750309944153\n",
      "Iteration 48890 Training loss 0.0025295766536146402 Validation loss 0.04530077427625656 Accuracy 0.8896250128746033\n",
      "Iteration 48900 Training loss 0.0025172834284603596 Validation loss 0.04527787119150162 Accuracy 0.889875054359436\n",
      "Iteration 48910 Training loss 0.002521677641198039 Validation loss 0.04525754228234291 Accuracy 0.8896250128746033\n",
      "Iteration 48920 Training loss 1.835632065194659e-05 Validation loss 0.045214150100946426 Accuracy 0.8892500400543213\n",
      "Iteration 48930 Training loss 0.0012733613839372993 Validation loss 0.04523306339979172 Accuracy 0.8893750309944153\n",
      "Iteration 48940 Training loss 2.9347762392717414e-05 Validation loss 0.04521426931023598 Accuracy 0.8895000219345093\n",
      "Iteration 48950 Training loss 0.0012732450850307941 Validation loss 0.04525349289178848 Accuracy 0.8892500400543213\n",
      "Iteration 48960 Training loss 2.5291541533079e-05 Validation loss 0.04526308923959732 Accuracy 0.8893750309944153\n",
      "Iteration 48970 Training loss 2.3627369955647737e-05 Validation loss 0.04524244740605354 Accuracy 0.8893750309944153\n",
      "Iteration 48980 Training loss 0.0012754128547385335 Validation loss 0.04523318260908127 Accuracy 0.889875054359436\n",
      "Iteration 48990 Training loss 0.001280203927308321 Validation loss 0.04531584307551384 Accuracy 0.8892500400543213\n",
      "Iteration 49000 Training loss 2.0380419300636277e-05 Validation loss 0.04529038444161415 Accuracy 0.8896250128746033\n",
      "Iteration 49010 Training loss 2.317802864126861e-05 Validation loss 0.04528354853391647 Accuracy 0.8893750309944153\n",
      "Iteration 49020 Training loss 0.0012753177434206009 Validation loss 0.04530354589223862 Accuracy 0.8890000581741333\n",
      "Iteration 49030 Training loss 0.002526255790144205 Validation loss 0.04529128596186638 Accuracy 0.8886250257492065\n",
      "Iteration 49040 Training loss 0.0012738057412207127 Validation loss 0.04530416429042816 Accuracy 0.8886250257492065\n",
      "Iteration 49050 Training loss 0.001267641200684011 Validation loss 0.04524850845336914 Accuracy 0.889750063419342\n",
      "Iteration 49060 Training loss 2.268080788780935e-05 Validation loss 0.04527512937784195 Accuracy 0.8891250491142273\n",
      "Iteration 49070 Training loss 0.0012762665282934904 Validation loss 0.045307185500860214 Accuracy 0.8888750672340393\n",
      "Iteration 49080 Training loss 2.9778419047943316e-05 Validation loss 0.04527981951832771 Accuracy 0.8891250491142273\n",
      "Iteration 49090 Training loss 0.002521711168810725 Validation loss 0.045249056071043015 Accuracy 0.8896250128746033\n",
      "Iteration 49100 Training loss 0.0012694182805716991 Validation loss 0.045279502868652344 Accuracy 0.8892500400543213\n",
      "Iteration 49110 Training loss 0.0025275396183133125 Validation loss 0.04528887942433357 Accuracy 0.8895000219345093\n",
      "Iteration 49120 Training loss 0.002538149245083332 Validation loss 0.04529232159256935 Accuracy 0.8891250491142273\n",
      "Iteration 49130 Training loss 1.648550824029371e-05 Validation loss 0.045292507857084274 Accuracy 0.8892500400543213\n",
      "Iteration 49140 Training loss 0.002526308177039027 Validation loss 0.04531335085630417 Accuracy 0.8891250491142273\n",
      "Iteration 49150 Training loss 0.0025243484415113926 Validation loss 0.04527899622917175 Accuracy 0.889875054359436\n",
      "Iteration 49160 Training loss 0.005028501618653536 Validation loss 0.045308876782655716 Accuracy 0.8887500166893005\n",
      "Iteration 49170 Training loss 0.0012726102722808719 Validation loss 0.04528211057186127 Accuracy 0.8893750309944153\n",
      "Iteration 49180 Training loss 0.0025211593601852655 Validation loss 0.04527026042342186 Accuracy 0.8892500400543213\n",
      "Iteration 49190 Training loss 0.0012780699180439115 Validation loss 0.045278776437044144 Accuracy 0.8890000581741333\n",
      "Iteration 49200 Training loss 0.002517319517210126 Validation loss 0.04525237902998924 Accuracy 0.8893750309944153\n",
      "Iteration 49210 Training loss 0.0012748156441375613 Validation loss 0.04528909549117088 Accuracy 0.8895000219345093\n",
      "Iteration 49220 Training loss 0.005015597213059664 Validation loss 0.045273829251527786 Accuracy 0.8895000219345093\n",
      "Iteration 49230 Training loss 2.1778907466796227e-05 Validation loss 0.0453178733587265 Accuracy 0.8887500166893005\n",
      "Iteration 49240 Training loss 3.297590956208296e-05 Validation loss 0.04529932513833046 Accuracy 0.8891250491142273\n",
      "Iteration 49250 Training loss 0.002526208059862256 Validation loss 0.045291949063539505 Accuracy 0.8892500400543213\n",
      "Iteration 49260 Training loss 0.0025344237219542265 Validation loss 0.045312296599149704 Accuracy 0.8890000581741333\n",
      "Iteration 49270 Training loss 0.0037785018794238567 Validation loss 0.04527519270777702 Accuracy 0.8893750309944153\n",
      "Iteration 49280 Training loss 0.0012741085374727845 Validation loss 0.04527868703007698 Accuracy 0.8891250491142273\n",
      "Iteration 49290 Training loss 0.001282060518860817 Validation loss 0.04530443251132965 Accuracy 0.8886250257492065\n",
      "Iteration 49300 Training loss 0.0012718323851004243 Validation loss 0.04526887089014053 Accuracy 0.8893750309944153\n",
      "Iteration 49310 Training loss 0.0025328039191663265 Validation loss 0.04528987407684326 Accuracy 0.8891250491142273\n",
      "Iteration 49320 Training loss 2.448300256219227e-05 Validation loss 0.04530995339155197 Accuracy 0.8888750672340393\n",
      "Iteration 49330 Training loss 0.001272203284315765 Validation loss 0.04526107758283615 Accuracy 0.8891250491142273\n",
      "Iteration 49340 Training loss 2.9857912522857077e-05 Validation loss 0.04523671790957451 Accuracy 0.8893750309944153\n",
      "Iteration 49350 Training loss 0.002523450180888176 Validation loss 0.045275673270225525 Accuracy 0.8893750309944153\n",
      "Iteration 49360 Training loss 0.0012767752632498741 Validation loss 0.045257627964019775 Accuracy 0.8893750309944153\n",
      "Iteration 49370 Training loss 2.977424992423039e-05 Validation loss 0.04527556896209717 Accuracy 0.8896250128746033\n",
      "Iteration 49380 Training loss 0.0012708374997600913 Validation loss 0.04527857527136803 Accuracy 0.8895000219345093\n",
      "Iteration 49390 Training loss 3.5284669138491154e-05 Validation loss 0.045292410999536514 Accuracy 0.8890000581741333\n",
      "Iteration 49400 Training loss 0.0037776126991957426 Validation loss 0.045258454978466034 Accuracy 0.8895000219345093\n",
      "Iteration 49410 Training loss 2.105273051711265e-05 Validation loss 0.04535461589694023 Accuracy 0.8890000581741333\n",
      "Iteration 49420 Training loss 0.0037812485825270414 Validation loss 0.045311495661735535 Accuracy 0.8887500166893005\n",
      "Iteration 49430 Training loss 3.2468615245306864e-05 Validation loss 0.045260991901159286 Accuracy 0.89000004529953\n",
      "Iteration 49440 Training loss 0.0025231095496565104 Validation loss 0.04529039189219475 Accuracy 0.8892500400543213\n",
      "Iteration 49450 Training loss 1.762897227308713e-05 Validation loss 0.04525616019964218 Accuracy 0.8895000219345093\n",
      "Iteration 49460 Training loss 1.712075936666224e-05 Validation loss 0.0452747605741024 Accuracy 0.8896250128746033\n",
      "Iteration 49470 Training loss 0.003775262041017413 Validation loss 0.045306406915187836 Accuracy 0.8892500400543213\n",
      "Iteration 49480 Training loss 0.002530621597543359 Validation loss 0.0452900305390358 Accuracy 0.8893750309944153\n",
      "Iteration 49490 Training loss 0.002527622040361166 Validation loss 0.04526262357831001 Accuracy 0.890125036239624\n",
      "Iteration 49500 Training loss 0.0012708670692518353 Validation loss 0.04530385509133339 Accuracy 0.889875054359436\n",
      "Iteration 49510 Training loss 0.0037776906974613667 Validation loss 0.04531801864504814 Accuracy 0.8890000581741333\n",
      "Iteration 49520 Training loss 1.5597015590174124e-05 Validation loss 0.0453023761510849 Accuracy 0.8891250491142273\n",
      "Iteration 49530 Training loss 1.8149230527342297e-05 Validation loss 0.04533242806792259 Accuracy 0.8888750672340393\n",
      "Iteration 49540 Training loss 0.0025160685181617737 Validation loss 0.0452767014503479 Accuracy 0.8895000219345093\n",
      "Iteration 49550 Training loss 0.002517350949347019 Validation loss 0.04526914656162262 Accuracy 0.8895000219345093\n",
      "Iteration 49560 Training loss 2.9146496672183275e-05 Validation loss 0.0452742800116539 Accuracy 0.8893750309944153\n",
      "Iteration 49570 Training loss 0.002523155650123954 Validation loss 0.04528117552399635 Accuracy 0.8893750309944153\n",
      "Iteration 49580 Training loss 1.8268397980136797e-05 Validation loss 0.04526141658425331 Accuracy 0.8895000219345093\n",
      "Iteration 49590 Training loss 0.00376988691277802 Validation loss 0.04530731216073036 Accuracy 0.8893750309944153\n",
      "Iteration 49600 Training loss 0.0012737924698740244 Validation loss 0.04530442878603935 Accuracy 0.8891250491142273\n",
      "Iteration 49610 Training loss 0.0037844523321837187 Validation loss 0.04531608894467354 Accuracy 0.8888750672340393\n",
      "Iteration 49620 Training loss 0.0012777125230059028 Validation loss 0.045326877385377884 Accuracy 0.8892500400543213\n",
      "Iteration 49630 Training loss 0.0012810471234843135 Validation loss 0.045298315584659576 Accuracy 0.8891250491142273\n",
      "Iteration 49640 Training loss 0.0012713975738734007 Validation loss 0.0453060045838356 Accuracy 0.8890000581741333\n",
      "Iteration 49650 Training loss 0.0025269719772040844 Validation loss 0.04532957822084427 Accuracy 0.8886250257492065\n",
      "Iteration 49660 Training loss 0.0012807246530428529 Validation loss 0.04532486945390701 Accuracy 0.8890000581741333\n",
      "Iteration 49670 Training loss 0.0025248678866773844 Validation loss 0.045292627066373825 Accuracy 0.8890000581741333\n",
      "Iteration 49680 Training loss 2.5893430574797094e-05 Validation loss 0.045334432274103165 Accuracy 0.8888750672340393\n",
      "Iteration 49690 Training loss 0.0025167246349155903 Validation loss 0.04532540217041969 Accuracy 0.8885000348091125\n",
      "Iteration 49700 Training loss 0.002520232927054167 Validation loss 0.04527071863412857 Accuracy 0.8893750309944153\n",
      "Iteration 49710 Training loss 0.0012696236371994019 Validation loss 0.045284997671842575 Accuracy 0.889750063419342\n",
      "Iteration 49720 Training loss 1.6393447367590852e-05 Validation loss 0.04531801864504814 Accuracy 0.8892500400543213\n",
      "Iteration 49730 Training loss 2.9466813430190086e-05 Validation loss 0.045326702296733856 Accuracy 0.8888750672340393\n",
      "Iteration 49740 Training loss 3.303631456219591e-05 Validation loss 0.045300595462322235 Accuracy 0.889750063419342\n",
      "Iteration 49750 Training loss 0.0012694522738456726 Validation loss 0.045321665704250336 Accuracy 0.8896250128746033\n",
      "Iteration 49760 Training loss 0.0012870957143604755 Validation loss 0.045311711728572845 Accuracy 0.889750063419342\n",
      "Iteration 49770 Training loss 0.0012703906977549195 Validation loss 0.045302774757146835 Accuracy 0.889750063419342\n",
      "Iteration 49780 Training loss 2.9334029022720642e-05 Validation loss 0.04532208666205406 Accuracy 0.8893750309944153\n",
      "Iteration 49790 Training loss 0.0012687889393419027 Validation loss 0.04526861384510994 Accuracy 0.8905000686645508\n",
      "Iteration 49800 Training loss 0.0012719559017568827 Validation loss 0.045298654586076736 Accuracy 0.8892500400543213\n",
      "Iteration 49810 Training loss 0.002518059452995658 Validation loss 0.04528500512242317 Accuracy 0.8893750309944153\n",
      "Iteration 49820 Training loss 0.0012737092329189181 Validation loss 0.04526830464601517 Accuracy 0.889750063419342\n",
      "Iteration 49830 Training loss 0.0025245253928005695 Validation loss 0.045260071754455566 Accuracy 0.8892500400543213\n",
      "Iteration 49840 Training loss 2.316188511031214e-05 Validation loss 0.0452999621629715 Accuracy 0.8893750309944153\n",
      "Iteration 49850 Training loss 3.0413162676268257e-05 Validation loss 0.04526442289352417 Accuracy 0.89000004529953\n",
      "Iteration 49860 Training loss 0.001272582565434277 Validation loss 0.0452992282807827 Accuracy 0.8893750309944153\n",
      "Iteration 49870 Training loss 3.4856922866310924e-05 Validation loss 0.04535243287682533 Accuracy 0.8890000581741333\n",
      "Iteration 49880 Training loss 1.807014450605493e-05 Validation loss 0.045315902680158615 Accuracy 0.889750063419342\n",
      "Iteration 49890 Training loss 0.0012708978028967977 Validation loss 0.045307472348213196 Accuracy 0.8892500400543213\n",
      "Iteration 49900 Training loss 2.100193341902923e-05 Validation loss 0.0453011728823185 Accuracy 0.8896250128746033\n",
      "Iteration 49910 Training loss 2.0771274648723193e-05 Validation loss 0.045320674777030945 Accuracy 0.8895000219345093\n",
      "Iteration 49920 Training loss 0.003769568633288145 Validation loss 0.04532022401690483 Accuracy 0.8896250128746033\n",
      "Iteration 49930 Training loss 0.0025259798858314753 Validation loss 0.045324862003326416 Accuracy 0.8892500400543213\n",
      "Iteration 49940 Training loss 0.00502407131716609 Validation loss 0.045296430587768555 Accuracy 0.8896250128746033\n",
      "Iteration 49950 Training loss 0.0025222590193152428 Validation loss 0.04527337849140167 Accuracy 0.8896250128746033\n",
      "Iteration 49960 Training loss 0.002523069968447089 Validation loss 0.04530251398682594 Accuracy 0.8892500400543213\n",
      "Iteration 49970 Training loss 0.0012735422933474183 Validation loss 0.04532041773200035 Accuracy 0.8891250491142273\n",
      "Iteration 49980 Training loss 0.002525028306990862 Validation loss 0.0453089214861393 Accuracy 0.889750063419342\n",
      "Iteration 49990 Training loss 1.994458398257848e-05 Validation loss 0.045287102460861206 Accuracy 0.8896250128746033\n",
      "Iteration 50000 Training loss 2.91743126581423e-05 Validation loss 0.045324258506298065 Accuracy 0.889875054359436\n",
      "Iteration 50010 Training loss 0.0012692681048065424 Validation loss 0.045348603278398514 Accuracy 0.8887500166893005\n",
      "Iteration 50020 Training loss 0.0012701601954177022 Validation loss 0.04531785100698471 Accuracy 0.8892500400543213\n",
      "Iteration 50030 Training loss 2.072124152618926e-05 Validation loss 0.04531894996762276 Accuracy 0.889750063419342\n",
      "Iteration 50040 Training loss 2.75271540886024e-05 Validation loss 0.0453227162361145 Accuracy 0.8892500400543213\n",
      "Iteration 50050 Training loss 0.0012638091575354338 Validation loss 0.04530992731451988 Accuracy 0.8893750309944153\n",
      "Iteration 50060 Training loss 0.0012697817292064428 Validation loss 0.04529142007231712 Accuracy 0.8892500400543213\n",
      "Iteration 50070 Training loss 0.0025217770598828793 Validation loss 0.045343395322561264 Accuracy 0.8892500400543213\n",
      "Iteration 50080 Training loss 0.003775740275159478 Validation loss 0.04530714079737663 Accuracy 0.889750063419342\n",
      "Iteration 50090 Training loss 0.0012720711529254913 Validation loss 0.0453806072473526 Accuracy 0.8892500400543213\n",
      "Iteration 50100 Training loss 0.0012741066748276353 Validation loss 0.04529711976647377 Accuracy 0.889750063419342\n",
      "Iteration 50110 Training loss 2.8577884222613648e-05 Validation loss 0.045317940413951874 Accuracy 0.8892500400543213\n",
      "Iteration 50120 Training loss 0.0012618617620319128 Validation loss 0.045371852815151215 Accuracy 0.8890000581741333\n",
      "Iteration 50130 Training loss 0.0025285121519118547 Validation loss 0.04532858356833458 Accuracy 0.8895000219345093\n",
      "Iteration 50140 Training loss 0.0012723953695967793 Validation loss 0.045316752046346664 Accuracy 0.889875054359436\n",
      "Iteration 50150 Training loss 0.002527061849832535 Validation loss 0.045338474214076996 Accuracy 0.8891250491142273\n",
      "Iteration 50160 Training loss 2.0846748157055117e-05 Validation loss 0.04531707987189293 Accuracy 0.8891250491142273\n",
      "Iteration 50170 Training loss 2.7846645025420003e-05 Validation loss 0.045307643711566925 Accuracy 0.8905000686645508\n",
      "Iteration 50180 Training loss 0.0025342884473502636 Validation loss 0.04531601071357727 Accuracy 0.8892500400543213\n",
      "Iteration 50190 Training loss 2.2781065126764588e-05 Validation loss 0.04531705752015114 Accuracy 0.8892500400543213\n",
      "Iteration 50200 Training loss 0.0037699216045439243 Validation loss 0.04533732682466507 Accuracy 0.8891250491142273\n",
      "Iteration 50210 Training loss 0.0012761637335643172 Validation loss 0.0453135184943676 Accuracy 0.8893750309944153\n",
      "Iteration 50220 Training loss 0.0025088137481361628 Validation loss 0.04532254859805107 Accuracy 0.8895000219345093\n",
      "Iteration 50230 Training loss 3.0793005862506106e-05 Validation loss 0.04536943510174751 Accuracy 0.8891250491142273\n",
      "Iteration 50240 Training loss 0.001268949592486024 Validation loss 0.04533831775188446 Accuracy 0.8891250491142273\n",
      "Iteration 50250 Training loss 0.0012701747473329306 Validation loss 0.045337166637182236 Accuracy 0.8892500400543213\n",
      "Iteration 50260 Training loss 0.001272300141863525 Validation loss 0.04537922888994217 Accuracy 0.8888750672340393\n",
      "Iteration 50270 Training loss 0.002523429226130247 Validation loss 0.045344430953264236 Accuracy 0.8887500166893005\n",
      "Iteration 50280 Training loss 0.0037732322234660387 Validation loss 0.04533249884843826 Accuracy 0.8892500400543213\n",
      "Iteration 50290 Training loss 0.0012882372830063105 Validation loss 0.04534228891134262 Accuracy 0.8886250257492065\n",
      "Iteration 50300 Training loss 0.0037643825635313988 Validation loss 0.045328304171562195 Accuracy 0.8892500400543213\n",
      "Iteration 50310 Training loss 2.3377573597826995e-05 Validation loss 0.04533560574054718 Accuracy 0.8892500400543213\n",
      "Iteration 50320 Training loss 2.585010588518344e-05 Validation loss 0.045416925102472305 Accuracy 0.8886250257492065\n",
      "Iteration 50330 Training loss 2.1095895135658793e-05 Validation loss 0.04534383490681648 Accuracy 0.8895000219345093\n",
      "Iteration 50340 Training loss 0.0012753581395372748 Validation loss 0.04538881033658981 Accuracy 0.8891250491142273\n",
      "Iteration 50350 Training loss 0.0012733795447275043 Validation loss 0.04533582925796509 Accuracy 0.8892500400543213\n",
      "Iteration 50360 Training loss 2.20789424929535e-05 Validation loss 0.04532456025481224 Accuracy 0.8892500400543213\n",
      "Iteration 50370 Training loss 0.002521954011172056 Validation loss 0.04532952606678009 Accuracy 0.8893750309944153\n",
      "Iteration 50380 Training loss 0.0012673187302425504 Validation loss 0.04536781460046768 Accuracy 0.8893750309944153\n",
      "Iteration 50390 Training loss 2.34327744692564e-05 Validation loss 0.04531332850456238 Accuracy 0.8895000219345093\n",
      "Iteration 50400 Training loss 0.002521298360079527 Validation loss 0.045313578099012375 Accuracy 0.8896250128746033\n",
      "Iteration 50410 Training loss 0.005016116425395012 Validation loss 0.04532010108232498 Accuracy 0.8893750309944153\n",
      "Iteration 50420 Training loss 0.002526338677853346 Validation loss 0.04535450041294098 Accuracy 0.8888750672340393\n",
      "Iteration 50430 Training loss 0.002526932628825307 Validation loss 0.045348361134529114 Accuracy 0.8893750309944153\n",
      "Iteration 50440 Training loss 0.0012722430983558297 Validation loss 0.045319460332393646 Accuracy 0.8887500166893005\n",
      "Iteration 50450 Training loss 2.4786097128526308e-05 Validation loss 0.045324910432100296 Accuracy 0.8891250491142273\n",
      "Iteration 50460 Training loss 0.0025222760159522295 Validation loss 0.04532448947429657 Accuracy 0.8893750309944153\n",
      "Iteration 50470 Training loss 0.005021465476602316 Validation loss 0.04529997706413269 Accuracy 0.8891250491142273\n",
      "Iteration 50480 Training loss 0.0012706827837973833 Validation loss 0.04529672861099243 Accuracy 0.8895000219345093\n",
      "Iteration 50490 Training loss 0.0025352761149406433 Validation loss 0.04530307278037071 Accuracy 0.890375018119812\n",
      "Iteration 50500 Training loss 0.0012708374997600913 Validation loss 0.045288048684597015 Accuracy 0.889750063419342\n",
      "Iteration 50510 Training loss 0.002519072499126196 Validation loss 0.045337799936532974 Accuracy 0.8891250491142273\n",
      "Iteration 50520 Training loss 1.8504899344407022e-05 Validation loss 0.045322008430957794 Accuracy 0.8888750672340393\n",
      "Iteration 50530 Training loss 3.0481518479064107e-05 Validation loss 0.04529198631644249 Accuracy 0.889750063419342\n",
      "Iteration 50540 Training loss 0.00127101328689605 Validation loss 0.04534224793314934 Accuracy 0.889750063419342\n",
      "Iteration 50550 Training loss 0.002500994363799691 Validation loss 0.045314155519008636 Accuracy 0.8895000219345093\n",
      "Iteration 50560 Training loss 0.001274489564821124 Validation loss 0.04532690718770027 Accuracy 0.8887500166893005\n",
      "Iteration 50570 Training loss 0.00252950144931674 Validation loss 0.04531628638505936 Accuracy 0.8890000581741333\n",
      "Iteration 50580 Training loss 0.00128275528550148 Validation loss 0.04531978443264961 Accuracy 0.8887500166893005\n",
      "Iteration 50590 Training loss 0.0012696803314611316 Validation loss 0.0453396700322628 Accuracy 0.8892500400543213\n",
      "Iteration 50600 Training loss 1.8118367734132335e-05 Validation loss 0.04532123729586601 Accuracy 0.890125036239624\n",
      "Iteration 50610 Training loss 0.003772572847083211 Validation loss 0.04531113803386688 Accuracy 0.8890000581741333\n",
      "Iteration 50620 Training loss 0.002526849042624235 Validation loss 0.04530829191207886 Accuracy 0.8893750309944153\n",
      "Iteration 50630 Training loss 0.001282700919546187 Validation loss 0.04531349614262581 Accuracy 0.889750063419342\n",
      "Iteration 50640 Training loss 2.326802132301964e-05 Validation loss 0.04532838612794876 Accuracy 0.8896250128746033\n",
      "Iteration 50650 Training loss 2.183252217946574e-05 Validation loss 0.045294586569070816 Accuracy 0.8892500400543213\n",
      "Iteration 50660 Training loss 0.001281603705137968 Validation loss 0.045335184782743454 Accuracy 0.8888750672340393\n",
      "Iteration 50670 Training loss 1.7942673366633244e-05 Validation loss 0.0453031025826931 Accuracy 0.8895000219345093\n",
      "Iteration 50680 Training loss 0.0025266592856496572 Validation loss 0.04531634598970413 Accuracy 0.8893750309944153\n",
      "Iteration 50690 Training loss 0.005019292701035738 Validation loss 0.04533351585268974 Accuracy 0.8891250491142273\n",
      "Iteration 50700 Training loss 0.0012737773358821869 Validation loss 0.04530465230345726 Accuracy 0.8893750309944153\n",
      "Iteration 50710 Training loss 0.0012803376885131001 Validation loss 0.045339833945035934 Accuracy 0.8883750438690186\n",
      "Iteration 50720 Training loss 0.0012701012892648578 Validation loss 0.045295558869838715 Accuracy 0.8893750309944153\n",
      "Iteration 50730 Training loss 0.0012702582171186805 Validation loss 0.0452985055744648 Accuracy 0.8895000219345093\n",
      "Iteration 50740 Training loss 0.00127244065515697 Validation loss 0.04527826979756355 Accuracy 0.8895000219345093\n",
      "Iteration 50750 Training loss 2.0228359062457457e-05 Validation loss 0.045350126922130585 Accuracy 0.889750063419342\n",
      "Iteration 50760 Training loss 0.0025463062338531017 Validation loss 0.04552648216485977 Accuracy 0.8895000219345093\n",
      "Iteration 50770 Training loss 0.0025175129994750023 Validation loss 0.0454036109149456 Accuracy 0.890250027179718\n",
      "Iteration 50780 Training loss 1.809089008020237e-05 Validation loss 0.04536598548293114 Accuracy 0.8891250491142273\n",
      "Iteration 50790 Training loss 1.6612435501883738e-05 Validation loss 0.045368678867816925 Accuracy 0.8893750309944153\n",
      "Iteration 50800 Training loss 0.0012689835857599974 Validation loss 0.04540544003248215 Accuracy 0.8883750438690186\n",
      "Iteration 50810 Training loss 2.3778122340445407e-05 Validation loss 0.045436471700668335 Accuracy 0.8878750205039978\n",
      "Iteration 50820 Training loss 0.002528388751670718 Validation loss 0.04537399113178253 Accuracy 0.8876250386238098\n",
      "Iteration 50830 Training loss 0.0012779838871210814 Validation loss 0.0453416146337986 Accuracy 0.8883750438690186\n",
      "Iteration 50840 Training loss 0.001266959705390036 Validation loss 0.04535606876015663 Accuracy 0.8886250257492065\n",
      "Iteration 50850 Training loss 0.001271971152164042 Validation loss 0.04539848491549492 Accuracy 0.8887500166893005\n",
      "Iteration 50860 Training loss 0.00501847080886364 Validation loss 0.04536982253193855 Accuracy 0.8893750309944153\n",
      "Iteration 50870 Training loss 0.0025178007781505585 Validation loss 0.045354340225458145 Accuracy 0.8891250491142273\n",
      "Iteration 50880 Training loss 0.003769845934584737 Validation loss 0.04541254788637161 Accuracy 0.8886250257492065\n",
      "Iteration 50890 Training loss 0.0012734682532027364 Validation loss 0.045418962836265564 Accuracy 0.8885000348091125\n",
      "Iteration 50900 Training loss 0.0012771375477313995 Validation loss 0.04539595916867256 Accuracy 0.8890000581741333\n",
      "Iteration 50910 Training loss 2.0165838577668183e-05 Validation loss 0.04540744051337242 Accuracy 0.8885000348091125\n",
      "Iteration 50920 Training loss 0.0037764410953968763 Validation loss 0.045423008501529694 Accuracy 0.8880000710487366\n",
      "Iteration 50930 Training loss 2.2117232219898142e-05 Validation loss 0.04536696523427963 Accuracy 0.8883750438690186\n",
      "Iteration 50940 Training loss 0.0037782383151352406 Validation loss 0.045379042625427246 Accuracy 0.8888750672340393\n",
      "Iteration 50950 Training loss 0.0025243305135518312 Validation loss 0.04543394222855568 Accuracy 0.8880000710487366\n",
      "Iteration 50960 Training loss 0.001270404551178217 Validation loss 0.04539080709218979 Accuracy 0.8890000581741333\n",
      "Iteration 50970 Training loss 0.0012841317802667618 Validation loss 0.045408204197883606 Accuracy 0.8887500166893005\n",
      "Iteration 50980 Training loss 0.001276580267585814 Validation loss 0.04539155587553978 Accuracy 0.8890000581741333\n",
      "Iteration 50990 Training loss 0.0012775999493896961 Validation loss 0.04539542272686958 Accuracy 0.8888750672340393\n",
      "Iteration 51000 Training loss 0.0012747050495818257 Validation loss 0.04541501775383949 Accuracy 0.8883750438690186\n",
      "Iteration 51010 Training loss 2.7375332138035446e-05 Validation loss 0.045364804565906525 Accuracy 0.8892500400543213\n",
      "Iteration 51020 Training loss 0.0012729302980005741 Validation loss 0.045407358556985855 Accuracy 0.8892500400543213\n",
      "Iteration 51030 Training loss 0.0012747065629810095 Validation loss 0.04538635537028313 Accuracy 0.8891250491142273\n",
      "Iteration 51040 Training loss 0.005023239646106958 Validation loss 0.04539576545357704 Accuracy 0.8893750309944153\n",
      "Iteration 51050 Training loss 0.0050193797796964645 Validation loss 0.04546051099896431 Accuracy 0.8883750438690186\n",
      "Iteration 51060 Training loss 0.003766890149563551 Validation loss 0.045405857264995575 Accuracy 0.8890000581741333\n",
      "Iteration 51070 Training loss 0.000231932383030653 Validation loss 0.04647185653448105 Accuracy 0.8875000476837158\n",
      "Iteration 51080 Training loss 2.248597957077436e-05 Validation loss 0.04540901258587837 Accuracy 0.890250027179718\n",
      "Iteration 51090 Training loss 0.0025494121946394444 Validation loss 0.0454091876745224 Accuracy 0.8888750672340393\n",
      "Iteration 51100 Training loss 0.0012679356150329113 Validation loss 0.04546528682112694 Accuracy 0.8886250257492065\n",
      "Iteration 51110 Training loss 3.082240073126741e-05 Validation loss 0.045452166348695755 Accuracy 0.8885000348091125\n",
      "Iteration 51120 Training loss 0.0012799210380762815 Validation loss 0.04538322612643242 Accuracy 0.8888750672340393\n",
      "Iteration 51130 Training loss 0.0025249531026929617 Validation loss 0.04542279988527298 Accuracy 0.8882500529289246\n",
      "Iteration 51140 Training loss 0.0012761071557179093 Validation loss 0.04537208750844002 Accuracy 0.8886250257492065\n",
      "Iteration 51150 Training loss 2.2026977603673004e-05 Validation loss 0.04540129378437996 Accuracy 0.8891250491142273\n",
      "Iteration 51160 Training loss 0.002519384492188692 Validation loss 0.04539817199110985 Accuracy 0.8886250257492065\n",
      "Iteration 51170 Training loss 0.0012774815550073981 Validation loss 0.04539547860622406 Accuracy 0.8890000581741333\n",
      "Iteration 51180 Training loss 2.075304655591026e-05 Validation loss 0.045402444899082184 Accuracy 0.8888750672340393\n",
      "Iteration 51190 Training loss 0.001281246542930603 Validation loss 0.04538358375430107 Accuracy 0.889875054359436\n",
      "Iteration 51200 Training loss 0.001271994085982442 Validation loss 0.045371104031801224 Accuracy 0.889750063419342\n",
      "Iteration 51210 Training loss 0.001276105409488082 Validation loss 0.045402634888887405 Accuracy 0.8895000219345093\n",
      "Iteration 51220 Training loss 0.0012685011606663465 Validation loss 0.04539194703102112 Accuracy 0.8890000581741333\n",
      "Iteration 51230 Training loss 0.0025215756613761187 Validation loss 0.04545110464096069 Accuracy 0.8886250257492065\n",
      "Iteration 51240 Training loss 2.870900061680004e-05 Validation loss 0.0454399436712265 Accuracy 0.8887500166893005\n",
      "Iteration 51250 Training loss 2.115107054123655e-05 Validation loss 0.04542408883571625 Accuracy 0.8886250257492065\n",
      "Iteration 51260 Training loss 0.0037686622235924006 Validation loss 0.04539866000413895 Accuracy 0.8895000219345093\n",
      "Iteration 51270 Training loss 0.0012687784619629383 Validation loss 0.04544388875365257 Accuracy 0.8887500166893005\n",
      "Iteration 51280 Training loss 0.003769888309761882 Validation loss 0.04539072513580322 Accuracy 0.8891250491142273\n",
      "Iteration 51290 Training loss 2.578372004791163e-05 Validation loss 0.04536856338381767 Accuracy 0.8891250491142273\n",
      "Iteration 51300 Training loss 0.0012706543784588575 Validation loss 0.04536456614732742 Accuracy 0.8890000581741333\n",
      "Iteration 51310 Training loss 0.003776300698518753 Validation loss 0.045553576201200485 Accuracy 0.8878750205039978\n",
      "Iteration 51320 Training loss 0.001272378722205758 Validation loss 0.045378584414720535 Accuracy 0.8890000581741333\n",
      "Iteration 51330 Training loss 0.0025193558540195227 Validation loss 0.04536793753504753 Accuracy 0.8887500166893005\n",
      "Iteration 51340 Training loss 0.0012949899537488818 Validation loss 0.04563501104712486 Accuracy 0.8880000710487366\n",
      "Iteration 51350 Training loss 3.2875042961677536e-05 Validation loss 0.045616213232278824 Accuracy 0.8880000710487366\n",
      "Iteration 51360 Training loss 0.0012995421420782804 Validation loss 0.045456238090991974 Accuracy 0.8887500166893005\n",
      "Iteration 51370 Training loss 0.00128032430075109 Validation loss 0.045487429946660995 Accuracy 0.8890000581741333\n",
      "Iteration 51380 Training loss 0.0012823750730603933 Validation loss 0.045507851988077164 Accuracy 0.8883750438690186\n",
      "Iteration 51390 Training loss 0.0025278558023273945 Validation loss 0.045561064034700394 Accuracy 0.8877500295639038\n",
      "Iteration 51400 Training loss 0.0025194466579705477 Validation loss 0.045450396835803986 Accuracy 0.8896250128746033\n",
      "Iteration 51410 Training loss 2.3523407435277477e-05 Validation loss 0.04552604630589485 Accuracy 0.8885000348091125\n",
      "Iteration 51420 Training loss 0.005027875769883394 Validation loss 0.04545457661151886 Accuracy 0.8885000348091125\n",
      "Iteration 51430 Training loss 2.8139604182797484e-05 Validation loss 0.045462921261787415 Accuracy 0.8886250257492065\n",
      "Iteration 51440 Training loss 0.0012733559124171734 Validation loss 0.0453985296189785 Accuracy 0.8886250257492065\n",
      "Iteration 51450 Training loss 0.0025190894957631826 Validation loss 0.04547271877527237 Accuracy 0.8885000348091125\n",
      "Iteration 51460 Training loss 0.0025202783290296793 Validation loss 0.045440446585416794 Accuracy 0.8885000348091125\n",
      "Iteration 51470 Training loss 0.0037697134539484978 Validation loss 0.04541711509227753 Accuracy 0.8880000710487366\n",
      "Iteration 51480 Training loss 1.705248541838955e-05 Validation loss 0.045409541577100754 Accuracy 0.8890000581741333\n",
      "Iteration 51490 Training loss 0.0012703366810455918 Validation loss 0.045413773506879807 Accuracy 0.8887500166893005\n",
      "Iteration 51500 Training loss 0.00127502903342247 Validation loss 0.0454477034509182 Accuracy 0.8885000348091125\n",
      "Iteration 51510 Training loss 0.003777736332267523 Validation loss 0.045405901968479156 Accuracy 0.8886250257492065\n",
      "Iteration 51520 Training loss 2.3417531338054687e-05 Validation loss 0.04542481526732445 Accuracy 0.8882500529289246\n",
      "Iteration 51530 Training loss 0.0012711251620203257 Validation loss 0.04544783756136894 Accuracy 0.8883750438690186\n",
      "Iteration 51540 Training loss 0.0012747518485412002 Validation loss 0.045450251549482346 Accuracy 0.8890000581741333\n",
      "Iteration 51550 Training loss 2.8184556867927313e-05 Validation loss 0.0454280786216259 Accuracy 0.8887500166893005\n",
      "Iteration 51560 Training loss 2.4524539185222238e-05 Validation loss 0.04543394595384598 Accuracy 0.8885000348091125\n",
      "Iteration 51570 Training loss 0.0025263321585953236 Validation loss 0.04546675831079483 Accuracy 0.8880000710487366\n",
      "Iteration 51580 Training loss 2.6268189685652032e-05 Validation loss 0.045532237738370895 Accuracy 0.8881250619888306\n",
      "Iteration 51590 Training loss 0.0012730562593787909 Validation loss 0.045469731092453 Accuracy 0.8885000348091125\n",
      "Iteration 51600 Training loss 0.001280135242268443 Validation loss 0.04545848071575165 Accuracy 0.8887500166893005\n",
      "Iteration 51610 Training loss 0.001272047869861126 Validation loss 0.0454537533223629 Accuracy 0.8882500529289246\n",
      "Iteration 51620 Training loss 2.454987588862423e-05 Validation loss 0.04544156789779663 Accuracy 0.8891250491142273\n",
      "Iteration 51630 Training loss 1.962784335773904e-05 Validation loss 0.0454849973320961 Accuracy 0.8890000581741333\n",
      "Iteration 51640 Training loss 0.0012709047878161073 Validation loss 0.04543047025799751 Accuracy 0.8890000581741333\n",
      "Iteration 51650 Training loss 2.2724712835042737e-05 Validation loss 0.04544683173298836 Accuracy 0.8887500166893005\n",
      "Iteration 51660 Training loss 1.8650214769877493e-05 Validation loss 0.04542357474565506 Accuracy 0.8890000581741333\n",
      "Iteration 51670 Training loss 1.986786264751572e-05 Validation loss 0.04541296139359474 Accuracy 0.8892500400543213\n",
      "Iteration 51680 Training loss 2.1286312403390184e-05 Validation loss 0.04541328176856041 Accuracy 0.8893750309944153\n",
      "Iteration 51690 Training loss 0.0012707312125712633 Validation loss 0.045389506965875626 Accuracy 0.8893750309944153\n",
      "Iteration 51700 Training loss 3.511738759698346e-05 Validation loss 0.045397400856018066 Accuracy 0.8892500400543213\n",
      "Iteration 51710 Training loss 1.8818747776094824e-05 Validation loss 0.04540621116757393 Accuracy 0.8891250491142273\n",
      "Iteration 51720 Training loss 0.0012776015792042017 Validation loss 0.045410577207803726 Accuracy 0.8895000219345093\n",
      "Iteration 51730 Training loss 8.61731605255045e-05 Validation loss 0.04541860520839691 Accuracy 0.8893750309944153\n",
      "Iteration 51740 Training loss 2.7900536224478856e-05 Validation loss 0.045492976903915405 Accuracy 0.8891250491142273\n",
      "Iteration 51750 Training loss 1.6971169316093437e-05 Validation loss 0.04539643973112106 Accuracy 0.8891250491142273\n",
      "Iteration 51760 Training loss 2.8409413062036037e-05 Validation loss 0.04549803212285042 Accuracy 0.8890000581741333\n",
      "Iteration 51770 Training loss 0.001269606756977737 Validation loss 0.04543193429708481 Accuracy 0.8887500166893005\n",
      "Iteration 51780 Training loss 2.8558240956044756e-05 Validation loss 0.04543028771877289 Accuracy 0.8887500166893005\n",
      "Iteration 51790 Training loss 0.003765728324651718 Validation loss 0.045416250824928284 Accuracy 0.8892500400543213\n",
      "Iteration 51800 Training loss 0.003777110483497381 Validation loss 0.045410774648189545 Accuracy 0.8890000581741333\n",
      "Iteration 51810 Training loss 3.149828262394294e-05 Validation loss 0.04548657312989235 Accuracy 0.8888750672340393\n",
      "Iteration 51820 Training loss 0.0025236760266125202 Validation loss 0.045446477830410004 Accuracy 0.8891250491142273\n",
      "Iteration 51830 Training loss 0.0025208296719938517 Validation loss 0.04542246460914612 Accuracy 0.8891250491142273\n",
      "Iteration 51840 Training loss 0.00127217557746917 Validation loss 0.045424479991197586 Accuracy 0.8893750309944153\n",
      "Iteration 51850 Training loss 0.002518431982025504 Validation loss 0.04539461433887482 Accuracy 0.8892500400543213\n",
      "Iteration 51860 Training loss 2.049613067356404e-05 Validation loss 0.045435961335897446 Accuracy 0.8887500166893005\n",
      "Iteration 51870 Training loss 0.0012764972634613514 Validation loss 0.04543173685669899 Accuracy 0.8891250491142273\n",
      "Iteration 51880 Training loss 2.3606786271557212e-05 Validation loss 0.04546375945210457 Accuracy 0.8888750672340393\n",
      "Iteration 51890 Training loss 0.003772872034460306 Validation loss 0.04542902484536171 Accuracy 0.8887500166893005\n",
      "Iteration 51900 Training loss 0.003770465264096856 Validation loss 0.0454220324754715 Accuracy 0.8891250491142273\n",
      "Iteration 51910 Training loss 0.002534208819270134 Validation loss 0.04539685323834419 Accuracy 0.8893750309944153\n",
      "Iteration 51920 Training loss 0.0012748584849759936 Validation loss 0.045423474162817 Accuracy 0.8891250491142273\n",
      "Iteration 51930 Training loss 0.0012692534364759922 Validation loss 0.04545045644044876 Accuracy 0.8888750672340393\n",
      "Iteration 51940 Training loss 0.0037758469115942717 Validation loss 0.045440416783094406 Accuracy 0.8893750309944153\n",
      "Iteration 51950 Training loss 0.001273858710192144 Validation loss 0.04549209401011467 Accuracy 0.8888750672340393\n",
      "Iteration 51960 Training loss 3.360258415341377e-05 Validation loss 0.04544968903064728 Accuracy 0.8890000581741333\n",
      "Iteration 51970 Training loss 0.0025192489847540855 Validation loss 0.04546145349740982 Accuracy 0.8888750672340393\n",
      "Iteration 51980 Training loss 0.002530723111703992 Validation loss 0.045440129935741425 Accuracy 0.8896250128746033\n",
      "Iteration 51990 Training loss 0.002525976160541177 Validation loss 0.04546590894460678 Accuracy 0.8892500400543213\n",
      "Iteration 52000 Training loss 0.0012780012330040336 Validation loss 0.04541933909058571 Accuracy 0.8890000581741333\n",
      "Iteration 52010 Training loss 0.002526159631088376 Validation loss 0.04545371234416962 Accuracy 0.8892500400543213\n",
      "Iteration 52020 Training loss 0.002521797316148877 Validation loss 0.04545363411307335 Accuracy 0.8892500400543213\n",
      "Iteration 52030 Training loss 1.7505421055830084e-05 Validation loss 0.045430585741996765 Accuracy 0.8896250128746033\n",
      "Iteration 52040 Training loss 0.0025194238405674696 Validation loss 0.04544541612267494 Accuracy 0.8893750309944153\n",
      "Iteration 52050 Training loss 0.0012678171042352915 Validation loss 0.045415863394737244 Accuracy 0.8892500400543213\n",
      "Iteration 52060 Training loss 2.5020892280736007e-05 Validation loss 0.04544032737612724 Accuracy 0.8888750672340393\n",
      "Iteration 52070 Training loss 0.0012686684494838119 Validation loss 0.04543551802635193 Accuracy 0.8890000581741333\n",
      "Iteration 52080 Training loss 2.4872770154615864e-05 Validation loss 0.045461542904376984 Accuracy 0.8886250257492065\n",
      "Iteration 52090 Training loss 0.001294031972065568 Validation loss 0.045462947338819504 Accuracy 0.8888750672340393\n",
      "Iteration 52100 Training loss 3.122336056549102e-05 Validation loss 0.04548697918653488 Accuracy 0.8892500400543213\n",
      "Iteration 52110 Training loss 2.153770037693903e-05 Validation loss 0.04543615132570267 Accuracy 0.889750063419342\n",
      "Iteration 52120 Training loss 2.453822162351571e-05 Validation loss 0.045434921979904175 Accuracy 0.8895000219345093\n",
      "Iteration 52130 Training loss 0.0025219228118658066 Validation loss 0.045429546386003494 Accuracy 0.8891250491142273\n",
      "Iteration 52140 Training loss 0.0037735127843916416 Validation loss 0.045442551374435425 Accuracy 0.8892500400543213\n",
      "Iteration 52150 Training loss 0.001268179970793426 Validation loss 0.04541943222284317 Accuracy 0.8887500166893005\n",
      "Iteration 52160 Training loss 0.0012682829983532429 Validation loss 0.04538727179169655 Accuracy 0.8890000581741333\n",
      "Iteration 52170 Training loss 3.389229823369533e-05 Validation loss 0.04541095346212387 Accuracy 0.8893750309944153\n",
      "Iteration 52180 Training loss 2.313251025043428e-05 Validation loss 0.045498162508010864 Accuracy 0.8891250491142273\n",
      "Iteration 52190 Training loss 2.4140317691490054e-05 Validation loss 0.04548783227801323 Accuracy 0.8890000581741333\n",
      "Iteration 52200 Training loss 0.002521338639780879 Validation loss 0.04549594223499298 Accuracy 0.8888750672340393\n",
      "Iteration 52210 Training loss 2.3317019440582953e-05 Validation loss 0.045450326055288315 Accuracy 0.8892500400543213\n",
      "Iteration 52220 Training loss 1.9705092199728824e-05 Validation loss 0.04543440416455269 Accuracy 0.8895000219345093\n",
      "Iteration 52230 Training loss 1.5219058695947751e-05 Validation loss 0.0453869104385376 Accuracy 0.8895000219345093\n",
      "Iteration 52240 Training loss 2.055654658761341e-05 Validation loss 0.045430853962898254 Accuracy 0.8890000581741333\n",
      "Iteration 52250 Training loss 0.0012736107455566525 Validation loss 0.0453728511929512 Accuracy 0.889750063419342\n",
      "Iteration 52260 Training loss 0.0025285878218710423 Validation loss 0.04543399065732956 Accuracy 0.8892500400543213\n",
      "Iteration 52270 Training loss 0.0012670919531956315 Validation loss 0.04541066661477089 Accuracy 0.8895000219345093\n",
      "Iteration 52280 Training loss 0.0050225816667079926 Validation loss 0.04542890563607216 Accuracy 0.8895000219345093\n",
      "Iteration 52290 Training loss 0.0012751099420711398 Validation loss 0.0454435721039772 Accuracy 0.8891250491142273\n",
      "Iteration 52300 Training loss 0.0012726632412523031 Validation loss 0.04542111977934837 Accuracy 0.8888750672340393\n",
      "Iteration 52310 Training loss 0.0012733236653730273 Validation loss 0.0454157255589962 Accuracy 0.8891250491142273\n",
      "Iteration 52320 Training loss 0.0037722003180533648 Validation loss 0.04539676010608673 Accuracy 0.8892500400543213\n",
      "Iteration 52330 Training loss 2.837869578797836e-05 Validation loss 0.04538961872458458 Accuracy 0.8892500400543213\n",
      "Iteration 52340 Training loss 2.3309714379138313e-05 Validation loss 0.0453890860080719 Accuracy 0.8888750672340393\n",
      "Iteration 52350 Training loss 0.001273765112273395 Validation loss 0.04539216682314873 Accuracy 0.8891250491142273\n",
      "Iteration 52360 Training loss 2.659404526639264e-05 Validation loss 0.045454997569322586 Accuracy 0.8892500400543213\n",
      "Iteration 52370 Training loss 0.0012721148086711764 Validation loss 0.04547484219074249 Accuracy 0.8890000581741333\n",
      "Iteration 52380 Training loss 0.0012909488286823034 Validation loss 0.045439768582582474 Accuracy 0.8890000581741333\n",
      "Iteration 52390 Training loss 0.0012680032523348927 Validation loss 0.04545808583498001 Accuracy 0.8887500166893005\n",
      "Iteration 52400 Training loss 0.00126893469132483 Validation loss 0.045437220484018326 Accuracy 0.8891250491142273\n",
      "Iteration 52410 Training loss 0.001269534695893526 Validation loss 0.04547138512134552 Accuracy 0.8890000581741333\n",
      "Iteration 52420 Training loss 0.0012690717121586204 Validation loss 0.04544249176979065 Accuracy 0.8892500400543213\n",
      "Iteration 52430 Training loss 2.1507954443222843e-05 Validation loss 0.04543067514896393 Accuracy 0.8896250128746033\n",
      "Iteration 52440 Training loss 0.0012655106838792562 Validation loss 0.045451607555150986 Accuracy 0.8890000581741333\n",
      "Iteration 52450 Training loss 0.0012699165381491184 Validation loss 0.04548241198062897 Accuracy 0.8893750309944153\n",
      "Iteration 52460 Training loss 0.0025216073263436556 Validation loss 0.04545196145772934 Accuracy 0.8895000219345093\n",
      "Iteration 52470 Training loss 0.00377041008323431 Validation loss 0.04545384272933006 Accuracy 0.8893750309944153\n",
      "Iteration 52480 Training loss 3.0861156119499356e-05 Validation loss 0.04546518623828888 Accuracy 0.8892500400543213\n",
      "Iteration 52490 Training loss 0.0012770340545102954 Validation loss 0.04546797275543213 Accuracy 0.8892500400543213\n",
      "Iteration 52500 Training loss 2.8025080609950237e-05 Validation loss 0.045427460223436356 Accuracy 0.8888750672340393\n",
      "Iteration 52510 Training loss 2.2865917344461195e-05 Validation loss 0.04542619735002518 Accuracy 0.8893750309944153\n",
      "Iteration 52520 Training loss 0.001276885042898357 Validation loss 0.04544214904308319 Accuracy 0.8892500400543213\n",
      "Iteration 52530 Training loss 0.0025296732783317566 Validation loss 0.0453881174325943 Accuracy 0.8895000219345093\n",
      "Iteration 52540 Training loss 1.824476021283772e-05 Validation loss 0.04541333392262459 Accuracy 0.8892500400543213\n",
      "Iteration 52550 Training loss 0.0012734646443277597 Validation loss 0.045405276119709015 Accuracy 0.8892500400543213\n",
      "Iteration 52560 Training loss 0.001278645358979702 Validation loss 0.045426204800605774 Accuracy 0.8890000581741333\n",
      "Iteration 52570 Training loss 0.0012676165206357837 Validation loss 0.045409683138132095 Accuracy 0.8892500400543213\n",
      "Iteration 52580 Training loss 0.0012764736311510205 Validation loss 0.04543044790625572 Accuracy 0.8893750309944153\n",
      "Iteration 52590 Training loss 0.0037730431649833918 Validation loss 0.04549121484160423 Accuracy 0.8891250491142273\n",
      "Iteration 52600 Training loss 0.0025207381695508957 Validation loss 0.04549887403845787 Accuracy 0.8885000348091125\n",
      "Iteration 52610 Training loss 0.0012953203404322267 Validation loss 0.04544873163104057 Accuracy 0.8891250491142273\n",
      "Iteration 52620 Training loss 2.6090490791830234e-05 Validation loss 0.045448239892721176 Accuracy 0.8891250491142273\n",
      "Iteration 52630 Training loss 0.0012685027904808521 Validation loss 0.04542124643921852 Accuracy 0.889750063419342\n",
      "Iteration 52640 Training loss 0.006276577711105347 Validation loss 0.04548207297921181 Accuracy 0.8888750672340393\n",
      "Iteration 52650 Training loss 2.4559893063269556e-05 Validation loss 0.04543128237128258 Accuracy 0.8893750309944153\n",
      "Iteration 52660 Training loss 1.7586495232535526e-05 Validation loss 0.045427434146404266 Accuracy 0.8893750309944153\n",
      "Iteration 52670 Training loss 0.0012649777345359325 Validation loss 0.045448992401361465 Accuracy 0.8895000219345093\n",
      "Iteration 52680 Training loss 0.0012758165830746293 Validation loss 0.04544810578227043 Accuracy 0.8892500400543213\n",
      "Iteration 52690 Training loss 0.0025274138897657394 Validation loss 0.04541368782520294 Accuracy 0.8892500400543213\n",
      "Iteration 52700 Training loss 0.0012766803847625852 Validation loss 0.04544392600655556 Accuracy 0.8892500400543213\n",
      "Iteration 52710 Training loss 0.001270469045266509 Validation loss 0.04546041414141655 Accuracy 0.8895000219345093\n",
      "Iteration 52720 Training loss 0.0037690429016947746 Validation loss 0.045476723462343216 Accuracy 0.8891250491142273\n",
      "Iteration 52730 Training loss 0.001269848900847137 Validation loss 0.04548057168722153 Accuracy 0.8893750309944153\n",
      "Iteration 52740 Training loss 0.0012722857063636184 Validation loss 0.04545349255204201 Accuracy 0.8892500400543213\n",
      "Iteration 52750 Training loss 0.002527612494304776 Validation loss 0.04542625695466995 Accuracy 0.8890000581741333\n",
      "Iteration 52760 Training loss 2.7552399842534214e-05 Validation loss 0.04547284543514252 Accuracy 0.8895000219345093\n",
      "Iteration 52770 Training loss 0.001278158277273178 Validation loss 0.04546983912587166 Accuracy 0.8893750309944153\n",
      "Iteration 52780 Training loss 0.0025243365671485662 Validation loss 0.045471515506505966 Accuracy 0.8892500400543213\n",
      "Iteration 52790 Training loss 0.00252335867844522 Validation loss 0.04547761380672455 Accuracy 0.8895000219345093\n",
      "Iteration 52800 Training loss 0.0012824786826968193 Validation loss 0.045461077243089676 Accuracy 0.8891250491142273\n",
      "Iteration 52810 Training loss 0.0025263421703130007 Validation loss 0.0454820916056633 Accuracy 0.8887500166893005\n",
      "Iteration 52820 Training loss 2.7247217076364905e-05 Validation loss 0.045473288744688034 Accuracy 0.8888750672340393\n",
      "Iteration 52830 Training loss 0.0012725061969831586 Validation loss 0.04547838121652603 Accuracy 0.8888750672340393\n",
      "Iteration 52840 Training loss 0.003769788658246398 Validation loss 0.0454508475959301 Accuracy 0.8890000581741333\n",
      "Iteration 52850 Training loss 1.773328949639108e-05 Validation loss 0.045479271560907364 Accuracy 0.8892500400543213\n",
      "Iteration 52860 Training loss 2.392150963714812e-05 Validation loss 0.04548090696334839 Accuracy 0.8892500400543213\n",
      "Iteration 52870 Training loss 0.0025339066050946712 Validation loss 0.0454685315489769 Accuracy 0.8891250491142273\n",
      "Iteration 52880 Training loss 0.0037736629601567984 Validation loss 0.04545082896947861 Accuracy 0.8891250491142273\n",
      "Iteration 52890 Training loss 2.1006595488870516e-05 Validation loss 0.04548794403672218 Accuracy 0.8891250491142273\n",
      "Iteration 52900 Training loss 0.002520106267184019 Validation loss 0.04546970874071121 Accuracy 0.8892500400543213\n",
      "Iteration 52910 Training loss 0.0012710427399724722 Validation loss 0.0454791821539402 Accuracy 0.8893750309944153\n",
      "Iteration 52920 Training loss 0.001273976406082511 Validation loss 0.045430988073349 Accuracy 0.8892500400543213\n",
      "Iteration 52930 Training loss 0.0025226327124983072 Validation loss 0.0454295314848423 Accuracy 0.8892500400543213\n",
      "Iteration 52940 Training loss 0.003772928612306714 Validation loss 0.04544262960553169 Accuracy 0.8896250128746033\n",
      "Iteration 52950 Training loss 0.0012704040855169296 Validation loss 0.04545518383383751 Accuracy 0.8891250491142273\n",
      "Iteration 52960 Training loss 0.0012707365676760674 Validation loss 0.04548191651701927 Accuracy 0.8890000581741333\n",
      "Iteration 52970 Training loss 0.0037722091656178236 Validation loss 0.045445747673511505 Accuracy 0.8893750309944153\n",
      "Iteration 52980 Training loss 3.0379425879800692e-05 Validation loss 0.04541812464594841 Accuracy 0.8895000219345093\n",
      "Iteration 52990 Training loss 0.0012674672761932015 Validation loss 0.045447640120983124 Accuracy 0.8892500400543213\n",
      "Iteration 53000 Training loss 0.0025256252847611904 Validation loss 0.04548175260424614 Accuracy 0.8888750672340393\n",
      "Iteration 53010 Training loss 2.219554880866781e-05 Validation loss 0.04547218233346939 Accuracy 0.8888750672340393\n",
      "Iteration 53020 Training loss 0.0037688075099140406 Validation loss 0.045437831431627274 Accuracy 0.8893750309944153\n",
      "Iteration 53030 Training loss 0.0025240962859243155 Validation loss 0.045447953045368195 Accuracy 0.8892500400543213\n",
      "Iteration 53040 Training loss 1.929363133967854e-05 Validation loss 0.04548872634768486 Accuracy 0.8891250491142273\n",
      "Iteration 53050 Training loss 3.220072539988905e-05 Validation loss 0.04545273631811142 Accuracy 0.8895000219345093\n",
      "Iteration 53060 Training loss 0.0012673938181251287 Validation loss 0.04547189548611641 Accuracy 0.8893750309944153\n",
      "Iteration 53070 Training loss 0.0025264874566346407 Validation loss 0.045454684644937515 Accuracy 0.8893750309944153\n",
      "Iteration 53080 Training loss 2.0663148461608216e-05 Validation loss 0.045457858592271805 Accuracy 0.8892500400543213\n",
      "Iteration 53090 Training loss 0.0012716528726741672 Validation loss 0.04548931494355202 Accuracy 0.8892500400543213\n",
      "Iteration 53100 Training loss 2.472278538334649e-05 Validation loss 0.04547671228647232 Accuracy 0.889750063419342\n",
      "Iteration 53110 Training loss 0.001273273373953998 Validation loss 0.04548418149352074 Accuracy 0.8892500400543213\n",
      "Iteration 53120 Training loss 0.0025194832123816013 Validation loss 0.045499321073293686 Accuracy 0.8892500400543213\n",
      "Iteration 53130 Training loss 0.0025258308742195368 Validation loss 0.045521706342697144 Accuracy 0.8890000581741333\n",
      "Iteration 53140 Training loss 3.1797284464119e-05 Validation loss 0.045472897589206696 Accuracy 0.8895000219345093\n",
      "Iteration 53150 Training loss 0.001281198114156723 Validation loss 0.04548802599310875 Accuracy 0.8890000581741333\n",
      "Iteration 53160 Training loss 0.0025202438700944185 Validation loss 0.0454343743622303 Accuracy 0.8895000219345093\n",
      "Iteration 53170 Training loss 0.0025261472910642624 Validation loss 0.04546591266989708 Accuracy 0.8895000219345093\n",
      "Iteration 53180 Training loss 0.0025301065761595964 Validation loss 0.0454351082444191 Accuracy 0.8895000219345093\n",
      "Iteration 53190 Training loss 0.0012762093683704734 Validation loss 0.04546080157160759 Accuracy 0.8896250128746033\n",
      "Iteration 53200 Training loss 0.0012658671475946903 Validation loss 0.04547455161809921 Accuracy 0.8892500400543213\n",
      "Iteration 53210 Training loss 0.0012734410120174289 Validation loss 0.045494161546230316 Accuracy 0.8893750309944153\n",
      "Iteration 53220 Training loss 0.0012660165084525943 Validation loss 0.045489318668842316 Accuracy 0.8895000219345093\n",
      "Iteration 53230 Training loss 0.0012737774522975087 Validation loss 0.04547421634197235 Accuracy 0.8893750309944153\n",
      "Iteration 53240 Training loss 0.0037674447521567345 Validation loss 0.04546833038330078 Accuracy 0.8892500400543213\n",
      "Iteration 53250 Training loss 2.212619801866822e-05 Validation loss 0.04548683390021324 Accuracy 0.8893750309944153\n",
      "Iteration 53260 Training loss 2.105712701450102e-05 Validation loss 0.045462463051080704 Accuracy 0.8893750309944153\n",
      "Iteration 53270 Training loss 0.0012726320419460535 Validation loss 0.04546624794602394 Accuracy 0.8892500400543213\n",
      "Iteration 53280 Training loss 0.0012786175357177854 Validation loss 0.04548770934343338 Accuracy 0.8891250491142273\n",
      "Iteration 53290 Training loss 2.7717933335225098e-05 Validation loss 0.045459527522325516 Accuracy 0.8896250128746033\n",
      "Iteration 53300 Training loss 1.5678868294344284e-05 Validation loss 0.04545556753873825 Accuracy 0.8896250128746033\n",
      "Iteration 53310 Training loss 0.002522194292396307 Validation loss 0.045491788536310196 Accuracy 0.8886250257492065\n",
      "Iteration 53320 Training loss 0.0012708281865343451 Validation loss 0.04543611407279968 Accuracy 0.8891250491142273\n",
      "Iteration 53330 Training loss 0.001265420112758875 Validation loss 0.04546770080924034 Accuracy 0.8893750309944153\n",
      "Iteration 53340 Training loss 0.00251650158315897 Validation loss 0.04545479640364647 Accuracy 0.889750063419342\n",
      "Iteration 53350 Training loss 0.002523635048419237 Validation loss 0.045490000396966934 Accuracy 0.8892500400543213\n",
      "Iteration 53360 Training loss 0.0025229051243513823 Validation loss 0.04548250883817673 Accuracy 0.8896250128746033\n",
      "Iteration 53370 Training loss 1.816508847696241e-05 Validation loss 0.0454670712351799 Accuracy 0.8895000219345093\n",
      "Iteration 53380 Training loss 0.0012741535902023315 Validation loss 0.04547284170985222 Accuracy 0.8896250128746033\n",
      "Iteration 53390 Training loss 0.0012783666606992483 Validation loss 0.04547363519668579 Accuracy 0.8892500400543213\n",
      "Iteration 53400 Training loss 2.153148307115771e-05 Validation loss 0.045468125492334366 Accuracy 0.8891250491142273\n",
      "Iteration 53410 Training loss 0.0012755938805639744 Validation loss 0.045471154153347015 Accuracy 0.8895000219345093\n",
      "Iteration 53420 Training loss 0.0012615456944331527 Validation loss 0.04548247531056404 Accuracy 0.8892500400543213\n",
      "Iteration 53430 Training loss 0.0037715064827352762 Validation loss 0.04547625780105591 Accuracy 0.8890000581741333\n",
      "Iteration 53440 Training loss 0.0012684536632150412 Validation loss 0.04544065147638321 Accuracy 0.8893750309944153\n",
      "Iteration 53450 Training loss 0.0050201378762722015 Validation loss 0.045499514788389206 Accuracy 0.8888750672340393\n",
      "Iteration 53460 Training loss 0.0012670571450144053 Validation loss 0.045524727553129196 Accuracy 0.8888750672340393\n",
      "Iteration 53470 Training loss 0.0012692426098510623 Validation loss 0.04545595124363899 Accuracy 0.8887500166893005\n",
      "Iteration 53480 Training loss 0.002528540324419737 Validation loss 0.04545687139034271 Accuracy 0.8892500400543213\n",
      "Iteration 53490 Training loss 2.073870928143151e-05 Validation loss 0.045448608696460724 Accuracy 0.8890000581741333\n",
      "Iteration 53500 Training loss 0.0012718599755316973 Validation loss 0.04543702304363251 Accuracy 0.8895000219345093\n",
      "Iteration 53510 Training loss 0.0012826111633330584 Validation loss 0.04542135074734688 Accuracy 0.8895000219345093\n",
      "Iteration 53520 Training loss 0.002524811774492264 Validation loss 0.04543963819742203 Accuracy 0.8888750672340393\n",
      "Iteration 53530 Training loss 0.0012689990689978004 Validation loss 0.04544588178396225 Accuracy 0.8893750309944153\n",
      "Iteration 53540 Training loss 1.878781222330872e-05 Validation loss 0.04543379694223404 Accuracy 0.8893750309944153\n",
      "Iteration 53550 Training loss 0.0025253223720937967 Validation loss 0.04543907195329666 Accuracy 0.8891250491142273\n",
      "Iteration 53560 Training loss 0.0012749736197292805 Validation loss 0.04545072466135025 Accuracy 0.8892500400543213\n",
      "Iteration 53570 Training loss 0.0012734235497191548 Validation loss 0.04541664198040962 Accuracy 0.8896250128746033\n",
      "Iteration 53580 Training loss 0.0025298630353063345 Validation loss 0.04542884975671768 Accuracy 0.8896250128746033\n",
      "Iteration 53590 Training loss 0.002526169875636697 Validation loss 0.04546569287776947 Accuracy 0.8892500400543213\n",
      "Iteration 53600 Training loss 0.0012748204171657562 Validation loss 0.04546027630567551 Accuracy 0.8893750309944153\n",
      "Iteration 53610 Training loss 0.0012702782405540347 Validation loss 0.045474208891391754 Accuracy 0.8893750309944153\n",
      "Iteration 53620 Training loss 0.001267837593331933 Validation loss 0.04550087824463844 Accuracy 0.8891250491142273\n",
      "Iteration 53630 Training loss 0.0012733055045828223 Validation loss 0.04545819014310837 Accuracy 0.8892500400543213\n",
      "Iteration 53640 Training loss 2.0114770450163633e-05 Validation loss 0.04545428603887558 Accuracy 0.8896250128746033\n",
      "Iteration 53650 Training loss 0.0012657357146963477 Validation loss 0.045480962842702866 Accuracy 0.8895000219345093\n",
      "Iteration 53660 Training loss 0.001270677661523223 Validation loss 0.045476216822862625 Accuracy 0.8892500400543213\n",
      "Iteration 53670 Training loss 0.001272363937459886 Validation loss 0.045467618852853775 Accuracy 0.8891250491142273\n",
      "Iteration 53680 Training loss 2.6428386263432913e-05 Validation loss 0.04545813426375389 Accuracy 0.8895000219345093\n",
      "Iteration 53690 Training loss 2.064363070530817e-05 Validation loss 0.04546444118022919 Accuracy 0.8892500400543213\n",
      "Iteration 53700 Training loss 0.006269597448408604 Validation loss 0.04550868272781372 Accuracy 0.8891250491142273\n",
      "Iteration 53710 Training loss 3.204017048119567e-05 Validation loss 0.04553000628948212 Accuracy 0.8895000219345093\n",
      "Iteration 53720 Training loss 1.9846183931804262e-05 Validation loss 0.04545808583498001 Accuracy 0.8892500400543213\n",
      "Iteration 53730 Training loss 0.0012817218666896224 Validation loss 0.04542919993400574 Accuracy 0.8893750309944153\n",
      "Iteration 53740 Training loss 1.838610478444025e-05 Validation loss 0.045441512018442154 Accuracy 0.8896250128746033\n",
      "Iteration 53750 Training loss 0.0012827536556869745 Validation loss 0.04547964781522751 Accuracy 0.8891250491142273\n",
      "Iteration 53760 Training loss 2.193873660871759e-05 Validation loss 0.04548618569970131 Accuracy 0.8887500166893005\n",
      "Iteration 53770 Training loss 0.0012673147721216083 Validation loss 0.04543245956301689 Accuracy 0.8891250491142273\n",
      "Iteration 53780 Training loss 1.6765427062637173e-05 Validation loss 0.04542728513479233 Accuracy 0.8891250491142273\n",
      "Iteration 53790 Training loss 0.0012721845414489508 Validation loss 0.045461252331733704 Accuracy 0.8891250491142273\n",
      "Iteration 53800 Training loss 0.0012678169878199697 Validation loss 0.045489635318517685 Accuracy 0.8890000581741333\n",
      "Iteration 53810 Training loss 0.0025250946637243032 Validation loss 0.04551712051033974 Accuracy 0.8887500166893005\n",
      "Iteration 53820 Training loss 0.0012684570392593741 Validation loss 0.04548121243715286 Accuracy 0.8895000219345093\n",
      "Iteration 53830 Training loss 0.0012692107120528817 Validation loss 0.045510079711675644 Accuracy 0.8887500166893005\n",
      "Iteration 53840 Training loss 0.0012737513752654195 Validation loss 0.04552776739001274 Accuracy 0.8888750672340393\n",
      "Iteration 53850 Training loss 2.2835141862742603e-05 Validation loss 0.0454796701669693 Accuracy 0.8892500400543213\n",
      "Iteration 53860 Training loss 0.0025335655082017183 Validation loss 0.045456577092409134 Accuracy 0.8891250491142273\n",
      "Iteration 53870 Training loss 0.0012656047474592924 Validation loss 0.04545584321022034 Accuracy 0.8891250491142273\n",
      "Iteration 53880 Training loss 0.001271486864425242 Validation loss 0.04549412429332733 Accuracy 0.8893750309944153\n",
      "Iteration 53890 Training loss 0.0025174575857818127 Validation loss 0.045432478189468384 Accuracy 0.889750063419342\n",
      "Iteration 53900 Training loss 1.84062919288408e-05 Validation loss 0.04544176161289215 Accuracy 0.8896250128746033\n",
      "Iteration 53910 Training loss 3.9852420741226524e-05 Validation loss 0.045459453016519547 Accuracy 0.8895000219345093\n",
      "Iteration 53920 Training loss 0.0012733626645058393 Validation loss 0.04546181112527847 Accuracy 0.889875054359436\n",
      "Iteration 53930 Training loss 1.9179580704076216e-05 Validation loss 0.045527417212724686 Accuracy 0.8890000581741333\n",
      "Iteration 53940 Training loss 0.0012656664475798607 Validation loss 0.04550831764936447 Accuracy 0.8893750309944153\n",
      "Iteration 53950 Training loss 0.001272939844056964 Validation loss 0.04548964276909828 Accuracy 0.8896250128746033\n",
      "Iteration 53960 Training loss 0.001277489005587995 Validation loss 0.04549780488014221 Accuracy 0.8891250491142273\n",
      "Iteration 53970 Training loss 3.1264433346223086e-05 Validation loss 0.045495714992284775 Accuracy 0.8892500400543213\n",
      "Iteration 53980 Training loss 1.9558126950869337e-05 Validation loss 0.04550418630242348 Accuracy 0.8891250491142273\n",
      "Iteration 53990 Training loss 0.0012745161075145006 Validation loss 0.04549555107951164 Accuracy 0.8891250491142273\n",
      "Iteration 54000 Training loss 0.0025200294330716133 Validation loss 0.045491550117731094 Accuracy 0.8895000219345093\n",
      "Iteration 54010 Training loss 0.002524837153032422 Validation loss 0.045475803315639496 Accuracy 0.8895000219345093\n",
      "Iteration 54020 Training loss 1.563640580570791e-05 Validation loss 0.045521754771471024 Accuracy 0.8893750309944153\n",
      "Iteration 54030 Training loss 1.968939795915503e-05 Validation loss 0.04549945518374443 Accuracy 0.8892500400543213\n",
      "Iteration 54040 Training loss 0.003769529052078724 Validation loss 0.045479271560907364 Accuracy 0.8890000581741333\n",
      "Iteration 54050 Training loss 1.8169062968809158e-05 Validation loss 0.04545507952570915 Accuracy 0.8891250491142273\n",
      "Iteration 54060 Training loss 0.002519212896004319 Validation loss 0.04546261951327324 Accuracy 0.889750063419342\n",
      "Iteration 54070 Training loss 2.539359229558613e-05 Validation loss 0.045511674135923386 Accuracy 0.8893750309944153\n",
      "Iteration 54080 Training loss 2.359347308811266e-05 Validation loss 0.04551558941602707 Accuracy 0.8892500400543213\n",
      "Iteration 54090 Training loss 2.489224061719142e-05 Validation loss 0.045486170798540115 Accuracy 0.8890000581741333\n",
      "Iteration 54100 Training loss 0.00252411887049675 Validation loss 0.04548756033182144 Accuracy 0.8892500400543213\n",
      "Iteration 54110 Training loss 0.0050146449357271194 Validation loss 0.045505449175834656 Accuracy 0.8893750309944153\n",
      "Iteration 54120 Training loss 1.7261339962715283e-05 Validation loss 0.045465901494026184 Accuracy 0.8893750309944153\n",
      "Iteration 54130 Training loss 0.0012709738221019506 Validation loss 0.04551975801587105 Accuracy 0.8895000219345093\n",
      "Iteration 54140 Training loss 0.00252806325443089 Validation loss 0.04548102244734764 Accuracy 0.8893750309944153\n",
      "Iteration 54150 Training loss 0.0037731605116277933 Validation loss 0.04550892114639282 Accuracy 0.8891250491142273\n",
      "Iteration 54160 Training loss 1.7492171537014656e-05 Validation loss 0.04548431187868118 Accuracy 0.8896250128746033\n",
      "Iteration 54170 Training loss 2.5798477508942597e-05 Validation loss 0.045470230281353 Accuracy 0.8892500400543213\n",
      "Iteration 54180 Training loss 0.002524136332795024 Validation loss 0.04549859091639519 Accuracy 0.8893750309944153\n",
      "Iteration 54190 Training loss 1.9739507479243912e-05 Validation loss 0.04553401470184326 Accuracy 0.8893750309944153\n",
      "Iteration 54200 Training loss 0.0012736081844195724 Validation loss 0.04553733393549919 Accuracy 0.8887500166893005\n",
      "Iteration 54210 Training loss 0.001267037820070982 Validation loss 0.04552405700087547 Accuracy 0.8890000581741333\n",
      "Iteration 54220 Training loss 1.8280112271895632e-05 Validation loss 0.04551520571112633 Accuracy 0.8890000581741333\n",
      "Iteration 54230 Training loss 0.0012682476080954075 Validation loss 0.04547996073961258 Accuracy 0.8893750309944153\n",
      "Iteration 54240 Training loss 0.001270575332455337 Validation loss 0.04550229758024216 Accuracy 0.8892500400543213\n",
      "Iteration 54250 Training loss 0.0012710137525573373 Validation loss 0.04550715908408165 Accuracy 0.8893750309944153\n",
      "Iteration 54260 Training loss 1.893987791845575e-05 Validation loss 0.04548609256744385 Accuracy 0.8891250491142273\n",
      "Iteration 54270 Training loss 0.001268108026124537 Validation loss 0.0454862080514431 Accuracy 0.8892500400543213\n",
      "Iteration 54280 Training loss 0.001273825764656067 Validation loss 0.04546580836176872 Accuracy 0.8890000581741333\n",
      "Iteration 54290 Training loss 1.424611036782153e-05 Validation loss 0.04547415301203728 Accuracy 0.8893750309944153\n",
      "Iteration 54300 Training loss 1.8498607460060157e-05 Validation loss 0.0455162487924099 Accuracy 0.8892500400543213\n",
      "Iteration 54310 Training loss 0.0012722129467874765 Validation loss 0.04549751430749893 Accuracy 0.8896250128746033\n",
      "Iteration 54320 Training loss 0.0012686856789514422 Validation loss 0.04549097269773483 Accuracy 0.8895000219345093\n",
      "Iteration 54330 Training loss 0.001275795977562666 Validation loss 0.04548053815960884 Accuracy 0.889875054359436\n",
      "Iteration 54340 Training loss 0.003778546117246151 Validation loss 0.045513056218624115 Accuracy 0.8892500400543213\n",
      "Iteration 54350 Training loss 0.003765099449083209 Validation loss 0.04550064355134964 Accuracy 0.8896250128746033\n",
      "Iteration 54360 Training loss 0.002525107702240348 Validation loss 0.04548846557736397 Accuracy 0.8896250128746033\n",
      "Iteration 54370 Training loss 2.3825828975532204e-05 Validation loss 0.04549882560968399 Accuracy 0.8887500166893005\n",
      "Iteration 54380 Training loss 0.0025204818230122328 Validation loss 0.04548715054988861 Accuracy 0.8892500400543213\n",
      "Iteration 54390 Training loss 0.003776252968236804 Validation loss 0.04550964757800102 Accuracy 0.8893750309944153\n",
      "Iteration 54400 Training loss 0.002514204941689968 Validation loss 0.04551355168223381 Accuracy 0.8893750309944153\n",
      "Iteration 54410 Training loss 1.8591061234474182e-05 Validation loss 0.04549848288297653 Accuracy 0.8891250491142273\n",
      "Iteration 54420 Training loss 0.0012711979215964675 Validation loss 0.04547415301203728 Accuracy 0.8890000581741333\n",
      "Iteration 54430 Training loss 0.002518130000680685 Validation loss 0.0454864539206028 Accuracy 0.8893750309944153\n",
      "Iteration 54440 Training loss 0.0012662213994190097 Validation loss 0.045474857091903687 Accuracy 0.8892500400543213\n",
      "Iteration 54450 Training loss 0.0025159218348562717 Validation loss 0.04550536721944809 Accuracy 0.8892500400543213\n",
      "Iteration 54460 Training loss 2.1507436031242833e-05 Validation loss 0.04549489915370941 Accuracy 0.8893750309944153\n",
      "Iteration 54470 Training loss 0.001268020598217845 Validation loss 0.045481227338314056 Accuracy 0.8896250128746033\n",
      "Iteration 54480 Training loss 0.0012766303261741996 Validation loss 0.04549737647175789 Accuracy 0.8895000219345093\n",
      "Iteration 54490 Training loss 0.0012765179853886366 Validation loss 0.04549594968557358 Accuracy 0.889750063419342\n",
      "Iteration 54500 Training loss 0.0012760576792061329 Validation loss 0.045538563281297684 Accuracy 0.8893750309944153\n",
      "Iteration 54510 Training loss 0.002517921384423971 Validation loss 0.04546887427568436 Accuracy 0.8895000219345093\n",
      "Iteration 54520 Training loss 0.00251809717155993 Validation loss 0.0454842746257782 Accuracy 0.8893750309944153\n",
      "Iteration 54530 Training loss 0.00127036077901721 Validation loss 0.04550955817103386 Accuracy 0.8890000581741333\n",
      "Iteration 54540 Training loss 1.9584476831369102e-05 Validation loss 0.045465417206287384 Accuracy 0.8895000219345093\n",
      "Iteration 54550 Training loss 0.002528573852032423 Validation loss 0.045503828674554825 Accuracy 0.8892500400543213\n",
      "Iteration 54560 Training loss 2.257055530208163e-05 Validation loss 0.045455798506736755 Accuracy 0.8893750309944153\n",
      "Iteration 54570 Training loss 2.1118492441019043e-05 Validation loss 0.045438263565301895 Accuracy 0.8893750309944153\n",
      "Iteration 54580 Training loss 0.0012684770626947284 Validation loss 0.045451030135154724 Accuracy 0.8888750672340393\n",
      "Iteration 54590 Training loss 0.0012757034273818135 Validation loss 0.045449431985616684 Accuracy 0.8893750309944153\n",
      "Iteration 54600 Training loss 1.7857666534837335e-05 Validation loss 0.04546188563108444 Accuracy 0.8890000581741333\n",
      "Iteration 54610 Training loss 2.2090323909651488e-05 Validation loss 0.045501504093408585 Accuracy 0.8888750672340393\n",
      "Iteration 54620 Training loss 2.2141686713439412e-05 Validation loss 0.04546429589390755 Accuracy 0.8892500400543213\n",
      "Iteration 54630 Training loss 0.001268876134417951 Validation loss 0.04549441114068031 Accuracy 0.8891250491142273\n",
      "Iteration 54640 Training loss 0.0025235172361135483 Validation loss 0.045497920364141464 Accuracy 0.8888750672340393\n",
      "Iteration 54650 Training loss 2.3817925466573797e-05 Validation loss 0.04549672082066536 Accuracy 0.8892500400543213\n",
      "Iteration 54660 Training loss 0.0025138999335467815 Validation loss 0.04547062888741493 Accuracy 0.8896250128746033\n",
      "Iteration 54670 Training loss 1.4344493138196412e-05 Validation loss 0.04549047350883484 Accuracy 0.8892500400543213\n",
      "Iteration 54680 Training loss 2.0540603145491332e-05 Validation loss 0.04545854404568672 Accuracy 0.889750063419342\n",
      "Iteration 54690 Training loss 0.001269591273739934 Validation loss 0.045463692396879196 Accuracy 0.8895000219345093\n",
      "Iteration 54700 Training loss 0.0012697771890088916 Validation loss 0.045442819595336914 Accuracy 0.8892500400543213\n",
      "Iteration 54710 Training loss 1.633338615647517e-05 Validation loss 0.04545196145772934 Accuracy 0.8893750309944153\n",
      "Iteration 54720 Training loss 0.0012664609821513295 Validation loss 0.04546127840876579 Accuracy 0.889750063419342\n",
      "Iteration 54730 Training loss 1.9993452951894142e-05 Validation loss 0.045477837324142456 Accuracy 0.8892500400543213\n",
      "Iteration 54740 Training loss 2.676595795492176e-05 Validation loss 0.04548172652721405 Accuracy 0.8890000581741333\n",
      "Iteration 54750 Training loss 2.358168967475649e-05 Validation loss 0.0454719103872776 Accuracy 0.8891250491142273\n",
      "Iteration 54760 Training loss 2.3846980184316635e-05 Validation loss 0.04546672850847244 Accuracy 0.8895000219345093\n",
      "Iteration 54770 Training loss 0.0012703664833679795 Validation loss 0.04550468921661377 Accuracy 0.8892500400543213\n",
      "Iteration 54780 Training loss 2.189151018683333e-05 Validation loss 0.045501649379730225 Accuracy 0.8887500166893005\n",
      "Iteration 54790 Training loss 0.0025237344671040773 Validation loss 0.04550958052277565 Accuracy 0.8887500166893005\n",
      "Iteration 54800 Training loss 0.0025144624523818493 Validation loss 0.045489486306905746 Accuracy 0.8892500400543213\n",
      "Iteration 54810 Training loss 2.2607709979638457e-05 Validation loss 0.04552546888589859 Accuracy 0.8888750672340393\n",
      "Iteration 54820 Training loss 2.4214978111558594e-05 Validation loss 0.045502956956624985 Accuracy 0.8895000219345093\n",
      "Iteration 54830 Training loss 0.005023618694394827 Validation loss 0.04549708589911461 Accuracy 0.8893750309944153\n",
      "Iteration 54840 Training loss 1.817131669668015e-05 Validation loss 0.04546904191374779 Accuracy 0.8893750309944153\n",
      "Iteration 54850 Training loss 0.0025184222031384706 Validation loss 0.04549030587077141 Accuracy 0.8896250128746033\n",
      "Iteration 54860 Training loss 2.6616105969878845e-05 Validation loss 0.045498210936784744 Accuracy 0.8895000219345093\n",
      "Iteration 54870 Training loss 0.00251630786806345 Validation loss 0.045511920005083084 Accuracy 0.8891250491142273\n",
      "Iteration 54880 Training loss 0.0012733697658404708 Validation loss 0.045512475073337555 Accuracy 0.8893750309944153\n",
      "Iteration 54890 Training loss 0.0012662545777857304 Validation loss 0.045516952872276306 Accuracy 0.8890000581741333\n",
      "Iteration 54900 Training loss 0.0012734944466501474 Validation loss 0.045495904982089996 Accuracy 0.8891250491142273\n",
      "Iteration 54910 Training loss 2.0655514163081534e-05 Validation loss 0.04552777484059334 Accuracy 0.8890000581741333\n",
      "Iteration 54920 Training loss 3.0994877306511626e-05 Validation loss 0.04552830755710602 Accuracy 0.8890000581741333\n",
      "Iteration 54930 Training loss 1.7646361811785027e-05 Validation loss 0.045506902039051056 Accuracy 0.8893750309944153\n",
      "Iteration 54940 Training loss 0.002520017558708787 Validation loss 0.04548477381467819 Accuracy 0.8892500400543213\n",
      "Iteration 54950 Training loss 0.003768562339246273 Validation loss 0.04548749700188637 Accuracy 0.8895000219345093\n",
      "Iteration 54960 Training loss 2.232707993243821e-05 Validation loss 0.04547826945781708 Accuracy 0.8893750309944153\n",
      "Iteration 54970 Training loss 2.1345582354115322e-05 Validation loss 0.045508723706007004 Accuracy 0.8893750309944153\n",
      "Iteration 54980 Training loss 0.001270459615625441 Validation loss 0.04550519213080406 Accuracy 0.8890000581741333\n",
      "Iteration 54990 Training loss 0.005016328766942024 Validation loss 0.04550629109144211 Accuracy 0.8893750309944153\n",
      "Iteration 55000 Training loss 0.0012620665365830064 Validation loss 0.045495785772800446 Accuracy 0.8892500400543213\n",
      "Iteration 55010 Training loss 0.00126975669991225 Validation loss 0.04549695551395416 Accuracy 0.8892500400543213\n",
      "Iteration 55020 Training loss 1.8745498891803436e-05 Validation loss 0.045478951185941696 Accuracy 0.8893750309944153\n",
      "Iteration 55030 Training loss 0.0012705645058304071 Validation loss 0.04553230479359627 Accuracy 0.8888750672340393\n",
      "Iteration 55040 Training loss 1.4581838513549883e-05 Validation loss 0.04547284543514252 Accuracy 0.8890000581741333\n",
      "Iteration 55050 Training loss 0.0012732542818412185 Validation loss 0.045526035130023956 Accuracy 0.8887500166893005\n",
      "Iteration 55060 Training loss 0.003774152370169759 Validation loss 0.045508336275815964 Accuracy 0.8888750672340393\n",
      "Iteration 55070 Training loss 1.9823284674203023e-05 Validation loss 0.04545426368713379 Accuracy 0.8891250491142273\n",
      "Iteration 55080 Training loss 1.4429727343667764e-05 Validation loss 0.04545896500349045 Accuracy 0.8891250491142273\n",
      "Iteration 55090 Training loss 0.002520768204703927 Validation loss 0.0454714410007 Accuracy 0.8886250257492065\n",
      "Iteration 55100 Training loss 3.77372998627834e-05 Validation loss 0.04548868164420128 Accuracy 0.8887500166893005\n",
      "Iteration 55110 Training loss 0.0012720361119136214 Validation loss 0.04548783227801323 Accuracy 0.8890000581741333\n",
      "Iteration 55120 Training loss 0.0012651938013732433 Validation loss 0.04547082632780075 Accuracy 0.8892500400543213\n",
      "Iteration 55130 Training loss 0.0012702916283160448 Validation loss 0.04548792168498039 Accuracy 0.8887500166893005\n",
      "Iteration 55140 Training loss 1.962399073818233e-05 Validation loss 0.04548054561018944 Accuracy 0.8887500166893005\n",
      "Iteration 55150 Training loss 2.1484782337211072e-05 Validation loss 0.045490801334381104 Accuracy 0.8892500400543213\n",
      "Iteration 55160 Training loss 0.0012662127846851945 Validation loss 0.0454777330160141 Accuracy 0.8890000581741333\n",
      "Iteration 55170 Training loss 2.145388316421304e-05 Validation loss 0.045438867062330246 Accuracy 0.8893750309944153\n",
      "Iteration 55180 Training loss 1.684658127487637e-05 Validation loss 0.045451462268829346 Accuracy 0.8896250128746033\n",
      "Iteration 55190 Training loss 1.3705486708204262e-05 Validation loss 0.04547838866710663 Accuracy 0.889750063419342\n",
      "Iteration 55200 Training loss 0.0012766601284965873 Validation loss 0.04546928033232689 Accuracy 0.8893750309944153\n",
      "Iteration 55210 Training loss 0.0012731169117614627 Validation loss 0.045461565256118774 Accuracy 0.8893750309944153\n",
      "Iteration 55220 Training loss 0.0025205265264958143 Validation loss 0.04541183263063431 Accuracy 0.8895000219345093\n",
      "Iteration 55230 Training loss 0.0012710674200206995 Validation loss 0.04543323814868927 Accuracy 0.889750063419342\n",
      "Iteration 55240 Training loss 2.1811774786328897e-05 Validation loss 0.045461256057024 Accuracy 0.8888750672340393\n",
      "Iteration 55250 Training loss 0.00252223527058959 Validation loss 0.04546016454696655 Accuracy 0.8892500400543213\n",
      "Iteration 55260 Training loss 0.0012682991800829768 Validation loss 0.04544619843363762 Accuracy 0.8890000581741333\n",
      "Iteration 55270 Training loss 2.0364126612548716e-05 Validation loss 0.04546093940734863 Accuracy 0.8891250491142273\n",
      "Iteration 55280 Training loss 0.0012731229653581977 Validation loss 0.04546757787466049 Accuracy 0.8891250491142273\n",
      "Iteration 55290 Training loss 0.0037649895530194044 Validation loss 0.04545355588197708 Accuracy 0.8892500400543213\n",
      "Iteration 55300 Training loss 0.0025247158482670784 Validation loss 0.04548632353544235 Accuracy 0.8891250491142273\n",
      "Iteration 55310 Training loss 2.394856528553646e-05 Validation loss 0.045491307973861694 Accuracy 0.8892500400543213\n",
      "Iteration 55320 Training loss 0.0012764337006956339 Validation loss 0.045479439198970795 Accuracy 0.8892500400543213\n",
      "Iteration 55330 Training loss 0.0012717058416455984 Validation loss 0.04548656940460205 Accuracy 0.8895000219345093\n",
      "Iteration 55340 Training loss 0.0012695048935711384 Validation loss 0.04544482380151749 Accuracy 0.8895000219345093\n",
      "Iteration 55350 Training loss 0.002513952786102891 Validation loss 0.0455005057156086 Accuracy 0.889875054359436\n",
      "Iteration 55360 Training loss 0.0037798085249960423 Validation loss 0.04552500322461128 Accuracy 0.8895000219345093\n",
      "Iteration 55370 Training loss 0.002522197086364031 Validation loss 0.045517634600400925 Accuracy 0.8887500166893005\n",
      "Iteration 55380 Training loss 2.3462305762222968e-05 Validation loss 0.04550563171505928 Accuracy 0.8890000581741333\n",
      "Iteration 55390 Training loss 0.0012682542437687516 Validation loss 0.04549416899681091 Accuracy 0.8893750309944153\n",
      "Iteration 55400 Training loss 0.0012654304737225175 Validation loss 0.04551764205098152 Accuracy 0.8891250491142273\n",
      "Iteration 55410 Training loss 0.002520776353776455 Validation loss 0.04551023244857788 Accuracy 0.8895000219345093\n",
      "Iteration 55420 Training loss 0.002521251328289509 Validation loss 0.04550537094473839 Accuracy 0.8895000219345093\n",
      "Iteration 55430 Training loss 1.4694041965412907e-05 Validation loss 0.04549359902739525 Accuracy 0.8891250491142273\n",
      "Iteration 55440 Training loss 0.0012698780046775937 Validation loss 0.045484255999326706 Accuracy 0.8895000219345093\n",
      "Iteration 55450 Training loss 0.0025180785451084375 Validation loss 0.0455157645046711 Accuracy 0.8896250128746033\n",
      "Iteration 55460 Training loss 0.0025157821364700794 Validation loss 0.04549022018909454 Accuracy 0.8892500400543213\n",
      "Iteration 55470 Training loss 0.0037702363915741444 Validation loss 0.045501958578825 Accuracy 0.8891250491142273\n",
      "Iteration 55480 Training loss 0.0025116666220128536 Validation loss 0.045505404472351074 Accuracy 0.8895000219345093\n",
      "Iteration 55490 Training loss 1.548456566524692e-05 Validation loss 0.045525919646024704 Accuracy 0.8888750672340393\n",
      "Iteration 55500 Training loss 2.47577245318098e-05 Validation loss 0.045539796352386475 Accuracy 0.8892500400543213\n",
      "Iteration 55510 Training loss 0.003769572824239731 Validation loss 0.04549180716276169 Accuracy 0.889750063419342\n",
      "Iteration 55520 Training loss 0.0037719151005148888 Validation loss 0.04548381268978119 Accuracy 0.8896250128746033\n",
      "Iteration 55530 Training loss 0.0012677386403083801 Validation loss 0.04550381377339363 Accuracy 0.8895000219345093\n",
      "Iteration 55540 Training loss 0.0025244976859539747 Validation loss 0.045517731457948685 Accuracy 0.8895000219345093\n",
      "Iteration 55550 Training loss 0.003770097391679883 Validation loss 0.04551297053694725 Accuracy 0.8892500400543213\n",
      "Iteration 55560 Training loss 0.0037686070427298546 Validation loss 0.04554294794797897 Accuracy 0.8892500400543213\n",
      "Iteration 55570 Training loss 0.002522895811125636 Validation loss 0.04551708325743675 Accuracy 0.8892500400543213\n",
      "Iteration 55580 Training loss 0.0012718039797618985 Validation loss 0.04556841030716896 Accuracy 0.8890000581741333\n",
      "Iteration 55590 Training loss 0.0012684519169852138 Validation loss 0.04551570490002632 Accuracy 0.8892500400543213\n",
      "Iteration 55600 Training loss 0.0025152736343443394 Validation loss 0.04549926891922951 Accuracy 0.8891250491142273\n",
      "Iteration 55610 Training loss 2.115318420692347e-05 Validation loss 0.045481082051992416 Accuracy 0.889750063419342\n",
      "Iteration 55620 Training loss 1.8216456737718545e-05 Validation loss 0.0454903244972229 Accuracy 0.889750063419342\n",
      "Iteration 55630 Training loss 0.0012651612050831318 Validation loss 0.0455077700316906 Accuracy 0.8895000219345093\n",
      "Iteration 55640 Training loss 1.773633084667381e-05 Validation loss 0.045526329427957535 Accuracy 0.8896250128746033\n",
      "Iteration 55650 Training loss 0.002529357559978962 Validation loss 0.04549827054142952 Accuracy 0.8891250491142273\n",
      "Iteration 55660 Training loss 1.3654878785018809e-05 Validation loss 0.04550794139504433 Accuracy 0.8891250491142273\n",
      "Iteration 55670 Training loss 2.531902828195598e-05 Validation loss 0.045542191714048386 Accuracy 0.8887500166893005\n",
      "Iteration 55680 Training loss 0.0012670623837038875 Validation loss 0.04553542286157608 Accuracy 0.8891250491142273\n",
      "Iteration 55690 Training loss 0.0012669783318415284 Validation loss 0.04553601145744324 Accuracy 0.8890000581741333\n",
      "Iteration 55700 Training loss 0.0012687209527939558 Validation loss 0.04552913084626198 Accuracy 0.8888750672340393\n",
      "Iteration 55710 Training loss 0.001269698259420693 Validation loss 0.04556705057621002 Accuracy 0.8891250491142273\n",
      "Iteration 55720 Training loss 0.0012702790554612875 Validation loss 0.045531969517469406 Accuracy 0.8890000581741333\n",
      "Iteration 55730 Training loss 0.0025274071376770735 Validation loss 0.04553449526429176 Accuracy 0.8890000581741333\n",
      "Iteration 55740 Training loss 2.1667628971044905e-05 Validation loss 0.045531678944826126 Accuracy 0.8888750672340393\n",
      "Iteration 55750 Training loss 1.9747763872146606e-05 Validation loss 0.0455169752240181 Accuracy 0.8891250491142273\n",
      "Iteration 55760 Training loss 0.001272261724807322 Validation loss 0.045505862683057785 Accuracy 0.8891250491142273\n",
      "Iteration 55770 Training loss 0.0037702720146626234 Validation loss 0.045533888041973114 Accuracy 0.8890000581741333\n",
      "Iteration 55780 Training loss 0.001270448206923902 Validation loss 0.045501261949539185 Accuracy 0.8891250491142273\n",
      "Iteration 55790 Training loss 0.002520028268918395 Validation loss 0.04547974467277527 Accuracy 0.8893750309944153\n",
      "Iteration 55800 Training loss 0.0025200876407325268 Validation loss 0.04548303037881851 Accuracy 0.8893750309944153\n",
      "Iteration 55810 Training loss 0.0012725860578939319 Validation loss 0.045506518334150314 Accuracy 0.8887500166893005\n",
      "Iteration 55820 Training loss 0.002520723035559058 Validation loss 0.04555634409189224 Accuracy 0.8883750438690186\n",
      "Iteration 55830 Training loss 0.0012647270923480392 Validation loss 0.04548577219247818 Accuracy 0.8892500400543213\n",
      "Iteration 55840 Training loss 1.6865198631421663e-05 Validation loss 0.045501355081796646 Accuracy 0.8891250491142273\n",
      "Iteration 55850 Training loss 1.8209273548563942e-05 Validation loss 0.045488059520721436 Accuracy 0.8892500400543213\n",
      "Iteration 55860 Training loss 2.0892710381303914e-05 Validation loss 0.045487940311431885 Accuracy 0.8893750309944153\n",
      "Iteration 55870 Training loss 0.0025211258325725794 Validation loss 0.04550487920641899 Accuracy 0.8888750672340393\n",
      "Iteration 55880 Training loss 0.002520594745874405 Validation loss 0.045560792088508606 Accuracy 0.8883750438690186\n",
      "Iteration 55890 Training loss 0.0012720617232844234 Validation loss 0.04550611227750778 Accuracy 0.8886250257492065\n",
      "Iteration 55900 Training loss 0.002521499991416931 Validation loss 0.04547532647848129 Accuracy 0.8891250491142273\n",
      "Iteration 55910 Training loss 0.002519224537536502 Validation loss 0.04547249898314476 Accuracy 0.8891250491142273\n",
      "Iteration 55920 Training loss 2.302106804563664e-05 Validation loss 0.045490704476833344 Accuracy 0.8892500400543213\n",
      "Iteration 55930 Training loss 2.106093779730145e-05 Validation loss 0.045523881912231445 Accuracy 0.8890000581741333\n",
      "Iteration 55940 Training loss 0.0012778380187228322 Validation loss 0.04551684856414795 Accuracy 0.8891250491142273\n",
      "Iteration 55950 Training loss 0.0037712561897933483 Validation loss 0.045501526445150375 Accuracy 0.8886250257492065\n",
      "Iteration 55960 Training loss 0.002519557485356927 Validation loss 0.04552523046731949 Accuracy 0.8888750672340393\n",
      "Iteration 55970 Training loss 0.0012725291308015585 Validation loss 0.04551256075501442 Accuracy 0.8890000581741333\n",
      "Iteration 55980 Training loss 1.4586538782168645e-05 Validation loss 0.045521654188632965 Accuracy 0.8890000581741333\n",
      "Iteration 55990 Training loss 0.005022474564611912 Validation loss 0.04550347477197647 Accuracy 0.8887500166893005\n",
      "Iteration 56000 Training loss 0.0025217276997864246 Validation loss 0.04551735147833824 Accuracy 0.8888750672340393\n",
      "Iteration 56010 Training loss 1.62623509822879e-05 Validation loss 0.04557252675294876 Accuracy 0.8890000581741333\n",
      "Iteration 56020 Training loss 0.0012725678971037269 Validation loss 0.04551616683602333 Accuracy 0.8888750672340393\n",
      "Iteration 56030 Training loss 0.002525190357118845 Validation loss 0.045537468045949936 Accuracy 0.8892500400543213\n",
      "Iteration 56040 Training loss 0.0025213416665792465 Validation loss 0.0454939566552639 Accuracy 0.8895000219345093\n",
      "Iteration 56050 Training loss 0.0025183919351547956 Validation loss 0.045503854751586914 Accuracy 0.8892500400543213\n",
      "Iteration 56060 Training loss 2.148867497453466e-05 Validation loss 0.045489124953746796 Accuracy 0.8893750309944153\n",
      "Iteration 56070 Training loss 2.404213410045486e-05 Validation loss 0.045498598366975784 Accuracy 0.8892500400543213\n",
      "Iteration 56080 Training loss 1.938681089086458e-05 Validation loss 0.045487139374017715 Accuracy 0.8888750672340393\n",
      "Iteration 56090 Training loss 0.001272396999411285 Validation loss 0.04550139978528023 Accuracy 0.8891250491142273\n",
      "Iteration 56100 Training loss 0.0012698721839115024 Validation loss 0.045517534017562866 Accuracy 0.8891250491142273\n",
      "Iteration 56110 Training loss 0.0025223984848707914 Validation loss 0.045483749359846115 Accuracy 0.8890000581741333\n",
      "Iteration 56120 Training loss 0.002516607055440545 Validation loss 0.04548215866088867 Accuracy 0.8890000581741333\n",
      "Iteration 56130 Training loss 2.1387273591244593e-05 Validation loss 0.04546361416578293 Accuracy 0.8890000581741333\n",
      "Iteration 56140 Training loss 0.001263484125956893 Validation loss 0.04544501379132271 Accuracy 0.8892500400543213\n",
      "Iteration 56150 Training loss 2.010066782531794e-05 Validation loss 0.045493144541978836 Accuracy 0.8890000581741333\n",
      "Iteration 56160 Training loss 0.0025205572601407766 Validation loss 0.04548997804522514 Accuracy 0.8887500166893005\n",
      "Iteration 56170 Training loss 0.0012726126005873084 Validation loss 0.045533325523138046 Accuracy 0.8886250257492065\n",
      "Iteration 56180 Training loss 1.6651269106660038e-05 Validation loss 0.04550628364086151 Accuracy 0.8887500166893005\n",
      "Iteration 56190 Training loss 0.0012638107873499393 Validation loss 0.045497167855501175 Accuracy 0.8888750672340393\n",
      "Iteration 56200 Training loss 0.002533421153202653 Validation loss 0.045527826994657516 Accuracy 0.8888750672340393\n",
      "Iteration 56210 Training loss 2.0629386199289e-05 Validation loss 0.04550750181078911 Accuracy 0.8892500400543213\n",
      "Iteration 56220 Training loss 0.0012686866102740169 Validation loss 0.045527245849370956 Accuracy 0.8891250491142273\n",
      "Iteration 56230 Training loss 0.0012656814651563764 Validation loss 0.045534972101449966 Accuracy 0.8893750309944153\n",
      "Iteration 56240 Training loss 0.001269951113499701 Validation loss 0.045507125556468964 Accuracy 0.8895000219345093\n",
      "Iteration 56250 Training loss 0.0037714140489697456 Validation loss 0.04550905525684357 Accuracy 0.889750063419342\n",
      "Iteration 56260 Training loss 0.0037726550363004208 Validation loss 0.04549471661448479 Accuracy 0.8892500400543213\n",
      "Iteration 56270 Training loss 1.6669782780809328e-05 Validation loss 0.045488812029361725 Accuracy 0.8893750309944153\n",
      "Iteration 56280 Training loss 0.001272205263376236 Validation loss 0.045518044382333755 Accuracy 0.8896250128746033\n",
      "Iteration 56290 Training loss 0.0025176655035465956 Validation loss 0.045536577701568604 Accuracy 0.8893750309944153\n",
      "Iteration 56300 Training loss 0.0025226420257240534 Validation loss 0.04549657553434372 Accuracy 0.8896250128746033\n",
      "Iteration 56310 Training loss 0.0037662270478904247 Validation loss 0.04550683870911598 Accuracy 0.8893750309944153\n",
      "Iteration 56320 Training loss 2.0012541426694952e-05 Validation loss 0.045468613505363464 Accuracy 0.8893750309944153\n",
      "Iteration 56330 Training loss 0.0012682972010225058 Validation loss 0.0454811230301857 Accuracy 0.8895000219345093\n",
      "Iteration 56340 Training loss 0.0025164454709738493 Validation loss 0.045487407594919205 Accuracy 0.8892500400543213\n",
      "Iteration 56350 Training loss 0.0012685252586379647 Validation loss 0.045492373406887054 Accuracy 0.8890000581741333\n",
      "Iteration 56360 Training loss 2.4512959498679265e-05 Validation loss 0.0454595722258091 Accuracy 0.889875054359436\n",
      "Iteration 56370 Training loss 0.0012802276760339737 Validation loss 0.04547858610749245 Accuracy 0.8895000219345093\n",
      "Iteration 56380 Training loss 0.0025208299048244953 Validation loss 0.04547618329524994 Accuracy 0.8892500400543213\n",
      "Iteration 56390 Training loss 0.0012706599663943052 Validation loss 0.04551587626338005 Accuracy 0.8893750309944153\n",
      "Iteration 56400 Training loss 2.2166168491821736e-05 Validation loss 0.045508481562137604 Accuracy 0.8891250491142273\n",
      "Iteration 56410 Training loss 0.0012686020927503705 Validation loss 0.045524712651968 Accuracy 0.8893750309944153\n",
      "Iteration 56420 Training loss 0.002518831519410014 Validation loss 0.045529864728450775 Accuracy 0.8890000581741333\n",
      "Iteration 56430 Training loss 0.0012720689410343766 Validation loss 0.04553359001874924 Accuracy 0.8888750672340393\n",
      "Iteration 56440 Training loss 0.0012705647386610508 Validation loss 0.045555319637060165 Accuracy 0.8893750309944153\n",
      "Iteration 56450 Training loss 0.0012710181763395667 Validation loss 0.04557686671614647 Accuracy 0.8888750672340393\n",
      "Iteration 56460 Training loss 0.0012653048615902662 Validation loss 0.045483753085136414 Accuracy 0.8888750672340393\n",
      "Iteration 56470 Training loss 0.0012673657620325685 Validation loss 0.04548489674925804 Accuracy 0.8890000581741333\n",
      "Iteration 56480 Training loss 1.3899079021939542e-05 Validation loss 0.04549300670623779 Accuracy 0.8888750672340393\n",
      "Iteration 56490 Training loss 0.002519437577575445 Validation loss 0.04548884928226471 Accuracy 0.8891250491142273\n",
      "Iteration 56500 Training loss 2.2816415366833098e-05 Validation loss 0.045473840087652206 Accuracy 0.8891250491142273\n",
      "Iteration 56510 Training loss 0.003772477153688669 Validation loss 0.04551978409290314 Accuracy 0.8886250257492065\n",
      "Iteration 56520 Training loss 1.7274478523177095e-05 Validation loss 0.045542992651462555 Accuracy 0.8882500529289246\n",
      "Iteration 56530 Training loss 0.002515912288799882 Validation loss 0.04553307592868805 Accuracy 0.8887500166893005\n",
      "Iteration 56540 Training loss 1.6654797946102917e-05 Validation loss 0.045497965067625046 Accuracy 0.8888750672340393\n",
      "Iteration 56550 Training loss 0.005022244527935982 Validation loss 0.045509882271289825 Accuracy 0.8895000219345093\n",
      "Iteration 56560 Training loss 0.0037701171822845936 Validation loss 0.045525431632995605 Accuracy 0.8887500166893005\n",
      "Iteration 56570 Training loss 0.0012662503868341446 Validation loss 0.045541759580373764 Accuracy 0.8887500166893005\n",
      "Iteration 56580 Training loss 0.002522561466321349 Validation loss 0.045476723462343216 Accuracy 0.8893750309944153\n",
      "Iteration 56590 Training loss 0.0025240087416023016 Validation loss 0.0454954169690609 Accuracy 0.8891250491142273\n",
      "Iteration 56600 Training loss 0.001277501811273396 Validation loss 0.0454726442694664 Accuracy 0.8888750672340393\n",
      "Iteration 56610 Training loss 0.003765261732041836 Validation loss 0.04550550878047943 Accuracy 0.8891250491142273\n",
      "Iteration 56620 Training loss 0.0012667939299717546 Validation loss 0.04547829553484917 Accuracy 0.8890000581741333\n",
      "Iteration 56630 Training loss 0.0012728318106383085 Validation loss 0.045515336096286774 Accuracy 0.8891250491142273\n",
      "Iteration 56640 Training loss 2.3157617761171423e-05 Validation loss 0.04549800977110863 Accuracy 0.8892500400543213\n",
      "Iteration 56650 Training loss 0.0012684285175055265 Validation loss 0.04551173001527786 Accuracy 0.8892500400543213\n",
      "Iteration 56660 Training loss 2.0129200493101962e-05 Validation loss 0.045500148087739944 Accuracy 0.8891250491142273\n",
      "Iteration 56670 Training loss 0.003761483123525977 Validation loss 0.0454678051173687 Accuracy 0.8890000581741333\n",
      "Iteration 56680 Training loss 0.0012694225879386067 Validation loss 0.045502740889787674 Accuracy 0.8891250491142273\n",
      "Iteration 56690 Training loss 1.939831599884201e-05 Validation loss 0.04548775777220726 Accuracy 0.8890000581741333\n",
      "Iteration 56700 Training loss 0.0012677686754614115 Validation loss 0.04551301151514053 Accuracy 0.889750063419342\n",
      "Iteration 56710 Training loss 0.0025224052369594574 Validation loss 0.04547271877527237 Accuracy 0.8891250491142273\n",
      "Iteration 56720 Training loss 0.0012651841389015317 Validation loss 0.045496970415115356 Accuracy 0.8891250491142273\n",
      "Iteration 56730 Training loss 0.005019667092710733 Validation loss 0.04550289362668991 Accuracy 0.8892500400543213\n",
      "Iteration 56740 Training loss 2.0268746084184386e-05 Validation loss 0.04549010843038559 Accuracy 0.8893750309944153\n",
      "Iteration 56750 Training loss 0.0012737036449834704 Validation loss 0.04551874101161957 Accuracy 0.8896250128746033\n",
      "Iteration 56760 Training loss 0.0037754641380161047 Validation loss 0.04550383985042572 Accuracy 0.8888750672340393\n",
      "Iteration 56770 Training loss 1.2319557754381094e-05 Validation loss 0.04549260810017586 Accuracy 0.8887500166893005\n",
      "Iteration 56780 Training loss 1.8503178580431268e-05 Validation loss 0.04553312808275223 Accuracy 0.8887500166893005\n",
      "Iteration 56790 Training loss 0.003767237765714526 Validation loss 0.04550660401582718 Accuracy 0.8887500166893005\n",
      "Iteration 56800 Training loss 0.0012625762028619647 Validation loss 0.04551776498556137 Accuracy 0.8890000581741333\n",
      "Iteration 56810 Training loss 0.0025174568872898817 Validation loss 0.04548031836748123 Accuracy 0.8890000581741333\n",
      "Iteration 56820 Training loss 0.0012721132952719927 Validation loss 0.04549003019928932 Accuracy 0.8887500166893005\n",
      "Iteration 56830 Training loss 0.0012660188367590308 Validation loss 0.04550834000110626 Accuracy 0.8888750672340393\n",
      "Iteration 56840 Training loss 0.0012684838147833943 Validation loss 0.045504916459321976 Accuracy 0.8888750672340393\n",
      "Iteration 56850 Training loss 0.001272830879315734 Validation loss 0.0454910509288311 Accuracy 0.8891250491142273\n",
      "Iteration 56860 Training loss 1.595308094692882e-05 Validation loss 0.045527223497629166 Accuracy 0.8888750672340393\n",
      "Iteration 56870 Training loss 2.6651978259906173e-05 Validation loss 0.045510340481996536 Accuracy 0.8888750672340393\n",
      "Iteration 56880 Training loss 1.994440935959574e-05 Validation loss 0.04551003873348236 Accuracy 0.8887500166893005\n",
      "Iteration 56890 Training loss 0.002524680458009243 Validation loss 0.04549943655729294 Accuracy 0.8891250491142273\n",
      "Iteration 56900 Training loss 0.0012687500566244125 Validation loss 0.04550845921039581 Accuracy 0.8888750672340393\n",
      "Iteration 56910 Training loss 0.0012710444862022996 Validation loss 0.04553348943591118 Accuracy 0.8886250257492065\n",
      "Iteration 56920 Training loss 0.0025173984467983246 Validation loss 0.04551877826452255 Accuracy 0.8888750672340393\n",
      "Iteration 56930 Training loss 1.9364217223483138e-05 Validation loss 0.04553927853703499 Accuracy 0.8890000581741333\n",
      "Iteration 56940 Training loss 0.003765957662835717 Validation loss 0.04557915776968002 Accuracy 0.8886250257492065\n",
      "Iteration 56950 Training loss 1.929870995809324e-05 Validation loss 0.04550773277878761 Accuracy 0.8892500400543213\n",
      "Iteration 56960 Training loss 0.0012667987029999495 Validation loss 0.04551481828093529 Accuracy 0.8893750309944153\n",
      "Iteration 56970 Training loss 0.0025162699166685343 Validation loss 0.04549754038453102 Accuracy 0.8893750309944153\n",
      "Iteration 56980 Training loss 0.006269723642617464 Validation loss 0.045495692640542984 Accuracy 0.8890000581741333\n",
      "Iteration 56990 Training loss 1.5428535334649496e-05 Validation loss 0.04550782963633537 Accuracy 0.8891250491142273\n",
      "Iteration 57000 Training loss 1.3014037904213183e-05 Validation loss 0.045545175671577454 Accuracy 0.8891250491142273\n",
      "Iteration 57010 Training loss 2.5299410481238738e-05 Validation loss 0.04552935063838959 Accuracy 0.8895000219345093\n",
      "Iteration 57020 Training loss 1.2122494808863848e-05 Validation loss 0.04554043337702751 Accuracy 0.8891250491142273\n",
      "Iteration 57030 Training loss 0.001272861147299409 Validation loss 0.04556531831622124 Accuracy 0.8888750672340393\n",
      "Iteration 57040 Training loss 2.036808291450143e-05 Validation loss 0.04554366692900658 Accuracy 0.8893750309944153\n",
      "Iteration 57050 Training loss 0.002515497151762247 Validation loss 0.04553984850645065 Accuracy 0.8893750309944153\n",
      "Iteration 57060 Training loss 0.0037689434830099344 Validation loss 0.045544907450675964 Accuracy 0.8892500400543213\n",
      "Iteration 57070 Training loss 0.0025155763141810894 Validation loss 0.045536819845438004 Accuracy 0.8893750309944153\n",
      "Iteration 57080 Training loss 0.0012639127671718597 Validation loss 0.04549997299909592 Accuracy 0.8893750309944153\n",
      "Iteration 57090 Training loss 0.0012692830059677362 Validation loss 0.045514632016420364 Accuracy 0.8893750309944153\n",
      "Iteration 57100 Training loss 0.001273551257327199 Validation loss 0.0455029159784317 Accuracy 0.8892500400543213\n",
      "Iteration 57110 Training loss 1.5927316781016998e-05 Validation loss 0.04553473740816116 Accuracy 0.8891250491142273\n",
      "Iteration 57120 Training loss 0.001269975327886641 Validation loss 0.045538753271102905 Accuracy 0.8890000581741333\n",
      "Iteration 57130 Training loss 1.3160827620595228e-05 Validation loss 0.045513879507780075 Accuracy 0.8886250257492065\n",
      "Iteration 57140 Training loss 0.0012691605370491743 Validation loss 0.04554390162229538 Accuracy 0.8890000581741333\n",
      "Iteration 57150 Training loss 2.5927693059202284e-05 Validation loss 0.045534081757068634 Accuracy 0.8890000581741333\n",
      "Iteration 57160 Training loss 1.8557611838332377e-05 Validation loss 0.04555126279592514 Accuracy 0.8885000348091125\n",
      "Iteration 57170 Training loss 2.383498940616846e-05 Validation loss 0.045502062886953354 Accuracy 0.8888750672340393\n",
      "Iteration 57180 Training loss 0.003764829831197858 Validation loss 0.045501574873924255 Accuracy 0.8891250491142273\n",
      "Iteration 57190 Training loss 2.654131458257325e-05 Validation loss 0.04550723731517792 Accuracy 0.8892500400543213\n",
      "Iteration 57200 Training loss 0.005018480122089386 Validation loss 0.045491982251405716 Accuracy 0.8891250491142273\n",
      "Iteration 57210 Training loss 1.8379896573605947e-05 Validation loss 0.045503877103328705 Accuracy 0.8891250491142273\n",
      "Iteration 57220 Training loss 1.6393418263760395e-05 Validation loss 0.04553115367889404 Accuracy 0.8890000581741333\n",
      "Iteration 57230 Training loss 1.9488763427943923e-05 Validation loss 0.04550881311297417 Accuracy 0.8890000581741333\n",
      "Iteration 57240 Training loss 2.008377487072721e-05 Validation loss 0.04548565298318863 Accuracy 0.8888750672340393\n",
      "Iteration 57250 Training loss 0.0037714301142841578 Validation loss 0.04551135376095772 Accuracy 0.8892500400543213\n",
      "Iteration 57260 Training loss 0.0025244723074138165 Validation loss 0.04549572616815567 Accuracy 0.8886250257492065\n",
      "Iteration 57270 Training loss 0.0037640829104930162 Validation loss 0.0455324612557888 Accuracy 0.8888750672340393\n",
      "Iteration 57280 Training loss 0.001265012426301837 Validation loss 0.045514486730098724 Accuracy 0.8890000581741333\n",
      "Iteration 57290 Training loss 0.003765109460800886 Validation loss 0.04555020481348038 Accuracy 0.8890000581741333\n",
      "Iteration 57300 Training loss 0.0025211209431290627 Validation loss 0.045538127422332764 Accuracy 0.8888750672340393\n",
      "Iteration 57310 Training loss 0.001273596310056746 Validation loss 0.045531488955020905 Accuracy 0.8888750672340393\n",
      "Iteration 57320 Training loss 0.0037745183799415827 Validation loss 0.04554428532719612 Accuracy 0.8891250491142273\n",
      "Iteration 57330 Training loss 0.0025161337107419968 Validation loss 0.045501817017793655 Accuracy 0.8890000581741333\n",
      "Iteration 57340 Training loss 0.001266583800315857 Validation loss 0.04549431800842285 Accuracy 0.8891250491142273\n",
      "Iteration 57350 Training loss 0.002517628250643611 Validation loss 0.04547476768493652 Accuracy 0.8890000581741333\n",
      "Iteration 57360 Training loss 0.0012651269789785147 Validation loss 0.04552144184708595 Accuracy 0.8888750672340393\n",
      "Iteration 57370 Training loss 0.001263134996406734 Validation loss 0.04548228904604912 Accuracy 0.8888750672340393\n",
      "Iteration 57380 Training loss 0.0012690317817032337 Validation loss 0.045523807406425476 Accuracy 0.8891250491142273\n",
      "Iteration 57390 Training loss 0.0037671688478440046 Validation loss 0.04548816755414009 Accuracy 0.8888750672340393\n",
      "Iteration 57400 Training loss 0.0012692559976130724 Validation loss 0.04550284892320633 Accuracy 0.8890000581741333\n",
      "Iteration 57410 Training loss 0.0025155835319310427 Validation loss 0.04549120366573334 Accuracy 0.8891250491142273\n",
      "Iteration 57420 Training loss 0.003772864816710353 Validation loss 0.045468878000974655 Accuracy 0.8891250491142273\n",
      "Iteration 57430 Training loss 0.002526679541915655 Validation loss 0.04551266133785248 Accuracy 0.8887500166893005\n",
      "Iteration 57440 Training loss 0.0012669448042288423 Validation loss 0.045530207455158234 Accuracy 0.8888750672340393\n",
      "Iteration 57450 Training loss 0.0012673565652221441 Validation loss 0.04555342718958855 Accuracy 0.8887500166893005\n",
      "Iteration 57460 Training loss 0.0025204813573509455 Validation loss 0.04554789140820503 Accuracy 0.8891250491142273\n",
      "Iteration 57470 Training loss 2.20109195652185e-05 Validation loss 0.04552365094423294 Accuracy 0.8892500400543213\n",
      "Iteration 57480 Training loss 0.002517140470445156 Validation loss 0.045516930520534515 Accuracy 0.8890000581741333\n",
      "Iteration 57490 Training loss 0.0025163348764181137 Validation loss 0.045543234795331955 Accuracy 0.8890000581741333\n",
      "Iteration 57500 Training loss 0.0025223903357982635 Validation loss 0.045525431632995605 Accuracy 0.8887500166893005\n",
      "Iteration 57510 Training loss 0.0025159327778965235 Validation loss 0.04550986364483833 Accuracy 0.8888750672340393\n",
      "Iteration 57520 Training loss 0.0025122601073235273 Validation loss 0.04554606229066849 Accuracy 0.8891250491142273\n",
      "Iteration 57530 Training loss 1.720066757116001e-05 Validation loss 0.04552851989865303 Accuracy 0.8893750309944153\n",
      "Iteration 57540 Training loss 0.003768668742850423 Validation loss 0.04553578048944473 Accuracy 0.8892500400543213\n",
      "Iteration 57550 Training loss 1.9642246115836315e-05 Validation loss 0.04553757607936859 Accuracy 0.8888750672340393\n",
      "Iteration 57560 Training loss 0.0037671232130378485 Validation loss 0.045512210577726364 Accuracy 0.8887500166893005\n",
      "Iteration 57570 Training loss 1.922694173117634e-05 Validation loss 0.045523855835199356 Accuracy 0.8887500166893005\n",
      "Iteration 57580 Training loss 0.0012690159492194653 Validation loss 0.045535530894994736 Accuracy 0.8893750309944153\n",
      "Iteration 57590 Training loss 0.0037648454308509827 Validation loss 0.04553147405385971 Accuracy 0.8891250491142273\n",
      "Iteration 57600 Training loss 0.0012638543266803026 Validation loss 0.04552663490176201 Accuracy 0.8892500400543213\n",
      "Iteration 57610 Training loss 0.001269153319299221 Validation loss 0.0455489456653595 Accuracy 0.8895000219345093\n",
      "Iteration 57620 Training loss 0.003771415213122964 Validation loss 0.045541562139987946 Accuracy 0.8890000581741333\n",
      "Iteration 57630 Training loss 1.6858513845363632e-05 Validation loss 0.04557393491268158 Accuracy 0.8892500400543213\n",
      "Iteration 57640 Training loss 2.2244315914576873e-05 Validation loss 0.045575086027383804 Accuracy 0.8891250491142273\n",
      "Iteration 57650 Training loss 0.001276884344406426 Validation loss 0.04557677358388901 Accuracy 0.8890000581741333\n",
      "Iteration 57660 Training loss 2.831557867466472e-05 Validation loss 0.045539289712905884 Accuracy 0.8887500166893005\n",
      "Iteration 57670 Training loss 0.0025128319393843412 Validation loss 0.04553297162055969 Accuracy 0.8887500166893005\n",
      "Iteration 57680 Training loss 0.0012671106960624456 Validation loss 0.04554082453250885 Accuracy 0.8895000219345093\n",
      "Iteration 57690 Training loss 0.00126259692478925 Validation loss 0.045522525906562805 Accuracy 0.8893750309944153\n",
      "Iteration 57700 Training loss 0.0037757449317723513 Validation loss 0.04553825780749321 Accuracy 0.8892500400543213\n",
      "Iteration 57710 Training loss 0.002514712978154421 Validation loss 0.04554281756281853 Accuracy 0.8888750672340393\n",
      "Iteration 57720 Training loss 0.0012665782123804092 Validation loss 0.045539744198322296 Accuracy 0.8890000581741333\n",
      "Iteration 57730 Training loss 0.00377366179600358 Validation loss 0.04552292078733444 Accuracy 0.8890000581741333\n",
      "Iteration 57740 Training loss 0.003778700716793537 Validation loss 0.04554915428161621 Accuracy 0.8893750309944153\n",
      "Iteration 57750 Training loss 0.0012703309766948223 Validation loss 0.04550207033753395 Accuracy 0.8892500400543213\n",
      "Iteration 57760 Training loss 1.718330713629257e-05 Validation loss 0.045504163950681686 Accuracy 0.8888750672340393\n",
      "Iteration 57770 Training loss 0.0012662927620112896 Validation loss 0.04552488774061203 Accuracy 0.8892500400543213\n",
      "Iteration 57780 Training loss 0.0012720492668449879 Validation loss 0.04553932324051857 Accuracy 0.8890000581741333\n",
      "Iteration 57790 Training loss 0.00502176396548748 Validation loss 0.04555892571806908 Accuracy 0.8888750672340393\n",
      "Iteration 57800 Training loss 0.0037685036659240723 Validation loss 0.04553147777915001 Accuracy 0.8892500400543213\n",
      "Iteration 57810 Training loss 0.0025144999381154776 Validation loss 0.04554043337702751 Accuracy 0.8888750672340393\n",
      "Iteration 57820 Training loss 1.599318056832999e-05 Validation loss 0.04553018882870674 Accuracy 0.8888750672340393\n",
      "Iteration 57830 Training loss 0.001266668550670147 Validation loss 0.04557101055979729 Accuracy 0.8887500166893005\n",
      "Iteration 57840 Training loss 0.001264536171220243 Validation loss 0.0455174595117569 Accuracy 0.8890000581741333\n",
      "Iteration 57850 Training loss 2.647364635777194e-05 Validation loss 0.045548249036073685 Accuracy 0.8891250491142273\n",
      "Iteration 57860 Training loss 0.001266194973140955 Validation loss 0.045561306178569794 Accuracy 0.8890000581741333\n",
      "Iteration 57870 Training loss 0.003771752119064331 Validation loss 0.04552416875958443 Accuracy 0.8890000581741333\n",
      "Iteration 57880 Training loss 0.0012631439603865147 Validation loss 0.04552770406007767 Accuracy 0.8891250491142273\n",
      "Iteration 57890 Training loss 0.001270358799956739 Validation loss 0.045578982681035995 Accuracy 0.8891250491142273\n",
      "Iteration 57900 Training loss 0.002519499510526657 Validation loss 0.04558659717440605 Accuracy 0.8890000581741333\n",
      "Iteration 57910 Training loss 1.8405395167064853e-05 Validation loss 0.04554037004709244 Accuracy 0.8895000219345093\n",
      "Iteration 57920 Training loss 1.4301356713986024e-05 Validation loss 0.04556073993444443 Accuracy 0.8895000219345093\n",
      "Iteration 57930 Training loss 1.4337415450427216e-05 Validation loss 0.045555803924798965 Accuracy 0.8895000219345093\n",
      "Iteration 57940 Training loss 1.5220578461594414e-05 Validation loss 0.045523349195718765 Accuracy 0.8893750309944153\n",
      "Iteration 57950 Training loss 1.4261417163652368e-05 Validation loss 0.045548535883426666 Accuracy 0.8893750309944153\n",
      "Iteration 57960 Training loss 0.0025170939043164253 Validation loss 0.045553505420684814 Accuracy 0.8895000219345093\n",
      "Iteration 57970 Training loss 1.8557266230345704e-05 Validation loss 0.04554661363363266 Accuracy 0.889750063419342\n",
      "Iteration 57980 Training loss 1.4283673408499453e-05 Validation loss 0.045567650347948074 Accuracy 0.8893750309944153\n",
      "Iteration 57990 Training loss 0.0012677563354372978 Validation loss 0.045546840876340866 Accuracy 0.8895000219345093\n",
      "Iteration 58000 Training loss 0.002516744192689657 Validation loss 0.04552335664629936 Accuracy 0.8896250128746033\n",
      "Iteration 58010 Training loss 0.0025191723834723234 Validation loss 0.04555650055408478 Accuracy 0.8893750309944153\n",
      "Iteration 58020 Training loss 2.552242949604988e-05 Validation loss 0.04554811492562294 Accuracy 0.8892500400543213\n",
      "Iteration 58030 Training loss 0.0025160147342830896 Validation loss 0.04559259116649628 Accuracy 0.8892500400543213\n",
      "Iteration 58040 Training loss 0.001266226638108492 Validation loss 0.04555284604430199 Accuracy 0.889875054359436\n",
      "Iteration 58050 Training loss 1.7011992895277217e-05 Validation loss 0.045564405620098114 Accuracy 0.889750063419342\n",
      "Iteration 58060 Training loss 0.002513724844902754 Validation loss 0.04555444419384003 Accuracy 0.8892500400543213\n",
      "Iteration 58070 Training loss 1.9447556041995995e-05 Validation loss 0.04555166885256767 Accuracy 0.8891250491142273\n",
      "Iteration 58080 Training loss 0.0012684653047472239 Validation loss 0.04553607851266861 Accuracy 0.8891250491142273\n",
      "Iteration 58090 Training loss 0.0012648196425288916 Validation loss 0.04553813487291336 Accuracy 0.8893750309944153\n",
      "Iteration 58100 Training loss 2.1074200049042702e-05 Validation loss 0.04553741589188576 Accuracy 0.8888750672340393\n",
      "Iteration 58110 Training loss 0.001270090346224606 Validation loss 0.0455288365483284 Accuracy 0.8891250491142273\n",
      "Iteration 58120 Training loss 1.608597085578367e-05 Validation loss 0.0455431304872036 Accuracy 0.8893750309944153\n",
      "Iteration 58130 Training loss 0.002526539843529463 Validation loss 0.04558112844824791 Accuracy 0.8896250128746033\n",
      "Iteration 58140 Training loss 1.6772604794823565e-05 Validation loss 0.04554414004087448 Accuracy 0.8890000581741333\n",
      "Iteration 58150 Training loss 2.3099681129679084e-05 Validation loss 0.04553716629743576 Accuracy 0.8892500400543213\n",
      "Iteration 58160 Training loss 0.0012756657088175416 Validation loss 0.04555479809641838 Accuracy 0.8892500400543213\n",
      "Iteration 58170 Training loss 0.001269073924049735 Validation loss 0.0455428883433342 Accuracy 0.8891250491142273\n",
      "Iteration 58180 Training loss 0.001263773301616311 Validation loss 0.045591242611408234 Accuracy 0.8887500166893005\n",
      "Iteration 58190 Training loss 0.0012706578709185123 Validation loss 0.0455649271607399 Accuracy 0.8888750672340393\n",
      "Iteration 58200 Training loss 0.0012750427704304457 Validation loss 0.04554644599556923 Accuracy 0.8891250491142273\n",
      "Iteration 58210 Training loss 2.2117625121609308e-05 Validation loss 0.04551318287849426 Accuracy 0.8891250491142273\n",
      "Iteration 58220 Training loss 0.0012700810329988599 Validation loss 0.04552679508924484 Accuracy 0.8891250491142273\n",
      "Iteration 58230 Training loss 1.7214304534718394e-05 Validation loss 0.04554794356226921 Accuracy 0.8888750672340393\n",
      "Iteration 58240 Training loss 1.928486381075345e-05 Validation loss 0.04556584358215332 Accuracy 0.8890000581741333\n",
      "Iteration 58250 Training loss 1.7841153749031946e-05 Validation loss 0.045574434101581573 Accuracy 0.8888750672340393\n",
      "Iteration 58260 Training loss 0.0012639827327802777 Validation loss 0.045587312430143356 Accuracy 0.8887500166893005\n",
      "Iteration 58270 Training loss 0.0012700808001682162 Validation loss 0.0455663725733757 Accuracy 0.8888750672340393\n",
      "Iteration 58280 Training loss 0.002511399332433939 Validation loss 0.04555659741163254 Accuracy 0.8890000581741333\n",
      "Iteration 58290 Training loss 0.001267810701392591 Validation loss 0.04552111402153969 Accuracy 0.8892500400543213\n",
      "Iteration 58300 Training loss 0.0037660591769963503 Validation loss 0.04558476805686951 Accuracy 0.8886250257492065\n",
      "Iteration 58310 Training loss 1.969201366591733e-05 Validation loss 0.04552413895726204 Accuracy 0.8891250491142273\n",
      "Iteration 58320 Training loss 0.002513542305678129 Validation loss 0.04552873596549034 Accuracy 0.8891250491142273\n",
      "Iteration 58330 Training loss 2.0030431187478825e-05 Validation loss 0.04553952068090439 Accuracy 0.8892500400543213\n",
      "Iteration 58340 Training loss 2.1268850105116144e-05 Validation loss 0.045564569532871246 Accuracy 0.8892500400543213\n",
      "Iteration 58350 Training loss 0.001265775179490447 Validation loss 0.04557472839951515 Accuracy 0.8891250491142273\n",
      "Iteration 58360 Training loss 0.0025171111337840557 Validation loss 0.04554690420627594 Accuracy 0.8896250128746033\n",
      "Iteration 58370 Training loss 0.0037649988662451506 Validation loss 0.045548953115940094 Accuracy 0.8892500400543213\n",
      "Iteration 58380 Training loss 1.5898689525783993e-05 Validation loss 0.04554736986756325 Accuracy 0.8891250491142273\n",
      "Iteration 58390 Training loss 0.0012689665891230106 Validation loss 0.04556205868721008 Accuracy 0.8888750672340393\n",
      "Iteration 58400 Training loss 0.001267378916963935 Validation loss 0.045537810772657394 Accuracy 0.8893750309944153\n",
      "Iteration 58410 Training loss 2.09212248591939e-05 Validation loss 0.04557536169886589 Accuracy 0.8890000581741333\n",
      "Iteration 58420 Training loss 1.4615162399422843e-05 Validation loss 0.04555046185851097 Accuracy 0.8890000581741333\n",
      "Iteration 58430 Training loss 0.0025163476821035147 Validation loss 0.04551079124212265 Accuracy 0.8891250491142273\n",
      "Iteration 58440 Training loss 1.4821204786130693e-05 Validation loss 0.045514896512031555 Accuracy 0.8892500400543213\n",
      "Iteration 58450 Training loss 0.0025199525989592075 Validation loss 0.045532990247011185 Accuracy 0.8896250128746033\n",
      "Iteration 58460 Training loss 1.963110116776079e-05 Validation loss 0.04554407671093941 Accuracy 0.8890000581741333\n",
      "Iteration 58470 Training loss 0.0012667400296777487 Validation loss 0.045579660683870316 Accuracy 0.8888750672340393\n",
      "Iteration 58480 Training loss 0.0012655758764594793 Validation loss 0.04553379490971565 Accuracy 0.8888750672340393\n",
      "Iteration 58490 Training loss 2.3238993890117854e-05 Validation loss 0.04554198309779167 Accuracy 0.8892500400543213\n",
      "Iteration 58500 Training loss 0.0025160580407828093 Validation loss 0.045569200068712234 Accuracy 0.8892500400543213\n",
      "Iteration 58510 Training loss 0.0025163546670228243 Validation loss 0.0455564484000206 Accuracy 0.8892500400543213\n",
      "Iteration 58520 Training loss 0.001268802909180522 Validation loss 0.04554814472794533 Accuracy 0.8890000581741333\n",
      "Iteration 58530 Training loss 0.002520343055948615 Validation loss 0.04554847255349159 Accuracy 0.8890000581741333\n",
      "Iteration 58540 Training loss 0.0025146864354610443 Validation loss 0.04555028676986694 Accuracy 0.8890000581741333\n",
      "Iteration 58550 Training loss 2.9913358957855962e-05 Validation loss 0.04554712027311325 Accuracy 0.8888750672340393\n",
      "Iteration 58560 Training loss 0.002522164024412632 Validation loss 0.04555646702647209 Accuracy 0.8890000581741333\n",
      "Iteration 58570 Training loss 1.3189456694817636e-05 Validation loss 0.045527588576078415 Accuracy 0.8887500166893005\n",
      "Iteration 58580 Training loss 0.0012650854187086225 Validation loss 0.045539263635873795 Accuracy 0.8886250257492065\n",
      "Iteration 58590 Training loss 1.6118865460157394e-05 Validation loss 0.04557313024997711 Accuracy 0.8885000348091125\n",
      "Iteration 58600 Training loss 0.0012671396834775805 Validation loss 0.04555121436715126 Accuracy 0.8890000581741333\n",
      "Iteration 58610 Training loss 0.002519855508580804 Validation loss 0.04555000737309456 Accuracy 0.8891250491142273\n",
      "Iteration 58620 Training loss 0.0025261729024350643 Validation loss 0.0455712266266346 Accuracy 0.8890000581741333\n",
      "Iteration 58630 Training loss 0.0012670065043494105 Validation loss 0.0455426350235939 Accuracy 0.8893750309944153\n",
      "Iteration 58640 Training loss 1.7224820112460293e-05 Validation loss 0.04555317386984825 Accuracy 0.8885000348091125\n",
      "Iteration 58650 Training loss 0.0025205803103744984 Validation loss 0.04552152007818222 Accuracy 0.8890000581741333\n",
      "Iteration 58660 Training loss 1.6398656953242607e-05 Validation loss 0.04555114358663559 Accuracy 0.8892500400543213\n",
      "Iteration 58670 Training loss 1.1293197530903853e-05 Validation loss 0.04556502774357796 Accuracy 0.8888750672340393\n",
      "Iteration 58680 Training loss 0.005017308983951807 Validation loss 0.045560143887996674 Accuracy 0.8892500400543213\n",
      "Iteration 58690 Training loss 0.0012713990872725844 Validation loss 0.04554247856140137 Accuracy 0.8891250491142273\n",
      "Iteration 58700 Training loss 0.0012707927962765098 Validation loss 0.045525360852479935 Accuracy 0.8893750309944153\n",
      "Iteration 58710 Training loss 1.7773561921785586e-05 Validation loss 0.04554346576333046 Accuracy 0.8895000219345093\n",
      "Iteration 58720 Training loss 0.002515072701498866 Validation loss 0.045548319816589355 Accuracy 0.8895000219345093\n",
      "Iteration 58730 Training loss 0.0012679591309279203 Validation loss 0.045562051236629486 Accuracy 0.8892500400543213\n",
      "Iteration 58740 Training loss 0.0012692657765001059 Validation loss 0.04559461399912834 Accuracy 0.8888750672340393\n",
      "Iteration 58750 Training loss 0.0012699260842055082 Validation loss 0.045565418899059296 Accuracy 0.8888750672340393\n",
      "Iteration 58760 Training loss 0.001274631591513753 Validation loss 0.045528437942266464 Accuracy 0.8892500400543213\n",
      "Iteration 58770 Training loss 0.00377090647816658 Validation loss 0.04555249586701393 Accuracy 0.8896250128746033\n",
      "Iteration 58780 Training loss 0.002512784209102392 Validation loss 0.04553517326712608 Accuracy 0.8895000219345093\n",
      "Iteration 58790 Training loss 0.0012696718331426382 Validation loss 0.04557083174586296 Accuracy 0.8893750309944153\n",
      "Iteration 58800 Training loss 2.087333814415615e-05 Validation loss 0.045570384711027145 Accuracy 0.8892500400543213\n",
      "Iteration 58810 Training loss 0.001266914769075811 Validation loss 0.04558180272579193 Accuracy 0.8895000219345093\n",
      "Iteration 58820 Training loss 0.00502037163823843 Validation loss 0.04557276517152786 Accuracy 0.889750063419342\n",
      "Iteration 58830 Training loss 0.006271911785006523 Validation loss 0.045543041080236435 Accuracy 0.8891250491142273\n",
      "Iteration 58840 Training loss 1.5464915122720413e-05 Validation loss 0.04557366296648979 Accuracy 0.8892500400543213\n",
      "Iteration 58850 Training loss 0.0012646301183849573 Validation loss 0.045571304857730865 Accuracy 0.8896250128746033\n",
      "Iteration 58860 Training loss 0.0025181984528899193 Validation loss 0.045552078634500504 Accuracy 0.8896250128746033\n",
      "Iteration 58870 Training loss 0.0012676266487687826 Validation loss 0.045548830181360245 Accuracy 0.8895000219345093\n",
      "Iteration 58880 Training loss 1.7254698832402937e-05 Validation loss 0.04555044695734978 Accuracy 0.8891250491142273\n",
      "Iteration 58890 Training loss 1.8490245565772057e-05 Validation loss 0.04554864019155502 Accuracy 0.8890000581741333\n",
      "Iteration 58900 Training loss 0.002516387263312936 Validation loss 0.04555836692452431 Accuracy 0.8893750309944153\n",
      "Iteration 58910 Training loss 0.0012658309424296021 Validation loss 0.045538660138845444 Accuracy 0.8892500400543213\n",
      "Iteration 58920 Training loss 0.001265570754185319 Validation loss 0.04558617249131203 Accuracy 0.8885000348091125\n",
      "Iteration 58930 Training loss 2.0042713003931567e-05 Validation loss 0.04553716629743576 Accuracy 0.8890000581741333\n",
      "Iteration 58940 Training loss 2.323068110854365e-05 Validation loss 0.04555707424879074 Accuracy 0.8888750672340393\n",
      "Iteration 58950 Training loss 0.0012675359612330794 Validation loss 0.04554756358265877 Accuracy 0.8888750672340393\n",
      "Iteration 58960 Training loss 2.175737972720526e-05 Validation loss 0.045538004487752914 Accuracy 0.8890000581741333\n",
      "Iteration 58970 Training loss 1.4524891412293073e-05 Validation loss 0.045527223497629166 Accuracy 0.8887500166893005\n",
      "Iteration 58980 Training loss 1.9295348465675488e-05 Validation loss 0.045546650886535645 Accuracy 0.8891250491142273\n",
      "Iteration 58990 Training loss 0.0012730751186609268 Validation loss 0.04552990570664406 Accuracy 0.8888750672340393\n",
      "Iteration 59000 Training loss 0.002519340254366398 Validation loss 0.04553873836994171 Accuracy 0.8887500166893005\n",
      "Iteration 59010 Training loss 1.690196768322494e-05 Validation loss 0.04555594548583031 Accuracy 0.8888750672340393\n",
      "Iteration 59020 Training loss 0.001263607875443995 Validation loss 0.045565951615571976 Accuracy 0.8887500166893005\n",
      "Iteration 59030 Training loss 0.003768888069316745 Validation loss 0.04560607671737671 Accuracy 0.8887500166893005\n",
      "Iteration 59040 Training loss 1.9360046280780807e-05 Validation loss 0.04555250704288483 Accuracy 0.8892500400543213\n",
      "Iteration 59050 Training loss 1.8269762222189456e-05 Validation loss 0.0455356165766716 Accuracy 0.8885000348091125\n",
      "Iteration 59060 Training loss 0.0012662860099226236 Validation loss 0.045565035194158554 Accuracy 0.8890000581741333\n",
      "Iteration 59070 Training loss 1.6261230484815314e-05 Validation loss 0.04554908350110054 Accuracy 0.8890000581741333\n",
      "Iteration 59080 Training loss 0.0012670441064983606 Validation loss 0.04555703327059746 Accuracy 0.8890000581741333\n",
      "Iteration 59090 Training loss 1.9182936739525758e-05 Validation loss 0.04558304697275162 Accuracy 0.8888750672340393\n",
      "Iteration 59100 Training loss 2.1835210645804182e-05 Validation loss 0.04556890204548836 Accuracy 0.8885000348091125\n",
      "Iteration 59110 Training loss 1.4774595001654234e-05 Validation loss 0.045565977692604065 Accuracy 0.8888750672340393\n",
      "Iteration 59120 Training loss 0.0025155399926006794 Validation loss 0.04553306847810745 Accuracy 0.8890000581741333\n",
      "Iteration 59130 Training loss 1.6369724107789807e-05 Validation loss 0.045594725757837296 Accuracy 0.8890000581741333\n",
      "Iteration 59140 Training loss 0.0012639755150303245 Validation loss 0.04555003345012665 Accuracy 0.8890000581741333\n",
      "Iteration 59150 Training loss 0.0012733557960018516 Validation loss 0.04555540531873703 Accuracy 0.8893750309944153\n",
      "Iteration 59160 Training loss 2.2370983060682192e-05 Validation loss 0.045569613575935364 Accuracy 0.8892500400543213\n",
      "Iteration 59170 Training loss 1.7433249013265595e-05 Validation loss 0.04558287188410759 Accuracy 0.8892500400543213\n",
      "Iteration 59180 Training loss 1.3289421076478902e-05 Validation loss 0.04559396579861641 Accuracy 0.8891250491142273\n",
      "Iteration 59190 Training loss 1.564105878060218e-05 Validation loss 0.045562922954559326 Accuracy 0.8892500400543213\n",
      "Iteration 59200 Training loss 1.5378929674625397e-05 Validation loss 0.04560544714331627 Accuracy 0.8888750672340393\n",
      "Iteration 59210 Training loss 0.0012630980927497149 Validation loss 0.04556667432188988 Accuracy 0.8890000581741333\n",
      "Iteration 59220 Training loss 1.8913851818069816e-05 Validation loss 0.04557238146662712 Accuracy 0.8890000581741333\n",
      "Iteration 59230 Training loss 0.0012683264212682843 Validation loss 0.04554423317313194 Accuracy 0.8888750672340393\n",
      "Iteration 59240 Training loss 0.0025176480412483215 Validation loss 0.045527443289756775 Accuracy 0.8891250491142273\n",
      "Iteration 59250 Training loss 1.3366061466513202e-05 Validation loss 0.04552161321043968 Accuracy 0.8893750309944153\n",
      "Iteration 59260 Training loss 1.9146020349580795e-05 Validation loss 0.04556470736861229 Accuracy 0.8890000581741333\n",
      "Iteration 59270 Training loss 1.7638669305597432e-05 Validation loss 0.04555467516183853 Accuracy 0.8891250491142273\n",
      "Iteration 59280 Training loss 0.001269923523068428 Validation loss 0.04555317014455795 Accuracy 0.8896250128746033\n",
      "Iteration 59290 Training loss 0.001264765509404242 Validation loss 0.04557577520608902 Accuracy 0.8893750309944153\n",
      "Iteration 59300 Training loss 0.0012679861392825842 Validation loss 0.04560556262731552 Accuracy 0.8890000581741333\n",
      "Iteration 59310 Training loss 1.1264961358392611e-05 Validation loss 0.04558081179857254 Accuracy 0.8893750309944153\n",
      "Iteration 59320 Training loss 0.0012687771813943982 Validation loss 0.045559801161289215 Accuracy 0.8887500166893005\n",
      "Iteration 59330 Training loss 1.5682095181546174e-05 Validation loss 0.04559055343270302 Accuracy 0.8890000581741333\n",
      "Iteration 59340 Training loss 0.0012755930656567216 Validation loss 0.04557880014181137 Accuracy 0.8890000581741333\n",
      "Iteration 59350 Training loss 1.7135038433480076e-05 Validation loss 0.04560120776295662 Accuracy 0.8888750672340393\n",
      "Iteration 59360 Training loss 0.002517976798117161 Validation loss 0.045607782900333405 Accuracy 0.8888750672340393\n",
      "Iteration 59370 Training loss 1.366904780297773e-05 Validation loss 0.045568980276584625 Accuracy 0.8892500400543213\n",
      "Iteration 59380 Training loss 0.0025117420591413975 Validation loss 0.04560597985982895 Accuracy 0.8887500166893005\n",
      "Iteration 59390 Training loss 0.0012672168668359518 Validation loss 0.04560098424553871 Accuracy 0.8891250491142273\n",
      "Iteration 59400 Training loss 0.0025194245390594006 Validation loss 0.04563170671463013 Accuracy 0.8890000581741333\n",
      "Iteration 59410 Training loss 0.0012620787601917982 Validation loss 0.04562940075993538 Accuracy 0.8891250491142273\n",
      "Iteration 59420 Training loss 2.91746728180442e-05 Validation loss 0.04562576487660408 Accuracy 0.8890000581741333\n",
      "Iteration 59430 Training loss 0.003763430519029498 Validation loss 0.04559653624892235 Accuracy 0.8892500400543213\n",
      "Iteration 59440 Training loss 0.001271295826882124 Validation loss 0.04556296393275261 Accuracy 0.8892500400543213\n",
      "Iteration 59450 Training loss 0.0025154261384159327 Validation loss 0.04561077430844307 Accuracy 0.8888750672340393\n",
      "Iteration 59460 Training loss 0.003764917841181159 Validation loss 0.04557794705033302 Accuracy 0.8886250257492065\n",
      "Iteration 59470 Training loss 0.0012796246446669102 Validation loss 0.04554755613207817 Accuracy 0.8890000581741333\n",
      "Iteration 59480 Training loss 0.0012712491443380713 Validation loss 0.04557675123214722 Accuracy 0.8890000581741333\n",
      "Iteration 59490 Training loss 0.0025164217222481966 Validation loss 0.04556053876876831 Accuracy 0.8890000581741333\n",
      "Iteration 59500 Training loss 0.0025138109922409058 Validation loss 0.04554634913802147 Accuracy 0.8888750672340393\n",
      "Iteration 59510 Training loss 0.0012664486421272159 Validation loss 0.04556192830204964 Accuracy 0.8891250491142273\n",
      "Iteration 59520 Training loss 0.0025139092467725277 Validation loss 0.04555939882993698 Accuracy 0.8892500400543213\n",
      "Iteration 59530 Training loss 1.4077606465434656e-05 Validation loss 0.04556911066174507 Accuracy 0.8893750309944153\n",
      "Iteration 59540 Training loss 1.3545783986046445e-05 Validation loss 0.045568693429231644 Accuracy 0.8888750672340393\n",
      "Iteration 59550 Training loss 1.8114729755325243e-05 Validation loss 0.04555758833885193 Accuracy 0.8888750672340393\n",
      "Iteration 59560 Training loss 0.001261611352674663 Validation loss 0.0455738864839077 Accuracy 0.8890000581741333\n",
      "Iteration 59570 Training loss 1.334659282292705e-05 Validation loss 0.04556971788406372 Accuracy 0.8892500400543213\n",
      "Iteration 59580 Training loss 0.0025195812340825796 Validation loss 0.045572198927402496 Accuracy 0.8893750309944153\n",
      "Iteration 59590 Training loss 1.1841150808322709e-05 Validation loss 0.045540597289800644 Accuracy 0.8895000219345093\n",
      "Iteration 59600 Training loss 2.4045244572334923e-05 Validation loss 0.04557503014802933 Accuracy 0.8895000219345093\n",
      "Iteration 59610 Training loss 0.0012683204840868711 Validation loss 0.04555932432413101 Accuracy 0.8893750309944153\n",
      "Iteration 59620 Training loss 0.0012702203821390867 Validation loss 0.045558951795101166 Accuracy 0.8892500400543213\n",
      "Iteration 59630 Training loss 0.0037667774595320225 Validation loss 0.04554884880781174 Accuracy 0.8892500400543213\n",
      "Iteration 59640 Training loss 1.535555384180043e-05 Validation loss 0.045602623373270035 Accuracy 0.8892500400543213\n",
      "Iteration 59650 Training loss 1.3315511750988662e-05 Validation loss 0.0455826073884964 Accuracy 0.8890000581741333\n",
      "Iteration 59660 Training loss 0.002518606372177601 Validation loss 0.045556407421827316 Accuracy 0.8885000348091125\n",
      "Iteration 59670 Training loss 0.003776226658374071 Validation loss 0.04556754603981972 Accuracy 0.8886250257492065\n",
      "Iteration 59680 Training loss 0.0012672385200858116 Validation loss 0.04560363292694092 Accuracy 0.8891250491142273\n",
      "Iteration 59690 Training loss 0.0012665033573284745 Validation loss 0.045577432960271835 Accuracy 0.8891250491142273\n",
      "Iteration 59700 Training loss 0.0012655730824917555 Validation loss 0.04556943103671074 Accuracy 0.8890000581741333\n",
      "Iteration 59710 Training loss 2.3215356122818775e-05 Validation loss 0.0455714650452137 Accuracy 0.8893750309944153\n",
      "Iteration 59720 Training loss 1.4679969353892375e-05 Validation loss 0.04557400569319725 Accuracy 0.8891250491142273\n",
      "Iteration 59730 Training loss 0.0025187875144183636 Validation loss 0.04556935653090477 Accuracy 0.8892500400543213\n",
      "Iteration 59740 Training loss 0.001263776677660644 Validation loss 0.04556836560368538 Accuracy 0.8892500400543213\n",
      "Iteration 59750 Training loss 0.003761640749871731 Validation loss 0.045544639229774475 Accuracy 0.8888750672340393\n",
      "Iteration 59760 Training loss 0.002515144180506468 Validation loss 0.04558894410729408 Accuracy 0.8890000581741333\n",
      "Iteration 59770 Training loss 2.021793625317514e-05 Validation loss 0.045568253844976425 Accuracy 0.8891250491142273\n",
      "Iteration 59780 Training loss 0.0037635895423591137 Validation loss 0.04556255042552948 Accuracy 0.8890000581741333\n",
      "Iteration 59790 Training loss 0.002519297879189253 Validation loss 0.045571550726890564 Accuracy 0.8890000581741333\n",
      "Iteration 59800 Training loss 2.1157224182388745e-05 Validation loss 0.045581310987472534 Accuracy 0.8891250491142273\n",
      "Iteration 59810 Training loss 1.3249769835965708e-05 Validation loss 0.0455898754298687 Accuracy 0.8891250491142273\n",
      "Iteration 59820 Training loss 0.0025142901577055454 Validation loss 0.04559146240353584 Accuracy 0.8891250491142273\n",
      "Iteration 59830 Training loss 0.0012645431561395526 Validation loss 0.04555562138557434 Accuracy 0.8890000581741333\n",
      "Iteration 59840 Training loss 2.4025746824918315e-05 Validation loss 0.045577213168144226 Accuracy 0.8891250491142273\n",
      "Iteration 59850 Training loss 0.002519832691177726 Validation loss 0.04558859020471573 Accuracy 0.8892500400543213\n",
      "Iteration 59860 Training loss 1.735466685204301e-05 Validation loss 0.04560134932398796 Accuracy 0.8888750672340393\n",
      "Iteration 59870 Training loss 0.0012694045435637236 Validation loss 0.04558394104242325 Accuracy 0.8890000581741333\n",
      "Iteration 59880 Training loss 1.9239256289438345e-05 Validation loss 0.04560249671339989 Accuracy 0.8890000581741333\n",
      "Iteration 59890 Training loss 0.002513972809538245 Validation loss 0.04554755613207817 Accuracy 0.8892500400543213\n",
      "Iteration 59900 Training loss 0.002515987493097782 Validation loss 0.045581161975860596 Accuracy 0.8890000581741333\n",
      "Iteration 59910 Training loss 0.0012682314263656735 Validation loss 0.045540131628513336 Accuracy 0.8893750309944153\n",
      "Iteration 59920 Training loss 1.4965540685807355e-05 Validation loss 0.04554443061351776 Accuracy 0.8895000219345093\n",
      "Iteration 59930 Training loss 0.0025129206478595734 Validation loss 0.045575547963380814 Accuracy 0.8892500400543213\n",
      "Iteration 59940 Training loss 0.001269203727133572 Validation loss 0.045558277517557144 Accuracy 0.8893750309944153\n",
      "Iteration 59950 Training loss 0.0012678164057433605 Validation loss 0.04553299769759178 Accuracy 0.8893750309944153\n",
      "Iteration 59960 Training loss 0.001270184526219964 Validation loss 0.045546356588602066 Accuracy 0.8893750309944153\n",
      "Iteration 59970 Training loss 1.0402734005765524e-05 Validation loss 0.04559078440070152 Accuracy 0.8888750672340393\n",
      "Iteration 59980 Training loss 1.5623962099198252e-05 Validation loss 0.045613422989845276 Accuracy 0.8890000581741333\n",
      "Iteration 59990 Training loss 0.0012657298939302564 Validation loss 0.045579347759485245 Accuracy 0.8888750672340393\n",
      "Iteration 60000 Training loss 0.001266070525161922 Validation loss 0.045558419078588486 Accuracy 0.8888750672340393\n",
      "Iteration 60010 Training loss 1.6040599803091027e-05 Validation loss 0.04557479918003082 Accuracy 0.8892500400543213\n",
      "Iteration 60020 Training loss 0.002521848538890481 Validation loss 0.04557318240404129 Accuracy 0.8895000219345093\n",
      "Iteration 60030 Training loss 1.4823649507889058e-05 Validation loss 0.0455632209777832 Accuracy 0.8893750309944153\n",
      "Iteration 60040 Training loss 1.2760294339386746e-05 Validation loss 0.04558788239955902 Accuracy 0.8893750309944153\n",
      "Iteration 60050 Training loss 0.0012652953155338764 Validation loss 0.04556294158101082 Accuracy 0.8892500400543213\n",
      "Iteration 60060 Training loss 0.00376461585983634 Validation loss 0.04557706043124199 Accuracy 0.8893750309944153\n",
      "Iteration 60070 Training loss 1.718201019684784e-05 Validation loss 0.045582711696624756 Accuracy 0.8888750672340393\n",
      "Iteration 60080 Training loss 2.553826561779715e-05 Validation loss 0.04558391869068146 Accuracy 0.8888750672340393\n",
      "Iteration 60090 Training loss 0.0012661434011533856 Validation loss 0.04554753005504608 Accuracy 0.8887500166893005\n",
      "Iteration 60100 Training loss 2.0007561033708043e-05 Validation loss 0.045593392103910446 Accuracy 0.8890000581741333\n",
      "Iteration 60110 Training loss 0.002515804022550583 Validation loss 0.04556233435869217 Accuracy 0.8887500166893005\n",
      "Iteration 60120 Training loss 1.7595215467736125e-05 Validation loss 0.045552633702754974 Accuracy 0.8888750672340393\n",
      "Iteration 60130 Training loss 0.002517383312806487 Validation loss 0.045573052018880844 Accuracy 0.8892500400543213\n",
      "Iteration 60140 Training loss 0.003767858725041151 Validation loss 0.04556144401431084 Accuracy 0.8896250128746033\n",
      "Iteration 60150 Training loss 0.00252011651173234 Validation loss 0.045596372336149216 Accuracy 0.8891250491142273\n",
      "Iteration 60160 Training loss 0.001275507966056466 Validation loss 0.04559055343270302 Accuracy 0.8891250491142273\n",
      "Iteration 60170 Training loss 1.2068148862454109e-05 Validation loss 0.04559406638145447 Accuracy 0.8890000581741333\n",
      "Iteration 60180 Training loss 2.205791861342732e-05 Validation loss 0.045580554753541946 Accuracy 0.8891250491142273\n",
      "Iteration 60190 Training loss 1.3640962606586982e-05 Validation loss 0.045572955161333084 Accuracy 0.8891250491142273\n",
      "Iteration 60200 Training loss 0.0012646334944292903 Validation loss 0.04560316726565361 Accuracy 0.8890000581741333\n",
      "Iteration 60210 Training loss 0.0025202673859894276 Validation loss 0.04560427367687225 Accuracy 0.8892500400543213\n",
      "Iteration 60220 Training loss 1.922989213198889e-05 Validation loss 0.04558436945080757 Accuracy 0.8888750672340393\n",
      "Iteration 60230 Training loss 0.0025145900435745716 Validation loss 0.04563293606042862 Accuracy 0.8890000581741333\n",
      "Iteration 60240 Training loss 0.00252539268694818 Validation loss 0.04557914286851883 Accuracy 0.8887500166893005\n",
      "Iteration 60250 Training loss 0.002512876410037279 Validation loss 0.04558205604553223 Accuracy 0.8892500400543213\n",
      "Iteration 60260 Training loss 0.001270829583518207 Validation loss 0.04557546600699425 Accuracy 0.8890000581741333\n",
      "Iteration 60270 Training loss 1.4304013348009903e-05 Validation loss 0.04558253660798073 Accuracy 0.8891250491142273\n",
      "Iteration 60280 Training loss 0.0012655217433348298 Validation loss 0.04555768147110939 Accuracy 0.8888750672340393\n",
      "Iteration 60290 Training loss 1.0814021152327769e-05 Validation loss 0.045582327991724014 Accuracy 0.8887500166893005\n",
      "Iteration 60300 Training loss 0.001260375021956861 Validation loss 0.04560498148202896 Accuracy 0.8886250257492065\n",
      "Iteration 60310 Training loss 0.002518510678783059 Validation loss 0.04557817429304123 Accuracy 0.8890000581741333\n",
      "Iteration 60320 Training loss 0.0025275968946516514 Validation loss 0.04556071758270264 Accuracy 0.8887500166893005\n",
      "Iteration 60330 Training loss 1.9410508684813976e-05 Validation loss 0.04556892439723015 Accuracy 0.8892500400543213\n",
      "Iteration 60340 Training loss 0.001267236191779375 Validation loss 0.04559775069355965 Accuracy 0.8888750672340393\n",
      "Iteration 60350 Training loss 0.001270182547159493 Validation loss 0.04557487741112709 Accuracy 0.8890000581741333\n",
      "Iteration 60360 Training loss 0.0037666219286620617 Validation loss 0.045589931309223175 Accuracy 0.8888750672340393\n",
      "Iteration 60370 Training loss 0.0037632882595062256 Validation loss 0.04558749124407768 Accuracy 0.8888750672340393\n",
      "Iteration 60380 Training loss 0.0012645722599700093 Validation loss 0.045609116554260254 Accuracy 0.8888750672340393\n",
      "Iteration 60390 Training loss 0.002514404011890292 Validation loss 0.04558141529560089 Accuracy 0.8888750672340393\n",
      "Iteration 60400 Training loss 1.957663698703982e-05 Validation loss 0.04555147886276245 Accuracy 0.8887500166893005\n",
      "Iteration 60410 Training loss 2.023713204835076e-05 Validation loss 0.04556441679596901 Accuracy 0.8890000581741333\n",
      "Iteration 60420 Training loss 0.0012595587177202106 Validation loss 0.045610375702381134 Accuracy 0.8887500166893005\n",
      "Iteration 60430 Training loss 0.002512743230909109 Validation loss 0.045584630221128464 Accuracy 0.8890000581741333\n",
      "Iteration 60440 Training loss 0.0012638107873499393 Validation loss 0.04557502269744873 Accuracy 0.8893750309944153\n",
      "Iteration 60450 Training loss 0.002525635529309511 Validation loss 0.04558606073260307 Accuracy 0.8888750672340393\n",
      "Iteration 60460 Training loss 0.001263910555280745 Validation loss 0.04555672034621239 Accuracy 0.8890000581741333\n",
      "Iteration 60470 Training loss 0.002521866699680686 Validation loss 0.04558546841144562 Accuracy 0.8890000581741333\n",
      "Iteration 60480 Training loss 0.001268849358893931 Validation loss 0.04561442881822586 Accuracy 0.8886250257492065\n",
      "Iteration 60490 Training loss 0.003770273644477129 Validation loss 0.045614589005708694 Accuracy 0.8888750672340393\n",
      "Iteration 60500 Training loss 2.70259115495719e-05 Validation loss 0.0455903485417366 Accuracy 0.8888750672340393\n",
      "Iteration 60510 Training loss 1.3335925359569956e-05 Validation loss 0.045595698058605194 Accuracy 0.8890000581741333\n",
      "Iteration 60520 Training loss 0.0012613451108336449 Validation loss 0.04559954255819321 Accuracy 0.8893750309944153\n",
      "Iteration 60530 Training loss 0.0025153576862066984 Validation loss 0.04559179022908211 Accuracy 0.8891250491142273\n",
      "Iteration 60540 Training loss 0.0025140296202152967 Validation loss 0.04559767246246338 Accuracy 0.8893750309944153\n",
      "Iteration 60550 Training loss 0.0012673820601776242 Validation loss 0.04560015723109245 Accuracy 0.8887500166893005\n",
      "Iteration 60560 Training loss 0.0025210657622665167 Validation loss 0.04557601362466812 Accuracy 0.8890000581741333\n",
      "Iteration 60570 Training loss 0.0025241905823349953 Validation loss 0.04555048048496246 Accuracy 0.8890000581741333\n",
      "Iteration 60580 Training loss 0.001267559826374054 Validation loss 0.0455966517329216 Accuracy 0.8887500166893005\n",
      "Iteration 60590 Training loss 1.9085775420535356e-05 Validation loss 0.0456063412129879 Accuracy 0.8890000581741333\n",
      "Iteration 60600 Training loss 0.00251089152880013 Validation loss 0.04556846246123314 Accuracy 0.8893750309944153\n",
      "Iteration 60610 Training loss 0.0012685428373515606 Validation loss 0.04556993395090103 Accuracy 0.8890000581741333\n",
      "Iteration 60620 Training loss 0.0012634327867999673 Validation loss 0.04557988792657852 Accuracy 0.8890000581741333\n",
      "Iteration 60630 Training loss 0.001263546058908105 Validation loss 0.045569244772195816 Accuracy 0.8892500400543213\n",
      "Iteration 60640 Training loss 0.0025158284697681665 Validation loss 0.04557304456830025 Accuracy 0.8887500166893005\n",
      "Iteration 60650 Training loss 1.453342611057451e-05 Validation loss 0.0455743633210659 Accuracy 0.8892500400543213\n",
      "Iteration 60660 Training loss 1.2560749382828362e-05 Validation loss 0.04559400677680969 Accuracy 0.8891250491142273\n",
      "Iteration 60670 Training loss 0.0037666207645088434 Validation loss 0.04556475952267647 Accuracy 0.8888750672340393\n",
      "Iteration 60680 Training loss 0.005017430055886507 Validation loss 0.045557405799627304 Accuracy 0.8891250491142273\n",
      "Iteration 60690 Training loss 0.001269215950742364 Validation loss 0.04556695371866226 Accuracy 0.8890000581741333\n",
      "Iteration 60700 Training loss 1.937369779625442e-05 Validation loss 0.045541100203990936 Accuracy 0.8892500400543213\n",
      "Iteration 60710 Training loss 0.0012674990575760603 Validation loss 0.0456017442047596 Accuracy 0.8892500400543213\n",
      "Iteration 60720 Training loss 0.0025123110972344875 Validation loss 0.04555726796388626 Accuracy 0.8893750309944153\n",
      "Iteration 60730 Training loss 0.0012637192849069834 Validation loss 0.04558224603533745 Accuracy 0.8896250128746033\n",
      "Iteration 60740 Training loss 0.0012716425117105246 Validation loss 0.04561431705951691 Accuracy 0.8892500400543213\n",
      "Iteration 60750 Training loss 1.3158192814444192e-05 Validation loss 0.045577019453048706 Accuracy 0.8892500400543213\n",
      "Iteration 60760 Training loss 1.130354303313652e-05 Validation loss 0.04558847099542618 Accuracy 0.8895000219345093\n",
      "Iteration 60770 Training loss 0.0012680776417255402 Validation loss 0.04557766020298004 Accuracy 0.8892500400543213\n",
      "Iteration 60780 Training loss 0.003765314817428589 Validation loss 0.04557390883564949 Accuracy 0.8892500400543213\n",
      "Iteration 60790 Training loss 0.003764544613659382 Validation loss 0.04557415097951889 Accuracy 0.8891250491142273\n",
      "Iteration 60800 Training loss 0.0012613609433174133 Validation loss 0.04558053985238075 Accuracy 0.8892500400543213\n",
      "Iteration 60810 Training loss 0.001269430504180491 Validation loss 0.04558629170060158 Accuracy 0.8891250491142273\n",
      "Iteration 60820 Training loss 2.536167266953271e-05 Validation loss 0.04560147970914841 Accuracy 0.8888750672340393\n",
      "Iteration 60830 Training loss 0.003761051455512643 Validation loss 0.04556652158498764 Accuracy 0.8891250491142273\n",
      "Iteration 60840 Training loss 0.0012606289237737656 Validation loss 0.045563314110040665 Accuracy 0.8895000219345093\n",
      "Iteration 60850 Training loss 0.0025179593358188868 Validation loss 0.04555777087807655 Accuracy 0.8891250491142273\n",
      "Iteration 60860 Training loss 1.586251710250508e-05 Validation loss 0.04555676504969597 Accuracy 0.8890000581741333\n",
      "Iteration 60870 Training loss 0.0012678579660132527 Validation loss 0.04557099565863609 Accuracy 0.8891250491142273\n",
      "Iteration 60880 Training loss 0.0012703047832474113 Validation loss 0.04556402191519737 Accuracy 0.8890000581741333\n",
      "Iteration 60890 Training loss 1.4458426448982209e-05 Validation loss 0.04556494206190109 Accuracy 0.8890000581741333\n",
      "Iteration 60900 Training loss 1.6915046217036434e-05 Validation loss 0.045563194900751114 Accuracy 0.8892500400543213\n",
      "Iteration 60910 Training loss 0.001264044432900846 Validation loss 0.04558916389942169 Accuracy 0.8888750672340393\n",
      "Iteration 60920 Training loss 1.6171456081792712e-05 Validation loss 0.04555574432015419 Accuracy 0.8891250491142273\n",
      "Iteration 60930 Training loss 0.0012639806373044848 Validation loss 0.04557720944285393 Accuracy 0.8888750672340393\n",
      "Iteration 60940 Training loss 1.2997679732507095e-05 Validation loss 0.04558195546269417 Accuracy 0.8891250491142273\n",
      "Iteration 60950 Training loss 1.761529165378306e-05 Validation loss 0.04556809365749359 Accuracy 0.8893750309944153\n",
      "Iteration 60960 Training loss 1.4632403690484352e-05 Validation loss 0.04559801518917084 Accuracy 0.8891250491142273\n",
      "Iteration 60970 Training loss 0.001265940023586154 Validation loss 0.04559429734945297 Accuracy 0.8888750672340393\n",
      "Iteration 60980 Training loss 0.002512100851163268 Validation loss 0.04556940123438835 Accuracy 0.8892500400543213\n",
      "Iteration 60990 Training loss 0.0012694107135757804 Validation loss 0.04556156322360039 Accuracy 0.8892500400543213\n",
      "Iteration 61000 Training loss 0.0012742506805807352 Validation loss 0.0455804243683815 Accuracy 0.8890000581741333\n",
      "Iteration 61010 Training loss 0.0012748024892061949 Validation loss 0.04557435214519501 Accuracy 0.8890000581741333\n",
      "Iteration 61020 Training loss 1.8858176190406084e-05 Validation loss 0.045594193041324615 Accuracy 0.8890000581741333\n",
      "Iteration 61030 Training loss 0.0025139222852885723 Validation loss 0.04558394476771355 Accuracy 0.8888750672340393\n",
      "Iteration 61040 Training loss 2.1388275854405947e-05 Validation loss 0.04558344930410385 Accuracy 0.8883750438690186\n",
      "Iteration 61050 Training loss 0.0037650021258741617 Validation loss 0.04559902101755142 Accuracy 0.8888750672340393\n",
      "Iteration 61060 Training loss 2.4435301384073682e-05 Validation loss 0.045590564608573914 Accuracy 0.8890000581741333\n",
      "Iteration 61070 Training loss 2.2375241314875893e-05 Validation loss 0.04557610675692558 Accuracy 0.8887500166893005\n",
      "Iteration 61080 Training loss 0.0025209852028638124 Validation loss 0.04558047652244568 Accuracy 0.8890000581741333\n",
      "Iteration 61090 Training loss 0.0012656409526243806 Validation loss 0.045614518225193024 Accuracy 0.8888750672340393\n",
      "Iteration 61100 Training loss 0.0012661657528951764 Validation loss 0.04557589814066887 Accuracy 0.8890000581741333\n",
      "Iteration 61110 Training loss 0.0025109914131462574 Validation loss 0.04557080194354057 Accuracy 0.8891250491142273\n",
      "Iteration 61120 Training loss 0.002519341418519616 Validation loss 0.04555274546146393 Accuracy 0.8888750672340393\n",
      "Iteration 61130 Training loss 0.0012671772856265306 Validation loss 0.04556489363312721 Accuracy 0.8885000348091125\n",
      "Iteration 61140 Training loss 0.003761391621083021 Validation loss 0.045585278421640396 Accuracy 0.8890000581741333\n",
      "Iteration 61150 Training loss 1.3733101695834193e-05 Validation loss 0.04563920199871063 Accuracy 0.8888750672340393\n",
      "Iteration 61160 Training loss 0.0012630012352019548 Validation loss 0.04560241848230362 Accuracy 0.8887500166893005\n",
      "Iteration 61170 Training loss 0.0012710854643955827 Validation loss 0.04559836909174919 Accuracy 0.8891250491142273\n",
      "Iteration 61180 Training loss 0.0012623576913028955 Validation loss 0.04563117399811745 Accuracy 0.8893750309944153\n",
      "Iteration 61190 Training loss 0.00126470101531595 Validation loss 0.04559220373630524 Accuracy 0.8895000219345093\n",
      "Iteration 61200 Training loss 1.819154385884758e-05 Validation loss 0.04561489447951317 Accuracy 0.8893750309944153\n",
      "Iteration 61210 Training loss 0.001261908677406609 Validation loss 0.04558756574988365 Accuracy 0.8892500400543213\n",
      "Iteration 61220 Training loss 0.0012653530575335026 Validation loss 0.04560695216059685 Accuracy 0.8892500400543213\n",
      "Iteration 61230 Training loss 1.5098564290383365e-05 Validation loss 0.045605387538671494 Accuracy 0.8892500400543213\n",
      "Iteration 61240 Training loss 0.0037644628901034594 Validation loss 0.045588765293359756 Accuracy 0.8892500400543213\n",
      "Iteration 61250 Training loss 0.0025201188400387764 Validation loss 0.045596420764923096 Accuracy 0.8890000581741333\n",
      "Iteration 61260 Training loss 1.3041241800237913e-05 Validation loss 0.04563962668180466 Accuracy 0.8890000581741333\n",
      "Iteration 61270 Training loss 0.0012607378885149956 Validation loss 0.04560999572277069 Accuracy 0.8890000581741333\n",
      "Iteration 61280 Training loss 0.0025164319667965174 Validation loss 0.04559784755110741 Accuracy 0.8888750672340393\n",
      "Iteration 61290 Training loss 1.7202608432853594e-05 Validation loss 0.04558892548084259 Accuracy 0.8888750672340393\n",
      "Iteration 61300 Training loss 0.0012682635569944978 Validation loss 0.045599423348903656 Accuracy 0.8892500400543213\n",
      "Iteration 61310 Training loss 1.8222704966319725e-05 Validation loss 0.045609500259160995 Accuracy 0.8892500400543213\n",
      "Iteration 61320 Training loss 0.002510702470317483 Validation loss 0.045627910643815994 Accuracy 0.8890000581741333\n",
      "Iteration 61330 Training loss 0.0012654089368879795 Validation loss 0.04562261328101158 Accuracy 0.8891250491142273\n",
      "Iteration 61340 Training loss 0.002516289707273245 Validation loss 0.04560041427612305 Accuracy 0.8887500166893005\n",
      "Iteration 61350 Training loss 0.0025199451483786106 Validation loss 0.04562010616064072 Accuracy 0.8885000348091125\n",
      "Iteration 61360 Training loss 0.0012728014262393117 Validation loss 0.04563393443822861 Accuracy 0.8886250257492065\n",
      "Iteration 61370 Training loss 0.002511502942070365 Validation loss 0.04563728719949722 Accuracy 0.8885000348091125\n",
      "Iteration 61380 Training loss 0.002518400549888611 Validation loss 0.045595888048410416 Accuracy 0.8888750672340393\n",
      "Iteration 61390 Training loss 0.0012670800788328052 Validation loss 0.045568596571683884 Accuracy 0.8892500400543213\n",
      "Iteration 61400 Training loss 0.0012672628508880734 Validation loss 0.04559216648340225 Accuracy 0.8887500166893005\n",
      "Iteration 61410 Training loss 1.7255737475352362e-05 Validation loss 0.04558242857456207 Accuracy 0.8892500400543213\n",
      "Iteration 61420 Training loss 1.2452856935851742e-05 Validation loss 0.045593276619911194 Accuracy 0.8887500166893005\n",
      "Iteration 61430 Training loss 2.344579297641758e-05 Validation loss 0.045580264180898666 Accuracy 0.8892500400543213\n",
      "Iteration 61440 Training loss 0.003777620615437627 Validation loss 0.045593444257974625 Accuracy 0.8890000581741333\n",
      "Iteration 61450 Training loss 0.0025165032129734755 Validation loss 0.04557662084698677 Accuracy 0.8891250491142273\n",
      "Iteration 61460 Training loss 1.797556433302816e-05 Validation loss 0.045593347400426865 Accuracy 0.8892500400543213\n",
      "Iteration 61470 Training loss 0.005020009353756905 Validation loss 0.045586906373500824 Accuracy 0.8895000219345093\n",
      "Iteration 61480 Training loss 0.0012720488011837006 Validation loss 0.04559715464711189 Accuracy 0.8892500400543213\n",
      "Iteration 61490 Training loss 0.0012656463077291846 Validation loss 0.0455971360206604 Accuracy 0.8892500400543213\n",
      "Iteration 61500 Training loss 1.932779559865594e-05 Validation loss 0.04558970034122467 Accuracy 0.8891250491142273\n",
      "Iteration 61510 Training loss 0.002515371423214674 Validation loss 0.04559633135795593 Accuracy 0.8890000581741333\n",
      "Iteration 61520 Training loss 0.0037629834841936827 Validation loss 0.04562419280409813 Accuracy 0.8890000581741333\n",
      "Iteration 61530 Training loss 2.3827224140404724e-05 Validation loss 0.04560267925262451 Accuracy 0.8893750309944153\n",
      "Iteration 61540 Training loss 1.3345902516448405e-05 Validation loss 0.04563629999756813 Accuracy 0.8892500400543213\n",
      "Iteration 61550 Training loss 1.210443315358134e-05 Validation loss 0.045610349625349045 Accuracy 0.8893750309944153\n",
      "Iteration 61560 Training loss 0.001263669808395207 Validation loss 0.045625995844602585 Accuracy 0.8890000581741333\n",
      "Iteration 61570 Training loss 1.6083275113487616e-05 Validation loss 0.045607440173625946 Accuracy 0.8891250491142273\n",
      "Iteration 61580 Training loss 0.0012669855495914817 Validation loss 0.04561043158173561 Accuracy 0.8890000581741333\n",
      "Iteration 61590 Training loss 0.0012715522898361087 Validation loss 0.04560350999236107 Accuracy 0.8888750672340393\n",
      "Iteration 61600 Training loss 0.0025112375151365995 Validation loss 0.04559967666864395 Accuracy 0.8890000581741333\n",
      "Iteration 61610 Training loss 1.5888635971350595e-05 Validation loss 0.04559219628572464 Accuracy 0.8890000581741333\n",
      "Iteration 61620 Training loss 1.8613507563713938e-05 Validation loss 0.045583199709653854 Accuracy 0.8892500400543213\n",
      "Iteration 61630 Training loss 0.003765908069908619 Validation loss 0.04560581594705582 Accuracy 0.8887500166893005\n",
      "Iteration 61640 Training loss 0.0025151558220386505 Validation loss 0.04562514275312424 Accuracy 0.8892500400543213\n",
      "Iteration 61650 Training loss 0.0012759914388880134 Validation loss 0.0455876961350441 Accuracy 0.8892500400543213\n",
      "Iteration 61660 Training loss 0.0012645891401916742 Validation loss 0.04560137540102005 Accuracy 0.8891250491142273\n",
      "Iteration 61670 Training loss 1.366665765090147e-05 Validation loss 0.04562129080295563 Accuracy 0.8890000581741333\n",
      "Iteration 61680 Training loss 0.0012630592100322247 Validation loss 0.04563978686928749 Accuracy 0.8887500166893005\n",
      "Iteration 61690 Training loss 0.002513356739655137 Validation loss 0.04562004283070564 Accuracy 0.8888750672340393\n",
      "Iteration 61700 Training loss 0.0012630835408344865 Validation loss 0.045638177543878555 Accuracy 0.8890000581741333\n",
      "Iteration 61710 Training loss 0.0012673778692260385 Validation loss 0.045624688267707825 Accuracy 0.8888750672340393\n",
      "Iteration 61720 Training loss 1.1595152500376571e-05 Validation loss 0.04563106596469879 Accuracy 0.8886250257492065\n",
      "Iteration 61730 Training loss 2.054250035143923e-05 Validation loss 0.04560581594705582 Accuracy 0.8888750672340393\n",
      "Iteration 61740 Training loss 0.002516799606382847 Validation loss 0.04564417526125908 Accuracy 0.8893750309944153\n",
      "Iteration 61750 Training loss 1.409425567544531e-05 Validation loss 0.04563402011990547 Accuracy 0.8890000581741333\n",
      "Iteration 61760 Training loss 1.1056514267693274e-05 Validation loss 0.04559694603085518 Accuracy 0.8891250491142273\n",
      "Iteration 61770 Training loss 0.0012617834145203233 Validation loss 0.045613039284944534 Accuracy 0.8892500400543213\n",
      "Iteration 61780 Training loss 0.0012664366513490677 Validation loss 0.045610468834638596 Accuracy 0.8891250491142273\n",
      "Iteration 61790 Training loss 0.0012670474825426936 Validation loss 0.045647721737623215 Accuracy 0.8882500529289246\n",
      "Iteration 61800 Training loss 1.946850352396723e-05 Validation loss 0.04561217129230499 Accuracy 0.8892500400543213\n",
      "Iteration 61810 Training loss 0.0025215274654328823 Validation loss 0.045572612434625626 Accuracy 0.8893750309944153\n",
      "Iteration 61820 Training loss 0.0012660265201702714 Validation loss 0.04560650512576103 Accuracy 0.8890000581741333\n",
      "Iteration 61830 Training loss 0.0025144778192043304 Validation loss 0.04559159278869629 Accuracy 0.8890000581741333\n",
      "Iteration 61840 Training loss 0.0025159597862511873 Validation loss 0.045600421726703644 Accuracy 0.8895000219345093\n",
      "Iteration 61850 Training loss 0.001267078099772334 Validation loss 0.04559321701526642 Accuracy 0.8892500400543213\n",
      "Iteration 61860 Training loss 1.4268547602114268e-05 Validation loss 0.045609209686517715 Accuracy 0.8890000581741333\n",
      "Iteration 61870 Training loss 1.6700118067092262e-05 Validation loss 0.045647699385881424 Accuracy 0.8887500166893005\n",
      "Iteration 61880 Training loss 0.0012636765604838729 Validation loss 0.04563094675540924 Accuracy 0.8890000581741333\n",
      "Iteration 61890 Training loss 0.002511264057829976 Validation loss 0.0456041544675827 Accuracy 0.8891250491142273\n",
      "Iteration 61900 Training loss 0.0012641332577914 Validation loss 0.0456123910844326 Accuracy 0.8891250491142273\n",
      "Iteration 61910 Training loss 0.0025176273193210363 Validation loss 0.04561370611190796 Accuracy 0.8891250491142273\n",
      "Iteration 61920 Training loss 1.4115208614384755e-05 Validation loss 0.045609891414642334 Accuracy 0.8887500166893005\n",
      "Iteration 61930 Training loss 1.7184498574351892e-05 Validation loss 0.04565506801009178 Accuracy 0.8885000348091125\n",
      "Iteration 61940 Training loss 0.0037634854670614004 Validation loss 0.045626528561115265 Accuracy 0.8886250257492065\n",
      "Iteration 61950 Training loss 0.002513999817892909 Validation loss 0.0456446073949337 Accuracy 0.8886250257492065\n",
      "Iteration 61960 Training loss 0.0012663438683375716 Validation loss 0.04564080014824867 Accuracy 0.8887500166893005\n",
      "Iteration 61970 Training loss 0.0012712382012978196 Validation loss 0.04563426598906517 Accuracy 0.8888750672340393\n",
      "Iteration 61980 Training loss 0.0012700152583420277 Validation loss 0.04563409462571144 Accuracy 0.8891250491142273\n",
      "Iteration 61990 Training loss 0.003768028225749731 Validation loss 0.04560154676437378 Accuracy 0.8890000581741333\n",
      "Iteration 62000 Training loss 0.0037666752468794584 Validation loss 0.045584846287965775 Accuracy 0.8888750672340393\n",
      "Iteration 62010 Training loss 0.0025151558220386505 Validation loss 0.045630138367414474 Accuracy 0.8895000219345093\n",
      "Iteration 62020 Training loss 0.001266981940716505 Validation loss 0.045608170330524445 Accuracy 0.8888750672340393\n",
      "Iteration 62030 Training loss 0.002517297165468335 Validation loss 0.0456339530646801 Accuracy 0.8891250491142273\n",
      "Iteration 62040 Training loss 0.002516580978408456 Validation loss 0.045655474066734314 Accuracy 0.8885000348091125\n",
      "Iteration 62050 Training loss 0.002517335582524538 Validation loss 0.045654747635126114 Accuracy 0.8885000348091125\n",
      "Iteration 62060 Training loss 0.0025153669994324446 Validation loss 0.04564396291971207 Accuracy 0.8885000348091125\n",
      "Iteration 62070 Training loss 0.0025131436996161938 Validation loss 0.04559994116425514 Accuracy 0.8888750672340393\n",
      "Iteration 62080 Training loss 1.951165722857695e-05 Validation loss 0.04565062373876572 Accuracy 0.8886250257492065\n",
      "Iteration 62090 Training loss 0.0025157888885587454 Validation loss 0.045618265867233276 Accuracy 0.8892500400543213\n",
      "Iteration 62100 Training loss 1.3614639101433568e-05 Validation loss 0.045616649091243744 Accuracy 0.8890000581741333\n",
      "Iteration 62110 Training loss 0.0012661332730203867 Validation loss 0.045622434467077255 Accuracy 0.8886250257492065\n",
      "Iteration 62120 Training loss 0.0012649853015318513 Validation loss 0.04561755806207657 Accuracy 0.8886250257492065\n",
      "Iteration 62130 Training loss 0.0037685371935367584 Validation loss 0.04566812142729759 Accuracy 0.8887500166893005\n",
      "Iteration 62140 Training loss 1.3936508366896305e-05 Validation loss 0.04561033844947815 Accuracy 0.8887500166893005\n",
      "Iteration 62150 Training loss 0.0012619452318176627 Validation loss 0.04562840983271599 Accuracy 0.8886250257492065\n",
      "Iteration 62160 Training loss 0.0012660163920372725 Validation loss 0.045632269233465195 Accuracy 0.8887500166893005\n",
      "Iteration 62170 Training loss 0.0012599356705322862 Validation loss 0.04566424340009689 Accuracy 0.8886250257492065\n",
      "Iteration 62180 Training loss 0.0012619058834388852 Validation loss 0.045643337070941925 Accuracy 0.8887500166893005\n",
      "Iteration 62190 Training loss 1.4382884728547651e-05 Validation loss 0.045667946338653564 Accuracy 0.8881250619888306\n",
      "Iteration 62200 Training loss 0.0012635620078071952 Validation loss 0.04565101116895676 Accuracy 0.8888750672340393\n",
      "Iteration 62210 Training loss 2.0866742488578893e-05 Validation loss 0.045626506209373474 Accuracy 0.8888750672340393\n",
      "Iteration 62220 Training loss 1.839107062551193e-05 Validation loss 0.045614518225193024 Accuracy 0.8888750672340393\n",
      "Iteration 62230 Training loss 0.0037718515377491713 Validation loss 0.04560316354036331 Accuracy 0.8888750672340393\n",
      "Iteration 62240 Training loss 1.8990958778886124e-05 Validation loss 0.04560147970914841 Accuracy 0.8890000581741333\n",
      "Iteration 62250 Training loss 0.0012643375666812062 Validation loss 0.04565022140741348 Accuracy 0.8890000581741333\n",
      "Iteration 62260 Training loss 0.0037638640496879816 Validation loss 0.04562259092926979 Accuracy 0.8893750309944153\n",
      "Iteration 62270 Training loss 1.5087151041370817e-05 Validation loss 0.045610424131155014 Accuracy 0.8883750438690186\n",
      "Iteration 62280 Training loss 0.003768296679481864 Validation loss 0.04561975970864296 Accuracy 0.8887500166893005\n",
      "Iteration 62290 Training loss 1.9323833839735016e-05 Validation loss 0.04560309648513794 Accuracy 0.8890000581741333\n",
      "Iteration 62300 Training loss 0.0012678607599809766 Validation loss 0.045626282691955566 Accuracy 0.8893750309944153\n",
      "Iteration 62310 Training loss 0.002520797774195671 Validation loss 0.04561581462621689 Accuracy 0.8888750672340393\n",
      "Iteration 62320 Training loss 0.0012735668569803238 Validation loss 0.04561435803771019 Accuracy 0.8886250257492065\n",
      "Iteration 62330 Training loss 0.0037626945413649082 Validation loss 0.04560733214020729 Accuracy 0.8886250257492065\n",
      "Iteration 62340 Training loss 0.0012620958732441068 Validation loss 0.0456516407430172 Accuracy 0.8886250257492065\n",
      "Iteration 62350 Training loss 2.0887939172098413e-05 Validation loss 0.045626450330019 Accuracy 0.8885000348091125\n",
      "Iteration 62360 Training loss 0.002517959801480174 Validation loss 0.04564318433403969 Accuracy 0.8887500166893005\n",
      "Iteration 62370 Training loss 0.0012613878352567554 Validation loss 0.04561218246817589 Accuracy 0.8886250257492065\n",
      "Iteration 62380 Training loss 0.0012647990370169282 Validation loss 0.045640137046575546 Accuracy 0.8890000581741333\n",
      "Iteration 62390 Training loss 0.0012661012588068843 Validation loss 0.04561698064208031 Accuracy 0.8888750672340393\n",
      "Iteration 62400 Training loss 0.002523441333323717 Validation loss 0.04561029002070427 Accuracy 0.8887500166893005\n",
      "Iteration 62410 Training loss 0.0025127308908849955 Validation loss 0.045618463307619095 Accuracy 0.8892500400543213\n",
      "Iteration 62420 Training loss 0.003768460126593709 Validation loss 0.045623887330293655 Accuracy 0.8890000581741333\n",
      "Iteration 62430 Training loss 1.7080430552596226e-05 Validation loss 0.04559681937098503 Accuracy 0.8887500166893005\n",
      "Iteration 62440 Training loss 0.001264120452105999 Validation loss 0.04563959687948227 Accuracy 0.8886250257492065\n",
      "Iteration 62450 Training loss 0.007514087948948145 Validation loss 0.04563925042748451 Accuracy 0.8890000581741333\n",
      "Iteration 62460 Training loss 0.0012649965938180685 Validation loss 0.04563680663704872 Accuracy 0.8888750672340393\n",
      "Iteration 62470 Training loss 0.0037648158613592386 Validation loss 0.04562144726514816 Accuracy 0.8892500400543213\n",
      "Iteration 62480 Training loss 0.003771541640162468 Validation loss 0.0456235371530056 Accuracy 0.8891250491142273\n",
      "Iteration 62490 Training loss 0.002515431959182024 Validation loss 0.045614469796419144 Accuracy 0.8891250491142273\n",
      "Iteration 62500 Training loss 8.837857421895023e-06 Validation loss 0.04563073068857193 Accuracy 0.8886250257492065\n",
      "Iteration 62510 Training loss 1.409910782967927e-05 Validation loss 0.045649122446775436 Accuracy 0.8885000348091125\n",
      "Iteration 62520 Training loss 0.0012662125518545508 Validation loss 0.0456131249666214 Accuracy 0.8890000581741333\n",
      "Iteration 62530 Training loss 0.0012634886661544442 Validation loss 0.045597344636917114 Accuracy 0.8888750672340393\n",
      "Iteration 62540 Training loss 0.0012637738836929202 Validation loss 0.04559504985809326 Accuracy 0.8888750672340393\n",
      "Iteration 62550 Training loss 1.4650318007625174e-05 Validation loss 0.045648083090782166 Accuracy 0.8891250491142273\n",
      "Iteration 62560 Training loss 0.0012643950758501887 Validation loss 0.045647408813238144 Accuracy 0.8890000581741333\n",
      "Iteration 62570 Training loss 0.0012645371025428176 Validation loss 0.045627299696207047 Accuracy 0.8888750672340393\n",
      "Iteration 62580 Training loss 0.0012587689561769366 Validation loss 0.045620840042829514 Accuracy 0.8887500166893005\n",
      "Iteration 62590 Training loss 0.006261082831770182 Validation loss 0.045632295310497284 Accuracy 0.8888750672340393\n",
      "Iteration 62600 Training loss 1.43658271554159e-05 Validation loss 0.04563308134675026 Accuracy 0.8888750672340393\n",
      "Iteration 62610 Training loss 1.312024051003391e-05 Validation loss 0.045639779418706894 Accuracy 0.8890000581741333\n",
      "Iteration 62620 Training loss 1.6470110494992696e-05 Validation loss 0.045613519847393036 Accuracy 0.8891250491142273\n",
      "Iteration 62630 Training loss 0.005010461900383234 Validation loss 0.04566575959324837 Accuracy 0.8890000581741333\n",
      "Iteration 62640 Training loss 0.0025267452001571655 Validation loss 0.045646388083696365 Accuracy 0.8892500400543213\n",
      "Iteration 62650 Training loss 0.0012738510267809033 Validation loss 0.04564821720123291 Accuracy 0.8890000581741333\n",
      "Iteration 62660 Training loss 2.0988461983506568e-05 Validation loss 0.04563809186220169 Accuracy 0.8891250491142273\n",
      "Iteration 62670 Training loss 1.1056046787416562e-05 Validation loss 0.045646242797374725 Accuracy 0.8888750672340393\n",
      "Iteration 62680 Training loss 0.0050144619308412075 Validation loss 0.04562899470329285 Accuracy 0.8887500166893005\n",
      "Iteration 62690 Training loss 0.0037652018945664167 Validation loss 0.04562120884656906 Accuracy 0.8887500166893005\n",
      "Iteration 62700 Training loss 2.2458787498180754e-05 Validation loss 0.04567767679691315 Accuracy 0.8887500166893005\n",
      "Iteration 62710 Training loss 1.3075139577267691e-05 Validation loss 0.04560926556587219 Accuracy 0.8888750672340393\n",
      "Iteration 62720 Training loss 0.0012681236257776618 Validation loss 0.045605409890413284 Accuracy 0.8890000581741333\n",
      "Iteration 62730 Training loss 0.0012672835728153586 Validation loss 0.045657746493816376 Accuracy 0.8890000581741333\n",
      "Iteration 62740 Training loss 1.771254937921185e-05 Validation loss 0.045652009546756744 Accuracy 0.8890000581741333\n",
      "Iteration 62750 Training loss 0.0012667279224842787 Validation loss 0.04561273381114006 Accuracy 0.8888750672340393\n",
      "Iteration 62760 Training loss 2.13808125408832e-05 Validation loss 0.04561874642968178 Accuracy 0.8887500166893005\n",
      "Iteration 62770 Training loss 0.002511618658900261 Validation loss 0.045623376965522766 Accuracy 0.8887500166893005\n",
      "Iteration 62780 Training loss 1.621945557417348e-05 Validation loss 0.04562130942940712 Accuracy 0.8885000348091125\n",
      "Iteration 62790 Training loss 0.0012647095136344433 Validation loss 0.045601412653923035 Accuracy 0.8885000348091125\n",
      "Iteration 62800 Training loss 1.9311548385303468e-05 Validation loss 0.04561997950077057 Accuracy 0.8887500166893005\n",
      "Iteration 62810 Training loss 0.002517274348065257 Validation loss 0.045636873692274094 Accuracy 0.8887500166893005\n",
      "Iteration 62820 Training loss 0.0012619311455637217 Validation loss 0.045604001730680466 Accuracy 0.8887500166893005\n",
      "Iteration 62830 Training loss 0.0012667436385527253 Validation loss 0.045611169189214706 Accuracy 0.8883750438690186\n",
      "Iteration 62840 Training loss 0.001261568977497518 Validation loss 0.04565686732530594 Accuracy 0.8886250257492065\n",
      "Iteration 62850 Training loss 0.0012639907654374838 Validation loss 0.045621082186698914 Accuracy 0.8891250491142273\n",
      "Iteration 62860 Training loss 1.4068561540625524e-05 Validation loss 0.045650605112314224 Accuracy 0.8886250257492065\n",
      "Iteration 62870 Training loss 0.0012630718993023038 Validation loss 0.0456637442111969 Accuracy 0.8888750672340393\n",
      "Iteration 62880 Training loss 1.3461586604535114e-05 Validation loss 0.04566139727830887 Accuracy 0.8886250257492065\n",
      "Iteration 62890 Training loss 0.0012686375994235277 Validation loss 0.045656319707632065 Accuracy 0.8886250257492065\n",
      "Iteration 62900 Training loss 1.696162871667184e-05 Validation loss 0.045670654624700546 Accuracy 0.8887500166893005\n",
      "Iteration 62910 Training loss 1.475664885219885e-05 Validation loss 0.04567920044064522 Accuracy 0.8887500166893005\n",
      "Iteration 62920 Training loss 1.3759618923359085e-05 Validation loss 0.045659665018320084 Accuracy 0.8888750672340393\n",
      "Iteration 62930 Training loss 1.4016123714100104e-05 Validation loss 0.04562689736485481 Accuracy 0.8890000581741333\n",
      "Iteration 62940 Training loss 0.0012686684494838119 Validation loss 0.045620210468769073 Accuracy 0.8895000219345093\n",
      "Iteration 62950 Training loss 0.002518681576475501 Validation loss 0.045652568340301514 Accuracy 0.8892500400543213\n",
      "Iteration 62960 Training loss 0.0012633285950869322 Validation loss 0.04562816023826599 Accuracy 0.8893750309944153\n",
      "Iteration 62970 Training loss 0.0012637916952371597 Validation loss 0.045627571642398834 Accuracy 0.8890000581741333\n",
      "Iteration 62980 Training loss 1.328509006270906e-05 Validation loss 0.04564126953482628 Accuracy 0.8888750672340393\n",
      "Iteration 62990 Training loss 1.328799407929182e-05 Validation loss 0.04562397301197052 Accuracy 0.8887500166893005\n",
      "Iteration 63000 Training loss 0.0012695572804659605 Validation loss 0.045669086277484894 Accuracy 0.8888750672340393\n",
      "Iteration 63010 Training loss 1.461569991079159e-05 Validation loss 0.045650679618120193 Accuracy 0.8890000581741333\n",
      "Iteration 63020 Training loss 1.8282007658854127e-05 Validation loss 0.04563884437084198 Accuracy 0.8888750672340393\n",
      "Iteration 63030 Training loss 1.3711573046748526e-05 Validation loss 0.04562606289982796 Accuracy 0.8888750672340393\n",
      "Iteration 63040 Training loss 0.0025151576846837997 Validation loss 0.04566873237490654 Accuracy 0.8883750438690186\n",
      "Iteration 63050 Training loss 0.001269989530555904 Validation loss 0.045630309730768204 Accuracy 0.8891250491142273\n",
      "Iteration 63060 Training loss 0.001265340717509389 Validation loss 0.045625247061252594 Accuracy 0.8885000348091125\n",
      "Iteration 63070 Training loss 2.3663516913075e-05 Validation loss 0.045644812285900116 Accuracy 0.8887500166893005\n",
      "Iteration 63080 Training loss 0.001261610072106123 Validation loss 0.0456363670527935 Accuracy 0.8887500166893005\n",
      "Iteration 63090 Training loss 0.0012640597997233272 Validation loss 0.0456315279006958 Accuracy 0.8886250257492065\n",
      "Iteration 63100 Training loss 0.0025129790883511305 Validation loss 0.04564427584409714 Accuracy 0.8888750672340393\n",
      "Iteration 63110 Training loss 0.0012651723809540272 Validation loss 0.045642271637916565 Accuracy 0.8890000581741333\n",
      "Iteration 63120 Training loss 1.1557023753994144e-05 Validation loss 0.04562120884656906 Accuracy 0.8887500166893005\n",
      "Iteration 63130 Training loss 0.0037648326251655817 Validation loss 0.045610662549734116 Accuracy 0.8886250257492065\n",
      "Iteration 63140 Training loss 1.639961737964768e-05 Validation loss 0.0456300787627697 Accuracy 0.8890000581741333\n",
      "Iteration 63150 Training loss 0.001267692307010293 Validation loss 0.04566548392176628 Accuracy 0.8887500166893005\n",
      "Iteration 63160 Training loss 0.005011292640119791 Validation loss 0.04562513902783394 Accuracy 0.8892500400543213\n",
      "Iteration 63170 Training loss 0.001263418118469417 Validation loss 0.045621246099472046 Accuracy 0.8891250491142273\n",
      "Iteration 63180 Training loss 0.0012654358288273215 Validation loss 0.04562516137957573 Accuracy 0.8891250491142273\n",
      "Iteration 63190 Training loss 1.4665364687971305e-05 Validation loss 0.04564187675714493 Accuracy 0.8890000581741333\n",
      "Iteration 63200 Training loss 0.0012640373315662146 Validation loss 0.04563434049487114 Accuracy 0.8891250491142273\n",
      "Iteration 63210 Training loss 1.899202834465541e-05 Validation loss 0.04566376656293869 Accuracy 0.8887500166893005\n",
      "Iteration 63220 Training loss 0.0012625224189832807 Validation loss 0.045620936900377274 Accuracy 0.8887500166893005\n",
      "Iteration 63230 Training loss 0.0012645418755710125 Validation loss 0.045638229697942734 Accuracy 0.8887500166893005\n",
      "Iteration 63240 Training loss 0.0012612927239388227 Validation loss 0.045640237629413605 Accuracy 0.8887500166893005\n",
      "Iteration 63250 Training loss 2.2446125512942672e-05 Validation loss 0.04563802480697632 Accuracy 0.8887500166893005\n",
      "Iteration 63260 Training loss 2.1475507310242392e-05 Validation loss 0.04563634842634201 Accuracy 0.8887500166893005\n",
      "Iteration 63270 Training loss 1.6221503756241873e-05 Validation loss 0.045651331543922424 Accuracy 0.8891250491142273\n",
      "Iteration 63280 Training loss 0.0012709727743640542 Validation loss 0.04563090577721596 Accuracy 0.8888750672340393\n",
      "Iteration 63290 Training loss 0.00251608993858099 Validation loss 0.04565609619021416 Accuracy 0.8890000581741333\n",
      "Iteration 63300 Training loss 1.3065349776297808e-05 Validation loss 0.04561973735690117 Accuracy 0.8887500166893005\n",
      "Iteration 63310 Training loss 0.002521074377000332 Validation loss 0.045657698065042496 Accuracy 0.8887500166893005\n",
      "Iteration 63320 Training loss 1.8045020624413155e-05 Validation loss 0.045621875673532486 Accuracy 0.8888750672340393\n",
      "Iteration 63330 Training loss 1.881823300209362e-05 Validation loss 0.04560345038771629 Accuracy 0.8891250491142273\n",
      "Iteration 63340 Training loss 0.0025157234631478786 Validation loss 0.04560753330588341 Accuracy 0.8888750672340393\n",
      "Iteration 63350 Training loss 0.001267902785912156 Validation loss 0.04561219364404678 Accuracy 0.8891250491142273\n",
      "Iteration 63360 Training loss 0.0025154585018754005 Validation loss 0.04561304673552513 Accuracy 0.8893750309944153\n",
      "Iteration 63370 Training loss 1.0629703865561169e-05 Validation loss 0.04562387987971306 Accuracy 0.8893750309944153\n",
      "Iteration 63380 Training loss 1.8146101865568198e-05 Validation loss 0.04561777785420418 Accuracy 0.8887500166893005\n",
      "Iteration 63390 Training loss 1.70529729075497e-05 Validation loss 0.04566610977053642 Accuracy 0.8891250491142273\n",
      "Iteration 63400 Training loss 0.0012623370857909322 Validation loss 0.04563526436686516 Accuracy 0.8891250491142273\n",
      "Iteration 63410 Training loss 0.0012664139503613114 Validation loss 0.045641109347343445 Accuracy 0.8891250491142273\n",
      "Iteration 63420 Training loss 0.005012184847146273 Validation loss 0.045616280287504196 Accuracy 0.8890000581741333\n",
      "Iteration 63430 Training loss 0.0025125460233539343 Validation loss 0.04562532156705856 Accuracy 0.8893750309944153\n",
      "Iteration 63440 Training loss 0.00251080933958292 Validation loss 0.04564142972230911 Accuracy 0.8891250491142273\n",
      "Iteration 63450 Training loss 1.8049455320579e-05 Validation loss 0.045641086995601654 Accuracy 0.8888750672340393\n",
      "Iteration 63460 Training loss 1.502124359831214e-05 Validation loss 0.04564524441957474 Accuracy 0.8890000581741333\n",
      "Iteration 63470 Training loss 2.1766050849691965e-05 Validation loss 0.04562763497233391 Accuracy 0.8891250491142273\n",
      "Iteration 63480 Training loss 1.4045988791622221e-05 Validation loss 0.045638203620910645 Accuracy 0.8892500400543213\n",
      "Iteration 63490 Training loss 0.002513492712751031 Validation loss 0.0456320121884346 Accuracy 0.8891250491142273\n",
      "Iteration 63500 Training loss 0.0012618605978786945 Validation loss 0.04562896490097046 Accuracy 0.8888750672340393\n",
      "Iteration 63510 Training loss 0.0012674418976530433 Validation loss 0.04568004980683327 Accuracy 0.8891250491142273\n",
      "Iteration 63520 Training loss 0.001265580183826387 Validation loss 0.04567965492606163 Accuracy 0.8888750672340393\n",
      "Iteration 63530 Training loss 0.0012710163136944175 Validation loss 0.04565228521823883 Accuracy 0.8888750672340393\n",
      "Iteration 63540 Training loss 1.918222551466897e-05 Validation loss 0.04563719779253006 Accuracy 0.8886250257492065\n",
      "Iteration 63550 Training loss 0.0012611806159839034 Validation loss 0.0456765778362751 Accuracy 0.8887500166893005\n",
      "Iteration 63560 Training loss 0.002514803549274802 Validation loss 0.0456698015332222 Accuracy 0.8888750672340393\n",
      "Iteration 63570 Training loss 0.001265450264327228 Validation loss 0.04567630961537361 Accuracy 0.8882500529289246\n",
      "Iteration 63580 Training loss 0.0025168524589389563 Validation loss 0.04565788432955742 Accuracy 0.8888750672340393\n",
      "Iteration 63590 Training loss 0.0012651358265429735 Validation loss 0.04562922194600105 Accuracy 0.8891250491142273\n",
      "Iteration 63600 Training loss 0.005015410017222166 Validation loss 0.045645635575056076 Accuracy 0.8892500400543213\n",
      "Iteration 63610 Training loss 0.0037675504572689533 Validation loss 0.04564231261610985 Accuracy 0.8891250491142273\n",
      "Iteration 63620 Training loss 0.005014204420149326 Validation loss 0.04563937708735466 Accuracy 0.8891250491142273\n",
      "Iteration 63630 Training loss 0.0012657820479944348 Validation loss 0.04564680904150009 Accuracy 0.8892500400543213\n",
      "Iteration 63640 Training loss 0.002518711145967245 Validation loss 0.045662716031074524 Accuracy 0.8891250491142273\n",
      "Iteration 63650 Training loss 0.0012676321202889085 Validation loss 0.04563279077410698 Accuracy 0.8892500400543213\n",
      "Iteration 63660 Training loss 0.0012687117559835315 Validation loss 0.04561825096607208 Accuracy 0.8892500400543213\n",
      "Iteration 63670 Training loss 2.188676444347948e-05 Validation loss 0.04563194513320923 Accuracy 0.8891250491142273\n",
      "Iteration 63680 Training loss 1.4506906154565513e-05 Validation loss 0.045628949999809265 Accuracy 0.8893750309944153\n",
      "Iteration 63690 Training loss 0.0012679543578997254 Validation loss 0.04564743861556053 Accuracy 0.8890000581741333\n",
      "Iteration 63700 Training loss 0.0012661819346249104 Validation loss 0.04562034457921982 Accuracy 0.8890000581741333\n",
      "Iteration 63710 Training loss 1.4439491678785998e-05 Validation loss 0.04563642665743828 Accuracy 0.8895000219345093\n",
      "Iteration 63720 Training loss 0.005014164373278618 Validation loss 0.04565107822418213 Accuracy 0.8891250491142273\n",
      "Iteration 63730 Training loss 0.0025148759596049786 Validation loss 0.04563106596469879 Accuracy 0.8891250491142273\n",
      "Iteration 63740 Training loss 0.0025157216005027294 Validation loss 0.045647989958524704 Accuracy 0.8888750672340393\n",
      "Iteration 63750 Training loss 0.0012633433798328042 Validation loss 0.045664604753255844 Accuracy 0.8891250491142273\n",
      "Iteration 63760 Training loss 1.9100090867141262e-05 Validation loss 0.045651208609342575 Accuracy 0.8891250491142273\n",
      "Iteration 63770 Training loss 0.002510990481823683 Validation loss 0.045645274221897125 Accuracy 0.8892500400543213\n",
      "Iteration 63780 Training loss 1.3715150998905301e-05 Validation loss 0.0456247441470623 Accuracy 0.8891250491142273\n",
      "Iteration 63790 Training loss 0.0012625160161405802 Validation loss 0.045600779354572296 Accuracy 0.8892500400543213\n",
      "Iteration 63800 Training loss 0.0025160410441458225 Validation loss 0.0456056222319603 Accuracy 0.889750063419342\n",
      "Iteration 63810 Training loss 1.9832084944937378e-05 Validation loss 0.04562860727310181 Accuracy 0.8895000219345093\n",
      "Iteration 63820 Training loss 0.0012637345353141427 Validation loss 0.04562918841838837 Accuracy 0.8892500400543213\n",
      "Iteration 63830 Training loss 2.477788984833751e-05 Validation loss 0.04566139727830887 Accuracy 0.8891250491142273\n",
      "Iteration 63840 Training loss 2.0385106836329214e-05 Validation loss 0.04566981643438339 Accuracy 0.8888750672340393\n",
      "Iteration 63850 Training loss 0.002515470376238227 Validation loss 0.04565214738249779 Accuracy 0.8886250257492065\n",
      "Iteration 63860 Training loss 2.517795110179577e-05 Validation loss 0.04565192013978958 Accuracy 0.8887500166893005\n",
      "Iteration 63870 Training loss 0.003769678296521306 Validation loss 0.04565010219812393 Accuracy 0.8885000348091125\n",
      "Iteration 63880 Training loss 0.0012641450157389045 Validation loss 0.04567846283316612 Accuracy 0.8888750672340393\n",
      "Iteration 63890 Training loss 1.5827652532607317e-05 Validation loss 0.045630309730768204 Accuracy 0.8888750672340393\n",
      "Iteration 63900 Training loss 0.001268323278054595 Validation loss 0.04565402492880821 Accuracy 0.8886250257492065\n",
      "Iteration 63910 Training loss 0.0012658585328608751 Validation loss 0.045650988817214966 Accuracy 0.8887500166893005\n",
      "Iteration 63920 Training loss 0.001269304077140987 Validation loss 0.045659758150577545 Accuracy 0.8882500529289246\n",
      "Iteration 63930 Training loss 0.0012668599374592304 Validation loss 0.04562968388199806 Accuracy 0.8888750672340393\n",
      "Iteration 63940 Training loss 0.002515781205147505 Validation loss 0.04562770202755928 Accuracy 0.8888750672340393\n",
      "Iteration 63950 Training loss 0.0012617436004802585 Validation loss 0.04565100744366646 Accuracy 0.8888750672340393\n",
      "Iteration 63960 Training loss 0.0012651322176679969 Validation loss 0.04563398286700249 Accuracy 0.8890000581741333\n",
      "Iteration 63970 Training loss 1.0810315870912746e-05 Validation loss 0.04559672623872757 Accuracy 0.8893750309944153\n",
      "Iteration 63980 Training loss 1.8336850189371035e-05 Validation loss 0.04560348764061928 Accuracy 0.8891250491142273\n",
      "Iteration 63990 Training loss 1.2975684512639418e-05 Validation loss 0.04565286636352539 Accuracy 0.8890000581741333\n",
      "Iteration 64000 Training loss 1.1973732398473658e-05 Validation loss 0.045635886490345 Accuracy 0.8890000581741333\n",
      "Iteration 64010 Training loss 2.598502760520205e-05 Validation loss 0.045625656843185425 Accuracy 0.8891250491142273\n",
      "Iteration 64020 Training loss 0.003764672437682748 Validation loss 0.04562677815556526 Accuracy 0.8891250491142273\n",
      "Iteration 64030 Training loss 0.0012662276858463883 Validation loss 0.045663684606552124 Accuracy 0.8887500166893005\n",
      "Iteration 64040 Training loss 0.0012701537925750017 Validation loss 0.04563356563448906 Accuracy 0.8891250491142273\n",
      "Iteration 64050 Training loss 0.002511713420972228 Validation loss 0.045635633170604706 Accuracy 0.8886250257492065\n",
      "Iteration 64060 Training loss 1.4490884495899081e-05 Validation loss 0.0456562377512455 Accuracy 0.8890000581741333\n",
      "Iteration 64070 Training loss 1.545407394587528e-05 Validation loss 0.04565589502453804 Accuracy 0.8890000581741333\n",
      "Iteration 64080 Training loss 0.0012696291087195277 Validation loss 0.045641280710697174 Accuracy 0.8886250257492065\n",
      "Iteration 64090 Training loss 0.0012641645735129714 Validation loss 0.04562464356422424 Accuracy 0.8885000348091125\n",
      "Iteration 64100 Training loss 0.001268846681341529 Validation loss 0.04560080170631409 Accuracy 0.8888750672340393\n",
      "Iteration 64110 Training loss 0.0012664763489738107 Validation loss 0.04561987146735191 Accuracy 0.8891250491142273\n",
      "Iteration 64120 Training loss 0.0037654314655810595 Validation loss 0.045616671442985535 Accuracy 0.8888750672340393\n",
      "Iteration 64130 Training loss 1.5231931683956645e-05 Validation loss 0.04562205821275711 Accuracy 0.8892500400543213\n",
      "Iteration 64140 Training loss 2.16476710193092e-05 Validation loss 0.045672766864299774 Accuracy 0.8887500166893005\n",
      "Iteration 64150 Training loss 1.643419818719849e-05 Validation loss 0.045614343136548996 Accuracy 0.8895000219345093\n",
      "Iteration 64160 Training loss 0.0012646057875826955 Validation loss 0.04563680663704872 Accuracy 0.8895000219345093\n",
      "Iteration 64170 Training loss 2.1830461264471523e-05 Validation loss 0.04564695805311203 Accuracy 0.8891250491142273\n",
      "Iteration 64180 Training loss 0.0012640382628887892 Validation loss 0.0456552691757679 Accuracy 0.8893750309944153\n",
      "Iteration 64190 Training loss 0.0012615213636308908 Validation loss 0.04565274342894554 Accuracy 0.8895000219345093\n",
      "Iteration 64200 Training loss 1.0809629202412907e-05 Validation loss 0.045675646513700485 Accuracy 0.8895000219345093\n",
      "Iteration 64210 Training loss 0.002512529492378235 Validation loss 0.04563034325838089 Accuracy 0.8893750309944153\n",
      "Iteration 64220 Training loss 1.0872594430111349e-05 Validation loss 0.045632243156433105 Accuracy 0.8892500400543213\n",
      "Iteration 64230 Training loss 0.0012638999614864588 Validation loss 0.04560602456331253 Accuracy 0.8891250491142273\n",
      "Iteration 64240 Training loss 0.0037667874712496996 Validation loss 0.04561587795615196 Accuracy 0.8892500400543213\n",
      "Iteration 64250 Training loss 1.424330457666656e-05 Validation loss 0.04562700539827347 Accuracy 0.8891250491142273\n",
      "Iteration 64260 Training loss 0.0012664125533774495 Validation loss 0.04563748836517334 Accuracy 0.8893750309944153\n",
      "Iteration 64270 Training loss 0.0025231060571968555 Validation loss 0.04567362368106842 Accuracy 0.8893750309944153\n",
      "Iteration 64280 Training loss 1.731221163936425e-05 Validation loss 0.045689281076192856 Accuracy 0.8892500400543213\n",
      "Iteration 64290 Training loss 0.0012676347978413105 Validation loss 0.045646145939826965 Accuracy 0.8892500400543213\n",
      "Iteration 64300 Training loss 0.001262739533558488 Validation loss 0.045636147260665894 Accuracy 0.8891250491142273\n",
      "Iteration 64310 Training loss 0.0025158640928566456 Validation loss 0.04563368484377861 Accuracy 0.8892500400543213\n",
      "Iteration 64320 Training loss 1.8704342437558807e-05 Validation loss 0.04565064236521721 Accuracy 0.8890000581741333\n",
      "Iteration 64330 Training loss 0.0037658740766346455 Validation loss 0.045648809522390366 Accuracy 0.8890000581741333\n",
      "Iteration 64340 Training loss 0.001265119411982596 Validation loss 0.045654766261577606 Accuracy 0.8895000219345093\n",
      "Iteration 64350 Training loss 0.0012602153001353145 Validation loss 0.04564708098769188 Accuracy 0.8891250491142273\n",
      "Iteration 64360 Training loss 0.0012671208241954446 Validation loss 0.045657381415367126 Accuracy 0.8888750672340393\n",
      "Iteration 64370 Training loss 1.271790824830532e-05 Validation loss 0.0456617996096611 Accuracy 0.8891250491142273\n",
      "Iteration 64380 Training loss 0.0037677614018321037 Validation loss 0.04565679281949997 Accuracy 0.8890000581741333\n",
      "Iteration 64390 Training loss 0.0025147940032184124 Validation loss 0.04570150747895241 Accuracy 0.8890000581741333\n",
      "Iteration 64400 Training loss 0.0012700851075351238 Validation loss 0.045665714889764786 Accuracy 0.8887500166893005\n",
      "Iteration 64410 Training loss 1.1647570318018552e-05 Validation loss 0.04567711800336838 Accuracy 0.8892500400543213\n",
      "Iteration 64420 Training loss 1.6330914149875753e-05 Validation loss 0.04568563401699066 Accuracy 0.8891250491142273\n",
      "Iteration 64430 Training loss 1.3943111298431177e-05 Validation loss 0.045657169073820114 Accuracy 0.8893750309944153\n",
      "Iteration 64440 Training loss 0.0012677311897277832 Validation loss 0.0456467904150486 Accuracy 0.8892500400543213\n",
      "Iteration 64450 Training loss 0.001271729706786573 Validation loss 0.04566696286201477 Accuracy 0.8893750309944153\n",
      "Iteration 64460 Training loss 0.0012644397793337703 Validation loss 0.0456838458776474 Accuracy 0.8893750309944153\n",
      "Iteration 64470 Training loss 0.0012647120747715235 Validation loss 0.04565594717860222 Accuracy 0.8891250491142273\n",
      "Iteration 64480 Training loss 0.0012608650140464306 Validation loss 0.045684169977903366 Accuracy 0.8892500400543213\n",
      "Iteration 64490 Training loss 1.5517724023084156e-05 Validation loss 0.04570489376783371 Accuracy 0.8893750309944153\n",
      "Iteration 64500 Training loss 0.002511819824576378 Validation loss 0.04567153379321098 Accuracy 0.8891250491142273\n",
      "Iteration 64510 Training loss 0.0012669163988903165 Validation loss 0.04569488391280174 Accuracy 0.8888750672340393\n",
      "Iteration 64520 Training loss 0.0012592814164236188 Validation loss 0.04566274583339691 Accuracy 0.8891250491142273\n",
      "Iteration 64530 Training loss 0.0012682348024100065 Validation loss 0.04567645117640495 Accuracy 0.8886250257492065\n",
      "Iteration 64540 Training loss 1.6636580767226405e-05 Validation loss 0.04567126929759979 Accuracy 0.8888750672340393\n",
      "Iteration 64550 Training loss 0.0012687590206041932 Validation loss 0.04565760865807533 Accuracy 0.8888750672340393\n",
      "Iteration 64560 Training loss 0.002512433798983693 Validation loss 0.04565154388546944 Accuracy 0.8890000581741333\n",
      "Iteration 64570 Training loss 0.0012660453794524074 Validation loss 0.045640382915735245 Accuracy 0.8891250491142273\n",
      "Iteration 64580 Training loss 0.0025167218409478664 Validation loss 0.045634675770998 Accuracy 0.8890000581741333\n",
      "Iteration 64590 Training loss 0.0025135630276054144 Validation loss 0.04564949870109558 Accuracy 0.8890000581741333\n",
      "Iteration 64600 Training loss 0.001262884819880128 Validation loss 0.04566362872719765 Accuracy 0.8890000581741333\n",
      "Iteration 64610 Training loss 1.6092279111035168e-05 Validation loss 0.045652780681848526 Accuracy 0.8891250491142273\n",
      "Iteration 64620 Training loss 1.9675097064464353e-05 Validation loss 0.04568949341773987 Accuracy 0.8890000581741333\n",
      "Iteration 64630 Training loss 1.769494883774314e-05 Validation loss 0.04569822922348976 Accuracy 0.8885000348091125\n",
      "Iteration 64640 Training loss 0.0012668026611208916 Validation loss 0.045643698424100876 Accuracy 0.8887500166893005\n",
      "Iteration 64650 Training loss 1.4860315786791034e-05 Validation loss 0.045637164264917374 Accuracy 0.8888750672340393\n",
      "Iteration 64660 Training loss 0.0012597421882674098 Validation loss 0.04564282298088074 Accuracy 0.8892500400543213\n",
      "Iteration 64670 Training loss 1.3005195796722546e-05 Validation loss 0.04566394165158272 Accuracy 0.8886250257492065\n",
      "Iteration 64680 Training loss 2.0763323846040294e-05 Validation loss 0.045632608234882355 Accuracy 0.8890000581741333\n",
      "Iteration 64690 Training loss 0.001271081273443997 Validation loss 0.04564151167869568 Accuracy 0.8885000348091125\n",
      "Iteration 64700 Training loss 0.0025153153110295534 Validation loss 0.04565124213695526 Accuracy 0.8888750672340393\n",
      "Iteration 64710 Training loss 0.002516480628401041 Validation loss 0.04564410820603371 Accuracy 0.8887500166893005\n",
      "Iteration 64720 Training loss 0.0012666425900533795 Validation loss 0.04563547670841217 Accuracy 0.8886250257492065\n",
      "Iteration 64730 Training loss 0.0012629092670977116 Validation loss 0.045653391629457474 Accuracy 0.8890000581741333\n",
      "Iteration 64740 Training loss 0.0012601895723491907 Validation loss 0.045673828572034836 Accuracy 0.8887500166893005\n",
      "Iteration 64750 Training loss 0.0012649958953261375 Validation loss 0.045664019882678986 Accuracy 0.8885000348091125\n",
      "Iteration 64760 Training loss 0.0025106098037213087 Validation loss 0.04568221792578697 Accuracy 0.8885000348091125\n",
      "Iteration 64770 Training loss 0.0012651911238208413 Validation loss 0.045681145042181015 Accuracy 0.8886250257492065\n",
      "Iteration 64780 Training loss 0.0012636256869882345 Validation loss 0.045657165348529816 Accuracy 0.8886250257492065\n",
      "Iteration 64790 Training loss 0.0012664056848734617 Validation loss 0.04566555097699165 Accuracy 0.8886250257492065\n",
      "Iteration 64800 Training loss 1.603495911695063e-05 Validation loss 0.045635588467121124 Accuracy 0.8888750672340393\n",
      "Iteration 64810 Training loss 0.0012666750699281693 Validation loss 0.04562731459736824 Accuracy 0.8890000581741333\n",
      "Iteration 64820 Training loss 0.0012723762774839997 Validation loss 0.04563527554273605 Accuracy 0.8886250257492065\n",
      "Iteration 64830 Training loss 0.0037654575426131487 Validation loss 0.045638617128133774 Accuracy 0.8892500400543213\n",
      "Iteration 64840 Training loss 0.00251303194090724 Validation loss 0.04563827067613602 Accuracy 0.8890000581741333\n",
      "Iteration 64850 Training loss 0.0025182527024298906 Validation loss 0.04568249732255936 Accuracy 0.8888750672340393\n",
      "Iteration 64860 Training loss 0.005019263364374638 Validation loss 0.04568050801753998 Accuracy 0.8890000581741333\n",
      "Iteration 64870 Training loss 0.00126361392904073 Validation loss 0.04567355662584305 Accuracy 0.8886250257492065\n",
      "Iteration 64880 Training loss 0.002512766979634762 Validation loss 0.04565635696053505 Accuracy 0.8887500166893005\n",
      "Iteration 64890 Training loss 0.0012639128835871816 Validation loss 0.04568689316511154 Accuracy 0.8886250257492065\n",
      "Iteration 64900 Training loss 0.0012672352604568005 Validation loss 0.045667845755815506 Accuracy 0.8886250257492065\n",
      "Iteration 64910 Training loss 1.454167522751959e-05 Validation loss 0.045660462230443954 Accuracy 0.8885000348091125\n",
      "Iteration 64920 Training loss 0.0012673197779804468 Validation loss 0.04565693438053131 Accuracy 0.8888750672340393\n",
      "Iteration 64930 Training loss 1.255140159628354e-05 Validation loss 0.04566188156604767 Accuracy 0.8887500166893005\n",
      "Iteration 64940 Training loss 0.0012666494585573673 Validation loss 0.04566218703985214 Accuracy 0.8888750672340393\n",
      "Iteration 64950 Training loss 0.006265199743211269 Validation loss 0.04565931856632233 Accuracy 0.8886250257492065\n",
      "Iteration 64960 Training loss 0.0025175302289426327 Validation loss 0.045674219727516174 Accuracy 0.8888750672340393\n",
      "Iteration 64970 Training loss 1.5114447705855127e-05 Validation loss 0.045653216540813446 Accuracy 0.8891250491142273\n",
      "Iteration 64980 Training loss 0.0012648627161979675 Validation loss 0.04566444456577301 Accuracy 0.8888750672340393\n",
      "Iteration 64990 Training loss 0.002514057792723179 Validation loss 0.04569563642144203 Accuracy 0.8890000581741333\n",
      "Iteration 65000 Training loss 0.002512536942958832 Validation loss 0.04565616697072983 Accuracy 0.8892500400543213\n",
      "Iteration 65010 Training loss 0.0025127127300947905 Validation loss 0.04566088318824768 Accuracy 0.8891250491142273\n",
      "Iteration 65020 Training loss 0.0012700719526037574 Validation loss 0.04564649239182472 Accuracy 0.8885000348091125\n",
      "Iteration 65030 Training loss 2.0375535314087756e-05 Validation loss 0.04566222429275513 Accuracy 0.8888750672340393\n",
      "Iteration 65040 Training loss 0.002523081609979272 Validation loss 0.04565402492880821 Accuracy 0.8886250257492065\n",
      "Iteration 65050 Training loss 0.0025178962387144566 Validation loss 0.045610908418893814 Accuracy 0.8883750438690186\n",
      "Iteration 65060 Training loss 1.5632576833013445e-05 Validation loss 0.045653458684682846 Accuracy 0.8885000348091125\n",
      "Iteration 65070 Training loss 0.0037635646294802427 Validation loss 0.045621976256370544 Accuracy 0.8883750438690186\n",
      "Iteration 65080 Training loss 0.003764204913750291 Validation loss 0.04561997577548027 Accuracy 0.8886250257492065\n",
      "Iteration 65090 Training loss 0.0012636634055525064 Validation loss 0.04563755914568901 Accuracy 0.8887500166893005\n",
      "Iteration 65100 Training loss 0.0025058963801711798 Validation loss 0.04566224291920662 Accuracy 0.8887500166893005\n",
      "Iteration 65110 Training loss 0.0012640961213037372 Validation loss 0.04567486792802811 Accuracy 0.8885000348091125\n",
      "Iteration 65120 Training loss 0.0012623652582988143 Validation loss 0.045665156096220016 Accuracy 0.8887500166893005\n",
      "Iteration 65130 Training loss 0.0012632774887606502 Validation loss 0.04567473754286766 Accuracy 0.8887500166893005\n",
      "Iteration 65140 Training loss 0.00251760589890182 Validation loss 0.04563990607857704 Accuracy 0.8886250257492065\n",
      "Iteration 65150 Training loss 0.0025108426343649626 Validation loss 0.04565494880080223 Accuracy 0.8886250257492065\n",
      "Iteration 65160 Training loss 1.680294553807471e-05 Validation loss 0.04562646150588989 Accuracy 0.8890000581741333\n",
      "Iteration 65170 Training loss 0.002515707863494754 Validation loss 0.0456530936062336 Accuracy 0.8888750672340393\n",
      "Iteration 65180 Training loss 1.3804708942188881e-05 Validation loss 0.04565424472093582 Accuracy 0.8890000581741333\n",
      "Iteration 65190 Training loss 0.001273424131795764 Validation loss 0.04565584659576416 Accuracy 0.8892500400543213\n",
      "Iteration 65200 Training loss 0.0012623716611415148 Validation loss 0.04564812779426575 Accuracy 0.8891250491142273\n",
      "Iteration 65210 Training loss 0.0025122440420091152 Validation loss 0.045658037066459656 Accuracy 0.8892500400543213\n",
      "Iteration 65220 Training loss 1.8351882317801937e-05 Validation loss 0.0456639789044857 Accuracy 0.8887500166893005\n",
      "Iteration 65230 Training loss 0.001267526880837977 Validation loss 0.04567362368106842 Accuracy 0.8891250491142273\n",
      "Iteration 65240 Training loss 1.4911247490090318e-05 Validation loss 0.0456601120531559 Accuracy 0.8893750309944153\n",
      "Iteration 65250 Training loss 0.003763047279790044 Validation loss 0.045689988881349564 Accuracy 0.8888750672340393\n",
      "Iteration 65260 Training loss 0.0012681159423664212 Validation loss 0.04567063972353935 Accuracy 0.8888750672340393\n",
      "Iteration 65270 Training loss 1.5318581063183956e-05 Validation loss 0.04566203057765961 Accuracy 0.8888750672340393\n",
      "Iteration 65280 Training loss 1.3635572940984275e-05 Validation loss 0.04565839841961861 Accuracy 0.8888750672340393\n",
      "Iteration 65290 Training loss 0.0012671308359131217 Validation loss 0.04561649262905121 Accuracy 0.8890000581741333\n",
      "Iteration 65300 Training loss 2.4371163817704655e-05 Validation loss 0.0456264428794384 Accuracy 0.8891250491142273\n",
      "Iteration 65310 Training loss 0.0012666067341342568 Validation loss 0.04562285915017128 Accuracy 0.8886250257492065\n",
      "Iteration 65320 Training loss 0.002514050342142582 Validation loss 0.04562107101082802 Accuracy 0.8890000581741333\n",
      "Iteration 65330 Training loss 2.1097952412674204e-05 Validation loss 0.04564435034990311 Accuracy 0.8888750672340393\n",
      "Iteration 65340 Training loss 1.7294689314439893e-05 Validation loss 0.045647032558918 Accuracy 0.8890000581741333\n",
      "Iteration 65350 Training loss 0.0012596941087394953 Validation loss 0.045656707137823105 Accuracy 0.8887500166893005\n",
      "Iteration 65360 Training loss 1.2181933016108815e-05 Validation loss 0.04565257579088211 Accuracy 0.8887500166893005\n",
      "Iteration 65370 Training loss 0.0012622709618881345 Validation loss 0.04568273946642876 Accuracy 0.8886250257492065\n",
      "Iteration 65380 Training loss 0.0012610417325049639 Validation loss 0.04566921666264534 Accuracy 0.8890000581741333\n",
      "Iteration 65390 Training loss 0.0025206278078258038 Validation loss 0.04566827788949013 Accuracy 0.8890000581741333\n",
      "Iteration 65400 Training loss 0.002516021952033043 Validation loss 0.04568393528461456 Accuracy 0.8892500400543213\n",
      "Iteration 65410 Training loss 0.0037635182961821556 Validation loss 0.04566412419080734 Accuracy 0.8890000581741333\n",
      "Iteration 65420 Training loss 0.001267515355721116 Validation loss 0.04568928852677345 Accuracy 0.8891250491142273\n",
      "Iteration 65430 Training loss 0.00251512392424047 Validation loss 0.04566172510385513 Accuracy 0.8888750672340393\n",
      "Iteration 65440 Training loss 1.618329406483099e-05 Validation loss 0.045675359666347504 Accuracy 0.8890000581741333\n",
      "Iteration 65450 Training loss 0.0012697300408035517 Validation loss 0.0456547737121582 Accuracy 0.8890000581741333\n",
      "Iteration 65460 Training loss 0.001264236867427826 Validation loss 0.04569178819656372 Accuracy 0.8887500166893005\n",
      "Iteration 65470 Training loss 0.0012719595106318593 Validation loss 0.04567636176943779 Accuracy 0.8885000348091125\n",
      "Iteration 65480 Training loss 2.5247516532544978e-05 Validation loss 0.04563770443201065 Accuracy 0.8892500400543213\n",
      "Iteration 65490 Training loss 1.6554600733797997e-05 Validation loss 0.045660652220249176 Accuracy 0.8891250491142273\n",
      "Iteration 65500 Training loss 0.0037637667264789343 Validation loss 0.04565001279115677 Accuracy 0.8888750672340393\n",
      "Iteration 65510 Training loss 0.0012646203394979239 Validation loss 0.04564277082681656 Accuracy 0.8887500166893005\n",
      "Iteration 65520 Training loss 0.0024978630244731903 Validation loss 0.0456407256424427 Accuracy 0.8888750672340393\n",
      "Iteration 65530 Training loss 0.0012664024252444506 Validation loss 0.045649755746126175 Accuracy 0.8890000581741333\n",
      "Iteration 65540 Training loss 1.2909762517665513e-05 Validation loss 0.04561706259846687 Accuracy 0.8892500400543213\n",
      "Iteration 65550 Training loss 1.4090945114730857e-05 Validation loss 0.04561962932348251 Accuracy 0.8891250491142273\n",
      "Iteration 65560 Training loss 1.4615840882470366e-05 Validation loss 0.0456639789044857 Accuracy 0.8893750309944153\n",
      "Iteration 65570 Training loss 0.0012693885946646333 Validation loss 0.045661136507987976 Accuracy 0.8891250491142273\n",
      "Iteration 65580 Training loss 0.0012600149493664503 Validation loss 0.04565246030688286 Accuracy 0.8890000581741333\n",
      "Iteration 65590 Training loss 0.0012631380232051015 Validation loss 0.04563681408762932 Accuracy 0.8891250491142273\n",
      "Iteration 65600 Training loss 0.003762379754334688 Validation loss 0.04563693329691887 Accuracy 0.8891250491142273\n",
      "Iteration 65610 Training loss 0.0025159120559692383 Validation loss 0.04559408128261566 Accuracy 0.8893750309944153\n",
      "Iteration 65620 Training loss 0.001267690327949822 Validation loss 0.04562176391482353 Accuracy 0.889750063419342\n",
      "Iteration 65630 Training loss 0.002518469700589776 Validation loss 0.04563337191939354 Accuracy 0.889750063419342\n",
      "Iteration 65640 Training loss 0.0037652987521141768 Validation loss 0.04564952477812767 Accuracy 0.8892500400543213\n",
      "Iteration 65650 Training loss 0.0012591694248840213 Validation loss 0.04565151780843735 Accuracy 0.8896250128746033\n",
      "Iteration 65660 Training loss 0.0012719877995550632 Validation loss 0.04566923901438713 Accuracy 0.8893750309944153\n",
      "Iteration 65670 Training loss 0.0025193477049469948 Validation loss 0.045670852065086365 Accuracy 0.8895000219345093\n",
      "Iteration 65680 Training loss 0.00251275347545743 Validation loss 0.04565054178237915 Accuracy 0.8896250128746033\n",
      "Iteration 65690 Training loss 0.0037635634653270245 Validation loss 0.04566701874136925 Accuracy 0.8891250491142273\n",
      "Iteration 65700 Training loss 1.6767327906563878e-05 Validation loss 0.0456840917468071 Accuracy 0.8892500400543213\n",
      "Iteration 65710 Training loss 1.5768360754009336e-05 Validation loss 0.04565185680985451 Accuracy 0.8893750309944153\n",
      "Iteration 65720 Training loss 0.002514934865757823 Validation loss 0.045631419867277145 Accuracy 0.8896250128746033\n",
      "Iteration 65730 Training loss 0.001265732804313302 Validation loss 0.04567806050181389 Accuracy 0.8891250491142273\n",
      "Iteration 65740 Training loss 0.001276311231777072 Validation loss 0.045644015073776245 Accuracy 0.8891250491142273\n",
      "Iteration 65750 Training loss 1.2956607861269731e-05 Validation loss 0.04565860703587532 Accuracy 0.8896250128746033\n",
      "Iteration 65760 Training loss 0.0025177872739732265 Validation loss 0.045625776052474976 Accuracy 0.8895000219345093\n",
      "Iteration 65770 Training loss 0.0012634089216589928 Validation loss 0.04559886455535889 Accuracy 0.8893750309944153\n",
      "Iteration 65780 Training loss 0.0012650805292651057 Validation loss 0.04560669884085655 Accuracy 0.8895000219345093\n",
      "Iteration 65790 Training loss 0.006271413527429104 Validation loss 0.04562819004058838 Accuracy 0.8891250491142273\n",
      "Iteration 65800 Training loss 0.0025107464753091335 Validation loss 0.04565570130944252 Accuracy 0.8892500400543213\n",
      "Iteration 65810 Training loss 1.4319368347059935e-05 Validation loss 0.0456472672522068 Accuracy 0.8891250491142273\n",
      "Iteration 65820 Training loss 0.0037639157380908728 Validation loss 0.04564685747027397 Accuracy 0.8888750672340393\n",
      "Iteration 65830 Training loss 0.0012608433607965708 Validation loss 0.04564429074525833 Accuracy 0.8890000581741333\n",
      "Iteration 65840 Training loss 0.0012611652491614223 Validation loss 0.045657772570848465 Accuracy 0.8890000581741333\n",
      "Iteration 65850 Training loss 0.0012670388678088784 Validation loss 0.0456792376935482 Accuracy 0.8890000581741333\n",
      "Iteration 65860 Training loss 0.002512872451916337 Validation loss 0.04561512917280197 Accuracy 0.8890000581741333\n",
      "Iteration 65870 Training loss 1.3614750969281886e-05 Validation loss 0.045629244297742844 Accuracy 0.8892500400543213\n",
      "Iteration 65880 Training loss 1.5436133253388107e-05 Validation loss 0.0456806980073452 Accuracy 0.8892500400543213\n",
      "Iteration 65890 Training loss 0.0025201677344739437 Validation loss 0.045639291405677795 Accuracy 0.8892500400543213\n",
      "Iteration 65900 Training loss 0.002514164661988616 Validation loss 0.04565870016813278 Accuracy 0.8892500400543213\n",
      "Iteration 65910 Training loss 1.2057958883815445e-05 Validation loss 0.04564488306641579 Accuracy 0.8887500166893005\n",
      "Iteration 65920 Training loss 1.798076118575409e-05 Validation loss 0.04568010941147804 Accuracy 0.8892500400543213\n",
      "Iteration 65930 Training loss 0.002515873871743679 Validation loss 0.045668117702007294 Accuracy 0.8890000581741333\n",
      "Iteration 65940 Training loss 1.3445971490000375e-05 Validation loss 0.04563436657190323 Accuracy 0.8890000581741333\n",
      "Iteration 65950 Training loss 0.0012615962186828256 Validation loss 0.0456424281001091 Accuracy 0.8896250128746033\n",
      "Iteration 65960 Training loss 0.0025146417319774628 Validation loss 0.04563894867897034 Accuracy 0.8888750672340393\n",
      "Iteration 65970 Training loss 1.276330749533372e-05 Validation loss 0.04564054682850838 Accuracy 0.8885000348091125\n",
      "Iteration 65980 Training loss 0.005013842601329088 Validation loss 0.045638307929039 Accuracy 0.8887500166893005\n",
      "Iteration 65990 Training loss 0.001269743894226849 Validation loss 0.04562877491116524 Accuracy 0.8887500166893005\n",
      "Iteration 66000 Training loss 1.6745021639508195e-05 Validation loss 0.04563840851187706 Accuracy 0.8886250257492065\n",
      "Iteration 66010 Training loss 1.1379944226064254e-05 Validation loss 0.04562176764011383 Accuracy 0.8892500400543213\n",
      "Iteration 66020 Training loss 0.0012678260682150722 Validation loss 0.045613426715135574 Accuracy 0.889875054359436\n",
      "Iteration 66030 Training loss 0.0012658387422561646 Validation loss 0.045647911727428436 Accuracy 0.8895000219345093\n",
      "Iteration 66040 Training loss 0.0050174943171441555 Validation loss 0.045630499720573425 Accuracy 0.8896250128746033\n",
      "Iteration 66050 Training loss 2.0362047507660463e-05 Validation loss 0.04565613716840744 Accuracy 0.8895000219345093\n",
      "Iteration 66060 Training loss 0.0012671517906710505 Validation loss 0.04568316042423248 Accuracy 0.8890000581741333\n",
      "Iteration 66070 Training loss 0.0025154270697385073 Validation loss 0.04568342864513397 Accuracy 0.8893750309944153\n",
      "Iteration 66080 Training loss 1.7547055904287845e-05 Validation loss 0.0456453301012516 Accuracy 0.8896250128746033\n",
      "Iteration 66090 Training loss 0.002516531851142645 Validation loss 0.04565877839922905 Accuracy 0.8888750672340393\n",
      "Iteration 66100 Training loss 1.840396180341486e-05 Validation loss 0.045623648911714554 Accuracy 0.8896250128746033\n",
      "Iteration 66110 Training loss 0.002520675538107753 Validation loss 0.045610640197992325 Accuracy 0.8892500400543213\n",
      "Iteration 66120 Training loss 1.4853468201181386e-05 Validation loss 0.04563166946172714 Accuracy 0.8896250128746033\n",
      "Iteration 66130 Training loss 0.001272283960133791 Validation loss 0.04563575237989426 Accuracy 0.8895000219345093\n",
      "Iteration 66140 Training loss 1.1899734090548009e-05 Validation loss 0.04562659189105034 Accuracy 0.8895000219345093\n",
      "Iteration 66150 Training loss 2.2942969735595398e-05 Validation loss 0.04564707353711128 Accuracy 0.89000004529953\n",
      "Iteration 66160 Training loss 1.4658869986305945e-05 Validation loss 0.04563665762543678 Accuracy 0.8896250128746033\n",
      "Iteration 66170 Training loss 0.002512073377147317 Validation loss 0.04568852484226227 Accuracy 0.8892500400543213\n",
      "Iteration 66180 Training loss 0.0025155108887702227 Validation loss 0.04562867805361748 Accuracy 0.889750063419342\n",
      "Iteration 66190 Training loss 1.7574529920239e-05 Validation loss 0.04562694951891899 Accuracy 0.8896250128746033\n",
      "Iteration 66200 Training loss 0.0012656274484470487 Validation loss 0.04565959796309471 Accuracy 0.8895000219345093\n",
      "Iteration 66210 Training loss 0.003801061538979411 Validation loss 0.04565396159887314 Accuracy 0.8893750309944153\n",
      "Iteration 66220 Training loss 0.002510680118575692 Validation loss 0.04564082249999046 Accuracy 0.889750063419342\n",
      "Iteration 66230 Training loss 0.0025137080810964108 Validation loss 0.04564035311341286 Accuracy 0.8892500400543213\n",
      "Iteration 66240 Training loss 0.001268110703676939 Validation loss 0.04563767462968826 Accuracy 0.8891250491142273\n",
      "Iteration 66250 Training loss 0.0012639465276151896 Validation loss 0.045652490109205246 Accuracy 0.890125036239624\n",
      "Iteration 66260 Training loss 0.002512993523851037 Validation loss 0.045618075877428055 Accuracy 0.89000004529953\n",
      "Iteration 66270 Training loss 0.002516726730391383 Validation loss 0.04564524441957474 Accuracy 0.889875054359436\n",
      "Iteration 66280 Training loss 0.0037573014851659536 Validation loss 0.045623455196619034 Accuracy 0.8896250128746033\n",
      "Iteration 66290 Training loss 0.0012720825616270304 Validation loss 0.04564535617828369 Accuracy 0.8895000219345093\n",
      "Iteration 66300 Training loss 0.0012644440867006779 Validation loss 0.04566778987646103 Accuracy 0.8886250257492065\n",
      "Iteration 66310 Training loss 0.0012606054078787565 Validation loss 0.04566814750432968 Accuracy 0.8890000581741333\n",
      "Iteration 66320 Training loss 0.002511213067919016 Validation loss 0.04564527049660683 Accuracy 0.8895000219345093\n",
      "Iteration 66330 Training loss 0.0012639651540666819 Validation loss 0.045645661652088165 Accuracy 0.8893750309944153\n",
      "Iteration 66340 Training loss 1.4310660844785161e-05 Validation loss 0.04566796123981476 Accuracy 0.8895000219345093\n",
      "Iteration 66350 Training loss 0.002510815393179655 Validation loss 0.04569085314869881 Accuracy 0.8893750309944153\n",
      "Iteration 66360 Training loss 2.0298964955145493e-05 Validation loss 0.04564866051077843 Accuracy 0.8895000219345093\n",
      "Iteration 66370 Training loss 0.0025121583603322506 Validation loss 0.0456446073949337 Accuracy 0.889750063419342\n",
      "Iteration 66380 Training loss 0.0025154659524559975 Validation loss 0.045634374022483826 Accuracy 0.889750063419342\n",
      "Iteration 66390 Training loss 2.2406247808248736e-05 Validation loss 0.045627228915691376 Accuracy 0.8892500400543213\n",
      "Iteration 66400 Training loss 0.0012861134018748999 Validation loss 0.04563545435667038 Accuracy 0.8895000219345093\n",
      "Iteration 66410 Training loss 1.3744001989834942e-05 Validation loss 0.045620158314704895 Accuracy 0.8893750309944153\n",
      "Iteration 66420 Training loss 1.234424598806072e-05 Validation loss 0.04562455043196678 Accuracy 0.8895000219345093\n",
      "Iteration 66430 Training loss 0.003766194451600313 Validation loss 0.04563590884208679 Accuracy 0.8893750309944153\n",
      "Iteration 66440 Training loss 0.0025112973526120186 Validation loss 0.045652080327272415 Accuracy 0.8895000219345093\n",
      "Iteration 66450 Training loss 1.4789154192840215e-05 Validation loss 0.04565376788377762 Accuracy 0.889750063419342\n",
      "Iteration 66460 Training loss 0.001273750327527523 Validation loss 0.04565020278096199 Accuracy 0.889750063419342\n",
      "Iteration 66470 Training loss 0.0012595915468409657 Validation loss 0.0456206239759922 Accuracy 0.889750063419342\n",
      "Iteration 66480 Training loss 0.005012829788029194 Validation loss 0.04562131315469742 Accuracy 0.8896250128746033\n",
      "Iteration 66490 Training loss 0.0025196033529937267 Validation loss 0.04560385271906853 Accuracy 0.8893750309944153\n",
      "Iteration 66500 Training loss 0.0012584185460582376 Validation loss 0.04560820385813713 Accuracy 0.889875054359436\n",
      "Iteration 66510 Training loss 0.0012614408042281866 Validation loss 0.04562525451183319 Accuracy 0.889750063419342\n",
      "Iteration 66520 Training loss 1.684215931163635e-05 Validation loss 0.045635104179382324 Accuracy 0.8895000219345093\n",
      "Iteration 66530 Training loss 0.0012677110498771071 Validation loss 0.045648276805877686 Accuracy 0.8892500400543213\n",
      "Iteration 66540 Training loss 1.547823558212258e-05 Validation loss 0.04564385488629341 Accuracy 0.8893750309944153\n",
      "Iteration 66550 Training loss 0.003763592801988125 Validation loss 0.04565390571951866 Accuracy 0.8892500400543213\n",
      "Iteration 66560 Training loss 1.233360671903938e-05 Validation loss 0.045637451112270355 Accuracy 0.8893750309944153\n",
      "Iteration 66570 Training loss 1.850592889240943e-05 Validation loss 0.04564182460308075 Accuracy 0.8891250491142273\n",
      "Iteration 66580 Training loss 0.0037620074581354856 Validation loss 0.04566348344087601 Accuracy 0.8891250491142273\n",
      "Iteration 66590 Training loss 0.0025146701373159885 Validation loss 0.045677561312913895 Accuracy 0.8892500400543213\n",
      "Iteration 66600 Training loss 3.0003640858922154e-05 Validation loss 0.045641977339982986 Accuracy 0.889750063419342\n",
      "Iteration 66610 Training loss 0.0012584517244249582 Validation loss 0.04563988372683525 Accuracy 0.8893750309944153\n",
      "Iteration 66620 Training loss 0.0012657006736844778 Validation loss 0.0456472672522068 Accuracy 0.8896250128746033\n",
      "Iteration 66630 Training loss 0.005017084535211325 Validation loss 0.04564565792679787 Accuracy 0.8895000219345093\n",
      "Iteration 66640 Training loss 1.311475716647692e-05 Validation loss 0.045662034302949905 Accuracy 0.8892500400543213\n",
      "Iteration 66650 Training loss 0.001263745711185038 Validation loss 0.045674920082092285 Accuracy 0.8892500400543213\n",
      "Iteration 66660 Training loss 0.0012642110232263803 Validation loss 0.04567231237888336 Accuracy 0.8893750309944153\n",
      "Iteration 66670 Training loss 1.0127746463695075e-05 Validation loss 0.04567965492606163 Accuracy 0.8888750672340393\n",
      "Iteration 66680 Training loss 0.003764712018892169 Validation loss 0.04569641873240471 Accuracy 0.8892500400543213\n",
      "Iteration 66690 Training loss 1.7785971067496575e-05 Validation loss 0.04568158835172653 Accuracy 0.8893750309944153\n",
      "Iteration 66700 Training loss 0.0012708095600828528 Validation loss 0.04564933851361275 Accuracy 0.889875054359436\n",
      "Iteration 66710 Training loss 1.4298229871201329e-05 Validation loss 0.04567340388894081 Accuracy 0.8891250491142273\n",
      "Iteration 66720 Training loss 0.001261553610675037 Validation loss 0.04563523456454277 Accuracy 0.8890000581741333\n",
      "Iteration 66730 Training loss 0.001264731166884303 Validation loss 0.04565518721938133 Accuracy 0.8892500400543213\n",
      "Iteration 66740 Training loss 0.002514353021979332 Validation loss 0.04563930630683899 Accuracy 0.8895000219345093\n",
      "Iteration 66750 Training loss 0.002515550237149 Validation loss 0.04566742107272148 Accuracy 0.8895000219345093\n",
      "Iteration 66760 Training loss 1.9927962057408877e-05 Validation loss 0.045622773468494415 Accuracy 0.8895000219345093\n",
      "Iteration 66770 Training loss 0.0012632858706638217 Validation loss 0.04566672816872597 Accuracy 0.889750063419342\n",
      "Iteration 66780 Training loss 0.0025173660833388567 Validation loss 0.045660875737667084 Accuracy 0.89000004529953\n",
      "Iteration 66790 Training loss 0.0012637434992939234 Validation loss 0.04568157717585564 Accuracy 0.8893750309944153\n",
      "Iteration 66800 Training loss 0.001266622799448669 Validation loss 0.04568412899971008 Accuracy 0.889750063419342\n",
      "Iteration 66810 Training loss 0.002515612170100212 Validation loss 0.04569817706942558 Accuracy 0.8891250491142273\n",
      "Iteration 66820 Training loss 0.0012662571389228106 Validation loss 0.045686159282922745 Accuracy 0.8895000219345093\n",
      "Iteration 66830 Training loss 1.7697086150292307e-05 Validation loss 0.04565731808543205 Accuracy 0.8891250491142273\n",
      "Iteration 66840 Training loss 1.99688074644655e-05 Validation loss 0.045669276267290115 Accuracy 0.8890000581741333\n",
      "Iteration 66850 Training loss 0.0037615783512592316 Validation loss 0.04570845142006874 Accuracy 0.8888750672340393\n",
      "Iteration 66860 Training loss 0.001264473656192422 Validation loss 0.045675843954086304 Accuracy 0.8890000581741333\n",
      "Iteration 66870 Training loss 0.0037638142239302397 Validation loss 0.04570409655570984 Accuracy 0.8890000581741333\n",
      "Iteration 66880 Training loss 0.0037612724117934704 Validation loss 0.0456981286406517 Accuracy 0.8891250491142273\n",
      "Iteration 66890 Training loss 0.0025201141834259033 Validation loss 0.04565398767590523 Accuracy 0.8890000581741333\n",
      "Iteration 66900 Training loss 0.002515076892450452 Validation loss 0.04564943164587021 Accuracy 0.8890000581741333\n",
      "Iteration 66910 Training loss 0.0025109397247433662 Validation loss 0.045659150928258896 Accuracy 0.8888750672340393\n",
      "Iteration 66920 Training loss 1.7093536371248774e-05 Validation loss 0.04567056521773338 Accuracy 0.8886250257492065\n",
      "Iteration 66930 Training loss 0.003767246613278985 Validation loss 0.04564218968153 Accuracy 0.8895000219345093\n",
      "Iteration 66940 Training loss 0.0012681798543781042 Validation loss 0.045685265213251114 Accuracy 0.8892500400543213\n",
      "Iteration 66950 Training loss 0.0012633322039619088 Validation loss 0.04568352922797203 Accuracy 0.8892500400543213\n",
      "Iteration 66960 Training loss 1.3411223335424438e-05 Validation loss 0.04565411061048508 Accuracy 0.8893750309944153\n",
      "Iteration 66970 Training loss 0.002516244538128376 Validation loss 0.04568667337298393 Accuracy 0.8893750309944153\n",
      "Iteration 66980 Training loss 0.003761646104976535 Validation loss 0.045694947242736816 Accuracy 0.8893750309944153\n",
      "Iteration 66990 Training loss 0.0012655557366088033 Validation loss 0.04564668610692024 Accuracy 0.8891250491142273\n",
      "Iteration 67000 Training loss 1.1609206012508366e-05 Validation loss 0.045671455562114716 Accuracy 0.8896250128746033\n",
      "Iteration 67010 Training loss 0.0012662404915317893 Validation loss 0.04564181715250015 Accuracy 0.8892500400543213\n",
      "Iteration 67020 Training loss 0.001265660161152482 Validation loss 0.04566214978694916 Accuracy 0.8888750672340393\n",
      "Iteration 67030 Training loss 0.0012665675021708012 Validation loss 0.045662302523851395 Accuracy 0.8892500400543213\n",
      "Iteration 67040 Training loss 0.0012664137175306678 Validation loss 0.045670438557863235 Accuracy 0.8892500400543213\n",
      "Iteration 67050 Training loss 0.0012699090875685215 Validation loss 0.04569012671709061 Accuracy 0.8890000581741333\n",
      "Iteration 67060 Training loss 0.0025123129598796368 Validation loss 0.0456792488694191 Accuracy 0.8892500400543213\n",
      "Iteration 67070 Training loss 0.0012651629513129592 Validation loss 0.04565974324941635 Accuracy 0.8891250491142273\n",
      "Iteration 67080 Training loss 0.0025115059688687325 Validation loss 0.04565564915537834 Accuracy 0.8891250491142273\n",
      "Iteration 67090 Training loss 0.00377015583217144 Validation loss 0.04567810148000717 Accuracy 0.89000004529953\n",
      "Iteration 67100 Training loss 1.142955352406716e-05 Validation loss 0.04567476734519005 Accuracy 0.89000004529953\n",
      "Iteration 67110 Training loss 1.144631278293673e-05 Validation loss 0.045645445585250854 Accuracy 0.8892500400543213\n",
      "Iteration 67120 Training loss 0.0012709349393844604 Validation loss 0.04571446031332016 Accuracy 0.889750063419342\n",
      "Iteration 67130 Training loss 1.2340876310190652e-05 Validation loss 0.04569662734866142 Accuracy 0.8893750309944153\n",
      "Iteration 67140 Training loss 0.0012721983948722482 Validation loss 0.04567050561308861 Accuracy 0.8892500400543213\n",
      "Iteration 67150 Training loss 0.0025189989246428013 Validation loss 0.04569246619939804 Accuracy 0.8888750672340393\n",
      "Iteration 67160 Training loss 0.0012649550335481763 Validation loss 0.045702219009399414 Accuracy 0.8890000581741333\n",
      "Iteration 67170 Training loss 0.002513572573661804 Validation loss 0.045681338757276535 Accuracy 0.8895000219345093\n",
      "Iteration 67180 Training loss 0.002512488281354308 Validation loss 0.045685913413763046 Accuracy 0.8893750309944153\n",
      "Iteration 67190 Training loss 1.6336533008143306e-05 Validation loss 0.04569488391280174 Accuracy 0.8890000581741333\n",
      "Iteration 67200 Training loss 0.0012649475829675794 Validation loss 0.045691683888435364 Accuracy 0.8890000581741333\n",
      "Iteration 67210 Training loss 0.0025144063401967287 Validation loss 0.04566626623272896 Accuracy 0.8892500400543213\n",
      "Iteration 67220 Training loss 0.003766527632251382 Validation loss 0.045683231204748154 Accuracy 0.8893750309944153\n",
      "Iteration 67230 Training loss 2.2199968952918425e-05 Validation loss 0.04566539078950882 Accuracy 0.8888750672340393\n",
      "Iteration 67240 Training loss 0.002514542080461979 Validation loss 0.04565448313951492 Accuracy 0.8888750672340393\n",
      "Iteration 67250 Training loss 1.0749226930784062e-05 Validation loss 0.04566781967878342 Accuracy 0.8896250128746033\n",
      "Iteration 67260 Training loss 1.0762696547317319e-05 Validation loss 0.045684076845645905 Accuracy 0.8893750309944153\n",
      "Iteration 67270 Training loss 1.3703383046959061e-05 Validation loss 0.0456959530711174 Accuracy 0.8895000219345093\n",
      "Iteration 67280 Training loss 0.0012613184517249465 Validation loss 0.04569356516003609 Accuracy 0.8895000219345093\n",
      "Iteration 67290 Training loss 0.0050102900713682175 Validation loss 0.0456557422876358 Accuracy 0.8891250491142273\n",
      "Iteration 67300 Training loss 0.001264095539227128 Validation loss 0.04566849023103714 Accuracy 0.8892500400543213\n",
      "Iteration 67310 Training loss 0.0025183670222759247 Validation loss 0.04571838676929474 Accuracy 0.8893750309944153\n",
      "Iteration 67320 Training loss 0.0012647849507629871 Validation loss 0.04568247124552727 Accuracy 0.8893750309944153\n",
      "Iteration 67330 Training loss 0.0037592563312500715 Validation loss 0.04566804692149162 Accuracy 0.8890000581741333\n",
      "Iteration 67340 Training loss 1.2967958355147857e-05 Validation loss 0.045685525983572006 Accuracy 0.8896250128746033\n",
      "Iteration 67350 Training loss 1.4802424630033784e-05 Validation loss 0.04566282406449318 Accuracy 0.8893750309944153\n",
      "Iteration 67360 Training loss 1.2583681382238865e-05 Validation loss 0.04567837715148926 Accuracy 0.8893750309944153\n",
      "Iteration 67370 Training loss 0.0012629582779482007 Validation loss 0.045661602169275284 Accuracy 0.8892500400543213\n",
      "Iteration 67380 Training loss 0.0012689458671957254 Validation loss 0.0456349216401577 Accuracy 0.8891250491142273\n",
      "Iteration 67390 Training loss 0.0025184350088238716 Validation loss 0.04566665366292 Accuracy 0.8888750672340393\n",
      "Iteration 67400 Training loss 0.0012617133324965835 Validation loss 0.045639391988515854 Accuracy 0.8892500400543213\n",
      "Iteration 67410 Training loss 0.002512949053198099 Validation loss 0.04565797001123428 Accuracy 0.889750063419342\n",
      "Iteration 67420 Training loss 0.002512778853997588 Validation loss 0.04564569145441055 Accuracy 0.8893750309944153\n",
      "Iteration 67430 Training loss 1.0311183359590359e-05 Validation loss 0.04563428834080696 Accuracy 0.8895000219345093\n",
      "Iteration 67440 Training loss 0.005014000926166773 Validation loss 0.04568328335881233 Accuracy 0.8895000219345093\n",
      "Iteration 67450 Training loss 0.0025131364818662405 Validation loss 0.04570166394114494 Accuracy 0.8896250128746033\n",
      "Iteration 67460 Training loss 7.690896381973289e-06 Validation loss 0.045700449496507645 Accuracy 0.8895000219345093\n",
      "Iteration 67470 Training loss 1.738444552756846e-05 Validation loss 0.0456872433423996 Accuracy 0.8895000219345093\n",
      "Iteration 67480 Training loss 0.0025109779089689255 Validation loss 0.04568000137805939 Accuracy 0.8892500400543213\n",
      "Iteration 67490 Training loss 0.0012640992645174265 Validation loss 0.045689936727285385 Accuracy 0.8893750309944153\n",
      "Iteration 67500 Training loss 0.0012618034379556775 Validation loss 0.04567006975412369 Accuracy 0.8895000219345093\n",
      "Iteration 67510 Training loss 2.0558503820211627e-05 Validation loss 0.04565531015396118 Accuracy 0.8893750309944153\n",
      "Iteration 67520 Training loss 1.4290515537140891e-05 Validation loss 0.04566916450858116 Accuracy 0.8892500400543213\n",
      "Iteration 67530 Training loss 0.0037591534201055765 Validation loss 0.04566501826047897 Accuracy 0.8895000219345093\n",
      "Iteration 67540 Training loss 0.0025146466214209795 Validation loss 0.0456601083278656 Accuracy 0.8893750309944153\n",
      "Iteration 67550 Training loss 0.0025127248372882605 Validation loss 0.04565931856632233 Accuracy 0.8895000219345093\n",
      "Iteration 67560 Training loss 0.002514523221179843 Validation loss 0.0456727035343647 Accuracy 0.8895000219345093\n",
      "Iteration 67570 Training loss 0.003764266613870859 Validation loss 0.0456676185131073 Accuracy 0.8892500400543213\n",
      "Iteration 67580 Training loss 0.00126173987518996 Validation loss 0.045645423233509064 Accuracy 0.8892500400543213\n",
      "Iteration 67590 Training loss 0.0012630330165848136 Validation loss 0.04567640274763107 Accuracy 0.8892500400543213\n",
      "Iteration 67600 Training loss 0.0012692050077021122 Validation loss 0.045687783509492874 Accuracy 0.8891250491142273\n",
      "Iteration 67610 Training loss 0.002512607490643859 Validation loss 0.04570024088025093 Accuracy 0.889750063419342\n",
      "Iteration 67620 Training loss 1.583665107318666e-05 Validation loss 0.04569544643163681 Accuracy 0.8896250128746033\n",
      "Iteration 67630 Training loss 0.0012614669976755977 Validation loss 0.04568292573094368 Accuracy 0.8896250128746033\n",
      "Iteration 67640 Training loss 1.0746751286205836e-05 Validation loss 0.045690204948186874 Accuracy 0.8896250128746033\n",
      "Iteration 67650 Training loss 0.0012663996312767267 Validation loss 0.045708924531936646 Accuracy 0.8893750309944153\n",
      "Iteration 67660 Training loss 1.4673895748273935e-05 Validation loss 0.04568851366639137 Accuracy 0.8895000219345093\n",
      "Iteration 67670 Training loss 0.0025208001025021076 Validation loss 0.045677222311496735 Accuracy 0.8891250491142273\n",
      "Iteration 67680 Training loss 1.1052195986849256e-05 Validation loss 0.045688752084970474 Accuracy 0.8893750309944153\n",
      "Iteration 67690 Training loss 1.6421316104242578e-05 Validation loss 0.0457034558057785 Accuracy 0.8891250491142273\n",
      "Iteration 67700 Training loss 0.001266835955902934 Validation loss 0.04569388926029205 Accuracy 0.8891250491142273\n",
      "Iteration 67710 Training loss 0.0037621462251991034 Validation loss 0.04571058228611946 Accuracy 0.8893750309944153\n",
      "Iteration 67720 Training loss 0.0012636338360607624 Validation loss 0.04569442942738533 Accuracy 0.8893750309944153\n",
      "Iteration 67730 Training loss 0.0025147045962512493 Validation loss 0.045693159103393555 Accuracy 0.8892500400543213\n",
      "Iteration 67740 Training loss 1.832449561334215e-05 Validation loss 0.04569302499294281 Accuracy 0.8895000219345093\n",
      "Iteration 67750 Training loss 0.00251785502769053 Validation loss 0.04570246487855911 Accuracy 0.8893750309944153\n",
      "Iteration 67760 Training loss 0.002513257320970297 Validation loss 0.04572398588061333 Accuracy 0.8895000219345093\n",
      "Iteration 67770 Training loss 0.0012713288888335228 Validation loss 0.04573864862322807 Accuracy 0.8892500400543213\n",
      "Iteration 67780 Training loss 0.005013851448893547 Validation loss 0.04572131857275963 Accuracy 0.8893750309944153\n",
      "Iteration 67790 Training loss 1.5837209502933547e-05 Validation loss 0.045707762241363525 Accuracy 0.889750063419342\n",
      "Iteration 67800 Training loss 0.002514672465622425 Validation loss 0.04572616145014763 Accuracy 0.8892500400543213\n",
      "Iteration 67810 Training loss 1.3804037735098973e-05 Validation loss 0.045734744518995285 Accuracy 0.8896250128746033\n",
      "Iteration 67820 Training loss 1.381961101287743e-05 Validation loss 0.0456872433423996 Accuracy 0.8890000581741333\n",
      "Iteration 67830 Training loss 1.9072163922828622e-05 Validation loss 0.0456942543387413 Accuracy 0.8895000219345093\n",
      "Iteration 67840 Training loss 0.001269577071070671 Validation loss 0.045710451900959015 Accuracy 0.8893750309944153\n",
      "Iteration 67850 Training loss 1.8358216038905084e-05 Validation loss 0.04571940749883652 Accuracy 0.8891250491142273\n",
      "Iteration 67860 Training loss 0.001264645834453404 Validation loss 0.04574218764901161 Accuracy 0.8887500166893005\n",
      "Iteration 67870 Training loss 1.5256745427905116e-05 Validation loss 0.045725591480731964 Accuracy 0.8888750672340393\n",
      "Iteration 67880 Training loss 0.0025136175099760294 Validation loss 0.04571346938610077 Accuracy 0.8893750309944153\n",
      "Iteration 67890 Training loss 0.0012608971446752548 Validation loss 0.04568314179778099 Accuracy 0.8895000219345093\n",
      "Iteration 67900 Training loss 0.0025114284362643957 Validation loss 0.04572127014398575 Accuracy 0.889750063419342\n",
      "Iteration 67910 Training loss 1.8188811736763455e-05 Validation loss 0.04570939019322395 Accuracy 0.8893750309944153\n",
      "Iteration 67920 Training loss 0.0012747610453516245 Validation loss 0.045712847262620926 Accuracy 0.8893750309944153\n",
      "Iteration 67930 Training loss 0.0012622562935575843 Validation loss 0.0457145981490612 Accuracy 0.8893750309944153\n",
      "Iteration 67940 Training loss 0.0012680238578468561 Validation loss 0.045696381479501724 Accuracy 0.8888750672340393\n",
      "Iteration 67950 Training loss 0.0012631710851565003 Validation loss 0.04570695012807846 Accuracy 0.8893750309944153\n",
      "Iteration 67960 Training loss 0.0025135106407105923 Validation loss 0.04571716859936714 Accuracy 0.8891250491142273\n",
      "Iteration 67970 Training loss 0.001265238388441503 Validation loss 0.04570264741778374 Accuracy 0.8892500400543213\n",
      "Iteration 67980 Training loss 1.6206853615585715e-05 Validation loss 0.045720454305410385 Accuracy 0.8890000581741333\n",
      "Iteration 67990 Training loss 0.0012613600119948387 Validation loss 0.045712344348430634 Accuracy 0.8890000581741333\n",
      "Iteration 68000 Training loss 0.0037632861640304327 Validation loss 0.045699089765548706 Accuracy 0.8896250128746033\n",
      "Iteration 68010 Training loss 2.2397705834009685e-05 Validation loss 0.04572521150112152 Accuracy 0.8891250491142273\n",
      "Iteration 68020 Training loss 0.0037707595620304346 Validation loss 0.04573849216103554 Accuracy 0.8888750672340393\n",
      "Iteration 68030 Training loss 0.0025144123937934637 Validation loss 0.045711178332567215 Accuracy 0.8892500400543213\n",
      "Iteration 68040 Training loss 0.001268905820325017 Validation loss 0.04573005065321922 Accuracy 0.8891250491142273\n",
      "Iteration 68050 Training loss 0.0025136142503470182 Validation loss 0.045703914016485214 Accuracy 0.8892500400543213\n",
      "Iteration 68060 Training loss 9.950389539881144e-06 Validation loss 0.04569346830248833 Accuracy 0.8892500400543213\n",
      "Iteration 68070 Training loss 1.4616888620366808e-05 Validation loss 0.0457107238471508 Accuracy 0.8891250491142273\n",
      "Iteration 68080 Training loss 0.0012623305665329099 Validation loss 0.045704033225774765 Accuracy 0.8892500400543213\n",
      "Iteration 68090 Training loss 0.00627184147015214 Validation loss 0.04570458456873894 Accuracy 0.8891250491142273\n",
      "Iteration 68100 Training loss 0.0012651700526475906 Validation loss 0.045736897736787796 Accuracy 0.8891250491142273\n",
      "Iteration 68110 Training loss 0.001266272971406579 Validation loss 0.04574258625507355 Accuracy 0.8891250491142273\n",
      "Iteration 68120 Training loss 0.002516011008992791 Validation loss 0.04575120657682419 Accuracy 0.8890000581741333\n",
      "Iteration 68130 Training loss 0.0012665502727031708 Validation loss 0.04572558403015137 Accuracy 0.8890000581741333\n",
      "Iteration 68140 Training loss 0.0025162920355796814 Validation loss 0.04571681097149849 Accuracy 0.8888750672340393\n",
      "Iteration 68150 Training loss 1.3129163562553003e-05 Validation loss 0.045713428407907486 Accuracy 0.8891250491142273\n",
      "Iteration 68160 Training loss 1.625955155759584e-05 Validation loss 0.04571821540594101 Accuracy 0.8893750309944153\n",
      "Iteration 68170 Training loss 0.0012649556156247854 Validation loss 0.04575323686003685 Accuracy 0.8893750309944153\n",
      "Iteration 68180 Training loss 0.0025125446263700724 Validation loss 0.04572732746601105 Accuracy 0.8892500400543213\n",
      "Iteration 68190 Training loss 0.0012639787746593356 Validation loss 0.0457540825009346 Accuracy 0.8893750309944153\n",
      "Iteration 68200 Training loss 0.0037654771003872156 Validation loss 0.04575113579630852 Accuracy 0.889750063419342\n",
      "Iteration 68210 Training loss 0.003766670124605298 Validation loss 0.04574304446578026 Accuracy 0.889750063419342\n",
      "Iteration 68220 Training loss 0.001260428107343614 Validation loss 0.04576944559812546 Accuracy 0.8891250491142273\n",
      "Iteration 68230 Training loss 1.556058668938931e-05 Validation loss 0.045748624950647354 Accuracy 0.8891250491142273\n",
      "Iteration 68240 Training loss 0.002517862943932414 Validation loss 0.04572472721338272 Accuracy 0.8891250491142273\n",
      "Iteration 68250 Training loss 0.00251325243152678 Validation loss 0.045732393860816956 Accuracy 0.8890000581741333\n",
      "Iteration 68260 Training loss 0.0012653785524889827 Validation loss 0.045757144689559937 Accuracy 0.8890000581741333\n",
      "Iteration 68270 Training loss 0.0025150941219180822 Validation loss 0.04573105275630951 Accuracy 0.8890000581741333\n",
      "Iteration 68280 Training loss 0.0012656444450840354 Validation loss 0.04571609944105148 Accuracy 0.8890000581741333\n",
      "Iteration 68290 Training loss 0.0025103758089244366 Validation loss 0.0457184761762619 Accuracy 0.8893750309944153\n",
      "Iteration 68300 Training loss 0.0012618746841326356 Validation loss 0.04570819064974785 Accuracy 0.8893750309944153\n",
      "Iteration 68310 Training loss 1.6617908840999007e-05 Validation loss 0.04581855982542038 Accuracy 0.8892500400543213\n",
      "Iteration 68320 Training loss 0.0012657609768211842 Validation loss 0.045791201293468475 Accuracy 0.8895000219345093\n",
      "Iteration 68330 Training loss 0.0012604256626218557 Validation loss 0.04577546566724777 Accuracy 0.8890000581741333\n",
      "Iteration 68340 Training loss 1.499179325037403e-05 Validation loss 0.045741692185401917 Accuracy 0.8893750309944153\n",
      "Iteration 68350 Training loss 0.0012654648162424564 Validation loss 0.045740336179733276 Accuracy 0.8893750309944153\n",
      "Iteration 68360 Training loss 0.0025234180502593517 Validation loss 0.0460931658744812 Accuracy 0.8888750672340393\n",
      "Iteration 68370 Training loss 1.578898445586674e-05 Validation loss 0.04594233259558678 Accuracy 0.8890000581741333\n",
      "Iteration 68380 Training loss 1.6224972569034435e-05 Validation loss 0.0458582267165184 Accuracy 0.8896250128746033\n",
      "Iteration 68390 Training loss 0.001267711748369038 Validation loss 0.046153899282217026 Accuracy 0.8886250257492065\n",
      "Iteration 68400 Training loss 0.0025190149899572134 Validation loss 0.046038657426834106 Accuracy 0.8881250619888306\n",
      "Iteration 68410 Training loss 0.003773915348574519 Validation loss 0.04579435661435127 Accuracy 0.8887500166893005\n",
      "Iteration 68420 Training loss 1.328644975728821e-05 Validation loss 0.045748550444841385 Accuracy 0.8890000581741333\n",
      "Iteration 68430 Training loss 0.0012686725240200758 Validation loss 0.04575323313474655 Accuracy 0.8893750309944153\n",
      "Iteration 68440 Training loss 0.001263488782569766 Validation loss 0.04572655260562897 Accuracy 0.8892500400543213\n",
      "Iteration 68450 Training loss 0.005014654248952866 Validation loss 0.04574863240122795 Accuracy 0.8893750309944153\n",
      "Iteration 68460 Training loss 0.0012626948300749063 Validation loss 0.04576941579580307 Accuracy 0.889750063419342\n",
      "Iteration 68470 Training loss 0.001263947575353086 Validation loss 0.04575362056493759 Accuracy 0.8896250128746033\n",
      "Iteration 68480 Training loss 1.4423883840208873e-05 Validation loss 0.04574650898575783 Accuracy 0.8893750309944153\n",
      "Iteration 68490 Training loss 0.0012641125358641148 Validation loss 0.04576365277171135 Accuracy 0.8896250128746033\n",
      "Iteration 68500 Training loss 0.0025198578368872404 Validation loss 0.045769378542900085 Accuracy 0.8893750309944153\n",
      "Iteration 68510 Training loss 0.005053969565778971 Validation loss 0.045745816081762314 Accuracy 0.8892500400543213\n",
      "Iteration 68520 Training loss 0.001263455837033689 Validation loss 0.04587263613939285 Accuracy 0.8887500166893005\n",
      "Iteration 68530 Training loss 1.720186810416635e-05 Validation loss 0.045846812427043915 Accuracy 0.889750063419342\n",
      "Iteration 68540 Training loss 1.5214918676065281e-05 Validation loss 0.04584016278386116 Accuracy 0.8896250128746033\n",
      "Iteration 68550 Training loss 0.0012744767591357231 Validation loss 0.04578995704650879 Accuracy 0.8892500400543213\n",
      "Iteration 68560 Training loss 0.0012696704361587763 Validation loss 0.04576147347688675 Accuracy 0.889750063419342\n",
      "Iteration 68570 Training loss 0.0012620900524780154 Validation loss 0.04574961215257645 Accuracy 0.890250027179718\n",
      "Iteration 68580 Training loss 1.6689735275576822e-05 Validation loss 0.04574093595147133 Accuracy 0.89000004529953\n",
      "Iteration 68590 Training loss 1.6414651327067986e-05 Validation loss 0.045745600014925 Accuracy 0.890250027179718\n",
      "Iteration 68600 Training loss 1.2842203432228416e-05 Validation loss 0.04575604572892189 Accuracy 0.8896250128746033\n",
      "Iteration 68610 Training loss 0.002514548134058714 Validation loss 0.045762889087200165 Accuracy 0.889875054359436\n",
      "Iteration 68620 Training loss 0.0012632813304662704 Validation loss 0.0457584373652935 Accuracy 0.889750063419342\n",
      "Iteration 68630 Training loss 0.0050194417126476765 Validation loss 0.04573090001940727 Accuracy 0.889750063419342\n",
      "Iteration 68640 Training loss 1.4973697943787556e-05 Validation loss 0.04573569819331169 Accuracy 0.8896250128746033\n",
      "Iteration 68650 Training loss 0.002514475490897894 Validation loss 0.04577457532286644 Accuracy 0.8896250128746033\n",
      "Iteration 68660 Training loss 0.002522276248782873 Validation loss 0.04575153440237045 Accuracy 0.889750063419342\n",
      "Iteration 68670 Training loss 0.002508149016648531 Validation loss 0.045748140662908554 Accuracy 0.8896250128746033\n",
      "Iteration 68680 Training loss 0.001267382176592946 Validation loss 0.045741815119981766 Accuracy 0.889750063419342\n",
      "Iteration 68690 Training loss 0.0012690145522356033 Validation loss 0.04574880376458168 Accuracy 0.8893750309944153\n",
      "Iteration 68700 Training loss 1.3618025150208268e-05 Validation loss 0.04575187340378761 Accuracy 0.8896250128746033\n",
      "Iteration 68710 Training loss 0.0012633108999580145 Validation loss 0.04577316343784332 Accuracy 0.8892500400543213\n",
      "Iteration 68720 Training loss 1.3110342479194514e-05 Validation loss 0.04575987160205841 Accuracy 0.8891250491142273\n",
      "Iteration 68730 Training loss 0.0037619739305227995 Validation loss 0.04573575779795647 Accuracy 0.8895000219345093\n",
      "Iteration 68740 Training loss 1.5042644008644857e-05 Validation loss 0.04574207961559296 Accuracy 0.889750063419342\n",
      "Iteration 68750 Training loss 0.003767919959500432 Validation loss 0.04572996497154236 Accuracy 0.8892500400543213\n",
      "Iteration 68760 Training loss 1.2780022188962903e-05 Validation loss 0.045722294598817825 Accuracy 0.8893750309944153\n",
      "Iteration 68770 Training loss 0.002517096232622862 Validation loss 0.045721620321273804 Accuracy 0.8893750309944153\n",
      "Iteration 68780 Training loss 0.003770987968891859 Validation loss 0.04573475569486618 Accuracy 0.8893750309944153\n",
      "Iteration 68790 Training loss 0.002521070884540677 Validation loss 0.04576192796230316 Accuracy 0.889750063419342\n",
      "Iteration 68800 Training loss 0.0012627459364011884 Validation loss 0.04582476243376732 Accuracy 0.8891250491142273\n",
      "Iteration 68810 Training loss 0.002516751177608967 Validation loss 0.045767832547426224 Accuracy 0.8892500400543213\n",
      "Iteration 68820 Training loss 0.001263859448954463 Validation loss 0.04577649012207985 Accuracy 0.889875054359436\n",
      "Iteration 68830 Training loss 0.0012585925869643688 Validation loss 0.04577388986945152 Accuracy 0.8896250128746033\n",
      "Iteration 68840 Training loss 1.8459335478837602e-05 Validation loss 0.045727748423814774 Accuracy 0.889875054359436\n",
      "Iteration 68850 Training loss 0.0012660969514399767 Validation loss 0.04571631923317909 Accuracy 0.89000004529953\n",
      "Iteration 68860 Training loss 0.0012642081128433347 Validation loss 0.04571108520030975 Accuracy 0.889750063419342\n",
      "Iteration 68870 Training loss 0.0025134251918643713 Validation loss 0.04571324959397316 Accuracy 0.889875054359436\n",
      "Iteration 68880 Training loss 0.003765152534469962 Validation loss 0.04573143273591995 Accuracy 0.8895000219345093\n",
      "Iteration 68890 Training loss 0.0012639255728572607 Validation loss 0.04573572799563408 Accuracy 0.889750063419342\n",
      "Iteration 68900 Training loss 0.0025134107563644648 Validation loss 0.045748040080070496 Accuracy 0.889875054359436\n",
      "Iteration 68910 Training loss 1.5442601579707116e-05 Validation loss 0.04574701189994812 Accuracy 0.8896250128746033\n",
      "Iteration 68920 Training loss 0.0025124039966613054 Validation loss 0.04578087851405144 Accuracy 0.8892500400543213\n",
      "Iteration 68930 Training loss 0.0012692869640886784 Validation loss 0.045744989067316055 Accuracy 0.8896250128746033\n",
      "Iteration 68940 Training loss 0.0025152850430458784 Validation loss 0.045742813497781754 Accuracy 0.8893750309944153\n",
      "Iteration 68950 Training loss 0.001262854551896453 Validation loss 0.04575150087475777 Accuracy 0.8893750309944153\n",
      "Iteration 68960 Training loss 0.0012656579492613673 Validation loss 0.04573846235871315 Accuracy 0.8890000581741333\n",
      "Iteration 68970 Training loss 0.002513585612177849 Validation loss 0.04572511836886406 Accuracy 0.8893750309944153\n",
      "Iteration 68980 Training loss 0.001261580386199057 Validation loss 0.04575170949101448 Accuracy 0.8892500400543213\n",
      "Iteration 68990 Training loss 0.001268662977963686 Validation loss 0.04574696719646454 Accuracy 0.8895000219345093\n",
      "Iteration 69000 Training loss 0.002513980260118842 Validation loss 0.04575539007782936 Accuracy 0.8888750672340393\n",
      "Iteration 69010 Training loss 1.4087408089835662e-05 Validation loss 0.04573111981153488 Accuracy 0.8895000219345093\n",
      "Iteration 69020 Training loss 0.0025161318480968475 Validation loss 0.045744020491838455 Accuracy 0.8891250491142273\n",
      "Iteration 69030 Training loss 0.0012636580504477024 Validation loss 0.04572545737028122 Accuracy 0.8895000219345093\n",
      "Iteration 69040 Training loss 0.0012653387384489179 Validation loss 0.045716218650341034 Accuracy 0.890125036239624\n",
      "Iteration 69050 Training loss 0.001263733021914959 Validation loss 0.045718591660261154 Accuracy 0.8896250128746033\n",
      "Iteration 69060 Training loss 0.001262593548744917 Validation loss 0.04570569097995758 Accuracy 0.8892500400543213\n",
      "Iteration 69070 Training loss 1.851472188718617e-05 Validation loss 0.04572637751698494 Accuracy 0.8893750309944153\n",
      "Iteration 69080 Training loss 0.0012612214777618647 Validation loss 0.045703642070293427 Accuracy 0.889750063419342\n",
      "Iteration 69090 Training loss 1.06451097963145e-05 Validation loss 0.04570859670639038 Accuracy 0.8891250491142273\n",
      "Iteration 69100 Training loss 1.1970643754466437e-05 Validation loss 0.045716915279626846 Accuracy 0.8893750309944153\n",
      "Iteration 69110 Training loss 0.0012601237976923585 Validation loss 0.04568912833929062 Accuracy 0.8892500400543213\n",
      "Iteration 69120 Training loss 1.659362533246167e-05 Validation loss 0.045786283910274506 Accuracy 0.890125036239624\n",
      "Iteration 69130 Training loss 0.0012633528094738722 Validation loss 0.045765869319438934 Accuracy 0.89000004529953\n",
      "Iteration 69140 Training loss 0.0012623593211174011 Validation loss 0.04574177786707878 Accuracy 0.889875054359436\n",
      "Iteration 69150 Training loss 0.002511349506676197 Validation loss 0.04574700444936752 Accuracy 0.8896250128746033\n",
      "Iteration 69160 Training loss 0.0012638575863093138 Validation loss 0.04574739560484886 Accuracy 0.889750063419342\n",
      "Iteration 69170 Training loss 1.2776117728208192e-05 Validation loss 0.045741189271211624 Accuracy 0.8895000219345093\n",
      "Iteration 69180 Training loss 0.002523488365113735 Validation loss 0.045718561857938766 Accuracy 0.889750063419342\n",
      "Iteration 69190 Training loss 0.0012669044081121683 Validation loss 0.04575961455702782 Accuracy 0.8892500400543213\n",
      "Iteration 69200 Training loss 0.0012725330889225006 Validation loss 0.045723576098680496 Accuracy 0.8896250128746033\n",
      "Iteration 69210 Training loss 0.0025191456079483032 Validation loss 0.04575120657682419 Accuracy 0.8895000219345093\n",
      "Iteration 69220 Training loss 1.3097866940370295e-05 Validation loss 0.045752011239528656 Accuracy 0.889875054359436\n",
      "Iteration 69230 Training loss 1.1445780728536192e-05 Validation loss 0.04574783146381378 Accuracy 0.889750063419342\n",
      "Iteration 69240 Training loss 1.593214983586222e-05 Validation loss 0.045751381665468216 Accuracy 0.889750063419342\n",
      "Iteration 69250 Training loss 0.003759565530344844 Validation loss 0.04575030878186226 Accuracy 0.889750063419342\n",
      "Iteration 69260 Training loss 1.799182609829586e-05 Validation loss 0.045763757079839706 Accuracy 0.8896250128746033\n",
      "Iteration 69270 Training loss 1.6508993212482892e-05 Validation loss 0.04574966058135033 Accuracy 0.8893750309944153\n",
      "Iteration 69280 Training loss 1.1859419828397222e-05 Validation loss 0.045753322541713715 Accuracy 0.8896250128746033\n",
      "Iteration 69290 Training loss 0.005013794172555208 Validation loss 0.0457686111330986 Accuracy 0.8887500166893005\n",
      "Iteration 69300 Training loss 1.3703040167456493e-05 Validation loss 0.04577835276722908 Accuracy 0.8888750672340393\n",
      "Iteration 69310 Training loss 0.001266517210751772 Validation loss 0.04575777053833008 Accuracy 0.8893750309944153\n",
      "Iteration 69320 Training loss 0.00127111014444381 Validation loss 0.04575483873486519 Accuracy 0.8893750309944153\n",
      "Iteration 69330 Training loss 1.2687725757132284e-05 Validation loss 0.0457947701215744 Accuracy 0.8893750309944153\n",
      "Iteration 69340 Training loss 0.0025165253318846226 Validation loss 0.0457843653857708 Accuracy 0.8888750672340393\n",
      "Iteration 69350 Training loss 0.0012669728603214025 Validation loss 0.04576728120446205 Accuracy 0.8888750672340393\n",
      "Iteration 69360 Training loss 0.0012638289481401443 Validation loss 0.045764774084091187 Accuracy 0.8888750672340393\n",
      "Iteration 69370 Training loss 0.003758214646950364 Validation loss 0.045777566730976105 Accuracy 0.8886250257492065\n",
      "Iteration 69380 Training loss 0.0025194755289703608 Validation loss 0.04579168185591698 Accuracy 0.8886250257492065\n",
      "Iteration 69390 Training loss 0.0012598177418112755 Validation loss 0.045755140483379364 Accuracy 0.8892500400543213\n",
      "Iteration 69400 Training loss 0.0037695036735385656 Validation loss 0.045732468366622925 Accuracy 0.8891250491142273\n",
      "Iteration 69410 Training loss 0.002512309467419982 Validation loss 0.04572137072682381 Accuracy 0.8895000219345093\n",
      "Iteration 69420 Training loss 0.0025109502021223307 Validation loss 0.04577293619513512 Accuracy 0.8890000581741333\n",
      "Iteration 69430 Training loss 0.0012660817010328174 Validation loss 0.045744430273771286 Accuracy 0.8895000219345093\n",
      "Iteration 69440 Training loss 1.099203473131638e-05 Validation loss 0.045728590339422226 Accuracy 0.8895000219345093\n",
      "Iteration 69450 Training loss 1.1844021173601504e-05 Validation loss 0.0457657128572464 Accuracy 0.8891250491142273\n",
      "Iteration 69460 Training loss 1.5101602002687287e-05 Validation loss 0.0457884706556797 Accuracy 0.8887500166893005\n",
      "Iteration 69470 Training loss 1.7665171981207095e-05 Validation loss 0.045736342668533325 Accuracy 0.8890000581741333\n",
      "Iteration 69480 Training loss 1.4034904779691715e-05 Validation loss 0.045750945806503296 Accuracy 0.8896250128746033\n",
      "Iteration 69490 Training loss 1.4259036106523126e-05 Validation loss 0.04575745016336441 Accuracy 0.8891250491142273\n",
      "Iteration 69500 Training loss 0.0050114733166992664 Validation loss 0.04577900096774101 Accuracy 0.8896250128746033\n",
      "Iteration 69510 Training loss 0.0012632583966478705 Validation loss 0.0457613505423069 Accuracy 0.8895000219345093\n",
      "Iteration 69520 Training loss 0.0037660938687622547 Validation loss 0.04573509097099304 Accuracy 0.889750063419342\n",
      "Iteration 69530 Training loss 0.0012614962179213762 Validation loss 0.04576306790113449 Accuracy 0.8893750309944153\n",
      "Iteration 69540 Training loss 1.217286080645863e-05 Validation loss 0.045757051557302475 Accuracy 0.8887500166893005\n",
      "Iteration 69550 Training loss 1.5540337699349038e-05 Validation loss 0.04576487839221954 Accuracy 0.8890000581741333\n",
      "Iteration 69560 Training loss 1.3962779121357016e-05 Validation loss 0.04577776789665222 Accuracy 0.8888750672340393\n",
      "Iteration 69570 Training loss 1.0199512871622574e-05 Validation loss 0.045774444937705994 Accuracy 0.8888750672340393\n",
      "Iteration 69580 Training loss 0.0025117876939475536 Validation loss 0.04574403539299965 Accuracy 0.8890000581741333\n",
      "Iteration 69590 Training loss 0.0025107734836637974 Validation loss 0.0457223542034626 Accuracy 0.8892500400543213\n",
      "Iteration 69600 Training loss 0.001262392965145409 Validation loss 0.04572952538728714 Accuracy 0.8893750309944153\n",
      "Iteration 69610 Training loss 1.9183024051017128e-05 Validation loss 0.045725222676992416 Accuracy 0.8896250128746033\n",
      "Iteration 69620 Training loss 1.4190417459758464e-05 Validation loss 0.04575846344232559 Accuracy 0.8893750309944153\n",
      "Iteration 69630 Training loss 1.1445668860687874e-05 Validation loss 0.045722704380750656 Accuracy 0.8891250491142273\n",
      "Iteration 69640 Training loss 0.0012677147751674056 Validation loss 0.04579654708504677 Accuracy 0.889750063419342\n",
      "Iteration 69650 Training loss 0.005014440976083279 Validation loss 0.04577130451798439 Accuracy 0.8892500400543213\n",
      "Iteration 69660 Training loss 0.0037610852159559727 Validation loss 0.04576127231121063 Accuracy 0.8892500400543213\n",
      "Iteration 69670 Training loss 0.002516736276447773 Validation loss 0.04575329273939133 Accuracy 0.8893750309944153\n",
      "Iteration 69680 Training loss 0.0037676107604056597 Validation loss 0.045779164880514145 Accuracy 0.8890000581741333\n",
      "Iteration 69690 Training loss 0.005010105203837156 Validation loss 0.04576749727129936 Accuracy 0.8890000581741333\n",
      "Iteration 69700 Training loss 0.0012612935388460755 Validation loss 0.045782264322042465 Accuracy 0.8887500166893005\n",
      "Iteration 69710 Training loss 0.0025164082180708647 Validation loss 0.04577091708779335 Accuracy 0.8890000581741333\n",
      "Iteration 69720 Training loss 0.001259206677787006 Validation loss 0.04576749727129936 Accuracy 0.8888750672340393\n",
      "Iteration 69730 Training loss 0.0012602959759533405 Validation loss 0.04577217996120453 Accuracy 0.8888750672340393\n",
      "Iteration 69740 Training loss 0.0012659912463277578 Validation loss 0.04577594995498657 Accuracy 0.8888750672340393\n",
      "Iteration 69750 Training loss 0.0012647517723962665 Validation loss 0.04575078561902046 Accuracy 0.8893750309944153\n",
      "Iteration 69760 Training loss 0.0025146135594695807 Validation loss 0.0457414947450161 Accuracy 0.8890000581741333\n",
      "Iteration 69770 Training loss 0.0012628587428480387 Validation loss 0.04573237895965576 Accuracy 0.8891250491142273\n",
      "Iteration 69780 Training loss 1.2450651411199942e-05 Validation loss 0.04573763161897659 Accuracy 0.8891250491142273\n",
      "Iteration 69790 Training loss 0.0012648619012907147 Validation loss 0.04573914036154747 Accuracy 0.8891250491142273\n",
      "Iteration 69800 Training loss 0.0012629517586901784 Validation loss 0.04572676122188568 Accuracy 0.8890000581741333\n",
      "Iteration 69810 Training loss 1.0550886145210825e-05 Validation loss 0.04570873826742172 Accuracy 0.8891250491142273\n",
      "Iteration 69820 Training loss 0.003766931127756834 Validation loss 0.04574419930577278 Accuracy 0.8890000581741333\n",
      "Iteration 69830 Training loss 0.0012645360548049212 Validation loss 0.045718684792518616 Accuracy 0.8892500400543213\n",
      "Iteration 69840 Training loss 0.001265042694285512 Validation loss 0.04573623836040497 Accuracy 0.8887500166893005\n",
      "Iteration 69850 Training loss 1.1749405530281365e-05 Validation loss 0.04572595655918121 Accuracy 0.8890000581741333\n",
      "Iteration 69860 Training loss 0.0012610572157427669 Validation loss 0.04572248086333275 Accuracy 0.8892500400543213\n",
      "Iteration 69870 Training loss 0.003764207474887371 Validation loss 0.045746974647045135 Accuracy 0.8893750309944153\n",
      "Iteration 69880 Training loss 1.4630757505074143e-05 Validation loss 0.045724932104349136 Accuracy 0.8893750309944153\n",
      "Iteration 69890 Training loss 0.00251319189555943 Validation loss 0.04572083428502083 Accuracy 0.8893750309944153\n",
      "Iteration 69900 Training loss 0.001265641418285668 Validation loss 0.045730624347925186 Accuracy 0.8892500400543213\n",
      "Iteration 69910 Training loss 0.0025208324659615755 Validation loss 0.045728616416454315 Accuracy 0.889750063419342\n",
      "Iteration 69920 Training loss 1.733098724798765e-05 Validation loss 0.04574381560087204 Accuracy 0.8895000219345093\n",
      "Iteration 69930 Training loss 0.0025100859347730875 Validation loss 0.04570420831441879 Accuracy 0.8891250491142273\n",
      "Iteration 69940 Training loss 0.0012609210098162293 Validation loss 0.04570779576897621 Accuracy 0.8893750309944153\n",
      "Iteration 69950 Training loss 0.0012616781750693917 Validation loss 0.045772798359394073 Accuracy 0.8892500400543213\n",
      "Iteration 69960 Training loss 1.1971509593422525e-05 Validation loss 0.04576009511947632 Accuracy 0.8892500400543213\n",
      "Iteration 69970 Training loss 0.0012619964545592666 Validation loss 0.04573987051844597 Accuracy 0.8892500400543213\n",
      "Iteration 69980 Training loss 0.0025114906020462513 Validation loss 0.0457359254360199 Accuracy 0.8892500400543213\n",
      "Iteration 69990 Training loss 0.0012645269744098186 Validation loss 0.04576895013451576 Accuracy 0.8891250491142273\n",
      "Iteration 70000 Training loss 0.001263718120753765 Validation loss 0.045758433640003204 Accuracy 0.8891250491142273\n",
      "Iteration 70010 Training loss 0.0012677505146712065 Validation loss 0.04576805233955383 Accuracy 0.8893750309944153\n",
      "Iteration 70020 Training loss 0.0012665700633078814 Validation loss 0.04575955495238304 Accuracy 0.8890000581741333\n",
      "Iteration 70030 Training loss 0.003761672880500555 Validation loss 0.045754168182611465 Accuracy 0.8895000219345093\n",
      "Iteration 70040 Training loss 1.2230440916027874e-05 Validation loss 0.04570874571800232 Accuracy 0.8891250491142273\n",
      "Iteration 70050 Training loss 0.0012615267187356949 Validation loss 0.04573670029640198 Accuracy 0.8890000581741333\n",
      "Iteration 70060 Training loss 0.001262775040231645 Validation loss 0.045767560601234436 Accuracy 0.8893750309944153\n",
      "Iteration 70070 Training loss 1.226870335813146e-05 Validation loss 0.045748330652713776 Accuracy 0.8890000581741333\n",
      "Iteration 70080 Training loss 0.0012640500208362937 Validation loss 0.0457572266459465 Accuracy 0.8892500400543213\n",
      "Iteration 70090 Training loss 0.0025127208791673183 Validation loss 0.04574330523610115 Accuracy 0.8895000219345093\n",
      "Iteration 70100 Training loss 1.201122813654365e-05 Validation loss 0.04572714865207672 Accuracy 0.8892500400543213\n",
      "Iteration 70110 Training loss 0.006263736169785261 Validation loss 0.04574859142303467 Accuracy 0.8893750309944153\n",
      "Iteration 70120 Training loss 0.0012615940067917109 Validation loss 0.045735735446214676 Accuracy 0.8891250491142273\n",
      "Iteration 70130 Training loss 0.0012679051142185926 Validation loss 0.04570891708135605 Accuracy 0.8893750309944153\n",
      "Iteration 70140 Training loss 0.001266423729248345 Validation loss 0.04573605954647064 Accuracy 0.8888750672340393\n",
      "Iteration 70150 Training loss 0.0012616879539564252 Validation loss 0.045750051736831665 Accuracy 0.8893750309944153\n",
      "Iteration 70160 Training loss 0.002511309226974845 Validation loss 0.04574545845389366 Accuracy 0.8891250491142273\n",
      "Iteration 70170 Training loss 0.0037699767854064703 Validation loss 0.04577341675758362 Accuracy 0.8893750309944153\n",
      "Iteration 70180 Training loss 0.001266144448891282 Validation loss 0.045735619962215424 Accuracy 0.8893750309944153\n",
      "Iteration 70190 Training loss 1.0325837138225324e-05 Validation loss 0.04571909457445145 Accuracy 0.8892500400543213\n",
      "Iteration 70200 Training loss 0.002516260137781501 Validation loss 0.04575921967625618 Accuracy 0.8893750309944153\n",
      "Iteration 70210 Training loss 1.4561400348611642e-05 Validation loss 0.04571721330285072 Accuracy 0.8888750672340393\n",
      "Iteration 70220 Training loss 0.005012763198465109 Validation loss 0.045774705708026886 Accuracy 0.8888750672340393\n",
      "Iteration 70230 Training loss 0.0025123623199760914 Validation loss 0.04577513039112091 Accuracy 0.8891250491142273\n",
      "Iteration 70240 Training loss 1.0905048839049414e-05 Validation loss 0.0457753911614418 Accuracy 0.8888750672340393\n",
      "Iteration 70250 Training loss 0.0012672635493800044 Validation loss 0.04578161984682083 Accuracy 0.8892500400543213\n",
      "Iteration 70260 Training loss 1.0904440387093928e-05 Validation loss 0.045752108097076416 Accuracy 0.8891250491142273\n",
      "Iteration 70270 Training loss 1.4687557268189266e-05 Validation loss 0.04572570323944092 Accuracy 0.8891250491142273\n",
      "Iteration 70280 Training loss 0.0012660569045692682 Validation loss 0.04576642066240311 Accuracy 0.8885000348091125\n",
      "Iteration 70290 Training loss 1.1927747436857317e-05 Validation loss 0.04577019810676575 Accuracy 0.8886250257492065\n",
      "Iteration 70300 Training loss 0.0012593644205480814 Validation loss 0.045742813497781754 Accuracy 0.8888750672340393\n",
      "Iteration 70310 Training loss 0.001267240266315639 Validation loss 0.045785460621118546 Accuracy 0.8886250257492065\n",
      "Iteration 70320 Training loss 1.2890283869637642e-05 Validation loss 0.045756347477436066 Accuracy 0.8887500166893005\n",
      "Iteration 70330 Training loss 1.073221028491389e-05 Validation loss 0.045759811997413635 Accuracy 0.8887500166893005\n",
      "Iteration 70340 Training loss 0.0012611986603587866 Validation loss 0.04573346674442291 Accuracy 0.8890000581741333\n",
      "Iteration 70350 Training loss 1.4254498637455981e-05 Validation loss 0.04574940726161003 Accuracy 0.8886250257492065\n",
      "Iteration 70360 Training loss 0.00127304473426193 Validation loss 0.04574861750006676 Accuracy 0.8888750672340393\n",
      "Iteration 70370 Training loss 0.0012603843351826072 Validation loss 0.045759137719869614 Accuracy 0.8887500166893005\n",
      "Iteration 70380 Training loss 1.6046002201619558e-05 Validation loss 0.0457935705780983 Accuracy 0.8887500166893005\n",
      "Iteration 70390 Training loss 0.002514606574550271 Validation loss 0.04579583927989006 Accuracy 0.8892500400543213\n",
      "Iteration 70400 Training loss 0.001264067366719246 Validation loss 0.04576574265956879 Accuracy 0.8890000581741333\n",
      "Iteration 70410 Training loss 0.001259088283404708 Validation loss 0.04575032368302345 Accuracy 0.8892500400543213\n",
      "Iteration 70420 Training loss 0.001269318163394928 Validation loss 0.04578504338860512 Accuracy 0.8890000581741333\n",
      "Iteration 70430 Training loss 0.001269786385819316 Validation loss 0.045781198889017105 Accuracy 0.8891250491142273\n",
      "Iteration 70440 Training loss 0.0025201370008289814 Validation loss 0.045781634747982025 Accuracy 0.8891250491142273\n",
      "Iteration 70450 Training loss 0.0050086150877177715 Validation loss 0.04580000042915344 Accuracy 0.8891250491142273\n",
      "Iteration 70460 Training loss 0.0012624385999515653 Validation loss 0.045786093920469284 Accuracy 0.8891250491142273\n",
      "Iteration 70470 Training loss 1.1288053428870626e-05 Validation loss 0.04575692117214203 Accuracy 0.8890000581741333\n",
      "Iteration 70480 Training loss 0.005013518035411835 Validation loss 0.04575624316930771 Accuracy 0.8890000581741333\n",
      "Iteration 70490 Training loss 0.002513832412660122 Validation loss 0.045751072466373444 Accuracy 0.8891250491142273\n",
      "Iteration 70500 Training loss 0.0025111897848546505 Validation loss 0.04575946182012558 Accuracy 0.8891250491142273\n",
      "Iteration 70510 Training loss 0.0012645323295146227 Validation loss 0.04574855417013168 Accuracy 0.8887500166893005\n",
      "Iteration 70520 Training loss 0.0012606068048626184 Validation loss 0.045753009617328644 Accuracy 0.8890000581741333\n",
      "Iteration 70530 Training loss 0.001263271551579237 Validation loss 0.045756611973047256 Accuracy 0.8891250491142273\n",
      "Iteration 70540 Training loss 1.3834279343427625e-05 Validation loss 0.04576171189546585 Accuracy 0.8892500400543213\n",
      "Iteration 70550 Training loss 0.0025141166988760233 Validation loss 0.04579152539372444 Accuracy 0.8886250257492065\n",
      "Iteration 70560 Training loss 0.0012650233693420887 Validation loss 0.04581092298030853 Accuracy 0.8885000348091125\n",
      "Iteration 70570 Training loss 0.0025104754604399204 Validation loss 0.04578469321131706 Accuracy 0.8890000581741333\n",
      "Iteration 70580 Training loss 0.0012619083281606436 Validation loss 0.04576937481760979 Accuracy 0.8887500166893005\n",
      "Iteration 70590 Training loss 0.0025139441713690758 Validation loss 0.04575061425566673 Accuracy 0.8888750672340393\n",
      "Iteration 70600 Training loss 1.1607005035330076e-05 Validation loss 0.04575885087251663 Accuracy 0.8893750309944153\n",
      "Iteration 70610 Training loss 0.0025095385499298573 Validation loss 0.04574437066912651 Accuracy 0.8891250491142273\n",
      "Iteration 70620 Training loss 0.002508706646040082 Validation loss 0.04575983062386513 Accuracy 0.8891250491142273\n",
      "Iteration 70630 Training loss 0.0012606773525476456 Validation loss 0.04575106129050255 Accuracy 0.8891250491142273\n",
      "Iteration 70640 Training loss 1.0847539670066908e-05 Validation loss 0.04574960470199585 Accuracy 0.8890000581741333\n",
      "Iteration 70650 Training loss 1.4244783415051643e-05 Validation loss 0.045774202793836594 Accuracy 0.8893750309944153\n",
      "Iteration 70660 Training loss 0.002511515049263835 Validation loss 0.04574970901012421 Accuracy 0.8891250491142273\n",
      "Iteration 70670 Training loss 1.7891299648908898e-05 Validation loss 0.04577610269188881 Accuracy 0.8895000219345093\n",
      "Iteration 70680 Training loss 0.002519279718399048 Validation loss 0.04576663300395012 Accuracy 0.8891250491142273\n",
      "Iteration 70690 Training loss 1.1778039151977282e-05 Validation loss 0.0457751490175724 Accuracy 0.8891250491142273\n",
      "Iteration 70700 Training loss 0.003769289469346404 Validation loss 0.04576703533530235 Accuracy 0.8893750309944153\n",
      "Iteration 70710 Training loss 0.002512851497158408 Validation loss 0.04579224810004234 Accuracy 0.8891250491142273\n",
      "Iteration 70720 Training loss 0.0012699727667495608 Validation loss 0.045780908316373825 Accuracy 0.8891250491142273\n",
      "Iteration 70730 Training loss 0.0025087681133300066 Validation loss 0.04576199874281883 Accuracy 0.8890000581741333\n",
      "Iteration 70740 Training loss 0.0012641192879527807 Validation loss 0.045766621828079224 Accuracy 0.8891250491142273\n",
      "Iteration 70750 Training loss 0.0025185714475810528 Validation loss 0.045815855264663696 Accuracy 0.8886250257492065\n",
      "Iteration 70760 Training loss 0.0012655920581892133 Validation loss 0.0457664430141449 Accuracy 0.8891250491142273\n",
      "Iteration 70770 Training loss 1.567619437992107e-05 Validation loss 0.045764241367578506 Accuracy 0.8891250491142273\n",
      "Iteration 70780 Training loss 0.001263283658772707 Validation loss 0.04578881338238716 Accuracy 0.8891250491142273\n",
      "Iteration 70790 Training loss 0.0025191479362547398 Validation loss 0.04577166214585304 Accuracy 0.8892500400543213\n",
      "Iteration 70800 Training loss 0.006260331720113754 Validation loss 0.04577312618494034 Accuracy 0.8888750672340393\n",
      "Iteration 70810 Training loss 1.6985399270197377e-05 Validation loss 0.04575537145137787 Accuracy 0.8888750672340393\n",
      "Iteration 70820 Training loss 1.2742834769596811e-05 Validation loss 0.04578379914164543 Accuracy 0.8892500400543213\n",
      "Iteration 70830 Training loss 0.0037631557788699865 Validation loss 0.04577447474002838 Accuracy 0.8892500400543213\n",
      "Iteration 70840 Training loss 0.0012615754967555404 Validation loss 0.045752327889204025 Accuracy 0.8888750672340393\n",
      "Iteration 70850 Training loss 1.6294099623337388e-05 Validation loss 0.04576512798666954 Accuracy 0.8891250491142273\n",
      "Iteration 70860 Training loss 1.112015434046043e-05 Validation loss 0.0457635372877121 Accuracy 0.8890000581741333\n",
      "Iteration 70870 Training loss 0.0012627213727682829 Validation loss 0.04575725644826889 Accuracy 0.8892500400543213\n",
      "Iteration 70880 Training loss 0.0012627614196389914 Validation loss 0.04577162489295006 Accuracy 0.8892500400543213\n",
      "Iteration 70890 Training loss 0.001267191837541759 Validation loss 0.04576936736702919 Accuracy 0.8892500400543213\n",
      "Iteration 70900 Training loss 1.4420846127904952e-05 Validation loss 0.04579427093267441 Accuracy 0.8892500400543213\n",
      "Iteration 70910 Training loss 0.0012657996267080307 Validation loss 0.045793671160936356 Accuracy 0.8891250491142273\n",
      "Iteration 70920 Training loss 1.2495396731537767e-05 Validation loss 0.04579325392842293 Accuracy 0.8891250491142273\n",
      "Iteration 70930 Training loss 0.0012644876260310411 Validation loss 0.04576282575726509 Accuracy 0.8892500400543213\n",
      "Iteration 70940 Training loss 0.0025130948051810265 Validation loss 0.045768000185489655 Accuracy 0.8893750309944153\n",
      "Iteration 70950 Training loss 1.5979305317159742e-05 Validation loss 0.04579615220427513 Accuracy 0.8888750672340393\n",
      "Iteration 70960 Training loss 0.0012667931150645018 Validation loss 0.04576750472187996 Accuracy 0.8893750309944153\n",
      "Iteration 70970 Training loss 1.6350415535271168e-05 Validation loss 0.0457700751721859 Accuracy 0.8895000219345093\n",
      "Iteration 70980 Training loss 0.0012632308062165976 Validation loss 0.045760829001665115 Accuracy 0.8892500400543213\n",
      "Iteration 70990 Training loss 0.002513530198484659 Validation loss 0.04577569663524628 Accuracy 0.8892500400543213\n",
      "Iteration 71000 Training loss 0.002515203319489956 Validation loss 0.045754700899124146 Accuracy 0.8891250491142273\n",
      "Iteration 71010 Training loss 0.001263175974600017 Validation loss 0.045757945626974106 Accuracy 0.8893750309944153\n",
      "Iteration 71020 Training loss 1.2631136996787973e-05 Validation loss 0.04577318951487541 Accuracy 0.8890000581741333\n",
      "Iteration 71030 Training loss 0.0012605591909959912 Validation loss 0.04575089365243912 Accuracy 0.8892500400543213\n",
      "Iteration 71040 Training loss 1.61260504683014e-05 Validation loss 0.04577692598104477 Accuracy 0.8891250491142273\n",
      "Iteration 71050 Training loss 0.0012615130981430411 Validation loss 0.04575257748365402 Accuracy 0.8891250491142273\n",
      "Iteration 71060 Training loss 1.297069411521079e-05 Validation loss 0.04577522352337837 Accuracy 0.8890000581741333\n",
      "Iteration 71070 Training loss 1.2842547221225686e-05 Validation loss 0.045764654874801636 Accuracy 0.8895000219345093\n",
      "Iteration 71080 Training loss 1.2523488294391427e-05 Validation loss 0.045758266001939774 Accuracy 0.8890000581741333\n",
      "Iteration 71090 Training loss 0.0012626346433535218 Validation loss 0.04575450345873833 Accuracy 0.8891250491142273\n",
      "Iteration 71100 Training loss 1.9159906514687464e-05 Validation loss 0.04576822742819786 Accuracy 0.8887500166893005\n",
      "Iteration 71110 Training loss 1.3201854926592205e-05 Validation loss 0.04578420892357826 Accuracy 0.8891250491142273\n",
      "Iteration 71120 Training loss 1.3504992239177227e-05 Validation loss 0.045799024403095245 Accuracy 0.8892500400543213\n",
      "Iteration 71130 Training loss 1.365856223856099e-05 Validation loss 0.04579934477806091 Accuracy 0.8890000581741333\n",
      "Iteration 71140 Training loss 0.002513968851417303 Validation loss 0.045774850994348526 Accuracy 0.8890000581741333\n",
      "Iteration 71150 Training loss 1.4058295164431911e-05 Validation loss 0.04575006663799286 Accuracy 0.8891250491142273\n",
      "Iteration 71160 Training loss 0.0012646623654291034 Validation loss 0.04575055092573166 Accuracy 0.8890000581741333\n",
      "Iteration 71170 Training loss 0.0012698409846052527 Validation loss 0.0457601398229599 Accuracy 0.8890000581741333\n",
      "Iteration 71180 Training loss 1.6307467376464047e-05 Validation loss 0.045755550265312195 Accuracy 0.8895000219345093\n",
      "Iteration 71190 Training loss 0.002515957225114107 Validation loss 0.04576210305094719 Accuracy 0.8892500400543213\n",
      "Iteration 71200 Training loss 1.818869714043103e-05 Validation loss 0.0457485094666481 Accuracy 0.8887500166893005\n",
      "Iteration 71210 Training loss 1.1162992450408638e-05 Validation loss 0.045769333839416504 Accuracy 0.8887500166893005\n",
      "Iteration 71220 Training loss 0.0012595495209097862 Validation loss 0.04578360170125961 Accuracy 0.8892500400543213\n",
      "Iteration 71230 Training loss 1.4108902178122662e-05 Validation loss 0.04574531689286232 Accuracy 0.8895000219345093\n",
      "Iteration 71240 Training loss 1.424616675649304e-05 Validation loss 0.045771706849336624 Accuracy 0.8896250128746033\n",
      "Iteration 71250 Training loss 0.0025067823007702827 Validation loss 0.0457732230424881 Accuracy 0.8890000581741333\n",
      "Iteration 71260 Training loss 0.0025109313428401947 Validation loss 0.045838356018066406 Accuracy 0.8888750672340393\n",
      "Iteration 71270 Training loss 0.0012722143437713385 Validation loss 0.04577990248799324 Accuracy 0.8892500400543213\n",
      "Iteration 71280 Training loss 0.0012600247282534838 Validation loss 0.04579305276274681 Accuracy 0.8891250491142273\n",
      "Iteration 71290 Training loss 0.002512909471988678 Validation loss 0.04577724635601044 Accuracy 0.8893750309944153\n",
      "Iteration 71300 Training loss 0.0012610029662027955 Validation loss 0.04582526162266731 Accuracy 0.8886250257492065\n",
      "Iteration 71310 Training loss 0.0025143991224467754 Validation loss 0.045791156589984894 Accuracy 0.8890000581741333\n",
      "Iteration 71320 Training loss 0.001260999240912497 Validation loss 0.04581143707036972 Accuracy 0.8885000348091125\n",
      "Iteration 71330 Training loss 0.0012646024115383625 Validation loss 0.04578842222690582 Accuracy 0.8888750672340393\n",
      "Iteration 71340 Training loss 0.0037618742790073156 Validation loss 0.04580041021108627 Accuracy 0.8886250257492065\n",
      "Iteration 71350 Training loss 0.0012663875240832567 Validation loss 0.04580285772681236 Accuracy 0.8886250257492065\n",
      "Iteration 71360 Training loss 1.2261820302228443e-05 Validation loss 0.04577344283461571 Accuracy 0.8888750672340393\n",
      "Iteration 71370 Training loss 0.0012604594230651855 Validation loss 0.0457664355635643 Accuracy 0.8887500166893005\n",
      "Iteration 71380 Training loss 0.0012602726928889751 Validation loss 0.04577266797423363 Accuracy 0.8887500166893005\n",
      "Iteration 71390 Training loss 1.0430132533656433e-05 Validation loss 0.04576338827610016 Accuracy 0.8888750672340393\n",
      "Iteration 71400 Training loss 0.0012634305749088526 Validation loss 0.0457644909620285 Accuracy 0.8888750672340393\n",
      "Iteration 71410 Training loss 0.0012632135767489672 Validation loss 0.04577098414301872 Accuracy 0.8890000581741333\n",
      "Iteration 71420 Training loss 0.0012630453566089272 Validation loss 0.0457853339612484 Accuracy 0.8887500166893005\n",
      "Iteration 71430 Training loss 0.0012641040375456214 Validation loss 0.045800868421792984 Accuracy 0.8890000581741333\n",
      "Iteration 71440 Training loss 0.003761380910873413 Validation loss 0.045801855623722076 Accuracy 0.8888750672340393\n",
      "Iteration 71450 Training loss 0.002519172616302967 Validation loss 0.045797985047101974 Accuracy 0.8888750672340393\n",
      "Iteration 71460 Training loss 0.001261649071238935 Validation loss 0.04580828174948692 Accuracy 0.8891250491142273\n",
      "Iteration 71470 Training loss 1.1657806680887006e-05 Validation loss 0.045781318098306656 Accuracy 0.8891250491142273\n",
      "Iteration 71480 Training loss 0.0037612037267535925 Validation loss 0.045779433101415634 Accuracy 0.8888750672340393\n",
      "Iteration 71490 Training loss 0.005009675398468971 Validation loss 0.04578748717904091 Accuracy 0.8891250491142273\n",
      "Iteration 71500 Training loss 1.085383519239258e-05 Validation loss 0.04576626047492027 Accuracy 0.8891250491142273\n",
      "Iteration 71510 Training loss 1.3835917343385518e-05 Validation loss 0.04576864838600159 Accuracy 0.8890000581741333\n",
      "Iteration 71520 Training loss 1.1767120668082498e-05 Validation loss 0.04579651355743408 Accuracy 0.8888750672340393\n",
      "Iteration 71530 Training loss 1.4720720173500013e-05 Validation loss 0.04574224352836609 Accuracy 0.8895000219345093\n",
      "Iteration 71540 Training loss 0.0025222357362508774 Validation loss 0.04576820880174637 Accuracy 0.8891250491142273\n",
      "Iteration 71550 Training loss 1.6711470379959792e-05 Validation loss 0.04577386751770973 Accuracy 0.8891250491142273\n",
      "Iteration 71560 Training loss 1.2757924196193926e-05 Validation loss 0.04577051103115082 Accuracy 0.8892500400543213\n",
      "Iteration 71570 Training loss 1.879294177342672e-05 Validation loss 0.04576815292239189 Accuracy 0.8892500400543213\n",
      "Iteration 71580 Training loss 0.00376295973546803 Validation loss 0.04575630649924278 Accuracy 0.8893750309944153\n",
      "Iteration 71590 Training loss 0.0012640234781429172 Validation loss 0.04575957730412483 Accuracy 0.8891250491142273\n",
      "Iteration 71600 Training loss 9.161923117062543e-06 Validation loss 0.045749273151159286 Accuracy 0.8892500400543213\n",
      "Iteration 71610 Training loss 0.0037603923119604588 Validation loss 0.045799579471349716 Accuracy 0.8891250491142273\n",
      "Iteration 71620 Training loss 0.0025070367846637964 Validation loss 0.04577001556754112 Accuracy 0.8891250491142273\n",
      "Iteration 71630 Training loss 0.0012676744954660535 Validation loss 0.045756060630083084 Accuracy 0.8892500400543213\n",
      "Iteration 71640 Training loss 0.002511800965294242 Validation loss 0.04577351734042168 Accuracy 0.8890000581741333\n",
      "Iteration 71650 Training loss 0.0025151458103209734 Validation loss 0.04576435312628746 Accuracy 0.8888750672340393\n",
      "Iteration 71660 Training loss 9.096443136513699e-06 Validation loss 0.045777853578329086 Accuracy 0.8891250491142273\n",
      "Iteration 71670 Training loss 0.0037622968666255474 Validation loss 0.04579279571771622 Accuracy 0.8892500400543213\n",
      "Iteration 71680 Training loss 1.2214297385071404e-05 Validation loss 0.04577241092920303 Accuracy 0.8888750672340393\n",
      "Iteration 71690 Training loss 0.0012626308016479015 Validation loss 0.045760806649923325 Accuracy 0.8890000581741333\n",
      "Iteration 71700 Training loss 1.1870569323946256e-05 Validation loss 0.04576748237013817 Accuracy 0.8890000581741333\n",
      "Iteration 71710 Training loss 0.002510282676666975 Validation loss 0.04575341194868088 Accuracy 0.8888750672340393\n",
      "Iteration 71720 Training loss 0.0012681292137131095 Validation loss 0.04575066268444061 Accuracy 0.8890000581741333\n",
      "Iteration 71730 Training loss 0.002516003092750907 Validation loss 0.045782845467329025 Accuracy 0.8890000581741333\n",
      "Iteration 71740 Training loss 0.0012612193822860718 Validation loss 0.045775823295116425 Accuracy 0.8890000581741333\n",
      "Iteration 71750 Training loss 0.0012611527927219868 Validation loss 0.04578292369842529 Accuracy 0.8892500400543213\n",
      "Iteration 71760 Training loss 0.0012664892710745335 Validation loss 0.0457899272441864 Accuracy 0.8890000581741333\n",
      "Iteration 71770 Training loss 1.2684268767770845e-05 Validation loss 0.04577205702662468 Accuracy 0.8893750309944153\n",
      "Iteration 71780 Training loss 0.0025143425446003675 Validation loss 0.04579315334558487 Accuracy 0.8893750309944153\n",
      "Iteration 71790 Training loss 0.0037624870892614126 Validation loss 0.04577546939253807 Accuracy 0.8895000219345093\n",
      "Iteration 71800 Training loss 0.002509703626856208 Validation loss 0.04578950256109238 Accuracy 0.8893750309944153\n",
      "Iteration 71810 Training loss 0.005015005823224783 Validation loss 0.04576602205634117 Accuracy 0.8892500400543213\n",
      "Iteration 71820 Training loss 0.001262297504581511 Validation loss 0.04575174301862717 Accuracy 0.8892500400543213\n",
      "Iteration 71830 Training loss 0.0012719259830191731 Validation loss 0.04575478658080101 Accuracy 0.8892500400543213\n",
      "Iteration 71840 Training loss 2.0795285308849998e-05 Validation loss 0.045780766755342484 Accuracy 0.8891250491142273\n",
      "Iteration 71850 Training loss 0.001266204402782023 Validation loss 0.04580964148044586 Accuracy 0.8888750672340393\n",
      "Iteration 71860 Training loss 0.0037623774260282516 Validation loss 0.04581724852323532 Accuracy 0.8887500166893005\n",
      "Iteration 71870 Training loss 0.0025110370479524136 Validation loss 0.04575679451227188 Accuracy 0.8891250491142273\n",
      "Iteration 71880 Training loss 0.0012624005321413279 Validation loss 0.04578433558344841 Accuracy 0.8891250491142273\n",
      "Iteration 71890 Training loss 0.00126135244499892 Validation loss 0.04577315226197243 Accuracy 0.8891250491142273\n",
      "Iteration 71900 Training loss 1.3618173397844657e-05 Validation loss 0.04576616361737251 Accuracy 0.8890000581741333\n",
      "Iteration 71910 Training loss 1.2722583051072434e-05 Validation loss 0.04579665884375572 Accuracy 0.8890000581741333\n",
      "Iteration 71920 Training loss 0.0012636545579880476 Validation loss 0.04578222334384918 Accuracy 0.8888750672340393\n",
      "Iteration 71930 Training loss 1.3445862350636162e-05 Validation loss 0.045789122581481934 Accuracy 0.8890000581741333\n",
      "Iteration 71940 Training loss 1.1082824130426161e-05 Validation loss 0.04579092189669609 Accuracy 0.8890000581741333\n",
      "Iteration 71950 Training loss 0.003762756008654833 Validation loss 0.04579437896609306 Accuracy 0.8891250491142273\n",
      "Iteration 71960 Training loss 0.0012629944831132889 Validation loss 0.045774657279253006 Accuracy 0.8890000581741333\n",
      "Iteration 71970 Training loss 1.4238659787224606e-05 Validation loss 0.04576491937041283 Accuracy 0.8890000581741333\n",
      "Iteration 71980 Training loss 0.002523097675293684 Validation loss 0.04575502127408981 Accuracy 0.8893750309944153\n",
      "Iteration 71990 Training loss 0.0012663369998335838 Validation loss 0.0457853339612484 Accuracy 0.8891250491142273\n",
      "Iteration 72000 Training loss 1.3323821804078761e-05 Validation loss 0.0457792691886425 Accuracy 0.8887500166893005\n",
      "Iteration 72010 Training loss 0.002513319253921509 Validation loss 0.04578377678990364 Accuracy 0.8891250491142273\n",
      "Iteration 72020 Training loss 1.1858800462505314e-05 Validation loss 0.04578966647386551 Accuracy 0.8890000581741333\n",
      "Iteration 72030 Training loss 0.0012654403690248728 Validation loss 0.04582582414150238 Accuracy 0.8892500400543213\n",
      "Iteration 72040 Training loss 1.0834737622644752e-05 Validation loss 0.04580060765147209 Accuracy 0.8895000219345093\n",
      "Iteration 72050 Training loss 0.002514065243303776 Validation loss 0.04578569903969765 Accuracy 0.8893750309944153\n",
      "Iteration 72060 Training loss 0.0012629496632143855 Validation loss 0.04577939212322235 Accuracy 0.8895000219345093\n",
      "Iteration 72070 Training loss 0.0025145201943814754 Validation loss 0.04577935114502907 Accuracy 0.8893750309944153\n",
      "Iteration 72080 Training loss 0.002516578184440732 Validation loss 0.04577155411243439 Accuracy 0.8893750309944153\n",
      "Iteration 72090 Training loss 0.0025112112052738667 Validation loss 0.04580572620034218 Accuracy 0.8893750309944153\n",
      "Iteration 72100 Training loss 0.002515310188755393 Validation loss 0.04579755663871765 Accuracy 0.8891250491142273\n",
      "Iteration 72110 Training loss 1.5035988326417282e-05 Validation loss 0.04582547768950462 Accuracy 0.8890000581741333\n",
      "Iteration 72120 Training loss 9.106218385568354e-06 Validation loss 0.04578789696097374 Accuracy 0.8888750672340393\n",
      "Iteration 72130 Training loss 0.0025133842136710882 Validation loss 0.04577736184000969 Accuracy 0.8890000581741333\n",
      "Iteration 72140 Training loss 0.0012609029654413462 Validation loss 0.04578257352113724 Accuracy 0.8888750672340393\n",
      "Iteration 72150 Training loss 0.001266469364054501 Validation loss 0.045770540833473206 Accuracy 0.8895000219345093\n",
      "Iteration 72160 Training loss 0.002516823587939143 Validation loss 0.04579606279730797 Accuracy 0.8891250491142273\n",
      "Iteration 72170 Training loss 0.0012711667222902179 Validation loss 0.04578941687941551 Accuracy 0.8891250491142273\n",
      "Iteration 72180 Training loss 0.001262496691197157 Validation loss 0.04577992856502533 Accuracy 0.8893750309944153\n",
      "Iteration 72190 Training loss 0.0012639203341677785 Validation loss 0.04578675702214241 Accuracy 0.8896250128746033\n",
      "Iteration 72200 Training loss 0.0012592090060934424 Validation loss 0.04578809067606926 Accuracy 0.8896250128746033\n",
      "Iteration 72210 Training loss 1.2514477020886261e-05 Validation loss 0.045788638293743134 Accuracy 0.8896250128746033\n",
      "Iteration 72220 Training loss 1.254863673239015e-05 Validation loss 0.045794401317834854 Accuracy 0.8892500400543213\n",
      "Iteration 72230 Training loss 1.1449331395851914e-05 Validation loss 0.04577723890542984 Accuracy 0.8895000219345093\n",
      "Iteration 72240 Training loss 0.0037630705628544092 Validation loss 0.045788612216711044 Accuracy 0.8896250128746033\n",
      "Iteration 72250 Training loss 0.002511069644242525 Validation loss 0.04581868276000023 Accuracy 0.8890000581741333\n",
      "Iteration 72260 Training loss 1.4193399692885578e-05 Validation loss 0.04579868167638779 Accuracy 0.8890000581741333\n",
      "Iteration 72270 Training loss 1.3665768165083136e-05 Validation loss 0.04576604813337326 Accuracy 0.8888750672340393\n",
      "Iteration 72280 Training loss 0.001262626494280994 Validation loss 0.04579110071063042 Accuracy 0.8895000219345093\n",
      "Iteration 72290 Training loss 0.00874460581690073 Validation loss 0.04580654203891754 Accuracy 0.8890000581741333\n",
      "Iteration 72300 Training loss 0.0012642118381336331 Validation loss 0.04578465595841408 Accuracy 0.8888750672340393\n",
      "Iteration 72310 Training loss 0.0012633127626031637 Validation loss 0.04577862098813057 Accuracy 0.8892500400543213\n",
      "Iteration 72320 Training loss 0.0012695530895143747 Validation loss 0.04578251764178276 Accuracy 0.8888750672340393\n",
      "Iteration 72330 Training loss 0.005015225149691105 Validation loss 0.045782748609781265 Accuracy 0.8890000581741333\n",
      "Iteration 72340 Training loss 0.005012528970837593 Validation loss 0.04578351974487305 Accuracy 0.8887500166893005\n",
      "Iteration 72350 Training loss 1.1418750545999501e-05 Validation loss 0.045782670378685 Accuracy 0.8891250491142273\n",
      "Iteration 72360 Training loss 0.0012659403728321195 Validation loss 0.045771755278110504 Accuracy 0.8891250491142273\n",
      "Iteration 72370 Training loss 1.6612391846138053e-05 Validation loss 0.045789286494255066 Accuracy 0.8891250491142273\n",
      "Iteration 72380 Training loss 0.0012591438135132194 Validation loss 0.04579458013176918 Accuracy 0.8891250491142273\n",
      "Iteration 72390 Training loss 0.0012607267126441002 Validation loss 0.0457407683134079 Accuracy 0.8891250491142273\n",
      "Iteration 72400 Training loss 0.0025143688544631004 Validation loss 0.04576105251908302 Accuracy 0.8895000219345093\n",
      "Iteration 72410 Training loss 0.002512361854314804 Validation loss 0.04578074812889099 Accuracy 0.8892500400543213\n",
      "Iteration 72420 Training loss 0.0012599374167621136 Validation loss 0.04574774205684662 Accuracy 0.8891250491142273\n",
      "Iteration 72430 Training loss 1.3539571227738634e-05 Validation loss 0.04576362669467926 Accuracy 0.8893750309944153\n",
      "Iteration 72440 Training loss 0.0012593046994879842 Validation loss 0.04575824365019798 Accuracy 0.8892500400543213\n",
      "Iteration 72450 Training loss 0.0012606005184352398 Validation loss 0.045755524188280106 Accuracy 0.8893750309944153\n",
      "Iteration 72460 Training loss 0.0012608794495463371 Validation loss 0.04575898125767708 Accuracy 0.8891250491142273\n",
      "Iteration 72470 Training loss 0.0025105858221650124 Validation loss 0.04576800391077995 Accuracy 0.8895000219345093\n",
      "Iteration 72480 Training loss 0.00126286328304559 Validation loss 0.045762304216623306 Accuracy 0.8892500400543213\n",
      "Iteration 72490 Training loss 1.0081032087327912e-05 Validation loss 0.04580026492476463 Accuracy 0.8890000581741333\n",
      "Iteration 72500 Training loss 8.907158189686015e-06 Validation loss 0.04578440263867378 Accuracy 0.8895000219345093\n",
      "Iteration 72510 Training loss 0.00626269169151783 Validation loss 0.04579556733369827 Accuracy 0.8891250491142273\n",
      "Iteration 72520 Training loss 8.406873348576482e-06 Validation loss 0.045779891312122345 Accuracy 0.8890000581741333\n",
      "Iteration 72530 Training loss 1.4852672393317334e-05 Validation loss 0.045786499977111816 Accuracy 0.8891250491142273\n",
      "Iteration 72540 Training loss 0.0012622372014448047 Validation loss 0.04579000174999237 Accuracy 0.8890000581741333\n",
      "Iteration 72550 Training loss 0.00624465849250555 Validation loss 0.0457661934196949 Accuracy 0.8891250491142273\n",
      "Iteration 72560 Training loss 0.0012583067873492837 Validation loss 0.04578541964292526 Accuracy 0.8890000581741333\n",
      "Iteration 72570 Training loss 0.0025153420865535736 Validation loss 0.04577694088220596 Accuracy 0.8891250491142273\n",
      "Iteration 72580 Training loss 1.1622390957199968e-05 Validation loss 0.045760683715343475 Accuracy 0.8892500400543213\n",
      "Iteration 72590 Training loss 1.4050423487788066e-05 Validation loss 0.04579475149512291 Accuracy 0.8891250491142273\n",
      "Iteration 72600 Training loss 0.001270932494662702 Validation loss 0.04580586031079292 Accuracy 0.8892500400543213\n",
      "Iteration 72610 Training loss 1.2050894838466775e-05 Validation loss 0.045780155807733536 Accuracy 0.8892500400543213\n",
      "Iteration 72620 Training loss 9.419592061021831e-06 Validation loss 0.04577559977769852 Accuracy 0.8892500400543213\n",
      "Iteration 72630 Training loss 1.267981315322686e-05 Validation loss 0.045763637870550156 Accuracy 0.8890000581741333\n",
      "Iteration 72640 Training loss 1.2471198715502396e-05 Validation loss 0.04577673226594925 Accuracy 0.8892500400543213\n",
      "Iteration 72650 Training loss 0.0037644945550709963 Validation loss 0.04577179253101349 Accuracy 0.8887500166893005\n",
      "Iteration 72660 Training loss 0.0037625106051564217 Validation loss 0.04581440985202789 Accuracy 0.8887500166893005\n",
      "Iteration 72670 Training loss 1.7542217392474413e-05 Validation loss 0.045777495950460434 Accuracy 0.8890000581741333\n",
      "Iteration 72680 Training loss 8.904550668376032e-06 Validation loss 0.0457732155919075 Accuracy 0.8887500166893005\n",
      "Iteration 72690 Training loss 2.1334364646463655e-05 Validation loss 0.045776400715112686 Accuracy 0.8891250491142273\n",
      "Iteration 72700 Training loss 0.0037588649429380894 Validation loss 0.04575970768928528 Accuracy 0.8892500400543213\n",
      "Iteration 72710 Training loss 0.002517026849091053 Validation loss 0.04577156528830528 Accuracy 0.8892500400543213\n",
      "Iteration 72720 Training loss 1.197017900267383e-05 Validation loss 0.04576602205634117 Accuracy 0.8890000581741333\n",
      "Iteration 72730 Training loss 1.518741191830486e-05 Validation loss 0.04576551914215088 Accuracy 0.8893750309944153\n",
      "Iteration 72740 Training loss 0.0037625457625836134 Validation loss 0.04579370096325874 Accuracy 0.8892500400543213\n",
      "Iteration 72750 Training loss 0.0012603056384250522 Validation loss 0.04581030458211899 Accuracy 0.8890000581741333\n",
      "Iteration 72760 Training loss 0.003765028901398182 Validation loss 0.04577860236167908 Accuracy 0.8892500400543213\n",
      "Iteration 72770 Training loss 1.7223754184669815e-05 Validation loss 0.04580283910036087 Accuracy 0.8890000581741333\n",
      "Iteration 72780 Training loss 0.0025122573133558035 Validation loss 0.045793261379003525 Accuracy 0.8890000581741333\n",
      "Iteration 72790 Training loss 0.0012634755112230778 Validation loss 0.04578351974487305 Accuracy 0.8891250491142273\n",
      "Iteration 72800 Training loss 0.0012635221937671304 Validation loss 0.045793671160936356 Accuracy 0.8893750309944153\n",
      "Iteration 72810 Training loss 0.0012592700077220798 Validation loss 0.04576151445508003 Accuracy 0.8891250491142273\n",
      "Iteration 72820 Training loss 0.0012620171764865518 Validation loss 0.04576093330979347 Accuracy 0.8893750309944153\n",
      "Iteration 72830 Training loss 0.0025114386808127165 Validation loss 0.04576161503791809 Accuracy 0.8892500400543213\n",
      "Iteration 72840 Training loss 0.002512661973014474 Validation loss 0.04576661065220833 Accuracy 0.8893750309944153\n",
      "Iteration 72850 Training loss 0.0012629744596779346 Validation loss 0.0458291731774807 Accuracy 0.8893750309944153\n",
      "Iteration 72860 Training loss 1.6055300875450484e-05 Validation loss 0.04580054059624672 Accuracy 0.8892500400543213\n",
      "Iteration 72870 Training loss 0.0012658479390665889 Validation loss 0.04577358439564705 Accuracy 0.8893750309944153\n",
      "Iteration 72880 Training loss 0.00250963750295341 Validation loss 0.0457674041390419 Accuracy 0.8895000219345093\n",
      "Iteration 72890 Training loss 1.2450939721020404e-05 Validation loss 0.04577735811471939 Accuracy 0.8892500400543213\n",
      "Iteration 72900 Training loss 0.0037607091944664717 Validation loss 0.04579063504934311 Accuracy 0.8892500400543213\n",
      "Iteration 72910 Training loss 1.2513662113633472e-05 Validation loss 0.04577038064599037 Accuracy 0.8891250491142273\n",
      "Iteration 72920 Training loss 0.0012633930891752243 Validation loss 0.04581503942608833 Accuracy 0.8891250491142273\n",
      "Iteration 72930 Training loss 0.0012595860753208399 Validation loss 0.045766811817884445 Accuracy 0.8895000219345093\n",
      "Iteration 72940 Training loss 0.00626347167417407 Validation loss 0.045993372797966 Accuracy 0.8892500400543213\n",
      "Iteration 72950 Training loss 0.0025136093609035015 Validation loss 0.04583192616701126 Accuracy 0.889750063419342\n",
      "Iteration 72960 Training loss 0.0012651394354179502 Validation loss 0.04577997699379921 Accuracy 0.889875054359436\n",
      "Iteration 72970 Training loss 0.0012623545480892062 Validation loss 0.04576366767287254 Accuracy 0.8895000219345093\n",
      "Iteration 72980 Training loss 3.5646553442347795e-05 Validation loss 0.046019360423088074 Accuracy 0.8892500400543213\n",
      "Iteration 72990 Training loss 0.0025224792771041393 Validation loss 0.04588540270924568 Accuracy 0.8895000219345093\n",
      "Iteration 73000 Training loss 0.0012628277763724327 Validation loss 0.045861005783081055 Accuracy 0.8896250128746033\n",
      "Iteration 73010 Training loss 0.0012654592283070087 Validation loss 0.04582757130265236 Accuracy 0.889750063419342\n",
      "Iteration 73020 Training loss 0.0025178759824484587 Validation loss 0.045775409787893295 Accuracy 0.89000004529953\n",
      "Iteration 73030 Training loss 1.6698060790076852e-05 Validation loss 0.04575714096426964 Accuracy 0.889875054359436\n",
      "Iteration 73040 Training loss 1.3080019925837405e-05 Validation loss 0.045844536274671555 Accuracy 0.8888750672340393\n",
      "Iteration 73050 Training loss 0.0012666239636018872 Validation loss 0.0457763634622097 Accuracy 0.8896250128746033\n",
      "Iteration 73060 Training loss 0.001263595768250525 Validation loss 0.04578506201505661 Accuracy 0.8895000219345093\n",
      "Iteration 73070 Training loss 0.0037676733918488026 Validation loss 0.045731205493211746 Accuracy 0.890375018119812\n",
      "Iteration 73080 Training loss 0.0025098007172346115 Validation loss 0.04575341194868088 Accuracy 0.89000004529953\n",
      "Iteration 73090 Training loss 0.0012661650544032454 Validation loss 0.04574121907353401 Accuracy 0.890250027179718\n",
      "Iteration 73100 Training loss 0.001264516031369567 Validation loss 0.0457489900290966 Accuracy 0.89000004529953\n",
      "Iteration 73110 Training loss 0.0012616562889888883 Validation loss 0.0457681268453598 Accuracy 0.89000004529953\n",
      "Iteration 73120 Training loss 0.003761824918910861 Validation loss 0.04579416289925575 Accuracy 0.8895000219345093\n",
      "Iteration 73130 Training loss 0.0037635122425854206 Validation loss 0.04577155411243439 Accuracy 0.889875054359436\n",
      "Iteration 73140 Training loss 0.0012617659522220492 Validation loss 0.0457686185836792 Accuracy 0.8895000219345093\n",
      "Iteration 73150 Training loss 0.0012611343991011381 Validation loss 0.04577549919486046 Accuracy 0.89000004529953\n",
      "Iteration 73160 Training loss 0.0012610310222953558 Validation loss 0.04578770324587822 Accuracy 0.889750063419342\n",
      "Iteration 73170 Training loss 0.002517844783142209 Validation loss 0.04580722376704216 Accuracy 0.8893750309944153\n",
      "Iteration 73180 Training loss 1.4431883755605668e-05 Validation loss 0.04575721174478531 Accuracy 0.889875054359436\n",
      "Iteration 73190 Training loss 0.0012615937739610672 Validation loss 0.04572688415646553 Accuracy 0.890250027179718\n",
      "Iteration 73200 Training loss 1.3761082300334238e-05 Validation loss 0.04577569290995598 Accuracy 0.89000004529953\n",
      "Iteration 73210 Training loss 1.925542346725706e-05 Validation loss 0.045769985765218735 Accuracy 0.89000004529953\n",
      "Iteration 73220 Training loss 0.0012680381769314408 Validation loss 0.045779429376125336 Accuracy 0.890125036239624\n",
      "Iteration 73230 Training loss 0.0012618760811164975 Validation loss 0.045813556760549545 Accuracy 0.8895000219345093\n",
      "Iteration 73240 Training loss 0.0012618524488061666 Validation loss 0.04578682780265808 Accuracy 0.8896250128746033\n",
      "Iteration 73250 Training loss 0.0012670044088736176 Validation loss 0.045798785984516144 Accuracy 0.8895000219345093\n",
      "Iteration 73260 Training loss 0.0012611832935363054 Validation loss 0.04580911621451378 Accuracy 0.8892500400543213\n",
      "Iteration 73270 Training loss 0.002520150737836957 Validation loss 0.04579760134220123 Accuracy 0.8892500400543213\n",
      "Iteration 73280 Training loss 9.407330253452528e-06 Validation loss 0.045801013708114624 Accuracy 0.8895000219345093\n",
      "Iteration 73290 Training loss 1.363996125292033e-05 Validation loss 0.04577191174030304 Accuracy 0.89000004529953\n",
      "Iteration 73300 Training loss 0.0012655231403186917 Validation loss 0.04580716788768768 Accuracy 0.8891250491142273\n",
      "Iteration 73310 Training loss 0.00126128108240664 Validation loss 0.045801304280757904 Accuracy 0.8895000219345093\n",
      "Iteration 73320 Training loss 1.1352324690960813e-05 Validation loss 0.04578935354948044 Accuracy 0.890250027179718\n",
      "Iteration 73330 Training loss 1.300868098041974e-05 Validation loss 0.0457923598587513 Accuracy 0.8896250128746033\n",
      "Iteration 73340 Training loss 0.0012585384538397193 Validation loss 0.0457523837685585 Accuracy 0.889875054359436\n",
      "Iteration 73350 Training loss 0.0025141334626823664 Validation loss 0.04575745761394501 Accuracy 0.889750063419342\n",
      "Iteration 73360 Training loss 1.2555436114780605e-05 Validation loss 0.045759979635477066 Accuracy 0.889875054359436\n",
      "Iteration 73370 Training loss 0.0037598751951009035 Validation loss 0.04579418525099754 Accuracy 0.889750063419342\n",
      "Iteration 73380 Training loss 1.4010281120135915e-05 Validation loss 0.04573303461074829 Accuracy 0.889875054359436\n",
      "Iteration 73390 Training loss 1.344437896477757e-05 Validation loss 0.04577428475022316 Accuracy 0.889750063419342\n",
      "Iteration 73400 Training loss 0.0012649884447455406 Validation loss 0.04576650634407997 Accuracy 0.889875054359436\n",
      "Iteration 73410 Training loss 1.817432166717481e-05 Validation loss 0.04577390477061272 Accuracy 0.889875054359436\n",
      "Iteration 73420 Training loss 1.536496711196378e-05 Validation loss 0.045779451727867126 Accuracy 0.8896250128746033\n",
      "Iteration 73430 Training loss 0.0025170694570988417 Validation loss 0.04576767981052399 Accuracy 0.8896250128746033\n",
      "Iteration 73440 Training loss 0.0037659304216504097 Validation loss 0.04573554918169975 Accuracy 0.889875054359436\n",
      "Iteration 73450 Training loss 0.0037674754858016968 Validation loss 0.04575883597135544 Accuracy 0.8895000219345093\n",
      "Iteration 73460 Training loss 0.0012643731897696853 Validation loss 0.04576707258820534 Accuracy 0.8895000219345093\n",
      "Iteration 73470 Training loss 0.0012655470054596663 Validation loss 0.0458548329770565 Accuracy 0.8890000581741333\n",
      "Iteration 73480 Training loss 0.0012623009970411658 Validation loss 0.04578522592782974 Accuracy 0.889875054359436\n",
      "Iteration 73490 Training loss 1.5343210179707967e-05 Validation loss 0.04573436081409454 Accuracy 0.889875054359436\n",
      "Iteration 73500 Training loss 0.0012691565789282322 Validation loss 0.04577351734042168 Accuracy 0.8895000219345093\n",
      "Iteration 73510 Training loss 0.002513033803552389 Validation loss 0.045758187770843506 Accuracy 0.8896250128746033\n",
      "Iteration 73520 Training loss 0.003763872664421797 Validation loss 0.04575135558843613 Accuracy 0.889750063419342\n",
      "Iteration 73530 Training loss 0.002510661957785487 Validation loss 0.045775145292282104 Accuracy 0.8895000219345093\n",
      "Iteration 73540 Training loss 0.0012619723565876484 Validation loss 0.04575837403535843 Accuracy 0.889750063419342\n",
      "Iteration 73550 Training loss 0.0012645923998206854 Validation loss 0.04579329490661621 Accuracy 0.8895000219345093\n",
      "Iteration 73560 Training loss 1.13458545456524e-05 Validation loss 0.04576260596513748 Accuracy 0.8896250128746033\n",
      "Iteration 73570 Training loss 0.0037593694869428873 Validation loss 0.04570787027478218 Accuracy 0.890250027179718\n",
      "Iteration 73580 Training loss 0.0012695370241999626 Validation loss 0.045751575380563736 Accuracy 0.8895000219345093\n",
      "Iteration 73590 Training loss 0.0012650208082050085 Validation loss 0.045741647481918335 Accuracy 0.890125036239624\n",
      "Iteration 73600 Training loss 0.0012737518409267068 Validation loss 0.04572679102420807 Accuracy 0.89000004529953\n",
      "Iteration 73610 Training loss 1.7114769434556365e-05 Validation loss 0.045730721205472946 Accuracy 0.889875054359436\n",
      "Iteration 73620 Training loss 0.0012646823888644576 Validation loss 0.045760251581668854 Accuracy 0.889750063419342\n",
      "Iteration 73630 Training loss 0.001261321478523314 Validation loss 0.045736316591501236 Accuracy 0.889750063419342\n",
      "Iteration 73640 Training loss 0.0025146091356873512 Validation loss 0.045755334198474884 Accuracy 0.8896250128746033\n",
      "Iteration 73650 Training loss 1.6194033378269523e-05 Validation loss 0.04577010124921799 Accuracy 0.8891250491142273\n",
      "Iteration 73660 Training loss 8.297752174257766e-06 Validation loss 0.045742765069007874 Accuracy 0.889750063419342\n",
      "Iteration 73670 Training loss 1.756423262122553e-05 Validation loss 0.04574834555387497 Accuracy 0.8895000219345093\n",
      "Iteration 73680 Training loss 0.0012673259479925036 Validation loss 0.04572366550564766 Accuracy 0.889750063419342\n",
      "Iteration 73690 Training loss 0.0025195239577442408 Validation loss 0.04573768377304077 Accuracy 0.889750063419342\n",
      "Iteration 73700 Training loss 0.001261173514649272 Validation loss 0.045724574476480484 Accuracy 0.8896250128746033\n",
      "Iteration 73710 Training loss 0.001263623358681798 Validation loss 0.04576127231121063 Accuracy 0.8895000219345093\n",
      "Iteration 73720 Training loss 0.0012640960048884153 Validation loss 0.04578095301985741 Accuracy 0.8895000219345093\n",
      "Iteration 73730 Training loss 1.3239777217677329e-05 Validation loss 0.04577701911330223 Accuracy 0.8896250128746033\n",
      "Iteration 73740 Training loss 0.0012646820396184921 Validation loss 0.04582278057932854 Accuracy 0.8890000581741333\n",
      "Iteration 73750 Training loss 2.137609953933861e-05 Validation loss 0.04575437307357788 Accuracy 0.8895000219345093\n",
      "Iteration 73760 Training loss 0.0012660262873396277 Validation loss 0.04577091336250305 Accuracy 0.8895000219345093\n",
      "Iteration 73770 Training loss 0.0025157611817121506 Validation loss 0.045766547322273254 Accuracy 0.8895000219345093\n",
      "Iteration 73780 Training loss 1.0782679964904673e-05 Validation loss 0.045791979879140854 Accuracy 0.8893750309944153\n",
      "Iteration 73790 Training loss 0.0012639614287763834 Validation loss 0.04575229063630104 Accuracy 0.8896250128746033\n",
      "Iteration 73800 Training loss 0.0012645380338653922 Validation loss 0.045785725116729736 Accuracy 0.8896250128746033\n",
      "Iteration 73810 Training loss 0.0012582979397848248 Validation loss 0.04576990753412247 Accuracy 0.889750063419342\n",
      "Iteration 73820 Training loss 0.0012644632952287793 Validation loss 0.04580322653055191 Accuracy 0.8893750309944153\n",
      "Iteration 73830 Training loss 0.0025211325846612453 Validation loss 0.0457921102643013 Accuracy 0.8892500400543213\n",
      "Iteration 73840 Training loss 0.0012633763253688812 Validation loss 0.045754365622997284 Accuracy 0.8891250491142273\n",
      "Iteration 73850 Training loss 0.001264864462427795 Validation loss 0.04572654888033867 Accuracy 0.8896250128746033\n",
      "Iteration 73860 Training loss 0.0012612290447577834 Validation loss 0.045737650245428085 Accuracy 0.8895000219345093\n",
      "Iteration 73870 Training loss 0.0025140647776424885 Validation loss 0.04574614018201828 Accuracy 0.8895000219345093\n",
      "Iteration 73880 Training loss 0.001264746650122106 Validation loss 0.04577822610735893 Accuracy 0.8896250128746033\n",
      "Iteration 73890 Training loss 0.0012622361537069082 Validation loss 0.0457475520670414 Accuracy 0.889750063419342\n",
      "Iteration 73900 Training loss 0.0025100656785070896 Validation loss 0.045738473534584045 Accuracy 0.8896250128746033\n",
      "Iteration 73910 Training loss 0.0012647737748920918 Validation loss 0.04575298726558685 Accuracy 0.8895000219345093\n",
      "Iteration 73920 Training loss 0.0025136726908385754 Validation loss 0.04575852304697037 Accuracy 0.8896250128746033\n",
      "Iteration 73930 Training loss 0.0012644182424992323 Validation loss 0.04574379324913025 Accuracy 0.8896250128746033\n",
      "Iteration 73940 Training loss 1.4080709661357105e-05 Validation loss 0.04578712582588196 Accuracy 0.889750063419342\n",
      "Iteration 73950 Training loss 0.00626092916354537 Validation loss 0.04575330391526222 Accuracy 0.889750063419342\n",
      "Iteration 73960 Training loss 1.3128947102813981e-05 Validation loss 0.04574901983141899 Accuracy 0.8896250128746033\n",
      "Iteration 73970 Training loss 0.0012613448780030012 Validation loss 0.045772407203912735 Accuracy 0.889750063419342\n",
      "Iteration 73980 Training loss 0.0012601874768733978 Validation loss 0.04578550532460213 Accuracy 0.889750063419342\n",
      "Iteration 73990 Training loss 0.0037592612206935883 Validation loss 0.04577048122882843 Accuracy 0.889875054359436\n",
      "Iteration 74000 Training loss 0.002511657075956464 Validation loss 0.0457688607275486 Accuracy 0.89000004529953\n",
      "Iteration 74010 Training loss 1.5166646335273981e-05 Validation loss 0.04575221613049507 Accuracy 0.889875054359436\n",
      "Iteration 74020 Training loss 0.002513215411454439 Validation loss 0.04575773701071739 Accuracy 0.89000004529953\n",
      "Iteration 74030 Training loss 1.1261504369031172e-05 Validation loss 0.045778825879096985 Accuracy 0.8896250128746033\n",
      "Iteration 74040 Training loss 0.0012633090373128653 Validation loss 0.04579530656337738 Accuracy 0.8893750309944153\n",
      "Iteration 74050 Training loss 1.2129322385590058e-05 Validation loss 0.045733023434877396 Accuracy 0.889750063419342\n",
      "Iteration 74060 Training loss 0.0012660376960411668 Validation loss 0.04577360674738884 Accuracy 0.8892500400543213\n",
      "Iteration 74070 Training loss 0.0012595203006640077 Validation loss 0.045771852135658264 Accuracy 0.8895000219345093\n",
      "Iteration 74080 Training loss 0.001261277822777629 Validation loss 0.04575611278414726 Accuracy 0.8896250128746033\n",
      "Iteration 74090 Training loss 0.0012708439026027918 Validation loss 0.04574497416615486 Accuracy 0.8895000219345093\n",
      "Iteration 74100 Training loss 1.2947110008099116e-05 Validation loss 0.045738495886325836 Accuracy 0.889875054359436\n",
      "Iteration 74110 Training loss 0.0012622898211702704 Validation loss 0.04582492262125015 Accuracy 0.8887500166893005\n",
      "Iteration 74120 Training loss 1.752149182721041e-05 Validation loss 0.045743219554424286 Accuracy 0.889750063419342\n",
      "Iteration 74130 Training loss 0.0012664824025705457 Validation loss 0.045769061893224716 Accuracy 0.889875054359436\n",
      "Iteration 74140 Training loss 0.0025101862847805023 Validation loss 0.04573556035757065 Accuracy 0.89000004529953\n",
      "Iteration 74150 Training loss 0.001264425227418542 Validation loss 0.045732393860816956 Accuracy 0.890125036239624\n",
      "Iteration 74160 Training loss 1.4966557500883937e-05 Validation loss 0.04577227309346199 Accuracy 0.8896250128746033\n",
      "Iteration 74170 Training loss 0.003760040970519185 Validation loss 0.04578780382871628 Accuracy 0.8892500400543213\n",
      "Iteration 74180 Training loss 0.0012633033329620957 Validation loss 0.045762572437524796 Accuracy 0.889750063419342\n",
      "Iteration 74190 Training loss 1.8338954760110937e-05 Validation loss 0.045749273151159286 Accuracy 0.889875054359436\n",
      "Iteration 74200 Training loss 0.0050111375749111176 Validation loss 0.04573214799165726 Accuracy 0.889875054359436\n",
      "Iteration 74210 Training loss 0.0012652309378609061 Validation loss 0.04573696479201317 Accuracy 0.889750063419342\n",
      "Iteration 74220 Training loss 0.0025106719695031643 Validation loss 0.0457359217107296 Accuracy 0.889750063419342\n",
      "Iteration 74230 Training loss 0.005015223287045956 Validation loss 0.04573607072234154 Accuracy 0.8896250128746033\n",
      "Iteration 74240 Training loss 0.0025148375425487757 Validation loss 0.0457184799015522 Accuracy 0.89000004529953\n",
      "Iteration 74250 Training loss 0.0012631563004106283 Validation loss 0.045766327530145645 Accuracy 0.8895000219345093\n",
      "Iteration 74260 Training loss 0.0012611986603587866 Validation loss 0.04574291408061981 Accuracy 0.8893750309944153\n",
      "Iteration 74270 Training loss 0.0025113290175795555 Validation loss 0.04576219245791435 Accuracy 0.8893750309944153\n",
      "Iteration 74280 Training loss 1.54516728798626e-05 Validation loss 0.04575338587164879 Accuracy 0.8893750309944153\n",
      "Iteration 74290 Training loss 0.002510923193767667 Validation loss 0.045755136758089066 Accuracy 0.8893750309944153\n",
      "Iteration 74300 Training loss 1.806165892048739e-05 Validation loss 0.045732975006103516 Accuracy 0.8893750309944153\n",
      "Iteration 74310 Training loss 1.0150363777938765e-05 Validation loss 0.04574724659323692 Accuracy 0.8893750309944153\n",
      "Iteration 74320 Training loss 0.0012613350991159678 Validation loss 0.04573734477162361 Accuracy 0.8892500400543213\n",
      "Iteration 74330 Training loss 0.0025122433435171843 Validation loss 0.045768145471811295 Accuracy 0.8891250491142273\n",
      "Iteration 74340 Training loss 0.0012670134892687201 Validation loss 0.04572343826293945 Accuracy 0.8893750309944153\n",
      "Iteration 74350 Training loss 0.0012634696904569864 Validation loss 0.04574548453092575 Accuracy 0.8893750309944153\n",
      "Iteration 74360 Training loss 1.098042594094295e-05 Validation loss 0.045746251940727234 Accuracy 0.8892500400543213\n",
      "Iteration 74370 Training loss 0.003763268468901515 Validation loss 0.04580491781234741 Accuracy 0.8891250491142273\n",
      "Iteration 74380 Training loss 0.0012665496906265616 Validation loss 0.04573697969317436 Accuracy 0.8892500400543213\n",
      "Iteration 74390 Training loss 1.7056061551556922e-05 Validation loss 0.045780032873153687 Accuracy 0.8895000219345093\n",
      "Iteration 74400 Training loss 1.2111368050682358e-05 Validation loss 0.045751288533210754 Accuracy 0.8893750309944153\n",
      "Iteration 74410 Training loss 0.002518133260309696 Validation loss 0.04573611915111542 Accuracy 0.8893750309944153\n",
      "Iteration 74420 Training loss 0.0012631961144506931 Validation loss 0.045746371150016785 Accuracy 0.8895000219345093\n",
      "Iteration 74430 Training loss 1.5166259800025728e-05 Validation loss 0.0457305945456028 Accuracy 0.8895000219345093\n",
      "Iteration 74440 Training loss 0.0012612775899469852 Validation loss 0.04574766010046005 Accuracy 0.8895000219345093\n",
      "Iteration 74450 Training loss 0.0025131264701485634 Validation loss 0.045729052275419235 Accuracy 0.8896250128746033\n",
      "Iteration 74460 Training loss 0.001261810539290309 Validation loss 0.045730266720056534 Accuracy 0.8895000219345093\n",
      "Iteration 74470 Training loss 1.7099198885262012e-05 Validation loss 0.04574763774871826 Accuracy 0.8896250128746033\n",
      "Iteration 74480 Training loss 0.0012604990042746067 Validation loss 0.045729052275419235 Accuracy 0.8893750309944153\n",
      "Iteration 74490 Training loss 1.1390644431230612e-05 Validation loss 0.04576275125145912 Accuracy 0.889750063419342\n",
      "Iteration 74500 Training loss 0.0012615361483767629 Validation loss 0.04574073851108551 Accuracy 0.8895000219345093\n",
      "Iteration 74510 Training loss 0.0025114475283771753 Validation loss 0.04573376104235649 Accuracy 0.8892500400543213\n",
      "Iteration 74520 Training loss 0.00252596870996058 Validation loss 0.04577213525772095 Accuracy 0.8893750309944153\n",
      "Iteration 74530 Training loss 1.2213594345666934e-05 Validation loss 0.04575275629758835 Accuracy 0.8895000219345093\n",
      "Iteration 74540 Training loss 9.250009497918654e-06 Validation loss 0.045753419399261475 Accuracy 0.8896250128746033\n",
      "Iteration 74550 Training loss 1.2179096302133985e-05 Validation loss 0.04578038305044174 Accuracy 0.8895000219345093\n",
      "Iteration 74560 Training loss 0.0012592286802828312 Validation loss 0.04579813778400421 Accuracy 0.8893750309944153\n",
      "Iteration 74570 Training loss 0.005013507325202227 Validation loss 0.045793402940034866 Accuracy 0.8896250128746033\n",
      "Iteration 74580 Training loss 0.0025140196084976196 Validation loss 0.04576998949050903 Accuracy 0.8895000219345093\n",
      "Iteration 74590 Training loss 1.3661498087458313e-05 Validation loss 0.04576730728149414 Accuracy 0.8895000219345093\n",
      "Iteration 74600 Training loss 0.0012616150779649615 Validation loss 0.045795124024152756 Accuracy 0.8892500400543213\n",
      "Iteration 74610 Training loss 0.0037585594691336155 Validation loss 0.04577738046646118 Accuracy 0.8895000219345093\n",
      "Iteration 74620 Training loss 0.002511570928618312 Validation loss 0.045758917927742004 Accuracy 0.8895000219345093\n",
      "Iteration 74630 Training loss 0.005009836982935667 Validation loss 0.04575783759355545 Accuracy 0.8895000219345093\n",
      "Iteration 74640 Training loss 0.0012620184570550919 Validation loss 0.0457700677216053 Accuracy 0.8892500400543213\n",
      "Iteration 74650 Training loss 0.0037679385859519243 Validation loss 0.04577767848968506 Accuracy 0.8895000219345093\n",
      "Iteration 74660 Training loss 0.001263767946511507 Validation loss 0.04575910419225693 Accuracy 0.8896250128746033\n",
      "Iteration 74670 Training loss 0.0012635220773518085 Validation loss 0.045750126242637634 Accuracy 0.8891250491142273\n",
      "Iteration 74680 Training loss 9.188902367895935e-06 Validation loss 0.045765191316604614 Accuracy 0.8892500400543213\n",
      "Iteration 74690 Training loss 1.3647987543663476e-05 Validation loss 0.0457489937543869 Accuracy 0.8892500400543213\n",
      "Iteration 74700 Training loss 0.00126285117585212 Validation loss 0.04577762261033058 Accuracy 0.8893750309944153\n",
      "Iteration 74710 Training loss 0.001259715179912746 Validation loss 0.04576623812317848 Accuracy 0.8895000219345093\n",
      "Iteration 74720 Training loss 0.0025162643287330866 Validation loss 0.04576905816793442 Accuracy 0.8893750309944153\n",
      "Iteration 74730 Training loss 0.0012623653747141361 Validation loss 0.04576902836561203 Accuracy 0.8893750309944153\n",
      "Iteration 74740 Training loss 0.0025139786303043365 Validation loss 0.04575781896710396 Accuracy 0.8895000219345093\n",
      "Iteration 74750 Training loss 0.0025137783959507942 Validation loss 0.045786213129758835 Accuracy 0.8892500400543213\n",
      "Iteration 74760 Training loss 0.0012645868118852377 Validation loss 0.045796673744916916 Accuracy 0.8893750309944153\n",
      "Iteration 74770 Training loss 0.001260306453332305 Validation loss 0.04578088968992233 Accuracy 0.8892500400543213\n",
      "Iteration 74780 Training loss 0.0012627339456230402 Validation loss 0.04575863853096962 Accuracy 0.8895000219345093\n",
      "Iteration 74790 Training loss 0.001264163525775075 Validation loss 0.04581335559487343 Accuracy 0.8893750309944153\n",
      "Iteration 74800 Training loss 1.4907253898854833e-05 Validation loss 0.04582107067108154 Accuracy 0.8895000219345093\n",
      "Iteration 74810 Training loss 0.00251253810711205 Validation loss 0.04578591510653496 Accuracy 0.8896250128746033\n",
      "Iteration 74820 Training loss 0.00376233272254467 Validation loss 0.04576850309967995 Accuracy 0.8893750309944153\n",
      "Iteration 74830 Training loss 0.0012693002354353666 Validation loss 0.0458017997443676 Accuracy 0.8896250128746033\n",
      "Iteration 74840 Training loss 0.001262771780602634 Validation loss 0.045801859349012375 Accuracy 0.889750063419342\n",
      "Iteration 74850 Training loss 0.002511619823053479 Validation loss 0.04580479487776756 Accuracy 0.8893750309944153\n",
      "Iteration 74860 Training loss 0.0012617793399840593 Validation loss 0.04580748826265335 Accuracy 0.8895000219345093\n",
      "Iteration 74870 Training loss 0.0037661229725927114 Validation loss 0.04582100361585617 Accuracy 0.8892500400543213\n",
      "Iteration 74880 Training loss 0.002508225617930293 Validation loss 0.04586510732769966 Accuracy 0.8891250491142273\n",
      "Iteration 74890 Training loss 0.001265798811800778 Validation loss 0.045835573226213455 Accuracy 0.8893750309944153\n",
      "Iteration 74900 Training loss 0.002514058258384466 Validation loss 0.045821525156497955 Accuracy 0.889750063419342\n",
      "Iteration 74910 Training loss 0.0012617422034963965 Validation loss 0.04582623392343521 Accuracy 0.8895000219345093\n",
      "Iteration 74920 Training loss 0.001267149462364614 Validation loss 0.04580875113606453 Accuracy 0.8896250128746033\n",
      "Iteration 74930 Training loss 0.0012600511545315385 Validation loss 0.04580437019467354 Accuracy 0.8895000219345093\n",
      "Iteration 74940 Training loss 9.990015314542688e-06 Validation loss 0.0457959920167923 Accuracy 0.8896250128746033\n",
      "Iteration 74950 Training loss 1.4478970115305856e-05 Validation loss 0.04580108821392059 Accuracy 0.8896250128746033\n",
      "Iteration 74960 Training loss 1.2921680536237545e-05 Validation loss 0.04578060656785965 Accuracy 0.8896250128746033\n",
      "Iteration 74970 Training loss 0.001259645912796259 Validation loss 0.04582676291465759 Accuracy 0.8896250128746033\n",
      "Iteration 74980 Training loss 1.1358934898453299e-05 Validation loss 0.04584059491753578 Accuracy 0.8895000219345093\n",
      "Iteration 74990 Training loss 0.0012610650155693293 Validation loss 0.04582731053233147 Accuracy 0.889750063419342\n",
      "Iteration 75000 Training loss 1.3531914191844407e-05 Validation loss 0.045796025544404984 Accuracy 0.889875054359436\n",
      "Iteration 75010 Training loss 0.0012627948308363557 Validation loss 0.04579237848520279 Accuracy 0.8896250128746033\n",
      "Iteration 75020 Training loss 0.0012644616654142737 Validation loss 0.04581380635499954 Accuracy 0.889750063419342\n",
      "Iteration 75030 Training loss 0.0012646069517359138 Validation loss 0.04578985646367073 Accuracy 0.8896250128746033\n",
      "Iteration 75040 Training loss 1.3637150914291851e-05 Validation loss 0.04581805691123009 Accuracy 0.8895000219345093\n",
      "Iteration 75050 Training loss 0.0012624061200767756 Validation loss 0.04577546939253807 Accuracy 0.8895000219345093\n",
      "Iteration 75060 Training loss 1.283995334233623e-05 Validation loss 0.04576069116592407 Accuracy 0.8893750309944153\n",
      "Iteration 75070 Training loss 0.0025090123526751995 Validation loss 0.045789483934640884 Accuracy 0.8893750309944153\n",
      "Iteration 75080 Training loss 0.0025193311739712954 Validation loss 0.045856401324272156 Accuracy 0.8891250491142273\n",
      "Iteration 75090 Training loss 0.005012995097786188 Validation loss 0.04579698294401169 Accuracy 0.8896250128746033\n",
      "Iteration 75100 Training loss 0.0012627557152882218 Validation loss 0.045807380229234695 Accuracy 0.8896250128746033\n",
      "Iteration 75110 Training loss 0.0012591509148478508 Validation loss 0.04579393193125725 Accuracy 0.889875054359436\n",
      "Iteration 75120 Training loss 0.0025125315878540277 Validation loss 0.045792192220687866 Accuracy 0.889750063419342\n",
      "Iteration 75130 Training loss 0.002510539023205638 Validation loss 0.0457787849009037 Accuracy 0.889750063419342\n",
      "Iteration 75140 Training loss 0.002512305276468396 Validation loss 0.045769814401865005 Accuracy 0.8895000219345093\n",
      "Iteration 75150 Training loss 1.085236272047041e-05 Validation loss 0.04576724395155907 Accuracy 0.889750063419342\n",
      "Iteration 75160 Training loss 0.0037616114132106304 Validation loss 0.04574957489967346 Accuracy 0.8896250128746033\n",
      "Iteration 75170 Training loss 0.002512992825359106 Validation loss 0.04575087130069733 Accuracy 0.889750063419342\n",
      "Iteration 75180 Training loss 1.070453981810715e-05 Validation loss 0.045765627175569534 Accuracy 0.890125036239624\n",
      "Iteration 75190 Training loss 0.0037640195805579424 Validation loss 0.04576020687818527 Accuracy 0.889750063419342\n",
      "Iteration 75200 Training loss 0.003762537147849798 Validation loss 0.04576244577765465 Accuracy 0.8896250128746033\n",
      "Iteration 75210 Training loss 0.0012612019199877977 Validation loss 0.04577946662902832 Accuracy 0.889750063419342\n",
      "Iteration 75220 Training loss 1.0013166502176318e-05 Validation loss 0.045792486518621445 Accuracy 0.889750063419342\n",
      "Iteration 75230 Training loss 0.0037662929389625788 Validation loss 0.04578278586268425 Accuracy 0.889750063419342\n",
      "Iteration 75240 Training loss 0.0012632494326680899 Validation loss 0.04575018212199211 Accuracy 0.890125036239624\n",
      "Iteration 75250 Training loss 0.0012573873391374946 Validation loss 0.045775797218084335 Accuracy 0.889875054359436\n",
      "Iteration 75260 Training loss 0.0037660354282706976 Validation loss 0.04577460512518883 Accuracy 0.889750063419342\n",
      "Iteration 75270 Training loss 0.0012667111586779356 Validation loss 0.04578860104084015 Accuracy 0.889875054359436\n",
      "Iteration 75280 Training loss 0.0012644425733014941 Validation loss 0.04581611976027489 Accuracy 0.889750063419342\n",
      "Iteration 75290 Training loss 0.001266018720343709 Validation loss 0.0458032451570034 Accuracy 0.8895000219345093\n",
      "Iteration 75300 Training loss 0.002514328109100461 Validation loss 0.04583389684557915 Accuracy 0.8895000219345093\n",
      "Iteration 75310 Training loss 0.0025135367177426815 Validation loss 0.045817866921424866 Accuracy 0.8896250128746033\n",
      "Iteration 75320 Training loss 0.0012668573763221502 Validation loss 0.04585125669836998 Accuracy 0.8892500400543213\n",
      "Iteration 75330 Training loss 0.0025126587133854628 Validation loss 0.04582131281495094 Accuracy 0.8896250128746033\n",
      "Iteration 75340 Training loss 1.1415730114094913e-05 Validation loss 0.04581845924258232 Accuracy 0.8896250128746033\n",
      "Iteration 75350 Training loss 1.566556420584675e-05 Validation loss 0.04578469693660736 Accuracy 0.8895000219345093\n",
      "Iteration 75360 Training loss 0.0012667461996898055 Validation loss 0.04579990729689598 Accuracy 0.8895000219345093\n",
      "Iteration 75370 Training loss 0.0012668531853705645 Validation loss 0.04580482840538025 Accuracy 0.8896250128746033\n",
      "Iteration 75380 Training loss 9.180695997201838e-06 Validation loss 0.04582035169005394 Accuracy 0.8895000219345093\n",
      "Iteration 75390 Training loss 0.0025117676705121994 Validation loss 0.045788396149873734 Accuracy 0.8893750309944153\n",
      "Iteration 75400 Training loss 9.326143299404066e-06 Validation loss 0.04576721414923668 Accuracy 0.8892500400543213\n",
      "Iteration 75410 Training loss 0.0012608106480911374 Validation loss 0.0457720048725605 Accuracy 0.8893750309944153\n",
      "Iteration 75420 Training loss 1.3236962331575342e-05 Validation loss 0.045817408710718155 Accuracy 0.8895000219345093\n",
      "Iteration 75430 Training loss 1.3397283510130364e-05 Validation loss 0.0457974448800087 Accuracy 0.8891250491142273\n",
      "Iteration 75440 Training loss 0.0012640755157917738 Validation loss 0.04579152539372444 Accuracy 0.8893750309944153\n",
      "Iteration 75450 Training loss 1.4960501175664831e-05 Validation loss 0.0458301417529583 Accuracy 0.8893750309944153\n",
      "Iteration 75460 Training loss 0.001263024052605033 Validation loss 0.04585341736674309 Accuracy 0.8891250491142273\n",
      "Iteration 75470 Training loss 0.0012589285615831614 Validation loss 0.04582042992115021 Accuracy 0.8893750309944153\n",
      "Iteration 75480 Training loss 0.0012613901635631919 Validation loss 0.04579572379589081 Accuracy 0.889750063419342\n",
      "Iteration 75490 Training loss 0.002513636602088809 Validation loss 0.04580574110150337 Accuracy 0.889750063419342\n",
      "Iteration 75500 Training loss 0.003761064726859331 Validation loss 0.04582013189792633 Accuracy 0.889750063419342\n",
      "Iteration 75510 Training loss 0.0025163614191114902 Validation loss 0.04581001400947571 Accuracy 0.8893750309944153\n",
      "Iteration 75520 Training loss 1.244938812305918e-05 Validation loss 0.045800354331731796 Accuracy 0.8895000219345093\n",
      "Iteration 75530 Training loss 0.0012602355564013124 Validation loss 0.04579046741127968 Accuracy 0.8890000581741333\n",
      "Iteration 75540 Training loss 1.5355381037807092e-05 Validation loss 0.045825500041246414 Accuracy 0.8891250491142273\n",
      "Iteration 75550 Training loss 0.001264230697415769 Validation loss 0.04580750688910484 Accuracy 0.8893750309944153\n",
      "Iteration 75560 Training loss 0.0012652170844376087 Validation loss 0.04582516476511955 Accuracy 0.8892500400543213\n",
      "Iteration 75570 Training loss 0.0012596399756148458 Validation loss 0.04580826312303543 Accuracy 0.8893750309944153\n",
      "Iteration 75580 Training loss 0.001261602039448917 Validation loss 0.04582672193646431 Accuracy 0.8892500400543213\n",
      "Iteration 75590 Training loss 0.0012630780693143606 Validation loss 0.04581416770815849 Accuracy 0.8895000219345093\n",
      "Iteration 75600 Training loss 0.0012638915795832872 Validation loss 0.04578304663300514 Accuracy 0.8893750309944153\n",
      "Iteration 75610 Training loss 1.382289883622434e-05 Validation loss 0.04579192399978638 Accuracy 0.8891250491142273\n",
      "Iteration 75620 Training loss 0.0012652480509132147 Validation loss 0.045783475041389465 Accuracy 0.8891250491142273\n",
      "Iteration 75630 Training loss 1.4511392691929359e-05 Validation loss 0.04578522592782974 Accuracy 0.8888750672340393\n",
      "Iteration 75640 Training loss 1.2926393537782133e-05 Validation loss 0.04579201713204384 Accuracy 0.8890000581741333\n",
      "Iteration 75650 Training loss 7.319616543099983e-06 Validation loss 0.04579693824052811 Accuracy 0.8891250491142273\n",
      "Iteration 75660 Training loss 0.001258272910490632 Validation loss 0.04583219438791275 Accuracy 0.8892500400543213\n",
      "Iteration 75670 Training loss 0.0025151034351438284 Validation loss 0.04580899327993393 Accuracy 0.8892500400543213\n",
      "Iteration 75680 Training loss 0.0012592364801093936 Validation loss 0.045828524976968765 Accuracy 0.8893750309944153\n",
      "Iteration 75690 Training loss 1.2690997209574562e-05 Validation loss 0.04582619667053223 Accuracy 0.8893750309944153\n",
      "Iteration 75700 Training loss 0.003764617722481489 Validation loss 0.04580274596810341 Accuracy 0.8895000219345093\n",
      "Iteration 75710 Training loss 1.2583510397234932e-05 Validation loss 0.04580410569906235 Accuracy 0.8895000219345093\n",
      "Iteration 75720 Training loss 9.743838745635003e-06 Validation loss 0.04582362622022629 Accuracy 0.8893750309944153\n",
      "Iteration 75730 Training loss 0.0025133085437119007 Validation loss 0.045855507254600525 Accuracy 0.8895000219345093\n",
      "Iteration 75740 Training loss 0.001263872836716473 Validation loss 0.045803532004356384 Accuracy 0.8892500400543213\n",
      "Iteration 75750 Training loss 0.002518316265195608 Validation loss 0.045786138623952866 Accuracy 0.8893750309944153\n",
      "Iteration 75760 Training loss 0.0012621940113604069 Validation loss 0.04582955688238144 Accuracy 0.8893750309944153\n",
      "Iteration 75770 Training loss 0.002511716913431883 Validation loss 0.04578779265284538 Accuracy 0.8890000581741333\n",
      "Iteration 75780 Training loss 1.2664666428463534e-05 Validation loss 0.045802775770425797 Accuracy 0.8893750309944153\n",
      "Iteration 75790 Training loss 0.0025143674574792385 Validation loss 0.04583004117012024 Accuracy 0.889750063419342\n",
      "Iteration 75800 Training loss 0.0012628367403522134 Validation loss 0.045816075056791306 Accuracy 0.8891250491142273\n",
      "Iteration 75810 Training loss 0.002509354380890727 Validation loss 0.04580007120966911 Accuracy 0.8891250491142273\n",
      "Iteration 75820 Training loss 0.0012592317070811987 Validation loss 0.045820534229278564 Accuracy 0.8893750309944153\n",
      "Iteration 75830 Training loss 0.0012595262378454208 Validation loss 0.04579072818160057 Accuracy 0.8893750309944153\n",
      "Iteration 75840 Training loss 0.0012636350002139807 Validation loss 0.045797087252140045 Accuracy 0.8892500400543213\n",
      "Iteration 75850 Training loss 1.006954516924452e-05 Validation loss 0.04579037055373192 Accuracy 0.8893750309944153\n",
      "Iteration 75860 Training loss 0.0025142659433186054 Validation loss 0.04581663757562637 Accuracy 0.8893750309944153\n",
      "Iteration 75870 Training loss 1.843528116296511e-05 Validation loss 0.04577402025461197 Accuracy 0.8892500400543213\n",
      "Iteration 75880 Training loss 0.00500982441008091 Validation loss 0.045812997967004776 Accuracy 0.8892500400543213\n",
      "Iteration 75890 Training loss 1.7016576748574153e-05 Validation loss 0.04577784240245819 Accuracy 0.8892500400543213\n",
      "Iteration 75900 Training loss 0.0012633827282115817 Validation loss 0.045805636793375015 Accuracy 0.8892500400543213\n",
      "Iteration 75910 Training loss 0.0037683951668441296 Validation loss 0.04579238221049309 Accuracy 0.8891250491142273\n",
      "Iteration 75920 Training loss 0.0025088470429182053 Validation loss 0.045798301696777344 Accuracy 0.8891250491142273\n",
      "Iteration 75930 Training loss 1.1162471309944522e-05 Validation loss 0.045808907598257065 Accuracy 0.8892500400543213\n",
      "Iteration 75940 Training loss 1.3157324247003999e-05 Validation loss 0.04579006880521774 Accuracy 0.8892500400543213\n",
      "Iteration 75950 Training loss 1.623394200578332e-05 Validation loss 0.045812297612428665 Accuracy 0.8892500400543213\n",
      "Iteration 75960 Training loss 0.0025113075971603394 Validation loss 0.04580662027001381 Accuracy 0.8891250491142273\n",
      "Iteration 75970 Training loss 0.0012606121599674225 Validation loss 0.045792438089847565 Accuracy 0.8893750309944153\n",
      "Iteration 75980 Training loss 0.0012594673316925764 Validation loss 0.04577825963497162 Accuracy 0.8891250491142273\n",
      "Iteration 75990 Training loss 0.0025099883787333965 Validation loss 0.045785676687955856 Accuracy 0.8888750672340393\n",
      "Iteration 76000 Training loss 0.0012665778631344438 Validation loss 0.04580118879675865 Accuracy 0.8890000581741333\n",
      "Iteration 76010 Training loss 0.003760850988328457 Validation loss 0.04580556973814964 Accuracy 0.8893750309944153\n",
      "Iteration 76020 Training loss 0.0012602092465385795 Validation loss 0.04579877108335495 Accuracy 0.8892500400543213\n",
      "Iteration 76030 Training loss 0.0012620814377442002 Validation loss 0.04578644409775734 Accuracy 0.8892500400543213\n",
      "Iteration 76040 Training loss 0.0037650445010513067 Validation loss 0.04580217972397804 Accuracy 0.8893750309944153\n",
      "Iteration 76050 Training loss 0.0012610061094164848 Validation loss 0.045801565051078796 Accuracy 0.8895000219345093\n",
      "Iteration 76060 Training loss 0.0012634610757231712 Validation loss 0.045793719589710236 Accuracy 0.8895000219345093\n",
      "Iteration 76070 Training loss 0.003761564614251256 Validation loss 0.045806583017110825 Accuracy 0.8896250128746033\n",
      "Iteration 76080 Training loss 0.003760050516575575 Validation loss 0.045822929590940475 Accuracy 0.8896250128746033\n",
      "Iteration 76090 Training loss 0.003760846797376871 Validation loss 0.04582682251930237 Accuracy 0.8896250128746033\n",
      "Iteration 76100 Training loss 1.0478147487447131e-05 Validation loss 0.04580949619412422 Accuracy 0.8895000219345093\n",
      "Iteration 76110 Training loss 0.001263950252905488 Validation loss 0.04578655958175659 Accuracy 0.8893750309944153\n",
      "Iteration 76120 Training loss 1.3906759704696015e-05 Validation loss 0.04578834027051926 Accuracy 0.8896250128746033\n",
      "Iteration 76130 Training loss 0.0012640237109735608 Validation loss 0.04581277817487717 Accuracy 0.8896250128746033\n",
      "Iteration 76140 Training loss 0.0012624410446733236 Validation loss 0.04580486938357353 Accuracy 0.8896250128746033\n",
      "Iteration 76150 Training loss 1.1880038982781116e-05 Validation loss 0.04576829448342323 Accuracy 0.8891250491142273\n",
      "Iteration 76160 Training loss 0.0012655791360884905 Validation loss 0.04579298570752144 Accuracy 0.8895000219345093\n",
      "Iteration 76170 Training loss 0.001263950252905488 Validation loss 0.04581293836236 Accuracy 0.8896250128746033\n",
      "Iteration 76180 Training loss 1.503077510278672e-05 Validation loss 0.04578196257352829 Accuracy 0.8892500400543213\n",
      "Iteration 76190 Training loss 1.529375367681496e-05 Validation loss 0.045812416821718216 Accuracy 0.889750063419342\n",
      "Iteration 76200 Training loss 0.001259255688637495 Validation loss 0.04579748213291168 Accuracy 0.8896250128746033\n",
      "Iteration 76210 Training loss 9.248421520169359e-06 Validation loss 0.04578639566898346 Accuracy 0.8896250128746033\n",
      "Iteration 76220 Training loss 1.0382937944086734e-05 Validation loss 0.04582567885518074 Accuracy 0.889875054359436\n",
      "Iteration 76230 Training loss 0.0012628287076950073 Validation loss 0.045812781900167465 Accuracy 0.8896250128746033\n",
      "Iteration 76240 Training loss 0.002511335536837578 Validation loss 0.04580873250961304 Accuracy 0.8895000219345093\n",
      "Iteration 76250 Training loss 0.0012658116174861789 Validation loss 0.04579915106296539 Accuracy 0.889875054359436\n",
      "Iteration 76260 Training loss 0.001265551894903183 Validation loss 0.04580916836857796 Accuracy 0.889750063419342\n",
      "Iteration 76270 Training loss 0.0012591381091624498 Validation loss 0.0457875095307827 Accuracy 0.8895000219345093\n",
      "Iteration 76280 Training loss 9.881581718218513e-06 Validation loss 0.0457819327712059 Accuracy 0.8893750309944153\n",
      "Iteration 76290 Training loss 1.0622937224979978e-05 Validation loss 0.045794252306222916 Accuracy 0.8895000219345093\n",
      "Iteration 76300 Training loss 0.0012619703775271773 Validation loss 0.04581572860479355 Accuracy 0.889750063419342\n",
      "Iteration 76310 Training loss 1.1969427760050166e-05 Validation loss 0.04578249901533127 Accuracy 0.8895000219345093\n",
      "Iteration 76320 Training loss 1.0741711776063312e-05 Validation loss 0.045784540474414825 Accuracy 0.8893750309944153\n",
      "Iteration 76330 Training loss 0.0025114677846431732 Validation loss 0.04578986391425133 Accuracy 0.8896250128746033\n",
      "Iteration 76340 Training loss 0.0025139388162642717 Validation loss 0.04580477625131607 Accuracy 0.889750063419342\n",
      "Iteration 76350 Training loss 0.001261747325770557 Validation loss 0.045793112367391586 Accuracy 0.889750063419342\n",
      "Iteration 76360 Training loss 0.00251179956831038 Validation loss 0.045819804072380066 Accuracy 0.889750063419342\n",
      "Iteration 76370 Training loss 0.0012615392915904522 Validation loss 0.04579024761915207 Accuracy 0.8896250128746033\n",
      "Iteration 76380 Training loss 1.414321468473645e-05 Validation loss 0.045810677111148834 Accuracy 0.889750063419342\n",
      "Iteration 76390 Training loss 1.0146892236662097e-05 Validation loss 0.0458044596016407 Accuracy 0.8896250128746033\n",
      "Iteration 76400 Training loss 0.002515953965485096 Validation loss 0.045820415019989014 Accuracy 0.8896250128746033\n",
      "Iteration 76410 Training loss 1.9949144189013168e-05 Validation loss 0.04579449072480202 Accuracy 0.889750063419342\n",
      "Iteration 76420 Training loss 0.0012608454562723637 Validation loss 0.045826077461242676 Accuracy 0.889875054359436\n",
      "Iteration 76430 Training loss 0.00251667108386755 Validation loss 0.04581567272543907 Accuracy 0.889750063419342\n",
      "Iteration 76440 Training loss 0.0012639111373573542 Validation loss 0.045804306864738464 Accuracy 0.8895000219345093\n",
      "Iteration 76450 Training loss 1.732781674945727e-05 Validation loss 0.04580346867442131 Accuracy 0.8895000219345093\n",
      "Iteration 76460 Training loss 0.0012606916716322303 Validation loss 0.04580865427851677 Accuracy 0.8893750309944153\n",
      "Iteration 76470 Training loss 0.0012581972405314445 Validation loss 0.045829448848962784 Accuracy 0.8896250128746033\n",
      "Iteration 76480 Training loss 0.0012608295073732734 Validation loss 0.045783236622810364 Accuracy 0.8896250128746033\n",
      "Iteration 76490 Training loss 0.0012605504598468542 Validation loss 0.04583184793591499 Accuracy 0.8895000219345093\n",
      "Iteration 76500 Training loss 0.001265114638954401 Validation loss 0.045783013105392456 Accuracy 0.8896250128746033\n",
      "Iteration 76510 Training loss 9.996404514822643e-06 Validation loss 0.045825935900211334 Accuracy 0.8896250128746033\n",
      "Iteration 76520 Training loss 1.1517286111484282e-05 Validation loss 0.04580925032496452 Accuracy 0.8895000219345093\n",
      "Iteration 76530 Training loss 1.1452361832198221e-05 Validation loss 0.045799873769283295 Accuracy 0.8896250128746033\n",
      "Iteration 76540 Training loss 0.0012604116927832365 Validation loss 0.04580402746796608 Accuracy 0.8893750309944153\n",
      "Iteration 76550 Training loss 0.0050125992856919765 Validation loss 0.04585558548569679 Accuracy 0.8893750309944153\n",
      "Iteration 76560 Training loss 1.3529208445106633e-05 Validation loss 0.0458049401640892 Accuracy 0.889750063419342\n",
      "Iteration 76570 Training loss 0.0012633211445063353 Validation loss 0.045807287096977234 Accuracy 0.8896250128746033\n",
      "Iteration 76580 Training loss 0.0037597946356981993 Validation loss 0.04578632488846779 Accuracy 0.8896250128746033\n",
      "Iteration 76590 Training loss 0.0037603285163640976 Validation loss 0.045854635536670685 Accuracy 0.8892500400543213\n",
      "Iteration 76600 Training loss 0.0037641911767423153 Validation loss 0.04579825699329376 Accuracy 0.8896250128746033\n",
      "Iteration 76610 Training loss 9.61854948400287e-06 Validation loss 0.04582139477133751 Accuracy 0.8895000219345093\n",
      "Iteration 76620 Training loss 9.877309821604285e-06 Validation loss 0.045826856046915054 Accuracy 0.8895000219345093\n",
      "Iteration 76630 Training loss 1.0692768228182103e-05 Validation loss 0.045840539038181305 Accuracy 0.8896250128746033\n",
      "Iteration 76640 Training loss 1.1875538802996743e-05 Validation loss 0.04582942649722099 Accuracy 0.8893750309944153\n",
      "Iteration 76650 Training loss 0.0012646823888644576 Validation loss 0.04581838846206665 Accuracy 0.8895000219345093\n",
      "Iteration 76660 Training loss 0.001261050347238779 Validation loss 0.04580970108509064 Accuracy 0.889875054359436\n",
      "Iteration 76670 Training loss 0.0012589894467964768 Validation loss 0.04582070931792259 Accuracy 0.8895000219345093\n",
      "Iteration 76680 Training loss 0.0025192173197865486 Validation loss 0.04580207169055939 Accuracy 0.8895000219345093\n",
      "Iteration 76690 Training loss 0.0012629508273676038 Validation loss 0.045831505209207535 Accuracy 0.8893750309944153\n",
      "Iteration 76700 Training loss 0.0025145183317363262 Validation loss 0.04583423212170601 Accuracy 0.8893750309944153\n",
      "Iteration 76710 Training loss 0.0025109259877353907 Validation loss 0.04582354798913002 Accuracy 0.8895000219345093\n",
      "Iteration 76720 Training loss 1.0833921805897262e-05 Validation loss 0.04581727460026741 Accuracy 0.8895000219345093\n",
      "Iteration 76730 Training loss 0.0012625239323824644 Validation loss 0.045809026807546616 Accuracy 0.889875054359436\n",
      "Iteration 76740 Training loss 0.00251195696182549 Validation loss 0.045804113149642944 Accuracy 0.889750063419342\n",
      "Iteration 76750 Training loss 0.0012611491838470101 Validation loss 0.04584036394953728 Accuracy 0.8895000219345093\n",
      "Iteration 76760 Training loss 0.001258870935998857 Validation loss 0.045859407633543015 Accuracy 0.8895000219345093\n",
      "Iteration 76770 Training loss 1.3146397577656899e-05 Validation loss 0.04583144560456276 Accuracy 0.8896250128746033\n",
      "Iteration 76780 Training loss 0.0012613883009180427 Validation loss 0.045820318162441254 Accuracy 0.889875054359436\n",
      "Iteration 76790 Training loss 0.0012588138924911618 Validation loss 0.04587476700544357 Accuracy 0.8896250128746033\n",
      "Iteration 76800 Training loss 0.0012599814217537642 Validation loss 0.0458134189248085 Accuracy 0.8896250128746033\n",
      "Iteration 76810 Training loss 1.9154767869622447e-05 Validation loss 0.04581543803215027 Accuracy 0.889750063419342\n",
      "Iteration 76820 Training loss 0.0037597976624965668 Validation loss 0.04582256078720093 Accuracy 0.89000004529953\n",
      "Iteration 76830 Training loss 0.005007736384868622 Validation loss 0.045861463993787766 Accuracy 0.889750063419342\n",
      "Iteration 76840 Training loss 0.0012650425778701901 Validation loss 0.0458427369594574 Accuracy 0.889750063419342\n",
      "Iteration 76850 Training loss 1.1520869520609267e-05 Validation loss 0.04585373401641846 Accuracy 0.889750063419342\n",
      "Iteration 76860 Training loss 0.0025147548876702785 Validation loss 0.045831598341464996 Accuracy 0.889750063419342\n",
      "Iteration 76870 Training loss 1.2381224223645404e-05 Validation loss 0.04583519324660301 Accuracy 0.889750063419342\n",
      "Iteration 76880 Training loss 0.002514349762350321 Validation loss 0.04583915323019028 Accuracy 0.8895000219345093\n",
      "Iteration 76890 Training loss 0.007513514719903469 Validation loss 0.04587146267294884 Accuracy 0.8895000219345093\n",
      "Iteration 76900 Training loss 0.0012610943522304296 Validation loss 0.045867227017879486 Accuracy 0.8895000219345093\n",
      "Iteration 76910 Training loss 0.003761243773624301 Validation loss 0.04584333673119545 Accuracy 0.8893750309944153\n",
      "Iteration 76920 Training loss 0.0012633842416107655 Validation loss 0.04585030674934387 Accuracy 0.8892500400543213\n",
      "Iteration 76930 Training loss 0.0025141816586256027 Validation loss 0.0458475761115551 Accuracy 0.8895000219345093\n",
      "Iteration 76940 Training loss 1.3012100680498406e-05 Validation loss 0.04584283381700516 Accuracy 0.8895000219345093\n",
      "Iteration 76950 Training loss 1.1686319339787588e-05 Validation loss 0.045830514281988144 Accuracy 0.889875054359436\n",
      "Iteration 76960 Training loss 0.002511897822842002 Validation loss 0.045834824442863464 Accuracy 0.889750063419342\n",
      "Iteration 76970 Training loss 0.0012610747944563627 Validation loss 0.04581531509757042 Accuracy 0.889875054359436\n",
      "Iteration 76980 Training loss 0.0025142452213913202 Validation loss 0.045845989137887955 Accuracy 0.889875054359436\n",
      "Iteration 76990 Training loss 0.0012608023826032877 Validation loss 0.04583040624856949 Accuracy 0.889875054359436\n",
      "Iteration 77000 Training loss 9.290253728977405e-06 Validation loss 0.045831628143787384 Accuracy 0.889750063419342\n",
      "Iteration 77010 Training loss 0.0012596403248608112 Validation loss 0.04585025832056999 Accuracy 0.8893750309944153\n",
      "Iteration 77020 Training loss 0.0012616395251825452 Validation loss 0.04584488272666931 Accuracy 0.889875054359436\n",
      "Iteration 77030 Training loss 9.35803291213233e-06 Validation loss 0.04582127556204796 Accuracy 0.8896250128746033\n",
      "Iteration 77040 Training loss 0.0037642251700162888 Validation loss 0.045807741582393646 Accuracy 0.8896250128746033\n",
      "Iteration 77050 Training loss 0.0012666531838476658 Validation loss 0.04581166058778763 Accuracy 0.8893750309944153\n",
      "Iteration 77060 Training loss 0.001265382394194603 Validation loss 0.04582715034484863 Accuracy 0.889750063419342\n",
      "Iteration 77070 Training loss 0.0012606732780113816 Validation loss 0.045815031975507736 Accuracy 0.8893750309944153\n",
      "Iteration 77080 Training loss 0.006261267699301243 Validation loss 0.045815013349056244 Accuracy 0.8896250128746033\n",
      "Iteration 77090 Training loss 0.002513973042368889 Validation loss 0.04582590237259865 Accuracy 0.8896250128746033\n",
      "Iteration 77100 Training loss 0.005011959467083216 Validation loss 0.04582386091351509 Accuracy 0.8895000219345093\n",
      "Iteration 77110 Training loss 0.0025127932894974947 Validation loss 0.04582669213414192 Accuracy 0.889875054359436\n",
      "Iteration 77120 Training loss 0.0012653757585212588 Validation loss 0.045824695378541946 Accuracy 0.8893750309944153\n",
      "Iteration 77130 Training loss 1.2815732588933315e-05 Validation loss 0.045810818672180176 Accuracy 0.8892500400543213\n",
      "Iteration 77140 Training loss 1.2457418961275835e-05 Validation loss 0.04583551734685898 Accuracy 0.8892500400543213\n",
      "Iteration 77150 Training loss 1.0514290806895588e-05 Validation loss 0.045857641845941544 Accuracy 0.8892500400543213\n",
      "Iteration 77160 Training loss 0.005014019086956978 Validation loss 0.04581698775291443 Accuracy 0.889750063419342\n",
      "Iteration 77170 Training loss 8.108471774903592e-06 Validation loss 0.045804888010025024 Accuracy 0.8895000219345093\n",
      "Iteration 77180 Training loss 1.611688821867574e-05 Validation loss 0.04581008851528168 Accuracy 0.8891250491142273\n",
      "Iteration 77190 Training loss 0.0012599785113707185 Validation loss 0.045808158814907074 Accuracy 0.8895000219345093\n",
      "Iteration 77200 Training loss 0.0012612909777089953 Validation loss 0.04580693691968918 Accuracy 0.8896250128746033\n",
      "Iteration 77210 Training loss 0.002509591868147254 Validation loss 0.04583042114973068 Accuracy 0.8896250128746033\n",
      "Iteration 77220 Training loss 1.8039858332485892e-05 Validation loss 0.045847080647945404 Accuracy 0.8896250128746033\n",
      "Iteration 77230 Training loss 1.6731459254515357e-05 Validation loss 0.04585690423846245 Accuracy 0.8896250128746033\n",
      "Iteration 77240 Training loss 9.153520295512863e-06 Validation loss 0.04584600403904915 Accuracy 0.8893750309944153\n",
      "Iteration 77250 Training loss 1.0232253771391697e-05 Validation loss 0.04581919312477112 Accuracy 0.889750063419342\n",
      "Iteration 77260 Training loss 1.4835349247732665e-05 Validation loss 0.0458434596657753 Accuracy 0.8892500400543213\n",
      "Iteration 77270 Training loss 0.0012616836465895176 Validation loss 0.04587329179048538 Accuracy 0.8893750309944153\n",
      "Iteration 77280 Training loss 0.001259936485439539 Validation loss 0.04586540907621384 Accuracy 0.8895000219345093\n",
      "Iteration 77290 Training loss 0.0025159029755741358 Validation loss 0.045890506356954575 Accuracy 0.8896250128746033\n",
      "Iteration 77300 Training loss 0.0012613455764949322 Validation loss 0.045840226113796234 Accuracy 0.8893750309944153\n",
      "Iteration 77310 Training loss 0.0037640368100255728 Validation loss 0.04584847763180733 Accuracy 0.8893750309944153\n",
      "Iteration 77320 Training loss 0.0037597226910293102 Validation loss 0.04584275931119919 Accuracy 0.8895000219345093\n",
      "Iteration 77330 Training loss 1.1327847460051998e-05 Validation loss 0.045840442180633545 Accuracy 0.8896250128746033\n",
      "Iteration 77340 Training loss 0.0025175060145556927 Validation loss 0.04582656919956207 Accuracy 0.8895000219345093\n",
      "Iteration 77350 Training loss 1.1092120075772982e-05 Validation loss 0.045853134244680405 Accuracy 0.889750063419342\n",
      "Iteration 77360 Training loss 1.1556013305380475e-05 Validation loss 0.045824456959962845 Accuracy 0.8896250128746033\n",
      "Iteration 77370 Training loss 1.514344876341056e-05 Validation loss 0.04583600163459778 Accuracy 0.8896250128746033\n",
      "Iteration 77380 Training loss 1.001825148705393e-05 Validation loss 0.04587038606405258 Accuracy 0.889750063419342\n",
      "Iteration 77390 Training loss 0.002514322753995657 Validation loss 0.04583950340747833 Accuracy 0.889750063419342\n",
      "Iteration 77400 Training loss 1.3051741916569881e-05 Validation loss 0.04582812637090683 Accuracy 0.8893750309944153\n",
      "Iteration 77410 Training loss 0.0037586663383990526 Validation loss 0.04583442583680153 Accuracy 0.8893750309944153\n",
      "Iteration 77420 Training loss 0.0012679416686296463 Validation loss 0.045842595398426056 Accuracy 0.8895000219345093\n",
      "Iteration 77430 Training loss 0.00376117997802794 Validation loss 0.04584795609116554 Accuracy 0.8895000219345093\n",
      "Iteration 77440 Training loss 0.001260740915313363 Validation loss 0.04585062712430954 Accuracy 0.8895000219345093\n",
      "Iteration 77450 Training loss 0.0012607644312083721 Validation loss 0.045831307768821716 Accuracy 0.8895000219345093\n",
      "Iteration 77460 Training loss 0.0012614380102604628 Validation loss 0.04583490267395973 Accuracy 0.8893750309944153\n",
      "Iteration 77470 Training loss 0.0025114223826676607 Validation loss 0.0458730012178421 Accuracy 0.8891250491142273\n",
      "Iteration 77480 Training loss 0.0012594708241522312 Validation loss 0.045910559594631195 Accuracy 0.8892500400543213\n",
      "Iteration 77490 Training loss 9.383506949234288e-06 Validation loss 0.04589403048157692 Accuracy 0.8891250491142273\n",
      "Iteration 77500 Training loss 0.0050147478468716145 Validation loss 0.04583197832107544 Accuracy 0.8896250128746033\n",
      "Iteration 77510 Training loss 0.0025150638539344072 Validation loss 0.04585244879126549 Accuracy 0.8896250128746033\n",
      "Iteration 77520 Training loss 0.0012642776127904654 Validation loss 0.04585687071084976 Accuracy 0.8893750309944153\n",
      "Iteration 77530 Training loss 0.0025158545468002558 Validation loss 0.04583398252725601 Accuracy 0.8895000219345093\n",
      "Iteration 77540 Training loss 0.002511999337002635 Validation loss 0.045831698924303055 Accuracy 0.889875054359436\n",
      "Iteration 77550 Training loss 0.001260538469068706 Validation loss 0.045850396156311035 Accuracy 0.8895000219345093\n",
      "Iteration 77560 Training loss 0.002512267790734768 Validation loss 0.045835793018341064 Accuracy 0.8896250128746033\n",
      "Iteration 77570 Training loss 9.622660400054883e-06 Validation loss 0.0458611324429512 Accuracy 0.8896250128746033\n",
      "Iteration 77580 Training loss 0.0012583474162966013 Validation loss 0.04583607241511345 Accuracy 0.8896250128746033\n",
      "Iteration 77590 Training loss 1.1630577319010627e-05 Validation loss 0.04582758620381355 Accuracy 0.8895000219345093\n",
      "Iteration 77600 Training loss 0.0012650974094867706 Validation loss 0.045810483396053314 Accuracy 0.8893750309944153\n",
      "Iteration 77610 Training loss 1.2504464393714443e-05 Validation loss 0.04583073407411575 Accuracy 0.8896250128746033\n",
      "Iteration 77620 Training loss 1.3131532796251122e-05 Validation loss 0.04583354294300079 Accuracy 0.8896250128746033\n",
      "Iteration 77630 Training loss 1.2367558156256564e-05 Validation loss 0.04585576802492142 Accuracy 0.8896250128746033\n",
      "Iteration 77640 Training loss 1.3528100680559874e-05 Validation loss 0.045850206166505814 Accuracy 0.8896250128746033\n",
      "Iteration 77650 Training loss 0.0012630452401936054 Validation loss 0.04587356746196747 Accuracy 0.8896250128746033\n",
      "Iteration 77660 Training loss 0.0012641854118555784 Validation loss 0.04585764557123184 Accuracy 0.8893750309944153\n",
      "Iteration 77670 Training loss 7.605491191498004e-06 Validation loss 0.0458650141954422 Accuracy 0.8892500400543213\n",
      "Iteration 77680 Training loss 0.0012610327685251832 Validation loss 0.04586882144212723 Accuracy 0.8893750309944153\n",
      "Iteration 77690 Training loss 0.0012632429134100676 Validation loss 0.04586666822433472 Accuracy 0.8892500400543213\n",
      "Iteration 77700 Training loss 1.2356571460259147e-05 Validation loss 0.045861341059207916 Accuracy 0.8892500400543213\n",
      "Iteration 77710 Training loss 0.0025130710564553738 Validation loss 0.04586222022771835 Accuracy 0.8891250491142273\n",
      "Iteration 77720 Training loss 0.0012635346502065659 Validation loss 0.0458732470870018 Accuracy 0.8891250491142273\n",
      "Iteration 77730 Training loss 0.00126111030112952 Validation loss 0.04586176574230194 Accuracy 0.8892500400543213\n",
      "Iteration 77740 Training loss 1.3082024452160113e-05 Validation loss 0.045871980488300323 Accuracy 0.8891250491142273\n",
      "Iteration 77750 Training loss 0.0025108789559453726 Validation loss 0.04591219499707222 Accuracy 0.8896250128746033\n",
      "Iteration 77760 Training loss 0.0012626766692847013 Validation loss 0.04586419835686684 Accuracy 0.8893750309944153\n",
      "Iteration 77770 Training loss 0.0012657340848818421 Validation loss 0.04585972800850868 Accuracy 0.8895000219345093\n",
      "Iteration 77780 Training loss 1.4399257452168968e-05 Validation loss 0.045869242399930954 Accuracy 0.8895000219345093\n",
      "Iteration 77790 Training loss 0.0012618520995602012 Validation loss 0.045853037387132645 Accuracy 0.8892500400543213\n",
      "Iteration 77800 Training loss 0.001260568737052381 Validation loss 0.045845016837120056 Accuracy 0.8893750309944153\n",
      "Iteration 77810 Training loss 0.0012624202063307166 Validation loss 0.04585287719964981 Accuracy 0.8895000219345093\n",
      "Iteration 77820 Training loss 0.0025171905290335417 Validation loss 0.04590883105993271 Accuracy 0.889750063419342\n",
      "Iteration 77830 Training loss 0.0012597129680216312 Validation loss 0.04583679884672165 Accuracy 0.8896250128746033\n",
      "Iteration 77840 Training loss 0.0012645275564864278 Validation loss 0.04585535451769829 Accuracy 0.8893750309944153\n",
      "Iteration 77850 Training loss 1.4426265806832816e-05 Validation loss 0.04592170566320419 Accuracy 0.889750063419342\n",
      "Iteration 77860 Training loss 0.0025120440404862165 Validation loss 0.04590485617518425 Accuracy 0.8895000219345093\n",
      "Iteration 77870 Training loss 1.0142040082428139e-05 Validation loss 0.04586393013596535 Accuracy 0.8892500400543213\n",
      "Iteration 77880 Training loss 7.877935786382295e-06 Validation loss 0.04586705192923546 Accuracy 0.8895000219345093\n",
      "Iteration 77890 Training loss 0.0025107969995588064 Validation loss 0.045857008546590805 Accuracy 0.8893750309944153\n",
      "Iteration 77900 Training loss 1.252115544048138e-05 Validation loss 0.045851245522499084 Accuracy 0.8892500400543213\n",
      "Iteration 77910 Training loss 0.0025097557809203863 Validation loss 0.04586349055171013 Accuracy 0.8892500400543213\n",
      "Iteration 77920 Training loss 0.002510994905605912 Validation loss 0.04587196931242943 Accuracy 0.8893750309944153\n",
      "Iteration 77930 Training loss 0.0012596233282238245 Validation loss 0.04587810859084129 Accuracy 0.8893750309944153\n",
      "Iteration 77940 Training loss 1.1500005712150596e-05 Validation loss 0.045871347188949585 Accuracy 0.8892500400543213\n",
      "Iteration 77950 Training loss 0.001258848118595779 Validation loss 0.04587288945913315 Accuracy 0.8892500400543213\n",
      "Iteration 77960 Training loss 8.024595445021987e-06 Validation loss 0.045851826667785645 Accuracy 0.8895000219345093\n",
      "Iteration 77970 Training loss 0.0012634353479370475 Validation loss 0.045837871730327606 Accuracy 0.8893750309944153\n",
      "Iteration 77980 Training loss 0.0025101841893047094 Validation loss 0.04582594335079193 Accuracy 0.8892500400543213\n",
      "Iteration 77990 Training loss 0.00501085864380002 Validation loss 0.04588613286614418 Accuracy 0.8893750309944153\n",
      "Iteration 78000 Training loss 0.0025092673022300005 Validation loss 0.04586370289325714 Accuracy 0.8891250491142273\n",
      "Iteration 78010 Training loss 9.129574209509883e-06 Validation loss 0.04587249085307121 Accuracy 0.8892500400543213\n",
      "Iteration 78020 Training loss 0.0012673299061134458 Validation loss 0.045850642025470734 Accuracy 0.8895000219345093\n",
      "Iteration 78030 Training loss 1.134384820034029e-05 Validation loss 0.04586341977119446 Accuracy 0.889750063419342\n",
      "Iteration 78040 Training loss 0.001259631011635065 Validation loss 0.045862920582294464 Accuracy 0.889750063419342\n",
      "Iteration 78050 Training loss 1.3742795999860391e-05 Validation loss 0.04586302489042282 Accuracy 0.8895000219345093\n",
      "Iteration 78060 Training loss 0.0012624929659068584 Validation loss 0.04584308713674545 Accuracy 0.8893750309944153\n",
      "Iteration 78070 Training loss 1.1630135304585565e-05 Validation loss 0.045838747173547745 Accuracy 0.8893750309944153\n",
      "Iteration 78080 Training loss 0.0050195977091789246 Validation loss 0.04584907740354538 Accuracy 0.8895000219345093\n",
      "Iteration 78090 Training loss 0.002510232850909233 Validation loss 0.04584299772977829 Accuracy 0.8893750309944153\n",
      "Iteration 78100 Training loss 1.4775823728996329e-05 Validation loss 0.045849766582250595 Accuracy 0.8892500400543213\n",
      "Iteration 78110 Training loss 0.0025125995744019747 Validation loss 0.0458645261824131 Accuracy 0.8893750309944153\n",
      "Iteration 78120 Training loss 0.001261136494576931 Validation loss 0.04584772512316704 Accuracy 0.8893750309944153\n",
      "Iteration 78130 Training loss 0.0012583265779539943 Validation loss 0.045872583985328674 Accuracy 0.8892500400543213\n",
      "Iteration 78140 Training loss 0.0012586135417222977 Validation loss 0.04585495963692665 Accuracy 0.8895000219345093\n",
      "Iteration 78150 Training loss 1.487592908233637e-05 Validation loss 0.0458485372364521 Accuracy 0.8896250128746033\n",
      "Iteration 78160 Training loss 0.0025092826690524817 Validation loss 0.04585299640893936 Accuracy 0.8895000219345093\n",
      "Iteration 78170 Training loss 0.0012609276454895735 Validation loss 0.045874811708927155 Accuracy 0.8896250128746033\n",
      "Iteration 78180 Training loss 8.15472685644636e-06 Validation loss 0.04586820304393768 Accuracy 0.8895000219345093\n",
      "Iteration 78190 Training loss 0.0012649435084313154 Validation loss 0.04586995020508766 Accuracy 0.8896250128746033\n",
      "Iteration 78200 Training loss 0.0012589747784659266 Validation loss 0.04587878659367561 Accuracy 0.8895000219345093\n",
      "Iteration 78210 Training loss 0.0012628731783479452 Validation loss 0.04588509723544121 Accuracy 0.8892500400543213\n",
      "Iteration 78220 Training loss 1.1237815670028795e-05 Validation loss 0.04587440565228462 Accuracy 0.8893750309944153\n",
      "Iteration 78230 Training loss 1.4317866771307308e-05 Validation loss 0.045861490070819855 Accuracy 0.8895000219345093\n",
      "Iteration 78240 Training loss 0.0025120920035988092 Validation loss 0.045858606696128845 Accuracy 0.8895000219345093\n",
      "Iteration 78250 Training loss 0.0012625664239749312 Validation loss 0.04589005559682846 Accuracy 0.8888750672340393\n",
      "Iteration 78260 Training loss 0.0025089227128773928 Validation loss 0.04587133973836899 Accuracy 0.8891250491142273\n",
      "Iteration 78270 Training loss 7.89213754615048e-06 Validation loss 0.045883506536483765 Accuracy 0.8890000581741333\n",
      "Iteration 78280 Training loss 0.0012606249656528234 Validation loss 0.04586157947778702 Accuracy 0.8893750309944153\n",
      "Iteration 78290 Training loss 0.001261869678273797 Validation loss 0.04587256535887718 Accuracy 0.8892500400543213\n",
      "Iteration 78300 Training loss 1.1093793546024244e-05 Validation loss 0.04589486122131348 Accuracy 0.8895000219345093\n",
      "Iteration 78310 Training loss 9.407453035237268e-06 Validation loss 0.04588538780808449 Accuracy 0.8893750309944153\n",
      "Iteration 78320 Training loss 0.0012711884919553995 Validation loss 0.04588180035352707 Accuracy 0.8892500400543213\n",
      "Iteration 78330 Training loss 1.4398367966350634e-05 Validation loss 0.04587293416261673 Accuracy 0.8893750309944153\n",
      "Iteration 78340 Training loss 0.0012628036784008145 Validation loss 0.04585246369242668 Accuracy 0.8896250128746033\n",
      "Iteration 78350 Training loss 1.2561829862534069e-05 Validation loss 0.04584557190537453 Accuracy 0.8895000219345093\n",
      "Iteration 78360 Training loss 1.6800604498712346e-05 Validation loss 0.045875199139118195 Accuracy 0.8895000219345093\n",
      "Iteration 78370 Training loss 0.001260820310562849 Validation loss 0.0458572655916214 Accuracy 0.8893750309944153\n",
      "Iteration 78380 Training loss 0.00251484801992774 Validation loss 0.04586130753159523 Accuracy 0.8893750309944153\n",
      "Iteration 78390 Training loss 0.00126385479234159 Validation loss 0.045853789895772934 Accuracy 0.8895000219345093\n",
      "Iteration 78400 Training loss 0.0012629000702872872 Validation loss 0.04584590345621109 Accuracy 0.8895000219345093\n",
      "Iteration 78410 Training loss 7.73731244407827e-06 Validation loss 0.04582466185092926 Accuracy 0.8892500400543213\n",
      "Iteration 78420 Training loss 0.002508155070245266 Validation loss 0.045861780643463135 Accuracy 0.8895000219345093\n",
      "Iteration 78430 Training loss 0.002512493636459112 Validation loss 0.0458509735763073 Accuracy 0.8892500400543213\n",
      "Iteration 78440 Training loss 0.0012612036662176251 Validation loss 0.04585428163409233 Accuracy 0.8892500400543213\n",
      "Iteration 78450 Training loss 0.0012673562159761786 Validation loss 0.04581502079963684 Accuracy 0.8892500400543213\n",
      "Iteration 78460 Training loss 1.1761503628804348e-05 Validation loss 0.04585547372698784 Accuracy 0.8895000219345093\n",
      "Iteration 78470 Training loss 0.001259133219718933 Validation loss 0.04584305360913277 Accuracy 0.8893750309944153\n",
      "Iteration 78480 Training loss 1.5932746464386582e-05 Validation loss 0.045819737017154694 Accuracy 0.8892500400543213\n",
      "Iteration 78490 Training loss 1.1627784260781482e-05 Validation loss 0.045821648091077805 Accuracy 0.8893750309944153\n",
      "Iteration 78500 Training loss 9.37399181566434e-06 Validation loss 0.04582075774669647 Accuracy 0.8891250491142273\n",
      "Iteration 78510 Training loss 0.00250830315053463 Validation loss 0.045835673809051514 Accuracy 0.8893750309944153\n",
      "Iteration 78520 Training loss 0.0025132542941719294 Validation loss 0.04581531882286072 Accuracy 0.8893750309944153\n",
      "Iteration 78530 Training loss 0.0012647259281948209 Validation loss 0.04583191126585007 Accuracy 0.8895000219345093\n",
      "Iteration 78540 Training loss 0.0012646603863686323 Validation loss 0.045842450112104416 Accuracy 0.8893750309944153\n",
      "Iteration 78550 Training loss 0.0012577567249536514 Validation loss 0.04588926210999489 Accuracy 0.8892500400543213\n",
      "Iteration 78560 Training loss 0.002509520621970296 Validation loss 0.045861322432756424 Accuracy 0.8891250491142273\n",
      "Iteration 78570 Training loss 0.0012646567774936557 Validation loss 0.04584130272269249 Accuracy 0.8893750309944153\n",
      "Iteration 78580 Training loss 9.458336535317358e-06 Validation loss 0.04583811014890671 Accuracy 0.8895000219345093\n",
      "Iteration 78590 Training loss 8.027830517676193e-06 Validation loss 0.04582904279232025 Accuracy 0.8892500400543213\n",
      "Iteration 78600 Training loss 9.668070561019704e-06 Validation loss 0.045827027410268784 Accuracy 0.8892500400543213\n",
      "Iteration 78610 Training loss 0.001260001678019762 Validation loss 0.0458526536822319 Accuracy 0.8893750309944153\n",
      "Iteration 78620 Training loss 0.0012639241758733988 Validation loss 0.04585857689380646 Accuracy 0.8892500400543213\n",
      "Iteration 78630 Training loss 1.4190463843988255e-05 Validation loss 0.04583410173654556 Accuracy 0.8895000219345093\n",
      "Iteration 78640 Training loss 0.0025105581153184175 Validation loss 0.04586188867688179 Accuracy 0.8895000219345093\n",
      "Iteration 78650 Training loss 0.0012621725909411907 Validation loss 0.04584439471364021 Accuracy 0.8895000219345093\n",
      "Iteration 78660 Training loss 1.293703189730877e-05 Validation loss 0.04584548994898796 Accuracy 0.8895000219345093\n",
      "Iteration 78670 Training loss 1.0310653124179225e-05 Validation loss 0.04585239663720131 Accuracy 0.8892500400543213\n",
      "Iteration 78680 Training loss 0.0012663854286074638 Validation loss 0.045822251588106155 Accuracy 0.889750063419342\n",
      "Iteration 78690 Training loss 1.331786552327685e-05 Validation loss 0.045874208211898804 Accuracy 0.8888750672340393\n",
      "Iteration 78700 Training loss 1.2230626452947035e-05 Validation loss 0.04585971310734749 Accuracy 0.8888750672340393\n",
      "Iteration 78710 Training loss 0.002508951583877206 Validation loss 0.04584600031375885 Accuracy 0.8892500400543213\n",
      "Iteration 78720 Training loss 0.0012604547664523125 Validation loss 0.04583023115992546 Accuracy 0.8895000219345093\n",
      "Iteration 78730 Training loss 0.0012599230976775289 Validation loss 0.04586504399776459 Accuracy 0.8891250491142273\n",
      "Iteration 78740 Training loss 1.2802755918528419e-05 Validation loss 0.04584803804755211 Accuracy 0.8893750309944153\n",
      "Iteration 78750 Training loss 0.0012592020211741328 Validation loss 0.04585803300142288 Accuracy 0.8893750309944153\n",
      "Iteration 78760 Training loss 1.3746851436735597e-05 Validation loss 0.045830778777599335 Accuracy 0.8896250128746033\n",
      "Iteration 78770 Training loss 8.485459147777874e-06 Validation loss 0.045848727226257324 Accuracy 0.8895000219345093\n",
      "Iteration 78780 Training loss 0.002513594925403595 Validation loss 0.04584600031375885 Accuracy 0.8893750309944153\n",
      "Iteration 78790 Training loss 1.4388595445780084e-05 Validation loss 0.04583504796028137 Accuracy 0.8896250128746033\n",
      "Iteration 78800 Training loss 0.0025174932088702917 Validation loss 0.04585794359445572 Accuracy 0.8896250128746033\n",
      "Iteration 78810 Training loss 0.002510686405003071 Validation loss 0.04587412625551224 Accuracy 0.8893750309944153\n",
      "Iteration 78820 Training loss 1.0494022717466578e-05 Validation loss 0.045864664018154144 Accuracy 0.8896250128746033\n",
      "Iteration 78830 Training loss 1.2705081644526217e-05 Validation loss 0.04584978520870209 Accuracy 0.8895000219345093\n",
      "Iteration 78840 Training loss 0.0025124584790319204 Validation loss 0.045847538858652115 Accuracy 0.8896250128746033\n",
      "Iteration 78850 Training loss 1.3052138456259854e-05 Validation loss 0.04583965614438057 Accuracy 0.8893750309944153\n",
      "Iteration 78860 Training loss 0.0012619481422007084 Validation loss 0.045847147703170776 Accuracy 0.8893750309944153\n",
      "Iteration 78870 Training loss 0.0025086156092584133 Validation loss 0.04583553597331047 Accuracy 0.889875054359436\n",
      "Iteration 78880 Training loss 1.060008980857674e-05 Validation loss 0.04584567993879318 Accuracy 0.889750063419342\n",
      "Iteration 78890 Training loss 0.0012612638529390097 Validation loss 0.045845527201890945 Accuracy 0.889750063419342\n",
      "Iteration 78900 Training loss 0.0025124703533947468 Validation loss 0.04588336497545242 Accuracy 0.8892500400543213\n",
      "Iteration 78910 Training loss 1.2338723536231555e-05 Validation loss 0.045854050666093826 Accuracy 0.8895000219345093\n",
      "Iteration 78920 Training loss 8.98286270967219e-06 Validation loss 0.045855917036533356 Accuracy 0.8895000219345093\n",
      "Iteration 78930 Training loss 1.0304275747330394e-05 Validation loss 0.045850083231925964 Accuracy 0.8892500400543213\n",
      "Iteration 78940 Training loss 9.675757610239089e-06 Validation loss 0.04583539068698883 Accuracy 0.8893750309944153\n",
      "Iteration 78950 Training loss 9.40168320084922e-06 Validation loss 0.04585939273238182 Accuracy 0.8893750309944153\n",
      "Iteration 78960 Training loss 8.698893907421734e-06 Validation loss 0.04586721956729889 Accuracy 0.8893750309944153\n",
      "Iteration 78970 Training loss 8.913258170650806e-06 Validation loss 0.045847658067941666 Accuracy 0.8895000219345093\n",
      "Iteration 78980 Training loss 0.002507027005776763 Validation loss 0.04583105072379112 Accuracy 0.8893750309944153\n",
      "Iteration 78990 Training loss 0.0037623499520123005 Validation loss 0.045866500586271286 Accuracy 0.8893750309944153\n",
      "Iteration 79000 Training loss 0.0012634360464289784 Validation loss 0.045862700790166855 Accuracy 0.8895000219345093\n",
      "Iteration 79010 Training loss 0.0012613580329343677 Validation loss 0.04585401341319084 Accuracy 0.8891250491142273\n",
      "Iteration 79020 Training loss 0.0012597247259691358 Validation loss 0.04588145762681961 Accuracy 0.8891250491142273\n",
      "Iteration 79030 Training loss 0.0025110957212746143 Validation loss 0.045879822224378586 Accuracy 0.8892500400543213\n",
      "Iteration 79040 Training loss 1.6774461983004585e-05 Validation loss 0.045860085636377335 Accuracy 0.8893750309944153\n",
      "Iteration 79050 Training loss 0.005014865193516016 Validation loss 0.0458434596657753 Accuracy 0.8892500400543213\n",
      "Iteration 79060 Training loss 0.0012599460314959288 Validation loss 0.04586805775761604 Accuracy 0.8892500400543213\n",
      "Iteration 79070 Training loss 0.00250998348928988 Validation loss 0.045857157558202744 Accuracy 0.8892500400543213\n",
      "Iteration 79080 Training loss 1.75776585820131e-05 Validation loss 0.04588102176785469 Accuracy 0.8893750309944153\n",
      "Iteration 79090 Training loss 1.5225794413709082e-05 Validation loss 0.04585692659020424 Accuracy 0.8893750309944153\n",
      "Iteration 79100 Training loss 0.0025148913264274597 Validation loss 0.045863077044487 Accuracy 0.8891250491142273\n",
      "Iteration 79110 Training loss 0.0012592902639880776 Validation loss 0.04587525501847267 Accuracy 0.8891250491142273\n",
      "Iteration 79120 Training loss 9.877117918222211e-06 Validation loss 0.04585624858736992 Accuracy 0.8891250491142273\n",
      "Iteration 79130 Training loss 9.143899660557508e-06 Validation loss 0.04586276412010193 Accuracy 0.8893750309944153\n",
      "Iteration 79140 Training loss 0.001263451878912747 Validation loss 0.04586641490459442 Accuracy 0.8895000219345093\n",
      "Iteration 79150 Training loss 1.280354626942426e-05 Validation loss 0.04586349055171013 Accuracy 0.889750063419342\n",
      "Iteration 79160 Training loss 0.003758383681997657 Validation loss 0.04587823525071144 Accuracy 0.8893750309944153\n",
      "Iteration 79170 Training loss 0.0012604676885530353 Validation loss 0.04587065801024437 Accuracy 0.889750063419342\n",
      "Iteration 79180 Training loss 1.3845920875610318e-05 Validation loss 0.04586874693632126 Accuracy 0.8896250128746033\n",
      "Iteration 79190 Training loss 0.001262259902432561 Validation loss 0.045854371041059494 Accuracy 0.8896250128746033\n",
      "Iteration 79200 Training loss 0.0012589205289259553 Validation loss 0.045854173600673676 Accuracy 0.8892500400543213\n",
      "Iteration 79210 Training loss 1.1680707757477649e-05 Validation loss 0.04582845792174339 Accuracy 0.8895000219345093\n",
      "Iteration 79220 Training loss 1.2722864994429983e-05 Validation loss 0.045836206525564194 Accuracy 0.8893750309944153\n",
      "Iteration 79230 Training loss 9.697287168819457e-06 Validation loss 0.04583464190363884 Accuracy 0.8893750309944153\n",
      "Iteration 79240 Training loss 0.001259518670849502 Validation loss 0.04585176706314087 Accuracy 0.8892500400543213\n",
      "Iteration 79250 Training loss 0.002512506442144513 Validation loss 0.04586281627416611 Accuracy 0.8892500400543213\n",
      "Iteration 79260 Training loss 0.002508437028154731 Validation loss 0.04585609957575798 Accuracy 0.8891250491142273\n",
      "Iteration 79270 Training loss 8.91799027158413e-06 Validation loss 0.04583998769521713 Accuracy 0.8891250491142273\n",
      "Iteration 79280 Training loss 1.1563030966499355e-05 Validation loss 0.04587579146027565 Accuracy 0.8896250128746033\n",
      "Iteration 79290 Training loss 0.001266620703972876 Validation loss 0.04586775600910187 Accuracy 0.8891250491142273\n",
      "Iteration 79300 Training loss 0.0012631673598662019 Validation loss 0.04585718363523483 Accuracy 0.8892500400543213\n",
      "Iteration 79310 Training loss 0.002515282016247511 Validation loss 0.04583632946014404 Accuracy 0.8891250491142273\n",
      "Iteration 79320 Training loss 0.001265953411348164 Validation loss 0.04584884271025658 Accuracy 0.8893750309944153\n",
      "Iteration 79330 Training loss 1.0872874554479495e-05 Validation loss 0.04586441442370415 Accuracy 0.8891250491142273\n",
      "Iteration 79340 Training loss 0.0012610986595973372 Validation loss 0.045853402465581894 Accuracy 0.8891250491142273\n",
      "Iteration 79350 Training loss 0.002515049884095788 Validation loss 0.04588664323091507 Accuracy 0.8890000581741333\n",
      "Iteration 79360 Training loss 0.0012607319513335824 Validation loss 0.045883819460868835 Accuracy 0.8893750309944153\n",
      "Iteration 79370 Training loss 0.0012623786460608244 Validation loss 0.04589274525642395 Accuracy 0.8891250491142273\n",
      "Iteration 79380 Training loss 0.0037590074352920055 Validation loss 0.045865923166275024 Accuracy 0.8891250491142273\n",
      "Iteration 79390 Training loss 0.0012592856073752046 Validation loss 0.045869406312704086 Accuracy 0.8890000581741333\n",
      "Iteration 79400 Training loss 1.4836598893452901e-05 Validation loss 0.045872461050748825 Accuracy 0.8892500400543213\n",
      "Iteration 79410 Training loss 0.0012618384789675474 Validation loss 0.04587820544838905 Accuracy 0.8892500400543213\n",
      "Iteration 79420 Training loss 1.248779062734684e-05 Validation loss 0.04587535187602043 Accuracy 0.8892500400543213\n",
      "Iteration 79430 Training loss 8.157723641488701e-06 Validation loss 0.04587443917989731 Accuracy 0.8891250491142273\n",
      "Iteration 79440 Training loss 0.0012625184608623385 Validation loss 0.04587971791625023 Accuracy 0.8891250491142273\n",
      "Iteration 79450 Training loss 0.002513926476240158 Validation loss 0.04587452858686447 Accuracy 0.8892500400543213\n",
      "Iteration 79460 Training loss 0.0012677254853770137 Validation loss 0.04586511105298996 Accuracy 0.8895000219345093\n",
      "Iteration 79470 Training loss 0.0012605431256815791 Validation loss 0.04587966576218605 Accuracy 0.8891250491142273\n",
      "Iteration 79480 Training loss 0.0025124559178948402 Validation loss 0.045864421874284744 Accuracy 0.8893750309944153\n",
      "Iteration 79490 Training loss 0.0025105136446654797 Validation loss 0.04586893692612648 Accuracy 0.8891250491142273\n",
      "Iteration 79500 Training loss 0.002510765800252557 Validation loss 0.0459003746509552 Accuracy 0.8891250491142273\n",
      "Iteration 79510 Training loss 0.002511674305424094 Validation loss 0.04588675498962402 Accuracy 0.8892500400543213\n",
      "Iteration 79520 Training loss 0.001260492135770619 Validation loss 0.04589546471834183 Accuracy 0.8891250491142273\n",
      "Iteration 79530 Training loss 0.001261963858269155 Validation loss 0.04589558020234108 Accuracy 0.8890000581741333\n",
      "Iteration 79540 Training loss 8.017893378564622e-06 Validation loss 0.04588082432746887 Accuracy 0.8893750309944153\n",
      "Iteration 79550 Training loss 0.001267764950171113 Validation loss 0.04590057581663132 Accuracy 0.8892500400543213\n",
      "Iteration 79560 Training loss 1.1897488548129331e-05 Validation loss 0.04588739946484566 Accuracy 0.8891250491142273\n",
      "Iteration 79570 Training loss 1.009676452667918e-05 Validation loss 0.04586882144212723 Accuracy 0.8892500400543213\n",
      "Iteration 79580 Training loss 1.1953075954806991e-05 Validation loss 0.04585612565279007 Accuracy 0.8891250491142273\n",
      "Iteration 79590 Training loss 0.0012594402069225907 Validation loss 0.04588828608393669 Accuracy 0.8892500400543213\n",
      "Iteration 79600 Training loss 1.2652150871872436e-05 Validation loss 0.045889172703027725 Accuracy 0.8892500400543213\n",
      "Iteration 79610 Training loss 1.1077236194978468e-05 Validation loss 0.04590044170618057 Accuracy 0.8892500400543213\n",
      "Iteration 79620 Training loss 0.002512959763407707 Validation loss 0.045864287763834 Accuracy 0.8890000581741333\n",
      "Iteration 79630 Training loss 0.0025078654289245605 Validation loss 0.04589004069566727 Accuracy 0.8895000219345093\n",
      "Iteration 79640 Training loss 0.0025111015420407057 Validation loss 0.04586416855454445 Accuracy 0.8892500400543213\n",
      "Iteration 79650 Training loss 0.0025074821896851063 Validation loss 0.045882854610681534 Accuracy 0.8893750309944153\n",
      "Iteration 79660 Training loss 0.0012623989023268223 Validation loss 0.045849498361349106 Accuracy 0.8892500400543213\n",
      "Iteration 79670 Training loss 0.001260113436728716 Validation loss 0.04585615172982216 Accuracy 0.8892500400543213\n",
      "Iteration 79680 Training loss 8.404862455790862e-06 Validation loss 0.045874666422605515 Accuracy 0.8892500400543213\n",
      "Iteration 79690 Training loss 0.001259165583178401 Validation loss 0.04584797844290733 Accuracy 0.8892500400543213\n",
      "Iteration 79700 Training loss 0.0012622581562027335 Validation loss 0.04583698511123657 Accuracy 0.8892500400543213\n",
      "Iteration 79710 Training loss 0.0025072793941944838 Validation loss 0.045864351093769073 Accuracy 0.8892500400543213\n",
      "Iteration 79720 Training loss 0.001262057339772582 Validation loss 0.04584871232509613 Accuracy 0.8892500400543213\n",
      "Iteration 79730 Training loss 0.002517018234357238 Validation loss 0.04583989828824997 Accuracy 0.8893750309944153\n",
      "Iteration 79740 Training loss 1.038895788951777e-05 Validation loss 0.04586796835064888 Accuracy 0.8892500400543213\n",
      "Iteration 79750 Training loss 0.0037625436671078205 Validation loss 0.04585961997509003 Accuracy 0.8891250491142273\n",
      "Iteration 79760 Training loss 0.0012591441627591848 Validation loss 0.045850589871406555 Accuracy 0.8893750309944153\n",
      "Iteration 79770 Training loss 0.005009387154132128 Validation loss 0.04584158584475517 Accuracy 0.8892500400543213\n",
      "Iteration 79780 Training loss 0.002510697115212679 Validation loss 0.0458434633910656 Accuracy 0.8892500400543213\n",
      "Iteration 79790 Training loss 0.0012677472550421953 Validation loss 0.04582386091351509 Accuracy 0.8895000219345093\n",
      "Iteration 79800 Training loss 0.0012601773487403989 Validation loss 0.04586295410990715 Accuracy 0.8892500400543213\n",
      "Iteration 79810 Training loss 1.2403525943227578e-05 Validation loss 0.04583593085408211 Accuracy 0.8893750309944153\n",
      "Iteration 79820 Training loss 1.7242073226952925e-05 Validation loss 0.04584473744034767 Accuracy 0.8893750309944153\n",
      "Iteration 79830 Training loss 0.0025128417182713747 Validation loss 0.045853856950998306 Accuracy 0.8893750309944153\n",
      "Iteration 79840 Training loss 1.4516003830067348e-05 Validation loss 0.04584607109427452 Accuracy 0.8893750309944153\n",
      "Iteration 79850 Training loss 0.0012655945029109716 Validation loss 0.045815665274858475 Accuracy 0.8896250128746033\n",
      "Iteration 79860 Training loss 0.0012587038800120354 Validation loss 0.045844364911317825 Accuracy 0.8893750309944153\n",
      "Iteration 79870 Training loss 1.0646179362083785e-05 Validation loss 0.04584703966975212 Accuracy 0.8895000219345093\n",
      "Iteration 79880 Training loss 0.00375797669403255 Validation loss 0.04586395248770714 Accuracy 0.8895000219345093\n",
      "Iteration 79890 Training loss 0.002510149497538805 Validation loss 0.04585273191332817 Accuracy 0.8893750309944153\n",
      "Iteration 79900 Training loss 1.207713739859173e-05 Validation loss 0.04584990069270134 Accuracy 0.8893750309944153\n",
      "Iteration 79910 Training loss 1.2153040188422892e-05 Validation loss 0.04587278142571449 Accuracy 0.8891250491142273\n",
      "Iteration 79920 Training loss 0.0025103879161179066 Validation loss 0.04585280641913414 Accuracy 0.8890000581741333\n",
      "Iteration 79930 Training loss 1.1681812793540303e-05 Validation loss 0.04587291181087494 Accuracy 0.8893750309944153\n",
      "Iteration 79940 Training loss 1.383346625516424e-05 Validation loss 0.0458802729845047 Accuracy 0.8892500400543213\n",
      "Iteration 79950 Training loss 0.002511009108275175 Validation loss 0.04589018598198891 Accuracy 0.8891250491142273\n",
      "Iteration 79960 Training loss 0.0012596386950463057 Validation loss 0.04588698968291283 Accuracy 0.8891250491142273\n",
      "Iteration 79970 Training loss 0.00126416957937181 Validation loss 0.045854851603507996 Accuracy 0.8895000219345093\n",
      "Iteration 79980 Training loss 1.2169097317382693e-05 Validation loss 0.045858774334192276 Accuracy 0.8893750309944153\n",
      "Iteration 79990 Training loss 1.0434931027702987e-05 Validation loss 0.045838162302970886 Accuracy 0.8895000219345093\n",
      "Iteration 80000 Training loss 0.0025110221467912197 Validation loss 0.04582654684782028 Accuracy 0.8893750309944153\n",
      "Iteration 80010 Training loss 9.509641131444369e-06 Validation loss 0.04583238065242767 Accuracy 0.8893750309944153\n",
      "Iteration 80020 Training loss 9.904240869218484e-06 Validation loss 0.045860763639211655 Accuracy 0.8891250491142273\n",
      "Iteration 80030 Training loss 9.174691513180733e-06 Validation loss 0.04584061726927757 Accuracy 0.8892500400543213\n",
      "Iteration 80040 Training loss 0.0012641516514122486 Validation loss 0.045831650495529175 Accuracy 0.8892500400543213\n",
      "Iteration 80050 Training loss 0.001262357458472252 Validation loss 0.04586697742342949 Accuracy 0.8891250491142273\n",
      "Iteration 80060 Training loss 0.0012605441734194756 Validation loss 0.045839447528123856 Accuracy 0.8892500400543213\n",
      "Iteration 80070 Training loss 0.001260081771761179 Validation loss 0.0458468422293663 Accuracy 0.8893750309944153\n",
      "Iteration 80080 Training loss 0.0025138077326118946 Validation loss 0.04585130885243416 Accuracy 0.889750063419342\n",
      "Iteration 80090 Training loss 1.801600410544779e-05 Validation loss 0.0458473265171051 Accuracy 0.8893750309944153\n",
      "Iteration 80100 Training loss 0.0012591260019689798 Validation loss 0.04581589624285698 Accuracy 0.8896250128746033\n",
      "Iteration 80110 Training loss 0.0025115825701504946 Validation loss 0.04584088176488876 Accuracy 0.8895000219345093\n",
      "Iteration 80120 Training loss 0.0025108978152275085 Validation loss 0.0458340086042881 Accuracy 0.8893750309944153\n",
      "Iteration 80130 Training loss 7.642632226634305e-06 Validation loss 0.04581712558865547 Accuracy 0.8893750309944153\n",
      "Iteration 80140 Training loss 0.0037633059546351433 Validation loss 0.045834947377443314 Accuracy 0.8896250128746033\n",
      "Iteration 80150 Training loss 0.0012588814133778214 Validation loss 0.045854516327381134 Accuracy 0.8892500400543213\n",
      "Iteration 80160 Training loss 0.0025063776411116123 Validation loss 0.04583991691470146 Accuracy 0.8895000219345093\n",
      "Iteration 80170 Training loss 0.006262080743908882 Validation loss 0.04583682119846344 Accuracy 0.8892500400543213\n",
      "Iteration 80180 Training loss 0.0025095806922763586 Validation loss 0.04584662988781929 Accuracy 0.8892500400543213\n",
      "Iteration 80190 Training loss 9.107867299462669e-06 Validation loss 0.04587286710739136 Accuracy 0.8893750309944153\n",
      "Iteration 80200 Training loss 0.0025103441439568996 Validation loss 0.0458601675927639 Accuracy 0.8893750309944153\n",
      "Iteration 80210 Training loss 0.0012569715036079288 Validation loss 0.0458420030772686 Accuracy 0.8891250491142273\n",
      "Iteration 80220 Training loss 9.964890523406211e-06 Validation loss 0.04588482156395912 Accuracy 0.8892500400543213\n",
      "Iteration 80230 Training loss 1.1219406587770209e-05 Validation loss 0.04586265608668327 Accuracy 0.8890000581741333\n",
      "Iteration 80240 Training loss 0.0012626200914382935 Validation loss 0.04584376886487007 Accuracy 0.8891250491142273\n",
      "Iteration 80250 Training loss 1.3174311789043713e-05 Validation loss 0.04582921788096428 Accuracy 0.8890000581741333\n",
      "Iteration 80260 Training loss 1.2789100765076e-05 Validation loss 0.04584473744034767 Accuracy 0.8891250491142273\n",
      "Iteration 80270 Training loss 0.0012575782602652907 Validation loss 0.045855700969696045 Accuracy 0.8892500400543213\n",
      "Iteration 80280 Training loss 0.0012617465108633041 Validation loss 0.04584382846951485 Accuracy 0.8892500400543213\n",
      "Iteration 80290 Training loss 0.002510033082216978 Validation loss 0.04585189372301102 Accuracy 0.8893750309944153\n",
      "Iteration 80300 Training loss 0.0012606092495843768 Validation loss 0.04583007097244263 Accuracy 0.8895000219345093\n",
      "Iteration 80310 Training loss 0.0012607889948412776 Validation loss 0.04584492743015289 Accuracy 0.8888750672340393\n",
      "Iteration 80320 Training loss 0.002510285936295986 Validation loss 0.045812997967004776 Accuracy 0.8892500400543213\n",
      "Iteration 80330 Training loss 0.002508759731426835 Validation loss 0.04583250731229782 Accuracy 0.8890000581741333\n",
      "Iteration 80340 Training loss 0.0012627799296751618 Validation loss 0.0458575077354908 Accuracy 0.8890000581741333\n",
      "Iteration 80350 Training loss 0.005011383909732103 Validation loss 0.04586208984255791 Accuracy 0.8892500400543213\n",
      "Iteration 80360 Training loss 0.0012598151806741953 Validation loss 0.04586251452565193 Accuracy 0.8891250491142273\n",
      "Iteration 80370 Training loss 0.001259263837710023 Validation loss 0.04587358981370926 Accuracy 0.8888750672340393\n",
      "Iteration 80380 Training loss 0.0012619465123862028 Validation loss 0.045858386904001236 Accuracy 0.8891250491142273\n",
      "Iteration 80390 Training loss 0.0012594903819262981 Validation loss 0.04585196077823639 Accuracy 0.8888750672340393\n",
      "Iteration 80400 Training loss 0.0012623494258150458 Validation loss 0.045883264392614365 Accuracy 0.8886250257492065\n",
      "Iteration 80410 Training loss 1.0116363228007685e-05 Validation loss 0.04587291181087494 Accuracy 0.8890000581741333\n",
      "Iteration 80420 Training loss 8.257426088675857e-06 Validation loss 0.04588570445775986 Accuracy 0.8890000581741333\n",
      "Iteration 80430 Training loss 0.0037670338060706854 Validation loss 0.045841675251722336 Accuracy 0.8893750309944153\n",
      "Iteration 80440 Training loss 0.0050106649287045 Validation loss 0.04586122930049896 Accuracy 0.8890000581741333\n",
      "Iteration 80450 Training loss 0.0025166948325932026 Validation loss 0.04587147384881973 Accuracy 0.8891250491142273\n",
      "Iteration 80460 Training loss 0.0012601775815710425 Validation loss 0.04588303714990616 Accuracy 0.8891250491142273\n",
      "Iteration 80470 Training loss 0.0012569327373057604 Validation loss 0.045856017619371414 Accuracy 0.8890000581741333\n",
      "Iteration 80480 Training loss 0.0012601279886439443 Validation loss 0.045846618711948395 Accuracy 0.8890000581741333\n",
      "Iteration 80490 Training loss 1.1270401955698617e-05 Validation loss 0.04587022587656975 Accuracy 0.8892500400543213\n",
      "Iteration 80500 Training loss 0.0012617059983313084 Validation loss 0.04587304964661598 Accuracy 0.8891250491142273\n",
      "Iteration 80510 Training loss 0.0012614777078852057 Validation loss 0.04586128517985344 Accuracy 0.8888750672340393\n",
      "Iteration 80520 Training loss 1.0997751815011725e-05 Validation loss 0.045855503529310226 Accuracy 0.8888750672340393\n",
      "Iteration 80530 Training loss 0.0012577751185745 Validation loss 0.04586080089211464 Accuracy 0.8890000581741333\n",
      "Iteration 80540 Training loss 0.0012677870690822601 Validation loss 0.04586559906601906 Accuracy 0.8893750309944153\n",
      "Iteration 80550 Training loss 0.00251170271076262 Validation loss 0.04584876820445061 Accuracy 0.8888750672340393\n",
      "Iteration 80560 Training loss 0.005007259547710419 Validation loss 0.04584326595067978 Accuracy 0.8891250491142273\n",
      "Iteration 80570 Training loss 8.768449333729222e-06 Validation loss 0.045831628143787384 Accuracy 0.8887500166893005\n",
      "Iteration 80580 Training loss 0.0012630665441974998 Validation loss 0.045875128358602524 Accuracy 0.8890000581741333\n",
      "Iteration 80590 Training loss 0.0012598232133314013 Validation loss 0.045842431485652924 Accuracy 0.8893750309944153\n",
      "Iteration 80600 Training loss 0.0012625488452613354 Validation loss 0.04584937542676926 Accuracy 0.8890000581741333\n",
      "Iteration 80610 Training loss 8.718220669834409e-06 Validation loss 0.04582737386226654 Accuracy 0.8887500166893005\n",
      "Iteration 80620 Training loss 1.1121802344860043e-05 Validation loss 0.04585365578532219 Accuracy 0.8892500400543213\n",
      "Iteration 80630 Training loss 0.006260615307837725 Validation loss 0.04586033150553703 Accuracy 0.8895000219345093\n",
      "Iteration 80640 Training loss 1.4025798918737564e-05 Validation loss 0.04583541303873062 Accuracy 0.8896250128746033\n",
      "Iteration 80650 Training loss 0.0012645120732486248 Validation loss 0.045848291367292404 Accuracy 0.8895000219345093\n",
      "Iteration 80660 Training loss 0.0012629994889721274 Validation loss 0.04586215689778328 Accuracy 0.8891250491142273\n",
      "Iteration 80670 Training loss 0.0012579649919643998 Validation loss 0.045855455100536346 Accuracy 0.8892500400543213\n",
      "Iteration 80680 Training loss 1.2770348803314846e-05 Validation loss 0.045851804316043854 Accuracy 0.8890000581741333\n",
      "Iteration 80690 Training loss 0.002509764861315489 Validation loss 0.04583429917693138 Accuracy 0.8895000219345093\n",
      "Iteration 80700 Training loss 0.001263669691979885 Validation loss 0.04584825783967972 Accuracy 0.8888750672340393\n",
      "Iteration 80710 Training loss 0.0012611993588507175 Validation loss 0.04586164280772209 Accuracy 0.8890000581741333\n",
      "Iteration 80720 Training loss 0.0037616079207509756 Validation loss 0.04583568498492241 Accuracy 0.8892500400543213\n",
      "Iteration 80730 Training loss 0.00501232361420989 Validation loss 0.045835912227630615 Accuracy 0.8890000581741333\n",
      "Iteration 80740 Training loss 0.0012622593203559518 Validation loss 0.04582454264163971 Accuracy 0.8895000219345093\n",
      "Iteration 80750 Training loss 8.909295502235182e-06 Validation loss 0.045864034444093704 Accuracy 0.8891250491142273\n",
      "Iteration 80760 Training loss 0.0025131069123744965 Validation loss 0.04585118219256401 Accuracy 0.8891250491142273\n",
      "Iteration 80770 Training loss 0.002512517152354121 Validation loss 0.04588260501623154 Accuracy 0.8891250491142273\n",
      "Iteration 80780 Training loss 0.002509976038709283 Validation loss 0.04584995284676552 Accuracy 0.8893750309944153\n",
      "Iteration 80790 Training loss 0.00251154531724751 Validation loss 0.04584353044629097 Accuracy 0.8893750309944153\n",
      "Iteration 80800 Training loss 0.002511611208319664 Validation loss 0.045851293951272964 Accuracy 0.8892500400543213\n",
      "Iteration 80810 Training loss 0.002514263615012169 Validation loss 0.04584888741374016 Accuracy 0.8896250128746033\n",
      "Iteration 80820 Training loss 1.05324061223655e-05 Validation loss 0.04582122340798378 Accuracy 0.8892500400543213\n",
      "Iteration 80830 Training loss 0.0012651069555431604 Validation loss 0.04586071893572807 Accuracy 0.8890000581741333\n",
      "Iteration 80840 Training loss 0.001260039978660643 Validation loss 0.04588121548295021 Accuracy 0.8893750309944153\n",
      "Iteration 80850 Training loss 1.3518550076696556e-05 Validation loss 0.04587504640221596 Accuracy 0.8888750672340393\n",
      "Iteration 80860 Training loss 1.8814715076587163e-05 Validation loss 0.04587257280945778 Accuracy 0.8892500400543213\n",
      "Iteration 80870 Training loss 0.0025118491612374783 Validation loss 0.04587561637163162 Accuracy 0.8891250491142273\n",
      "Iteration 80880 Training loss 0.0037617869675159454 Validation loss 0.045880820602178574 Accuracy 0.8892500400543213\n",
      "Iteration 80890 Training loss 0.0025147453416138887 Validation loss 0.04589146003127098 Accuracy 0.8892500400543213\n",
      "Iteration 80900 Training loss 0.002509784884750843 Validation loss 0.04587920010089874 Accuracy 0.8890000581741333\n",
      "Iteration 80910 Training loss 5.772173153673066e-06 Validation loss 0.045886702835559845 Accuracy 0.8896250128746033\n",
      "Iteration 80920 Training loss 9.668203347246163e-06 Validation loss 0.045885469764471054 Accuracy 0.8892500400543213\n",
      "Iteration 80930 Training loss 0.002509472891688347 Validation loss 0.04587149992585182 Accuracy 0.8890000581741333\n",
      "Iteration 80940 Training loss 0.0012614340521395206 Validation loss 0.045875705778598785 Accuracy 0.8891250491142273\n",
      "Iteration 80950 Training loss 1.1848211215692572e-05 Validation loss 0.0459083653986454 Accuracy 0.8890000581741333\n",
      "Iteration 80960 Training loss 7.929907951620407e-06 Validation loss 0.04590195044875145 Accuracy 0.8892500400543213\n",
      "Iteration 80970 Training loss 6.370631126628723e-06 Validation loss 0.0458960197865963 Accuracy 0.8890000581741333\n",
      "Iteration 80980 Training loss 1.5118979717954062e-05 Validation loss 0.04589199274778366 Accuracy 0.8892500400543213\n",
      "Iteration 80990 Training loss 9.73061196418712e-06 Validation loss 0.04587516188621521 Accuracy 0.8891250491142273\n",
      "Iteration 81000 Training loss 0.0037609292194247246 Validation loss 0.04587717726826668 Accuracy 0.8888750672340393\n",
      "Iteration 81010 Training loss 0.0012595879379659891 Validation loss 0.04588507115840912 Accuracy 0.8891250491142273\n",
      "Iteration 81020 Training loss 8.445095772913191e-06 Validation loss 0.045882489532232285 Accuracy 0.8895000219345093\n",
      "Iteration 81030 Training loss 0.0025094475131481886 Validation loss 0.045887261629104614 Accuracy 0.8892500400543213\n",
      "Iteration 81040 Training loss 1.0680199920898303e-05 Validation loss 0.045868150889873505 Accuracy 0.8888750672340393\n",
      "Iteration 81050 Training loss 6.672172730759485e-06 Validation loss 0.045889582484960556 Accuracy 0.8892500400543213\n",
      "Iteration 81060 Training loss 0.002511384664103389 Validation loss 0.045869845896959305 Accuracy 0.8890000581741333\n",
      "Iteration 81070 Training loss 1.076347143680323e-05 Validation loss 0.04586246982216835 Accuracy 0.8890000581741333\n",
      "Iteration 81080 Training loss 0.003759115468710661 Validation loss 0.04586966335773468 Accuracy 0.8892500400543213\n",
      "Iteration 81090 Training loss 0.0012604829389601946 Validation loss 0.04586789757013321 Accuracy 0.8891250491142273\n",
      "Iteration 81100 Training loss 7.741952686046716e-06 Validation loss 0.04589855298399925 Accuracy 0.8891250491142273\n",
      "Iteration 81110 Training loss 0.0012620975030586123 Validation loss 0.04590493440628052 Accuracy 0.8891250491142273\n",
      "Iteration 81120 Training loss 0.0012615494197234511 Validation loss 0.045908838510513306 Accuracy 0.8891250491142273\n",
      "Iteration 81130 Training loss 1.0203418241871987e-05 Validation loss 0.045911844819784164 Accuracy 0.8896250128746033\n",
      "Iteration 81140 Training loss 0.0012605987722054124 Validation loss 0.0458679161965847 Accuracy 0.8891250491142273\n",
      "Iteration 81150 Training loss 1.065247579390416e-05 Validation loss 0.04587909206748009 Accuracy 0.8891250491142273\n",
      "Iteration 81160 Training loss 0.0025096130557358265 Validation loss 0.04589052125811577 Accuracy 0.8895000219345093\n",
      "Iteration 81170 Training loss 0.0012608052929863334 Validation loss 0.045886844396591187 Accuracy 0.8893750309944153\n",
      "Iteration 81180 Training loss 0.002508819103240967 Validation loss 0.04590722918510437 Accuracy 0.8891250491142273\n",
      "Iteration 81190 Training loss 2.2079666450736113e-05 Validation loss 0.045892134308815 Accuracy 0.8892500400543213\n",
      "Iteration 81200 Training loss 0.0012644624803215265 Validation loss 0.04588218778371811 Accuracy 0.8891250491142273\n",
      "Iteration 81210 Training loss 0.0037592898588627577 Validation loss 0.04588455706834793 Accuracy 0.8893750309944153\n",
      "Iteration 81220 Training loss 0.0012584637152031064 Validation loss 0.04586689919233322 Accuracy 0.8893750309944153\n",
      "Iteration 81230 Training loss 1.115802297135815e-05 Validation loss 0.0458819754421711 Accuracy 0.8890000581741333\n",
      "Iteration 81240 Training loss 0.001260212971828878 Validation loss 0.04588474705815315 Accuracy 0.8891250491142273\n",
      "Iteration 81250 Training loss 0.0025103650987148285 Validation loss 0.04589611291885376 Accuracy 0.8893750309944153\n",
      "Iteration 81260 Training loss 1.0941977961920202e-05 Validation loss 0.045899923890829086 Accuracy 0.8888750672340393\n",
      "Iteration 81270 Training loss 0.0025119888596236706 Validation loss 0.045907363295555115 Accuracy 0.8888750672340393\n",
      "Iteration 81280 Training loss 0.0025122296065092087 Validation loss 0.04588736593723297 Accuracy 0.8888750672340393\n",
      "Iteration 81290 Training loss 1.1143400115543045e-05 Validation loss 0.04591502621769905 Accuracy 0.8888750672340393\n",
      "Iteration 81300 Training loss 0.0012593563878908753 Validation loss 0.04589419811964035 Accuracy 0.8885000348091125\n",
      "Iteration 81310 Training loss 0.0012611119309440255 Validation loss 0.04588515684008598 Accuracy 0.8888750672340393\n",
      "Iteration 81320 Training loss 1.3299772945174482e-05 Validation loss 0.045886654406785965 Accuracy 0.8888750672340393\n",
      "Iteration 81330 Training loss 0.0012597331078723073 Validation loss 0.04588434845209122 Accuracy 0.8888750672340393\n",
      "Iteration 81340 Training loss 0.005010971799492836 Validation loss 0.04588282108306885 Accuracy 0.8888750672340393\n",
      "Iteration 81350 Training loss 0.002509164856746793 Validation loss 0.045891404151916504 Accuracy 0.8888750672340393\n",
      "Iteration 81360 Training loss 0.0012585797812789679 Validation loss 0.04589004069566727 Accuracy 0.8888750672340393\n",
      "Iteration 81370 Training loss 1.182040068670176e-05 Validation loss 0.04587545990943909 Accuracy 0.8891250491142273\n",
      "Iteration 81380 Training loss 0.003760106395930052 Validation loss 0.04586319997906685 Accuracy 0.8890000581741333\n",
      "Iteration 81390 Training loss 0.0025108358822762966 Validation loss 0.04586924612522125 Accuracy 0.8890000581741333\n",
      "Iteration 81400 Training loss 1.1143611118313856e-05 Validation loss 0.045890647917985916 Accuracy 0.8891250491142273\n",
      "Iteration 81410 Training loss 8.705680556886364e-06 Validation loss 0.04588726535439491 Accuracy 0.8888750672340393\n",
      "Iteration 81420 Training loss 0.0025107944384217262 Validation loss 0.04588506743311882 Accuracy 0.8890000581741333\n",
      "Iteration 81430 Training loss 1.1111538697150536e-05 Validation loss 0.04587115719914436 Accuracy 0.8887500166893005\n",
      "Iteration 81440 Training loss 0.0012598694302141666 Validation loss 0.04587323218584061 Accuracy 0.8890000581741333\n",
      "Iteration 81450 Training loss 0.0012615008745342493 Validation loss 0.04587818309664726 Accuracy 0.8888750672340393\n",
      "Iteration 81460 Training loss 8.967953363026027e-06 Validation loss 0.045882221311330795 Accuracy 0.8891250491142273\n",
      "Iteration 81470 Training loss 0.002511936705559492 Validation loss 0.04588085412979126 Accuracy 0.8887500166893005\n",
      "Iteration 81480 Training loss 0.002510773716494441 Validation loss 0.045866917818784714 Accuracy 0.8888750672340393\n",
      "Iteration 81490 Training loss 1.3724989003094379e-05 Validation loss 0.045879434794187546 Accuracy 0.8891250491142273\n",
      "Iteration 81500 Training loss 1.304486704611918e-05 Validation loss 0.045884791761636734 Accuracy 0.8890000581741333\n",
      "Iteration 81510 Training loss 0.0012573364656418562 Validation loss 0.04589307680726051 Accuracy 0.8891250491142273\n",
      "Iteration 81520 Training loss 0.0012585881631821394 Validation loss 0.045916054397821426 Accuracy 0.8891250491142273\n",
      "Iteration 81530 Training loss 6.917571681697154e-06 Validation loss 0.045879557728767395 Accuracy 0.8891250491142273\n",
      "Iteration 81540 Training loss 0.0012629278935492039 Validation loss 0.045880649238824844 Accuracy 0.8892500400543213\n",
      "Iteration 81550 Training loss 0.0037601410876959562 Validation loss 0.04587865248322487 Accuracy 0.8890000581741333\n",
      "Iteration 81560 Training loss 8.76489320944529e-06 Validation loss 0.04590345174074173 Accuracy 0.8888750672340393\n",
      "Iteration 81570 Training loss 0.0025115511380136013 Validation loss 0.045919548720121384 Accuracy 0.8888750672340393\n",
      "Iteration 81580 Training loss 1.174373483081581e-05 Validation loss 0.045887503772974014 Accuracy 0.8891250491142273\n",
      "Iteration 81590 Training loss 0.002511103404685855 Validation loss 0.045907434076070786 Accuracy 0.8892500400543213\n",
      "Iteration 81600 Training loss 0.00125969760119915 Validation loss 0.04587062448263168 Accuracy 0.8888750672340393\n",
      "Iteration 81610 Training loss 1.0925051356025506e-05 Validation loss 0.045870136469602585 Accuracy 0.8890000581741333\n",
      "Iteration 81620 Training loss 0.002511517843231559 Validation loss 0.04588247835636139 Accuracy 0.8892500400543213\n",
      "Iteration 81630 Training loss 6.708643468300579e-06 Validation loss 0.0458647720515728 Accuracy 0.8890000581741333\n",
      "Iteration 81640 Training loss 0.0037595676258206367 Validation loss 0.04591379687190056 Accuracy 0.8892500400543213\n",
      "Iteration 81650 Training loss 0.0025109366979449987 Validation loss 0.045885395258665085 Accuracy 0.8893750309944153\n",
      "Iteration 81660 Training loss 0.0025084519293159246 Validation loss 0.045901890844106674 Accuracy 0.8891250491142273\n",
      "Iteration 81670 Training loss 0.001260895747691393 Validation loss 0.04591810330748558 Accuracy 0.8892500400543213\n",
      "Iteration 81680 Training loss 6.451225999626331e-06 Validation loss 0.0458841435611248 Accuracy 0.8891250491142273\n",
      "Iteration 81690 Training loss 1.0382977961853612e-05 Validation loss 0.04591156169772148 Accuracy 0.8888750672340393\n",
      "Iteration 81700 Training loss 9.210254575009458e-06 Validation loss 0.0459192618727684 Accuracy 0.8890000581741333\n",
      "Iteration 81710 Training loss 0.0025116759352385998 Validation loss 0.04590587690472603 Accuracy 0.8892500400543213\n",
      "Iteration 81720 Training loss 0.0012618968030437827 Validation loss 0.045879509299993515 Accuracy 0.8892500400543213\n",
      "Iteration 81730 Training loss 0.002514682011678815 Validation loss 0.045910004526376724 Accuracy 0.8891250491142273\n",
      "Iteration 81740 Training loss 9.058085197466426e-06 Validation loss 0.04590252786874771 Accuracy 0.8891250491142273\n",
      "Iteration 81750 Training loss 0.001263425569050014 Validation loss 0.045912619680166245 Accuracy 0.8891250491142273\n",
      "Iteration 81760 Training loss 0.003756869351491332 Validation loss 0.04590647295117378 Accuracy 0.8891250491142273\n",
      "Iteration 81770 Training loss 1.5272749806172214e-05 Validation loss 0.04591211676597595 Accuracy 0.8891250491142273\n",
      "Iteration 81780 Training loss 0.0012600293848663568 Validation loss 0.04589030519127846 Accuracy 0.8890000581741333\n",
      "Iteration 81790 Training loss 0.0025094368029385805 Validation loss 0.04589705169200897 Accuracy 0.8891250491142273\n",
      "Iteration 81800 Training loss 0.003763726679608226 Validation loss 0.045880675315856934 Accuracy 0.8890000581741333\n",
      "Iteration 81810 Training loss 8.478037671011407e-06 Validation loss 0.04587726667523384 Accuracy 0.8890000581741333\n",
      "Iteration 81820 Training loss 0.0037613885942846537 Validation loss 0.04592091962695122 Accuracy 0.8891250491142273\n",
      "Iteration 81830 Training loss 0.0012575319269672036 Validation loss 0.0459025613963604 Accuracy 0.8891250491142273\n",
      "Iteration 81840 Training loss 0.002512738574296236 Validation loss 0.04588409140706062 Accuracy 0.8892500400543213\n",
      "Iteration 81850 Training loss 0.0012595393927767873 Validation loss 0.04587449133396149 Accuracy 0.8892500400543213\n",
      "Iteration 81860 Training loss 0.0012606224045157433 Validation loss 0.04588713124394417 Accuracy 0.8892500400543213\n",
      "Iteration 81870 Training loss 0.0050104279071092606 Validation loss 0.045893434435129166 Accuracy 0.8891250491142273\n",
      "Iteration 81880 Training loss 0.006257790606468916 Validation loss 0.04591153934597969 Accuracy 0.8892500400543213\n",
      "Iteration 81890 Training loss 0.0012587688397616148 Validation loss 0.04591755196452141 Accuracy 0.8895000219345093\n",
      "Iteration 81900 Training loss 1.2689324648818001e-05 Validation loss 0.045889079570770264 Accuracy 0.8892500400543213\n",
      "Iteration 81910 Training loss 6.434395345422672e-06 Validation loss 0.04593487083911896 Accuracy 0.8892500400543213\n",
      "Iteration 81920 Training loss 0.0012591031845659018 Validation loss 0.04591233283281326 Accuracy 0.8892500400543213\n",
      "Iteration 81930 Training loss 9.288095498050097e-06 Validation loss 0.045891664922237396 Accuracy 0.8890000581741333\n",
      "Iteration 81940 Training loss 0.0012618571054190397 Validation loss 0.04590379446744919 Accuracy 0.8892500400543213\n",
      "Iteration 81950 Training loss 0.0025134494062513113 Validation loss 0.045891497284173965 Accuracy 0.8892500400543213\n",
      "Iteration 81960 Training loss 1.6356991181964986e-05 Validation loss 0.0458998903632164 Accuracy 0.8895000219345093\n",
      "Iteration 81970 Training loss 0.0012590985279530287 Validation loss 0.04589337483048439 Accuracy 0.8893750309944153\n",
      "Iteration 81980 Training loss 0.0025138440541923046 Validation loss 0.04591633379459381 Accuracy 0.8893750309944153\n",
      "Iteration 81990 Training loss 9.583777682564687e-06 Validation loss 0.04591314122080803 Accuracy 0.8895000219345093\n",
      "Iteration 82000 Training loss 0.003762726439163089 Validation loss 0.04591638967394829 Accuracy 0.8893750309944153\n",
      "Iteration 82010 Training loss 0.0012562350602820516 Validation loss 0.04590059444308281 Accuracy 0.8895000219345093\n",
      "Iteration 82020 Training loss 0.0012635558377951384 Validation loss 0.04591071605682373 Accuracy 0.8892500400543213\n",
      "Iteration 82030 Training loss 1.4763541912543587e-05 Validation loss 0.04589689522981644 Accuracy 0.8892500400543213\n",
      "Iteration 82040 Training loss 0.0025114407762885094 Validation loss 0.045881081372499466 Accuracy 0.8891250491142273\n",
      "Iteration 82050 Training loss 0.00251144845969975 Validation loss 0.04590684548020363 Accuracy 0.8892500400543213\n",
      "Iteration 82060 Training loss 1.0223281606158707e-05 Validation loss 0.0458969920873642 Accuracy 0.8892500400543213\n",
      "Iteration 82070 Training loss 0.0037642037495970726 Validation loss 0.04590070992708206 Accuracy 0.8892500400543213\n",
      "Iteration 82080 Training loss 1.0841032235475723e-05 Validation loss 0.045898452401161194 Accuracy 0.8892500400543213\n",
      "Iteration 82090 Training loss 1.2179310033388902e-05 Validation loss 0.04590211808681488 Accuracy 0.8892500400543213\n",
      "Iteration 82100 Training loss 0.0012599793262779713 Validation loss 0.04589961841702461 Accuracy 0.8891250491142273\n",
      "Iteration 82110 Training loss 0.001259003533050418 Validation loss 0.04591003805398941 Accuracy 0.8890000581741333\n",
      "Iteration 82120 Training loss 0.0050087277777493 Validation loss 0.04589030519127846 Accuracy 0.8891250491142273\n",
      "Iteration 82130 Training loss 1.2243491255503614e-05 Validation loss 0.04589168727397919 Accuracy 0.8891250491142273\n",
      "Iteration 82140 Training loss 1.709861862764228e-05 Validation loss 0.045906443148851395 Accuracy 0.8891250491142273\n",
      "Iteration 82150 Training loss 0.001261819852516055 Validation loss 0.045900098979473114 Accuracy 0.8891250491142273\n",
      "Iteration 82160 Training loss 1.0470335837453604e-05 Validation loss 0.04589414596557617 Accuracy 0.8892500400543213\n",
      "Iteration 82170 Training loss 0.0012618708424270153 Validation loss 0.04586615785956383 Accuracy 0.8892500400543213\n",
      "Iteration 82180 Training loss 9.569986104907002e-06 Validation loss 0.04588370397686958 Accuracy 0.8892500400543213\n",
      "Iteration 82190 Training loss 0.0025093494914472103 Validation loss 0.045867715030908585 Accuracy 0.8891250491142273\n",
      "Iteration 82200 Training loss 0.0025080740451812744 Validation loss 0.04589162766933441 Accuracy 0.8891250491142273\n",
      "Iteration 82210 Training loss 0.0012594906147569418 Validation loss 0.04588479548692703 Accuracy 0.8895000219345093\n",
      "Iteration 82220 Training loss 0.0037606200203299522 Validation loss 0.04589132219552994 Accuracy 0.8892500400543213\n",
      "Iteration 82230 Training loss 0.001261095516383648 Validation loss 0.04588116705417633 Accuracy 0.8892500400543213\n",
      "Iteration 82240 Training loss 9.495783160673454e-06 Validation loss 0.04587191343307495 Accuracy 0.8895000219345093\n",
      "Iteration 82250 Training loss 0.001258412143215537 Validation loss 0.045898403972387314 Accuracy 0.8890000581741333\n",
      "Iteration 82260 Training loss 0.002511857310310006 Validation loss 0.045895226299762726 Accuracy 0.8895000219345093\n",
      "Iteration 82270 Training loss 1.2733104085782543e-05 Validation loss 0.04589683935046196 Accuracy 0.8891250491142273\n",
      "Iteration 82280 Training loss 9.833121112023946e-06 Validation loss 0.04591033235192299 Accuracy 0.8893750309944153\n",
      "Iteration 82290 Training loss 0.0012653818121179938 Validation loss 0.04590792953968048 Accuracy 0.8893750309944153\n",
      "Iteration 82300 Training loss 0.0037583729717880487 Validation loss 0.045920196920633316 Accuracy 0.8896250128746033\n",
      "Iteration 82310 Training loss 0.002508967649191618 Validation loss 0.04592480883002281 Accuracy 0.889750063419342\n",
      "Iteration 82320 Training loss 0.0025096845347434282 Validation loss 0.04591480270028114 Accuracy 0.8892500400543213\n",
      "Iteration 82330 Training loss 0.0012643731897696853 Validation loss 0.045898448675870895 Accuracy 0.8892500400543213\n",
      "Iteration 82340 Training loss 0.0012647908879444003 Validation loss 0.04590030387043953 Accuracy 0.8890000581741333\n",
      "Iteration 82350 Training loss 0.0012583317002281547 Validation loss 0.04589250311255455 Accuracy 0.8888750672340393\n",
      "Iteration 82360 Training loss 0.002509513171389699 Validation loss 0.04591541364789009 Accuracy 0.8890000581741333\n",
      "Iteration 82370 Training loss 0.0025094537995755672 Validation loss 0.04593052715063095 Accuracy 0.8890000581741333\n",
      "Iteration 82380 Training loss 8.61143234942574e-06 Validation loss 0.045924436300992966 Accuracy 0.8890000581741333\n",
      "Iteration 82390 Training loss 0.0062587508000433445 Validation loss 0.0459393672645092 Accuracy 0.8888750672340393\n",
      "Iteration 82400 Training loss 9.199729902320541e-06 Validation loss 0.04591180011630058 Accuracy 0.8886250257492065\n",
      "Iteration 82410 Training loss 0.0012583860661834478 Validation loss 0.045915406197309494 Accuracy 0.8887500166893005\n",
      "Iteration 82420 Training loss 0.003758711740374565 Validation loss 0.04591423273086548 Accuracy 0.8890000581741333\n",
      "Iteration 82430 Training loss 0.00501126516610384 Validation loss 0.04588694870471954 Accuracy 0.8888750672340393\n",
      "Iteration 82440 Training loss 0.001261083292774856 Validation loss 0.04588810354471207 Accuracy 0.8888750672340393\n",
      "Iteration 82450 Training loss 0.0012589144753292203 Validation loss 0.04592334106564522 Accuracy 0.8887500166893005\n",
      "Iteration 82460 Training loss 9.480544576945249e-06 Validation loss 0.04592648521065712 Accuracy 0.8887500166893005\n",
      "Iteration 82470 Training loss 0.00251270760782063 Validation loss 0.04593029245734215 Accuracy 0.8886250257492065\n",
      "Iteration 82480 Training loss 1.1421534509281628e-05 Validation loss 0.04592520743608475 Accuracy 0.8890000581741333\n",
      "Iteration 82490 Training loss 0.0012583902571350336 Validation loss 0.045907072722911835 Accuracy 0.8887500166893005\n",
      "Iteration 82500 Training loss 1.0566710443526972e-05 Validation loss 0.045922067016363144 Accuracy 0.8887500166893005\n",
      "Iteration 82510 Training loss 0.0012634003069251776 Validation loss 0.045952264219522476 Accuracy 0.8890000581741333\n",
      "Iteration 82520 Training loss 0.00126171566080302 Validation loss 0.04594578966498375 Accuracy 0.8890000581741333\n",
      "Iteration 82530 Training loss 1.803261329769157e-05 Validation loss 0.04591180756688118 Accuracy 0.8888750672340393\n",
      "Iteration 82540 Training loss 1.0979936632793397e-05 Validation loss 0.045891329646110535 Accuracy 0.8891250491142273\n",
      "Iteration 82550 Training loss 0.0037567235995084047 Validation loss 0.04592422768473625 Accuracy 0.8891250491142273\n",
      "Iteration 82560 Training loss 1.0739346180344e-05 Validation loss 0.04594694823026657 Accuracy 0.8890000581741333\n",
      "Iteration 82570 Training loss 0.0012579674366861582 Validation loss 0.04593611881136894 Accuracy 0.8891250491142273\n",
      "Iteration 82580 Training loss 1.3080010830890387e-05 Validation loss 0.04591517522931099 Accuracy 0.8892500400543213\n",
      "Iteration 82590 Training loss 1.1159246241732035e-05 Validation loss 0.04591825231909752 Accuracy 0.8891250491142273\n",
      "Iteration 82600 Training loss 0.003756801364943385 Validation loss 0.04590076580643654 Accuracy 0.8892500400543213\n",
      "Iteration 82610 Training loss 0.0012587676756083965 Validation loss 0.04588326811790466 Accuracy 0.8887500166893005\n",
      "Iteration 82620 Training loss 0.0025112717412412167 Validation loss 0.045921679586172104 Accuracy 0.8886250257492065\n",
      "Iteration 82630 Training loss 1.25682581710862e-05 Validation loss 0.04591323807835579 Accuracy 0.8892500400543213\n",
      "Iteration 82640 Training loss 1.3715630302613135e-05 Validation loss 0.045912183821201324 Accuracy 0.8886250257492065\n",
      "Iteration 82650 Training loss 0.0012585080694407225 Validation loss 0.045881226658821106 Accuracy 0.8893750309944153\n",
      "Iteration 82660 Training loss 0.003760478924959898 Validation loss 0.045895762741565704 Accuracy 0.8888750672340393\n",
      "Iteration 82670 Training loss 0.0012578890891745687 Validation loss 0.04586851969361305 Accuracy 0.8892500400543213\n",
      "Iteration 82680 Training loss 0.003760591149330139 Validation loss 0.045905951410532 Accuracy 0.8890000581741333\n",
      "Iteration 82690 Training loss 1.321329727943521e-05 Validation loss 0.0459246002137661 Accuracy 0.8893750309944153\n",
      "Iteration 82700 Training loss 0.0012608577962964773 Validation loss 0.04589860513806343 Accuracy 0.8890000581741333\n",
      "Iteration 82710 Training loss 0.0012623151997104287 Validation loss 0.045906033366918564 Accuracy 0.8891250491142273\n",
      "Iteration 82720 Training loss 1.4975556041463278e-05 Validation loss 0.04590505361557007 Accuracy 0.8891250491142273\n",
      "Iteration 82730 Training loss 0.0012612243881449103 Validation loss 0.04591688513755798 Accuracy 0.8893750309944153\n",
      "Iteration 82740 Training loss 1.0090586329170037e-05 Validation loss 0.045897457748651505 Accuracy 0.8890000581741333\n",
      "Iteration 82750 Training loss 9.528500413580332e-06 Validation loss 0.04590817540884018 Accuracy 0.8893750309944153\n",
      "Iteration 82760 Training loss 0.0062591638416051865 Validation loss 0.045888423919677734 Accuracy 0.8892500400543213\n",
      "Iteration 82770 Training loss 0.0012604818912222981 Validation loss 0.045893628150224686 Accuracy 0.8891250491142273\n",
      "Iteration 82780 Training loss 0.0012595693115144968 Validation loss 0.04589381441473961 Accuracy 0.8890000581741333\n",
      "Iteration 82790 Training loss 0.002510033082216978 Validation loss 0.045929063111543655 Accuracy 0.8891250491142273\n",
      "Iteration 82800 Training loss 0.0012624089140444994 Validation loss 0.045904673635959625 Accuracy 0.8891250491142273\n",
      "Iteration 82810 Training loss 1.511986101832008e-05 Validation loss 0.045884743332862854 Accuracy 0.8892500400543213\n",
      "Iteration 82820 Training loss 0.0012596898013725877 Validation loss 0.04586638882756233 Accuracy 0.8888750672340393\n",
      "Iteration 82830 Training loss 0.002511034021154046 Validation loss 0.0459105409681797 Accuracy 0.8893750309944153\n",
      "Iteration 82840 Training loss 0.0012619112385436893 Validation loss 0.045910485088825226 Accuracy 0.8891250491142273\n",
      "Iteration 82850 Training loss 9.711576240079012e-06 Validation loss 0.04587573558092117 Accuracy 0.8892500400543213\n",
      "Iteration 82860 Training loss 0.0012607760727405548 Validation loss 0.04595044255256653 Accuracy 0.8887500166893005\n",
      "Iteration 82870 Training loss 0.0025103515945374966 Validation loss 0.04592088237404823 Accuracy 0.8891250491142273\n",
      "Iteration 82880 Training loss 7.903067853476387e-06 Validation loss 0.04592674598097801 Accuracy 0.8893750309944153\n",
      "Iteration 82890 Training loss 0.0025115006137639284 Validation loss 0.04590409994125366 Accuracy 0.8893750309944153\n",
      "Iteration 82900 Training loss 0.003761656815186143 Validation loss 0.04590015113353729 Accuracy 0.8896250128746033\n",
      "Iteration 82910 Training loss 9.374368346470874e-06 Validation loss 0.04591863602399826 Accuracy 0.8891250491142273\n",
      "Iteration 82920 Training loss 9.633165973355062e-06 Validation loss 0.0459110401570797 Accuracy 0.8895000219345093\n",
      "Iteration 82930 Training loss 0.0012500535231083632 Validation loss 0.04592213034629822 Accuracy 0.8892500400543213\n",
      "Iteration 82940 Training loss 0.0012621731730177999 Validation loss 0.04589591547846794 Accuracy 0.8890000581741333\n",
      "Iteration 82950 Training loss 0.0012605440570041537 Validation loss 0.04588454216718674 Accuracy 0.8892500400543213\n",
      "Iteration 82960 Training loss 0.0025112261064350605 Validation loss 0.045891739428043365 Accuracy 0.8892500400543213\n",
      "Iteration 82970 Training loss 1.1169751815032214e-05 Validation loss 0.04589517414569855 Accuracy 0.8891250491142273\n",
      "Iteration 82980 Training loss 1.6257674360531382e-05 Validation loss 0.04588477686047554 Accuracy 0.8891250491142273\n",
      "Iteration 82990 Training loss 1.4931140867702197e-05 Validation loss 0.045915063470602036 Accuracy 0.8892500400543213\n",
      "Iteration 83000 Training loss 9.205443348037079e-06 Validation loss 0.04591388255357742 Accuracy 0.8892500400543213\n",
      "Iteration 83010 Training loss 0.003761814208701253 Validation loss 0.0459257997572422 Accuracy 0.8893750309944153\n",
      "Iteration 83020 Training loss 0.002514983993023634 Validation loss 0.045949600636959076 Accuracy 0.8892500400543213\n",
      "Iteration 83030 Training loss 0.003761210711672902 Validation loss 0.04594440013170242 Accuracy 0.8893750309944153\n",
      "Iteration 83040 Training loss 1.152754975919379e-05 Validation loss 0.04589327797293663 Accuracy 0.8892500400543213\n",
      "Iteration 83050 Training loss 1.1409225407987833e-05 Validation loss 0.04592778906226158 Accuracy 0.8893750309944153\n",
      "Iteration 83060 Training loss 0.0012625327799469233 Validation loss 0.04588437080383301 Accuracy 0.8893750309944153\n",
      "Iteration 83070 Training loss 8.803309356153477e-06 Validation loss 0.045907747000455856 Accuracy 0.8893750309944153\n",
      "Iteration 83080 Training loss 0.002511502243578434 Validation loss 0.0458960123360157 Accuracy 0.8896250128746033\n",
      "Iteration 83090 Training loss 0.0012611254351213574 Validation loss 0.045892518013715744 Accuracy 0.8892500400543213\n",
      "Iteration 83100 Training loss 0.0012655631871894002 Validation loss 0.04592745006084442 Accuracy 0.8895000219345093\n",
      "Iteration 83110 Training loss 1.4435874618357047e-05 Validation loss 0.045958712697029114 Accuracy 0.8895000219345093\n",
      "Iteration 83120 Training loss 0.0012598889879882336 Validation loss 0.0459507554769516 Accuracy 0.8895000219345093\n",
      "Iteration 83130 Training loss 0.0025082421489059925 Validation loss 0.04594946652650833 Accuracy 0.8890000581741333\n",
      "Iteration 83140 Training loss 9.402125215274282e-06 Validation loss 0.045914605259895325 Accuracy 0.8893750309944153\n",
      "Iteration 83150 Training loss 1.3150195627531502e-05 Validation loss 0.04590762406587601 Accuracy 0.8893750309944153\n",
      "Iteration 83160 Training loss 1.0947249393211678e-05 Validation loss 0.04589654877781868 Accuracy 0.8896250128746033\n",
      "Iteration 83170 Training loss 0.0012627072865143418 Validation loss 0.045927636325359344 Accuracy 0.8890000581741333\n",
      "Iteration 83180 Training loss 0.0037600372452288866 Validation loss 0.045950762927532196 Accuracy 0.8891250491142273\n",
      "Iteration 83190 Training loss 0.001260533113963902 Validation loss 0.04589385539293289 Accuracy 0.8895000219345093\n",
      "Iteration 83200 Training loss 0.0025114738382399082 Validation loss 0.045937132090330124 Accuracy 0.8893750309944153\n",
      "Iteration 83210 Training loss 9.56457824941026e-06 Validation loss 0.045929308980703354 Accuracy 0.8890000581741333\n",
      "Iteration 83220 Training loss 0.0025093895383179188 Validation loss 0.04593631252646446 Accuracy 0.8891250491142273\n",
      "Iteration 83230 Training loss 8.002051799849141e-06 Validation loss 0.04589457809925079 Accuracy 0.8890000581741333\n",
      "Iteration 83240 Training loss 0.0012597230961546302 Validation loss 0.04593738168478012 Accuracy 0.8893750309944153\n",
      "Iteration 83250 Training loss 0.001261607394553721 Validation loss 0.0458962582051754 Accuracy 0.8890000581741333\n",
      "Iteration 83260 Training loss 1.1643678590189666e-05 Validation loss 0.04592166468501091 Accuracy 0.8892500400543213\n",
      "Iteration 83270 Training loss 0.001260693883523345 Validation loss 0.04592389240860939 Accuracy 0.8892500400543213\n",
      "Iteration 83280 Training loss 1.0998453944921494e-05 Validation loss 0.04590718075633049 Accuracy 0.8891250491142273\n",
      "Iteration 83290 Training loss 0.0025119432248175144 Validation loss 0.045920390635728836 Accuracy 0.8895000219345093\n",
      "Iteration 83300 Training loss 0.0012587226228788495 Validation loss 0.04593442752957344 Accuracy 0.8896250128746033\n",
      "Iteration 83310 Training loss 8.161719961208291e-06 Validation loss 0.045948442071676254 Accuracy 0.8892500400543213\n",
      "Iteration 83320 Training loss 0.0012588247191160917 Validation loss 0.04595014452934265 Accuracy 0.8893750309944153\n",
      "Iteration 83330 Training loss 0.0037605243269354105 Validation loss 0.04597306251525879 Accuracy 0.8892500400543213\n",
      "Iteration 83340 Training loss 8.008854820218403e-06 Validation loss 0.045949939638376236 Accuracy 0.8887500166893005\n",
      "Iteration 83350 Training loss 0.001260679797269404 Validation loss 0.04595283791422844 Accuracy 0.8893750309944153\n",
      "Iteration 83360 Training loss 0.001264499151147902 Validation loss 0.04598177596926689 Accuracy 0.8887500166893005\n",
      "Iteration 83370 Training loss 0.0037615455221384764 Validation loss 0.0459408275783062 Accuracy 0.8893750309944153\n",
      "Iteration 83380 Training loss 0.0025101453065872192 Validation loss 0.045910730957984924 Accuracy 0.8888750672340393\n",
      "Iteration 83390 Training loss 0.001263139653019607 Validation loss 0.045897502452135086 Accuracy 0.8890000581741333\n",
      "Iteration 83400 Training loss 7.632028427906334e-06 Validation loss 0.04591158404946327 Accuracy 0.8895000219345093\n",
      "Iteration 83410 Training loss 0.0012603753712028265 Validation loss 0.04590795189142227 Accuracy 0.8896250128746033\n",
      "Iteration 83420 Training loss 0.0012600773479789495 Validation loss 0.04591713100671768 Accuracy 0.8892500400543213\n",
      "Iteration 83430 Training loss 1.0186745384999085e-05 Validation loss 0.0459187813103199 Accuracy 0.8891250491142273\n",
      "Iteration 83440 Training loss 0.00126111158169806 Validation loss 0.045904818922281265 Accuracy 0.8890000581741333\n",
      "Iteration 83450 Training loss 8.36388880998129e-06 Validation loss 0.04587874934077263 Accuracy 0.8896250128746033\n",
      "Iteration 83460 Training loss 1.377064018015517e-05 Validation loss 0.04588592052459717 Accuracy 0.8896250128746033\n",
      "Iteration 83470 Training loss 0.0012631566496565938 Validation loss 0.04588482156395912 Accuracy 0.8896250128746033\n",
      "Iteration 83480 Training loss 0.0012612579157575965 Validation loss 0.045888785272836685 Accuracy 0.8888750672340393\n",
      "Iteration 83490 Training loss 0.0012630855198949575 Validation loss 0.04588710144162178 Accuracy 0.8891250491142273\n",
      "Iteration 83500 Training loss 0.0037618984933942556 Validation loss 0.04589110612869263 Accuracy 0.8892500400543213\n",
      "Iteration 83510 Training loss 0.00250978278927505 Validation loss 0.04588101804256439 Accuracy 0.8893750309944153\n",
      "Iteration 83520 Training loss 0.0012588612735271454 Validation loss 0.04589325934648514 Accuracy 0.8891250491142273\n",
      "Iteration 83530 Training loss 1.0914669474004768e-05 Validation loss 0.04591035097837448 Accuracy 0.8893750309944153\n",
      "Iteration 83540 Training loss 8.566279575461522e-06 Validation loss 0.04593462496995926 Accuracy 0.8890000581741333\n",
      "Iteration 83550 Training loss 0.0012633147416636348 Validation loss 0.04593439772725105 Accuracy 0.8888750672340393\n",
      "Iteration 83560 Training loss 0.0025116002652794123 Validation loss 0.04591745138168335 Accuracy 0.8893750309944153\n",
      "Iteration 83570 Training loss 0.0012624820228666067 Validation loss 0.045936714857816696 Accuracy 0.8893750309944153\n",
      "Iteration 83580 Training loss 0.002511247294023633 Validation loss 0.0459250807762146 Accuracy 0.8893750309944153\n",
      "Iteration 83590 Training loss 0.0012648988049477339 Validation loss 0.045952145010232925 Accuracy 0.8892500400543213\n",
      "Iteration 83600 Training loss 1.1458202607173007e-05 Validation loss 0.04593556746840477 Accuracy 0.8887500166893005\n",
      "Iteration 83610 Training loss 0.0037586006801575422 Validation loss 0.04592173174023628 Accuracy 0.8886250257492065\n",
      "Iteration 83620 Training loss 1.1735160114767496e-05 Validation loss 0.045905083417892456 Accuracy 0.8890000581741333\n",
      "Iteration 83630 Training loss 8.32385467219865e-06 Validation loss 0.04593149945139885 Accuracy 0.8892500400543213\n",
      "Iteration 83640 Training loss 0.0012586291413754225 Validation loss 0.04594357684254646 Accuracy 0.8891250491142273\n",
      "Iteration 83650 Training loss 7.715110768913291e-06 Validation loss 0.04592970758676529 Accuracy 0.8890000581741333\n",
      "Iteration 83660 Training loss 0.0025096884928643703 Validation loss 0.045954976230859756 Accuracy 0.8892500400543213\n",
      "Iteration 83670 Training loss 1.0331397788831964e-05 Validation loss 0.04593610763549805 Accuracy 0.8887500166893005\n",
      "Iteration 83680 Training loss 0.0012620509369298816 Validation loss 0.045917391777038574 Accuracy 0.8891250491142273\n",
      "Iteration 83690 Training loss 8.78224727784982e-06 Validation loss 0.04592215269804001 Accuracy 0.8891250491142273\n",
      "Iteration 83700 Training loss 0.001260506920516491 Validation loss 0.045903272926807404 Accuracy 0.8890000581741333\n",
      "Iteration 83710 Training loss 1.3622387086797971e-05 Validation loss 0.045906223356723785 Accuracy 0.8892500400543213\n",
      "Iteration 83720 Training loss 7.868426109780557e-06 Validation loss 0.04590504243969917 Accuracy 0.8890000581741333\n",
      "Iteration 83730 Training loss 0.002511681756004691 Validation loss 0.04590839520096779 Accuracy 0.8888750672340393\n",
      "Iteration 83740 Training loss 1.9623250409495085e-05 Validation loss 0.04590017721056938 Accuracy 0.8890000581741333\n",
      "Iteration 83750 Training loss 7.5102620940015186e-06 Validation loss 0.04589641094207764 Accuracy 0.8891250491142273\n",
      "Iteration 83760 Training loss 0.002509726444259286 Validation loss 0.04591163620352745 Accuracy 0.8890000581741333\n",
      "Iteration 83770 Training loss 0.0037611352745443583 Validation loss 0.04591483622789383 Accuracy 0.8888750672340393\n",
      "Iteration 83780 Training loss 0.0012602776987478137 Validation loss 0.045910295099020004 Accuracy 0.8888750672340393\n",
      "Iteration 83790 Training loss 1.1348107364028692e-05 Validation loss 0.04593520611524582 Accuracy 0.8891250491142273\n",
      "Iteration 83800 Training loss 0.003758631879463792 Validation loss 0.04595399275422096 Accuracy 0.8892500400543213\n",
      "Iteration 83810 Training loss 0.0025101073551923037 Validation loss 0.04597993195056915 Accuracy 0.8888750672340393\n",
      "Iteration 83820 Training loss 0.0025144885294139385 Validation loss 0.04597187042236328 Accuracy 0.8891250491142273\n",
      "Iteration 83830 Training loss 9.929353836923838e-06 Validation loss 0.045942503958940506 Accuracy 0.8892500400543213\n",
      "Iteration 83840 Training loss 0.0012592710554599762 Validation loss 0.04592946544289589 Accuracy 0.8888750672340393\n",
      "Iteration 83850 Training loss 1.2084891750419047e-05 Validation loss 0.04599752649664879 Accuracy 0.8887500166893005\n",
      "Iteration 83860 Training loss 1.2115032404835802e-05 Validation loss 0.046016909182071686 Accuracy 0.8888750672340393\n",
      "Iteration 83870 Training loss 0.0012600512709468603 Validation loss 0.04599202796816826 Accuracy 0.8887500166893005\n",
      "Iteration 83880 Training loss 0.0012633163714781404 Validation loss 0.04598912596702576 Accuracy 0.8891250491142273\n",
      "Iteration 83890 Training loss 0.0012599200708791614 Validation loss 0.04599921405315399 Accuracy 0.8888750672340393\n",
      "Iteration 83900 Training loss 1.058149337040959e-05 Validation loss 0.046000994741916656 Accuracy 0.8888750672340393\n",
      "Iteration 83910 Training loss 0.0012615264859050512 Validation loss 0.045988425612449646 Accuracy 0.8887500166893005\n",
      "Iteration 83920 Training loss 0.001261505181901157 Validation loss 0.045949481427669525 Accuracy 0.8887500166893005\n",
      "Iteration 83930 Training loss 0.0012596180895343423 Validation loss 0.04594852775335312 Accuracy 0.8887500166893005\n",
      "Iteration 83940 Training loss 0.002511236583814025 Validation loss 0.045962221920490265 Accuracy 0.8888750672340393\n",
      "Iteration 83950 Training loss 0.002508870791643858 Validation loss 0.045956578105688095 Accuracy 0.8892500400543213\n",
      "Iteration 83960 Training loss 0.006264951545745134 Validation loss 0.04594987630844116 Accuracy 0.8891250491142273\n",
      "Iteration 83970 Training loss 0.0012621328933164477 Validation loss 0.04594847187399864 Accuracy 0.8891250491142273\n",
      "Iteration 83980 Training loss 1.2956954378751107e-05 Validation loss 0.0459490530192852 Accuracy 0.8891250491142273\n",
      "Iteration 83990 Training loss 0.0012603465002030134 Validation loss 0.04592784121632576 Accuracy 0.8892500400543213\n",
      "Iteration 84000 Training loss 0.0012609482510015368 Validation loss 0.04592547193169594 Accuracy 0.8891250491142273\n",
      "Iteration 84010 Training loss 0.0012614522129297256 Validation loss 0.04589686170220375 Accuracy 0.8892500400543213\n",
      "Iteration 84020 Training loss 0.0012630335986614227 Validation loss 0.04588955640792847 Accuracy 0.8895000219345093\n",
      "Iteration 84030 Training loss 0.0012696079211309552 Validation loss 0.04587540030479431 Accuracy 0.8888750672340393\n",
      "Iteration 84040 Training loss 0.0025121381040662527 Validation loss 0.04592875391244888 Accuracy 0.8890000581741333\n",
      "Iteration 84050 Training loss 0.002511153230443597 Validation loss 0.04590882360935211 Accuracy 0.8891250491142273\n",
      "Iteration 84060 Training loss 0.0012576156295835972 Validation loss 0.04590601474046707 Accuracy 0.8891250491142273\n",
      "Iteration 84070 Training loss 0.005011299625039101 Validation loss 0.04590534418821335 Accuracy 0.8892500400543213\n",
      "Iteration 84080 Training loss 9.271190720028244e-06 Validation loss 0.045936986804008484 Accuracy 0.8892500400543213\n",
      "Iteration 84090 Training loss 9.205411515722517e-06 Validation loss 0.04590136185288429 Accuracy 0.8895000219345093\n",
      "Iteration 84100 Training loss 1.1323557373543736e-05 Validation loss 0.045910608023405075 Accuracy 0.8895000219345093\n",
      "Iteration 84110 Training loss 0.0025103320367634296 Validation loss 0.045879121869802475 Accuracy 0.8891250491142273\n",
      "Iteration 84120 Training loss 7.919704330561217e-06 Validation loss 0.04592366889119148 Accuracy 0.8892500400543213\n",
      "Iteration 84130 Training loss 0.0025103013031184673 Validation loss 0.0459134466946125 Accuracy 0.8892500400543213\n",
      "Iteration 84140 Training loss 0.002512603998184204 Validation loss 0.045896854251623154 Accuracy 0.8895000219345093\n",
      "Iteration 84150 Training loss 0.0025123946834355593 Validation loss 0.04589173570275307 Accuracy 0.8890000581741333\n",
      "Iteration 84160 Training loss 1.0471201676409692e-05 Validation loss 0.04591604694724083 Accuracy 0.8892500400543213\n",
      "Iteration 84170 Training loss 0.002509268233552575 Validation loss 0.045905083417892456 Accuracy 0.8890000581741333\n",
      "Iteration 84180 Training loss 1.0049645425169729e-05 Validation loss 0.04590674117207527 Accuracy 0.8892500400543213\n",
      "Iteration 84190 Training loss 8.183523277693894e-06 Validation loss 0.045918986201286316 Accuracy 0.8892500400543213\n",
      "Iteration 84200 Training loss 0.003762149950489402 Validation loss 0.045918338000774384 Accuracy 0.8891250491142273\n",
      "Iteration 84210 Training loss 1.2259173672646284e-05 Validation loss 0.04588575288653374 Accuracy 0.8893750309944153\n",
      "Iteration 84220 Training loss 0.0012659428175538778 Validation loss 0.0459013357758522 Accuracy 0.8891250491142273\n",
      "Iteration 84230 Training loss 0.0012621217174455523 Validation loss 0.045871660113334656 Accuracy 0.8895000219345093\n",
      "Iteration 84240 Training loss 0.003755828132852912 Validation loss 0.04588546231389046 Accuracy 0.8891250491142273\n",
      "Iteration 84250 Training loss 9.024916835187469e-06 Validation loss 0.04591108113527298 Accuracy 0.8893750309944153\n",
      "Iteration 84260 Training loss 0.0025071685668081045 Validation loss 0.04594351351261139 Accuracy 0.8891250491142273\n",
      "Iteration 84270 Training loss 0.0037649585865437984 Validation loss 0.04591625928878784 Accuracy 0.8888750672340393\n",
      "Iteration 84280 Training loss 0.00126474944408983 Validation loss 0.04594528675079346 Accuracy 0.8892500400543213\n",
      "Iteration 84290 Training loss 1.1714615538949147e-05 Validation loss 0.0459381602704525 Accuracy 0.8892500400543213\n",
      "Iteration 84300 Training loss 0.0012596308952197433 Validation loss 0.04591137543320656 Accuracy 0.8893750309944153\n",
      "Iteration 84310 Training loss 0.0012681648368015885 Validation loss 0.04632114991545677 Accuracy 0.8881250619888306\n",
      "Iteration 84320 Training loss 0.002511601662263274 Validation loss 0.046196348965168 Accuracy 0.8885000348091125\n",
      "Iteration 84330 Training loss 0.0025128039997071028 Validation loss 0.046139929443597794 Accuracy 0.8891250491142273\n",
      "Iteration 84340 Training loss 0.0025105050299316645 Validation loss 0.04612702503800392 Accuracy 0.8892500400543213\n",
      "Iteration 84350 Training loss 0.002510200021788478 Validation loss 0.04613904282450676 Accuracy 0.8887500166893005\n",
      "Iteration 84360 Training loss 1.2235709618835244e-05 Validation loss 0.04610536992549896 Accuracy 0.8887500166893005\n",
      "Iteration 84370 Training loss 0.003767447778955102 Validation loss 0.04610319808125496 Accuracy 0.8887500166893005\n",
      "Iteration 84380 Training loss 0.0025123762898147106 Validation loss 0.046062011271715164 Accuracy 0.8888750672340393\n",
      "Iteration 84390 Training loss 7.920048119558487e-06 Validation loss 0.04605139046907425 Accuracy 0.8891250491142273\n",
      "Iteration 84400 Training loss 0.001260551973246038 Validation loss 0.04605318605899811 Accuracy 0.8891250491142273\n",
      "Iteration 84410 Training loss 0.002513057552278042 Validation loss 0.04612041264772415 Accuracy 0.8890000581741333\n",
      "Iteration 84420 Training loss 1.2459122444852255e-05 Validation loss 0.04610072448849678 Accuracy 0.8890000581741333\n",
      "Iteration 84430 Training loss 1.200218412122922e-05 Validation loss 0.04609966650605202 Accuracy 0.8891250491142273\n",
      "Iteration 84440 Training loss 0.002513733459636569 Validation loss 0.046088092029094696 Accuracy 0.8892500400543213\n",
      "Iteration 84450 Training loss 0.002510008169338107 Validation loss 0.04605475813150406 Accuracy 0.8890000581741333\n",
      "Iteration 84460 Training loss 1.2840232557209674e-05 Validation loss 0.046037424355745316 Accuracy 0.8888750672340393\n",
      "Iteration 84470 Training loss 0.0012647005496546626 Validation loss 0.04605969041585922 Accuracy 0.8887500166893005\n",
      "Iteration 84480 Training loss 0.0012680126819759607 Validation loss 0.04606495797634125 Accuracy 0.8888750672340393\n",
      "Iteration 84490 Training loss 0.003758690319955349 Validation loss 0.046071603894233704 Accuracy 0.8888750672340393\n",
      "Iteration 84500 Training loss 0.0037583427038043737 Validation loss 0.046057697385549545 Accuracy 0.8885000348091125\n",
      "Iteration 84510 Training loss 9.186290299112443e-06 Validation loss 0.04606160521507263 Accuracy 0.8885000348091125\n",
      "Iteration 84520 Training loss 0.0025138305500149727 Validation loss 0.046053383499383926 Accuracy 0.8887500166893005\n",
      "Iteration 84530 Training loss 1.221017100760946e-05 Validation loss 0.04602593556046486 Accuracy 0.8890000581741333\n",
      "Iteration 84540 Training loss 0.0012604602379724383 Validation loss 0.0459979884326458 Accuracy 0.8888750672340393\n",
      "Iteration 84550 Training loss 8.786870239418931e-06 Validation loss 0.04603974521160126 Accuracy 0.8888750672340393\n",
      "Iteration 84560 Training loss 9.426207725482527e-06 Validation loss 0.04602313041687012 Accuracy 0.8890000581741333\n",
      "Iteration 84570 Training loss 0.0025083646178245544 Validation loss 0.04601869359612465 Accuracy 0.8888750672340393\n",
      "Iteration 84580 Training loss 0.0012611850397661328 Validation loss 0.04604059085249901 Accuracy 0.8890000581741333\n",
      "Iteration 84590 Training loss 0.0012607654789462686 Validation loss 0.0460270456969738 Accuracy 0.8890000581741333\n",
      "Iteration 84600 Training loss 1.1173709026479628e-05 Validation loss 0.04600907489657402 Accuracy 0.8891250491142273\n",
      "Iteration 84610 Training loss 0.0012615452287718654 Validation loss 0.045992035418748856 Accuracy 0.8883750438690186\n",
      "Iteration 84620 Training loss 0.006262229755520821 Validation loss 0.04600448161363602 Accuracy 0.8891250491142273\n",
      "Iteration 84630 Training loss 0.005011792294681072 Validation loss 0.04603523388504982 Accuracy 0.8891250491142273\n",
      "Iteration 84640 Training loss 0.0012593985302373767 Validation loss 0.046002015471458435 Accuracy 0.8888750672340393\n",
      "Iteration 84650 Training loss 1.3004999345866963e-05 Validation loss 0.046004682779312134 Accuracy 0.8888750672340393\n",
      "Iteration 84660 Training loss 0.003757646307349205 Validation loss 0.04603953659534454 Accuracy 0.8890000581741333\n",
      "Iteration 84670 Training loss 1.2074419828422833e-05 Validation loss 0.04601068049669266 Accuracy 0.8892500400543213\n",
      "Iteration 84680 Training loss 1.2351534678600729e-05 Validation loss 0.04597855731844902 Accuracy 0.8890000581741333\n",
      "Iteration 84690 Training loss 0.0012610758421942592 Validation loss 0.04598448425531387 Accuracy 0.8891250491142273\n",
      "Iteration 84700 Training loss 0.0012602340430021286 Validation loss 0.04602406173944473 Accuracy 0.8890000581741333\n",
      "Iteration 84710 Training loss 1.4025098607817199e-05 Validation loss 0.04600029066205025 Accuracy 0.8891250491142273\n",
      "Iteration 84720 Training loss 0.003759238636121154 Validation loss 0.04600297659635544 Accuracy 0.8890000581741333\n",
      "Iteration 84730 Training loss 0.002508574165403843 Validation loss 0.04598488658666611 Accuracy 0.8888750672340393\n",
      "Iteration 84740 Training loss 9.80913591774879e-06 Validation loss 0.0460018627345562 Accuracy 0.8890000581741333\n",
      "Iteration 84750 Training loss 0.006259560585021973 Validation loss 0.046014755964279175 Accuracy 0.8891250491142273\n",
      "Iteration 84760 Training loss 0.0025110661517828703 Validation loss 0.04603258892893791 Accuracy 0.8892500400543213\n",
      "Iteration 84770 Training loss 1.1427930985519197e-05 Validation loss 0.04601345956325531 Accuracy 0.8890000581741333\n",
      "Iteration 84780 Training loss 0.005011214874684811 Validation loss 0.04599887132644653 Accuracy 0.8888750672340393\n",
      "Iteration 84790 Training loss 0.0012593073770403862 Validation loss 0.04597501456737518 Accuracy 0.8887500166893005\n",
      "Iteration 84800 Training loss 0.0012625117087736726 Validation loss 0.04598833993077278 Accuracy 0.8888750672340393\n",
      "Iteration 84810 Training loss 0.0025112186558544636 Validation loss 0.046014271676540375 Accuracy 0.8896250128746033\n",
      "Iteration 84820 Training loss 0.0012595506850630045 Validation loss 0.04599514603614807 Accuracy 0.8893750309944153\n",
      "Iteration 84830 Training loss 1.1055967661377508e-05 Validation loss 0.045969296246767044 Accuracy 0.8888750672340393\n",
      "Iteration 84840 Training loss 0.001257170457392931 Validation loss 0.04601408168673515 Accuracy 0.8893750309944153\n",
      "Iteration 84850 Training loss 9.625205166230444e-06 Validation loss 0.04598071053624153 Accuracy 0.8888750672340393\n",
      "Iteration 84860 Training loss 1.1542222637217492e-05 Validation loss 0.046001896262168884 Accuracy 0.8885000348091125\n",
      "Iteration 84870 Training loss 0.0025183239486068487 Validation loss 0.04604404792189598 Accuracy 0.8888750672340393\n",
      "Iteration 84880 Training loss 1.4409670257009566e-05 Validation loss 0.04604261368513107 Accuracy 0.8891250491142273\n",
      "Iteration 84890 Training loss 0.002512899227440357 Validation loss 0.04604560509324074 Accuracy 0.8888750672340393\n",
      "Iteration 84900 Training loss 1.0827365258592181e-05 Validation loss 0.04604244977235794 Accuracy 0.8893750309944153\n",
      "Iteration 84910 Training loss 8.824888936942443e-06 Validation loss 0.04605656489729881 Accuracy 0.8887500166893005\n",
      "Iteration 84920 Training loss 0.002510501304641366 Validation loss 0.04603586718440056 Accuracy 0.8891250491142273\n",
      "Iteration 84930 Training loss 8.385284672840498e-06 Validation loss 0.04602706432342529 Accuracy 0.8891250491142273\n",
      "Iteration 84940 Training loss 0.0025162233505398035 Validation loss 0.04601318761706352 Accuracy 0.8887500166893005\n",
      "Iteration 84950 Training loss 0.0012593516148626804 Validation loss 0.046006910502910614 Accuracy 0.8890000581741333\n",
      "Iteration 84960 Training loss 0.0012601780472323298 Validation loss 0.045996181666851044 Accuracy 0.8887500166893005\n",
      "Iteration 84970 Training loss 0.005007816944271326 Validation loss 0.04598098248243332 Accuracy 0.8887500166893005\n",
      "Iteration 84980 Training loss 0.001261805067770183 Validation loss 0.045995257794857025 Accuracy 0.8891250491142273\n",
      "Iteration 84990 Training loss 1.44700670716702e-05 Validation loss 0.04599763825535774 Accuracy 0.8892500400543213\n",
      "Iteration 85000 Training loss 0.001263124868273735 Validation loss 0.04598074406385422 Accuracy 0.8891250491142273\n",
      "Iteration 85010 Training loss 0.001260063610970974 Validation loss 0.04598326236009598 Accuracy 0.8893750309944153\n",
      "Iteration 85020 Training loss 0.0012761862017214298 Validation loss 0.04596460983157158 Accuracy 0.8888750672340393\n",
      "Iteration 85030 Training loss 0.0012599821202456951 Validation loss 0.045986611396074295 Accuracy 0.8890000581741333\n",
      "Iteration 85040 Training loss 1.3559844774135854e-05 Validation loss 0.045991554856300354 Accuracy 0.8890000581741333\n",
      "Iteration 85050 Training loss 0.001260213553905487 Validation loss 0.04598521813750267 Accuracy 0.8890000581741333\n",
      "Iteration 85060 Training loss 0.001260494813323021 Validation loss 0.046004023402929306 Accuracy 0.8892500400543213\n",
      "Iteration 85070 Training loss 0.0012624484952539206 Validation loss 0.04597475752234459 Accuracy 0.8892500400543213\n",
      "Iteration 85080 Training loss 0.0012599964393302798 Validation loss 0.04597034677863121 Accuracy 0.8891250491142273\n",
      "Iteration 85090 Training loss 0.001255821087397635 Validation loss 0.045963846147060394 Accuracy 0.8893750309944153\n",
      "Iteration 85100 Training loss 1.0071717952087056e-05 Validation loss 0.04598313942551613 Accuracy 0.8895000219345093\n",
      "Iteration 85110 Training loss 8.40559459902579e-06 Validation loss 0.045974425971508026 Accuracy 0.8893750309944153\n",
      "Iteration 85120 Training loss 1.0782519893837161e-05 Validation loss 0.045959338545799255 Accuracy 0.8893750309944153\n",
      "Iteration 85130 Training loss 1.0121543709828984e-05 Validation loss 0.0460115410387516 Accuracy 0.8893750309944153\n",
      "Iteration 85140 Training loss 0.0025113821029663086 Validation loss 0.046007122844457626 Accuracy 0.8890000581741333\n",
      "Iteration 85150 Training loss 1.072048326022923e-05 Validation loss 0.046018801629543304 Accuracy 0.8891250491142273\n",
      "Iteration 85160 Training loss 0.0025083976797759533 Validation loss 0.04600968584418297 Accuracy 0.8891250491142273\n",
      "Iteration 85170 Training loss 0.0037598111666738987 Validation loss 0.04599783569574356 Accuracy 0.8892500400543213\n",
      "Iteration 85180 Training loss 1.2005527423752937e-05 Validation loss 0.04597794637084007 Accuracy 0.8893750309944153\n",
      "Iteration 85190 Training loss 0.002508343430235982 Validation loss 0.04597493261098862 Accuracy 0.8896250128746033\n",
      "Iteration 85200 Training loss 0.0025094239972531796 Validation loss 0.04596826806664467 Accuracy 0.8893750309944153\n",
      "Iteration 85210 Training loss 1.6229061657213606e-05 Validation loss 0.04598138481378555 Accuracy 0.8892500400543213\n",
      "Iteration 85220 Training loss 1.1758368600567337e-05 Validation loss 0.04597999155521393 Accuracy 0.8892500400543213\n",
      "Iteration 85230 Training loss 0.001258587813936174 Validation loss 0.045991890132427216 Accuracy 0.8895000219345093\n",
      "Iteration 85240 Training loss 0.0012605712981894612 Validation loss 0.04596870020031929 Accuracy 0.8893750309944153\n",
      "Iteration 85250 Training loss 0.0012580550974234939 Validation loss 0.04596606642007828 Accuracy 0.8890000581741333\n",
      "Iteration 85260 Training loss 0.002508962992578745 Validation loss 0.045946232974529266 Accuracy 0.889875054359436\n",
      "Iteration 85270 Training loss 0.0012584078358486295 Validation loss 0.04596654698252678 Accuracy 0.8892500400543213\n",
      "Iteration 85280 Training loss 9.675468390923925e-06 Validation loss 0.04597713425755501 Accuracy 0.8893750309944153\n",
      "Iteration 85290 Training loss 0.00251014088280499 Validation loss 0.045969538390636444 Accuracy 0.8893750309944153\n",
      "Iteration 85300 Training loss 0.0012612923746928573 Validation loss 0.045986372977495193 Accuracy 0.8893750309944153\n",
      "Iteration 85310 Training loss 9.216599210049026e-06 Validation loss 0.045980341732501984 Accuracy 0.8891250491142273\n",
      "Iteration 85320 Training loss 1.1656826245598495e-05 Validation loss 0.04596555233001709 Accuracy 0.8895000219345093\n",
      "Iteration 85330 Training loss 1.2180969861219637e-05 Validation loss 0.04595018923282623 Accuracy 0.8892500400543213\n",
      "Iteration 85340 Training loss 0.0025051396805793047 Validation loss 0.045937035232782364 Accuracy 0.8890000581741333\n",
      "Iteration 85350 Training loss 0.0012607688549906015 Validation loss 0.045955657958984375 Accuracy 0.8895000219345093\n",
      "Iteration 85360 Training loss 0.001264885300770402 Validation loss 0.04595484212040901 Accuracy 0.8888750672340393\n",
      "Iteration 85370 Training loss 0.0012603856157511473 Validation loss 0.04599795863032341 Accuracy 0.8891250491142273\n",
      "Iteration 85380 Training loss 0.0012600966729223728 Validation loss 0.04600778594613075 Accuracy 0.8893750309944153\n",
      "Iteration 85390 Training loss 1.1687483493005857e-05 Validation loss 0.04597863927483559 Accuracy 0.8895000219345093\n",
      "Iteration 85400 Training loss 0.0037587827537208796 Validation loss 0.045994024723768234 Accuracy 0.8892500400543213\n",
      "Iteration 85410 Training loss 1.359943507850403e-05 Validation loss 0.04597602039575577 Accuracy 0.8892500400543213\n",
      "Iteration 85420 Training loss 0.0025107546243816614 Validation loss 0.045951683074235916 Accuracy 0.8891250491142273\n",
      "Iteration 85430 Training loss 0.0012611007550731301 Validation loss 0.045973312109708786 Accuracy 0.8895000219345093\n",
      "Iteration 85440 Training loss 8.872791113390122e-06 Validation loss 0.045973971486091614 Accuracy 0.8892500400543213\n",
      "Iteration 85450 Training loss 0.0037595003377646208 Validation loss 0.046005625277757645 Accuracy 0.8895000219345093\n",
      "Iteration 85460 Training loss 0.0012670604046434164 Validation loss 0.04596026986837387 Accuracy 0.8893750309944153\n",
      "Iteration 85470 Training loss 1.0861619557545055e-05 Validation loss 0.045967426151037216 Accuracy 0.8895000219345093\n",
      "Iteration 85480 Training loss 0.0025107902474701405 Validation loss 0.04596191272139549 Accuracy 0.8893750309944153\n",
      "Iteration 85490 Training loss 1.2132335541537032e-05 Validation loss 0.04597175121307373 Accuracy 0.8892500400543213\n",
      "Iteration 85500 Training loss 0.0012612491846084595 Validation loss 0.04598398134112358 Accuracy 0.8892500400543213\n",
      "Iteration 85510 Training loss 0.0012608930701389909 Validation loss 0.04595063254237175 Accuracy 0.8891250491142273\n",
      "Iteration 85520 Training loss 9.723253242555074e-06 Validation loss 0.04596934840083122 Accuracy 0.8892500400543213\n",
      "Iteration 85530 Training loss 8.885059287422337e-06 Validation loss 0.04597285017371178 Accuracy 0.8896250128746033\n",
      "Iteration 85540 Training loss 1.4354078302858397e-05 Validation loss 0.0459689125418663 Accuracy 0.8887500166893005\n",
      "Iteration 85550 Training loss 0.006259456742554903 Validation loss 0.045975059270858765 Accuracy 0.8892500400543213\n",
      "Iteration 85560 Training loss 7.380165243375814e-06 Validation loss 0.04601208120584488 Accuracy 0.8892500400543213\n",
      "Iteration 85570 Training loss 0.0012635424500331283 Validation loss 0.04599779099225998 Accuracy 0.8890000581741333\n",
      "Iteration 85580 Training loss 0.0012593759456649423 Validation loss 0.04600895568728447 Accuracy 0.8895000219345093\n",
      "Iteration 85590 Training loss 0.001261350349523127 Validation loss 0.04599647596478462 Accuracy 0.8893750309944153\n",
      "Iteration 85600 Training loss 0.0012612303253263235 Validation loss 0.045987099409103394 Accuracy 0.8891250491142273\n",
      "Iteration 85610 Training loss 0.0025148745626211166 Validation loss 0.04599430412054062 Accuracy 0.8886250257492065\n",
      "Iteration 85620 Training loss 0.0012619822518900037 Validation loss 0.0460185781121254 Accuracy 0.8892500400543213\n",
      "Iteration 85630 Training loss 1.0248951184621546e-05 Validation loss 0.04598960652947426 Accuracy 0.8892500400543213\n",
      "Iteration 85640 Training loss 0.002511595841497183 Validation loss 0.04598984122276306 Accuracy 0.8892500400543213\n",
      "Iteration 85650 Training loss 1.2352888006716967e-05 Validation loss 0.04598762467503548 Accuracy 0.8891250491142273\n",
      "Iteration 85660 Training loss 0.0012593296123668551 Validation loss 0.04597777500748634 Accuracy 0.8891250491142273\n",
      "Iteration 85670 Training loss 1.1833714779641014e-05 Validation loss 0.04598979279398918 Accuracy 0.8890000581741333\n",
      "Iteration 85680 Training loss 0.0012622138019651175 Validation loss 0.0460033118724823 Accuracy 0.8891250491142273\n",
      "Iteration 85690 Training loss 7.962799827510025e-06 Validation loss 0.04597456753253937 Accuracy 0.8891250491142273\n",
      "Iteration 85700 Training loss 0.0012585576623678207 Validation loss 0.045976411551237106 Accuracy 0.8891250491142273\n",
      "Iteration 85710 Training loss 0.001262952690012753 Validation loss 0.045977331697940826 Accuracy 0.8890000581741333\n",
      "Iteration 85720 Training loss 0.0025095015298575163 Validation loss 0.04596039652824402 Accuracy 0.8891250491142273\n",
      "Iteration 85730 Training loss 1.0383892913523596e-05 Validation loss 0.04595920071005821 Accuracy 0.8891250491142273\n",
      "Iteration 85740 Training loss 1.0551910236245021e-05 Validation loss 0.04595476761460304 Accuracy 0.8890000581741333\n",
      "Iteration 85750 Training loss 9.915751434164122e-06 Validation loss 0.045979712158441544 Accuracy 0.8888750672340393\n",
      "Iteration 85760 Training loss 0.0012625515228137374 Validation loss 0.04597918316721916 Accuracy 0.8888750672340393\n",
      "Iteration 85770 Training loss 1.1101623385911807e-05 Validation loss 0.045949533581733704 Accuracy 0.8892500400543213\n",
      "Iteration 85780 Training loss 0.0037610994186252356 Validation loss 0.04596533626317978 Accuracy 0.8892500400543213\n",
      "Iteration 85790 Training loss 0.0025119087658822536 Validation loss 0.0459769144654274 Accuracy 0.8892500400543213\n",
      "Iteration 85800 Training loss 0.0025109359994530678 Validation loss 0.04597342759370804 Accuracy 0.8891250491142273\n",
      "Iteration 85810 Training loss 0.0012610413832589984 Validation loss 0.045982323586940765 Accuracy 0.8893750309944153\n",
      "Iteration 85820 Training loss 0.0012582835042849183 Validation loss 0.04599570482969284 Accuracy 0.8890000581741333\n",
      "Iteration 85830 Training loss 1.0836545698111877e-05 Validation loss 0.04599524661898613 Accuracy 0.8887500166893005\n",
      "Iteration 85840 Training loss 0.0012614619918167591 Validation loss 0.04600321501493454 Accuracy 0.8887500166893005\n",
      "Iteration 85850 Training loss 0.006259504240006208 Validation loss 0.045981213450431824 Accuracy 0.8890000581741333\n",
      "Iteration 85860 Training loss 0.0012579252943396568 Validation loss 0.04598964750766754 Accuracy 0.8891250491142273\n",
      "Iteration 85870 Training loss 0.0012605737429112196 Validation loss 0.046009521931409836 Accuracy 0.8891250491142273\n",
      "Iteration 85880 Training loss 0.002509754616767168 Validation loss 0.04600048437714577 Accuracy 0.8888750672340393\n",
      "Iteration 85890 Training loss 0.0012586073717102408 Validation loss 0.04598101228475571 Accuracy 0.8888750672340393\n",
      "Iteration 85900 Training loss 0.0012727163266390562 Validation loss 0.04601173475384712 Accuracy 0.8890000581741333\n",
      "Iteration 85910 Training loss 8.0372219599667e-06 Validation loss 0.045990969985723495 Accuracy 0.8891250491142273\n",
      "Iteration 85920 Training loss 1.2536576832644641e-05 Validation loss 0.04598499462008476 Accuracy 0.8891250491142273\n",
      "Iteration 85930 Training loss 0.003764198860153556 Validation loss 0.04597168043255806 Accuracy 0.8895000219345093\n",
      "Iteration 85940 Training loss 1.179403352580266e-05 Validation loss 0.045980777591466904 Accuracy 0.8893750309944153\n",
      "Iteration 85950 Training loss 0.001262551173567772 Validation loss 0.04600854590535164 Accuracy 0.8890000581741333\n",
      "Iteration 85960 Training loss 1.054911354003707e-05 Validation loss 0.046012308448553085 Accuracy 0.8888750672340393\n",
      "Iteration 85970 Training loss 1.0358451618230902e-05 Validation loss 0.04600423574447632 Accuracy 0.8888750672340393\n",
      "Iteration 85980 Training loss 9.874906936602201e-06 Validation loss 0.045981813222169876 Accuracy 0.8892500400543213\n",
      "Iteration 85990 Training loss 9.013634553411976e-06 Validation loss 0.045960865914821625 Accuracy 0.8892500400543213\n",
      "Iteration 86000 Training loss 0.0025105364620685577 Validation loss 0.04595367610454559 Accuracy 0.8892500400543213\n",
      "Iteration 86010 Training loss 1.9775930923060514e-05 Validation loss 0.04599683731794357 Accuracy 0.8890000581741333\n",
      "Iteration 86020 Training loss 0.001262960722669959 Validation loss 0.045982714742422104 Accuracy 0.8891250491142273\n",
      "Iteration 86030 Training loss 9.341154509456828e-06 Validation loss 0.04597477242350578 Accuracy 0.8895000219345093\n",
      "Iteration 86040 Training loss 0.002510685008019209 Validation loss 0.04602951928973198 Accuracy 0.8890000581741333\n",
      "Iteration 86050 Training loss 1.0230575753666926e-05 Validation loss 0.04602886363863945 Accuracy 0.8891250491142273\n",
      "Iteration 86060 Training loss 0.002509410958737135 Validation loss 0.04599880799651146 Accuracy 0.8892500400543213\n",
      "Iteration 86070 Training loss 1.294341291213641e-05 Validation loss 0.04598885774612427 Accuracy 0.8891250491142273\n",
      "Iteration 86080 Training loss 0.0012566705700010061 Validation loss 0.046021878719329834 Accuracy 0.8891250491142273\n",
      "Iteration 86090 Training loss 1.0301992006134242e-05 Validation loss 0.04599627107381821 Accuracy 0.8896250128746033\n",
      "Iteration 86100 Training loss 0.0037592053413391113 Validation loss 0.04599110782146454 Accuracy 0.8895000219345093\n",
      "Iteration 86110 Training loss 0.0012568362290039659 Validation loss 0.04598900303244591 Accuracy 0.8896250128746033\n",
      "Iteration 86120 Training loss 0.0012613568687811494 Validation loss 0.04598230868577957 Accuracy 0.8895000219345093\n",
      "Iteration 86130 Training loss 1.3049148947175127e-05 Validation loss 0.04598129540681839 Accuracy 0.8895000219345093\n",
      "Iteration 86140 Training loss 0.0025120603386312723 Validation loss 0.04601365700364113 Accuracy 0.8896250128746033\n",
      "Iteration 86150 Training loss 0.0012613709550350904 Validation loss 0.04595543071627617 Accuracy 0.8891250491142273\n",
      "Iteration 86160 Training loss 1.7924790881806985e-05 Validation loss 0.045958247035741806 Accuracy 0.8891250491142273\n",
      "Iteration 86170 Training loss 1.2991432413400616e-05 Validation loss 0.04599261283874512 Accuracy 0.8890000581741333\n",
      "Iteration 86180 Training loss 0.0012565415818244219 Validation loss 0.04602137207984924 Accuracy 0.8893750309944153\n",
      "Iteration 86190 Training loss 0.0025076097808778286 Validation loss 0.04600184038281441 Accuracy 0.8896250128746033\n",
      "Iteration 86200 Training loss 0.0025104456581175327 Validation loss 0.04597366228699684 Accuracy 0.8893750309944153\n",
      "Iteration 86210 Training loss 0.0012612449936568737 Validation loss 0.04597734287381172 Accuracy 0.8891250491142273\n",
      "Iteration 86220 Training loss 0.0012582609197124839 Validation loss 0.04597628861665726 Accuracy 0.8895000219345093\n",
      "Iteration 86230 Training loss 8.851170605339576e-06 Validation loss 0.045953985303640366 Accuracy 0.8893750309944153\n",
      "Iteration 86240 Training loss 0.002509067067876458 Validation loss 0.04597604647278786 Accuracy 0.8891250491142273\n",
      "Iteration 86250 Training loss 0.0050073773600161076 Validation loss 0.04598269239068031 Accuracy 0.8890000581741333\n",
      "Iteration 86260 Training loss 0.002512799808755517 Validation loss 0.04594755917787552 Accuracy 0.8892500400543213\n",
      "Iteration 86270 Training loss 0.0012578044552356005 Validation loss 0.0459769032895565 Accuracy 0.8892500400543213\n",
      "Iteration 86280 Training loss 8.73468798090471e-06 Validation loss 0.04601077362895012 Accuracy 0.8893750309944153\n",
      "Iteration 86290 Training loss 0.0025156543124467134 Validation loss 0.04600165784358978 Accuracy 0.8896250128746033\n",
      "Iteration 86300 Training loss 1.173161763290409e-05 Validation loss 0.045975279062986374 Accuracy 0.8896250128746033\n",
      "Iteration 86310 Training loss 0.0025103618390858173 Validation loss 0.04599650949239731 Accuracy 0.8896250128746033\n",
      "Iteration 86320 Training loss 0.0012584924697875977 Validation loss 0.04599832743406296 Accuracy 0.8895000219345093\n",
      "Iteration 86330 Training loss 0.0025160168297588825 Validation loss 0.04597922042012215 Accuracy 0.8893750309944153\n",
      "Iteration 86340 Training loss 7.273633855220396e-06 Validation loss 0.04595514014363289 Accuracy 0.8893750309944153\n",
      "Iteration 86350 Training loss 0.002509012119844556 Validation loss 0.04595671966671944 Accuracy 0.8896250128746033\n",
      "Iteration 86360 Training loss 8.46786133479327e-06 Validation loss 0.04597366973757744 Accuracy 0.8892500400543213\n",
      "Iteration 86370 Training loss 0.0025092398282140493 Validation loss 0.04596880450844765 Accuracy 0.8892500400543213\n",
      "Iteration 86380 Training loss 0.0012597214663401246 Validation loss 0.0460040308535099 Accuracy 0.8892500400543213\n",
      "Iteration 86390 Training loss 0.0012613632716238499 Validation loss 0.04597380384802818 Accuracy 0.8891250491142273\n",
      "Iteration 86400 Training loss 0.0012605540687218308 Validation loss 0.0459878109395504 Accuracy 0.8895000219345093\n",
      "Iteration 86410 Training loss 0.002508152276277542 Validation loss 0.045973099768161774 Accuracy 0.8892500400543213\n",
      "Iteration 86420 Training loss 1.0425806976854801e-05 Validation loss 0.04599862918257713 Accuracy 0.8892500400543213\n",
      "Iteration 86430 Training loss 8.601296030974481e-06 Validation loss 0.04599621146917343 Accuracy 0.8893750309944153\n",
      "Iteration 86440 Training loss 0.0012606829404830933 Validation loss 0.045991815626621246 Accuracy 0.8890000581741333\n",
      "Iteration 86450 Training loss 0.002511323895305395 Validation loss 0.04598449915647507 Accuracy 0.8890000581741333\n",
      "Iteration 86460 Training loss 0.0025080787017941475 Validation loss 0.045976970344781876 Accuracy 0.8893750309944153\n",
      "Iteration 86470 Training loss 1.1359013114997651e-05 Validation loss 0.04596540331840515 Accuracy 0.8893750309944153\n",
      "Iteration 86480 Training loss 0.001261831377632916 Validation loss 0.04595503583550453 Accuracy 0.8896250128746033\n",
      "Iteration 86490 Training loss 0.003763900836929679 Validation loss 0.04598159343004227 Accuracy 0.8895000219345093\n",
      "Iteration 86500 Training loss 1.2803990102838725e-05 Validation loss 0.045950066298246384 Accuracy 0.8895000219345093\n",
      "Iteration 86510 Training loss 1.1426072887843475e-05 Validation loss 0.045972079038619995 Accuracy 0.8895000219345093\n",
      "Iteration 86520 Training loss 1.2974347555427812e-05 Validation loss 0.04599146544933319 Accuracy 0.8893750309944153\n",
      "Iteration 86530 Training loss 0.0012580326292663813 Validation loss 0.04602312669157982 Accuracy 0.8892500400543213\n",
      "Iteration 86540 Training loss 0.002507584635168314 Validation loss 0.04598207771778107 Accuracy 0.8892500400543213\n",
      "Iteration 86550 Training loss 0.002511502942070365 Validation loss 0.04597796872258186 Accuracy 0.8896250128746033\n",
      "Iteration 86560 Training loss 0.0012580816401168704 Validation loss 0.04598822817206383 Accuracy 0.8895000219345093\n",
      "Iteration 86570 Training loss 0.0012582328636199236 Validation loss 0.045989733189344406 Accuracy 0.8895000219345093\n",
      "Iteration 86580 Training loss 1.3435135770123452e-05 Validation loss 0.04598434641957283 Accuracy 0.8895000219345093\n",
      "Iteration 86590 Training loss 0.0012620312627404928 Validation loss 0.04600229859352112 Accuracy 0.8892500400543213\n",
      "Iteration 86600 Training loss 0.0025115131866186857 Validation loss 0.045981958508491516 Accuracy 0.8892500400543213\n",
      "Iteration 86610 Training loss 0.0012641388457268476 Validation loss 0.04596002399921417 Accuracy 0.8892500400543213\n",
      "Iteration 86620 Training loss 8.882960173650645e-06 Validation loss 0.04597875475883484 Accuracy 0.8895000219345093\n",
      "Iteration 86630 Training loss 0.0025082179345190525 Validation loss 0.04599129036068916 Accuracy 0.8893750309944153\n",
      "Iteration 86640 Training loss 0.0012559638125821948 Validation loss 0.04599377140402794 Accuracy 0.8895000219345093\n",
      "Iteration 86650 Training loss 0.00125977979041636 Validation loss 0.04599309712648392 Accuracy 0.8893750309944153\n",
      "Iteration 86660 Training loss 0.0012624989030882716 Validation loss 0.04600068926811218 Accuracy 0.8895000219345093\n",
      "Iteration 86670 Training loss 1.2649285963561852e-05 Validation loss 0.04596082866191864 Accuracy 0.8896250128746033\n",
      "Iteration 86680 Training loss 1.1925910257559735e-05 Validation loss 0.045960716903209686 Accuracy 0.8892500400543213\n",
      "Iteration 86690 Training loss 0.0037587671540677547 Validation loss 0.04596869274973869 Accuracy 0.8892500400543213\n",
      "Iteration 86700 Training loss 7.443149570462992e-06 Validation loss 0.04598543420433998 Accuracy 0.8893750309944153\n",
      "Iteration 86710 Training loss 0.0025099243503063917 Validation loss 0.045994844287633896 Accuracy 0.8891250491142273\n",
      "Iteration 86720 Training loss 0.002510823542252183 Validation loss 0.04599377140402794 Accuracy 0.8891250491142273\n",
      "Iteration 86730 Training loss 9.498608960711863e-06 Validation loss 0.045988790690898895 Accuracy 0.8893750309944153\n",
      "Iteration 86740 Training loss 1.0164643754251301e-05 Validation loss 0.04601281136274338 Accuracy 0.8893750309944153\n",
      "Iteration 86750 Training loss 1.5012992662377656e-05 Validation loss 0.045967649668455124 Accuracy 0.8891250491142273\n",
      "Iteration 86760 Training loss 0.0012590775731950998 Validation loss 0.045996543020009995 Accuracy 0.8896250128746033\n",
      "Iteration 86770 Training loss 0.002508130855858326 Validation loss 0.04596070200204849 Accuracy 0.8892500400543213\n",
      "Iteration 86780 Training loss 1.1830213225039188e-05 Validation loss 0.045972563326358795 Accuracy 0.8891250491142273\n",
      "Iteration 86790 Training loss 0.0025094260927289724 Validation loss 0.04597073420882225 Accuracy 0.8890000581741333\n",
      "Iteration 86800 Training loss 0.0012639745837077498 Validation loss 0.04596368595957756 Accuracy 0.8893750309944153\n",
      "Iteration 86810 Training loss 0.0012608624529093504 Validation loss 0.04598601162433624 Accuracy 0.8895000219345093\n",
      "Iteration 86820 Training loss 9.53031576500507e-06 Validation loss 0.04596894234418869 Accuracy 0.8893750309944153\n",
      "Iteration 86830 Training loss 1.1319438272039406e-05 Validation loss 0.04597337543964386 Accuracy 0.8893750309944153\n",
      "Iteration 86840 Training loss 0.0025079709012061357 Validation loss 0.04600988328456879 Accuracy 0.8892500400543213\n",
      "Iteration 86850 Training loss 0.0012629760894924402 Validation loss 0.045988939702510834 Accuracy 0.8893750309944153\n",
      "Iteration 86860 Training loss 1.293014702241635e-05 Validation loss 0.0459929034113884 Accuracy 0.8895000219345093\n",
      "Iteration 86870 Training loss 0.0012569200480356812 Validation loss 0.04600087180733681 Accuracy 0.8891250491142273\n",
      "Iteration 86880 Training loss 0.0012595452135428786 Validation loss 0.04595249146223068 Accuracy 0.8892500400543213\n",
      "Iteration 86890 Training loss 0.0025078689213842154 Validation loss 0.045984916388988495 Accuracy 0.8892500400543213\n",
      "Iteration 86900 Training loss 0.0025104559026658535 Validation loss 0.04598546028137207 Accuracy 0.8893750309944153\n",
      "Iteration 86910 Training loss 0.0025078298058360815 Validation loss 0.04598318785429001 Accuracy 0.8893750309944153\n",
      "Iteration 86920 Training loss 0.0012590237893164158 Validation loss 0.0459599606692791 Accuracy 0.8891250491142273\n",
      "Iteration 86930 Training loss 0.0025093709118664265 Validation loss 0.045957621186971664 Accuracy 0.8893750309944153\n",
      "Iteration 86940 Training loss 0.0012606101809069514 Validation loss 0.04596443101763725 Accuracy 0.8891250491142273\n",
      "Iteration 86950 Training loss 0.001259740092791617 Validation loss 0.045977119356393814 Accuracy 0.8888750672340393\n",
      "Iteration 86960 Training loss 9.041275916388258e-06 Validation loss 0.04598083719611168 Accuracy 0.8886250257492065\n",
      "Iteration 86970 Training loss 0.00251383357681334 Validation loss 0.04600285738706589 Accuracy 0.8891250491142273\n",
      "Iteration 86980 Training loss 1.0239193215966225e-05 Validation loss 0.04599543288350105 Accuracy 0.8891250491142273\n",
      "Iteration 86990 Training loss 1.1800480933743529e-05 Validation loss 0.04598761349916458 Accuracy 0.8890000581741333\n",
      "Iteration 87000 Training loss 0.0025078263133764267 Validation loss 0.04599190130829811 Accuracy 0.8887500166893005\n",
      "Iteration 87010 Training loss 0.0012618539622053504 Validation loss 0.045985668897628784 Accuracy 0.8891250491142273\n",
      "Iteration 87020 Training loss 8.605492439528462e-06 Validation loss 0.04597229138016701 Accuracy 0.8892500400543213\n",
      "Iteration 87030 Training loss 0.0025091127026826143 Validation loss 0.04598177596926689 Accuracy 0.8890000581741333\n",
      "Iteration 87040 Training loss 0.0025083106011152267 Validation loss 0.04598028585314751 Accuracy 0.8893750309944153\n",
      "Iteration 87050 Training loss 0.0012580567272379994 Validation loss 0.04597784951329231 Accuracy 0.8890000581741333\n",
      "Iteration 87060 Training loss 0.00375949963927269 Validation loss 0.045991528779268265 Accuracy 0.8888750672340393\n",
      "Iteration 87070 Training loss 0.005008106119930744 Validation loss 0.045992638915777206 Accuracy 0.8892500400543213\n",
      "Iteration 87080 Training loss 0.0012581783812493086 Validation loss 0.045972298830747604 Accuracy 0.8890000581741333\n",
      "Iteration 87090 Training loss 0.001262902864255011 Validation loss 0.0459827221930027 Accuracy 0.8892500400543213\n",
      "Iteration 87100 Training loss 8.949822586146183e-06 Validation loss 0.045977603644132614 Accuracy 0.8893750309944153\n",
      "Iteration 87110 Training loss 0.001265458995476365 Validation loss 0.04600398987531662 Accuracy 0.8893750309944153\n",
      "Iteration 87120 Training loss 0.0012617871398106217 Validation loss 0.04600222036242485 Accuracy 0.8895000219345093\n",
      "Iteration 87130 Training loss 0.002511869417503476 Validation loss 0.04599389433860779 Accuracy 0.8895000219345093\n",
      "Iteration 87140 Training loss 1.0481936442374717e-05 Validation loss 0.046019334346055984 Accuracy 0.8891250491142273\n",
      "Iteration 87150 Training loss 0.0037635480985045433 Validation loss 0.04600538685917854 Accuracy 0.8888750672340393\n",
      "Iteration 87160 Training loss 0.0037588919512927532 Validation loss 0.0459853820502758 Accuracy 0.8887500166893005\n",
      "Iteration 87170 Training loss 1.0880034096771851e-05 Validation loss 0.046001359820365906 Accuracy 0.8892500400543213\n",
      "Iteration 87180 Training loss 0.0012580245966091752 Validation loss 0.04602113738656044 Accuracy 0.8892500400543213\n",
      "Iteration 87190 Training loss 0.0050080278888344765 Validation loss 0.04599592834711075 Accuracy 0.8888750672340393\n",
      "Iteration 87200 Training loss 0.0025115625467151403 Validation loss 0.045958537608385086 Accuracy 0.8892500400543213\n",
      "Iteration 87210 Training loss 0.00250763026997447 Validation loss 0.04600811377167702 Accuracy 0.8892500400543213\n",
      "Iteration 87220 Training loss 0.001260766526684165 Validation loss 0.045996036380529404 Accuracy 0.8891250491142273\n",
      "Iteration 87230 Training loss 0.002511113416403532 Validation loss 0.045997269451618195 Accuracy 0.8892500400543213\n",
      "Iteration 87240 Training loss 1.0925100468739402e-05 Validation loss 0.04599298536777496 Accuracy 0.8890000581741333\n",
      "Iteration 87250 Training loss 0.0025092880241572857 Validation loss 0.04597490653395653 Accuracy 0.8891250491142273\n",
      "Iteration 87260 Training loss 0.0012573604471981525 Validation loss 0.045995455235242844 Accuracy 0.8891250491142273\n",
      "Iteration 87270 Training loss 0.0012614638544619083 Validation loss 0.046007927507162094 Accuracy 0.8885000348091125\n",
      "Iteration 87280 Training loss 1.3275637684273534e-05 Validation loss 0.04603508114814758 Accuracy 0.8891250491142273\n",
      "Iteration 87290 Training loss 9.824496373767033e-06 Validation loss 0.04602305218577385 Accuracy 0.8887500166893005\n",
      "Iteration 87300 Training loss 0.0037587853148579597 Validation loss 0.04601043835282326 Accuracy 0.8887500166893005\n",
      "Iteration 87310 Training loss 8.481371878588106e-06 Validation loss 0.04599379003047943 Accuracy 0.8886250257492065\n",
      "Iteration 87320 Training loss 9.4064953373163e-06 Validation loss 0.04599202424287796 Accuracy 0.8890000581741333\n",
      "Iteration 87330 Training loss 1.3473191756929737e-05 Validation loss 0.04601562023162842 Accuracy 0.8888750672340393\n",
      "Iteration 87340 Training loss 0.0025089967530220747 Validation loss 0.04599260166287422 Accuracy 0.8891250491142273\n",
      "Iteration 87350 Training loss 0.0037593585439026356 Validation loss 0.04602324962615967 Accuracy 0.8895000219345093\n",
      "Iteration 87360 Training loss 1.069001700670924e-05 Validation loss 0.04600048437714577 Accuracy 0.8893750309944153\n",
      "Iteration 87370 Training loss 1.2421269275364466e-05 Validation loss 0.04600145295262337 Accuracy 0.8892500400543213\n",
      "Iteration 87380 Training loss 0.0025094677694141865 Validation loss 0.04600803554058075 Accuracy 0.8887500166893005\n",
      "Iteration 87390 Training loss 0.0025086714886128902 Validation loss 0.04599536955356598 Accuracy 0.8890000581741333\n",
      "Iteration 87400 Training loss 0.002507769502699375 Validation loss 0.0459899865090847 Accuracy 0.8890000581741333\n",
      "Iteration 87410 Training loss 0.0012590507976710796 Validation loss 0.04601316526532173 Accuracy 0.8893750309944153\n",
      "Iteration 87420 Training loss 0.0037611904554069042 Validation loss 0.04599388316273689 Accuracy 0.8888750672340393\n",
      "Iteration 87430 Training loss 0.0012582264607772231 Validation loss 0.04600493982434273 Accuracy 0.8892500400543213\n",
      "Iteration 87440 Training loss 1.2518372386693954e-05 Validation loss 0.046055130660533905 Accuracy 0.8893750309944153\n",
      "Iteration 87450 Training loss 0.002510929247364402 Validation loss 0.04603218287229538 Accuracy 0.8892500400543213\n",
      "Iteration 87460 Training loss 1.1898341654159594e-05 Validation loss 0.04601390287280083 Accuracy 0.8888750672340393\n",
      "Iteration 87470 Training loss 0.0025108610279858112 Validation loss 0.046034831553697586 Accuracy 0.8888750672340393\n",
      "Iteration 87480 Training loss 0.0025060074403882027 Validation loss 0.04601321369409561 Accuracy 0.8885000348091125\n",
      "Iteration 87490 Training loss 1.0347823263145983e-05 Validation loss 0.045996539294719696 Accuracy 0.8888750672340393\n",
      "Iteration 87500 Training loss 1.218104353029048e-05 Validation loss 0.04602161422371864 Accuracy 0.8892500400543213\n",
      "Iteration 87510 Training loss 0.002510388847440481 Validation loss 0.046008557081222534 Accuracy 0.8896250128746033\n",
      "Iteration 87520 Training loss 1.6426829461124726e-05 Validation loss 0.04602215066552162 Accuracy 0.8891250491142273\n",
      "Iteration 87530 Training loss 8.853264262143057e-06 Validation loss 0.046029232442379 Accuracy 0.8890000581741333\n",
      "Iteration 87540 Training loss 0.002510620281100273 Validation loss 0.0460018552839756 Accuracy 0.8888750672340393\n",
      "Iteration 87550 Training loss 0.0012600275222212076 Validation loss 0.0459902361035347 Accuracy 0.8892500400543213\n",
      "Iteration 87560 Training loss 0.0012574940919876099 Validation loss 0.04600143805146217 Accuracy 0.8891250491142273\n",
      "Iteration 87570 Training loss 0.003760164836421609 Validation loss 0.04599431902170181 Accuracy 0.8890000581741333\n",
      "Iteration 87580 Training loss 1.1682586773531511e-05 Validation loss 0.04600051790475845 Accuracy 0.8892500400543213\n",
      "Iteration 87590 Training loss 0.002508164383471012 Validation loss 0.045987557619810104 Accuracy 0.8892500400543213\n",
      "Iteration 87600 Training loss 0.005010702647268772 Validation loss 0.04599474370479584 Accuracy 0.8892500400543213\n",
      "Iteration 87610 Training loss 0.002511201659217477 Validation loss 0.04597580060362816 Accuracy 0.8892500400543213\n",
      "Iteration 87620 Training loss 0.0012608901597559452 Validation loss 0.04599305987358093 Accuracy 0.8893750309944153\n",
      "Iteration 87630 Training loss 0.0012578449677675962 Validation loss 0.04598543792963028 Accuracy 0.8890000581741333\n",
      "Iteration 87640 Training loss 0.002512193052098155 Validation loss 0.04598500207066536 Accuracy 0.8888750672340393\n",
      "Iteration 87650 Training loss 0.001261493656784296 Validation loss 0.04598594084382057 Accuracy 0.8887500166893005\n",
      "Iteration 87660 Training loss 0.0012569763930514455 Validation loss 0.046003151684999466 Accuracy 0.8883750438690186\n",
      "Iteration 87670 Training loss 0.0012589981779456139 Validation loss 0.046006012707948685 Accuracy 0.8887500166893005\n",
      "Iteration 87680 Training loss 0.003759391140192747 Validation loss 0.04600144550204277 Accuracy 0.8887500166893005\n",
      "Iteration 87690 Training loss 0.0012573696440085769 Validation loss 0.045987796038389206 Accuracy 0.8887500166893005\n",
      "Iteration 87700 Training loss 0.0012599476613104343 Validation loss 0.0460161454975605 Accuracy 0.8890000581741333\n",
      "Iteration 87710 Training loss 0.0012619394110515714 Validation loss 0.045999642461538315 Accuracy 0.8890000581741333\n",
      "Iteration 87720 Training loss 0.0025103699881583452 Validation loss 0.04598545283079147 Accuracy 0.8890000581741333\n",
      "Iteration 87730 Training loss 0.0037582165095955133 Validation loss 0.04600418731570244 Accuracy 0.8893750309944153\n",
      "Iteration 87740 Training loss 0.0012622145004570484 Validation loss 0.04598667472600937 Accuracy 0.8892500400543213\n",
      "Iteration 87750 Training loss 0.005011200904846191 Validation loss 0.04598107188940048 Accuracy 0.889750063419342\n",
      "Iteration 87760 Training loss 1.1600567631830927e-05 Validation loss 0.045984722673892975 Accuracy 0.8895000219345093\n",
      "Iteration 87770 Training loss 0.0025098358746618032 Validation loss 0.04599646106362343 Accuracy 0.8896250128746033\n",
      "Iteration 87780 Training loss 1.0102738087880425e-05 Validation loss 0.04597954824566841 Accuracy 0.889875054359436\n",
      "Iteration 87790 Training loss 0.0012620645575225353 Validation loss 0.045982468873262405 Accuracy 0.8892500400543213\n",
      "Iteration 87800 Training loss 0.00875766109675169 Validation loss 0.04599512740969658 Accuracy 0.8896250128746033\n",
      "Iteration 87810 Training loss 1.0454116818436887e-05 Validation loss 0.04599016159772873 Accuracy 0.8895000219345093\n",
      "Iteration 87820 Training loss 0.0012626850511878729 Validation loss 0.045991625636816025 Accuracy 0.889750063419342\n",
      "Iteration 87830 Training loss 0.003758074948564172 Validation loss 0.04605316370725632 Accuracy 0.8896250128746033\n",
      "Iteration 87840 Training loss 0.0050065540708601475 Validation loss 0.046015627682209015 Accuracy 0.8891250491142273\n",
      "Iteration 87850 Training loss 0.0012609293917194009 Validation loss 0.04600708186626434 Accuracy 0.8895000219345093\n",
      "Iteration 87860 Training loss 1.1962016287725419e-05 Validation loss 0.04602375254034996 Accuracy 0.889750063419342\n",
      "Iteration 87870 Training loss 8.52126413519727e-06 Validation loss 0.04601794481277466 Accuracy 0.8892500400543213\n",
      "Iteration 87880 Training loss 0.0012594095896929502 Validation loss 0.04599958285689354 Accuracy 0.8896250128746033\n",
      "Iteration 87890 Training loss 7.220936822704971e-06 Validation loss 0.04601838067173958 Accuracy 0.8887500166893005\n",
      "Iteration 87900 Training loss 0.002509242622181773 Validation loss 0.04600052908062935 Accuracy 0.8888750672340393\n",
      "Iteration 87910 Training loss 0.003762237261980772 Validation loss 0.046010490506887436 Accuracy 0.8887500166893005\n",
      "Iteration 87920 Training loss 0.0012583605712279677 Validation loss 0.04599827527999878 Accuracy 0.8890000581741333\n",
      "Iteration 87930 Training loss 9.504793524683919e-06 Validation loss 0.046001918613910675 Accuracy 0.8892500400543213\n",
      "Iteration 87940 Training loss 1.1580527825572062e-05 Validation loss 0.045996714383363724 Accuracy 0.8895000219345093\n",
      "Iteration 87950 Training loss 1.0787873179651797e-05 Validation loss 0.046000778675079346 Accuracy 0.8892500400543213\n",
      "Iteration 87960 Training loss 1.5882878869888373e-05 Validation loss 0.04599803313612938 Accuracy 0.8893750309944153\n",
      "Iteration 87970 Training loss 0.002506809774786234 Validation loss 0.04599081724882126 Accuracy 0.8896250128746033\n",
      "Iteration 87980 Training loss 1.7040903912857175e-05 Validation loss 0.045969028025865555 Accuracy 0.8890000581741333\n",
      "Iteration 87990 Training loss 0.0012588533572852612 Validation loss 0.04598940536379814 Accuracy 0.8888750672340393\n",
      "Iteration 88000 Training loss 0.0050087738782167435 Validation loss 0.046000413596630096 Accuracy 0.8888750672340393\n",
      "Iteration 88010 Training loss 1.51508556882618e-05 Validation loss 0.046022623777389526 Accuracy 0.8887500166893005\n",
      "Iteration 88020 Training loss 0.0012614838778972626 Validation loss 0.045995958149433136 Accuracy 0.8892500400543213\n",
      "Iteration 88030 Training loss 0.001260728226043284 Validation loss 0.04600821062922478 Accuracy 0.8890000581741333\n",
      "Iteration 88040 Training loss 0.0012591843260452151 Validation loss 0.046006035059690475 Accuracy 0.8888750672340393\n",
      "Iteration 88050 Training loss 0.0025116221513599157 Validation loss 0.04602091386914253 Accuracy 0.8887500166893005\n",
      "Iteration 88060 Training loss 0.00125891191419214 Validation loss 0.045987848192453384 Accuracy 0.8887500166893005\n",
      "Iteration 88070 Training loss 0.0012584490468725562 Validation loss 0.04599624127149582 Accuracy 0.8891250491142273\n",
      "Iteration 88080 Training loss 1.0574281986919232e-05 Validation loss 0.045982327312231064 Accuracy 0.8892500400543213\n",
      "Iteration 88090 Training loss 0.0037580356001853943 Validation loss 0.046004630625247955 Accuracy 0.8893750309944153\n",
      "Iteration 88100 Training loss 8.573898412578274e-06 Validation loss 0.04600304737687111 Accuracy 0.8892500400543213\n",
      "Iteration 88110 Training loss 0.0025082649663090706 Validation loss 0.04599326476454735 Accuracy 0.8896250128746033\n",
      "Iteration 88120 Training loss 0.0025065510999411345 Validation loss 0.04601056128740311 Accuracy 0.8892500400543213\n",
      "Iteration 88130 Training loss 0.0012574882712215185 Validation loss 0.04600721225142479 Accuracy 0.8893750309944153\n",
      "Iteration 88140 Training loss 9.816356396186166e-06 Validation loss 0.046016812324523926 Accuracy 0.8895000219345093\n",
      "Iteration 88150 Training loss 0.0012659531785175204 Validation loss 0.045997969806194305 Accuracy 0.8896250128746033\n",
      "Iteration 88160 Training loss 0.005008848384022713 Validation loss 0.04601361230015755 Accuracy 0.8895000219345093\n",
      "Iteration 88170 Training loss 0.0012594208819791675 Validation loss 0.04599267616868019 Accuracy 0.8895000219345093\n",
      "Iteration 88180 Training loss 0.0025109355337917805 Validation loss 0.04598517715930939 Accuracy 0.8895000219345093\n",
      "Iteration 88190 Training loss 0.0012596223969012499 Validation loss 0.045995768159627914 Accuracy 0.8892500400543213\n",
      "Iteration 88200 Training loss 0.00126081018242985 Validation loss 0.04598121717572212 Accuracy 0.8895000219345093\n",
      "Iteration 88210 Training loss 0.0012583596399053931 Validation loss 0.045997507870197296 Accuracy 0.8893750309944153\n",
      "Iteration 88220 Training loss 0.0012601077323779464 Validation loss 0.04599948972463608 Accuracy 0.8892500400543213\n",
      "Iteration 88230 Training loss 0.0025091110728681087 Validation loss 0.04601144790649414 Accuracy 0.8896250128746033\n",
      "Iteration 88240 Training loss 0.0012590353144332767 Validation loss 0.0460137277841568 Accuracy 0.8896250128746033\n",
      "Iteration 88250 Training loss 0.0012582194758579135 Validation loss 0.046000268310308456 Accuracy 0.8895000219345093\n",
      "Iteration 88260 Training loss 0.0012577716261148453 Validation loss 0.04598962143063545 Accuracy 0.8895000219345093\n",
      "Iteration 88270 Training loss 0.001261663157492876 Validation loss 0.04597041755914688 Accuracy 0.8895000219345093\n",
      "Iteration 88280 Training loss 1.0132449460797943e-05 Validation loss 0.04597591608762741 Accuracy 0.889750063419342\n",
      "Iteration 88290 Training loss 1.0849912541743834e-05 Validation loss 0.04597019404172897 Accuracy 0.8895000219345093\n",
      "Iteration 88300 Training loss 0.003757748520001769 Validation loss 0.04600371792912483 Accuracy 0.8892500400543213\n",
      "Iteration 88310 Training loss 0.0012592057464644313 Validation loss 0.04596893489360809 Accuracy 0.8891250491142273\n",
      "Iteration 88320 Training loss 0.0012590208789333701 Validation loss 0.04595905542373657 Accuracy 0.8891250491142273\n",
      "Iteration 88330 Training loss 1.4278206435847096e-05 Validation loss 0.045965686440467834 Accuracy 0.8892500400543213\n",
      "Iteration 88340 Training loss 0.0012587581295520067 Validation loss 0.045960839837789536 Accuracy 0.8892500400543213\n",
      "Iteration 88350 Training loss 0.0012620242778211832 Validation loss 0.04597286507487297 Accuracy 0.8896250128746033\n",
      "Iteration 88360 Training loss 9.634984962758608e-06 Validation loss 0.045960888266563416 Accuracy 0.889750063419342\n",
      "Iteration 88370 Training loss 0.0037570036947727203 Validation loss 0.04597960785031319 Accuracy 0.889875054359436\n",
      "Iteration 88380 Training loss 0.001258794218301773 Validation loss 0.04599236696958542 Accuracy 0.889750063419342\n",
      "Iteration 88390 Training loss 9.216974831360858e-06 Validation loss 0.04599218815565109 Accuracy 0.8895000219345093\n",
      "Iteration 88400 Training loss 0.0012619117042049766 Validation loss 0.04597104713320732 Accuracy 0.8895000219345093\n",
      "Iteration 88410 Training loss 0.0012620724737644196 Validation loss 0.045977503061294556 Accuracy 0.8896250128746033\n",
      "Iteration 88420 Training loss 0.0012708366848528385 Validation loss 0.04599989578127861 Accuracy 0.8892500400543213\n",
      "Iteration 88430 Training loss 0.0037607119884341955 Validation loss 0.0460246205329895 Accuracy 0.8892500400543213\n",
      "Iteration 88440 Training loss 1.4777187971048988e-05 Validation loss 0.04601401463150978 Accuracy 0.8892500400543213\n",
      "Iteration 88450 Training loss 0.0012590715195983648 Validation loss 0.04600318521261215 Accuracy 0.8893750309944153\n",
      "Iteration 88460 Training loss 0.00375926960259676 Validation loss 0.04599037766456604 Accuracy 0.8893750309944153\n",
      "Iteration 88470 Training loss 9.1252741185599e-06 Validation loss 0.045997682958841324 Accuracy 0.8893750309944153\n",
      "Iteration 88480 Training loss 0.002509529236704111 Validation loss 0.046006496995687485 Accuracy 0.8895000219345093\n",
      "Iteration 88490 Training loss 0.0025109765119850636 Validation loss 0.046009138226509094 Accuracy 0.8893750309944153\n",
      "Iteration 88500 Training loss 0.0025130563881248236 Validation loss 0.04601669684052467 Accuracy 0.8896250128746033\n",
      "Iteration 88510 Training loss 0.0012611481361091137 Validation loss 0.046022575348615646 Accuracy 0.8892500400543213\n",
      "Iteration 88520 Training loss 0.0037590800784528255 Validation loss 0.04601682350039482 Accuracy 0.8891250491142273\n",
      "Iteration 88530 Training loss 0.0012601411435753107 Validation loss 0.04600802809000015 Accuracy 0.8892500400543213\n",
      "Iteration 88540 Training loss 0.0037599890492856503 Validation loss 0.04598619043827057 Accuracy 0.8893750309944153\n",
      "Iteration 88550 Training loss 1.031027113640448e-05 Validation loss 0.04597727581858635 Accuracy 0.8893750309944153\n",
      "Iteration 88560 Training loss 0.002509542042389512 Validation loss 0.046013105660676956 Accuracy 0.8893750309944153\n",
      "Iteration 88570 Training loss 7.192990779003594e-06 Validation loss 0.046005602926015854 Accuracy 0.8891250491142273\n",
      "Iteration 88580 Training loss 0.0012597426539286971 Validation loss 0.04602298140525818 Accuracy 0.8895000219345093\n",
      "Iteration 88590 Training loss 1.1423399882914964e-05 Validation loss 0.04602684825658798 Accuracy 0.8896250128746033\n",
      "Iteration 88600 Training loss 0.003760275198146701 Validation loss 0.04600086435675621 Accuracy 0.8891250491142273\n",
      "Iteration 88610 Training loss 0.002511418191716075 Validation loss 0.04597323387861252 Accuracy 0.8893750309944153\n",
      "Iteration 88620 Training loss 0.0012579848989844322 Validation loss 0.045968201011419296 Accuracy 0.8891250491142273\n",
      "Iteration 88630 Training loss 0.0012600721092894673 Validation loss 0.046002380549907684 Accuracy 0.8891250491142273\n",
      "Iteration 88640 Training loss 0.0025118538178503513 Validation loss 0.046016793698072433 Accuracy 0.8893750309944153\n",
      "Iteration 88650 Training loss 1.1675692803692073e-05 Validation loss 0.04599994793534279 Accuracy 0.8895000219345093\n",
      "Iteration 88660 Training loss 0.0025105681270360947 Validation loss 0.0459953136742115 Accuracy 0.8895000219345093\n",
      "Iteration 88670 Training loss 9.45849205891136e-06 Validation loss 0.04596750810742378 Accuracy 0.889750063419342\n",
      "Iteration 88680 Training loss 1.1501568224048242e-05 Validation loss 0.04596632346510887 Accuracy 0.8895000219345093\n",
      "Iteration 88690 Training loss 1.366406195302261e-05 Validation loss 0.04598481208086014 Accuracy 0.8895000219345093\n",
      "Iteration 88700 Training loss 0.0012607021490111947 Validation loss 0.04598629102110863 Accuracy 0.8893750309944153\n",
      "Iteration 88710 Training loss 0.0025108763948082924 Validation loss 0.04598955065011978 Accuracy 0.8892500400543213\n",
      "Iteration 88720 Training loss 0.0012621948262676597 Validation loss 0.045987557619810104 Accuracy 0.8891250491142273\n",
      "Iteration 88730 Training loss 1.0267037396261003e-05 Validation loss 0.045986708253622055 Accuracy 0.8891250491142273\n",
      "Iteration 88740 Training loss 9.659273928264156e-06 Validation loss 0.04600301757454872 Accuracy 0.8890000581741333\n",
      "Iteration 88750 Training loss 0.003759980434551835 Validation loss 0.0459735281765461 Accuracy 0.8887500166893005\n",
      "Iteration 88760 Training loss 7.68276640883414e-06 Validation loss 0.04599842429161072 Accuracy 0.8888750672340393\n",
      "Iteration 88770 Training loss 0.0012617658358067274 Validation loss 0.0459885410964489 Accuracy 0.8895000219345093\n",
      "Iteration 88780 Training loss 0.001260716700926423 Validation loss 0.04599913954734802 Accuracy 0.889750063419342\n",
      "Iteration 88790 Training loss 0.0025093250442296267 Validation loss 0.04598134011030197 Accuracy 0.8891250491142273\n",
      "Iteration 88800 Training loss 1.4838268725725356e-05 Validation loss 0.04596160724759102 Accuracy 0.8891250491142273\n",
      "Iteration 88810 Training loss 1.0513633242226206e-05 Validation loss 0.04596969857811928 Accuracy 0.8893750309944153\n",
      "Iteration 88820 Training loss 0.0025077147874981165 Validation loss 0.04596351087093353 Accuracy 0.8896250128746033\n",
      "Iteration 88830 Training loss 1.1618360986176413e-05 Validation loss 0.04595232382416725 Accuracy 0.8891250491142273\n",
      "Iteration 88840 Training loss 0.0012589497491717339 Validation loss 0.04598189517855644 Accuracy 0.8895000219345093\n",
      "Iteration 88850 Training loss 1.0193922207690775e-05 Validation loss 0.04599018394947052 Accuracy 0.8895000219345093\n",
      "Iteration 88860 Training loss 0.0012595608131960034 Validation loss 0.0460086315870285 Accuracy 0.8893750309944153\n",
      "Iteration 88870 Training loss 8.0435829659109e-06 Validation loss 0.045969050377607346 Accuracy 0.8896250128746033\n",
      "Iteration 88880 Training loss 0.0012583824573084712 Validation loss 0.046012476086616516 Accuracy 0.8892500400543213\n",
      "Iteration 88890 Training loss 0.0012576889712363482 Validation loss 0.04599433019757271 Accuracy 0.8893750309944153\n",
      "Iteration 88900 Training loss 0.0012593844439834356 Validation loss 0.04599209129810333 Accuracy 0.8895000219345093\n",
      "Iteration 88910 Training loss 0.0037578518968075514 Validation loss 0.04596739634871483 Accuracy 0.8895000219345093\n",
      "Iteration 88920 Training loss 8.606232768215705e-06 Validation loss 0.04596799239516258 Accuracy 0.8895000219345093\n",
      "Iteration 88930 Training loss 0.0012584073701873422 Validation loss 0.04600053280591965 Accuracy 0.8896250128746033\n",
      "Iteration 88940 Training loss 1.0923245099547785e-05 Validation loss 0.045976314693689346 Accuracy 0.8895000219345093\n",
      "Iteration 88950 Training loss 1.09549555418198e-05 Validation loss 0.04596089571714401 Accuracy 0.8895000219345093\n",
      "Iteration 88960 Training loss 7.894322152424138e-06 Validation loss 0.045964471995830536 Accuracy 0.8896250128746033\n",
      "Iteration 88970 Training loss 0.0012601081980392337 Validation loss 0.04599114507436752 Accuracy 0.889875054359436\n",
      "Iteration 88980 Training loss 0.0012577550951391459 Validation loss 0.045982904732227325 Accuracy 0.889750063419342\n",
      "Iteration 88990 Training loss 1.3994434084452223e-05 Validation loss 0.04597390443086624 Accuracy 0.889875054359436\n",
      "Iteration 89000 Training loss 0.0037583105731755495 Validation loss 0.04595740884542465 Accuracy 0.8895000219345093\n",
      "Iteration 89010 Training loss 9.509356459602714e-06 Validation loss 0.04596230760216713 Accuracy 0.8895000219345093\n",
      "Iteration 89020 Training loss 1.041415634972509e-05 Validation loss 0.045958612114191055 Accuracy 0.8893750309944153\n",
      "Iteration 89030 Training loss 0.0012561788316816092 Validation loss 0.04597601667046547 Accuracy 0.8896250128746033\n",
      "Iteration 89040 Training loss 7.698621629970148e-06 Validation loss 0.04598211869597435 Accuracy 0.889750063419342\n",
      "Iteration 89050 Training loss 1.3996898815094028e-05 Validation loss 0.045968472957611084 Accuracy 0.889750063419342\n",
      "Iteration 89060 Training loss 0.0012607357930392027 Validation loss 0.04597523808479309 Accuracy 0.8895000219345093\n",
      "Iteration 89070 Training loss 0.002512639621272683 Validation loss 0.045963939279317856 Accuracy 0.8896250128746033\n",
      "Iteration 89080 Training loss 0.0012578454334288836 Validation loss 0.045953698456287384 Accuracy 0.8891250491142273\n",
      "Iteration 89090 Training loss 8.876618267095182e-06 Validation loss 0.045975327491760254 Accuracy 0.8890000581741333\n",
      "Iteration 89100 Training loss 9.017189768201206e-06 Validation loss 0.04595775157213211 Accuracy 0.8890000581741333\n",
      "Iteration 89110 Training loss 0.002510510617867112 Validation loss 0.04599776491522789 Accuracy 0.8893750309944153\n",
      "Iteration 89120 Training loss 1.0629681128193624e-05 Validation loss 0.045976825058460236 Accuracy 0.8888750672340393\n",
      "Iteration 89130 Training loss 0.0012601132038980722 Validation loss 0.045969776809215546 Accuracy 0.8895000219345093\n",
      "Iteration 89140 Training loss 0.0012605913216248155 Validation loss 0.04597628489136696 Accuracy 0.8893750309944153\n",
      "Iteration 89150 Training loss 0.0025096971075981855 Validation loss 0.045963358134031296 Accuracy 0.8895000219345093\n",
      "Iteration 89160 Training loss 0.003758491249755025 Validation loss 0.04597561061382294 Accuracy 0.8895000219345093\n",
      "Iteration 89170 Training loss 8.227829312090762e-06 Validation loss 0.04600062593817711 Accuracy 0.890125036239624\n",
      "Iteration 89180 Training loss 8.243961019616108e-06 Validation loss 0.04599407687783241 Accuracy 0.889875054359436\n",
      "Iteration 89190 Training loss 0.001258129719644785 Validation loss 0.04601017385721207 Accuracy 0.8896250128746033\n",
      "Iteration 89200 Training loss 0.0012587570818141103 Validation loss 0.046015817672014236 Accuracy 0.8895000219345093\n",
      "Iteration 89210 Training loss 0.0012590070255100727 Validation loss 0.04601320996880531 Accuracy 0.8896250128746033\n",
      "Iteration 89220 Training loss 0.001260352903045714 Validation loss 0.04598763957619667 Accuracy 0.8893750309944153\n",
      "Iteration 89230 Training loss 0.001260414719581604 Validation loss 0.045975182205438614 Accuracy 0.8895000219345093\n",
      "Iteration 89240 Training loss 0.0012586284428834915 Validation loss 0.04600868374109268 Accuracy 0.889750063419342\n",
      "Iteration 89250 Training loss 0.0025096482131630182 Validation loss 0.04598402976989746 Accuracy 0.8895000219345093\n",
      "Iteration 89260 Training loss 0.003758298698812723 Validation loss 0.04598018154501915 Accuracy 0.8893750309944153\n",
      "Iteration 89270 Training loss 0.0025130456779152155 Validation loss 0.04599502310156822 Accuracy 0.889750063419342\n",
      "Iteration 89280 Training loss 0.0025083257351070642 Validation loss 0.045967597514390945 Accuracy 0.8895000219345093\n",
      "Iteration 89290 Training loss 1.3662745914189145e-05 Validation loss 0.045985251665115356 Accuracy 0.889750063419342\n",
      "Iteration 89300 Training loss 0.0025098135229200125 Validation loss 0.045973047614097595 Accuracy 0.8895000219345093\n",
      "Iteration 89310 Training loss 8.1565431173658e-06 Validation loss 0.04597944766283035 Accuracy 0.8891250491142273\n",
      "Iteration 89320 Training loss 0.0012630868004634976 Validation loss 0.04600799083709717 Accuracy 0.8896250128746033\n",
      "Iteration 89330 Training loss 0.0012576284352689981 Validation loss 0.045988839119672775 Accuracy 0.8895000219345093\n",
      "Iteration 89340 Training loss 0.0012630346463993192 Validation loss 0.0459672287106514 Accuracy 0.8893750309944153\n",
      "Iteration 89350 Training loss 0.0025078337639570236 Validation loss 0.04599098861217499 Accuracy 0.8888750672340393\n",
      "Iteration 89360 Training loss 0.002508473116904497 Validation loss 0.045987870544195175 Accuracy 0.8893750309944153\n",
      "Iteration 89370 Training loss 9.684537872090004e-06 Validation loss 0.04600539430975914 Accuracy 0.8893750309944153\n",
      "Iteration 89380 Training loss 0.0012577272718772292 Validation loss 0.045982711017131805 Accuracy 0.8893750309944153\n",
      "Iteration 89390 Training loss 0.0012609140248969197 Validation loss 0.046000465750694275 Accuracy 0.8891250491142273\n",
      "Iteration 89400 Training loss 0.0012583066709339619 Validation loss 0.04600612074136734 Accuracy 0.8890000581741333\n",
      "Iteration 89410 Training loss 9.910172593663447e-06 Validation loss 0.0460410937666893 Accuracy 0.8892500400543213\n",
      "Iteration 89420 Training loss 1.197959227283718e-05 Validation loss 0.04600606486201286 Accuracy 0.8892500400543213\n",
      "Iteration 89430 Training loss 0.0012600114569067955 Validation loss 0.046006496995687485 Accuracy 0.8891250491142273\n",
      "Iteration 89440 Training loss 0.0012597634922713041 Validation loss 0.04598873481154442 Accuracy 0.8890000581741333\n",
      "Iteration 89450 Training loss 0.0012591370614245534 Validation loss 0.046023108065128326 Accuracy 0.8890000581741333\n",
      "Iteration 89460 Training loss 0.002510279417037964 Validation loss 0.04598035663366318 Accuracy 0.8891250491142273\n",
      "Iteration 89470 Training loss 0.0012618113541975617 Validation loss 0.0459822341799736 Accuracy 0.8891250491142273\n",
      "Iteration 89480 Training loss 0.001263796235434711 Validation loss 0.04599020630121231 Accuracy 0.8893750309944153\n",
      "Iteration 89490 Training loss 9.379044058732688e-06 Validation loss 0.04600074142217636 Accuracy 0.8890000581741333\n",
      "Iteration 89500 Training loss 0.0012657041661441326 Validation loss 0.04601501673460007 Accuracy 0.8890000581741333\n",
      "Iteration 89510 Training loss 0.002509803744032979 Validation loss 0.045994631946086884 Accuracy 0.8888750672340393\n",
      "Iteration 89520 Training loss 0.002509835409000516 Validation loss 0.046001680195331573 Accuracy 0.8890000581741333\n",
      "Iteration 89530 Training loss 0.0012610105331987143 Validation loss 0.04602043703198433 Accuracy 0.8893750309944153\n",
      "Iteration 89540 Training loss 0.001260027987882495 Validation loss 0.04599703475832939 Accuracy 0.8895000219345093\n",
      "Iteration 89550 Training loss 0.0025146747939288616 Validation loss 0.04600530490279198 Accuracy 0.8892500400543213\n",
      "Iteration 89560 Training loss 0.0012592475395649672 Validation loss 0.04600786790251732 Accuracy 0.8896250128746033\n",
      "Iteration 89570 Training loss 0.0025089378468692303 Validation loss 0.0459974966943264 Accuracy 0.8891250491142273\n",
      "Iteration 89580 Training loss 0.002510668011382222 Validation loss 0.0460246205329895 Accuracy 0.8891250491142273\n",
      "Iteration 89590 Training loss 0.002511780709028244 Validation loss 0.04600786045193672 Accuracy 0.8890000581741333\n",
      "Iteration 89600 Training loss 9.488454452366568e-06 Validation loss 0.04601409286260605 Accuracy 0.8887500166893005\n",
      "Iteration 89610 Training loss 0.0012593537103384733 Validation loss 0.046025533229112625 Accuracy 0.8890000581741333\n",
      "Iteration 89620 Training loss 0.003757186932489276 Validation loss 0.046009354293346405 Accuracy 0.8888750672340393\n",
      "Iteration 89630 Training loss 7.595168426632881e-06 Validation loss 0.04601472243666649 Accuracy 0.8888750672340393\n",
      "Iteration 89640 Training loss 0.0012577720917761326 Validation loss 0.046023257076740265 Accuracy 0.8887500166893005\n",
      "Iteration 89650 Training loss 0.001258890493772924 Validation loss 0.04600846394896507 Accuracy 0.8890000581741333\n",
      "Iteration 89660 Training loss 0.0025081527419388294 Validation loss 0.04600336775183678 Accuracy 0.8890000581741333\n",
      "Iteration 89670 Training loss 0.0012588410172611475 Validation loss 0.046022456139326096 Accuracy 0.8895000219345093\n",
      "Iteration 89680 Training loss 8.933540811995044e-06 Validation loss 0.04600796848535538 Accuracy 0.8888750672340393\n",
      "Iteration 89690 Training loss 9.775332728167996e-06 Validation loss 0.046016354113817215 Accuracy 0.8891250491142273\n",
      "Iteration 89700 Training loss 9.122305527853314e-06 Validation loss 0.04601946845650673 Accuracy 0.8890000581741333\n",
      "Iteration 89710 Training loss 0.001260524382814765 Validation loss 0.0460113100707531 Accuracy 0.8891250491142273\n",
      "Iteration 89720 Training loss 0.0012621685164049268 Validation loss 0.04600100964307785 Accuracy 0.8888750672340393\n",
      "Iteration 89730 Training loss 8.637952305434737e-06 Validation loss 0.04600774496793747 Accuracy 0.8890000581741333\n",
      "Iteration 89740 Training loss 0.0012614392908290029 Validation loss 0.046002525836229324 Accuracy 0.8891250491142273\n",
      "Iteration 89750 Training loss 0.0012603318318724632 Validation loss 0.04598548263311386 Accuracy 0.8891250491142273\n",
      "Iteration 89760 Training loss 0.001260662917047739 Validation loss 0.04602064564824104 Accuracy 0.8890000581741333\n",
      "Iteration 89770 Training loss 0.0012570597464218736 Validation loss 0.0459863618016243 Accuracy 0.8888750672340393\n",
      "Iteration 89780 Training loss 8.650328709336463e-06 Validation loss 0.04600025713443756 Accuracy 0.8891250491142273\n",
      "Iteration 89790 Training loss 1.0708214176702313e-05 Validation loss 0.04600059613585472 Accuracy 0.8892500400543213\n",
      "Iteration 89800 Training loss 6.268066954362439e-06 Validation loss 0.04602145776152611 Accuracy 0.8893750309944153\n",
      "Iteration 89810 Training loss 0.0025081245694309473 Validation loss 0.0460396371781826 Accuracy 0.8896250128746033\n",
      "Iteration 89820 Training loss 0.0025089781265705824 Validation loss 0.04599796235561371 Accuracy 0.8888750672340393\n",
      "Iteration 89830 Training loss 0.0012611652491614223 Validation loss 0.04599575698375702 Accuracy 0.8890000581741333\n",
      "Iteration 89840 Training loss 1.1323475519020576e-05 Validation loss 0.0460180826485157 Accuracy 0.8888750672340393\n",
      "Iteration 89850 Training loss 0.0025111346039921045 Validation loss 0.04600372910499573 Accuracy 0.8895000219345093\n",
      "Iteration 89860 Training loss 0.002510389080271125 Validation loss 0.046024683862924576 Accuracy 0.8895000219345093\n",
      "Iteration 89870 Training loss 0.0012578043388202786 Validation loss 0.04601673409342766 Accuracy 0.8887500166893005\n",
      "Iteration 89880 Training loss 0.002506790915504098 Validation loss 0.04600967466831207 Accuracy 0.8888750672340393\n",
      "Iteration 89890 Training loss 0.0025142445228993893 Validation loss 0.04600704461336136 Accuracy 0.8890000581741333\n",
      "Iteration 89900 Training loss 8.693124073033687e-06 Validation loss 0.04601871967315674 Accuracy 0.8890000581741333\n",
      "Iteration 89910 Training loss 1.3395945643424056e-05 Validation loss 0.04600141942501068 Accuracy 0.8888750672340393\n",
      "Iteration 89920 Training loss 0.0025102472864091396 Validation loss 0.046008653938770294 Accuracy 0.8893750309944153\n",
      "Iteration 89930 Training loss 9.833345757215284e-06 Validation loss 0.04599739983677864 Accuracy 0.8890000581741333\n",
      "Iteration 89940 Training loss 0.002510928548872471 Validation loss 0.0460054874420166 Accuracy 0.8893750309944153\n",
      "Iteration 89950 Training loss 0.0012578772148117423 Validation loss 0.0460125096142292 Accuracy 0.8892500400543213\n",
      "Iteration 89960 Training loss 0.0025085757952183485 Validation loss 0.04600539430975914 Accuracy 0.8891250491142273\n",
      "Iteration 89970 Training loss 0.0012598010944202542 Validation loss 0.04599424824118614 Accuracy 0.8888750672340393\n",
      "Iteration 89980 Training loss 0.002512768842279911 Validation loss 0.04601093754172325 Accuracy 0.8896250128746033\n",
      "Iteration 89990 Training loss 1.020182935462799e-05 Validation loss 0.046018410474061966 Accuracy 0.8890000581741333\n",
      "Iteration 90000 Training loss 0.002510039834305644 Validation loss 0.0460028313100338 Accuracy 0.8895000219345093\n",
      "Iteration 90010 Training loss 0.002508960198611021 Validation loss 0.046033114194869995 Accuracy 0.8892500400543213\n",
      "Iteration 90020 Training loss 9.027842679643072e-06 Validation loss 0.04601816087961197 Accuracy 0.8895000219345093\n",
      "Iteration 90030 Training loss 0.0012594739673659205 Validation loss 0.04602038860321045 Accuracy 0.8892500400543213\n",
      "Iteration 90040 Training loss 0.0025093634612858295 Validation loss 0.046022363007068634 Accuracy 0.8892500400543213\n",
      "Iteration 90050 Training loss 0.003766387701034546 Validation loss 0.04603034257888794 Accuracy 0.8896250128746033\n",
      "Iteration 90060 Training loss 7.586937499581836e-06 Validation loss 0.04601543769240379 Accuracy 0.8888750672340393\n",
      "Iteration 90070 Training loss 0.0012604694347828627 Validation loss 0.04601695388555527 Accuracy 0.8888750672340393\n",
      "Iteration 90080 Training loss 7.494030342058977e-06 Validation loss 0.04603097587823868 Accuracy 0.8892500400543213\n",
      "Iteration 90090 Training loss 0.00251022819429636 Validation loss 0.04601068049669266 Accuracy 0.8895000219345093\n",
      "Iteration 90100 Training loss 0.002509098034352064 Validation loss 0.046046365052461624 Accuracy 0.8891250491142273\n",
      "Iteration 90110 Training loss 1.0005499461840373e-05 Validation loss 0.04603198543190956 Accuracy 0.8892500400543213\n",
      "Iteration 90120 Training loss 0.0037586288526654243 Validation loss 0.04602213203907013 Accuracy 0.8890000581741333\n",
      "Iteration 90130 Training loss 0.0037594607565551996 Validation loss 0.046009499579668045 Accuracy 0.8891250491142273\n",
      "Iteration 90140 Training loss 1.2638926818908658e-05 Validation loss 0.04601646959781647 Accuracy 0.8891250491142273\n",
      "Iteration 90150 Training loss 1.1199829714314546e-05 Validation loss 0.04603027552366257 Accuracy 0.8888750672340393\n",
      "Iteration 90160 Training loss 0.0012570546241477132 Validation loss 0.046039532870054245 Accuracy 0.8890000581741333\n",
      "Iteration 90170 Training loss 0.0012587170349434018 Validation loss 0.046008139848709106 Accuracy 0.8891250491142273\n",
      "Iteration 90180 Training loss 0.0037588952109217644 Validation loss 0.046004462987184525 Accuracy 0.8890000581741333\n",
      "Iteration 90190 Training loss 0.002507390221580863 Validation loss 0.04600782319903374 Accuracy 0.8890000581741333\n",
      "Iteration 90200 Training loss 0.001259561744518578 Validation loss 0.046004533767700195 Accuracy 0.8892500400543213\n",
      "Iteration 90210 Training loss 1.2276819688850082e-05 Validation loss 0.04603936895728111 Accuracy 0.8891250491142273\n",
      "Iteration 90220 Training loss 0.0012607065727934241 Validation loss 0.04602683335542679 Accuracy 0.8895000219345093\n",
      "Iteration 90230 Training loss 0.002512195147573948 Validation loss 0.04600464552640915 Accuracy 0.8892500400543213\n",
      "Iteration 90240 Training loss 0.0025071310810744762 Validation loss 0.0460091158747673 Accuracy 0.8892500400543213\n",
      "Iteration 90250 Training loss 0.0012614610604941845 Validation loss 0.04604416713118553 Accuracy 0.8895000219345093\n",
      "Iteration 90260 Training loss 0.0012606681557372212 Validation loss 0.04601069912314415 Accuracy 0.8893750309944153\n",
      "Iteration 90270 Training loss 0.0012612405698746443 Validation loss 0.04600363224744797 Accuracy 0.8893750309944153\n",
      "Iteration 90280 Training loss 0.001259497250430286 Validation loss 0.04600682109594345 Accuracy 0.8893750309944153\n",
      "Iteration 90290 Training loss 9.373031389259268e-06 Validation loss 0.04600457847118378 Accuracy 0.8896250128746033\n",
      "Iteration 90300 Training loss 0.0025092116557061672 Validation loss 0.04602103680372238 Accuracy 0.8893750309944153\n",
      "Iteration 90310 Training loss 0.0012592817656695843 Validation loss 0.04601080343127251 Accuracy 0.8895000219345093\n",
      "Iteration 90320 Training loss 0.001259038457646966 Validation loss 0.04597931727766991 Accuracy 0.8887500166893005\n",
      "Iteration 90330 Training loss 1.0510104402783327e-05 Validation loss 0.04602304473519325 Accuracy 0.8895000219345093\n",
      "Iteration 90340 Training loss 0.001259245560504496 Validation loss 0.04602382704615593 Accuracy 0.8891250491142273\n",
      "Iteration 90350 Training loss 0.0012594128493219614 Validation loss 0.046013448387384415 Accuracy 0.8893750309944153\n",
      "Iteration 90360 Training loss 1.6797463104012422e-05 Validation loss 0.046026963740587234 Accuracy 0.8891250491142273\n",
      "Iteration 90370 Training loss 0.0012575289001688361 Validation loss 0.046016592532396317 Accuracy 0.8891250491142273\n",
      "Iteration 90380 Training loss 8.304296898131724e-06 Validation loss 0.04604927450418472 Accuracy 0.8891250491142273\n",
      "Iteration 90390 Training loss 0.002512968610972166 Validation loss 0.04603823646903038 Accuracy 0.8890000581741333\n",
      "Iteration 90400 Training loss 6.739544915035367e-06 Validation loss 0.04602580517530441 Accuracy 0.8891250491142273\n",
      "Iteration 90410 Training loss 7.99375629867427e-06 Validation loss 0.04601600021123886 Accuracy 0.8890000581741333\n",
      "Iteration 90420 Training loss 0.0012585456715896726 Validation loss 0.046004049479961395 Accuracy 0.8888750672340393\n",
      "Iteration 90430 Training loss 9.942983524524607e-06 Validation loss 0.0460258349776268 Accuracy 0.8890000581741333\n",
      "Iteration 90440 Training loss 0.002509231446310878 Validation loss 0.046034470200538635 Accuracy 0.8891250491142273\n",
      "Iteration 90450 Training loss 0.0025115511380136013 Validation loss 0.04603354632854462 Accuracy 0.8890000581741333\n",
      "Iteration 90460 Training loss 0.0012601955095306039 Validation loss 0.04601908475160599 Accuracy 0.8891250491142273\n",
      "Iteration 90470 Training loss 1.1821948646684177e-05 Validation loss 0.04603489488363266 Accuracy 0.8890000581741333\n",
      "Iteration 90480 Training loss 0.0012618363834917545 Validation loss 0.04602677375078201 Accuracy 0.8893750309944153\n",
      "Iteration 90490 Training loss 8.79065555636771e-06 Validation loss 0.046058472245931625 Accuracy 0.8895000219345093\n",
      "Iteration 90500 Training loss 0.002508776029571891 Validation loss 0.04602205753326416 Accuracy 0.8890000581741333\n",
      "Iteration 90510 Training loss 0.0037589706480503082 Validation loss 0.046036697924137115 Accuracy 0.8890000581741333\n",
      "Iteration 90520 Training loss 0.0012609271798282862 Validation loss 0.04601701721549034 Accuracy 0.8888750672340393\n",
      "Iteration 90530 Training loss 7.791278221702669e-06 Validation loss 0.04602822661399841 Accuracy 0.8887500166893005\n",
      "Iteration 90540 Training loss 7.720628673268948e-06 Validation loss 0.046019453555345535 Accuracy 0.8886250257492065\n",
      "Iteration 90550 Training loss 0.0025111085269600153 Validation loss 0.04602179303765297 Accuracy 0.8888750672340393\n",
      "Iteration 90560 Training loss 0.0012580733746290207 Validation loss 0.046025484800338745 Accuracy 0.8887500166893005\n",
      "Iteration 90570 Training loss 7.341921445913613e-06 Validation loss 0.046028099954128265 Accuracy 0.8888750672340393\n",
      "Iteration 90580 Training loss 0.005007592495530844 Validation loss 0.04601601883769035 Accuracy 0.8887500166893005\n",
      "Iteration 90590 Training loss 0.00375807611271739 Validation loss 0.04604887589812279 Accuracy 0.8887500166893005\n",
      "Iteration 90600 Training loss 0.001261353143490851 Validation loss 0.046036940068006516 Accuracy 0.8888750672340393\n",
      "Iteration 90610 Training loss 9.304072591476142e-06 Validation loss 0.0460401214659214 Accuracy 0.8893750309944153\n",
      "Iteration 90620 Training loss 0.0012570084072649479 Validation loss 0.046041104942560196 Accuracy 0.8888750672340393\n",
      "Iteration 90630 Training loss 1.0121311788680032e-05 Validation loss 0.04603604972362518 Accuracy 0.8891250491142273\n",
      "Iteration 90640 Training loss 0.0012594381114467978 Validation loss 0.04602710157632828 Accuracy 0.8890000581741333\n",
      "Iteration 90650 Training loss 0.0012591193662956357 Validation loss 0.04603620991110802 Accuracy 0.8890000581741333\n",
      "Iteration 90660 Training loss 1.021713705995353e-05 Validation loss 0.046058546751737595 Accuracy 0.8891250491142273\n",
      "Iteration 90670 Training loss 0.003758752252906561 Validation loss 0.04603756219148636 Accuracy 0.8890000581741333\n",
      "Iteration 90680 Training loss 1.0001025657402351e-05 Validation loss 0.04606960341334343 Accuracy 0.8892500400543213\n",
      "Iteration 90690 Training loss 1.0438753633934539e-05 Validation loss 0.04602427780628204 Accuracy 0.8891250491142273\n",
      "Iteration 90700 Training loss 7.402757546515204e-06 Validation loss 0.04602910205721855 Accuracy 0.8891250491142273\n",
      "Iteration 90710 Training loss 0.0012600155314430594 Validation loss 0.04602395370602608 Accuracy 0.8887500166893005\n",
      "Iteration 90720 Training loss 0.0012588290264829993 Validation loss 0.046007633209228516 Accuracy 0.8892500400543213\n",
      "Iteration 90730 Training loss 0.0012631658464670181 Validation loss 0.046010684221982956 Accuracy 0.8892500400543213\n",
      "Iteration 90740 Training loss 0.002509735757485032 Validation loss 0.04602513089776039 Accuracy 0.8893750309944153\n",
      "Iteration 90750 Training loss 8.604652975918725e-06 Validation loss 0.04602406919002533 Accuracy 0.8895000219345093\n",
      "Iteration 90760 Training loss 0.0012597356690093875 Validation loss 0.0460529588162899 Accuracy 0.8893750309944153\n",
      "Iteration 90770 Training loss 0.0012588583631440997 Validation loss 0.046035610139369965 Accuracy 0.8892500400543213\n",
      "Iteration 90780 Training loss 0.001258368487469852 Validation loss 0.04602355137467384 Accuracy 0.8888750672340393\n",
      "Iteration 90790 Training loss 0.001258997479453683 Validation loss 0.04601886123418808 Accuracy 0.8892500400543213\n",
      "Iteration 90800 Training loss 9.578878234606236e-06 Validation loss 0.04599623382091522 Accuracy 0.8891250491142273\n",
      "Iteration 90810 Training loss 0.002512450562790036 Validation loss 0.045990120619535446 Accuracy 0.8893750309944153\n",
      "Iteration 90820 Training loss 0.001259466866031289 Validation loss 0.04601268470287323 Accuracy 0.8896250128746033\n",
      "Iteration 90830 Training loss 1.0122304047399666e-05 Validation loss 0.04600264132022858 Accuracy 0.8890000581741333\n",
      "Iteration 90840 Training loss 0.0012594733852893114 Validation loss 0.04599316418170929 Accuracy 0.8892500400543213\n",
      "Iteration 90850 Training loss 0.0025087136309593916 Validation loss 0.0460069440305233 Accuracy 0.8891250491142273\n",
      "Iteration 90860 Training loss 0.0025077550671994686 Validation loss 0.04601415619254112 Accuracy 0.8891250491142273\n",
      "Iteration 90870 Training loss 0.003762217704206705 Validation loss 0.046022675931453705 Accuracy 0.8892500400543213\n",
      "Iteration 90880 Training loss 0.0012609136756509542 Validation loss 0.04599934443831444 Accuracy 0.8895000219345093\n",
      "Iteration 90890 Training loss 1.0365578418713994e-05 Validation loss 0.04600647836923599 Accuracy 0.8893750309944153\n",
      "Iteration 90900 Training loss 8.846051059663296e-06 Validation loss 0.0460222102701664 Accuracy 0.8895000219345093\n",
      "Iteration 90910 Training loss 0.0012586330994963646 Validation loss 0.04605552926659584 Accuracy 0.8896250128746033\n",
      "Iteration 90920 Training loss 0.002509668469429016 Validation loss 0.04601801559329033 Accuracy 0.8890000581741333\n",
      "Iteration 90930 Training loss 0.0025096102617681026 Validation loss 0.04600042477250099 Accuracy 0.8891250491142273\n",
      "Iteration 90940 Training loss 0.0012592084240168333 Validation loss 0.0460103414952755 Accuracy 0.8892500400543213\n",
      "Iteration 90950 Training loss 0.001257562660612166 Validation loss 0.046026624739170074 Accuracy 0.8891250491142273\n",
      "Iteration 90960 Training loss 0.002512473613023758 Validation loss 0.046016544103622437 Accuracy 0.8890000581741333\n",
      "Iteration 90970 Training loss 9.792139280762058e-06 Validation loss 0.046017128974199295 Accuracy 0.8891250491142273\n",
      "Iteration 90980 Training loss 8.025953320611734e-06 Validation loss 0.046020664274692535 Accuracy 0.8893750309944153\n",
      "Iteration 90990 Training loss 0.0037571247667074203 Validation loss 0.04601425305008888 Accuracy 0.8890000581741333\n",
      "Iteration 91000 Training loss 0.0012594180880114436 Validation loss 0.045998621731996536 Accuracy 0.8888750672340393\n",
      "Iteration 91010 Training loss 8.394039468839765e-06 Validation loss 0.04599875211715698 Accuracy 0.8891250491142273\n",
      "Iteration 91020 Training loss 0.0012582745403051376 Validation loss 0.04600095748901367 Accuracy 0.8890000581741333\n",
      "Iteration 91030 Training loss 9.101254363486078e-06 Validation loss 0.04600897431373596 Accuracy 0.8891250491142273\n",
      "Iteration 91040 Training loss 7.896093848103192e-06 Validation loss 0.04600336030125618 Accuracy 0.8888750672340393\n",
      "Iteration 91050 Training loss 0.001263582380488515 Validation loss 0.046021994203329086 Accuracy 0.8886250257492065\n",
      "Iteration 91060 Training loss 1.0016964552050922e-05 Validation loss 0.0460079051554203 Accuracy 0.8888750672340393\n",
      "Iteration 91070 Training loss 6.641889740421902e-06 Validation loss 0.046034254133701324 Accuracy 0.8891250491142273\n",
      "Iteration 91080 Training loss 0.0012594892177730799 Validation loss 0.0459984689950943 Accuracy 0.8890000581741333\n",
      "Iteration 91090 Training loss 0.0012611413840204477 Validation loss 0.046019844710826874 Accuracy 0.8890000581741333\n",
      "Iteration 91100 Training loss 0.002508376259356737 Validation loss 0.046019669622182846 Accuracy 0.8890000581741333\n",
      "Iteration 91110 Training loss 8.313057151099201e-06 Validation loss 0.04604131355881691 Accuracy 0.8890000581741333\n",
      "Iteration 91120 Training loss 9.274927833757829e-06 Validation loss 0.04601644352078438 Accuracy 0.8892500400543213\n",
      "Iteration 91130 Training loss 0.0012571531115099788 Validation loss 0.04600685462355614 Accuracy 0.8887500166893005\n",
      "Iteration 91140 Training loss 0.00125817209482193 Validation loss 0.046042244881391525 Accuracy 0.8890000581741333\n",
      "Iteration 91150 Training loss 0.0037601192016154528 Validation loss 0.04604300856590271 Accuracy 0.8891250491142273\n",
      "Iteration 91160 Training loss 0.0012634886661544442 Validation loss 0.046050217002630234 Accuracy 0.8886250257492065\n",
      "Iteration 91170 Training loss 0.0012570510152727365 Validation loss 0.04604639858007431 Accuracy 0.8888750672340393\n",
      "Iteration 91180 Training loss 0.002507791155949235 Validation loss 0.046027474105358124 Accuracy 0.8890000581741333\n",
      "Iteration 91190 Training loss 1.1716908375092316e-05 Validation loss 0.04602973908185959 Accuracy 0.8887500166893005\n",
      "Iteration 91200 Training loss 8.511456144333351e-06 Validation loss 0.04605057090520859 Accuracy 0.8888750672340393\n",
      "Iteration 91210 Training loss 0.0025069492403417826 Validation loss 0.046015817672014236 Accuracy 0.8888750672340393\n",
      "Iteration 91220 Training loss 0.001262577949091792 Validation loss 0.04600021243095398 Accuracy 0.8888750672340393\n",
      "Iteration 91230 Training loss 0.0037576602771878242 Validation loss 0.046043943613767624 Accuracy 0.8891250491142273\n",
      "Iteration 91240 Training loss 0.0012630324345082045 Validation loss 0.04604092612862587 Accuracy 0.8890000581741333\n",
      "Iteration 91250 Training loss 0.0025089578703045845 Validation loss 0.04604971781373024 Accuracy 0.8890000581741333\n",
      "Iteration 91260 Training loss 8.319292646774556e-06 Validation loss 0.04603048786520958 Accuracy 0.8888750672340393\n",
      "Iteration 91270 Training loss 0.002513749059289694 Validation loss 0.046034716069698334 Accuracy 0.8890000581741333\n",
      "Iteration 91280 Training loss 0.001259888056665659 Validation loss 0.04601734131574631 Accuracy 0.8890000581741333\n",
      "Iteration 91290 Training loss 0.002508401172235608 Validation loss 0.04603758454322815 Accuracy 0.8887500166893005\n",
      "Iteration 91300 Training loss 0.0012583828065544367 Validation loss 0.04601742699742317 Accuracy 0.8886250257492065\n",
      "Iteration 91310 Training loss 0.0012601169291883707 Validation loss 0.046029992401599884 Accuracy 0.8887500166893005\n",
      "Iteration 91320 Training loss 8.282988346763887e-06 Validation loss 0.04602738469839096 Accuracy 0.8890000581741333\n",
      "Iteration 91330 Training loss 0.0012587627861648798 Validation loss 0.046011727303266525 Accuracy 0.8890000581741333\n",
      "Iteration 91340 Training loss 0.001257990370504558 Validation loss 0.04603223875164986 Accuracy 0.8892500400543213\n",
      "Iteration 91350 Training loss 0.0012583405477926135 Validation loss 0.046031404286623 Accuracy 0.8892500400543213\n",
      "Iteration 91360 Training loss 0.0025078391190618277 Validation loss 0.04604494571685791 Accuracy 0.8892500400543213\n",
      "Iteration 91370 Training loss 0.0025148987770080566 Validation loss 0.04604021832346916 Accuracy 0.8888750672340393\n",
      "Iteration 91380 Training loss 0.002508771140128374 Validation loss 0.04605851694941521 Accuracy 0.8890000581741333\n",
      "Iteration 91390 Training loss 1.1247964721405879e-05 Validation loss 0.04604516178369522 Accuracy 0.8888750672340393\n",
      "Iteration 91400 Training loss 0.0050092642195522785 Validation loss 0.046025585383176804 Accuracy 0.8890000581741333\n",
      "Iteration 91410 Training loss 8.95809534995351e-06 Validation loss 0.046033747494220734 Accuracy 0.8890000581741333\n",
      "Iteration 91420 Training loss 8.906055882107466e-06 Validation loss 0.046039216220378876 Accuracy 0.8891250491142273\n",
      "Iteration 91430 Training loss 0.0012581342598423362 Validation loss 0.046037860214710236 Accuracy 0.8890000581741333\n",
      "Iteration 91440 Training loss 0.0012586925877258182 Validation loss 0.04603831470012665 Accuracy 0.8887500166893005\n",
      "Iteration 91450 Training loss 0.0012604983057826757 Validation loss 0.04601769521832466 Accuracy 0.8892500400543213\n",
      "Iteration 91460 Training loss 0.002510077552869916 Validation loss 0.04600124433636665 Accuracy 0.8888750672340393\n",
      "Iteration 91470 Training loss 0.0012584020150825381 Validation loss 0.046027544885873795 Accuracy 0.8892500400543213\n",
      "Iteration 91480 Training loss 7.134253337426344e-06 Validation loss 0.04600249603390694 Accuracy 0.8888750672340393\n",
      "Iteration 91490 Training loss 1.0861163900699466e-05 Validation loss 0.04597543179988861 Accuracy 0.8890000581741333\n",
      "Iteration 91500 Training loss 0.0012582213385030627 Validation loss 0.04601122438907623 Accuracy 0.8887500166893005\n",
      "Iteration 91510 Training loss 0.001260617165826261 Validation loss 0.046022117137908936 Accuracy 0.8888750672340393\n",
      "Iteration 91520 Training loss 0.0037597648333758116 Validation loss 0.04601167514920235 Accuracy 0.8888750672340393\n",
      "Iteration 91530 Training loss 9.76679348241305e-06 Validation loss 0.046025630086660385 Accuracy 0.8891250491142273\n",
      "Iteration 91540 Training loss 1.0254789231112227e-05 Validation loss 0.046043023467063904 Accuracy 0.8888750672340393\n",
      "Iteration 91550 Training loss 9.375748049933463e-06 Validation loss 0.046026527881622314 Accuracy 0.8888750672340393\n",
      "Iteration 91560 Training loss 0.0037575410678982735 Validation loss 0.046031419187784195 Accuracy 0.8888750672340393\n",
      "Iteration 91570 Training loss 0.0025101436767727137 Validation loss 0.04603966698050499 Accuracy 0.8890000581741333\n",
      "Iteration 91580 Training loss 7.225451099657221e-06 Validation loss 0.046010952442884445 Accuracy 0.8886250257492065\n",
      "Iteration 91590 Training loss 0.0025105841923505068 Validation loss 0.046038996428251266 Accuracy 0.8887500166893005\n",
      "Iteration 91600 Training loss 1.0005711374105886e-05 Validation loss 0.04600551724433899 Accuracy 0.8888750672340393\n",
      "Iteration 91610 Training loss 0.0012576922308653593 Validation loss 0.046020250767469406 Accuracy 0.8891250491142273\n",
      "Iteration 91620 Training loss 0.0012588667450472713 Validation loss 0.046009257435798645 Accuracy 0.8892500400543213\n",
      "Iteration 91630 Training loss 0.0012604111107066274 Validation loss 0.046008799225091934 Accuracy 0.8888750672340393\n",
      "Iteration 91640 Training loss 0.001259454875253141 Validation loss 0.04600519686937332 Accuracy 0.8892500400543213\n",
      "Iteration 91650 Training loss 1.1234693374717608e-05 Validation loss 0.046012792736291885 Accuracy 0.8891250491142273\n",
      "Iteration 91660 Training loss 0.001260946854017675 Validation loss 0.04606373980641365 Accuracy 0.8890000581741333\n",
      "Iteration 91670 Training loss 0.002509737852960825 Validation loss 0.04603763669729233 Accuracy 0.8888750672340393\n",
      "Iteration 91680 Training loss 6.997995114943478e-06 Validation loss 0.04603251814842224 Accuracy 0.8888750672340393\n",
      "Iteration 91690 Training loss 0.0037591129075735807 Validation loss 0.04604657366871834 Accuracy 0.8890000581741333\n",
      "Iteration 91700 Training loss 0.0012622007634490728 Validation loss 0.04603834077715874 Accuracy 0.8890000581741333\n",
      "Iteration 91710 Training loss 1.0275691238348372e-05 Validation loss 0.0460347980260849 Accuracy 0.8891250491142273\n",
      "Iteration 91720 Training loss 0.0012625870294868946 Validation loss 0.046020980924367905 Accuracy 0.8890000581741333\n",
      "Iteration 91730 Training loss 0.00500535499304533 Validation loss 0.046038515865802765 Accuracy 0.8887500166893005\n",
      "Iteration 91740 Training loss 8.559354682802223e-06 Validation loss 0.046028681099414825 Accuracy 0.8887500166893005\n",
      "Iteration 91750 Training loss 0.0025080405175685883 Validation loss 0.04601983353495598 Accuracy 0.8888750672340393\n",
      "Iteration 91760 Training loss 0.002510212827473879 Validation loss 0.046035971492528915 Accuracy 0.8890000581741333\n",
      "Iteration 91770 Training loss 0.0012591710546985269 Validation loss 0.046031106263399124 Accuracy 0.8890000581741333\n",
      "Iteration 91780 Training loss 7.125390311557567e-06 Validation loss 0.046026796102523804 Accuracy 0.8888750672340393\n",
      "Iteration 91790 Training loss 9.30294299905654e-06 Validation loss 0.04602942243218422 Accuracy 0.8888750672340393\n",
      "Iteration 91800 Training loss 0.0012621839996427298 Validation loss 0.04600630700588226 Accuracy 0.8891250491142273\n",
      "Iteration 91810 Training loss 1.0829377970367204e-05 Validation loss 0.04602134972810745 Accuracy 0.8893750309944153\n",
      "Iteration 91820 Training loss 9.713737199490424e-06 Validation loss 0.045996394008398056 Accuracy 0.8895000219345093\n",
      "Iteration 91830 Training loss 0.0012586574302986264 Validation loss 0.04600584879517555 Accuracy 0.8895000219345093\n",
      "Iteration 91840 Training loss 0.001258918666280806 Validation loss 0.04600714147090912 Accuracy 0.8892500400543213\n",
      "Iteration 91850 Training loss 0.0012579576577991247 Validation loss 0.04599837213754654 Accuracy 0.8890000581741333\n",
      "Iteration 91860 Training loss 0.001258673146367073 Validation loss 0.046035896986722946 Accuracy 0.8891250491142273\n",
      "Iteration 91870 Training loss 0.002509180223569274 Validation loss 0.04605122283101082 Accuracy 0.8891250491142273\n",
      "Iteration 91880 Training loss 0.005009839311242104 Validation loss 0.046028293669223785 Accuracy 0.8891250491142273\n",
      "Iteration 91890 Training loss 9.610012057237327e-06 Validation loss 0.04603543132543564 Accuracy 0.8892500400543213\n",
      "Iteration 91900 Training loss 0.0012612264836207032 Validation loss 0.046035781502723694 Accuracy 0.8893750309944153\n",
      "Iteration 91910 Training loss 7.066239959385712e-06 Validation loss 0.04600955545902252 Accuracy 0.8891250491142273\n",
      "Iteration 91920 Training loss 8.725276529730763e-06 Validation loss 0.046035975217819214 Accuracy 0.8893750309944153\n",
      "Iteration 91930 Training loss 0.003760511288419366 Validation loss 0.04602079838514328 Accuracy 0.8892500400543213\n",
      "Iteration 91940 Training loss 0.0037577920593321323 Validation loss 0.04601318761706352 Accuracy 0.8890000581741333\n",
      "Iteration 91950 Training loss 0.0012620993657037616 Validation loss 0.04601738229393959 Accuracy 0.8891250491142273\n",
      "Iteration 91960 Training loss 8.116843673633412e-06 Validation loss 0.04601886123418808 Accuracy 0.8891250491142273\n",
      "Iteration 91970 Training loss 0.0012545923236757517 Validation loss 0.046027567237615585 Accuracy 0.8892500400543213\n",
      "Iteration 91980 Training loss 6.412185939552728e-06 Validation loss 0.04603371024131775 Accuracy 0.8892500400543213\n",
      "Iteration 91990 Training loss 0.0050069731660187244 Validation loss 0.046068206429481506 Accuracy 0.8895000219345093\n",
      "Iteration 92000 Training loss 0.0025101189967244864 Validation loss 0.04605204239487648 Accuracy 0.8895000219345093\n",
      "Iteration 92010 Training loss 0.003759437007829547 Validation loss 0.046037741005420685 Accuracy 0.8892500400543213\n",
      "Iteration 92020 Training loss 0.0012589020188897848 Validation loss 0.04604042321443558 Accuracy 0.8892500400543213\n",
      "Iteration 92030 Training loss 1.2077756764483638e-05 Validation loss 0.046034473925828934 Accuracy 0.8887500166893005\n",
      "Iteration 92040 Training loss 0.001256914809346199 Validation loss 0.04603150114417076 Accuracy 0.8891250491142273\n",
      "Iteration 92050 Training loss 0.0012575398432090878 Validation loss 0.04601610079407692 Accuracy 0.8890000581741333\n",
      "Iteration 92060 Training loss 0.0025106424000114202 Validation loss 0.04602741450071335 Accuracy 0.8887500166893005\n",
      "Iteration 92070 Training loss 0.0012549972161650658 Validation loss 0.046004049479961395 Accuracy 0.8888750672340393\n",
      "Iteration 92080 Training loss 0.0025081762578338385 Validation loss 0.046031657606363297 Accuracy 0.8893750309944153\n",
      "Iteration 92090 Training loss 8.23408663563896e-06 Validation loss 0.04601332172751427 Accuracy 0.8888750672340393\n",
      "Iteration 92100 Training loss 0.0025074344594031572 Validation loss 0.04601728171110153 Accuracy 0.8887500166893005\n",
      "Iteration 92110 Training loss 0.0012610246194526553 Validation loss 0.046034060418605804 Accuracy 0.8888750672340393\n",
      "Iteration 92120 Training loss 0.0012598418397828937 Validation loss 0.046058379113674164 Accuracy 0.8888750672340393\n",
      "Iteration 92130 Training loss 7.614161404490005e-06 Validation loss 0.04602912813425064 Accuracy 0.8890000581741333\n",
      "Iteration 92140 Training loss 0.001258846023119986 Validation loss 0.04607023298740387 Accuracy 0.8887500166893005\n",
      "Iteration 92150 Training loss 0.0012576297158375382 Validation loss 0.04607704281806946 Accuracy 0.8886250257492065\n",
      "Iteration 92160 Training loss 0.0012604984221979976 Validation loss 0.04605963081121445 Accuracy 0.8888750672340393\n",
      "Iteration 92170 Training loss 0.003757533384487033 Validation loss 0.04603857547044754 Accuracy 0.8888750672340393\n",
      "Iteration 92180 Training loss 0.001262255827896297 Validation loss 0.04603320360183716 Accuracy 0.8888750672340393\n",
      "Iteration 92190 Training loss 0.0025082239881157875 Validation loss 0.04604937881231308 Accuracy 0.8891250491142273\n",
      "Iteration 92200 Training loss 1.2406170753820334e-05 Validation loss 0.046019893139600754 Accuracy 0.8888750672340393\n",
      "Iteration 92210 Training loss 1.0249309525534045e-05 Validation loss 0.04603460431098938 Accuracy 0.8893750309944153\n",
      "Iteration 92220 Training loss 0.002506151795387268 Validation loss 0.04603134095668793 Accuracy 0.8892500400543213\n",
      "Iteration 92230 Training loss 0.0037575536407530308 Validation loss 0.046021170914173126 Accuracy 0.8890000581741333\n",
      "Iteration 92240 Training loss 8.921670996642206e-06 Validation loss 0.046060752123594284 Accuracy 0.8891250491142273\n",
      "Iteration 92250 Training loss 7.243730124173453e-06 Validation loss 0.046035464853048325 Accuracy 0.8892500400543213\n",
      "Iteration 92260 Training loss 7.669659680686891e-06 Validation loss 0.046017929911613464 Accuracy 0.8891250491142273\n",
      "Iteration 92270 Training loss 0.002509461250156164 Validation loss 0.04605603218078613 Accuracy 0.8892500400543213\n",
      "Iteration 92280 Training loss 0.0012584134237840772 Validation loss 0.04604451358318329 Accuracy 0.8892500400543213\n",
      "Iteration 92290 Training loss 0.0012570489197969437 Validation loss 0.046043138951063156 Accuracy 0.8891250491142273\n",
      "Iteration 92300 Training loss 1.1524464753165375e-05 Validation loss 0.046044427901506424 Accuracy 0.8891250491142273\n",
      "Iteration 92310 Training loss 0.002507769037038088 Validation loss 0.046050675213336945 Accuracy 0.8888750672340393\n",
      "Iteration 92320 Training loss 1.1846842426166404e-05 Validation loss 0.04603208228945732 Accuracy 0.8893750309944153\n",
      "Iteration 92330 Training loss 0.0025078551843762398 Validation loss 0.04605158418416977 Accuracy 0.8892500400543213\n",
      "Iteration 92340 Training loss 0.0012590285623446107 Validation loss 0.04604695737361908 Accuracy 0.8892500400543213\n",
      "Iteration 92350 Training loss 0.0012603399809449911 Validation loss 0.04599958285689354 Accuracy 0.8891250491142273\n",
      "Iteration 92360 Training loss 0.0025078244507312775 Validation loss 0.04601545259356499 Accuracy 0.8896250128746033\n",
      "Iteration 92370 Training loss 0.0012591376435011625 Validation loss 0.046033475548028946 Accuracy 0.8891250491142273\n",
      "Iteration 92380 Training loss 0.0025092826690524817 Validation loss 0.04604830965399742 Accuracy 0.8888750672340393\n",
      "Iteration 92390 Training loss 0.0012610068079084158 Validation loss 0.046024300158023834 Accuracy 0.8893750309944153\n",
      "Iteration 92400 Training loss 8.25330334919272e-06 Validation loss 0.046007875353097916 Accuracy 0.8893750309944153\n",
      "Iteration 92410 Training loss 1.1611804438871332e-05 Validation loss 0.04600704461336136 Accuracy 0.8895000219345093\n",
      "Iteration 92420 Training loss 1.1795853424700908e-05 Validation loss 0.04602356255054474 Accuracy 0.8891250491142273\n",
      "Iteration 92430 Training loss 0.003760592546314001 Validation loss 0.04605695605278015 Accuracy 0.8892500400543213\n",
      "Iteration 92440 Training loss 0.006256232038140297 Validation loss 0.046040378510951996 Accuracy 0.8890000581741333\n",
      "Iteration 92450 Training loss 9.043449608725496e-06 Validation loss 0.04603629186749458 Accuracy 0.8890000581741333\n",
      "Iteration 92460 Training loss 0.0037583697121590376 Validation loss 0.046024687588214874 Accuracy 0.8891250491142273\n",
      "Iteration 92470 Training loss 1.6790576410130598e-05 Validation loss 0.046043261885643005 Accuracy 0.8891250491142273\n",
      "Iteration 92480 Training loss 0.001259075477719307 Validation loss 0.0460231713950634 Accuracy 0.8892500400543213\n",
      "Iteration 92490 Training loss 0.001257662777788937 Validation loss 0.045984718948602676 Accuracy 0.8888750672340393\n",
      "Iteration 92500 Training loss 0.0012601881753653288 Validation loss 0.04601027071475983 Accuracy 0.8890000581741333\n",
      "Iteration 92510 Training loss 0.0012605043593794107 Validation loss 0.045998554676771164 Accuracy 0.8891250491142273\n",
      "Iteration 92520 Training loss 8.855413398123346e-06 Validation loss 0.04598558694124222 Accuracy 0.8893750309944153\n",
      "Iteration 92530 Training loss 1.0765714250737801e-05 Validation loss 0.046007826924324036 Accuracy 0.8892500400543213\n",
      "Iteration 92540 Training loss 0.0012584184296429157 Validation loss 0.04600369557738304 Accuracy 0.8890000581741333\n",
      "Iteration 92550 Training loss 0.0012594589497894049 Validation loss 0.04603198543190956 Accuracy 0.8896250128746033\n",
      "Iteration 92560 Training loss 0.0012585390359163284 Validation loss 0.04602513089776039 Accuracy 0.8895000219345093\n",
      "Iteration 92570 Training loss 0.0012574692955240607 Validation loss 0.0460200235247612 Accuracy 0.8896250128746033\n",
      "Iteration 92580 Training loss 8.746300409256946e-06 Validation loss 0.046016398817300797 Accuracy 0.8893750309944153\n",
      "Iteration 92590 Training loss 0.0025084710214287043 Validation loss 0.04602568596601486 Accuracy 0.8895000219345093\n",
      "Iteration 92600 Training loss 0.001263870159164071 Validation loss 0.04602081701159477 Accuracy 0.8892500400543213\n",
      "Iteration 92610 Training loss 0.0012591296108439565 Validation loss 0.04600509628653526 Accuracy 0.8887500166893005\n",
      "Iteration 92620 Training loss 0.001257731462828815 Validation loss 0.04603707417845726 Accuracy 0.8891250491142273\n",
      "Iteration 92630 Training loss 1.475373483117437e-05 Validation loss 0.04601183533668518 Accuracy 0.8892500400543213\n",
      "Iteration 92640 Training loss 0.002505283337086439 Validation loss 0.046024564653635025 Accuracy 0.8895000219345093\n",
      "Iteration 92650 Training loss 1.4126269888947718e-05 Validation loss 0.04601150006055832 Accuracy 0.8888750672340393\n",
      "Iteration 92660 Training loss 6.909125204401789e-06 Validation loss 0.045996978878974915 Accuracy 0.8890000581741333\n",
      "Iteration 92670 Training loss 0.0012595404405146837 Validation loss 0.0459960512816906 Accuracy 0.8893750309944153\n",
      "Iteration 92680 Training loss 0.002509612590074539 Validation loss 0.045990534126758575 Accuracy 0.8892500400543213\n",
      "Iteration 92690 Training loss 0.0025078998878598213 Validation loss 0.04600882902741432 Accuracy 0.8890000581741333\n",
      "Iteration 92700 Training loss 9.31634167500306e-06 Validation loss 0.04603251442313194 Accuracy 0.8892500400543213\n",
      "Iteration 92710 Training loss 0.0012558195739984512 Validation loss 0.04602599889039993 Accuracy 0.8890000581741333\n",
      "Iteration 92720 Training loss 0.002507644472643733 Validation loss 0.04598953574895859 Accuracy 0.8888750672340393\n",
      "Iteration 92730 Training loss 0.0012590643018484116 Validation loss 0.04600170999765396 Accuracy 0.8888750672340393\n",
      "Iteration 92740 Training loss 1.666177013248671e-05 Validation loss 0.04600711911916733 Accuracy 0.8892500400543213\n",
      "Iteration 92750 Training loss 0.0025095422752201557 Validation loss 0.0459989570081234 Accuracy 0.8892500400543213\n",
      "Iteration 92760 Training loss 0.003757962491363287 Validation loss 0.04600583761930466 Accuracy 0.8893750309944153\n",
      "Iteration 92770 Training loss 0.003759313840419054 Validation loss 0.04598606377840042 Accuracy 0.8890000581741333\n",
      "Iteration 92780 Training loss 0.0012595123844221234 Validation loss 0.04602421820163727 Accuracy 0.8893750309944153\n",
      "Iteration 92790 Training loss 0.0012602433562278748 Validation loss 0.04602106660604477 Accuracy 0.8893750309944153\n",
      "Iteration 92800 Training loss 0.0012573535786941648 Validation loss 0.04602926969528198 Accuracy 0.8892500400543213\n",
      "Iteration 92810 Training loss 0.0025075101293623447 Validation loss 0.046038951724767685 Accuracy 0.8892500400543213\n",
      "Iteration 92820 Training loss 1.1141301911266055e-05 Validation loss 0.0460294671356678 Accuracy 0.8891250491142273\n",
      "Iteration 92830 Training loss 0.0025109972339123487 Validation loss 0.046015843749046326 Accuracy 0.8891250491142273\n",
      "Iteration 92840 Training loss 0.0012616583844646811 Validation loss 0.04601289704442024 Accuracy 0.8891250491142273\n",
      "Iteration 92850 Training loss 0.003762295236811042 Validation loss 0.046015508472919464 Accuracy 0.8892500400543213\n",
      "Iteration 92860 Training loss 0.0012596686137840152 Validation loss 0.04602280631661415 Accuracy 0.8890000581741333\n",
      "Iteration 92870 Training loss 0.0012602609349414706 Validation loss 0.046020206063985825 Accuracy 0.8892500400543213\n",
      "Iteration 92880 Training loss 1.0527258382353466e-05 Validation loss 0.04600532725453377 Accuracy 0.8895000219345093\n",
      "Iteration 92890 Training loss 0.001257280819118023 Validation loss 0.046032410115003586 Accuracy 0.8892500400543213\n",
      "Iteration 92900 Training loss 8.494883331877645e-06 Validation loss 0.0460163876414299 Accuracy 0.8888750672340393\n",
      "Iteration 92910 Training loss 0.002510347869247198 Validation loss 0.04600901156663895 Accuracy 0.8895000219345093\n",
      "Iteration 92920 Training loss 0.001256622956134379 Validation loss 0.045996300876140594 Accuracy 0.8890000581741333\n",
      "Iteration 92930 Training loss 1.2522031283879187e-05 Validation loss 0.04599551856517792 Accuracy 0.8895000219345093\n",
      "Iteration 92940 Training loss 8.05906984169269e-06 Validation loss 0.04601847752928734 Accuracy 0.8895000219345093\n",
      "Iteration 92950 Training loss 7.261526207003044e-06 Validation loss 0.046000316739082336 Accuracy 0.8895000219345093\n",
      "Iteration 92960 Training loss 7.899604497652035e-06 Validation loss 0.04598568379878998 Accuracy 0.8892500400543213\n",
      "Iteration 92970 Training loss 0.00125797837972641 Validation loss 0.04598289728164673 Accuracy 0.8895000219345093\n",
      "Iteration 92980 Training loss 0.002510498045012355 Validation loss 0.0459880568087101 Accuracy 0.8895000219345093\n",
      "Iteration 92990 Training loss 0.0012618574546650052 Validation loss 0.04599142447113991 Accuracy 0.8893750309944153\n",
      "Iteration 93000 Training loss 1.0784163350763265e-05 Validation loss 0.04598260298371315 Accuracy 0.8895000219345093\n",
      "Iteration 93010 Training loss 0.001259085489436984 Validation loss 0.04602155089378357 Accuracy 0.8896250128746033\n",
      "Iteration 93020 Training loss 0.0012624382507055998 Validation loss 0.046031367033720016 Accuracy 0.8895000219345093\n",
      "Iteration 93030 Training loss 9.228145245288033e-06 Validation loss 0.04600324109196663 Accuracy 0.8893750309944153\n",
      "Iteration 93040 Training loss 1.1886392712767702e-05 Validation loss 0.04599599540233612 Accuracy 0.8895000219345093\n",
      "Iteration 93050 Training loss 0.002508571371436119 Validation loss 0.04600634053349495 Accuracy 0.8895000219345093\n",
      "Iteration 93060 Training loss 1.3320131074578967e-05 Validation loss 0.04600773751735687 Accuracy 0.8893750309944153\n",
      "Iteration 93070 Training loss 0.0025108035188168287 Validation loss 0.04600384086370468 Accuracy 0.8893750309944153\n",
      "Iteration 93080 Training loss 0.003758339211344719 Validation loss 0.04600181430578232 Accuracy 0.8893750309944153\n",
      "Iteration 93090 Training loss 0.0012588610406965017 Validation loss 0.046040963381528854 Accuracy 0.8896250128746033\n",
      "Iteration 93100 Training loss 6.819890131737338e-06 Validation loss 0.04602678120136261 Accuracy 0.8896250128746033\n",
      "Iteration 93110 Training loss 0.0037595226895064116 Validation loss 0.046007536351680756 Accuracy 0.8895000219345093\n",
      "Iteration 93120 Training loss 0.0012599428882822394 Validation loss 0.04603962227702141 Accuracy 0.8896250128746033\n",
      "Iteration 93130 Training loss 1.1560272469068877e-05 Validation loss 0.04601547494530678 Accuracy 0.8895000219345093\n",
      "Iteration 93140 Training loss 0.0025117225013673306 Validation loss 0.04602237045764923 Accuracy 0.889750063419342\n",
      "Iteration 93150 Training loss 0.001261281780898571 Validation loss 0.04602568596601486 Accuracy 0.889750063419342\n",
      "Iteration 93160 Training loss 0.0012621519854292274 Validation loss 0.046013880521059036 Accuracy 0.8896250128746033\n",
      "Iteration 93170 Training loss 0.0012599964393302798 Validation loss 0.04604678601026535 Accuracy 0.8895000219345093\n",
      "Iteration 93180 Training loss 0.005011307075619698 Validation loss 0.04602406546473503 Accuracy 0.8891250491142273\n",
      "Iteration 93190 Training loss 8.746215826249681e-06 Validation loss 0.046016063541173935 Accuracy 0.8896250128746033\n",
      "Iteration 93200 Training loss 9.673452950664796e-06 Validation loss 0.046034809201955795 Accuracy 0.8895000219345093\n",
      "Iteration 93210 Training loss 1.0594848390610423e-05 Validation loss 0.046025536954402924 Accuracy 0.8895000219345093\n",
      "Iteration 93220 Training loss 7.619926691404544e-06 Validation loss 0.04601669684052467 Accuracy 0.8893750309944153\n",
      "Iteration 93230 Training loss 7.0947826316114515e-06 Validation loss 0.046030618250370026 Accuracy 0.8895000219345093\n",
      "Iteration 93240 Training loss 0.0012583392672240734 Validation loss 0.04603508487343788 Accuracy 0.8892500400543213\n",
      "Iteration 93250 Training loss 0.005009498912841082 Validation loss 0.046007219702005386 Accuracy 0.8893750309944153\n",
      "Iteration 93260 Training loss 0.0025067583192139864 Validation loss 0.04602314159274101 Accuracy 0.8893750309944153\n",
      "Iteration 93270 Training loss 0.0025103730149567127 Validation loss 0.04601988568902016 Accuracy 0.8891250491142273\n",
      "Iteration 93280 Training loss 0.003761137370020151 Validation loss 0.0460079200565815 Accuracy 0.8892500400543213\n",
      "Iteration 93290 Training loss 0.0025073091965168715 Validation loss 0.046016935259103775 Accuracy 0.8895000219345093\n",
      "Iteration 93300 Training loss 8.803147466096561e-06 Validation loss 0.046016063541173935 Accuracy 0.8896250128746033\n",
      "Iteration 93310 Training loss 0.0025078998878598213 Validation loss 0.04601242393255234 Accuracy 0.8895000219345093\n",
      "Iteration 93320 Training loss 0.0012575022410601377 Validation loss 0.04601433500647545 Accuracy 0.8890000581741333\n",
      "Iteration 93330 Training loss 0.0012571999104693532 Validation loss 0.04602572321891785 Accuracy 0.8892500400543213\n",
      "Iteration 93340 Training loss 8.57083887240151e-06 Validation loss 0.046019189059734344 Accuracy 0.8891250491142273\n",
      "Iteration 93350 Training loss 0.001257170457392931 Validation loss 0.04601506143808365 Accuracy 0.8893750309944153\n",
      "Iteration 93360 Training loss 1.4069280041439924e-05 Validation loss 0.04602403938770294 Accuracy 0.8895000219345093\n",
      "Iteration 93370 Training loss 0.002509242156520486 Validation loss 0.04603544995188713 Accuracy 0.889750063419342\n",
      "Iteration 93380 Training loss 9.332802619610447e-06 Validation loss 0.046021509915590286 Accuracy 0.889750063419342\n",
      "Iteration 93390 Training loss 0.0025076954625546932 Validation loss 0.0460028350353241 Accuracy 0.8896250128746033\n",
      "Iteration 93400 Training loss 0.001257782569155097 Validation loss 0.04601460322737694 Accuracy 0.8890000581741333\n",
      "Iteration 93410 Training loss 0.0012618639739230275 Validation loss 0.04601459577679634 Accuracy 0.8893750309944153\n",
      "Iteration 93420 Training loss 0.0037573196459561586 Validation loss 0.04602615907788277 Accuracy 0.8893750309944153\n",
      "Iteration 93430 Training loss 6.995681815169519e-06 Validation loss 0.04600750654935837 Accuracy 0.8893750309944153\n",
      "Iteration 93440 Training loss 0.0012602695496752858 Validation loss 0.04599196836352348 Accuracy 0.8893750309944153\n",
      "Iteration 93450 Training loss 1.06643465187517e-05 Validation loss 0.046006545424461365 Accuracy 0.8895000219345093\n",
      "Iteration 93460 Training loss 0.0012568555539473891 Validation loss 0.046036697924137115 Accuracy 0.8895000219345093\n",
      "Iteration 93470 Training loss 0.0025090062990784645 Validation loss 0.04600691422820091 Accuracy 0.8891250491142273\n",
      "Iteration 93480 Training loss 0.0012594942236319184 Validation loss 0.04601302370429039 Accuracy 0.8888750672340393\n",
      "Iteration 93490 Training loss 0.0012590535916388035 Validation loss 0.04600822553038597 Accuracy 0.8888750672340393\n",
      "Iteration 93500 Training loss 0.0012605867814272642 Validation loss 0.046013087034225464 Accuracy 0.8893750309944153\n",
      "Iteration 93510 Training loss 7.03963542036945e-06 Validation loss 0.04603447765111923 Accuracy 0.8891250491142273\n",
      "Iteration 93520 Training loss 0.0037568712141364813 Validation loss 0.04602846875786781 Accuracy 0.8892500400543213\n",
      "Iteration 93530 Training loss 0.001259015523828566 Validation loss 0.046017009764909744 Accuracy 0.8892500400543213\n",
      "Iteration 93540 Training loss 0.0012598922476172447 Validation loss 0.04601719230413437 Accuracy 0.8891250491142273\n",
      "Iteration 93550 Training loss 1.2474552931962535e-05 Validation loss 0.046016838401556015 Accuracy 0.8893750309944153\n",
      "Iteration 93560 Training loss 0.0012592290295287967 Validation loss 0.04601513221859932 Accuracy 0.8888750672340393\n",
      "Iteration 93570 Training loss 0.0025066619273275137 Validation loss 0.04600829258561134 Accuracy 0.8896250128746033\n",
      "Iteration 93580 Training loss 0.0025124233216047287 Validation loss 0.046027909964323044 Accuracy 0.8891250491142273\n",
      "Iteration 93590 Training loss 0.0025075215380638838 Validation loss 0.046016037464141846 Accuracy 0.8891250491142273\n",
      "Iteration 93600 Training loss 0.005007992964237928 Validation loss 0.04600529745221138 Accuracy 0.8888750672340393\n",
      "Iteration 93610 Training loss 0.0012586858356371522 Validation loss 0.046019457280635834 Accuracy 0.8892500400543213\n",
      "Iteration 93620 Training loss 0.001257793395780027 Validation loss 0.04601839557290077 Accuracy 0.8891250491142273\n",
      "Iteration 93630 Training loss 0.0012586788507178426 Validation loss 0.04601120203733444 Accuracy 0.8891250491142273\n",
      "Iteration 93640 Training loss 0.002509291749447584 Validation loss 0.046030040830373764 Accuracy 0.8890000581741333\n",
      "Iteration 93650 Training loss 0.0025081722997128963 Validation loss 0.04601982235908508 Accuracy 0.8890000581741333\n",
      "Iteration 93660 Training loss 9.155777661362663e-06 Validation loss 0.04602902755141258 Accuracy 0.8895000219345093\n",
      "Iteration 93670 Training loss 8.327499017468654e-06 Validation loss 0.04601302370429039 Accuracy 0.8892500400543213\n",
      "Iteration 93680 Training loss 0.005009767133742571 Validation loss 0.04602750018239021 Accuracy 0.8893750309944153\n",
      "Iteration 93690 Training loss 0.0012604109942913055 Validation loss 0.046015698462724686 Accuracy 0.8893750309944153\n",
      "Iteration 93700 Training loss 0.0012619247427210212 Validation loss 0.04601864889264107 Accuracy 0.8890000581741333\n",
      "Iteration 93710 Training loss 9.130641046795063e-06 Validation loss 0.04602942243218422 Accuracy 0.8891250491142273\n",
      "Iteration 93720 Training loss 9.274879630538635e-06 Validation loss 0.04601028561592102 Accuracy 0.8891250491142273\n",
      "Iteration 93730 Training loss 9.617577234166674e-06 Validation loss 0.04602113738656044 Accuracy 0.8895000219345093\n",
      "Iteration 93740 Training loss 0.003757661208510399 Validation loss 0.04600505530834198 Accuracy 0.8893750309944153\n",
      "Iteration 93750 Training loss 0.001259130542166531 Validation loss 0.04601975902915001 Accuracy 0.8896250128746033\n",
      "Iteration 93760 Training loss 8.492136657878291e-06 Validation loss 0.04603348299860954 Accuracy 0.8895000219345093\n",
      "Iteration 93770 Training loss 0.0012596523156389594 Validation loss 0.04602432623505592 Accuracy 0.889750063419342\n",
      "Iteration 93780 Training loss 0.0025086021050810814 Validation loss 0.04601559415459633 Accuracy 0.8895000219345093\n",
      "Iteration 93790 Training loss 6.697975095448783e-06 Validation loss 0.04601846635341644 Accuracy 0.8896250128746033\n",
      "Iteration 93800 Training loss 0.00376347778365016 Validation loss 0.04601829871535301 Accuracy 0.889750063419342\n",
      "Iteration 93810 Training loss 1.1710699254763313e-05 Validation loss 0.04602343589067459 Accuracy 0.889750063419342\n",
      "Iteration 93820 Training loss 5.939239599683788e-06 Validation loss 0.04602237418293953 Accuracy 0.8893750309944153\n",
      "Iteration 93830 Training loss 0.002508678939193487 Validation loss 0.046028222888708115 Accuracy 0.8891250491142273\n",
      "Iteration 93840 Training loss 0.006265023723244667 Validation loss 0.04601385071873665 Accuracy 0.8890000581741333\n",
      "Iteration 93850 Training loss 0.0012585979420691729 Validation loss 0.04602924734354019 Accuracy 0.8888750672340393\n",
      "Iteration 93860 Training loss 0.0012556869769468904 Validation loss 0.04603221267461777 Accuracy 0.8890000581741333\n",
      "Iteration 93870 Training loss 0.0012596576707437634 Validation loss 0.04601534456014633 Accuracy 0.8888750672340393\n",
      "Iteration 93880 Training loss 0.001257942640222609 Validation loss 0.04604822024703026 Accuracy 0.8891250491142273\n",
      "Iteration 93890 Training loss 0.0012588741956278682 Validation loss 0.046051859855651855 Accuracy 0.8888750672340393\n",
      "Iteration 93900 Training loss 8.656886166136246e-06 Validation loss 0.04605870321393013 Accuracy 0.8888750672340393\n",
      "Iteration 93910 Training loss 6.227230187505484e-06 Validation loss 0.046043895184993744 Accuracy 0.8892500400543213\n",
      "Iteration 93920 Training loss 0.0012623472139239311 Validation loss 0.04601713642477989 Accuracy 0.8890000581741333\n",
      "Iteration 93930 Training loss 0.0012603963259607553 Validation loss 0.046015795320272446 Accuracy 0.8896250128746033\n",
      "Iteration 93940 Training loss 0.002510552993044257 Validation loss 0.04604494571685791 Accuracy 0.8893750309944153\n",
      "Iteration 93950 Training loss 0.0025092961732298136 Validation loss 0.04601256921887398 Accuracy 0.8892500400543213\n",
      "Iteration 93960 Training loss 7.550777809228748e-06 Validation loss 0.046021945774555206 Accuracy 0.8892500400543213\n",
      "Iteration 93970 Training loss 0.0012584839714691043 Validation loss 0.0460221953690052 Accuracy 0.8888750672340393\n",
      "Iteration 93980 Training loss 0.0025087797548621893 Validation loss 0.04602443054318428 Accuracy 0.8888750672340393\n",
      "Iteration 93990 Training loss 0.0012573817512020469 Validation loss 0.046030353754758835 Accuracy 0.8888750672340393\n",
      "Iteration 94000 Training loss 0.0012559680035337806 Validation loss 0.04602789506316185 Accuracy 0.8893750309944153\n",
      "Iteration 94010 Training loss 0.0012558820890262723 Validation loss 0.04602842777967453 Accuracy 0.8893750309944153\n",
      "Iteration 94020 Training loss 0.002507580677047372 Validation loss 0.04604687541723251 Accuracy 0.8895000219345093\n",
      "Iteration 94030 Training loss 0.0012612281134352088 Validation loss 0.04602527990937233 Accuracy 0.8890000581741333\n",
      "Iteration 94040 Training loss 1.006828824756667e-05 Validation loss 0.046030353754758835 Accuracy 0.8892500400543213\n",
      "Iteration 94050 Training loss 0.0037597643677145243 Validation loss 0.04602966457605362 Accuracy 0.8895000219345093\n",
      "Iteration 94060 Training loss 9.859319106908515e-06 Validation loss 0.04603630304336548 Accuracy 0.8895000219345093\n",
      "Iteration 94070 Training loss 0.002508144360035658 Validation loss 0.046028729528188705 Accuracy 0.8893750309944153\n",
      "Iteration 94080 Training loss 0.0025083255022764206 Validation loss 0.0460517592728138 Accuracy 0.8890000581741333\n",
      "Iteration 94090 Training loss 8.12153120932635e-06 Validation loss 0.046032827347517014 Accuracy 0.8891250491142273\n",
      "Iteration 94100 Training loss 0.0025072814896702766 Validation loss 0.04603571444749832 Accuracy 0.8892500400543213\n",
      "Iteration 94110 Training loss 0.002508927136659622 Validation loss 0.046003811061382294 Accuracy 0.8895000219345093\n",
      "Iteration 94120 Training loss 0.0012576448498293757 Validation loss 0.04599989578127861 Accuracy 0.8893750309944153\n",
      "Iteration 94130 Training loss 0.0012597672175616026 Validation loss 0.04600854590535164 Accuracy 0.8895000219345093\n",
      "Iteration 94140 Training loss 0.002507350640371442 Validation loss 0.04599716514348984 Accuracy 0.8893750309944153\n",
      "Iteration 94150 Training loss 0.00375650217756629 Validation loss 0.04604659602046013 Accuracy 0.8892500400543213\n",
      "Iteration 94160 Training loss 0.002508387202396989 Validation loss 0.04602997750043869 Accuracy 0.8895000219345093\n",
      "Iteration 94170 Training loss 8.848722245602403e-06 Validation loss 0.04602954909205437 Accuracy 0.8892500400543213\n",
      "Iteration 94180 Training loss 0.0012579341419041157 Validation loss 0.046013958752155304 Accuracy 0.8890000581741333\n",
      "Iteration 94190 Training loss 9.2101226982777e-06 Validation loss 0.045998714864254 Accuracy 0.8890000581741333\n",
      "Iteration 94200 Training loss 6.844323252153117e-06 Validation loss 0.045998796820640564 Accuracy 0.8893750309944153\n",
      "Iteration 94210 Training loss 0.0025074707809835672 Validation loss 0.04601193964481354 Accuracy 0.8895000219345093\n",
      "Iteration 94220 Training loss 6.5269964579783846e-06 Validation loss 0.046016138046979904 Accuracy 0.8896250128746033\n",
      "Iteration 94230 Training loss 0.0012586108641698956 Validation loss 0.04599602892994881 Accuracy 0.8892500400543213\n",
      "Iteration 94240 Training loss 0.0012621503556147218 Validation loss 0.04602045938372612 Accuracy 0.8893750309944153\n",
      "Iteration 94250 Training loss 0.002507132012397051 Validation loss 0.04601328447461128 Accuracy 0.8893750309944153\n",
      "Iteration 94260 Training loss 0.0012569907121360302 Validation loss 0.046018850058317184 Accuracy 0.8896250128746033\n",
      "Iteration 94270 Training loss 0.002512183506041765 Validation loss 0.046040404587984085 Accuracy 0.8895000219345093\n",
      "Iteration 94280 Training loss 0.0012555911671370268 Validation loss 0.04606727138161659 Accuracy 0.8887500166893005\n",
      "Iteration 94290 Training loss 7.2637631092220545e-06 Validation loss 0.046043336391448975 Accuracy 0.8890000581741333\n",
      "Iteration 94300 Training loss 7.092859505064553e-06 Validation loss 0.046009454876184464 Accuracy 0.8888750672340393\n",
      "Iteration 94310 Training loss 0.006259197369217873 Validation loss 0.046023376286029816 Accuracy 0.8890000581741333\n",
      "Iteration 94320 Training loss 0.005010922905057669 Validation loss 0.04602515324950218 Accuracy 0.8891250491142273\n",
      "Iteration 94330 Training loss 0.0012580710463225842 Validation loss 0.046020928770303726 Accuracy 0.8891250491142273\n",
      "Iteration 94340 Training loss 0.0025077867321670055 Validation loss 0.0460185781121254 Accuracy 0.8891250491142273\n",
      "Iteration 94350 Training loss 0.0012579795438796282 Validation loss 0.046019744127988815 Accuracy 0.8895000219345093\n",
      "Iteration 94360 Training loss 1.2405395864334423e-05 Validation loss 0.046016626060009 Accuracy 0.8896250128746033\n",
      "Iteration 94370 Training loss 0.002510729478672147 Validation loss 0.046011533588171005 Accuracy 0.8895000219345093\n",
      "Iteration 94380 Training loss 0.00500641530379653 Validation loss 0.046020183712244034 Accuracy 0.8892500400543213\n",
      "Iteration 94390 Training loss 0.0025090225972235203 Validation loss 0.046021707355976105 Accuracy 0.8890000581741333\n",
      "Iteration 94400 Training loss 8.959869774116669e-06 Validation loss 0.04602929949760437 Accuracy 0.8891250491142273\n",
      "Iteration 94410 Training loss 0.0012570273829624057 Validation loss 0.04603360965847969 Accuracy 0.8892500400543213\n",
      "Iteration 94420 Training loss 0.0012592175044119358 Validation loss 0.04603606462478638 Accuracy 0.8893750309944153\n",
      "Iteration 94430 Training loss 0.0025097772013396025 Validation loss 0.04606599360704422 Accuracy 0.8891250491142273\n",
      "Iteration 94440 Training loss 8.563912160752807e-06 Validation loss 0.046014219522476196 Accuracy 0.8892500400543213\n",
      "Iteration 94450 Training loss 7.946023288241122e-06 Validation loss 0.04600481316447258 Accuracy 0.8890000581741333\n",
      "Iteration 94460 Training loss 0.0062573980540037155 Validation loss 0.04601336643099785 Accuracy 0.8890000581741333\n",
      "Iteration 94470 Training loss 0.0025082698557525873 Validation loss 0.04602866619825363 Accuracy 0.8893750309944153\n",
      "Iteration 94480 Training loss 1.2854633496317547e-05 Validation loss 0.04602888599038124 Accuracy 0.8896250128746033\n",
      "Iteration 94490 Training loss 0.0012582108611240983 Validation loss 0.046052493155002594 Accuracy 0.8892500400543213\n",
      "Iteration 94500 Training loss 1.0291913895343896e-05 Validation loss 0.046029478311538696 Accuracy 0.8892500400543213\n",
      "Iteration 94510 Training loss 0.0012582434574142098 Validation loss 0.0460335873067379 Accuracy 0.8892500400543213\n",
      "Iteration 94520 Training loss 0.0012607063399627805 Validation loss 0.04602707177400589 Accuracy 0.8893750309944153\n",
      "Iteration 94530 Training loss 1.6203941413550638e-05 Validation loss 0.04603877291083336 Accuracy 0.8892500400543213\n",
      "Iteration 94540 Training loss 0.002509542042389512 Validation loss 0.04602178558707237 Accuracy 0.8890000581741333\n",
      "Iteration 94550 Training loss 0.0012568195816129446 Validation loss 0.046014271676540375 Accuracy 0.8891250491142273\n",
      "Iteration 94560 Training loss 0.0012587002711370587 Validation loss 0.04602362960577011 Accuracy 0.8891250491142273\n",
      "Iteration 94570 Training loss 0.0025100302882492542 Validation loss 0.046027835458517075 Accuracy 0.8890000581741333\n",
      "Iteration 94580 Training loss 0.0012572859413921833 Validation loss 0.04603580757975578 Accuracy 0.8887500166893005\n",
      "Iteration 94590 Training loss 0.0025069620460271835 Validation loss 0.046020638197660446 Accuracy 0.8890000581741333\n",
      "Iteration 94600 Training loss 0.0025080707855522633 Validation loss 0.046015676110982895 Accuracy 0.8891250491142273\n",
      "Iteration 94610 Training loss 0.001260042772628367 Validation loss 0.04601841792464256 Accuracy 0.8892500400543213\n",
      "Iteration 94620 Training loss 0.001257366850040853 Validation loss 0.046022992581129074 Accuracy 0.8892500400543213\n",
      "Iteration 94630 Training loss 0.0025101297069340944 Validation loss 0.04605550318956375 Accuracy 0.8892500400543213\n",
      "Iteration 94640 Training loss 0.0012613896979019046 Validation loss 0.046030595898628235 Accuracy 0.8890000581741333\n",
      "Iteration 94650 Training loss 0.001258566975593567 Validation loss 0.04602653905749321 Accuracy 0.8892500400543213\n",
      "Iteration 94660 Training loss 0.0025105129461735487 Validation loss 0.04603663086891174 Accuracy 0.8888750672340393\n",
      "Iteration 94670 Training loss 0.001258397358469665 Validation loss 0.04602724686264992 Accuracy 0.8891250491142273\n",
      "Iteration 94680 Training loss 1.1437728062446695e-05 Validation loss 0.046044591814279556 Accuracy 0.8890000581741333\n",
      "Iteration 94690 Training loss 7.336169801419601e-06 Validation loss 0.04600968211889267 Accuracy 0.8890000581741333\n",
      "Iteration 94700 Training loss 0.003761628409847617 Validation loss 0.04604274034500122 Accuracy 0.8893750309944153\n",
      "Iteration 94710 Training loss 0.001259411103092134 Validation loss 0.04604380205273628 Accuracy 0.8890000581741333\n",
      "Iteration 94720 Training loss 9.371299711347092e-06 Validation loss 0.04603659361600876 Accuracy 0.8890000581741333\n",
      "Iteration 94730 Training loss 8.73361113917781e-06 Validation loss 0.04603898152709007 Accuracy 0.8891250491142273\n",
      "Iteration 94740 Training loss 0.0012585362419486046 Validation loss 0.046035610139369965 Accuracy 0.8891250491142273\n",
      "Iteration 94750 Training loss 0.0012610078556463122 Validation loss 0.046023178845644 Accuracy 0.8892500400543213\n",
      "Iteration 94760 Training loss 0.0012592928251251578 Validation loss 0.046031009405851364 Accuracy 0.8891250491142273\n",
      "Iteration 94770 Training loss 0.001260003773495555 Validation loss 0.046074070036411285 Accuracy 0.8890000581741333\n",
      "Iteration 94780 Training loss 6.747282895958051e-06 Validation loss 0.04605236276984215 Accuracy 0.8888750672340393\n",
      "Iteration 94790 Training loss 0.0025070065166801214 Validation loss 0.04606178402900696 Accuracy 0.8887500166893005\n",
      "Iteration 94800 Training loss 7.203741915873252e-06 Validation loss 0.04602039232850075 Accuracy 0.8888750672340393\n",
      "Iteration 94810 Training loss 0.0012599187903106213 Validation loss 0.046035777777433395 Accuracy 0.8893750309944153\n",
      "Iteration 94820 Training loss 0.001259388867765665 Validation loss 0.0460248701274395 Accuracy 0.8890000581741333\n",
      "Iteration 94830 Training loss 0.001258712843991816 Validation loss 0.04604129120707512 Accuracy 0.8892500400543213\n",
      "Iteration 94840 Training loss 0.0025092207361012697 Validation loss 0.046042606234550476 Accuracy 0.8890000581741333\n",
      "Iteration 94850 Training loss 0.0012566447257995605 Validation loss 0.046035051345825195 Accuracy 0.8891250491142273\n",
      "Iteration 94860 Training loss 0.005008404143154621 Validation loss 0.04603751748800278 Accuracy 0.8891250491142273\n",
      "Iteration 94870 Training loss 0.002508442848920822 Validation loss 0.0460369810461998 Accuracy 0.8890000581741333\n",
      "Iteration 94880 Training loss 0.0025106496177613735 Validation loss 0.04604266583919525 Accuracy 0.8891250491142273\n",
      "Iteration 94890 Training loss 7.241792900458677e-06 Validation loss 0.04604247584939003 Accuracy 0.8892500400543213\n",
      "Iteration 94900 Training loss 0.00501098670065403 Validation loss 0.04605085030198097 Accuracy 0.8890000581741333\n",
      "Iteration 94910 Training loss 0.0025066882371902466 Validation loss 0.046025462448596954 Accuracy 0.8891250491142273\n",
      "Iteration 94920 Training loss 8.632787285023369e-06 Validation loss 0.04602667689323425 Accuracy 0.8891250491142273\n",
      "Iteration 94930 Training loss 0.0012599123874679208 Validation loss 0.04602762684226036 Accuracy 0.8888750672340393\n",
      "Iteration 94940 Training loss 0.0012568013044074178 Validation loss 0.04603147506713867 Accuracy 0.8890000581741333\n",
      "Iteration 94950 Training loss 0.0012563036289066076 Validation loss 0.046042099595069885 Accuracy 0.8891250491142273\n",
      "Iteration 94960 Training loss 0.0012578156311064959 Validation loss 0.04605064168572426 Accuracy 0.8891250491142273\n",
      "Iteration 94970 Training loss 7.343870493059512e-06 Validation loss 0.046061038970947266 Accuracy 0.8887500166893005\n",
      "Iteration 94980 Training loss 6.640870196861215e-06 Validation loss 0.04603005200624466 Accuracy 0.8890000581741333\n",
      "Iteration 94990 Training loss 0.0025098060723394156 Validation loss 0.046046510338783264 Accuracy 0.8888750672340393\n",
      "Iteration 95000 Training loss 1.2259690265636891e-05 Validation loss 0.04602530971169472 Accuracy 0.8890000581741333\n",
      "Iteration 95010 Training loss 9.509796655038372e-06 Validation loss 0.046046119183301926 Accuracy 0.8890000581741333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training done'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model_3_layer.train_layers(x_train, y_train, x_valid, y_valid, 1.6, 1e-4, 0, 0, 0, 1, 0.01, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "347eb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer_1_untrained = binary_classification_three_layer_NN(3072, 2048, 2048, eps_init = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3149bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 1.5, the number of datas used for the training is 17026752.25872509 and the number of iterations is 42566.\n",
      "Iteration 0 Training loss 0.12500081956386566 Validation loss 0.1250009983778 Accuracy 0.5\n",
      "Iteration 10 Training loss 0.12426601350307465 Validation loss 0.12450466305017471 Accuracy 0.5\n",
      "Iteration 20 Training loss 0.12398973107337952 Validation loss 0.12418626993894577 Accuracy 0.5\n",
      "Iteration 30 Training loss 0.1239936351776123 Validation loss 0.12389950454235077 Accuracy 0.5003750324249268\n",
      "Iteration 40 Training loss 0.12299794703722 Validation loss 0.12358492612838745 Accuracy 0.5003750324249268\n",
      "Iteration 50 Training loss 0.1238676980137825 Validation loss 0.12329433858394623 Accuracy 0.5015000104904175\n",
      "Iteration 60 Training loss 0.12382584065198898 Validation loss 0.12299849838018417 Accuracy 0.5056250095367432\n",
      "Iteration 70 Training loss 0.12160582095384598 Validation loss 0.12270954251289368 Accuracy 0.5121250152587891\n",
      "Iteration 80 Training loss 0.12226653844118118 Validation loss 0.12241272628307343 Accuracy 0.5383750200271606\n",
      "Iteration 90 Training loss 0.12233316153287888 Validation loss 0.12208987772464752 Accuracy 0.530750036239624\n",
      "Iteration 100 Training loss 0.12150275707244873 Validation loss 0.12177664786577225 Accuracy 0.5545000433921814\n",
      "Iteration 110 Training loss 0.12068451941013336 Validation loss 0.12143169343471527 Accuracy 0.5478750467300415\n",
      "Iteration 120 Training loss 0.12114730477333069 Validation loss 0.12109870463609695 Accuracy 0.5532500147819519\n",
      "Iteration 130 Training loss 0.12187673151493073 Validation loss 0.12075270712375641 Accuracy 0.5758750438690186\n",
      "Iteration 140 Training loss 0.12150336802005768 Validation loss 0.1203947365283966 Accuracy 0.5901250243186951\n",
      "Iteration 150 Training loss 0.11914005130529404 Validation loss 0.12002328038215637 Accuracy 0.6076250076293945\n",
      "Iteration 160 Training loss 0.11922638863325119 Validation loss 0.11964116990566254 Accuracy 0.6163750290870667\n",
      "Iteration 170 Training loss 0.11833209544420242 Validation loss 0.11923659592866898 Accuracy 0.643125057220459\n",
      "Iteration 180 Training loss 0.11932384222745895 Validation loss 0.11882791668176651 Accuracy 0.6541250348091125\n",
      "Iteration 190 Training loss 0.11824792623519897 Validation loss 0.1184067577123642 Accuracy 0.6630000472068787\n",
      "Iteration 200 Training loss 0.1171611025929451 Validation loss 0.11796920001506805 Accuracy 0.6808750033378601\n",
      "Iteration 210 Training loss 0.11763301491737366 Validation loss 0.11749573051929474 Accuracy 0.6797500252723694\n",
      "Iteration 220 Training loss 0.1177871897816658 Validation loss 0.11701357364654541 Accuracy 0.6805000305175781\n",
      "Iteration 230 Training loss 0.1171034574508667 Validation loss 0.11656138300895691 Accuracy 0.6876250505447388\n",
      "Iteration 240 Training loss 0.11567439883947372 Validation loss 0.11603683978319168 Accuracy 0.6953750252723694\n",
      "Iteration 250 Training loss 0.11566507071256638 Validation loss 0.11552048474550247 Accuracy 0.6852500438690186\n",
      "Iteration 260 Training loss 0.11386704444885254 Validation loss 0.11498410999774933 Accuracy 0.6816250085830688\n",
      "Iteration 270 Training loss 0.11195962876081467 Validation loss 0.11439892649650574 Accuracy 0.6955000162124634\n",
      "Iteration 280 Training loss 0.11295853555202484 Validation loss 0.11378650367259979 Accuracy 0.7035000324249268\n",
      "Iteration 290 Training loss 0.11095702648162842 Validation loss 0.11316952109336853 Accuracy 0.7051250338554382\n",
      "Iteration 300 Training loss 0.11122428625822067 Validation loss 0.11253130435943604 Accuracy 0.7175000309944153\n",
      "Iteration 310 Training loss 0.11130460351705551 Validation loss 0.1119128167629242 Accuracy 0.7251250147819519\n",
      "Iteration 320 Training loss 0.10985273122787476 Validation loss 0.11126255989074707 Accuracy 0.721875011920929\n",
      "Iteration 330 Training loss 0.11197946965694427 Validation loss 0.11063212901353836 Accuracy 0.7240000367164612\n",
      "Iteration 340 Training loss 0.11040907353162766 Validation loss 0.10995689779520035 Accuracy 0.7357500195503235\n",
      "Iteration 350 Training loss 0.10559430718421936 Validation loss 0.109294094145298 Accuracy 0.7305000424385071\n",
      "Iteration 360 Training loss 0.10954295843839645 Validation loss 0.10865043848752975 Accuracy 0.7255000472068787\n",
      "Iteration 370 Training loss 0.1082962229847908 Validation loss 0.1079115942120552 Accuracy 0.7361250519752502\n",
      "Iteration 380 Training loss 0.10929186642169952 Validation loss 0.10722336918115616 Accuracy 0.7378750443458557\n",
      "Iteration 390 Training loss 0.10715131461620331 Validation loss 0.10651329904794693 Accuracy 0.7392500638961792\n",
      "Iteration 400 Training loss 0.10558973252773285 Validation loss 0.10580719262361526 Accuracy 0.7402500510215759\n",
      "Iteration 410 Training loss 0.10467255115509033 Validation loss 0.10512010008096695 Accuracy 0.7395000457763672\n",
      "Iteration 420 Training loss 0.10309846699237823 Validation loss 0.10440416634082794 Accuracy 0.7416250109672546\n",
      "Iteration 430 Training loss 0.10294951498508453 Validation loss 0.10370064526796341 Accuracy 0.7435000538825989\n",
      "Iteration 440 Training loss 0.10257615149021149 Validation loss 0.103031225502491 Accuracy 0.7450000643730164\n",
      "Iteration 450 Training loss 0.1024819165468216 Validation loss 0.10238690674304962 Accuracy 0.7455000281333923\n",
      "Iteration 460 Training loss 0.10018076002597809 Validation loss 0.10169459879398346 Accuracy 0.7457500100135803\n",
      "Iteration 470 Training loss 0.09848402440547943 Validation loss 0.10101952403783798 Accuracy 0.7457500100135803\n",
      "Iteration 480 Training loss 0.10178877413272858 Validation loss 0.10037302225828171 Accuracy 0.7453750371932983\n",
      "Iteration 490 Training loss 0.09751712530851364 Validation loss 0.09975741803646088 Accuracy 0.7455000281333923\n",
      "Iteration 500 Training loss 0.10108835995197296 Validation loss 0.09909859299659729 Accuracy 0.7473750114440918\n",
      "Iteration 510 Training loss 0.09647805988788605 Validation loss 0.09849175810813904 Accuracy 0.7472500205039978\n",
      "Iteration 520 Training loss 0.09702835232019424 Validation loss 0.09790407866239548 Accuracy 0.7490000128746033\n",
      "Iteration 530 Training loss 0.09752210229635239 Validation loss 0.097310870885849 Accuracy 0.7486250400543213\n",
      "Iteration 540 Training loss 0.09715737402439117 Validation loss 0.0967499241232872 Accuracy 0.7487500309944153\n",
      "Iteration 550 Training loss 0.09880515187978745 Validation loss 0.09619378298521042 Accuracy 0.7485000491142273\n",
      "Iteration 560 Training loss 0.09609565883874893 Validation loss 0.09567249566316605 Accuracy 0.749125063419342\n",
      "Iteration 570 Training loss 0.09655209630727768 Validation loss 0.09515262395143509 Accuracy 0.749750018119812\n",
      "Iteration 580 Training loss 0.0950784683227539 Validation loss 0.09468814730644226 Accuracy 0.7508750557899475\n",
      "Iteration 590 Training loss 0.09321288019418716 Validation loss 0.0942227765917778 Accuracy 0.7503750324249268\n",
      "Iteration 600 Training loss 0.09369157254695892 Validation loss 0.09377056360244751 Accuracy 0.7535000443458557\n",
      "Iteration 610 Training loss 0.10002002120018005 Validation loss 0.09334088861942291 Accuracy 0.7532500624656677\n",
      "Iteration 620 Training loss 0.09706538915634155 Validation loss 0.0928640365600586 Accuracy 0.752875030040741\n",
      "Iteration 630 Training loss 0.0896449089050293 Validation loss 0.09247201681137085 Accuracy 0.7538750171661377\n",
      "Iteration 640 Training loss 0.08701477199792862 Validation loss 0.09204709529876709 Accuracy 0.7533750534057617\n",
      "Iteration 650 Training loss 0.0869930163025856 Validation loss 0.0916605293750763 Accuracy 0.7541250586509705\n",
      "Iteration 660 Training loss 0.09268945455551147 Validation loss 0.09128878265619278 Accuracy 0.7548750638961792\n",
      "Iteration 670 Training loss 0.09121432900428772 Validation loss 0.09091450273990631 Accuracy 0.7562500238418579\n",
      "Iteration 680 Training loss 0.09003812819719315 Validation loss 0.09053632616996765 Accuracy 0.7557500600814819\n",
      "Iteration 690 Training loss 0.0978630781173706 Validation loss 0.09019993990659714 Accuracy 0.7561250329017639\n",
      "Iteration 700 Training loss 0.08488410711288452 Validation loss 0.08983557671308517 Accuracy 0.7571250200271606\n",
      "Iteration 710 Training loss 0.08829464018344879 Validation loss 0.0894998162984848 Accuracy 0.7576250433921814\n",
      "Iteration 720 Training loss 0.0869097113609314 Validation loss 0.089225172996521 Accuracy 0.7572500109672546\n",
      "Iteration 730 Training loss 0.09241233021020889 Validation loss 0.08887903392314911 Accuracy 0.7592500448226929\n",
      "Iteration 740 Training loss 0.08989213407039642 Validation loss 0.08862049132585526 Accuracy 0.7592500448226929\n",
      "Iteration 750 Training loss 0.08999384939670563 Validation loss 0.08833251148462296 Accuracy 0.7597500085830688\n",
      "Iteration 760 Training loss 0.09232745319604874 Validation loss 0.08807039260864258 Accuracy 0.7608750462532043\n",
      "Iteration 770 Training loss 0.08918840438127518 Validation loss 0.08774840086698532 Accuracy 0.7612500190734863\n",
      "Iteration 780 Training loss 0.08537967503070831 Validation loss 0.0875166729092598 Accuracy 0.7601250410079956\n",
      "Iteration 790 Training loss 0.083948515355587 Validation loss 0.0871991440653801 Accuracy 0.7617500424385071\n",
      "Iteration 800 Training loss 0.08654917776584625 Validation loss 0.08696941286325455 Accuracy 0.7617500424385071\n",
      "Iteration 810 Training loss 0.08588296920061111 Validation loss 0.08670075982809067 Accuracy 0.7632500529289246\n",
      "Iteration 820 Training loss 0.08182067424058914 Validation loss 0.08645156025886536 Accuracy 0.7635000348091125\n",
      "Iteration 830 Training loss 0.0886494591832161 Validation loss 0.08622951060533524 Accuracy 0.7633750438690186\n",
      "Iteration 840 Training loss 0.0856594443321228 Validation loss 0.08598906546831131 Accuracy 0.7645000219345093\n",
      "Iteration 850 Training loss 0.08595116436481476 Validation loss 0.08575630187988281 Accuracy 0.764875054359436\n",
      "Iteration 860 Training loss 0.08998819440603256 Validation loss 0.0856635794043541 Accuracy 0.7636250257492065\n",
      "Iteration 870 Training loss 0.08833852410316467 Validation loss 0.08532962203025818 Accuracy 0.7657500505447388\n",
      "Iteration 880 Training loss 0.08810758590698242 Validation loss 0.08513540029525757 Accuracy 0.7667500376701355\n",
      "Iteration 890 Training loss 0.09075355529785156 Validation loss 0.08493229746818542 Accuracy 0.7662500143051147\n",
      "Iteration 900 Training loss 0.08255898952484131 Validation loss 0.08474907279014587 Accuracy 0.7671250104904175\n",
      "Iteration 910 Training loss 0.08788169920444489 Validation loss 0.08456875383853912 Accuracy 0.7678750157356262\n",
      "Iteration 920 Training loss 0.07703004032373428 Validation loss 0.08435263484716415 Accuracy 0.768375039100647\n",
      "Iteration 930 Training loss 0.0860258936882019 Validation loss 0.08413396030664444 Accuracy 0.768375039100647\n",
      "Iteration 940 Training loss 0.07857412099838257 Validation loss 0.08395860344171524 Accuracy 0.7688750624656677\n",
      "Iteration 950 Training loss 0.08394858986139297 Validation loss 0.08378169685602188 Accuracy 0.768375039100647\n",
      "Iteration 960 Training loss 0.0870414525270462 Validation loss 0.0835854709148407 Accuracy 0.7690000534057617\n",
      "Iteration 970 Training loss 0.08496209979057312 Validation loss 0.08340293914079666 Accuracy 0.7688750624656677\n",
      "Iteration 980 Training loss 0.08516985923051834 Validation loss 0.08323801308870316 Accuracy 0.7688750624656677\n",
      "Iteration 990 Training loss 0.08830976486206055 Validation loss 0.08306573331356049 Accuracy 0.7702500224113464\n",
      "Iteration 1000 Training loss 0.0902993381023407 Validation loss 0.08292009681463242 Accuracy 0.7712500095367432\n",
      "Iteration 1010 Training loss 0.08626513928174973 Validation loss 0.08276107907295227 Accuracy 0.7701250314712524\n",
      "Iteration 1020 Training loss 0.08271272480487823 Validation loss 0.08262576907873154 Accuracy 0.7712500095367432\n",
      "Iteration 1030 Training loss 0.07569755613803864 Validation loss 0.08250627666711807 Accuracy 0.7711250185966492\n",
      "Iteration 1040 Training loss 0.0806470438838005 Validation loss 0.08233862370252609 Accuracy 0.7716250419616699\n",
      "Iteration 1050 Training loss 0.08433479070663452 Validation loss 0.08217282593250275 Accuracy 0.7726250290870667\n",
      "Iteration 1060 Training loss 0.0827551856637001 Validation loss 0.08209371566772461 Accuracy 0.7731250524520874\n",
      "Iteration 1070 Training loss 0.08452484011650085 Validation loss 0.08189354836940765 Accuracy 0.7723750472068787\n",
      "Iteration 1080 Training loss 0.08294137567281723 Validation loss 0.0817965641617775 Accuracy 0.7726250290870667\n",
      "Iteration 1090 Training loss 0.0835752859711647 Validation loss 0.0816134512424469 Accuracy 0.7733750343322754\n",
      "Iteration 1100 Training loss 0.0804288238286972 Validation loss 0.08149406313896179 Accuracy 0.7738750576972961\n",
      "Iteration 1110 Training loss 0.08161729574203491 Validation loss 0.08136319369077682 Accuracy 0.7740000486373901\n",
      "Iteration 1120 Training loss 0.07789608836174011 Validation loss 0.08124019205570221 Accuracy 0.7740000486373901\n",
      "Iteration 1130 Training loss 0.08513131737709045 Validation loss 0.0810912474989891 Accuracy 0.7751250267028809\n",
      "Iteration 1140 Training loss 0.07651800662279129 Validation loss 0.08101620525121689 Accuracy 0.7750000357627869\n",
      "Iteration 1150 Training loss 0.08178745210170746 Validation loss 0.08085707575082779 Accuracy 0.7753750085830688\n",
      "Iteration 1160 Training loss 0.0875249058008194 Validation loss 0.08072426170110703 Accuracy 0.7766250371932983\n",
      "Iteration 1170 Training loss 0.08908329159021378 Validation loss 0.08061575889587402 Accuracy 0.7768750190734863\n",
      "Iteration 1180 Training loss 0.0788378119468689 Validation loss 0.0805370956659317 Accuracy 0.7760000228881836\n",
      "Iteration 1190 Training loss 0.0844629630446434 Validation loss 0.08041328936815262 Accuracy 0.7763750553131104\n",
      "Iteration 1200 Training loss 0.07579416781663895 Validation loss 0.08026114851236343 Accuracy 0.7773750424385071\n",
      "Iteration 1210 Training loss 0.0815722718834877 Validation loss 0.08021536469459534 Accuracy 0.7767500281333923\n",
      "Iteration 1220 Training loss 0.08250720053911209 Validation loss 0.08006010949611664 Accuracy 0.7777500152587891\n",
      "Iteration 1230 Training loss 0.07611890882253647 Validation loss 0.07994731515645981 Accuracy 0.7776250243186951\n",
      "Iteration 1240 Training loss 0.08220675587654114 Validation loss 0.07987350225448608 Accuracy 0.7788750529289246\n",
      "Iteration 1250 Training loss 0.07933767884969711 Validation loss 0.07974812388420105 Accuracy 0.7790000438690186\n",
      "Iteration 1260 Training loss 0.08456609398126602 Validation loss 0.07965018600225449 Accuracy 0.7791250348091125\n",
      "Iteration 1270 Training loss 0.07414901256561279 Validation loss 0.07953554391860962 Accuracy 0.7796250581741333\n",
      "Iteration 1280 Training loss 0.07269716262817383 Validation loss 0.07946469634771347 Accuracy 0.7797500491142273\n",
      "Iteration 1290 Training loss 0.07197772711515427 Validation loss 0.07934992760419846 Accuracy 0.780500054359436\n",
      "Iteration 1300 Training loss 0.0796135812997818 Validation loss 0.07923373579978943 Accuracy 0.78062504529953\n",
      "Iteration 1310 Training loss 0.08395934849977493 Validation loss 0.0791463553905487 Accuracy 0.7816250324249268\n",
      "Iteration 1320 Training loss 0.08301802724599838 Validation loss 0.07906699180603027 Accuracy 0.7816250324249268\n",
      "Iteration 1330 Training loss 0.07675515115261078 Validation loss 0.07921020686626434 Accuracy 0.7795000076293945\n",
      "Iteration 1340 Training loss 0.07745920121669769 Validation loss 0.07888954877853394 Accuracy 0.7821250557899475\n",
      "Iteration 1350 Training loss 0.08142798393964767 Validation loss 0.07879985123872757 Accuracy 0.7826250195503235\n",
      "Iteration 1360 Training loss 0.07890051603317261 Validation loss 0.07872089743614197 Accuracy 0.7827500104904175\n",
      "Iteration 1370 Training loss 0.07304943352937698 Validation loss 0.07866278290748596 Accuracy 0.7830000519752502\n",
      "Iteration 1380 Training loss 0.07631705701351166 Validation loss 0.07855980098247528 Accuracy 0.7832500338554382\n",
      "Iteration 1390 Training loss 0.06942962855100632 Validation loss 0.07847470790147781 Accuracy 0.784250020980835\n",
      "Iteration 1400 Training loss 0.08236724883317947 Validation loss 0.0784090980887413 Accuracy 0.7832500338554382\n",
      "Iteration 1410 Training loss 0.07567863911390305 Validation loss 0.07834114134311676 Accuracy 0.783625066280365\n",
      "Iteration 1420 Training loss 0.07866965979337692 Validation loss 0.07826743274927139 Accuracy 0.783625066280365\n",
      "Iteration 1430 Training loss 0.07591080665588379 Validation loss 0.0781686007976532 Accuracy 0.784250020980835\n",
      "Iteration 1440 Training loss 0.07879028469324112 Validation loss 0.07806336134672165 Accuracy 0.7860000133514404\n",
      "Iteration 1450 Training loss 0.08202651143074036 Validation loss 0.07808768004179001 Accuracy 0.784000039100647\n",
      "Iteration 1460 Training loss 0.08068237453699112 Validation loss 0.07797626405954361 Accuracy 0.7853750586509705\n",
      "Iteration 1470 Training loss 0.080562524497509 Validation loss 0.07784796506166458 Accuracy 0.7863750457763672\n",
      "Iteration 1480 Training loss 0.07605735957622528 Validation loss 0.0777961015701294 Accuracy 0.7853750586509705\n",
      "Iteration 1490 Training loss 0.07448433339595795 Validation loss 0.07785169780254364 Accuracy 0.784375011920929\n",
      "Iteration 1500 Training loss 0.07990968227386475 Validation loss 0.07765179127454758 Accuracy 0.7855000495910645\n",
      "Iteration 1510 Training loss 0.0838869959115982 Validation loss 0.07757455855607986 Accuracy 0.7866250276565552\n",
      "Iteration 1520 Training loss 0.07133245468139648 Validation loss 0.07752121984958649 Accuracy 0.7860000133514404\n",
      "Iteration 1530 Training loss 0.07873836159706116 Validation loss 0.07742434740066528 Accuracy 0.7871250510215759\n",
      "Iteration 1540 Training loss 0.08019859343767166 Validation loss 0.07738204300403595 Accuracy 0.7861250638961792\n",
      "Iteration 1550 Training loss 0.07601220160722733 Validation loss 0.07729317992925644 Accuracy 0.7870000600814819\n",
      "Iteration 1560 Training loss 0.08470995724201202 Validation loss 0.07731877267360687 Accuracy 0.7871250510215759\n",
      "Iteration 1570 Training loss 0.08084079623222351 Validation loss 0.07716485112905502 Accuracy 0.7878750562667847\n",
      "Iteration 1580 Training loss 0.06936134397983551 Validation loss 0.07710935920476913 Accuracy 0.7873750329017639\n",
      "Iteration 1590 Training loss 0.07760916650295258 Validation loss 0.07705733925104141 Accuracy 0.7875000238418579\n",
      "Iteration 1600 Training loss 0.08148814737796783 Validation loss 0.07698780298233032 Accuracy 0.7875000238418579\n",
      "Iteration 1610 Training loss 0.07906291633844376 Validation loss 0.07693081349134445 Accuracy 0.7880000472068787\n",
      "Iteration 1620 Training loss 0.07367800176143646 Validation loss 0.07686685770750046 Accuracy 0.7876250147819519\n",
      "Iteration 1630 Training loss 0.07657156139612198 Validation loss 0.07684598118066788 Accuracy 0.7883750200271606\n",
      "Iteration 1640 Training loss 0.0811157375574112 Validation loss 0.07676874846220016 Accuracy 0.7881250381469727\n",
      "Iteration 1650 Training loss 0.07620014995336533 Validation loss 0.07669717073440552 Accuracy 0.7880000472068787\n",
      "Iteration 1660 Training loss 0.07851108908653259 Validation loss 0.07666098326444626 Accuracy 0.7891250252723694\n",
      "Iteration 1670 Training loss 0.07520639151334763 Validation loss 0.0765959694981575 Accuracy 0.7901250123977661\n",
      "Iteration 1680 Training loss 0.07305534183979034 Validation loss 0.07652217894792557 Accuracy 0.7891250252723694\n",
      "Iteration 1690 Training loss 0.07734179496765137 Validation loss 0.07653726637363434 Accuracy 0.7887500524520874\n",
      "Iteration 1700 Training loss 0.07776688039302826 Validation loss 0.07647203654050827 Accuracy 0.7892500162124634\n",
      "Iteration 1710 Training loss 0.07652387768030167 Validation loss 0.0764005184173584 Accuracy 0.7897500395774841\n",
      "Iteration 1720 Training loss 0.08166800439357758 Validation loss 0.07633833587169647 Accuracy 0.7891250252723694\n",
      "Iteration 1730 Training loss 0.06782712042331696 Validation loss 0.07631752640008926 Accuracy 0.7900000214576721\n",
      "Iteration 1740 Training loss 0.07304937392473221 Validation loss 0.0762556940317154 Accuracy 0.7902500629425049\n",
      "Iteration 1750 Training loss 0.07613351941108704 Validation loss 0.07616215199232101 Accuracy 0.7906250357627869\n",
      "Iteration 1760 Training loss 0.07892165333032608 Validation loss 0.0761197954416275 Accuracy 0.7905000448226929\n",
      "Iteration 1770 Training loss 0.07501106709241867 Validation loss 0.07606765627861023 Accuracy 0.7912500500679016\n",
      "Iteration 1780 Training loss 0.06460000574588776 Validation loss 0.07601669430732727 Accuracy 0.7911250591278076\n",
      "Iteration 1790 Training loss 0.07084934413433075 Validation loss 0.07605762034654617 Accuracy 0.7915000319480896\n",
      "Iteration 1800 Training loss 0.07434269040822983 Validation loss 0.0759955644607544 Accuracy 0.7915000319480896\n",
      "Iteration 1810 Training loss 0.07568871974945068 Validation loss 0.07592558860778809 Accuracy 0.7916250228881836\n",
      "Iteration 1820 Training loss 0.07313165813684464 Validation loss 0.07583063095808029 Accuracy 0.7912500500679016\n",
      "Iteration 1830 Training loss 0.07097706943750381 Validation loss 0.07577585428953171 Accuracy 0.7908750176429749\n",
      "Iteration 1840 Training loss 0.07496620714664459 Validation loss 0.07577577978372574 Accuracy 0.7916250228881836\n",
      "Iteration 1850 Training loss 0.07472603023052216 Validation loss 0.07571398466825485 Accuracy 0.7913750410079956\n",
      "Iteration 1860 Training loss 0.08507903665304184 Validation loss 0.07570895552635193 Accuracy 0.7927500605583191\n",
      "Iteration 1870 Training loss 0.0747726783156395 Validation loss 0.07560334354639053 Accuracy 0.7920000553131104\n",
      "Iteration 1880 Training loss 0.06905543804168701 Validation loss 0.0755612850189209 Accuracy 0.7917500138282776\n",
      "Iteration 1890 Training loss 0.07380829751491547 Validation loss 0.07550891488790512 Accuracy 0.7928750514984131\n",
      "Iteration 1900 Training loss 0.07556455582380295 Validation loss 0.07547669112682343 Accuracy 0.7925000190734863\n",
      "Iteration 1910 Training loss 0.07837358862161636 Validation loss 0.07547091692686081 Accuracy 0.7918750643730164\n",
      "Iteration 1920 Training loss 0.06604398787021637 Validation loss 0.07540234923362732 Accuracy 0.7930000424385071\n",
      "Iteration 1930 Training loss 0.0748293474316597 Validation loss 0.0753852128982544 Accuracy 0.7917500138282776\n",
      "Iteration 1940 Training loss 0.08128052204847336 Validation loss 0.07533309608697891 Accuracy 0.7927500605583191\n",
      "Iteration 1950 Training loss 0.07277613133192062 Validation loss 0.07529819011688232 Accuracy 0.7923750281333923\n",
      "Iteration 1960 Training loss 0.0684836208820343 Validation loss 0.07537209987640381 Accuracy 0.7928750514984131\n",
      "Iteration 1970 Training loss 0.07772310078144073 Validation loss 0.07524769753217697 Accuracy 0.7936250567436218\n",
      "Iteration 1980 Training loss 0.07132722437381744 Validation loss 0.07517683506011963 Accuracy 0.7938750386238098\n",
      "Iteration 1990 Training loss 0.07373211532831192 Validation loss 0.07515423744916916 Accuracy 0.7937500476837158\n",
      "Iteration 2000 Training loss 0.06784453988075256 Validation loss 0.07510066777467728 Accuracy 0.7930000424385071\n",
      "Iteration 2010 Training loss 0.08176133036613464 Validation loss 0.07506067305803299 Accuracy 0.7933750152587891\n",
      "Iteration 2020 Training loss 0.07753005623817444 Validation loss 0.07508715987205505 Accuracy 0.7932500243186951\n",
      "Iteration 2030 Training loss 0.07543663680553436 Validation loss 0.07499469071626663 Accuracy 0.7932500243186951\n",
      "Iteration 2040 Training loss 0.0752001702785492 Validation loss 0.07495597004890442 Accuracy 0.7938750386238098\n",
      "Iteration 2050 Training loss 0.0747697651386261 Validation loss 0.07492338120937347 Accuracy 0.7941250205039978\n",
      "Iteration 2060 Training loss 0.0718640387058258 Validation loss 0.07493016868829727 Accuracy 0.7940000295639038\n",
      "Iteration 2070 Training loss 0.076322041451931 Validation loss 0.0748581662774086 Accuracy 0.7937500476837158\n",
      "Iteration 2080 Training loss 0.07322431355714798 Validation loss 0.07482489943504333 Accuracy 0.7952500581741333\n",
      "Iteration 2090 Training loss 0.07038703560829163 Validation loss 0.07478520274162292 Accuracy 0.7945000529289246\n",
      "Iteration 2100 Training loss 0.07230309396982193 Validation loss 0.07480881363153458 Accuracy 0.7947500348091125\n",
      "Iteration 2110 Training loss 0.07040201872587204 Validation loss 0.07473766058683395 Accuracy 0.7946250438690186\n",
      "Iteration 2120 Training loss 0.07287418097257614 Validation loss 0.07471123337745667 Accuracy 0.7947500348091125\n",
      "Iteration 2130 Training loss 0.07190269231796265 Validation loss 0.0746530145406723 Accuracy 0.7951250672340393\n",
      "Iteration 2140 Training loss 0.06641519069671631 Validation loss 0.0747576430439949 Accuracy 0.7948750257492065\n",
      "Iteration 2150 Training loss 0.07893022149801254 Validation loss 0.07469239830970764 Accuracy 0.7948750257492065\n",
      "Iteration 2160 Training loss 0.0824098214507103 Validation loss 0.07456795126199722 Accuracy 0.7946250438690186\n",
      "Iteration 2170 Training loss 0.07512049376964569 Validation loss 0.0745367631316185 Accuracy 0.7951250672340393\n",
      "Iteration 2180 Training loss 0.07608155161142349 Validation loss 0.07454229891300201 Accuracy 0.7941250205039978\n",
      "Iteration 2190 Training loss 0.07665325701236725 Validation loss 0.07448773086071014 Accuracy 0.7945000529289246\n",
      "Iteration 2200 Training loss 0.08410421013832092 Validation loss 0.07443573325872421 Accuracy 0.7956250309944153\n",
      "Iteration 2210 Training loss 0.07432086765766144 Validation loss 0.07439981400966644 Accuracy 0.7958750128746033\n",
      "Iteration 2220 Training loss 0.07341577112674713 Validation loss 0.07440076768398285 Accuracy 0.7951250672340393\n",
      "Iteration 2230 Training loss 0.07147843390703201 Validation loss 0.07439509779214859 Accuracy 0.7955000400543213\n",
      "Iteration 2240 Training loss 0.07534076273441315 Validation loss 0.07439205795526505 Accuracy 0.7950000166893005\n",
      "Iteration 2250 Training loss 0.07949277758598328 Validation loss 0.07429081946611404 Accuracy 0.796000063419342\n",
      "Iteration 2260 Training loss 0.07523565739393234 Validation loss 0.07424736022949219 Accuracy 0.7956250309944153\n",
      "Iteration 2270 Training loss 0.07175001502037048 Validation loss 0.07422690093517303 Accuracy 0.7958750128746033\n",
      "Iteration 2280 Training loss 0.07212706655263901 Validation loss 0.07421012967824936 Accuracy 0.7957500219345093\n",
      "Iteration 2290 Training loss 0.06996498256921768 Validation loss 0.07420067489147186 Accuracy 0.7946250438690186\n",
      "Iteration 2300 Training loss 0.07471217960119247 Validation loss 0.07413876801729202 Accuracy 0.796375036239624\n",
      "Iteration 2310 Training loss 0.06969838589429855 Validation loss 0.0740988478064537 Accuracy 0.79625004529953\n",
      "Iteration 2320 Training loss 0.08004437386989594 Validation loss 0.07411834597587585 Accuracy 0.7957500219345093\n",
      "Iteration 2330 Training loss 0.08049610257148743 Validation loss 0.07405346632003784 Accuracy 0.796000063419342\n",
      "Iteration 2340 Training loss 0.07445331662893295 Validation loss 0.0741269588470459 Accuracy 0.7951250672340393\n",
      "Iteration 2350 Training loss 0.08056192100048065 Validation loss 0.07400095462799072 Accuracy 0.796500027179718\n",
      "Iteration 2360 Training loss 0.07664041221141815 Validation loss 0.07396408170461655 Accuracy 0.796375036239624\n",
      "Iteration 2370 Training loss 0.06798074394464493 Validation loss 0.0739496573805809 Accuracy 0.796625018119812\n",
      "Iteration 2380 Training loss 0.08278919756412506 Validation loss 0.07390126585960388 Accuracy 0.7970000505447388\n",
      "Iteration 2390 Training loss 0.07392948865890503 Validation loss 0.07387613505125046 Accuracy 0.7970000505447388\n",
      "Iteration 2400 Training loss 0.0665614902973175 Validation loss 0.07388699799776077 Accuracy 0.79625004529953\n",
      "Iteration 2410 Training loss 0.0744505226612091 Validation loss 0.07393117249011993 Accuracy 0.7951250672340393\n",
      "Iteration 2420 Training loss 0.07603314518928528 Validation loss 0.07379617542028427 Accuracy 0.7972500324249268\n",
      "Iteration 2430 Training loss 0.08418529480695724 Validation loss 0.07376988232135773 Accuracy 0.7971250414848328\n",
      "Iteration 2440 Training loss 0.06890098750591278 Validation loss 0.07374103367328644 Accuracy 0.7968750596046448\n",
      "Iteration 2450 Training loss 0.07786723226308823 Validation loss 0.07370993494987488 Accuracy 0.796625018119812\n",
      "Iteration 2460 Training loss 0.07351399213075638 Validation loss 0.0738418772816658 Accuracy 0.79625004529953\n",
      "Iteration 2470 Training loss 0.0671362355351448 Validation loss 0.07367382943630219 Accuracy 0.796750009059906\n",
      "Iteration 2480 Training loss 0.07665778696537018 Validation loss 0.07368507236242294 Accuracy 0.796500027179718\n",
      "Iteration 2490 Training loss 0.06935188174247742 Validation loss 0.07361213862895966 Accuracy 0.7972500324249268\n",
      "Iteration 2500 Training loss 0.07708201557397842 Validation loss 0.07358492165803909 Accuracy 0.7976250648498535\n",
      "Iteration 2510 Training loss 0.07594063878059387 Validation loss 0.07357074320316315 Accuracy 0.796500027179718\n",
      "Iteration 2520 Training loss 0.07247944921255112 Validation loss 0.07353894412517548 Accuracy 0.7983750104904175\n",
      "Iteration 2530 Training loss 0.07226338982582092 Validation loss 0.07351043820381165 Accuracy 0.7976250648498535\n",
      "Iteration 2540 Training loss 0.07405360043048859 Validation loss 0.07350537180900574 Accuracy 0.796750009059906\n",
      "Iteration 2550 Training loss 0.06517679244279861 Validation loss 0.07346460223197937 Accuracy 0.7977500557899475\n",
      "Iteration 2560 Training loss 0.07659529894590378 Validation loss 0.07344119250774384 Accuracy 0.7977500557899475\n",
      "Iteration 2570 Training loss 0.07652585953474045 Validation loss 0.07344365119934082 Accuracy 0.7980000376701355\n",
      "Iteration 2580 Training loss 0.07323537021875381 Validation loss 0.07340479642152786 Accuracy 0.7975000143051147\n",
      "Iteration 2590 Training loss 0.07621943950653076 Validation loss 0.07342185080051422 Accuracy 0.7968750596046448\n",
      "Iteration 2600 Training loss 0.06671798974275589 Validation loss 0.07333491742610931 Accuracy 0.7968750596046448\n",
      "Iteration 2610 Training loss 0.08007659018039703 Validation loss 0.07335580885410309 Accuracy 0.7976250648498535\n",
      "Iteration 2620 Training loss 0.0667579248547554 Validation loss 0.07329860329627991 Accuracy 0.7977500557899475\n",
      "Iteration 2630 Training loss 0.07311851531267166 Validation loss 0.07326474785804749 Accuracy 0.7982500195503235\n",
      "Iteration 2640 Training loss 0.073794424533844 Validation loss 0.0732751339673996 Accuracy 0.7973750233650208\n",
      "Iteration 2650 Training loss 0.08104858547449112 Validation loss 0.07328470051288605 Accuracy 0.7976250648498535\n",
      "Iteration 2660 Training loss 0.06798277050256729 Validation loss 0.07321249693632126 Accuracy 0.7980000376701355\n",
      "Iteration 2670 Training loss 0.07401538640260696 Validation loss 0.07318928837776184 Accuracy 0.7978750467300415\n",
      "Iteration 2680 Training loss 0.06321489065885544 Validation loss 0.07317650318145752 Accuracy 0.7978750467300415\n",
      "Iteration 2690 Training loss 0.0681290552020073 Validation loss 0.07315851002931595 Accuracy 0.7986250519752502\n",
      "Iteration 2700 Training loss 0.07777782529592514 Validation loss 0.07314177602529526 Accuracy 0.7987500429153442\n",
      "Iteration 2710 Training loss 0.0777454823255539 Validation loss 0.07310072332620621 Accuracy 0.799250066280365\n",
      "Iteration 2720 Training loss 0.07757590711116791 Validation loss 0.07307213544845581 Accuracy 0.799375057220459\n",
      "Iteration 2730 Training loss 0.06805232167243958 Validation loss 0.07302254438400269 Accuracy 0.7981250286102295\n",
      "Iteration 2740 Training loss 0.06987344473600388 Validation loss 0.07314115017652512 Accuracy 0.7977500557899475\n",
      "Iteration 2750 Training loss 0.07721387594938278 Validation loss 0.07318083196878433 Accuracy 0.796750009059906\n",
      "Iteration 2760 Training loss 0.0830935463309288 Validation loss 0.07295485585927963 Accuracy 0.7981250286102295\n",
      "Iteration 2770 Training loss 0.07425507158041 Validation loss 0.07293600589036942 Accuracy 0.7986250519752502\n",
      "Iteration 2780 Training loss 0.07418768107891083 Validation loss 0.0729382336139679 Accuracy 0.7987500429153442\n",
      "Iteration 2790 Training loss 0.08010537922382355 Validation loss 0.07295316457748413 Accuracy 0.7991250157356262\n",
      "Iteration 2800 Training loss 0.06773335486650467 Validation loss 0.07287026196718216 Accuracy 0.7980000376701355\n",
      "Iteration 2810 Training loss 0.07761643081903458 Validation loss 0.07284707576036453 Accuracy 0.7985000610351562\n",
      "Iteration 2820 Training loss 0.0810185968875885 Validation loss 0.0728253573179245 Accuracy 0.7982500195503235\n",
      "Iteration 2830 Training loss 0.07200945913791656 Validation loss 0.07294786721467972 Accuracy 0.7983750104904175\n",
      "Iteration 2840 Training loss 0.07330203056335449 Validation loss 0.07284219563007355 Accuracy 0.7987500429153442\n",
      "Iteration 2850 Training loss 0.07288617640733719 Validation loss 0.07290605455636978 Accuracy 0.7983750104904175\n",
      "Iteration 2860 Training loss 0.07183211296796799 Validation loss 0.07273608446121216 Accuracy 0.7988750338554382\n",
      "Iteration 2870 Training loss 0.07036435604095459 Validation loss 0.07275611907243729 Accuracy 0.7983750104904175\n",
      "Iteration 2880 Training loss 0.0693356916308403 Validation loss 0.07275790721178055 Accuracy 0.799375057220459\n",
      "Iteration 2890 Training loss 0.06160421296954155 Validation loss 0.07277558743953705 Accuracy 0.7982500195503235\n",
      "Iteration 2900 Training loss 0.07903008908033371 Validation loss 0.07265381515026093 Accuracy 0.7988750338554382\n",
      "Iteration 2910 Training loss 0.07936158776283264 Validation loss 0.07266044616699219 Accuracy 0.7990000247955322\n",
      "Iteration 2920 Training loss 0.07377505302429199 Validation loss 0.07260335236787796 Accuracy 0.799250066280365\n",
      "Iteration 2930 Training loss 0.07184424251317978 Validation loss 0.0725816935300827 Accuracy 0.799750030040741\n",
      "Iteration 2940 Training loss 0.06748507171869278 Validation loss 0.07257507741451263 Accuracy 0.799625039100647\n",
      "Iteration 2950 Training loss 0.07010187208652496 Validation loss 0.07254014909267426 Accuracy 0.799750030040741\n",
      "Iteration 2960 Training loss 0.0748240277171135 Validation loss 0.07260973751544952 Accuracy 0.7988750338554382\n",
      "Iteration 2970 Training loss 0.08080741763114929 Validation loss 0.07251520454883575 Accuracy 0.799500048160553\n",
      "Iteration 2980 Training loss 0.06905092298984528 Validation loss 0.07249002158641815 Accuracy 0.800000011920929\n",
      "Iteration 2990 Training loss 0.06816404312849045 Validation loss 0.07245989143848419 Accuracy 0.8008750677108765\n",
      "Iteration 3000 Training loss 0.06646289676427841 Validation loss 0.07271667569875717 Accuracy 0.7976250648498535\n",
      "Iteration 3010 Training loss 0.0635305643081665 Validation loss 0.07241691648960114 Accuracy 0.8003750443458557\n",
      "Iteration 3020 Training loss 0.07410811632871628 Validation loss 0.07239628583192825 Accuracy 0.8003750443458557\n",
      "Iteration 3030 Training loss 0.07020808011293411 Validation loss 0.07245334982872009 Accuracy 0.799625039100647\n",
      "Iteration 3040 Training loss 0.06367260217666626 Validation loss 0.07235564291477203 Accuracy 0.8010000586509705\n",
      "Iteration 3050 Training loss 0.06599414348602295 Validation loss 0.07234388589859009 Accuracy 0.799875020980835\n",
      "Iteration 3060 Training loss 0.07245224714279175 Validation loss 0.07241240888834 Accuracy 0.8002500534057617\n",
      "Iteration 3070 Training loss 0.06819896399974823 Validation loss 0.0723203495144844 Accuracy 0.799750030040741\n",
      "Iteration 3080 Training loss 0.07162871211767197 Validation loss 0.07227475196123123 Accuracy 0.8011250495910645\n",
      "Iteration 3090 Training loss 0.06790843605995178 Validation loss 0.07231926172971725 Accuracy 0.799875020980835\n",
      "Iteration 3100 Training loss 0.07166637480258942 Validation loss 0.07223203033208847 Accuracy 0.8010000586509705\n",
      "Iteration 3110 Training loss 0.08292218297719955 Validation loss 0.0724823847413063 Accuracy 0.7977500557899475\n",
      "Iteration 3120 Training loss 0.06740809231996536 Validation loss 0.07224304974079132 Accuracy 0.8002500534057617\n",
      "Iteration 3130 Training loss 0.07655785232782364 Validation loss 0.07217229157686234 Accuracy 0.8012500405311584\n",
      "Iteration 3140 Training loss 0.06691630184650421 Validation loss 0.07216601818799973 Accuracy 0.8003750443458557\n",
      "Iteration 3150 Training loss 0.06835442036390305 Validation loss 0.07213626801967621 Accuracy 0.8015000224113464\n",
      "Iteration 3160 Training loss 0.06792858242988586 Validation loss 0.0721987932920456 Accuracy 0.8007500171661377\n",
      "Iteration 3170 Training loss 0.06714006513357162 Validation loss 0.07218316942453384 Accuracy 0.8010000586509705\n",
      "Iteration 3180 Training loss 0.07053600251674652 Validation loss 0.07214803248643875 Accuracy 0.8012500405311584\n",
      "Iteration 3190 Training loss 0.06826357543468475 Validation loss 0.0722007304430008 Accuracy 0.8005000352859497\n",
      "Iteration 3200 Training loss 0.06902074068784714 Validation loss 0.07203204929828644 Accuracy 0.8008750677108765\n",
      "Iteration 3210 Training loss 0.0675649493932724 Validation loss 0.07201743870973587 Accuracy 0.8007500171661377\n",
      "Iteration 3220 Training loss 0.06949525326490402 Validation loss 0.07212666422128677 Accuracy 0.8006250262260437\n",
      "Iteration 3230 Training loss 0.07956857979297638 Validation loss 0.0719914585351944 Accuracy 0.8001250624656677\n",
      "Iteration 3240 Training loss 0.06908312439918518 Validation loss 0.07219959795475006 Accuracy 0.7988750338554382\n",
      "Iteration 3250 Training loss 0.06450214982032776 Validation loss 0.07201758027076721 Accuracy 0.8006250262260437\n",
      "Iteration 3260 Training loss 0.07913318276405334 Validation loss 0.07198917865753174 Accuracy 0.8008750677108765\n",
      "Iteration 3270 Training loss 0.07065603882074356 Validation loss 0.07190076261758804 Accuracy 0.8015000224113464\n",
      "Iteration 3280 Training loss 0.07214716821908951 Validation loss 0.0719069316983223 Accuracy 0.8010000586509705\n",
      "Iteration 3290 Training loss 0.07585066556930542 Validation loss 0.07187128067016602 Accuracy 0.8006250262260437\n",
      "Iteration 3300 Training loss 0.07491893321275711 Validation loss 0.07187323272228241 Accuracy 0.8010000586509705\n",
      "Iteration 3310 Training loss 0.07561793923377991 Validation loss 0.07182042300701141 Accuracy 0.8016250133514404\n",
      "Iteration 3320 Training loss 0.06969986110925674 Validation loss 0.07182435691356659 Accuracy 0.8007500171661377\n",
      "Iteration 3330 Training loss 0.06826253980398178 Validation loss 0.07178961485624313 Accuracy 0.8011250495910645\n",
      "Iteration 3340 Training loss 0.06758937984704971 Validation loss 0.0717688500881195 Accuracy 0.8016250133514404\n",
      "Iteration 3350 Training loss 0.07804305106401443 Validation loss 0.07175830751657486 Accuracy 0.8013750314712524\n",
      "Iteration 3360 Training loss 0.07600688934326172 Validation loss 0.07172349840402603 Accuracy 0.8023750185966492\n",
      "Iteration 3370 Training loss 0.07542187720537186 Validation loss 0.07201462239027023 Accuracy 0.799875020980835\n",
      "Iteration 3380 Training loss 0.06393507122993469 Validation loss 0.07175282388925552 Accuracy 0.8013750314712524\n",
      "Iteration 3390 Training loss 0.06652601808309555 Validation loss 0.07177285850048065 Accuracy 0.8010000586509705\n",
      "Iteration 3400 Training loss 0.06993720680475235 Validation loss 0.07177326083183289 Accuracy 0.8012500405311584\n",
      "Iteration 3410 Training loss 0.0810505673289299 Validation loss 0.0716598704457283 Accuracy 0.8022500276565552\n",
      "Iteration 3420 Training loss 0.06376972794532776 Validation loss 0.07163985073566437 Accuracy 0.8025000095367432\n",
      "Iteration 3430 Training loss 0.0766749233007431 Validation loss 0.07162924110889435 Accuracy 0.8017500638961792\n",
      "Iteration 3440 Training loss 0.0735900029540062 Validation loss 0.07157022505998611 Accuracy 0.8020000457763672\n",
      "Iteration 3450 Training loss 0.07012838870286942 Validation loss 0.07155757397413254 Accuracy 0.8015000224113464\n",
      "Iteration 3460 Training loss 0.07474269717931747 Validation loss 0.0715525671839714 Accuracy 0.8021250367164612\n",
      "Iteration 3470 Training loss 0.06787716597318649 Validation loss 0.07151184976100922 Accuracy 0.8026250600814819\n",
      "Iteration 3480 Training loss 0.07139566540718079 Validation loss 0.07154211401939392 Accuracy 0.8016250133514404\n",
      "Iteration 3490 Training loss 0.07222888618707657 Validation loss 0.07154755294322968 Accuracy 0.8017500638961792\n",
      "Iteration 3500 Training loss 0.06294973939657211 Validation loss 0.07144627720117569 Accuracy 0.8025000095367432\n",
      "Iteration 3510 Training loss 0.0728740319609642 Validation loss 0.07142693549394608 Accuracy 0.8025000095367432\n",
      "Iteration 3520 Training loss 0.07374553382396698 Validation loss 0.07141807675361633 Accuracy 0.8030000329017639\n",
      "Iteration 3530 Training loss 0.05924046412110329 Validation loss 0.07140010595321655 Accuracy 0.8022500276565552\n",
      "Iteration 3540 Training loss 0.07696978747844696 Validation loss 0.07137741893529892 Accuracy 0.8030000329017639\n",
      "Iteration 3550 Training loss 0.0710626170039177 Validation loss 0.07136940211057663 Accuracy 0.8022500276565552\n",
      "Iteration 3560 Training loss 0.08694321662187576 Validation loss 0.07133767753839493 Accuracy 0.8031250238418579\n",
      "Iteration 3570 Training loss 0.06885123252868652 Validation loss 0.07133247703313828 Accuracy 0.8022500276565552\n",
      "Iteration 3580 Training loss 0.06793428957462311 Validation loss 0.07130545377731323 Accuracy 0.8028750419616699\n",
      "Iteration 3590 Training loss 0.06855963915586472 Validation loss 0.07135222852230072 Accuracy 0.8018750548362732\n",
      "Iteration 3600 Training loss 0.07102790474891663 Validation loss 0.07130034267902374 Accuracy 0.8028750419616699\n",
      "Iteration 3610 Training loss 0.07044090330600739 Validation loss 0.0712505504488945 Accuracy 0.8035000562667847\n",
      "Iteration 3620 Training loss 0.07808980345726013 Validation loss 0.07124673575162888 Accuracy 0.8038750290870667\n",
      "Iteration 3630 Training loss 0.07485304772853851 Validation loss 0.0712398812174797 Accuracy 0.8036250472068787\n",
      "Iteration 3640 Training loss 0.07483182102441788 Validation loss 0.07119931280612946 Accuracy 0.8036250472068787\n",
      "Iteration 3650 Training loss 0.06283745914697647 Validation loss 0.07119546085596085 Accuracy 0.8040000200271606\n",
      "Iteration 3660 Training loss 0.08067134767770767 Validation loss 0.07117345184087753 Accuracy 0.8037500381469727\n",
      "Iteration 3670 Training loss 0.07517638802528381 Validation loss 0.0711456760764122 Accuracy 0.8031250238418579\n",
      "Iteration 3680 Training loss 0.07413721829652786 Validation loss 0.07128606736660004 Accuracy 0.8022500276565552\n",
      "Iteration 3690 Training loss 0.06837095320224762 Validation loss 0.07110269367694855 Accuracy 0.8033750653266907\n",
      "Iteration 3700 Training loss 0.07507756352424622 Validation loss 0.07110258936882019 Accuracy 0.8026250600814819\n",
      "Iteration 3710 Training loss 0.07259897887706757 Validation loss 0.07118728756904602 Accuracy 0.8042500615119934\n",
      "Iteration 3720 Training loss 0.06690250337123871 Validation loss 0.07105691730976105 Accuracy 0.8035000562667847\n",
      "Iteration 3730 Training loss 0.06680051982402802 Validation loss 0.07114484161138535 Accuracy 0.8023750185966492\n",
      "Iteration 3740 Training loss 0.06831053644418716 Validation loss 0.07101564854383469 Accuracy 0.8035000562667847\n",
      "Iteration 3750 Training loss 0.0671226903796196 Validation loss 0.07102087885141373 Accuracy 0.8035000562667847\n",
      "Iteration 3760 Training loss 0.07300592213869095 Validation loss 0.07099299132823944 Accuracy 0.8040000200271606\n",
      "Iteration 3770 Training loss 0.07020771503448486 Validation loss 0.07105197012424469 Accuracy 0.8027500510215759\n",
      "Iteration 3780 Training loss 0.07413887232542038 Validation loss 0.070942722260952 Accuracy 0.8043750524520874\n",
      "Iteration 3790 Training loss 0.07013528048992157 Validation loss 0.07098214328289032 Accuracy 0.8032500147819519\n",
      "Iteration 3800 Training loss 0.07206670939922333 Validation loss 0.07091034203767776 Accuracy 0.8050000667572021\n",
      "Iteration 3810 Training loss 0.0753210112452507 Validation loss 0.07091527432203293 Accuracy 0.8037500381469727\n",
      "Iteration 3820 Training loss 0.07017415016889572 Validation loss 0.07091449201107025 Accuracy 0.8035000562667847\n",
      "Iteration 3830 Training loss 0.07375708967447281 Validation loss 0.07112288475036621 Accuracy 0.8020000457763672\n",
      "Iteration 3840 Training loss 0.06951861083507538 Validation loss 0.07082030177116394 Accuracy 0.8047500252723694\n",
      "Iteration 3850 Training loss 0.07362071424722672 Validation loss 0.07081124186515808 Accuracy 0.8042500615119934\n",
      "Iteration 3860 Training loss 0.07044078409671783 Validation loss 0.07080009579658508 Accuracy 0.8036250472068787\n",
      "Iteration 3870 Training loss 0.0711682140827179 Validation loss 0.07076672464609146 Accuracy 0.8043750524520874\n",
      "Iteration 3880 Training loss 0.07046423852443695 Validation loss 0.07074493914842606 Accuracy 0.8043750524520874\n",
      "Iteration 3890 Training loss 0.07432675361633301 Validation loss 0.07072463631629944 Accuracy 0.8045000433921814\n",
      "Iteration 3900 Training loss 0.07795611023902893 Validation loss 0.07070963829755783 Accuracy 0.8050000667572021\n",
      "Iteration 3910 Training loss 0.06380810588598251 Validation loss 0.07080995291471481 Accuracy 0.8032500147819519\n",
      "Iteration 3920 Training loss 0.06873860210180283 Validation loss 0.07070338726043701 Accuracy 0.8035000562667847\n",
      "Iteration 3930 Training loss 0.07232432812452316 Validation loss 0.07068213075399399 Accuracy 0.8040000200271606\n",
      "Iteration 3940 Training loss 0.06635141372680664 Validation loss 0.07063914090394974 Accuracy 0.8050000667572021\n",
      "Iteration 3950 Training loss 0.07281038165092468 Validation loss 0.07071256637573242 Accuracy 0.8037500381469727\n",
      "Iteration 3960 Training loss 0.06840934604406357 Validation loss 0.07077590376138687 Accuracy 0.8040000200271606\n",
      "Iteration 3970 Training loss 0.07174690067768097 Validation loss 0.07063945382833481 Accuracy 0.8041250109672546\n",
      "Iteration 3980 Training loss 0.06857209652662277 Validation loss 0.07058035582304001 Accuracy 0.8048750162124634\n",
      "Iteration 3990 Training loss 0.07111220806837082 Validation loss 0.07054620236158371 Accuracy 0.8055000305175781\n",
      "Iteration 4000 Training loss 0.0728607252240181 Validation loss 0.07055456191301346 Accuracy 0.8048750162124634\n",
      "Iteration 4010 Training loss 0.07305591553449631 Validation loss 0.07050725817680359 Accuracy 0.8048750162124634\n",
      "Iteration 4020 Training loss 0.07473781704902649 Validation loss 0.07052989304065704 Accuracy 0.8045000433921814\n",
      "Iteration 4030 Training loss 0.07077868282794952 Validation loss 0.07055310159921646 Accuracy 0.8036250472068787\n",
      "Iteration 4040 Training loss 0.06346543878316879 Validation loss 0.0704461857676506 Accuracy 0.8050000667572021\n",
      "Iteration 4050 Training loss 0.07824940979480743 Validation loss 0.07042615115642548 Accuracy 0.8052500486373901\n",
      "Iteration 4060 Training loss 0.07174387574195862 Validation loss 0.07045600563287735 Accuracy 0.8048750162124634\n",
      "Iteration 4070 Training loss 0.06589169055223465 Validation loss 0.07041531056165695 Accuracy 0.8052500486373901\n",
      "Iteration 4080 Training loss 0.06842599809169769 Validation loss 0.07041043788194656 Accuracy 0.8050000667572021\n",
      "Iteration 4090 Training loss 0.07280638813972473 Validation loss 0.0704447403550148 Accuracy 0.8040000200271606\n",
      "Iteration 4100 Training loss 0.06912905722856522 Validation loss 0.07042339444160461 Accuracy 0.8037500381469727\n",
      "Iteration 4110 Training loss 0.0735563263297081 Validation loss 0.07039929181337357 Accuracy 0.8041250109672546\n",
      "Iteration 4120 Training loss 0.06082690879702568 Validation loss 0.07035132497549057 Accuracy 0.8045000433921814\n",
      "Iteration 4130 Training loss 0.07423931360244751 Validation loss 0.07034362852573395 Accuracy 0.8042500615119934\n",
      "Iteration 4140 Training loss 0.06079094856977463 Validation loss 0.07030320912599564 Accuracy 0.8050000667572021\n",
      "Iteration 4150 Training loss 0.06263775378465652 Validation loss 0.07023900747299194 Accuracy 0.8058750629425049\n",
      "Iteration 4160 Training loss 0.0676441639661789 Validation loss 0.07021704316139221 Accuracy 0.8058750629425049\n",
      "Iteration 4170 Training loss 0.0668819472193718 Validation loss 0.07021971046924591 Accuracy 0.8050000667572021\n",
      "Iteration 4180 Training loss 0.07239525765180588 Validation loss 0.07022395730018616 Accuracy 0.8050000667572021\n",
      "Iteration 4190 Training loss 0.07188630849123001 Validation loss 0.07016925513744354 Accuracy 0.8052500486373901\n",
      "Iteration 4200 Training loss 0.06977832317352295 Validation loss 0.07015345990657806 Accuracy 0.8055000305175781\n",
      "Iteration 4210 Training loss 0.06527801603078842 Validation loss 0.07018397748470306 Accuracy 0.8051250576972961\n",
      "Iteration 4220 Training loss 0.0666932687163353 Validation loss 0.07013580203056335 Accuracy 0.8051250576972961\n",
      "Iteration 4230 Training loss 0.0750841349363327 Validation loss 0.07009028643369675 Accuracy 0.8058750629425049\n",
      "Iteration 4240 Training loss 0.0703980103135109 Validation loss 0.07011764496564865 Accuracy 0.8062500357627869\n",
      "Iteration 4250 Training loss 0.0717308446764946 Validation loss 0.07007016241550446 Accuracy 0.8067500591278076\n",
      "Iteration 4260 Training loss 0.06941141188144684 Validation loss 0.07003741711378098 Accuracy 0.8057500123977661\n",
      "Iteration 4270 Training loss 0.06596838682889938 Validation loss 0.07001042366027832 Accuracy 0.8063750267028809\n",
      "Iteration 4280 Training loss 0.07276669144630432 Validation loss 0.07002885639667511 Accuracy 0.8065000176429749\n",
      "Iteration 4290 Training loss 0.06939446926116943 Validation loss 0.07006236165761948 Accuracy 0.8047500252723694\n",
      "Iteration 4300 Training loss 0.0704224556684494 Validation loss 0.07006068527698517 Accuracy 0.8060000538825989\n",
      "Iteration 4310 Training loss 0.06813626736402512 Validation loss 0.06995537132024765 Accuracy 0.8057500123977661\n",
      "Iteration 4320 Training loss 0.07472784072160721 Validation loss 0.06992397457361221 Accuracy 0.8066250085830688\n",
      "Iteration 4330 Training loss 0.07230310142040253 Validation loss 0.06995682418346405 Accuracy 0.8067500591278076\n",
      "Iteration 4340 Training loss 0.0761503204703331 Validation loss 0.06993617862462997 Accuracy 0.8056250214576721\n",
      "Iteration 4350 Training loss 0.06747172772884369 Validation loss 0.06987523287534714 Accuracy 0.8066250085830688\n",
      "Iteration 4360 Training loss 0.0638599544763565 Validation loss 0.06993753463029861 Accuracy 0.8055000305175781\n",
      "Iteration 4370 Training loss 0.0675746500492096 Validation loss 0.06984753161668777 Accuracy 0.8075000643730164\n",
      "Iteration 4380 Training loss 0.07025221735239029 Validation loss 0.06984969228506088 Accuracy 0.8063750267028809\n",
      "Iteration 4390 Training loss 0.06253715604543686 Validation loss 0.06982719898223877 Accuracy 0.8066250085830688\n",
      "Iteration 4400 Training loss 0.06914077699184418 Validation loss 0.06994496285915375 Accuracy 0.8068750500679016\n",
      "Iteration 4410 Training loss 0.07156899571418762 Validation loss 0.0697813406586647 Accuracy 0.8062500357627869\n",
      "Iteration 4420 Training loss 0.0704060047864914 Validation loss 0.06978371739387512 Accuracy 0.8063750267028809\n",
      "Iteration 4430 Training loss 0.0720221996307373 Validation loss 0.06973081827163696 Accuracy 0.8067500591278076\n",
      "Iteration 4440 Training loss 0.06248336285352707 Validation loss 0.06977079063653946 Accuracy 0.8070000410079956\n",
      "Iteration 4450 Training loss 0.07056073844432831 Validation loss 0.06972838193178177 Accuracy 0.8068750500679016\n",
      "Iteration 4460 Training loss 0.06569996476173401 Validation loss 0.06967476010322571 Accuracy 0.8075000643730164\n",
      "Iteration 4470 Training loss 0.07035113126039505 Validation loss 0.06969550251960754 Accuracy 0.8067500591278076\n",
      "Iteration 4480 Training loss 0.0620996467769146 Validation loss 0.06963786482810974 Accuracy 0.8068750500679016\n",
      "Iteration 4490 Training loss 0.06380527466535568 Validation loss 0.06963192671537399 Accuracy 0.8058750629425049\n",
      "Iteration 4500 Training loss 0.06722178310155869 Validation loss 0.06960739195346832 Accuracy 0.8068750500679016\n",
      "Iteration 4510 Training loss 0.06817004829645157 Validation loss 0.06959381699562073 Accuracy 0.8065000176429749\n",
      "Iteration 4520 Training loss 0.06363127380609512 Validation loss 0.06958264857530594 Accuracy 0.8068750500679016\n",
      "Iteration 4530 Training loss 0.06276129931211472 Validation loss 0.06965992599725723 Accuracy 0.8063750267028809\n",
      "Iteration 4540 Training loss 0.06318415701389313 Validation loss 0.06960809975862503 Accuracy 0.8065000176429749\n",
      "Iteration 4550 Training loss 0.07300708442926407 Validation loss 0.06952162832021713 Accuracy 0.8085000514984131\n",
      "Iteration 4560 Training loss 0.07337844371795654 Validation loss 0.06955328583717346 Accuracy 0.8070000410079956\n",
      "Iteration 4570 Training loss 0.06913930922746658 Validation loss 0.0695687010884285 Accuracy 0.8068750500679016\n",
      "Iteration 4580 Training loss 0.06757329404354095 Validation loss 0.0696350634098053 Accuracy 0.8062500357627869\n",
      "Iteration 4590 Training loss 0.07166654616594315 Validation loss 0.06956791877746582 Accuracy 0.8066250085830688\n",
      "Iteration 4600 Training loss 0.0696079358458519 Validation loss 0.0695377066731453 Accuracy 0.8070000410079956\n",
      "Iteration 4610 Training loss 0.06423448771238327 Validation loss 0.0694504976272583 Accuracy 0.8075000643730164\n",
      "Iteration 4620 Training loss 0.06534391641616821 Validation loss 0.06952480971813202 Accuracy 0.8071250319480896\n",
      "Iteration 4630 Training loss 0.06607230007648468 Validation loss 0.06939452141523361 Accuracy 0.8091250658035278\n",
      "Iteration 4640 Training loss 0.0713273286819458 Validation loss 0.06946435570716858 Accuracy 0.8072500228881836\n",
      "Iteration 4650 Training loss 0.06579814106225967 Validation loss 0.06935615092515945 Accuracy 0.8095000386238098\n",
      "Iteration 4660 Training loss 0.07248109579086304 Validation loss 0.0693328008055687 Accuracy 0.8088750243186951\n",
      "Iteration 4670 Training loss 0.06966856867074966 Validation loss 0.06938852369785309 Accuracy 0.8080000281333923\n",
      "Iteration 4680 Training loss 0.06465445458889008 Validation loss 0.0693042203783989 Accuracy 0.8080000281333923\n",
      "Iteration 4690 Training loss 0.07390394061803818 Validation loss 0.06929074227809906 Accuracy 0.8078750371932983\n",
      "Iteration 4700 Training loss 0.06467465311288834 Validation loss 0.06930246204137802 Accuracy 0.8086250424385071\n",
      "Iteration 4710 Training loss 0.06227733567357063 Validation loss 0.06926851719617844 Accuracy 0.8080000281333923\n",
      "Iteration 4720 Training loss 0.07141076773405075 Validation loss 0.06925106793642044 Accuracy 0.8080000281333923\n",
      "Iteration 4730 Training loss 0.06927764415740967 Validation loss 0.06920473277568817 Accuracy 0.8095000386238098\n",
      "Iteration 4740 Training loss 0.06735017895698547 Validation loss 0.06926828622817993 Accuracy 0.8082500100135803\n",
      "Iteration 4750 Training loss 0.07463537901639938 Validation loss 0.06925973296165466 Accuracy 0.8082500100135803\n",
      "Iteration 4760 Training loss 0.07601236552000046 Validation loss 0.06924581527709961 Accuracy 0.8081250190734863\n",
      "Iteration 4770 Training loss 0.07097075134515762 Validation loss 0.06912713497877121 Accuracy 0.8090000152587891\n",
      "Iteration 4780 Training loss 0.0671858936548233 Validation loss 0.06911734491586685 Accuracy 0.8095000386238098\n",
      "Iteration 4790 Training loss 0.06821039319038391 Validation loss 0.06911627948284149 Accuracy 0.8091250658035278\n",
      "Iteration 4800 Training loss 0.07075590640306473 Validation loss 0.06914499402046204 Accuracy 0.8086250424385071\n",
      "Iteration 4810 Training loss 0.07128152996301651 Validation loss 0.06906753033399582 Accuracy 0.8090000152587891\n",
      "Iteration 4820 Training loss 0.06410770118236542 Validation loss 0.06905001401901245 Accuracy 0.8092500567436218\n",
      "Iteration 4830 Training loss 0.07062831521034241 Validation loss 0.06905572861433029 Accuracy 0.8090000152587891\n",
      "Iteration 4840 Training loss 0.06536655128002167 Validation loss 0.06912684440612793 Accuracy 0.8088750243186951\n",
      "Iteration 4850 Training loss 0.062318313866853714 Validation loss 0.06904605776071548 Accuracy 0.8093750476837158\n",
      "Iteration 4860 Training loss 0.07205020636320114 Validation loss 0.06897902488708496 Accuracy 0.8097500205039978\n",
      "Iteration 4870 Training loss 0.06202457845211029 Validation loss 0.06895472854375839 Accuracy 0.8101250529289246\n",
      "Iteration 4880 Training loss 0.07891467213630676 Validation loss 0.06892988830804825 Accuracy 0.8093750476837158\n",
      "Iteration 4890 Training loss 0.0771636813879013 Validation loss 0.06908141821622849 Accuracy 0.8090000152587891\n",
      "Iteration 4900 Training loss 0.07399594038724899 Validation loss 0.06890730559825897 Accuracy 0.8090000152587891\n",
      "Iteration 4910 Training loss 0.06922633945941925 Validation loss 0.06889785826206207 Accuracy 0.8103750348091125\n",
      "Iteration 4920 Training loss 0.07402683794498444 Validation loss 0.06893371790647507 Accuracy 0.8095000386238098\n",
      "Iteration 4930 Training loss 0.07560610771179199 Validation loss 0.06886932998895645 Accuracy 0.8098750114440918\n",
      "Iteration 4940 Training loss 0.07733318209648132 Validation loss 0.06882762908935547 Accuracy 0.8095000386238098\n",
      "Iteration 4950 Training loss 0.07058291137218475 Validation loss 0.06881909817457199 Accuracy 0.8103750348091125\n",
      "Iteration 4960 Training loss 0.0634264424443245 Validation loss 0.06879822164773941 Accuracy 0.8097500205039978\n",
      "Iteration 4970 Training loss 0.07671688497066498 Validation loss 0.06877990067005157 Accuracy 0.8102500438690186\n",
      "Iteration 4980 Training loss 0.06384564936161041 Validation loss 0.06886526197195053 Accuracy 0.8103750348091125\n",
      "Iteration 4990 Training loss 0.06347727030515671 Validation loss 0.06886731088161469 Accuracy 0.8096250295639038\n",
      "Iteration 5000 Training loss 0.06813990324735641 Validation loss 0.068762868642807 Accuracy 0.8100000619888306\n",
      "Iteration 5010 Training loss 0.0676223635673523 Validation loss 0.06872769445180893 Accuracy 0.8103750348091125\n",
      "Iteration 5020 Training loss 0.06656154245138168 Validation loss 0.06872157752513885 Accuracy 0.8105000257492065\n",
      "Iteration 5030 Training loss 0.06161394342780113 Validation loss 0.06868526339530945 Accuracy 0.8107500672340393\n",
      "Iteration 5040 Training loss 0.06993299722671509 Validation loss 0.06868334114551544 Accuracy 0.8095000386238098\n",
      "Iteration 5050 Training loss 0.06212587654590607 Validation loss 0.06865768134593964 Accuracy 0.8088750243186951\n",
      "Iteration 5060 Training loss 0.07530692219734192 Validation loss 0.06863418966531754 Accuracy 0.8096250295639038\n",
      "Iteration 5070 Training loss 0.06481023877859116 Validation loss 0.06871747225522995 Accuracy 0.8100000619888306\n",
      "Iteration 5080 Training loss 0.07173888385295868 Validation loss 0.06863075494766235 Accuracy 0.8105000257492065\n",
      "Iteration 5090 Training loss 0.06902382522821426 Validation loss 0.06858514994382858 Accuracy 0.8107500672340393\n",
      "Iteration 5100 Training loss 0.07141461968421936 Validation loss 0.06858143955469131 Accuracy 0.811625063419342\n",
      "Iteration 5110 Training loss 0.06766408681869507 Validation loss 0.0685431957244873 Accuracy 0.8102500438690186\n",
      "Iteration 5120 Training loss 0.07488507777452469 Validation loss 0.06853432953357697 Accuracy 0.8110000491142273\n",
      "Iteration 5130 Training loss 0.0709400400519371 Validation loss 0.06903249025344849 Accuracy 0.8071250319480896\n",
      "Iteration 5140 Training loss 0.06664574146270752 Validation loss 0.06852646172046661 Accuracy 0.8112500309944153\n",
      "Iteration 5150 Training loss 0.06909778714179993 Validation loss 0.06849633902311325 Accuracy 0.811625063419342\n",
      "Iteration 5160 Training loss 0.06328463554382324 Validation loss 0.0684666857123375 Accuracy 0.8098750114440918\n",
      "Iteration 5170 Training loss 0.06741790473461151 Validation loss 0.06845347583293915 Accuracy 0.8097500205039978\n",
      "Iteration 5180 Training loss 0.06865940988063812 Validation loss 0.06846454739570618 Accuracy 0.81187504529953\n",
      "Iteration 5190 Training loss 0.060323309153318405 Validation loss 0.06844642758369446 Accuracy 0.812000036239624\n",
      "Iteration 5200 Training loss 0.07353369891643524 Validation loss 0.06846814602613449 Accuracy 0.8115000128746033\n",
      "Iteration 5210 Training loss 0.07299325615167618 Validation loss 0.06846136599779129 Accuracy 0.8111250400543213\n",
      "Iteration 5220 Training loss 0.06084202602505684 Validation loss 0.06852369755506516 Accuracy 0.8096250295639038\n",
      "Iteration 5230 Training loss 0.05677320808172226 Validation loss 0.06836771219968796 Accuracy 0.8111250400543213\n",
      "Iteration 5240 Training loss 0.06538720428943634 Validation loss 0.06845082342624664 Accuracy 0.8105000257492065\n",
      "Iteration 5250 Training loss 0.0715891569852829 Validation loss 0.06835642457008362 Accuracy 0.8125000596046448\n",
      "Iteration 5260 Training loss 0.06391393393278122 Validation loss 0.06843792647123337 Accuracy 0.8101250529289246\n",
      "Iteration 5270 Training loss 0.06934122741222382 Validation loss 0.06835326552391052 Accuracy 0.8113750219345093\n",
      "Iteration 5280 Training loss 0.06957779824733734 Validation loss 0.0682913213968277 Accuracy 0.81187504529953\n",
      "Iteration 5290 Training loss 0.06957679986953735 Validation loss 0.06829030066728592 Accuracy 0.811750054359436\n",
      "Iteration 5300 Training loss 0.08151497691869736 Validation loss 0.06828390806913376 Accuracy 0.812250018119812\n",
      "Iteration 5310 Training loss 0.0741339847445488 Validation loss 0.06822627782821655 Accuracy 0.8103750348091125\n",
      "Iteration 5320 Training loss 0.07742121815681458 Validation loss 0.06820158660411835 Accuracy 0.8107500672340393\n",
      "Iteration 5330 Training loss 0.06332281231880188 Validation loss 0.06828337907791138 Accuracy 0.8108750581741333\n",
      "Iteration 5340 Training loss 0.0747729167342186 Validation loss 0.06817898899316788 Accuracy 0.8112500309944153\n",
      "Iteration 5350 Training loss 0.06549639254808426 Validation loss 0.0683104395866394 Accuracy 0.8101250529289246\n",
      "Iteration 5360 Training loss 0.07491707801818848 Validation loss 0.06815484911203384 Accuracy 0.8108750581741333\n",
      "Iteration 5370 Training loss 0.06354418396949768 Validation loss 0.06815026700496674 Accuracy 0.812125027179718\n",
      "Iteration 5380 Training loss 0.06997144222259521 Validation loss 0.06821473687887192 Accuracy 0.8107500672340393\n",
      "Iteration 5390 Training loss 0.0738506093621254 Validation loss 0.06809442490339279 Accuracy 0.8107500672340393\n",
      "Iteration 5400 Training loss 0.06715433299541473 Validation loss 0.06809848546981812 Accuracy 0.812125027179718\n",
      "Iteration 5410 Training loss 0.06910700350999832 Validation loss 0.0682380422949791 Accuracy 0.8102500438690186\n",
      "Iteration 5420 Training loss 0.0657014325261116 Validation loss 0.06803032755851746 Accuracy 0.8111250400543213\n",
      "Iteration 5430 Training loss 0.07112831622362137 Validation loss 0.06827376782894135 Accuracy 0.8101250529289246\n",
      "Iteration 5440 Training loss 0.06428921222686768 Validation loss 0.06803019344806671 Accuracy 0.812250018119812\n",
      "Iteration 5450 Training loss 0.0745135024189949 Validation loss 0.06805814057588577 Accuracy 0.812125027179718\n",
      "Iteration 5460 Training loss 0.06127610057592392 Validation loss 0.06812437623739243 Accuracy 0.8110000491142273\n",
      "Iteration 5470 Training loss 0.0655306726694107 Validation loss 0.06802229583263397 Accuracy 0.812125027179718\n",
      "Iteration 5480 Training loss 0.07158353924751282 Validation loss 0.06793513894081116 Accuracy 0.8110000491142273\n",
      "Iteration 5490 Training loss 0.06500132381916046 Validation loss 0.06804244965314865 Accuracy 0.8113750219345093\n",
      "Iteration 5500 Training loss 0.07164785265922546 Validation loss 0.06801297515630722 Accuracy 0.812125027179718\n",
      "Iteration 5510 Training loss 0.07379264384508133 Validation loss 0.06821222603321075 Accuracy 0.8106250166893005\n",
      "Iteration 5520 Training loss 0.062062423676252365 Validation loss 0.06792964786291122 Accuracy 0.812250018119812\n",
      "Iteration 5530 Training loss 0.058985065668821335 Validation loss 0.0678604319691658 Accuracy 0.8113750219345093\n",
      "Iteration 5540 Training loss 0.06891385465860367 Validation loss 0.06782405823469162 Accuracy 0.812000036239624\n",
      "Iteration 5550 Training loss 0.07414619624614716 Validation loss 0.0678061693906784 Accuracy 0.812000036239624\n",
      "Iteration 5560 Training loss 0.06799854338169098 Validation loss 0.0678112804889679 Accuracy 0.8115000128746033\n",
      "Iteration 5570 Training loss 0.07395044714212418 Validation loss 0.06778611242771149 Accuracy 0.811750054359436\n",
      "Iteration 5580 Training loss 0.05888330191373825 Validation loss 0.06778064370155334 Accuracy 0.811625063419342\n",
      "Iteration 5590 Training loss 0.06269029527902603 Validation loss 0.06776396930217743 Accuracy 0.8115000128746033\n",
      "Iteration 5600 Training loss 0.067670077085495 Validation loss 0.06782568246126175 Accuracy 0.8125000596046448\n",
      "Iteration 5610 Training loss 0.07756190747022629 Validation loss 0.06771358102560043 Accuracy 0.812250018119812\n",
      "Iteration 5620 Training loss 0.0718170553445816 Validation loss 0.06768383830785751 Accuracy 0.812000036239624\n",
      "Iteration 5630 Training loss 0.060091469436883926 Validation loss 0.06769368052482605 Accuracy 0.812375009059906\n",
      "Iteration 5640 Training loss 0.06318109482526779 Validation loss 0.06770037859678268 Accuracy 0.81187504529953\n",
      "Iteration 5650 Training loss 0.06420603394508362 Validation loss 0.06787123531103134 Accuracy 0.8112500309944153\n",
      "Iteration 5660 Training loss 0.07081302255392075 Validation loss 0.06762554496526718 Accuracy 0.812125027179718\n",
      "Iteration 5670 Training loss 0.07489240169525146 Validation loss 0.06760311871767044 Accuracy 0.8127500414848328\n",
      "Iteration 5680 Training loss 0.06801357120275497 Validation loss 0.06773500144481659 Accuracy 0.812250018119812\n",
      "Iteration 5690 Training loss 0.06921539455652237 Validation loss 0.06759871542453766 Accuracy 0.81187504529953\n",
      "Iteration 5700 Training loss 0.07083968073129654 Validation loss 0.06755435466766357 Accuracy 0.8130000233650208\n",
      "Iteration 5710 Training loss 0.061791323125362396 Validation loss 0.06763888895511627 Accuracy 0.8125000596046448\n",
      "Iteration 5720 Training loss 0.07171528041362762 Validation loss 0.06753165274858475 Accuracy 0.812000036239624\n",
      "Iteration 5730 Training loss 0.06295082718133926 Validation loss 0.06753633916378021 Accuracy 0.8125000596046448\n",
      "Iteration 5740 Training loss 0.06696009635925293 Validation loss 0.06747746467590332 Accuracy 0.8126250505447388\n",
      "Iteration 5750 Training loss 0.06800226867198944 Validation loss 0.06748190522193909 Accuracy 0.8128750324249268\n",
      "Iteration 5760 Training loss 0.07028143852949142 Validation loss 0.06749800592660904 Accuracy 0.8125000596046448\n",
      "Iteration 5770 Training loss 0.07751080393791199 Validation loss 0.06741175800561905 Accuracy 0.8130000233650208\n",
      "Iteration 5780 Training loss 0.060945603996515274 Validation loss 0.06749621778726578 Accuracy 0.8133750557899475\n",
      "Iteration 5790 Training loss 0.06140337511897087 Validation loss 0.06744915246963501 Accuracy 0.8132500648498535\n",
      "Iteration 5800 Training loss 0.053180232644081116 Validation loss 0.06746207922697067 Accuracy 0.8135000467300415\n",
      "Iteration 5810 Training loss 0.0611632876098156 Validation loss 0.06745556741952896 Accuracy 0.8133750557899475\n",
      "Iteration 5820 Training loss 0.06909959018230438 Validation loss 0.06737016141414642 Accuracy 0.8133750557899475\n",
      "Iteration 5830 Training loss 0.06353221833705902 Validation loss 0.06731695681810379 Accuracy 0.8130000233650208\n",
      "Iteration 5840 Training loss 0.06993842869997025 Validation loss 0.06751477718353271 Accuracy 0.81187504529953\n",
      "Iteration 5850 Training loss 0.06844541430473328 Validation loss 0.06732930988073349 Accuracy 0.8137500286102295\n",
      "Iteration 5860 Training loss 0.0693671852350235 Validation loss 0.06732885539531708 Accuracy 0.8136250376701355\n",
      "Iteration 5870 Training loss 0.07209201902151108 Validation loss 0.06732385605573654 Accuracy 0.8137500286102295\n",
      "Iteration 5880 Training loss 0.06787533313035965 Validation loss 0.06729882210493088 Accuracy 0.8142500519752502\n",
      "Iteration 5890 Training loss 0.06990060210227966 Validation loss 0.06723140925168991 Accuracy 0.8132500648498535\n",
      "Iteration 5900 Training loss 0.06515707820653915 Validation loss 0.06728368252515793 Accuracy 0.8136250376701355\n",
      "Iteration 5910 Training loss 0.0707579180598259 Validation loss 0.0671984925866127 Accuracy 0.8131250143051147\n",
      "Iteration 5920 Training loss 0.07884246110916138 Validation loss 0.06720919907093048 Accuracy 0.8133750557899475\n",
      "Iteration 5930 Training loss 0.06905848532915115 Validation loss 0.0671568289399147 Accuracy 0.8135000467300415\n",
      "Iteration 5940 Training loss 0.059458520263433456 Validation loss 0.06728573888540268 Accuracy 0.8138750195503235\n",
      "Iteration 5950 Training loss 0.06127655506134033 Validation loss 0.06714073568582535 Accuracy 0.8141250610351562\n",
      "Iteration 5960 Training loss 0.06962031871080399 Validation loss 0.06751534342765808 Accuracy 0.8131250143051147\n",
      "Iteration 5970 Training loss 0.062109917402267456 Validation loss 0.06714053452014923 Accuracy 0.8136250376701355\n",
      "Iteration 5980 Training loss 0.06201697140932083 Validation loss 0.06706871837377548 Accuracy 0.8140000104904175\n",
      "Iteration 5990 Training loss 0.07180998474359512 Validation loss 0.06710544973611832 Accuracy 0.8136250376701355\n",
      "Iteration 6000 Training loss 0.0696384459733963 Validation loss 0.0670362263917923 Accuracy 0.8140000104904175\n",
      "Iteration 6010 Training loss 0.06365544348955154 Validation loss 0.06714435666799545 Accuracy 0.8133750557899475\n",
      "Iteration 6020 Training loss 0.06122339144349098 Validation loss 0.06699086725711823 Accuracy 0.8141250610351562\n",
      "Iteration 6030 Training loss 0.07146988809108734 Validation loss 0.06697886437177658 Accuracy 0.8143750429153442\n",
      "Iteration 6040 Training loss 0.06444161385297775 Validation loss 0.06696029007434845 Accuracy 0.8140000104904175\n",
      "Iteration 6050 Training loss 0.06955195963382721 Validation loss 0.06746967136859894 Accuracy 0.8125000596046448\n",
      "Iteration 6060 Training loss 0.06885996460914612 Validation loss 0.0669315904378891 Accuracy 0.8143750429153442\n",
      "Iteration 6070 Training loss 0.06951221823692322 Validation loss 0.06698374450206757 Accuracy 0.8138750195503235\n",
      "Iteration 6080 Training loss 0.06464351713657379 Validation loss 0.06714721769094467 Accuracy 0.815250039100647\n",
      "Iteration 6090 Training loss 0.06484196335077286 Validation loss 0.06690876185894012 Accuracy 0.8138750195503235\n",
      "Iteration 6100 Training loss 0.06134750694036484 Validation loss 0.06688965857028961 Accuracy 0.8141250610351562\n",
      "Iteration 6110 Training loss 0.06604740768671036 Validation loss 0.06688820570707321 Accuracy 0.8147500157356262\n",
      "Iteration 6120 Training loss 0.07030268758535385 Validation loss 0.06684661656618118 Accuracy 0.8141250610351562\n",
      "Iteration 6130 Training loss 0.0694299191236496 Validation loss 0.06681974232196808 Accuracy 0.8147500157356262\n",
      "Iteration 6140 Training loss 0.05934039130806923 Validation loss 0.06684854626655579 Accuracy 0.8143750429153442\n",
      "Iteration 6150 Training loss 0.06355643272399902 Validation loss 0.06680586189031601 Accuracy 0.8138750195503235\n",
      "Iteration 6160 Training loss 0.062158968299627304 Validation loss 0.06701521575450897 Accuracy 0.814875066280365\n",
      "Iteration 6170 Training loss 0.06529438495635986 Validation loss 0.06683012843132019 Accuracy 0.814875066280365\n",
      "Iteration 6180 Training loss 0.06011557579040527 Validation loss 0.06683474034070969 Accuracy 0.8145000338554382\n",
      "Iteration 6190 Training loss 0.06511890143156052 Validation loss 0.06688551604747772 Accuracy 0.815125048160553\n",
      "Iteration 6200 Training loss 0.05802155286073685 Validation loss 0.06676550209522247 Accuracy 0.8145000338554382\n",
      "Iteration 6210 Training loss 0.05804198607802391 Validation loss 0.06668265163898468 Accuracy 0.8147500157356262\n",
      "Iteration 6220 Training loss 0.06416597217321396 Validation loss 0.06717368960380554 Accuracy 0.8136250376701355\n",
      "Iteration 6230 Training loss 0.06478268653154373 Validation loss 0.06695225834846497 Accuracy 0.8145000338554382\n",
      "Iteration 6240 Training loss 0.06392757594585419 Validation loss 0.06664230674505234 Accuracy 0.8147500157356262\n",
      "Iteration 6250 Training loss 0.06463950872421265 Validation loss 0.06665302067995071 Accuracy 0.815125048160553\n",
      "Iteration 6260 Training loss 0.07338415086269379 Validation loss 0.06668159365653992 Accuracy 0.815250039100647\n",
      "Iteration 6270 Training loss 0.07666267454624176 Validation loss 0.0665920227766037 Accuracy 0.815125048160553\n",
      "Iteration 6280 Training loss 0.057215455919504166 Validation loss 0.06662444025278091 Accuracy 0.815250039100647\n",
      "Iteration 6290 Training loss 0.06484883278608322 Validation loss 0.06656254082918167 Accuracy 0.815250039100647\n",
      "Iteration 6300 Training loss 0.06753669679164886 Validation loss 0.06654231250286102 Accuracy 0.815250039100647\n",
      "Iteration 6310 Training loss 0.06204644963145256 Validation loss 0.06655927747488022 Accuracy 0.815625011920929\n",
      "Iteration 6320 Training loss 0.06569284200668335 Validation loss 0.06678388267755508 Accuracy 0.8146250247955322\n",
      "Iteration 6330 Training loss 0.06575359404087067 Validation loss 0.06649381667375565 Accuracy 0.815500020980835\n",
      "Iteration 6340 Training loss 0.06954080611467361 Validation loss 0.06648688018321991 Accuracy 0.815375030040741\n",
      "Iteration 6350 Training loss 0.06352047622203827 Validation loss 0.066510871052742 Accuracy 0.8161250352859497\n",
      "Iteration 6360 Training loss 0.06885291635990143 Validation loss 0.06647554039955139 Accuracy 0.815500020980835\n",
      "Iteration 6370 Training loss 0.06693705171346664 Validation loss 0.06648856401443481 Accuracy 0.815125048160553\n",
      "Iteration 6380 Training loss 0.0676521360874176 Validation loss 0.06650693714618683 Accuracy 0.8161250352859497\n",
      "Iteration 6390 Training loss 0.06498828530311584 Validation loss 0.06646312028169632 Accuracy 0.815625011920929\n",
      "Iteration 6400 Training loss 0.07558368146419525 Validation loss 0.06637006253004074 Accuracy 0.8157500624656677\n",
      "Iteration 6410 Training loss 0.06242396682500839 Validation loss 0.06636635214090347 Accuracy 0.815375030040741\n",
      "Iteration 6420 Training loss 0.06085114926099777 Validation loss 0.06640893220901489 Accuracy 0.8158750534057617\n",
      "Iteration 6430 Training loss 0.05742424353957176 Validation loss 0.06634160131216049 Accuracy 0.815500020980835\n",
      "Iteration 6440 Training loss 0.07654811441898346 Validation loss 0.06637532263994217 Accuracy 0.8160000443458557\n",
      "Iteration 6450 Training loss 0.07264188677072525 Validation loss 0.06646301597356796 Accuracy 0.8166250586509705\n",
      "Iteration 6460 Training loss 0.06627220660448074 Validation loss 0.06630758941173553 Accuracy 0.8160000443458557\n",
      "Iteration 6470 Training loss 0.07352431863546371 Validation loss 0.06627347320318222 Accuracy 0.8160000443458557\n",
      "Iteration 6480 Training loss 0.06849682331085205 Validation loss 0.0663248598575592 Accuracy 0.8168750405311584\n",
      "Iteration 6490 Training loss 0.06786877661943436 Validation loss 0.06623814254999161 Accuracy 0.815375030040741\n",
      "Iteration 6500 Training loss 0.07306573539972305 Validation loss 0.06623640656471252 Accuracy 0.8168750405311584\n",
      "Iteration 6510 Training loss 0.0679226666688919 Validation loss 0.06618831306695938 Accuracy 0.8160000443458557\n",
      "Iteration 6520 Training loss 0.07867877930402756 Validation loss 0.06619240343570709 Accuracy 0.8170000314712524\n",
      "Iteration 6530 Training loss 0.06963752955198288 Validation loss 0.0661531388759613 Accuracy 0.8162500262260437\n",
      "Iteration 6540 Training loss 0.06919605284929276 Validation loss 0.06617972254753113 Accuracy 0.8163750171661377\n",
      "Iteration 6550 Training loss 0.06868761032819748 Validation loss 0.06612777709960938 Accuracy 0.8160000443458557\n",
      "Iteration 6560 Training loss 0.06656426191329956 Validation loss 0.06610679626464844 Accuracy 0.8166250586509705\n",
      "Iteration 6570 Training loss 0.06588927656412125 Validation loss 0.06611263006925583 Accuracy 0.8175000548362732\n",
      "Iteration 6580 Training loss 0.06273451447486877 Validation loss 0.06612387299537659 Accuracy 0.8167500495910645\n",
      "Iteration 6590 Training loss 0.07128636538982391 Validation loss 0.06607582420110703 Accuracy 0.8172500133514404\n",
      "Iteration 6600 Training loss 0.06122797355055809 Validation loss 0.06617046147584915 Accuracy 0.8175000548362732\n",
      "Iteration 6610 Training loss 0.06364850699901581 Validation loss 0.06618800759315491 Accuracy 0.8172500133514404\n",
      "Iteration 6620 Training loss 0.061800550669431686 Validation loss 0.06606514751911163 Accuracy 0.8170000314712524\n",
      "Iteration 6630 Training loss 0.05932433903217316 Validation loss 0.06601110100746155 Accuracy 0.8166250586509705\n",
      "Iteration 6640 Training loss 0.0663241297006607 Validation loss 0.06603292375802994 Accuracy 0.8176250457763672\n",
      "Iteration 6650 Training loss 0.06500868499279022 Validation loss 0.06598573923110962 Accuracy 0.8165000677108765\n",
      "Iteration 6660 Training loss 0.0659283846616745 Validation loss 0.06595534086227417 Accuracy 0.8171250224113464\n",
      "Iteration 6670 Training loss 0.06111398711800575 Validation loss 0.06593203544616699 Accuracy 0.8170000314712524\n",
      "Iteration 6680 Training loss 0.06576399505138397 Validation loss 0.06593814492225647 Accuracy 0.8166250586509705\n",
      "Iteration 6690 Training loss 0.06674408912658691 Validation loss 0.06589721143245697 Accuracy 0.8173750638961792\n",
      "Iteration 6700 Training loss 0.05981559306383133 Validation loss 0.06587304919958115 Accuracy 0.8171250224113464\n",
      "Iteration 6710 Training loss 0.06411616504192352 Validation loss 0.06594458222389221 Accuracy 0.8185000419616699\n",
      "Iteration 6720 Training loss 0.07587116956710815 Validation loss 0.06595205515623093 Accuracy 0.8183750510215759\n",
      "Iteration 6730 Training loss 0.0621701218187809 Validation loss 0.06584685295820236 Accuracy 0.8180000185966492\n",
      "Iteration 6740 Training loss 0.06147259473800659 Validation loss 0.06593780964612961 Accuracy 0.8181250095367432\n",
      "Iteration 6750 Training loss 0.05532769858837128 Validation loss 0.06580080837011337 Accuracy 0.8180000185966492\n",
      "Iteration 6760 Training loss 0.061986491084098816 Validation loss 0.06584802269935608 Accuracy 0.8185000419616699\n",
      "Iteration 6770 Training loss 0.06526541709899902 Validation loss 0.06578461825847626 Accuracy 0.8181250095367432\n",
      "Iteration 6780 Training loss 0.06275837868452072 Validation loss 0.06592737883329391 Accuracy 0.8186250329017639\n",
      "Iteration 6790 Training loss 0.0628361701965332 Validation loss 0.06585020571947098 Accuracy 0.8178750276565552\n",
      "Iteration 6800 Training loss 0.05680691450834274 Validation loss 0.06579364091157913 Accuracy 0.8185000419616699\n",
      "Iteration 6810 Training loss 0.06343197077512741 Validation loss 0.06598860770463943 Accuracy 0.8191250562667847\n",
      "Iteration 6820 Training loss 0.07937970757484436 Validation loss 0.06572052091360092 Accuracy 0.8183750510215759\n",
      "Iteration 6830 Training loss 0.05972816422581673 Validation loss 0.06571338325738907 Accuracy 0.8183750510215759\n",
      "Iteration 6840 Training loss 0.06153246387839317 Validation loss 0.06567111611366272 Accuracy 0.8176250457763672\n",
      "Iteration 6850 Training loss 0.07239577174186707 Validation loss 0.06567619740962982 Accuracy 0.8180000185966492\n",
      "Iteration 6860 Training loss 0.06378145515918732 Validation loss 0.06563730537891388 Accuracy 0.8175000548362732\n",
      "Iteration 6870 Training loss 0.0645333006978035 Validation loss 0.06560754776000977 Accuracy 0.8178750276565552\n",
      "Iteration 6880 Training loss 0.06605759263038635 Validation loss 0.06559233367443085 Accuracy 0.8178750276565552\n",
      "Iteration 6890 Training loss 0.06375133991241455 Validation loss 0.0655781552195549 Accuracy 0.8183750510215759\n",
      "Iteration 6900 Training loss 0.056093666702508926 Validation loss 0.06555455178022385 Accuracy 0.8177500367164612\n",
      "Iteration 6910 Training loss 0.06245870515704155 Validation loss 0.06554847955703735 Accuracy 0.8177500367164612\n",
      "Iteration 6920 Training loss 0.06143127381801605 Validation loss 0.06561457365751266 Accuracy 0.8188750147819519\n",
      "Iteration 6930 Training loss 0.06488392502069473 Validation loss 0.06553332507610321 Accuracy 0.8182500600814819\n",
      "Iteration 6940 Training loss 0.07253771275281906 Validation loss 0.06552177667617798 Accuracy 0.8183750510215759\n",
      "Iteration 6950 Training loss 0.058245182037353516 Validation loss 0.06549205631017685 Accuracy 0.8190000653266907\n",
      "Iteration 6960 Training loss 0.07019993662834167 Validation loss 0.06547053158283234 Accuracy 0.8185000419616699\n",
      "Iteration 6970 Training loss 0.06535323709249496 Validation loss 0.06553738564252853 Accuracy 0.8182500600814819\n",
      "Iteration 6980 Training loss 0.06164005026221275 Validation loss 0.06542995572090149 Accuracy 0.8188750147819519\n",
      "Iteration 6990 Training loss 0.0704704150557518 Validation loss 0.06541652232408524 Accuracy 0.8190000653266907\n",
      "Iteration 7000 Training loss 0.059174373745918274 Validation loss 0.06538428366184235 Accuracy 0.8188750147819519\n",
      "Iteration 7010 Training loss 0.060783594846725464 Validation loss 0.06550928205251694 Accuracy 0.8201250433921814\n",
      "Iteration 7020 Training loss 0.06283921748399734 Validation loss 0.06540346145629883 Accuracy 0.8196250200271606\n",
      "Iteration 7030 Training loss 0.06568188965320587 Validation loss 0.06535187363624573 Accuracy 0.8197500109672546\n",
      "Iteration 7040 Training loss 0.07636140286922455 Validation loss 0.06532035768032074 Accuracy 0.8196250200271606\n",
      "Iteration 7050 Training loss 0.06487905234098434 Validation loss 0.06531520932912827 Accuracy 0.8196250200271606\n",
      "Iteration 7060 Training loss 0.05892190337181091 Validation loss 0.06536171585321426 Accuracy 0.8198750615119934\n",
      "Iteration 7070 Training loss 0.06221131980419159 Validation loss 0.06535772234201431 Accuracy 0.8181250095367432\n",
      "Iteration 7080 Training loss 0.06644958257675171 Validation loss 0.06542645394802094 Accuracy 0.8197500109672546\n",
      "Iteration 7090 Training loss 0.0686681941151619 Validation loss 0.06526752561330795 Accuracy 0.8198750615119934\n",
      "Iteration 7100 Training loss 0.06249213218688965 Validation loss 0.06530024856328964 Accuracy 0.8188750147819519\n",
      "Iteration 7110 Training loss 0.06513381004333496 Validation loss 0.06521221995353699 Accuracy 0.8191250562667847\n",
      "Iteration 7120 Training loss 0.06606797873973846 Validation loss 0.06522060185670853 Accuracy 0.8197500109672546\n",
      "Iteration 7130 Training loss 0.05679646506905556 Validation loss 0.06518199294805527 Accuracy 0.8196250200271606\n",
      "Iteration 7140 Training loss 0.068801648914814 Validation loss 0.06527859717607498 Accuracy 0.8197500109672546\n",
      "Iteration 7150 Training loss 0.06098879128694534 Validation loss 0.06515397876501083 Accuracy 0.8193750381469727\n",
      "Iteration 7160 Training loss 0.065358467400074 Validation loss 0.0651698186993599 Accuracy 0.8202500343322754\n",
      "Iteration 7170 Training loss 0.0592239685356617 Validation loss 0.06512140482664108 Accuracy 0.8200000524520874\n",
      "Iteration 7180 Training loss 0.06453320384025574 Validation loss 0.06510630995035172 Accuracy 0.8200000524520874\n",
      "Iteration 7190 Training loss 0.06213752552866936 Validation loss 0.06509480625391006 Accuracy 0.8203750252723694\n",
      "Iteration 7200 Training loss 0.06841178983449936 Validation loss 0.06506866216659546 Accuracy 0.8200000524520874\n",
      "Iteration 7210 Training loss 0.05824866145849228 Validation loss 0.0650637075304985 Accuracy 0.8207500576972961\n",
      "Iteration 7220 Training loss 0.06995488703250885 Validation loss 0.06508273631334305 Accuracy 0.8210000395774841\n",
      "Iteration 7230 Training loss 0.06352919340133667 Validation loss 0.06506050378084183 Accuracy 0.8212500214576721\n",
      "Iteration 7240 Training loss 0.06504819542169571 Validation loss 0.06501471251249313 Accuracy 0.8203750252723694\n",
      "Iteration 7250 Training loss 0.06101822853088379 Validation loss 0.06499026715755463 Accuracy 0.8202500343322754\n",
      "Iteration 7260 Training loss 0.0573812760412693 Validation loss 0.06501699984073639 Accuracy 0.8211250305175781\n",
      "Iteration 7270 Training loss 0.07067101448774338 Validation loss 0.06495317816734314 Accuracy 0.8205000162124634\n",
      "Iteration 7280 Training loss 0.06803537905216217 Validation loss 0.06495924293994904 Accuracy 0.8205000162124634\n",
      "Iteration 7290 Training loss 0.0682692676782608 Validation loss 0.06492208689451218 Accuracy 0.8212500214576721\n",
      "Iteration 7300 Training loss 0.06732482463121414 Validation loss 0.06489667296409607 Accuracy 0.8205000162124634\n",
      "Iteration 7310 Training loss 0.06475352495908737 Validation loss 0.06488486379384995 Accuracy 0.8215000629425049\n",
      "Iteration 7320 Training loss 0.06423351168632507 Validation loss 0.06487151235342026 Accuracy 0.8205000162124634\n",
      "Iteration 7330 Training loss 0.05664129927754402 Validation loss 0.06485437601804733 Accuracy 0.8208750486373901\n",
      "Iteration 7340 Training loss 0.0659378245472908 Validation loss 0.064947709441185 Accuracy 0.8211250305175781\n",
      "Iteration 7350 Training loss 0.05988754704594612 Validation loss 0.06495656073093414 Accuracy 0.8208750486373901\n",
      "Iteration 7360 Training loss 0.07373979687690735 Validation loss 0.06491722911596298 Accuracy 0.8208750486373901\n",
      "Iteration 7370 Training loss 0.06114506721496582 Validation loss 0.06486544758081436 Accuracy 0.8216250538825989\n",
      "Iteration 7380 Training loss 0.05167557671666145 Validation loss 0.06482452154159546 Accuracy 0.8213750123977661\n",
      "Iteration 7390 Training loss 0.06665558367967606 Validation loss 0.06476671248674393 Accuracy 0.8211250305175781\n",
      "Iteration 7400 Training loss 0.06468239426612854 Validation loss 0.06487250328063965 Accuracy 0.8213750123977661\n",
      "Iteration 7410 Training loss 0.06984629482030869 Validation loss 0.0647544264793396 Accuracy 0.8213750123977661\n",
      "Iteration 7420 Training loss 0.06132214888930321 Validation loss 0.06471320241689682 Accuracy 0.8211250305175781\n",
      "Iteration 7430 Training loss 0.06295667588710785 Validation loss 0.0649951696395874 Accuracy 0.8220000267028809\n",
      "Iteration 7440 Training loss 0.05657610669732094 Validation loss 0.06468679010868073 Accuracy 0.8213750123977661\n",
      "Iteration 7450 Training loss 0.05879834666848183 Validation loss 0.06475677341222763 Accuracy 0.8210000395774841\n",
      "Iteration 7460 Training loss 0.06640230864286423 Validation loss 0.06472204625606537 Accuracy 0.8225000500679016\n",
      "Iteration 7470 Training loss 0.06576027721166611 Validation loss 0.06463424116373062 Accuracy 0.8215000629425049\n",
      "Iteration 7480 Training loss 0.0679720938205719 Validation loss 0.0646134614944458 Accuracy 0.8213750123977661\n",
      "Iteration 7490 Training loss 0.06747990101575851 Validation loss 0.06460929661989212 Accuracy 0.8222500681877136\n",
      "Iteration 7500 Training loss 0.06675033271312714 Validation loss 0.06459496915340424 Accuracy 0.8221250176429749\n",
      "Iteration 7510 Training loss 0.07028558850288391 Validation loss 0.06464210897684097 Accuracy 0.8222500681877136\n",
      "Iteration 7520 Training loss 0.06208143010735512 Validation loss 0.06454013288021088 Accuracy 0.8222500681877136\n",
      "Iteration 7530 Training loss 0.06676601618528366 Validation loss 0.06464597582817078 Accuracy 0.8227500319480896\n",
      "Iteration 7540 Training loss 0.06740011274814606 Validation loss 0.06460293382406235 Accuracy 0.8225000500679016\n",
      "Iteration 7550 Training loss 0.06251496076583862 Validation loss 0.06450843811035156 Accuracy 0.8231250643730164\n",
      "Iteration 7560 Training loss 0.05575350672006607 Validation loss 0.06448228657245636 Accuracy 0.8213750123977661\n",
      "Iteration 7570 Training loss 0.06300414353609085 Validation loss 0.06446444988250732 Accuracy 0.8212500214576721\n",
      "Iteration 7580 Training loss 0.057811055332422256 Validation loss 0.06445436179637909 Accuracy 0.8236250281333923\n",
      "Iteration 7590 Training loss 0.058705948293209076 Validation loss 0.06445199251174927 Accuracy 0.8233750462532043\n",
      "Iteration 7600 Training loss 0.06362738460302353 Validation loss 0.06463358551263809 Accuracy 0.8228750228881836\n",
      "Iteration 7610 Training loss 0.07020117342472076 Validation loss 0.0645533949136734 Accuracy 0.8226250410079956\n",
      "Iteration 7620 Training loss 0.06250862777233124 Validation loss 0.06442099064588547 Accuracy 0.8235000371932983\n",
      "Iteration 7630 Training loss 0.06060314178466797 Validation loss 0.06439575552940369 Accuracy 0.8217500448226929\n",
      "Iteration 7640 Training loss 0.058280814439058304 Validation loss 0.06442642211914062 Accuracy 0.8223750591278076\n",
      "Iteration 7650 Training loss 0.057554781436920166 Validation loss 0.06434386223554611 Accuracy 0.8227500319480896\n",
      "Iteration 7660 Training loss 0.06690476089715958 Validation loss 0.06451370567083359 Accuracy 0.8236250281333923\n",
      "Iteration 7670 Training loss 0.06534121185541153 Validation loss 0.06430882960557938 Accuracy 0.8231250643730164\n",
      "Iteration 7680 Training loss 0.06893236935138702 Validation loss 0.06434354186058044 Accuracy 0.8230000138282776\n",
      "Iteration 7690 Training loss 0.05858687311410904 Validation loss 0.06429849565029144 Accuracy 0.8230000138282776\n",
      "Iteration 7700 Training loss 0.06713304668664932 Validation loss 0.06426837295293808 Accuracy 0.8226250410079956\n",
      "Iteration 7710 Training loss 0.07071702927350998 Validation loss 0.06449083983898163 Accuracy 0.8245000243186951\n",
      "Iteration 7720 Training loss 0.06458428502082825 Validation loss 0.06422484666109085 Accuracy 0.8227500319480896\n",
      "Iteration 7730 Training loss 0.06383545696735382 Validation loss 0.06422499567270279 Accuracy 0.8235000371932983\n",
      "Iteration 7740 Training loss 0.0666830837726593 Validation loss 0.06419548392295837 Accuracy 0.8233750462532043\n",
      "Iteration 7750 Training loss 0.06117631122469902 Validation loss 0.06433077901601791 Accuracy 0.8235000371932983\n",
      "Iteration 7760 Training loss 0.05795083940029144 Validation loss 0.0641668364405632 Accuracy 0.8236250281333923\n",
      "Iteration 7770 Training loss 0.06332764029502869 Validation loss 0.0642726942896843 Accuracy 0.8236250281333923\n",
      "Iteration 7780 Training loss 0.0647461861371994 Validation loss 0.06439542025327682 Accuracy 0.8248750567436218\n",
      "Iteration 7790 Training loss 0.05940411612391472 Validation loss 0.0642106831073761 Accuracy 0.8241250514984131\n",
      "Iteration 7800 Training loss 0.05950479209423065 Validation loss 0.06425195932388306 Accuracy 0.8232500553131104\n",
      "Iteration 7810 Training loss 0.059675201773643494 Validation loss 0.06409170478582382 Accuracy 0.8235000371932983\n",
      "Iteration 7820 Training loss 0.06121901795268059 Validation loss 0.06415107101202011 Accuracy 0.8236250281333923\n",
      "Iteration 7830 Training loss 0.06377773731946945 Validation loss 0.06409305334091187 Accuracy 0.8241250514984131\n",
      "Iteration 7840 Training loss 0.0704035684466362 Validation loss 0.06405504047870636 Accuracy 0.8238750100135803\n",
      "Iteration 7850 Training loss 0.0673663541674614 Validation loss 0.0640232190489769 Accuracy 0.8235000371932983\n",
      "Iteration 7860 Training loss 0.06666581332683563 Validation loss 0.0640380010008812 Accuracy 0.8247500658035278\n",
      "Iteration 7870 Training loss 0.0593809150159359 Validation loss 0.06401482224464417 Accuracy 0.8238750100135803\n",
      "Iteration 7880 Training loss 0.06596700102090836 Validation loss 0.06406188011169434 Accuracy 0.8243750333786011\n",
      "Iteration 7890 Training loss 0.06085066497325897 Validation loss 0.0640425831079483 Accuracy 0.8238750100135803\n",
      "Iteration 7900 Training loss 0.060429323464632034 Validation loss 0.06395772099494934 Accuracy 0.8237500190734863\n",
      "Iteration 7910 Training loss 0.059117019176483154 Validation loss 0.06448879092931747 Accuracy 0.8243750333786011\n",
      "Iteration 7920 Training loss 0.06078754737973213 Validation loss 0.06390845775604248 Accuracy 0.8250000476837158\n",
      "Iteration 7930 Training loss 0.06546617299318314 Validation loss 0.06389353424310684 Accuracy 0.8247500658035278\n",
      "Iteration 7940 Training loss 0.0631253570318222 Validation loss 0.06388434022665024 Accuracy 0.8247500658035278\n",
      "Iteration 7950 Training loss 0.06551475077867508 Validation loss 0.06398577988147736 Accuracy 0.8238750100135803\n",
      "Iteration 7960 Training loss 0.06627746671438217 Validation loss 0.06385333091020584 Accuracy 0.8238750100135803\n",
      "Iteration 7970 Training loss 0.06158829480409622 Validation loss 0.06413561850786209 Accuracy 0.8252500295639038\n",
      "Iteration 7980 Training loss 0.07449058443307877 Validation loss 0.06383226066827774 Accuracy 0.8245000243186951\n",
      "Iteration 7990 Training loss 0.06676703691482544 Validation loss 0.06384895741939545 Accuracy 0.8246250152587891\n",
      "Iteration 8000 Training loss 0.06579456478357315 Validation loss 0.06380448490381241 Accuracy 0.8247500658035278\n",
      "Iteration 8010 Training loss 0.06273773312568665 Validation loss 0.06378350406885147 Accuracy 0.8248750567436218\n",
      "Iteration 8020 Training loss 0.06536094099283218 Validation loss 0.063778817653656 Accuracy 0.8245000243186951\n",
      "Iteration 8030 Training loss 0.05883318930864334 Validation loss 0.06375737488269806 Accuracy 0.8255000114440918\n",
      "Iteration 8040 Training loss 0.06286898255348206 Validation loss 0.06397468596696854 Accuracy 0.8246250152587891\n",
      "Iteration 8050 Training loss 0.05775174871087074 Validation loss 0.06404710561037064 Accuracy 0.8255000114440918\n",
      "Iteration 8060 Training loss 0.07355529814958572 Validation loss 0.0637185350060463 Accuracy 0.8242500424385071\n",
      "Iteration 8070 Training loss 0.07035409659147263 Validation loss 0.0637255311012268 Accuracy 0.8242500424385071\n",
      "Iteration 8080 Training loss 0.05761409550905228 Validation loss 0.06378041952848434 Accuracy 0.8245000243186951\n",
      "Iteration 8090 Training loss 0.06804726272821426 Validation loss 0.06370091438293457 Accuracy 0.8245000243186951\n",
      "Iteration 8100 Training loss 0.06062045320868492 Validation loss 0.06364040821790695 Accuracy 0.8252500295639038\n",
      "Iteration 8110 Training loss 0.05697614699602127 Validation loss 0.06389676779508591 Accuracy 0.8246250152587891\n",
      "Iteration 8120 Training loss 0.05813146382570267 Validation loss 0.06383495777845383 Accuracy 0.8243750333786011\n",
      "Iteration 8130 Training loss 0.06266575306653976 Validation loss 0.06359969824552536 Accuracy 0.8251250386238098\n",
      "Iteration 8140 Training loss 0.06772144883871078 Validation loss 0.06371352076530457 Accuracy 0.8262500166893005\n",
      "Iteration 8150 Training loss 0.06479518115520477 Validation loss 0.06356547027826309 Accuracy 0.8255000114440918\n",
      "Iteration 8160 Training loss 0.0630655363202095 Validation loss 0.06362144649028778 Accuracy 0.8258750438690186\n",
      "Iteration 8170 Training loss 0.058712128549814224 Validation loss 0.06355379521846771 Accuracy 0.8253750205039978\n",
      "Iteration 8180 Training loss 0.06086951121687889 Validation loss 0.0636327713727951 Accuracy 0.8263750672340393\n",
      "Iteration 8190 Training loss 0.061709463596343994 Validation loss 0.06364304572343826 Accuracy 0.8256250619888306\n",
      "Iteration 8200 Training loss 0.06007910892367363 Validation loss 0.06352023035287857 Accuracy 0.8267500400543213\n",
      "Iteration 8210 Training loss 0.055049389600753784 Validation loss 0.0634731724858284 Accuracy 0.8260000348091125\n",
      "Iteration 8220 Training loss 0.06462343782186508 Validation loss 0.06361540406942368 Accuracy 0.8255000114440918\n",
      "Iteration 8230 Training loss 0.06589678674936295 Validation loss 0.06380491703748703 Accuracy 0.8271250128746033\n",
      "Iteration 8240 Training loss 0.06446638703346252 Validation loss 0.06347116827964783 Accuracy 0.8263750672340393\n",
      "Iteration 8250 Training loss 0.05815356969833374 Validation loss 0.06341861188411713 Accuracy 0.8266250491142273\n",
      "Iteration 8260 Training loss 0.057723596692085266 Validation loss 0.06340299546718597 Accuracy 0.8268750309944153\n",
      "Iteration 8270 Training loss 0.06554900109767914 Validation loss 0.0634191632270813 Accuracy 0.8262500166893005\n",
      "Iteration 8280 Training loss 0.06899344176054001 Validation loss 0.06339739263057709 Accuracy 0.8270000219345093\n",
      "Iteration 8290 Training loss 0.05962804704904556 Validation loss 0.0633796751499176 Accuracy 0.8266250491142273\n",
      "Iteration 8300 Training loss 0.06243416294455528 Validation loss 0.0634177029132843 Accuracy 0.8270000219345093\n",
      "Iteration 8310 Training loss 0.06766363978385925 Validation loss 0.06333498656749725 Accuracy 0.827875018119812\n",
      "Iteration 8320 Training loss 0.06187113747000694 Validation loss 0.06345817446708679 Accuracy 0.827250063419342\n",
      "Iteration 8330 Training loss 0.06309868395328522 Validation loss 0.06361176818609238 Accuracy 0.8270000219345093\n",
      "Iteration 8340 Training loss 0.06934244185686111 Validation loss 0.06336937099695206 Accuracy 0.827625036239624\n",
      "Iteration 8350 Training loss 0.06250031292438507 Validation loss 0.06328681111335754 Accuracy 0.827750027179718\n",
      "Iteration 8360 Training loss 0.05867442488670349 Validation loss 0.06326641887426376 Accuracy 0.827750027179718\n",
      "Iteration 8370 Training loss 0.05699126049876213 Validation loss 0.06338091939687729 Accuracy 0.8268750309944153\n",
      "Iteration 8380 Training loss 0.06391111016273499 Validation loss 0.06325899809598923 Accuracy 0.827250063419342\n",
      "Iteration 8390 Training loss 0.05970574542880058 Validation loss 0.06331667304039001 Accuracy 0.827875018119812\n",
      "Iteration 8400 Training loss 0.05195623263716698 Validation loss 0.06322073936462402 Accuracy 0.8280000686645508\n",
      "Iteration 8410 Training loss 0.06607142090797424 Validation loss 0.06322398036718369 Accuracy 0.827875018119812\n",
      "Iteration 8420 Training loss 0.061275601387023926 Validation loss 0.06328698247671127 Accuracy 0.8281250596046448\n",
      "Iteration 8430 Training loss 0.07757452130317688 Validation loss 0.06321229040622711 Accuracy 0.8286250233650208\n",
      "Iteration 8440 Training loss 0.06550489366054535 Validation loss 0.06319081038236618 Accuracy 0.82750004529953\n",
      "Iteration 8450 Training loss 0.06027007848024368 Validation loss 0.06317513436079025 Accuracy 0.827375054359436\n",
      "Iteration 8460 Training loss 0.06219421699643135 Validation loss 0.06314347684383392 Accuracy 0.8287500143051147\n",
      "Iteration 8470 Training loss 0.05830104649066925 Validation loss 0.06318280100822449 Accuracy 0.8280000686645508\n",
      "Iteration 8480 Training loss 0.06643068790435791 Validation loss 0.06332395225763321 Accuracy 0.827750027179718\n",
      "Iteration 8490 Training loss 0.055936772376298904 Validation loss 0.06318441033363342 Accuracy 0.827375054359436\n",
      "Iteration 8500 Training loss 0.060718778520822525 Validation loss 0.06323815882205963 Accuracy 0.8270000219345093\n",
      "Iteration 8510 Training loss 0.05892210826277733 Validation loss 0.06306347250938416 Accuracy 0.8286250233650208\n",
      "Iteration 8520 Training loss 0.06316913664340973 Validation loss 0.06304727494716644 Accuracy 0.8286250233650208\n",
      "Iteration 8530 Training loss 0.06742783635854721 Validation loss 0.06305672973394394 Accuracy 0.8285000324249268\n",
      "Iteration 8540 Training loss 0.055141057819128036 Validation loss 0.06310997903347015 Accuracy 0.8268750309944153\n",
      "Iteration 8550 Training loss 0.06478102505207062 Validation loss 0.06309791654348373 Accuracy 0.8268750309944153\n",
      "Iteration 8560 Training loss 0.06336572021245956 Validation loss 0.0629761815071106 Accuracy 0.8280000686645508\n",
      "Iteration 8570 Training loss 0.06242198869585991 Validation loss 0.06295983493328094 Accuracy 0.827750027179718\n",
      "Iteration 8580 Training loss 0.06100780516862869 Validation loss 0.06294353306293488 Accuracy 0.8281250596046448\n",
      "Iteration 8590 Training loss 0.06455179303884506 Validation loss 0.0629744604229927 Accuracy 0.8281250596046448\n",
      "Iteration 8600 Training loss 0.06169450655579567 Validation loss 0.06311909854412079 Accuracy 0.827750027179718\n",
      "Iteration 8610 Training loss 0.0569692887365818 Validation loss 0.0629458948969841 Accuracy 0.8283750414848328\n",
      "Iteration 8620 Training loss 0.06120028346776962 Validation loss 0.0629521980881691 Accuracy 0.8283750414848328\n",
      "Iteration 8630 Training loss 0.057939350605010986 Validation loss 0.0629049688577652 Accuracy 0.827750027179718\n",
      "Iteration 8640 Training loss 0.06366004049777985 Validation loss 0.0628744587302208 Accuracy 0.8286250233650208\n",
      "Iteration 8650 Training loss 0.05588121712207794 Validation loss 0.0629473477602005 Accuracy 0.8270000219345093\n",
      "Iteration 8660 Training loss 0.06396863609552383 Validation loss 0.06284452974796295 Accuracy 0.8282500505447388\n",
      "Iteration 8670 Training loss 0.06972324848175049 Validation loss 0.06285765022039413 Accuracy 0.8283750414848328\n",
      "Iteration 8680 Training loss 0.06543035060167313 Validation loss 0.06281612068414688 Accuracy 0.8283750414848328\n",
      "Iteration 8690 Training loss 0.06104384735226631 Validation loss 0.06279504299163818 Accuracy 0.8283750414848328\n",
      "Iteration 8700 Training loss 0.05459421128034592 Validation loss 0.06278718262910843 Accuracy 0.827875018119812\n",
      "Iteration 8710 Training loss 0.061413366347551346 Validation loss 0.06280215084552765 Accuracy 0.8283750414848328\n",
      "Iteration 8720 Training loss 0.06371971219778061 Validation loss 0.06275486946105957 Accuracy 0.8287500143051147\n",
      "Iteration 8730 Training loss 0.06068936362862587 Validation loss 0.06275871396064758 Accuracy 0.8288750648498535\n",
      "Iteration 8740 Training loss 0.0623430535197258 Validation loss 0.06282125413417816 Accuracy 0.82750004529953\n",
      "Iteration 8750 Training loss 0.055310625582933426 Validation loss 0.0627286434173584 Accuracy 0.8286250233650208\n",
      "Iteration 8760 Training loss 0.07338967174291611 Validation loss 0.0628606528043747 Accuracy 0.8283750414848328\n",
      "Iteration 8770 Training loss 0.06634344160556793 Validation loss 0.06301600486040115 Accuracy 0.827875018119812\n",
      "Iteration 8780 Training loss 0.05508169159293175 Validation loss 0.06270846724510193 Accuracy 0.8286250233650208\n",
      "Iteration 8790 Training loss 0.06142221763730049 Validation loss 0.06275387853384018 Accuracy 0.8282500505447388\n",
      "Iteration 8800 Training loss 0.06059122830629349 Validation loss 0.06267163157463074 Accuracy 0.8288750648498535\n",
      "Iteration 8810 Training loss 0.059424757957458496 Validation loss 0.06289251148700714 Accuracy 0.827750027179718\n",
      "Iteration 8820 Training loss 0.060651443898677826 Validation loss 0.06264347583055496 Accuracy 0.8283750414848328\n",
      "Iteration 8830 Training loss 0.06353291869163513 Validation loss 0.06286728382110596 Accuracy 0.8281250596046448\n",
      "Iteration 8840 Training loss 0.06247570365667343 Validation loss 0.06263428926467896 Accuracy 0.8293750286102295\n",
      "Iteration 8850 Training loss 0.06022094935178757 Validation loss 0.06275512278079987 Accuracy 0.8288750648498535\n",
      "Iteration 8860 Training loss 0.074077308177948 Validation loss 0.06257563829421997 Accuracy 0.8285000324249268\n",
      "Iteration 8870 Training loss 0.06029944866895676 Validation loss 0.06258858740329742 Accuracy 0.8295000195503235\n",
      "Iteration 8880 Training loss 0.05660772696137428 Validation loss 0.06263484805822372 Accuracy 0.8297500610351562\n",
      "Iteration 8890 Training loss 0.05531863868236542 Validation loss 0.06252165883779526 Accuracy 0.8297500610351562\n",
      "Iteration 8900 Training loss 0.0636223554611206 Validation loss 0.0625220537185669 Accuracy 0.8295000195503235\n",
      "Iteration 8910 Training loss 0.05915731191635132 Validation loss 0.06251490116119385 Accuracy 0.8286250233650208\n",
      "Iteration 8920 Training loss 0.0700436532497406 Validation loss 0.06249425187706947 Accuracy 0.830625057220459\n",
      "Iteration 8930 Training loss 0.062014978379011154 Validation loss 0.06247835233807564 Accuracy 0.8301250338554382\n",
      "Iteration 8940 Training loss 0.06304941326379776 Validation loss 0.062459927052259445 Accuracy 0.8301250338554382\n",
      "Iteration 8950 Training loss 0.061479661613702774 Validation loss 0.06243712082505226 Accuracy 0.8300000429153442\n",
      "Iteration 8960 Training loss 0.04882488027215004 Validation loss 0.062436703592538834 Accuracy 0.8292500376701355\n",
      "Iteration 8970 Training loss 0.05826449394226074 Validation loss 0.06243481487035751 Accuracy 0.830625057220459\n",
      "Iteration 8980 Training loss 0.05678556486964226 Validation loss 0.062477219849824905 Accuracy 0.8295000195503235\n",
      "Iteration 8990 Training loss 0.06313569098711014 Validation loss 0.06256032735109329 Accuracy 0.8302500247955322\n",
      "Iteration 9000 Training loss 0.07253398001194 Validation loss 0.06250855326652527 Accuracy 0.8283750414848328\n",
      "Iteration 9010 Training loss 0.06410873681306839 Validation loss 0.06238720938563347 Accuracy 0.830875039100647\n",
      "Iteration 9020 Training loss 0.060641635209321976 Validation loss 0.06234614923596382 Accuracy 0.830500066280365\n",
      "Iteration 9030 Training loss 0.060803402215242386 Validation loss 0.06248365342617035 Accuracy 0.830500066280365\n",
      "Iteration 9040 Training loss 0.06263427436351776 Validation loss 0.062426015734672546 Accuracy 0.8298750519752502\n",
      "Iteration 9050 Training loss 0.06058548018336296 Validation loss 0.062305301427841187 Accuracy 0.830625057220459\n",
      "Iteration 9060 Training loss 0.059764061123132706 Validation loss 0.0622808113694191 Accuracy 0.8300000429153442\n",
      "Iteration 9070 Training loss 0.06757082045078278 Validation loss 0.06232551857829094 Accuracy 0.830500066280365\n",
      "Iteration 9080 Training loss 0.05957592651247978 Validation loss 0.062546007335186 Accuracy 0.8302500247955322\n",
      "Iteration 9090 Training loss 0.06231047585606575 Validation loss 0.062260277569293976 Accuracy 0.8293750286102295\n",
      "Iteration 9100 Training loss 0.0597890168428421 Validation loss 0.062275055795907974 Accuracy 0.8302500247955322\n",
      "Iteration 9110 Training loss 0.05433391407132149 Validation loss 0.06222475692629814 Accuracy 0.830875039100647\n",
      "Iteration 9120 Training loss 0.06483665108680725 Validation loss 0.06222722679376602 Accuracy 0.831000030040741\n",
      "Iteration 9130 Training loss 0.06041062995791435 Validation loss 0.06233394145965576 Accuracy 0.830875039100647\n",
      "Iteration 9140 Training loss 0.06913720816373825 Validation loss 0.0621831975877285 Accuracy 0.8303750157356262\n",
      "Iteration 9150 Training loss 0.06390277296304703 Validation loss 0.06226220354437828 Accuracy 0.8302500247955322\n",
      "Iteration 9160 Training loss 0.05781654641032219 Validation loss 0.06219852715730667 Accuracy 0.8301250338554382\n",
      "Iteration 9170 Training loss 0.059800803661346436 Validation loss 0.06217190995812416 Accuracy 0.8298750519752502\n",
      "Iteration 9180 Training loss 0.05999474227428436 Validation loss 0.06212804839015007 Accuracy 0.8303750157356262\n",
      "Iteration 9190 Training loss 0.0600135512650013 Validation loss 0.06211366876959801 Accuracy 0.8301250338554382\n",
      "Iteration 9200 Training loss 0.06132279708981514 Validation loss 0.06210160255432129 Accuracy 0.8302500247955322\n",
      "Iteration 9210 Training loss 0.061099063605070114 Validation loss 0.06218720227479935 Accuracy 0.8303750157356262\n",
      "Iteration 9220 Training loss 0.06217128783464432 Validation loss 0.062375590205192566 Accuracy 0.8301250338554382\n",
      "Iteration 9230 Training loss 0.0601874478161335 Validation loss 0.06219496950507164 Accuracy 0.831000030040741\n",
      "Iteration 9240 Training loss 0.06172236427664757 Validation loss 0.06210112199187279 Accuracy 0.8303750157356262\n",
      "Iteration 9250 Training loss 0.05777747556567192 Validation loss 0.06216387450695038 Accuracy 0.830625057220459\n",
      "Iteration 9260 Training loss 0.07030071318149567 Validation loss 0.06202363967895508 Accuracy 0.831000030040741\n",
      "Iteration 9270 Training loss 0.06262604147195816 Validation loss 0.06200169771909714 Accuracy 0.831000030040741\n",
      "Iteration 9280 Training loss 0.057538993656635284 Validation loss 0.062049586325883865 Accuracy 0.830625057220459\n",
      "Iteration 9290 Training loss 0.05816974490880966 Validation loss 0.06197896972298622 Accuracy 0.831125020980835\n",
      "Iteration 9300 Training loss 0.07465141266584396 Validation loss 0.061989642679691315 Accuracy 0.831250011920929\n",
      "Iteration 9310 Training loss 0.05701766535639763 Validation loss 0.06201820820569992 Accuracy 0.830750048160553\n",
      "Iteration 9320 Training loss 0.05747956037521362 Validation loss 0.06195361912250519 Accuracy 0.8313750624656677\n",
      "Iteration 9330 Training loss 0.057163432240486145 Validation loss 0.06214643269777298 Accuracy 0.8316250443458557\n",
      "Iteration 9340 Training loss 0.05546308308839798 Validation loss 0.06201031059026718 Accuracy 0.8316250443458557\n",
      "Iteration 9350 Training loss 0.0608053021132946 Validation loss 0.06191359832882881 Accuracy 0.8313750624656677\n",
      "Iteration 9360 Training loss 0.06652727723121643 Validation loss 0.061919569969177246 Accuracy 0.8313750624656677\n",
      "Iteration 9370 Training loss 0.058009564876556396 Validation loss 0.06191880255937576 Accuracy 0.8316250443458557\n",
      "Iteration 9380 Training loss 0.060131337493658066 Validation loss 0.06189605966210365 Accuracy 0.831000030040741\n",
      "Iteration 9390 Training loss 0.05797988176345825 Validation loss 0.061887226998806 Accuracy 0.831250011920929\n",
      "Iteration 9400 Training loss 0.055684298276901245 Validation loss 0.06185819208621979 Accuracy 0.831000030040741\n",
      "Iteration 9410 Training loss 0.05995018407702446 Validation loss 0.06185014545917511 Accuracy 0.8320000171661377\n",
      "Iteration 9420 Training loss 0.06564249098300934 Validation loss 0.06184917688369751 Accuracy 0.8316250443458557\n",
      "Iteration 9430 Training loss 0.0671771764755249 Validation loss 0.06185649335384369 Accuracy 0.831125020980835\n",
      "Iteration 9440 Training loss 0.0687437504529953 Validation loss 0.06181354448199272 Accuracy 0.8320000171661377\n",
      "Iteration 9450 Training loss 0.057040341198444366 Validation loss 0.06190013885498047 Accuracy 0.830875039100647\n",
      "Iteration 9460 Training loss 0.05315463989973068 Validation loss 0.061782438308000565 Accuracy 0.8320000171661377\n",
      "Iteration 9470 Training loss 0.06005672365427017 Validation loss 0.061760660260915756 Accuracy 0.8320000171661377\n",
      "Iteration 9480 Training loss 0.0637679249048233 Validation loss 0.06198808550834656 Accuracy 0.8317500352859497\n",
      "Iteration 9490 Training loss 0.06515093892812729 Validation loss 0.061747871339321136 Accuracy 0.8321250677108765\n",
      "Iteration 9500 Training loss 0.05848204344511032 Validation loss 0.06175821274518967 Accuracy 0.8320000171661377\n",
      "Iteration 9510 Training loss 0.06445484608411789 Validation loss 0.0617254413664341 Accuracy 0.8317500352859497\n",
      "Iteration 9520 Training loss 0.06492580473423004 Validation loss 0.061708372086286545 Accuracy 0.8317500352859497\n",
      "Iteration 9530 Training loss 0.05499007925391197 Validation loss 0.061696916818618774 Accuracy 0.8323750495910645\n",
      "Iteration 9540 Training loss 0.06681739538908005 Validation loss 0.061688750982284546 Accuracy 0.8317500352859497\n",
      "Iteration 9550 Training loss 0.05900375172495842 Validation loss 0.061901479959487915 Accuracy 0.8315000534057617\n",
      "Iteration 9560 Training loss 0.05866876617074013 Validation loss 0.06204397231340408 Accuracy 0.8313750624656677\n",
      "Iteration 9570 Training loss 0.06342672556638718 Validation loss 0.061638232320547104 Accuracy 0.8320000171661377\n",
      "Iteration 9580 Training loss 0.07109182327985764 Validation loss 0.06161094829440117 Accuracy 0.8327500224113464\n",
      "Iteration 9590 Training loss 0.06709613651037216 Validation loss 0.0616043396294117 Accuracy 0.8328750133514404\n",
      "Iteration 9600 Training loss 0.05865989252924919 Validation loss 0.061580635607242584 Accuracy 0.8327500224113464\n",
      "Iteration 9610 Training loss 0.05933103337883949 Validation loss 0.06162498891353607 Accuracy 0.8326250314712524\n",
      "Iteration 9620 Training loss 0.04635726660490036 Validation loss 0.061571210622787476 Accuracy 0.8322500586509705\n",
      "Iteration 9630 Training loss 0.06075919419527054 Validation loss 0.06160390377044678 Accuracy 0.8321250677108765\n",
      "Iteration 9640 Training loss 0.059752922505140305 Validation loss 0.061589889228343964 Accuracy 0.8318750262260437\n",
      "Iteration 9650 Training loss 0.07100678980350494 Validation loss 0.0615789070725441 Accuracy 0.8330000638961792\n",
      "Iteration 9660 Training loss 0.05616699159145355 Validation loss 0.061545755714178085 Accuracy 0.8326250314712524\n",
      "Iteration 9670 Training loss 0.05681871250271797 Validation loss 0.06152000278234482 Accuracy 0.8321250677108765\n",
      "Iteration 9680 Training loss 0.05907059088349342 Validation loss 0.06151900440454483 Accuracy 0.8322500586509705\n",
      "Iteration 9690 Training loss 0.05971716716885567 Validation loss 0.06168586015701294 Accuracy 0.8327500224113464\n",
      "Iteration 9700 Training loss 0.05804628133773804 Validation loss 0.06165319308638573 Accuracy 0.8325000405311584\n",
      "Iteration 9710 Training loss 0.054661162197589874 Validation loss 0.06145570054650307 Accuracy 0.8325000405311584\n",
      "Iteration 9720 Training loss 0.06323660910129547 Validation loss 0.06161251664161682 Accuracy 0.8323750495910645\n",
      "Iteration 9730 Training loss 0.06582736223936081 Validation loss 0.061432577669620514 Accuracy 0.8318750262260437\n",
      "Iteration 9740 Training loss 0.051516760140657425 Validation loss 0.06144721806049347 Accuracy 0.8325000405311584\n",
      "Iteration 9750 Training loss 0.0702643096446991 Validation loss 0.061439089477062225 Accuracy 0.8326250314712524\n",
      "Iteration 9760 Training loss 0.0590965636074543 Validation loss 0.06149857118725777 Accuracy 0.8313750624656677\n",
      "Iteration 9770 Training loss 0.06441880762577057 Validation loss 0.06143394857645035 Accuracy 0.8325000405311584\n",
      "Iteration 9780 Training loss 0.05804551765322685 Validation loss 0.06143629923462868 Accuracy 0.8328750133514404\n",
      "Iteration 9790 Training loss 0.057424016296863556 Validation loss 0.061430059373378754 Accuracy 0.8315000534057617\n",
      "Iteration 9800 Training loss 0.05983257666230202 Validation loss 0.06141671538352966 Accuracy 0.8322500586509705\n",
      "Iteration 9810 Training loss 0.061636026948690414 Validation loss 0.06147322803735733 Accuracy 0.8336250185966492\n",
      "Iteration 9820 Training loss 0.06487969309091568 Validation loss 0.06143219769001007 Accuracy 0.8325000405311584\n",
      "Iteration 9830 Training loss 0.06521597504615784 Validation loss 0.06143178418278694 Accuracy 0.8320000171661377\n",
      "Iteration 9840 Training loss 0.06052454933524132 Validation loss 0.06140538677573204 Accuracy 0.8337500691413879\n",
      "Iteration 9850 Training loss 0.06086629629135132 Validation loss 0.06139146536588669 Accuracy 0.8326250314712524\n",
      "Iteration 9860 Training loss 0.06165337562561035 Validation loss 0.06134482100605965 Accuracy 0.8337500691413879\n",
      "Iteration 9870 Training loss 0.06427982449531555 Validation loss 0.062084559351205826 Accuracy 0.830625057220459\n",
      "Iteration 9880 Training loss 0.05523554980754852 Validation loss 0.06128415837883949 Accuracy 0.8333750367164612\n",
      "Iteration 9890 Training loss 0.06337251514196396 Validation loss 0.06125548109412193 Accuracy 0.8335000276565552\n",
      "Iteration 9900 Training loss 0.06293679028749466 Validation loss 0.061254385858774185 Accuracy 0.8326250314712524\n",
      "Iteration 9910 Training loss 0.05446687713265419 Validation loss 0.06124520301818848 Accuracy 0.8343750238418579\n",
      "Iteration 9920 Training loss 0.05852711945772171 Validation loss 0.06124630942940712 Accuracy 0.8336250185966492\n",
      "Iteration 9930 Training loss 0.05779675021767616 Validation loss 0.06119069457054138 Accuracy 0.8343750238418579\n",
      "Iteration 9940 Training loss 0.06815511733293533 Validation loss 0.06119289994239807 Accuracy 0.8342500329017639\n",
      "Iteration 9950 Training loss 0.061049871146678925 Validation loss 0.0611807145178318 Accuracy 0.8341250419616699\n",
      "Iteration 9960 Training loss 0.06361058354377747 Validation loss 0.06131015345454216 Accuracy 0.8323750495910645\n",
      "Iteration 9970 Training loss 0.06943662464618683 Validation loss 0.06115225329995155 Accuracy 0.8338750600814819\n",
      "Iteration 9980 Training loss 0.06458202004432678 Validation loss 0.061169691383838654 Accuracy 0.8335000276565552\n",
      "Iteration 9990 Training loss 0.062369346618652344 Validation loss 0.061143215745687485 Accuracy 0.8335000276565552\n",
      "Iteration 10000 Training loss 0.061643704771995544 Validation loss 0.06122385337948799 Accuracy 0.8330000638961792\n",
      "Iteration 10010 Training loss 0.058652620762586594 Validation loss 0.0612952895462513 Accuracy 0.8323750495910645\n",
      "Iteration 10020 Training loss 0.056075166910886765 Validation loss 0.06113538518548012 Accuracy 0.8340000510215759\n",
      "Iteration 10030 Training loss 0.06136776879429817 Validation loss 0.06121569871902466 Accuracy 0.8330000638961792\n",
      "Iteration 10040 Training loss 0.05965045839548111 Validation loss 0.06110880523920059 Accuracy 0.8332500457763672\n",
      "Iteration 10050 Training loss 0.06535176932811737 Validation loss 0.06133938208222389 Accuracy 0.8328750133514404\n",
      "Iteration 10060 Training loss 0.056208934634923935 Validation loss 0.061106692999601364 Accuracy 0.8356250524520874\n",
      "Iteration 10070 Training loss 0.06431347876787186 Validation loss 0.061044562608003616 Accuracy 0.8346250653266907\n",
      "Iteration 10080 Training loss 0.06349523365497589 Validation loss 0.06105339899659157 Accuracy 0.8348750472068787\n",
      "Iteration 10090 Training loss 0.060988254845142365 Validation loss 0.06129058822989464 Accuracy 0.8331250548362732\n",
      "Iteration 10100 Training loss 0.05600178986787796 Validation loss 0.06118191406130791 Accuracy 0.8341250419616699\n",
      "Iteration 10110 Training loss 0.05896935239434242 Validation loss 0.06099987402558327 Accuracy 0.8342500329017639\n",
      "Iteration 10120 Training loss 0.06019657850265503 Validation loss 0.06100098416209221 Accuracy 0.8348750472068787\n",
      "Iteration 10130 Training loss 0.05652981624007225 Validation loss 0.06103162840008736 Accuracy 0.8343750238418579\n",
      "Iteration 10140 Training loss 0.05822256952524185 Validation loss 0.06098911538720131 Accuracy 0.8348750472068787\n",
      "Iteration 10150 Training loss 0.05656985193490982 Validation loss 0.060967978090047836 Accuracy 0.8346250653266907\n",
      "Iteration 10160 Training loss 0.06556518375873566 Validation loss 0.061100978404283524 Accuracy 0.8337500691413879\n",
      "Iteration 10170 Training loss 0.05884891375899315 Validation loss 0.06096462160348892 Accuracy 0.8342500329017639\n",
      "Iteration 10180 Training loss 0.05850359424948692 Validation loss 0.06114189326763153 Accuracy 0.8333750367164612\n",
      "Iteration 10190 Training loss 0.05812946334481239 Validation loss 0.06092504411935806 Accuracy 0.8340000510215759\n",
      "Iteration 10200 Training loss 0.05818253383040428 Validation loss 0.060913898050785065 Accuracy 0.8352500200271606\n",
      "Iteration 10210 Training loss 0.06108352541923523 Validation loss 0.060947444289922714 Accuracy 0.8347500562667847\n",
      "Iteration 10220 Training loss 0.0662495344877243 Validation loss 0.060878511518239975 Accuracy 0.8347500562667847\n",
      "Iteration 10230 Training loss 0.05902595445513725 Validation loss 0.06089315935969353 Accuracy 0.8348750472068787\n",
      "Iteration 10240 Training loss 0.062352489680051804 Validation loss 0.06087141111493111 Accuracy 0.8352500200271606\n",
      "Iteration 10250 Training loss 0.05497507005929947 Validation loss 0.06097578629851341 Accuracy 0.8342500329017639\n",
      "Iteration 10260 Training loss 0.05873136222362518 Validation loss 0.060843102633953094 Accuracy 0.8353750109672546\n",
      "Iteration 10270 Training loss 0.06453724950551987 Validation loss 0.06105458363890648 Accuracy 0.8330000638961792\n",
      "Iteration 10280 Training loss 0.06124653294682503 Validation loss 0.060817912220954895 Accuracy 0.8348750472068787\n",
      "Iteration 10290 Training loss 0.05601595342159271 Validation loss 0.06081249192357063 Accuracy 0.8352500200271606\n",
      "Iteration 10300 Training loss 0.05570853129029274 Validation loss 0.06080411747097969 Accuracy 0.8348750472068787\n",
      "Iteration 10310 Training loss 0.0604938343167305 Validation loss 0.0609799288213253 Accuracy 0.8338750600814819\n",
      "Iteration 10320 Training loss 0.058933552354574203 Validation loss 0.060748107731342316 Accuracy 0.8355000615119934\n",
      "Iteration 10330 Training loss 0.06566210836172104 Validation loss 0.06084209308028221 Accuracy 0.8341250419616699\n",
      "Iteration 10340 Training loss 0.05880890414118767 Validation loss 0.06073339283466339 Accuracy 0.8352500200271606\n",
      "Iteration 10350 Training loss 0.052128035575151443 Validation loss 0.06090154871344566 Accuracy 0.8341250419616699\n",
      "Iteration 10360 Training loss 0.057407986372709274 Validation loss 0.060707759112119675 Accuracy 0.8347500562667847\n",
      "Iteration 10370 Training loss 0.053926169872283936 Validation loss 0.06076979637145996 Accuracy 0.8343750238418579\n",
      "Iteration 10380 Training loss 0.061534710228443146 Validation loss 0.06070423871278763 Accuracy 0.8351250290870667\n",
      "Iteration 10390 Training loss 0.05219841003417969 Validation loss 0.06080794706940651 Accuracy 0.8346250653266907\n",
      "Iteration 10400 Training loss 0.05815291404724121 Validation loss 0.060695718973875046 Accuracy 0.8345000147819519\n",
      "Iteration 10410 Training loss 0.059124905616045 Validation loss 0.0606951005756855 Accuracy 0.8352500200271606\n",
      "Iteration 10420 Training loss 0.06105823069810867 Validation loss 0.060695260763168335 Accuracy 0.8342500329017639\n",
      "Iteration 10430 Training loss 0.05952335521578789 Validation loss 0.06070869415998459 Accuracy 0.8332500457763672\n",
      "Iteration 10440 Training loss 0.06851956248283386 Validation loss 0.06065927818417549 Accuracy 0.8343750238418579\n",
      "Iteration 10450 Training loss 0.05334091931581497 Validation loss 0.06066097691655159 Accuracy 0.8356250524520874\n",
      "Iteration 10460 Training loss 0.05567879602313042 Validation loss 0.060600221157073975 Accuracy 0.8351250290870667\n",
      "Iteration 10470 Training loss 0.06534167379140854 Validation loss 0.06059951335191727 Accuracy 0.8346250653266907\n",
      "Iteration 10480 Training loss 0.059125788509845734 Validation loss 0.06059729680418968 Accuracy 0.8351250290870667\n",
      "Iteration 10490 Training loss 0.06316080689430237 Validation loss 0.06061631813645363 Accuracy 0.8365000486373901\n",
      "Iteration 10500 Training loss 0.060882844030857086 Validation loss 0.060591623187065125 Accuracy 0.8366250395774841\n",
      "Iteration 10510 Training loss 0.05601062998175621 Validation loss 0.06056267023086548 Accuracy 0.8365000486373901\n",
      "Iteration 10520 Training loss 0.06282287836074829 Validation loss 0.060541871935129166 Accuracy 0.8350000381469727\n",
      "Iteration 10530 Training loss 0.0663829818367958 Validation loss 0.06057615205645561 Accuracy 0.8352500200271606\n",
      "Iteration 10540 Training loss 0.05995020642876625 Validation loss 0.06056603416800499 Accuracy 0.8348750472068787\n",
      "Iteration 10550 Training loss 0.058601949363946915 Validation loss 0.06072394177317619 Accuracy 0.8345000147819519\n",
      "Iteration 10560 Training loss 0.04603057727217674 Validation loss 0.06058184430003166 Accuracy 0.8337500691413879\n",
      "Iteration 10570 Training loss 0.055008336901664734 Validation loss 0.06052376702427864 Accuracy 0.8345000147819519\n",
      "Iteration 10580 Training loss 0.0570051483809948 Validation loss 0.060519758611917496 Accuracy 0.8350000381469727\n",
      "Iteration 10590 Training loss 0.05827292427420616 Validation loss 0.06046878919005394 Accuracy 0.8353750109672546\n",
      "Iteration 10600 Training loss 0.06094168499112129 Validation loss 0.06046684458851814 Accuracy 0.8360000252723694\n",
      "Iteration 10610 Training loss 0.05440815910696983 Validation loss 0.060453422367572784 Accuracy 0.8355000615119934\n",
      "Iteration 10620 Training loss 0.05755364149808884 Validation loss 0.060500793159008026 Accuracy 0.8357500433921814\n",
      "Iteration 10630 Training loss 0.06722918152809143 Validation loss 0.06083778664469719 Accuracy 0.8345000147819519\n",
      "Iteration 10640 Training loss 0.06641658395528793 Validation loss 0.060506902635097504 Accuracy 0.8362500667572021\n",
      "Iteration 10650 Training loss 0.06473975628614426 Validation loss 0.06042545288801193 Accuracy 0.8355000615119934\n",
      "Iteration 10660 Training loss 0.05072722211480141 Validation loss 0.06049840897321701 Accuracy 0.8363750576972961\n",
      "Iteration 10670 Training loss 0.06288579106330872 Validation loss 0.06045141816139221 Accuracy 0.8368750214576721\n",
      "Iteration 10680 Training loss 0.04885728284716606 Validation loss 0.0603933222591877 Accuracy 0.8367500305175781\n",
      "Iteration 10690 Training loss 0.0545099712908268 Validation loss 0.060415126383304596 Accuracy 0.8357500433921814\n",
      "Iteration 10700 Training loss 0.05546797811985016 Validation loss 0.06042959913611412 Accuracy 0.8350000381469727\n",
      "Iteration 10710 Training loss 0.06478247046470642 Validation loss 0.06051965802907944 Accuracy 0.8351250290870667\n",
      "Iteration 10720 Training loss 0.0549435093998909 Validation loss 0.06037333980202675 Accuracy 0.8366250395774841\n",
      "Iteration 10730 Training loss 0.0634787306189537 Validation loss 0.06045187637209892 Accuracy 0.8347500562667847\n",
      "Iteration 10740 Training loss 0.059151165187358856 Validation loss 0.06033630296587944 Accuracy 0.8356250524520874\n",
      "Iteration 10750 Training loss 0.06841747462749481 Validation loss 0.0603281706571579 Accuracy 0.8356250524520874\n",
      "Iteration 10760 Training loss 0.07167518138885498 Validation loss 0.06032685190439224 Accuracy 0.8353750109672546\n",
      "Iteration 10770 Training loss 0.05453085899353027 Validation loss 0.06033497303724289 Accuracy 0.8377500176429749\n",
      "Iteration 10780 Training loss 0.061920490115880966 Validation loss 0.06029285863041878 Accuracy 0.8356250524520874\n",
      "Iteration 10790 Training loss 0.04721067473292351 Validation loss 0.06035307049751282 Accuracy 0.8357500433921814\n",
      "Iteration 10800 Training loss 0.06375111639499664 Validation loss 0.060267023742198944 Accuracy 0.8363750576972961\n",
      "Iteration 10810 Training loss 0.05681310594081879 Validation loss 0.06029706820845604 Accuracy 0.8367500305175781\n",
      "Iteration 10820 Training loss 0.060860030353069305 Validation loss 0.06022702530026436 Accuracy 0.8353750109672546\n",
      "Iteration 10830 Training loss 0.061237651854753494 Validation loss 0.06021760776638985 Accuracy 0.8358750343322754\n",
      "Iteration 10840 Training loss 0.05757461115717888 Validation loss 0.060207974165678024 Accuracy 0.8351250290870667\n",
      "Iteration 10850 Training loss 0.05353357270359993 Validation loss 0.060312096029520035 Accuracy 0.8360000252723694\n",
      "Iteration 10860 Training loss 0.056788451969623566 Validation loss 0.060190487653017044 Accuracy 0.8375000357627869\n",
      "Iteration 10870 Training loss 0.06087913736701012 Validation loss 0.06024814769625664 Accuracy 0.8362500667572021\n",
      "Iteration 10880 Training loss 0.05948806554079056 Validation loss 0.06017450615763664 Accuracy 0.8358750343322754\n",
      "Iteration 10890 Training loss 0.05185327306389809 Validation loss 0.06015222147107124 Accuracy 0.8371250629425049\n",
      "Iteration 10900 Training loss 0.061734285205602646 Validation loss 0.06017325073480606 Accuracy 0.8377500176429749\n",
      "Iteration 10910 Training loss 0.056460823863744736 Validation loss 0.060376740992069244 Accuracy 0.8363750576972961\n",
      "Iteration 10920 Training loss 0.05267434939742088 Validation loss 0.060112178325653076 Accuracy 0.8382500410079956\n",
      "Iteration 10930 Training loss 0.04974210634827614 Validation loss 0.06009872630238533 Accuracy 0.8381250500679016\n",
      "Iteration 10940 Training loss 0.057614028453826904 Validation loss 0.060124147683382034 Accuracy 0.8382500410079956\n",
      "Iteration 10950 Training loss 0.06185255944728851 Validation loss 0.060076866298913956 Accuracy 0.8373750448226929\n",
      "Iteration 10960 Training loss 0.05325544625520706 Validation loss 0.06006874516606331 Accuracy 0.8375000357627869\n",
      "Iteration 10970 Training loss 0.06480344384908676 Validation loss 0.06026564911007881 Accuracy 0.8368750214576721\n",
      "Iteration 10980 Training loss 0.05287845432758331 Validation loss 0.06007431447505951 Accuracy 0.8363750576972961\n",
      "Iteration 10990 Training loss 0.05375208705663681 Validation loss 0.06018882617354393 Accuracy 0.8391250371932983\n",
      "Iteration 11000 Training loss 0.058369528502225876 Validation loss 0.060145314782857895 Accuracy 0.8368750214576721\n",
      "Iteration 11010 Training loss 0.056760337203741074 Validation loss 0.06009475514292717 Accuracy 0.8370000123977661\n",
      "Iteration 11020 Training loss 0.05929994583129883 Validation loss 0.06035660579800606 Accuracy 0.8363750576972961\n",
      "Iteration 11030 Training loss 0.05856533721089363 Validation loss 0.06003083288669586 Accuracy 0.8372500538825989\n",
      "Iteration 11040 Training loss 0.06963049620389938 Validation loss 0.06002048775553703 Accuracy 0.8373750448226929\n",
      "Iteration 11050 Training loss 0.06278582662343979 Validation loss 0.06000648811459541 Accuracy 0.8383750319480896\n",
      "Iteration 11060 Training loss 0.05433505401015282 Validation loss 0.05998857319355011 Accuracy 0.8376250267028809\n",
      "Iteration 11070 Training loss 0.06159108877182007 Validation loss 0.059990327805280685 Accuracy 0.8388750553131104\n",
      "Iteration 11080 Training loss 0.06852876394987106 Validation loss 0.060026880353689194 Accuracy 0.8383750319480896\n",
      "Iteration 11090 Training loss 0.06363645941019058 Validation loss 0.060005344450473785 Accuracy 0.8365000486373901\n",
      "Iteration 11100 Training loss 0.0641007348895073 Validation loss 0.0599675215780735 Accuracy 0.8365000486373901\n",
      "Iteration 11110 Training loss 0.051067184656858444 Validation loss 0.06014971807599068 Accuracy 0.8361250162124634\n",
      "Iteration 11120 Training loss 0.05433327704668045 Validation loss 0.05991186946630478 Accuracy 0.8385000228881836\n",
      "Iteration 11130 Training loss 0.061045531183481216 Validation loss 0.05996455252170563 Accuracy 0.8385000228881836\n",
      "Iteration 11140 Training loss 0.06023307517170906 Validation loss 0.059911374002695084 Accuracy 0.8373750448226929\n",
      "Iteration 11150 Training loss 0.06265823543071747 Validation loss 0.059893377125263214 Accuracy 0.8368750214576721\n",
      "Iteration 11160 Training loss 0.0567815862596035 Validation loss 0.06002827361226082 Accuracy 0.8362500667572021\n",
      "Iteration 11170 Training loss 0.05600695312023163 Validation loss 0.05994652211666107 Accuracy 0.8391250371932983\n",
      "Iteration 11180 Training loss 0.05386223644018173 Validation loss 0.060014285147190094 Accuracy 0.8358750343322754\n",
      "Iteration 11190 Training loss 0.058414820581674576 Validation loss 0.059981394559144974 Accuracy 0.8362500667572021\n",
      "Iteration 11200 Training loss 0.06071190536022186 Validation loss 0.059853266924619675 Accuracy 0.8386250138282776\n",
      "Iteration 11210 Training loss 0.05902830883860588 Validation loss 0.05981598049402237 Accuracy 0.8385000228881836\n",
      "Iteration 11220 Training loss 0.05764656886458397 Validation loss 0.05983014777302742 Accuracy 0.8397500514984131\n",
      "Iteration 11230 Training loss 0.04555526748299599 Validation loss 0.059815824031829834 Accuracy 0.8378750681877136\n",
      "Iteration 11240 Training loss 0.06388945132493973 Validation loss 0.05980739742517471 Accuracy 0.8382500410079956\n",
      "Iteration 11250 Training loss 0.061285827308893204 Validation loss 0.0598333403468132 Accuracy 0.8383750319480896\n",
      "Iteration 11260 Training loss 0.06343626230955124 Validation loss 0.059785082936286926 Accuracy 0.8381250500679016\n",
      "Iteration 11270 Training loss 0.05837155133485794 Validation loss 0.0597890205681324 Accuracy 0.8380000591278076\n",
      "Iteration 11280 Training loss 0.06387314200401306 Validation loss 0.059858813881874084 Accuracy 0.8370000123977661\n",
      "Iteration 11290 Training loss 0.059055015444755554 Validation loss 0.059823065996170044 Accuracy 0.8401250243186951\n",
      "Iteration 11300 Training loss 0.06987511366605759 Validation loss 0.05994885414838791 Accuracy 0.8377500176429749\n",
      "Iteration 11310 Training loss 0.058882780373096466 Validation loss 0.05973165109753609 Accuracy 0.8387500643730164\n",
      "Iteration 11320 Training loss 0.062393318861722946 Validation loss 0.05973416194319725 Accuracy 0.8385000228881836\n",
      "Iteration 11330 Training loss 0.06000332161784172 Validation loss 0.059961531311273575 Accuracy 0.8366250395774841\n",
      "Iteration 11340 Training loss 0.06211785227060318 Validation loss 0.059718403965234756 Accuracy 0.8386250138282776\n",
      "Iteration 11350 Training loss 0.05110052600502968 Validation loss 0.05969887971878052 Accuracy 0.8390000462532043\n",
      "Iteration 11360 Training loss 0.058828458189964294 Validation loss 0.05970189347863197 Accuracy 0.8397500514984131\n",
      "Iteration 11370 Training loss 0.05564024671912193 Validation loss 0.05978076532483101 Accuracy 0.8380000591278076\n",
      "Iteration 11380 Training loss 0.06311263144016266 Validation loss 0.05968224257230759 Accuracy 0.8390000462532043\n",
      "Iteration 11390 Training loss 0.07036788016557693 Validation loss 0.05973906069993973 Accuracy 0.8382500410079956\n",
      "Iteration 11400 Training loss 0.05842607468366623 Validation loss 0.05979928746819496 Accuracy 0.8381250500679016\n",
      "Iteration 11410 Training loss 0.06158897280693054 Validation loss 0.05963031202554703 Accuracy 0.8395000696182251\n",
      "Iteration 11420 Training loss 0.05912217125296593 Validation loss 0.059601668268442154 Accuracy 0.8400000333786011\n",
      "Iteration 11430 Training loss 0.05485108122229576 Validation loss 0.05963800475001335 Accuracy 0.8385000228881836\n",
      "Iteration 11440 Training loss 0.05709671974182129 Validation loss 0.05985979735851288 Accuracy 0.8381250500679016\n",
      "Iteration 11450 Training loss 0.05438348650932312 Validation loss 0.05965278297662735 Accuracy 0.8382500410079956\n",
      "Iteration 11460 Training loss 0.05236617848277092 Validation loss 0.05960124731063843 Accuracy 0.8390000462532043\n",
      "Iteration 11470 Training loss 0.053878918290138245 Validation loss 0.059703368693590164 Accuracy 0.8383750319480896\n",
      "Iteration 11480 Training loss 0.06745821982622147 Validation loss 0.0596066415309906 Accuracy 0.8386250138282776\n",
      "Iteration 11490 Training loss 0.06428265571594238 Validation loss 0.059607889503240585 Accuracy 0.8381250500679016\n",
      "Iteration 11500 Training loss 0.058691829442977905 Validation loss 0.0595984049141407 Accuracy 0.8375000357627869\n",
      "Iteration 11510 Training loss 0.05300190672278404 Validation loss 0.05961740389466286 Accuracy 0.8381250500679016\n",
      "Iteration 11520 Training loss 0.058357495814561844 Validation loss 0.05954832211136818 Accuracy 0.8378750681877136\n",
      "Iteration 11530 Training loss 0.060204897075891495 Validation loss 0.05951443687081337 Accuracy 0.8402500152587891\n",
      "Iteration 11540 Training loss 0.0645410344004631 Validation loss 0.05967294052243233 Accuracy 0.8376250267028809\n",
      "Iteration 11550 Training loss 0.05662795528769493 Validation loss 0.05969288945198059 Accuracy 0.8387500643730164\n",
      "Iteration 11560 Training loss 0.05784473195672035 Validation loss 0.05967729538679123 Accuracy 0.8383750319480896\n",
      "Iteration 11570 Training loss 0.05931558459997177 Validation loss 0.05966153368353844 Accuracy 0.8386250138282776\n",
      "Iteration 11580 Training loss 0.05617864429950714 Validation loss 0.05949879065155983 Accuracy 0.8390000462532043\n",
      "Iteration 11590 Training loss 0.06785035878419876 Validation loss 0.059457872062921524 Accuracy 0.8401250243186951\n",
      "Iteration 11600 Training loss 0.059974391013383865 Validation loss 0.059551090002059937 Accuracy 0.8385000228881836\n",
      "Iteration 11610 Training loss 0.0593731589615345 Validation loss 0.059429366141557693 Accuracy 0.8397500514984131\n",
      "Iteration 11620 Training loss 0.055637769401073456 Validation loss 0.05941310524940491 Accuracy 0.8408750295639038\n",
      "Iteration 11630 Training loss 0.058284785598516464 Validation loss 0.059411562979221344 Accuracy 0.8402500152587891\n",
      "Iteration 11640 Training loss 0.054420214146375656 Validation loss 0.05942271649837494 Accuracy 0.8397500514984131\n",
      "Iteration 11650 Training loss 0.0506298802793026 Validation loss 0.05943923443555832 Accuracy 0.8402500152587891\n",
      "Iteration 11660 Training loss 0.05993793532252312 Validation loss 0.05940695479512215 Accuracy 0.8395000696182251\n",
      "Iteration 11670 Training loss 0.05801244080066681 Validation loss 0.05939009413123131 Accuracy 0.8400000333786011\n",
      "Iteration 11680 Training loss 0.06071499362587929 Validation loss 0.0593838207423687 Accuracy 0.8402500152587891\n",
      "Iteration 11690 Training loss 0.054928481578826904 Validation loss 0.059401508420705795 Accuracy 0.8417500257492065\n",
      "Iteration 11700 Training loss 0.05783338472247124 Validation loss 0.05934590846300125 Accuracy 0.8402500152587891\n",
      "Iteration 11710 Training loss 0.06293056905269623 Validation loss 0.05942322686314583 Accuracy 0.8392500281333923\n",
      "Iteration 11720 Training loss 0.0548536479473114 Validation loss 0.05943769961595535 Accuracy 0.8393750190734863\n",
      "Iteration 11730 Training loss 0.070744089782238 Validation loss 0.05934032052755356 Accuracy 0.8418750166893005\n",
      "Iteration 11740 Training loss 0.05605945363640785 Validation loss 0.059692200273275375 Accuracy 0.8387500643730164\n",
      "Iteration 11750 Training loss 0.06179344654083252 Validation loss 0.05930172652006149 Accuracy 0.8403750658035278\n",
      "Iteration 11760 Training loss 0.0629647970199585 Validation loss 0.05935866758227348 Accuracy 0.8406250476837158\n",
      "Iteration 11770 Training loss 0.06253200024366379 Validation loss 0.05929557979106903 Accuracy 0.8406250476837158\n",
      "Iteration 11780 Training loss 0.06233308091759682 Validation loss 0.05933170020580292 Accuracy 0.8403750658035278\n",
      "Iteration 11790 Training loss 0.06556510180234909 Validation loss 0.059321023523807526 Accuracy 0.8406250476837158\n",
      "Iteration 11800 Training loss 0.059859514236450195 Validation loss 0.05926302447915077 Accuracy 0.8412500619888306\n",
      "Iteration 11810 Training loss 0.05641559511423111 Validation loss 0.059324439615011215 Accuracy 0.8408750295639038\n",
      "Iteration 11820 Training loss 0.06314655393362045 Validation loss 0.05926729738712311 Accuracy 0.8401250243186951\n",
      "Iteration 11830 Training loss 0.06223849207162857 Validation loss 0.059262972325086594 Accuracy 0.8403750658035278\n",
      "Iteration 11840 Training loss 0.056808970868587494 Validation loss 0.05925417318940163 Accuracy 0.8402500152587891\n",
      "Iteration 11850 Training loss 0.06101999059319496 Validation loss 0.05930561572313309 Accuracy 0.8391250371932983\n",
      "Iteration 11860 Training loss 0.060802459716796875 Validation loss 0.05923129618167877 Accuracy 0.8397500514984131\n",
      "Iteration 11870 Training loss 0.05158855393528938 Validation loss 0.05961492285132408 Accuracy 0.8396250605583191\n",
      "Iteration 11880 Training loss 0.051695480942726135 Validation loss 0.05957664176821709 Accuracy 0.8398750424385071\n",
      "Iteration 11890 Training loss 0.051883332431316376 Validation loss 0.059304285794496536 Accuracy 0.8397500514984131\n",
      "Iteration 11900 Training loss 0.054043129086494446 Validation loss 0.059248026460409164 Accuracy 0.8415000438690186\n",
      "Iteration 11910 Training loss 0.0577378086745739 Validation loss 0.05917147174477577 Accuracy 0.8418750166893005\n",
      "Iteration 11920 Training loss 0.060419850051403046 Validation loss 0.05918336659669876 Accuracy 0.8408750295639038\n",
      "Iteration 11930 Training loss 0.05796882510185242 Validation loss 0.0592324323952198 Accuracy 0.8401250243186951\n",
      "Iteration 11940 Training loss 0.05265704169869423 Validation loss 0.05913757532835007 Accuracy 0.8416250348091125\n",
      "Iteration 11950 Training loss 0.06272220611572266 Validation loss 0.05932608246803284 Accuracy 0.8402500152587891\n",
      "Iteration 11960 Training loss 0.05664999037981033 Validation loss 0.059134770184755325 Accuracy 0.8417500257492065\n",
      "Iteration 11970 Training loss 0.061749376356601715 Validation loss 0.05925510451197624 Accuracy 0.8410000205039978\n",
      "Iteration 11980 Training loss 0.056647151708602905 Validation loss 0.059193942695856094 Accuracy 0.8408750295639038\n",
      "Iteration 11990 Training loss 0.0576435923576355 Validation loss 0.0592113733291626 Accuracy 0.8410000205039978\n",
      "Iteration 12000 Training loss 0.054660335183143616 Validation loss 0.05930382385849953 Accuracy 0.8406250476837158\n",
      "Iteration 12010 Training loss 0.06293127685785294 Validation loss 0.05910452455282211 Accuracy 0.8410000205039978\n",
      "Iteration 12020 Training loss 0.05145423859357834 Validation loss 0.0590890534222126 Accuracy 0.8411250114440918\n",
      "Iteration 12030 Training loss 0.057985857129096985 Validation loss 0.059071846306324005 Accuracy 0.8417500257492065\n",
      "Iteration 12040 Training loss 0.05320802703499794 Validation loss 0.05981588363647461 Accuracy 0.8387500643730164\n",
      "Iteration 12050 Training loss 0.06279351562261581 Validation loss 0.059067387133836746 Accuracy 0.8422500491142273\n",
      "Iteration 12060 Training loss 0.05833040550351143 Validation loss 0.05912427976727486 Accuracy 0.8413750529289246\n",
      "Iteration 12070 Training loss 0.05900602042675018 Validation loss 0.059092357754707336 Accuracy 0.8415000438690186\n",
      "Iteration 12080 Training loss 0.0630417987704277 Validation loss 0.05910155922174454 Accuracy 0.8408750295639038\n",
      "Iteration 12090 Training loss 0.05773531273007393 Validation loss 0.05910491198301315 Accuracy 0.8413750529289246\n",
      "Iteration 12100 Training loss 0.051226403564214706 Validation loss 0.05906647816300392 Accuracy 0.8403750658035278\n",
      "Iteration 12110 Training loss 0.06076812744140625 Validation loss 0.05913807824254036 Accuracy 0.8415000438690186\n",
      "Iteration 12120 Training loss 0.05887981131672859 Validation loss 0.05907182767987251 Accuracy 0.8412500619888306\n",
      "Iteration 12130 Training loss 0.06197870150208473 Validation loss 0.05897746607661247 Accuracy 0.8423750400543213\n",
      "Iteration 12140 Training loss 0.05659956857562065 Validation loss 0.05908830091357231 Accuracy 0.8412500619888306\n",
      "Iteration 12150 Training loss 0.05826393514871597 Validation loss 0.059070881456136703 Accuracy 0.8417500257492065\n",
      "Iteration 12160 Training loss 0.06361042708158493 Validation loss 0.059011392295360565 Accuracy 0.8403750658035278\n",
      "Iteration 12170 Training loss 0.05611870810389519 Validation loss 0.05897551402449608 Accuracy 0.8418750166893005\n",
      "Iteration 12180 Training loss 0.0573270209133625 Validation loss 0.05906854569911957 Accuracy 0.8415000438690186\n",
      "Iteration 12190 Training loss 0.056516073644161224 Validation loss 0.058914944529533386 Accuracy 0.843250036239624\n",
      "Iteration 12200 Training loss 0.06810074299573898 Validation loss 0.058894503861665726 Accuracy 0.84312504529953\n",
      "Iteration 12210 Training loss 0.05387384444475174 Validation loss 0.058916639536619186 Accuracy 0.8417500257492065\n",
      "Iteration 12220 Training loss 0.05452428385615349 Validation loss 0.058939266949892044 Accuracy 0.8423750400543213\n",
      "Iteration 12230 Training loss 0.06493140012025833 Validation loss 0.05900095775723457 Accuracy 0.8416250348091125\n",
      "Iteration 12240 Training loss 0.05524349957704544 Validation loss 0.05884460732340813 Accuracy 0.8426250219345093\n",
      "Iteration 12250 Training loss 0.05683834105730057 Validation loss 0.05898968130350113 Accuracy 0.8413750529289246\n",
      "Iteration 12260 Training loss 0.05416315048933029 Validation loss 0.05891871079802513 Accuracy 0.8408750295639038\n",
      "Iteration 12270 Training loss 0.06632063537836075 Validation loss 0.058836184442043304 Accuracy 0.8427500128746033\n",
      "Iteration 12280 Training loss 0.06191160902380943 Validation loss 0.05885941535234451 Accuracy 0.8423750400543213\n",
      "Iteration 12290 Training loss 0.05867614597082138 Validation loss 0.05883662402629852 Accuracy 0.8423750400543213\n",
      "Iteration 12300 Training loss 0.06423872709274292 Validation loss 0.05884828791022301 Accuracy 0.8426250219345093\n",
      "Iteration 12310 Training loss 0.05277642980217934 Validation loss 0.05880998820066452 Accuracy 0.84312504529953\n",
      "Iteration 12320 Training loss 0.061959464102983475 Validation loss 0.05947897210717201 Accuracy 0.8391250371932983\n",
      "Iteration 12330 Training loss 0.05264019966125488 Validation loss 0.05881347879767418 Accuracy 0.8421250581741333\n",
      "Iteration 12340 Training loss 0.059841081500053406 Validation loss 0.058813340961933136 Accuracy 0.84312504529953\n",
      "Iteration 12350 Training loss 0.06190372258424759 Validation loss 0.05881837010383606 Accuracy 0.8421250581741333\n",
      "Iteration 12360 Training loss 0.06534549593925476 Validation loss 0.05884727090597153 Accuracy 0.8417500257492065\n",
      "Iteration 12370 Training loss 0.05983920022845268 Validation loss 0.05878531187772751 Accuracy 0.8418750166893005\n",
      "Iteration 12380 Training loss 0.057099368423223495 Validation loss 0.058805957436561584 Accuracy 0.8425000309944153\n",
      "Iteration 12390 Training loss 0.05400539189577103 Validation loss 0.059319622814655304 Accuracy 0.8403750658035278\n",
      "Iteration 12400 Training loss 0.05452634394168854 Validation loss 0.058870408684015274 Accuracy 0.8425000309944153\n",
      "Iteration 12410 Training loss 0.054678935557603836 Validation loss 0.05881287530064583 Accuracy 0.8425000309944153\n",
      "Iteration 12420 Training loss 0.05372314155101776 Validation loss 0.05878390744328499 Accuracy 0.8411250114440918\n",
      "Iteration 12430 Training loss 0.06112349033355713 Validation loss 0.05875670909881592 Accuracy 0.8408750295639038\n",
      "Iteration 12440 Training loss 0.05174078047275543 Validation loss 0.05874289199709892 Accuracy 0.8411250114440918\n",
      "Iteration 12450 Training loss 0.06354443728923798 Validation loss 0.05872802436351776 Accuracy 0.8423750400543213\n",
      "Iteration 12460 Training loss 0.05714985728263855 Validation loss 0.05873144790530205 Accuracy 0.8418750166893005\n",
      "Iteration 12470 Training loss 0.05971904471516609 Validation loss 0.05870119109749794 Accuracy 0.8437500596046448\n",
      "Iteration 12480 Training loss 0.06120891124010086 Validation loss 0.05871954560279846 Accuracy 0.8426250219345093\n",
      "Iteration 12490 Training loss 0.05285384878516197 Validation loss 0.05871303007006645 Accuracy 0.8426250219345093\n",
      "Iteration 12500 Training loss 0.06501125544309616 Validation loss 0.05869986116886139 Accuracy 0.843375027179718\n",
      "Iteration 12510 Training loss 0.04981962963938713 Validation loss 0.05871450901031494 Accuracy 0.8421250581741333\n",
      "Iteration 12520 Training loss 0.0639273077249527 Validation loss 0.05867641791701317 Accuracy 0.843375027179718\n",
      "Iteration 12530 Training loss 0.057947855442762375 Validation loss 0.05868217721581459 Accuracy 0.8437500596046448\n",
      "Iteration 12540 Training loss 0.060375671833753586 Validation loss 0.05868929624557495 Accuracy 0.843250036239624\n",
      "Iteration 12550 Training loss 0.05297744646668434 Validation loss 0.05874057114124298 Accuracy 0.842875063419342\n",
      "Iteration 12560 Training loss 0.05233514681458473 Validation loss 0.0586777999997139 Accuracy 0.843250036239624\n",
      "Iteration 12570 Training loss 0.060817621648311615 Validation loss 0.05868563801050186 Accuracy 0.8415000438690186\n",
      "Iteration 12580 Training loss 0.06293249130249023 Validation loss 0.058655306696891785 Accuracy 0.8421250581741333\n",
      "Iteration 12590 Training loss 0.056792326271533966 Validation loss 0.058726731687784195 Accuracy 0.8420000672340393\n",
      "Iteration 12600 Training loss 0.05822159722447395 Validation loss 0.0586218424141407 Accuracy 0.8420000672340393\n",
      "Iteration 12610 Training loss 0.054389819502830505 Validation loss 0.05862896144390106 Accuracy 0.8423750400543213\n",
      "Iteration 12620 Training loss 0.055246833711862564 Validation loss 0.05860046669840813 Accuracy 0.8441250324249268\n",
      "Iteration 12630 Training loss 0.054292354732751846 Validation loss 0.058952972292900085 Accuracy 0.8417500257492065\n",
      "Iteration 12640 Training loss 0.05075245350599289 Validation loss 0.05863356217741966 Accuracy 0.8422500491142273\n",
      "Iteration 12650 Training loss 0.06165735051035881 Validation loss 0.05858331173658371 Accuracy 0.84312504529953\n",
      "Iteration 12660 Training loss 0.06583625823259354 Validation loss 0.058553729206323624 Accuracy 0.8422500491142273\n",
      "Iteration 12670 Training loss 0.052788011729717255 Validation loss 0.05855758115649223 Accuracy 0.8426250219345093\n",
      "Iteration 12680 Training loss 0.057397861033678055 Validation loss 0.05857254937291145 Accuracy 0.8437500596046448\n",
      "Iteration 12690 Training loss 0.050326645374298096 Validation loss 0.058510832488536835 Accuracy 0.84312504529953\n",
      "Iteration 12700 Training loss 0.05380234122276306 Validation loss 0.05851226672530174 Accuracy 0.843500018119812\n",
      "Iteration 12710 Training loss 0.06400751322507858 Validation loss 0.058691974729299545 Accuracy 0.8425000309944153\n",
      "Iteration 12720 Training loss 0.0516887903213501 Validation loss 0.058476291596889496 Accuracy 0.84312504529953\n",
      "Iteration 12730 Training loss 0.06174628064036369 Validation loss 0.05844337120652199 Accuracy 0.8438750505447388\n",
      "Iteration 12740 Training loss 0.05140332132577896 Validation loss 0.05846097320318222 Accuracy 0.8436250686645508\n",
      "Iteration 12750 Training loss 0.05404318869113922 Validation loss 0.05843222141265869 Accuracy 0.8437500596046448\n",
      "Iteration 12760 Training loss 0.057802267372608185 Validation loss 0.058588866144418716 Accuracy 0.8427500128746033\n",
      "Iteration 12770 Training loss 0.06176012009382248 Validation loss 0.05842247232794762 Accuracy 0.8448750376701355\n",
      "Iteration 12780 Training loss 0.05482171103358269 Validation loss 0.058433715254068375 Accuracy 0.8446250557899475\n",
      "Iteration 12790 Training loss 0.05075351521372795 Validation loss 0.058420274406671524 Accuracy 0.8438750505447388\n",
      "Iteration 12800 Training loss 0.05870573967695236 Validation loss 0.058462101966142654 Accuracy 0.8437500596046448\n",
      "Iteration 12810 Training loss 0.05434601753950119 Validation loss 0.058397937566041946 Accuracy 0.8445000648498535\n",
      "Iteration 12820 Training loss 0.05911651626229286 Validation loss 0.05841876566410065 Accuracy 0.8443750143051147\n",
      "Iteration 12830 Training loss 0.05463787913322449 Validation loss 0.05845414847135544 Accuracy 0.842875063419342\n",
      "Iteration 12840 Training loss 0.04982564225792885 Validation loss 0.05844280868768692 Accuracy 0.8436250686645508\n",
      "Iteration 12850 Training loss 0.05691905319690704 Validation loss 0.058490149676799774 Accuracy 0.8436250686645508\n",
      "Iteration 12860 Training loss 0.06308627128601074 Validation loss 0.05868377164006233 Accuracy 0.84312504529953\n",
      "Iteration 12870 Training loss 0.06213495135307312 Validation loss 0.05886618420481682 Accuracy 0.8420000672340393\n",
      "Iteration 12880 Training loss 0.05737645551562309 Validation loss 0.058370865881443024 Accuracy 0.8436250686645508\n",
      "Iteration 12890 Training loss 0.06023643910884857 Validation loss 0.058672647923231125 Accuracy 0.842875063419342\n",
      "Iteration 12900 Training loss 0.0546838752925396 Validation loss 0.05854913219809532 Accuracy 0.8423750400543213\n",
      "Iteration 12910 Training loss 0.06183210760354996 Validation loss 0.05834213271737099 Accuracy 0.8438750505447388\n",
      "Iteration 12920 Training loss 0.05525066331028938 Validation loss 0.05834977328777313 Accuracy 0.8442500233650208\n",
      "Iteration 12930 Training loss 0.05807885527610779 Validation loss 0.05838114395737648 Accuracy 0.843500018119812\n",
      "Iteration 12940 Training loss 0.05826446786522865 Validation loss 0.058402273803949356 Accuracy 0.8442500233650208\n",
      "Iteration 12950 Training loss 0.0673767626285553 Validation loss 0.05827809125185013 Accuracy 0.8452500104904175\n",
      "Iteration 12960 Training loss 0.053074758499860764 Validation loss 0.058287136256694794 Accuracy 0.8440000414848328\n",
      "Iteration 12970 Training loss 0.05424336716532707 Validation loss 0.05828757584095001 Accuracy 0.8442500233650208\n",
      "Iteration 12980 Training loss 0.06347381323575974 Validation loss 0.058398425579071045 Accuracy 0.8440000414848328\n",
      "Iteration 12990 Training loss 0.06243934482336044 Validation loss 0.05828333646059036 Accuracy 0.8448750376701355\n",
      "Iteration 13000 Training loss 0.05686004459857941 Validation loss 0.05840613320469856 Accuracy 0.842875063419342\n",
      "Iteration 13010 Training loss 0.0587199330329895 Validation loss 0.05829516798257828 Accuracy 0.8448750376701355\n",
      "Iteration 13020 Training loss 0.057624444365501404 Validation loss 0.05829555541276932 Accuracy 0.8447500467300415\n",
      "Iteration 13030 Training loss 0.053140923380851746 Validation loss 0.05835741013288498 Accuracy 0.843500018119812\n",
      "Iteration 13040 Training loss 0.05092909187078476 Validation loss 0.058246638625860214 Accuracy 0.8448750376701355\n",
      "Iteration 13050 Training loss 0.06033682823181152 Validation loss 0.058299869298934937 Accuracy 0.8443750143051147\n",
      "Iteration 13060 Training loss 0.06879988312721252 Validation loss 0.058209337294101715 Accuracy 0.8440000414848328\n",
      "Iteration 13070 Training loss 0.05913669615983963 Validation loss 0.05819357931613922 Accuracy 0.8442500233650208\n",
      "Iteration 13080 Training loss 0.053538549691438675 Validation loss 0.05818483233451843 Accuracy 0.843250036239624\n",
      "Iteration 13090 Training loss 0.0544683113694191 Validation loss 0.058173585683107376 Accuracy 0.8442500233650208\n",
      "Iteration 13100 Training loss 0.06274347007274628 Validation loss 0.05817464366555214 Accuracy 0.8441250324249268\n",
      "Iteration 13110 Training loss 0.05648795887827873 Validation loss 0.05818569287657738 Accuracy 0.8442500233650208\n",
      "Iteration 13120 Training loss 0.056250713765621185 Validation loss 0.05830719321966171 Accuracy 0.843000054359436\n",
      "Iteration 13130 Training loss 0.06123415753245354 Validation loss 0.05819287523627281 Accuracy 0.8442500233650208\n",
      "Iteration 13140 Training loss 0.06021309643983841 Validation loss 0.05825043469667435 Accuracy 0.843000054359436\n",
      "Iteration 13150 Training loss 0.048743836581707 Validation loss 0.058262646198272705 Accuracy 0.8445000648498535\n",
      "Iteration 13160 Training loss 0.05852777510881424 Validation loss 0.058126144111156464 Accuracy 0.8442500233650208\n",
      "Iteration 13170 Training loss 0.05882059782743454 Validation loss 0.058129481971263885 Accuracy 0.843500018119812\n",
      "Iteration 13180 Training loss 0.06103420630097389 Validation loss 0.058111026883125305 Accuracy 0.8436250686645508\n",
      "Iteration 13190 Training loss 0.05898793414235115 Validation loss 0.05815569683909416 Accuracy 0.8438750505447388\n",
      "Iteration 13200 Training loss 0.0615302175283432 Validation loss 0.05817048251628876 Accuracy 0.8443750143051147\n",
      "Iteration 13210 Training loss 0.05550527572631836 Validation loss 0.05860484391450882 Accuracy 0.8423750400543213\n",
      "Iteration 13220 Training loss 0.056223392486572266 Validation loss 0.058161988854408264 Accuracy 0.8450000286102295\n",
      "Iteration 13230 Training loss 0.06698858737945557 Validation loss 0.05805889144539833 Accuracy 0.8456250429153442\n",
      "Iteration 13240 Training loss 0.0587894581258297 Validation loss 0.05806193873286247 Accuracy 0.8451250195503235\n",
      "Iteration 13250 Training loss 0.056541964411735535 Validation loss 0.05805679038167 Accuracy 0.8455000519752502\n",
      "Iteration 13260 Training loss 0.05113939940929413 Validation loss 0.05806327238678932 Accuracy 0.8450000286102295\n",
      "Iteration 13270 Training loss 0.061316099017858505 Validation loss 0.05802975967526436 Accuracy 0.8457500338554382\n",
      "Iteration 13280 Training loss 0.062058404088020325 Validation loss 0.058019112795591354 Accuracy 0.8448750376701355\n",
      "Iteration 13290 Training loss 0.06448782980442047 Validation loss 0.05815090611577034 Accuracy 0.8451250195503235\n",
      "Iteration 13300 Training loss 0.056448668241500854 Validation loss 0.05815541371703148 Accuracy 0.8445000648498535\n",
      "Iteration 13310 Training loss 0.07011017948389053 Validation loss 0.057994257658720016 Accuracy 0.8458750247955322\n",
      "Iteration 13320 Training loss 0.06023078411817551 Validation loss 0.05798742547631264 Accuracy 0.8452500104904175\n",
      "Iteration 13330 Training loss 0.0532526895403862 Validation loss 0.05801256746053696 Accuracy 0.846375048160553\n",
      "Iteration 13340 Training loss 0.05922871083021164 Validation loss 0.0582856684923172 Accuracy 0.8437500596046448\n",
      "Iteration 13350 Training loss 0.057277850806713104 Validation loss 0.057991378009319305 Accuracy 0.846375048160553\n",
      "Iteration 13360 Training loss 0.0700841024518013 Validation loss 0.057967912405729294 Accuracy 0.8451250195503235\n",
      "Iteration 13370 Training loss 0.05264822021126747 Validation loss 0.05800959840416908 Accuracy 0.8470000624656677\n",
      "Iteration 13380 Training loss 0.0521199069917202 Validation loss 0.057999175041913986 Accuracy 0.8476250171661377\n",
      "Iteration 13390 Training loss 0.0614243745803833 Validation loss 0.05804121494293213 Accuracy 0.846500039100647\n",
      "Iteration 13400 Training loss 0.059696536511182785 Validation loss 0.05790097266435623 Accuracy 0.8448750376701355\n",
      "Iteration 13410 Training loss 0.05688988417387009 Validation loss 0.058135997503995895 Accuracy 0.8445000648498535\n",
      "Iteration 13420 Training loss 0.06302334368228912 Validation loss 0.05788552388548851 Accuracy 0.8456250429153442\n",
      "Iteration 13430 Training loss 0.053768184036016464 Validation loss 0.05789712443947792 Accuracy 0.8452500104904175\n",
      "Iteration 13440 Training loss 0.05979907512664795 Validation loss 0.05809516832232475 Accuracy 0.8450000286102295\n",
      "Iteration 13450 Training loss 0.057068441063165665 Validation loss 0.05784318968653679 Accuracy 0.8448750376701355\n",
      "Iteration 13460 Training loss 0.05975469574332237 Validation loss 0.05786348506808281 Accuracy 0.8471250534057617\n",
      "Iteration 13470 Training loss 0.055989041924476624 Validation loss 0.05802244320511818 Accuracy 0.846125066280365\n",
      "Iteration 13480 Training loss 0.06399247795343399 Validation loss 0.05783698707818985 Accuracy 0.8450000286102295\n",
      "Iteration 13490 Training loss 0.06549648195505142 Validation loss 0.05784925818443298 Accuracy 0.8453750610351562\n",
      "Iteration 13500 Training loss 0.05868053436279297 Validation loss 0.0579078234732151 Accuracy 0.846125066280365\n",
      "Iteration 13510 Training loss 0.06374157965183258 Validation loss 0.05787232145667076 Accuracy 0.846125066280365\n",
      "Iteration 13520 Training loss 0.059746403247117996 Validation loss 0.057930175215005875 Accuracy 0.8470000624656677\n",
      "Iteration 13530 Training loss 0.06876780837774277 Validation loss 0.057901062071323395 Accuracy 0.8448750376701355\n",
      "Iteration 13540 Training loss 0.05276670679450035 Validation loss 0.05785476043820381 Accuracy 0.846125066280365\n",
      "Iteration 13550 Training loss 0.06262433528900146 Validation loss 0.057812776416540146 Accuracy 0.8460000157356262\n",
      "Iteration 13560 Training loss 0.0506092831492424 Validation loss 0.05780620127916336 Accuracy 0.8456250429153442\n",
      "Iteration 13570 Training loss 0.055864255875349045 Validation loss 0.05781731754541397 Accuracy 0.8457500338554382\n",
      "Iteration 13580 Training loss 0.0502786710858345 Validation loss 0.05780170112848282 Accuracy 0.8457500338554382\n",
      "Iteration 13590 Training loss 0.05802503973245621 Validation loss 0.05785596743226051 Accuracy 0.8456250429153442\n",
      "Iteration 13600 Training loss 0.06311824917793274 Validation loss 0.05850227549672127 Accuracy 0.8437500596046448\n",
      "Iteration 13610 Training loss 0.05086454376578331 Validation loss 0.05783892050385475 Accuracy 0.8460000157356262\n",
      "Iteration 13620 Training loss 0.053161878138780594 Validation loss 0.057768840342760086 Accuracy 0.846125066280365\n",
      "Iteration 13630 Training loss 0.055009875446558 Validation loss 0.057758670300245285 Accuracy 0.846125066280365\n",
      "Iteration 13640 Training loss 0.06127851456403732 Validation loss 0.05784233286976814 Accuracy 0.8460000157356262\n",
      "Iteration 13650 Training loss 0.0558026023209095 Validation loss 0.05798321217298508 Accuracy 0.8453750610351562\n",
      "Iteration 13660 Training loss 0.05008362606167793 Validation loss 0.05805572122335434 Accuracy 0.8456250429153442\n",
      "Iteration 13670 Training loss 0.06610589474439621 Validation loss 0.05772506445646286 Accuracy 0.8471250534057617\n",
      "Iteration 13680 Training loss 0.05137541517615318 Validation loss 0.05771743878722191 Accuracy 0.8481250405311584\n",
      "Iteration 13690 Training loss 0.05534294620156288 Validation loss 0.057710979133844376 Accuracy 0.8472500443458557\n",
      "Iteration 13700 Training loss 0.06366821378469467 Validation loss 0.05767546221613884 Accuracy 0.8457500338554382\n",
      "Iteration 13710 Training loss 0.0483214370906353 Validation loss 0.0576859787106514 Accuracy 0.846375048160553\n",
      "Iteration 13720 Training loss 0.0560297854244709 Validation loss 0.05770540609955788 Accuracy 0.846750020980835\n",
      "Iteration 13730 Training loss 0.049154751002788544 Validation loss 0.05799395218491554 Accuracy 0.8455000519752502\n",
      "Iteration 13740 Training loss 0.05831734463572502 Validation loss 0.0576942078769207 Accuracy 0.8470000624656677\n",
      "Iteration 13750 Training loss 0.05505950003862381 Validation loss 0.05767657980322838 Accuracy 0.8470000624656677\n",
      "Iteration 13760 Training loss 0.055355824530124664 Validation loss 0.05766401067376137 Accuracy 0.846250057220459\n",
      "Iteration 13770 Training loss 0.053236979991197586 Validation loss 0.057842448353767395 Accuracy 0.8458750247955322\n",
      "Iteration 13780 Training loss 0.05143672972917557 Validation loss 0.05767720192670822 Accuracy 0.8471250534057617\n",
      "Iteration 13790 Training loss 0.06303542852401733 Validation loss 0.057647354900836945 Accuracy 0.846250057220459\n",
      "Iteration 13800 Training loss 0.0639912411570549 Validation loss 0.057681821286678314 Accuracy 0.8472500443458557\n",
      "Iteration 13810 Training loss 0.056310221552848816 Validation loss 0.05771177262067795 Accuracy 0.8450000286102295\n",
      "Iteration 13820 Training loss 0.04965488240122795 Validation loss 0.057650964707136154 Accuracy 0.8477500677108765\n",
      "Iteration 13830 Training loss 0.0554688423871994 Validation loss 0.057772960513830185 Accuracy 0.846375048160553\n",
      "Iteration 13840 Training loss 0.06412137299776077 Validation loss 0.057693205773830414 Accuracy 0.8476250171661377\n",
      "Iteration 13850 Training loss 0.05145459994673729 Validation loss 0.05785255879163742 Accuracy 0.846375048160553\n",
      "Iteration 13860 Training loss 0.05502127483487129 Validation loss 0.05759444087743759 Accuracy 0.8455000519752502\n",
      "Iteration 13870 Training loss 0.05816440284252167 Validation loss 0.05762776359915733 Accuracy 0.8456250429153442\n",
      "Iteration 13880 Training loss 0.05950517579913139 Validation loss 0.05755623057484627 Accuracy 0.846125066280365\n",
      "Iteration 13890 Training loss 0.05637533217668533 Validation loss 0.057678475975990295 Accuracy 0.846875011920929\n",
      "Iteration 13900 Training loss 0.05881844460964203 Validation loss 0.05755995213985443 Accuracy 0.8456250429153442\n",
      "Iteration 13910 Training loss 0.05190311372280121 Validation loss 0.05756603181362152 Accuracy 0.8460000157356262\n",
      "Iteration 13920 Training loss 0.05226646363735199 Validation loss 0.05781600624322891 Accuracy 0.846500039100647\n",
      "Iteration 13930 Training loss 0.05126720294356346 Validation loss 0.057590335607528687 Accuracy 0.8470000624656677\n",
      "Iteration 13940 Training loss 0.053040388971567154 Validation loss 0.05755436047911644 Accuracy 0.8473750352859497\n",
      "Iteration 13950 Training loss 0.054317593574523926 Validation loss 0.057521045207977295 Accuracy 0.8457500338554382\n",
      "Iteration 13960 Training loss 0.04888003319501877 Validation loss 0.057603754103183746 Accuracy 0.8477500677108765\n",
      "Iteration 13970 Training loss 0.05606365576386452 Validation loss 0.057525698095560074 Accuracy 0.8472500443458557\n",
      "Iteration 13980 Training loss 0.061003487557172775 Validation loss 0.057498835027217865 Accuracy 0.8480000495910645\n",
      "Iteration 13990 Training loss 0.05835377424955368 Validation loss 0.05765678733587265 Accuracy 0.846500039100647\n",
      "Iteration 14000 Training loss 0.06077735871076584 Validation loss 0.05763007700443268 Accuracy 0.846500039100647\n",
      "Iteration 14010 Training loss 0.061044786125421524 Validation loss 0.057468995451927185 Accuracy 0.8471250534057617\n",
      "Iteration 14020 Training loss 0.058159392327070236 Validation loss 0.057469531893730164 Accuracy 0.8470000624656677\n",
      "Iteration 14030 Training loss 0.05228089168667793 Validation loss 0.057466812431812286 Accuracy 0.8470000624656677\n",
      "Iteration 14040 Training loss 0.059133775532245636 Validation loss 0.057457469403743744 Accuracy 0.8472500443458557\n",
      "Iteration 14050 Training loss 0.04864847660064697 Validation loss 0.05754098668694496 Accuracy 0.8471250534057617\n",
      "Iteration 14060 Training loss 0.053932350128889084 Validation loss 0.05741517245769501 Accuracy 0.846500039100647\n",
      "Iteration 14070 Training loss 0.05771635100245476 Validation loss 0.057415109127759933 Accuracy 0.846125066280365\n",
      "Iteration 14080 Training loss 0.06222159415483475 Validation loss 0.057637378573417664 Accuracy 0.846125066280365\n",
      "Iteration 14090 Training loss 0.05355292558670044 Validation loss 0.05745069310069084 Accuracy 0.8476250171661377\n",
      "Iteration 14100 Training loss 0.05954771861433983 Validation loss 0.05740859732031822 Accuracy 0.8470000624656677\n",
      "Iteration 14110 Training loss 0.05548526719212532 Validation loss 0.05740763992071152 Accuracy 0.8481250405311584\n",
      "Iteration 14120 Training loss 0.05840618908405304 Validation loss 0.057416364550590515 Accuracy 0.8471250534057617\n",
      "Iteration 14130 Training loss 0.06424427777528763 Validation loss 0.057406261563301086 Accuracy 0.8475000262260437\n",
      "Iteration 14140 Training loss 0.05621837452054024 Validation loss 0.05739757418632507 Accuracy 0.8472500443458557\n",
      "Iteration 14150 Training loss 0.06307999789714813 Validation loss 0.05738942325115204 Accuracy 0.8483750224113464\n",
      "Iteration 14160 Training loss 0.06194823980331421 Validation loss 0.057489026337862015 Accuracy 0.8475000262260437\n",
      "Iteration 14170 Training loss 0.06119373068213463 Validation loss 0.05740071088075638 Accuracy 0.8478750586509705\n",
      "Iteration 14180 Training loss 0.05641733482480049 Validation loss 0.0573992021381855 Accuracy 0.846625030040741\n",
      "Iteration 14190 Training loss 0.052630890160799026 Validation loss 0.05742907151579857 Accuracy 0.8477500677108765\n",
      "Iteration 14200 Training loss 0.06485334783792496 Validation loss 0.0574239082634449 Accuracy 0.8478750586509705\n",
      "Iteration 14210 Training loss 0.0499577522277832 Validation loss 0.057323817163705826 Accuracy 0.8475000262260437\n",
      "Iteration 14220 Training loss 0.054477594792842865 Validation loss 0.057482436299324036 Accuracy 0.846875011920929\n",
      "Iteration 14230 Training loss 0.0602039135992527 Validation loss 0.057695962488651276 Accuracy 0.846375048160553\n",
      "Iteration 14240 Training loss 0.0567605160176754 Validation loss 0.05737972632050514 Accuracy 0.8476250171661377\n",
      "Iteration 14250 Training loss 0.05905338004231453 Validation loss 0.05733712762594223 Accuracy 0.8471250534057617\n",
      "Iteration 14260 Training loss 0.06588367372751236 Validation loss 0.057283058762550354 Accuracy 0.8476250171661377\n",
      "Iteration 14270 Training loss 0.05252368003129959 Validation loss 0.057255037128925323 Accuracy 0.8481250405311584\n",
      "Iteration 14280 Training loss 0.060832131654024124 Validation loss 0.0573749765753746 Accuracy 0.8477500677108765\n",
      "Iteration 14290 Training loss 0.05622829496860504 Validation loss 0.057319026440382004 Accuracy 0.8482500314712524\n",
      "Iteration 14300 Training loss 0.04943879321217537 Validation loss 0.05768775939941406 Accuracy 0.846375048160553\n",
      "Iteration 14310 Training loss 0.05361798033118248 Validation loss 0.057606350630521774 Accuracy 0.846125066280365\n",
      "Iteration 14320 Training loss 0.058507613837718964 Validation loss 0.05726010724902153 Accuracy 0.8481250405311584\n",
      "Iteration 14330 Training loss 0.0573585070669651 Validation loss 0.05724390596151352 Accuracy 0.8477500677108765\n",
      "Iteration 14340 Training loss 0.05924595147371292 Validation loss 0.05735957622528076 Accuracy 0.8472500443458557\n",
      "Iteration 14350 Training loss 0.062302712351083755 Validation loss 0.05722098425030708 Accuracy 0.8483750224113464\n",
      "Iteration 14360 Training loss 0.0585189089179039 Validation loss 0.05728987976908684 Accuracy 0.8481250405311584\n",
      "Iteration 14370 Training loss 0.05127481371164322 Validation loss 0.057260408997535706 Accuracy 0.8478750586509705\n",
      "Iteration 14380 Training loss 0.05602960288524628 Validation loss 0.05723007768392563 Accuracy 0.8480000495910645\n",
      "Iteration 14390 Training loss 0.058769892901182175 Validation loss 0.0572098046541214 Accuracy 0.8488750457763672\n",
      "Iteration 14400 Training loss 0.06183589622378349 Validation loss 0.05729062855243683 Accuracy 0.8480000495910645\n",
      "Iteration 14410 Training loss 0.05988899990916252 Validation loss 0.05725519731640816 Accuracy 0.8490000367164612\n",
      "Iteration 14420 Training loss 0.05552653595805168 Validation loss 0.05718143656849861 Accuracy 0.8488750457763672\n",
      "Iteration 14430 Training loss 0.05419856309890747 Validation loss 0.057253215461969376 Accuracy 0.8487500548362732\n",
      "Iteration 14440 Training loss 0.05328434705734253 Validation loss 0.05717219039797783 Accuracy 0.8483750224113464\n",
      "Iteration 14450 Training loss 0.05995851755142212 Validation loss 0.057222988456487656 Accuracy 0.8483750224113464\n",
      "Iteration 14460 Training loss 0.054388854652643204 Validation loss 0.05720486491918564 Accuracy 0.8486250638961792\n",
      "Iteration 14470 Training loss 0.053926363587379456 Validation loss 0.057205941528081894 Accuracy 0.8488750457763672\n",
      "Iteration 14480 Training loss 0.056084975600242615 Validation loss 0.057603150606155396 Accuracy 0.8471250534057617\n",
      "Iteration 14490 Training loss 0.06110748276114464 Validation loss 0.05712103471159935 Accuracy 0.8492500185966492\n",
      "Iteration 14500 Training loss 0.061098143458366394 Validation loss 0.05730608478188515 Accuracy 0.8476250171661377\n",
      "Iteration 14510 Training loss 0.05579468980431557 Validation loss 0.057311952114105225 Accuracy 0.8475000262260437\n",
      "Iteration 14520 Training loss 0.06054425984621048 Validation loss 0.0572122223675251 Accuracy 0.8471250534057617\n",
      "Iteration 14530 Training loss 0.05747617036104202 Validation loss 0.05716119706630707 Accuracy 0.8485000133514404\n",
      "Iteration 14540 Training loss 0.04813329502940178 Validation loss 0.057103481143713 Accuracy 0.8491250276565552\n",
      "Iteration 14550 Training loss 0.05864565819501877 Validation loss 0.05709254741668701 Accuracy 0.8496250510215759\n",
      "Iteration 14560 Training loss 0.05731956660747528 Validation loss 0.05709515139460564 Accuracy 0.8487500548362732\n",
      "Iteration 14570 Training loss 0.0529182106256485 Validation loss 0.057155970484018326 Accuracy 0.8487500548362732\n",
      "Iteration 14580 Training loss 0.07215237617492676 Validation loss 0.0574851855635643 Accuracy 0.846125066280365\n",
      "Iteration 14590 Training loss 0.051724448800086975 Validation loss 0.057079415768384933 Accuracy 0.8485000133514404\n",
      "Iteration 14600 Training loss 0.06268635392189026 Validation loss 0.05708475038409233 Accuracy 0.8490000367164612\n",
      "Iteration 14610 Training loss 0.05277689918875694 Validation loss 0.05711711570620537 Accuracy 0.8485000133514404\n",
      "Iteration 14620 Training loss 0.06419043242931366 Validation loss 0.05711021646857262 Accuracy 0.8488750457763672\n",
      "Iteration 14630 Training loss 0.057626981288194656 Validation loss 0.05708855018019676 Accuracy 0.8486250638961792\n",
      "Iteration 14640 Training loss 0.05750510096549988 Validation loss 0.05715049430727959 Accuracy 0.8486250638961792\n",
      "Iteration 14650 Training loss 0.05513615533709526 Validation loss 0.05707141011953354 Accuracy 0.8478750586509705\n",
      "Iteration 14660 Training loss 0.05371912941336632 Validation loss 0.057093482464551926 Accuracy 0.8487500548362732\n",
      "Iteration 14670 Training loss 0.05802494287490845 Validation loss 0.05704960227012634 Accuracy 0.8490000367164612\n",
      "Iteration 14680 Training loss 0.060278329998254776 Validation loss 0.05702170729637146 Accuracy 0.8493750691413879\n",
      "Iteration 14690 Training loss 0.05675909295678139 Validation loss 0.0570223331451416 Accuracy 0.8495000600814819\n",
      "Iteration 14700 Training loss 0.05289310961961746 Validation loss 0.05703219771385193 Accuracy 0.8493750691413879\n",
      "Iteration 14710 Training loss 0.06832891702651978 Validation loss 0.05732933431863785 Accuracy 0.8470000624656677\n",
      "Iteration 14720 Training loss 0.05792330577969551 Validation loss 0.0570165291428566 Accuracy 0.8492500185966492\n",
      "Iteration 14730 Training loss 0.055289898067712784 Validation loss 0.0569966584444046 Accuracy 0.8492500185966492\n",
      "Iteration 14740 Training loss 0.06733982264995575 Validation loss 0.057147614657878876 Accuracy 0.8473750352859497\n",
      "Iteration 14750 Training loss 0.057320959866046906 Validation loss 0.05700288340449333 Accuracy 0.8492500185966492\n",
      "Iteration 14760 Training loss 0.052733954042196274 Validation loss 0.05698586627840996 Accuracy 0.8497500419616699\n",
      "Iteration 14770 Training loss 0.06167672947049141 Validation loss 0.0570426769554615 Accuracy 0.8486250638961792\n",
      "Iteration 14780 Training loss 0.05182238295674324 Validation loss 0.05698193982243538 Accuracy 0.8500000238418579\n",
      "Iteration 14790 Training loss 0.04967811331152916 Validation loss 0.056963931769132614 Accuracy 0.8506250381469727\n",
      "Iteration 14800 Training loss 0.063311867415905 Validation loss 0.05697900429368019 Accuracy 0.8496250510215759\n",
      "Iteration 14810 Training loss 0.06335554271936417 Validation loss 0.05711217597126961 Accuracy 0.8483750224113464\n",
      "Iteration 14820 Training loss 0.06211361661553383 Validation loss 0.05693250149488449 Accuracy 0.8498750329017639\n",
      "Iteration 14830 Training loss 0.050543367862701416 Validation loss 0.05696115270256996 Accuracy 0.8490000367164612\n",
      "Iteration 14840 Training loss 0.05933045223355293 Validation loss 0.05744984373450279 Accuracy 0.846750020980835\n",
      "Iteration 14850 Training loss 0.056300610303878784 Validation loss 0.056929104030132294 Accuracy 0.8506250381469727\n",
      "Iteration 14860 Training loss 0.06128522753715515 Validation loss 0.05717269703745842 Accuracy 0.8473750352859497\n",
      "Iteration 14870 Training loss 0.049153245985507965 Validation loss 0.05693308636546135 Accuracy 0.8496250510215759\n",
      "Iteration 14880 Training loss 0.053041234612464905 Validation loss 0.05691125616431236 Accuracy 0.8495000600814819\n",
      "Iteration 14890 Training loss 0.055537812411785126 Validation loss 0.05700300261378288 Accuracy 0.8492500185966492\n",
      "Iteration 14900 Training loss 0.05838390067219734 Validation loss 0.05690276622772217 Accuracy 0.8497500419616699\n",
      "Iteration 14910 Training loss 0.05723021551966667 Validation loss 0.05728738009929657 Accuracy 0.846750020980835\n",
      "Iteration 14920 Training loss 0.06153169646859169 Validation loss 0.056886255741119385 Accuracy 0.8497500419616699\n",
      "Iteration 14930 Training loss 0.05599317327141762 Validation loss 0.05691349506378174 Accuracy 0.8492500185966492\n",
      "Iteration 14940 Training loss 0.05348212271928787 Validation loss 0.05694034695625305 Accuracy 0.8495000600814819\n",
      "Iteration 14950 Training loss 0.0503363311290741 Validation loss 0.05690451338887215 Accuracy 0.8501250147819519\n",
      "Iteration 14960 Training loss 0.056245267391204834 Validation loss 0.056878700852394104 Accuracy 0.8508750200271606\n",
      "Iteration 14970 Training loss 0.05565037950873375 Validation loss 0.056934088468551636 Accuracy 0.8491250276565552\n",
      "Iteration 14980 Training loss 0.05755399540066719 Validation loss 0.05727877467870712 Accuracy 0.846750020980835\n",
      "Iteration 14990 Training loss 0.06202219799160957 Validation loss 0.05710689723491669 Accuracy 0.8481250405311584\n",
      "Iteration 15000 Training loss 0.05810694023966789 Validation loss 0.056851595640182495 Accuracy 0.8508750200271606\n",
      "Iteration 15010 Training loss 0.06031918525695801 Validation loss 0.056830745190382004 Accuracy 0.8508750200271606\n",
      "Iteration 15020 Training loss 0.05086357891559601 Validation loss 0.05688222870230675 Accuracy 0.8497500419616699\n",
      "Iteration 15030 Training loss 0.05835637450218201 Validation loss 0.05682393163442612 Accuracy 0.8505000472068787\n",
      "Iteration 15040 Training loss 0.052328143268823624 Validation loss 0.05697821453213692 Accuracy 0.8480000495910645\n",
      "Iteration 15050 Training loss 0.06061772629618645 Validation loss 0.056833431124687195 Accuracy 0.8507500290870667\n",
      "Iteration 15060 Training loss 0.05317375063896179 Validation loss 0.056907761842012405 Accuracy 0.8480000495910645\n",
      "Iteration 15070 Training loss 0.0590413473546505 Validation loss 0.05678891763091087 Accuracy 0.8515000343322754\n",
      "Iteration 15080 Training loss 0.05919872224330902 Validation loss 0.056760042905807495 Accuracy 0.8506250381469727\n",
      "Iteration 15090 Training loss 0.05612370744347572 Validation loss 0.056767258793115616 Accuracy 0.8502500653266907\n",
      "Iteration 15100 Training loss 0.0526127964258194 Validation loss 0.05673038586974144 Accuracy 0.8521250486373901\n",
      "Iteration 15110 Training loss 0.053870439529418945 Validation loss 0.05678068473935127 Accuracy 0.8488750457763672\n",
      "Iteration 15120 Training loss 0.05432213842868805 Validation loss 0.056859422475099564 Accuracy 0.8481250405311584\n",
      "Iteration 15130 Training loss 0.04693056643009186 Validation loss 0.0567716620862484 Accuracy 0.8500000238418579\n",
      "Iteration 15140 Training loss 0.05235231667757034 Validation loss 0.05671520531177521 Accuracy 0.8507500290870667\n",
      "Iteration 15150 Training loss 0.05187758430838585 Validation loss 0.056696366518735886 Accuracy 0.8506250381469727\n",
      "Iteration 15160 Training loss 0.041409868746995926 Validation loss 0.05668853223323822 Accuracy 0.8515000343322754\n",
      "Iteration 15170 Training loss 0.05714564025402069 Validation loss 0.056829821318387985 Accuracy 0.8490000367164612\n",
      "Iteration 15180 Training loss 0.060369424521923065 Validation loss 0.05666612833738327 Accuracy 0.8511250615119934\n",
      "Iteration 15190 Training loss 0.06349515169858932 Validation loss 0.05665060877799988 Accuracy 0.8516250252723694\n",
      "Iteration 15200 Training loss 0.05913815274834633 Validation loss 0.05667455494403839 Accuracy 0.8510000109672546\n",
      "Iteration 15210 Training loss 0.0601937472820282 Validation loss 0.056724268943071365 Accuracy 0.8497500419616699\n",
      "Iteration 15220 Training loss 0.06162368506193161 Validation loss 0.05677545815706253 Accuracy 0.8488750457763672\n",
      "Iteration 15230 Training loss 0.053943932056427 Validation loss 0.05740689858794212 Accuracy 0.8453750610351562\n",
      "Iteration 15240 Training loss 0.05483634024858475 Validation loss 0.056777290999889374 Accuracy 0.8498750329017639\n",
      "Iteration 15250 Training loss 0.04669015482068062 Validation loss 0.05684804916381836 Accuracy 0.8498750329017639\n",
      "Iteration 15260 Training loss 0.051240697503089905 Validation loss 0.05665970966219902 Accuracy 0.8517500162124634\n",
      "Iteration 15270 Training loss 0.05833104997873306 Validation loss 0.056926179677248 Accuracy 0.8491250276565552\n",
      "Iteration 15280 Training loss 0.053576644510030746 Validation loss 0.0567016564309597 Accuracy 0.8513750433921814\n",
      "Iteration 15290 Training loss 0.057853784412145615 Validation loss 0.056662265211343765 Accuracy 0.8512500524520874\n",
      "Iteration 15300 Training loss 0.05413365364074707 Validation loss 0.056589219719171524 Accuracy 0.8527500629425049\n",
      "Iteration 15310 Training loss 0.05607239529490471 Validation loss 0.05666666850447655 Accuracy 0.8501250147819519\n",
      "Iteration 15320 Training loss 0.053084611892700195 Validation loss 0.05656467005610466 Accuracy 0.8531250357627869\n",
      "Iteration 15330 Training loss 0.05978722497820854 Validation loss 0.056559037417173386 Accuracy 0.8526250123977661\n",
      "Iteration 15340 Training loss 0.05342685431241989 Validation loss 0.05655167996883392 Accuracy 0.8516250252723694\n",
      "Iteration 15350 Training loss 0.04895061254501343 Validation loss 0.05658228322863579 Accuracy 0.8502500653266907\n",
      "Iteration 15360 Training loss 0.062310151755809784 Validation loss 0.056542035192251205 Accuracy 0.8525000214576721\n",
      "Iteration 15370 Training loss 0.05418505519628525 Validation loss 0.056546494364738464 Accuracy 0.8513750433921814\n",
      "Iteration 15380 Training loss 0.05269230902194977 Validation loss 0.05668451264500618 Accuracy 0.8498750329017639\n",
      "Iteration 15390 Training loss 0.05502992868423462 Validation loss 0.05655103549361229 Accuracy 0.8520000576972961\n",
      "Iteration 15400 Training loss 0.05178726464509964 Validation loss 0.05658290162682533 Accuracy 0.8505000472068787\n",
      "Iteration 15410 Training loss 0.06025455892086029 Validation loss 0.05669450759887695 Accuracy 0.8498750329017639\n",
      "Iteration 15420 Training loss 0.04412991553544998 Validation loss 0.05656185373663902 Accuracy 0.8507500290870667\n",
      "Iteration 15430 Training loss 0.051082003861665726 Validation loss 0.056551042944192886 Accuracy 0.8517500162124634\n",
      "Iteration 15440 Training loss 0.046261295676231384 Validation loss 0.05661172419786453 Accuracy 0.8502500653266907\n",
      "Iteration 15450 Training loss 0.05451742187142372 Validation loss 0.05650106444954872 Accuracy 0.8525000214576721\n",
      "Iteration 15460 Training loss 0.05111785605549812 Validation loss 0.0564817450940609 Accuracy 0.8525000214576721\n",
      "Iteration 15470 Training loss 0.049245938658714294 Validation loss 0.056491222232580185 Accuracy 0.8515000343322754\n",
      "Iteration 15480 Training loss 0.05674992501735687 Validation loss 0.056474871933460236 Accuracy 0.8518750667572021\n",
      "Iteration 15490 Training loss 0.04917558655142784 Validation loss 0.05648002400994301 Accuracy 0.8521250486373901\n",
      "Iteration 15500 Training loss 0.05702344700694084 Validation loss 0.0564572811126709 Accuracy 0.8517500162124634\n",
      "Iteration 15510 Training loss 0.05333733558654785 Validation loss 0.056511636823415756 Accuracy 0.8511250615119934\n",
      "Iteration 15520 Training loss 0.054824844002723694 Validation loss 0.056454189121723175 Accuracy 0.8511250615119934\n",
      "Iteration 15530 Training loss 0.06538869440555573 Validation loss 0.056690096855163574 Accuracy 0.8506250381469727\n",
      "Iteration 15540 Training loss 0.05747606232762337 Validation loss 0.05647418648004532 Accuracy 0.8510000109672546\n",
      "Iteration 15550 Training loss 0.05816446244716644 Validation loss 0.05650961771607399 Accuracy 0.8507500290870667\n",
      "Iteration 15560 Training loss 0.04932991415262222 Validation loss 0.056461550295352936 Accuracy 0.8511250615119934\n",
      "Iteration 15570 Training loss 0.04996080324053764 Validation loss 0.0565343014895916 Accuracy 0.8495000600814819\n",
      "Iteration 15580 Training loss 0.06303846091032028 Validation loss 0.05647847428917885 Accuracy 0.8508750200271606\n",
      "Iteration 15590 Training loss 0.055342793464660645 Validation loss 0.056725986301898956 Accuracy 0.8492500185966492\n",
      "Iteration 15600 Training loss 0.049353837966918945 Validation loss 0.05665719881653786 Accuracy 0.8498750329017639\n",
      "Iteration 15610 Training loss 0.052260883152484894 Validation loss 0.056844860315322876 Accuracy 0.8492500185966492\n",
      "Iteration 15620 Training loss 0.06465581059455872 Validation loss 0.0565510131418705 Accuracy 0.8507500290870667\n",
      "Iteration 15630 Training loss 0.05341588705778122 Validation loss 0.05641510337591171 Accuracy 0.8511250615119934\n",
      "Iteration 15640 Training loss 0.05663025751709938 Validation loss 0.05653312802314758 Accuracy 0.8496250510215759\n",
      "Iteration 15650 Training loss 0.05113779753446579 Validation loss 0.05638786405324936 Accuracy 0.8517500162124634\n",
      "Iteration 15660 Training loss 0.055856846272945404 Validation loss 0.05640324577689171 Accuracy 0.8516250252723694\n",
      "Iteration 15670 Training loss 0.05734649673104286 Validation loss 0.05638619139790535 Accuracy 0.8517500162124634\n",
      "Iteration 15680 Training loss 0.05181765556335449 Validation loss 0.05667915567755699 Accuracy 0.8487500548362732\n",
      "Iteration 15690 Training loss 0.05782795697450638 Validation loss 0.05636502429842949 Accuracy 0.8530000448226929\n",
      "Iteration 15700 Training loss 0.05300593376159668 Validation loss 0.056459102779626846 Accuracy 0.8501250147819519\n",
      "Iteration 15710 Training loss 0.05336688831448555 Validation loss 0.05637332424521446 Accuracy 0.8523750305175781\n",
      "Iteration 15720 Training loss 0.05802193656563759 Validation loss 0.05635682865977287 Accuracy 0.8526250123977661\n",
      "Iteration 15730 Training loss 0.05440584197640419 Validation loss 0.056373462080955505 Accuracy 0.8520000576972961\n",
      "Iteration 15740 Training loss 0.06202490255236626 Validation loss 0.056386128067970276 Accuracy 0.8516250252723694\n",
      "Iteration 15750 Training loss 0.0500907264649868 Validation loss 0.05681460723280907 Accuracy 0.8485000133514404\n",
      "Iteration 15760 Training loss 0.05679130554199219 Validation loss 0.0563628114759922 Accuracy 0.8531250357627869\n",
      "Iteration 15770 Training loss 0.04904981330037117 Validation loss 0.056362736970186234 Accuracy 0.8506250381469727\n",
      "Iteration 15780 Training loss 0.06291746348142624 Validation loss 0.0565650649368763 Accuracy 0.8491250276565552\n",
      "Iteration 15790 Training loss 0.05370044708251953 Validation loss 0.056369632482528687 Accuracy 0.8517500162124634\n",
      "Iteration 15800 Training loss 0.05276806280016899 Validation loss 0.056469082832336426 Accuracy 0.8497500419616699\n",
      "Iteration 15810 Training loss 0.05056662857532501 Validation loss 0.05632631853222847 Accuracy 0.8516250252723694\n",
      "Iteration 15820 Training loss 0.054551463574171066 Validation loss 0.056345537304878235 Accuracy 0.8525000214576721\n",
      "Iteration 15830 Training loss 0.0538809560239315 Validation loss 0.056318849325180054 Accuracy 0.8528750538825989\n",
      "Iteration 15840 Training loss 0.06359194964170456 Validation loss 0.056362733244895935 Accuracy 0.8508750200271606\n",
      "Iteration 15850 Training loss 0.05973859503865242 Validation loss 0.05630286782979965 Accuracy 0.8521250486373901\n",
      "Iteration 15860 Training loss 0.05441899970173836 Validation loss 0.05636262893676758 Accuracy 0.8502500653266907\n",
      "Iteration 15870 Training loss 0.057233672589063644 Validation loss 0.056668445467948914 Accuracy 0.8486250638961792\n",
      "Iteration 15880 Training loss 0.0542779341340065 Validation loss 0.056222591549158096 Accuracy 0.8537500500679016\n",
      "Iteration 15890 Training loss 0.05233708396553993 Validation loss 0.05626029521226883 Accuracy 0.8520000576972961\n",
      "Iteration 15900 Training loss 0.055114924907684326 Validation loss 0.056225892156362534 Accuracy 0.8533750176429749\n",
      "Iteration 15910 Training loss 0.051819004118442535 Validation loss 0.05627445504069328 Accuracy 0.8511250615119934\n",
      "Iteration 15920 Training loss 0.058569762855768204 Validation loss 0.056258391588926315 Accuracy 0.8516250252723694\n",
      "Iteration 15930 Training loss 0.046569664031267166 Validation loss 0.056211650371551514 Accuracy 0.8525000214576721\n",
      "Iteration 15940 Training loss 0.0461277961730957 Validation loss 0.05618049576878548 Accuracy 0.8541250228881836\n",
      "Iteration 15950 Training loss 0.05118929594755173 Validation loss 0.05622788146138191 Accuracy 0.8525000214576721\n",
      "Iteration 15960 Training loss 0.05602974817156792 Validation loss 0.056187935173511505 Accuracy 0.8537500500679016\n",
      "Iteration 15970 Training loss 0.05870857089757919 Validation loss 0.05618753656744957 Accuracy 0.8536250591278076\n",
      "Iteration 15980 Training loss 0.05375251546502113 Validation loss 0.05676316097378731 Accuracy 0.8485000133514404\n",
      "Iteration 15990 Training loss 0.04835887625813484 Validation loss 0.056250255554914474 Accuracy 0.8513750433921814\n",
      "Iteration 16000 Training loss 0.05214225500822067 Validation loss 0.056248780339956284 Accuracy 0.8511250615119934\n",
      "Iteration 16010 Training loss 0.04967173561453819 Validation loss 0.05614418536424637 Accuracy 0.8536250591278076\n",
      "Iteration 16020 Training loss 0.04946610704064369 Validation loss 0.056229785084724426 Accuracy 0.8518750667572021\n",
      "Iteration 16030 Training loss 0.060709305107593536 Validation loss 0.05645168945193291 Accuracy 0.8498750329017639\n",
      "Iteration 16040 Training loss 0.06762981414794922 Validation loss 0.056279972195625305 Accuracy 0.8515000343322754\n",
      "Iteration 16050 Training loss 0.052556008100509644 Validation loss 0.05615769326686859 Accuracy 0.8535000681877136\n",
      "Iteration 16060 Training loss 0.04889510199427605 Validation loss 0.0562509261071682 Accuracy 0.8515000343322754\n",
      "Iteration 16070 Training loss 0.052267007529735565 Validation loss 0.05618860945105553 Accuracy 0.8516250252723694\n",
      "Iteration 16080 Training loss 0.05045692250132561 Validation loss 0.05615353211760521 Accuracy 0.8531250357627869\n",
      "Iteration 16090 Training loss 0.054498203098773956 Validation loss 0.05620463192462921 Accuracy 0.8515000343322754\n",
      "Iteration 16100 Training loss 0.05116927996277809 Validation loss 0.05625661835074425 Accuracy 0.8510000109672546\n",
      "Iteration 16110 Training loss 0.053924258798360825 Validation loss 0.056208569556474686 Accuracy 0.8516250252723694\n",
      "Iteration 16120 Training loss 0.05811182036995888 Validation loss 0.05612267553806305 Accuracy 0.8517500162124634\n",
      "Iteration 16130 Training loss 0.05235745757818222 Validation loss 0.05614330992102623 Accuracy 0.8512500524520874\n",
      "Iteration 16140 Training loss 0.05300459638237953 Validation loss 0.05612841993570328 Accuracy 0.8517500162124634\n",
      "Iteration 16150 Training loss 0.05311107635498047 Validation loss 0.05676387995481491 Accuracy 0.8480000495910645\n",
      "Iteration 16160 Training loss 0.0539029985666275 Validation loss 0.05617543309926987 Accuracy 0.8511250615119934\n",
      "Iteration 16170 Training loss 0.05697911977767944 Validation loss 0.05606972053647041 Accuracy 0.8518750667572021\n",
      "Iteration 16180 Training loss 0.0546734519302845 Validation loss 0.056084971874952316 Accuracy 0.8531250357627869\n",
      "Iteration 16190 Training loss 0.06023469939827919 Validation loss 0.05604809895157814 Accuracy 0.8532500267028809\n",
      "Iteration 16200 Training loss 0.048308875411748886 Validation loss 0.05606187880039215 Accuracy 0.8528750538825989\n",
      "Iteration 16210 Training loss 0.05921591445803642 Validation loss 0.056109603494405746 Accuracy 0.8526250123977661\n",
      "Iteration 16220 Training loss 0.05407604202628136 Validation loss 0.05604218319058418 Accuracy 0.8536250591278076\n",
      "Iteration 16230 Training loss 0.04904822260141373 Validation loss 0.056098852306604385 Accuracy 0.8522500395774841\n",
      "Iteration 16240 Training loss 0.059308987110853195 Validation loss 0.05627761781215668 Accuracy 0.8511250615119934\n",
      "Iteration 16250 Training loss 0.05541706830263138 Validation loss 0.05605296418070793 Accuracy 0.8533750176429749\n",
      "Iteration 16260 Training loss 0.05298725515604019 Validation loss 0.05607366934418678 Accuracy 0.8530000448226929\n",
      "Iteration 16270 Training loss 0.06454894691705704 Validation loss 0.056245967745780945 Accuracy 0.8496250510215759\n",
      "Iteration 16280 Training loss 0.05294905602931976 Validation loss 0.05601000413298607 Accuracy 0.8536250591278076\n",
      "Iteration 16290 Training loss 0.048510607331991196 Validation loss 0.05599364638328552 Accuracy 0.8532500267028809\n",
      "Iteration 16300 Training loss 0.05219988524913788 Validation loss 0.056105442345142365 Accuracy 0.8513750433921814\n",
      "Iteration 16310 Training loss 0.050322797149419785 Validation loss 0.055991534143686295 Accuracy 0.8533750176429749\n",
      "Iteration 16320 Training loss 0.05146535113453865 Validation loss 0.05608534440398216 Accuracy 0.8517500162124634\n",
      "Iteration 16330 Training loss 0.05724409967660904 Validation loss 0.055976495146751404 Accuracy 0.8538750410079956\n",
      "Iteration 16340 Training loss 0.06004810333251953 Validation loss 0.05599427968263626 Accuracy 0.8537500500679016\n",
      "Iteration 16350 Training loss 0.04861989989876747 Validation loss 0.056033551692962646 Accuracy 0.8526250123977661\n",
      "Iteration 16360 Training loss 0.04927133396267891 Validation loss 0.055975157767534256 Accuracy 0.8527500629425049\n",
      "Iteration 16370 Training loss 0.058516938239336014 Validation loss 0.056041110306978226 Accuracy 0.8520000576972961\n",
      "Iteration 16380 Training loss 0.05143364891409874 Validation loss 0.05593985319137573 Accuracy 0.8530000448226929\n",
      "Iteration 16390 Training loss 0.054960183799266815 Validation loss 0.05604451149702072 Accuracy 0.8518750667572021\n",
      "Iteration 16400 Training loss 0.05965317413210869 Validation loss 0.056002285331487656 Accuracy 0.8525000214576721\n",
      "Iteration 16410 Training loss 0.04765785112977028 Validation loss 0.05593664199113846 Accuracy 0.8528750538825989\n",
      "Iteration 16420 Training loss 0.05257507041096687 Validation loss 0.05611632019281387 Accuracy 0.8510000109672546\n",
      "Iteration 16430 Training loss 0.050087690353393555 Validation loss 0.05610604211688042 Accuracy 0.8512500524520874\n",
      "Iteration 16440 Training loss 0.052957210689783096 Validation loss 0.05592940375208855 Accuracy 0.8535000681877136\n",
      "Iteration 16450 Training loss 0.055414337664842606 Validation loss 0.05595950409770012 Accuracy 0.8518750667572021\n",
      "Iteration 16460 Training loss 0.044933538883924484 Validation loss 0.0559152327477932 Accuracy 0.8530000448226929\n",
      "Iteration 16470 Training loss 0.05609128996729851 Validation loss 0.056078217923641205 Accuracy 0.8503750562667847\n",
      "Iteration 16480 Training loss 0.05765479430556297 Validation loss 0.0559120737016201 Accuracy 0.8545000553131104\n",
      "Iteration 16490 Training loss 0.052871912717819214 Validation loss 0.05603303015232086 Accuracy 0.8523750305175781\n",
      "Iteration 16500 Training loss 0.05248846486210823 Validation loss 0.05626391991972923 Accuracy 0.8518750667572021\n",
      "Iteration 16510 Training loss 0.05656302347779274 Validation loss 0.05644436925649643 Accuracy 0.8498750329017639\n",
      "Iteration 16520 Training loss 0.05727056413888931 Validation loss 0.05586889013648033 Accuracy 0.8538750410079956\n",
      "Iteration 16530 Training loss 0.04455440491437912 Validation loss 0.05637520179152489 Accuracy 0.8496250510215759\n",
      "Iteration 16540 Training loss 0.05882192403078079 Validation loss 0.05585088953375816 Accuracy 0.8537500500679016\n",
      "Iteration 16550 Training loss 0.05996266379952431 Validation loss 0.05586248263716698 Accuracy 0.8540000319480896\n",
      "Iteration 16560 Training loss 0.05277664214372635 Validation loss 0.055901192128658295 Accuracy 0.8527500629425049\n",
      "Iteration 16570 Training loss 0.0633918046951294 Validation loss 0.05580637603998184 Accuracy 0.8537500500679016\n",
      "Iteration 16580 Training loss 0.06572610139846802 Validation loss 0.05619525909423828 Accuracy 0.8508750200271606\n",
      "Iteration 16590 Training loss 0.059724483639001846 Validation loss 0.05579296499490738 Accuracy 0.8542500138282776\n",
      "Iteration 16600 Training loss 0.05332329124212265 Validation loss 0.05582153797149658 Accuracy 0.8538750410079956\n",
      "Iteration 16610 Training loss 0.05765558406710625 Validation loss 0.05581643059849739 Accuracy 0.8543750643730164\n",
      "Iteration 16620 Training loss 0.05110381916165352 Validation loss 0.05577964335680008 Accuracy 0.8545000553131104\n",
      "Iteration 16630 Training loss 0.05882362276315689 Validation loss 0.055766962468624115 Accuracy 0.8541250228881836\n",
      "Iteration 16640 Training loss 0.06977315992116928 Validation loss 0.05577797815203667 Accuracy 0.8541250228881836\n",
      "Iteration 16650 Training loss 0.05732119455933571 Validation loss 0.055768635123968124 Accuracy 0.8545000553131104\n",
      "Iteration 16660 Training loss 0.04978556931018829 Validation loss 0.05613341182470322 Accuracy 0.8500000238418579\n",
      "Iteration 16670 Training loss 0.04838040843605995 Validation loss 0.05583500117063522 Accuracy 0.8535000681877136\n",
      "Iteration 16680 Training loss 0.053500037640333176 Validation loss 0.05577469244599342 Accuracy 0.8541250228881836\n",
      "Iteration 16690 Training loss 0.05466608703136444 Validation loss 0.055762879550457 Accuracy 0.8541250228881836\n",
      "Iteration 16700 Training loss 0.049244172871112823 Validation loss 0.05610235035419464 Accuracy 0.8493750691413879\n",
      "Iteration 16710 Training loss 0.05547718703746796 Validation loss 0.05577385798096657 Accuracy 0.8543750643730164\n",
      "Iteration 16720 Training loss 0.05852997303009033 Validation loss 0.05571579560637474 Accuracy 0.8543750643730164\n",
      "Iteration 16730 Training loss 0.05342923104763031 Validation loss 0.05591011419892311 Accuracy 0.8521250486373901\n",
      "Iteration 16740 Training loss 0.048819731920957565 Validation loss 0.05611962825059891 Accuracy 0.8511250615119934\n",
      "Iteration 16750 Training loss 0.05548249930143356 Validation loss 0.055697694420814514 Accuracy 0.8545000553131104\n",
      "Iteration 16760 Training loss 0.06116986274719238 Validation loss 0.055740609765052795 Accuracy 0.8538750410079956\n",
      "Iteration 16770 Training loss 0.04961534962058067 Validation loss 0.05580548942089081 Accuracy 0.8532500267028809\n",
      "Iteration 16780 Training loss 0.054253432899713516 Validation loss 0.05571252852678299 Accuracy 0.8547500371932983\n",
      "Iteration 16790 Training loss 0.05661344900727272 Validation loss 0.05594749376177788 Accuracy 0.8517500162124634\n",
      "Iteration 16800 Training loss 0.048558760434389114 Validation loss 0.0557672455906868 Accuracy 0.8536250591278076\n",
      "Iteration 16810 Training loss 0.05342831835150719 Validation loss 0.055678024888038635 Accuracy 0.8541250228881836\n",
      "Iteration 16820 Training loss 0.057984787970781326 Validation loss 0.055801909416913986 Accuracy 0.8521250486373901\n",
      "Iteration 16830 Training loss 0.06380724161863327 Validation loss 0.0556814968585968 Accuracy 0.8533750176429749\n",
      "Iteration 16840 Training loss 0.057683832943439484 Validation loss 0.05565302446484566 Accuracy 0.8540000319480896\n",
      "Iteration 16850 Training loss 0.05353529006242752 Validation loss 0.05563036724925041 Accuracy 0.8542500138282776\n",
      "Iteration 16860 Training loss 0.05320069193840027 Validation loss 0.05570080131292343 Accuracy 0.8540000319480896\n",
      "Iteration 16870 Training loss 0.04632050544023514 Validation loss 0.05561821162700653 Accuracy 0.8545000553131104\n",
      "Iteration 16880 Training loss 0.054050855338573456 Validation loss 0.05568506941199303 Accuracy 0.8540000319480896\n",
      "Iteration 16890 Training loss 0.0459870770573616 Validation loss 0.05565890297293663 Accuracy 0.8538750410079956\n",
      "Iteration 16900 Training loss 0.05161772668361664 Validation loss 0.05567988008260727 Accuracy 0.8538750410079956\n",
      "Iteration 16910 Training loss 0.04992513731122017 Validation loss 0.05558416247367859 Accuracy 0.8543750643730164\n",
      "Iteration 16920 Training loss 0.05681465566158295 Validation loss 0.05558108538389206 Accuracy 0.8551250696182251\n",
      "Iteration 16930 Training loss 0.04334833845496178 Validation loss 0.055574771016836166 Accuracy 0.8550000190734863\n",
      "Iteration 16940 Training loss 0.05299248546361923 Validation loss 0.055632904171943665 Accuracy 0.8527500629425049\n",
      "Iteration 16950 Training loss 0.05733388662338257 Validation loss 0.055634837597608566 Accuracy 0.8536250591278076\n",
      "Iteration 16960 Training loss 0.053123630583286285 Validation loss 0.05560992285609245 Accuracy 0.8541250228881836\n",
      "Iteration 16970 Training loss 0.0604349784553051 Validation loss 0.05554790794849396 Accuracy 0.8541250228881836\n",
      "Iteration 16980 Training loss 0.052649516612291336 Validation loss 0.05553973838686943 Accuracy 0.8543750643730164\n",
      "Iteration 16990 Training loss 0.05876313894987106 Validation loss 0.05556720867753029 Accuracy 0.8543750643730164\n",
      "Iteration 17000 Training loss 0.050004616379737854 Validation loss 0.055714964866638184 Accuracy 0.8521250486373901\n",
      "Iteration 17010 Training loss 0.05426415428519249 Validation loss 0.05556991696357727 Accuracy 0.8550000190734863\n",
      "Iteration 17020 Training loss 0.04870321601629257 Validation loss 0.05561244860291481 Accuracy 0.8541250228881836\n",
      "Iteration 17030 Training loss 0.04743476212024689 Validation loss 0.05563966929912567 Accuracy 0.8533750176429749\n",
      "Iteration 17040 Training loss 0.055708497762680054 Validation loss 0.05556269735097885 Accuracy 0.8547500371932983\n",
      "Iteration 17050 Training loss 0.05423632636666298 Validation loss 0.055538762360811234 Accuracy 0.8543750643730164\n",
      "Iteration 17060 Training loss 0.0586700513958931 Validation loss 0.055832233279943466 Accuracy 0.8508750200271606\n",
      "Iteration 17070 Training loss 0.05988175794482231 Validation loss 0.05551595613360405 Accuracy 0.8545000553131104\n",
      "Iteration 17080 Training loss 0.0546836256980896 Validation loss 0.055538471788167953 Accuracy 0.8548750281333923\n",
      "Iteration 17090 Training loss 0.06315702944993973 Validation loss 0.05552350729703903 Accuracy 0.8552500605583191\n",
      "Iteration 17100 Training loss 0.053154680877923965 Validation loss 0.05549550801515579 Accuracy 0.8546250462532043\n",
      "Iteration 17110 Training loss 0.04984678328037262 Validation loss 0.055530160665512085 Accuracy 0.8543750643730164\n",
      "Iteration 17120 Training loss 0.05589773878455162 Validation loss 0.055948853492736816 Accuracy 0.8522500395774841\n",
      "Iteration 17130 Training loss 0.04946041852235794 Validation loss 0.055507052689790726 Accuracy 0.8538750410079956\n",
      "Iteration 17140 Training loss 0.0610772967338562 Validation loss 0.055482737720012665 Accuracy 0.8537500500679016\n",
      "Iteration 17150 Training loss 0.05515842139720917 Validation loss 0.05552499741315842 Accuracy 0.8538750410079956\n",
      "Iteration 17160 Training loss 0.05909609794616699 Validation loss 0.05552660673856735 Accuracy 0.8541250228881836\n",
      "Iteration 17170 Training loss 0.05933982878923416 Validation loss 0.055619075894355774 Accuracy 0.8525000214576721\n",
      "Iteration 17180 Training loss 0.05885617434978485 Validation loss 0.05548561364412308 Accuracy 0.8542500138282776\n",
      "Iteration 17190 Training loss 0.05297017842531204 Validation loss 0.05562926456332207 Accuracy 0.8527500629425049\n",
      "Iteration 17200 Training loss 0.04719769209623337 Validation loss 0.055574946105480194 Accuracy 0.8533750176429749\n",
      "Iteration 17210 Training loss 0.057447269558906555 Validation loss 0.05543695390224457 Accuracy 0.8543750643730164\n",
      "Iteration 17220 Training loss 0.05091268569231033 Validation loss 0.05544394254684448 Accuracy 0.8542500138282776\n",
      "Iteration 17230 Training loss 0.058737415820360184 Validation loss 0.05541650205850601 Accuracy 0.8548750281333923\n",
      "Iteration 17240 Training loss 0.05046224594116211 Validation loss 0.05541887879371643 Accuracy 0.8541250228881836\n",
      "Iteration 17250 Training loss 0.051602672785520554 Validation loss 0.055428918451070786 Accuracy 0.8542500138282776\n",
      "Iteration 17260 Training loss 0.05661085620522499 Validation loss 0.05541227385401726 Accuracy 0.8548750281333923\n",
      "Iteration 17270 Training loss 0.05553159490227699 Validation loss 0.05542288348078728 Accuracy 0.8545000553131104\n",
      "Iteration 17280 Training loss 0.05536237731575966 Validation loss 0.055403932929039 Accuracy 0.8550000190734863\n",
      "Iteration 17290 Training loss 0.054655950516462326 Validation loss 0.05538869649171829 Accuracy 0.8547500371932983\n",
      "Iteration 17300 Training loss 0.05640431120991707 Validation loss 0.05540198087692261 Accuracy 0.8547500371932983\n",
      "Iteration 17310 Training loss 0.06194661185145378 Validation loss 0.05550337955355644 Accuracy 0.8528750538825989\n",
      "Iteration 17320 Training loss 0.04821933060884476 Validation loss 0.05544283241033554 Accuracy 0.8527500629425049\n",
      "Iteration 17330 Training loss 0.05885454639792442 Validation loss 0.05544167757034302 Accuracy 0.8540000319480896\n",
      "Iteration 17340 Training loss 0.043565452098846436 Validation loss 0.055348701775074005 Accuracy 0.8555000424385071\n",
      "Iteration 17350 Training loss 0.0493382029235363 Validation loss 0.05538806691765785 Accuracy 0.8546250462532043\n",
      "Iteration 17360 Training loss 0.05683385580778122 Validation loss 0.055567704141139984 Accuracy 0.8531250357627869\n",
      "Iteration 17370 Training loss 0.04911818355321884 Validation loss 0.05542684346437454 Accuracy 0.8537500500679016\n",
      "Iteration 17380 Training loss 0.05330396071076393 Validation loss 0.05563434585928917 Accuracy 0.8516250252723694\n",
      "Iteration 17390 Training loss 0.05315537378191948 Validation loss 0.05536169558763504 Accuracy 0.8545000553131104\n",
      "Iteration 17400 Training loss 0.0488680936396122 Validation loss 0.05535626411437988 Accuracy 0.8540000319480896\n",
      "Iteration 17410 Training loss 0.05905551835894585 Validation loss 0.055528368800878525 Accuracy 0.8523750305175781\n",
      "Iteration 17420 Training loss 0.05809426307678223 Validation loss 0.0554562471807003 Accuracy 0.8527500629425049\n",
      "Iteration 17430 Training loss 0.05673269182443619 Validation loss 0.05569871887564659 Accuracy 0.8516250252723694\n",
      "Iteration 17440 Training loss 0.05592725798487663 Validation loss 0.05529032275080681 Accuracy 0.8547500371932983\n",
      "Iteration 17450 Training loss 0.06310810148715973 Validation loss 0.055286165326833725 Accuracy 0.8553750514984131\n",
      "Iteration 17460 Training loss 0.05884392559528351 Validation loss 0.055415909737348557 Accuracy 0.8540000319480896\n",
      "Iteration 17470 Training loss 0.0546085387468338 Validation loss 0.055292438715696335 Accuracy 0.8561250567436218\n",
      "Iteration 17480 Training loss 0.06155714392662048 Validation loss 0.05533652380108833 Accuracy 0.8551250696182251\n",
      "Iteration 17490 Training loss 0.05900534614920616 Validation loss 0.05562527850270271 Accuracy 0.8512500524520874\n",
      "Iteration 17500 Training loss 0.05169422924518585 Validation loss 0.05528324097394943 Accuracy 0.8557500243186951\n",
      "Iteration 17510 Training loss 0.04853259027004242 Validation loss 0.055322062224149704 Accuracy 0.8553750514984131\n",
      "Iteration 17520 Training loss 0.05472668632864952 Validation loss 0.055270250886678696 Accuracy 0.8550000190734863\n",
      "Iteration 17530 Training loss 0.05705254524946213 Validation loss 0.05526584014296532 Accuracy 0.8556250333786011\n",
      "Iteration 17540 Training loss 0.056716859340667725 Validation loss 0.05527632683515549 Accuracy 0.8562500476837158\n",
      "Iteration 17550 Training loss 0.04834609106183052 Validation loss 0.05532281845808029 Accuracy 0.8542500138282776\n",
      "Iteration 17560 Training loss 0.04474997892975807 Validation loss 0.05527693033218384 Accuracy 0.8547500371932983\n",
      "Iteration 17570 Training loss 0.053882844746112823 Validation loss 0.05537218973040581 Accuracy 0.8532500267028809\n",
      "Iteration 17580 Training loss 0.05229899659752846 Validation loss 0.05525275692343712 Accuracy 0.8545000553131104\n",
      "Iteration 17590 Training loss 0.05907301604747772 Validation loss 0.05526388809084892 Accuracy 0.8548750281333923\n",
      "Iteration 17600 Training loss 0.049014363437891006 Validation loss 0.05527959764003754 Accuracy 0.8537500500679016\n",
      "Iteration 17610 Training loss 0.05027416720986366 Validation loss 0.05542130768299103 Accuracy 0.8528750538825989\n",
      "Iteration 17620 Training loss 0.05257626250386238 Validation loss 0.055285822600126266 Accuracy 0.8536250591278076\n",
      "Iteration 17630 Training loss 0.056361522525548935 Validation loss 0.055329445749521255 Accuracy 0.8537500500679016\n",
      "Iteration 17640 Training loss 0.05968104302883148 Validation loss 0.05544590204954147 Accuracy 0.8523750305175781\n",
      "Iteration 17650 Training loss 0.052561063319444656 Validation loss 0.055199358612298965 Accuracy 0.8546250462532043\n",
      "Iteration 17660 Training loss 0.05366681516170502 Validation loss 0.05565924197435379 Accuracy 0.8517500162124634\n",
      "Iteration 17670 Training loss 0.05859231948852539 Validation loss 0.05529889091849327 Accuracy 0.8532500267028809\n",
      "Iteration 17680 Training loss 0.044172219932079315 Validation loss 0.05524969846010208 Accuracy 0.8550000190734863\n",
      "Iteration 17690 Training loss 0.04344560578465462 Validation loss 0.055220142006874084 Accuracy 0.8538750410079956\n",
      "Iteration 17700 Training loss 0.055657919496297836 Validation loss 0.05538361147046089 Accuracy 0.8532500267028809\n",
      "Iteration 17710 Training loss 0.05749332904815674 Validation loss 0.05518519878387451 Accuracy 0.8546250462532043\n",
      "Iteration 17720 Training loss 0.05822042375802994 Validation loss 0.05518811196088791 Accuracy 0.8542500138282776\n",
      "Iteration 17730 Training loss 0.05212012305855751 Validation loss 0.05523372069001198 Accuracy 0.8532500267028809\n",
      "Iteration 17740 Training loss 0.04928514361381531 Validation loss 0.055168747901916504 Accuracy 0.8546250462532043\n",
      "Iteration 17750 Training loss 0.04333190992474556 Validation loss 0.05518689379096031 Accuracy 0.8538750410079956\n",
      "Iteration 17760 Training loss 0.05559141933917999 Validation loss 0.05517854169011116 Accuracy 0.8546250462532043\n",
      "Iteration 17770 Training loss 0.05117868259549141 Validation loss 0.055239517241716385 Accuracy 0.8533750176429749\n",
      "Iteration 17780 Training loss 0.051032159477472305 Validation loss 0.05518997460603714 Accuracy 0.8540000319480896\n",
      "Iteration 17790 Training loss 0.05427783355116844 Validation loss 0.055134519934654236 Accuracy 0.8551250696182251\n",
      "Iteration 17800 Training loss 0.04345834255218506 Validation loss 0.0555880144238472 Accuracy 0.8516250252723694\n",
      "Iteration 17810 Training loss 0.05608726292848587 Validation loss 0.05513886734843254 Accuracy 0.8545000553131104\n",
      "Iteration 17820 Training loss 0.05259397253394127 Validation loss 0.0554409883916378 Accuracy 0.8520000576972961\n",
      "Iteration 17830 Training loss 0.0544821061193943 Validation loss 0.05514201521873474 Accuracy 0.8553750514984131\n",
      "Iteration 17840 Training loss 0.0556262731552124 Validation loss 0.055117372423410416 Accuracy 0.8560000658035278\n",
      "Iteration 17850 Training loss 0.05192266404628754 Validation loss 0.05546318739652634 Accuracy 0.8530000448226929\n",
      "Iteration 17860 Training loss 0.05139211565256119 Validation loss 0.05536308512091637 Accuracy 0.8531250357627869\n",
      "Iteration 17870 Training loss 0.05442716181278229 Validation loss 0.055195439606904984 Accuracy 0.8536250591278076\n",
      "Iteration 17880 Training loss 0.056876812130212784 Validation loss 0.05507863312959671 Accuracy 0.8552500605583191\n",
      "Iteration 17890 Training loss 0.054629307240247726 Validation loss 0.055094435811042786 Accuracy 0.8561250567436218\n",
      "Iteration 17900 Training loss 0.054916009306907654 Validation loss 0.05507584661245346 Accuracy 0.8560000658035278\n",
      "Iteration 17910 Training loss 0.05320396646857262 Validation loss 0.05531534552574158 Accuracy 0.8535000681877136\n",
      "Iteration 17920 Training loss 0.05619223043322563 Validation loss 0.0551149807870388 Accuracy 0.8551250696182251\n",
      "Iteration 17930 Training loss 0.04667448252439499 Validation loss 0.0550728477537632 Accuracy 0.8566250205039978\n",
      "Iteration 17940 Training loss 0.04916653409600258 Validation loss 0.05509514361619949 Accuracy 0.8555000424385071\n",
      "Iteration 17950 Training loss 0.05403786525130272 Validation loss 0.055074021220207214 Accuracy 0.8550000190734863\n",
      "Iteration 17960 Training loss 0.05276230722665787 Validation loss 0.055423084646463394 Accuracy 0.8535000681877136\n",
      "Iteration 17970 Training loss 0.053640563040971756 Validation loss 0.05510224401950836 Accuracy 0.8542500138282776\n",
      "Iteration 17980 Training loss 0.059545304626226425 Validation loss 0.05515160411596298 Accuracy 0.8530000448226929\n",
      "Iteration 17990 Training loss 0.04384644329547882 Validation loss 0.05506076291203499 Accuracy 0.8548750281333923\n",
      "Iteration 18000 Training loss 0.050521593540906906 Validation loss 0.05505188927054405 Accuracy 0.8553750514984131\n",
      "Iteration 18010 Training loss 0.055049359798431396 Validation loss 0.05501273646950722 Accuracy 0.8548750281333923\n",
      "Iteration 18020 Training loss 0.05064908042550087 Validation loss 0.055304765701293945 Accuracy 0.8517500162124634\n",
      "Iteration 18030 Training loss 0.0544544979929924 Validation loss 0.0550566092133522 Accuracy 0.8565000295639038\n",
      "Iteration 18040 Training loss 0.05126186087727547 Validation loss 0.05501072108745575 Accuracy 0.8562500476837158\n",
      "Iteration 18050 Training loss 0.04822699353098869 Validation loss 0.05499701201915741 Accuracy 0.8553750514984131\n",
      "Iteration 18060 Training loss 0.059048864990472794 Validation loss 0.05500102415680885 Accuracy 0.8563750386238098\n",
      "Iteration 18070 Training loss 0.05607728660106659 Validation loss 0.05500207468867302 Accuracy 0.8562500476837158\n",
      "Iteration 18080 Training loss 0.05529998615384102 Validation loss 0.055045682936906815 Accuracy 0.8552500605583191\n",
      "Iteration 18090 Training loss 0.05892748758196831 Validation loss 0.055220723152160645 Accuracy 0.8528750538825989\n",
      "Iteration 18100 Training loss 0.06300762295722961 Validation loss 0.054993655532598495 Accuracy 0.8548750281333923\n",
      "Iteration 18110 Training loss 0.05370130389928818 Validation loss 0.055187828838825226 Accuracy 0.8527500629425049\n",
      "Iteration 18120 Training loss 0.049563661217689514 Validation loss 0.05493674799799919 Accuracy 0.8565000295639038\n",
      "Iteration 18130 Training loss 0.061448171734809875 Validation loss 0.05496245250105858 Accuracy 0.8553750514984131\n",
      "Iteration 18140 Training loss 0.05786534771323204 Validation loss 0.055006857961416245 Accuracy 0.8553750514984131\n",
      "Iteration 18150 Training loss 0.05593622103333473 Validation loss 0.05502846837043762 Accuracy 0.8543750643730164\n",
      "Iteration 18160 Training loss 0.05207715928554535 Validation loss 0.05505741387605667 Accuracy 0.8538750410079956\n",
      "Iteration 18170 Training loss 0.05627094954252243 Validation loss 0.05504447966814041 Accuracy 0.8547500371932983\n",
      "Iteration 18180 Training loss 0.05262397602200508 Validation loss 0.05524508282542229 Accuracy 0.8527500629425049\n",
      "Iteration 18190 Training loss 0.04874080419540405 Validation loss 0.055119216442108154 Accuracy 0.8527500629425049\n",
      "Iteration 18200 Training loss 0.04816234111785889 Validation loss 0.054953716695308685 Accuracy 0.8556250333786011\n",
      "Iteration 18210 Training loss 0.05527739226818085 Validation loss 0.05489934980869293 Accuracy 0.8561250567436218\n",
      "Iteration 18220 Training loss 0.04994581267237663 Validation loss 0.05499432235956192 Accuracy 0.8552500605583191\n",
      "Iteration 18230 Training loss 0.04898778721690178 Validation loss 0.0549527071416378 Accuracy 0.8556250333786011\n",
      "Iteration 18240 Training loss 0.053558871150016785 Validation loss 0.05503333359956741 Accuracy 0.8552500605583191\n",
      "Iteration 18250 Training loss 0.046290624886751175 Validation loss 0.05498133599758148 Accuracy 0.8551250696182251\n",
      "Iteration 18260 Training loss 0.050139836966991425 Validation loss 0.05492567643523216 Accuracy 0.8566250205039978\n",
      "Iteration 18270 Training loss 0.048213012516498566 Validation loss 0.05488638952374458 Accuracy 0.8562500476837158\n",
      "Iteration 18280 Training loss 0.05493983253836632 Validation loss 0.05505108833312988 Accuracy 0.8546250462532043\n",
      "Iteration 18290 Training loss 0.059656210243701935 Validation loss 0.05491382256150246 Accuracy 0.8561250567436218\n",
      "Iteration 18300 Training loss 0.049990177154541016 Validation loss 0.05490851029753685 Accuracy 0.8557500243186951\n",
      "Iteration 18310 Training loss 0.05821283534169197 Validation loss 0.05487661063671112 Accuracy 0.8552500605583191\n",
      "Iteration 18320 Training loss 0.05862832069396973 Validation loss 0.05488194525241852 Accuracy 0.8558750152587891\n",
      "Iteration 18330 Training loss 0.0584695041179657 Validation loss 0.05505330488085747 Accuracy 0.8555000424385071\n",
      "Iteration 18340 Training loss 0.04941609129309654 Validation loss 0.05487874150276184 Accuracy 0.8560000658035278\n",
      "Iteration 18350 Training loss 0.05007654055953026 Validation loss 0.054867006838321686 Accuracy 0.8563750386238098\n",
      "Iteration 18360 Training loss 0.0522853285074234 Validation loss 0.05510291829705238 Accuracy 0.8532500267028809\n",
      "Iteration 18370 Training loss 0.05834057554602623 Validation loss 0.05485772341489792 Accuracy 0.8557500243186951\n",
      "Iteration 18380 Training loss 0.056689534336328506 Validation loss 0.05494685471057892 Accuracy 0.8545000553131104\n",
      "Iteration 18390 Training loss 0.04824502021074295 Validation loss 0.054863475263118744 Accuracy 0.8551250696182251\n",
      "Iteration 18400 Training loss 0.04812314361333847 Validation loss 0.054807670414447784 Accuracy 0.8553750514984131\n",
      "Iteration 18410 Training loss 0.05205968767404556 Validation loss 0.054861605167388916 Accuracy 0.8550000190734863\n",
      "Iteration 18420 Training loss 0.04975137487053871 Validation loss 0.05479168891906738 Accuracy 0.8560000658035278\n",
      "Iteration 18430 Training loss 0.057046279311180115 Validation loss 0.054805878549814224 Accuracy 0.8556250333786011\n",
      "Iteration 18440 Training loss 0.04958946257829666 Validation loss 0.054809603840112686 Accuracy 0.8556250333786011\n",
      "Iteration 18450 Training loss 0.049733731895685196 Validation loss 0.054947905242443085 Accuracy 0.8543750643730164\n",
      "Iteration 18460 Training loss 0.05920984223484993 Validation loss 0.05484114959836006 Accuracy 0.8552500605583191\n",
      "Iteration 18470 Training loss 0.05183232203125954 Validation loss 0.05487750470638275 Accuracy 0.8545000553131104\n",
      "Iteration 18480 Training loss 0.04890424758195877 Validation loss 0.05475720390677452 Accuracy 0.8557500243186951\n",
      "Iteration 18490 Training loss 0.045667391270399094 Validation loss 0.054824039340019226 Accuracy 0.8557500243186951\n",
      "Iteration 18500 Training loss 0.051700275391340256 Validation loss 0.05476411059498787 Accuracy 0.8568750619888306\n",
      "Iteration 18510 Training loss 0.04551494121551514 Validation loss 0.05480343848466873 Accuracy 0.8553750514984131\n",
      "Iteration 18520 Training loss 0.05559851974248886 Validation loss 0.05496366322040558 Accuracy 0.8537500500679016\n",
      "Iteration 18530 Training loss 0.054968204349279404 Validation loss 0.05490263178944588 Accuracy 0.8540000319480896\n",
      "Iteration 18540 Training loss 0.06090758368372917 Validation loss 0.05475245416164398 Accuracy 0.8561250567436218\n",
      "Iteration 18550 Training loss 0.044504277408123016 Validation loss 0.05473149195313454 Accuracy 0.8568750619888306\n",
      "Iteration 18560 Training loss 0.04925492778420448 Validation loss 0.05484161525964737 Accuracy 0.8543750643730164\n",
      "Iteration 18570 Training loss 0.04943355545401573 Validation loss 0.05505792424082756 Accuracy 0.8531250357627869\n",
      "Iteration 18580 Training loss 0.053860366344451904 Validation loss 0.05481322109699249 Accuracy 0.8557500243186951\n",
      "Iteration 18590 Training loss 0.05826018378138542 Validation loss 0.05491786077618599 Accuracy 0.8550000190734863\n",
      "Iteration 18600 Training loss 0.051808807998895645 Validation loss 0.05507450923323631 Accuracy 0.8525000214576721\n",
      "Iteration 18610 Training loss 0.05416105315089226 Validation loss 0.05476754158735275 Accuracy 0.8555000424385071\n",
      "Iteration 18620 Training loss 0.05490938946604729 Validation loss 0.05470113083720207 Accuracy 0.8567500114440918\n",
      "Iteration 18630 Training loss 0.05322163552045822 Validation loss 0.054891061037778854 Accuracy 0.8548750281333923\n",
      "Iteration 18640 Training loss 0.053941503167152405 Validation loss 0.054769936949014664 Accuracy 0.8560000658035278\n",
      "Iteration 18650 Training loss 0.0517287440598011 Validation loss 0.05486728623509407 Accuracy 0.8552500605583191\n",
      "Iteration 18660 Training loss 0.05529789626598358 Validation loss 0.05473433807492256 Accuracy 0.8558750152587891\n",
      "Iteration 18670 Training loss 0.0597846582531929 Validation loss 0.054939426481723785 Accuracy 0.8541250228881836\n",
      "Iteration 18680 Training loss 0.061526086181402206 Validation loss 0.05473502725362778 Accuracy 0.8557500243186951\n",
      "Iteration 18690 Training loss 0.05786263942718506 Validation loss 0.05473601445555687 Accuracy 0.8560000658035278\n",
      "Iteration 18700 Training loss 0.04896588996052742 Validation loss 0.054842036217451096 Accuracy 0.8541250228881836\n",
      "Iteration 18710 Training loss 0.0517784021794796 Validation loss 0.05483699217438698 Accuracy 0.8547500371932983\n",
      "Iteration 18720 Training loss 0.05478915944695473 Validation loss 0.054701387882232666 Accuracy 0.8557500243186951\n",
      "Iteration 18730 Training loss 0.05787905678153038 Validation loss 0.05476414039731026 Accuracy 0.8552500605583191\n",
      "Iteration 18740 Training loss 0.05902685597538948 Validation loss 0.054673925042152405 Accuracy 0.8561250567436218\n",
      "Iteration 18750 Training loss 0.05208395794034004 Validation loss 0.05479533597826958 Accuracy 0.8540000319480896\n",
      "Iteration 18760 Training loss 0.05452654883265495 Validation loss 0.05462133139371872 Accuracy 0.8565000295639038\n",
      "Iteration 18770 Training loss 0.04707113653421402 Validation loss 0.05488544702529907 Accuracy 0.8550000190734863\n",
      "Iteration 18780 Training loss 0.060587406158447266 Validation loss 0.05484180152416229 Accuracy 0.8535000681877136\n",
      "Iteration 18790 Training loss 0.053681179881095886 Validation loss 0.05503229796886444 Accuracy 0.8536250591278076\n",
      "Iteration 18800 Training loss 0.05304818972945213 Validation loss 0.05482775717973709 Accuracy 0.8542500138282776\n",
      "Iteration 18810 Training loss 0.06042138487100601 Validation loss 0.054662469774484634 Accuracy 0.8548750281333923\n",
      "Iteration 18820 Training loss 0.05816160514950752 Validation loss 0.054679617285728455 Accuracy 0.8567500114440918\n",
      "Iteration 18830 Training loss 0.05756736546754837 Validation loss 0.054716698825359344 Accuracy 0.8555000424385071\n",
      "Iteration 18840 Training loss 0.04921702295541763 Validation loss 0.054609060287475586 Accuracy 0.8568750619888306\n",
      "Iteration 18850 Training loss 0.057270314544439316 Validation loss 0.054632242769002914 Accuracy 0.8557500243186951\n",
      "Iteration 18860 Training loss 0.05169958993792534 Validation loss 0.054570186883211136 Accuracy 0.8565000295639038\n",
      "Iteration 18870 Training loss 0.059892747551202774 Validation loss 0.05477435886859894 Accuracy 0.8545000553131104\n",
      "Iteration 18880 Training loss 0.05768144130706787 Validation loss 0.05457903444766998 Accuracy 0.8568750619888306\n",
      "Iteration 18890 Training loss 0.053981442004442215 Validation loss 0.0546734444797039 Accuracy 0.8561250567436218\n",
      "Iteration 18900 Training loss 0.052595410495996475 Validation loss 0.0546366386115551 Accuracy 0.8562500476837158\n",
      "Iteration 18910 Training loss 0.054713085293769836 Validation loss 0.054682303220033646 Accuracy 0.8556250333786011\n",
      "Iteration 18920 Training loss 0.05828644707798958 Validation loss 0.0546812079846859 Accuracy 0.8550000190734863\n",
      "Iteration 18930 Training loss 0.058212537318468094 Validation loss 0.05453268811106682 Accuracy 0.8567500114440918\n",
      "Iteration 18940 Training loss 0.05823889747262001 Validation loss 0.05457136034965515 Accuracy 0.8568750619888306\n",
      "Iteration 18950 Training loss 0.055859245359897614 Validation loss 0.05456965044140816 Accuracy 0.8578750491142273\n",
      "Iteration 18960 Training loss 0.04729669541120529 Validation loss 0.0546746701002121 Accuracy 0.8552500605583191\n",
      "Iteration 18970 Training loss 0.04184355586767197 Validation loss 0.0544770210981369 Accuracy 0.8567500114440918\n",
      "Iteration 18980 Training loss 0.0483151413500309 Validation loss 0.054672449827194214 Accuracy 0.8550000190734863\n",
      "Iteration 18990 Training loss 0.05084070935845375 Validation loss 0.054485950618982315 Accuracy 0.8557500243186951\n",
      "Iteration 19000 Training loss 0.0563732385635376 Validation loss 0.054505605250597 Accuracy 0.8562500476837158\n",
      "Iteration 19010 Training loss 0.047773294150829315 Validation loss 0.054625254124403 Accuracy 0.8545000553131104\n",
      "Iteration 19020 Training loss 0.05157534405589104 Validation loss 0.05463738739490509 Accuracy 0.8552500605583191\n",
      "Iteration 19030 Training loss 0.05661192908883095 Validation loss 0.054769985377788544 Accuracy 0.8541250228881836\n",
      "Iteration 19040 Training loss 0.056086309254169464 Validation loss 0.054496634751558304 Accuracy 0.8561250567436218\n",
      "Iteration 19050 Training loss 0.056236766278743744 Validation loss 0.05449041351675987 Accuracy 0.8562500476837158\n",
      "Iteration 19060 Training loss 0.053160276263952255 Validation loss 0.05464041605591774 Accuracy 0.8546250462532043\n",
      "Iteration 19070 Training loss 0.049692943692207336 Validation loss 0.0549168586730957 Accuracy 0.8541250228881836\n",
      "Iteration 19080 Training loss 0.051702830940485 Validation loss 0.05449112877249718 Accuracy 0.8566250205039978\n",
      "Iteration 19090 Training loss 0.05706257373094559 Validation loss 0.054471757262945175 Accuracy 0.8560000658035278\n",
      "Iteration 19100 Training loss 0.05286167562007904 Validation loss 0.05453099310398102 Accuracy 0.8562500476837158\n",
      "Iteration 19110 Training loss 0.05914707109332085 Validation loss 0.054526813328266144 Accuracy 0.8560000658035278\n",
      "Iteration 19120 Training loss 0.051931727677583694 Validation loss 0.054735612124204636 Accuracy 0.8551250696182251\n",
      "Iteration 19130 Training loss 0.05221056938171387 Validation loss 0.0544237457215786 Accuracy 0.8567500114440918\n",
      "Iteration 19140 Training loss 0.05009615421295166 Validation loss 0.05445289611816406 Accuracy 0.8560000658035278\n",
      "Iteration 19150 Training loss 0.049857307225465775 Validation loss 0.05455996096134186 Accuracy 0.8547500371932983\n",
      "Iteration 19160 Training loss 0.046197984367609024 Validation loss 0.054809484630823135 Accuracy 0.8532500267028809\n",
      "Iteration 19170 Training loss 0.05731409043073654 Validation loss 0.05478730425238609 Accuracy 0.8530000448226929\n",
      "Iteration 19180 Training loss 0.057270441204309464 Validation loss 0.054426100105047226 Accuracy 0.8556250333786011\n",
      "Iteration 19190 Training loss 0.05099670588970184 Validation loss 0.054649654775857925 Accuracy 0.8546250462532043\n",
      "Iteration 19200 Training loss 0.06743662804365158 Validation loss 0.0545133501291275 Accuracy 0.8553750514984131\n",
      "Iteration 19210 Training loss 0.04774361476302147 Validation loss 0.05448780208826065 Accuracy 0.8567500114440918\n",
      "Iteration 19220 Training loss 0.053985510021448135 Validation loss 0.054466333240270615 Accuracy 0.8568750619888306\n",
      "Iteration 19230 Training loss 0.05022154375910759 Validation loss 0.05443186312913895 Accuracy 0.8570000529289246\n",
      "Iteration 19240 Training loss 0.052974846214056015 Validation loss 0.05437532439827919 Accuracy 0.8572500348091125\n",
      "Iteration 19250 Training loss 0.04942991957068443 Validation loss 0.05438198894262314 Accuracy 0.8573750257492065\n",
      "Iteration 19260 Training loss 0.06007295474410057 Validation loss 0.05445929989218712 Accuracy 0.8566250205039978\n",
      "Iteration 19270 Training loss 0.05227508395910263 Validation loss 0.05435977876186371 Accuracy 0.8570000529289246\n",
      "Iteration 19280 Training loss 0.058922383934259415 Validation loss 0.05446314811706543 Accuracy 0.8567500114440918\n",
      "Iteration 19290 Training loss 0.056075457483530045 Validation loss 0.054345957934856415 Accuracy 0.8572500348091125\n",
      "Iteration 19300 Training loss 0.056116268038749695 Validation loss 0.05437067151069641 Accuracy 0.8573750257492065\n",
      "Iteration 19310 Training loss 0.052400216460227966 Validation loss 0.05442899093031883 Accuracy 0.8567500114440918\n",
      "Iteration 19320 Training loss 0.049418170005083084 Validation loss 0.054331324994564056 Accuracy 0.8575000166893005\n",
      "Iteration 19330 Training loss 0.044299058616161346 Validation loss 0.05439671501517296 Accuracy 0.8563750386238098\n",
      "Iteration 19340 Training loss 0.04330980032682419 Validation loss 0.054490476846694946 Accuracy 0.8556250333786011\n",
      "Iteration 19350 Training loss 0.058342669159173965 Validation loss 0.05459293723106384 Accuracy 0.8547500371932983\n",
      "Iteration 19360 Training loss 0.046898867934942245 Validation loss 0.05465436726808548 Accuracy 0.8545000553131104\n",
      "Iteration 19370 Training loss 0.03841526806354523 Validation loss 0.05434991791844368 Accuracy 0.8567500114440918\n",
      "Iteration 19380 Training loss 0.0436277873814106 Validation loss 0.054340943694114685 Accuracy 0.8570000529289246\n",
      "Iteration 19390 Training loss 0.055647529661655426 Validation loss 0.05436577647924423 Accuracy 0.8566250205039978\n",
      "Iteration 19400 Training loss 0.05379649996757507 Validation loss 0.054773952811956406 Accuracy 0.8546250462532043\n",
      "Iteration 19410 Training loss 0.05254342034459114 Validation loss 0.05436041206121445 Accuracy 0.8563750386238098\n",
      "Iteration 19420 Training loss 0.05339035391807556 Validation loss 0.0543401725590229 Accuracy 0.8567500114440918\n",
      "Iteration 19430 Training loss 0.056697528809309006 Validation loss 0.054369356483221054 Accuracy 0.8561250567436218\n",
      "Iteration 19440 Training loss 0.05226842686533928 Validation loss 0.054438550025224686 Accuracy 0.8557500243186951\n",
      "Iteration 19450 Training loss 0.04486515745520592 Validation loss 0.0542890727519989 Accuracy 0.8575000166893005\n",
      "Iteration 19460 Training loss 0.047869663685560226 Validation loss 0.05427732318639755 Accuracy 0.8572500348091125\n",
      "Iteration 19470 Training loss 0.048322685062885284 Validation loss 0.05444673076272011 Accuracy 0.8553750514984131\n",
      "Iteration 19480 Training loss 0.057420339435338974 Validation loss 0.054302629083395004 Accuracy 0.8561250567436218\n",
      "Iteration 19490 Training loss 0.0529097355902195 Validation loss 0.054317768663167953 Accuracy 0.8556250333786011\n",
      "Iteration 19500 Training loss 0.06139400228857994 Validation loss 0.054290495812892914 Accuracy 0.8577500581741333\n",
      "Iteration 19510 Training loss 0.054375533014535904 Validation loss 0.05445919930934906 Accuracy 0.8550000190734863\n",
      "Iteration 19520 Training loss 0.04905920475721359 Validation loss 0.0542825423181057 Accuracy 0.8572500348091125\n",
      "Iteration 19530 Training loss 0.0483882911503315 Validation loss 0.05431396886706352 Accuracy 0.8572500348091125\n",
      "Iteration 19540 Training loss 0.047950249165296555 Validation loss 0.054298579692840576 Accuracy 0.8575000166893005\n",
      "Iteration 19550 Training loss 0.04745396599173546 Validation loss 0.05425693094730377 Accuracy 0.8566250205039978\n",
      "Iteration 19560 Training loss 0.06359567493200302 Validation loss 0.054491449147462845 Accuracy 0.8553750514984131\n",
      "Iteration 19570 Training loss 0.04754982516169548 Validation loss 0.054285887628793716 Accuracy 0.8575000166893005\n",
      "Iteration 19580 Training loss 0.05361134931445122 Validation loss 0.05424584448337555 Accuracy 0.8573750257492065\n",
      "Iteration 19590 Training loss 0.04999544098973274 Validation loss 0.05426627770066261 Accuracy 0.8568750619888306\n",
      "Iteration 19600 Training loss 0.05317180976271629 Validation loss 0.054203473031520844 Accuracy 0.8573750257492065\n",
      "Iteration 19610 Training loss 0.05217524990439415 Validation loss 0.05419398844242096 Accuracy 0.8581250309944153\n",
      "Iteration 19620 Training loss 0.04933202639222145 Validation loss 0.05423697829246521 Accuracy 0.8573750257492065\n",
      "Iteration 19630 Training loss 0.047302570194005966 Validation loss 0.054179735481739044 Accuracy 0.8577500581741333\n",
      "Iteration 19640 Training loss 0.04748404026031494 Validation loss 0.05416273698210716 Accuracy 0.8580000400543213\n",
      "Iteration 19650 Training loss 0.051360998302698135 Validation loss 0.05417228862643242 Accuracy 0.8577500581741333\n",
      "Iteration 19660 Training loss 0.05064013972878456 Validation loss 0.054495371878147125 Accuracy 0.8558750152587891\n",
      "Iteration 19670 Training loss 0.04934873431921005 Validation loss 0.05415603145956993 Accuracy 0.8578750491142273\n",
      "Iteration 19680 Training loss 0.06130647659301758 Validation loss 0.054368432611227036 Accuracy 0.8573750257492065\n",
      "Iteration 19690 Training loss 0.051703937351703644 Validation loss 0.05426590144634247 Accuracy 0.8576250672340393\n",
      "Iteration 19700 Training loss 0.045259226113557816 Validation loss 0.05424075573682785 Accuracy 0.8571250438690186\n",
      "Iteration 19710 Training loss 0.053101442754268646 Validation loss 0.05413595959544182 Accuracy 0.8578750491142273\n",
      "Iteration 19720 Training loss 0.05209442600607872 Validation loss 0.05415085703134537 Accuracy 0.85875004529953\n",
      "Iteration 19730 Training loss 0.04626357927918434 Validation loss 0.05414096266031265 Accuracy 0.8581250309944153\n",
      "Iteration 19740 Training loss 0.04978908598423004 Validation loss 0.0544426366686821 Accuracy 0.8556250333786011\n",
      "Iteration 19750 Training loss 0.05085405334830284 Validation loss 0.05413505807518959 Accuracy 0.8581250309944153\n",
      "Iteration 19760 Training loss 0.04888853803277016 Validation loss 0.05415746569633484 Accuracy 0.8577500581741333\n",
      "Iteration 19770 Training loss 0.05283641070127487 Validation loss 0.05415375903248787 Accuracy 0.8573750257492065\n",
      "Iteration 19780 Training loss 0.051332369446754456 Validation loss 0.05406970530748367 Accuracy 0.8581250309944153\n",
      "Iteration 19790 Training loss 0.054656729102134705 Validation loss 0.05407901853322983 Accuracy 0.8573750257492065\n",
      "Iteration 19800 Training loss 0.0532221682369709 Validation loss 0.05408426746726036 Accuracy 0.8567500114440918\n",
      "Iteration 19810 Training loss 0.04901132360100746 Validation loss 0.05407130718231201 Accuracy 0.8575000166893005\n",
      "Iteration 19820 Training loss 0.04957609996199608 Validation loss 0.054109301418066025 Accuracy 0.8581250309944153\n",
      "Iteration 19830 Training loss 0.05600878596305847 Validation loss 0.05409032851457596 Accuracy 0.8573750257492065\n",
      "Iteration 19840 Training loss 0.0583939291536808 Validation loss 0.05447882041335106 Accuracy 0.8536250591278076\n",
      "Iteration 19850 Training loss 0.050648316740989685 Validation loss 0.05409420654177666 Accuracy 0.8580000400543213\n",
      "Iteration 19860 Training loss 0.0573929026722908 Validation loss 0.05411948636174202 Accuracy 0.8568750619888306\n",
      "Iteration 19870 Training loss 0.054061003029346466 Validation loss 0.054255999624729156 Accuracy 0.8570000529289246\n",
      "Iteration 19880 Training loss 0.057475823909044266 Validation loss 0.05409570410847664 Accuracy 0.8568750619888306\n",
      "Iteration 19890 Training loss 0.05550113692879677 Validation loss 0.05407819524407387 Accuracy 0.8581250309944153\n",
      "Iteration 19900 Training loss 0.04950545355677605 Validation loss 0.05448456108570099 Accuracy 0.8546250462532043\n",
      "Iteration 19910 Training loss 0.05030517280101776 Validation loss 0.05404911935329437 Accuracy 0.8576250672340393\n",
      "Iteration 19920 Training loss 0.057717278599739075 Validation loss 0.05402781069278717 Accuracy 0.8580000400543213\n",
      "Iteration 19930 Training loss 0.045665282756090164 Validation loss 0.054262202233076096 Accuracy 0.8563750386238098\n",
      "Iteration 19940 Training loss 0.05552133917808533 Validation loss 0.05406477674841881 Accuracy 0.8581250309944153\n",
      "Iteration 19950 Training loss 0.04345669597387314 Validation loss 0.054018307477235794 Accuracy 0.858500063419342\n",
      "Iteration 19960 Training loss 0.045964181423187256 Validation loss 0.05399806797504425 Accuracy 0.8582500219345093\n",
      "Iteration 19970 Training loss 0.04828186333179474 Validation loss 0.05408253148198128 Accuracy 0.8565000295639038\n",
      "Iteration 19980 Training loss 0.05015743151307106 Validation loss 0.054259639233350754 Accuracy 0.8552500605583191\n",
      "Iteration 19990 Training loss 0.05693656951189041 Validation loss 0.05407308042049408 Accuracy 0.8576250672340393\n",
      "Iteration 20000 Training loss 0.05164346098899841 Validation loss 0.05397561937570572 Accuracy 0.8582500219345093\n",
      "Iteration 20010 Training loss 0.05185523256659508 Validation loss 0.05417748540639877 Accuracy 0.8568750619888306\n",
      "Iteration 20020 Training loss 0.053598612546920776 Validation loss 0.053987544029951096 Accuracy 0.8578750491142273\n",
      "Iteration 20030 Training loss 0.04961394518613815 Validation loss 0.05397039279341698 Accuracy 0.8582500219345093\n",
      "Iteration 20040 Training loss 0.054861828684806824 Validation loss 0.05399404466152191 Accuracy 0.8581250309944153\n",
      "Iteration 20050 Training loss 0.048021964728832245 Validation loss 0.05398216098546982 Accuracy 0.858500063419342\n",
      "Iteration 20060 Training loss 0.04685652628540993 Validation loss 0.05402374640107155 Accuracy 0.8572500348091125\n",
      "Iteration 20070 Training loss 0.0525972917675972 Validation loss 0.053948841989040375 Accuracy 0.858500063419342\n",
      "Iteration 20080 Training loss 0.06323235481977463 Validation loss 0.053981270641088486 Accuracy 0.8580000400543213\n",
      "Iteration 20090 Training loss 0.053911320865154266 Validation loss 0.05402752012014389 Accuracy 0.8572500348091125\n",
      "Iteration 20100 Training loss 0.053672030568122864 Validation loss 0.05395039543509483 Accuracy 0.858500063419342\n",
      "Iteration 20110 Training loss 0.04859739542007446 Validation loss 0.0539398230612278 Accuracy 0.8572500348091125\n",
      "Iteration 20120 Training loss 0.0465359166264534 Validation loss 0.053995102643966675 Accuracy 0.8581250309944153\n",
      "Iteration 20130 Training loss 0.05102990195155144 Validation loss 0.05431443825364113 Accuracy 0.8548750281333923\n",
      "Iteration 20140 Training loss 0.04599588364362717 Validation loss 0.05395227670669556 Accuracy 0.8581250309944153\n",
      "Iteration 20150 Training loss 0.04741948843002319 Validation loss 0.05404900014400482 Accuracy 0.8563750386238098\n",
      "Iteration 20160 Training loss 0.05711040273308754 Validation loss 0.05402654781937599 Accuracy 0.8561250567436218\n",
      "Iteration 20170 Training loss 0.04272209107875824 Validation loss 0.05391153320670128 Accuracy 0.858625054359436\n",
      "Iteration 20180 Training loss 0.049279697239398956 Validation loss 0.053904514759778976 Accuracy 0.8570000529289246\n",
      "Iteration 20190 Training loss 0.048966262489557266 Validation loss 0.053915392607450485 Accuracy 0.8577500581741333\n",
      "Iteration 20200 Training loss 0.04486888274550438 Validation loss 0.0539047047495842 Accuracy 0.858625054359436\n",
      "Iteration 20210 Training loss 0.053894560784101486 Validation loss 0.05390576273202896 Accuracy 0.8566250205039978\n",
      "Iteration 20220 Training loss 0.0563676543533802 Validation loss 0.05434153974056244 Accuracy 0.8550000190734863\n",
      "Iteration 20230 Training loss 0.056925367563962936 Validation loss 0.053858134895563126 Accuracy 0.8581250309944153\n",
      "Iteration 20240 Training loss 0.06367369741201401 Validation loss 0.05393484607338905 Accuracy 0.8573750257492065\n",
      "Iteration 20250 Training loss 0.05802473425865173 Validation loss 0.05394510179758072 Accuracy 0.8567500114440918\n",
      "Iteration 20260 Training loss 0.05742422863841057 Validation loss 0.05388180539011955 Accuracy 0.8578750491142273\n",
      "Iteration 20270 Training loss 0.04813528060913086 Validation loss 0.05384819954633713 Accuracy 0.858875036239624\n",
      "Iteration 20280 Training loss 0.048189472407102585 Validation loss 0.05392962321639061 Accuracy 0.8568750619888306\n",
      "Iteration 20290 Training loss 0.061909016221761703 Validation loss 0.05386214703321457 Accuracy 0.858625054359436\n",
      "Iteration 20300 Training loss 0.054939381778240204 Validation loss 0.05387125536799431 Accuracy 0.858500063419342\n",
      "Iteration 20310 Training loss 0.055327873677015305 Validation loss 0.053861912339925766 Accuracy 0.8592500686645508\n",
      "Iteration 20320 Training loss 0.05335156247019768 Validation loss 0.05389193817973137 Accuracy 0.858875036239624\n",
      "Iteration 20330 Training loss 0.0469239316880703 Validation loss 0.05382908135652542 Accuracy 0.8573750257492065\n",
      "Iteration 20340 Training loss 0.05189572274684906 Validation loss 0.053952399641275406 Accuracy 0.8571250438690186\n",
      "Iteration 20350 Training loss 0.05800895392894745 Validation loss 0.05389268696308136 Accuracy 0.8592500686645508\n",
      "Iteration 20360 Training loss 0.05696900188922882 Validation loss 0.05381399765610695 Accuracy 0.8583750128746033\n",
      "Iteration 20370 Training loss 0.06063395366072655 Validation loss 0.05465216934680939 Accuracy 0.8536250591278076\n",
      "Iteration 20380 Training loss 0.05715445429086685 Validation loss 0.05396804213523865 Accuracy 0.8571250438690186\n",
      "Iteration 20390 Training loss 0.049607548862695694 Validation loss 0.05389031395316124 Accuracy 0.8571250438690186\n",
      "Iteration 20400 Training loss 0.05308283865451813 Validation loss 0.05452630668878555 Accuracy 0.8538750410079956\n",
      "Iteration 20410 Training loss 0.05141519755125046 Validation loss 0.05378314480185509 Accuracy 0.858625054359436\n",
      "Iteration 20420 Training loss 0.04913834482431412 Validation loss 0.053794343024492264 Accuracy 0.8582500219345093\n",
      "Iteration 20430 Training loss 0.04272777587175369 Validation loss 0.053913991898298264 Accuracy 0.8570000529289246\n",
      "Iteration 20440 Training loss 0.06159557029604912 Validation loss 0.053804680705070496 Accuracy 0.8578750491142273\n",
      "Iteration 20450 Training loss 0.04742254316806793 Validation loss 0.05406095087528229 Accuracy 0.8571250438690186\n",
      "Iteration 20460 Training loss 0.04887806624174118 Validation loss 0.054005034267902374 Accuracy 0.8562500476837158\n",
      "Iteration 20470 Training loss 0.05640678107738495 Validation loss 0.05375105142593384 Accuracy 0.858625054359436\n",
      "Iteration 20480 Training loss 0.05479482188820839 Validation loss 0.0538129061460495 Accuracy 0.859000027179718\n",
      "Iteration 20490 Training loss 0.04310058429837227 Validation loss 0.053880371153354645 Accuracy 0.8576250672340393\n",
      "Iteration 20500 Training loss 0.057194989174604416 Validation loss 0.05378960818052292 Accuracy 0.859125018119812\n",
      "Iteration 20510 Training loss 0.05129304900765419 Validation loss 0.053761519491672516 Accuracy 0.858625054359436\n",
      "Iteration 20520 Training loss 0.05337200686335564 Validation loss 0.05379069596529007 Accuracy 0.85875004529953\n",
      "Iteration 20530 Training loss 0.050942562520504 Validation loss 0.05377158522605896 Accuracy 0.858500063419342\n",
      "Iteration 20540 Training loss 0.05243108794093132 Validation loss 0.05376821756362915 Accuracy 0.8583750128746033\n",
      "Iteration 20550 Training loss 0.04987340420484543 Validation loss 0.05380591005086899 Accuracy 0.8583750128746033\n",
      "Iteration 20560 Training loss 0.0444827638566494 Validation loss 0.05409083515405655 Accuracy 0.8570000529289246\n",
      "Iteration 20570 Training loss 0.038934968411922455 Validation loss 0.05381312966346741 Accuracy 0.8583750128746033\n",
      "Iteration 20580 Training loss 0.05619592219591141 Validation loss 0.05380594730377197 Accuracy 0.8578750491142273\n",
      "Iteration 20590 Training loss 0.048189934343099594 Validation loss 0.05386151745915413 Accuracy 0.8573750257492065\n",
      "Iteration 20600 Training loss 0.05218419432640076 Validation loss 0.05400773882865906 Accuracy 0.8567500114440918\n",
      "Iteration 20610 Training loss 0.05133690685033798 Validation loss 0.05375547334551811 Accuracy 0.859000027179718\n",
      "Iteration 20620 Training loss 0.05514305830001831 Validation loss 0.05378160998225212 Accuracy 0.8575000166893005\n",
      "Iteration 20630 Training loss 0.0582914724946022 Validation loss 0.05374503880739212 Accuracy 0.8576250672340393\n",
      "Iteration 20640 Training loss 0.05684052035212517 Validation loss 0.0544218048453331 Accuracy 0.8552500605583191\n",
      "Iteration 20650 Training loss 0.057617537677288055 Validation loss 0.05374032258987427 Accuracy 0.8580000400543213\n",
      "Iteration 20660 Training loss 0.054508913308382034 Validation loss 0.05369485914707184 Accuracy 0.859125018119812\n",
      "Iteration 20670 Training loss 0.05543901026248932 Validation loss 0.05473397299647331 Accuracy 0.8531250357627869\n",
      "Iteration 20680 Training loss 0.05376853048801422 Validation loss 0.05369612202048302 Accuracy 0.8582500219345093\n",
      "Iteration 20690 Training loss 0.052021510899066925 Validation loss 0.05368395894765854 Accuracy 0.858875036239624\n",
      "Iteration 20700 Training loss 0.04690876230597496 Validation loss 0.05368823558092117 Accuracy 0.8582500219345093\n",
      "Iteration 20710 Training loss 0.05370094254612923 Validation loss 0.05381013825535774 Accuracy 0.8573750257492065\n",
      "Iteration 20720 Training loss 0.044662922620773315 Validation loss 0.05368475615978241 Accuracy 0.859000027179718\n",
      "Iteration 20730 Training loss 0.047355689108371735 Validation loss 0.053722865879535675 Accuracy 0.858875036239624\n",
      "Iteration 20740 Training loss 0.05146351829171181 Validation loss 0.05372887849807739 Accuracy 0.858625054359436\n",
      "Iteration 20750 Training loss 0.047857679426670074 Validation loss 0.05365080386400223 Accuracy 0.8581250309944153\n",
      "Iteration 20760 Training loss 0.06214861571788788 Validation loss 0.05382130295038223 Accuracy 0.8573750257492065\n",
      "Iteration 20770 Training loss 0.044715337455272675 Validation loss 0.05370645225048065 Accuracy 0.859000027179718\n",
      "Iteration 20780 Training loss 0.05902905389666557 Validation loss 0.053958795964717865 Accuracy 0.8571250438690186\n",
      "Iteration 20790 Training loss 0.055815935134887695 Validation loss 0.05362075939774513 Accuracy 0.858875036239624\n",
      "Iteration 20800 Training loss 0.05465809628367424 Validation loss 0.05367779731750488 Accuracy 0.8597500324249268\n",
      "Iteration 20810 Training loss 0.057610392570495605 Validation loss 0.053625378757715225 Accuracy 0.859125018119812\n",
      "Iteration 20820 Training loss 0.04858306795358658 Validation loss 0.05381776764988899 Accuracy 0.8566250205039978\n",
      "Iteration 20830 Training loss 0.050527047365903854 Validation loss 0.053711749613285065 Accuracy 0.8593750596046448\n",
      "Iteration 20840 Training loss 0.06259440630674362 Validation loss 0.05359845981001854 Accuracy 0.8582500219345093\n",
      "Iteration 20850 Training loss 0.048280972987413406 Validation loss 0.0537281408905983 Accuracy 0.859125018119812\n",
      "Iteration 20860 Training loss 0.05579298734664917 Validation loss 0.0537673719227314 Accuracy 0.8575000166893005\n",
      "Iteration 20870 Training loss 0.0557851605117321 Validation loss 0.05358516797423363 Accuracy 0.858625054359436\n",
      "Iteration 20880 Training loss 0.056907568126916885 Validation loss 0.05366438627243042 Accuracy 0.8578750491142273\n",
      "Iteration 20890 Training loss 0.057723693549633026 Validation loss 0.053549814969301224 Accuracy 0.8580000400543213\n",
      "Iteration 20900 Training loss 0.05230192840099335 Validation loss 0.053557030856609344 Accuracy 0.8583750128746033\n",
      "Iteration 20910 Training loss 0.05867207422852516 Validation loss 0.0536997988820076 Accuracy 0.8575000166893005\n",
      "Iteration 20920 Training loss 0.049653418362140656 Validation loss 0.05361229181289673 Accuracy 0.8582500219345093\n",
      "Iteration 20930 Training loss 0.05503334477543831 Validation loss 0.05364016443490982 Accuracy 0.85875004529953\n",
      "Iteration 20940 Training loss 0.04922928661108017 Validation loss 0.05360826477408409 Accuracy 0.8593750596046448\n",
      "Iteration 20950 Training loss 0.0503140352666378 Validation loss 0.053604550659656525 Accuracy 0.8595000505447388\n",
      "Iteration 20960 Training loss 0.04781248793005943 Validation loss 0.053584735840559006 Accuracy 0.859000027179718\n",
      "Iteration 20970 Training loss 0.04752287641167641 Validation loss 0.053612083196640015 Accuracy 0.8593750596046448\n",
      "Iteration 20980 Training loss 0.04722914472222328 Validation loss 0.05368812382221222 Accuracy 0.8570000529289246\n",
      "Iteration 20990 Training loss 0.059252459555864334 Validation loss 0.053586672991514206 Accuracy 0.8575000166893005\n",
      "Iteration 21000 Training loss 0.054420918226242065 Validation loss 0.05382074415683746 Accuracy 0.8568750619888306\n",
      "Iteration 21010 Training loss 0.056444015353918076 Validation loss 0.05355796962976456 Accuracy 0.859125018119812\n",
      "Iteration 21020 Training loss 0.05105224624276161 Validation loss 0.05351852625608444 Accuracy 0.858500063419342\n",
      "Iteration 21030 Training loss 0.05104542151093483 Validation loss 0.05356930196285248 Accuracy 0.8580000400543213\n",
      "Iteration 21040 Training loss 0.048859670758247375 Validation loss 0.053755469620227814 Accuracy 0.8577500581741333\n",
      "Iteration 21050 Training loss 0.046366527676582336 Validation loss 0.053512390702962875 Accuracy 0.858875036239624\n",
      "Iteration 21060 Training loss 0.04545803740620613 Validation loss 0.05348354950547218 Accuracy 0.8597500324249268\n",
      "Iteration 21070 Training loss 0.056556835770606995 Validation loss 0.053551677614450455 Accuracy 0.85875004529953\n",
      "Iteration 21080 Training loss 0.055655911564826965 Validation loss 0.053494058549404144 Accuracy 0.8592500686645508\n",
      "Iteration 21090 Training loss 0.05033795163035393 Validation loss 0.05350124463438988 Accuracy 0.85875004529953\n",
      "Iteration 21100 Training loss 0.04951351881027222 Validation loss 0.05346473678946495 Accuracy 0.85875004529953\n",
      "Iteration 21110 Training loss 0.057467687875032425 Validation loss 0.05406339839100838 Accuracy 0.8566250205039978\n",
      "Iteration 21120 Training loss 0.05588438734412193 Validation loss 0.053465183824300766 Accuracy 0.8592500686645508\n",
      "Iteration 21130 Training loss 0.04846625775098801 Validation loss 0.053463712334632874 Accuracy 0.859125018119812\n",
      "Iteration 21140 Training loss 0.056375425308942795 Validation loss 0.053486548364162445 Accuracy 0.858500063419342\n",
      "Iteration 21150 Training loss 0.05158571898937225 Validation loss 0.053452666848897934 Accuracy 0.859000027179718\n",
      "Iteration 21160 Training loss 0.05974173545837402 Validation loss 0.05352174863219261 Accuracy 0.8577500581741333\n",
      "Iteration 21170 Training loss 0.04685003682971001 Validation loss 0.05359938368201256 Accuracy 0.8582500219345093\n",
      "Iteration 21180 Training loss 0.056147802621126175 Validation loss 0.05345718562602997 Accuracy 0.859125018119812\n",
      "Iteration 21190 Training loss 0.04970858618617058 Validation loss 0.05356426537036896 Accuracy 0.859000027179718\n",
      "Iteration 21200 Training loss 0.04732143133878708 Validation loss 0.05346682295203209 Accuracy 0.858500063419342\n",
      "Iteration 21210 Training loss 0.05548717826604843 Validation loss 0.053487833589315414 Accuracy 0.858500063419342\n",
      "Iteration 21220 Training loss 0.04206039384007454 Validation loss 0.05346555635333061 Accuracy 0.8582500219345093\n",
      "Iteration 21230 Training loss 0.05916152894496918 Validation loss 0.05343521758913994 Accuracy 0.8582500219345093\n",
      "Iteration 21240 Training loss 0.05352579429745674 Validation loss 0.05342301353812218 Accuracy 0.85875004529953\n",
      "Iteration 21250 Training loss 0.04840188845992088 Validation loss 0.05352574586868286 Accuracy 0.8580000400543213\n",
      "Iteration 21260 Training loss 0.04706597328186035 Validation loss 0.05340193957090378 Accuracy 0.8593750596046448\n",
      "Iteration 21270 Training loss 0.04691663011908531 Validation loss 0.053384799510240555 Accuracy 0.8595000505447388\n",
      "Iteration 21280 Training loss 0.05088723078370094 Validation loss 0.05372919887304306 Accuracy 0.8576250672340393\n",
      "Iteration 21290 Training loss 0.04741918295621872 Validation loss 0.05364632233977318 Accuracy 0.8570000529289246\n",
      "Iteration 21300 Training loss 0.051831815391778946 Validation loss 0.05388595908880234 Accuracy 0.8563750386238098\n",
      "Iteration 21310 Training loss 0.05633044242858887 Validation loss 0.053697094321250916 Accuracy 0.8557500243186951\n",
      "Iteration 21320 Training loss 0.047796983271837234 Validation loss 0.053433775901794434 Accuracy 0.859000027179718\n",
      "Iteration 21330 Training loss 0.05349399521946907 Validation loss 0.05337819084525108 Accuracy 0.8598750233650208\n",
      "Iteration 21340 Training loss 0.04508739337325096 Validation loss 0.053458116948604584 Accuracy 0.8595000505447388\n",
      "Iteration 21350 Training loss 0.05042115971446037 Validation loss 0.05337351933121681 Accuracy 0.8593750596046448\n",
      "Iteration 21360 Training loss 0.0507059171795845 Validation loss 0.053368281573057175 Accuracy 0.8593750596046448\n",
      "Iteration 21370 Training loss 0.04325602948665619 Validation loss 0.05336013436317444 Accuracy 0.8592500686645508\n",
      "Iteration 21380 Training loss 0.05781973525881767 Validation loss 0.05334587022662163 Accuracy 0.8595000505447388\n",
      "Iteration 21390 Training loss 0.056900832802057266 Validation loss 0.053682129830121994 Accuracy 0.8566250205039978\n",
      "Iteration 21400 Training loss 0.05755668878555298 Validation loss 0.053511034697294235 Accuracy 0.8578750491142273\n",
      "Iteration 21410 Training loss 0.045849330723285675 Validation loss 0.05332987755537033 Accuracy 0.859000027179718\n",
      "Iteration 21420 Training loss 0.050998058170080185 Validation loss 0.053312018513679504 Accuracy 0.8593750596046448\n",
      "Iteration 21430 Training loss 0.04886867478489876 Validation loss 0.05329067260026932 Accuracy 0.8595000505447388\n",
      "Iteration 21440 Training loss 0.05985800549387932 Validation loss 0.053549036383628845 Accuracy 0.858625054359436\n",
      "Iteration 21450 Training loss 0.04782227426767349 Validation loss 0.05333966761827469 Accuracy 0.8598750233650208\n",
      "Iteration 21460 Training loss 0.048660438507795334 Validation loss 0.0532960407435894 Accuracy 0.8592500686645508\n",
      "Iteration 21470 Training loss 0.05663979426026344 Validation loss 0.054173074662685394 Accuracy 0.8550000190734863\n",
      "Iteration 21480 Training loss 0.05243920162320137 Validation loss 0.05423488840460777 Accuracy 0.8551250696182251\n",
      "Iteration 21490 Training loss 0.04935064539313316 Validation loss 0.053306058049201965 Accuracy 0.85875004529953\n",
      "Iteration 21500 Training loss 0.05105867236852646 Validation loss 0.05326222628355026 Accuracy 0.8601250648498535\n",
      "Iteration 21510 Training loss 0.052149638533592224 Validation loss 0.05329854041337967 Accuracy 0.8583750128746033\n",
      "Iteration 21520 Training loss 0.050668392330408096 Validation loss 0.053322773426771164 Accuracy 0.85875004529953\n",
      "Iteration 21530 Training loss 0.04770524799823761 Validation loss 0.05361150577664375 Accuracy 0.8580000400543213\n",
      "Iteration 21540 Training loss 0.04475288838148117 Validation loss 0.053312044590711594 Accuracy 0.85875004529953\n",
      "Iteration 21550 Training loss 0.046080321073532104 Validation loss 0.05330482870340347 Accuracy 0.8603750467300415\n",
      "Iteration 21560 Training loss 0.045812368392944336 Validation loss 0.053262658417224884 Accuracy 0.8601250648498535\n",
      "Iteration 21570 Training loss 0.05386470630764961 Validation loss 0.05342046916484833 Accuracy 0.858500063419342\n",
      "Iteration 21580 Training loss 0.05301174893975258 Validation loss 0.05325159803032875 Accuracy 0.8596250414848328\n",
      "Iteration 21590 Training loss 0.05463719367980957 Validation loss 0.05338563397526741 Accuracy 0.8600000143051147\n",
      "Iteration 21600 Training loss 0.06134415417909622 Validation loss 0.05336443707346916 Accuracy 0.858625054359436\n",
      "Iteration 21610 Training loss 0.057146959006786346 Validation loss 0.05353296920657158 Accuracy 0.85875004529953\n",
      "Iteration 21620 Training loss 0.061402447521686554 Validation loss 0.053252387791872025 Accuracy 0.8593750596046448\n",
      "Iteration 21630 Training loss 0.04836975410580635 Validation loss 0.05326392501592636 Accuracy 0.8600000143051147\n",
      "Iteration 21640 Training loss 0.05368147790431976 Validation loss 0.05330204218626022 Accuracy 0.8603750467300415\n",
      "Iteration 21650 Training loss 0.053460147231817245 Validation loss 0.05366964265704155 Accuracy 0.8583750128746033\n",
      "Iteration 21660 Training loss 0.05089174583554268 Validation loss 0.05322747677564621 Accuracy 0.8601250648498535\n",
      "Iteration 21670 Training loss 0.05471653863787651 Validation loss 0.05321536958217621 Accuracy 0.8600000143051147\n",
      "Iteration 21680 Training loss 0.05737147107720375 Validation loss 0.05325063318014145 Accuracy 0.8598750233650208\n",
      "Iteration 21690 Training loss 0.05358068272471428 Validation loss 0.05375731736421585 Accuracy 0.8563750386238098\n",
      "Iteration 21700 Training loss 0.05461834371089935 Validation loss 0.05316399037837982 Accuracy 0.8601250648498535\n",
      "Iteration 21710 Training loss 0.05016982927918434 Validation loss 0.05327465385198593 Accuracy 0.858875036239624\n",
      "Iteration 21720 Training loss 0.05226578563451767 Validation loss 0.05316707119345665 Accuracy 0.8598750233650208\n",
      "Iteration 21730 Training loss 0.04759930446743965 Validation loss 0.05352533608675003 Accuracy 0.8566250205039978\n",
      "Iteration 21740 Training loss 0.05890313908457756 Validation loss 0.05350661650300026 Accuracy 0.8572500348091125\n",
      "Iteration 21750 Training loss 0.055015504360198975 Validation loss 0.053258027881383896 Accuracy 0.8593750596046448\n",
      "Iteration 21760 Training loss 0.05324818938970566 Validation loss 0.05326303467154503 Accuracy 0.858875036239624\n",
      "Iteration 21770 Training loss 0.04949101433157921 Validation loss 0.053148139268159866 Accuracy 0.8600000143051147\n",
      "Iteration 21780 Training loss 0.056144509464502335 Validation loss 0.05313696712255478 Accuracy 0.8606250286102295\n",
      "Iteration 21790 Training loss 0.06038070470094681 Validation loss 0.053171563893556595 Accuracy 0.8606250286102295\n",
      "Iteration 21800 Training loss 0.06040486320853233 Validation loss 0.053252264857292175 Accuracy 0.8593750596046448\n",
      "Iteration 21810 Training loss 0.0558706559240818 Validation loss 0.05318109691143036 Accuracy 0.8592500686645508\n",
      "Iteration 21820 Training loss 0.052336301654577255 Validation loss 0.05311973765492439 Accuracy 0.8600000143051147\n",
      "Iteration 21830 Training loss 0.04589937999844551 Validation loss 0.05313177779316902 Accuracy 0.8592500686645508\n",
      "Iteration 21840 Training loss 0.039751384407281876 Validation loss 0.05351141467690468 Accuracy 0.8573750257492065\n",
      "Iteration 21850 Training loss 0.04610937461256981 Validation loss 0.053174939006567 Accuracy 0.8596250414848328\n",
      "Iteration 21860 Training loss 0.05021487921476364 Validation loss 0.05314399302005768 Accuracy 0.8603750467300415\n",
      "Iteration 21870 Training loss 0.049846142530441284 Validation loss 0.053504932671785355 Accuracy 0.8578750491142273\n",
      "Iteration 21880 Training loss 0.05573035031557083 Validation loss 0.05313658341765404 Accuracy 0.8596250414848328\n",
      "Iteration 21890 Training loss 0.04690557345747948 Validation loss 0.053195346146821976 Accuracy 0.8593750596046448\n",
      "Iteration 21900 Training loss 0.0475921630859375 Validation loss 0.053080156445503235 Accuracy 0.8603750467300415\n",
      "Iteration 21910 Training loss 0.0532422810792923 Validation loss 0.053096704185009 Accuracy 0.8600000143051147\n",
      "Iteration 21920 Training loss 0.05590376630425453 Validation loss 0.05315500125288963 Accuracy 0.8592500686645508\n",
      "Iteration 21930 Training loss 0.05397367477416992 Validation loss 0.05311516672372818 Accuracy 0.8605000376701355\n",
      "Iteration 21940 Training loss 0.052074041217565536 Validation loss 0.05310547351837158 Accuracy 0.859125018119812\n",
      "Iteration 21950 Training loss 0.05594315752387047 Validation loss 0.05314192548394203 Accuracy 0.8593750596046448\n",
      "Iteration 21960 Training loss 0.050743844360113144 Validation loss 0.053075242787599564 Accuracy 0.8602500557899475\n",
      "Iteration 21970 Training loss 0.04825008660554886 Validation loss 0.05305136367678642 Accuracy 0.8598750233650208\n",
      "Iteration 21980 Training loss 0.05750851333141327 Validation loss 0.05303794518113136 Accuracy 0.8593750596046448\n",
      "Iteration 21990 Training loss 0.05585165694355965 Validation loss 0.05304410681128502 Accuracy 0.8598750233650208\n",
      "Iteration 22000 Training loss 0.04214686155319214 Validation loss 0.05337578058242798 Accuracy 0.8577500581741333\n",
      "Iteration 22010 Training loss 0.047440607100725174 Validation loss 0.053366221487522125 Accuracy 0.8571250438690186\n",
      "Iteration 22020 Training loss 0.05562898516654968 Validation loss 0.05307154729962349 Accuracy 0.859000027179718\n",
      "Iteration 22030 Training loss 0.047315988689661026 Validation loss 0.05310169234871864 Accuracy 0.859125018119812\n",
      "Iteration 22040 Training loss 0.055014800280332565 Validation loss 0.053046900779008865 Accuracy 0.8597500324249268\n",
      "Iteration 22050 Training loss 0.052522141486406326 Validation loss 0.05306105688214302 Accuracy 0.85875004529953\n",
      "Iteration 22060 Training loss 0.05107958987355232 Validation loss 0.053158555179834366 Accuracy 0.858875036239624\n",
      "Iteration 22070 Training loss 0.04971519485116005 Validation loss 0.05303436517715454 Accuracy 0.8597500324249268\n",
      "Iteration 22080 Training loss 0.044549550861120224 Validation loss 0.05305151268839836 Accuracy 0.8601250648498535\n",
      "Iteration 22090 Training loss 0.053639911115169525 Validation loss 0.053251590579748154 Accuracy 0.859125018119812\n",
      "Iteration 22100 Training loss 0.04963255673646927 Validation loss 0.053395338356494904 Accuracy 0.859000027179718\n",
      "Iteration 22110 Training loss 0.04906203970313072 Validation loss 0.05304689705371857 Accuracy 0.8595000505447388\n",
      "Iteration 22120 Training loss 0.050704434514045715 Validation loss 0.05315038189291954 Accuracy 0.858875036239624\n",
      "Iteration 22130 Training loss 0.058268554508686066 Validation loss 0.053264107555150986 Accuracy 0.858625054359436\n",
      "Iteration 22140 Training loss 0.046914514154195786 Validation loss 0.05303115025162697 Accuracy 0.8598750233650208\n",
      "Iteration 22150 Training loss 0.0502249151468277 Validation loss 0.05303201824426651 Accuracy 0.8596250414848328\n",
      "Iteration 22160 Training loss 0.05154746025800705 Validation loss 0.05305415391921997 Accuracy 0.861750066280365\n",
      "Iteration 22170 Training loss 0.0534890815615654 Validation loss 0.05309341102838516 Accuracy 0.8595000505447388\n",
      "Iteration 22180 Training loss 0.06122330576181412 Validation loss 0.05300590768456459 Accuracy 0.8598750233650208\n",
      "Iteration 22190 Training loss 0.05736459419131279 Validation loss 0.053036052733659744 Accuracy 0.8600000143051147\n",
      "Iteration 22200 Training loss 0.045783694833517075 Validation loss 0.05302208289504051 Accuracy 0.8598750233650208\n",
      "Iteration 22210 Training loss 0.04455243796110153 Validation loss 0.05300401151180267 Accuracy 0.8598750233650208\n",
      "Iteration 22220 Training loss 0.04999314621090889 Validation loss 0.0530763640999794 Accuracy 0.8612500429153442\n",
      "Iteration 22230 Training loss 0.04814288020133972 Validation loss 0.05300891026854515 Accuracy 0.8603750467300415\n",
      "Iteration 22240 Training loss 0.04729384183883667 Validation loss 0.05353790521621704 Accuracy 0.8572500348091125\n",
      "Iteration 22250 Training loss 0.05369609221816063 Validation loss 0.05318053811788559 Accuracy 0.85875004529953\n",
      "Iteration 22260 Training loss 0.0543660931289196 Validation loss 0.053136326372623444 Accuracy 0.858625054359436\n",
      "Iteration 22270 Training loss 0.0553806908428669 Validation loss 0.053017374128103256 Accuracy 0.8593750596046448\n",
      "Iteration 22280 Training loss 0.05897200480103493 Validation loss 0.05317404493689537 Accuracy 0.8582500219345093\n",
      "Iteration 22290 Training loss 0.05067696422338486 Validation loss 0.05320582166314125 Accuracy 0.8580000400543213\n",
      "Iteration 22300 Training loss 0.05755942314863205 Validation loss 0.053032286465168 Accuracy 0.8605000376701355\n",
      "Iteration 22310 Training loss 0.04949541762471199 Validation loss 0.053020838648080826 Accuracy 0.8582500219345093\n",
      "Iteration 22320 Training loss 0.05711809918284416 Validation loss 0.0529770702123642 Accuracy 0.8612500429153442\n",
      "Iteration 22330 Training loss 0.05503835529088974 Validation loss 0.05293348804116249 Accuracy 0.8602500557899475\n",
      "Iteration 22340 Training loss 0.0513598807156086 Validation loss 0.05296522006392479 Accuracy 0.8592500686645508\n",
      "Iteration 22350 Training loss 0.0539994053542614 Validation loss 0.05297540873289108 Accuracy 0.8598750233650208\n",
      "Iteration 22360 Training loss 0.05308626964688301 Validation loss 0.05292985960841179 Accuracy 0.8603750467300415\n",
      "Iteration 22370 Training loss 0.045488934963941574 Validation loss 0.05297643318772316 Accuracy 0.8592500686645508\n",
      "Iteration 22380 Training loss 0.04739517718553543 Validation loss 0.052931081503629684 Accuracy 0.8598750233650208\n",
      "Iteration 22390 Training loss 0.05606709420681 Validation loss 0.05319027230143547 Accuracy 0.8578750491142273\n",
      "Iteration 22400 Training loss 0.04966964200139046 Validation loss 0.052947480231523514 Accuracy 0.8607500195503235\n",
      "Iteration 22410 Training loss 0.05420372262597084 Validation loss 0.05291792377829552 Accuracy 0.8595000505447388\n",
      "Iteration 22420 Training loss 0.054873302578926086 Validation loss 0.05301615595817566 Accuracy 0.858500063419342\n",
      "Iteration 22430 Training loss 0.052659738808870316 Validation loss 0.052933815866708755 Accuracy 0.8608750700950623\n",
      "Iteration 22440 Training loss 0.048777129501104355 Validation loss 0.05321604013442993 Accuracy 0.8582500219345093\n",
      "Iteration 22450 Training loss 0.0512831024825573 Validation loss 0.05305492877960205 Accuracy 0.8608750700950623\n",
      "Iteration 22460 Training loss 0.050598811358213425 Validation loss 0.05300119146704674 Accuracy 0.8583750128746033\n",
      "Iteration 22470 Training loss 0.04699493944644928 Validation loss 0.05322634056210518 Accuracy 0.858625054359436\n",
      "Iteration 22480 Training loss 0.04779265820980072 Validation loss 0.05344977229833603 Accuracy 0.8581250309944153\n",
      "Iteration 22490 Training loss 0.05504826083779335 Validation loss 0.05397099629044533 Accuracy 0.8565000295639038\n",
      "Iteration 22500 Training loss 0.04824160411953926 Validation loss 0.05301378294825554 Accuracy 0.8580000400543213\n",
      "Iteration 22510 Training loss 0.05744398385286331 Validation loss 0.053244635462760925 Accuracy 0.859125018119812\n",
      "Iteration 22520 Training loss 0.05097511038184166 Validation loss 0.0528901070356369 Accuracy 0.8597500324249268\n",
      "Iteration 22530 Training loss 0.04857126623392105 Validation loss 0.05297957733273506 Accuracy 0.8602500557899475\n",
      "Iteration 22540 Training loss 0.0452326275408268 Validation loss 0.0532374382019043 Accuracy 0.858625054359436\n",
      "Iteration 22550 Training loss 0.04978189244866371 Validation loss 0.05294782668352127 Accuracy 0.8592500686645508\n",
      "Iteration 22560 Training loss 0.05578373000025749 Validation loss 0.05328942462801933 Accuracy 0.8577500581741333\n",
      "Iteration 22570 Training loss 0.055233754217624664 Validation loss 0.05282670259475708 Accuracy 0.8606250286102295\n",
      "Iteration 22580 Training loss 0.05422001704573631 Validation loss 0.05294262245297432 Accuracy 0.8592500686645508\n",
      "Iteration 22590 Training loss 0.04957493022084236 Validation loss 0.052820101380348206 Accuracy 0.8610000610351562\n",
      "Iteration 22600 Training loss 0.05038914084434509 Validation loss 0.05298784002661705 Accuracy 0.858625054359436\n",
      "Iteration 22610 Training loss 0.05251162871718407 Validation loss 0.05279083177447319 Accuracy 0.8603750467300415\n",
      "Iteration 22620 Training loss 0.04401664063334465 Validation loss 0.05281383916735649 Accuracy 0.8602500557899475\n",
      "Iteration 22630 Training loss 0.05067694932222366 Validation loss 0.05292695015668869 Accuracy 0.8593750596046448\n",
      "Iteration 22640 Training loss 0.05699579045176506 Validation loss 0.0529371052980423 Accuracy 0.8593750596046448\n",
      "Iteration 22650 Training loss 0.04614618793129921 Validation loss 0.05280618742108345 Accuracy 0.8607500195503235\n",
      "Iteration 22660 Training loss 0.04582207649946213 Validation loss 0.053091861307621 Accuracy 0.858875036239624\n",
      "Iteration 22670 Training loss 0.05080382525920868 Validation loss 0.05289221182465553 Accuracy 0.858875036239624\n",
      "Iteration 22680 Training loss 0.052753522992134094 Validation loss 0.05282573029398918 Accuracy 0.8602500557899475\n",
      "Iteration 22690 Training loss 0.05103149265050888 Validation loss 0.05293514207005501 Accuracy 0.8592500686645508\n",
      "Iteration 22700 Training loss 0.046628646552562714 Validation loss 0.052817631512880325 Accuracy 0.8592500686645508\n",
      "Iteration 22710 Training loss 0.04843434318900108 Validation loss 0.0527673065662384 Accuracy 0.8600000143051147\n",
      "Iteration 22720 Training loss 0.05831003561615944 Validation loss 0.052809007465839386 Accuracy 0.8613750338554382\n",
      "Iteration 22730 Training loss 0.051159895956516266 Validation loss 0.052770864218473434 Accuracy 0.8600000143051147\n",
      "Iteration 22740 Training loss 0.04385417327284813 Validation loss 0.052747342735528946 Accuracy 0.8602500557899475\n",
      "Iteration 22750 Training loss 0.05604398250579834 Validation loss 0.05285672843456268 Accuracy 0.8602500557899475\n",
      "Iteration 22760 Training loss 0.05133616924285889 Validation loss 0.05282067507505417 Accuracy 0.8593750596046448\n",
      "Iteration 22770 Training loss 0.061290841549634933 Validation loss 0.05309082940220833 Accuracy 0.858500063419342\n",
      "Iteration 22780 Training loss 0.055882856249809265 Validation loss 0.052816640585660934 Accuracy 0.8606250286102295\n",
      "Iteration 22790 Training loss 0.04509802535176277 Validation loss 0.052980199456214905 Accuracy 0.8596250414848328\n",
      "Iteration 22800 Training loss 0.049623358994722366 Validation loss 0.05308028310537338 Accuracy 0.859125018119812\n",
      "Iteration 22810 Training loss 0.046988144516944885 Validation loss 0.0527108795940876 Accuracy 0.8606250286102295\n",
      "Iteration 22820 Training loss 0.053351230919361115 Validation loss 0.05272787809371948 Accuracy 0.8605000376701355\n",
      "Iteration 22830 Training loss 0.05935315787792206 Validation loss 0.052831728011369705 Accuracy 0.8612500429153442\n",
      "Iteration 22840 Training loss 0.04861746355891228 Validation loss 0.052701257169246674 Accuracy 0.8600000143051147\n",
      "Iteration 22850 Training loss 0.049702245742082596 Validation loss 0.052706606686115265 Accuracy 0.8615000247955322\n",
      "Iteration 22860 Training loss 0.053020376712083817 Validation loss 0.052726976573467255 Accuracy 0.862250030040741\n",
      "Iteration 22870 Training loss 0.052635423839092255 Validation loss 0.052657805383205414 Accuracy 0.8606250286102295\n",
      "Iteration 22880 Training loss 0.05712959170341492 Validation loss 0.05282192677259445 Accuracy 0.8596250414848328\n",
      "Iteration 22890 Training loss 0.04796494543552399 Validation loss 0.05281253531575203 Accuracy 0.8608750700950623\n",
      "Iteration 22900 Training loss 0.05375375598669052 Validation loss 0.052609965205192566 Accuracy 0.8616250157356262\n",
      "Iteration 22910 Training loss 0.048077523708343506 Validation loss 0.052865467965602875 Accuracy 0.8595000505447388\n",
      "Iteration 22920 Training loss 0.05260370299220085 Validation loss 0.052687402814626694 Accuracy 0.8600000143051147\n",
      "Iteration 22930 Training loss 0.04730067029595375 Validation loss 0.052631087601184845 Accuracy 0.8615000247955322\n",
      "Iteration 22940 Training loss 0.0624031238257885 Validation loss 0.05266817286610603 Accuracy 0.8611250519752502\n",
      "Iteration 22950 Training loss 0.050961095839738846 Validation loss 0.052668772637844086 Accuracy 0.8612500429153442\n",
      "Iteration 22960 Training loss 0.05320687219500542 Validation loss 0.0526731051504612 Accuracy 0.8612500429153442\n",
      "Iteration 22970 Training loss 0.05211641266942024 Validation loss 0.052628111094236374 Accuracy 0.8611250519752502\n",
      "Iteration 22980 Training loss 0.04239194840192795 Validation loss 0.052640609443187714 Accuracy 0.8606250286102295\n",
      "Iteration 22990 Training loss 0.05519821122288704 Validation loss 0.05266088247299194 Accuracy 0.8611250519752502\n",
      "Iteration 23000 Training loss 0.04376494139432907 Validation loss 0.05294566601514816 Accuracy 0.858875036239624\n",
      "Iteration 23010 Training loss 0.052124567329883575 Validation loss 0.05264627933502197 Accuracy 0.8610000610351562\n",
      "Iteration 23020 Training loss 0.04449564963579178 Validation loss 0.052596643567085266 Accuracy 0.862000048160553\n",
      "Iteration 23030 Training loss 0.05285176634788513 Validation loss 0.05274302884936333 Accuracy 0.859125018119812\n",
      "Iteration 23040 Training loss 0.04647218436002731 Validation loss 0.052629563957452774 Accuracy 0.8616250157356262\n",
      "Iteration 23050 Training loss 0.05373712256550789 Validation loss 0.05262311175465584 Accuracy 0.8607500195503235\n",
      "Iteration 23060 Training loss 0.04902331158518791 Validation loss 0.052599627524614334 Accuracy 0.861750066280365\n",
      "Iteration 23070 Training loss 0.04596288502216339 Validation loss 0.052614886313676834 Accuracy 0.8608750700950623\n",
      "Iteration 23080 Training loss 0.05461631342768669 Validation loss 0.052791059017181396 Accuracy 0.8605000376701355\n",
      "Iteration 23090 Training loss 0.05186441168189049 Validation loss 0.052842944860458374 Accuracy 0.8598750233650208\n",
      "Iteration 23100 Training loss 0.050204962491989136 Validation loss 0.05260080099105835 Accuracy 0.8603750467300415\n",
      "Iteration 23110 Training loss 0.048418886959552765 Validation loss 0.05286233127117157 Accuracy 0.8597500324249268\n",
      "Iteration 23120 Training loss 0.04737634211778641 Validation loss 0.05272836610674858 Accuracy 0.8607500195503235\n",
      "Iteration 23130 Training loss 0.05935192480683327 Validation loss 0.052626390010118484 Accuracy 0.8603750467300415\n",
      "Iteration 23140 Training loss 0.05297383293509483 Validation loss 0.052658166736364365 Accuracy 0.8600000143051147\n",
      "Iteration 23150 Training loss 0.04887143895030022 Validation loss 0.052607759833335876 Accuracy 0.8598750233650208\n",
      "Iteration 23160 Training loss 0.05577215179800987 Validation loss 0.05261721834540367 Accuracy 0.8612500429153442\n",
      "Iteration 23170 Training loss 0.04932709410786629 Validation loss 0.052555833011865616 Accuracy 0.8600000143051147\n",
      "Iteration 23180 Training loss 0.04576978459954262 Validation loss 0.05257940664887428 Accuracy 0.8612500429153442\n",
      "Iteration 23190 Training loss 0.05335104465484619 Validation loss 0.052511170506477356 Accuracy 0.8602500557899475\n",
      "Iteration 23200 Training loss 0.05138318985700607 Validation loss 0.05259628966450691 Accuracy 0.8615000247955322\n",
      "Iteration 23210 Training loss 0.05123679339885712 Validation loss 0.05250123143196106 Accuracy 0.8606250286102295\n",
      "Iteration 23220 Training loss 0.051582079380750656 Validation loss 0.052583396434783936 Accuracy 0.8597500324249268\n",
      "Iteration 23230 Training loss 0.04659871384501457 Validation loss 0.05269499495625496 Accuracy 0.8595000505447388\n",
      "Iteration 23240 Training loss 0.04625346139073372 Validation loss 0.052499208599328995 Accuracy 0.8603750467300415\n",
      "Iteration 23250 Training loss 0.047408681362867355 Validation loss 0.052550703287124634 Accuracy 0.8597500324249268\n",
      "Iteration 23260 Training loss 0.047898709774017334 Validation loss 0.0525590218603611 Accuracy 0.8601250648498535\n",
      "Iteration 23270 Training loss 0.050857409834861755 Validation loss 0.05254150554537773 Accuracy 0.8596250414848328\n",
      "Iteration 23280 Training loss 0.04373377561569214 Validation loss 0.05249727517366409 Accuracy 0.8595000505447388\n",
      "Iteration 23290 Training loss 0.043182339519262314 Validation loss 0.05246018245816231 Accuracy 0.8615000247955322\n",
      "Iteration 23300 Training loss 0.05374075844883919 Validation loss 0.052492495626211166 Accuracy 0.8612500429153442\n",
      "Iteration 23310 Training loss 0.051058877259492874 Validation loss 0.05262856185436249 Accuracy 0.8613750338554382\n",
      "Iteration 23320 Training loss 0.04454483464360237 Validation loss 0.0526193343102932 Accuracy 0.8610000610351562\n",
      "Iteration 23330 Training loss 0.05122250318527222 Validation loss 0.05250032991170883 Accuracy 0.8605000376701355\n",
      "Iteration 23340 Training loss 0.05562416836619377 Validation loss 0.05265388637781143 Accuracy 0.8600000143051147\n",
      "Iteration 23350 Training loss 0.06023591011762619 Validation loss 0.052547745406627655 Accuracy 0.8611250519752502\n",
      "Iteration 23360 Training loss 0.0497514009475708 Validation loss 0.05243847146630287 Accuracy 0.8613750338554382\n",
      "Iteration 23370 Training loss 0.050774574279785156 Validation loss 0.05271249637007713 Accuracy 0.8597500324249268\n",
      "Iteration 23380 Training loss 0.05286189913749695 Validation loss 0.05256626009941101 Accuracy 0.8601250648498535\n",
      "Iteration 23390 Training loss 0.04899827018380165 Validation loss 0.052529893815517426 Accuracy 0.8612500429153442\n",
      "Iteration 23400 Training loss 0.05453891679644585 Validation loss 0.05247221514582634 Accuracy 0.8607500195503235\n",
      "Iteration 23410 Training loss 0.05004826933145523 Validation loss 0.05244998633861542 Accuracy 0.8607500195503235\n",
      "Iteration 23420 Training loss 0.04621557518839836 Validation loss 0.052422575652599335 Accuracy 0.8613750338554382\n",
      "Iteration 23430 Training loss 0.050088070333004 Validation loss 0.052419502288103104 Accuracy 0.8615000247955322\n",
      "Iteration 23440 Training loss 0.05174138769507408 Validation loss 0.05238521099090576 Accuracy 0.8612500429153442\n",
      "Iteration 23450 Training loss 0.04649689048528671 Validation loss 0.052402157336473465 Accuracy 0.8611250519752502\n",
      "Iteration 23460 Training loss 0.04211312532424927 Validation loss 0.052521366626024246 Accuracy 0.8595000505447388\n",
      "Iteration 23470 Training loss 0.054585978388786316 Validation loss 0.05280059203505516 Accuracy 0.8597500324249268\n",
      "Iteration 23480 Training loss 0.05729000270366669 Validation loss 0.05243009328842163 Accuracy 0.8605000376701355\n",
      "Iteration 23490 Training loss 0.040492042899131775 Validation loss 0.0525222010910511 Accuracy 0.8613750338554382\n",
      "Iteration 23500 Training loss 0.03682830557227135 Validation loss 0.05279621109366417 Accuracy 0.8595000505447388\n",
      "Iteration 23510 Training loss 0.04591437056660652 Validation loss 0.05243752524256706 Accuracy 0.8598750233650208\n",
      "Iteration 23520 Training loss 0.050318386405706406 Validation loss 0.05244000628590584 Accuracy 0.8603750467300415\n",
      "Iteration 23530 Training loss 0.0410119965672493 Validation loss 0.05242370069026947 Accuracy 0.8612500429153442\n",
      "Iteration 23540 Training loss 0.04893732815980911 Validation loss 0.05259961262345314 Accuracy 0.8598750233650208\n",
      "Iteration 23550 Training loss 0.05151870474219322 Validation loss 0.05242792144417763 Accuracy 0.8612500429153442\n",
      "Iteration 23560 Training loss 0.050463028252124786 Validation loss 0.052655357867479324 Accuracy 0.8596250414848328\n",
      "Iteration 23570 Training loss 0.05056329444050789 Validation loss 0.05236727371811867 Accuracy 0.8613750338554382\n",
      "Iteration 23580 Training loss 0.058752257376909256 Validation loss 0.05237041413784027 Accuracy 0.8615000247955322\n",
      "Iteration 23590 Training loss 0.0552484393119812 Validation loss 0.05313733220100403 Accuracy 0.8592500686645508\n",
      "Iteration 23600 Training loss 0.043935880064964294 Validation loss 0.05243357643485069 Accuracy 0.8610000610351562\n",
      "Iteration 23610 Training loss 0.059658922255039215 Validation loss 0.05236139893531799 Accuracy 0.8607500195503235\n",
      "Iteration 23620 Training loss 0.04676620289683342 Validation loss 0.052316684275865555 Accuracy 0.861750066280365\n",
      "Iteration 23630 Training loss 0.04643094539642334 Validation loss 0.05236915498971939 Accuracy 0.8611250519752502\n",
      "Iteration 23640 Training loss 0.04200267791748047 Validation loss 0.052295587956905365 Accuracy 0.862000048160553\n",
      "Iteration 23650 Training loss 0.052272114902734756 Validation loss 0.05264822021126747 Accuracy 0.8606250286102295\n",
      "Iteration 23660 Training loss 0.055684249848127365 Validation loss 0.052604567259550095 Accuracy 0.8610000610351562\n",
      "Iteration 23670 Training loss 0.04568958654999733 Validation loss 0.05238042026758194 Accuracy 0.8615000247955322\n",
      "Iteration 23680 Training loss 0.04853347688913345 Validation loss 0.05242454260587692 Accuracy 0.8606250286102295\n",
      "Iteration 23690 Training loss 0.0441334992647171 Validation loss 0.05235333368182182 Accuracy 0.8612500429153442\n",
      "Iteration 23700 Training loss 0.06175926700234413 Validation loss 0.05250868573784828 Accuracy 0.8597500324249268\n",
      "Iteration 23710 Training loss 0.04497213661670685 Validation loss 0.052349191159009933 Accuracy 0.8610000610351562\n",
      "Iteration 23720 Training loss 0.05785280093550682 Validation loss 0.052750878036022186 Accuracy 0.8597500324249268\n",
      "Iteration 23730 Training loss 0.047027792781591415 Validation loss 0.05282211676239967 Accuracy 0.8597500324249268\n",
      "Iteration 23740 Training loss 0.04252723604440689 Validation loss 0.052420858293771744 Accuracy 0.8602500557899475\n",
      "Iteration 23750 Training loss 0.04661306366324425 Validation loss 0.05257406830787659 Accuracy 0.8596250414848328\n",
      "Iteration 23760 Training loss 0.04670866206288338 Validation loss 0.05249427631497383 Accuracy 0.8600000143051147\n",
      "Iteration 23770 Training loss 0.05662935972213745 Validation loss 0.052348434925079346 Accuracy 0.8608750700950623\n",
      "Iteration 23780 Training loss 0.046379994601011276 Validation loss 0.052372902631759644 Accuracy 0.8607500195503235\n",
      "Iteration 23790 Training loss 0.054854631423950195 Validation loss 0.0523497499525547 Accuracy 0.861875057220459\n",
      "Iteration 23800 Training loss 0.04880750924348831 Validation loss 0.05230396240949631 Accuracy 0.862125039100647\n",
      "Iteration 23810 Training loss 0.05055107921361923 Validation loss 0.0522882342338562 Accuracy 0.861875057220459\n",
      "Iteration 23820 Training loss 0.045578908175230026 Validation loss 0.05228744074702263 Accuracy 0.8627500534057617\n",
      "Iteration 23830 Training loss 0.048574332147836685 Validation loss 0.05230507627129555 Accuracy 0.862500011920929\n",
      "Iteration 23840 Training loss 0.057206492871046066 Validation loss 0.05249441787600517 Accuracy 0.8603750467300415\n",
      "Iteration 23850 Training loss 0.04642771929502487 Validation loss 0.05235471576452255 Accuracy 0.8615000247955322\n",
      "Iteration 23860 Training loss 0.05092328414320946 Validation loss 0.05229444429278374 Accuracy 0.862125039100647\n",
      "Iteration 23870 Training loss 0.04754564166069031 Validation loss 0.05231352150440216 Accuracy 0.862500011920929\n",
      "Iteration 23880 Training loss 0.05523200333118439 Validation loss 0.052266791462898254 Accuracy 0.862000048160553\n",
      "Iteration 23890 Training loss 0.05082398280501366 Validation loss 0.052272532135248184 Accuracy 0.862500011920929\n",
      "Iteration 23900 Training loss 0.057137381285429 Validation loss 0.05243051052093506 Accuracy 0.8608750700950623\n",
      "Iteration 23910 Training loss 0.05505027994513512 Validation loss 0.052305109798908234 Accuracy 0.861750066280365\n",
      "Iteration 23920 Training loss 0.044469527900218964 Validation loss 0.052314117550849915 Accuracy 0.862125039100647\n",
      "Iteration 23930 Training loss 0.05594389885663986 Validation loss 0.05235607549548149 Accuracy 0.8608750700950623\n",
      "Iteration 23940 Training loss 0.04622916132211685 Validation loss 0.05227915197610855 Accuracy 0.8615000247955322\n",
      "Iteration 23950 Training loss 0.05537163093686104 Validation loss 0.05232942849397659 Accuracy 0.8610000610351562\n",
      "Iteration 23960 Training loss 0.048895079642534256 Validation loss 0.05223246291279793 Accuracy 0.8616250157356262\n",
      "Iteration 23970 Training loss 0.05262542515993118 Validation loss 0.05236465111374855 Accuracy 0.8608750700950623\n",
      "Iteration 23980 Training loss 0.05075132101774216 Validation loss 0.052191972732543945 Accuracy 0.862125039100647\n",
      "Iteration 23990 Training loss 0.05379718542098999 Validation loss 0.05218721181154251 Accuracy 0.862500011920929\n",
      "Iteration 24000 Training loss 0.05349091812968254 Validation loss 0.05217508226633072 Accuracy 0.8628750443458557\n",
      "Iteration 24010 Training loss 0.04207061976194382 Validation loss 0.052145425230264664 Accuracy 0.862000048160553\n",
      "Iteration 24020 Training loss 0.043209295719861984 Validation loss 0.0521387904882431 Accuracy 0.8626250624656677\n",
      "Iteration 24030 Training loss 0.054464854300022125 Validation loss 0.052156079560518265 Accuracy 0.8616250157356262\n",
      "Iteration 24040 Training loss 0.05057825893163681 Validation loss 0.05217086896300316 Accuracy 0.8607500195503235\n",
      "Iteration 24050 Training loss 0.049300555139780045 Validation loss 0.05224334076046944 Accuracy 0.8610000610351562\n",
      "Iteration 24060 Training loss 0.05287037789821625 Validation loss 0.052132852375507355 Accuracy 0.862000048160553\n",
      "Iteration 24070 Training loss 0.05100155621767044 Validation loss 0.05214519798755646 Accuracy 0.862500011920929\n",
      "Iteration 24080 Training loss 0.05585001781582832 Validation loss 0.052525829523801804 Accuracy 0.8601250648498535\n",
      "Iteration 24090 Training loss 0.04379772022366524 Validation loss 0.05210217460989952 Accuracy 0.862500011920929\n",
      "Iteration 24100 Training loss 0.05315553396940231 Validation loss 0.05211392790079117 Accuracy 0.8630000352859497\n",
      "Iteration 24110 Training loss 0.048659056425094604 Validation loss 0.052141353487968445 Accuracy 0.862250030040741\n",
      "Iteration 24120 Training loss 0.04929029196500778 Validation loss 0.05222323164343834 Accuracy 0.8610000610351562\n",
      "Iteration 24130 Training loss 0.04854053258895874 Validation loss 0.052144855260849 Accuracy 0.862250030040741\n",
      "Iteration 24140 Training loss 0.05177152156829834 Validation loss 0.05214247480034828 Accuracy 0.8616250157356262\n",
      "Iteration 24150 Training loss 0.04389726370573044 Validation loss 0.05211569741368294 Accuracy 0.8631250262260437\n",
      "Iteration 24160 Training loss 0.04816292226314545 Validation loss 0.052146486937999725 Accuracy 0.8615000247955322\n",
      "Iteration 24170 Training loss 0.05641881003975868 Validation loss 0.05213623121380806 Accuracy 0.862125039100647\n",
      "Iteration 24180 Training loss 0.0501587837934494 Validation loss 0.052276596426963806 Accuracy 0.8610000610351562\n",
      "Iteration 24190 Training loss 0.045350152999162674 Validation loss 0.052167970687150955 Accuracy 0.8608750700950623\n",
      "Iteration 24200 Training loss 0.04992508888244629 Validation loss 0.05208829045295715 Accuracy 0.861875057220459\n",
      "Iteration 24210 Training loss 0.052330732345581055 Validation loss 0.05206657573580742 Accuracy 0.862125039100647\n",
      "Iteration 24220 Training loss 0.05356524884700775 Validation loss 0.05273247882723808 Accuracy 0.858875036239624\n",
      "Iteration 24230 Training loss 0.052582625299692154 Validation loss 0.05207853019237518 Accuracy 0.8612500429153442\n",
      "Iteration 24240 Training loss 0.049865543842315674 Validation loss 0.052141934633255005 Accuracy 0.862250030040741\n",
      "Iteration 24250 Training loss 0.0376121886074543 Validation loss 0.05210155248641968 Accuracy 0.8608750700950623\n",
      "Iteration 24260 Training loss 0.049935441464185715 Validation loss 0.05211448669433594 Accuracy 0.8606250286102295\n",
      "Iteration 24270 Training loss 0.046911418437957764 Validation loss 0.052069757133722305 Accuracy 0.8606250286102295\n",
      "Iteration 24280 Training loss 0.04678267240524292 Validation loss 0.05207282677292824 Accuracy 0.8612500429153442\n",
      "Iteration 24290 Training loss 0.047414615750312805 Validation loss 0.052090417593717575 Accuracy 0.862375020980835\n",
      "Iteration 24300 Training loss 0.05599330738186836 Validation loss 0.05220013111829758 Accuracy 0.8613750338554382\n",
      "Iteration 24310 Training loss 0.055883120745420456 Validation loss 0.05208173021674156 Accuracy 0.861750066280365\n",
      "Iteration 24320 Training loss 0.0555843822658062 Validation loss 0.05216628313064575 Accuracy 0.8610000610351562\n",
      "Iteration 24330 Training loss 0.04704222455620766 Validation loss 0.052740197628736496 Accuracy 0.8610000610351562\n",
      "Iteration 24340 Training loss 0.046506576240062714 Validation loss 0.052035827189683914 Accuracy 0.862250030040741\n",
      "Iteration 24350 Training loss 0.05093011632561684 Validation loss 0.05199860781431198 Accuracy 0.8627500534057617\n",
      "Iteration 24360 Training loss 0.05759384110569954 Validation loss 0.05200311169028282 Accuracy 0.8631250262260437\n",
      "Iteration 24370 Training loss 0.05459900572896004 Validation loss 0.05202812701463699 Accuracy 0.8630000352859497\n",
      "Iteration 24380 Training loss 0.05657466873526573 Validation loss 0.0519905723631382 Accuracy 0.862250030040741\n",
      "Iteration 24390 Training loss 0.04890363663434982 Validation loss 0.05202076584100723 Accuracy 0.861750066280365\n",
      "Iteration 24400 Training loss 0.0387054942548275 Validation loss 0.052019741386175156 Accuracy 0.862000048160553\n",
      "Iteration 24410 Training loss 0.04675712808966637 Validation loss 0.05200609192252159 Accuracy 0.8633750677108765\n",
      "Iteration 24420 Training loss 0.053258076310157776 Validation loss 0.05251681059598923 Accuracy 0.8605000376701355\n",
      "Iteration 24430 Training loss 0.04686153307557106 Validation loss 0.05204144865274429 Accuracy 0.862125039100647\n",
      "Iteration 24440 Training loss 0.045037344098091125 Validation loss 0.05201086774468422 Accuracy 0.861875057220459\n",
      "Iteration 24450 Training loss 0.04866013303399086 Validation loss 0.05199841037392616 Accuracy 0.862375020980835\n",
      "Iteration 24460 Training loss 0.05414220690727234 Validation loss 0.052250076085329056 Accuracy 0.8611250519752502\n",
      "Iteration 24470 Training loss 0.0459422767162323 Validation loss 0.05253397673368454 Accuracy 0.8602500557899475\n",
      "Iteration 24480 Training loss 0.048113156110048294 Validation loss 0.05219557136297226 Accuracy 0.8608750700950623\n",
      "Iteration 24490 Training loss 0.04968541860580444 Validation loss 0.052055757492780685 Accuracy 0.8627500534057617\n",
      "Iteration 24500 Training loss 0.05013667047023773 Validation loss 0.052215807139873505 Accuracy 0.8607500195503235\n",
      "Iteration 24510 Training loss 0.046813711524009705 Validation loss 0.05203421413898468 Accuracy 0.862250030040741\n",
      "Iteration 24520 Training loss 0.04992111027240753 Validation loss 0.05200326070189476 Accuracy 0.8633750677108765\n",
      "Iteration 24530 Training loss 0.05842253565788269 Validation loss 0.05219158157706261 Accuracy 0.8605000376701355\n",
      "Iteration 24540 Training loss 0.04463433101773262 Validation loss 0.05208215489983559 Accuracy 0.861750066280365\n",
      "Iteration 24550 Training loss 0.050570353865623474 Validation loss 0.05252508446574211 Accuracy 0.8593750596046448\n",
      "Iteration 24560 Training loss 0.052547868341207504 Validation loss 0.05200217664241791 Accuracy 0.862250030040741\n",
      "Iteration 24570 Training loss 0.05179747939109802 Validation loss 0.05194877088069916 Accuracy 0.8637500405311584\n",
      "Iteration 24580 Training loss 0.04106699302792549 Validation loss 0.05200734734535217 Accuracy 0.8616250157356262\n",
      "Iteration 24590 Training loss 0.04728111997246742 Validation loss 0.051926422864198685 Accuracy 0.862000048160553\n",
      "Iteration 24600 Training loss 0.05430231988430023 Validation loss 0.05191446840763092 Accuracy 0.8630000352859497\n",
      "Iteration 24610 Training loss 0.047070443630218506 Validation loss 0.052034251391887665 Accuracy 0.862250030040741\n",
      "Iteration 24620 Training loss 0.05376593768596649 Validation loss 0.05191487818956375 Accuracy 0.8626250624656677\n",
      "Iteration 24630 Training loss 0.05232809856534004 Validation loss 0.051939066499471664 Accuracy 0.8628750443458557\n",
      "Iteration 24640 Training loss 0.048597678542137146 Validation loss 0.05186847969889641 Accuracy 0.862375020980835\n",
      "Iteration 24650 Training loss 0.04569659009575844 Validation loss 0.05201065167784691 Accuracy 0.862375020980835\n",
      "Iteration 24660 Training loss 0.0500461719930172 Validation loss 0.05187160149216652 Accuracy 0.861750066280365\n",
      "Iteration 24670 Training loss 0.04881201684474945 Validation loss 0.051920175552368164 Accuracy 0.862375020980835\n",
      "Iteration 24680 Training loss 0.05216808244585991 Validation loss 0.051894526928663254 Accuracy 0.862500011920929\n",
      "Iteration 24690 Training loss 0.03409130126237869 Validation loss 0.051883161067962646 Accuracy 0.862375020980835\n",
      "Iteration 24700 Training loss 0.04309086874127388 Validation loss 0.051872678101062775 Accuracy 0.862250030040741\n",
      "Iteration 24710 Training loss 0.05620031803846359 Validation loss 0.05195636302232742 Accuracy 0.8616250157356262\n",
      "Iteration 24720 Training loss 0.05886329337954521 Validation loss 0.051968272775411606 Accuracy 0.8615000247955322\n",
      "Iteration 24730 Training loss 0.04737585410475731 Validation loss 0.0519152507185936 Accuracy 0.8635000586509705\n",
      "Iteration 24740 Training loss 0.04323050007224083 Validation loss 0.051895394921302795 Accuracy 0.8628750443458557\n",
      "Iteration 24750 Training loss 0.0489453487098217 Validation loss 0.05190618708729744 Accuracy 0.8628750443458557\n",
      "Iteration 24760 Training loss 0.041968945413827896 Validation loss 0.05204791575670242 Accuracy 0.8612500429153442\n",
      "Iteration 24770 Training loss 0.05148673802614212 Validation loss 0.05184795334935188 Accuracy 0.862125039100647\n",
      "Iteration 24780 Training loss 0.044995177537202835 Validation loss 0.051882579922676086 Accuracy 0.862250030040741\n",
      "Iteration 24790 Training loss 0.057816751301288605 Validation loss 0.051861170679330826 Accuracy 0.8627500534057617\n",
      "Iteration 24800 Training loss 0.048012588173151016 Validation loss 0.051840610802173615 Accuracy 0.8626250624656677\n",
      "Iteration 24810 Training loss 0.04207421839237213 Validation loss 0.05183637514710426 Accuracy 0.8626250624656677\n",
      "Iteration 24820 Training loss 0.04483281075954437 Validation loss 0.0519099086523056 Accuracy 0.8626250624656677\n",
      "Iteration 24830 Training loss 0.0443403534591198 Validation loss 0.05185903236269951 Accuracy 0.8627500534057617\n",
      "Iteration 24840 Training loss 0.049420394003391266 Validation loss 0.052119165658950806 Accuracy 0.8610000610351562\n",
      "Iteration 24850 Training loss 0.05237575247883797 Validation loss 0.05209683999419212 Accuracy 0.8613750338554382\n",
      "Iteration 24860 Training loss 0.0553598627448082 Validation loss 0.05213603749871254 Accuracy 0.8612500429153442\n",
      "Iteration 24870 Training loss 0.049995459616184235 Validation loss 0.05185447260737419 Accuracy 0.8630000352859497\n",
      "Iteration 24880 Training loss 0.05613311752676964 Validation loss 0.051878198981285095 Accuracy 0.8632500171661377\n",
      "Iteration 24890 Training loss 0.0434318482875824 Validation loss 0.0519549623131752 Accuracy 0.8627500534057617\n",
      "Iteration 24900 Training loss 0.05366257578134537 Validation loss 0.05185861885547638 Accuracy 0.862500011920929\n",
      "Iteration 24910 Training loss 0.05328739061951637 Validation loss 0.051892999559640884 Accuracy 0.862500011920929\n",
      "Iteration 24920 Training loss 0.04929820075631142 Validation loss 0.052174508571624756 Accuracy 0.862375020980835\n",
      "Iteration 24930 Training loss 0.060726601630449295 Validation loss 0.05182330682873726 Accuracy 0.862250030040741\n",
      "Iteration 24940 Training loss 0.04934006556868553 Validation loss 0.05187729001045227 Accuracy 0.861875057220459\n",
      "Iteration 24950 Training loss 0.04961128532886505 Validation loss 0.05199456959962845 Accuracy 0.862500011920929\n",
      "Iteration 24960 Training loss 0.04409581050276756 Validation loss 0.051822975277900696 Accuracy 0.8631250262260437\n",
      "Iteration 24970 Training loss 0.056741952896118164 Validation loss 0.051807016134262085 Accuracy 0.8640000224113464\n",
      "Iteration 24980 Training loss 0.0496084950864315 Validation loss 0.0519227534532547 Accuracy 0.8626250624656677\n",
      "Iteration 24990 Training loss 0.04998660832643509 Validation loss 0.051826778799295425 Accuracy 0.8635000586509705\n",
      "Iteration 25000 Training loss 0.051230695098638535 Validation loss 0.052046578377485275 Accuracy 0.861875057220459\n",
      "Iteration 25010 Training loss 0.04149812459945679 Validation loss 0.051814302802085876 Accuracy 0.862500011920929\n",
      "Iteration 25020 Training loss 0.04850177839398384 Validation loss 0.05177536979317665 Accuracy 0.8632500171661377\n",
      "Iteration 25030 Training loss 0.04929402098059654 Validation loss 0.051918551325798035 Accuracy 0.862250030040741\n",
      "Iteration 25040 Training loss 0.05549544095993042 Validation loss 0.05193132907152176 Accuracy 0.862000048160553\n",
      "Iteration 25050 Training loss 0.05585285648703575 Validation loss 0.05183761194348335 Accuracy 0.8628750443458557\n",
      "Iteration 25060 Training loss 0.04851745441555977 Validation loss 0.05181334912776947 Accuracy 0.8637500405311584\n",
      "Iteration 25070 Training loss 0.04311665892601013 Validation loss 0.051772698760032654 Accuracy 0.8633750677108765\n",
      "Iteration 25080 Training loss 0.051321204751729965 Validation loss 0.05232271924614906 Accuracy 0.8615000247955322\n",
      "Iteration 25090 Training loss 0.05034820735454559 Validation loss 0.05177430808544159 Accuracy 0.8635000586509705\n",
      "Iteration 25100 Training loss 0.05038389191031456 Validation loss 0.05182769522070885 Accuracy 0.8632500171661377\n",
      "Iteration 25110 Training loss 0.043076351284980774 Validation loss 0.051791299134492874 Accuracy 0.8638750314712524\n",
      "Iteration 25120 Training loss 0.04904583469033241 Validation loss 0.05179634317755699 Accuracy 0.8635000586509705\n",
      "Iteration 25130 Training loss 0.05096571147441864 Validation loss 0.05178247392177582 Accuracy 0.8636250495910645\n",
      "Iteration 25140 Training loss 0.05525492504239082 Validation loss 0.05212899670004845 Accuracy 0.8610000610351562\n",
      "Iteration 25150 Training loss 0.042785391211509705 Validation loss 0.05188428610563278 Accuracy 0.862500011920929\n",
      "Iteration 25160 Training loss 0.049048393964767456 Validation loss 0.05180928483605385 Accuracy 0.8635000586509705\n",
      "Iteration 25170 Training loss 0.04990389198064804 Validation loss 0.05174039304256439 Accuracy 0.862375020980835\n",
      "Iteration 25180 Training loss 0.05610664561390877 Validation loss 0.051713116466999054 Accuracy 0.8626250624656677\n",
      "Iteration 25190 Training loss 0.04091845452785492 Validation loss 0.0517193041741848 Accuracy 0.8631250262260437\n",
      "Iteration 25200 Training loss 0.04035717993974686 Validation loss 0.05183248966932297 Accuracy 0.8633750677108765\n",
      "Iteration 25210 Training loss 0.05053965374827385 Validation loss 0.0516827292740345 Accuracy 0.8635000586509705\n",
      "Iteration 25220 Training loss 0.04632196202874184 Validation loss 0.05174203962087631 Accuracy 0.8641250133514404\n",
      "Iteration 25230 Training loss 0.04224403202533722 Validation loss 0.05168452113866806 Accuracy 0.8627500534057617\n",
      "Iteration 25240 Training loss 0.046751100569963455 Validation loss 0.05177650600671768 Accuracy 0.8632500171661377\n",
      "Iteration 25250 Training loss 0.048765409737825394 Validation loss 0.0517544150352478 Accuracy 0.8632500171661377\n",
      "Iteration 25260 Training loss 0.05180354043841362 Validation loss 0.05177881196141243 Accuracy 0.8633750677108765\n",
      "Iteration 25270 Training loss 0.05199269577860832 Validation loss 0.051686063408851624 Accuracy 0.8633750677108765\n",
      "Iteration 25280 Training loss 0.046974554657936096 Validation loss 0.05188027024269104 Accuracy 0.862125039100647\n",
      "Iteration 25290 Training loss 0.03727535530924797 Validation loss 0.0517345666885376 Accuracy 0.8643750548362732\n",
      "Iteration 25300 Training loss 0.044433627277612686 Validation loss 0.05167991295456886 Accuracy 0.8626250624656677\n",
      "Iteration 25310 Training loss 0.05450555309653282 Validation loss 0.05167306587100029 Accuracy 0.8641250133514404\n",
      "Iteration 25320 Training loss 0.057407770305871964 Validation loss 0.05171993747353554 Accuracy 0.8643750548362732\n",
      "Iteration 25330 Training loss 0.0503314808011055 Validation loss 0.051825735718011856 Accuracy 0.8635000586509705\n",
      "Iteration 25340 Training loss 0.061699628829956055 Validation loss 0.051728472113609314 Accuracy 0.8632500171661377\n",
      "Iteration 25350 Training loss 0.05607186257839203 Validation loss 0.051664918661117554 Accuracy 0.8628750443458557\n",
      "Iteration 25360 Training loss 0.04399718716740608 Validation loss 0.051659274846315384 Accuracy 0.8630000352859497\n",
      "Iteration 25370 Training loss 0.05998166278004646 Validation loss 0.05178265646100044 Accuracy 0.8632500171661377\n",
      "Iteration 25380 Training loss 0.04753375053405762 Validation loss 0.05178473889827728 Accuracy 0.8636250495910645\n",
      "Iteration 25390 Training loss 0.05871514230966568 Validation loss 0.0516466461122036 Accuracy 0.862500011920929\n",
      "Iteration 25400 Training loss 0.05348780378699303 Validation loss 0.05165177956223488 Accuracy 0.8630000352859497\n",
      "Iteration 25410 Training loss 0.04342634230852127 Validation loss 0.05169939622282982 Accuracy 0.8633750677108765\n",
      "Iteration 25420 Training loss 0.046261586248874664 Validation loss 0.051670048385858536 Accuracy 0.8636250495910645\n",
      "Iteration 25430 Training loss 0.04967649281024933 Validation loss 0.05162468180060387 Accuracy 0.8638750314712524\n",
      "Iteration 25440 Training loss 0.051051631569862366 Validation loss 0.05163130536675453 Accuracy 0.8633750677108765\n",
      "Iteration 25450 Training loss 0.03512585908174515 Validation loss 0.05176004022359848 Accuracy 0.862500011920929\n",
      "Iteration 25460 Training loss 0.055575210601091385 Validation loss 0.05177348479628563 Accuracy 0.8633750677108765\n",
      "Iteration 25470 Training loss 0.05851438269019127 Validation loss 0.05177254602313042 Accuracy 0.8631250262260437\n",
      "Iteration 25480 Training loss 0.047006674110889435 Validation loss 0.05186384171247482 Accuracy 0.862500011920929\n",
      "Iteration 25490 Training loss 0.049583643674850464 Validation loss 0.051670655608177185 Accuracy 0.8628750443458557\n",
      "Iteration 25500 Training loss 0.03888934105634689 Validation loss 0.05163697898387909 Accuracy 0.8638750314712524\n",
      "Iteration 25510 Training loss 0.05183662474155426 Validation loss 0.05159858986735344 Accuracy 0.8637500405311584\n",
      "Iteration 25520 Training loss 0.06198790296912193 Validation loss 0.05159062147140503 Accuracy 0.8628750443458557\n",
      "Iteration 25530 Training loss 0.05278979241847992 Validation loss 0.05177726224064827 Accuracy 0.8632500171661377\n",
      "Iteration 25540 Training loss 0.05122818052768707 Validation loss 0.05159810557961464 Accuracy 0.8637500405311584\n",
      "Iteration 25550 Training loss 0.040838997811079025 Validation loss 0.051633045077323914 Accuracy 0.8640000224113464\n",
      "Iteration 25560 Training loss 0.047118425369262695 Validation loss 0.051626577973365784 Accuracy 0.8627500534057617\n",
      "Iteration 25570 Training loss 0.048650551587343216 Validation loss 0.05160069465637207 Accuracy 0.8638750314712524\n",
      "Iteration 25580 Training loss 0.054425809532403946 Validation loss 0.051683541387319565 Accuracy 0.8628750443458557\n",
      "Iteration 25590 Training loss 0.04300137609243393 Validation loss 0.05165323242545128 Accuracy 0.8633750677108765\n",
      "Iteration 25600 Training loss 0.05185411497950554 Validation loss 0.05253882333636284 Accuracy 0.8611250519752502\n",
      "Iteration 25610 Training loss 0.054100941866636276 Validation loss 0.051578883081674576 Accuracy 0.8631250262260437\n",
      "Iteration 25620 Training loss 0.04600496590137482 Validation loss 0.051949914544820786 Accuracy 0.8607500195503235\n",
      "Iteration 25630 Training loss 0.05482421815395355 Validation loss 0.05163601413369179 Accuracy 0.8632500171661377\n",
      "Iteration 25640 Training loss 0.04813617467880249 Validation loss 0.051588986068964005 Accuracy 0.8631250262260437\n",
      "Iteration 25650 Training loss 0.05381790176033974 Validation loss 0.051565904170274734 Accuracy 0.8633750677108765\n",
      "Iteration 25660 Training loss 0.047107961028814316 Validation loss 0.05150596797466278 Accuracy 0.8630000352859497\n",
      "Iteration 25670 Training loss 0.05043407902121544 Validation loss 0.05149739980697632 Accuracy 0.8628750443458557\n",
      "Iteration 25680 Training loss 0.051613181829452515 Validation loss 0.05167161673307419 Accuracy 0.8635000586509705\n",
      "Iteration 25690 Training loss 0.04660015180706978 Validation loss 0.0516246072947979 Accuracy 0.862375020980835\n",
      "Iteration 25700 Training loss 0.041605137288570404 Validation loss 0.051594726741313934 Accuracy 0.8626250624656677\n",
      "Iteration 25710 Training loss 0.05216364562511444 Validation loss 0.05182502046227455 Accuracy 0.862500011920929\n",
      "Iteration 25720 Training loss 0.05186837911605835 Validation loss 0.052144601941108704 Accuracy 0.862125039100647\n",
      "Iteration 25730 Training loss 0.04611041769385338 Validation loss 0.05150974541902542 Accuracy 0.8633750677108765\n",
      "Iteration 25740 Training loss 0.05042770132422447 Validation loss 0.051590751856565475 Accuracy 0.8643750548362732\n",
      "Iteration 25750 Training loss 0.04836173728108406 Validation loss 0.05188753083348274 Accuracy 0.8633750677108765\n",
      "Iteration 25760 Training loss 0.04297070577740669 Validation loss 0.05185144394636154 Accuracy 0.8633750677108765\n",
      "Iteration 25770 Training loss 0.0522732138633728 Validation loss 0.051533713936805725 Accuracy 0.8638750314712524\n",
      "Iteration 25780 Training loss 0.05077485740184784 Validation loss 0.051514819264411926 Accuracy 0.8637500405311584\n",
      "Iteration 25790 Training loss 0.054609593003988266 Validation loss 0.05149116739630699 Accuracy 0.8633750677108765\n",
      "Iteration 25800 Training loss 0.04960053041577339 Validation loss 0.05151698365807533 Accuracy 0.8633750677108765\n",
      "Iteration 25810 Training loss 0.04827279970049858 Validation loss 0.05169132351875305 Accuracy 0.862375020980835\n",
      "Iteration 25820 Training loss 0.05239621922373772 Validation loss 0.051523324102163315 Accuracy 0.8640000224113464\n",
      "Iteration 25830 Training loss 0.04997977241873741 Validation loss 0.05147989094257355 Accuracy 0.8636250495910645\n",
      "Iteration 25840 Training loss 0.05261041224002838 Validation loss 0.051453761756420135 Accuracy 0.8641250133514404\n",
      "Iteration 25850 Training loss 0.050967223942279816 Validation loss 0.051545821130275726 Accuracy 0.8636250495910645\n",
      "Iteration 25860 Training loss 0.05116714909672737 Validation loss 0.051569536328315735 Accuracy 0.8636250495910645\n",
      "Iteration 25870 Training loss 0.055643558502197266 Validation loss 0.05150386691093445 Accuracy 0.8633750677108765\n",
      "Iteration 25880 Training loss 0.04314977675676346 Validation loss 0.05145128443837166 Accuracy 0.8641250133514404\n",
      "Iteration 25890 Training loss 0.057230617851018906 Validation loss 0.05145663022994995 Accuracy 0.8632500171661377\n",
      "Iteration 25900 Training loss 0.047186899930238724 Validation loss 0.05166332796216011 Accuracy 0.862500011920929\n",
      "Iteration 25910 Training loss 0.052962664514780045 Validation loss 0.05149015411734581 Accuracy 0.8640000224113464\n",
      "Iteration 25920 Training loss 0.04455361142754555 Validation loss 0.0513874776661396 Accuracy 0.8641250133514404\n",
      "Iteration 25930 Training loss 0.04824718460440636 Validation loss 0.05152727663516998 Accuracy 0.8630000352859497\n",
      "Iteration 25940 Training loss 0.05415807664394379 Validation loss 0.05138924717903137 Accuracy 0.8646250367164612\n",
      "Iteration 25950 Training loss 0.043574023991823196 Validation loss 0.05144783854484558 Accuracy 0.8636250495910645\n",
      "Iteration 25960 Training loss 0.05176771059632301 Validation loss 0.05159398168325424 Accuracy 0.8638750314712524\n",
      "Iteration 25970 Training loss 0.048873331397771835 Validation loss 0.051364459097385406 Accuracy 0.8647500276565552\n",
      "Iteration 25980 Training loss 0.04518960416316986 Validation loss 0.05140748247504234 Accuracy 0.8646250367164612\n",
      "Iteration 25990 Training loss 0.04610147327184677 Validation loss 0.051373548805713654 Accuracy 0.8653750419616699\n",
      "Iteration 26000 Training loss 0.045343805104494095 Validation loss 0.05137254670262337 Accuracy 0.8648750185966492\n",
      "Iteration 26010 Training loss 0.05416406691074371 Validation loss 0.05137782543897629 Accuracy 0.8650000691413879\n",
      "Iteration 26020 Training loss 0.049117643386125565 Validation loss 0.051480960100889206 Accuracy 0.8648750185966492\n",
      "Iteration 26030 Training loss 0.051487404853105545 Validation loss 0.05138842388987541 Accuracy 0.8636250495910645\n",
      "Iteration 26040 Training loss 0.05507602170109749 Validation loss 0.05138318985700607 Accuracy 0.8646250367164612\n",
      "Iteration 26050 Training loss 0.052697353065013885 Validation loss 0.051526278257369995 Accuracy 0.8633750677108765\n",
      "Iteration 26060 Training loss 0.04527679830789566 Validation loss 0.05178079381585121 Accuracy 0.8633750677108765\n",
      "Iteration 26070 Training loss 0.04970358684659004 Validation loss 0.05135329067707062 Accuracy 0.8643750548362732\n",
      "Iteration 26080 Training loss 0.047006066888570786 Validation loss 0.051476601511240005 Accuracy 0.8636250495910645\n",
      "Iteration 26090 Training loss 0.05770609900355339 Validation loss 0.051363978534936905 Accuracy 0.8643750548362732\n",
      "Iteration 26100 Training loss 0.052244871854782104 Validation loss 0.051443278789520264 Accuracy 0.8631250262260437\n",
      "Iteration 26110 Training loss 0.0463499017059803 Validation loss 0.05139783024787903 Accuracy 0.8647500276565552\n",
      "Iteration 26120 Training loss 0.05060224235057831 Validation loss 0.05136191099882126 Accuracy 0.8642500638961792\n",
      "Iteration 26130 Training loss 0.05212463438510895 Validation loss 0.05163273215293884 Accuracy 0.862125039100647\n",
      "Iteration 26140 Training loss 0.04596978425979614 Validation loss 0.0513305738568306 Accuracy 0.8642500638961792\n",
      "Iteration 26150 Training loss 0.049417443573474884 Validation loss 0.05166082829236984 Accuracy 0.8631250262260437\n",
      "Iteration 26160 Training loss 0.05401177331805229 Validation loss 0.05139520391821861 Accuracy 0.8645000457763672\n",
      "Iteration 26170 Training loss 0.04579212889075279 Validation loss 0.05138900876045227 Accuracy 0.8638750314712524\n",
      "Iteration 26180 Training loss 0.04723236337304115 Validation loss 0.05133098363876343 Accuracy 0.8643750548362732\n",
      "Iteration 26190 Training loss 0.04773911461234093 Validation loss 0.051361117511987686 Accuracy 0.8643750548362732\n",
      "Iteration 26200 Training loss 0.04945850372314453 Validation loss 0.05141036584973335 Accuracy 0.8636250495910645\n",
      "Iteration 26210 Training loss 0.04817147180438042 Validation loss 0.05139525979757309 Accuracy 0.8643750548362732\n",
      "Iteration 26220 Training loss 0.04715095832943916 Validation loss 0.05135186016559601 Accuracy 0.8645000457763672\n",
      "Iteration 26230 Training loss 0.0430181510746479 Validation loss 0.051374442875385284 Accuracy 0.8635000586509705\n",
      "Iteration 26240 Training loss 0.04806745424866676 Validation loss 0.05139267072081566 Accuracy 0.8638750314712524\n",
      "Iteration 26250 Training loss 0.04469090327620506 Validation loss 0.05184777081012726 Accuracy 0.8635000586509705\n",
      "Iteration 26260 Training loss 0.05452185496687889 Validation loss 0.051291316747665405 Accuracy 0.8645000457763672\n",
      "Iteration 26270 Training loss 0.04561327025294304 Validation loss 0.05128052085638046 Accuracy 0.8643750548362732\n",
      "Iteration 26280 Training loss 0.043900325894355774 Validation loss 0.05132025480270386 Accuracy 0.8646250367164612\n",
      "Iteration 26290 Training loss 0.04635104909539223 Validation loss 0.05150336027145386 Accuracy 0.8632500171661377\n",
      "Iteration 26300 Training loss 0.04781913757324219 Validation loss 0.0512956865131855 Accuracy 0.8643750548362732\n",
      "Iteration 26310 Training loss 0.04978618025779724 Validation loss 0.05142694339156151 Accuracy 0.8641250133514404\n",
      "Iteration 26320 Training loss 0.051841139793395996 Validation loss 0.051314182579517365 Accuracy 0.8642500638961792\n",
      "Iteration 26330 Training loss 0.05059577524662018 Validation loss 0.05139901116490364 Accuracy 0.8643750548362732\n",
      "Iteration 26340 Training loss 0.05868426337838173 Validation loss 0.051292650401592255 Accuracy 0.8651250600814819\n",
      "Iteration 26350 Training loss 0.04783111438155174 Validation loss 0.05178016051650047 Accuracy 0.8633750677108765\n",
      "Iteration 26360 Training loss 0.0467468686401844 Validation loss 0.051342010498046875 Accuracy 0.8636250495910645\n",
      "Iteration 26370 Training loss 0.048419076949357986 Validation loss 0.051402315497398376 Accuracy 0.8632500171661377\n",
      "Iteration 26380 Training loss 0.04620923846960068 Validation loss 0.05154198035597801 Accuracy 0.8628750443458557\n",
      "Iteration 26390 Training loss 0.04600696638226509 Validation loss 0.051491379737854004 Accuracy 0.8636250495910645\n",
      "Iteration 26400 Training loss 0.04784165695309639 Validation loss 0.051263730973005295 Accuracy 0.8636250495910645\n",
      "Iteration 26410 Training loss 0.05195900797843933 Validation loss 0.05136507749557495 Accuracy 0.8636250495910645\n",
      "Iteration 26420 Training loss 0.04120378941297531 Validation loss 0.051252543926239014 Accuracy 0.8655000329017639\n",
      "Iteration 26430 Training loss 0.04503778740763664 Validation loss 0.05131242051720619 Accuracy 0.8636250495910645\n",
      "Iteration 26440 Training loss 0.05394134297966957 Validation loss 0.05178016424179077 Accuracy 0.862125039100647\n",
      "Iteration 26450 Training loss 0.051851775497198105 Validation loss 0.05168250575661659 Accuracy 0.8628750443458557\n",
      "Iteration 26460 Training loss 0.04920167848467827 Validation loss 0.05120787397027016 Accuracy 0.8647500276565552\n",
      "Iteration 26470 Training loss 0.05176883563399315 Validation loss 0.051305703818798065 Accuracy 0.8638750314712524\n",
      "Iteration 26480 Training loss 0.046840064227581024 Validation loss 0.05145048350095749 Accuracy 0.8632500171661377\n",
      "Iteration 26490 Training loss 0.04786459729075432 Validation loss 0.05117672309279442 Accuracy 0.8650000691413879\n",
      "Iteration 26500 Training loss 0.04940690100193024 Validation loss 0.05122605338692665 Accuracy 0.8638750314712524\n",
      "Iteration 26510 Training loss 0.050127509981393814 Validation loss 0.05129776895046234 Accuracy 0.8637500405311584\n",
      "Iteration 26520 Training loss 0.04445495456457138 Validation loss 0.0512160062789917 Accuracy 0.8647500276565552\n",
      "Iteration 26530 Training loss 0.04988409951329231 Validation loss 0.05127207562327385 Accuracy 0.8636250495910645\n",
      "Iteration 26540 Training loss 0.04959266632795334 Validation loss 0.05139991268515587 Accuracy 0.8637500405311584\n",
      "Iteration 26550 Training loss 0.05991274118423462 Validation loss 0.051277559250593185 Accuracy 0.8630000352859497\n",
      "Iteration 26560 Training loss 0.049564164131879807 Validation loss 0.051467131823301315 Accuracy 0.862500011920929\n",
      "Iteration 26570 Training loss 0.04987950995564461 Validation loss 0.05115819722414017 Accuracy 0.8648750185966492\n",
      "Iteration 26580 Training loss 0.04927223548293114 Validation loss 0.05119284987449646 Accuracy 0.8657500147819519\n",
      "Iteration 26590 Training loss 0.03967426344752312 Validation loss 0.0512176975607872 Accuracy 0.8641250133514404\n",
      "Iteration 26600 Training loss 0.04801124334335327 Validation loss 0.05112380534410477 Accuracy 0.8655000329017639\n",
      "Iteration 26610 Training loss 0.03723037987947464 Validation loss 0.051120005548000336 Accuracy 0.8633750677108765\n",
      "Iteration 26620 Training loss 0.04135308042168617 Validation loss 0.051082223653793335 Accuracy 0.8653750419616699\n",
      "Iteration 26630 Training loss 0.052867479622364044 Validation loss 0.05133510008454323 Accuracy 0.8642500638961792\n",
      "Iteration 26640 Training loss 0.04758589714765549 Validation loss 0.051107294857501984 Accuracy 0.8643750548362732\n",
      "Iteration 26650 Training loss 0.05097854137420654 Validation loss 0.051106344908475876 Accuracy 0.8647500276565552\n",
      "Iteration 26660 Training loss 0.05197596922516823 Validation loss 0.05109965056180954 Accuracy 0.8650000691413879\n",
      "Iteration 26670 Training loss 0.053795523941516876 Validation loss 0.05109759047627449 Accuracy 0.8657500147819519\n",
      "Iteration 26680 Training loss 0.04300906881690025 Validation loss 0.05134090781211853 Accuracy 0.8637500405311584\n",
      "Iteration 26690 Training loss 0.04806462302803993 Validation loss 0.05122694373130798 Accuracy 0.8653750419616699\n",
      "Iteration 26700 Training loss 0.04768700525164604 Validation loss 0.05127275362610817 Accuracy 0.8633750677108765\n",
      "Iteration 26710 Training loss 0.050190214067697525 Validation loss 0.05113444849848747 Accuracy 0.8636250495910645\n",
      "Iteration 26720 Training loss 0.05183641240000725 Validation loss 0.05122129246592522 Accuracy 0.8650000691413879\n",
      "Iteration 26730 Training loss 0.04927925020456314 Validation loss 0.051505107432603836 Accuracy 0.8636250495910645\n",
      "Iteration 26740 Training loss 0.051209867000579834 Validation loss 0.051203928887844086 Accuracy 0.8650000691413879\n",
      "Iteration 26750 Training loss 0.043434351682662964 Validation loss 0.05150756984949112 Accuracy 0.8628750443458557\n",
      "Iteration 26760 Training loss 0.04290899261832237 Validation loss 0.05104943364858627 Accuracy 0.8655000329017639\n",
      "Iteration 26770 Training loss 0.039986565709114075 Validation loss 0.051067616790533066 Accuracy 0.8651250600814819\n",
      "Iteration 26780 Training loss 0.04792490974068642 Validation loss 0.05115356296300888 Accuracy 0.8642500638961792\n",
      "Iteration 26790 Training loss 0.049498338252305984 Validation loss 0.05107911676168442 Accuracy 0.8655000329017639\n",
      "Iteration 26800 Training loss 0.04400761425495148 Validation loss 0.05105903372168541 Accuracy 0.8652500510215759\n",
      "Iteration 26810 Training loss 0.05514506623148918 Validation loss 0.05132920295000076 Accuracy 0.8635000586509705\n",
      "Iteration 26820 Training loss 0.04891326278448105 Validation loss 0.05110689252614975 Accuracy 0.8650000691413879\n",
      "Iteration 26830 Training loss 0.05281955748796463 Validation loss 0.05126626417040825 Accuracy 0.8636250495910645\n",
      "Iteration 26840 Training loss 0.045717451721429825 Validation loss 0.051117055118083954 Accuracy 0.8645000457763672\n",
      "Iteration 26850 Training loss 0.04346897825598717 Validation loss 0.05112744867801666 Accuracy 0.8657500147819519\n",
      "Iteration 26860 Training loss 0.05156494677066803 Validation loss 0.05152653902769089 Accuracy 0.862000048160553\n",
      "Iteration 26870 Training loss 0.04836464673280716 Validation loss 0.05108892172574997 Accuracy 0.8647500276565552\n",
      "Iteration 26880 Training loss 0.04204428568482399 Validation loss 0.05121023580431938 Accuracy 0.8648750185966492\n",
      "Iteration 26890 Training loss 0.05132102966308594 Validation loss 0.05132623389363289 Accuracy 0.8637500405311584\n",
      "Iteration 26900 Training loss 0.0494198352098465 Validation loss 0.05112415552139282 Accuracy 0.8650000691413879\n",
      "Iteration 26910 Training loss 0.05653444305062294 Validation loss 0.05110423266887665 Accuracy 0.8636250495910645\n",
      "Iteration 26920 Training loss 0.058215294033288956 Validation loss 0.05102655664086342 Accuracy 0.8646250367164612\n",
      "Iteration 26930 Training loss 0.04371168091893196 Validation loss 0.05107344686985016 Accuracy 0.8652500510215759\n",
      "Iteration 26940 Training loss 0.04373668506741524 Validation loss 0.05116057023406029 Accuracy 0.8645000457763672\n",
      "Iteration 26950 Training loss 0.048675019294023514 Validation loss 0.05223472788929939 Accuracy 0.8611250519752502\n",
      "Iteration 26960 Training loss 0.05456492304801941 Validation loss 0.05101429671049118 Accuracy 0.8651250600814819\n",
      "Iteration 26970 Training loss 0.05897710099816322 Validation loss 0.051001887768507004 Accuracy 0.8657500147819519\n",
      "Iteration 26980 Training loss 0.0496007464826107 Validation loss 0.05133463069796562 Accuracy 0.8626250624656677\n",
      "Iteration 26990 Training loss 0.040539950132369995 Validation loss 0.051023218780756 Accuracy 0.8638750314712524\n",
      "Iteration 27000 Training loss 0.04867887496948242 Validation loss 0.050966035574674606 Accuracy 0.8645000457763672\n",
      "Iteration 27010 Training loss 0.039785269647836685 Validation loss 0.05105945095419884 Accuracy 0.8657500147819519\n",
      "Iteration 27020 Training loss 0.05219954997301102 Validation loss 0.05138954520225525 Accuracy 0.8637500405311584\n",
      "Iteration 27030 Training loss 0.05480263754725456 Validation loss 0.050986986607313156 Accuracy 0.8652500510215759\n",
      "Iteration 27040 Training loss 0.04493863880634308 Validation loss 0.05098474398255348 Accuracy 0.8651250600814819\n",
      "Iteration 27050 Training loss 0.040306758135557175 Validation loss 0.05123814567923546 Accuracy 0.8643750548362732\n",
      "Iteration 27060 Training loss 0.04626268148422241 Validation loss 0.051012035459280014 Accuracy 0.8651250600814819\n",
      "Iteration 27070 Training loss 0.04909973964095116 Validation loss 0.05094185471534729 Accuracy 0.8656250238418579\n",
      "Iteration 27080 Training loss 0.048506371676921844 Validation loss 0.050944436341524124 Accuracy 0.8648750185966492\n",
      "Iteration 27090 Training loss 0.04666849970817566 Validation loss 0.05093935877084732 Accuracy 0.8653750419616699\n",
      "Iteration 27100 Training loss 0.043816667050123215 Validation loss 0.050973981618881226 Accuracy 0.8657500147819519\n",
      "Iteration 27110 Training loss 0.039309825748205185 Validation loss 0.050988491624593735 Accuracy 0.8651250600814819\n",
      "Iteration 27120 Training loss 0.047078292816877365 Validation loss 0.05110006779432297 Accuracy 0.8646250367164612\n",
      "Iteration 27130 Training loss 0.04997127875685692 Validation loss 0.05098381265997887 Accuracy 0.8651250600814819\n",
      "Iteration 27140 Training loss 0.042390499264001846 Validation loss 0.050972238183021545 Accuracy 0.8648750185966492\n",
      "Iteration 27150 Training loss 0.04460156336426735 Validation loss 0.051048021763563156 Accuracy 0.8647500276565552\n",
      "Iteration 27160 Training loss 0.038988154381513596 Validation loss 0.050955288112163544 Accuracy 0.8662500381469727\n",
      "Iteration 27170 Training loss 0.046518225222826004 Validation loss 0.05093434453010559 Accuracy 0.8651250600814819\n",
      "Iteration 27180 Training loss 0.04755103960633278 Validation loss 0.05113675072789192 Accuracy 0.8638750314712524\n",
      "Iteration 27190 Training loss 0.0464203916490078 Validation loss 0.05127827078104019 Accuracy 0.8650000691413879\n",
      "Iteration 27200 Training loss 0.0450056828558445 Validation loss 0.05088364705443382 Accuracy 0.8653750419616699\n",
      "Iteration 27210 Training loss 0.046224191784858704 Validation loss 0.050894610583782196 Accuracy 0.8652500510215759\n",
      "Iteration 27220 Training loss 0.049836166203022 Validation loss 0.05107957869768143 Accuracy 0.8640000224113464\n",
      "Iteration 27230 Training loss 0.05024653300642967 Validation loss 0.0509321466088295 Accuracy 0.8650000691413879\n",
      "Iteration 27240 Training loss 0.0451851487159729 Validation loss 0.05119236186146736 Accuracy 0.8638750314712524\n",
      "Iteration 27250 Training loss 0.05065349489450455 Validation loss 0.050970714539289474 Accuracy 0.8652500510215759\n",
      "Iteration 27260 Training loss 0.052313435822725296 Validation loss 0.05114298313856125 Accuracy 0.8638750314712524\n",
      "Iteration 27270 Training loss 0.04020330682396889 Validation loss 0.0511639304459095 Accuracy 0.8638750314712524\n",
      "Iteration 27280 Training loss 0.05575643852353096 Validation loss 0.05098024383187294 Accuracy 0.8660000562667847\n",
      "Iteration 27290 Training loss 0.05662931501865387 Validation loss 0.05098489299416542 Accuracy 0.8653750419616699\n",
      "Iteration 27300 Training loss 0.05616849660873413 Validation loss 0.05106198042631149 Accuracy 0.8646250367164612\n",
      "Iteration 27310 Training loss 0.03662968426942825 Validation loss 0.05129576474428177 Accuracy 0.8636250495910645\n",
      "Iteration 27320 Training loss 0.05168784037232399 Validation loss 0.05102251097559929 Accuracy 0.8651250600814819\n",
      "Iteration 27330 Training loss 0.05742586776614189 Validation loss 0.05132192373275757 Accuracy 0.8637500405311584\n",
      "Iteration 27340 Training loss 0.057527653872966766 Validation loss 0.051402054727077484 Accuracy 0.862000048160553\n",
      "Iteration 27350 Training loss 0.04527639225125313 Validation loss 0.05119717866182327 Accuracy 0.8636250495910645\n",
      "Iteration 27360 Training loss 0.04958796873688698 Validation loss 0.050889261066913605 Accuracy 0.8656250238418579\n",
      "Iteration 27370 Training loss 0.04660545289516449 Validation loss 0.05088600516319275 Accuracy 0.8658750653266907\n",
      "Iteration 27380 Training loss 0.04944726452231407 Validation loss 0.051377132534980774 Accuracy 0.861750066280365\n",
      "Iteration 27390 Training loss 0.04802697151899338 Validation loss 0.051272373646497726 Accuracy 0.8632500171661377\n",
      "Iteration 27400 Training loss 0.049758508801460266 Validation loss 0.05098939687013626 Accuracy 0.8652500510215759\n",
      "Iteration 27410 Training loss 0.05246683955192566 Validation loss 0.05107143148779869 Accuracy 0.8640000224113464\n",
      "Iteration 27420 Training loss 0.04845808818936348 Validation loss 0.05092926323413849 Accuracy 0.8648750185966492\n",
      "Iteration 27430 Training loss 0.043589625507593155 Validation loss 0.05121244117617607 Accuracy 0.8631250262260437\n",
      "Iteration 27440 Training loss 0.05452960729598999 Validation loss 0.050838273018598557 Accuracy 0.8658750653266907\n",
      "Iteration 27450 Training loss 0.050430964678525925 Validation loss 0.05115462839603424 Accuracy 0.8638750314712524\n",
      "Iteration 27460 Training loss 0.04431818053126335 Validation loss 0.05101752653717995 Accuracy 0.8655000329017639\n",
      "Iteration 27470 Training loss 0.043878406286239624 Validation loss 0.05108683928847313 Accuracy 0.8642500638961792\n",
      "Iteration 27480 Training loss 0.04162224754691124 Validation loss 0.05084966495633125 Accuracy 0.8655000329017639\n",
      "Iteration 27490 Training loss 0.05258768051862717 Validation loss 0.051238518208265305 Accuracy 0.8635000586509705\n",
      "Iteration 27500 Training loss 0.047507550567388535 Validation loss 0.050853244960308075 Accuracy 0.8650000691413879\n",
      "Iteration 27510 Training loss 0.04509410634636879 Validation loss 0.05095742642879486 Accuracy 0.8646250367164612\n",
      "Iteration 27520 Training loss 0.04216720536351204 Validation loss 0.05083784833550453 Accuracy 0.8657500147819519\n",
      "Iteration 27530 Training loss 0.041345272213220596 Validation loss 0.050927773118019104 Accuracy 0.8651250600814819\n",
      "Iteration 27540 Training loss 0.04179198294878006 Validation loss 0.05096405744552612 Accuracy 0.8652500510215759\n",
      "Iteration 27550 Training loss 0.04283018037676811 Validation loss 0.0509687140583992 Accuracy 0.8643750548362732\n",
      "Iteration 27560 Training loss 0.04367241635918617 Validation loss 0.05092143639922142 Accuracy 0.8640000224113464\n",
      "Iteration 27570 Training loss 0.04696713015437126 Validation loss 0.05086560919880867 Accuracy 0.8645000457763672\n",
      "Iteration 27580 Training loss 0.044880617409944534 Validation loss 0.05090566724538803 Accuracy 0.8661250472068787\n",
      "Iteration 27590 Training loss 0.04882313683629036 Validation loss 0.05089876800775528 Accuracy 0.8656250238418579\n",
      "Iteration 27600 Training loss 0.04583078250288963 Validation loss 0.0509532131254673 Accuracy 0.8640000224113464\n",
      "Iteration 27610 Training loss 0.06350652873516083 Validation loss 0.050823431462049484 Accuracy 0.8660000562667847\n",
      "Iteration 27620 Training loss 0.0463610403239727 Validation loss 0.050857819616794586 Accuracy 0.8660000562667847\n",
      "Iteration 27630 Training loss 0.04484172537922859 Validation loss 0.050924986600875854 Accuracy 0.8662500381469727\n",
      "Iteration 27640 Training loss 0.04486404359340668 Validation loss 0.05094463750720024 Accuracy 0.8655000329017639\n",
      "Iteration 27650 Training loss 0.045839957892894745 Validation loss 0.05089754983782768 Accuracy 0.8640000224113464\n",
      "Iteration 27660 Training loss 0.046512868255376816 Validation loss 0.05132215470075607 Accuracy 0.8640000224113464\n",
      "Iteration 27670 Training loss 0.05701381713151932 Validation loss 0.05083771049976349 Accuracy 0.8661250472068787\n",
      "Iteration 27680 Training loss 0.050317343324422836 Validation loss 0.051007404923439026 Accuracy 0.8645000457763672\n",
      "Iteration 27690 Training loss 0.05366509407758713 Validation loss 0.05081494525074959 Accuracy 0.8657500147819519\n",
      "Iteration 27700 Training loss 0.04614397883415222 Validation loss 0.05161410942673683 Accuracy 0.8637500405311584\n",
      "Iteration 27710 Training loss 0.0604875274002552 Validation loss 0.050753988325595856 Accuracy 0.8660000562667847\n",
      "Iteration 27720 Training loss 0.049099188297986984 Validation loss 0.05076710134744644 Accuracy 0.8660000562667847\n",
      "Iteration 27730 Training loss 0.045753199607133865 Validation loss 0.051772940903902054 Accuracy 0.8627500534057617\n",
      "Iteration 27740 Training loss 0.047103289514780045 Validation loss 0.05076080933213234 Accuracy 0.8651250600814819\n",
      "Iteration 27750 Training loss 0.05118582770228386 Validation loss 0.05078161507844925 Accuracy 0.8663750290870667\n",
      "Iteration 27760 Training loss 0.041784659028053284 Validation loss 0.05092090740799904 Accuracy 0.8641250133514404\n",
      "Iteration 27770 Training loss 0.05371715873479843 Validation loss 0.05085103213787079 Accuracy 0.8648750185966492\n",
      "Iteration 27780 Training loss 0.04202971234917641 Validation loss 0.05082391947507858 Accuracy 0.8648750185966492\n",
      "Iteration 27790 Training loss 0.04510252922773361 Validation loss 0.05078783631324768 Accuracy 0.8658750653266907\n",
      "Iteration 27800 Training loss 0.06290259957313538 Validation loss 0.050846587866544724 Accuracy 0.8653750419616699\n",
      "Iteration 27810 Training loss 0.04728478193283081 Validation loss 0.0513073094189167 Accuracy 0.8646250367164612\n",
      "Iteration 27820 Training loss 0.04278144612908363 Validation loss 0.05082283169031143 Accuracy 0.8650000691413879\n",
      "Iteration 27830 Training loss 0.044604651629924774 Validation loss 0.050859756767749786 Accuracy 0.8660000562667847\n",
      "Iteration 27840 Training loss 0.04881076142191887 Validation loss 0.050704989582300186 Accuracy 0.8653750419616699\n",
      "Iteration 27850 Training loss 0.055501971393823624 Validation loss 0.05152016878128052 Accuracy 0.8642500638961792\n",
      "Iteration 27860 Training loss 0.05546801909804344 Validation loss 0.05073757842183113 Accuracy 0.8656250238418579\n",
      "Iteration 27870 Training loss 0.051760587841272354 Validation loss 0.05067775771021843 Accuracy 0.8663750290870667\n",
      "Iteration 27880 Training loss 0.05131153762340546 Validation loss 0.05072567239403725 Accuracy 0.8660000562667847\n",
      "Iteration 27890 Training loss 0.043533310294151306 Validation loss 0.05069343000650406 Accuracy 0.8665000200271606\n",
      "Iteration 27900 Training loss 0.04953176900744438 Validation loss 0.05070456117391586 Accuracy 0.8658750653266907\n",
      "Iteration 27910 Training loss 0.041776444762945175 Validation loss 0.050859563052654266 Accuracy 0.8651250600814819\n",
      "Iteration 27920 Training loss 0.04853232204914093 Validation loss 0.05065513774752617 Accuracy 0.8662500381469727\n",
      "Iteration 27930 Training loss 0.05373593047261238 Validation loss 0.050748832523822784 Accuracy 0.8646250367164612\n",
      "Iteration 27940 Training loss 0.04078100621700287 Validation loss 0.05066759139299393 Accuracy 0.8658750653266907\n",
      "Iteration 27950 Training loss 0.03978320583701134 Validation loss 0.050660088658332825 Accuracy 0.8665000200271606\n",
      "Iteration 27960 Training loss 0.047804106026887894 Validation loss 0.050815608352422714 Accuracy 0.8661250472068787\n",
      "Iteration 27970 Training loss 0.05075082555413246 Validation loss 0.05069070681929588 Accuracy 0.8666250705718994\n",
      "Iteration 27980 Training loss 0.05352235585451126 Validation loss 0.050636228173971176 Accuracy 0.8661250472068787\n",
      "Iteration 27990 Training loss 0.04229726269841194 Validation loss 0.050924722105264664 Accuracy 0.8652500510215759\n",
      "Iteration 28000 Training loss 0.04706476256251335 Validation loss 0.05064288526773453 Accuracy 0.8667500615119934\n",
      "Iteration 28010 Training loss 0.05050479620695114 Validation loss 0.05067458376288414 Accuracy 0.8671250343322754\n",
      "Iteration 28020 Training loss 0.05107670649886131 Validation loss 0.05112878978252411 Accuracy 0.8637500405311584\n",
      "Iteration 28030 Training loss 0.04602811858057976 Validation loss 0.05085653066635132 Accuracy 0.8662500381469727\n",
      "Iteration 28040 Training loss 0.03738142549991608 Validation loss 0.050851717591285706 Accuracy 0.8657500147819519\n",
      "Iteration 28050 Training loss 0.04121715575456619 Validation loss 0.05069303885102272 Accuracy 0.8652500510215759\n",
      "Iteration 28060 Training loss 0.046905163675546646 Validation loss 0.050708286464214325 Accuracy 0.8666250705718994\n",
      "Iteration 28070 Training loss 0.048839833587408066 Validation loss 0.05079608038067818 Accuracy 0.8661250472068787\n",
      "Iteration 28080 Training loss 0.05247541144490242 Validation loss 0.050696924328804016 Accuracy 0.8658750653266907\n",
      "Iteration 28090 Training loss 0.04650930315256119 Validation loss 0.05061287805438042 Accuracy 0.8672500252723694\n",
      "Iteration 28100 Training loss 0.05084481090307236 Validation loss 0.050604045391082764 Accuracy 0.8665000200271606\n",
      "Iteration 28110 Training loss 0.04061258211731911 Validation loss 0.0505998395383358 Accuracy 0.8661250472068787\n",
      "Iteration 28120 Training loss 0.05113692954182625 Validation loss 0.05075482279062271 Accuracy 0.8647500276565552\n",
      "Iteration 28130 Training loss 0.052333611994981766 Validation loss 0.05059904232621193 Accuracy 0.8672500252723694\n",
      "Iteration 28140 Training loss 0.03989898040890694 Validation loss 0.05066458880901337 Accuracy 0.8675000667572021\n",
      "Iteration 28150 Training loss 0.04637187719345093 Validation loss 0.0506972074508667 Accuracy 0.8670000433921814\n",
      "Iteration 28160 Training loss 0.04120596498250961 Validation loss 0.05100373178720474 Accuracy 0.8661250472068787\n",
      "Iteration 28170 Training loss 0.04546409845352173 Validation loss 0.050693903118371964 Accuracy 0.8668750524520874\n",
      "Iteration 28180 Training loss 0.051928263157606125 Validation loss 0.05074309557676315 Accuracy 0.8665000200271606\n",
      "Iteration 28190 Training loss 0.049626629799604416 Validation loss 0.05064884573221207 Accuracy 0.8653750419616699\n",
      "Iteration 28200 Training loss 0.054156165570020676 Validation loss 0.05081569030880928 Accuracy 0.8661250472068787\n",
      "Iteration 28210 Training loss 0.04047868773341179 Validation loss 0.05060160905122757 Accuracy 0.8656250238418579\n",
      "Iteration 28220 Training loss 0.047226179391145706 Validation loss 0.050694726407527924 Accuracy 0.8652500510215759\n",
      "Iteration 28230 Training loss 0.04180426523089409 Validation loss 0.05065307393670082 Accuracy 0.8672500252723694\n",
      "Iteration 28240 Training loss 0.05123616009950638 Validation loss 0.05099012330174446 Accuracy 0.8628750443458557\n",
      "Iteration 28250 Training loss 0.04233879595994949 Validation loss 0.050572264939546585 Accuracy 0.8660000562667847\n",
      "Iteration 28260 Training loss 0.04023444652557373 Validation loss 0.05056067928671837 Accuracy 0.8667500615119934\n",
      "Iteration 28270 Training loss 0.047279518097639084 Validation loss 0.0506129115819931 Accuracy 0.8665000200271606\n",
      "Iteration 28280 Training loss 0.04873758554458618 Validation loss 0.050653908401727676 Accuracy 0.8658750653266907\n",
      "Iteration 28290 Training loss 0.053399305790662766 Validation loss 0.05063130706548691 Accuracy 0.8673750162124634\n",
      "Iteration 28300 Training loss 0.058294426649808884 Validation loss 0.05081108957529068 Accuracy 0.8655000329017639\n",
      "Iteration 28310 Training loss 0.04622148349881172 Validation loss 0.050572190433740616 Accuracy 0.8673750162124634\n",
      "Iteration 28320 Training loss 0.04650646075606346 Validation loss 0.050520624965429306 Accuracy 0.8665000200271606\n",
      "Iteration 28330 Training loss 0.04771241173148155 Validation loss 0.0507720448076725 Accuracy 0.8663750290870667\n",
      "Iteration 28340 Training loss 0.049731653183698654 Validation loss 0.0506526380777359 Accuracy 0.8657500147819519\n",
      "Iteration 28350 Training loss 0.05552470684051514 Validation loss 0.05088163912296295 Accuracy 0.8666250705718994\n",
      "Iteration 28360 Training loss 0.045768335461616516 Validation loss 0.05051460862159729 Accuracy 0.8687500357627869\n",
      "Iteration 28370 Training loss 0.04253308102488518 Validation loss 0.05094693973660469 Accuracy 0.8656250238418579\n",
      "Iteration 28380 Training loss 0.04388531669974327 Validation loss 0.05047913268208504 Accuracy 0.8680000305175781\n",
      "Iteration 28390 Training loss 0.05742167681455612 Validation loss 0.05053222179412842 Accuracy 0.8671250343322754\n",
      "Iteration 28400 Training loss 0.04488882049918175 Validation loss 0.05056034028530121 Accuracy 0.8672500252723694\n",
      "Iteration 28410 Training loss 0.05442512780427933 Validation loss 0.05046199634671211 Accuracy 0.8673750162124634\n",
      "Iteration 28420 Training loss 0.03963566944003105 Validation loss 0.05051148310303688 Accuracy 0.8671250343322754\n",
      "Iteration 28430 Training loss 0.04366043955087662 Validation loss 0.050492435693740845 Accuracy 0.8672500252723694\n",
      "Iteration 28440 Training loss 0.04370943456888199 Validation loss 0.05059892684221268 Accuracy 0.8670000433921814\n",
      "Iteration 28450 Training loss 0.03588264808058739 Validation loss 0.050465814769268036 Accuracy 0.8668750524520874\n",
      "Iteration 28460 Training loss 0.04547892510890961 Validation loss 0.050523050129413605 Accuracy 0.8668750524520874\n",
      "Iteration 28470 Training loss 0.04416915774345398 Validation loss 0.0505690723657608 Accuracy 0.8672500252723694\n",
      "Iteration 28480 Training loss 0.056571029126644135 Validation loss 0.050512537360191345 Accuracy 0.8675000667572021\n",
      "Iteration 28490 Training loss 0.04403848573565483 Validation loss 0.05056074261665344 Accuracy 0.8658750653266907\n",
      "Iteration 28500 Training loss 0.040203265845775604 Validation loss 0.050461556762456894 Accuracy 0.8670000433921814\n",
      "Iteration 28510 Training loss 0.05392194166779518 Validation loss 0.05070074647665024 Accuracy 0.8638750314712524\n",
      "Iteration 28520 Training loss 0.0518643856048584 Validation loss 0.050617121160030365 Accuracy 0.8648750185966492\n",
      "Iteration 28530 Training loss 0.05076289921998978 Validation loss 0.050422873347997665 Accuracy 0.8670000433921814\n",
      "Iteration 28540 Training loss 0.047768499702215195 Validation loss 0.050511304289102554 Accuracy 0.8660000562667847\n",
      "Iteration 28550 Training loss 0.0565938875079155 Validation loss 0.050390321761369705 Accuracy 0.8678750395774841\n",
      "Iteration 28560 Training loss 0.050323743373155594 Validation loss 0.05041130259633064 Accuracy 0.8677500486373901\n",
      "Iteration 28570 Training loss 0.0523005872964859 Validation loss 0.0505281463265419 Accuracy 0.8668750524520874\n",
      "Iteration 28580 Training loss 0.05069582164287567 Validation loss 0.050431422889232635 Accuracy 0.8665000200271606\n",
      "Iteration 28590 Training loss 0.048455409705638885 Validation loss 0.050546687096357346 Accuracy 0.8655000329017639\n",
      "Iteration 28600 Training loss 0.04721830412745476 Validation loss 0.050578538328409195 Accuracy 0.8658750653266907\n",
      "Iteration 28610 Training loss 0.0448756106197834 Validation loss 0.05056087672710419 Accuracy 0.8668750524520874\n",
      "Iteration 28620 Training loss 0.0445474311709404 Validation loss 0.05038050562143326 Accuracy 0.8668750524520874\n",
      "Iteration 28630 Training loss 0.043070316314697266 Validation loss 0.05040310323238373 Accuracy 0.8670000433921814\n",
      "Iteration 28640 Training loss 0.043781936168670654 Validation loss 0.05050364136695862 Accuracy 0.8673750162124634\n",
      "Iteration 28650 Training loss 0.0465841181576252 Validation loss 0.05041857063770294 Accuracy 0.8675000667572021\n",
      "Iteration 28660 Training loss 0.04860846325755119 Validation loss 0.050402186810970306 Accuracy 0.8681250214576721\n",
      "Iteration 28670 Training loss 0.04868033155798912 Validation loss 0.050394438207149506 Accuracy 0.8667500615119934\n",
      "Iteration 28680 Training loss 0.048679009079933167 Validation loss 0.050571221858263016 Accuracy 0.8658750653266907\n",
      "Iteration 28690 Training loss 0.050978031009435654 Validation loss 0.05041133239865303 Accuracy 0.8678750395774841\n",
      "Iteration 28700 Training loss 0.05076197534799576 Validation loss 0.050448060035705566 Accuracy 0.8665000200271606\n",
      "Iteration 28710 Training loss 0.048347968608140945 Validation loss 0.05037938058376312 Accuracy 0.8678750395774841\n",
      "Iteration 28720 Training loss 0.04448819160461426 Validation loss 0.050471000373363495 Accuracy 0.8682500123977661\n",
      "Iteration 28730 Training loss 0.05311640724539757 Validation loss 0.05044789984822273 Accuracy 0.8676250576972961\n",
      "Iteration 28740 Training loss 0.046673670411109924 Validation loss 0.050473157316446304 Accuracy 0.8656250238418579\n",
      "Iteration 28750 Training loss 0.045663654804229736 Validation loss 0.050376035273075104 Accuracy 0.8677500486373901\n",
      "Iteration 28760 Training loss 0.047014884650707245 Validation loss 0.0509641058743 Accuracy 0.8647500276565552\n",
      "Iteration 28770 Training loss 0.05107437074184418 Validation loss 0.050573088228702545 Accuracy 0.8666250705718994\n",
      "Iteration 28780 Training loss 0.05308568850159645 Validation loss 0.050326310098171234 Accuracy 0.8671250343322754\n",
      "Iteration 28790 Training loss 0.04151124507188797 Validation loss 0.05035887286067009 Accuracy 0.8673750162124634\n",
      "Iteration 28800 Training loss 0.0569915771484375 Validation loss 0.05035590007901192 Accuracy 0.8680000305175781\n",
      "Iteration 28810 Training loss 0.05392534285783768 Validation loss 0.050344325602054596 Accuracy 0.8678750395774841\n",
      "Iteration 28820 Training loss 0.03860343247652054 Validation loss 0.05050346627831459 Accuracy 0.8675000667572021\n",
      "Iteration 28830 Training loss 0.04533139243721962 Validation loss 0.05127142742276192 Accuracy 0.8647500276565552\n",
      "Iteration 28840 Training loss 0.044208768755197525 Validation loss 0.05035989359021187 Accuracy 0.8682500123977661\n",
      "Iteration 28850 Training loss 0.04003698751330376 Validation loss 0.05045349895954132 Accuracy 0.8671250343322754\n",
      "Iteration 28860 Training loss 0.04254809394478798 Validation loss 0.05028171092271805 Accuracy 0.8680000305175781\n",
      "Iteration 28870 Training loss 0.040714800357818604 Validation loss 0.05027400329709053 Accuracy 0.8685000538825989\n",
      "Iteration 28880 Training loss 0.04900599271059036 Validation loss 0.05024202913045883 Accuracy 0.8690000176429749\n",
      "Iteration 28890 Training loss 0.04189961776137352 Validation loss 0.05030366778373718 Accuracy 0.8678750395774841\n",
      "Iteration 28900 Training loss 0.0474916435778141 Validation loss 0.05107078328728676 Accuracy 0.8658750653266907\n",
      "Iteration 28910 Training loss 0.052137039601802826 Validation loss 0.0502978079020977 Accuracy 0.8678750395774841\n",
      "Iteration 28920 Training loss 0.045664120465517044 Validation loss 0.050301164388656616 Accuracy 0.8675000667572021\n",
      "Iteration 28930 Training loss 0.05273885279893875 Validation loss 0.05030816048383713 Accuracy 0.8676250576972961\n",
      "Iteration 28940 Training loss 0.04448306933045387 Validation loss 0.05039402097463608 Accuracy 0.8671250343322754\n",
      "Iteration 28950 Training loss 0.05951869860291481 Validation loss 0.05034814029932022 Accuracy 0.8667500615119934\n",
      "Iteration 28960 Training loss 0.04879390075802803 Validation loss 0.05031084641814232 Accuracy 0.8662500381469727\n",
      "Iteration 28970 Training loss 0.04538591206073761 Validation loss 0.05035433545708656 Accuracy 0.8671250343322754\n",
      "Iteration 28980 Training loss 0.043192308396101 Validation loss 0.0503205768764019 Accuracy 0.8666250705718994\n",
      "Iteration 28990 Training loss 0.04743044823408127 Validation loss 0.05030052363872528 Accuracy 0.8676250576972961\n",
      "Iteration 29000 Training loss 0.0478285476565361 Validation loss 0.05028562620282173 Accuracy 0.8680000305175781\n",
      "Iteration 29010 Training loss 0.059460777789354324 Validation loss 0.05031198263168335 Accuracy 0.8671250343322754\n",
      "Iteration 29020 Training loss 0.05562613904476166 Validation loss 0.050456706434488297 Accuracy 0.8672500252723694\n",
      "Iteration 29030 Training loss 0.04334506019949913 Validation loss 0.05028262734413147 Accuracy 0.8680000305175781\n",
      "Iteration 29040 Training loss 0.046756647527217865 Validation loss 0.0506444051861763 Accuracy 0.8662500381469727\n",
      "Iteration 29050 Training loss 0.05423319339752197 Validation loss 0.05036517605185509 Accuracy 0.8675000667572021\n",
      "Iteration 29060 Training loss 0.047956470400094986 Validation loss 0.050384730100631714 Accuracy 0.8672500252723694\n",
      "Iteration 29070 Training loss 0.05598278343677521 Validation loss 0.05057704821228981 Accuracy 0.8666250705718994\n",
      "Iteration 29080 Training loss 0.041417840868234634 Validation loss 0.05034251883625984 Accuracy 0.8673750162124634\n",
      "Iteration 29090 Training loss 0.03935901075601578 Validation loss 0.05033735930919647 Accuracy 0.8668750524520874\n",
      "Iteration 29100 Training loss 0.04138088971376419 Validation loss 0.05031171068549156 Accuracy 0.8671250343322754\n",
      "Iteration 29110 Training loss 0.04955591633915901 Validation loss 0.050465237349271774 Accuracy 0.8676250576972961\n",
      "Iteration 29120 Training loss 0.048585906624794006 Validation loss 0.05045527219772339 Accuracy 0.8660000562667847\n",
      "Iteration 29130 Training loss 0.04671677574515343 Validation loss 0.05027342587709427 Accuracy 0.8680000305175781\n",
      "Iteration 29140 Training loss 0.0423285998404026 Validation loss 0.050235699862241745 Accuracy 0.8682500123977661\n",
      "Iteration 29150 Training loss 0.04360764101147652 Validation loss 0.05042245611548424 Accuracy 0.8668750524520874\n",
      "Iteration 29160 Training loss 0.0499972440302372 Validation loss 0.05060858279466629 Accuracy 0.8660000562667847\n",
      "Iteration 29170 Training loss 0.05477086827158928 Validation loss 0.05047304928302765 Accuracy 0.8668750524520874\n",
      "Iteration 29180 Training loss 0.03740890324115753 Validation loss 0.050499483942985535 Accuracy 0.8670000433921814\n",
      "Iteration 29190 Training loss 0.04399007558822632 Validation loss 0.05025028437376022 Accuracy 0.8686250448226929\n",
      "Iteration 29200 Training loss 0.044172100722789764 Validation loss 0.05022953823208809 Accuracy 0.8687500357627869\n",
      "Iteration 29210 Training loss 0.04107336327433586 Validation loss 0.05028267949819565 Accuracy 0.8665000200271606\n",
      "Iteration 29220 Training loss 0.04440625384449959 Validation loss 0.050276871770620346 Accuracy 0.8671250343322754\n",
      "Iteration 29230 Training loss 0.04044385999441147 Validation loss 0.0502653494477272 Accuracy 0.8678750395774841\n",
      "Iteration 29240 Training loss 0.04656078293919563 Validation loss 0.050324007868766785 Accuracy 0.8660000562667847\n",
      "Iteration 29250 Training loss 0.05142466351389885 Validation loss 0.050298430025577545 Accuracy 0.8668750524520874\n",
      "Iteration 29260 Training loss 0.047426749020814896 Validation loss 0.05025414749979973 Accuracy 0.8678750395774841\n",
      "Iteration 29270 Training loss 0.04891632869839668 Validation loss 0.05026562139391899 Accuracy 0.8665000200271606\n",
      "Iteration 29280 Training loss 0.04643673822283745 Validation loss 0.05020378902554512 Accuracy 0.8686250448226929\n",
      "Iteration 29290 Training loss 0.04682965949177742 Validation loss 0.05046658590435982 Accuracy 0.8661250472068787\n",
      "Iteration 29300 Training loss 0.05236268788576126 Validation loss 0.050212617963552475 Accuracy 0.8668750524520874\n",
      "Iteration 29310 Training loss 0.048515308648347855 Validation loss 0.05019429326057434 Accuracy 0.8673750162124634\n",
      "Iteration 29320 Training loss 0.05518653616309166 Validation loss 0.050226885825395584 Accuracy 0.8672500252723694\n",
      "Iteration 29330 Training loss 0.04891530051827431 Validation loss 0.05017685890197754 Accuracy 0.8685000538825989\n",
      "Iteration 29340 Training loss 0.04574356973171234 Validation loss 0.050179820507764816 Accuracy 0.8692500591278076\n",
      "Iteration 29350 Training loss 0.04693220183253288 Validation loss 0.050163302570581436 Accuracy 0.8686250448226929\n",
      "Iteration 29360 Training loss 0.051037974655628204 Validation loss 0.05034423992037773 Accuracy 0.8663750290870667\n",
      "Iteration 29370 Training loss 0.045782286673784256 Validation loss 0.050163935869932175 Accuracy 0.8680000305175781\n",
      "Iteration 29380 Training loss 0.05193376541137695 Validation loss 0.050366420298814774 Accuracy 0.8682500123977661\n",
      "Iteration 29390 Training loss 0.03817848488688469 Validation loss 0.050130587071180344 Accuracy 0.8685000538825989\n",
      "Iteration 29400 Training loss 0.04269370064139366 Validation loss 0.0501512810587883 Accuracy 0.8682500123977661\n",
      "Iteration 29410 Training loss 0.04879068583250046 Validation loss 0.05016764625906944 Accuracy 0.8673750162124634\n",
      "Iteration 29420 Training loss 0.045578110963106155 Validation loss 0.05010191723704338 Accuracy 0.8675000667572021\n",
      "Iteration 29430 Training loss 0.04530256241559982 Validation loss 0.050287097692489624 Accuracy 0.8662500381469727\n",
      "Iteration 29440 Training loss 0.05210479721426964 Validation loss 0.05007254332304001 Accuracy 0.8685000538825989\n",
      "Iteration 29450 Training loss 0.0403897799551487 Validation loss 0.05008343234658241 Accuracy 0.8682500123977661\n",
      "Iteration 29460 Training loss 0.050699956715106964 Validation loss 0.05021427571773529 Accuracy 0.8673750162124634\n",
      "Iteration 29470 Training loss 0.04927199333906174 Validation loss 0.05021991208195686 Accuracy 0.8670000433921814\n",
      "Iteration 29480 Training loss 0.04128658026456833 Validation loss 0.050114694982767105 Accuracy 0.8685000538825989\n",
      "Iteration 29490 Training loss 0.05338194593787193 Validation loss 0.05051812157034874 Accuracy 0.8658750653266907\n",
      "Iteration 29500 Training loss 0.05888766050338745 Validation loss 0.050615016371011734 Accuracy 0.8632500171661377\n",
      "Iteration 29510 Training loss 0.04876463860273361 Validation loss 0.05009178817272186 Accuracy 0.8672500252723694\n",
      "Iteration 29520 Training loss 0.05032970383763313 Validation loss 0.050080880522727966 Accuracy 0.8687500357627869\n",
      "Iteration 29530 Training loss 0.042808130383491516 Validation loss 0.05064735934138298 Accuracy 0.8668750524520874\n",
      "Iteration 29540 Training loss 0.04474479705095291 Validation loss 0.05033290386199951 Accuracy 0.8675000667572021\n",
      "Iteration 29550 Training loss 0.042153749614953995 Validation loss 0.0501258485019207 Accuracy 0.8670000433921814\n",
      "Iteration 29560 Training loss 0.04948972165584564 Validation loss 0.050272975116968155 Accuracy 0.8662500381469727\n",
      "Iteration 29570 Training loss 0.04507274180650711 Validation loss 0.050257351249456406 Accuracy 0.8673750162124634\n",
      "Iteration 29580 Training loss 0.04020193964242935 Validation loss 0.05058746412396431 Accuracy 0.8662500381469727\n",
      "Iteration 29590 Training loss 0.04471760615706444 Validation loss 0.05005183070898056 Accuracy 0.8682500123977661\n",
      "Iteration 29600 Training loss 0.0458843894302845 Validation loss 0.050059858709573746 Accuracy 0.8687500357627869\n",
      "Iteration 29610 Training loss 0.045628275722265244 Validation loss 0.05010102316737175 Accuracy 0.8677500486373901\n",
      "Iteration 29620 Training loss 0.036040689796209335 Validation loss 0.05021478608250618 Accuracy 0.8658750653266907\n",
      "Iteration 29630 Training loss 0.05131494998931885 Validation loss 0.05009216442704201 Accuracy 0.8680000305175781\n",
      "Iteration 29640 Training loss 0.04870140925049782 Validation loss 0.05012356862425804 Accuracy 0.8675000667572021\n",
      "Iteration 29650 Training loss 0.044053979218006134 Validation loss 0.05009147897362709 Accuracy 0.8682500123977661\n",
      "Iteration 29660 Training loss 0.05095761641860008 Validation loss 0.050038307905197144 Accuracy 0.8695000410079956\n",
      "Iteration 29670 Training loss 0.04435458034276962 Validation loss 0.05006934329867363 Accuracy 0.8687500357627869\n",
      "Iteration 29680 Training loss 0.04171547666192055 Validation loss 0.05025535821914673 Accuracy 0.8678750395774841\n",
      "Iteration 29690 Training loss 0.04595368355512619 Validation loss 0.05008435249328613 Accuracy 0.8682500123977661\n",
      "Iteration 29700 Training loss 0.055454835295677185 Validation loss 0.04999595135450363 Accuracy 0.8697500228881836\n",
      "Iteration 29710 Training loss 0.046813104301691055 Validation loss 0.0499904491007328 Accuracy 0.8688750267028809\n",
      "Iteration 29720 Training loss 0.046766314655542374 Validation loss 0.05004534497857094 Accuracy 0.8691250681877136\n",
      "Iteration 29730 Training loss 0.0488450787961483 Validation loss 0.05026772990822792 Accuracy 0.8663750290870667\n",
      "Iteration 29740 Training loss 0.05746918171644211 Validation loss 0.04996079206466675 Accuracy 0.8695000410079956\n",
      "Iteration 29750 Training loss 0.049354199320077896 Validation loss 0.050128206610679626 Accuracy 0.8681250214576721\n",
      "Iteration 29760 Training loss 0.051235608756542206 Validation loss 0.05026108771562576 Accuracy 0.8660000562667847\n",
      "Iteration 29770 Training loss 0.052719488739967346 Validation loss 0.04996013268828392 Accuracy 0.8696250319480896\n",
      "Iteration 29780 Training loss 0.038337551057338715 Validation loss 0.05034342408180237 Accuracy 0.8673750162124634\n",
      "Iteration 29790 Training loss 0.04529157653450966 Validation loss 0.04994082823395729 Accuracy 0.8681250214576721\n",
      "Iteration 29800 Training loss 0.05429317057132721 Validation loss 0.05025564879179001 Accuracy 0.8682500123977661\n",
      "Iteration 29810 Training loss 0.04712922126054764 Validation loss 0.04995999112725258 Accuracy 0.8697500228881836\n",
      "Iteration 29820 Training loss 0.04155413433909416 Validation loss 0.049979519098997116 Accuracy 0.8692500591278076\n",
      "Iteration 29830 Training loss 0.05748153105378151 Validation loss 0.05009876564145088 Accuracy 0.8683750629425049\n",
      "Iteration 29840 Training loss 0.045766446739435196 Validation loss 0.04997529461979866 Accuracy 0.8691250681877136\n",
      "Iteration 29850 Training loss 0.05191687494516373 Validation loss 0.04996873810887337 Accuracy 0.8681250214576721\n",
      "Iteration 29860 Training loss 0.05594668164849281 Validation loss 0.04992283135652542 Accuracy 0.8693750500679016\n",
      "Iteration 29870 Training loss 0.0428566075861454 Validation loss 0.049912240356206894 Accuracy 0.8692500591278076\n",
      "Iteration 29880 Training loss 0.055435530841350555 Validation loss 0.049920082092285156 Accuracy 0.8692500591278076\n",
      "Iteration 29890 Training loss 0.040729276835918427 Validation loss 0.05001053214073181 Accuracy 0.8680000305175781\n",
      "Iteration 29900 Training loss 0.04392743110656738 Validation loss 0.05010027810931206 Accuracy 0.8670000433921814\n",
      "Iteration 29910 Training loss 0.04507480561733246 Validation loss 0.04994874820113182 Accuracy 0.8690000176429749\n",
      "Iteration 29920 Training loss 0.050802797079086304 Validation loss 0.04997361823916435 Accuracy 0.8686250448226929\n",
      "Iteration 29930 Training loss 0.04517132788896561 Validation loss 0.04995564743876457 Accuracy 0.8685000538825989\n",
      "Iteration 29940 Training loss 0.04714201018214226 Validation loss 0.050116755068302155 Accuracy 0.8673750162124634\n",
      "Iteration 29950 Training loss 0.04164566844701767 Validation loss 0.05002974346280098 Accuracy 0.8672500252723694\n",
      "Iteration 29960 Training loss 0.05292782559990883 Validation loss 0.049941424280405045 Accuracy 0.8688750267028809\n",
      "Iteration 29970 Training loss 0.046926967799663544 Validation loss 0.04991902410984039 Accuracy 0.8696250319480896\n",
      "Iteration 29980 Training loss 0.0507417768239975 Validation loss 0.0510089211165905 Accuracy 0.8646250367164612\n",
      "Iteration 29990 Training loss 0.04757130891084671 Validation loss 0.049943070858716965 Accuracy 0.8688750267028809\n",
      "Iteration 30000 Training loss 0.0443638414144516 Validation loss 0.05000186711549759 Accuracy 0.8680000305175781\n",
      "Iteration 30010 Training loss 0.04520739987492561 Validation loss 0.050026100128889084 Accuracy 0.8678750395774841\n",
      "Iteration 30020 Training loss 0.05194399878382683 Validation loss 0.050095949321985245 Accuracy 0.8682500123977661\n",
      "Iteration 30030 Training loss 0.0502854660153389 Validation loss 0.049960389733314514 Accuracy 0.8687500357627869\n",
      "Iteration 30040 Training loss 0.04712678864598274 Validation loss 0.05027179792523384 Accuracy 0.8682500123977661\n",
      "Iteration 30050 Training loss 0.04327134042978287 Validation loss 0.049953754991292953 Accuracy 0.8672500252723694\n",
      "Iteration 30060 Training loss 0.051584843546152115 Validation loss 0.04988939315080643 Accuracy 0.8698750138282776\n",
      "Iteration 30070 Training loss 0.04594840854406357 Validation loss 0.05038653686642647 Accuracy 0.8663750290870667\n",
      "Iteration 30080 Training loss 0.04515208303928375 Validation loss 0.05002908408641815 Accuracy 0.8673750162124634\n",
      "Iteration 30090 Training loss 0.04286831617355347 Validation loss 0.049885209649801254 Accuracy 0.8690000176429749\n",
      "Iteration 30100 Training loss 0.04791492223739624 Validation loss 0.04992825537919998 Accuracy 0.8693750500679016\n",
      "Iteration 30110 Training loss 0.041013214737176895 Validation loss 0.049860090017318726 Accuracy 0.8690000176429749\n",
      "Iteration 30120 Training loss 0.04674225673079491 Validation loss 0.04989178106188774 Accuracy 0.8683750629425049\n",
      "Iteration 30130 Training loss 0.0383375883102417 Validation loss 0.05019370838999748 Accuracy 0.8678750395774841\n",
      "Iteration 30140 Training loss 0.05126523971557617 Validation loss 0.04986311495304108 Accuracy 0.8687500357627869\n",
      "Iteration 30150 Training loss 0.04892222210764885 Validation loss 0.04987654462456703 Accuracy 0.8683750629425049\n",
      "Iteration 30160 Training loss 0.05491182208061218 Validation loss 0.0498475506901741 Accuracy 0.8687500357627869\n",
      "Iteration 30170 Training loss 0.04587018862366676 Validation loss 0.04983644559979439 Accuracy 0.8696250319480896\n",
      "Iteration 30180 Training loss 0.04289579391479492 Validation loss 0.049903180450201035 Accuracy 0.8682500123977661\n",
      "Iteration 30190 Training loss 0.05006112903356552 Validation loss 0.04996350407600403 Accuracy 0.8675000667572021\n",
      "Iteration 30200 Training loss 0.052186835557222366 Validation loss 0.04988778382539749 Accuracy 0.8687500357627869\n",
      "Iteration 30210 Training loss 0.0444486029446125 Validation loss 0.04991587996482849 Accuracy 0.8673750162124634\n",
      "Iteration 30220 Training loss 0.048225633800029755 Validation loss 0.049825262278318405 Accuracy 0.8688750267028809\n",
      "Iteration 30230 Training loss 0.045867402106523514 Validation loss 0.0498243048787117 Accuracy 0.8695000410079956\n",
      "Iteration 30240 Training loss 0.05220832675695419 Validation loss 0.04984322562813759 Accuracy 0.8683750629425049\n",
      "Iteration 30250 Training loss 0.050890058279037476 Validation loss 0.04983418807387352 Accuracy 0.8681250214576721\n",
      "Iteration 30260 Training loss 0.049984175711870193 Validation loss 0.04983391985297203 Accuracy 0.8693750500679016\n",
      "Iteration 30270 Training loss 0.04845232889056206 Validation loss 0.0498703233897686 Accuracy 0.8683750629425049\n",
      "Iteration 30280 Training loss 0.04176460579037666 Validation loss 0.04982319846749306 Accuracy 0.8691250681877136\n",
      "Iteration 30290 Training loss 0.043913524597883224 Validation loss 0.05041949823498726 Accuracy 0.8637500405311584\n",
      "Iteration 30300 Training loss 0.04760563746094704 Validation loss 0.0498533733189106 Accuracy 0.8690000176429749\n",
      "Iteration 30310 Training loss 0.04648219048976898 Validation loss 0.04977898299694061 Accuracy 0.8700000643730164\n",
      "Iteration 30320 Training loss 0.0514749139547348 Validation loss 0.049837496131658554 Accuracy 0.8687500357627869\n",
      "Iteration 30330 Training loss 0.04084373265504837 Validation loss 0.0497405044734478 Accuracy 0.8708750605583191\n",
      "Iteration 30340 Training loss 0.04662076011300087 Validation loss 0.04976517707109451 Accuracy 0.8698750138282776\n",
      "Iteration 30350 Training loss 0.04725608602166176 Validation loss 0.0497860424220562 Accuracy 0.8697500228881836\n",
      "Iteration 30360 Training loss 0.05364900827407837 Validation loss 0.050058793276548386 Accuracy 0.8671250343322754\n",
      "Iteration 30370 Training loss 0.03626814857125282 Validation loss 0.049761056900024414 Accuracy 0.8693750500679016\n",
      "Iteration 30380 Training loss 0.04535285010933876 Validation loss 0.049775224179029465 Accuracy 0.8697500228881836\n",
      "Iteration 30390 Training loss 0.044530630111694336 Validation loss 0.050091877579689026 Accuracy 0.8672500252723694\n",
      "Iteration 30400 Training loss 0.04195624217391014 Validation loss 0.05028460547327995 Accuracy 0.8670000433921814\n",
      "Iteration 30410 Training loss 0.047889575362205505 Validation loss 0.04978073388338089 Accuracy 0.8692500591278076\n",
      "Iteration 30420 Training loss 0.04047404229640961 Validation loss 0.049840979278087616 Accuracy 0.8696250319480896\n",
      "Iteration 30430 Training loss 0.05154816433787346 Validation loss 0.050116166472435 Accuracy 0.8677500486373901\n",
      "Iteration 30440 Training loss 0.04686445742845535 Validation loss 0.0510173961520195 Accuracy 0.8651250600814819\n",
      "Iteration 30450 Training loss 0.0471080020070076 Validation loss 0.049779925495386124 Accuracy 0.8700000643730164\n",
      "Iteration 30460 Training loss 0.04763970896601677 Validation loss 0.04980430752038956 Accuracy 0.8695000410079956\n",
      "Iteration 30470 Training loss 0.04476993903517723 Validation loss 0.04976610094308853 Accuracy 0.8691250681877136\n",
      "Iteration 30480 Training loss 0.044003553688526154 Validation loss 0.049882326275110245 Accuracy 0.8683750629425049\n",
      "Iteration 30490 Training loss 0.05825695022940636 Validation loss 0.04975458234548569 Accuracy 0.8695000410079956\n",
      "Iteration 30500 Training loss 0.04014232009649277 Validation loss 0.049830660223960876 Accuracy 0.8682500123977661\n",
      "Iteration 30510 Training loss 0.04791660234332085 Validation loss 0.04988561570644379 Accuracy 0.8677500486373901\n",
      "Iteration 30520 Training loss 0.05106644332408905 Validation loss 0.04970052093267441 Accuracy 0.8683750629425049\n",
      "Iteration 30530 Training loss 0.040643032640218735 Validation loss 0.04971856623888016 Accuracy 0.8691250681877136\n",
      "Iteration 30540 Training loss 0.044021595269441605 Validation loss 0.04969443753361702 Accuracy 0.8690000176429749\n",
      "Iteration 30550 Training loss 0.04550129175186157 Validation loss 0.04968808591365814 Accuracy 0.8695000410079956\n",
      "Iteration 30560 Training loss 0.043783169239759445 Validation loss 0.049697209149599075 Accuracy 0.8692500591278076\n",
      "Iteration 30570 Training loss 0.049566201865673065 Validation loss 0.049817804247140884 Accuracy 0.8687500357627869\n",
      "Iteration 30580 Training loss 0.046894997358322144 Validation loss 0.04969625174999237 Accuracy 0.8688750267028809\n",
      "Iteration 30590 Training loss 0.04841145873069763 Validation loss 0.04994689300656319 Accuracy 0.8681250214576721\n",
      "Iteration 30600 Training loss 0.04236570745706558 Validation loss 0.049692410975694656 Accuracy 0.8697500228881836\n",
      "Iteration 30610 Training loss 0.05146478861570358 Validation loss 0.050861749798059464 Accuracy 0.8656250238418579\n",
      "Iteration 30620 Training loss 0.04222644865512848 Validation loss 0.04970209300518036 Accuracy 0.8695000410079956\n",
      "Iteration 30630 Training loss 0.050085656344890594 Validation loss 0.04970535635948181 Accuracy 0.8687500357627869\n",
      "Iteration 30640 Training loss 0.04780076816678047 Validation loss 0.049804456532001495 Accuracy 0.8675000667572021\n",
      "Iteration 30650 Training loss 0.04217773303389549 Validation loss 0.04965902864933014 Accuracy 0.8698750138282776\n",
      "Iteration 30660 Training loss 0.04883529618382454 Validation loss 0.049644920974969864 Accuracy 0.8695000410079956\n",
      "Iteration 30670 Training loss 0.0466967336833477 Validation loss 0.04988919198513031 Accuracy 0.8695000410079956\n",
      "Iteration 30680 Training loss 0.043575841933488846 Validation loss 0.04984476789832115 Accuracy 0.8697500228881836\n",
      "Iteration 30690 Training loss 0.05399496108293533 Validation loss 0.04987962543964386 Accuracy 0.8666250705718994\n",
      "Iteration 30700 Training loss 0.04781213775277138 Validation loss 0.04978165030479431 Accuracy 0.8681250214576721\n",
      "Iteration 30710 Training loss 0.04809480533003807 Validation loss 0.04962779954075813 Accuracy 0.8698750138282776\n",
      "Iteration 30720 Training loss 0.038346707820892334 Validation loss 0.049666497856378555 Accuracy 0.8698750138282776\n",
      "Iteration 30730 Training loss 0.050604939460754395 Validation loss 0.04991549253463745 Accuracy 0.8676250576972961\n",
      "Iteration 30740 Training loss 0.04571264982223511 Validation loss 0.04972356557846069 Accuracy 0.8691250681877136\n",
      "Iteration 30750 Training loss 0.03911516070365906 Validation loss 0.049897920340299606 Accuracy 0.8680000305175781\n",
      "Iteration 30760 Training loss 0.051302745938301086 Validation loss 0.05061180517077446 Accuracy 0.8671250343322754\n",
      "Iteration 30770 Training loss 0.05244771018624306 Validation loss 0.04968178644776344 Accuracy 0.8690000176429749\n",
      "Iteration 30780 Training loss 0.045688655227422714 Validation loss 0.049748294055461884 Accuracy 0.8690000176429749\n",
      "Iteration 30790 Training loss 0.0430402047932148 Validation loss 0.04976867884397507 Accuracy 0.8683750629425049\n",
      "Iteration 30800 Training loss 0.0424656867980957 Validation loss 0.04974709078669548 Accuracy 0.8686250448226929\n",
      "Iteration 30810 Training loss 0.05030612647533417 Validation loss 0.05057016387581825 Accuracy 0.8676250576972961\n",
      "Iteration 30820 Training loss 0.05268361419439316 Validation loss 0.05064794421195984 Accuracy 0.8671250343322754\n",
      "Iteration 30830 Training loss 0.04870746657252312 Validation loss 0.05030272528529167 Accuracy 0.8645000457763672\n",
      "Iteration 30840 Training loss 0.0478210486471653 Validation loss 0.04979509860277176 Accuracy 0.8680000305175781\n",
      "Iteration 30850 Training loss 0.03942018747329712 Validation loss 0.049760423600673676 Accuracy 0.8675000667572021\n",
      "Iteration 30860 Training loss 0.04407653212547302 Validation loss 0.049962326884269714 Accuracy 0.8686250448226929\n",
      "Iteration 30870 Training loss 0.04682275652885437 Validation loss 0.04979486018419266 Accuracy 0.8665000200271606\n",
      "Iteration 30880 Training loss 0.04762348160147667 Validation loss 0.04961834475398064 Accuracy 0.8683750629425049\n",
      "Iteration 30890 Training loss 0.04746195673942566 Validation loss 0.04976683109998703 Accuracy 0.8687500357627869\n",
      "Iteration 30900 Training loss 0.046951692551374435 Validation loss 0.049605999141931534 Accuracy 0.8696250319480896\n",
      "Iteration 30910 Training loss 0.049842946231365204 Validation loss 0.049720555543899536 Accuracy 0.8695000410079956\n",
      "Iteration 30920 Training loss 0.051956452429294586 Validation loss 0.049631524831056595 Accuracy 0.8688750267028809\n",
      "Iteration 30930 Training loss 0.03787469118833542 Validation loss 0.04986310005187988 Accuracy 0.8678750395774841\n",
      "Iteration 30940 Training loss 0.04042496904730797 Validation loss 0.04964316263794899 Accuracy 0.8690000176429749\n",
      "Iteration 30950 Training loss 0.0485830195248127 Validation loss 0.04961412772536278 Accuracy 0.8686250448226929\n",
      "Iteration 30960 Training loss 0.04587429389357567 Validation loss 0.049601249396800995 Accuracy 0.8698750138282776\n",
      "Iteration 30970 Training loss 0.05100206658244133 Validation loss 0.04967794194817543 Accuracy 0.8678750395774841\n",
      "Iteration 30980 Training loss 0.042549170553684235 Validation loss 0.05030696466565132 Accuracy 0.8635000586509705\n",
      "Iteration 30990 Training loss 0.04523032531142235 Validation loss 0.04990572854876518 Accuracy 0.8697500228881836\n",
      "Iteration 31000 Training loss 0.053299058228731155 Validation loss 0.0498160719871521 Accuracy 0.8661250472068787\n",
      "Iteration 31010 Training loss 0.05014592036604881 Validation loss 0.049646131694316864 Accuracy 0.8690000176429749\n",
      "Iteration 31020 Training loss 0.04130150377750397 Validation loss 0.04962312430143356 Accuracy 0.8698750138282776\n",
      "Iteration 31030 Training loss 0.0448082871735096 Validation loss 0.04972398281097412 Accuracy 0.8683750629425049\n",
      "Iteration 31040 Training loss 0.03825486823916435 Validation loss 0.04979073628783226 Accuracy 0.8690000176429749\n",
      "Iteration 31050 Training loss 0.04189542680978775 Validation loss 0.0496172197163105 Accuracy 0.8692500591278076\n",
      "Iteration 31060 Training loss 0.04690984636545181 Validation loss 0.0496259480714798 Accuracy 0.8690000176429749\n",
      "Iteration 31070 Training loss 0.054995451122522354 Validation loss 0.04964837804436684 Accuracy 0.8696250319480896\n",
      "Iteration 31080 Training loss 0.04141532629728317 Validation loss 0.05027453228831291 Accuracy 0.8680000305175781\n",
      "Iteration 31090 Training loss 0.04330039769411087 Validation loss 0.05002265423536301 Accuracy 0.8682500123977661\n",
      "Iteration 31100 Training loss 0.03992095962166786 Validation loss 0.0502639040350914 Accuracy 0.8673750162124634\n",
      "Iteration 31110 Training loss 0.04622700437903404 Validation loss 0.049915410578250885 Accuracy 0.8693750500679016\n",
      "Iteration 31120 Training loss 0.043799202889204025 Validation loss 0.04969320073723793 Accuracy 0.8691250681877136\n",
      "Iteration 31130 Training loss 0.044437993317842484 Validation loss 0.04969024285674095 Accuracy 0.8700000643730164\n",
      "Iteration 31140 Training loss 0.04252638667821884 Validation loss 0.049955353140830994 Accuracy 0.8693750500679016\n",
      "Iteration 31150 Training loss 0.0468897745013237 Validation loss 0.04953301325440407 Accuracy 0.8692500591278076\n",
      "Iteration 31160 Training loss 0.04702000692486763 Validation loss 0.04954008013010025 Accuracy 0.8701250553131104\n",
      "Iteration 31170 Training loss 0.04302624240517616 Validation loss 0.04999049752950668 Accuracy 0.8660000562667847\n",
      "Iteration 31180 Training loss 0.04493814706802368 Validation loss 0.049703359603881836 Accuracy 0.8678750395774841\n",
      "Iteration 31190 Training loss 0.04275600239634514 Validation loss 0.04953474923968315 Accuracy 0.8700000643730164\n",
      "Iteration 31200 Training loss 0.04161151871085167 Validation loss 0.04968754202127457 Accuracy 0.8672500252723694\n",
      "Iteration 31210 Training loss 0.051792919635772705 Validation loss 0.04951161891222 Accuracy 0.8688750267028809\n",
      "Iteration 31220 Training loss 0.050275132060050964 Validation loss 0.0494876392185688 Accuracy 0.8700000643730164\n",
      "Iteration 31230 Training loss 0.04477132856845856 Validation loss 0.049577876925468445 Accuracy 0.8692500591278076\n",
      "Iteration 31240 Training loss 0.0383487306535244 Validation loss 0.04950094223022461 Accuracy 0.8702500462532043\n",
      "Iteration 31250 Training loss 0.03652083873748779 Validation loss 0.04958455264568329 Accuracy 0.8690000176429749\n",
      "Iteration 31260 Training loss 0.05331110954284668 Validation loss 0.04972383379936218 Accuracy 0.8670000433921814\n",
      "Iteration 31270 Training loss 0.05428424850106239 Validation loss 0.04947866126894951 Accuracy 0.8707500696182251\n",
      "Iteration 31280 Training loss 0.048365868628025055 Validation loss 0.049454912543296814 Accuracy 0.8703750371932983\n",
      "Iteration 31290 Training loss 0.04574103280901909 Validation loss 0.049463823437690735 Accuracy 0.8706250190734863\n",
      "Iteration 31300 Training loss 0.037287387996912 Validation loss 0.04954046383500099 Accuracy 0.8692500591278076\n",
      "Iteration 31310 Training loss 0.04812576249241829 Validation loss 0.04943850636482239 Accuracy 0.8701250553131104\n",
      "Iteration 31320 Training loss 0.04664578288793564 Validation loss 0.04947778210043907 Accuracy 0.8700000643730164\n",
      "Iteration 31330 Training loss 0.055436328053474426 Validation loss 0.04942474514245987 Accuracy 0.8696250319480896\n",
      "Iteration 31340 Training loss 0.04686063155531883 Validation loss 0.049485884606838226 Accuracy 0.8698750138282776\n",
      "Iteration 31350 Training loss 0.05168847739696503 Validation loss 0.04970892518758774 Accuracy 0.8681250214576721\n",
      "Iteration 31360 Training loss 0.04369441047310829 Validation loss 0.04950815066695213 Accuracy 0.8696250319480896\n",
      "Iteration 31370 Training loss 0.043986670672893524 Validation loss 0.04950053244829178 Accuracy 0.8695000410079956\n",
      "Iteration 31380 Training loss 0.04473455622792244 Validation loss 0.04949158802628517 Accuracy 0.8693750500679016\n",
      "Iteration 31390 Training loss 0.045408643782138824 Validation loss 0.0496547631919384 Accuracy 0.8688750267028809\n",
      "Iteration 31400 Training loss 0.04003497213125229 Validation loss 0.04954368993639946 Accuracy 0.8686250448226929\n",
      "Iteration 31410 Training loss 0.04600710794329643 Validation loss 0.04947312921285629 Accuracy 0.8702500462532043\n",
      "Iteration 31420 Training loss 0.046662092208862305 Validation loss 0.049502428621053696 Accuracy 0.8706250190734863\n",
      "Iteration 31430 Training loss 0.04055047780275345 Validation loss 0.04944920539855957 Accuracy 0.8712500333786011\n",
      "Iteration 31440 Training loss 0.04642626643180847 Validation loss 0.04960658401250839 Accuracy 0.8693750500679016\n",
      "Iteration 31450 Training loss 0.04192645847797394 Validation loss 0.04947346821427345 Accuracy 0.8695000410079956\n",
      "Iteration 31460 Training loss 0.04642830416560173 Validation loss 0.04942961037158966 Accuracy 0.8700000643730164\n",
      "Iteration 31470 Training loss 0.050100214779376984 Validation loss 0.049453120678663254 Accuracy 0.8701250553131104\n",
      "Iteration 31480 Training loss 0.04260769113898277 Validation loss 0.049649689346551895 Accuracy 0.8682500123977661\n",
      "Iteration 31490 Training loss 0.04805576801300049 Validation loss 0.04966093599796295 Accuracy 0.8673750162124634\n",
      "Iteration 31500 Training loss 0.049822527915239334 Validation loss 0.049621399492025375 Accuracy 0.8678750395774841\n",
      "Iteration 31510 Training loss 0.04857302084565163 Validation loss 0.04945986345410347 Accuracy 0.8693750500679016\n",
      "Iteration 31520 Training loss 0.050573185086250305 Validation loss 0.04940063878893852 Accuracy 0.8695000410079956\n",
      "Iteration 31530 Training loss 0.04739425703883171 Validation loss 0.049491528421640396 Accuracy 0.8687500357627869\n",
      "Iteration 31540 Training loss 0.03662396967411041 Validation loss 0.04955922067165375 Accuracy 0.8676250576972961\n",
      "Iteration 31550 Training loss 0.05215878412127495 Validation loss 0.04964849725365639 Accuracy 0.8667500615119934\n",
      "Iteration 31560 Training loss 0.037969160825014114 Validation loss 0.04950911924242973 Accuracy 0.8676250576972961\n",
      "Iteration 31570 Training loss 0.039878711104393005 Validation loss 0.049402181059122086 Accuracy 0.8698750138282776\n",
      "Iteration 31580 Training loss 0.04013899713754654 Validation loss 0.049396153539419174 Accuracy 0.8703750371932983\n",
      "Iteration 31590 Training loss 0.043691229075193405 Validation loss 0.04946941137313843 Accuracy 0.8707500696182251\n",
      "Iteration 31600 Training loss 0.04526380077004433 Validation loss 0.04945871978998184 Accuracy 0.8710000514984131\n",
      "Iteration 31610 Training loss 0.04322507232427597 Validation loss 0.049469783902168274 Accuracy 0.8701250553131104\n",
      "Iteration 31620 Training loss 0.04829464480280876 Validation loss 0.04939817264676094 Accuracy 0.8695000410079956\n",
      "Iteration 31630 Training loss 0.0483323372900486 Validation loss 0.04948623105883598 Accuracy 0.8675000667572021\n",
      "Iteration 31640 Training loss 0.0498264878988266 Validation loss 0.049435924738645554 Accuracy 0.8702500462532043\n",
      "Iteration 31650 Training loss 0.041957635432481766 Validation loss 0.049409326165914536 Accuracy 0.8696250319480896\n",
      "Iteration 31660 Training loss 0.04248903691768646 Validation loss 0.049559444189071655 Accuracy 0.8692500591278076\n",
      "Iteration 31670 Training loss 0.047698725014925 Validation loss 0.049479890614748 Accuracy 0.8691250681877136\n",
      "Iteration 31680 Training loss 0.039557721465826035 Validation loss 0.049331631511449814 Accuracy 0.8701250553131104\n",
      "Iteration 31690 Training loss 0.05113472789525986 Validation loss 0.04936540499329567 Accuracy 0.8702500462532043\n",
      "Iteration 31700 Training loss 0.05073001608252525 Validation loss 0.049342937767505646 Accuracy 0.8706250190734863\n",
      "Iteration 31710 Training loss 0.04746472090482712 Validation loss 0.0498865470290184 Accuracy 0.8698750138282776\n",
      "Iteration 31720 Training loss 0.04221009090542793 Validation loss 0.04946988821029663 Accuracy 0.8695000410079956\n",
      "Iteration 31730 Training loss 0.04541133716702461 Validation loss 0.04933498799800873 Accuracy 0.8697500228881836\n",
      "Iteration 31740 Training loss 0.04811001569032669 Validation loss 0.04970037192106247 Accuracy 0.8673750162124634\n",
      "Iteration 31750 Training loss 0.04769256338477135 Validation loss 0.04934097081422806 Accuracy 0.8703750371932983\n",
      "Iteration 31760 Training loss 0.04872683435678482 Validation loss 0.04931369423866272 Accuracy 0.8700000643730164\n",
      "Iteration 31770 Training loss 0.041824452579021454 Validation loss 0.049370720982551575 Accuracy 0.8691250681877136\n",
      "Iteration 31780 Training loss 0.04971687123179436 Validation loss 0.04932216554880142 Accuracy 0.8696250319480896\n",
      "Iteration 31790 Training loss 0.04280632734298706 Validation loss 0.049330610781908035 Accuracy 0.8712500333786011\n",
      "Iteration 31800 Training loss 0.04361676424741745 Validation loss 0.04958249256014824 Accuracy 0.8701250553131104\n",
      "Iteration 31810 Training loss 0.0400245264172554 Validation loss 0.049453433603048325 Accuracy 0.8701250553131104\n",
      "Iteration 31820 Training loss 0.04706908017396927 Validation loss 0.049573492258787155 Accuracy 0.8696250319480896\n",
      "Iteration 31830 Training loss 0.043115586042404175 Validation loss 0.04933631420135498 Accuracy 0.8697500228881836\n",
      "Iteration 31840 Training loss 0.04457099735736847 Validation loss 0.049379702657461166 Accuracy 0.8685000538825989\n",
      "Iteration 31850 Training loss 0.03999362885951996 Validation loss 0.04939519613981247 Accuracy 0.8702500462532043\n",
      "Iteration 31860 Training loss 0.03972681984305382 Validation loss 0.04926681146025658 Accuracy 0.8696250319480896\n",
      "Iteration 31870 Training loss 0.044513098895549774 Validation loss 0.049483537673950195 Accuracy 0.8688750267028809\n",
      "Iteration 31880 Training loss 0.05103231221437454 Validation loss 0.04941904544830322 Accuracy 0.8698750138282776\n",
      "Iteration 31890 Training loss 0.04680384695529938 Validation loss 0.04940569028258324 Accuracy 0.8700000643730164\n",
      "Iteration 31900 Training loss 0.043203286826610565 Validation loss 0.049345195293426514 Accuracy 0.8693750500679016\n",
      "Iteration 31910 Training loss 0.04857160523533821 Validation loss 0.04936375841498375 Accuracy 0.8696250319480896\n",
      "Iteration 31920 Training loss 0.04968925192952156 Validation loss 0.049944184720516205 Accuracy 0.8685000538825989\n",
      "Iteration 31930 Training loss 0.05109822750091553 Validation loss 0.04942695051431656 Accuracy 0.8675000667572021\n",
      "Iteration 31940 Training loss 0.04539280757308006 Validation loss 0.049715351313352585 Accuracy 0.8665000200271606\n",
      "Iteration 31950 Training loss 0.05002216622233391 Validation loss 0.04943525791168213 Accuracy 0.8686250448226929\n",
      "Iteration 31960 Training loss 0.05085909739136696 Validation loss 0.04937680438160896 Accuracy 0.8688750267028809\n",
      "Iteration 31970 Training loss 0.05478818342089653 Validation loss 0.04940958321094513 Accuracy 0.8702500462532043\n",
      "Iteration 31980 Training loss 0.05790195241570473 Validation loss 0.04945804178714752 Accuracy 0.8687500357627869\n",
      "Iteration 31990 Training loss 0.044065237045288086 Validation loss 0.04928434267640114 Accuracy 0.8705000281333923\n",
      "Iteration 32000 Training loss 0.04259559512138367 Validation loss 0.04919654130935669 Accuracy 0.8715000152587891\n",
      "Iteration 32010 Training loss 0.05026652291417122 Validation loss 0.04923143982887268 Accuracy 0.8706250190734863\n",
      "Iteration 32020 Training loss 0.04727062210440636 Validation loss 0.04949387162923813 Accuracy 0.8698750138282776\n",
      "Iteration 32030 Training loss 0.04261814057826996 Validation loss 0.0494469553232193 Accuracy 0.8680000305175781\n",
      "Iteration 32040 Training loss 0.04292536526918411 Validation loss 0.049696844071149826 Accuracy 0.8668750524520874\n",
      "Iteration 32050 Training loss 0.040524229407310486 Validation loss 0.04917917028069496 Accuracy 0.8712500333786011\n",
      "Iteration 32060 Training loss 0.04509764164686203 Validation loss 0.04922407120466232 Accuracy 0.8707500696182251\n",
      "Iteration 32070 Training loss 0.04443443566560745 Validation loss 0.049226149916648865 Accuracy 0.8712500333786011\n",
      "Iteration 32080 Training loss 0.04710669070482254 Validation loss 0.04927690327167511 Accuracy 0.8706250190734863\n",
      "Iteration 32090 Training loss 0.04637965187430382 Validation loss 0.04941707104444504 Accuracy 0.8683750629425049\n",
      "Iteration 32100 Training loss 0.048136577010154724 Validation loss 0.04919067397713661 Accuracy 0.8702500462532043\n",
      "Iteration 32110 Training loss 0.042792461812496185 Validation loss 0.049172546714544296 Accuracy 0.8710000514984131\n",
      "Iteration 32120 Training loss 0.043125562369823456 Validation loss 0.0492367185652256 Accuracy 0.8706250190734863\n",
      "Iteration 32130 Training loss 0.045544013381004333 Validation loss 0.0492505244910717 Accuracy 0.8700000643730164\n",
      "Iteration 32140 Training loss 0.037224724888801575 Validation loss 0.04924697428941727 Accuracy 0.8702500462532043\n",
      "Iteration 32150 Training loss 0.04595726355910301 Validation loss 0.049281440675258636 Accuracy 0.8702500462532043\n",
      "Iteration 32160 Training loss 0.049175094813108444 Validation loss 0.049230802804231644 Accuracy 0.8711250424385071\n",
      "Iteration 32170 Training loss 0.04363495111465454 Validation loss 0.04957021772861481 Accuracy 0.8672500252723694\n",
      "Iteration 32180 Training loss 0.049818843603134155 Validation loss 0.04922033101320267 Accuracy 0.8705000281333923\n",
      "Iteration 32190 Training loss 0.045151472091674805 Validation loss 0.04938558489084244 Accuracy 0.8690000176429749\n",
      "Iteration 32200 Training loss 0.04583654925227165 Validation loss 0.04920166730880737 Accuracy 0.8700000643730164\n",
      "Iteration 32210 Training loss 0.04118838161230087 Validation loss 0.04916909709572792 Accuracy 0.8705000281333923\n",
      "Iteration 32220 Training loss 0.04067637398838997 Validation loss 0.049186863005161285 Accuracy 0.8703750371932983\n",
      "Iteration 32230 Training loss 0.049257174134254456 Validation loss 0.04977545514702797 Accuracy 0.8682500123977661\n",
      "Iteration 32240 Training loss 0.04443199932575226 Validation loss 0.04909785836935043 Accuracy 0.8703750371932983\n",
      "Iteration 32250 Training loss 0.0421573743224144 Validation loss 0.04910161346197128 Accuracy 0.8710000514984131\n",
      "Iteration 32260 Training loss 0.037553559988737106 Validation loss 0.049167849123477936 Accuracy 0.8698750138282776\n",
      "Iteration 32270 Training loss 0.04440778121352196 Validation loss 0.04917966201901436 Accuracy 0.8703750371932983\n",
      "Iteration 32280 Training loss 0.047550834715366364 Validation loss 0.04929307848215103 Accuracy 0.8701250553131104\n",
      "Iteration 32290 Training loss 0.053160410374403 Validation loss 0.04915826767683029 Accuracy 0.8711250424385071\n",
      "Iteration 32300 Training loss 0.04703177511692047 Validation loss 0.04914749786257744 Accuracy 0.8715000152587891\n",
      "Iteration 32310 Training loss 0.04173508659005165 Validation loss 0.04912082105875015 Accuracy 0.8701250553131104\n",
      "Iteration 32320 Training loss 0.04437969624996185 Validation loss 0.04964180290699005 Accuracy 0.8697500228881836\n",
      "Iteration 32330 Training loss 0.045076511800289154 Validation loss 0.04914895072579384 Accuracy 0.8712500333786011\n",
      "Iteration 32340 Training loss 0.05004628375172615 Validation loss 0.049139536917209625 Accuracy 0.8706250190734863\n",
      "Iteration 32350 Training loss 0.04668360576033592 Validation loss 0.04927707836031914 Accuracy 0.8690000176429749\n",
      "Iteration 32360 Training loss 0.0449172779917717 Validation loss 0.049159035086631775 Accuracy 0.8715000152587891\n",
      "Iteration 32370 Training loss 0.04413488879799843 Validation loss 0.04945606365799904 Accuracy 0.8670000433921814\n",
      "Iteration 32380 Training loss 0.039547257125377655 Validation loss 0.04913359507918358 Accuracy 0.8698750138282776\n",
      "Iteration 32390 Training loss 0.045382097363471985 Validation loss 0.049263596534729004 Accuracy 0.8686250448226929\n",
      "Iteration 32400 Training loss 0.04176332429051399 Validation loss 0.049178194254636765 Accuracy 0.8688750267028809\n",
      "Iteration 32410 Training loss 0.033423617482185364 Validation loss 0.049191005527973175 Accuracy 0.8702500462532043\n",
      "Iteration 32420 Training loss 0.04785468429327011 Validation loss 0.04917578771710396 Accuracy 0.8706250190734863\n",
      "Iteration 32430 Training loss 0.05407532677054405 Validation loss 0.04958001896739006 Accuracy 0.8701250553131104\n",
      "Iteration 32440 Training loss 0.0484066940844059 Validation loss 0.04908649995923042 Accuracy 0.8707500696182251\n",
      "Iteration 32450 Training loss 0.04289165511727333 Validation loss 0.04961341246962547 Accuracy 0.8708750605583191\n",
      "Iteration 32460 Training loss 0.04881800711154938 Validation loss 0.04918928071856499 Accuracy 0.8710000514984131\n",
      "Iteration 32470 Training loss 0.043630607426166534 Validation loss 0.04910924285650253 Accuracy 0.8710000514984131\n",
      "Iteration 32480 Training loss 0.045045290142297745 Validation loss 0.049069181084632874 Accuracy 0.8701250553131104\n",
      "Iteration 32490 Training loss 0.05455901846289635 Validation loss 0.04905252531170845 Accuracy 0.8713750243186951\n",
      "Iteration 32500 Training loss 0.04769362509250641 Validation loss 0.04909645393490791 Accuracy 0.8703750371932983\n",
      "Iteration 32510 Training loss 0.04024917632341385 Validation loss 0.04904238507151604 Accuracy 0.8711250424385071\n",
      "Iteration 32520 Training loss 0.05040182173252106 Validation loss 0.0490436889231205 Accuracy 0.8701250553131104\n",
      "Iteration 32530 Training loss 0.04531949758529663 Validation loss 0.04904945567250252 Accuracy 0.8720000386238098\n",
      "Iteration 32540 Training loss 0.04788563773036003 Validation loss 0.04931027442216873 Accuracy 0.8686250448226929\n",
      "Iteration 32550 Training loss 0.04776381701231003 Validation loss 0.049118731170892715 Accuracy 0.8708750605583191\n",
      "Iteration 32560 Training loss 0.0501658134162426 Validation loss 0.049050942063331604 Accuracy 0.8715000152587891\n",
      "Iteration 32570 Training loss 0.048898305743932724 Validation loss 0.04902211204171181 Accuracy 0.8710000514984131\n",
      "Iteration 32580 Training loss 0.04373099282383919 Validation loss 0.04915944114327431 Accuracy 0.8710000514984131\n",
      "Iteration 32590 Training loss 0.03902733698487282 Validation loss 0.04909123107790947 Accuracy 0.8708750605583191\n",
      "Iteration 32600 Training loss 0.04750822111964226 Validation loss 0.049218371510505676 Accuracy 0.8703750371932983\n",
      "Iteration 32610 Training loss 0.04667384549975395 Validation loss 0.04910169541835785 Accuracy 0.8712500333786011\n",
      "Iteration 32620 Training loss 0.041859474033117294 Validation loss 0.049089450389146805 Accuracy 0.8708750605583191\n",
      "Iteration 32630 Training loss 0.03989503160119057 Validation loss 0.04932329058647156 Accuracy 0.8705000281333923\n",
      "Iteration 32640 Training loss 0.040745023638010025 Validation loss 0.04906383901834488 Accuracy 0.8701250553131104\n",
      "Iteration 32650 Training loss 0.042126283049583435 Validation loss 0.04903849959373474 Accuracy 0.8717500567436218\n",
      "Iteration 32660 Training loss 0.0435919463634491 Validation loss 0.0491066500544548 Accuracy 0.8703750371932983\n",
      "Iteration 32670 Training loss 0.05047076940536499 Validation loss 0.049082208424806595 Accuracy 0.8712500333786011\n",
      "Iteration 32680 Training loss 0.042403072118759155 Validation loss 0.04914884269237518 Accuracy 0.8696250319480896\n",
      "Iteration 32690 Training loss 0.04796759411692619 Validation loss 0.04932539910078049 Accuracy 0.8706250190734863\n",
      "Iteration 32700 Training loss 0.04153810068964958 Validation loss 0.04981747269630432 Accuracy 0.8652500510215759\n",
      "Iteration 32710 Training loss 0.03829857334494591 Validation loss 0.04905546084046364 Accuracy 0.8715000152587891\n",
      "Iteration 32720 Training loss 0.052482202649116516 Validation loss 0.049208298325538635 Accuracy 0.8712500333786011\n",
      "Iteration 32730 Training loss 0.04474920034408569 Validation loss 0.04894925653934479 Accuracy 0.8711250424385071\n",
      "Iteration 32740 Training loss 0.045460257679224014 Validation loss 0.049805909395217896 Accuracy 0.8687500357627869\n",
      "Iteration 32750 Training loss 0.04994329437613487 Validation loss 0.04918334633111954 Accuracy 0.8691250681877136\n",
      "Iteration 32760 Training loss 0.03973877429962158 Validation loss 0.04906124994158745 Accuracy 0.8711250424385071\n",
      "Iteration 32770 Training loss 0.05040867626667023 Validation loss 0.04899272695183754 Accuracy 0.8710000514984131\n",
      "Iteration 32780 Training loss 0.05092252790927887 Validation loss 0.049036070704460144 Accuracy 0.8705000281333923\n",
      "Iteration 32790 Training loss 0.045215606689453125 Validation loss 0.049158334732055664 Accuracy 0.8712500333786011\n",
      "Iteration 32800 Training loss 0.04695666581392288 Validation loss 0.04909820854663849 Accuracy 0.8701250553131104\n",
      "Iteration 32810 Training loss 0.049129389226436615 Validation loss 0.048995569348335266 Accuracy 0.8710000514984131\n",
      "Iteration 32820 Training loss 0.05686641484498978 Validation loss 0.04894133284687996 Accuracy 0.8717500567436218\n",
      "Iteration 32830 Training loss 0.03893977403640747 Validation loss 0.048968493938446045 Accuracy 0.8715000152587891\n",
      "Iteration 32840 Training loss 0.04881502687931061 Validation loss 0.04893568903207779 Accuracy 0.8711250424385071\n",
      "Iteration 32850 Training loss 0.04615537449717522 Validation loss 0.04911272972822189 Accuracy 0.8698750138282776\n",
      "Iteration 32860 Training loss 0.044698379933834076 Validation loss 0.04930077865719795 Accuracy 0.8692500591278076\n",
      "Iteration 32870 Training loss 0.045148514211177826 Validation loss 0.0490877702832222 Accuracy 0.8715000152587891\n",
      "Iteration 32880 Training loss 0.050161972641944885 Validation loss 0.048951730132102966 Accuracy 0.8715000152587891\n",
      "Iteration 32890 Training loss 0.04707883298397064 Validation loss 0.048902906477451324 Accuracy 0.8717500567436218\n",
      "Iteration 32900 Training loss 0.04589464142918587 Validation loss 0.04915817454457283 Accuracy 0.8713750243186951\n",
      "Iteration 32910 Training loss 0.0356692336499691 Validation loss 0.048926182091236115 Accuracy 0.8720000386238098\n",
      "Iteration 32920 Training loss 0.041740067303180695 Validation loss 0.04906882718205452 Accuracy 0.8707500696182251\n",
      "Iteration 32930 Training loss 0.04656614363193512 Validation loss 0.04888666421175003 Accuracy 0.8722500205039978\n",
      "Iteration 32940 Training loss 0.052704665809869766 Validation loss 0.048904795199632645 Accuracy 0.8712500333786011\n",
      "Iteration 32950 Training loss 0.04066799208521843 Validation loss 0.048916809260845184 Accuracy 0.8708750605583191\n",
      "Iteration 32960 Training loss 0.052230510860681534 Validation loss 0.04974942281842232 Accuracy 0.8685000538825989\n",
      "Iteration 32970 Training loss 0.0480438657104969 Validation loss 0.05028148740530014 Accuracy 0.8663750290870667\n",
      "Iteration 32980 Training loss 0.04639584571123123 Validation loss 0.049363140016794205 Accuracy 0.8702500462532043\n",
      "Iteration 32990 Training loss 0.05168521776795387 Validation loss 0.04931849241256714 Accuracy 0.8710000514984131\n",
      "Iteration 33000 Training loss 0.04200316220521927 Validation loss 0.048906825482845306 Accuracy 0.8701250553131104\n",
      "Iteration 33010 Training loss 0.05041983351111412 Validation loss 0.04890289530158043 Accuracy 0.8702500462532043\n",
      "Iteration 33020 Training loss 0.04896407946944237 Validation loss 0.04899607226252556 Accuracy 0.8706250190734863\n",
      "Iteration 33030 Training loss 0.04344099760055542 Validation loss 0.04939195513725281 Accuracy 0.8706250190734863\n",
      "Iteration 33040 Training loss 0.05039570853114128 Validation loss 0.04923529922962189 Accuracy 0.8686250448226929\n",
      "Iteration 33050 Training loss 0.043207380920648575 Validation loss 0.0488879419863224 Accuracy 0.8713750243186951\n",
      "Iteration 33060 Training loss 0.040571101009845734 Validation loss 0.048892028629779816 Accuracy 0.8720000386238098\n",
      "Iteration 33070 Training loss 0.043404631316661835 Validation loss 0.04948744550347328 Accuracy 0.8718750476837158\n",
      "Iteration 33080 Training loss 0.04442324489355087 Validation loss 0.04911387711763382 Accuracy 0.8703750371932983\n",
      "Iteration 33090 Training loss 0.04412126913666725 Validation loss 0.04891284555196762 Accuracy 0.8717500567436218\n",
      "Iteration 33100 Training loss 0.05162570998072624 Validation loss 0.048913005739450455 Accuracy 0.8716250658035278\n",
      "Iteration 33110 Training loss 0.04672779142856598 Validation loss 0.048944659531116486 Accuracy 0.8701250553131104\n",
      "Iteration 33120 Training loss 0.04031888023018837 Validation loss 0.04889901354908943 Accuracy 0.8710000514984131\n",
      "Iteration 33130 Training loss 0.04530857130885124 Validation loss 0.048952218145132065 Accuracy 0.8718750476837158\n",
      "Iteration 33140 Training loss 0.04590413346886635 Validation loss 0.04890965297818184 Accuracy 0.8700000643730164\n",
      "Iteration 33150 Training loss 0.04593544453382492 Validation loss 0.04890747740864754 Accuracy 0.8701250553131104\n",
      "Iteration 33160 Training loss 0.04318510368466377 Validation loss 0.04892774298787117 Accuracy 0.8712500333786011\n",
      "Iteration 33170 Training loss 0.036129359155893326 Validation loss 0.04903656989336014 Accuracy 0.8681250214576721\n",
      "Iteration 33180 Training loss 0.04747088626027107 Validation loss 0.04884021729230881 Accuracy 0.8717500567436218\n",
      "Iteration 33190 Training loss 0.04756684973835945 Validation loss 0.048980455845594406 Accuracy 0.8700000643730164\n",
      "Iteration 33200 Training loss 0.05020812526345253 Validation loss 0.049124088138341904 Accuracy 0.8718750476837158\n",
      "Iteration 33210 Training loss 0.0489315539598465 Validation loss 0.04929261654615402 Accuracy 0.8717500567436218\n",
      "Iteration 33220 Training loss 0.04486100748181343 Validation loss 0.049038439989089966 Accuracy 0.8716250658035278\n",
      "Iteration 33230 Training loss 0.0472651869058609 Validation loss 0.048809122294187546 Accuracy 0.8721250295639038\n",
      "Iteration 33240 Training loss 0.05439719930291176 Validation loss 0.04907281696796417 Accuracy 0.8711250424385071\n",
      "Iteration 33250 Training loss 0.04367758706212044 Validation loss 0.04936223477125168 Accuracy 0.8711250424385071\n",
      "Iteration 33260 Training loss 0.043022263795137405 Validation loss 0.04884892329573631 Accuracy 0.8710000514984131\n",
      "Iteration 33270 Training loss 0.0463985875248909 Validation loss 0.048962634056806564 Accuracy 0.8713750243186951\n",
      "Iteration 33280 Training loss 0.04537727311253548 Validation loss 0.0500430203974247 Accuracy 0.8676250576972961\n",
      "Iteration 33290 Training loss 0.049321968108415604 Validation loss 0.04895123094320297 Accuracy 0.8710000514984131\n",
      "Iteration 33300 Training loss 0.04411550983786583 Validation loss 0.04888175055384636 Accuracy 0.8705000281333923\n",
      "Iteration 33310 Training loss 0.05267318710684776 Validation loss 0.04889430105686188 Accuracy 0.8698750138282776\n",
      "Iteration 33320 Training loss 0.04442736506462097 Validation loss 0.04893847927451134 Accuracy 0.8720000386238098\n",
      "Iteration 33330 Training loss 0.051407866179943085 Validation loss 0.04914766922593117 Accuracy 0.8720000386238098\n",
      "Iteration 33340 Training loss 0.04879537969827652 Validation loss 0.0492975190281868 Accuracy 0.8715000152587891\n",
      "Iteration 33350 Training loss 0.04439827799797058 Validation loss 0.048834558576345444 Accuracy 0.8720000386238098\n",
      "Iteration 33360 Training loss 0.039460863918066025 Validation loss 0.04876480624079704 Accuracy 0.8715000152587891\n",
      "Iteration 33370 Training loss 0.04251755028963089 Validation loss 0.04889470338821411 Accuracy 0.8697500228881836\n",
      "Iteration 33380 Training loss 0.05283114314079285 Validation loss 0.04893431439995766 Accuracy 0.8718750476837158\n",
      "Iteration 33390 Training loss 0.045732803642749786 Validation loss 0.048753708600997925 Accuracy 0.8730000257492065\n",
      "Iteration 33400 Training loss 0.051689162850379944 Validation loss 0.04892893508076668 Accuracy 0.8701250553131104\n",
      "Iteration 33410 Training loss 0.04969193786382675 Validation loss 0.049326688051223755 Accuracy 0.8672500252723694\n",
      "Iteration 33420 Training loss 0.05406804010272026 Validation loss 0.04879559949040413 Accuracy 0.8721250295639038\n",
      "Iteration 33430 Training loss 0.0510094054043293 Validation loss 0.04948262870311737 Accuracy 0.8713750243186951\n",
      "Iteration 33440 Training loss 0.04110186547040939 Validation loss 0.04878045246005058 Accuracy 0.8721250295639038\n",
      "Iteration 33450 Training loss 0.039672065526247025 Validation loss 0.04976717755198479 Accuracy 0.8663750290870667\n",
      "Iteration 33460 Training loss 0.04534490779042244 Validation loss 0.04878830537199974 Accuracy 0.8726250529289246\n",
      "Iteration 33470 Training loss 0.04322631657123566 Validation loss 0.04878642410039902 Accuracy 0.8713750243186951\n",
      "Iteration 33480 Training loss 0.04881640151143074 Validation loss 0.04877045750617981 Accuracy 0.8718750476837158\n",
      "Iteration 33490 Training loss 0.050746120512485504 Validation loss 0.04875612631440163 Accuracy 0.8725000619888306\n",
      "Iteration 33500 Training loss 0.05227968096733093 Validation loss 0.04882150515913963 Accuracy 0.8706250190734863\n",
      "Iteration 33510 Training loss 0.04849814251065254 Validation loss 0.048714231699705124 Accuracy 0.8723750710487366\n",
      "Iteration 33520 Training loss 0.04138942062854767 Validation loss 0.04873514175415039 Accuracy 0.8728750348091125\n",
      "Iteration 33530 Training loss 0.03745577484369278 Validation loss 0.04875338822603226 Accuracy 0.8708750605583191\n",
      "Iteration 33540 Training loss 0.051061466336250305 Validation loss 0.049395617097616196 Accuracy 0.8696250319480896\n",
      "Iteration 33550 Training loss 0.03811971843242645 Validation loss 0.049348361790180206 Accuracy 0.8700000643730164\n",
      "Iteration 33560 Training loss 0.04667382314801216 Validation loss 0.04876984655857086 Accuracy 0.8723750710487366\n",
      "Iteration 33570 Training loss 0.04878357797861099 Validation loss 0.04901451617479324 Accuracy 0.8706250190734863\n",
      "Iteration 33580 Training loss 0.04453303664922714 Validation loss 0.04874609783291817 Accuracy 0.8723750710487366\n",
      "Iteration 33590 Training loss 0.04819324240088463 Validation loss 0.04881894215941429 Accuracy 0.8721250295639038\n",
      "Iteration 33600 Training loss 0.052648548036813736 Validation loss 0.04877243936061859 Accuracy 0.8722500205039978\n",
      "Iteration 33610 Training loss 0.037671659141778946 Validation loss 0.048756130039691925 Accuracy 0.8715000152587891\n",
      "Iteration 33620 Training loss 0.04625605046749115 Validation loss 0.04923178255558014 Accuracy 0.8708750605583191\n",
      "Iteration 33630 Training loss 0.04406481608748436 Validation loss 0.04870268329977989 Accuracy 0.8727500438690186\n",
      "Iteration 33640 Training loss 0.044048357754945755 Validation loss 0.04884755238890648 Accuracy 0.8706250190734863\n",
      "Iteration 33650 Training loss 0.04108479246497154 Validation loss 0.04894118756055832 Accuracy 0.8723750710487366\n",
      "Iteration 33660 Training loss 0.04472685232758522 Validation loss 0.04879426211118698 Accuracy 0.8692500591278076\n",
      "Iteration 33670 Training loss 0.04546770080924034 Validation loss 0.04956910014152527 Accuracy 0.8701250553131104\n",
      "Iteration 33680 Training loss 0.04191751405596733 Validation loss 0.04868645966053009 Accuracy 0.8726250529289246\n",
      "Iteration 33690 Training loss 0.04141481965780258 Validation loss 0.04960353299975395 Accuracy 0.8701250553131104\n",
      "Iteration 33700 Training loss 0.04858298972249031 Validation loss 0.048719700425863266 Accuracy 0.8728750348091125\n",
      "Iteration 33710 Training loss 0.055819071829319 Validation loss 0.04933660477399826 Accuracy 0.8710000514984131\n",
      "Iteration 33720 Training loss 0.043378766626119614 Validation loss 0.048982713371515274 Accuracy 0.8723750710487366\n",
      "Iteration 33730 Training loss 0.04604965075850487 Validation loss 0.04869328439235687 Accuracy 0.8720000386238098\n",
      "Iteration 33740 Training loss 0.04751555249094963 Validation loss 0.04950416460633278 Accuracy 0.8691250681877136\n",
      "Iteration 33750 Training loss 0.04846055060625076 Validation loss 0.049015406519174576 Accuracy 0.8723750710487366\n",
      "Iteration 33760 Training loss 0.045033980160951614 Validation loss 0.048690877854824066 Accuracy 0.8725000619888306\n",
      "Iteration 33770 Training loss 0.04152379930019379 Validation loss 0.04878227412700653 Accuracy 0.8730000257492065\n",
      "Iteration 33780 Training loss 0.049712855368852615 Validation loss 0.04873711243271828 Accuracy 0.8726250529289246\n",
      "Iteration 33790 Training loss 0.042836859822273254 Validation loss 0.04920095205307007 Accuracy 0.8721250295639038\n",
      "Iteration 33800 Training loss 0.04257657378911972 Validation loss 0.048853132873773575 Accuracy 0.8728750348091125\n",
      "Iteration 33810 Training loss 0.04148345813155174 Validation loss 0.04872617498040199 Accuracy 0.8732500672340393\n",
      "Iteration 33820 Training loss 0.03679841384291649 Validation loss 0.048662446439266205 Accuracy 0.8721250295639038\n",
      "Iteration 33830 Training loss 0.04372360184788704 Validation loss 0.048784948885440826 Accuracy 0.8705000281333923\n",
      "Iteration 33840 Training loss 0.04170632362365723 Validation loss 0.048667605966329575 Accuracy 0.8722500205039978\n",
      "Iteration 33850 Training loss 0.04184798523783684 Validation loss 0.04865478724241257 Accuracy 0.8727500438690186\n",
      "Iteration 33860 Training loss 0.048985280096530914 Validation loss 0.04865667596459389 Accuracy 0.8718750476837158\n",
      "Iteration 33870 Training loss 0.05091526731848717 Validation loss 0.05010475218296051 Accuracy 0.8681250214576721\n",
      "Iteration 33880 Training loss 0.03653515875339508 Validation loss 0.048928312957286835 Accuracy 0.8716250658035278\n",
      "Iteration 33890 Training loss 0.05210365355014801 Validation loss 0.049197085201740265 Accuracy 0.8680000305175781\n",
      "Iteration 33900 Training loss 0.04345858842134476 Validation loss 0.04863642528653145 Accuracy 0.8720000386238098\n",
      "Iteration 33910 Training loss 0.051052626222372055 Validation loss 0.048771344125270844 Accuracy 0.8731250166893005\n",
      "Iteration 33920 Training loss 0.053251102566719055 Validation loss 0.0486554279923439 Accuracy 0.8716250658035278\n",
      "Iteration 33930 Training loss 0.04018731042742729 Validation loss 0.04924910143017769 Accuracy 0.8706250190734863\n",
      "Iteration 33940 Training loss 0.04507288709282875 Validation loss 0.04881511256098747 Accuracy 0.8723750710487366\n",
      "Iteration 33950 Training loss 0.046080078929662704 Validation loss 0.049256425350904465 Accuracy 0.8702500462532043\n",
      "Iteration 33960 Training loss 0.04261961951851845 Validation loss 0.04874607175588608 Accuracy 0.8711250424385071\n",
      "Iteration 33970 Training loss 0.05076243355870247 Validation loss 0.04868470877408981 Accuracy 0.8716250658035278\n",
      "Iteration 33980 Training loss 0.04367862269282341 Validation loss 0.048701196908950806 Accuracy 0.8717500567436218\n",
      "Iteration 33990 Training loss 0.04888061806559563 Validation loss 0.04871644079685211 Accuracy 0.8716250658035278\n",
      "Iteration 34000 Training loss 0.04719424992799759 Validation loss 0.04871759191155434 Accuracy 0.8715000152587891\n",
      "Iteration 34010 Training loss 0.043347716331481934 Validation loss 0.04884854704141617 Accuracy 0.8691250681877136\n",
      "Iteration 34020 Training loss 0.04529215767979622 Validation loss 0.04920302703976631 Accuracy 0.8712500333786011\n",
      "Iteration 34030 Training loss 0.046649713069200516 Validation loss 0.049336694180965424 Accuracy 0.8718750476837158\n",
      "Iteration 34040 Training loss 0.05309046804904938 Validation loss 0.04864763095974922 Accuracy 0.8715000152587891\n",
      "Iteration 34050 Training loss 0.047231778502464294 Validation loss 0.04883913695812225 Accuracy 0.8723750710487366\n",
      "Iteration 34060 Training loss 0.0506853312253952 Validation loss 0.04901178926229477 Accuracy 0.8686250448226929\n",
      "Iteration 34070 Training loss 0.04299900308251381 Validation loss 0.04862276464700699 Accuracy 0.8702500462532043\n",
      "Iteration 34080 Training loss 0.044628772884607315 Validation loss 0.04869098216295242 Accuracy 0.8711250424385071\n",
      "Iteration 34090 Training loss 0.04557601734995842 Validation loss 0.04868590086698532 Accuracy 0.8713750243186951\n",
      "Iteration 34100 Training loss 0.035404570400714874 Validation loss 0.048753298819065094 Accuracy 0.8732500672340393\n",
      "Iteration 34110 Training loss 0.04658612981438637 Validation loss 0.04887253791093826 Accuracy 0.8691250681877136\n",
      "Iteration 34120 Training loss 0.042558372020721436 Validation loss 0.04884391650557518 Accuracy 0.8718750476837158\n",
      "Iteration 34130 Training loss 0.04395231604576111 Validation loss 0.04859369993209839 Accuracy 0.8712500333786011\n",
      "Iteration 34140 Training loss 0.04900356009602547 Validation loss 0.04860265552997589 Accuracy 0.8715000152587891\n",
      "Iteration 34150 Training loss 0.050689201802015305 Validation loss 0.04860911890864372 Accuracy 0.8722500205039978\n",
      "Iteration 34160 Training loss 0.038859523832798004 Validation loss 0.04862538352608681 Accuracy 0.8716250658035278\n",
      "Iteration 34170 Training loss 0.044400881975889206 Validation loss 0.04884718731045723 Accuracy 0.8726250529289246\n",
      "Iteration 34180 Training loss 0.03352133557200432 Validation loss 0.04869496449828148 Accuracy 0.8708750605583191\n",
      "Iteration 34190 Training loss 0.050962552428245544 Validation loss 0.048558786511421204 Accuracy 0.8730000257492065\n",
      "Iteration 34200 Training loss 0.05036255717277527 Validation loss 0.04876153543591499 Accuracy 0.8718750476837158\n",
      "Iteration 34210 Training loss 0.03971722349524498 Validation loss 0.04856906458735466 Accuracy 0.8718750476837158\n",
      "Iteration 34220 Training loss 0.043287504464387894 Validation loss 0.04860835149884224 Accuracy 0.8713750243186951\n",
      "Iteration 34230 Training loss 0.044370636343955994 Validation loss 0.04875731095671654 Accuracy 0.8696250319480896\n",
      "Iteration 34240 Training loss 0.03982006385922432 Validation loss 0.04865699261426926 Accuracy 0.8707500696182251\n",
      "Iteration 34250 Training loss 0.04226244240999222 Validation loss 0.048644281923770905 Accuracy 0.8736250400543213\n",
      "Iteration 34260 Training loss 0.041702527552843094 Validation loss 0.0487433485686779 Accuracy 0.8726250529289246\n",
      "Iteration 34270 Training loss 0.04763789102435112 Validation loss 0.048824869096279144 Accuracy 0.8728750348091125\n",
      "Iteration 34280 Training loss 0.0419604629278183 Validation loss 0.04871218651533127 Accuracy 0.8725000619888306\n",
      "Iteration 34290 Training loss 0.04404473677277565 Validation loss 0.04861980676651001 Accuracy 0.8716250658035278\n",
      "Iteration 34300 Training loss 0.045239705592393875 Validation loss 0.048630405217409134 Accuracy 0.8717500567436218\n",
      "Iteration 34310 Training loss 0.04689165949821472 Validation loss 0.04861835017800331 Accuracy 0.8715000152587891\n",
      "Iteration 34320 Training loss 0.04563407972455025 Validation loss 0.04908749461174011 Accuracy 0.8675000667572021\n",
      "Iteration 34330 Training loss 0.044654734432697296 Validation loss 0.04864373803138733 Accuracy 0.8721250295639038\n",
      "Iteration 34340 Training loss 0.04728727415204048 Validation loss 0.04859491437673569 Accuracy 0.8718750476837158\n",
      "Iteration 34350 Training loss 0.04188638553023338 Validation loss 0.04883386939764023 Accuracy 0.8695000410079956\n",
      "Iteration 34360 Training loss 0.045966871082782745 Validation loss 0.048662129789590836 Accuracy 0.8715000152587891\n",
      "Iteration 34370 Training loss 0.04212773218750954 Validation loss 0.04851877689361572 Accuracy 0.8728750348091125\n",
      "Iteration 34380 Training loss 0.047785766422748566 Validation loss 0.048659708350896835 Accuracy 0.8712500333786011\n",
      "Iteration 34390 Training loss 0.04256588965654373 Validation loss 0.05060041323304176 Accuracy 0.8627500534057617\n",
      "Iteration 34400 Training loss 0.04498966038227081 Validation loss 0.04853155463933945 Accuracy 0.8716250658035278\n",
      "Iteration 34410 Training loss 0.05429946631193161 Validation loss 0.048485126346349716 Accuracy 0.8712500333786011\n",
      "Iteration 34420 Training loss 0.046395719051361084 Validation loss 0.04851019009947777 Accuracy 0.8718750476837158\n",
      "Iteration 34430 Training loss 0.04350120574235916 Validation loss 0.04871268570423126 Accuracy 0.8718750476837158\n",
      "Iteration 34440 Training loss 0.046450477093458176 Validation loss 0.048479046672582626 Accuracy 0.8717500567436218\n",
      "Iteration 34450 Training loss 0.052976761013269424 Validation loss 0.04894234240055084 Accuracy 0.8682500123977661\n",
      "Iteration 34460 Training loss 0.034171316772699356 Validation loss 0.04850928112864494 Accuracy 0.8721250295639038\n",
      "Iteration 34470 Training loss 0.04133063182234764 Validation loss 0.048568956553936005 Accuracy 0.8732500672340393\n",
      "Iteration 34480 Training loss 0.0415569469332695 Validation loss 0.04855223745107651 Accuracy 0.8720000386238098\n",
      "Iteration 34490 Training loss 0.045321960002183914 Validation loss 0.04861827194690704 Accuracy 0.8723750710487366\n",
      "Iteration 34500 Training loss 0.051033057272434235 Validation loss 0.048511914908885956 Accuracy 0.8716250658035278\n",
      "Iteration 34510 Training loss 0.05001489818096161 Validation loss 0.04848726838827133 Accuracy 0.8722500205039978\n",
      "Iteration 34520 Training loss 0.046596042811870575 Validation loss 0.04884970188140869 Accuracy 0.8717500567436218\n",
      "Iteration 34530 Training loss 0.04492779076099396 Validation loss 0.04864075407385826 Accuracy 0.8728750348091125\n",
      "Iteration 34540 Training loss 0.037458695471286774 Validation loss 0.04880482330918312 Accuracy 0.8728750348091125\n",
      "Iteration 34550 Training loss 0.04808177053928375 Validation loss 0.048530999571084976 Accuracy 0.8732500672340393\n",
      "Iteration 34560 Training loss 0.040457285940647125 Validation loss 0.04850217327475548 Accuracy 0.8720000386238098\n",
      "Iteration 34570 Training loss 0.04533444344997406 Validation loss 0.048479821532964706 Accuracy 0.8730000257492065\n",
      "Iteration 34580 Training loss 0.03688017651438713 Validation loss 0.048747073858976364 Accuracy 0.8728750348091125\n",
      "Iteration 34590 Training loss 0.04293128103017807 Validation loss 0.04847768694162369 Accuracy 0.8725000619888306\n",
      "Iteration 34600 Training loss 0.03908933699131012 Validation loss 0.048616185784339905 Accuracy 0.8712500333786011\n",
      "Iteration 34610 Training loss 0.043503254652023315 Validation loss 0.04850982502102852 Accuracy 0.8728750348091125\n",
      "Iteration 34620 Training loss 0.043491773307323456 Validation loss 0.048851508647203445 Accuracy 0.8688750267028809\n",
      "Iteration 34630 Training loss 0.047574520111083984 Validation loss 0.048471033573150635 Accuracy 0.8725000619888306\n",
      "Iteration 34640 Training loss 0.04642808809876442 Validation loss 0.048479169607162476 Accuracy 0.8727500438690186\n",
      "Iteration 34650 Training loss 0.0439237542450428 Validation loss 0.04848159849643707 Accuracy 0.8721250295639038\n",
      "Iteration 34660 Training loss 0.043851662427186966 Validation loss 0.048480987548828125 Accuracy 0.874250054359436\n",
      "Iteration 34670 Training loss 0.04202040657401085 Validation loss 0.048633843660354614 Accuracy 0.8728750348091125\n",
      "Iteration 34680 Training loss 0.05146924778819084 Validation loss 0.04896249249577522 Accuracy 0.8731250166893005\n",
      "Iteration 34690 Training loss 0.04903515800833702 Validation loss 0.04852453991770744 Accuracy 0.8731250166893005\n",
      "Iteration 34700 Training loss 0.042672932147979736 Validation loss 0.048490218818187714 Accuracy 0.8720000386238098\n",
      "Iteration 34710 Training loss 0.04285191372036934 Validation loss 0.04852686822414398 Accuracy 0.8737500309944153\n",
      "Iteration 34720 Training loss 0.04265850782394409 Validation loss 0.04844294860959053 Accuracy 0.8728750348091125\n",
      "Iteration 34730 Training loss 0.043652743101119995 Validation loss 0.048455122858285904 Accuracy 0.8731250166893005\n",
      "Iteration 34740 Training loss 0.04189109802246094 Validation loss 0.048753321170806885 Accuracy 0.8727500438690186\n",
      "Iteration 34750 Training loss 0.04134088754653931 Validation loss 0.048489343374967575 Accuracy 0.8720000386238098\n",
      "Iteration 34760 Training loss 0.04776914417743683 Validation loss 0.049220941960811615 Accuracy 0.8723750710487366\n",
      "Iteration 34770 Training loss 0.04360797628760338 Validation loss 0.04854602739214897 Accuracy 0.8711250424385071\n",
      "Iteration 34780 Training loss 0.04713486507534981 Validation loss 0.04974496364593506 Accuracy 0.8696250319480896\n",
      "Iteration 34790 Training loss 0.05352930352091789 Validation loss 0.048458244651556015 Accuracy 0.8722500205039978\n",
      "Iteration 34800 Training loss 0.04594655707478523 Validation loss 0.04843809828162193 Accuracy 0.8722500205039978\n",
      "Iteration 34810 Training loss 0.05309172347187996 Validation loss 0.04972830042243004 Accuracy 0.8701250553131104\n",
      "Iteration 34820 Training loss 0.04918039217591286 Validation loss 0.04835541546344757 Accuracy 0.8730000257492065\n",
      "Iteration 34830 Training loss 0.05450303480029106 Validation loss 0.048668283969163895 Accuracy 0.8737500309944153\n",
      "Iteration 34840 Training loss 0.04464014992117882 Validation loss 0.04836030304431915 Accuracy 0.8732500672340393\n",
      "Iteration 34850 Training loss 0.04228091984987259 Validation loss 0.04848857969045639 Accuracy 0.8738750219345093\n",
      "Iteration 34860 Training loss 0.04083189368247986 Validation loss 0.04835400730371475 Accuracy 0.8740000128746033\n",
      "Iteration 34870 Training loss 0.04710569232702255 Validation loss 0.04851846396923065 Accuracy 0.8731250166893005\n",
      "Iteration 34880 Training loss 0.042694032192230225 Validation loss 0.04838643595576286 Accuracy 0.8735000491142273\n",
      "Iteration 34890 Training loss 0.03697468340396881 Validation loss 0.04843728989362717 Accuracy 0.8732500672340393\n",
      "Iteration 34900 Training loss 0.04278995469212532 Validation loss 0.04839315637946129 Accuracy 0.874250054359436\n",
      "Iteration 34910 Training loss 0.04535401239991188 Validation loss 0.04851919040083885 Accuracy 0.8710000514984131\n",
      "Iteration 34920 Training loss 0.04192621260881424 Validation loss 0.04832955449819565 Accuracy 0.8730000257492065\n",
      "Iteration 34930 Training loss 0.04945982247591019 Validation loss 0.04875697195529938 Accuracy 0.8733750581741333\n",
      "Iteration 34940 Training loss 0.04842056334018707 Validation loss 0.04844946041703224 Accuracy 0.8738750219345093\n",
      "Iteration 34950 Training loss 0.04743234068155289 Validation loss 0.04833868891000748 Accuracy 0.8730000257492065\n",
      "Iteration 34960 Training loss 0.051940687000751495 Validation loss 0.04832989722490311 Accuracy 0.8728750348091125\n",
      "Iteration 34970 Training loss 0.05058100074529648 Validation loss 0.0483771450817585 Accuracy 0.8726250529289246\n",
      "Iteration 34980 Training loss 0.04609532654285431 Validation loss 0.048423875123262405 Accuracy 0.8715000152587891\n",
      "Iteration 34990 Training loss 0.04710612818598747 Validation loss 0.04837287962436676 Accuracy 0.8728750348091125\n",
      "Iteration 35000 Training loss 0.035876836627721786 Validation loss 0.04920302703976631 Accuracy 0.8723750710487366\n",
      "Iteration 35010 Training loss 0.04276444390416145 Validation loss 0.048943471163511276 Accuracy 0.8737500309944153\n",
      "Iteration 35020 Training loss 0.04810502380132675 Validation loss 0.048505496233701706 Accuracy 0.8711250424385071\n",
      "Iteration 35030 Training loss 0.03884308412671089 Validation loss 0.04840809106826782 Accuracy 0.8717500567436218\n",
      "Iteration 35040 Training loss 0.04733530804514885 Validation loss 0.04830553010106087 Accuracy 0.8731250166893005\n",
      "Iteration 35050 Training loss 0.04251492768526077 Validation loss 0.048432838171720505 Accuracy 0.8713750243186951\n",
      "Iteration 35060 Training loss 0.04625007137656212 Validation loss 0.04832521453499794 Accuracy 0.8728750348091125\n",
      "Iteration 35070 Training loss 0.04061213880777359 Validation loss 0.0483207032084465 Accuracy 0.8721250295639038\n",
      "Iteration 35080 Training loss 0.05400065332651138 Validation loss 0.049424998462200165 Accuracy 0.8715000152587891\n",
      "Iteration 35090 Training loss 0.04259761422872543 Validation loss 0.04837796092033386 Accuracy 0.8726250529289246\n",
      "Iteration 35100 Training loss 0.053131844848394394 Validation loss 0.048805829137563705 Accuracy 0.8737500309944153\n",
      "Iteration 35110 Training loss 0.04641616716980934 Validation loss 0.04856988415122032 Accuracy 0.8737500309944153\n",
      "Iteration 35120 Training loss 0.04374169185757637 Validation loss 0.04921144247055054 Accuracy 0.8730000257492065\n",
      "Iteration 35130 Training loss 0.051725950092077255 Validation loss 0.048558518290519714 Accuracy 0.874125063419342\n",
      "Iteration 35140 Training loss 0.04329530522227287 Validation loss 0.048279888927936554 Accuracy 0.8733750581741333\n",
      "Iteration 35150 Training loss 0.03614044561982155 Validation loss 0.04877530783414841 Accuracy 0.8687500357627869\n",
      "Iteration 35160 Training loss 0.0424480065703392 Validation loss 0.04841471463441849 Accuracy 0.8737500309944153\n",
      "Iteration 35170 Training loss 0.040717270225286484 Validation loss 0.04925493523478508 Accuracy 0.8723750710487366\n",
      "Iteration 35180 Training loss 0.03903651982545853 Validation loss 0.04829110950231552 Accuracy 0.8738750219345093\n",
      "Iteration 35190 Training loss 0.04347463324666023 Validation loss 0.04832448810338974 Accuracy 0.8740000128746033\n",
      "Iteration 35200 Training loss 0.041379936039447784 Validation loss 0.04825698956847191 Accuracy 0.8732500672340393\n",
      "Iteration 35210 Training loss 0.042843759059906006 Validation loss 0.04841116443276405 Accuracy 0.8715000152587891\n",
      "Iteration 35220 Training loss 0.04311937093734741 Validation loss 0.04836716875433922 Accuracy 0.8737500309944153\n",
      "Iteration 35230 Training loss 0.04840777441859245 Validation loss 0.04843924939632416 Accuracy 0.8728750348091125\n",
      "Iteration 35240 Training loss 0.046997539699077606 Validation loss 0.04838438704609871 Accuracy 0.8720000386238098\n",
      "Iteration 35250 Training loss 0.03932361677289009 Validation loss 0.04822716489434242 Accuracy 0.874125063419342\n",
      "Iteration 35260 Training loss 0.037463512271642685 Validation loss 0.048250921070575714 Accuracy 0.8738750219345093\n",
      "Iteration 35270 Training loss 0.043622493743896484 Validation loss 0.04828757047653198 Accuracy 0.8721250295639038\n",
      "Iteration 35280 Training loss 0.043728332966566086 Validation loss 0.04845649003982544 Accuracy 0.8725000619888306\n",
      "Iteration 35290 Training loss 0.04182305186986923 Validation loss 0.04829200729727745 Accuracy 0.8735000491142273\n",
      "Iteration 35300 Training loss 0.032692741602659225 Validation loss 0.04871708154678345 Accuracy 0.8701250553131104\n",
      "Iteration 35310 Training loss 0.0453779511153698 Validation loss 0.04833752661943436 Accuracy 0.8721250295639038\n",
      "Iteration 35320 Training loss 0.04681488871574402 Validation loss 0.04824979975819588 Accuracy 0.8736250400543213\n",
      "Iteration 35330 Training loss 0.046863969415426254 Validation loss 0.04864051192998886 Accuracy 0.8692500591278076\n",
      "Iteration 35340 Training loss 0.037463799118995667 Validation loss 0.04837585985660553 Accuracy 0.8733750581741333\n",
      "Iteration 35350 Training loss 0.05689852684736252 Validation loss 0.048262789845466614 Accuracy 0.8717500567436218\n",
      "Iteration 35360 Training loss 0.047736603766679764 Validation loss 0.04818335920572281 Accuracy 0.8732500672340393\n",
      "Iteration 35370 Training loss 0.04307398572564125 Validation loss 0.048233479261398315 Accuracy 0.8723750710487366\n",
      "Iteration 35380 Training loss 0.0388752743601799 Validation loss 0.0482928492128849 Accuracy 0.8736250400543213\n",
      "Iteration 35390 Training loss 0.04117245599627495 Validation loss 0.048199594020843506 Accuracy 0.8722500205039978\n",
      "Iteration 35400 Training loss 0.04825971648097038 Validation loss 0.04861224442720413 Accuracy 0.8730000257492065\n",
      "Iteration 35410 Training loss 0.044087089598178864 Validation loss 0.04819956421852112 Accuracy 0.8727500438690186\n",
      "Iteration 35420 Training loss 0.04567062109708786 Validation loss 0.04843854904174805 Accuracy 0.8732500672340393\n",
      "Iteration 35430 Training loss 0.047890499234199524 Validation loss 0.04816827550530434 Accuracy 0.8731250166893005\n",
      "Iteration 35440 Training loss 0.04440813139081001 Validation loss 0.04836270213127136 Accuracy 0.8731250166893005\n",
      "Iteration 35450 Training loss 0.04946878179907799 Validation loss 0.04826214164495468 Accuracy 0.8740000128746033\n",
      "Iteration 35460 Training loss 0.05012302100658417 Validation loss 0.04859634116292 Accuracy 0.8732500672340393\n",
      "Iteration 35470 Training loss 0.04294518381357193 Validation loss 0.04840471222996712 Accuracy 0.8717500567436218\n",
      "Iteration 35480 Training loss 0.05063575506210327 Validation loss 0.04838944599032402 Accuracy 0.8730000257492065\n",
      "Iteration 35490 Training loss 0.0459345243871212 Validation loss 0.04817604646086693 Accuracy 0.8740000128746033\n",
      "Iteration 35500 Training loss 0.04436517506837845 Validation loss 0.048227302730083466 Accuracy 0.874250054359436\n",
      "Iteration 35510 Training loss 0.043507665395736694 Validation loss 0.04820416495203972 Accuracy 0.8735000491142273\n",
      "Iteration 35520 Training loss 0.0381958894431591 Validation loss 0.048231080174446106 Accuracy 0.8740000128746033\n",
      "Iteration 35530 Training loss 0.05589132755994797 Validation loss 0.048565879464149475 Accuracy 0.874250054359436\n",
      "Iteration 35540 Training loss 0.05025840550661087 Validation loss 0.04824608191847801 Accuracy 0.8731250166893005\n",
      "Iteration 35550 Training loss 0.04661703109741211 Validation loss 0.048602160066366196 Accuracy 0.8740000128746033\n",
      "Iteration 35560 Training loss 0.040597692131996155 Validation loss 0.048257749527692795 Accuracy 0.8725000619888306\n",
      "Iteration 35570 Training loss 0.04799085110425949 Validation loss 0.0483759306371212 Accuracy 0.8735000491142273\n",
      "Iteration 35580 Training loss 0.032020583748817444 Validation loss 0.04840175062417984 Accuracy 0.8736250400543213\n",
      "Iteration 35590 Training loss 0.04074554517865181 Validation loss 0.04821149632334709 Accuracy 0.8730000257492065\n",
      "Iteration 35600 Training loss 0.050075359642505646 Validation loss 0.048652805387973785 Accuracy 0.8686250448226929\n",
      "Iteration 35610 Training loss 0.0400780625641346 Validation loss 0.048174869269132614 Accuracy 0.8731250166893005\n",
      "Iteration 35620 Training loss 0.05233879014849663 Validation loss 0.04822228476405144 Accuracy 0.8725000619888306\n",
      "Iteration 35630 Training loss 0.036872748285532 Validation loss 0.04813951626420021 Accuracy 0.8740000128746033\n",
      "Iteration 35640 Training loss 0.04738827049732208 Validation loss 0.04857344925403595 Accuracy 0.8708750605583191\n",
      "Iteration 35650 Training loss 0.04695451632142067 Validation loss 0.0481996163725853 Accuracy 0.874625027179718\n",
      "Iteration 35660 Training loss 0.046023719012737274 Validation loss 0.04850737005472183 Accuracy 0.8712500333786011\n",
      "Iteration 35670 Training loss 0.05086031183600426 Validation loss 0.048243213444948196 Accuracy 0.8726250529289246\n",
      "Iteration 35680 Training loss 0.040440578013658524 Validation loss 0.04814369976520538 Accuracy 0.8738750219345093\n",
      "Iteration 35690 Training loss 0.045382242649793625 Validation loss 0.04827330261468887 Accuracy 0.8732500672340393\n",
      "Iteration 35700 Training loss 0.04574938118457794 Validation loss 0.048474010080099106 Accuracy 0.8700000643730164\n",
      "Iteration 35710 Training loss 0.04013654589653015 Validation loss 0.048122555017471313 Accuracy 0.8732500672340393\n",
      "Iteration 35720 Training loss 0.05155225470662117 Validation loss 0.04815342277288437 Accuracy 0.874125063419342\n",
      "Iteration 35730 Training loss 0.05050908029079437 Validation loss 0.04863811284303665 Accuracy 0.8692500591278076\n",
      "Iteration 35740 Training loss 0.044751692563295364 Validation loss 0.0480462983250618 Accuracy 0.874125063419342\n",
      "Iteration 35750 Training loss 0.04194003343582153 Validation loss 0.04810108616948128 Accuracy 0.874125063419342\n",
      "Iteration 35760 Training loss 0.034551337361335754 Validation loss 0.048061590641736984 Accuracy 0.874750018119812\n",
      "Iteration 35770 Training loss 0.04313346743583679 Validation loss 0.0481022372841835 Accuracy 0.8727500438690186\n",
      "Iteration 35780 Training loss 0.04740336909890175 Validation loss 0.048241883516311646 Accuracy 0.874250054359436\n",
      "Iteration 35790 Training loss 0.04896637797355652 Validation loss 0.04868273437023163 Accuracy 0.8758750557899475\n",
      "Iteration 35800 Training loss 0.044235486537218094 Validation loss 0.04861655831336975 Accuracy 0.874750018119812\n",
      "Iteration 35810 Training loss 0.0422406904399395 Validation loss 0.04815289378166199 Accuracy 0.8740000128746033\n",
      "Iteration 35820 Training loss 0.04634946584701538 Validation loss 0.04846354201436043 Accuracy 0.8696250319480896\n",
      "Iteration 35830 Training loss 0.04687714949250221 Validation loss 0.048132821917533875 Accuracy 0.87437504529953\n",
      "Iteration 35840 Training loss 0.04525305703282356 Validation loss 0.0482737235724926 Accuracy 0.8706250190734863\n",
      "Iteration 35850 Training loss 0.03953256830573082 Validation loss 0.048297811299562454 Accuracy 0.8703750371932983\n",
      "Iteration 35860 Training loss 0.04261576384305954 Validation loss 0.048216018825769424 Accuracy 0.874250054359436\n",
      "Iteration 35870 Training loss 0.04094887524843216 Validation loss 0.048163384199142456 Accuracy 0.8736250400543213\n",
      "Iteration 35880 Training loss 0.043808963149785995 Validation loss 0.048129819333553314 Accuracy 0.8732500672340393\n",
      "Iteration 35890 Training loss 0.04493892565369606 Validation loss 0.048133641481399536 Accuracy 0.8736250400543213\n",
      "Iteration 35900 Training loss 0.037597425282001495 Validation loss 0.04823144152760506 Accuracy 0.8756250143051147\n",
      "Iteration 35910 Training loss 0.043902553617954254 Validation loss 0.0483408086001873 Accuracy 0.8716250658035278\n",
      "Iteration 35920 Training loss 0.054286591708660126 Validation loss 0.04840845614671707 Accuracy 0.874625027179718\n",
      "Iteration 35930 Training loss 0.0469573549926281 Validation loss 0.048607610166072845 Accuracy 0.874750018119812\n",
      "Iteration 35940 Training loss 0.0546896830201149 Validation loss 0.04804420843720436 Accuracy 0.87437504529953\n",
      "Iteration 35950 Training loss 0.04283000901341438 Validation loss 0.04810322821140289 Accuracy 0.8736250400543213\n",
      "Iteration 35960 Training loss 0.04179263859987259 Validation loss 0.04941849038004875 Accuracy 0.8716250658035278\n",
      "Iteration 35970 Training loss 0.03791940212249756 Validation loss 0.04815281182527542 Accuracy 0.8726250529289246\n",
      "Iteration 35980 Training loss 0.03953007981181145 Validation loss 0.048068538308143616 Accuracy 0.874125063419342\n",
      "Iteration 35990 Training loss 0.045561302453279495 Validation loss 0.04804759472608566 Accuracy 0.874250054359436\n",
      "Iteration 36000 Training loss 0.04885035380721092 Validation loss 0.04864615201950073 Accuracy 0.8737500309944153\n",
      "Iteration 36010 Training loss 0.03788397088646889 Validation loss 0.04813527315855026 Accuracy 0.8727500438690186\n",
      "Iteration 36020 Training loss 0.048572372645139694 Validation loss 0.04810795933008194 Accuracy 0.8718750476837158\n",
      "Iteration 36030 Training loss 0.03797416761517525 Validation loss 0.048073943704366684 Accuracy 0.8733750581741333\n",
      "Iteration 36040 Training loss 0.04717544466257095 Validation loss 0.04811432957649231 Accuracy 0.8733750581741333\n",
      "Iteration 36050 Training loss 0.03610476478934288 Validation loss 0.04829884693026543 Accuracy 0.8710000514984131\n",
      "Iteration 36060 Training loss 0.04211254045367241 Validation loss 0.04862223193049431 Accuracy 0.874125063419342\n",
      "Iteration 36070 Training loss 0.046477556228637695 Validation loss 0.04824460670351982 Accuracy 0.8751250505447388\n",
      "Iteration 36080 Training loss 0.05210115388035774 Validation loss 0.048157285898923874 Accuracy 0.8757500648498535\n",
      "Iteration 36090 Training loss 0.0497773177921772 Validation loss 0.04804525524377823 Accuracy 0.8725000619888306\n",
      "Iteration 36100 Training loss 0.043918807059526443 Validation loss 0.04819450527429581 Accuracy 0.8740000128746033\n",
      "Iteration 36110 Training loss 0.045216891914606094 Validation loss 0.04816116392612457 Accuracy 0.8726250529289246\n",
      "Iteration 36120 Training loss 0.04474280774593353 Validation loss 0.04882451891899109 Accuracy 0.8730000257492065\n",
      "Iteration 36130 Training loss 0.04666777327656746 Validation loss 0.048057373613119125 Accuracy 0.874250054359436\n",
      "Iteration 36140 Training loss 0.04020005092024803 Validation loss 0.04809951409697533 Accuracy 0.8738750219345093\n",
      "Iteration 36150 Training loss 0.04028327390551567 Validation loss 0.04803873598575592 Accuracy 0.8755000233650208\n",
      "Iteration 36160 Training loss 0.0476202629506588 Validation loss 0.04804325848817825 Accuracy 0.874625027179718\n",
      "Iteration 36170 Training loss 0.04439561814069748 Validation loss 0.04812920093536377 Accuracy 0.874500036239624\n",
      "Iteration 36180 Training loss 0.03646238148212433 Validation loss 0.04819754138588905 Accuracy 0.8723750710487366\n",
      "Iteration 36190 Training loss 0.03921915963292122 Validation loss 0.04828713461756706 Accuracy 0.8718750476837158\n",
      "Iteration 36200 Training loss 0.041844237595796585 Validation loss 0.0480615608394146 Accuracy 0.8733750581741333\n",
      "Iteration 36210 Training loss 0.051619499921798706 Validation loss 0.048219479620456696 Accuracy 0.8718750476837158\n",
      "Iteration 36220 Training loss 0.05151677504181862 Validation loss 0.048379942774772644 Accuracy 0.874625027179718\n",
      "Iteration 36230 Training loss 0.05003225803375244 Validation loss 0.0484333299100399 Accuracy 0.8701250553131104\n",
      "Iteration 36240 Training loss 0.047411464154720306 Validation loss 0.049470994621515274 Accuracy 0.8651250600814819\n",
      "Iteration 36250 Training loss 0.040337055921554565 Validation loss 0.04795905202627182 Accuracy 0.874625027179718\n",
      "Iteration 36260 Training loss 0.038573674857616425 Validation loss 0.048169028013944626 Accuracy 0.8750000596046448\n",
      "Iteration 36270 Training loss 0.04704848304390907 Validation loss 0.04849569872021675 Accuracy 0.8697500228881836\n",
      "Iteration 36280 Training loss 0.039513859897851944 Validation loss 0.04796183481812477 Accuracy 0.8736250400543213\n",
      "Iteration 36290 Training loss 0.044970978051424026 Validation loss 0.048521559685468674 Accuracy 0.8686250448226929\n",
      "Iteration 36300 Training loss 0.04321473091840744 Validation loss 0.04811881110072136 Accuracy 0.8720000386238098\n",
      "Iteration 36310 Training loss 0.05131510645151138 Validation loss 0.04807395115494728 Accuracy 0.8728750348091125\n",
      "Iteration 36320 Training loss 0.038160186260938644 Validation loss 0.04870471730828285 Accuracy 0.87437504529953\n",
      "Iteration 36330 Training loss 0.04188952222466469 Validation loss 0.04795095697045326 Accuracy 0.8751250505447388\n",
      "Iteration 36340 Training loss 0.04089302569627762 Validation loss 0.048000916838645935 Accuracy 0.8751250505447388\n",
      "Iteration 36350 Training loss 0.04801357164978981 Validation loss 0.04821775481104851 Accuracy 0.8751250505447388\n",
      "Iteration 36360 Training loss 0.042941369116306305 Validation loss 0.04801298305392265 Accuracy 0.8727500438690186\n",
      "Iteration 36370 Training loss 0.04078907519578934 Validation loss 0.04795489087700844 Accuracy 0.8750000596046448\n",
      "Iteration 36380 Training loss 0.046044740825891495 Validation loss 0.048249825835227966 Accuracy 0.8710000514984131\n",
      "Iteration 36390 Training loss 0.048701465129852295 Validation loss 0.047954075038433075 Accuracy 0.8733750581741333\n",
      "Iteration 36400 Training loss 0.04275359958410263 Validation loss 0.0481126643717289 Accuracy 0.8750000596046448\n",
      "Iteration 36410 Training loss 0.04636773094534874 Validation loss 0.048100199550390244 Accuracy 0.8725000619888306\n",
      "Iteration 36420 Training loss 0.0398377887904644 Validation loss 0.04799484461545944 Accuracy 0.8726250529289246\n",
      "Iteration 36430 Training loss 0.04800482466816902 Validation loss 0.04828023165464401 Accuracy 0.8711250424385071\n",
      "Iteration 36440 Training loss 0.03990209102630615 Validation loss 0.04797022417187691 Accuracy 0.8732500672340393\n",
      "Iteration 36450 Training loss 0.053954146802425385 Validation loss 0.047975990921258926 Accuracy 0.8732500672340393\n",
      "Iteration 36460 Training loss 0.03476230427622795 Validation loss 0.048683930188417435 Accuracy 0.8692500591278076\n",
      "Iteration 36470 Training loss 0.04577895998954773 Validation loss 0.04841925576329231 Accuracy 0.8702500462532043\n",
      "Iteration 36480 Training loss 0.048498377203941345 Validation loss 0.04795289784669876 Accuracy 0.8732500672340393\n",
      "Iteration 36490 Training loss 0.04482660070061684 Validation loss 0.048929911106824875 Accuracy 0.8738750219345093\n",
      "Iteration 36500 Training loss 0.042653921991586685 Validation loss 0.0481736920773983 Accuracy 0.874500036239624\n",
      "Iteration 36510 Training loss 0.05002773180603981 Validation loss 0.04802972823381424 Accuracy 0.8748750686645508\n",
      "Iteration 36520 Training loss 0.04176412522792816 Validation loss 0.0480104424059391 Accuracy 0.8728750348091125\n",
      "Iteration 36530 Training loss 0.0435965433716774 Validation loss 0.04792100936174393 Accuracy 0.8735000491142273\n",
      "Iteration 36540 Training loss 0.0442081019282341 Validation loss 0.04804059863090515 Accuracy 0.8722500205039978\n",
      "Iteration 36550 Training loss 0.04644004628062248 Validation loss 0.04788675904273987 Accuracy 0.874250054359436\n",
      "Iteration 36560 Training loss 0.04841810092329979 Validation loss 0.047977082431316376 Accuracy 0.8727500438690186\n",
      "Iteration 36570 Training loss 0.04130024462938309 Validation loss 0.04791263863444328 Accuracy 0.8736250400543213\n",
      "Iteration 36580 Training loss 0.04587097838521004 Validation loss 0.04788484424352646 Accuracy 0.874250054359436\n",
      "Iteration 36590 Training loss 0.03580455854535103 Validation loss 0.04800458997488022 Accuracy 0.8752500414848328\n",
      "Iteration 36600 Training loss 0.05069040134549141 Validation loss 0.048061054199934006 Accuracy 0.8725000619888306\n",
      "Iteration 36610 Training loss 0.04550553113222122 Validation loss 0.04899794980883598 Accuracy 0.8681250214576721\n",
      "Iteration 36620 Training loss 0.04060100018978119 Validation loss 0.04817988723516464 Accuracy 0.8722500205039978\n",
      "Iteration 36630 Training loss 0.03889157623052597 Validation loss 0.047986820340156555 Accuracy 0.8727500438690186\n",
      "Iteration 36640 Training loss 0.04306598752737045 Validation loss 0.04813261330127716 Accuracy 0.8728750348091125\n",
      "Iteration 36650 Training loss 0.04054315388202667 Validation loss 0.04790177196264267 Accuracy 0.8732500672340393\n",
      "Iteration 36660 Training loss 0.04681622236967087 Validation loss 0.0484081506729126 Accuracy 0.8756250143051147\n",
      "Iteration 36670 Training loss 0.03833156079053879 Validation loss 0.04792480543255806 Accuracy 0.874500036239624\n",
      "Iteration 36680 Training loss 0.044933442026376724 Validation loss 0.04803432896733284 Accuracy 0.8755000233650208\n",
      "Iteration 36690 Training loss 0.0407090000808239 Validation loss 0.04830685630440712 Accuracy 0.8755000233650208\n",
      "Iteration 36700 Training loss 0.044710416346788406 Validation loss 0.048059601336717606 Accuracy 0.8717500567436218\n",
      "Iteration 36710 Training loss 0.04636510834097862 Validation loss 0.048150040209293365 Accuracy 0.8727500438690186\n",
      "Iteration 36720 Training loss 0.04074995964765549 Validation loss 0.048019733279943466 Accuracy 0.8738750219345093\n",
      "Iteration 36730 Training loss 0.04951843246817589 Validation loss 0.04791073128581047 Accuracy 0.874125063419342\n",
      "Iteration 36740 Training loss 0.04278356581926346 Validation loss 0.047965601086616516 Accuracy 0.8732500672340393\n",
      "Iteration 36750 Training loss 0.043116550892591476 Validation loss 0.04790081828832626 Accuracy 0.8756250143051147\n",
      "Iteration 36760 Training loss 0.03974108770489693 Validation loss 0.04864704981446266 Accuracy 0.8686250448226929\n",
      "Iteration 36770 Training loss 0.03784204646945 Validation loss 0.04788100719451904 Accuracy 0.8753750324249268\n",
      "Iteration 36780 Training loss 0.04759841039776802 Validation loss 0.04786323755979538 Accuracy 0.8755000233650208\n",
      "Iteration 36790 Training loss 0.04202146455645561 Validation loss 0.04816356301307678 Accuracy 0.8711250424385071\n",
      "Iteration 36800 Training loss 0.04730570316314697 Validation loss 0.04824744910001755 Accuracy 0.87437504529953\n",
      "Iteration 36810 Training loss 0.03649463877081871 Validation loss 0.04790686443448067 Accuracy 0.8740000128746033\n",
      "Iteration 36820 Training loss 0.04464351385831833 Validation loss 0.04790967330336571 Accuracy 0.8738750219345093\n",
      "Iteration 36830 Training loss 0.041298069059848785 Validation loss 0.04794731363654137 Accuracy 0.8723750710487366\n",
      "Iteration 36840 Training loss 0.053433410823345184 Validation loss 0.04827747493982315 Accuracy 0.8750000596046448\n",
      "Iteration 36850 Training loss 0.03938277065753937 Validation loss 0.04783935472369194 Accuracy 0.874125063419342\n",
      "Iteration 36860 Training loss 0.03610670194029808 Validation loss 0.048046499490737915 Accuracy 0.8755000233650208\n",
      "Iteration 36870 Training loss 0.04425252974033356 Validation loss 0.04794381558895111 Accuracy 0.8752500414848328\n",
      "Iteration 36880 Training loss 0.05045582726597786 Validation loss 0.048324666917324066 Accuracy 0.8707500696182251\n",
      "Iteration 36890 Training loss 0.04458397626876831 Validation loss 0.048105355352163315 Accuracy 0.8760000467300415\n",
      "Iteration 36900 Training loss 0.04751387611031532 Validation loss 0.04788830876350403 Accuracy 0.8736250400543213\n",
      "Iteration 36910 Training loss 0.041894759982824326 Validation loss 0.047869741916656494 Accuracy 0.8735000491142273\n",
      "Iteration 36920 Training loss 0.04298629239201546 Validation loss 0.04791657626628876 Accuracy 0.8732500672340393\n",
      "Iteration 36930 Training loss 0.04682280868291855 Validation loss 0.047832027077674866 Accuracy 0.87437504529953\n",
      "Iteration 36940 Training loss 0.044359106570482254 Validation loss 0.04808855801820755 Accuracy 0.8721250295639038\n",
      "Iteration 36950 Training loss 0.04138496145606041 Validation loss 0.04799198359251022 Accuracy 0.8756250143051147\n",
      "Iteration 36960 Training loss 0.04307284206151962 Validation loss 0.0482320562005043 Accuracy 0.8748750686645508\n",
      "Iteration 36970 Training loss 0.043411023914813995 Validation loss 0.0483739972114563 Accuracy 0.8701250553131104\n",
      "Iteration 36980 Training loss 0.042009901255369186 Validation loss 0.04781191051006317 Accuracy 0.8758750557899475\n",
      "Iteration 36990 Training loss 0.04163427650928497 Validation loss 0.04802490398287773 Accuracy 0.8713750243186951\n",
      "Iteration 37000 Training loss 0.033975329250097275 Validation loss 0.04781954362988472 Accuracy 0.8748750686645508\n",
      "Iteration 37010 Training loss 0.05761552229523659 Validation loss 0.04800422117114067 Accuracy 0.8763750195503235\n",
      "Iteration 37020 Training loss 0.04379703477025032 Validation loss 0.04790131747722626 Accuracy 0.8725000619888306\n",
      "Iteration 37030 Training loss 0.04610283672809601 Validation loss 0.04812459275126457 Accuracy 0.8750000596046448\n",
      "Iteration 37040 Training loss 0.0441744364798069 Validation loss 0.04799098148941994 Accuracy 0.8721250295639038\n",
      "Iteration 37050 Training loss 0.048424091190099716 Validation loss 0.04789004847407341 Accuracy 0.8721250295639038\n",
      "Iteration 37060 Training loss 0.03891780227422714 Validation loss 0.04777878150343895 Accuracy 0.87437504529953\n",
      "Iteration 37070 Training loss 0.038105811923742294 Validation loss 0.04791347682476044 Accuracy 0.8765000700950623\n",
      "Iteration 37080 Training loss 0.0446174219250679 Validation loss 0.04778926447033882 Accuracy 0.8756250143051147\n",
      "Iteration 37090 Training loss 0.047467298805713654 Validation loss 0.047825828194618225 Accuracy 0.8755000233650208\n",
      "Iteration 37100 Training loss 0.0456848181784153 Validation loss 0.04783846437931061 Accuracy 0.8765000700950623\n",
      "Iteration 37110 Training loss 0.042195308953523636 Validation loss 0.04781582951545715 Accuracy 0.8730000257492065\n",
      "Iteration 37120 Training loss 0.03977065905928612 Validation loss 0.04879399761557579 Accuracy 0.8688750267028809\n",
      "Iteration 37130 Training loss 0.04131140559911728 Validation loss 0.04777604341506958 Accuracy 0.8728750348091125\n",
      "Iteration 37140 Training loss 0.049850624054670334 Validation loss 0.04792667180299759 Accuracy 0.8757500648498535\n",
      "Iteration 37150 Training loss 0.04051627218723297 Validation loss 0.04805247485637665 Accuracy 0.8723750710487366\n",
      "Iteration 37160 Training loss 0.04708530753850937 Validation loss 0.04771897941827774 Accuracy 0.874250054359436\n",
      "Iteration 37170 Training loss 0.05192567780613899 Validation loss 0.048793110996484756 Accuracy 0.8678750395774841\n",
      "Iteration 37180 Training loss 0.042293183505535126 Validation loss 0.04802507162094116 Accuracy 0.8722500205039978\n",
      "Iteration 37190 Training loss 0.047013059258461 Validation loss 0.047884501516819 Accuracy 0.8763750195503235\n",
      "Iteration 37200 Training loss 0.040834978222846985 Validation loss 0.04848852753639221 Accuracy 0.8695000410079956\n",
      "Iteration 37210 Training loss 0.045582160353660583 Validation loss 0.04785225912928581 Accuracy 0.8762500286102295\n",
      "Iteration 37220 Training loss 0.035536739975214005 Validation loss 0.04794991388916969 Accuracy 0.8753750324249268\n",
      "Iteration 37230 Training loss 0.04217500612139702 Validation loss 0.04800430312752724 Accuracy 0.8716250658035278\n",
      "Iteration 37240 Training loss 0.048699770122766495 Validation loss 0.047745898365974426 Accuracy 0.8751250505447388\n",
      "Iteration 37250 Training loss 0.03975164145231247 Validation loss 0.04772893711924553 Accuracy 0.8748750686645508\n",
      "Iteration 37260 Training loss 0.042559996247291565 Validation loss 0.04849519208073616 Accuracy 0.8691250681877136\n",
      "Iteration 37270 Training loss 0.04227262735366821 Validation loss 0.047759365290403366 Accuracy 0.874750018119812\n",
      "Iteration 37280 Training loss 0.04522157460451126 Validation loss 0.047771576792001724 Accuracy 0.874125063419342\n",
      "Iteration 37290 Training loss 0.04708665609359741 Validation loss 0.047987308353185654 Accuracy 0.8762500286102295\n",
      "Iteration 37300 Training loss 0.04458681121468544 Validation loss 0.04768206179141998 Accuracy 0.874625027179718\n",
      "Iteration 37310 Training loss 0.03524841368198395 Validation loss 0.0477370023727417 Accuracy 0.8736250400543213\n",
      "Iteration 37320 Training loss 0.04718076437711716 Validation loss 0.047775719314813614 Accuracy 0.8757500648498535\n",
      "Iteration 37330 Training loss 0.042109787464141846 Validation loss 0.0476691871881485 Accuracy 0.874125063419342\n",
      "Iteration 37340 Training loss 0.04669184610247612 Validation loss 0.04768580570816994 Accuracy 0.8740000128746033\n",
      "Iteration 37350 Training loss 0.04223174601793289 Validation loss 0.04779938608407974 Accuracy 0.8737500309944153\n",
      "Iteration 37360 Training loss 0.045613136142492294 Validation loss 0.04769672080874443 Accuracy 0.8752500414848328\n",
      "Iteration 37370 Training loss 0.036532897502183914 Validation loss 0.047749750316143036 Accuracy 0.8766250610351562\n",
      "Iteration 37380 Training loss 0.04705062136054039 Validation loss 0.047781433910131454 Accuracy 0.8731250166893005\n",
      "Iteration 37390 Training loss 0.046175260096788406 Validation loss 0.04767458513379097 Accuracy 0.8770000338554382\n",
      "Iteration 37400 Training loss 0.03704606741666794 Validation loss 0.047812510281801224 Accuracy 0.8717500567436218\n",
      "Iteration 37410 Training loss 0.04398474469780922 Validation loss 0.04765511676669121 Accuracy 0.874250054359436\n",
      "Iteration 37420 Training loss 0.03818060830235481 Validation loss 0.04773704707622528 Accuracy 0.8757500648498535\n",
      "Iteration 37430 Training loss 0.04634629935026169 Validation loss 0.04778563976287842 Accuracy 0.8765000700950623\n",
      "Iteration 37440 Training loss 0.039690032601356506 Validation loss 0.04785001277923584 Accuracy 0.8767500519752502\n",
      "Iteration 37450 Training loss 0.05056963860988617 Validation loss 0.04766179621219635 Accuracy 0.8761250376701355\n",
      "Iteration 37460 Training loss 0.04005681350827217 Validation loss 0.04766836762428284 Accuracy 0.8753750324249268\n",
      "Iteration 37470 Training loss 0.04365156963467598 Validation loss 0.047685861587524414 Accuracy 0.8750000596046448\n",
      "Iteration 37480 Training loss 0.03970777615904808 Validation loss 0.0477815605700016 Accuracy 0.8758750557899475\n",
      "Iteration 37490 Training loss 0.04844525083899498 Validation loss 0.04773341491818428 Accuracy 0.8758750557899475\n",
      "Iteration 37500 Training loss 0.039176538586616516 Validation loss 0.04819268360733986 Accuracy 0.8763750195503235\n",
      "Iteration 37510 Training loss 0.03499271348118782 Validation loss 0.0477299690246582 Accuracy 0.8748750686645508\n",
      "Iteration 37520 Training loss 0.048569392412900925 Validation loss 0.047744762152433395 Accuracy 0.874750018119812\n",
      "Iteration 37530 Training loss 0.04067515209317207 Validation loss 0.047754984349012375 Accuracy 0.8730000257492065\n",
      "Iteration 37540 Training loss 0.04821246117353439 Validation loss 0.048061493784189224 Accuracy 0.8758750557899475\n",
      "Iteration 37550 Training loss 0.03697853907942772 Validation loss 0.047727108001708984 Accuracy 0.8755000233650208\n",
      "Iteration 37560 Training loss 0.047781363129615784 Validation loss 0.047952134162187576 Accuracy 0.8748750686645508\n",
      "Iteration 37570 Training loss 0.04293859750032425 Validation loss 0.04797149822115898 Accuracy 0.8748750686645508\n",
      "Iteration 37580 Training loss 0.05080051347613335 Validation loss 0.047921184450387955 Accuracy 0.8752500414848328\n",
      "Iteration 37590 Training loss 0.04595223814249039 Validation loss 0.04772436246275902 Accuracy 0.874250054359436\n",
      "Iteration 37600 Training loss 0.043428801000118256 Validation loss 0.04768945276737213 Accuracy 0.8756250143051147\n",
      "Iteration 37610 Training loss 0.041782692074775696 Validation loss 0.0477164164185524 Accuracy 0.8738750219345093\n",
      "Iteration 37620 Training loss 0.05196765437722206 Validation loss 0.04778607562184334 Accuracy 0.8755000233650208\n",
      "Iteration 37630 Training loss 0.037963591516017914 Validation loss 0.048065897077322006 Accuracy 0.8707500696182251\n",
      "Iteration 37640 Training loss 0.04996814578771591 Validation loss 0.04794909805059433 Accuracy 0.8712500333786011\n",
      "Iteration 37650 Training loss 0.04685395956039429 Validation loss 0.04810614138841629 Accuracy 0.8758750557899475\n",
      "Iteration 37660 Training loss 0.05296855792403221 Validation loss 0.04769448563456535 Accuracy 0.8748750686645508\n",
      "Iteration 37670 Training loss 0.04522523656487465 Validation loss 0.04766349121928215 Accuracy 0.8751250505447388\n",
      "Iteration 37680 Training loss 0.05058074742555618 Validation loss 0.047837067395448685 Accuracy 0.8730000257492065\n",
      "Iteration 37690 Training loss 0.04697096347808838 Validation loss 0.04771106317639351 Accuracy 0.8727500438690186\n",
      "Iteration 37700 Training loss 0.05275589972734451 Validation loss 0.047720301896333694 Accuracy 0.8730000257492065\n",
      "Iteration 37710 Training loss 0.04240458086133003 Validation loss 0.04766390472650528 Accuracy 0.874125063419342\n",
      "Iteration 37720 Training loss 0.04320507124066353 Validation loss 0.04764951393008232 Accuracy 0.874250054359436\n",
      "Iteration 37730 Training loss 0.03996796905994415 Validation loss 0.047656938433647156 Accuracy 0.8752500414848328\n",
      "Iteration 37740 Training loss 0.04888853430747986 Validation loss 0.04772185906767845 Accuracy 0.8725000619888306\n",
      "Iteration 37750 Training loss 0.044677067548036575 Validation loss 0.04767274856567383 Accuracy 0.874500036239624\n",
      "Iteration 37760 Training loss 0.04344712197780609 Validation loss 0.047676753252744675 Accuracy 0.8730000257492065\n",
      "Iteration 37770 Training loss 0.0461677722632885 Validation loss 0.047730639576911926 Accuracy 0.8738750219345093\n",
      "Iteration 37780 Training loss 0.039825115352869034 Validation loss 0.047722116112709045 Accuracy 0.8731250166893005\n",
      "Iteration 37790 Training loss 0.04583091661334038 Validation loss 0.04769868403673172 Accuracy 0.8738750219345093\n",
      "Iteration 37800 Training loss 0.04555448517203331 Validation loss 0.04771692305803299 Accuracy 0.874125063419342\n",
      "Iteration 37810 Training loss 0.04280926659703255 Validation loss 0.04781029745936394 Accuracy 0.8763750195503235\n",
      "Iteration 37820 Training loss 0.04091736674308777 Validation loss 0.047655001282691956 Accuracy 0.8758750557899475\n",
      "Iteration 37830 Training loss 0.03653364256024361 Validation loss 0.04804941639304161 Accuracy 0.8762500286102295\n",
      "Iteration 37840 Training loss 0.03955874219536781 Validation loss 0.04767674580216408 Accuracy 0.8762500286102295\n",
      "Iteration 37850 Training loss 0.04113054648041725 Validation loss 0.047593455761671066 Accuracy 0.8755000233650208\n",
      "Iteration 37860 Training loss 0.04113953188061714 Validation loss 0.04785744845867157 Accuracy 0.8767500519752502\n",
      "Iteration 37870 Training loss 0.04556116461753845 Validation loss 0.0478774793446064 Accuracy 0.8771250247955322\n",
      "Iteration 37880 Training loss 0.04590579867362976 Validation loss 0.047861017286777496 Accuracy 0.8721250295639038\n",
      "Iteration 37890 Training loss 0.042285773903131485 Validation loss 0.04779369756579399 Accuracy 0.8723750710487366\n",
      "Iteration 37900 Training loss 0.04534919187426567 Validation loss 0.04784252494573593 Accuracy 0.8757500648498535\n",
      "Iteration 37910 Training loss 0.052813902497291565 Validation loss 0.04755319654941559 Accuracy 0.8753750324249268\n",
      "Iteration 37920 Training loss 0.048757534474134445 Validation loss 0.047509729862213135 Accuracy 0.8750000596046448\n",
      "Iteration 37930 Training loss 0.03684240207076073 Validation loss 0.047581929713487625 Accuracy 0.874125063419342\n",
      "Iteration 37940 Training loss 0.04206821322441101 Validation loss 0.04751673713326454 Accuracy 0.8755000233650208\n",
      "Iteration 37950 Training loss 0.03770381957292557 Validation loss 0.04795512557029724 Accuracy 0.8757500648498535\n",
      "Iteration 37960 Training loss 0.04690876975655556 Validation loss 0.04832247644662857 Accuracy 0.874625027179718\n",
      "Iteration 37970 Training loss 0.04534419998526573 Validation loss 0.04787641763687134 Accuracy 0.8765000700950623\n",
      "Iteration 37980 Training loss 0.038516681641340256 Validation loss 0.04753546416759491 Accuracy 0.8757500648498535\n",
      "Iteration 37990 Training loss 0.04178696498274803 Validation loss 0.04753110185265541 Accuracy 0.8757500648498535\n",
      "Iteration 38000 Training loss 0.04251653701066971 Validation loss 0.047797758132219315 Accuracy 0.8735000491142273\n",
      "Iteration 38010 Training loss 0.046586453914642334 Validation loss 0.04754631966352463 Accuracy 0.8750000596046448\n",
      "Iteration 38020 Training loss 0.04463978484272957 Validation loss 0.04753970727324486 Accuracy 0.8757500648498535\n",
      "Iteration 38030 Training loss 0.0447208397090435 Validation loss 0.047583866864442825 Accuracy 0.8760000467300415\n",
      "Iteration 38040 Training loss 0.04316491633653641 Validation loss 0.04751713201403618 Accuracy 0.8765000700950623\n",
      "Iteration 38050 Training loss 0.04435097053647041 Validation loss 0.04778456687927246 Accuracy 0.8763750195503235\n",
      "Iteration 38060 Training loss 0.04008400812745094 Validation loss 0.04756901040673256 Accuracy 0.8738750219345093\n",
      "Iteration 38070 Training loss 0.0390881709754467 Validation loss 0.04749784618616104 Accuracy 0.874500036239624\n",
      "Iteration 38080 Training loss 0.045378707349300385 Validation loss 0.04746640846133232 Accuracy 0.8757500648498535\n",
      "Iteration 38090 Training loss 0.0525643490254879 Validation loss 0.04764704406261444 Accuracy 0.8771250247955322\n",
      "Iteration 38100 Training loss 0.03945256769657135 Validation loss 0.04747268185019493 Accuracy 0.8753750324249268\n",
      "Iteration 38110 Training loss 0.044412530958652496 Validation loss 0.047681454569101334 Accuracy 0.8732500672340393\n",
      "Iteration 38120 Training loss 0.03711109235882759 Validation loss 0.047464121133089066 Accuracy 0.8753750324249268\n",
      "Iteration 38130 Training loss 0.043022945523262024 Validation loss 0.04746543988585472 Accuracy 0.8755000233650208\n",
      "Iteration 38140 Training loss 0.04493051394820213 Validation loss 0.04746830835938454 Accuracy 0.874625027179718\n",
      "Iteration 38150 Training loss 0.04009707272052765 Validation loss 0.047635145485401154 Accuracy 0.8732500672340393\n",
      "Iteration 38160 Training loss 0.04978847876191139 Validation loss 0.04755949601531029 Accuracy 0.8748750686645508\n",
      "Iteration 38170 Training loss 0.04540649428963661 Validation loss 0.047536451369524 Accuracy 0.8757500648498535\n",
      "Iteration 38180 Training loss 0.04849576950073242 Validation loss 0.04821465164422989 Accuracy 0.8711250424385071\n",
      "Iteration 38190 Training loss 0.03710184618830681 Validation loss 0.04752142354846001 Accuracy 0.874250054359436\n",
      "Iteration 38200 Training loss 0.04673415422439575 Validation loss 0.04745187610387802 Accuracy 0.8740000128746033\n",
      "Iteration 38210 Training loss 0.041970670223236084 Validation loss 0.04750669375061989 Accuracy 0.8760000467300415\n",
      "Iteration 38220 Training loss 0.0474361926317215 Validation loss 0.04744129627943039 Accuracy 0.8760000467300415\n",
      "Iteration 38230 Training loss 0.04146556183695793 Validation loss 0.04738321900367737 Accuracy 0.8755000233650208\n",
      "Iteration 38240 Training loss 0.04688353091478348 Validation loss 0.0473523810505867 Accuracy 0.8756250143051147\n",
      "Iteration 38250 Training loss 0.04612268507480621 Validation loss 0.04747272655367851 Accuracy 0.8740000128746033\n",
      "Iteration 38260 Training loss 0.0498378612101078 Validation loss 0.04740174859762192 Accuracy 0.8765000700950623\n",
      "Iteration 38270 Training loss 0.04227558895945549 Validation loss 0.04743306338787079 Accuracy 0.8755000233650208\n",
      "Iteration 38280 Training loss 0.04308508709073067 Validation loss 0.04749229922890663 Accuracy 0.878125011920929\n",
      "Iteration 38290 Training loss 0.05078844726085663 Validation loss 0.04807785525918007 Accuracy 0.8698750138282776\n",
      "Iteration 38300 Training loss 0.04152197390794754 Validation loss 0.04741751030087471 Accuracy 0.8758750557899475\n",
      "Iteration 38310 Training loss 0.04457346349954605 Validation loss 0.04751159995794296 Accuracy 0.8768750429153442\n",
      "Iteration 38320 Training loss 0.040833283215761185 Validation loss 0.047418542206287384 Accuracy 0.8751250505447388\n",
      "Iteration 38330 Training loss 0.04140990972518921 Validation loss 0.04740674048662186 Accuracy 0.8757500648498535\n",
      "Iteration 38340 Training loss 0.0450013242661953 Validation loss 0.04758705198764801 Accuracy 0.874125063419342\n",
      "Iteration 38350 Training loss 0.042154617607593536 Validation loss 0.04755330830812454 Accuracy 0.8750000596046448\n",
      "Iteration 38360 Training loss 0.03865881264209747 Validation loss 0.04746489226818085 Accuracy 0.8751250505447388\n",
      "Iteration 38370 Training loss 0.049301885068416595 Validation loss 0.047416023910045624 Accuracy 0.8763750195503235\n",
      "Iteration 38380 Training loss 0.03770184889435768 Validation loss 0.04745103046298027 Accuracy 0.8760000467300415\n",
      "Iteration 38390 Training loss 0.04350399971008301 Validation loss 0.047796960920095444 Accuracy 0.8717500567436218\n",
      "Iteration 38400 Training loss 0.03935014456510544 Validation loss 0.04756128415465355 Accuracy 0.8735000491142273\n",
      "Iteration 38410 Training loss 0.04944156855344772 Validation loss 0.04741806164383888 Accuracy 0.8748750686645508\n",
      "Iteration 38420 Training loss 0.03638177737593651 Validation loss 0.04745710268616676 Accuracy 0.8765000700950623\n",
      "Iteration 38430 Training loss 0.0462564080953598 Validation loss 0.04738537594676018 Accuracy 0.8748750686645508\n",
      "Iteration 38440 Training loss 0.046490032225847244 Validation loss 0.048154812306165695 Accuracy 0.874250054359436\n",
      "Iteration 38450 Training loss 0.04253266379237175 Validation loss 0.04736483842134476 Accuracy 0.8761250376701355\n",
      "Iteration 38460 Training loss 0.04343916475772858 Validation loss 0.04750702530145645 Accuracy 0.8735000491142273\n",
      "Iteration 38470 Training loss 0.04265991970896721 Validation loss 0.047346919775009155 Accuracy 0.877875030040741\n",
      "Iteration 38480 Training loss 0.04317444562911987 Validation loss 0.04732125997543335 Accuracy 0.8767500519752502\n",
      "Iteration 38490 Training loss 0.04294019937515259 Validation loss 0.04734395816922188 Accuracy 0.8748750686645508\n",
      "Iteration 38500 Training loss 0.0395687110722065 Validation loss 0.04753411188721657 Accuracy 0.8735000491142273\n",
      "Iteration 38510 Training loss 0.041133277118206024 Validation loss 0.04776870086789131 Accuracy 0.8753750324249268\n",
      "Iteration 38520 Training loss 0.027296820655465126 Validation loss 0.04739793390035629 Accuracy 0.874625027179718\n",
      "Iteration 38530 Training loss 0.04532033950090408 Validation loss 0.047393932938575745 Accuracy 0.8758750557899475\n",
      "Iteration 38540 Training loss 0.040953416377305984 Validation loss 0.04744809865951538 Accuracy 0.8756250143051147\n",
      "Iteration 38550 Training loss 0.042688921093940735 Validation loss 0.047831278294324875 Accuracy 0.8713750243186951\n",
      "Iteration 38560 Training loss 0.0479663647711277 Validation loss 0.047518666833639145 Accuracy 0.874250054359436\n",
      "Iteration 38570 Training loss 0.04209528863430023 Validation loss 0.04747111722826958 Accuracy 0.874500036239624\n",
      "Iteration 38580 Training loss 0.04200439527630806 Validation loss 0.04749320447444916 Accuracy 0.8737500309944153\n",
      "Iteration 38590 Training loss 0.04674135893583298 Validation loss 0.04740729182958603 Accuracy 0.8762500286102295\n",
      "Iteration 38600 Training loss 0.03693216294050217 Validation loss 0.047481801360845566 Accuracy 0.8757500648498535\n",
      "Iteration 38610 Training loss 0.046522367745637894 Validation loss 0.048554033041000366 Accuracy 0.8737500309944153\n",
      "Iteration 38620 Training loss 0.04717704653739929 Validation loss 0.0481865368783474 Accuracy 0.8753750324249268\n",
      "Iteration 38630 Training loss 0.04018193483352661 Validation loss 0.04742985963821411 Accuracy 0.8762500286102295\n",
      "Iteration 38640 Training loss 0.041670091450214386 Validation loss 0.047391586005687714 Accuracy 0.8753750324249268\n",
      "Iteration 38650 Training loss 0.03625495731830597 Validation loss 0.04743635281920433 Accuracy 0.874625027179718\n",
      "Iteration 38660 Training loss 0.037341874092817307 Validation loss 0.04738472029566765 Accuracy 0.8760000467300415\n",
      "Iteration 38670 Training loss 0.048746515065431595 Validation loss 0.04737016558647156 Accuracy 0.8760000467300415\n",
      "Iteration 38680 Training loss 0.04916567727923393 Validation loss 0.04769011214375496 Accuracy 0.8761250376701355\n",
      "Iteration 38690 Training loss 0.046953897923231125 Validation loss 0.04762818291783333 Accuracy 0.8765000700950623\n",
      "Iteration 38700 Training loss 0.03813505172729492 Validation loss 0.04733999818563461 Accuracy 0.8757500648498535\n",
      "Iteration 38710 Training loss 0.03737189993262291 Validation loss 0.04759644344449043 Accuracy 0.8768750429153442\n",
      "Iteration 38720 Training loss 0.03979610279202461 Validation loss 0.04767946898937225 Accuracy 0.8772500157356262\n",
      "Iteration 38730 Training loss 0.042378973215818405 Validation loss 0.04790642112493515 Accuracy 0.8756250143051147\n",
      "Iteration 38740 Training loss 0.04563334956765175 Validation loss 0.047382332384586334 Accuracy 0.8752500414848328\n",
      "Iteration 38750 Training loss 0.03835509717464447 Validation loss 0.04758547618985176 Accuracy 0.874250054359436\n",
      "Iteration 38760 Training loss 0.038684677332639694 Validation loss 0.04735179990530014 Accuracy 0.8757500648498535\n",
      "Iteration 38770 Training loss 0.04592609778046608 Validation loss 0.047395579516887665 Accuracy 0.8752500414848328\n",
      "Iteration 38780 Training loss 0.04532342404127121 Validation loss 0.04737840220332146 Accuracy 0.8753750324249268\n",
      "Iteration 38790 Training loss 0.04697732999920845 Validation loss 0.04776841029524803 Accuracy 0.8756250143051147\n",
      "Iteration 38800 Training loss 0.03846634551882744 Validation loss 0.04742736369371414 Accuracy 0.874125063419342\n",
      "Iteration 38810 Training loss 0.04888516291975975 Validation loss 0.047310229390859604 Accuracy 0.8765000700950623\n",
      "Iteration 38820 Training loss 0.03801896423101425 Validation loss 0.04747011512517929 Accuracy 0.8770000338554382\n",
      "Iteration 38830 Training loss 0.04549328610301018 Validation loss 0.047335948795080185 Accuracy 0.8760000467300415\n",
      "Iteration 38840 Training loss 0.04704727977514267 Validation loss 0.04751037806272507 Accuracy 0.8738750219345093\n",
      "Iteration 38850 Training loss 0.04610048234462738 Validation loss 0.04745648428797722 Accuracy 0.8772500157356262\n",
      "Iteration 38860 Training loss 0.04382618889212608 Validation loss 0.047375619411468506 Accuracy 0.874500036239624\n",
      "Iteration 38870 Training loss 0.03868973255157471 Validation loss 0.04746615141630173 Accuracy 0.8733750581741333\n",
      "Iteration 38880 Training loss 0.05248153954744339 Validation loss 0.047460611909627914 Accuracy 0.8758750557899475\n",
      "Iteration 38890 Training loss 0.044900137931108475 Validation loss 0.047323424369096756 Accuracy 0.8755000233650208\n",
      "Iteration 38900 Training loss 0.04510298743844032 Validation loss 0.047368135303258896 Accuracy 0.8753750324249268\n",
      "Iteration 38910 Training loss 0.04002321511507034 Validation loss 0.047374892979860306 Accuracy 0.8753750324249268\n",
      "Iteration 38920 Training loss 0.042957477271556854 Validation loss 0.04773001745343208 Accuracy 0.8765000700950623\n",
      "Iteration 38930 Training loss 0.041494958102703094 Validation loss 0.04730115085840225 Accuracy 0.8748750686645508\n",
      "Iteration 38940 Training loss 0.03761223331093788 Validation loss 0.047391556203365326 Accuracy 0.8757500648498535\n",
      "Iteration 38950 Training loss 0.041957926005125046 Validation loss 0.047336146235466 Accuracy 0.8756250143051147\n",
      "Iteration 38960 Training loss 0.04314853996038437 Validation loss 0.047391701489686966 Accuracy 0.8753750324249268\n",
      "Iteration 38970 Training loss 0.04445264860987663 Validation loss 0.047333572059869766 Accuracy 0.8740000128746033\n",
      "Iteration 38980 Training loss 0.0444338321685791 Validation loss 0.04729372635483742 Accuracy 0.8761250376701355\n",
      "Iteration 38990 Training loss 0.05107685923576355 Validation loss 0.047354958951473236 Accuracy 0.8756250143051147\n",
      "Iteration 39000 Training loss 0.044507306069135666 Validation loss 0.04730620235204697 Accuracy 0.8751250505447388\n",
      "Iteration 39010 Training loss 0.0442221574485302 Validation loss 0.047815170139074326 Accuracy 0.8711250424385071\n",
      "Iteration 39020 Training loss 0.041691310703754425 Validation loss 0.04742686450481415 Accuracy 0.87437504529953\n",
      "Iteration 39030 Training loss 0.045927371829748154 Validation loss 0.04749332740902901 Accuracy 0.8768750429153442\n",
      "Iteration 39040 Training loss 0.041852276772260666 Validation loss 0.04733327776193619 Accuracy 0.8757500648498535\n",
      "Iteration 39050 Training loss 0.05176340416073799 Validation loss 0.047265443950891495 Accuracy 0.8751250505447388\n",
      "Iteration 39060 Training loss 0.04177853465080261 Validation loss 0.047330088913440704 Accuracy 0.8761250376701355\n",
      "Iteration 39070 Training loss 0.043493397533893585 Validation loss 0.047278665006160736 Accuracy 0.8756250143051147\n",
      "Iteration 39080 Training loss 0.04254193231463432 Validation loss 0.0473184660077095 Accuracy 0.8748750686645508\n",
      "Iteration 39090 Training loss 0.044274814426898956 Validation loss 0.04729282483458519 Accuracy 0.8760000467300415\n",
      "Iteration 39100 Training loss 0.041752465069293976 Validation loss 0.04733729735016823 Accuracy 0.8752500414848328\n",
      "Iteration 39110 Training loss 0.03677409142255783 Validation loss 0.04752035811543465 Accuracy 0.8763750195503235\n",
      "Iteration 39120 Training loss 0.04933174327015877 Validation loss 0.04725213348865509 Accuracy 0.8752500414848328\n",
      "Iteration 39130 Training loss 0.04446813091635704 Validation loss 0.0473676398396492 Accuracy 0.8751250505447388\n",
      "Iteration 39140 Training loss 0.042966216802597046 Validation loss 0.0479300431907177 Accuracy 0.8748750686645508\n",
      "Iteration 39150 Training loss 0.04357270523905754 Validation loss 0.04768846556544304 Accuracy 0.8723750710487366\n",
      "Iteration 39160 Training loss 0.041037850081920624 Validation loss 0.04724503681063652 Accuracy 0.8760000467300415\n",
      "Iteration 39170 Training loss 0.04597261920571327 Validation loss 0.04738626256585121 Accuracy 0.8770000338554382\n",
      "Iteration 39180 Training loss 0.03689657896757126 Validation loss 0.047291070222854614 Accuracy 0.877625048160553\n",
      "Iteration 39190 Training loss 0.03815080597996712 Validation loss 0.047557588666677475 Accuracy 0.8760000467300415\n",
      "Iteration 39200 Training loss 0.0427689254283905 Validation loss 0.047213755548000336 Accuracy 0.874250054359436\n",
      "Iteration 39210 Training loss 0.041265662759542465 Validation loss 0.047672729939222336 Accuracy 0.8763750195503235\n",
      "Iteration 39220 Training loss 0.04174747318029404 Validation loss 0.04746421426534653 Accuracy 0.8730000257492065\n",
      "Iteration 39230 Training loss 0.04108728468418121 Validation loss 0.04725221171975136 Accuracy 0.8738750219345093\n",
      "Iteration 39240 Training loss 0.03869267553091049 Validation loss 0.04732483997941017 Accuracy 0.8768750429153442\n",
      "Iteration 39250 Training loss 0.04024519771337509 Validation loss 0.04715694859623909 Accuracy 0.8771250247955322\n",
      "Iteration 39260 Training loss 0.03756439685821533 Validation loss 0.047318655997514725 Accuracy 0.87437504529953\n",
      "Iteration 39270 Training loss 0.04952072352170944 Validation loss 0.04750913009047508 Accuracy 0.8740000128746033\n",
      "Iteration 39280 Training loss 0.037002500146627426 Validation loss 0.04717564582824707 Accuracy 0.8752500414848328\n",
      "Iteration 39290 Training loss 0.03963695839047432 Validation loss 0.04766707867383957 Accuracy 0.8725000619888306\n",
      "Iteration 39300 Training loss 0.04260651394724846 Validation loss 0.04806945100426674 Accuracy 0.8707500696182251\n",
      "Iteration 39310 Training loss 0.04411191865801811 Validation loss 0.04749758541584015 Accuracy 0.8758750557899475\n",
      "Iteration 39320 Training loss 0.04184465482831001 Validation loss 0.04727064073085785 Accuracy 0.8765000700950623\n",
      "Iteration 39330 Training loss 0.05726027861237526 Validation loss 0.047229379415512085 Accuracy 0.8758750557899475\n",
      "Iteration 39340 Training loss 0.04492154344916344 Validation loss 0.04753679037094116 Accuracy 0.8761250376701355\n",
      "Iteration 39350 Training loss 0.03689524903893471 Validation loss 0.04726305603981018 Accuracy 0.8758750557899475\n",
      "Iteration 39360 Training loss 0.043159570544958115 Validation loss 0.04727058857679367 Accuracy 0.8763750195503235\n",
      "Iteration 39370 Training loss 0.04693486541509628 Validation loss 0.047345101833343506 Accuracy 0.8758750557899475\n",
      "Iteration 39380 Training loss 0.04302534461021423 Validation loss 0.04755420237779617 Accuracy 0.8755000233650208\n",
      "Iteration 39390 Training loss 0.046591050922870636 Validation loss 0.047193657606840134 Accuracy 0.8762500286102295\n",
      "Iteration 39400 Training loss 0.034531887620687485 Validation loss 0.04756768047809601 Accuracy 0.8765000700950623\n",
      "Iteration 39410 Training loss 0.04561540484428406 Validation loss 0.0472569465637207 Accuracy 0.8760000467300415\n",
      "Iteration 39420 Training loss 0.04314766824245453 Validation loss 0.047555599361658096 Accuracy 0.8770000338554382\n",
      "Iteration 39430 Training loss 0.040544040501117706 Validation loss 0.04725397005677223 Accuracy 0.874625027179718\n",
      "Iteration 39440 Training loss 0.04241640865802765 Validation loss 0.04737083986401558 Accuracy 0.8738750219345093\n",
      "Iteration 39450 Training loss 0.042436279356479645 Validation loss 0.04759618639945984 Accuracy 0.8756250143051147\n",
      "Iteration 39460 Training loss 0.03962172567844391 Validation loss 0.04735476151108742 Accuracy 0.874500036239624\n",
      "Iteration 39470 Training loss 0.03636068105697632 Validation loss 0.04763098806142807 Accuracy 0.8722500205039978\n",
      "Iteration 39480 Training loss 0.04743752256035805 Validation loss 0.047376323491334915 Accuracy 0.8762500286102295\n",
      "Iteration 39490 Training loss 0.04693274572491646 Validation loss 0.047255244106054306 Accuracy 0.8748750686645508\n",
      "Iteration 39500 Training loss 0.041870005428791046 Validation loss 0.04726159945130348 Accuracy 0.878125011920929\n",
      "Iteration 39510 Training loss 0.040433380752801895 Validation loss 0.04728111997246742 Accuracy 0.874500036239624\n",
      "Iteration 39520 Training loss 0.03653347119688988 Validation loss 0.047270677983760834 Accuracy 0.877750039100647\n",
      "Iteration 39530 Training loss 0.045810602605342865 Validation loss 0.04805813357234001 Accuracy 0.874250054359436\n",
      "Iteration 39540 Training loss 0.051669079810380936 Validation loss 0.04759141802787781 Accuracy 0.8735000491142273\n",
      "Iteration 39550 Training loss 0.03974241763353348 Validation loss 0.04728736728429794 Accuracy 0.877375066280365\n",
      "Iteration 39560 Training loss 0.039662089198827744 Validation loss 0.04729054495692253 Accuracy 0.877625048160553\n",
      "Iteration 39570 Training loss 0.04132131487131119 Validation loss 0.04750499129295349 Accuracy 0.877750039100647\n",
      "Iteration 39580 Training loss 0.04415152966976166 Validation loss 0.04801472648978233 Accuracy 0.8707500696182251\n",
      "Iteration 39590 Training loss 0.045516613870859146 Validation loss 0.047860536724328995 Accuracy 0.8757500648498535\n",
      "Iteration 39600 Training loss 0.04342033341526985 Validation loss 0.047187838703393936 Accuracy 0.8762500286102295\n",
      "Iteration 39610 Training loss 0.04190867021679878 Validation loss 0.0471983402967453 Accuracy 0.8770000338554382\n",
      "Iteration 39620 Training loss 0.04642757400870323 Validation loss 0.047203026711940765 Accuracy 0.8763750195503235\n",
      "Iteration 39630 Training loss 0.039928682148456573 Validation loss 0.04728611931204796 Accuracy 0.8765000700950623\n",
      "Iteration 39640 Training loss 0.04102756083011627 Validation loss 0.047099411487579346 Accuracy 0.877500057220459\n",
      "Iteration 39650 Training loss 0.03753454238176346 Validation loss 0.04770423844456673 Accuracy 0.8757500648498535\n",
      "Iteration 39660 Training loss 0.040924519300460815 Validation loss 0.04716802015900612 Accuracy 0.8765000700950623\n",
      "Iteration 39670 Training loss 0.03902069106698036 Validation loss 0.04711555317044258 Accuracy 0.8761250376701355\n",
      "Iteration 39680 Training loss 0.045570969581604004 Validation loss 0.047166720032691956 Accuracy 0.8768750429153442\n",
      "Iteration 39690 Training loss 0.04787883162498474 Validation loss 0.04710659384727478 Accuracy 0.8772500157356262\n",
      "Iteration 39700 Training loss 0.04157336801290512 Validation loss 0.047908201813697815 Accuracy 0.8712500333786011\n",
      "Iteration 39710 Training loss 0.04254596680402756 Validation loss 0.04707745835185051 Accuracy 0.8766250610351562\n",
      "Iteration 39720 Training loss 0.043997038155794144 Validation loss 0.0471770353615284 Accuracy 0.8762500286102295\n",
      "Iteration 39730 Training loss 0.04393540322780609 Validation loss 0.04732630401849747 Accuracy 0.8740000128746033\n",
      "Iteration 39740 Training loss 0.041796669363975525 Validation loss 0.04705347493290901 Accuracy 0.8756250143051147\n",
      "Iteration 39750 Training loss 0.04780927672982216 Validation loss 0.04717577248811722 Accuracy 0.8762500286102295\n",
      "Iteration 39760 Training loss 0.041694268584251404 Validation loss 0.047047801315784454 Accuracy 0.8768750429153442\n",
      "Iteration 39770 Training loss 0.0421457476913929 Validation loss 0.04702780395746231 Accuracy 0.8762500286102295\n",
      "Iteration 39780 Training loss 0.03841651231050491 Validation loss 0.047046251595020294 Accuracy 0.8760000467300415\n",
      "Iteration 39790 Training loss 0.04025131091475487 Validation loss 0.04718811437487602 Accuracy 0.8765000700950623\n",
      "Iteration 39800 Training loss 0.03782213479280472 Validation loss 0.04701252654194832 Accuracy 0.8772500157356262\n",
      "Iteration 39810 Training loss 0.052154283970594406 Validation loss 0.047362037003040314 Accuracy 0.8770000338554382\n",
      "Iteration 39820 Training loss 0.04544495791196823 Validation loss 0.04705327749252319 Accuracy 0.8762500286102295\n",
      "Iteration 39830 Training loss 0.039009712636470795 Validation loss 0.04725617542862892 Accuracy 0.8765000700950623\n",
      "Iteration 39840 Training loss 0.041341960430145264 Validation loss 0.04706161096692085 Accuracy 0.8771250247955322\n",
      "Iteration 39850 Training loss 0.038764167577028275 Validation loss 0.04803475737571716 Accuracy 0.87437504529953\n",
      "Iteration 39860 Training loss 0.039728231728076935 Validation loss 0.04704941064119339 Accuracy 0.8755000233650208\n",
      "Iteration 39870 Training loss 0.045592937618494034 Validation loss 0.047007966786623 Accuracy 0.8756250143051147\n",
      "Iteration 39880 Training loss 0.04114792123436928 Validation loss 0.04704718664288521 Accuracy 0.874125063419342\n",
      "Iteration 39890 Training loss 0.03830597177147865 Validation loss 0.047201789915561676 Accuracy 0.8762500286102295\n",
      "Iteration 39900 Training loss 0.049951568245887756 Validation loss 0.048353683203458786 Accuracy 0.8688750267028809\n",
      "Iteration 39910 Training loss 0.045290060341358185 Validation loss 0.04727144539356232 Accuracy 0.8736250400543213\n",
      "Iteration 39920 Training loss 0.0420033223927021 Validation loss 0.047184642404317856 Accuracy 0.8737500309944153\n",
      "Iteration 39930 Training loss 0.04039433225989342 Validation loss 0.04700639098882675 Accuracy 0.8756250143051147\n",
      "Iteration 39940 Training loss 0.0411858931183815 Validation loss 0.04802361875772476 Accuracy 0.874125063419342\n",
      "Iteration 39950 Training loss 0.04316221922636032 Validation loss 0.047248583287000656 Accuracy 0.87437504529953\n",
      "Iteration 39960 Training loss 0.05332232266664505 Validation loss 0.04698667675256729 Accuracy 0.8760000467300415\n",
      "Iteration 39970 Training loss 0.0463155061006546 Validation loss 0.04698958992958069 Accuracy 0.877875030040741\n",
      "Iteration 39980 Training loss 0.03822150453925133 Validation loss 0.04697521775960922 Accuracy 0.877750039100647\n",
      "Iteration 39990 Training loss 0.038025952875614166 Validation loss 0.04695839807391167 Accuracy 0.8772500157356262\n",
      "Iteration 40000 Training loss 0.04539550095796585 Validation loss 0.0470607690513134 Accuracy 0.878125011920929\n",
      "Iteration 40010 Training loss 0.04766395315527916 Validation loss 0.046935904771089554 Accuracy 0.8772500157356262\n",
      "Iteration 40020 Training loss 0.04468638449907303 Validation loss 0.047079045325517654 Accuracy 0.8762500286102295\n",
      "Iteration 40030 Training loss 0.04518403112888336 Validation loss 0.04706651344895363 Accuracy 0.8762500286102295\n",
      "Iteration 40040 Training loss 0.04188302904367447 Validation loss 0.047345805913209915 Accuracy 0.8728750348091125\n",
      "Iteration 40050 Training loss 0.03624729812145233 Validation loss 0.04701899364590645 Accuracy 0.8751250505447388\n",
      "Iteration 40060 Training loss 0.04823892191052437 Validation loss 0.04774124547839165 Accuracy 0.8760000467300415\n",
      "Iteration 40070 Training loss 0.037770770490169525 Validation loss 0.046940822154283524 Accuracy 0.8767500519752502\n",
      "Iteration 40080 Training loss 0.04417344555258751 Validation loss 0.046938683837652206 Accuracy 0.8756250143051147\n",
      "Iteration 40090 Training loss 0.052151069045066833 Validation loss 0.047088541090488434 Accuracy 0.8757500648498535\n",
      "Iteration 40100 Training loss 0.046106968075037 Validation loss 0.04697731137275696 Accuracy 0.8760000467300415\n",
      "Iteration 40110 Training loss 0.04239000380039215 Validation loss 0.046984970569610596 Accuracy 0.877750039100647\n",
      "Iteration 40120 Training loss 0.04717318341135979 Validation loss 0.047013841569423676 Accuracy 0.8766250610351562\n",
      "Iteration 40130 Training loss 0.04088042303919792 Validation loss 0.04832702502608299 Accuracy 0.8685000538825989\n",
      "Iteration 40140 Training loss 0.05096138268709183 Validation loss 0.046994853764772415 Accuracy 0.8763750195503235\n",
      "Iteration 40150 Training loss 0.038565635681152344 Validation loss 0.04703393951058388 Accuracy 0.8768750429153442\n",
      "Iteration 40160 Training loss 0.04376503452658653 Validation loss 0.04740118980407715 Accuracy 0.8772500157356262\n",
      "Iteration 40170 Training loss 0.043636396527290344 Validation loss 0.04708375036716461 Accuracy 0.877500057220459\n",
      "Iteration 40180 Training loss 0.04224953427910805 Validation loss 0.04699259251356125 Accuracy 0.8765000700950623\n",
      "Iteration 40190 Training loss 0.043050508946180344 Validation loss 0.04713861271739006 Accuracy 0.8750000596046448\n",
      "Iteration 40200 Training loss 0.04259902983903885 Validation loss 0.046986937522888184 Accuracy 0.8768750429153442\n",
      "Iteration 40210 Training loss 0.038906827569007874 Validation loss 0.046939052641391754 Accuracy 0.8765000700950623\n",
      "Iteration 40220 Training loss 0.04029424116015434 Validation loss 0.04697871580719948 Accuracy 0.8760000467300415\n",
      "Iteration 40230 Training loss 0.03743651509284973 Validation loss 0.04692431166768074 Accuracy 0.8762500286102295\n",
      "Iteration 40240 Training loss 0.03652257099747658 Validation loss 0.046957939863204956 Accuracy 0.8761250376701355\n",
      "Iteration 40250 Training loss 0.04691895470023155 Validation loss 0.04699673131108284 Accuracy 0.8758750557899475\n",
      "Iteration 40260 Training loss 0.03645511716604233 Validation loss 0.04694391041994095 Accuracy 0.8756250143051147\n",
      "Iteration 40270 Training loss 0.03660096973180771 Validation loss 0.04744182154536247 Accuracy 0.8765000700950623\n",
      "Iteration 40280 Training loss 0.04045726731419563 Validation loss 0.04713309556245804 Accuracy 0.8751250505447388\n",
      "Iteration 40290 Training loss 0.04759825021028519 Validation loss 0.0481710322201252 Accuracy 0.8731250166893005\n",
      "Iteration 40300 Training loss 0.04637691751122475 Validation loss 0.04739190265536308 Accuracy 0.8736250400543213\n",
      "Iteration 40310 Training loss 0.04649882763624191 Validation loss 0.04710019379854202 Accuracy 0.877375066280365\n",
      "Iteration 40320 Training loss 0.04531047120690346 Validation loss 0.04698437079787254 Accuracy 0.8771250247955322\n",
      "Iteration 40330 Training loss 0.04930600896477699 Validation loss 0.047826554626226425 Accuracy 0.8711250424385071\n",
      "Iteration 40340 Training loss 0.04219019412994385 Validation loss 0.046969544142484665 Accuracy 0.8768750429153442\n",
      "Iteration 40350 Training loss 0.040815237909555435 Validation loss 0.04697326198220253 Accuracy 0.8758750557899475\n",
      "Iteration 40360 Training loss 0.04270397871732712 Validation loss 0.04697741940617561 Accuracy 0.8760000467300415\n",
      "Iteration 40370 Training loss 0.051076795905828476 Validation loss 0.0469825305044651 Accuracy 0.8761250376701355\n",
      "Iteration 40380 Training loss 0.04634576663374901 Validation loss 0.0469227209687233 Accuracy 0.8760000467300415\n",
      "Iteration 40390 Training loss 0.0435919463634491 Validation loss 0.047173768281936646 Accuracy 0.877500057220459\n",
      "Iteration 40400 Training loss 0.0384407564997673 Validation loss 0.0470007061958313 Accuracy 0.877375066280365\n",
      "Iteration 40410 Training loss 0.04299160838127136 Validation loss 0.04697958752512932 Accuracy 0.8765000700950623\n",
      "Iteration 40420 Training loss 0.039966095238924026 Validation loss 0.04691798612475395 Accuracy 0.8758750557899475\n",
      "Iteration 40430 Training loss 0.051546573638916016 Validation loss 0.04741066321730614 Accuracy 0.8767500519752502\n",
      "Iteration 40440 Training loss 0.039844922721385956 Validation loss 0.046934425830841064 Accuracy 0.8760000467300415\n",
      "Iteration 40450 Training loss 0.04462622106075287 Validation loss 0.04694626107811928 Accuracy 0.8765000700950623\n",
      "Iteration 40460 Training loss 0.04169687256217003 Validation loss 0.04696207493543625 Accuracy 0.8765000700950623\n",
      "Iteration 40470 Training loss 0.05043565481901169 Validation loss 0.04693274199962616 Accuracy 0.8762500286102295\n",
      "Iteration 40480 Training loss 0.04693808779120445 Validation loss 0.046917401254177094 Accuracy 0.8762500286102295\n",
      "Iteration 40490 Training loss 0.03368678316473961 Validation loss 0.04746273159980774 Accuracy 0.8763750195503235\n",
      "Iteration 40500 Training loss 0.04581528156995773 Validation loss 0.047240063548088074 Accuracy 0.8732500672340393\n",
      "Iteration 40510 Training loss 0.05681554228067398 Validation loss 0.047112978994846344 Accuracy 0.8758750557899475\n",
      "Iteration 40520 Training loss 0.04364996775984764 Validation loss 0.046924982219934464 Accuracy 0.8766250610351562\n",
      "Iteration 40530 Training loss 0.04349781945347786 Validation loss 0.04689701274037361 Accuracy 0.8771250247955322\n",
      "Iteration 40540 Training loss 0.04649600014090538 Validation loss 0.04705851525068283 Accuracy 0.877500057220459\n",
      "Iteration 40550 Training loss 0.041491370648145676 Validation loss 0.04714887589216232 Accuracy 0.874625027179718\n",
      "Iteration 40560 Training loss 0.03543645888566971 Validation loss 0.046975672245025635 Accuracy 0.877500057220459\n",
      "Iteration 40570 Training loss 0.04299319162964821 Validation loss 0.04825938120484352 Accuracy 0.8728750348091125\n",
      "Iteration 40580 Training loss 0.04590805992484093 Validation loss 0.046926744282245636 Accuracy 0.877875030040741\n",
      "Iteration 40590 Training loss 0.041273653507232666 Validation loss 0.04691322520375252 Accuracy 0.8767500519752502\n",
      "Iteration 40600 Training loss 0.04272341728210449 Validation loss 0.04716448485851288 Accuracy 0.877875030040741\n",
      "Iteration 40610 Training loss 0.039856962859630585 Validation loss 0.04695454612374306 Accuracy 0.8755000233650208\n",
      "Iteration 40620 Training loss 0.03971691429615021 Validation loss 0.04690568521618843 Accuracy 0.8762500286102295\n",
      "Iteration 40630 Training loss 0.04276387020945549 Validation loss 0.04688045009970665 Accuracy 0.8772500157356262\n",
      "Iteration 40640 Training loss 0.04179265722632408 Validation loss 0.04687400907278061 Accuracy 0.8772500157356262\n",
      "Iteration 40650 Training loss 0.04193601384758949 Validation loss 0.046876732259988785 Accuracy 0.878125011920929\n",
      "Iteration 40660 Training loss 0.04102964326739311 Validation loss 0.04699542373418808 Accuracy 0.8763750195503235\n",
      "Iteration 40670 Training loss 0.03396012634038925 Validation loss 0.04690602049231529 Accuracy 0.877625048160553\n",
      "Iteration 40680 Training loss 0.04159180447459221 Validation loss 0.047054536640644073 Accuracy 0.8782500624656677\n",
      "Iteration 40690 Training loss 0.04467124864459038 Validation loss 0.04694412276148796 Accuracy 0.8788750171661377\n",
      "Iteration 40700 Training loss 0.05057873949408531 Validation loss 0.046940527856349945 Accuracy 0.8768750429153442\n",
      "Iteration 40710 Training loss 0.042595818638801575 Validation loss 0.04748833552002907 Accuracy 0.8733750581741333\n",
      "Iteration 40720 Training loss 0.03472956269979477 Validation loss 0.04710086062550545 Accuracy 0.8786250352859497\n",
      "Iteration 40730 Training loss 0.03655650466680527 Validation loss 0.046927232295274734 Accuracy 0.8767500519752502\n",
      "Iteration 40740 Training loss 0.04535144567489624 Validation loss 0.04692689701914787 Accuracy 0.877625048160553\n",
      "Iteration 40750 Training loss 0.041773781180381775 Validation loss 0.046897053718566895 Accuracy 0.8766250610351562\n",
      "Iteration 40760 Training loss 0.04157339781522751 Validation loss 0.04706763103604317 Accuracy 0.877375066280365\n",
      "Iteration 40770 Training loss 0.038517165929079056 Validation loss 0.04693066328763962 Accuracy 0.8762500286102295\n",
      "Iteration 40780 Training loss 0.04652153328061104 Validation loss 0.046877432614564896 Accuracy 0.8768750429153442\n",
      "Iteration 40790 Training loss 0.04427637159824371 Validation loss 0.047008875757455826 Accuracy 0.8787500262260437\n",
      "Iteration 40800 Training loss 0.04986787214875221 Validation loss 0.0468982569873333 Accuracy 0.8771250247955322\n",
      "Iteration 40810 Training loss 0.04421404376626015 Validation loss 0.04725975915789604 Accuracy 0.8768750429153442\n",
      "Iteration 40820 Training loss 0.04428533464670181 Validation loss 0.04770646244287491 Accuracy 0.8752500414848328\n",
      "Iteration 40830 Training loss 0.04193919897079468 Validation loss 0.04850464314222336 Accuracy 0.8703750371932983\n",
      "Iteration 40840 Training loss 0.04311827942728996 Validation loss 0.04691930115222931 Accuracy 0.878125011920929\n",
      "Iteration 40850 Training loss 0.03735140711069107 Validation loss 0.04682708904147148 Accuracy 0.8783750534057617\n",
      "Iteration 40860 Training loss 0.046402521431446075 Validation loss 0.04682019352912903 Accuracy 0.8782500624656677\n",
      "Iteration 40870 Training loss 0.04098769277334213 Validation loss 0.046914976090192795 Accuracy 0.8767500519752502\n",
      "Iteration 40880 Training loss 0.042940668761730194 Validation loss 0.04720963537693024 Accuracy 0.874250054359436\n",
      "Iteration 40890 Training loss 0.043803948909044266 Validation loss 0.046953216195106506 Accuracy 0.877750039100647\n",
      "Iteration 40900 Training loss 0.04318591207265854 Validation loss 0.0477716438472271 Accuracy 0.8758750557899475\n",
      "Iteration 40910 Training loss 0.045894868671894073 Validation loss 0.0467904694378376 Accuracy 0.877500057220459\n",
      "Iteration 40920 Training loss 0.03843450918793678 Validation loss 0.04693117365241051 Accuracy 0.8785000443458557\n",
      "Iteration 40930 Training loss 0.04455462470650673 Validation loss 0.04675428196787834 Accuracy 0.878000020980835\n",
      "Iteration 40940 Training loss 0.03901052102446556 Validation loss 0.046740952879190445 Accuracy 0.8772500157356262\n",
      "Iteration 40950 Training loss 0.047060687094926834 Validation loss 0.048619769513607025 Accuracy 0.8706250190734863\n",
      "Iteration 40960 Training loss 0.04116593301296234 Validation loss 0.04681849479675293 Accuracy 0.877875030040741\n",
      "Iteration 40970 Training loss 0.04331362247467041 Validation loss 0.04681381210684776 Accuracy 0.877625048160553\n",
      "Iteration 40980 Training loss 0.037700504064559937 Validation loss 0.047346506267786026 Accuracy 0.874250054359436\n",
      "Iteration 40990 Training loss 0.04634712263941765 Validation loss 0.046820979565382004 Accuracy 0.878000020980835\n",
      "Iteration 41000 Training loss 0.0375664159655571 Validation loss 0.04688810184597969 Accuracy 0.8786250352859497\n",
      "Iteration 41010 Training loss 0.054459162056446075 Validation loss 0.04678720235824585 Accuracy 0.8790000677108765\n",
      "Iteration 41020 Training loss 0.04458102583885193 Validation loss 0.04692428559064865 Accuracy 0.8755000233650208\n",
      "Iteration 41030 Training loss 0.04376881569623947 Validation loss 0.04694009944796562 Accuracy 0.8783750534057617\n",
      "Iteration 41040 Training loss 0.041712190955877304 Validation loss 0.046768367290496826 Accuracy 0.877500057220459\n",
      "Iteration 41050 Training loss 0.04159869998693466 Validation loss 0.046848539263010025 Accuracy 0.877500057220459\n",
      "Iteration 41060 Training loss 0.04960217326879501 Validation loss 0.04678205028176308 Accuracy 0.8770000338554382\n",
      "Iteration 41070 Training loss 0.04265567660331726 Validation loss 0.04684208706021309 Accuracy 0.877875030040741\n",
      "Iteration 41080 Training loss 0.04278038442134857 Validation loss 0.046759456396102905 Accuracy 0.878125011920929\n",
      "Iteration 41090 Training loss 0.0366983599960804 Validation loss 0.04685255140066147 Accuracy 0.8787500262260437\n",
      "Iteration 41100 Training loss 0.04164549708366394 Validation loss 0.0469360388815403 Accuracy 0.8771250247955322\n",
      "Iteration 41110 Training loss 0.04113046079874039 Validation loss 0.04754406958818436 Accuracy 0.8726250529289246\n",
      "Iteration 41120 Training loss 0.03226489573717117 Validation loss 0.04729144647717476 Accuracy 0.874625027179718\n",
      "Iteration 41130 Training loss 0.04983179643750191 Validation loss 0.04684378206729889 Accuracy 0.8767500519752502\n",
      "Iteration 41140 Training loss 0.037818919867277145 Validation loss 0.04674097150564194 Accuracy 0.8787500262260437\n",
      "Iteration 41150 Training loss 0.042802099138498306 Validation loss 0.04742977395653725 Accuracy 0.8735000491142273\n",
      "Iteration 41160 Training loss 0.0452071838080883 Validation loss 0.04705677926540375 Accuracy 0.874500036239624\n",
      "Iteration 41170 Training loss 0.03949807956814766 Validation loss 0.04677872732281685 Accuracy 0.8768750429153442\n",
      "Iteration 41180 Training loss 0.042486608028411865 Validation loss 0.046858806163072586 Accuracy 0.8770000338554382\n",
      "Iteration 41190 Training loss 0.04033937305212021 Validation loss 0.046719394624233246 Accuracy 0.8782500624656677\n",
      "Iteration 41200 Training loss 0.039670638740062714 Validation loss 0.047044090926647186 Accuracy 0.8768750429153442\n",
      "Iteration 41210 Training loss 0.04494413733482361 Validation loss 0.046724844723939896 Accuracy 0.877750039100647\n",
      "Iteration 41220 Training loss 0.03289675712585449 Validation loss 0.04673673212528229 Accuracy 0.8771250247955322\n",
      "Iteration 41230 Training loss 0.038603831082582474 Validation loss 0.04676812142133713 Accuracy 0.877625048160553\n",
      "Iteration 41240 Training loss 0.04327801614999771 Validation loss 0.046876490116119385 Accuracy 0.8782500624656677\n",
      "Iteration 41250 Training loss 0.04569718614220619 Validation loss 0.046832673251628876 Accuracy 0.8771250247955322\n",
      "Iteration 41260 Training loss 0.03898429125547409 Validation loss 0.04692970588803291 Accuracy 0.8765000700950623\n",
      "Iteration 41270 Training loss 0.042184896767139435 Validation loss 0.04672667011618614 Accuracy 0.878125011920929\n",
      "Iteration 41280 Training loss 0.036594703793525696 Validation loss 0.0467383973300457 Accuracy 0.878125011920929\n",
      "Iteration 41290 Training loss 0.04451339691877365 Validation loss 0.046855758875608444 Accuracy 0.877750039100647\n",
      "Iteration 41300 Training loss 0.039971914142370224 Validation loss 0.04688210412859917 Accuracy 0.8757500648498535\n",
      "Iteration 41310 Training loss 0.04166889190673828 Validation loss 0.047442734241485596 Accuracy 0.8723750710487366\n",
      "Iteration 41320 Training loss 0.04223398491740227 Validation loss 0.04705765098333359 Accuracy 0.8771250247955322\n",
      "Iteration 41330 Training loss 0.041905730962753296 Validation loss 0.046804167330265045 Accuracy 0.8768750429153442\n",
      "Iteration 41340 Training loss 0.04735102877020836 Validation loss 0.04696744307875633 Accuracy 0.8763750195503235\n",
      "Iteration 41350 Training loss 0.0392770878970623 Validation loss 0.04725146293640137 Accuracy 0.8767500519752502\n",
      "Iteration 41360 Training loss 0.04548131674528122 Validation loss 0.04711133614182472 Accuracy 0.8770000338554382\n",
      "Iteration 41370 Training loss 0.04577012360095978 Validation loss 0.047100234776735306 Accuracy 0.8750000596046448\n",
      "Iteration 41380 Training loss 0.041142839938402176 Validation loss 0.04664216190576553 Accuracy 0.8783750534057617\n",
      "Iteration 41390 Training loss 0.04499812796711922 Validation loss 0.046679604798555374 Accuracy 0.8786250352859497\n",
      "Iteration 41400 Training loss 0.05316166579723358 Validation loss 0.046693965792655945 Accuracy 0.8788750171661377\n",
      "Iteration 41410 Training loss 0.05024697631597519 Validation loss 0.04667118191719055 Accuracy 0.878000020980835\n",
      "Iteration 41420 Training loss 0.04263676702976227 Validation loss 0.046697281301021576 Accuracy 0.8771250247955322\n",
      "Iteration 41430 Training loss 0.04388487711548805 Validation loss 0.0471169538795948 Accuracy 0.8767500519752502\n",
      "Iteration 41440 Training loss 0.04420770704746246 Validation loss 0.046730026602745056 Accuracy 0.8782500624656677\n",
      "Iteration 41450 Training loss 0.04291342571377754 Validation loss 0.047202449291944504 Accuracy 0.877500057220459\n",
      "Iteration 41460 Training loss 0.03937229514122009 Validation loss 0.04671298339962959 Accuracy 0.878125011920929\n",
      "Iteration 41470 Training loss 0.04194449260830879 Validation loss 0.04665958508849144 Accuracy 0.878125011920929\n",
      "Iteration 41480 Training loss 0.04151111841201782 Validation loss 0.047456782311201096 Accuracy 0.8750000596046448\n",
      "Iteration 41490 Training loss 0.03746086359024048 Validation loss 0.046739306300878525 Accuracy 0.877625048160553\n",
      "Iteration 41500 Training loss 0.035189468413591385 Validation loss 0.046755243092775345 Accuracy 0.8768750429153442\n",
      "Iteration 41510 Training loss 0.038995396345853806 Validation loss 0.04673057049512863 Accuracy 0.8767500519752502\n",
      "Iteration 41520 Training loss 0.04203370586037636 Validation loss 0.047483012080192566 Accuracy 0.8728750348091125\n",
      "Iteration 41530 Training loss 0.03834587335586548 Validation loss 0.04666108265519142 Accuracy 0.8786250352859497\n",
      "Iteration 41540 Training loss 0.0449204258620739 Validation loss 0.04680660739541054 Accuracy 0.877500057220459\n",
      "Iteration 41550 Training loss 0.048134759068489075 Validation loss 0.0470455102622509 Accuracy 0.877500057220459\n",
      "Iteration 41560 Training loss 0.0401545986533165 Validation loss 0.046711213886737823 Accuracy 0.878000020980835\n",
      "Iteration 41570 Training loss 0.04460589960217476 Validation loss 0.04671993479132652 Accuracy 0.877625048160553\n",
      "Iteration 41580 Training loss 0.04367892071604729 Validation loss 0.04699935391545296 Accuracy 0.8788750171661377\n",
      "Iteration 41590 Training loss 0.04753856733441353 Validation loss 0.04684150964021683 Accuracy 0.8792500495910645\n",
      "Iteration 41600 Training loss 0.04454965889453888 Validation loss 0.04673153534531593 Accuracy 0.8786250352859497\n",
      "Iteration 41610 Training loss 0.037919677793979645 Validation loss 0.04670697823166847 Accuracy 0.8782500624656677\n",
      "Iteration 41620 Training loss 0.042725466191768646 Validation loss 0.046827174723148346 Accuracy 0.8786250352859497\n",
      "Iteration 41630 Training loss 0.04873524233698845 Validation loss 0.04744832217693329 Accuracy 0.874625027179718\n",
      "Iteration 41640 Training loss 0.0345979668200016 Validation loss 0.04661885276436806 Accuracy 0.878000020980835\n",
      "Iteration 41650 Training loss 0.04042435437440872 Validation loss 0.046715203672647476 Accuracy 0.8783750534057617\n",
      "Iteration 41660 Training loss 0.042830657213926315 Validation loss 0.04670850560069084 Accuracy 0.8786250352859497\n",
      "Iteration 41670 Training loss 0.04226303845643997 Validation loss 0.04668941721320152 Accuracy 0.8783750534057617\n",
      "Iteration 41680 Training loss 0.03951798751950264 Validation loss 0.04759080708026886 Accuracy 0.8710000514984131\n",
      "Iteration 41690 Training loss 0.03423086926341057 Validation loss 0.04676574096083641 Accuracy 0.8763750195503235\n",
      "Iteration 41700 Training loss 0.047875355929136276 Validation loss 0.04708171263337135 Accuracy 0.8750000596046448\n",
      "Iteration 41710 Training loss 0.03660273551940918 Validation loss 0.04726152867078781 Accuracy 0.8771250247955322\n",
      "Iteration 41720 Training loss 0.03637994825839996 Validation loss 0.04674476012587547 Accuracy 0.877875030040741\n",
      "Iteration 41730 Training loss 0.04430929198861122 Validation loss 0.04672696813941002 Accuracy 0.878000020980835\n",
      "Iteration 41740 Training loss 0.041698601096868515 Validation loss 0.04681392014026642 Accuracy 0.8753750324249268\n",
      "Iteration 41750 Training loss 0.03850916400551796 Validation loss 0.046654071658849716 Accuracy 0.877375066280365\n",
      "Iteration 41760 Training loss 0.04739544913172722 Validation loss 0.0466546006500721 Accuracy 0.878000020980835\n",
      "Iteration 41770 Training loss 0.048226673156023026 Validation loss 0.04689362645149231 Accuracy 0.8782500624656677\n",
      "Iteration 41780 Training loss 0.03880270570516586 Validation loss 0.046708617359399796 Accuracy 0.8767500519752502\n",
      "Iteration 41790 Training loss 0.045520417392253876 Validation loss 0.04656291380524635 Accuracy 0.8785000443458557\n",
      "Iteration 41800 Training loss 0.03685217350721359 Validation loss 0.046621453016996384 Accuracy 0.8783750534057617\n",
      "Iteration 41810 Training loss 0.043265704065561295 Validation loss 0.04686698317527771 Accuracy 0.8748750686645508\n",
      "Iteration 41820 Training loss 0.042327649891376495 Validation loss 0.04672388732433319 Accuracy 0.8783750534057617\n",
      "Iteration 41830 Training loss 0.03873366862535477 Validation loss 0.0473349392414093 Accuracy 0.8750000596046448\n",
      "Iteration 41840 Training loss 0.04307999089360237 Validation loss 0.04726245999336243 Accuracy 0.8756250143051147\n",
      "Iteration 41850 Training loss 0.04867270216345787 Validation loss 0.047908611595630646 Accuracy 0.874125063419342\n",
      "Iteration 41860 Training loss 0.04186958819627762 Validation loss 0.047891926020383835 Accuracy 0.8740000128746033\n",
      "Iteration 41870 Training loss 0.04251553863286972 Validation loss 0.04662424698472023 Accuracy 0.877625048160553\n",
      "Iteration 41880 Training loss 0.048114076256752014 Validation loss 0.04664885252714157 Accuracy 0.878000020980835\n",
      "Iteration 41890 Training loss 0.04305819794535637 Validation loss 0.046729471534490585 Accuracy 0.8786250352859497\n",
      "Iteration 41900 Training loss 0.04657566919922829 Validation loss 0.04711920768022537 Accuracy 0.877375066280365\n",
      "Iteration 41910 Training loss 0.042502760887145996 Validation loss 0.047352295368909836 Accuracy 0.8737500309944153\n",
      "Iteration 41920 Training loss 0.040624819695949554 Validation loss 0.04660218954086304 Accuracy 0.8783750534057617\n",
      "Iteration 41930 Training loss 0.04119977727532387 Validation loss 0.047083932906389236 Accuracy 0.8765000700950623\n",
      "Iteration 41940 Training loss 0.0368184968829155 Validation loss 0.04665244370698929 Accuracy 0.878000020980835\n",
      "Iteration 41950 Training loss 0.0433964729309082 Validation loss 0.04701191931962967 Accuracy 0.8765000700950623\n",
      "Iteration 41960 Training loss 0.04381348937749863 Validation loss 0.046646736562252045 Accuracy 0.8770000338554382\n",
      "Iteration 41970 Training loss 0.03913380205631256 Validation loss 0.046574387699365616 Accuracy 0.878000020980835\n",
      "Iteration 41980 Training loss 0.04620080068707466 Validation loss 0.04680737107992172 Accuracy 0.8770000338554382\n",
      "Iteration 41990 Training loss 0.041320111602544785 Validation loss 0.04657459259033203 Accuracy 0.877500057220459\n",
      "Iteration 42000 Training loss 0.03729825094342232 Validation loss 0.048108138144016266 Accuracy 0.8727500438690186\n",
      "Iteration 42010 Training loss 0.04173006862401962 Validation loss 0.046916358172893524 Accuracy 0.8765000700950623\n",
      "Iteration 42020 Training loss 0.039565376937389374 Validation loss 0.04660630226135254 Accuracy 0.877625048160553\n",
      "Iteration 42030 Training loss 0.04114986211061478 Validation loss 0.046761516481637955 Accuracy 0.8765000700950623\n",
      "Iteration 42040 Training loss 0.04487433284521103 Validation loss 0.04663146287202835 Accuracy 0.877375066280365\n",
      "Iteration 42050 Training loss 0.03578557074069977 Validation loss 0.04675210639834404 Accuracy 0.877625048160553\n",
      "Iteration 42060 Training loss 0.05282365158200264 Validation loss 0.04657093435525894 Accuracy 0.878125011920929\n",
      "Iteration 42070 Training loss 0.03633498400449753 Validation loss 0.046701692044734955 Accuracy 0.8765000700950623\n",
      "Iteration 42080 Training loss 0.03843941539525986 Validation loss 0.047373779118061066 Accuracy 0.8728750348091125\n",
      "Iteration 42090 Training loss 0.039720457047224045 Validation loss 0.046588946133852005 Accuracy 0.8785000443458557\n",
      "Iteration 42100 Training loss 0.04790776968002319 Validation loss 0.04686933383345604 Accuracy 0.877500057220459\n",
      "Iteration 42110 Training loss 0.04359113425016403 Validation loss 0.04654966667294502 Accuracy 0.877500057220459\n",
      "Iteration 42120 Training loss 0.04188917949795723 Validation loss 0.04688788205385208 Accuracy 0.878125011920929\n",
      "Iteration 42130 Training loss 0.04912981763482094 Validation loss 0.04657289385795593 Accuracy 0.878000020980835\n",
      "Iteration 42140 Training loss 0.0324777252972126 Validation loss 0.04658733680844307 Accuracy 0.8787500262260437\n",
      "Iteration 42150 Training loss 0.036740440875291824 Validation loss 0.046625103801488876 Accuracy 0.8782500624656677\n",
      "Iteration 42160 Training loss 0.03860168531537056 Validation loss 0.046719301491975784 Accuracy 0.8792500495910645\n",
      "Iteration 42170 Training loss 0.04670887440443039 Validation loss 0.046569596976041794 Accuracy 0.877875030040741\n",
      "Iteration 42180 Training loss 0.04876387491822243 Validation loss 0.04676401987671852 Accuracy 0.8785000443458557\n",
      "Iteration 42190 Training loss 0.03896903991699219 Validation loss 0.04662670940160751 Accuracy 0.8791250586509705\n",
      "Iteration 42200 Training loss 0.04301758110523224 Validation loss 0.046518586575984955 Accuracy 0.8772500157356262\n",
      "Iteration 42210 Training loss 0.04256075248122215 Validation loss 0.046828873455524445 Accuracy 0.8765000700950623\n",
      "Iteration 42220 Training loss 0.03523290902376175 Validation loss 0.046719636768102646 Accuracy 0.877750039100647\n",
      "Iteration 42230 Training loss 0.04161932319402695 Validation loss 0.04695073515176773 Accuracy 0.877875030040741\n",
      "Iteration 42240 Training loss 0.037073198705911636 Validation loss 0.04649991914629936 Accuracy 0.8770000338554382\n",
      "Iteration 42250 Training loss 0.03863589093089104 Validation loss 0.04648267477750778 Accuracy 0.8765000700950623\n",
      "Iteration 42260 Training loss 0.04641987755894661 Validation loss 0.046536583453416824 Accuracy 0.8786250352859497\n",
      "Iteration 42270 Training loss 0.04526730254292488 Validation loss 0.047020476311445236 Accuracy 0.877375066280365\n",
      "Iteration 42280 Training loss 0.036507751792669296 Validation loss 0.04702452942728996 Accuracy 0.87437504529953\n",
      "Iteration 42290 Training loss 0.03509807959198952 Validation loss 0.04663991183042526 Accuracy 0.8795000314712524\n",
      "Iteration 42300 Training loss 0.03984331712126732 Validation loss 0.047022294253110886 Accuracy 0.8783750534057617\n",
      "Iteration 42310 Training loss 0.03885302320122719 Validation loss 0.04685036465525627 Accuracy 0.8762500286102295\n",
      "Iteration 42320 Training loss 0.03917285427451134 Validation loss 0.047564439475536346 Accuracy 0.8716250658035278\n",
      "Iteration 42330 Training loss 0.0382281169295311 Validation loss 0.047226432710886 Accuracy 0.8771250247955322\n",
      "Iteration 42340 Training loss 0.04229661822319031 Validation loss 0.046503931283950806 Accuracy 0.8783750534057617\n",
      "Iteration 42350 Training loss 0.03837358206510544 Validation loss 0.04646565392613411 Accuracy 0.878125011920929\n",
      "Iteration 42360 Training loss 0.045251186937093735 Validation loss 0.0464923121035099 Accuracy 0.8786250352859497\n",
      "Iteration 42370 Training loss 0.037725914269685745 Validation loss 0.04669896140694618 Accuracy 0.877375066280365\n",
      "Iteration 42380 Training loss 0.03704821318387985 Validation loss 0.04691701754927635 Accuracy 0.8751250505447388\n",
      "Iteration 42390 Training loss 0.04352520778775215 Validation loss 0.04786036163568497 Accuracy 0.8733750581741333\n",
      "Iteration 42400 Training loss 0.04434698447585106 Validation loss 0.0466325506567955 Accuracy 0.8771250247955322\n",
      "Iteration 42410 Training loss 0.04557659104466438 Validation loss 0.04763910174369812 Accuracy 0.8705000281333923\n",
      "Iteration 42420 Training loss 0.04233168065547943 Validation loss 0.04653850197792053 Accuracy 0.8782500624656677\n",
      "Iteration 42430 Training loss 0.04227332025766373 Validation loss 0.04647206515073776 Accuracy 0.877750039100647\n",
      "Iteration 42440 Training loss 0.04257091507315636 Validation loss 0.04647946357727051 Accuracy 0.8790000677108765\n",
      "Iteration 42450 Training loss 0.04003534093499184 Validation loss 0.046839211136102676 Accuracy 0.8756250143051147\n",
      "Iteration 42460 Training loss 0.03621673211455345 Validation loss 0.047043222934007645 Accuracy 0.8772500157356262\n",
      "Iteration 42470 Training loss 0.0445575937628746 Validation loss 0.04686363413929939 Accuracy 0.8783750534057617\n",
      "Iteration 42480 Training loss 0.04771290719509125 Validation loss 0.04670363664627075 Accuracy 0.877375066280365\n",
      "Iteration 42490 Training loss 0.04499567300081253 Validation loss 0.04652577266097069 Accuracy 0.8787500262260437\n",
      "Iteration 42500 Training loss 0.037032317370176315 Validation loss 0.046588290482759476 Accuracy 0.8798750638961792\n",
      "Iteration 42510 Training loss 0.046189308166503906 Validation loss 0.04670002683997154 Accuracy 0.8791250586509705\n",
      "Iteration 42520 Training loss 0.04064662754535675 Validation loss 0.04646201804280281 Accuracy 0.8788750171661377\n",
      "Iteration 42530 Training loss 0.03935462236404419 Validation loss 0.04676548019051552 Accuracy 0.8785000443458557\n",
      "Iteration 42540 Training loss 0.046368587762117386 Validation loss 0.04789271950721741 Accuracy 0.8727500438690186\n",
      "Iteration 42550 Training loss 0.044055551290512085 Validation loss 0.046862222254276276 Accuracy 0.878000020980835\n",
      "Iteration 42560 Training loss 0.04215564206242561 Validation loss 0.04738985747098923 Accuracy 0.8728750348091125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training done'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 1.5, 1e-3, 0, 0, 0, 1, 0.01, 10, True, False, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "16479094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer = two_layer_NN(3072,2048,10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2, 1e-5, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer = three_layer_NN(784, 512, 512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44572e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained = three_layer_NN(784, 2048, 2048, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10, True, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a65f0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAH1CAYAAACgD9t1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAoKVJREFUeJzt3QWYVFUbB/B3Yenu7m4UkFABBQRBMEAQFAQRMFFBBRRELPgUBDEwUUQ6FUHBIkVKSkq6u3OX3b3f8z/rGe7OTu/EnZn/z2ecZebO7Xjvuee8J8YwDEOIiIiIiMhS0oV6BoiIiIiIKDUG6kREREREFsRAnYiIiIjIghioExERERFZEAN1IiIiIiILYqBORERERGRBDNSJiIiIiCyIgToRERERkQUxUCciIiIisiAG6kREUSQpKSnUs0BEAcBjOzIxUA+hzZs3S/fu3aVMmTKSNWtWqV69uowYMUKuXLni8zjj4+Nl6tSp0qRJE7njjjv8Or9E0Wrnzp3y4osvSv78+WXx4sWWHacr+/btk549e8rGjRsDPi0iCj6cUx5//HE5ePBgqGeF/Mnw0MSJEw0M7urVpk0bT0cX9b755hujbNmyxtq1a43z588b77zzjm093nbbbT6Nc+TIkUapUqVs42nSpIlhRRcvXjS++OILo127dkbJkiWNTJkyGZkzZzYqVKhgPProo8ZPP/1kJCUlGdevX1efnT17Vv1u+PDhbvfBgwcPGtOnTzcee+wxo3Dhwm6Ht3/98ccfbuf/hx9+MB566CGPlxfb+IknnjBq1KjhcJrp06c3smXLptbFHXfcYQwbNsw4efKk1+t1//79xrPPPmtUqVLF498kJCSobXHTTTcZWbNmVfvPiy++qPZJMoydO3ca9957rxETE+PVPuLKvn37jPvuu09td3+N05158+YZDRo0MLZt2+ZwHxg7dqxRq1YtdRzmyJFDnYNmzJgRsH1oz549Rrp06Tw6Jlu3bp3q93Fxccb7779v3HzzzWqaOH7w93vvvWdcuXLFCLQLFy4Yr7/+ulGtWjW1znLmzGnceuutxueff67WR7CPuxdeeEGtq6+//trlcH///bfx8MMPG8WLFzcyZMhg5MqVy7j99tuNr776ykhMTDQCyR/bbObMmUajRo2M7NmzG0WKFDF69+5tHDlyxKf5+eCDD9Q6Gzp0qMvhdu3aZfTp00ddrzNmzKiOj3r16hmjRo1SyxQsJ06cMAYPHmzkzp3b5XA4xnGtmTt3btDmjW5YsmSJim3y58+vjjFc13v27Gns2LHD8JXHgToCp0uXLhkLFiww8ubNazuJYqeZMGGCOliCudOGMwRusbGxxrvvvpvi8379+ql1iu/cnbBx0K5fvz7FZzjZYRtUrFjRkoE69qEPP/zQyJcvn9qBccH49ddfjQMHDhiXL19WQREuXtWrVzfKlSunTuJYDh2o4/e4QE6ePNnIkiWLbR+sU6eOWhfXrl1LMb1Tp06pC6gebuXKlSqQ1y8Etv/8848xadIko2rVqh4HTE2bNlXb6NChQ14tPy7OuMjo+RkyZIixfPlyY+vWrergxvbHhQDf4UKE4MoTW7ZsMbp166bmCb/FRd/TG6Y777xTrUtcqM+cOWP89ddfRvny5dVFCesn2uF4QgCD9eOvoBr7AV5TpkwJSqD+3XffqZtW3CDYw7I98MADToPk//3vfwHZhxCUenrzPG7cuBS/xTmgYcOGToevWbOmcfToUa/WEaaB8Xri8OHDtnOsoxdutnGt9Pc6c+a3336z3Ui6CtQxLZwjcEOxdOlSdV7dvn278dxzz6nf33333anOoe72K6wLT6R1m+F46dKli7q5xT6J69/mzZuN+vXrq4AINyDewDkTN1juAnXEO7ihwA0Z/sY1BTeZb731lrqG4dqD7eep77//Xq1zXwpgzNc8d3bv3q1uZD755BOvpkVpg5jOXKhjfmH74UYzoIG6GUor9cRfe+01nyYczdq3b6/W3ezZs1N8jkAUJ79ffvnF7TjeeOMNpyflDh06WC5QRyCOu0zMV+nSpVWA7AxK0p9//nnbPqYDdTNcVPT3CNydqVu3rm24vXv3Oh3u3LlzqpTJXcCEC4Ie3yuvvGJ4q3///iluHOxh+rgA4Hu844LqrvQET1Jws1GwYEGvAvW2bduq4XFzZIYLIE42KJm/evWql0sYmbBO/B1UI1gIdKCOcwmCs/nz5zv8HvswjhE8xcJxhtJDBC56H8SNIz7z5z6EAgUU9nTq1EnNF26ysR/bv1Cqj8AMQZkZbvARPL388svqRh/HJAJtHL96fSKA86aE2N35wQzzhcIGBGuLFy9WBS+4QOfJk8c2/QcffNCv68wZbDPzcju7JmzYsEGtSxSCOBp3jx491O8HDBhgeArnGU/327RuMwSqGObVV19N8fmxY8dUoQbOffb7iTPx8fHqSYaerrNAHYWPKD3HTa6jJ5x48onfYz/2FK7J7p562C/f22+/rZ4SV6pUyeNAHXBjgWMfN3IUeIsWLVLb5q677lJPI3Fe+/HHH9UTQb3dcD719qbS50AdB4uesP3JhtzDCQvrDhvWF7iDRymCswP+kUcesVSgjpOvvjjhpPrvv/96fHJ3FqjrZcTL1YmocePGHgXqMHr0aLcXnq5du9rGh23gbSCLG1tXgbr5MTZeKIH3FB7Pehqo4+YGw+JC6ehR/T333KO+HzhwoMfTj2QIVv0dVAdinPbniaJFi6qSTEfwJAvb2dE+jMf6et6clcr5ug99+eWXqqqfK3iiiCpxLVq0SPE5Hh/juENQa+/48ePqSZyeb28e/XsaqOOcjVJvR0/TcF5DAK+nj+A40McdquDp37gK1J988kn1/YgRIxx+j3MovkdQ6u9APa3bbMWKFeoGBiXgKFCx98wzz6jfelodETcjeFqLY8NVoI6Se3yPaovOSq3xPW6AXD1BSUugbmau+ukpXHex7nGsU2DhWo1t5Ejfvn1t2w7VHr3lU2PS2NhYh3+Te2fPnpXLly/7vO5wTenVq5ecOnXK6TDp06cXKxkzZozMmzdP/f3qq69KhQoVPPrd2LFjJUeOHG6XMV0657txTEyMx/P52GOPSa1atZx+f/ToUZk2bZpq9AvYBpMmTfJ4/J7Oz/3332/7+6+//rLtL+7kzZvX4/l488031fs999zjcH/R8/DRRx/JxYsXJdoF4pgK9HE6YMAAOXLkiLzwwgsOv8dx880330jmzJlTfffkk0/a5i85jvXfPpQnTx4ZNGiQy3n/4YcfJC4uTjp16pTi8zlz5qjzgj4GzQoWLCgffvih7d9LliwRf5s9e7Z89913UqxYsVTf4bym14mz6fvzuMO5548//pDx48e7ne89e/a4vOYULVpUvWOd+1tatxnWGfbBpk2bSq5cuZyuM5ybd+/e7XJeli1bJh9//LFadxkyZPDLOktMTJSEhAQJNG/O7xoaq+M6hXMBBc6xY8dUrDJw4ECH37/33ntSvHhxn89LzPoSZBcuXPApiAScDPr06SOzZs2ScHHu3Dl5/fXX1d8ICJ566imvTkxowR4sOXPmVEGEM7iglChRQmbOnGnbdh988IHf56Nw4cIp0m2dP3/eo9+5u/Boq1evlm3btqm/69Sp43CYevXqqfdLly6pCyCFFwQZX3/9tQoyWrZs6XAYBJv58uVz+F2WLFnUdwgmHf0+LfvQAw884Hb+Z8yYofZn800r1KhRI1XwbtaiRQs17zoDlr+1bt1aGjZs6PT7e++91/a3/fT9edwdOHBAnnnmGRWkFyhQwO1866AS69VZRiBo1aqV+FtathkKRxYtWuTROkMwj33e1bW3W7du8u6770rlypU9Xmfff/+9wxsYvc6wPzi6gfA3T8/vZrfffrsUKVJE7Utbt24NyHyRqP0DwbgzGTNmlLvvvtvn81LIA/UzZ86olIQ33XSTLVCqX7++jBo1Sq5du+b0dz/99JPcdtttKq0hArp27dqpUggcNPZ3t1iJr732mpQtW1YyZcok5cqVUwHjJ598otKV+eLff/+VZ599VipVqqROMoUKFVIbwtmJEKUBCO5Kly5t+wzpE/EZXvjeXcB7yy23yBdffGH7rEePHrbfP//8805/i5M+7vRKliwp2bNnl+bNm8s///zjdHgEhxMmTFDzh3WLABsntldeeUXNhzdwIdElQ3feeafaxt7AjQm2WSD169fP7TBImfnZZ5+p9Yxt3qZNG1uKTZRq+dPhw4dtf2Pf8uRC7M2Nn77wAVKDOlKxYsUUpVD+sn79erXfmktxp0+fri62WFaUSuI4Nps4caI6J2TLlk3tw2+99ZbTUt60nFMA3+NCjt+ihAQXYASLO3bscLtsuHCjJBrnFywfAl2cE3755RcJxVMsnAcbNWrk9TEHp0+flpMnT6pgEMsTzH0IAdXChQtVAGdfiohA2dUTNNyY5M6d2+V8pUXbtm1dfo9Um5r99P21znB+fvTRR+Xhhx9W68MTDz30kHpftWqVjBw5MtX33377rVrXb7/9tvhbWrbZb7/9ZssN7myd4ThFMOpuP8O+XK1aNXWMegI3FzinItVh//79Ha4zXJtwrAWDtwV7+jc4B2Ad4rzpTxgnlr1KlSpqPaAQC+f2zz//PNUNNly/fl0VbOFcjG2mf9OlSxdZu3ZtimFRQKdjmxjTC0+izAYPHpzie3N8pWGf6NChgyoAQ8CMkm2kxfbknO6pUqVKqRtST84NPp2XfKmLgzpd7urEeQKNcEqUKKHSQ61evVq1hke9Xd06HC3rHWUqQLYE1AtDSjG0wsYwqBukM2agMaJ9w0OMa9myZaru47p164xWrVqpYVEP2lvIcoP6ckgNhXpqGCda8+o6b0jnZt96HvURMV/meqloVIPP8PIkpZceVv8erfj1Z+ZGOEhxqOuoo0EMUjWhoVOhQoVsv8XfjjLLoHV+8+bNVcNPNHrAMKiHqFMLlilTRmVM8RQaXulpovGNv+hldFe/F+vAVR11LC8af7iDerpYh7ouIradHi+2ty/HjrM66r169bIN07FjR6/H7a6Oum7MjJerlFFI3YZh0AAtrZDVxrwv4IXG00899ZSqi2xuVIYXjm0cE927d1cNonCeMKczfPPNN/16TgHUO0ZDPjRMQ+Mt7Bto8InG2ZhHV/vbrFmzjGLFiqk2O2gAhnEhq49OQYgGYfawPwaijjrWmz7WkdHDF2hkivOms0xegdyH0KAev8F51ls4F+rsR8giFYjGpK5g39IZHuyzyPhrnaHhKvZTc0pDT67HOJdgGNT3Nh8/qDePeuKO6pD7qzGpr9vM3Ph+4cKFTseB1KK6DZQjOJ5xXKNOvHn+XdVRh5deesk2fdRV17EFzmc43tGY2BtpqaOO33lbR91ctx0xi6eZjTyB9YF1ikariBNwrtRtuJBZyAznEWRDwneDBg1ScRuy2Tz++OO2RpbmLHa4zmJ5zZluvvnmm1SxHc51SEyBfRr7t32DYrRHwHH0888/q7ZuGFYfh0gR6ms7QV/gWuaoQbQnQhaoI4BEy39kALHPoYqNpNPl4QRi3rlwccdFHY0k7CH1n32grlvi4kJqv4HRytzbQB0b3FkAhTR7OuWTs1b//rg4u1v3OohFcN2sWTPVQAfrTTeY1L/H+rKHhg5owGXf+h6ps/RBg3F6AuPAwaCnhwuwlQJ17GfIb+qu0S3WHQI8+wZe+uYFwRhSdvkjUMcNnw7ucHx4Ezx4GqgjpZieB5wwnUF6LwxToEABI61wgUTmBHMqQASRyJyhj28cP7j44Tu844YFKSv1POJEq4N9BDP2N7e+nlMAw+N7BOT2jQCx/ZGS09n+hhsC/A4Xb1cNg+0z+AQqUP/zzz9t4/UlPduYMWPU8uBcF4p9CDe+mL6jhoPuYD1iep7cfAciUNfBFApxArHONm7cqDKR2GeO8OR6jMIjnXkLLwRV06ZNU40xfQng/BWou9pm5psb3IQ7Y079iAxjZkghiUa+yMBhP//uAnUc+whI9bhxbURgiiw53qYADVWgjpsU/Tv7OMhXuNFEcIxG5462mX2grmMzNGy1jxFQsILvcL531YB21apVDucFGaJwTJw+fTrF54h1cCzZB++Yps4Gh+uFL+cZX+C6hBsSZwVFlgzU9QGIzg4c0QE2Xrir1nBw4LPatWs7TLuEOzxzoK43NC4+jnKaehOo40SnA4k1a9Y4LYnS8+0oD3YwA3WU8Nvn5MWJBxcAfaK2T+WGz9GhjyPo5EFP25PMLQjM9PB4IQVcKAN1nCRQ0ogX9hMdELsL1LE+kK7OPmcwslfocSMg8zVQR2kD/sZNg87Biv3bVQrLtATq6ERKz4Orjkb0CRTL7i+6kxFnxxDOB67SbmIf0t8jsPfHOUWXvDgLsPS5wtn+hosSskg4gvSD+nfIAx2MQB1PIzwphbS/kfrss89SpK3Dq3PnzqlKsQK5DyFgRGGHN0+pzHBOQ+msfR8TeAqLYM3ZC/OIJ2bOvsfvPYEnOei7wVF+8bSuM2TnQemgff8b4On1GDe3SHWo02/qY8HRNkYfD67WGc6fWFZn37vLwe9umwECY2fHu/1618OZ1z2udxgHntzZ8yRQ15Atx/xUDXGDo0wvyK7iap1hOVHq7+x73DT5O1DHedbb65Q7ug8IpFF2dDNpH6jrrEM4BhwVDuI71HKwh8KZbP9lyXP2dBD7L/Zp+99hPaOgx92TEnRSFmg6qxL6j/BFSAJ1HEj6MTY6enAEBxjSYGEY3C3pR7AIlnWAhZ3a/oSHYMd80sGjaAyLE4r9HTXGhZR2npo6daoaF0qJdQm1s0efzkoIghmoOwtA9d0kHm07SnloDmjNL/PJ3ZM7c2xn80Xfk/zwgQzUUfKJGz28UPqKklOk9HIXqKM01f6mRl849U0PSnhRzcKbYwd38wgOzFU6cLLSPbN6y9NAHR2r6Om5yjWte3ZFKYC/mG9uHMHNrf7eUQknLtb6e6Rt88c5BSUqOKnjc/tzhH0qNvv9TVdlQ3Dp6Jgx59bGE5hgBOrmqlPOSqHsYf1h2VGwYd+DrqObl0DtQ7rai6u+EZxByRoCIUePlnGd0Me+o5f9+cH+5UnPmbqgw1nK4rSuMwRFqD7g6LeeXo8RwDz99NOqigm2q/4dOmCyT4OL48PVOsOTbVwHnH3vyfnQ1TYDVMPU84hj0Bn0uquHM/dUigK6ypUrO9x+ngbqiBMQ2KFEH9U3dWEKqtvYpz3EjZCrdYaSf8yTs+9dle76GqijGp7+nf0131d6X8c+iiq4jq7P9jcLWPZPP/001bC6ugz2bUeeeuopWwxnv09hu+IGx74TKX2dwW8cnZd18I+XfZDvb7ju4EkwnsrbP+2xdKCOx7GuLsb2nTDYVxMw59BG6cPHH3/sNJ81HjGi91Q9fMuWLVVddV/oOn7uAiHU48ZwqCpiH3BZIVDXucXtDwydzxYBkKuTDV6e5A/HTmm+4ONGx2p11HESdpXXFKU8znIiA7p0dlWVyNWxg5MW1uX48eNtn6FTC1+7Efc0UDeXmrrajvq4sX9cmRbuLjb6Mbiz7eXs+EnLOUXfgLt6UuRsuhMnTlSf3X///W6PGftOUwIVqGNe9HhRb9RbOGdhO+lqfLgBsq87HKh9CCXpOG96EuTZzzNKVXG8e9Lex567/cYdBAwIxHFtciYt6wxtYnBj76x9kCfXYwSVqNuOknJHVQsQeDrqsyJQVV882WbmfRlBvTN4AqmH0/sO9n0Eas46mPEkUMf6QH5s840jjnldpx7xhzdVGUJR9QX9KZh7gPUH3MSZ89+jGjEKWTwtYELtB1S7QsGULiRxFq9s377ddnNkf43FTYKjAlEU2Oqnq+7Oy75ebz2FG3cU7uJJg69CkvVl165dtr9dtQY3p1BCmibt008/VS2FAS2yn376adXaF1lckNPUDJkXkHVBZy5ANgGkLGrcuHGqlsaezrereTbP99WrV73OkhLM1uO6Nb05F6hePrSQdvVylHvZHjLy6NyhcPz4cbEatDw3Z1qw9/7776v1hRR1jtYDcvKa0ze6ykZiDxlFMA60lEd2G0BLdGR08GY83kLmFM1ZrmZzWki0zLe6tJxT/v777xT7rDf0MYOsQO6OGXNGkGClgPXkOLWH/R1ZEZAlB3BOXbFiRcD3IYwH52dkVEJ2Km8MGzZMnV+QujYU/Uggixgyf5mzctnzdZ2h7w2cE5DhAg4dOpTqpWFY/Zn5Woh9Atm+cB289dZbbZ8jGxhynMPGjRtVXxLB4sk282Sd6UxPgAxP2HeQAg9ZcZDVDVmzHK0zvX6wbvRn5jSM+B5ZfvB9586dbZ8/8sgjKlMV5hnxBzKK2F9LrcR8TvO0Tw53kD3l559/tqXMRDYhrCv0Q+IqyxW24TvvvCNVq1ZV2ViQgah9+/Yup1WpUiVbilhkBDNfG3H9RTYfZ+dlZL5yd172JSuWp7Zs2aIyBk2ePFlq1qzp83hCEqibdxZXHffolE1gzm+NNG3osOD3339XqYcABzwCduSBtd8Z69atq9IRIs+lvlhiJ0GaoHHjxnk930hd5oqebxzIzjrssSKkT9InbH8xp51E5z1W9L///c9lB0dTp06VDRs2OHwhN61ORYWUnUgb6gukrUJKQJg7d646mQWKuVMncypIsxMnTthOiK46gbKKtJxT9EUevL2xDsQx4898yygs8BVSpOl1ZC4oCdQ+hE7RkB7TVc5tR5Dn+quvvlKBg7O88IGEG/Q1a9aoTppc3Rj5us4wXgyPNLII3h29NPMwCCI1pBzF+clRekmkGX7ppZdsnRMFY1/2dJt5ss6wvrDezMOvXLlSnZ9Hjx7tdJ3pGxzzMPidhpzsy5cvd7jOcM7XNzgo8NMd+lmR+SZI56v3h/Lly6sA/csvv1TpCXW64rvuusthh2a4NqJQDOt06dKl6rhBukxP9O3b11aQpdOcIp7AjalOlWy18zKuK+gzAseeuX8FywfqukTVfGJxlc/bfOfkqDdL5PlGSc/8+fNtpaK//vqr6v3SHk6g6KULuY7RAQ9KUnEXjODeXKLmip5v3GGjwwl38418meHUc6u+icHJ2hVctPfu3evROM0lNDhQg9GDm7/gRILc+w8++KDLO3JzHnZfO0DC/ogc/LrjDOT9R+liIJg7sNEdsNgz9/CH/PdWl5ZzirlEBRcaX44ZlOC4uxG1L5UOFHPu8bSUoKHUTBeEoLQ40PsQSilRGurowuvMn3/+qUrUEPAFIm+6OygNRsCG4MFdz5G+rjN/PF1DiZ59CbV97596G/u7X4i0bDMEffoJsLN1ht53df8IwVxneIqiC1cCvc7Swr5XYH/fBOCpxc6dO9VxoK9f6McCBU7m4wTHNWo+4MZT5733VKtWrWwxnr5BQmk6toGjJ6j6vIx9zF1hBfZHf8MTViwv4h/9tDwsAnV0noFqBNCsWTPb565KIPEbQMm37rIZJV72HRagQwXcPevSW5SAakjIj7s+c2n80KFD1UUTd5c4oHGB8IS38+3ukY7VoEMlwEUHd73OIBj1tOQRN1P65InfePMEQz86clbiHUjmDo7cdTSBjrduvvlm9Tce+zm7oLiDx9K6O3DcRKJ6l6c3RN5Ap2AoDQFnwaWuFoYnQvfdd59YXVrOKeZHkp70wmquUqCPGX1z5QxK+VE1Lxj0tgVPe7V1BoEz9n8cx4Hch3S1F5Reelrqh86zcIygYMHTkjl/wvy+/PLL6nype7F0xdd1hmpI/7Unc/oylwLrz8ydv+gnIs56RURBgb4pC2RhirfbDMeo3vfcrTMEbKjuAogF3K0zXQqMeEB/Zn4C7G6d6emAlQugzIG6o87LfIEg3HyuxFM8PJlBHKZv+HQcpgtEsX6x7X0pvIyJibGVquP8juAaBbTOOqzU52XEHDrudASBvDk+9AdUn0Lsh/12wIABfhln0AJ19HimS0oaNGigqqMADlj7x6raunXr1Ds2shnuzuy79MXJXfeqZl+KhGoy9lC3CnXNHA3vDO6OEOgDLrqO7tpxEcfOihMfHh3bMx/0vh7c+lEWehzVzHeNnpYm2A+nT3L4HD3Zmev9mrcJqhzpkgRPoN6mvsPF0w5zqZEr2C+wozuqg2auE2jfLsHM/J03pSx4nIft07VrV4+G19sa09B1e32ZHzwqe+6552yPzvDIzFXdTDM9PnfLiZOeru+KE66j4VHqATg5+rP6lj/r3pvHlZZzCnq70/U4sT5wAXA1LfNxh0ft1atXt92koc6vo99iH7a/cTeP05/rRa8HczfnvsAxhotYx44dU/X45+99yNtqLwj4cGyg11rz8pphOw0fPlwCFaT37t1bfvzxR6elwijpNbdhCeVxp29GXZUeohoBOFufaeXrNhsyZIjt+DIfe/brDPXI8QQ0ktaZP84X2A81XaDkD47iKpwndECt4yoUjujaFI7aqelqKq6u44B2Giixx7Kj6hHOp86eEOApOJ4I6rYQqGrlaNvhRlv32usPiEtxg42nta6qr6IatlfVLH1pgYqOX7zJdIEcxMjpbU4rhVRHOi8pOkGxby2MVEz4HumZ7Fth4zfo0coeOhzBd+YsHkh6jxbajnoQ0xkg0CGQpz766CPbsiMntL1Jkya57DlRz6OvGXNApwREp09Yb2h1bk4zqfOSIg2jq0wo9rnosX3MHbugxTyWA+nukCng9ddfV2kIkRPUW2h5r1PVYf7tO3+xhxb7yH/rLBMDWnrr+UTrcWfMuYud5b531Eoeeda9yeWMDo/0dJDG0lnOX90Tm7P9R7eIRyt6PRxSO3mSjUH34udJhg3sN3odokMMM2xvtLJ3ltYsLZCaTC+Xo05WzHnSHeWRRwYA/T3OK2a+nlMAPYfq8SLrCFrqYzsAMsGYswchpSf2Z93JFVKV6ZSxOtXd7NmzVcYgpK9DZgtHqcd0RiFHy5IW2Fd0KlX7Trrs93P0EYD0bY4gYwIyO9hnqwnEPoRzFs439j06O4KUk9jHkekHmUDML5w3cJxj+1WrVs3jcyzOn646ITJDdgucy2bMmJFq+thnsezYz0uWLJkqK0qgjjt31xRc4/A95ttR5hhkpECqPZxrPM3cgT5DXPWw6s9tps+b9jnkcQwi1R5S7nnbAZG7rC84nyB2wLHkKMc7UpoiEw+OEU+yoAE6CEKHZL7QnQbh5SiHuzPo0dPVOdUXc+bMcdqJIVJYmvuuQVyhM0gho5FOGYvzMWIw3SkiUudiWMQZzrI+9evXz7Ys7rKooGdoPSzOz0hbi9gF2xW509EBEablL8hyh6yCSIFpv4/jhflFZ1noTwNxljd8CtTNKZNwEcIOixOs7s4eqXvQSxQOPiTYx86OhPf2cHHS+SzRkycOWOyAuPAh1RUW2j6vqA7U8brnnnvUTo9hsPHRGQRyu5qDO90TJ3YGXIxx0cUFCicFzJe3PZMCcr7ihIqNj4Mcy4/5Qu5OLA+6yLU/2WEHxHzpbnTxQr5iHDhYZ97kzUagoMeBExROFtgJsA2wPnRAjIMDvQvq3J34HinpdDokLD92HPPFEb146Tzr9i8ssyc3Zs7gpIqdWB84WA7sA1h/ONEhYMCB9Nhjj6lUmObungHrCNsaN0PmzifQEdO6detsebGxrhFcmHthxat169Yq0HPUuYceP7rRRs+rGB77ElJ5ujsJY9vjBGyeFtIsYllwwsF48Y6LJYIRPQz2V1wAHHXTjo6qsF31sJgXXKSwnXUAqeGijv0I09TDf/vtt2q+XOVrxrZGRz3YXxB8YN1iHpHHGWncfOlBzRnMB5bJnEYNJ3SdGgvp2bD99U2m7icB+wB+i3WIbapz6uKFfQk5k83Hji/nFD1/2O/M2xC51nExx/hwYtef47jC+cwc8OB8Ys6Hb34h+LDfl7FtzMuC+cKyuNpe3sC5Ud/MO9OpUyfbjSUuYjhf4+YJF1BcEHGusp/vQOxDODawTrt16+Z2WBxT+vzl7oXt5m2aR3cQmOjUfO5e2HccndcDcdx5UvjzzjvvqGGQPhjTw7GHY27ChAlq2kjPeOzYMcPf/LHNdBCEmwnML46f33//XeWmRiGgsxSMrniSnhHnURwfuMnAdBHX4JhAoIpCIIzDPoe3v+H6g2NS98qMF9Jq4nzoSSpSXbCK642/6EAdxwIKiHD9wX6MAgpc43DuMF/XcC43b2fs+7iGo7AKnQCZt//777/vMoZIly6dijvdwbqxP6ebXzj/+et8q9N4erKP44WUzAEJ1HERwcZBZwmezoz55awUFiseFz3cTWHDoctXlKSilNTRTmgO1M13SwhkkLjevvTHPljDC9NBd85YWb50LKNLxtETIuYX48PJD734Obtb1hdFZy/kZvUUThQIUnDzgZJXPU1zd8vmFzoEAN2rqv3LPn8pDjAEngiqULqIUnSUAuHE6A/I044bOOQVxo0GDnYERbhxwb5gzplvZs756+yF4AmlY66GwfZyZObMmQ6Hd1VCbd/7qqN1q3Ntu3o5enKADmh0/ljzy9yhCi5qrsbrrkQRv8eFChcdrBcE+7ig+7skfdy4cU7nETcrjo5T/UIvePrC4Ohl35GWt+cUM0wLJ1wcW7iYdOjQQZUaYvtgH0XHJ85K7nQHWngag4ACJW24abd/coB91NmypOVG2AxPEN3tu7i51T1p4hyK4xzbHxc3T3s09cc+hBzVrjqb0vCUynyD7u7lKqe5L5DL3NHx6OyFPhaCddx5erzjWoFrEYJbBKDY9uiIBgGTJ08zvOXPbYYCCpwncOOLGzsEyS+//LIqePOFpx0eIQhFyS+Gx3GNmw7caOFpczC6n9c95zp6edKrqn46i9oA/uLofIz9qWrVqsbIkSNTFYbhZgMBOfY7BOOIX3TfJCgcQ2yCJ1Ao/Xfnvvvuc/kU3R5uHvCUE/s64hnEfuiF2V9Bun3PuO5euLY4eprsSgz+55/aOUREZBVoxI1sFKtXr5Z69eqFenaIKMhQPxyZydDYGW3O0HaOwk9I8qgTEVFgoTETGp5/8803oZ4VIgoBpJhEo3AkOGCQHr5Yok5EFKFwgUampe3bt6fK3EJEkQsphpGOEWkCdU54Ck8M1ImIIhjS1iFd5W+//ea0q3YiiixI84t0tMjzr9PPUnhi1RciogiGvNXI69urVy+/5msnImv66KOPVA/NyPPPID38sUSdiNw6ePCgV51cObpw+LNjiUiFHpPRKYyv0LHHrbfe6rTjMXTegi64/dmZDhFZAzoNQyeBOXPmlNdff93hEzSey1PDORfnXl/gfOuoQyV/YqBORG6h1zhkEPAVepTztGv4aIaei9Ejra/y5s1r65HPkVOnTqneHVlfnSjyHD58WJ2rS5Ys6XQYnstTwznX3Gu8N3C+xXk3kBioExERERFZEOuoExERERFZEAN1IiIiIiILYqBORERERGRBDNSJiIiIiCyIgToRERERkQUxUCciIiIisiAG6kREREREFsRAnYiIiIjIghioExERERFZEAN1IiIiIiILYqBORERERGRBDNSJiIiIiCyIgToRERERkQUxUCciIiIisiAG6kREREREFsRAnYiIiIjIghioExERERFZUGyoZ4CCKykpSY4cOSI5cuSQmJiYUM8OERERecAwDLl48aIULVpU0qVjOWu0YKAeZRCklyhRItSzQURERD44ePCgFC9ePNSzQUHCQD3KoCQd9u/fL7lz5w717JCTpx4nT56UAgUKsNTEgrh9rI3bx/q4jXxz4cIFVdCmr+MUHRioRxld3SVnzpzqRda8iF27dk1tH17ErIfbx9q4fayP2yhtWG01uvAIISIiIiKyIAbqREREREQWxECdiIiIiMiCGKgTEREREVkQA3UiIiIiIgtioE5EREREZEEM1ImIiIiILIiBOhERERGRBTFQJyIiIiKyIPZMSkQUYQzDkOvXr6seIKMNlhnLjp4v2eulNUXzNsLyZsiQgb2LkscYqBMRRYjExEQ5deqUXLx4UQVC0XqTgkAQ64DBkDVF+zZCoJ4jRw7Jnz+/pE+fPtSzQxbHQJ2IKEKC9IMHD0pcXJzkypVLsmfProKAaAuEEAQmJCRIbGxs1C17uIjWbYTlxnF66dIlOXfunFy9elVKlCjBYJ1cYqBORBQBUJKOIL1kyZKSJUsWiVbRGgSGk2jfRriJxs30gQMH1HFbqFChUM8SWVh0VQ4jm5gtI0I9C0Tkx8AH1Qhw8Y/mIJ0oXOA4zZkzpzpucfwSOcNAPUrFbPtfqGeBiPwE9dHxQkkdEYUH1FPXxy6RMwzUo9QvV0I9B0TkLzq7C+u6EoUPfbxGY3Ym8hwD9SjV4Wio54CI/C0a6/sShSser+QJBupERERERBbEQJ2IiIiIyIIYqBMRERERWRADdSIiIot54403JE+ePLJo0aI0j+vIkSNSpkwZadmypaVSAe7Zs0deeuklyZcvn+zbty/Us0NkSQzUiYgo6iFQROM+vPLmzSvlypWT8uXLq7/xGfJe4994FStWzNbr6/PPPx+Q+Zk2bZrqvfKHH35I87hWrFihlg9B/+nTp8UKpkyZIn369JGRI0fKmTNnQj07RJbFQJ2IiEhEsmbNKgsXLlSB4+7du2XXrl3St29f9V2dOnXUv/E6fPiwCnxvvfXWgM3LgAEDpEGDBtKzZ880j+uuu+6SVq1aqWXJnz+/WEHnzp3VusY6JyLnYl18R0REFDWeffZZFdR6okSJEqpUeMyYMQGZl27duqmXP6DH2p9++kmsJl26dKp6z5Ur7NiDyBmWqBMRUdTLnDmz3HvvvV79BsF6w4YNAzZP0SA2luWFRK7wCCEioqhXuHBh9fJWhw4dAjI/RETAEnUiIqI0iI+Pl6+//lqqV68u33zzjRw8eFCaNGkiuXPnlsmTJ9uGmzVrltx2221So0YN9V2tWrXkgw8+SJWJBXXk33//falYsaIan9m///6rqsQ0b95c/XvdunXStGlTyZYtm6rT/s8//6Sav6VLl6o64ZUqVUrxeUJCgowfP141nF28eLH69+uvv65uWFCX/bXXXnO6zF988YXcdNNN6qkChn344YfVcvtTYmKifPLJJ+qpBea9YMGC0rZtW1m+fLnD4f/44w9p1KiRWp4MGTLYGgejUa6GdT1u3DipWbOmmndUv8EwtWvX9uu8E/mNQVHl/PnzuCIYMpCb3qoSExONo0ePqneyHitun6tXrxpbt25V704lJRnG9UuR9cIypVrMJCM+Pl69+8PQoUPVOfPWW291+P3vv/9u1K5dO/m8KmJ8+umnRp06dYzMmTOrf992221quHfeeUf9e9q0aerfp06dMurVq6c++/zzz23jW7ZsmfHAAw8Y6dOnV999/fXX6vO4uDijb9++tvE2adLEWLRokZE9e3ajRIkStuErVKhgJCQk2MbXv39/o2LFiuq7UqVK2T5fuHChUb9+fdt8Y1x33323kStXLiN//vy2zydMmJBqmXv16mXExMQY06dPV//etWuXUbJkSSNTpkxG8eLFjcqVKxtPPvmk03Vq3kaYJ0xn7969KYa5du2a0apVK6NRo0bqeIOdO3ca1atXV9PGejZbs2aNkTNnTuPPP/9U/7506ZLx+OOPq3GfPXvWNtyYMWOMKlWqGEeOHFH/3rdvn9GgQQOjVq1ahiWPWwfXb7xT9GDVFyKiaJB4RWR6dokoHS+JxGYL6Szccccdsn79elWSu3LlSvnss8/UC7nBhw0bJg8++KAa7t1331Xv+t/4/umnn5bu3bvLggULpFevXupzlLjjhUatv/zyi206GTNmlPfee0+qVaum0hqi9Bql+ChBL1WqlGzfvl2V0O/cuVPWrFmjStcB6Q8feOCBVBlqUAr/559/qt8eOnRI3n77benRo4fMmzdPpZ7EfE2YMEE9ETA3akWpNUrTUaKvlwUl2EOHDlUZavCkYPPmzWler1h3yAqDLDu6ShJSY86cOVM9kcC6u/nmm6VevXrqO5SSV6hQwdZmAE8Y8NmSJUtSjHfs2LFy//33S5EiRdS/sfyTJk2Sjh07pnmeiQKBVV+IiIjSqGzZsuodASyCR/wbge4999xjCzIRSKOahVa8eHH1fv78+VTjK1CgQKrPEKyXLl1a/Z09e3aZOHGiCjShcuXKKsCHAwcOeDQuVPtA9Q945pln5NFHH1VBOui0kPbj+vHHH9U7gmUzHbTjxmHv3r2SFmfPnlVVf7C+9HrVUAUGgTaqxaBTKO3EiROyYcMGlTPe3FAVNxxmGA7B/vHjx22fYRp33nlnmuaZKFBYok5EFA3SZ00ugY60ZbJY9pKqVas6/H7VqlW2v69fvy6zZ8+WTz/9VP07KSkp1fCoY+2I/hxpDXVQrelS4qtXr3o0LvN39vnVnY0L9fEdyZEjh5onBNnHjh1TPaH6CnX54+LibDcl9tq0aSPTp09XTxwwP7jpwJMN3EQg4Ea+eOShxzK98sorKX6L4fDUAHXUUR8fNyT4vX7iQWQ1LFEnIooGKMlFNZFIeplKp60OpdfXrl2T//3vfyrQvHjxovTv39/r8ZhL5J3dLNg3TvVlfM7GpauW7N+/P9Vv9LA6yPfV1q1bXc5blSpV1DuCeZSQA4JzBN0I3FHdB0H+4MGDU+VoR7UkVAvC75566in1pANPPrxZZ0TBxECdiIgowFCPHVU54Oeff5bHH39cVV8JN6jigvr4ixYtSpFNBXXm8W/04OqsJNxTOrhGD7COoORey5kzp+3G4ssvv1R16BGIX758WdW7RzUkHczrmwhUj/nqq6+kZMmSar5RPeahhx5y+GSDKNQYqBMREQUQqoKgcSiqW6BKBkrXwxWqyiBIR+pINGrFkwHUsUfjTvSAimA5rdA4FXbv3q3qottDGklAoK0DdXMjWTTqRT10pHNE6bx99Res/8cee0ylukQwj2VCVRpzKk0iqwjfs0WI4LHaiBEjVIMWnEyQKxc5an2Bx6DIEYvSh3379nl0skfveTo3rH6hQRLqPBIRkfUgaDx16pSt4ae9cCvJ/fDDD1VpN0q+cfNxyy23qCw2a9eu9Us+ct0A9/Tp0+qmwJ7O127O1PLEE0+kWI/t27e3Zc1Bdhutd+/etr8zZcqkgvhRo0alGo7IKhioewH14Vq1aqVa2uMEgLt9tJRHK/8ZM2Z4PB6c3HBiQIkESiEc1fVzBK3gMQ/2UDfPVWMhIiLyzaVLyQ1w7es629NBoqPh9HdTp061ZVFBVRidtQRBPEqJ0cDUvtGmfSEMCnjMpcqO2P/G2bg8GZ/9b/766y8V3GLecd1DOsht27apVJGo7+0tPX7zdFAHHVVRAB1C2fvpp59U9ZeXXnrJ9hlSTE6ZMiXFcLiJyJs3rxQrVsz22dy5c1XKR/tSeDAPR2QVDNS9gEeWqP+GExIeuen6euhCGvlnPU1JhUd5yEuLcXn6CBQt6fFYDumncFI0vxCoExGRf6FKhy7RxbkWucodQXCOczMg2NbBr9asWTOVoeXo0aMq1zeegiK3Oeqp63HjM51GEeNbvXq1+hs9hprpUl9U20CJs4ZAe9OmTervZcuWpfiNHgeeyu7YsSNFD6h6mexLk9Hjqf4NgnENQS4aXqLaS5YsWVQhEZYN1zJkfkG+dtTB9wTGhfEDrodmyIiDOv3IpY5rLwqpMF3c7KDxJ3Kfo2qLGeYJ3+nqMrhm4kYLjUo1rNvWrVvbejfFekO+dax/c2k7kWWEuselcIFe02JjY42qVaum+m7BggWqt7BOnTp5Pd6CBQs67JXN3htvvGEMGDDASCv2TGp9Vuz5kqy9fbzt4TCS+atn0ubNmxsZM2a09dCJV7p06VQPnIcOHbINN2PGDCNr1qwphsuWLZu6LphNnDjRKFOmjPquY8eOxokTJ1QPog0bNlS9eX7//fdquHnz5qkeNs3jK1CggNrn0Ouo+XNM97XXXjO+/fZb1aOo/W/OnDljNG3aVPXkqT/HMuFa8sUXX6Sa7yJFiqjfoKdVLKv5N5iOXr/du3dXy4LhMQ7zsHihl9R169a53Eb/+9//VE+m5t/ZX18vX75sDBkyxChfvryRL18+o0aNGka3bt2MLVu2pBpnmzZtbONB763ojRU9m9rPB9a/Hi537tyqF9WePXvaej8NJvZMSp6Iwf9CfbMQDpBSa+DAgaoEBL2ymaFUAvXzkIv1yJEj6m9Poc4iHoWiNN5ZS3m0Xsd3KLW/9957VXosXxsjXbhwQTX4kYEixnBueivCY3JkKUBpUTg3OotUVtw+KMHFOQS5q9GOJZrhkoZSUmQBcZXKkHyDpwLIkvLDDz+oOt5mKPVG/XGUgKMTIfSk6gi3kW/Hrb5+40mPfSNailzWuMqEgfnz56t3+17SQNeBQz1Ac69onvDkJIUbA9RhxEkPPc8haP/4448dtoYnIiIKFFTbRA+m9kE64DPUU0dGFSY4IPIP9kzqITT8MXf5bC937tyqFTzqKbZr186v00ZL+urVq6s7b5Suo8QCjViRSQD1Ic05Ze2hhMPcABV35OGaaSBaYLugxInbx5qsuH30POlXtNPrgOvCv1D3/Ndff1VPll2tW9QN79q1q8thuI2Sl12fSzw5n1jpnEPBw0Ddw8dTuuU/AnJHVHWS/1rv+9t3332n3lFiv2TJEtXiHsE7GgihQdJvv/3m9BH88OHDZdiwYQ6/M3cCQdaBkzEebeIEbpWqFWTt7YPSS8wXqhO4ygYSDbBd9NPGaK5WEQg6WwqyleEYwPVHX/sAaYaRlxwFSmhA62xf5DZKhvWD4xaNgj3J3Iac9RR9GKh7wNyyPmvWrA6H0Rds+9b+/oQ68C1atFAnwH79+qm0VQjWUXrxyCOPOPzNoEGD1LDmEvUSJUqov+1bzJM14MSNixcyQFglECRrbx+cd3ARR51f3fV7tGPKWv9D3XR0DISMKciwgtzl6OkT10UE7idPnlSZ0JAm0ZP9MNq3EdYRziFo1+ZJHfVob38SrXhG9zBA1pw9ptN5alFfPdBwYI8ZM0Y1Qp0zZ45KV+UsUEedQUd1CfV4yJoQCGL7cBtZk9W2D+bD3AlaNMM5Wq+DaF8X/ob0iygcQopiPOlFVU8E5wg00X4KVWKQ+tAdbqNk+nj19FxilfMNBRcDdQ8g+EawjmAcj/QcOXfunHrPnz9/0OYLPaSi8wZ0vERERBRoyJmOgFzngCeiwOLtmYcnpqpVq6q/kX7RkePHj6t3dNAQLOjZFB0vZc+ePWjTJCIiIqLgYKDuoZYtW6r3LVu2pPoODUhRPy9btmzSpEmToM4X6gc2aNAgqNMkIiIiosBjoO6hnj17qvphS5cuTfXdypUr1Xv79u1T1GcPRovxPXv2yJNPPhm0aRIRERFRcDBQ91CFChWkd+/esnnzZtWAxmzChAmSJUsWGTp0qO2zP/74Q+rXry9jx451OV6dvspV50XOUj5++OGH8txzz9mq5RARERFR5GCg7oWRI0dKnTp1VEqqM2fOqJbrCMTnzZsn3377bYpeS0eNGiWrV6+WV1991en40IGRzmX+119/ORzm/fffV2ng7r77btm+fbv6DB0YYboI7pFTnYiIiIgiDwN1L6AOOkrKUSe8bt26qpT9999/lzVr1kiHDh1SDNu5c2eVygpdLTtSqlQp1RhUd7OM9IpFixZNVVqPnLRt2rRRgfxNN90kjRs3ViX3d911l7z44osBXFoiIiIiCqUYI5r7741C6PBI9SQ3UMQYzk1v1Q518KQFHVIxb671WHH7oMMjPKErU6ZM1HeKgksaqhSiM5loztFtZdxGvh23+vqN5BU5c+YMyjxS6FnjKkNERERERCkwUCciIiIisiAG6kREREREFsRAnYiIiIjIghioExERERFZEAN1IiKiINu0aZP06dNHsmfPnuq7K1euqHS8eOFvTxw4cEAGDBgg+fLlk3379gVgjkW+/vprlW0E71bKIPPzzz/LPffcI82aNQv17BD5HQN1IiKKeh9//LHUqFFDpQvUL/R18dprrzkcfuPGjdK6dWvbsEWKFJHPPvvMo2mhwzr0dP3555/L5cuXU32/ZcsW1acGXlu3bnU7vu+++0713fHuu++qzvgCZebMmXLx4kWZNWuWWAE6/evbt688+eSTMn/+fJc9fBOFKwbqREQU9Z5++mlZu3at3HrrrbbPZsyYIW+88YbD4WvVqiULFixQvUYjpz5+ixJyTyC4nDt3rtPvUZL+0EMPqVft2rXdjg8d5i1evNhvOfSXLl3q8PPnnntO6tWrp+bfCtKnTy8ffvihDB8+PNSzQhQwDNSJiIhEJFOmTKrnZ23nzp1uf3Po0CEZPHiwFCtWzKtp5c+f3+l36AhoypQp6oW/PZEhQwbJmzev+KNDL9y0OIIesVevXq3ercTVuiQKdwzUiYiI/tOiRQupWrWq+nvChAkuh/3nn39k//798thjj3k9HU8DcG8gWE8rlE5jucJJINYlkVUwUCciIjLRVTtQtQVdvDuDOundunWTbNmySSQYP368DBkyJNSzQUQmDNSJiKIAsmNcjr8cUS8sUyB07dpV8uTJo6qBoA60I8jGgkacTzzxhO2z8+fPyyuvvKLqlZcpU0YKFCggbdq0UdVFvLF+/XrV2NRRRhhAo8lRo0apxq/lypVT0xo9erTL8d13332qXj2qiWD4559/XjUM1dAQFaXpep2WL19evbA8gEavX3zxhdx8883y+uuvO5zOX3/9JQ888ICaDurto679mDFjJCEhIcVwmMacOXPU99988436DOu5dOnSkitXLrVOr1+/Lv5y8OBBeeaZZ9R8oYpS2bJl5YUXXnDY8BbTRbuEatWqSdGiRW2NhbH+zE6ePCmPPvqoVKlSRe0rejgsL5E/8XkREVEUuHL9imQf7jjwC1eXBl2SbBn9X5qdNWtW6dWrlwpeUcqMwM0+aJ46daoKlBHQwbVr16Rx48Zy4cIFFbAWKlRIfv/9d2nZsqWsWLFCdu3a5VFd6hEjRsj06dNVcO0IAsl27drJtm3b5Mcff5Tq1auruvRIT4hqOI4ahjZv3lwF/rNnz1ZB8lNPPSUffPCBCjYnTZqkhnv55ZfVC8EmYH41ZJ5566235Pvvv1c3KJi+Paynfv36qXr1aGCL9TFw4EAVEM+bN0/Na5YsWVSjWwT/v/zyi+23mLfJkyerdYz1hycVCJKdZdzxxrp169T8YD7wNxqgYpl79OihstgsWbJEBe4a1gGy7fz555/qpmHz5s3Svn37FOPEjQcy/jRp0kRVE8I4cePx8MMPp3l+ieyxRJ2IiMgOGlQiAEMpuaO66p9++qlKC6gtXLhQ5UZHsI4gHe6880657bbb1DgQrHsCwS2CWmfQcBV5w1GajyAdKlSooObHEZS0I7hHsJkuXTq1TLqUHFV7PIE6+wiknQWiCFaxLpAVBkExIAMNSpcRzOKGBTneoWbNmmpdNWjQQP0b8125cmU5deqUHDt2zBacY3ppdfXqVZU5B6XegwYNUnXZcSOCLDlYB2gI3KFDB1taRwTg48aNU+sKQTrgZuzLL79MMd6VK1eqG47u3bur9Qn333+/2nZE/sYSdSKiKJA1Q1ZVAh1pyxQoJUuWVMEXSl1RLQOl0Lq0+e+//1adCplLWkuUKCEZM2ZU1TnMihcvrt4RrHsKVWYcOXz4sAq8ETziBsCsadOmqjMilEibofoKSqoRzKdlnlzNF544xMfHq2ov9hCgo9QaATmC48KFC6tS/VKlSqknDwiUURKvPf7442p86MAprdAxE54MoNqLPZSw/+9//1NPLpAqE9sS6yMuLk7dmCEI109RcPNlvnk6ceKELfc+AnutZ8+ean8h8ieWqBMRRQEEmagmEkkvHTgHCkqIYceOHaoUWEPQiUwvCMw11N2+dOmSqvsNR44cUdVFUJoMqO+e1uwtKGVG6fgtt9yS6jusC9SVtvfee++putg6OF+2bJkKhsHbOv6O5gt113/44Qf1N+qY20O1G6S9xHwvWrQo1bjsqwOh4yhdGp5WulTe0Xzlzp1bGjVqpP5GZ0mAXl1R4o/SctRnR/Umvd1QDUpr2LChemKA/QA3SLjhANR/1/sMkb8wUCciInIApdYIwAF1ugENMKdNm+awcyMEn9u3b1eZYFANAiWxSPfoL2vWrPEpbziqfGCeUV/+jz/+kDfffNNv87R7925VCg2ObpywTtDgFcyl5M5usvyZalH36upsWqgSYz9fWE+VKlWSPXv2qN5eUb0IdezNUH8edfFRPQZPCxC4t2rVSvUoS+RvDNSJiIic0CWkKFH/999/ZeLEiaokFplT7CETC+poI73jt99+qwJ1fzp79qx69yYjCup+I0BHQ1L0tIo64Lp03R/QuNRcNccRXdKPqjnBpOfNm/lCffmNGzfKyJEj1Q0RGu2i8eyLL76Y4rfIAoMnLagShScr2D9Q7QnrmcifGKgTERE5gcaIaByKaiJjx45VGUnMjUi1r776SgVzGKZu3boBmRfd86ij7C7OoN44MpfgBiMQgbI5YwoCV0d0ekbUrQ8mXZLv7Xyhqk7//v3V0wJdlQk3YfYNgrFfoJ46gnk8OcENFOqpowoUkb8wUCciInICpaU6V/rnn3+uSrWRG92eblSIRpKOeFNH3RlUsQBUt3BVqq7rniMbC+qko963uT69P+cL+dLr16+v/kZ1EGd5zNGI1N9PGNxp27ateke6S53ZxX6+oGPHjranDzojDuDGBo13dWNXHaijwai5vj1uVn766SepV6+enDt3zlblhsgfGKgTERG5gEAdgS6CY+RX1yn5HAW8KHlFUIhhkXUEAZwOApGBBBljwBxo2wfdyKDi6Dt0xIR60RgXSnKd0Q0x9TyhKoduDHv06FFVXUPDuFAvW0Ouc0AedHt6vuznd9iwYeodedaRDccMKSvRsBaZXMzrTY/fvjMkM0+r+Ojh7IdHZhc0GsUyo9qPGerVo6Fvly5dbLnwAR0wYZ2YocGobiyqYTubG+Ni2W6//fZUwxGlFQN1IiIiF1Aa3KlTJ9XQUWdMsYd64IC66ShlRv1mNNzUKQvRmycCbeQkB5R0axjObPHixQ7/RlYSpA7EfKBjHnQyhGAcwS5SDSIvOKDzHWQiQX1rpJnEMKg7j7/RUBKZWHQ1GDSoRMNJTQetKD1GHW8EpIBx6HnGu7mEGss+dOhQFfwipeXevXvV5wjascyoDmJebwjS0akQoGMhM3RKpJnXkSt6OFRBOX78eIqqKcjcgqosuDnRGXiQqQYdHiGlJqoqmSGoxxMTVBfSNz2o1oRsMEglqaFEHSkc0WkUYN0jzSMaGTNQJ78yKKqcP38eRQCGDOSmt6rExETj6NGj6p2sx4rb5+rVq8bWrVvVe7RLSkoy4uPj1bs/rV271ujQoYPT7y9dumT06NHDyJUrl1G8eHFj9OjR6vM///zTyJs3r9GsWTO138Dzzz9vxMbGJp+LRYx06dIZrVq1Ut9169bNSJ8+ve07/N2lS5cU01q+fLnRtGlTI0uWLEbp0qWNhx56yPj111+NcuXKGfXr1zcGDx5sLFmyxDbfdevWVcM2bNjQWLdunfq8f//+al6HDRuWYtyrVq0yKlSoYBQoUMB44YUXjIsXLxrr169Xy6DnCa88efKocZt9//33RuPGjY3cuXMblStXNm6//XZj8uTJKYb56aefjBw5cqQYV8GCBY3Nmzcbbdu2NTJkyJBi2bFOXcGymceVOXPmVMuEcWPb5c+f3yhTpoxRu3ZtY/jw4cbly5dTDHfy5MkU48I6qFq1qlpXuHZqM2bMsA0TExNjlCxZ0qhVq5Yxbtw4r84L3h63+vptnheKfDH4n39Df7IydIahelwbKGIM56a3IpRcoUMNlMqhJ0GyFituH5RQohQTmUiQ3zma4ZKGEmbdCyVZD7eRb8etvn6jY6ZgZ9Ch0LHGVYaIiIiIiFJgoE5EREREZEEM1ImIiIiILIiBOhERERGRBTFQJyIiIiKyIAbqREREREQWxECdiIiIiMiCGKgTEREREVkQA3UiIiIiIgtioE5EFCHY0TRR+ODxSp5goE5EFObSpUs+lScmJoZ6VojIQ/p41ccvkSPcO4iIwlyGDBnU69KlS6GeFSLy0MWLF23HLpEzDNSJiMJcTEyM5MiRQ86fPy9Xr14N9ewQkRs4Ti9cuKCOWxy/RM7EOv2GiIjCRv78+dXF/8CBA5IzZ04VAKRPnz7qggDU+01ISJDY2NioW/ZwEa3bCMuN6i4oSUeQnilTJnXcErnCQJ2IKAIgKC9RooScOnVKBQLnzp2TaIRgKCkpSdX7jaYgMJxE+zZCVZfcuXOrIB3HLZErDNSJiCIELvqFChWSggULyvXr11UwFG2wzKdPn5Z8+fKxkZ5FRfM2wvIiUI/GGxTyDQN1IqIIgyAgY8aMEq1BIAKhzJkzR10QGC64jYg8xyOEiIiIiMiCGKgTEREREVkQA3UiIiIiIgtioE5EREREZEEM1ImIiIiILIiBOhERERGRBTFQJyIiIiKyIAbqREREREQWxECdiIiIiMiCGKgTEREREVkQA3UiIiIiIgtioE5EREREZEEM1ImIiIiILIiBOhERERGRBTFQJyIiIiKyIAbqREREREQWxECdiIiIiMiCGKgTEREREVkQA3UiIiIiIgtioE5EREREZEEM1ImIiIiILIiBOhERERGRBTFQJyIiIiKyIAbqREREREQWxECdiIiIiMiCGKgTEREREVkQA3UiIiIiIgtioE5EREREZEEM1Il8tPHYRjl79WyoZ4OIiIgiFAN1Ih+sPLBcan9WW0qOLhbqWSEiIqIIxUCdyAcL1r6r3i9dvxrqWSEiIqIIxUCdyBdxrPJCREREgcVAnYiIiIjIghioExERERFZEAN1IiIiIiILYqBORERERGRBDNSJiIiIiCyIgTpFrFNXToV6FoiIiIh8xkCdItLolaOlwHsF5N0VyfnOiYiIiMINA3WKSP0W9VPvA34dEOpZISIiIvIJA3UiIiIiIgtioE5EREREZEEM1ImIiIiILIiBOhERERGRBTFQ91J8fLyMGDFCKlWqJOXKlZMmTZrI0qVLfRrXtWvX5JNPPpHSpUvLvn373A4/c+ZMqVevnpQtW1Zq1qwpX375pU/TJSIiIiLriw31DISTuLg4ufvuu+X48ePyyy+/SMmSJWXGjBnSvHlzmTRpkjz44IMejefKlSsybtw4+eCDD+TgwYMe/eaVV16RDz/8UH788Ud1c7B9+3Zp3LixbNq0ScaOHZvGJSMiIiIiq2GJuhcGDBggf/zxh3z99dcqSAcE5x06dJAePXrI3r17PRpPYmKidOvWTY0rXTr3m2Du3LkyfPhwGTJkiArSoXLlyvLWW2+p4H369OlpXDIiIiIishoG6h5C1ZSPP/5YqlatKrfcckuK77p27SqXL1+WQYMGeTSuHDlySIECBVTVmfz587scNikpSV5++WWJiYmR7t27p/iuS5cukj59eunXr58K/r3RJHder4YnIiIiouBioO6hadOmSUJCgjRq1CjVd/Xr11fvc+bMkdOnT3s13syZM7v8fs2aNbJz504V1BcsWDDFd9mzZ5dq1arJ4cOHZcGCBV5Nd8m5M14NT0RERETBxUDdQ/Pnz1fvaMhpL2/evFKsWDHV0HTFihVejRcl5b5OF2rUqKHeUY2GiIiIiCIHG5N6aP369eq9ePHiDr/PnTu3KtnesGGDtGvXLqjTBUzXWQNYvLQLFy7Y/k5CdRk3NwqRANWH/M0QI2Djx/gMwwjIfFPacftYG7eP9XEb+YbrKzoxUPcwjeKlS5dSBMb2cuXKpd5PnTrl12mfPHkyTdNFI9Rhw4Y5Hvex/WKkzyqR7sSJE34f5/Xr1wM2fpyMz58/ry5knjQ2puDi9rE2bh/r4zbyzcWLF0M9CxQCDNQ9YK53njWr48BWn2wQ1Adi2r5OFw1c0djUXKJeokQJ9TcatEpsNol09nX7/SFDhgwBGz8uYqgShe3Di5j1cPtYG7eP9XEb+cZdmzaKTAzUPZAxY0bb3ygBcAT103V99UBM29fpZsqUSb0cUSfIKDhJBuJCECMxgR1/TIwaLy9i1sTtY23cPtbHbeQ9rqvoxK3uAQTBOmBGGkZHzp07p97dpVv0VuHChQM43civn05EREQUrhioewC5ypE/HY4cOeJwGPRWCrVq1fLrtGvWrBmS6RIRERFRaDFQ91DLli3V+5YtW1J9h4acaBiTLVs2W8+hwZgu7Nq1S723bt3ar9MlIiIiotBioO6hnj17qvphS5cuTfXdypUr1Xv79u1T1Gf3h+bNm0uZMmVk27Zttgww5mov+BzfN2jQwK/TJSIiIqLQYqDuoQoVKkjv3r1l8+bNqXKWT5gwQbJkySJDhw61fYYOiNBj6dixY12OF72dQiJymjsQGxurUiyilfykSZNSfPfdd9+pz99++21VPYeIiIiIIgcDdS+MHDlS6tSpI0888YScOXNGZWJBID5v3jz59ttvU/QeOmrUKFm9erW8+uqrTse3d+9eWw7uv/76y+lwnTp1kj59+shbb70lmzZtUp8tW7ZMjfuFF16Qzp07+7hEbExKREREZFVMz+gF1EFHSfmQIUOkbt26qipM9erVZc2aNbZGnxqCZ1ST6datm8NxlSpVSjUQ1SXqjzzyiLz00kuyYMECqV27dqrhx40bp6b10EMPqZ5GCxUqpG4O7r333gAtLRERERGFUozhLEE3RSR0eKR6Mx0oYrx5RSQ2i0SimGE3nhYYQ/2/iw+Zeru8tWN5QMaP6kx40oKOlJg313q4fayN28f6uI3Sdv1G8oqcOXOGenYoSHiEEPmCt7dEREQUYAzUiYiIiIgsiIF6NIthY1IiIiIiq2KgTkRha+fpnfL99u9DPRtEREQBwawv0YztiCnMVfyoonpf9MgiaVGuRahnh4iIyK9Yok5EYW/14dWhngUiIiK/i+gS9QMHDqhOh4oVKyYNGzYM9exQBGH1fmsxmIaHiIgiUNgH6v369bP9nSNHDhk2bJj6++OPP1bf6Q6F7r77bpkzZ45kyJAhZPNKkYO1hoiIiCjQwr7qy5gxY2Ty5Mly8803y6uvvqo+W7lypfTt21euX78u999/v3zwwQdy+vRpGTVqVKhnl4iIiIgoOkrUYdasWXLrrbfa/v3CCy+o94cfflgmTpxo+7tp06YycODAkM2n9bBYmIiIiMiqwr5EPX/+/CmC9IULF6p66dmzZ5f333/f9nmePHnkzJkzIZpLIiIiIqIoC9QLFCigqrhAYmKiKjGPiYmR5557Tn2n7d+/X44cORLCOSUiIiIiiqJA/a677pLu3bvLggUL5IEHHpCNGzdK0aJF5eWXX7YNEx8fL08++WRI55OIiIiIKKoC9bfeekuuXLki99xzj8ybN08KFSok06ZNU1Vf4IsvvpB69erJzz//HOpZtZ79U0M9B0R+YTANDxERRaCwb0yaLVs2lXbx0KFDcuLECalatapkzpzZ9j2ywYwfPz6k82hZq3qKFGsnkjm/hIO/Dv0lkzZNkjfvfFNyZ87t13HHJ8bL8UvHpUSuEn4dLxEREVHUBupa8eLF1ctenTp1QjI/YSPhIprkSjho+FVDW1D9WdvP/DruRl81knVH18mfj/0pDUu47xwL7SCIiIiIAinsq764gvzqzzzzjIwYMULlUaeUJl7A/8Mv4Nx+ervfx4kgHb7d+K2Hv2BVCyIiIgqssC9RR9UWiI2NlTvvvFMF5dC5c2eZPn26re7quHHjZM2aNVKwYMGQzq+VdDsu0urqGSmQvXSoZyXssEo0ERERBVrYl6hv2LBBsmTJohqQ6iD922+/Vf/OkCGDjB07VjZt2iRt2rSRwYMHh3p2LedS/JVQzwIRERERRWKJOuoKT506VUqUSG4EiJzqQ4YMUZ+//vrrquoLIGCvVatWiOeWrM5glRYiIiKyiLAvUS9WrJgtSIevvvpKDh48KCVLlpT+/fvbPkfVmGPHjoVoLomIiIiIoixQz5Mnj0rLCHhHKTpK04cOHaqqvmgrVqyQc+fOhXBOrSr8GpMyZzYRERFFg7Cv+vLEE0+oRqStW7eWmTNnqmC9YcOGqrdSbc+ePfLYY4+FdD6tKiZd2N+rEbHKEhERRaSwD9SffPJJiY+Plw8//FBOnTolbdu2lc8+u5Fju0+fPjJ37lzVeyk6R6LwL1FnDnMiIiKKBhFRnPrcc8/Jrl275MKFC/L9999L4cKFbd8haD9+/LhcvHhRfU/2GPQSERERWVFEBOqUBkvaiJzdEOq5CLv67yzUJyIiokCLmED90qVLMnr0aGnevLlUrFhR6tatKz169JCff/451LNmbXGnRZa0DfVcEBEREVGk1VGHdevWyQMPPCCHDh1KUSL6999/q86Pbr/9dpkwYYKUKlUqpPNpRapgOP6shBULtBtk4hkiIiIKtLAP1JEzHaXo58+flyJFikirVq2kSpUqKm1jQkKC+v6nn36SFi1ayKpVq9TnFN6Ma8cDN24r3AWQ15iyk4iIIlHYB+pvvPGGJCUlyddffy1du3aVdA7SDb711lvy0ksvyahRo9TfFOauHQ31HBAREREFXNjXUV+4cKHMmjVLHn30UYdBuvbOO+/I/PnzgzpvFBjLLlyQCh9WkK0nt/p93DGeZsFhY1IiIiIKsLAP1NOnT6+qvriDXkrPnDkTlHkKO2FYbWDXmV3SdU5Xv4+XVV+IiIjIKsI+UEed87i4OLfDTZ8+XdVZp9CKS4iTup/XlafmP5XmcV1LuCYhw3ieiIiIAizsA/W7775b1T9HPXVH0Mj03Xffle7du6thKbQ1OObvnC/rjq6TcWvHBXnKREREROEl7BuTvvjii3LLLbeofOlt27aV0qVLqy7mDx8+LDt27FCfo8Q9V65c8vrrr4d6dqNeYlKiRAJ2eERERESBFhsJVV8WLVok3bp1Ux0eIUi3T9lWpkwZmT17thQvXjyEc2pN11iFIwWm+QtPbFtARESRKOwDdR2IL1u2TBYsWCBTpkyRrVu3ypUrV6RcuXLSrl07FcRnzpw51LNpSW+eEZlYTCwdOJ+7dk7yZLFW/nvG80RERBRoERGoa61bt1YvbdOmTeqFqi8M1B1bfhX/t27U+cicR2Ty5smyrMeyoEzP/ESGiIiIKJTCvjGpKzVr1pTq1avLfffdpwL48ePHh3qWLG3K5iky8NeBQa/+MXf7XHl49sNyKf5Squ8QpMP/VvwvKPPiatnx3cqDK+X8tfNBmRciIiKKbhEdqEPt2rXl119/VX/36tUr1LNjTYlXRa4ckS6zu6iAeOHuhUGd/P3T7lcB+f+W/8+7ADrIDwJmbJ0hjcY3kps+uym4EyYiIqKoFPGBuu4UaezYsaGeDUuyxbpzb1RUP3n5ZGCmZRiy+vBqp98fu3TMuxHGnZJgmrZlmnrfe25vUKdLRERE0SkqAnUoX768StFI7neAQGXQ+PHfH2XkypE+/dbhPF33fxUUV8seE/Ss80RERBTNIqoxqSepHCmldEGMPWdum+lxyTvqypfJU0ashA1NiYiIKJiiKlBHFRhKKcmwXsnx2iNr5d0/3w3pPBARERGFWlhVfbnpJjbi87e9CamD9VB3HoO86VZkvoFh4bq1sKMqIiKKRGEVqG/btk3i4+N9/v21a9f8Oj+R4lCC3QcXd4nVWC8QY6ROREREgRVWVV8QpD/77LPy1FNPSfbs2T2uM5yQkCBr1qyRo0ePBnwew9HqayIlM5g+uHY8hHNjXeb9LdRPHYiIiCjyhVWgDl9++aV6kf88eEzEyGH6wHKl16hM7/uTFH+V3Ie67j4RERFFl7AL1NNSBYJZO5x7+sSNvy0YpotxfrtYihVXEhEREUWUsAvUhw4dKh06dFBVX7wJ7o8dOybPPfdcQOctnH3i/5TkXnNZnSRIJequ8EaPiIiIgimsAvV8+fKpQN0XpUuXlsGDB/t9niKR23A04aqIcV0kQ86wLMBeeXCllMxVUorlvNEbK4U3thkgIqJIFFaB+muvvZam39euXdtv8xLJ3IY8s/KKJF4T6XhJJDab36YbjDrg646sk0bjG6m/jaHeBXeso05ERETBFFbpGZHxJS1KlCjht3mJeJf3i6x+QuTCjtTfIUiHCxarN+6B5QeWh3oWiIiIiCIvUKcgWtJOZNdnIouSS58pZR11VlcnIiKiQAurqi8ULIbIuU3Jf8afufHx1eOSeGmfpA/NHAVpOkzPSERERNbAQJ1SMeLPOfy8+7jC8uNlkX9Li+QNRbRO5IwVc/8TERGlEau+UGoHZzr8eMJFkdNJIrcfEtka5340L/z8grzy2yt+minDWj2Thn52yOzKwVDPARERkd8xUKdU3FXw2BovUu2A62j18IXDMmbVGBm+fLhcS/iv8amdLrO6yIKdCzyaJ8bF5IqRdD3Us0BEROR3rPpCAQmK4xPj3fYmO+WfKerliV/OnpJQYx11IiIiCiaWqJPPTl2zQHemQewYhz2TEhERUTAxUI9STbN4Ntz5RJEkI8nhdy8tGxUWvUb+svsXuXL9in/nlTE7ERERBRgD9Sh1mweB+q54kdx7RJp928zh90dOrHZY5aXtlLYyaqVnQXww3PXdXdJ5Vuc0j4dVX4iIiCiYWEc9SnkSciLLCyzet9jxANcvpPpo6j9T5cd/f7SbVoxciLugSrbd2XR8kzy/8HkP5k4k0YuS+h92/ODxsERERERWwBL1KPVwTj8E84Yhxy4dS9FY9HL85dSDiSF3TLhDjl466naatT6tJVtPbnU/cyKy41q8TNnsWWNUv/dMytJ1IiIiCjAG6lGqiAcdFrkLRRddTpAio4rIy7+87HZcfx/9WwKhy+wuEiwpg3MmjCQiIqLAYqBOqXjbmc/IlSP9M10fg98jF494PGxaSsJZik5ERETBxECdnPJXWOppgGvOve6NhbsWSjCwZ1LrCnVmISIiokBgoB6ljHQZnX63J0HkWpJIUpCDqImbJgZ0/B6NixE4ERERWQSzvkQrF533vHUm+UUpseoLERERBRNL1KOWf4POg+cPytXrVyXcxSXGyZtL3nTY+JU9kxIREVEwMVCPVlmLyeEy/htdyTElpeJHFSXcIQ/8a4tfkzqf1wn1rBAREVGUY9WXaBWbS4om+neUhy4ccvh5kuGv2u6hrVfOqi9EREQUTCxRj1JGnTFeDT/tn2k+TyvH8BwSaVgLxlrYCJiIiCIRA/VolbuGV4M/NOshiXaso05ERETBxECdyEOs+kJERETBxEA9ynWLvFopQcKgnYiIiAKLgXqUez2fhL1g9UqZomdS9oRJREREAcZAPcplYsEwERERkSUxUI9yRdL7d3xPLXhKIhXrqBMREVEwMVCPckxk4jlmfbEuVkUiIqJIxECdwl5IcmgzLiQiIqIAY6BOEeHbjd9KidElZMOxDQGbBqu+EBERUTAxUCe5WE7C3qNzH5VDFw5Jl1ldgjI91oIhIiKiQGOgTpI9gvaC60nXAzZu1lEnIiKiYIqgEI281ux3ibSGhK7qqx88f5CNDomIiChsMFD3Unx8vIwYMUIqVaok5cqVkyZNmsjSpUu9Hs+xY8ekT58+UrZsWSlTpox06tRJDhw44PY3mTNnViW75lfx4sXl+nUfSpIL3WH785+SEvFKjikpn637zOffs446ERERBRMDdS/ExcVJq1atZOLEifLLL7/I7t275ZlnnpHmzZvLjBkzPB7P3r17pW7dunLu3DnZsmWL7Nq1S4oWLao+27Fjh9Pfvf/++2oe7PXt21cyZMggaVEtk0QEdyXm209t93ncMUk31n0oEs2QCwlXQj0HREREfhfr/1FGrgEDBsgff/whq1atkpIlk4ugH3zwQZkzZ4706NFDBdooHXclMTFR/QYl8+PHj5csWbKoz0eOHCmzZs2Sjh07ytq1a1MF3mfPnpXJkyfLhg0bJFOmlFF16dKlJZoN+HVAcCZ0/p/gTIe8d3Z9qOeAiIjI71ii7qF9+/bJxx9/LFWrVpVbbrklxXddu3aVy5cvy6BBg9yOZ8qUKbJu3ToVrGfLls32efr06aVz586yadMm+eqrr1L97qOPPpJHHnlEatWqJZUrV07xQnWYaHbu2jnb33vO7pGzV88GZDoxLEUnIiKiIGKg7qFp06ZJQkKCNGrUKNV39evXV+8oWT99+rTL8UyaNEm9OxpPgwYN1PsXX3yR4nPcBIwdO1aSkpJkxYoV6j0QvikkEeHxeY+HehYoGjq9IiIiCjAG6h6aP3++ekfjT3t58+aVYsWKqeosCKSduXLliixevNjpeGrUqKHe169fL+fPn7d9jsD91KlT8t5778ltt92mqrqgdB/VaPzp0ZwSEX7d82tAxsvsjFbGQJ2IiCIP66h7CMEzIMOKI7lz55bDhw+rOuTt2rVzOMy2bdvk2rVrTseDcejSwY0bN0rjxo3Vv1FnvXr16qoRKkrXDx48qBqxzpw5U2bPni158uRxOt9ofGpugHrhwgX1jlJ5vNJFcemquycTqb+/EamnM0Xt/n7CgfFhGQL15CQSYYsHa31x+1gbt4/1cRv5husrOjFQ9wCC60uXLqUIpu3lypVLvaPk25mTJ0/a/nY0Hj0O+/F899136h0l9kuWLJFXXnlFBe8onX/ggQfkt99+k3TpHIfcw4cPl2HDhjmcF4wvd4E2kvlk8tMCGJVfpL/zRYioQP3EiRNefX89Pt72d+L1BI/H48vJGE9UsBzOtiulXmf+3g6upsXtY13cPtbHbeSbixcvhnoWKAQYqHvAXO88a9asDofRJxtdYu7LeMwnLEfjyZgxo7Ro0UKaNWsm/fr1kw8++EAF68gGg4amjqCBK4Y1l6iXKFFCChQooG4WYrKmvGHolyf8A3VPexAtWLCgV99nzHgjE09mU+Ydd+Px5SKGZcA24kXMM+nSxfh9OzjD7WNt3D7Wx23km2hPHBGtGKh7AAGyu9JalE7r+uq+jkePw914cGIbM2aM6iAJDVinTp3qNFBHKkf7dI56HOoEGRO9J0l3Fwj772PkxmPHWNN6C8SFBhcx2zYit2KCXDLH7WNt3D7Wx23kPa6r6MSt7gEEzTrIRh1xR9B5EeTPn9/peAoXLmz729F49DjcjUdDD6k42aHjJZ85KH0+n7qdK2FVnV7rsI46ERERUSAwUPcAcpwjfzocOXLE4TDHjx9X78hz7gwahOpqGY7Go8eBm4IqVaq4na+KFSuqjpeyZ88uPnNQop4zve+ji2QxxnXb3+lNDUsp9JjzhYiIIhEDdQ+1bNlSvW/ZsiXVd2j4iYYx6MCoSZMmTseB7Cy6syRH49m1a5d6R7YXc2dIrhQpUsSWf903DDh9wRJ1IiIiCjQG6h7q2bOnqh+2dOnSVN+tXLlSvbdv3z5FPXRHevfurd5djadLly4ezRM6YNqzZ488+eST4jMnddQvlvN9lJHKHJozULcaa5apH75wOCw7Y0pMSpTuc7vLx6s/DvWsRDzsHxfjmM3DH45ePCrvr3xfrl6/GupZIfIbNib1UIUKFVSQ/emnn6pc6bVr17Z9N2HCBMmSJYsMHTrU9tkff/whAwcOlIcfflj69u1r+7xr166qIej06dPl3XfftbXiRkNSNApF9Rj7hqEosXdUZ/3DDz+U5557zlYtxyfF7xPZ/VWqj7OnE3kvv8hLYZ4BJlDSuwjUZ2+bLWsOr5FcmXNJ41KNpVGJG73QHjh/QP4++rfcW+neFNlppv4zVX7Z/YuMu2ecxMY4PyxxQU+fLr1kzeA4+1D0sl4w/Pm6z6XPj32kf8P+MvKukUGd9snLJyV/1vweZ0CyN3/nfJmwcYJ69anbR2LTxYZFwKuXV98cJRlJEoP//vv8euJ1SUhKkIzpM8q5a+cke8bscuLyCfWeaCTKqSunJHNsZskSm8X2m/Qx6dUxh9/iNxnSZ1DvOTLmUL/B+DANrKPL15PbHhXKVkh9lyHdjUxRar7EkDNXz6hA8sr1K2r+mk5oqr67v/L9cnvJ29Xv8mTOo7bfwQsHJV+WfOp4n/LPFCmXp5xUyFdBVh9ereazYLaCsuP0DnVDeGeZO+VS/CUpn7e8mmfVWDMmnTw8+2Hb9LHcZfOUlQ5VOsi4tePk9NXT0rZiW/V6YeELcjXhqjQp1UQKZS8k9YrWU8sImNe4xDi1PPGJ8Wo4LC8+X3lopVpv56+dl0r5K8lNhW+yrXPzO+ZFzxOWP+5qnCRlSN4+WC8YH9Yr5hHrOj4pXrLGZlXrA+v1nxP/qBdge2FZi2QvIjcXuVkuxl9UN5crDiZ3ONh/UX/1jmXF+DHPRy8dVf++HH9ZDl88LDUK1lDjuBB3QUrkKiH7z+2Xs9fOqnVaIGsByRSbSc0Httf1pOtqP6lTpI7aF7DN957dq/7GdgJ8tuvMLrXuS+YqqZZz99nd6h3bM0uGLJIpfSa1rJguxpc7c261fTXsD1gPmF9ME3/jM/1KuHojLTBFjxgjHIt7QgQNQFG1JTY2VhYsWKCqsiBYfumll2TSpEnSoUMH27D33HOP6s0U9cftc5/+888/0rRpU+nYsaOMHTtWBem4CUA+dKRbrFSpkm3Y999/X/r37y+tWrWS0aNHS+XKlVUHRp999pn63YsvvujVMiA9I/K1nz17NjmXOzb/kZ9EVj8ucvVoimETDZHK+0V23aiaHRb0SdwdY+iNXT9mWIzL72Hg2Bj539nkv3uWqi1f7d/gcDj7cTmazrQO06RjtY5y6MIheXTuo/L73t/V5+PajJPeN/dWOcGRbjBFys6Ea5Ll7SwqiDjS/4g6yYcCbhbGrx8v7au2l+I5HXcA5g4uav4I/vT6HFCkoIzondzGI1g52+23j9nKgyul0fgbN2h/PPqHTNw4UQUES/cvlR61e6iACjdu64+tlx92/CA1CtWQNhXaqKAHF3RcmNcdXSfNyzaX01dOyw///iDP3vKsCmZe+uUl6VStkwo8Fu5eqIIbjGvT8U2p5uXRWo9Kzkw51fS+3/G9CjwQ4Kw6vEoFtAjAEHBplfJVUsGft4rmKCq1C9eWBTsXOPweQQ4CoT1n90jh7IXV/rv91Hbb9/g3ls2RvFnyqvkG7DfYfwDrCUENgh89jriEOBXkIHDTn5t/QxS2kLV5hKiqtjlzRkhX4uQWA3UvIegeMmSI/PDDD+oijRLwN954Q2rWrJliOATuqJLSrVs3+eijj1KNZ+fOnarE/e+//5YMGTLIXXfdJa+99lqqXNDohRTjWbFihcqtXq9ePWnUqJF0795dBe3eShWom012XPp26LpIiX0i0R6oDxobIyP+C9TN4gbHqUAJVh1aJQ2+auB2OqVylZJahWvJztM7ZdupbbbvhzUdJoNvH2wLBNceXase56pSsnTppcrHNxoZz+8yX1pXaG37NwKT3vN6yy3FbrEFQk1KN/GoBNKRg+cPqhI3BF96/JiHHt/3kG82fCOlc5eWvc/tVesaL9xsVM5fWQVEyw8sl/sq36eeFDQs3lCVOqHkDYF9m8ltktfnbYPksZseU6VWKMHDI2uUKmJdInj9btN3qgQR099ycov8vOtnubXkrWraKIXCvGnlM2WSfwdcVcuDkiqUZqOkDYEsAk89LII63BwhQEQJ2ogVI6RM7jKy88xO9Z07KCk9fjk4NwRkTSghxU2Uu88Apblm+sYB+2+eLHnUDYT5ZsUMJdprj6xVxw6OZdyo46YKpb04HuxVzFdR3XBVL1g9uddPI0lN79c9v3q9jChtxrGM8WHZMJ6FuxZKq/Kt1HyjpBs3Q5ifudvnqmMNN6Dtq7RX5wVMH/+pefjvb/2O8whcuHxBcmTLoUrDcYNWNX9V9TduynDzWb9YfXXzh3WIUnXc4G0+sVndjOIJAM4R91S4R9384ukCSv9Raq7X7ZGLR9TfuB7gN7gBvqfiPXI+7rx66tmlRhc1TK5MuSRf1nzqvIVzA84HmE+cl/STF5TkY72ilF1vZ3WDaBjqXKvPoxgWN494morlxDbDOLAOsa7wN0rc9dMI/E6XomN6OL9inBgO84AnGPgtPsf7lUtXpGH5hgzUowwD9SjjS6AOz5wQ+fi8RFSg/lz95+SDVR/I+YHnJdeIG73CaomvJcqoP0fJ3B1zZcaDM2Ts18VsJepmvW7uJZ+3/VwFfqU/KJ3qewSfLcq2kGoFq0mTb5wHzlrHqh3l/Vvfl+tZrkuZsWVsn791x1sy+I/Btn/jZH60/1E13SI5isiSfUvkoVkPpRjX9qe3q4vegF8HqIsGkRmCjGOXjqUK+P49/a/D4RFk3FTkJhVAAm7CELygKgNK+LNlzCafrv1UBVGj7hqlSvlRwo1xojoAjktUYyiWo5jaf1H6/u+Zf1V1AwRmqLpQJX8VNV+4ScO+i6AR/0b1DwQveKKDQAiBF8aD4A5BGgIZjBOB0sFjB6VE4RKSNWNWVbUB32H/R0CEJ1IYDuNDsIUgGEEnAiYEfLq6Ab7DcDpQxGc6yEKAZb50Ovu3oxthdzfI0cCTp1Lk/PrNQD26MFCPMr4G6v/EidQ4EPj5C1e7nt2l6o12me1ZQ2AKTyjtQ+Cp6yI7gjq6aJfwQJUH1PCodqJfKEVDsIpSUZSw4abSXIea0o5BoPVxG/mGgXp0sn4LIQqee3aI/HijfrxZ9UwixWNFDrGap0PlPywv0QylpSg9NataoKp6rNysTDPVqBZVRhCknr16VgWo+87tk62ntqqnDSiRRTUelM6isRcapaGUVD8aRnkCHv/ae+ujGBlyowaMU3iE/GKjF1VDOYyzWoFqUixnsaAHGSih1W0LsKxERESuMFCnG3JWdPn1wTIim+JEarFk3bJQJ7NdpXayaPciVdUGdTwfqvaQqseJevGomhBuUG3BWbr/wXlFWmYVyZirktRq77iuLxERUbhioE4p1ftUZM0TTr+umUlkdH6RF5i20S9Qurq8x3JVGo3S5lK5S0nOjDlly4Etcl/t+1SJrav6riRSDxlOsySnOSUiIookDNQppVj3Ja59col8dF5kd5ilbQyErjlEjhe8S9VLRvWOu8vf7VNAjdJvc9WK3Im5k/MP/zcuBulERETRh4E62XHftjhLOpF/S4nsuC4y4JTIPOft6izn45vaytPr59n+/dHdH6mORD6PWSFVUWW4dFf5t9JgWyoypBpDByfK5Bi5nCSyLk7k1rqvSvotbyd/3mVhiJaGiIiIIhmbW5NP0sWIVMko8kNREaOCyCt5RLLGiLyaJ9RzltxwcGAekZsziUwtnPzZq7e/qvKZP1WugSSVFzlWJjn94tO3PC3LH1ueHKQrhkol92z9Z1UPgLYg/T/Z0ok0zoI0dTx0LIVPHIiIKAKxRJ384u38ya8EQ+RtB7nG/eGLgiLZ04n8fkXkiwsiAwrnk7d7HVedVDw+73H5qt1XKvuIMjlGhv/3u469k1JUHcGfhbDnM9iOHMwyS0REEYiBOvlVbExyCTskGSIXk0QmXxSJN0ROJIrcklmkeVaRK0kolRbJnU5kbZzI/UdE5hYVqdf4C9m/e5ocP/yrGtaRh3KIfF5IRHIVEkmXXjXA/KXrL07nybv63S4CvtJdRfZN9GJcRERERL5joE4pZS3u1+oxudKLPGnXr5KuQqIhID9c9r9/rO4lpUSklBWTeKQ35b1mCa7FcHsQEVHk4bN/SqlgU4lsrMscmRioExFR5GGgTimxUZ4LXDdEREQUPAzUicxYpYWIiIgsgoE6pVaiQ6jnwKIYxFsWb7CIiCgCMVCn1HJXl+itvsKAj4iIiKyBgTo5wLrYjnG9EBERUfAwUKfIt2eCSNwZDxvLskSdiIiIrIGBOoVvyfH5rSKX9rkf7q/uIovbBGOOKGR4g0VERJGHgTqFd4rG3+7wbLjTf/l3vYTTOiIiIqKwxECdUsuQS8LGZQ9K1P2FmUWIiIgoiBioU2rlHhcp2lqk7kdR0FOpHQbjREREZBGxoZ4BsqDYLCJN5yf/fe2kyInFoZ4jP2KVlcjEGywiIoo8LFEnN6ItAHKxvKyXbl18EkJERBGIgTq5EWkBEINtIiIiCg8M1Mm1qCup9HB5o269EBERUbAxUCc3GJASERERhQIDdXIjygJ1lpQTERGRRTBQJ9eMJIkobhuEGiKH54tcPRakGSIiIiJyjIE6uVa+t0SVg7NEltwjMq9CqOeEiIiIohwDdXItexmR1pvF0pIS/T/OhEv+HycFEKssERFR5GGgTu7lri7SYrlY1qG5QUrPyNSO1sVAnYiIIg8DdfJMpoJiWdcvhHoOiIiIiPyOgTpFAJamEhERUeRhoE4UkOwxRERERGnDQJ08kyGnRAYG2BGJ+e+JiCgCMVAnz2QpJFL/q1DPBZETDNSJiCjyMFAnz5V7TCRTfgnrII1VVoiIiChMMFAn75jTNDb7XaKrigSDfOvitiEiosjDQJ28kzHfjb/TZw3lnBARERFFtNhQzwCFmUx5RbIWFzESRbIUkbDDqi8RinXUiYgo8jBQJ+/EpBNptzc5MEqXIfmVdD36Mn4wy4i15KgQ6jkgIiLyO1Z9Ie+li00O0OGheLEEjwNnlqhHpAK3hnoOiIiI/I6BOoW/be+KzC0hcmlfqOeEiIiIyG8YqFP4u7hT5OphkY2DQj0nFCqsikRERBGIgTpFDiMp1HNARERE5DcM1CnKpKGOOjPGWBhL1ImIKPIwUCciIiIisiAG6hQ5DkwXuXbS9TAsFSciIqIwwUCdIsvqPv/9EeCAnAE/ERERBRg7PKK0u2uVyNn1Iqf/EtnzTWjn5dymG4E0M4FED25rIiKKQCxRp7TLf4tIhT4iDb4O9ZywUSERERFFDAbqFKElq6yaEl14g0ZERJGHgTpFGHeBeloCeAb/REREFDwM1Mm/MhWwRqDOxp5RhiXqREQUeRiok3/dOsXijQrTEsAzGCQiIqLgYaBO/lW4mUiz361Z9YWZQSIXty0REUUgBurkf4XuCOHEXQRsswuKnFqRhnGbgn8GhkRERBRgDNQpMG6bEZrpGknOv4s7JbJ/ajDnhoiIiMhnDNQpMEp2EGl/OvjTvXpE5PoFZmghIiKisMdAnQInU97QTHd1H2Z9ISIiorDHQJ0Cq/aI4E/zyIIAlagz+LcuthkgIqLIw0CdAqtsT4kcDAaJiIgoeBioU2Blzi9y06jgTlNlZGHpd3ThTRQREUUeBuoUeFX6BX+aAamjzuCfiIiIgoeBOpEvYnjoWArz2hMRUQRitEHBka9+ECcWjKovLF0nIiKiwGKgTsFR/0uREh2COMEAB9JM/2gxLFEnIqLIw0CdgiN3dZHbpgdvegEPpBmoExERUWAxUKfgiahS6EhaFiIiIrIiBupEvmBjUiIiIgowRhsUOuV6hm8WkIh6OhAJWEediIgiDwN1Cq6qA5LfK/YVSZ8lPII2HfSnCM4ZqBMREVFgMVCn4Kr1jsjdG0XqjBbJXjYw0zCSPB82KcH19zs/FZldUOTsRrsvGKhbCvOoE92QeC3UcxD+rl8M9RwQKQzUKfh1u/PUTH6v8LRIrur+n0biVZEED06yp9eKTM8usvVd58OseVIk7pTIXz38OotERAFxYqnItCwiGweHek7C1+H5IjNyimx4JdRzQsRAnUIofUaRBl+HbvprnhBJihPZ8F91HHtXDpv+YVdiyzrqRGRF655Lft/ydqjnJHyt65v8vnV4qOeEiIE6hVi+utatLmG4qhYThYfO1v+JbP8gNNNOui6yYZDIsd+cDMCqL5QGZ9aLbH0veT8j1xKuiGwZIXJ+e6jnhCgqRGG0QZYTiOovHnET3F054rzee7SVqF89LrJhoMjfz4skxgV/+ru/FNk6QuT35sGfNkW+n28W2fCyyL+fhHpOrA9VajYOEplfJdRzQhQVGKhT6JXsEJrpnl3v+vtfGt34+9wmuy8dBOqnVons/io5qF3Xz7MSJ9wMrB8gcmlP8r9RondqpaS/vFMkMd6Tpfhv/jaLxJ32rREUpuPu6ULiFcc3LZhfd79NSvSscRtK6jAu9fpvGvr94u6Uw9pPM/6syJm/Q3MTEWjO1q+nDWjN28ufjW71tvJ1vtyN25/DeercBu+Gt5++t/PjzfBpWVZ/rqeTy8WS/L1vh5oV5oEsgYE6hV7VQSK3zRTLu3rESf31/yxqILLqcZE5hUV2jBb5uY77cS7vILLtXZFfmyT/e91zku7X26TAqsYS8/udns3X2Q0iC2qKzCkicmBWciOozW94uExHRaZnFVnRWbyGG4MZuUWWPeB8mCuHRKbGJjduO7fF+XCX9olMzyay7H6RpfeKzK+efBMzu7DImqdSDnt2U/I63jnuxme7Pkte39MyR9YF7uAckZl5RY78nPLz9S+LzC0hcu2k+5ufH8qLrHg4eZ+YU1Tk8kH/zNuStsnrHDdi5ptVbJu9E30fL/aFucVEtripH7zrc5HZhZL3f3/Z843n+w+qgc0tLnJhZ/K/L+39b75HeL6c2B7ulhP+eUvk+9LJx0TCVZH5NURWP+n+d5NjRBa3TT6/mG/Yto8WmVvyRgGBV5JSbgNfxJ2S/H82kJhNQ5ILNeZVSl3AcHl/8jJvez+5AAT7+1+PicwtlbzuzE4s/+/8NyN5+/1QIXnZdVU9FBRgnc3KLzKnmMjJlRJS1y+JzK+WvOyOrO4jMr+qyIllycuBV6rMYxQtGKiTNRqVlmwvkr2cWNrB2Tf+RiCuxZ1xXwrtzKmVNwLaHR+lCD5jTnt4MTn6y43S7dW9k//ePNSz3+76UsRIFDkwTby2f2ryMh6a63wYc0adLe84H273F8nvh74XOTxP5MI2kZXdROJOpgzIYVVPkWsnUgfwGpYnUuAm6Po5kcV3p/x823siVw+L7HDTZuDQDyKX94rsn5y8T1w7JrL5Nf/M25H5yU+lzpkCiGXtk7cNtp2vUK0CN5Ab3WTcQDCD/WNld/ErjNMTqAaGm3fdeBM3T2q+B3n2ewyH7eFuOQEB7ZUDIv+8mXweOv+PyK5PPZvOkR9FTi5L+VTq734iVw6K/P2ieM18fGEb+CBm+2iJvbZfYra+k3wuvfivyO7xKQfCk0YE6+v7JxeA4By55+vk9YBqSmZL2ohcOy6yvKPI9Qsil3bd2Eb6hhfrDIUL2GZL20pI7Zsocn5ryuuIGW6ALmwX+bXxjc9Wdg3a7JG1xIZ6BojC3qx8znPCL6wvcnp18t9dDJH4c8lBTIFbRY4vSTnsumdT/x4lKUhjWbhZ8kUqZyWRIneLFL5T5Py25FKXXFVvDB9vumlY+oDIoTkiJRDsXRKp1De5njeqyWTIKZKlyI15s7f76+QUlyixxYUtt6kdAS4uGXKLrH3mxmeo5oNAEPNZvveNUi7Ms4Zg8cRikTt/SZ7nM+uSL8SYP0dViZJMVX/2fnPj7zNrxTUjda78fd+JFGycvJ2uHhP5q7tIjWEi+es7rgq0f0rysmQtnvKGbN9kVTqX7cJpkdO5RPLUSr5BwnhPrxGJP51c4pm5YPI2RnWcUg+JHPtV5PRfIkXvSR4W26VUp+TtgGo7+6aIxKQXic0qkr+hyL8fixz/PeV8YX/BOinS4sZnGG+xdiL5b0n+97l/RPZOECnVWeTEEpF9kxyXGueoIFLtlRvjRedjOcqJrHlapODtyftpuowixe9L3k4Fb7uxLi7sELnwb+r1jZJ63DxoB+eK5K0jkq1EcvCEUtGMeURKPyKS7r9LT8Ll5JuJorgRiUkuiccNoIZ9PFcVkeOLRTLkSB6f2kbmqlSG+zSsyO6E7aHHeXnff9N0APOEJwM4lhDc4kakwlMieW9y0+jcVNKMYBrbcdcXydPFvoT9AsdDbDaRPLVTbt8l9yYvX92PRTLmEjn1V3KghlL6wqZ2GQjOjy5MOX2sW+wH2UrfqP7mCFLi2sN+eHKFyN7vknuKzownFOtFirVNboeD8SHdY7F7RNJlEDn2e+onGEcWimQvI5Ipf/JNR4UnkpdPw+8z6GMlMfnG4eoh508sj/8hkrmwiHHd8/4vzFXe7JcT29D+5j3+fHLwXriFSIbsyZ/tn568/5Z9LPnmVtv5mUiZR5K3mydwfOCGrVDT/6Z1Nnn/xTGH4xHjibELvbBekKUnVzXnBQ24AfbxxojCW4xhRNJzYnLnwoULkitXLjl79qzkzp1bLOWHcj4+ig0T9x1MfozrrBTFGwj6EcT7E8YJ+gbAVy3XJGfzcTV/5vlv9XdyYLPlLfGLTtdE0me68e8dY2+UetqvN73MZpvfTC51RnDx4Lkbny+6VeTUn97PT9nuycGxlq9BctBeooPI7TNEFuLfqzwf393rRX6yCxr1cnizT+CGKc9NydUB3EmfWaTTVcfT0Nt7SqyDICNGpEuSyOJ7kkvg4eYxIpX/2x4oDceNBW6i0mUWObYo9bTvP5Zcnca8nOteENkxJvnv3LVEWt8IHpOSkuTEiRNSsGBBSYcgHlWvoMNZkYy5b8y/nm9Hy+SIeV/BzcvP//0WgTTW5bIOIgdnmRYd6+O/gLLdnuTqK3vsSo3tFb4reZ+Ykcv9/Oh5+r2l4/Vmr93uGwUK7pb31mkipTomV89AyW+N10VKdnR9XkCArKvX6HWFqjqoDqQ/Qwdy6JvCmXu2i/xYOfnvEu1Trk8z3EA2nnPj31Mz3sjY02KFyC//3ZRpDSc6LpHW48HNwW8uqhrixvfWyeIRvW7bbE2+yfypjsjZv50Pr9bLZ8npgt24cEUkVy+R8+fPS86cOT2bHwp7rPpC1oGSNjCXxkQS1F31R5AO7uom+0I3KkWd+bTAo2lvXNzp3yw6KBE1Q2mWN47+Vx/8+vmUn/sSpIM5SAcE6XDwv3YZ3gTp4K+6qqin6+l+5EljYIclgf8FbDpIt/9777c3Sl2dBZvmtiG6XAlPZ1yVFNuGN5W8ouMys7TUbUc7iVRinE8bBRDugnTAOnBVKu7sNx7x4hg7/l8aVATpcGB68pM4b3uExhMzs8M/uh6Hnp6382uetjfbVVfbw9M/V/CEzVs6AYGrIN3RMUFkhyXqUcbSJeooEUGjOTxmRwM6Il9lKSpStM2Nuu+uoLTM2YU4Z+XkKggBg2DET6fgTPm8D/LCTZP5yfWR7dUa7nndcH+o+IzIvx8FZtyF7kgu4fXE/UeTG1EGQ8VnRf790LNhdYn63kkiK/8rgEE1L1TTsSJUVUPVEm9VHZhcPQlVg4JQh5wl6tGJgXqUsXSgboZW/Shtq/h0cn1RXJwLNhUp3eVGg0kiomjmTVAfTDpQR7YnT57GkEcYqEcnVn3xUnx8vIwYMUIqVaok5cqVkyZNmsjSpUu9Hs+xY8ekT58+UrZsWSlTpox06tRJDhxwXWVg5syZUq9ePfWbmjVrypdffikRK3tpkRqvJZcSFmudfOJv/odI+V6hnjMiImuwYpAOSDm5/iUG6UR+wEDdC3FxcdKqVSuZOHGi/PLLL7J792555plnpHnz5jJjxgyPx7N3716pW7eunDt3TrZs2SK7du2SokWLqs927Njh8DevvPKK9OjRQ0aOHCl79uyR6dOnq8/69u0rUafMozf+RhYUIiKyDmR/2TYy1HNBFBEYqHthwIAB8scff8jXX38tJUuWVJ89+OCD0qFDBxVEIwB3JzExUf0GJfPjx4+XLFmySPr06VUAnjlzZunYsaNcv54yLdXcuXNl+PDhMmTIEFWCD5UrV5a33npLPvzwQxW0RxVzvvWWa5OzSDTyobEPERERkYUxUPfQvn375OOPP5aqVavKLbf8l7P4P127dpXLly/LoEHuGzNNmTJF1q1bp4L1bNlu5GVFsN65c2fZtGmTfPXVVylSjb388ssSExMj3bun7NijS5cu6nf9+vVTNwBRo9IzIjkqJueBRg5cpHpTubiJiIiIIgcDdQ9NmzZNEhISpFGjRqm+q18/udOUOXPmyOnTrrMuTJqU3AGJo/E0aNBAvX/xxY1MFWvWrJGdO3eq+vDIC2yWPXt2qVatmhw+fFgWLFggUQOdprTdIVLr7ZS9m3ZOFCnbI5RzRkREROQ3DNQ9NH9+cp5TNOS0lzdvXilWrJiqzrJixQqn47hy5YosXrzY6Xhq1Kih3tevX69adbubrvk3qJIT9ZBPucF4kZtGimQrk9zJSK7/etSM/a/3OUAnL9WHhGw2iYiIiDxh148tOYPgGYoXN3UpboJUhyjZ3rBhg7Rr187hMNu2bZNr1645HY9Ol4iMmRs3bpTGjRt7NF3AdJ01gMXLnJ5RV6nBKyJVeiH5Ba3WJ3eSki59cvfd6GGx+AOqO+yYK4clxtQJiZGpoKpSY9T7VNL9VF0M9BKYt57EbDWV3BMREREFCQN1DyC4vnTpkvrbWe5x5CaHU6fser8zOXnyRi+Ajsajx2Eej/6Nr9NFI9Rhw4Y5nBc8AYgq2f/rKOX0fz1Oln47+WUP9zV3HrX9MyZfV0l/dZ9kOrtcMp5ZLOerjpV0cSck88n5kpC1gsRe2Snp4k9L1qMedjFNRERE5AEG6h4w1zvPmjWrw2HSpUuuRaRLzH0Zjx6HeTz6N75OFw1c0djUXKJeokQJKVCggLU7PLIUtA2oICIt1L/y64/LJ/9bS5KJfpkannTgRgrbyLxPkDVw+1gbt4/1cRv5JglPxHvlCfVsUJAxUPdAxowZbX8768hVl06jvrqv4zGXcOvx6N/4Ot1MmTKplz2cHHmCtC5k+eE2si5uH2vj9rE+biPvcV1FJ251DyAI1gEz0jA6gs6LIH9+W3lrKoULF7b97Wg8ehzm8ejfpGW6RERERBR+GKh7ALnKkT8djhw54nCY48ePq/datWo5HU/16tVVKYKz8ehx4KagSpUq6u+aNWumebpEREREFH4YqHuoZcuW6n3Lli2pvkNDTqRTRAdGuudQR/LkyWPrLMnReHbt2qXeke1Fd4bkarrm37Ru3dqHpSIiIiIiq2Kg7qGePXuq+mFLly5N9d3KlSvVe/v27VPUQ3ekd+/e6t3VeNDjqNa8eXMpU6aMSu1ozhqjq73gc3yvO0siIiIiosjAQN1DFSpUUEH25s2bU+UsnzBhgmTJkkWGDh1q+wwdEKHH0rFjx6YYtmvXrqqTounTp6fI1IJGoVOnTlXVYx555BHb57GxsSrFIlrJ615Nte+++059/vbbb6vqOUREREQUORioe2HkyJFSp04deeKJJ+TMmTMqEwsC8Xnz5sm3336bovfQUaNGyerVq+XVV19NMY4MGTLI5MmTJSEhQaVNxDt6LH3sscdU0D1z5kw1jFmnTp2kT58+8tZbb8mmTZvUZ8uWLVPjfuGFF6Rz585BWgNEREREFCxMz+gF1BtHSfmQIUOkbt26qioMSsDXrFlja/SpIXhG9ZZu3bqlGg9+g2ouAwcOVCX1CMzvuusu1RtpwYLI2Z3auHHj1O8eeugh1dNooUKF1M3BvffeG7DlJSIiIqLQiTGcJeimiIQOj9Cb6dmzZ9nhkUXhycqJEyfUTRvz5loPt4+1cftYH7dR2q7fSF6RM2fOUM8OBQmPECIiIiIiC2KgTkRERERkQQzUiYiIiIgsiIE6EREREZEFMVAnIiIiIrIgpmeMMjrJD1qPs7W9dTMiXLx4UTJnzsxtZEHcPtbG7WN93Ea+wXUbmKwvujBQjzKnT59W76VKlQr1rBAREZGXcJODNI0UHRioR5m8efOq9wMHDvBAt3CpSYkSJeTgwYPMlWtB3D7Wxu1jfdxGvkFJOoL0okWLhnpWKIgYqEcZ/ZgRQTpPkNaG7cNtZF3cPtbG7WN93EbeYwFb9GHlMCIiIiIiC2KgTkRERERkQQzUo0ymTJlk6NCh6p2sidvI2rh9rI3bx/q4jYg8F2Mwzw8RERERkeWwRJ2IiIiIyIIYqBMRERERWRADdSIiIiIiC2KgTkRERERkQQzUo0h8fLyMGDFCKlWqJOXKlZMmTZrI0qVLQz1blof21p999pnUqlVLMmfOrHp3vffee2Xt2rVOf/P3339LmzZtpEyZMlK+fHkZMGCAXL161a/bJhjTCFc//vijxMTEyDfffOPwe26f0ElISJBJkyZJ586dpWvXrjJo0CDZu3dvimF27dolDz30kFp3ZcuWlT59+siZM2fcHqM1atRQ665evXoyd+5cl/MRjGmEk+XLl0vr1q2lcOHCUrx4cbUfIjPLtWvXHA7PY4goSJD1hSLftWvXjDvuuMOoWrWqsX//fvXZ9OnTjQwZMqh3cq5Xr17IjKRe6dOnt/2NdTdr1qxUw//www9GpkyZjFGjRql/nzt3zrj11luNhg0bGpcuXfLLtgnGNMLVyZMnjcKFC6tt9PXXX6f6ntsndNatW2dUqVLFuP/++419+/Y5HGb16tVGrly5jOeff95ISEgwrl69anTo0MGoUKGCcezYsVTDJyUlGQ8//LBRtGhRY9OmTeqzpUuXGlmyZLGt/1BMI5xgH0uXLp3x+uuvG/Hx8eqzv//+2yhRooTRqFEjIy4uLsXwPIaIgoeBepR47rnnVOCyatWqFJ937tzZyJYtm7Fnz56QzZuVLViwwMifP78xYcIE48KFC8b169eNuXPnGgUKFFDrM2fOnCow1A4cOGDkyJHDuPvuu1OMZ/v27UZMTIzx5JNPpnnbBGMa4QwBV/bs2R0G6tw+oYPjJnPmzMawYcOcDoNjDMFh9erVjcTERNvnZ8+eNbJmzWq0bt061W9Gjx6t1t20adNSfD5o0CAVfK5cuTLo0wgnCHDz5ctn3HXXXam++/bbb9Vyf/LJJ7bPeAwRBRcD9Siwd+9eIzY2VpU0OApEcXLr1KlTSObN6jp27GisX78+1ee//vqrrWT9q6++sn3es2dP9ZmjEpxbbrlFXWS2bt2apm0TjGmEq++++864/fbbja5duzoM1Ll9QmPx4sWqdPTpp592Odybb76plvfdd991eCziu59++ilF0J0nTx5VOo6baDOsYwxfv379oE8jnODpApbh5ZdfTvXdP//8o74zB8Y8hoiCi3XUo8C0adNUvdBGjRql+q5+/frqfc6cOXL69OkQzJ213X777VK7du1Unzdr1kxuuukm9ffJkyfV+/Xr12XGjBnqb0frukGDBqqe65dffunztgnGNMLV4cOH5ZVXXpEJEyZIunSpT23cPqFx/Phxuf/++6VIkSIycuRIl8Oi7rqrdQdffPGF7bMFCxbI2bNnVX3x2NjYFMNXrlxZcuXKJatWrZLNmzcHdRrhJFu2bOody2Dv4sWL6l2fA3kMEQUfA/UoMH/+fPWOBlP20DCyWLFiqhHOihUrQjB31vbMM884/a5ChQrqvVSpUup92bJlcuHCBdUtNtapPTRCgz/++MPnbROMaYSrxx57TDV+Q8MzR7h9QmPgwIEq0EVDQDTGdmbPnj2yfft2p+tCr7vFixd7tO7QmLh69eop1ncwphFuqlSpos5lS5YskSlTpqT4DsEt1smjjz6q/s1jiCj4GKhHgfXr16t3tOR3JHfu3Op9w4YNQZ2vcHfq1Cl1MWnVqlWK9ezo4mJezyh5S0xM9GnbBGMa4WjcuHGSJUsWFaw7w+0TfIcOHVJPOBCgIxhEZhVk4ihRooQ0b95cfv/9d9uwej2g1LpQoUJO1wMysxw4cCBN2yeQ0wg3uNn4/PPPJWPGjNK9e3eZPHmy+hxB7bp16+S3335T5zngMUQUfAzUIxxSa126dCnFycoeHt3qwJM8c+XKFVm5cqU8/vjjtvWqq8C4W894RHv+/Hmftk0wphFukGbvvffeU8GGK9w+wTdz5kxVTSFDhgzy119/ydtvv61KbrGtkN60RYsWtsBQr7ucOXM6rLqk14Mv69t++EBOIxw1bdpUZs2aJenTp5dHHnlEnn/+eVWavnDhQilQoIBtOB5DRMGXssIdRRxznbysWbM6HEZfsJzly6XUUD8yR44c8sYbb6Ra1+7Ws17X5nzAnm6bYEwjnCQlJanH8mPGjJGCBQu6HJbbJ/gQlAOedLz66qu2z++++25VX71Xr17Su3dvFbB7u+7A09/4un18mUa4uueee+Tdd99VN75jx45VT0Hq1q2rcs1rPIaIgo8l6hEOjzM1lGw5grp7ui4fuYcLCUoG8UjfvM70una3ngG/82XbBGMa4QSBBRr0tWvXzu2w3D6hqfrirBrDww8/rEq2L1++LNOnT/d63YGnv/F1+/gyjXCFYwnbCTe9U6dOVSXWXbp0kY8++sg2DI8houBjoB7hzCcyXBAdOXfunHrPnz9/UOctXKEU8KWXXrLVTdfQo58n6xlZFnQPp95um2BMI1xs2rRJ9TyKwMIT3D7Bh0aBgIDcHtoU3HnnnervrVu3erzufFnf3g6flmmEIzzdmD17tsrOAx07dlQ3TyiJ7tu3r60RJo8houBjoB7hUOewatWq6u8jR444TZ8GtWrVCuq8haN33nlHSpYsKS+++GKq72rWrOnVevZl2wRjGuHigw8+kB07dqggEA3izC887YAePXqof6ORHLdP8On6zTpgt6cb/6EkVK87BFVoA+JsPaDUVwdc3q7vYEwj3Ozfv18GDx4sbdq0SfH5fffdJyNGjFDbZtiwYeozHkNEwcdAPQq0bNlSvW/ZsiXVd2hcgwY5KJ1ANgZybuLEiSowHD16tMPv77jjDlWyc+LECYeNllD3E1q3bu3ztgnGNMIF6qRXqlTJ4UuX4KJ0Dv9GDm9un+BDHWdnywY6XWPFihVVgIbtpEvYna071G/3ZN0hwEQ6RvP6DsY0wg1SG8bFxTls44FGpbjZWr16tfo3jyGiEAhyB0sUAv/++6/q5rpGjRqpvvvhhx9Ur23dunULybyFi1mzZhnt27dP1TMhJCQkqC6vQfeIieHt3XzzzWo7YHukZdsEYxrh7tFHH3XYMym3T3CtXLlSzX/RokUdHjs4prDs6FkShgwZooYfNWpUqmEfeOAB9d3vv/9u++zs2bNGzpw5jbx586Ya/6ZNm9TwjRs3TvF5MKYRTkaPHq2WYfDgwQ6/r1evnlGgQAHbv3kMEQUXA/Uo8cQTT6iT1fr161NdKLNkyWLs3r07ZPNmdXPmzDHatWtnXLt2LdV3R48eNR555BHVRTrs2rXLyJYtm3HvvfemGG7z5s1q/ffu3TvN2yYY04jUQJ3bJ/juv/9+h9vi2LFjaj1h2bUzZ84YRYoUMWrXrp1i2JMnTxqZM2c27rrrrlTjHzFihBo/jlOz/v37q67mly9fnuLzYEwjnOzcudNInz69Ubly5VQ3IufOnTNy5MiRYhvxGCIKLgbqUeLSpUtGnTp1jPr16xunT582kpKSjA8++MDImDGjMWPGjFDPnmV99913RmxsrJE7d24jX758KV64gOHCUKJECbU+7X8zceJE9e/9+/cbtWrVMm699Vbj8uXLftk2wZhGJAbqwO0TXAj2qlevro6XpUuXqs+wjC1btjRuu+024+rVqymG/+2331Rg9fbbb6v1cOrUKaN58+YqkDx+/LjDJ1qtW7c2ypUrp9YzzJw5U627MWPGOJynYEwjnLz//vvqeEGhA7aXLoRo1aqV2na4uTHjMUQUPAzUo8iFCxeM5557zihTpoy64KC0YuPGjaGeLcv68ccfVWkZLmCuXi+//HKq3y5atMho2LChWtfVqlUzRo4cacTFxfl12wRjGpEYqAO3T3Ch+shTTz1lFC5c2ChVqpQKuBAkO1sfa9asMVq0aGGULl3aqFSpkqqWgfXjTHx8vDFs2DCjfPnyRtmyZY1mzZoZS5YscTlPwZhGOJk/f75x5513Gnny5DFKlixpVKxY0XjllVecrhMeQ0TBEYP/haJuPBEREREROcesL0REREREFsRAnYiIiIjIghioExERERFZEAN1IiIiIiILYqBORERERGRBDNSJiIiIiCyIgToRERERkQUxUCciIiIisiAG6kREREREFsRAnYiIiIjIghioExERERFZEAN1IopKhmHIzz//LPfcc480a9ZMIsnBgwfl6aefltq1a0uOHDnk9ttvl99++83p8Bs3bpRChQrJ448/LuHuypUrctNNN6kX/iYiCmcM1InIK9OnT5dcuXJJTEyM7dWvXz+nw586dUpKlSolsbGxtuGzZs0qjz32mIRKXFycPPXUU9KzZ0+ZP3++JCYmSqTYsmWLCsyfe+452bBhg7z33nuyfPlyadmypfz9998Of7No0SI5ceKETJ06VSJh+bHceG3dujXUs0NElCYM1InIKx07dpQzZ87IjBkzJE+ePOqz0aNHy3fffedw+Pz588v+/ftl+/btki1bNmnRooWcPXtWxo8fL6GSKVMmGTdunApiIw1uPipWrKhe8MQTT8igQYMkb968kiFDBoe/6dSpkzRu3FiGDBni8PtNmzbJuXPnxEqSkpJkxYoVqT5HSfpDDz2kXniiQEQUzmIMPP8lIvIBqlM0b95c/Z0lSxZVcnvzzTc7Hb5+/frSrVs3VS3DCn799Vd149CkSRNZvHixhLudO3eqAB1B6pQpU/w23jZt2sjHH38spUuXFqvAjSJKz19//fVQzwoRUcCwRJ2IfFauXDn1nj59erl69arcd999cvLkSafDI5hHqbpVoDpOJNm2bZt6z5gxo9/GOWnSJFmwYIFYybFjx6R///6hng0iooBjoE5Eafbuu+/aGjE++OCDkpCQEOpZikqoUgRoB+APaGwbyrYEzto8oAEw9jUiokjHQJ2I0gyNSXVAt2TJEnn++efd/uaWW26RdOnS2RqYav/++6/ky5fP9nn37t1T1Ze+//77pUePHurfS5cuVVVq0EAVVVhQ/QPQQPSdd96RkiVLqswnjzzyiFy+fNnlPH366afqKQHGdeedd8ratWsdDocgsVevXlKzZk3JmTOnqm6C+u6oN63h79mzZ0vDhg1V9QzU8cYTBwzvad14NIZ89NFHpVatWlK4cGGpUqWKGpd9NpMRI0ZI+fLl5eWXX1b/xnTxb7xGjhzpchqYTzSotc9+M3HiRNUgVS9T06ZN1fgwP2b4LX6HdZA9e3a13uzrjp8+fVreeOMNKViwoOzbt09VM8K4UJXmn3/+UcNcu3ZNrRfUMa9QoYLaBzBN3Cxo+C3W4Z49e9S/x44da1tOlLLD+vXrpXfv3mpeHMGTn+HDh0udOnXU77BeUVUI1WjsHT58WPr27avWO+zevVvatm2rxl2jRg2179nDEyWsI/wGbTj0fjxmzBiX24GIyCHUUSci8sXevXvRxkX9HRcXZ9x+++3q33iNHz8+1fBNmjQxvv76a9u/Fy1aZBveLCkpyXj88cfV548++qj67PTp08aTTz5pxMbG2j6fN2+ekTVrVqN48eK28VSrVs1ISEgwOnbsaGTPnt0oXLiw7TuM0+yPP/5Qn2O+XnrpJSNbtmxGiRIlbMNnyZLF+PPPP1P85u+//zZKlixpzJ071zZfd911lxq+e/fu6rNNmzYZt912m208Q4cONVq1aqXmB//G/LqzYMECI2fOnMY333yj1sf169eN9957T/2+Ro0axqlTp1L9BuvWvM7cuXbtmtGrVy+jWLFitvVgr1SpUuo7bGt7b7zxhtGgQQPj4MGD6t/Lli0z8uTJY2TIkEFtW/joo49s48fr559/NooWLWrExMSofw8ePFgN17JlSyNXrlzGjh07bOswd+7cantv2bIlxXSxPvV6NRs+fLhx0003Odyn9LaqW7eu8cADDxjnzp1Tn61du1bNX8aMGdX+pL322mtqWTAerIMNGzYY+fPnV9sOw+JzfK/HA9hGGH///v3VPgizZ89W+9Ho0aM92iZERGYM1InIL4E6nDhxwihdurT6LFOmTMZff/3lMlBPTEx0GlR9+eWXKYJOBEEIfgYNGqQ+v/nmm41+/foZJ0+eVN+vXLlSBYj4DoHY+++/rwJRQLCLzxHU6wDKHKgjIH711VeNK1euqM8RnBcsWFB9V6lSJRUoQ3x8vFG+fHljxIgRKeb12LFjRrp06dTwv//+u+3zzp07q8+qVq1qfP/992r9PPHEE8ZXX33lcr0ePXrUyJs3r9GtW7dU3+EzjLNt27ZpDtS1SZMmeR2o//bbb0bmzJmN/fv3p/j83XffVcOXKVPGtq4RzGJYfN6uXTvjzJkz6vedOnUy/v33XxUE47vGjRunGBdufPD5mDFjPArU4fDhw073KWwPbGtzcK2XBcPjRkovD248Fy5cqD7Ply+f8eCDDxqbN2+2bZ9ChQqp76ZMmWIbz9KlS9Vnejht2LBhDNSJyCes+kJEflOgQAH54YcfVNUA5Cp/4IEHbFUSHEHVF2fQQNW+4Sc+Q052QLWJUaNGqfSP0KBBA5ViEPD+wgsvqDSMgEwzaMSKKiOohmEPVVjeeust1dgVUF3lo48+Un/v2LFDVq5cqf6eO3eu7Nq1S9q3b5/i9+gsCNU6YObMmbbPy5Ytq96rVasm7dq1U+sHaSHd1ft+//33VQpMrD97AwcOVO/z5s2TdevWiT9gvryFdY/qI6haZL8uYe/evba87ci7j6os0KdPH1UlBFVkkLcd1VwwfVQJQrUXs+LFi6v38+fPp3lZUI0ImXAwXcyPGT5DVaxLly7ZqiWhQa7OcoP99JtvvpHq1aurf6O6DDLhwIEDB2zjQS56QIYc+5SZ/mo3QETRhYE6EfkV6u4iUwiCmyNHjqigNj4+3m/j18E36p3bK1q0qHq3D8QQJCGPuK6j7O6mADp06GALQlHvGX7//Xf1jvrclStXTvFCXW4Eo+YbAZ1VpmrVql4t4+TJk9W7o3SIqPtcpkwZW/1wf3CWX90Z1P9HW4TNmzenWg/oSArrAS/U8fZkXWC7oZGorseNmxT8PW3aNPVvc91/X5fF1ToFHXib16keF9os4GVWpEiRVPsTbvAyZ86s2jqgfv1ff/2lPi9WrJiq709E5C0G6kTkdyg9fvvtt9Xff/75pzz77LNBma6rEnr9naddRyC4150G6c5+dOkpAnd04GR+HT9+XAWbae3d88KFC7YA11kprG7caC7NDSYE0miY26pVq1TrAQ0usR7wQsNPTyEoPnTokMqxj8agKGlHR0z+onsp9WaduioF1zce5v0JNxwotceNIm5kELhjHTlqqEpE5AkG6kQUEKiigUwr8Pnnn6tSxnCjq7Pkzp1bveu0k8hMEyjmjC7mEmkz3SMsqouEQiDWA0q8UX0JN3moPoQSbkdPOtK6XgO9TnFzgupSeLKA6jMLFy5UVXqQiYeIyFsM1IkoYL744guVOhGQ5i6QAW4g85Lrruh1dQddJcMRXT3GV6hjrav1IOBzFSijmlEooFoLSsA3btzodB5R7cnZd/Z++eUX6dq1qwwYMEBatmwpgeycKxjrFG0WUE8dHVCh59vr16+reuqoA09E5A0G6kQUMKiviwaYaBSIYOXo0aOphtENOFGdwkw3IPRn/XZvoF40qrigPjhKekE3Vh09erSsXr061W+QH3zVqlVpmi5KkVu3bq3+RjUKZ3ncUVprX7XE02o93nBU/QPTxjrB9BCAOqr3P3jwYKe5zO199tlnan3rhsL27Ouo+9IwE/nPATcXugdXM92BUseOHcVXeBKwaNGiFI2Jf/rpJ6lXr56qPqWr3xAReYqBOhH5THcghM5qnEGGDGSCsW+Mp+ng7JNPPlHvCOjRkc348ePVv1Hn2RyE4ntw1PupDujQ2NGe/e8dfWeGqgqod45GjboKBjrGQak6lhed/KBHVnSwhA54EGyicyZkmLGfH/sOitxBkIs60LgZ0A0SNdT9RkdML774oq26hqZLbC9evOjV9PTNkKN1o2+k7LcxsuoAOjdq1KiRykKDaiW4uUFWG8wDGlF6si70dx9++KEK+rE9cIOHTpf0MiOjCm6EXM2TeVnslwcl9agzDh988EGq3yGgRqk7bjw0PX5XPe3arzNkwzHvT9h3br/9dvW3eX0QEXnEt6yORETJHd7gNDJ9+nS3w86YMUN1cmPOow7o8EbnvUZuauS5Rq5wdJikP7/llluMFStWqOG7du1q6/RH50mHy5cvG5UrV3bYsRE60dGd1JhzmP/zzz8qdzZyoCPXNcah82EXKFDA+OSTT1ItB75Dx0h63vQLyzZ16lTbcMj7jk58dCdMZ8+e9WrdYj4xX7qzHUDO+DvvvNNo3bq1cfXq1RTDI+93ixYt1PTQydORI0c8npbOTY8OfI4fP57iuw4dOqjvkNceedHRqZDOK//ss8+mWg94oUMond8edu/ebctxj+2tf699+umntt9i+yNvOZbl9ddfV5/ht9i2epw//vijLe86xoVOhdBBEujc53jpTpe0AwcOqA6tsK2Qmx15/LFM+Bt589esWZNqG2A86HRp165dts8xTeSDx3dNmzZV49H7OD7D/ouc+YDOoMqWLWv06dPH4+1BRKQxUCciryGwQoc85uCsSJEiKoByBcGwfaCOToQQxCBAQ4+Vb775pgp8MFyVKlWMb7/9VgW96CRHd0JkDuoQ0ONlHzwj2EPHM71797YFifqFnkQ1BLQIVDEt9IxZs2ZNo3379qrHSme2bt2qAlgEtujIp2HDhqrHTQ0dPWHezNPEcJ9//rlX6xk3BXfffbeaToUKFdQNC24ezJ026e2BeTdPDzcm5cqVS9X5jj10TGT+HXrR/PDDD23f79y5U3UuheXp2bOn6tzJbMKECUadOnVUB1dY5+hsyXyTgE6pdG+y5m2D7alheV588UXV8ydukND5FLY5AnzsE+jtc9u2bSkCZfRSiw6sECijF1dAgJw+fXrbdPB3ly5dUswvenTt27evupnAjSFu+J5++ulUHTc1a9bM1nuqXp+PPfaY6tAK829eHmwfrGcdqOsbN0yjVq1axrhx42zBPBGRN2LwP8/K3omIiIiIKFhYR52IiIiIyIIYqBMRERERWRADdSIiIiIiC2KgTkRERERkQQzUiYiIiIgsiIE6EREREZEFMVAnIiIiIrIgBupERERERBbEQJ2IiIiIyIIYqBMRERERWRADdSIiIiIiC2KgTkRERERkQQzUiYiIiIgsiIE6EREREZFYz/8BNdKgJIKaoNgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save = True\n",
    "date = \"02_05_25_\"\n",
    "if save :\n",
    "    # Save the model\n",
    "    model = binary_model_3_layer\n",
    "    model_name = \"CIFAR10_model_(3072+2048+2048+1)_save_2\"\n",
    "    save_path = \"Classifiers/\" + date + model_name + \"/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model , save_path + model_name + \".pt\")\n",
    "\n",
    "\n",
    "    # Save Architecture\n",
    "    with open(save_path + \"architecture.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model.architecture + str(model.training_time))\n",
    "\n",
    "    # Save performances of the model\n",
    "    os.makedirs(save_path + \"figures/\", exist_ok=True) \n",
    "    # Plot accuracy = f(n)\n",
    "    plt.plot(np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, len(model.accuracy_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory)) \n",
    "    np.savetxt(save_path +\"figures/accuracy_of_\" + model_name + \".txt\", data, delimiter =\",\", header=\"n,accuracy\")\n",
    "    plt.show() \n",
    "    \n",
    "    # Plot training and validation loss = f(n)\n",
    "    plt.plot(np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(np.linspace(0,len(model.validation_loss_trajectory)*model.observation_rate, len(model.validation_loss_trajectory)), model.validation_loss_trajectory, label=\"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, len(model.training_loss_trajectory)*model.observation_rate)\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/loss_training_\" + model_name + \".txt\", data, delimiter=\",\", header=\"n, training_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot accuracy = f(kappa)\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.accuracy_trajectory)*model.observation_rate+1, len(model.accuracy_trajectory))]\n",
    "    plt.plot(kappa, model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((kappa, model.accuracy_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_accuracy_\" + model_name + \".txt\", data, delimiter=\",\", header=\"kappa, accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory))]\n",
    "    plt.plot(kappa, model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(kappa, model.validation_loss_trajectory, label = \"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data_training = np.column_stack((kappa, model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_training_\" + model_name + \".txt\", data_training, delimiter=\",\", header=\"kappa, training_loss\")\n",
    "    data_validation = np.column_stack((kappa, model.validation_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_validation_\" + model_name + \".txt\", data_validation, delimiter=\",\", header=\"kappa, validation_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"datas\\models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "tensor(0.8692)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import model and plot performances\n",
    "\n",
    "model_name = \"model_2_layer_trained_save_1\"\n",
    "assessed_model = torch.load(\"datas/models/model_2_layer_save_1.pt\", weights_only=False)\n",
    "\n",
    "# Details of the model\n",
    "print(assessed_model.architecture)\n",
    "\n",
    "# Plots of performances\n",
    "accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(accuracy)\n",
    "kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_accuracy)\n",
    "loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(loss)\n",
    "kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_loss)\n",
    "plt.show()\n",
    "\n",
    "# Import datas\n",
    "accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8346c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).to(dtype)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), déjà softmaxé\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque échantillon\n",
    "    \"\"\"\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = dérivée de softmax au logit j du ième batch par rapport au logit k du même batch\n",
    "    for i in range(n):  # Pour chaque échantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-ème donnée du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des dérivées croisées) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "\n",
    "\n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, esp_init = 1, fraction_batch=0.01):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = esp_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = esp_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialisés aléatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi éviter l'explosion ou le vanishing des gradients\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est à dire sur les classes) # dim=0 correspond à la dimension des batchs\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # # hidden neurons layer 2\n",
    "        z3 = torch.mm(self.W3, h2.t() + self.b3)\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajusté en fonction du seuil kappa_eff d'apprentissage des données.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, coef_iter = 1, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=0.2, fraction_batch=1e-2, train_layer1=True, train_layer2=True, train_layer3=True):\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        kappa_max = 1 + coef_iter\n",
    "        max_iter = self.input_dimension**(kappa_max)\n",
    "        print(\"max_iter\", max_iter)\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_minibatches = int(max_iter / self.fraction_batch) # Nombre de minibatches utilisés pour l'apprentissage de la première couche\n",
    "        for i in range(N_minibatches):\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # sélection un lot de données aléatoires parmis les données d'entrainement \n",
    "            # Calcul de la prédiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            # Suivi de l'apprentissage\n",
    "            if i % 100 == 0:\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2, dim=0) # shape (number_of_classes, 1) # on divise par le nombre d'échantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                overall_training_loss = torch.mean(training_loss,dim=0)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2, dim=0)\n",
    "                overall_validation_loss = torch.mean(validation_loss,dim=0)\n",
    "                self.training_loss_trajectory.append(overall_training_loss.item())\n",
    "                self.validation_loss_trajectory.append(overall_validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", overall_training_loss.item(), \"Validation loss\", overall_validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1) # on ne pénalise pas les biais car ils sont déjà petits (initialisés aléatoirement entre -1 et 1)\n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0] # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = torch.mean(grad_z1, dim=0).unsqueeze(1) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0] # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = torch.mean(grad_z2, dim=0).unsqueeze(1)\n",
    "            \n",
    "            # Mise à jours des paramètres de la première couche\n",
    "            self.W1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/self.eps_init**2 + self.reg1*self.W1) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Réajustement d'échelle) # on pénalise l'augmentation de la norme des poids de W1\n",
    "            self.b1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/self.eps_init**2 + self.reg1*self.b1)\n",
    "            self.W2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/self.eps_init**2 +self.reg2*self.W2)\n",
    "            self.b2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/self.eps_init**2 + self.reg2*self.b2)\n",
    "        return \"Training done\"\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
