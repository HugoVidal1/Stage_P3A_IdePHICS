{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337dd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cb0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# improve the ploting style\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "rcParams['font.size'] = 22\n",
    "rcParams['mathtext.fontset'] = 'stix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7698be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 10]) torch.Size([10000, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 32, 32, 3]) torch.Size([10000, 1]) tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]) torch.Size([2000, 32, 32, 3]) torch.Size([2000, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1024]) torch.Size([2000, 1024])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "transform_data = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) # Apply the (x - mean)/var operation on the components of the data # if x is in [0,1] then Normalise(x) is in [-1,1] # is applied on the three channels RGB\n",
    "])\n",
    "\n",
    "# Data import\n",
    "dtype = torch.float32\n",
    "trainset = torchvision.datasets.CIFAR10(root = './datas', train= True, download = True, transform = transform_data)\n",
    "validset = torchvision.datasets.CIFAR10(root = './datas', train = False, download = True, transform = transform_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 1)\n",
    "\n",
    "x_train, y_train_raw, x_valid, y_valid_raw = torch.tensor(trainset.data), torch.tensor(trainset.targets), torch.tensor(validset.data), torch.tensor(validset.targets)\n",
    "\n",
    "# Modification du format des donnes shape (n_data,1) -> (n_data, n_classes)\n",
    "y_train = torch.zeros((y_train_raw.shape[0], torch.max(y_train_raw)+1))\n",
    "for i, y in enumerate(y_train_raw):\n",
    "    j = int(y.item())\n",
    "    y_train[i,j] = 1\n",
    "\n",
    "y_valid = torch.zeros((y_valid_raw.shape[0], torch.max(y_valid_raw)+1))\n",
    "for i,y in enumerate(y_valid_raw):\n",
    "    j = int(y.item())\n",
    "    y_valid[i,j] = 1 \n",
    "print(y_train.shape, y_valid.shape)\n",
    "\n",
    "# Binary reduction of the classes # To avoid using softmax, we regroup classes in two classes\n",
    "class_binary_reduction = True\n",
    "determination_des_classes = True\n",
    "black_and_white_images = True\n",
    "\n",
    "if class_binary_reduction :\n",
    "    if determination_des_classes :\n",
    "        # Determination des classes\n",
    "        class_list = []\n",
    "        class_index = 0\n",
    "        for i in range (x_train.shape[0]):\n",
    "            if y_train[i, class_index] == 1:\n",
    "                class_list.append(x_train[i])\n",
    "                class_index += 1\n",
    "            if len(class_list) == len(y_train[0]):\n",
    "                break\n",
    "        for i, image in enumerate(class_list):\n",
    "            plt.subplot(2, int(len(class_list)/2+1),i+1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "        plt.show()\n",
    "                \n",
    "    # classe1 = [0, 1, 8, 9]  # vehicles\n",
    "    # classe2 = [2, 3, 4, 5]  # animals\n",
    "    \n",
    "    classe1 = [4]  # elk\n",
    "    classe2 = [7]  # horse\n",
    "\n",
    "    # Cration des masques pour les chantillons appartenant  ces classes\n",
    "    mask_classe1_train = y_train[:, classe1].sum(dim=1) > 0  # True si appartient  classe1\n",
    "    mask_classe2_train = y_train[:, classe2].sum(dim=1) > 0  # True si appartient  classe2\n",
    "    \n",
    "    mask_classe1_valid = y_valid[:, classe1].sum(dim=1) > 0\n",
    "    mask_classe2_valid = y_valid[:, classe2].sum(dim=1) > 0\n",
    "    \n",
    "    # Filtrage des exemples concerns\n",
    "    mask_train = torch.logical_or(mask_classe1_train, mask_classe2_train)\n",
    "    mask_valid = torch.logical_or(mask_classe1_valid, mask_classe2_valid)\n",
    "    x_train, y_train = x_train[mask_train], y_train[mask_train]\n",
    "    x_valid, y_valid = x_valid[mask_valid], y_valid[mask_valid]\n",
    "\n",
    "    # Cration du vecteur de labels binaires (1 pour classe1, 0 pour classe2)\n",
    "    y_train = (y_train[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    y_valid = (y_valid[:, classe1].sum(dim=1) > 0).to(dtype).unsqueeze(1)\n",
    "    \n",
    "    # Avec tanh\n",
    "    \n",
    "    # x_train = 2*(x_train-0.5)\n",
    "    # y_train = 2*(y_train-0.5)\n",
    "    # x_valid = 2*(x_valid-0.5)\n",
    "    # y_valid = 2*(y_valid-0.5)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, y_train[0:10], x_valid.shape, y_valid.shape)\n",
    "\n",
    "if black_and_white_images :\n",
    "    x_train = 0.299*x_train[:,:,:,0] + 0.587*x_train[:,:,:,1] + 0.114*x_train[:,:,:,2]\n",
    "    x_valid = 0.299*x_valid[:,:,:,0] + 0.587*x_valid[:,:,:,1] + 0.114*x_valid[:,:,:,2]\n",
    "    for i, image in enumerate(x_train[0:10]):\n",
    "        plt.subplot(2, int(len(x_train[0:10])/2+1),i+1)\n",
    "        plt.imshow(image, cmap = 'grey')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "    x_train, x_valid = (x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])).to(dtype), x_valid.reshape(x_valid.shape[0], x_valid.shape[1]*x_valid.shape[2]).to(dtype)\n",
    "    print(x_train.shape, x_valid.shape)\n",
    "\n",
    "else :    \n",
    "    x_train, x_valid = (x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2]*x_train.shape[3])).to(dtype), x_valid.reshape(x_valid.shape[0], x_valid.shape[1]*x_valid.shape[2]*x_valid.shape[3]).to(dtype)\n",
    "    print(x_train.shape, x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45060724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing on :  mps\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Computing on : \", device)\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0).to(device),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).type(dtype).to(device)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), dj softmax\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque chantillon\n",
    "    \"\"\"\n",
    "    s = s.to(device)\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype).to(device) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = drive de softmax au logit j du ime batch par rapport au logit k du mme batch\n",
    "    for i in range(n):  # Pour chaque chantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-me donne du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des drives croises) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return torch.exp(-x)/((1 + torch.exp(-x))**2)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - (torch.tanh(x)**2)).to(device)\n",
    "\n",
    "class two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajust en fonction du seuil kappa_eff d'apprentissage des donnes.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prdiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output); grad_z2  = grad_z2.to(dtype) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la dure de l'entranement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "   \n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, number_of_classes,lr=0.01, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(three_layer_NN,self).__init__()\n",
    "        # Initialisation des proprits du rseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la proprit dure d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du rseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(number_of_classes, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t()\n",
    "        output = self.softmax(z3) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage alatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            \n",
    "            # Calcul de la prdiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = (torch.einsum('no,noz->nz',grad_output,softmax_derivative(output))).to(dtype) # shape (n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la dure d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "\n",
    "class binary_classification_two_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,lr=1e-3, reg1 = 0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 100):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_two_layer_NN,self).__init__()\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.lr = lr\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        self.training_time = 0\n",
    "        # Initializing layers and bias        \n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(1, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        output = torch.sigmoid(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajust en fonction du seuil kappa_eff d'apprentissage des donnes.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, reg1=0, reg2=0, eps_init=1, fraction_batch=0.01, observation_rate = 100, train_layer_1 = True, train_layer_2 = True):\n",
    "        # Initializing the training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"2 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch) \n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        # Moving training and validation datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            # Slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] \n",
    "            # Calcul de la prdiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            #print(output[0:5], z2[0:5], h1[0:5], z1[0:5])\n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                print(\"Output\", output[20:22])\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "                \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch; grad_output = grad_output.to(dtype)\n",
    "            grad_z2 = grad_output*sigmoid_derivative(z2); grad_z2  = grad_z2.to(dtype) # shape(n_data, 1)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2); grad_h1  = grad_h1.to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1); grad_z1  = grad_z1.to(dtype) # shape (n_data, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # if i == 250:\n",
    "            #     break\n",
    "\n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z2, h1, z1, grad_output, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcul de la dure de l'entranement    \n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "class binary_classification_three_layer_NN(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_1_size, hidden_2_size, lr=0.01, decay_rate = 0, reg1 =0, reg2 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10):\n",
    "        \"\"\"\n",
    "        Constructor of the three-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(binary_classification_three_layer_NN,self).__init__()\n",
    "        # Initialisation des proprits du rseau\n",
    "        self.architecture = \"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.eps_init = eps_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        \n",
    "        # Initialisation de la proprit dure d'entrainement\n",
    "        self.training_time = 0\n",
    "        \n",
    "        # Initialisation des couches et des biais du rseau\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = eps_init*torch.randn(hidden_2_size, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.W3 = eps_init*torch.randn(1, hidden_2_size, dtype=dtype)/np.sqrt(hidden_2_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = eps_init*(2*torch.rand(hidden_2_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        self.b3 = eps_init*(2*torch.rand(1,1,dtype=dtype)-1)\n",
    "        \n",
    "        # Moving to device \n",
    "        self.W1 = self.W1.to(device)\n",
    "        self.W2 = self.W2.to(device)\n",
    "        self.W3 = self.W3.to(device)\n",
    "        self.b1 = self.b1.to(device)\n",
    "        self.b2 = self.b2.to(device)\n",
    "        self.b3 = self.b3.to(device)\n",
    "        \n",
    "        # Initializing Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        \n",
    "        #Initializing losses and accuracies during training list\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, hidden_2_size ) # logits layer 2\n",
    "        h2 = ReLU(z2) # hidden neurons layer 2\n",
    "        z3 = (torch.mm(self.W3,h2.t()) + self.b3).t() # shape (n_data, 1)\n",
    "        output = sigmoid(z3) # output layer # shape (n_data, 1)\n",
    "        return output, z3, h2, z2, h1, z1\n",
    "    \n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, kappa = 2, lr=1e-3, decay_rate = 0, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=1, fraction_batch=0.01, observation_rate = 10, train_layer_1 = True, train_layer_2 = True, train_layer_3 = True, dropout_rate = 0):\n",
    "        # Initializing training chronometer\n",
    "        start = time.time()\n",
    "        unwanted_time = 0\n",
    "        # Initializing training parameters\n",
    "        self.architecture = \"3 layers\" + \" - Training first layer : \" + str(train_layer_1) + \" - Training second layer : \" + str(train_layer_2) + \" - Training third layer : \" + str(train_layer_3) + \" - kappa = \" + str(kappa) + \" - lr = \" + str(lr) + \" - decay_rate \" + str(decay_rate) + \" - reg1 = \" + str(reg1) + \" - reg2 = \" + str(reg2) + \" - eps_init = \" + str(eps_init) + \" - fraction_batch = \" + str(fraction_batch)\n",
    "        self.lr = torch.tensor(lr)\n",
    "        self.decay_rate = decay_rate\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.observation_rate = observation_rate\n",
    "        # Moving input datas to device\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        x_valid = x_valid.to(device)\n",
    "        y_valid = y_valid.to(device)        \n",
    "        # Initializing the number of training iterations \n",
    "        N_datas = self.input_dimension**(kappa)/self.fraction_batch # Number of datas that we want to use for the training\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_iterations = int(N_datas/minibatch_size)\n",
    "        print(f\"For kappa = {kappa}, the number of datas used for the training is {N_datas} and the number of iterations is {N_iterations}.\")\n",
    "        \n",
    "        for i in range(N_iterations):\n",
    "            \n",
    "            # Tirage alatoire d'un minibatch\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            \n",
    "            # Dropout\n",
    "            # dropout_mask = (torch.rand((minibatch_size, x_train.shape[1])) > dropout_rate).to(dtype)\n",
    "            # x_minibatch = x_minibatch*dropout_mask\n",
    "            \n",
    "            # Calcul de la prdiction\n",
    "            output, z3, h2, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            \n",
    "            # Suivi de l'apprentissage # l'chantillonnage dpend d'observation_rate\n",
    "            if i % self.observation_rate == 0:    \n",
    "                unwanted_time_begin = time.time() # Pour soustraire le temps li  la sauvegarde des donnes d'apprentissage au temps d'entrainement\n",
    "                # Calcul des losses et de l'accuracy et ajout aux trajectoires\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2)\n",
    "                self.training_loss_trajectory.append(training_loss.item())\n",
    "                self.validation_loss_trajectory.append(validation_loss.item())\n",
    "                accuracy = torch.mean(((self.forward(x_valid)[0] > 0.5).to(dtype) == y_valid).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", training_loss.item(), \"Validation loss\", validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "                # Soustraction du temps de sauvegarde\n",
    "                unwanted_time += time.time() - unwanted_time_begin \n",
    "            \n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 + reg3*(||W3||**2 + ||b3||**2) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            grad_output = (output - y_minibatch).to(dtype)\n",
    "            grad_z3 = grad_output*sigmoid_derivative(z3).to(dtype) # shape (n_data, 1) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h2 = (torch.mm(grad_z3, self.W3)).to(dtype) # shape (n_data, hidden_2_size)\n",
    "            grad_z2 = (grad_h2*ReLU_derivative(z2)).to(dtype) # shape(n_data, hidden_2_size)         \n",
    "            grad_h1 = (torch.mm(grad_z2, self.W2)).to(dtype)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = (grad_h1*ReLU_derivative(z1)).to(dtype) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = (torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0]).to(dtype) # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = (torch.mean(grad_z1, dim=0).unsqueeze(1)).to(dtype) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = (torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0]).to(dtype) # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = (torch.mean(grad_z2, dim=0).unsqueeze(1)).to(dtype)\n",
    "            # Calcul de la moyenne empirique de dLoss/dW3 par backpropagation\n",
    "            grad_W3 = (torch.mm(grad_z3.t(),h2)/x_minibatch.shape[0]).to(dtype)\n",
    "            # Calcul de la moyenne empirique du gradient dLoss/db\" par backpropagation\n",
    "            grad_b3 = (torch.mean(grad_z3,dim=0).unsqueeze(1)).to(dtype)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            if train_layer_1:\n",
    "                self.W1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/(self.eps_init**2) + self.reg1*self.W1)).to(dtype) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "                self.b1 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/(self.eps_init**2) + self.reg1*self.b1)).to(dtype)\n",
    "            if train_layer_2:\n",
    "                self.W2 -= (self.lr*(grad_W2/(self.eps_init**2) +self.reg2*self.W2)).to(dtype)\n",
    "                self.b2 -= (self.lr*(grad_b2/(self.eps_init**2) + self.reg2*self.b2)).to(dtype)\n",
    "            if train_layer_3:\n",
    "                self.W3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_W3/(eps_init**2) + self.reg3*self.W3)).to(dtype)\n",
    "                self.b3 -= (self.lr*(torch.sqrt(torch.tensor(self.hidden_2_size))*grad_b3/(eps_init**2) + self.reg3*self.b3)).to(dtype)\n",
    "            del x_minibatch, y_minibatch, output, z3, h2, z2, h1, z1, grad_output, grad_z3, grad_h2, grad_z2, grad_h1, grad_z1, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3\n",
    "            gc.collect()\n",
    "            \n",
    "            self.lr = lr*torch.exp(torch.tensor(-i/self.decay_rate))\n",
    "            # if i == 12000:\n",
    "            #     break\n",
    "        \n",
    "        # Calcul de la dure d'entrainement\n",
    "        self.training_time = time.time() - start - unwanted_time\n",
    "        return \"Training done\"\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36584c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_2_layer = binary_classification_two_layer_NN(1024, 512, eps_init = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57236d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 1.6, the number of datas used for the training is 6553600.000000005 and the number of iterations is 65536.\n",
      "Output tensor([[0.5301],\n",
      "        [0.5135]], device='mps:0')\n",
      "Iteration 0 Training loss 0.1261061280965805 Validation loss 0.12592941522598267 Accuracy 0.5035000443458557\n",
      "Output tensor([[0.5129],\n",
      "        [0.5191]], device='mps:0')\n",
      "Iteration 10 Training loss 0.1250447779893875 Validation loss 0.12558652460575104 Accuracy 0.5125000476837158\n",
      "Output tensor([[0.5045],\n",
      "        [0.4935]], device='mps:0')\n",
      "Iteration 20 Training loss 0.12453171610832214 Validation loss 0.12518012523651123 Accuracy 0.492000013589859\n",
      "Output tensor([[0.5001],\n",
      "        [0.4938]], device='mps:0')\n",
      "Iteration 30 Training loss 0.12535317242145538 Validation loss 0.12507618963718414 Accuracy 0.47600001096725464\n",
      "Output tensor([[0.4898],\n",
      "        [0.4966]], device='mps:0')\n",
      "Iteration 40 Training loss 0.12619490921497345 Validation loss 0.12493804097175598 Accuracy 0.47450003027915955\n",
      "Output tensor([[0.4933],\n",
      "        [0.4977]], device='mps:0')\n",
      "Iteration 50 Training loss 0.1241459846496582 Validation loss 0.12486832588911057 Accuracy 0.4805000126361847\n",
      "Output tensor([[0.4859],\n",
      "        [0.4849]], device='mps:0')\n",
      "Iteration 60 Training loss 0.1245701014995575 Validation loss 0.12476833164691925 Accuracy 0.4930000305175781\n",
      "Output tensor([[0.4976],\n",
      "        [0.4896]], device='mps:0')\n",
      "Iteration 70 Training loss 0.12497031688690186 Validation loss 0.12472888827323914 Accuracy 0.4905000329017639\n",
      "Output tensor([[0.4845],\n",
      "        [0.4947]], device='mps:0')\n",
      "Iteration 80 Training loss 0.12505067884922028 Validation loss 0.12466111034154892 Accuracy 0.49500003457069397\n",
      "Output tensor([[0.4918],\n",
      "        [0.4992]], device='mps:0')\n",
      "Iteration 90 Training loss 0.125136137008667 Validation loss 0.12465987354516983 Accuracy 0.5085000395774841\n",
      "Output tensor([[0.4966],\n",
      "        [0.5014]], device='mps:0')\n",
      "Iteration 100 Training loss 0.12490643560886383 Validation loss 0.12460251152515411 Accuracy 0.5045000314712524\n",
      "Output tensor([[0.5035],\n",
      "        [0.4760]], device='mps:0')\n",
      "Iteration 110 Training loss 0.12427616119384766 Validation loss 0.1244845911860466 Accuracy 0.5005000233650208\n",
      "Output tensor([[0.4836],\n",
      "        [0.4827]], device='mps:0')\n",
      "Iteration 120 Training loss 0.1253058761358261 Validation loss 0.12442273646593094 Accuracy 0.503000020980835\n",
      "Output tensor([[0.4954],\n",
      "        [0.5044]], device='mps:0')\n",
      "Iteration 130 Training loss 0.12393069267272949 Validation loss 0.12445560097694397 Accuracy 0.530500054359436\n",
      "Output tensor([[0.5003],\n",
      "        [0.5020]], device='mps:0')\n",
      "Iteration 140 Training loss 0.12363656610250473 Validation loss 0.12450266629457474 Accuracy 0.5730000138282776\n",
      "Output tensor([[0.4841],\n",
      "        [0.4944]], device='mps:0')\n",
      "Iteration 150 Training loss 0.12442302703857422 Validation loss 0.12436042726039886 Accuracy 0.5435000061988831\n",
      "Output tensor([[0.4960],\n",
      "        [0.4960]], device='mps:0')\n",
      "Iteration 160 Training loss 0.12315290421247482 Validation loss 0.12426289916038513 Accuracy 0.5160000324249268\n",
      "Output tensor([[0.4915],\n",
      "        [0.5055]], device='mps:0')\n",
      "Iteration 170 Training loss 0.12511730194091797 Validation loss 0.12423611432313919 Accuracy 0.5355000495910645\n",
      "Output tensor([[0.4686],\n",
      "        [0.4865]], device='mps:0')\n",
      "Iteration 180 Training loss 0.12439467757940292 Validation loss 0.12417976558208466 Accuracy 0.534000039100647\n",
      "Output tensor([[0.4991],\n",
      "        [0.4915]], device='mps:0')\n",
      "Iteration 190 Training loss 0.1248159185051918 Validation loss 0.1241544559597969 Accuracy 0.5605000257492065\n",
      "Output tensor([[0.4823],\n",
      "        [0.4911]], device='mps:0')\n",
      "Iteration 200 Training loss 0.12293019890785217 Validation loss 0.12402698397636414 Accuracy 0.5164999961853027\n",
      "Output tensor([[0.4796],\n",
      "        [0.4949]], device='mps:0')\n",
      "Iteration 210 Training loss 0.12405117601156235 Validation loss 0.12397072464227676 Accuracy 0.5035000443458557\n",
      "Output tensor([[0.4592],\n",
      "        [0.4758]], device='mps:0')\n",
      "Iteration 220 Training loss 0.12343302369117737 Validation loss 0.12393180280923843 Accuracy 0.503000020980835\n",
      "Output tensor([[0.4968],\n",
      "        [0.4604]], device='mps:0')\n",
      "Iteration 230 Training loss 0.12368390709161758 Validation loss 0.1238645613193512 Accuracy 0.5045000314712524\n",
      "Output tensor([[0.4820],\n",
      "        [0.4759]], device='mps:0')\n",
      "Iteration 240 Training loss 0.12425414472818375 Validation loss 0.1238027885556221 Accuracy 0.5090000033378601\n",
      "Output tensor([[0.4612],\n",
      "        [0.4787]], device='mps:0')\n",
      "Iteration 250 Training loss 0.12219325453042984 Validation loss 0.1237538754940033 Accuracy 0.5085000395774841\n",
      "Output tensor([[0.4912],\n",
      "        [0.4737]], device='mps:0')\n",
      "Iteration 260 Training loss 0.12231830507516861 Validation loss 0.12370512634515762 Accuracy 0.5075000524520874\n",
      "Output tensor([[0.4702],\n",
      "        [0.4366]], device='mps:0')\n",
      "Iteration 270 Training loss 0.12307704985141754 Validation loss 0.12366984784603119 Accuracy 0.5065000057220459\n",
      "Output tensor([[0.4893],\n",
      "        [0.4551]], device='mps:0')\n",
      "Iteration 280 Training loss 0.121790312230587 Validation loss 0.12360867857933044 Accuracy 0.5065000057220459\n",
      "Output tensor([[0.4800],\n",
      "        [0.4942]], device='mps:0')\n",
      "Iteration 290 Training loss 0.12391214817762375 Validation loss 0.12353644520044327 Accuracy 0.518500030040741\n",
      "Output tensor([[0.4932],\n",
      "        [0.4973]], device='mps:0')\n",
      "Iteration 300 Training loss 0.1248009204864502 Validation loss 0.12348988652229309 Accuracy 0.5394999980926514\n",
      "Output tensor([[0.4937],\n",
      "        [0.5021]], device='mps:0')\n",
      "Iteration 310 Training loss 0.12456043064594269 Validation loss 0.12346316874027252 Accuracy 0.5445000529289246\n",
      "Output tensor([[0.4797],\n",
      "        [0.4918]], device='mps:0')\n",
      "Iteration 320 Training loss 0.1245836615562439 Validation loss 0.12341050803661346 Accuracy 0.5445000529289246\n",
      "Output tensor([[0.4950],\n",
      "        [0.4958]], device='mps:0')\n",
      "Iteration 330 Training loss 0.12465406209230423 Validation loss 0.12339372932910919 Accuracy 0.5640000104904175\n",
      "Output tensor([[0.4810],\n",
      "        [0.5115]], device='mps:0')\n",
      "Iteration 340 Training loss 0.1236087754368782 Validation loss 0.1233135312795639 Accuracy 0.5520000457763672\n",
      "Output tensor([[0.4814],\n",
      "        [0.4871]], device='mps:0')\n",
      "Iteration 350 Training loss 0.12467511743307114 Validation loss 0.12327797710895538 Accuracy 0.5595000386238098\n",
      "Output tensor([[0.4995],\n",
      "        [0.4755]], device='mps:0')\n",
      "Iteration 360 Training loss 0.12389950454235077 Validation loss 0.12323740124702454 Accuracy 0.5690000057220459\n",
      "Output tensor([[0.4936],\n",
      "        [0.4777]], device='mps:0')\n",
      "Iteration 370 Training loss 0.12399787455797195 Validation loss 0.12320960313081741 Accuracy 0.5815000534057617\n",
      "Output tensor([[0.4920],\n",
      "        [0.4908]], device='mps:0')\n",
      "Iteration 380 Training loss 0.12267909944057465 Validation loss 0.12312646955251694 Accuracy 0.5565000176429749\n",
      "Output tensor([[0.5065],\n",
      "        [0.5156]], device='mps:0')\n",
      "Iteration 390 Training loss 0.12288171797990799 Validation loss 0.12309452891349792 Accuracy 0.5690000057220459\n",
      "Output tensor([[0.4809],\n",
      "        [0.4903]], device='mps:0')\n",
      "Iteration 400 Training loss 0.12252077460289001 Validation loss 0.12306460738182068 Accuracy 0.5805000066757202\n",
      "Output tensor([[0.5040],\n",
      "        [0.5103]], device='mps:0')\n",
      "Iteration 410 Training loss 0.12240736931562424 Validation loss 0.12307294458150864 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.4980],\n",
      "        [0.4835]], device='mps:0')\n",
      "Iteration 420 Training loss 0.12187249958515167 Validation loss 0.12298459559679031 Accuracy 0.5860000252723694\n",
      "Output tensor([[0.5052],\n",
      "        [0.5047]], device='mps:0')\n",
      "Iteration 430 Training loss 0.12507514655590057 Validation loss 0.12296062707901001 Accuracy 0.6020000576972961\n",
      "Output tensor([[0.4901],\n",
      "        [0.4921]], device='mps:0')\n",
      "Iteration 440 Training loss 0.12162011861801147 Validation loss 0.1229078397154808 Accuracy 0.5945000052452087\n",
      "Output tensor([[0.5020],\n",
      "        [0.4938]], device='mps:0')\n",
      "Iteration 450 Training loss 0.12344039976596832 Validation loss 0.122860386967659 Accuracy 0.5855000019073486\n",
      "Output tensor([[0.5015],\n",
      "        [0.4881]], device='mps:0')\n",
      "Iteration 460 Training loss 0.12279541045427322 Validation loss 0.12280088663101196 Accuracy 0.5795000195503235\n",
      "Output tensor([[0.5055],\n",
      "        [0.5141]], device='mps:0')\n",
      "Iteration 470 Training loss 0.12317699193954468 Validation loss 0.12276676297187805 Accuracy 0.5840000510215759\n",
      "Output tensor([[0.4720],\n",
      "        [0.5074]], device='mps:0')\n",
      "Iteration 480 Training loss 0.12224742770195007 Validation loss 0.1226830929517746 Accuracy 0.562000036239624\n",
      "Output tensor([[0.4668],\n",
      "        [0.4867]], device='mps:0')\n",
      "Iteration 490 Training loss 0.12112025916576385 Validation loss 0.12263116985559464 Accuracy 0.562000036239624\n",
      "Output tensor([[0.4927],\n",
      "        [0.4871]], device='mps:0')\n",
      "Iteration 500 Training loss 0.12342151254415512 Validation loss 0.12258093804121017 Accuracy 0.5460000038146973\n",
      "Output tensor([[0.5127],\n",
      "        [0.4515]], device='mps:0')\n",
      "Iteration 510 Training loss 0.12426800280809402 Validation loss 0.12254294753074646 Accuracy 0.5410000085830688\n",
      "Output tensor([[0.4613],\n",
      "        [0.4968]], device='mps:0')\n",
      "Iteration 520 Training loss 0.12566789984703064 Validation loss 0.12251552194356918 Accuracy 0.5254999995231628\n",
      "Output tensor([[0.4744],\n",
      "        [0.4494]], device='mps:0')\n",
      "Iteration 530 Training loss 0.12529848515987396 Validation loss 0.12246588617563248 Accuracy 0.5400000214576721\n",
      "Output tensor([[0.4773],\n",
      "        [0.4645]], device='mps:0')\n",
      "Iteration 540 Training loss 0.12254264205694199 Validation loss 0.12241902202367783 Accuracy 0.5515000224113464\n",
      "Output tensor([[0.4956],\n",
      "        [0.4870]], device='mps:0')\n",
      "Iteration 550 Training loss 0.1230638399720192 Validation loss 0.12239736318588257 Accuracy 0.5735000371932983\n",
      "Output tensor([[0.5061],\n",
      "        [0.4744]], device='mps:0')\n",
      "Iteration 560 Training loss 0.12258504331111908 Validation loss 0.1223679855465889 Accuracy 0.578000009059906\n",
      "Output tensor([[0.5002],\n",
      "        [0.4956]], device='mps:0')\n",
      "Iteration 570 Training loss 0.12372288107872009 Validation loss 0.1223035380244255 Accuracy 0.5590000152587891\n",
      "Output tensor([[0.4920],\n",
      "        [0.4942]], device='mps:0')\n",
      "Iteration 580 Training loss 0.12205089628696442 Validation loss 0.1222689226269722 Accuracy 0.5605000257492065\n",
      "Output tensor([[0.4985],\n",
      "        [0.5167]], device='mps:0')\n",
      "Iteration 590 Training loss 0.12056747078895569 Validation loss 0.12221574783325195 Accuracy 0.5630000233650208\n",
      "Output tensor([[0.4612],\n",
      "        [0.4528]], device='mps:0')\n",
      "Iteration 600 Training loss 0.1200091689825058 Validation loss 0.12217691540718079 Accuracy 0.565500020980835\n",
      "Output tensor([[0.4858],\n",
      "        [0.5167]], device='mps:0')\n",
      "Iteration 610 Training loss 0.12171290814876556 Validation loss 0.12214574217796326 Accuracy 0.5730000138282776\n",
      "Output tensor([[0.4929],\n",
      "        [0.4670]], device='mps:0')\n",
      "Iteration 620 Training loss 0.12147203087806702 Validation loss 0.12210353463888168 Accuracy 0.5764999985694885\n",
      "Output tensor([[0.4819],\n",
      "        [0.4452]], device='mps:0')\n",
      "Iteration 630 Training loss 0.1218496710062027 Validation loss 0.1221020445227623 Accuracy 0.6010000109672546\n",
      "Output tensor([[0.4822],\n",
      "        [0.4917]], device='mps:0')\n",
      "Iteration 640 Training loss 0.12382250279188156 Validation loss 0.1220620796084404 Accuracy 0.5980000495910645\n",
      "Output tensor([[0.5165],\n",
      "        [0.5163]], device='mps:0')\n",
      "Iteration 650 Training loss 0.1227031946182251 Validation loss 0.12208278477191925 Accuracy 0.6190000176429749\n",
      "Output tensor([[0.5089],\n",
      "        [0.4139]], device='mps:0')\n",
      "Iteration 660 Training loss 0.1233605146408081 Validation loss 0.12196116149425507 Accuracy 0.5845000147819519\n",
      "Output tensor([[0.5016],\n",
      "        [0.5197]], device='mps:0')\n",
      "Iteration 670 Training loss 0.12202923744916916 Validation loss 0.12192074954509735 Accuracy 0.5710000395774841\n",
      "Output tensor([[0.5085],\n",
      "        [0.5017]], device='mps:0')\n",
      "Iteration 680 Training loss 0.12368135899305344 Validation loss 0.12188025563955307 Accuracy 0.5675000548362732\n",
      "Output tensor([[0.4781],\n",
      "        [0.4809]], device='mps:0')\n",
      "Iteration 690 Training loss 0.12236705422401428 Validation loss 0.12184624373912811 Accuracy 0.5705000162124634\n",
      "Output tensor([[0.5158],\n",
      "        [0.5139]], device='mps:0')\n",
      "Iteration 700 Training loss 0.12120522558689117 Validation loss 0.12182822078466415 Accuracy 0.5870000123977661\n",
      "Output tensor([[0.4944],\n",
      "        [0.5211]], device='mps:0')\n",
      "Iteration 710 Training loss 0.1211497113108635 Validation loss 0.12179810553789139 Accuracy 0.6035000085830688\n",
      "Output tensor([[0.5100],\n",
      "        [0.4889]], device='mps:0')\n",
      "Iteration 720 Training loss 0.1205788180232048 Validation loss 0.12174268066883087 Accuracy 0.57750004529953\n",
      "Output tensor([[0.4816],\n",
      "        [0.4352]], device='mps:0')\n",
      "Iteration 730 Training loss 0.12082778662443161 Validation loss 0.1217205747961998 Accuracy 0.5534999966621399\n",
      "Output tensor([[0.4153],\n",
      "        [0.4668]], device='mps:0')\n",
      "Iteration 740 Training loss 0.12008197605609894 Validation loss 0.12166811525821686 Accuracy 0.5555000305175781\n",
      "Output tensor([[0.4981],\n",
      "        [0.5049]], device='mps:0')\n",
      "Iteration 750 Training loss 0.1215890496969223 Validation loss 0.12163093686103821 Accuracy 0.5895000100135803\n",
      "Output tensor([[0.5269],\n",
      "        [0.5015]], device='mps:0')\n",
      "Iteration 760 Training loss 0.1229681596159935 Validation loss 0.12160743027925491 Accuracy 0.6060000061988831\n",
      "Output tensor([[0.4764],\n",
      "        [0.4475]], device='mps:0')\n",
      "Iteration 770 Training loss 0.1221349909901619 Validation loss 0.12156210839748383 Accuracy 0.5910000205039978\n",
      "Output tensor([[0.4980],\n",
      "        [0.5218]], device='mps:0')\n",
      "Iteration 780 Training loss 0.12192338705062866 Validation loss 0.12153684347867966 Accuracy 0.6085000038146973\n",
      "Output tensor([[0.4839],\n",
      "        [0.5177]], device='mps:0')\n",
      "Iteration 790 Training loss 0.12576718628406525 Validation loss 0.12149034440517426 Accuracy 0.6010000109672546\n",
      "Output tensor([[0.5108],\n",
      "        [0.4991]], device='mps:0')\n",
      "Iteration 800 Training loss 0.1241820678114891 Validation loss 0.12145575881004333 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.5032],\n",
      "        [0.5166]], device='mps:0')\n",
      "Iteration 810 Training loss 0.12085861712694168 Validation loss 0.12142501026391983 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.4846],\n",
      "        [0.5230]], device='mps:0')\n",
      "Iteration 820 Training loss 0.12387141585350037 Validation loss 0.12145143747329712 Accuracy 0.6270000338554382\n",
      "Output tensor([[0.4774],\n",
      "        [0.4552]], device='mps:0')\n",
      "Iteration 830 Training loss 0.12142600864171982 Validation loss 0.12135259062051773 Accuracy 0.6095000505447388\n",
      "Output tensor([[0.4568],\n",
      "        [0.5250]], device='mps:0')\n",
      "Iteration 840 Training loss 0.12310687452554703 Validation loss 0.121322862803936 Accuracy 0.612500011920929\n",
      "Output tensor([[0.5124],\n",
      "        [0.5294]], device='mps:0')\n",
      "Iteration 850 Training loss 0.12460757791996002 Validation loss 0.12127191573381424 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.5049],\n",
      "        [0.5162]], device='mps:0')\n",
      "Iteration 860 Training loss 0.12095344066619873 Validation loss 0.12127170711755753 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5118],\n",
      "        [0.5328]], device='mps:0')\n",
      "Iteration 870 Training loss 0.12262383103370667 Validation loss 0.1212281882762909 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.4751],\n",
      "        [0.4927]], device='mps:0')\n",
      "Iteration 880 Training loss 0.11947418749332428 Validation loss 0.12119441479444504 Accuracy 0.6210000514984131\n",
      "Output tensor([[0.4378],\n",
      "        [0.5064]], device='mps:0')\n",
      "Iteration 890 Training loss 0.12361937016248703 Validation loss 0.12113528698682785 Accuracy 0.6105000376701355\n",
      "Output tensor([[0.5162],\n",
      "        [0.4981]], device='mps:0')\n",
      "Iteration 900 Training loss 0.12262405455112457 Validation loss 0.12115775048732758 Accuracy 0.6260000467300415\n",
      "Output tensor([[0.5204],\n",
      "        [0.5239]], device='mps:0')\n",
      "Iteration 910 Training loss 0.12280672043561935 Validation loss 0.12107963860034943 Accuracy 0.6155000329017639\n",
      "Output tensor([[0.5218],\n",
      "        [0.4503]], device='mps:0')\n",
      "Iteration 920 Training loss 0.12073493003845215 Validation loss 0.12110845744609833 Accuracy 0.6325000524520874\n",
      "Output tensor([[0.5151],\n",
      "        [0.5360]], device='mps:0')\n",
      "Iteration 930 Training loss 0.12437238544225693 Validation loss 0.12107764184474945 Accuracy 0.6345000267028809\n",
      "Output tensor([[0.5158],\n",
      "        [0.4620]], device='mps:0')\n",
      "Iteration 940 Training loss 0.12107791751623154 Validation loss 0.12101239711046219 Accuracy 0.6265000104904175\n",
      "Output tensor([[0.5198],\n",
      "        [0.5320]], device='mps:0')\n",
      "Iteration 950 Training loss 0.12330543249845505 Validation loss 0.12102601677179337 Accuracy 0.6355000138282776\n",
      "Output tensor([[0.4901],\n",
      "        [0.5226]], device='mps:0')\n",
      "Iteration 960 Training loss 0.12184542417526245 Validation loss 0.12100827693939209 Accuracy 0.6310000419616699\n",
      "Output tensor([[0.5418],\n",
      "        [0.5084]], device='mps:0')\n",
      "Iteration 970 Training loss 0.12230636179447174 Validation loss 0.12105501443147659 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.5300],\n",
      "        [0.5085]], device='mps:0')\n",
      "Iteration 980 Training loss 0.12053027749061584 Validation loss 0.12097591161727905 Accuracy 0.6315000057220459\n",
      "Output tensor([[0.5420],\n",
      "        [0.4471]], device='mps:0')\n",
      "Iteration 990 Training loss 0.12463315576314926 Validation loss 0.12089081853628159 Accuracy 0.6345000267028809\n",
      "Output tensor([[0.5121],\n",
      "        [0.5217]], device='mps:0')\n",
      "Iteration 1000 Training loss 0.12279603630304337 Validation loss 0.12082250416278839 Accuracy 0.6300000548362732\n",
      "Output tensor([[0.5597],\n",
      "        [0.5165]], device='mps:0')\n",
      "Iteration 1010 Training loss 0.12348461896181107 Validation loss 0.12075827270746231 Accuracy 0.6265000104904175\n",
      "Output tensor([[0.4810],\n",
      "        [0.4988]], device='mps:0')\n",
      "Iteration 1020 Training loss 0.11798930913209915 Validation loss 0.12070691585540771 Accuracy 0.625\n",
      "Output tensor([[0.4989],\n",
      "        [0.4475]], device='mps:0')\n",
      "Iteration 1030 Training loss 0.12281385064125061 Validation loss 0.12063786387443542 Accuracy 0.6005000472068787\n",
      "Output tensor([[0.4300],\n",
      "        [0.5085]], device='mps:0')\n",
      "Iteration 1040 Training loss 0.1200772076845169 Validation loss 0.12060758471488953 Accuracy 0.5905000567436218\n",
      "Output tensor([[0.5458],\n",
      "        [0.4957]], device='mps:0')\n",
      "Iteration 1050 Training loss 0.11977657675743103 Validation loss 0.12059061974287033 Accuracy 0.6150000095367432\n",
      "Output tensor([[0.5102],\n",
      "        [0.5259]], device='mps:0')\n",
      "Iteration 1060 Training loss 0.11971184611320496 Validation loss 0.12059364467859268 Accuracy 0.6265000104904175\n",
      "Output tensor([[0.5052],\n",
      "        [0.4496]], device='mps:0')\n",
      "Iteration 1070 Training loss 0.12005108594894409 Validation loss 0.12053564935922623 Accuracy 0.6145000457763672\n",
      "Output tensor([[0.5372],\n",
      "        [0.5059]], device='mps:0')\n",
      "Iteration 1080 Training loss 0.12119995057582855 Validation loss 0.12055070698261261 Accuracy 0.6295000314712524\n",
      "Output tensor([[0.5070],\n",
      "        [0.4936]], device='mps:0')\n",
      "Iteration 1090 Training loss 0.1200052872300148 Validation loss 0.12054508179426193 Accuracy 0.6365000009536743\n",
      "Output tensor([[0.5408],\n",
      "        [0.5344]], device='mps:0')\n",
      "Iteration 1100 Training loss 0.11651162058115005 Validation loss 0.12048646807670593 Accuracy 0.6305000185966492\n",
      "Output tensor([[0.5224],\n",
      "        [0.4833]], device='mps:0')\n",
      "Iteration 1110 Training loss 0.11809663474559784 Validation loss 0.12038973718881607 Accuracy 0.6145000457763672\n",
      "Output tensor([[0.4758],\n",
      "        [0.5232]], device='mps:0')\n",
      "Iteration 1120 Training loss 0.12197833508253098 Validation loss 0.12035253643989563 Accuracy 0.6155000329017639\n",
      "Output tensor([[0.5311],\n",
      "        [0.4480]], device='mps:0')\n",
      "Iteration 1130 Training loss 0.12203830480575562 Validation loss 0.12031254172325134 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5036],\n",
      "        [0.4486]], device='mps:0')\n",
      "Iteration 1140 Training loss 0.11989495903253555 Validation loss 0.12028972804546356 Accuracy 0.5895000100135803\n",
      "Output tensor([[0.5185],\n",
      "        [0.4831]], device='mps:0')\n",
      "Iteration 1150 Training loss 0.12358764559030533 Validation loss 0.1202632486820221 Accuracy 0.6135000586509705\n",
      "Output tensor([[0.3850],\n",
      "        [0.3948]], device='mps:0')\n",
      "Iteration 1160 Training loss 0.12135659903287888 Validation loss 0.12023824453353882 Accuracy 0.6165000200271606\n",
      "Output tensor([[0.4732],\n",
      "        [0.4882]], device='mps:0')\n",
      "Iteration 1170 Training loss 0.12246190011501312 Validation loss 0.12019935995340347 Accuracy 0.6130000352859497\n",
      "Output tensor([[0.4463],\n",
      "        [0.5246]], device='mps:0')\n",
      "Iteration 1180 Training loss 0.12222832441329956 Validation loss 0.12017887085676193 Accuracy 0.6160000562667847\n",
      "Output tensor([[0.4542],\n",
      "        [0.4958]], device='mps:0')\n",
      "Iteration 1190 Training loss 0.11836210638284683 Validation loss 0.12015188485383987 Accuracy 0.6075000166893005\n",
      "Output tensor([[0.4991],\n",
      "        [0.5077]], device='mps:0')\n",
      "Iteration 1200 Training loss 0.11945177614688873 Validation loss 0.1201224997639656 Accuracy 0.6065000295639038\n",
      "Output tensor([[0.4570],\n",
      "        [0.5255]], device='mps:0')\n",
      "Iteration 1210 Training loss 0.1174064353108406 Validation loss 0.12009120732545853 Accuracy 0.612000048160553\n",
      "Output tensor([[0.4173],\n",
      "        [0.4788]], device='mps:0')\n",
      "Iteration 1220 Training loss 0.12649862468242645 Validation loss 0.12006251513957977 Accuracy 0.6025000214576721\n",
      "Output tensor([[0.4998],\n",
      "        [0.4592]], device='mps:0')\n",
      "Iteration 1230 Training loss 0.12182547897100449 Validation loss 0.12004190683364868 Accuracy 0.5895000100135803\n",
      "Output tensor([[0.5470],\n",
      "        [0.4861]], device='mps:0')\n",
      "Iteration 1240 Training loss 0.1229085624217987 Validation loss 0.12002167850732803 Accuracy 0.5905000567436218\n",
      "Output tensor([[0.4601],\n",
      "        [0.4011]], device='mps:0')\n",
      "Iteration 1250 Training loss 0.12107396870851517 Validation loss 0.12000896036624908 Accuracy 0.5820000171661377\n",
      "Output tensor([[0.5286],\n",
      "        [0.4759]], device='mps:0')\n",
      "Iteration 1260 Training loss 0.11663109809160233 Validation loss 0.11996711045503616 Accuracy 0.6225000023841858\n",
      "Output tensor([[0.4760],\n",
      "        [0.5439]], device='mps:0')\n",
      "Iteration 1270 Training loss 0.1229652687907219 Validation loss 0.11998368054628372 Accuracy 0.6300000548362732\n",
      "Output tensor([[0.4370],\n",
      "        [0.5437]], device='mps:0')\n",
      "Iteration 1280 Training loss 0.12302068620920181 Validation loss 0.11992727220058441 Accuracy 0.627500057220459\n",
      "Output tensor([[0.5231],\n",
      "        [0.4804]], device='mps:0')\n",
      "Iteration 1290 Training loss 0.12107040733098984 Validation loss 0.11987318098545074 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.4194],\n",
      "        [0.4868]], device='mps:0')\n",
      "Iteration 1300 Training loss 0.1224040612578392 Validation loss 0.1198526993393898 Accuracy 0.6270000338554382\n",
      "Output tensor([[0.4691],\n",
      "        [0.4297]], device='mps:0')\n",
      "Iteration 1310 Training loss 0.12133315950632095 Validation loss 0.11982399970293045 Accuracy 0.624500036239624\n",
      "Output tensor([[0.5135],\n",
      "        [0.4882]], device='mps:0')\n",
      "Iteration 1320 Training loss 0.11812297254800797 Validation loss 0.11981776356697083 Accuracy 0.6320000290870667\n",
      "Output tensor([[0.5438],\n",
      "        [0.4863]], device='mps:0')\n",
      "Iteration 1330 Training loss 0.11901668459177017 Validation loss 0.11975667625665665 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.5031],\n",
      "        [0.4783]], device='mps:0')\n",
      "Iteration 1340 Training loss 0.12302186340093613 Validation loss 0.11973106861114502 Accuracy 0.6200000047683716\n",
      "Output tensor([[0.4215],\n",
      "        [0.5575]], device='mps:0')\n",
      "Iteration 1350 Training loss 0.11540202796459198 Validation loss 0.11973629146814346 Accuracy 0.6310000419616699\n",
      "Output tensor([[0.5050],\n",
      "        [0.5272]], device='mps:0')\n",
      "Iteration 1360 Training loss 0.11889173090457916 Validation loss 0.11970102041959763 Accuracy 0.6285000443458557\n",
      "Output tensor([[0.5147],\n",
      "        [0.5299]], device='mps:0')\n",
      "Iteration 1370 Training loss 0.1215670183300972 Validation loss 0.11967907845973969 Accuracy 0.6310000419616699\n",
      "Output tensor([[0.5320],\n",
      "        [0.5598]], device='mps:0')\n",
      "Iteration 1380 Training loss 0.12070520222187042 Validation loss 0.11964046210050583 Accuracy 0.6260000467300415\n",
      "Output tensor([[0.5108],\n",
      "        [0.5112]], device='mps:0')\n",
      "Iteration 1390 Training loss 0.11923772841691971 Validation loss 0.11959594488143921 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5152],\n",
      "        [0.4464]], device='mps:0')\n",
      "Iteration 1400 Training loss 0.11820752173662186 Validation loss 0.11956936120986938 Accuracy 0.6160000562667847\n",
      "Output tensor([[0.5005],\n",
      "        [0.4108]], device='mps:0')\n",
      "Iteration 1410 Training loss 0.12212758511304855 Validation loss 0.11955869197845459 Accuracy 0.6290000081062317\n",
      "Output tensor([[0.5264],\n",
      "        [0.5720]], device='mps:0')\n",
      "Iteration 1420 Training loss 0.12065322697162628 Validation loss 0.11953522264957428 Accuracy 0.628000020980835\n",
      "Output tensor([[0.4412],\n",
      "        [0.5689]], device='mps:0')\n",
      "Iteration 1430 Training loss 0.11676979064941406 Validation loss 0.11950388550758362 Accuracy 0.6140000224113464\n",
      "Output tensor([[0.5010],\n",
      "        [0.5285]], device='mps:0')\n",
      "Iteration 1440 Training loss 0.11932159960269928 Validation loss 0.11946720629930496 Accuracy 0.6180000305175781\n",
      "Output tensor([[0.4763],\n",
      "        [0.5532]], device='mps:0')\n",
      "Iteration 1450 Training loss 0.12065053731203079 Validation loss 0.1194426417350769 Accuracy 0.6155000329017639\n",
      "Output tensor([[0.4496],\n",
      "        [0.4023]], device='mps:0')\n",
      "Iteration 1460 Training loss 0.11883403360843658 Validation loss 0.11942145228385925 Accuracy 0.6160000562667847\n",
      "Output tensor([[0.4034],\n",
      "        [0.4821]], device='mps:0')\n",
      "Iteration 1470 Training loss 0.11833751201629639 Validation loss 0.11939018964767456 Accuracy 0.6170000433921814\n",
      "Output tensor([[0.4886],\n",
      "        [0.4062]], device='mps:0')\n",
      "Iteration 1480 Training loss 0.11968084424734116 Validation loss 0.11937807500362396 Accuracy 0.627500057220459\n",
      "Output tensor([[0.5224],\n",
      "        [0.5268]], device='mps:0')\n",
      "Iteration 1490 Training loss 0.1175684928894043 Validation loss 0.11937476694583893 Accuracy 0.6310000419616699\n",
      "Output tensor([[0.5512],\n",
      "        [0.4445]], device='mps:0')\n",
      "Iteration 1500 Training loss 0.12108837068080902 Validation loss 0.11932298541069031 Accuracy 0.627500057220459\n",
      "Output tensor([[0.4699],\n",
      "        [0.5546]], device='mps:0')\n",
      "Iteration 1510 Training loss 0.12232702225446701 Validation loss 0.11929818987846375 Accuracy 0.624500036239624\n",
      "Output tensor([[0.4376],\n",
      "        [0.4880]], device='mps:0')\n",
      "Iteration 1520 Training loss 0.11904856562614441 Validation loss 0.11927530914545059 Accuracy 0.6185000538825989\n",
      "Output tensor([[0.4245],\n",
      "        [0.5413]], device='mps:0')\n",
      "Iteration 1530 Training loss 0.12451039999723434 Validation loss 0.11925310641527176 Accuracy 0.6195000410079956\n",
      "Output tensor([[0.4159],\n",
      "        [0.4869]], device='mps:0')\n",
      "Iteration 1540 Training loss 0.1224454864859581 Validation loss 0.11927463114261627 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.5258],\n",
      "        [0.4754]], device='mps:0')\n",
      "Iteration 1550 Training loss 0.12065478414297104 Validation loss 0.11922959238290787 Accuracy 0.6315000057220459\n",
      "Output tensor([[0.5008],\n",
      "        [0.5129]], device='mps:0')\n",
      "Iteration 1560 Training loss 0.11793389171361923 Validation loss 0.11922116577625275 Accuracy 0.6340000033378601\n",
      "Output tensor([[0.4339],\n",
      "        [0.5258]], device='mps:0')\n",
      "Iteration 1570 Training loss 0.11803495138883591 Validation loss 0.11922307312488556 Accuracy 0.6365000009536743\n",
      "Output tensor([[0.4902],\n",
      "        [0.5531]], device='mps:0')\n",
      "Iteration 1580 Training loss 0.11815967410802841 Validation loss 0.11921559274196625 Accuracy 0.6360000371932983\n",
      "Output tensor([[0.5025],\n",
      "        [0.4968]], device='mps:0')\n",
      "Iteration 1590 Training loss 0.11584562063217163 Validation loss 0.11911510676145554 Accuracy 0.6290000081062317\n",
      "Output tensor([[0.4264],\n",
      "        [0.5079]], device='mps:0')\n",
      "Iteration 1600 Training loss 0.11865425854921341 Validation loss 0.1190950945019722 Accuracy 0.6310000419616699\n",
      "Output tensor([[0.5354],\n",
      "        [0.5242]], device='mps:0')\n",
      "Iteration 1610 Training loss 0.11854969710111618 Validation loss 0.11906356364488602 Accuracy 0.6255000233650208\n",
      "Output tensor([[0.5310],\n",
      "        [0.4108]], device='mps:0')\n",
      "Iteration 1620 Training loss 0.119040846824646 Validation loss 0.1190430074930191 Accuracy 0.6300000548362732\n",
      "Output tensor([[0.4494],\n",
      "        [0.4957]], device='mps:0')\n",
      "Iteration 1630 Training loss 0.11877783387899399 Validation loss 0.11902254819869995 Accuracy 0.627500057220459\n",
      "Output tensor([[0.5749],\n",
      "        [0.4917]], device='mps:0')\n",
      "Iteration 1640 Training loss 0.12051055580377579 Validation loss 0.11899355053901672 Accuracy 0.6260000467300415\n",
      "Output tensor([[0.4593],\n",
      "        [0.5268]], device='mps:0')\n",
      "Iteration 1650 Training loss 0.12124085426330566 Validation loss 0.11896639317274094 Accuracy 0.6185000538825989\n",
      "Output tensor([[0.5166],\n",
      "        [0.5221]], device='mps:0')\n",
      "Iteration 1660 Training loss 0.1209464967250824 Validation loss 0.11894430220127106 Accuracy 0.6205000281333923\n",
      "Output tensor([[0.5320],\n",
      "        [0.4686]], device='mps:0')\n",
      "Iteration 1670 Training loss 0.11570542305707932 Validation loss 0.1189177930355072 Accuracy 0.6210000514984131\n",
      "Output tensor([[0.5549],\n",
      "        [0.5642]], device='mps:0')\n",
      "Iteration 1680 Training loss 0.12054475396871567 Validation loss 0.11889563500881195 Accuracy 0.6260000467300415\n",
      "Output tensor([[0.4441],\n",
      "        [0.4256]], device='mps:0')\n",
      "Iteration 1690 Training loss 0.1176724061369896 Validation loss 0.11887967586517334 Accuracy 0.6300000548362732\n",
      "Output tensor([[0.4808],\n",
      "        [0.4867]], device='mps:0')\n",
      "Iteration 1700 Training loss 0.11806530505418777 Validation loss 0.11886049807071686 Accuracy 0.6320000290870667\n",
      "Output tensor([[0.4527],\n",
      "        [0.4865]], device='mps:0')\n",
      "Iteration 1710 Training loss 0.11882998794317245 Validation loss 0.11882798373699188 Accuracy 0.625\n",
      "Output tensor([[0.4048],\n",
      "        [0.4945]], device='mps:0')\n",
      "Iteration 1720 Training loss 0.11553717404603958 Validation loss 0.11880022287368774 Accuracy 0.6185000538825989\n",
      "Output tensor([[0.4120],\n",
      "        [0.5347]], device='mps:0')\n",
      "Iteration 1730 Training loss 0.12188142538070679 Validation loss 0.11877109110355377 Accuracy 0.6185000538825989\n",
      "Output tensor([[0.4857],\n",
      "        [0.4670]], device='mps:0')\n",
      "Iteration 1740 Training loss 0.12475957721471786 Validation loss 0.1187707781791687 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.5467],\n",
      "        [0.5352]], device='mps:0')\n",
      "Iteration 1750 Training loss 0.11847163736820221 Validation loss 0.11873730272054672 Accuracy 0.627500057220459\n",
      "Output tensor([[0.4760],\n",
      "        [0.5089]], device='mps:0')\n",
      "Iteration 1760 Training loss 0.11956895887851715 Validation loss 0.11872841417789459 Accuracy 0.6290000081062317\n",
      "Output tensor([[0.4655],\n",
      "        [0.5183]], device='mps:0')\n",
      "Iteration 1770 Training loss 0.12052571773529053 Validation loss 0.11873768270015717 Accuracy 0.6355000138282776\n",
      "Output tensor([[0.4022],\n",
      "        [0.5242]], device='mps:0')\n",
      "Iteration 1780 Training loss 0.12144189327955246 Validation loss 0.11868324130773544 Accuracy 0.6290000081062317\n",
      "Output tensor([[0.3987],\n",
      "        [0.4509]], device='mps:0')\n",
      "Iteration 1790 Training loss 0.1134762167930603 Validation loss 0.1186801940202713 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.4535],\n",
      "        [0.5063]], device='mps:0')\n",
      "Iteration 1800 Training loss 0.12605738639831543 Validation loss 0.11863616108894348 Accuracy 0.6195000410079956\n",
      "Output tensor([[0.4873],\n",
      "        [0.4967]], device='mps:0')\n",
      "Iteration 1810 Training loss 0.12355605512857437 Validation loss 0.11862684786319733 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.3998],\n",
      "        [0.5309]], device='mps:0')\n",
      "Iteration 1820 Training loss 0.12194819003343582 Validation loss 0.11864820122718811 Accuracy 0.609000027179718\n",
      "Output tensor([[0.4991],\n",
      "        [0.3770]], device='mps:0')\n",
      "Iteration 1830 Training loss 0.11565281450748444 Validation loss 0.1185961589217186 Accuracy 0.6210000514984131\n",
      "Output tensor([[0.3659],\n",
      "        [0.4978]], device='mps:0')\n",
      "Iteration 1840 Training loss 0.11851562559604645 Validation loss 0.11855681240558624 Accuracy 0.6255000233650208\n",
      "Output tensor([[0.4750],\n",
      "        [0.4693]], device='mps:0')\n",
      "Iteration 1850 Training loss 0.11901816725730896 Validation loss 0.11853917688131332 Accuracy 0.6290000081062317\n",
      "Output tensor([[0.5275],\n",
      "        [0.5321]], device='mps:0')\n",
      "Iteration 1860 Training loss 0.11848033964633942 Validation loss 0.11855906248092651 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.5649],\n",
      "        [0.4497]], device='mps:0')\n",
      "Iteration 1870 Training loss 0.11736904084682465 Validation loss 0.1185603141784668 Accuracy 0.6385000348091125\n",
      "Output tensor([[0.5524],\n",
      "        [0.4707]], device='mps:0')\n",
      "Iteration 1880 Training loss 0.11786279082298279 Validation loss 0.11862130463123322 Accuracy 0.6420000195503235\n",
      "Output tensor([[0.5538],\n",
      "        [0.4542]], device='mps:0')\n",
      "Iteration 1890 Training loss 0.1171637624502182 Validation loss 0.11852000653743744 Accuracy 0.6365000009536743\n",
      "Output tensor([[0.5455],\n",
      "        [0.5865]], device='mps:0')\n",
      "Iteration 1900 Training loss 0.11857256293296814 Validation loss 0.11854414641857147 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.4880],\n",
      "        [0.5260]], device='mps:0')\n",
      "Iteration 1910 Training loss 0.11511125415563583 Validation loss 0.11848445236682892 Accuracy 0.6385000348091125\n",
      "Output tensor([[0.4810],\n",
      "        [0.5632]], device='mps:0')\n",
      "Iteration 1920 Training loss 0.11845461279153824 Validation loss 0.11850861459970474 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.5421],\n",
      "        [0.4698]], device='mps:0')\n",
      "Iteration 1930 Training loss 0.11958912760019302 Validation loss 0.11844637244939804 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.5082],\n",
      "        [0.4398]], device='mps:0')\n",
      "Iteration 1940 Training loss 0.11870694905519485 Validation loss 0.11835923045873642 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.4333],\n",
      "        [0.5516]], device='mps:0')\n",
      "Iteration 1950 Training loss 0.11766939610242844 Validation loss 0.11831986159086227 Accuracy 0.6270000338554382\n",
      "Output tensor([[0.5130],\n",
      "        [0.3955]], device='mps:0')\n",
      "Iteration 1960 Training loss 0.11941950768232346 Validation loss 0.1183018907904625 Accuracy 0.6220000386238098\n",
      "Output tensor([[0.5492],\n",
      "        [0.5617]], device='mps:0')\n",
      "Iteration 1970 Training loss 0.11672748625278473 Validation loss 0.11827968060970306 Accuracy 0.6215000152587891\n",
      "Output tensor([[0.5134],\n",
      "        [0.5086]], device='mps:0')\n",
      "Iteration 1980 Training loss 0.11223245412111282 Validation loss 0.11826468259096146 Accuracy 0.6325000524520874\n",
      "Output tensor([[0.4535],\n",
      "        [0.5511]], device='mps:0')\n",
      "Iteration 1990 Training loss 0.11984796077013016 Validation loss 0.11823442578315735 Accuracy 0.6295000314712524\n",
      "Output tensor([[0.4092],\n",
      "        [0.5128]], device='mps:0')\n",
      "Iteration 2000 Training loss 0.11771251261234283 Validation loss 0.11821448057889938 Accuracy 0.6195000410079956\n",
      "Output tensor([[0.5575],\n",
      "        [0.3738]], device='mps:0')\n",
      "Iteration 2010 Training loss 0.11641403287649155 Validation loss 0.11819496750831604 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.4125],\n",
      "        [0.5246]], device='mps:0')\n",
      "Iteration 2020 Training loss 0.12014021724462509 Validation loss 0.11817209422588348 Accuracy 0.6210000514984131\n",
      "Output tensor([[0.5699],\n",
      "        [0.5417]], device='mps:0')\n",
      "Iteration 2030 Training loss 0.11794187128543854 Validation loss 0.11815205961465836 Accuracy 0.6230000257492065\n",
      "Output tensor([[0.4789],\n",
      "        [0.5106]], device='mps:0')\n",
      "Iteration 2040 Training loss 0.11773154139518738 Validation loss 0.1181425154209137 Accuracy 0.6310000419616699\n",
      "Output tensor([[0.5423],\n",
      "        [0.5038]], device='mps:0')\n",
      "Iteration 2050 Training loss 0.1164502501487732 Validation loss 0.11817173659801483 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.5338],\n",
      "        [0.5970]], device='mps:0')\n",
      "Iteration 2060 Training loss 0.11871480941772461 Validation loss 0.11822836101055145 Accuracy 0.643500030040741\n",
      "Output tensor([[0.5226],\n",
      "        [0.4438]], device='mps:0')\n",
      "Iteration 2070 Training loss 0.1212385892868042 Validation loss 0.11810807883739471 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.4795],\n",
      "        [0.4373]], device='mps:0')\n",
      "Iteration 2080 Training loss 0.11909568309783936 Validation loss 0.11812271922826767 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.5619],\n",
      "        [0.5647]], device='mps:0')\n",
      "Iteration 2090 Training loss 0.1220918819308281 Validation loss 0.11808017641305923 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.3683],\n",
      "        [0.5504]], device='mps:0')\n",
      "Iteration 2100 Training loss 0.11507968604564667 Validation loss 0.11804477125406265 Accuracy 0.6360000371932983\n",
      "Output tensor([[0.5186],\n",
      "        [0.5518]], device='mps:0')\n",
      "Iteration 2110 Training loss 0.11475427448749542 Validation loss 0.11800462752580643 Accuracy 0.6285000443458557\n",
      "Output tensor([[0.4964],\n",
      "        [0.4440]], device='mps:0')\n",
      "Iteration 2120 Training loss 0.12107625603675842 Validation loss 0.1179896667599678 Accuracy 0.6320000290870667\n",
      "Output tensor([[0.5521],\n",
      "        [0.4993]], device='mps:0')\n",
      "Iteration 2130 Training loss 0.11904294043779373 Validation loss 0.11796882003545761 Accuracy 0.6320000290870667\n",
      "Output tensor([[0.5597],\n",
      "        [0.5122]], device='mps:0')\n",
      "Iteration 2140 Training loss 0.11907059699296951 Validation loss 0.11798305809497833 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.4409],\n",
      "        [0.5259]], device='mps:0')\n",
      "Iteration 2150 Training loss 0.11501220613718033 Validation loss 0.11794988811016083 Accuracy 0.6360000371932983\n",
      "Output tensor([[0.4101],\n",
      "        [0.4259]], device='mps:0')\n",
      "Iteration 2160 Training loss 0.11867835372686386 Validation loss 0.1179133951663971 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.5167],\n",
      "        [0.5307]], device='mps:0')\n",
      "Iteration 2170 Training loss 0.11775374412536621 Validation loss 0.11791300028562546 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.4609],\n",
      "        [0.5125]], device='mps:0')\n",
      "Iteration 2180 Training loss 0.11799901723861694 Validation loss 0.1178811714053154 Accuracy 0.6355000138282776\n",
      "Output tensor([[0.4310],\n",
      "        [0.4135]], device='mps:0')\n",
      "Iteration 2190 Training loss 0.11424893885850906 Validation loss 0.11788353323936462 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.5757],\n",
      "        [0.4867]], device='mps:0')\n",
      "Iteration 2200 Training loss 0.11992606520652771 Validation loss 0.11782373487949371 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.4699],\n",
      "        [0.4710]], device='mps:0')\n",
      "Iteration 2210 Training loss 0.11665420234203339 Validation loss 0.11781047284603119 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.3476],\n",
      "        [0.5342]], device='mps:0')\n",
      "Iteration 2220 Training loss 0.11533006280660629 Validation loss 0.1177889034152031 Accuracy 0.6325000524520874\n",
      "Output tensor([[0.5104],\n",
      "        [0.4586]], device='mps:0')\n",
      "Iteration 2230 Training loss 0.12101010978221893 Validation loss 0.117787666618824 Accuracy 0.6235000491142273\n",
      "Output tensor([[0.5060],\n",
      "        [0.5866]], device='mps:0')\n",
      "Iteration 2240 Training loss 0.11377477645874023 Validation loss 0.11775413900613785 Accuracy 0.6290000081062317\n",
      "Output tensor([[0.4291],\n",
      "        [0.4947]], device='mps:0')\n",
      "Iteration 2250 Training loss 0.11679800599813461 Validation loss 0.11773061007261276 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.4251],\n",
      "        [0.5205]], device='mps:0')\n",
      "Iteration 2260 Training loss 0.11627385765314102 Validation loss 0.11771316826343536 Accuracy 0.6295000314712524\n",
      "Output tensor([[0.4644],\n",
      "        [0.5349]], device='mps:0')\n",
      "Iteration 2270 Training loss 0.12066080421209335 Validation loss 0.11769548803567886 Accuracy 0.6290000081062317\n",
      "Output tensor([[0.4200],\n",
      "        [0.5562]], device='mps:0')\n",
      "Iteration 2280 Training loss 0.11475994437932968 Validation loss 0.11768736690282822 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.3725],\n",
      "        [0.5227]], device='mps:0')\n",
      "Iteration 2290 Training loss 0.11716762185096741 Validation loss 0.11765426397323608 Accuracy 0.6320000290870667\n",
      "Output tensor([[0.5608],\n",
      "        [0.4731]], device='mps:0')\n",
      "Iteration 2300 Training loss 0.12297022342681885 Validation loss 0.1176404133439064 Accuracy 0.6320000290870667\n",
      "Output tensor([[0.5435],\n",
      "        [0.4468]], device='mps:0')\n",
      "Iteration 2310 Training loss 0.12055343389511108 Validation loss 0.11762552708387375 Accuracy 0.6340000033378601\n",
      "Output tensor([[0.5158],\n",
      "        [0.4805]], device='mps:0')\n",
      "Iteration 2320 Training loss 0.12019843608140945 Validation loss 0.11760446429252625 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.5098],\n",
      "        [0.5019]], device='mps:0')\n",
      "Iteration 2330 Training loss 0.11885892599821091 Validation loss 0.11761413514614105 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.4430],\n",
      "        [0.5929]], device='mps:0')\n",
      "Iteration 2340 Training loss 0.12189046293497086 Validation loss 0.1175835132598877 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.4789],\n",
      "        [0.4011]], device='mps:0')\n",
      "Iteration 2350 Training loss 0.12091018259525299 Validation loss 0.11755552142858505 Accuracy 0.6320000290870667\n",
      "Output tensor([[0.3650],\n",
      "        [0.5696]], device='mps:0')\n",
      "Iteration 2360 Training loss 0.11796997487545013 Validation loss 0.1175745278596878 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.5124],\n",
      "        [0.5222]], device='mps:0')\n",
      "Iteration 2370 Training loss 0.11918556690216064 Validation loss 0.11754020303487778 Accuracy 0.6385000348091125\n",
      "Output tensor([[0.4040],\n",
      "        [0.5027]], device='mps:0')\n",
      "Iteration 2380 Training loss 0.12115279585123062 Validation loss 0.11752855777740479 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.5293],\n",
      "        [0.5246]], device='mps:0')\n",
      "Iteration 2390 Training loss 0.1208987608551979 Validation loss 0.11750376969575882 Accuracy 0.6385000348091125\n",
      "Output tensor([[0.4661],\n",
      "        [0.5613]], device='mps:0')\n",
      "Iteration 2400 Training loss 0.11961447447538376 Validation loss 0.11755306273698807 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.5844],\n",
      "        [0.4784]], device='mps:0')\n",
      "Iteration 2410 Training loss 0.11609560996294022 Validation loss 0.11752933263778687 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.4652],\n",
      "        [0.4691]], device='mps:0')\n",
      "Iteration 2420 Training loss 0.12048469483852386 Validation loss 0.11746614426374435 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.5405],\n",
      "        [0.5485]], device='mps:0')\n",
      "Iteration 2430 Training loss 0.12075776606798172 Validation loss 0.1174309104681015 Accuracy 0.6360000371932983\n",
      "Output tensor([[0.5059],\n",
      "        [0.5189]], device='mps:0')\n",
      "Iteration 2440 Training loss 0.11408861726522446 Validation loss 0.11747252941131592 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.3428],\n",
      "        [0.5165]], device='mps:0')\n",
      "Iteration 2450 Training loss 0.11893758177757263 Validation loss 0.11740976572036743 Accuracy 0.6395000219345093\n",
      "Output tensor([[0.5592],\n",
      "        [0.5299]], device='mps:0')\n",
      "Iteration 2460 Training loss 0.12270331382751465 Validation loss 0.11737814545631409 Accuracy 0.6340000033378601\n",
      "Output tensor([[0.5324],\n",
      "        [0.4471]], device='mps:0')\n",
      "Iteration 2470 Training loss 0.11793830245733261 Validation loss 0.11738135665655136 Accuracy 0.6395000219345093\n",
      "Output tensor([[0.5145],\n",
      "        [0.4110]], device='mps:0')\n",
      "Iteration 2480 Training loss 0.11779378354549408 Validation loss 0.11735185980796814 Accuracy 0.6360000371932983\n",
      "Output tensor([[0.3090],\n",
      "        [0.5583]], device='mps:0')\n",
      "Iteration 2490 Training loss 0.11231762915849686 Validation loss 0.11733125150203705 Accuracy 0.6325000524520874\n",
      "Output tensor([[0.4953],\n",
      "        [0.5531]], device='mps:0')\n",
      "Iteration 2500 Training loss 0.11820386350154877 Validation loss 0.11732178926467896 Accuracy 0.6345000267028809\n",
      "Output tensor([[0.4769],\n",
      "        [0.4686]], device='mps:0')\n",
      "Iteration 2510 Training loss 0.11414974182844162 Validation loss 0.11729791760444641 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.5007],\n",
      "        [0.3569]], device='mps:0')\n",
      "Iteration 2520 Training loss 0.1202068105340004 Validation loss 0.11727561056613922 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.4297],\n",
      "        [0.3715]], device='mps:0')\n",
      "Iteration 2530 Training loss 0.12102989107370377 Validation loss 0.11726769059896469 Accuracy 0.6345000267028809\n",
      "Output tensor([[0.4563],\n",
      "        [0.5480]], device='mps:0')\n",
      "Iteration 2540 Training loss 0.1181865856051445 Validation loss 0.11725826561450958 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.5933],\n",
      "        [0.5256]], device='mps:0')\n",
      "Iteration 2550 Training loss 0.11378282308578491 Validation loss 0.11725330352783203 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.4940],\n",
      "        [0.5889]], device='mps:0')\n",
      "Iteration 2560 Training loss 0.11483070999383926 Validation loss 0.11721186339855194 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.5144],\n",
      "        [0.4689]], device='mps:0')\n",
      "Iteration 2570 Training loss 0.11663009226322174 Validation loss 0.11720623821020126 Accuracy 0.6385000348091125\n",
      "Output tensor([[0.4403],\n",
      "        [0.5289]], device='mps:0')\n",
      "Iteration 2580 Training loss 0.11920630931854248 Validation loss 0.11718127876520157 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.3876],\n",
      "        [0.4950]], device='mps:0')\n",
      "Iteration 2590 Training loss 0.12003852427005768 Validation loss 0.11717166751623154 Accuracy 0.6355000138282776\n",
      "Output tensor([[0.5055],\n",
      "        [0.5080]], device='mps:0')\n",
      "Iteration 2600 Training loss 0.11460468173027039 Validation loss 0.11717098206281662 Accuracy 0.6385000348091125\n",
      "Output tensor([[0.4814],\n",
      "        [0.5463]], device='mps:0')\n",
      "Iteration 2610 Training loss 0.11332881450653076 Validation loss 0.1171361431479454 Accuracy 0.6380000114440918\n",
      "Output tensor([[0.5384],\n",
      "        [0.5179]], device='mps:0')\n",
      "Iteration 2620 Training loss 0.11855481564998627 Validation loss 0.11712542176246643 Accuracy 0.6320000290870667\n",
      "Output tensor([[0.4421],\n",
      "        [0.3588]], device='mps:0')\n",
      "Iteration 2630 Training loss 0.12022299319505692 Validation loss 0.1171022430062294 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.5473],\n",
      "        [0.3994]], device='mps:0')\n",
      "Iteration 2640 Training loss 0.11988076567649841 Validation loss 0.11708807945251465 Accuracy 0.6340000033378601\n",
      "Output tensor([[0.5918],\n",
      "        [0.5473]], device='mps:0')\n",
      "Iteration 2650 Training loss 0.11471015959978104 Validation loss 0.11706355959177017 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.5187],\n",
      "        [0.5801]], device='mps:0')\n",
      "Iteration 2660 Training loss 0.12041833251714706 Validation loss 0.1170751228928566 Accuracy 0.627500057220459\n",
      "Output tensor([[0.5058],\n",
      "        [0.3621]], device='mps:0')\n",
      "Iteration 2670 Training loss 0.11703512817621231 Validation loss 0.11703068763017654 Accuracy 0.6325000524520874\n",
      "Output tensor([[0.5578],\n",
      "        [0.5422]], device='mps:0')\n",
      "Iteration 2680 Training loss 0.11834482103586197 Validation loss 0.1170189380645752 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.4025],\n",
      "        [0.4042]], device='mps:0')\n",
      "Iteration 2690 Training loss 0.12343189120292664 Validation loss 0.11703871190547943 Accuracy 0.64000004529953\n",
      "Output tensor([[0.5401],\n",
      "        [0.5162]], device='mps:0')\n",
      "Iteration 2700 Training loss 0.12334199994802475 Validation loss 0.11708377301692963 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.4969],\n",
      "        [0.5233]], device='mps:0')\n",
      "Iteration 2710 Training loss 0.1202143058180809 Validation loss 0.11717250943183899 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.4728],\n",
      "        [0.4816]], device='mps:0')\n",
      "Iteration 2720 Training loss 0.11861010640859604 Validation loss 0.11709840595722198 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.4685],\n",
      "        [0.5298]], device='mps:0')\n",
      "Iteration 2730 Training loss 0.11944007873535156 Validation loss 0.11716710776090622 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.5749],\n",
      "        [0.5935]], device='mps:0')\n",
      "Iteration 2740 Training loss 0.11061754822731018 Validation loss 0.11712450534105301 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.5496],\n",
      "        [0.6137]], device='mps:0')\n",
      "Iteration 2750 Training loss 0.11086931079626083 Validation loss 0.1169900894165039 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.5258],\n",
      "        [0.4900]], device='mps:0')\n",
      "Iteration 2760 Training loss 0.115165576338768 Validation loss 0.11692724376916885 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.4979],\n",
      "        [0.5687]], device='mps:0')\n",
      "Iteration 2770 Training loss 0.11876603215932846 Validation loss 0.1169082373380661 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.5090],\n",
      "        [0.4730]], device='mps:0')\n",
      "Iteration 2780 Training loss 0.11843082308769226 Validation loss 0.11687326431274414 Accuracy 0.6395000219345093\n",
      "Output tensor([[0.4109],\n",
      "        [0.4280]], device='mps:0')\n",
      "Iteration 2790 Training loss 0.11872968077659607 Validation loss 0.11685444414615631 Accuracy 0.64000004529953\n",
      "Output tensor([[0.3909],\n",
      "        [0.4269]], device='mps:0')\n",
      "Iteration 2800 Training loss 0.11659719049930573 Validation loss 0.11683918535709381 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.5338],\n",
      "        [0.5518]], device='mps:0')\n",
      "Iteration 2810 Training loss 0.12014301866292953 Validation loss 0.11680962145328522 Accuracy 0.6345000267028809\n",
      "Output tensor([[0.4061],\n",
      "        [0.5027]], device='mps:0')\n",
      "Iteration 2820 Training loss 0.11640546470880508 Validation loss 0.11680791527032852 Accuracy 0.6395000219345093\n",
      "Output tensor([[0.5293],\n",
      "        [0.5618]], device='mps:0')\n",
      "Iteration 2830 Training loss 0.12147864699363708 Validation loss 0.11680052429437637 Accuracy 0.64000004529953\n",
      "Output tensor([[0.6032],\n",
      "        [0.5110]], device='mps:0')\n",
      "Iteration 2840 Training loss 0.11475629359483719 Validation loss 0.11681496351957321 Accuracy 0.6420000195503235\n",
      "Output tensor([[0.4906],\n",
      "        [0.4747]], device='mps:0')\n",
      "Iteration 2850 Training loss 0.12084359675645828 Validation loss 0.11678412556648254 Accuracy 0.6395000219345093\n",
      "Output tensor([[0.5166],\n",
      "        [0.5216]], device='mps:0')\n",
      "Iteration 2860 Training loss 0.11919105052947998 Validation loss 0.11679797619581223 Accuracy 0.6420000195503235\n",
      "Output tensor([[0.4127],\n",
      "        [0.4582]], device='mps:0')\n",
      "Iteration 2870 Training loss 0.11384893208742142 Validation loss 0.11677704751491547 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.4619],\n",
      "        [0.5638]], device='mps:0')\n",
      "Iteration 2880 Training loss 0.11656924337148666 Validation loss 0.11673974990844727 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.4824],\n",
      "        [0.4531]], device='mps:0')\n",
      "Iteration 2890 Training loss 0.11528575420379639 Validation loss 0.11673750728368759 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.5028],\n",
      "        [0.4841]], device='mps:0')\n",
      "Iteration 2900 Training loss 0.11578720062971115 Validation loss 0.11671231687068939 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.5235],\n",
      "        [0.4258]], device='mps:0')\n",
      "Iteration 2910 Training loss 0.1208689883351326 Validation loss 0.11666245013475418 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.5527],\n",
      "        [0.6266]], device='mps:0')\n",
      "Iteration 2920 Training loss 0.1151266098022461 Validation loss 0.11664970964193344 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.4925],\n",
      "        [0.4039]], device='mps:0')\n",
      "Iteration 2930 Training loss 0.11988494545221329 Validation loss 0.11664602160453796 Accuracy 0.640500009059906\n",
      "Output tensor([[0.3629],\n",
      "        [0.3689]], device='mps:0')\n",
      "Iteration 2940 Training loss 0.12051408737897873 Validation loss 0.11668742448091507 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.5073],\n",
      "        [0.5725]], device='mps:0')\n",
      "Iteration 2950 Training loss 0.1165633425116539 Validation loss 0.11667422950267792 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.5949],\n",
      "        [0.5074]], device='mps:0')\n",
      "Iteration 2960 Training loss 0.11853551864624023 Validation loss 0.11666419357061386 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5346],\n",
      "        [0.5181]], device='mps:0')\n",
      "Iteration 2970 Training loss 0.1200416162610054 Validation loss 0.11665384471416473 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.4795],\n",
      "        [0.4684]], device='mps:0')\n",
      "Iteration 2980 Training loss 0.11937377601861954 Validation loss 0.11660262197256088 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.5565],\n",
      "        [0.4572]], device='mps:0')\n",
      "Iteration 2990 Training loss 0.1172272264957428 Validation loss 0.11659343540668488 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.5313],\n",
      "        [0.5089]], device='mps:0')\n",
      "Iteration 3000 Training loss 0.11929233372211456 Validation loss 0.1165393590927124 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.5616],\n",
      "        [0.5318]], device='mps:0')\n",
      "Iteration 3010 Training loss 0.11766336113214493 Validation loss 0.11653642356395721 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.4980],\n",
      "        [0.5195]], device='mps:0')\n",
      "Iteration 3020 Training loss 0.1160866916179657 Validation loss 0.11652857065200806 Accuracy 0.6320000290870667\n",
      "Output tensor([[0.5222],\n",
      "        [0.5618]], device='mps:0')\n",
      "Iteration 3030 Training loss 0.1165829747915268 Validation loss 0.11649898439645767 Accuracy 0.6340000033378601\n",
      "Output tensor([[0.3639],\n",
      "        [0.4656]], device='mps:0')\n",
      "Iteration 3040 Training loss 0.1174955815076828 Validation loss 0.11648326367139816 Accuracy 0.6345000267028809\n",
      "Output tensor([[0.4255],\n",
      "        [0.5662]], device='mps:0')\n",
      "Iteration 3050 Training loss 0.11649669706821442 Validation loss 0.11647030711174011 Accuracy 0.6385000348091125\n",
      "Output tensor([[0.4631],\n",
      "        [0.4755]], device='mps:0')\n",
      "Iteration 3060 Training loss 0.12109089642763138 Validation loss 0.11646156758069992 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.4883],\n",
      "        [0.2789]], device='mps:0')\n",
      "Iteration 3070 Training loss 0.12068739533424377 Validation loss 0.11643972992897034 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.5616],\n",
      "        [0.4124]], device='mps:0')\n",
      "Iteration 3080 Training loss 0.11786002665758133 Validation loss 0.11645583063364029 Accuracy 0.6320000290870667\n",
      "Output tensor([[0.4544],\n",
      "        [0.5162]], device='mps:0')\n",
      "Iteration 3090 Training loss 0.11393949389457703 Validation loss 0.1164432093501091 Accuracy 0.6315000057220459\n",
      "Output tensor([[0.4970],\n",
      "        [0.5255]], device='mps:0')\n",
      "Iteration 3100 Training loss 0.12070005387067795 Validation loss 0.11642079800367355 Accuracy 0.6320000290870667\n",
      "Output tensor([[0.4291],\n",
      "        [0.4528]], device='mps:0')\n",
      "Iteration 3110 Training loss 0.1155470460653305 Validation loss 0.11639168113470078 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.5235],\n",
      "        [0.5370]], device='mps:0')\n",
      "Iteration 3120 Training loss 0.1163065955042839 Validation loss 0.11639780551195145 Accuracy 0.640500009059906\n",
      "Output tensor([[0.5443],\n",
      "        [0.5982]], device='mps:0')\n",
      "Iteration 3130 Training loss 0.1156923845410347 Validation loss 0.11637202650308609 Accuracy 0.64000004529953\n",
      "Output tensor([[0.5903],\n",
      "        [0.5511]], device='mps:0')\n",
      "Iteration 3140 Training loss 0.1165982037782669 Validation loss 0.11634926497936249 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.4395],\n",
      "        [0.5758]], device='mps:0')\n",
      "Iteration 3150 Training loss 0.11410223692655563 Validation loss 0.11633911728858948 Accuracy 0.64000004529953\n",
      "Output tensor([[0.5962],\n",
      "        [0.4696]], device='mps:0')\n",
      "Iteration 3160 Training loss 0.11934468895196915 Validation loss 0.11632862687110901 Accuracy 0.64000004529953\n",
      "Output tensor([[0.5778],\n",
      "        [0.5001]], device='mps:0')\n",
      "Iteration 3170 Training loss 0.12127496302127838 Validation loss 0.11630561947822571 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.5148],\n",
      "        [0.5600]], device='mps:0')\n",
      "Iteration 3180 Training loss 0.12132062762975693 Validation loss 0.1163107231259346 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.5028],\n",
      "        [0.2903]], device='mps:0')\n",
      "Iteration 3190 Training loss 0.11873174458742142 Validation loss 0.11628775298595428 Accuracy 0.6385000348091125\n",
      "Output tensor([[0.4182],\n",
      "        [0.4955]], device='mps:0')\n",
      "Iteration 3200 Training loss 0.1140594556927681 Validation loss 0.11627476662397385 Accuracy 0.6350000500679016\n",
      "Output tensor([[0.3733],\n",
      "        [0.4953]], device='mps:0')\n",
      "Iteration 3210 Training loss 0.11809514462947845 Validation loss 0.11625728756189346 Accuracy 0.640500009059906\n",
      "Output tensor([[0.5394],\n",
      "        [0.5486]], device='mps:0')\n",
      "Iteration 3220 Training loss 0.115989089012146 Validation loss 0.11624440550804138 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.2860],\n",
      "        [0.5395]], device='mps:0')\n",
      "Iteration 3230 Training loss 0.11224410682916641 Validation loss 0.11623724550008774 Accuracy 0.6345000267028809\n",
      "Output tensor([[0.5080],\n",
      "        [0.5108]], device='mps:0')\n",
      "Iteration 3240 Training loss 0.11345724016427994 Validation loss 0.1162312850356102 Accuracy 0.6345000267028809\n",
      "Output tensor([[0.5001],\n",
      "        [0.4762]], device='mps:0')\n",
      "Iteration 3250 Training loss 0.11843647062778473 Validation loss 0.1162254810333252 Accuracy 0.6330000162124634\n",
      "Output tensor([[0.3502],\n",
      "        [0.4682]], device='mps:0')\n",
      "Iteration 3260 Training loss 0.11650509387254715 Validation loss 0.11620572209358215 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.5508],\n",
      "        [0.5142]], device='mps:0')\n",
      "Iteration 3270 Training loss 0.11473707854747772 Validation loss 0.11618465930223465 Accuracy 0.640500009059906\n",
      "Output tensor([[0.5543],\n",
      "        [0.4305]], device='mps:0')\n",
      "Iteration 3280 Training loss 0.1197163537144661 Validation loss 0.11617626249790192 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.5219],\n",
      "        [0.3675]], device='mps:0')\n",
      "Iteration 3290 Training loss 0.11436425149440765 Validation loss 0.11616866290569305 Accuracy 0.6375000476837158\n",
      "Output tensor([[0.3541],\n",
      "        [0.3565]], device='mps:0')\n",
      "Iteration 3300 Training loss 0.11957302689552307 Validation loss 0.11615543812513351 Accuracy 0.6395000219345093\n",
      "Output tensor([[0.4812],\n",
      "        [0.5741]], device='mps:0')\n",
      "Iteration 3310 Training loss 0.11252740770578384 Validation loss 0.11614038795232773 Accuracy 0.6370000243186951\n",
      "Output tensor([[0.3925],\n",
      "        [0.5318]], device='mps:0')\n",
      "Iteration 3320 Training loss 0.11122629791498184 Validation loss 0.11613645404577255 Accuracy 0.640500009059906\n",
      "Output tensor([[0.5076],\n",
      "        [0.4755]], device='mps:0')\n",
      "Iteration 3330 Training loss 0.11644086241722107 Validation loss 0.11611383408308029 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.5087],\n",
      "        [0.5180]], device='mps:0')\n",
      "Iteration 3340 Training loss 0.1216304749250412 Validation loss 0.11609287559986115 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.4974],\n",
      "        [0.3791]], device='mps:0')\n",
      "Iteration 3350 Training loss 0.12001927942037582 Validation loss 0.11608031392097473 Accuracy 0.6390000581741333\n",
      "Output tensor([[0.4960],\n",
      "        [0.5382]], device='mps:0')\n",
      "Iteration 3360 Training loss 0.11192415654659271 Validation loss 0.11608275026082993 Accuracy 0.6345000267028809\n",
      "Output tensor([[0.5201],\n",
      "        [0.4056]], device='mps:0')\n",
      "Iteration 3370 Training loss 0.11625352501869202 Validation loss 0.11608387529850006 Accuracy 0.6350000500679016\n",
      "Output tensor([[0.4784],\n",
      "        [0.5523]], device='mps:0')\n",
      "Iteration 3380 Training loss 0.11767697334289551 Validation loss 0.11604565382003784 Accuracy 0.6360000371932983\n",
      "Output tensor([[0.4564],\n",
      "        [0.4749]], device='mps:0')\n",
      "Iteration 3390 Training loss 0.11273003369569778 Validation loss 0.11603907495737076 Accuracy 0.6335000395774841\n",
      "Output tensor([[0.5248],\n",
      "        [0.6109]], device='mps:0')\n",
      "Iteration 3400 Training loss 0.12010201066732407 Validation loss 0.11601869016885757 Accuracy 0.640500009059906\n",
      "Output tensor([[0.3811],\n",
      "        [0.5513]], device='mps:0')\n",
      "Iteration 3410 Training loss 0.12049583345651627 Validation loss 0.11600342392921448 Accuracy 0.640500009059906\n",
      "Output tensor([[0.5676],\n",
      "        [0.4752]], device='mps:0')\n",
      "Iteration 3420 Training loss 0.12205778062343597 Validation loss 0.11600466072559357 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.6000],\n",
      "        [0.3907]], device='mps:0')\n",
      "Iteration 3430 Training loss 0.1175789162516594 Validation loss 0.11597222089767456 Accuracy 0.64000004529953\n",
      "Output tensor([[0.5770],\n",
      "        [0.5414]], device='mps:0')\n",
      "Iteration 3440 Training loss 0.11505336314439774 Validation loss 0.11596651375293732 Accuracy 0.6420000195503235\n",
      "Output tensor([[0.3429],\n",
      "        [0.3522]], device='mps:0')\n",
      "Iteration 3450 Training loss 0.11296027153730392 Validation loss 0.11596501618623734 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.3729],\n",
      "        [0.6200]], device='mps:0')\n",
      "Iteration 3460 Training loss 0.12048685550689697 Validation loss 0.11594092845916748 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.5727],\n",
      "        [0.5709]], device='mps:0')\n",
      "Iteration 3470 Training loss 0.11565133929252625 Validation loss 0.11592569202184677 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.5420],\n",
      "        [0.3827]], device='mps:0')\n",
      "Iteration 3480 Training loss 0.11307147145271301 Validation loss 0.11592410504817963 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.5186],\n",
      "        [0.4072]], device='mps:0')\n",
      "Iteration 3490 Training loss 0.11631545424461365 Validation loss 0.11590605974197388 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.5901],\n",
      "        [0.5055]], device='mps:0')\n",
      "Iteration 3500 Training loss 0.12012221664190292 Validation loss 0.1159411370754242 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.5713],\n",
      "        [0.3351]], device='mps:0')\n",
      "Iteration 3510 Training loss 0.11478857696056366 Validation loss 0.11588691920042038 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.5448],\n",
      "        [0.5157]], device='mps:0')\n",
      "Iteration 3520 Training loss 0.10896977037191391 Validation loss 0.11589621752500534 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.3879],\n",
      "        [0.3370]], device='mps:0')\n",
      "Iteration 3530 Training loss 0.11812448501586914 Validation loss 0.11589489877223969 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.5533],\n",
      "        [0.4800]], device='mps:0')\n",
      "Iteration 3540 Training loss 0.11023618280887604 Validation loss 0.1158563494682312 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.5361],\n",
      "        [0.5660]], device='mps:0')\n",
      "Iteration 3550 Training loss 0.11447376757860184 Validation loss 0.11589177697896957 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.6130],\n",
      "        [0.4800]], device='mps:0')\n",
      "Iteration 3560 Training loss 0.1221025213599205 Validation loss 0.1159789189696312 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.5699],\n",
      "        [0.4943]], device='mps:0')\n",
      "Iteration 3570 Training loss 0.1146472692489624 Validation loss 0.11589749902486801 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.2864],\n",
      "        [0.5121]], device='mps:0')\n",
      "Iteration 3580 Training loss 0.12010925263166428 Validation loss 0.11586522310972214 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.5236],\n",
      "        [0.5681]], device='mps:0')\n",
      "Iteration 3590 Training loss 0.1185230240225792 Validation loss 0.1158052608370781 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.5654],\n",
      "        [0.3449]], device='mps:0')\n",
      "Iteration 3600 Training loss 0.11220813542604446 Validation loss 0.11579668521881104 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.5448],\n",
      "        [0.3834]], device='mps:0')\n",
      "Iteration 3610 Training loss 0.11725708097219467 Validation loss 0.11582697927951813 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.4858],\n",
      "        [0.5363]], device='mps:0')\n",
      "Iteration 3620 Training loss 0.11210502684116364 Validation loss 0.11579672992229462 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.5080],\n",
      "        [0.4193]], device='mps:0')\n",
      "Iteration 3630 Training loss 0.12429340928792953 Validation loss 0.11574564129114151 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.6142],\n",
      "        [0.6113]], device='mps:0')\n",
      "Iteration 3640 Training loss 0.11654048413038254 Validation loss 0.11575154960155487 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5332],\n",
      "        [0.5786]], device='mps:0')\n",
      "Iteration 3650 Training loss 0.11406821757555008 Validation loss 0.11572816222906113 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.4400],\n",
      "        [0.5224]], device='mps:0')\n",
      "Iteration 3660 Training loss 0.11148711293935776 Validation loss 0.11571025103330612 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.4223],\n",
      "        [0.6101]], device='mps:0')\n",
      "Iteration 3670 Training loss 0.11198357492685318 Validation loss 0.11571550369262695 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.3250],\n",
      "        [0.5156]], device='mps:0')\n",
      "Iteration 3680 Training loss 0.1116548627614975 Validation loss 0.11566566675901413 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5347],\n",
      "        [0.4809]], device='mps:0')\n",
      "Iteration 3690 Training loss 0.1168835386633873 Validation loss 0.1156543642282486 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.3771],\n",
      "        [0.4539]], device='mps:0')\n",
      "Iteration 3700 Training loss 0.12091300636529922 Validation loss 0.11563891917467117 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.4523],\n",
      "        [0.3847]], device='mps:0')\n",
      "Iteration 3710 Training loss 0.11474184691905975 Validation loss 0.11562332510948181 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.4823],\n",
      "        [0.5358]], device='mps:0')\n",
      "Iteration 3720 Training loss 0.11344924569129944 Validation loss 0.11561226844787598 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.3708],\n",
      "        [0.5781]], device='mps:0')\n",
      "Iteration 3730 Training loss 0.1156625896692276 Validation loss 0.11559339612722397 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.4446],\n",
      "        [0.5494]], device='mps:0')\n",
      "Iteration 3740 Training loss 0.11394760012626648 Validation loss 0.11559126526117325 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.5666],\n",
      "        [0.4677]], device='mps:0')\n",
      "Iteration 3750 Training loss 0.11958006769418716 Validation loss 0.11565136909484863 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.5863],\n",
      "        [0.5199]], device='mps:0')\n",
      "Iteration 3760 Training loss 0.11273126304149628 Validation loss 0.11564092338085175 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.3888],\n",
      "        [0.5497]], device='mps:0')\n",
      "Iteration 3770 Training loss 0.11400886625051498 Validation loss 0.11561518162488937 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.4490],\n",
      "        [0.4823]], device='mps:0')\n",
      "Iteration 3780 Training loss 0.11901267617940903 Validation loss 0.11558102816343307 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.4984],\n",
      "        [0.5311]], device='mps:0')\n",
      "Iteration 3790 Training loss 0.11506162583827972 Validation loss 0.11557764559984207 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5798],\n",
      "        [0.5768]], device='mps:0')\n",
      "Iteration 3800 Training loss 0.10526037216186523 Validation loss 0.11558233946561813 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.3302],\n",
      "        [0.5179]], device='mps:0')\n",
      "Iteration 3810 Training loss 0.1202162653207779 Validation loss 0.1155552938580513 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.4970],\n",
      "        [0.5681]], device='mps:0')\n",
      "Iteration 3820 Training loss 0.11041383445262909 Validation loss 0.11551253497600555 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.5778],\n",
      "        [0.4533]], device='mps:0')\n",
      "Iteration 3830 Training loss 0.12107455730438232 Validation loss 0.11547492444515228 Accuracy 0.6420000195503235\n",
      "Output tensor([[0.5249],\n",
      "        [0.5445]], device='mps:0')\n",
      "Iteration 3840 Training loss 0.11772944033145905 Validation loss 0.11550135165452957 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.5177],\n",
      "        [0.5847]], device='mps:0')\n",
      "Iteration 3850 Training loss 0.11488377302885056 Validation loss 0.1154995933175087 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5212],\n",
      "        [0.5264]], device='mps:0')\n",
      "Iteration 3860 Training loss 0.11177706718444824 Validation loss 0.11544575542211533 Accuracy 0.6415000557899475\n",
      "Output tensor([[0.5671],\n",
      "        [0.4223]], device='mps:0')\n",
      "Iteration 3870 Training loss 0.11240331083536148 Validation loss 0.11543407291173935 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.4956],\n",
      "        [0.5130]], device='mps:0')\n",
      "Iteration 3880 Training loss 0.11731939017772675 Validation loss 0.11543925851583481 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.5102],\n",
      "        [0.4490]], device='mps:0')\n",
      "Iteration 3890 Training loss 0.12232705950737 Validation loss 0.11542855203151703 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.5189],\n",
      "        [0.5300]], device='mps:0')\n",
      "Iteration 3900 Training loss 0.10447830706834793 Validation loss 0.11540671437978745 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.4084],\n",
      "        [0.5666]], device='mps:0')\n",
      "Iteration 3910 Training loss 0.11764991283416748 Validation loss 0.1154002919793129 Accuracy 0.6425000429153442\n",
      "Output tensor([[0.4565],\n",
      "        [0.6606]], device='mps:0')\n",
      "Iteration 3920 Training loss 0.11019209027290344 Validation loss 0.11539078503847122 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.3762],\n",
      "        [0.3872]], device='mps:0')\n",
      "Iteration 3930 Training loss 0.1174813061952591 Validation loss 0.1154116541147232 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.3473],\n",
      "        [0.5904]], device='mps:0')\n",
      "Iteration 3940 Training loss 0.11409226804971695 Validation loss 0.11534984409809113 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.5225],\n",
      "        [0.5361]], device='mps:0')\n",
      "Iteration 3950 Training loss 0.11697995662689209 Validation loss 0.11534372717142105 Accuracy 0.6410000324249268\n",
      "Output tensor([[0.5282],\n",
      "        [0.5568]], device='mps:0')\n",
      "Iteration 3960 Training loss 0.11931003630161285 Validation loss 0.11536417156457901 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.4892],\n",
      "        [0.5513]], device='mps:0')\n",
      "Iteration 3970 Training loss 0.11698787659406662 Validation loss 0.1153852641582489 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.4961],\n",
      "        [0.5431]], device='mps:0')\n",
      "Iteration 3980 Training loss 0.11455676704645157 Validation loss 0.11533283442258835 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.5240],\n",
      "        [0.6118]], device='mps:0')\n",
      "Iteration 3990 Training loss 0.11913637816905975 Validation loss 0.11529313772916794 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.2431],\n",
      "        [0.5854]], device='mps:0')\n",
      "Iteration 4000 Training loss 0.11755532026290894 Validation loss 0.11527442932128906 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.4949],\n",
      "        [0.5825]], device='mps:0')\n",
      "Iteration 4010 Training loss 0.11690681427717209 Validation loss 0.11527132242918015 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.5588],\n",
      "        [0.4973]], device='mps:0')\n",
      "Iteration 4020 Training loss 0.10917671769857407 Validation loss 0.11527366936206818 Accuracy 0.643500030040741\n",
      "Output tensor([[0.4867],\n",
      "        [0.6140]], device='mps:0')\n",
      "Iteration 4030 Training loss 0.11209975928068161 Validation loss 0.11524046212434769 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.3822],\n",
      "        [0.4219]], device='mps:0')\n",
      "Iteration 4040 Training loss 0.114142045378685 Validation loss 0.11527884006500244 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.4987],\n",
      "        [0.4695]], device='mps:0')\n",
      "Iteration 4050 Training loss 0.11669111996889114 Validation loss 0.11536673456430435 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.5751],\n",
      "        [0.5351]], device='mps:0')\n",
      "Iteration 4060 Training loss 0.11478693038225174 Validation loss 0.11532715708017349 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5298],\n",
      "        [0.5720]], device='mps:0')\n",
      "Iteration 4070 Training loss 0.11984719336032867 Validation loss 0.1152690201997757 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5631],\n",
      "        [0.5792]], device='mps:0')\n",
      "Iteration 4080 Training loss 0.11292573064565659 Validation loss 0.1152137890458107 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.5877],\n",
      "        [0.5832]], device='mps:0')\n",
      "Iteration 4090 Training loss 0.11764075607061386 Validation loss 0.11518173664808273 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.6032],\n",
      "        [0.5337]], device='mps:0')\n",
      "Iteration 4100 Training loss 0.11270514875650406 Validation loss 0.11516914516687393 Accuracy 0.6420000195503235\n",
      "Output tensor([[0.5292],\n",
      "        [0.5744]], device='mps:0')\n",
      "Iteration 4110 Training loss 0.12060493230819702 Validation loss 0.11515024304389954 Accuracy 0.6420000195503235\n",
      "Output tensor([[0.3222],\n",
      "        [0.5956]], device='mps:0')\n",
      "Iteration 4120 Training loss 0.11873239278793335 Validation loss 0.11516987532377243 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.3175],\n",
      "        [0.5134]], device='mps:0')\n",
      "Iteration 4130 Training loss 0.11509517580270767 Validation loss 0.11514849215745926 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.3731],\n",
      "        [0.3533]], device='mps:0')\n",
      "Iteration 4140 Training loss 0.11855769902467728 Validation loss 0.1151200607419014 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.5958],\n",
      "        [0.4708]], device='mps:0')\n",
      "Iteration 4150 Training loss 0.11462012678384781 Validation loss 0.11516432464122772 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.4691],\n",
      "        [0.4108]], device='mps:0')\n",
      "Iteration 4160 Training loss 0.11655790358781815 Validation loss 0.11520282924175262 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5696],\n",
      "        [0.2747]], device='mps:0')\n",
      "Iteration 4170 Training loss 0.11450374126434326 Validation loss 0.11516217887401581 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.6377],\n",
      "        [0.3730]], device='mps:0')\n",
      "Iteration 4180 Training loss 0.11403688043355942 Validation loss 0.11515047401189804 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.4934],\n",
      "        [0.4983]], device='mps:0')\n",
      "Iteration 4190 Training loss 0.11543919891119003 Validation loss 0.11518123000860214 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.6185],\n",
      "        [0.5180]], device='mps:0')\n",
      "Iteration 4200 Training loss 0.11316201835870743 Validation loss 0.11508038640022278 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.2691],\n",
      "        [0.4915]], device='mps:0')\n",
      "Iteration 4210 Training loss 0.11017464101314545 Validation loss 0.11507534235715866 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.3838],\n",
      "        [0.5201]], device='mps:0')\n",
      "Iteration 4220 Training loss 0.11521448194980621 Validation loss 0.11505407094955444 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.5886],\n",
      "        [0.4372]], device='mps:0')\n",
      "Iteration 4230 Training loss 0.11585784703493118 Validation loss 0.11504143476486206 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.4685],\n",
      "        [0.4780]], device='mps:0')\n",
      "Iteration 4240 Training loss 0.10961945354938507 Validation loss 0.11507514864206314 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.3467],\n",
      "        [0.5484]], device='mps:0')\n",
      "Iteration 4250 Training loss 0.10943141579627991 Validation loss 0.11503932625055313 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.5809],\n",
      "        [0.5566]], device='mps:0')\n",
      "Iteration 4260 Training loss 0.12643316388130188 Validation loss 0.11500070989131927 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.5342],\n",
      "        [0.5206]], device='mps:0')\n",
      "Iteration 4270 Training loss 0.11180507391691208 Validation loss 0.11500761657953262 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.4309],\n",
      "        [0.4838]], device='mps:0')\n",
      "Iteration 4280 Training loss 0.1068868413567543 Validation loss 0.11504159867763519 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.4971],\n",
      "        [0.4186]], device='mps:0')\n",
      "Iteration 4290 Training loss 0.12230942398309708 Validation loss 0.115035280585289 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5319],\n",
      "        [0.5693]], device='mps:0')\n",
      "Iteration 4300 Training loss 0.11319144070148468 Validation loss 0.11497163772583008 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.6010],\n",
      "        [0.5768]], device='mps:0')\n",
      "Iteration 4310 Training loss 0.11572059988975525 Validation loss 0.11495580524206161 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.4707],\n",
      "        [0.4932]], device='mps:0')\n",
      "Iteration 4320 Training loss 0.11583250015974045 Validation loss 0.11495719850063324 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.2953],\n",
      "        [0.5460]], device='mps:0')\n",
      "Iteration 4330 Training loss 0.11582330614328384 Validation loss 0.11491523683071136 Accuracy 0.643500030040741\n",
      "Output tensor([[0.5375],\n",
      "        [0.4677]], device='mps:0')\n",
      "Iteration 4340 Training loss 0.11031219363212585 Validation loss 0.11490155756473541 Accuracy 0.640500009059906\n",
      "Output tensor([[0.6216],\n",
      "        [0.5255]], device='mps:0')\n",
      "Iteration 4350 Training loss 0.12192804366350174 Validation loss 0.11487963050603867 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.3544],\n",
      "        [0.5565]], device='mps:0')\n",
      "Iteration 4360 Training loss 0.12035323679447174 Validation loss 0.11486969888210297 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.4525],\n",
      "        [0.5215]], device='mps:0')\n",
      "Iteration 4370 Training loss 0.11773215234279633 Validation loss 0.11486006528139114 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.4547],\n",
      "        [0.4967]], device='mps:0')\n",
      "Iteration 4380 Training loss 0.11716067790985107 Validation loss 0.11486723273992538 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.3881],\n",
      "        [0.5350]], device='mps:0')\n",
      "Iteration 4390 Training loss 0.115526482462883 Validation loss 0.11484164744615555 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.4998],\n",
      "        [0.4275]], device='mps:0')\n",
      "Iteration 4400 Training loss 0.11586131900548935 Validation loss 0.1148369163274765 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.4531],\n",
      "        [0.5039]], device='mps:0')\n",
      "Iteration 4410 Training loss 0.12157775461673737 Validation loss 0.11483034491539001 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.4490],\n",
      "        [0.3715]], device='mps:0')\n",
      "Iteration 4420 Training loss 0.11006400734186172 Validation loss 0.11486046016216278 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.3885],\n",
      "        [0.5181]], device='mps:0')\n",
      "Iteration 4430 Training loss 0.11025126278400421 Validation loss 0.11482331156730652 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5406],\n",
      "        [0.3155]], device='mps:0')\n",
      "Iteration 4440 Training loss 0.12319549173116684 Validation loss 0.1147875040769577 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5592],\n",
      "        [0.2798]], device='mps:0')\n",
      "Iteration 4450 Training loss 0.1136249452829361 Validation loss 0.11477784067392349 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.5332],\n",
      "        [0.3966]], device='mps:0')\n",
      "Iteration 4460 Training loss 0.1157255545258522 Validation loss 0.11477915942668915 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.5272],\n",
      "        [0.5099]], device='mps:0')\n",
      "Iteration 4470 Training loss 0.1218915805220604 Validation loss 0.11476694047451019 Accuracy 0.643500030040741\n",
      "Output tensor([[0.4391],\n",
      "        [0.3886]], device='mps:0')\n",
      "Iteration 4480 Training loss 0.1145332083106041 Validation loss 0.1148063912987709 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.4619],\n",
      "        [0.4720]], device='mps:0')\n",
      "Iteration 4490 Training loss 0.11907919496297836 Validation loss 0.11474505811929703 Accuracy 0.643500030040741\n",
      "Output tensor([[0.5257],\n",
      "        [0.5487]], device='mps:0')\n",
      "Iteration 4500 Training loss 0.11334852129220963 Validation loss 0.11473117768764496 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.5885],\n",
      "        [0.5649]], device='mps:0')\n",
      "Iteration 4510 Training loss 0.11326879262924194 Validation loss 0.11471335589885712 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.5072],\n",
      "        [0.4442]], device='mps:0')\n",
      "Iteration 4520 Training loss 0.11151556670665741 Validation loss 0.11471063643693924 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.6037],\n",
      "        [0.5357]], device='mps:0')\n",
      "Iteration 4530 Training loss 0.11815176904201508 Validation loss 0.11469652503728867 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.4071],\n",
      "        [0.5915]], device='mps:0')\n",
      "Iteration 4540 Training loss 0.11549137532711029 Validation loss 0.11468007415533066 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.4900],\n",
      "        [0.3546]], device='mps:0')\n",
      "Iteration 4550 Training loss 0.11493490636348724 Validation loss 0.1146782711148262 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5428],\n",
      "        [0.5102]], device='mps:0')\n",
      "Iteration 4560 Training loss 0.10832429677248001 Validation loss 0.11466240137815475 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.5236],\n",
      "        [0.5305]], device='mps:0')\n",
      "Iteration 4570 Training loss 0.11963313817977905 Validation loss 0.11465117335319519 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5857],\n",
      "        [0.5651]], device='mps:0')\n",
      "Iteration 4580 Training loss 0.1197141706943512 Validation loss 0.11464150249958038 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.5306],\n",
      "        [0.5753]], device='mps:0')\n",
      "Iteration 4590 Training loss 0.12134279310703278 Validation loss 0.1146339699625969 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.3827],\n",
      "        [0.4042]], device='mps:0')\n",
      "Iteration 4600 Training loss 0.11870606988668442 Validation loss 0.11466274410486221 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.3171],\n",
      "        [0.5737]], device='mps:0')\n",
      "Iteration 4610 Training loss 0.11177141219377518 Validation loss 0.11468365788459778 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.4236],\n",
      "        [0.5935]], device='mps:0')\n",
      "Iteration 4620 Training loss 0.11435242742300034 Validation loss 0.11463838815689087 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5560],\n",
      "        [0.6073]], device='mps:0')\n",
      "Iteration 4630 Training loss 0.11659958213567734 Validation loss 0.11465828120708466 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.6204],\n",
      "        [0.4417]], device='mps:0')\n",
      "Iteration 4640 Training loss 0.1151285171508789 Validation loss 0.11464591324329376 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.4813],\n",
      "        [0.4036]], device='mps:0')\n",
      "Iteration 4650 Training loss 0.10917191952466965 Validation loss 0.11461710929870605 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.4077],\n",
      "        [0.5638]], device='mps:0')\n",
      "Iteration 4660 Training loss 0.11607222259044647 Validation loss 0.11463503539562225 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.4923],\n",
      "        [0.5724]], device='mps:0')\n",
      "Iteration 4670 Training loss 0.1188572496175766 Validation loss 0.11459760367870331 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5541],\n",
      "        [0.4780]], device='mps:0')\n",
      "Iteration 4680 Training loss 0.11590972542762756 Validation loss 0.11456732451915741 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.4485],\n",
      "        [0.4990]], device='mps:0')\n",
      "Iteration 4690 Training loss 0.12181055545806885 Validation loss 0.11454956233501434 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.4274],\n",
      "        [0.5240]], device='mps:0')\n",
      "Iteration 4700 Training loss 0.11499416083097458 Validation loss 0.11453201621770859 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.5314],\n",
      "        [0.4931]], device='mps:0')\n",
      "Iteration 4710 Training loss 0.11722396314144135 Validation loss 0.11453891545534134 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5065],\n",
      "        [0.6010]], device='mps:0')\n",
      "Iteration 4720 Training loss 0.11433535069227219 Validation loss 0.11451458930969238 Accuracy 0.6460000276565552\n",
      "Output tensor([[0.5620],\n",
      "        [0.4625]], device='mps:0')\n",
      "Iteration 4730 Training loss 0.11706387996673584 Validation loss 0.1145201176404953 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.4610],\n",
      "        [0.3898]], device='mps:0')\n",
      "Iteration 4740 Training loss 0.11375049501657486 Validation loss 0.1145138293504715 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5919],\n",
      "        [0.5452]], device='mps:0')\n",
      "Iteration 4750 Training loss 0.11604127287864685 Validation loss 0.11448664218187332 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.5545],\n",
      "        [0.5638]], device='mps:0')\n",
      "Iteration 4760 Training loss 0.11889618635177612 Validation loss 0.1145150437951088 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.6066],\n",
      "        [0.5732]], device='mps:0')\n",
      "Iteration 4770 Training loss 0.12438036501407623 Validation loss 0.11455795913934708 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.4981],\n",
      "        [0.3083]], device='mps:0')\n",
      "Iteration 4780 Training loss 0.11375904083251953 Validation loss 0.11455806344747543 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.6778],\n",
      "        [0.4612]], device='mps:0')\n",
      "Iteration 4790 Training loss 0.11647576093673706 Validation loss 0.11455873399972916 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5493],\n",
      "        [0.6443]], device='mps:0')\n",
      "Iteration 4800 Training loss 0.11334172636270523 Validation loss 0.11458955705165863 Accuracy 0.659000039100647\n",
      "Output tensor([[0.5690],\n",
      "        [0.5466]], device='mps:0')\n",
      "Iteration 4810 Training loss 0.11990513652563095 Validation loss 0.11450909823179245 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5770],\n",
      "        [0.3602]], device='mps:0')\n",
      "Iteration 4820 Training loss 0.11405544728040695 Validation loss 0.11445508897304535 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5818],\n",
      "        [0.5189]], device='mps:0')\n",
      "Iteration 4830 Training loss 0.1199386790394783 Validation loss 0.11447212845087051 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5948],\n",
      "        [0.6122]], device='mps:0')\n",
      "Iteration 4840 Training loss 0.11865928024053574 Validation loss 0.11442460864782333 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.2385],\n",
      "        [0.5174]], device='mps:0')\n",
      "Iteration 4850 Training loss 0.11692481487989426 Validation loss 0.11449487507343292 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.5493],\n",
      "        [0.4508]], device='mps:0')\n",
      "Iteration 4860 Training loss 0.11825583875179291 Validation loss 0.11442461609840393 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.6228],\n",
      "        [0.4220]], device='mps:0')\n",
      "Iteration 4870 Training loss 0.11235007643699646 Validation loss 0.11444061994552612 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.3512],\n",
      "        [0.5858]], device='mps:0')\n",
      "Iteration 4880 Training loss 0.11793027818202972 Validation loss 0.1144372746348381 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5992],\n",
      "        [0.3861]], device='mps:0')\n",
      "Iteration 4890 Training loss 0.1161227747797966 Validation loss 0.11446243524551392 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.5698],\n",
      "        [0.5077]], device='mps:0')\n",
      "Iteration 4900 Training loss 0.11434982717037201 Validation loss 0.11437741667032242 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5059],\n",
      "        [0.3393]], device='mps:0')\n",
      "Iteration 4910 Training loss 0.11402835696935654 Validation loss 0.11436895281076431 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.4994],\n",
      "        [0.4472]], device='mps:0')\n",
      "Iteration 4920 Training loss 0.11172554641962051 Validation loss 0.11433926969766617 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.4844],\n",
      "        [0.3756]], device='mps:0')\n",
      "Iteration 4930 Training loss 0.11374201625585556 Validation loss 0.11431189626455307 Accuracy 0.6490000486373901\n",
      "Output tensor([[0.5586],\n",
      "        [0.5316]], device='mps:0')\n",
      "Iteration 4940 Training loss 0.11948985606431961 Validation loss 0.11434189230203629 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5097],\n",
      "        [0.5729]], device='mps:0')\n",
      "Iteration 4950 Training loss 0.11568551510572433 Validation loss 0.11432495713233948 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.3836],\n",
      "        [0.4488]], device='mps:0')\n",
      "Iteration 4960 Training loss 0.10832549631595612 Validation loss 0.11429884284734726 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5407],\n",
      "        [0.4041]], device='mps:0')\n",
      "Iteration 4970 Training loss 0.11404185742139816 Validation loss 0.1142871305346489 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.4648],\n",
      "        [0.5978]], device='mps:0')\n",
      "Iteration 4980 Training loss 0.11574644595384598 Validation loss 0.11425342410802841 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.4336],\n",
      "        [0.4173]], device='mps:0')\n",
      "Iteration 4990 Training loss 0.12300634384155273 Validation loss 0.11424531787633896 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.2727],\n",
      "        [0.5127]], device='mps:0')\n",
      "Iteration 5000 Training loss 0.11250809580087662 Validation loss 0.11428862810134888 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5845],\n",
      "        [0.5432]], device='mps:0')\n",
      "Iteration 5010 Training loss 0.10418260097503662 Validation loss 0.11426814645528793 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5062],\n",
      "        [0.5272]], device='mps:0')\n",
      "Iteration 5020 Training loss 0.11075007915496826 Validation loss 0.11421988159418106 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5105],\n",
      "        [0.5581]], device='mps:0')\n",
      "Iteration 5030 Training loss 0.1178489550948143 Validation loss 0.11421430110931396 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.4988],\n",
      "        [0.6194]], device='mps:0')\n",
      "Iteration 5040 Training loss 0.11456188559532166 Validation loss 0.11418918520212173 Accuracy 0.643500030040741\n",
      "Output tensor([[0.5968],\n",
      "        [0.3970]], device='mps:0')\n",
      "Iteration 5050 Training loss 0.11653200536966324 Validation loss 0.11418236047029495 Accuracy 0.6430000066757202\n",
      "Output tensor([[0.3127],\n",
      "        [0.5401]], device='mps:0')\n",
      "Iteration 5060 Training loss 0.10453075170516968 Validation loss 0.1141742616891861 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.4865],\n",
      "        [0.2920]], device='mps:0')\n",
      "Iteration 5070 Training loss 0.11715111136436462 Validation loss 0.11416827887296677 Accuracy 0.643500030040741\n",
      "Output tensor([[0.5896],\n",
      "        [0.6295]], device='mps:0')\n",
      "Iteration 5080 Training loss 0.11586490273475647 Validation loss 0.11415372043848038 Accuracy 0.6450000405311584\n",
      "Output tensor([[0.6031],\n",
      "        [0.5077]], device='mps:0')\n",
      "Iteration 5090 Training loss 0.1209956705570221 Validation loss 0.11415645480155945 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.5783],\n",
      "        [0.4724]], device='mps:0')\n",
      "Iteration 5100 Training loss 0.11633171141147614 Validation loss 0.11413448303937912 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.3743],\n",
      "        [0.4782]], device='mps:0')\n",
      "Iteration 5110 Training loss 0.10761892050504684 Validation loss 0.11412914097309113 Accuracy 0.6440000534057617\n",
      "Output tensor([[0.6450],\n",
      "        [0.5935]], device='mps:0')\n",
      "Iteration 5120 Training loss 0.1156177669763565 Validation loss 0.11411260813474655 Accuracy 0.6445000171661377\n",
      "Output tensor([[0.5338],\n",
      "        [0.4832]], device='mps:0')\n",
      "Iteration 5130 Training loss 0.11662758886814117 Validation loss 0.11414505541324615 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.4211],\n",
      "        [0.5324]], device='mps:0')\n",
      "Iteration 5140 Training loss 0.11651655286550522 Validation loss 0.1141466423869133 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.3390],\n",
      "        [0.2964]], device='mps:0')\n",
      "Iteration 5150 Training loss 0.10833125561475754 Validation loss 0.1141190230846405 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.5246],\n",
      "        [0.4225]], device='mps:0')\n",
      "Iteration 5160 Training loss 0.11821496486663818 Validation loss 0.11412430554628372 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.5779],\n",
      "        [0.6476]], device='mps:0')\n",
      "Iteration 5170 Training loss 0.11534979194402695 Validation loss 0.11411896347999573 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.6079],\n",
      "        [0.5618]], device='mps:0')\n",
      "Iteration 5180 Training loss 0.11883453279733658 Validation loss 0.11408612132072449 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5253],\n",
      "        [0.6223]], device='mps:0')\n",
      "Iteration 5190 Training loss 0.11393075436353683 Validation loss 0.1140664592385292 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5959],\n",
      "        [0.4742]], device='mps:0')\n",
      "Iteration 5200 Training loss 0.11335605382919312 Validation loss 0.11404430121183395 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.5571],\n",
      "        [0.6128]], device='mps:0')\n",
      "Iteration 5210 Training loss 0.11677148193120956 Validation loss 0.1140635758638382 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5250],\n",
      "        [0.4736]], device='mps:0')\n",
      "Iteration 5220 Training loss 0.1079697236418724 Validation loss 0.11402827501296997 Accuracy 0.6455000042915344\n",
      "Output tensor([[0.5445],\n",
      "        [0.3038]], device='mps:0')\n",
      "Iteration 5230 Training loss 0.10785628855228424 Validation loss 0.11402984708547592 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.4030],\n",
      "        [0.3521]], device='mps:0')\n",
      "Iteration 5240 Training loss 0.10935849696397781 Validation loss 0.11400941759347916 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.6203],\n",
      "        [0.6060]], device='mps:0')\n",
      "Iteration 5250 Training loss 0.11434563994407654 Validation loss 0.11402412503957748 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5826],\n",
      "        [0.5164]], device='mps:0')\n",
      "Iteration 5260 Training loss 0.12090009450912476 Validation loss 0.11402241885662079 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.5556],\n",
      "        [0.5794]], device='mps:0')\n",
      "Iteration 5270 Training loss 0.10704406350851059 Validation loss 0.11405526846647263 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.5480],\n",
      "        [0.5216]], device='mps:0')\n",
      "Iteration 5280 Training loss 0.11370238661766052 Validation loss 0.1140994280576706 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.5343],\n",
      "        [0.5549]], device='mps:0')\n",
      "Iteration 5290 Training loss 0.1085696741938591 Validation loss 0.11401645094156265 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.4750],\n",
      "        [0.5910]], device='mps:0')\n",
      "Iteration 5300 Training loss 0.11145009845495224 Validation loss 0.11396727710962296 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5183],\n",
      "        [0.5660]], device='mps:0')\n",
      "Iteration 5310 Training loss 0.11844006925821304 Validation loss 0.11395911127328873 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5785],\n",
      "        [0.4458]], device='mps:0')\n",
      "Iteration 5320 Training loss 0.10809473693370819 Validation loss 0.11402370035648346 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.6438],\n",
      "        [0.4052]], device='mps:0')\n",
      "Iteration 5330 Training loss 0.11636330932378769 Validation loss 0.11404407024383545 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.5931],\n",
      "        [0.6142]], device='mps:0')\n",
      "Iteration 5340 Training loss 0.11606603115797043 Validation loss 0.11405668407678604 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.6530],\n",
      "        [0.6353]], device='mps:0')\n",
      "Iteration 5350 Training loss 0.11551963537931442 Validation loss 0.11396951973438263 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5405],\n",
      "        [0.5291]], device='mps:0')\n",
      "Iteration 5360 Training loss 0.12225597351789474 Validation loss 0.11391881853342056 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.6149],\n",
      "        [0.5881]], device='mps:0')\n",
      "Iteration 5370 Training loss 0.11478187143802643 Validation loss 0.11393476277589798 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.4202],\n",
      "        [0.5370]], device='mps:0')\n",
      "Iteration 5380 Training loss 0.11809959262609482 Validation loss 0.11392410844564438 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.6419],\n",
      "        [0.6198]], device='mps:0')\n",
      "Iteration 5390 Training loss 0.11109019070863724 Validation loss 0.11396776884794235 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.5665],\n",
      "        [0.4457]], device='mps:0')\n",
      "Iteration 5400 Training loss 0.11646520346403122 Validation loss 0.11389915645122528 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5566],\n",
      "        [0.4676]], device='mps:0')\n",
      "Iteration 5410 Training loss 0.11846546828746796 Validation loss 0.1138535737991333 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.3135],\n",
      "        [0.3309]], device='mps:0')\n",
      "Iteration 5420 Training loss 0.11266286671161652 Validation loss 0.11383513361215591 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.3968],\n",
      "        [0.5286]], device='mps:0')\n",
      "Iteration 5430 Training loss 0.11463653296232224 Validation loss 0.1138235330581665 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.6091],\n",
      "        [0.4073]], device='mps:0')\n",
      "Iteration 5440 Training loss 0.11546149849891663 Validation loss 0.1138143241405487 Accuracy 0.6475000381469727\n",
      "Output tensor([[0.5875],\n",
      "        [0.4724]], device='mps:0')\n",
      "Iteration 5450 Training loss 0.12361769378185272 Validation loss 0.11380762606859207 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.4975],\n",
      "        [0.4901]], device='mps:0')\n",
      "Iteration 5460 Training loss 0.1194121241569519 Validation loss 0.1138133853673935 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5916],\n",
      "        [0.4194]], device='mps:0')\n",
      "Iteration 5470 Training loss 0.11129003018140793 Validation loss 0.11385700851678848 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.3530],\n",
      "        [0.4185]], device='mps:0')\n",
      "Iteration 5480 Training loss 0.11153016984462738 Validation loss 0.11381251364946365 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.5591],\n",
      "        [0.4923]], device='mps:0')\n",
      "Iteration 5490 Training loss 0.1143239215016365 Validation loss 0.11377974599599838 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5471],\n",
      "        [0.6151]], device='mps:0')\n",
      "Iteration 5500 Training loss 0.10933784395456314 Validation loss 0.11381209641695023 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.5812],\n",
      "        [0.5687]], device='mps:0')\n",
      "Iteration 5510 Training loss 0.11304088681936264 Validation loss 0.11380455642938614 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.5482],\n",
      "        [0.5399]], device='mps:0')\n",
      "Iteration 5520 Training loss 0.11555806547403336 Validation loss 0.11379759758710861 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.4313],\n",
      "        [0.5327]], device='mps:0')\n",
      "Iteration 5530 Training loss 0.11248120665550232 Validation loss 0.11375541239976883 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.2807],\n",
      "        [0.6136]], device='mps:0')\n",
      "Iteration 5540 Training loss 0.12065788358449936 Validation loss 0.11375588178634644 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.4519],\n",
      "        [0.5852]], device='mps:0')\n",
      "Iteration 5550 Training loss 0.11358022689819336 Validation loss 0.11374152451753616 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.5275],\n",
      "        [0.4614]], device='mps:0')\n",
      "Iteration 5560 Training loss 0.10978452116250992 Validation loss 0.11372853070497513 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.5072],\n",
      "        [0.5685]], device='mps:0')\n",
      "Iteration 5570 Training loss 0.1167958602309227 Validation loss 0.11372759193181992 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.4731],\n",
      "        [0.6081]], device='mps:0')\n",
      "Iteration 5580 Training loss 0.11881692707538605 Validation loss 0.11374189704656601 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.5644],\n",
      "        [0.5804]], device='mps:0')\n",
      "Iteration 5590 Training loss 0.11726922541856766 Validation loss 0.1137450635433197 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.4508],\n",
      "        [0.5483]], device='mps:0')\n",
      "Iteration 5600 Training loss 0.11114155501127243 Validation loss 0.11371710896492004 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.4096],\n",
      "        [0.5646]], device='mps:0')\n",
      "Iteration 5610 Training loss 0.117568239569664 Validation loss 0.11370271444320679 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.6501],\n",
      "        [0.5579]], device='mps:0')\n",
      "Iteration 5620 Training loss 0.10830020904541016 Validation loss 0.11364661157131195 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.4981],\n",
      "        [0.3515]], device='mps:0')\n",
      "Iteration 5630 Training loss 0.12200459837913513 Validation loss 0.11366315186023712 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5686],\n",
      "        [0.6346]], device='mps:0')\n",
      "Iteration 5640 Training loss 0.10830235481262207 Validation loss 0.11363283544778824 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.5824],\n",
      "        [0.4515]], device='mps:0')\n",
      "Iteration 5650 Training loss 0.11273276805877686 Validation loss 0.11362022906541824 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.4770],\n",
      "        [0.5140]], device='mps:0')\n",
      "Iteration 5660 Training loss 0.10957574844360352 Validation loss 0.11363985389471054 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.2742],\n",
      "        [0.4921]], device='mps:0')\n",
      "Iteration 5670 Training loss 0.11877521872520447 Validation loss 0.11361507326364517 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.5572],\n",
      "        [0.5391]], device='mps:0')\n",
      "Iteration 5680 Training loss 0.11808109283447266 Validation loss 0.11360373347997665 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.4351],\n",
      "        [0.5351]], device='mps:0')\n",
      "Iteration 5690 Training loss 0.10652587562799454 Validation loss 0.11359076201915741 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.3951],\n",
      "        [0.5737]], device='mps:0')\n",
      "Iteration 5700 Training loss 0.11397646367549896 Validation loss 0.11357932537794113 Accuracy 0.6500000357627869\n",
      "Output tensor([[0.5921],\n",
      "        [0.3573]], device='mps:0')\n",
      "Iteration 5710 Training loss 0.11255408078432083 Validation loss 0.11358805000782013 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.4991],\n",
      "        [0.5645]], device='mps:0')\n",
      "Iteration 5720 Training loss 0.11173799633979797 Validation loss 0.11359906196594238 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.4933],\n",
      "        [0.5188]], device='mps:0')\n",
      "Iteration 5730 Training loss 0.11534562706947327 Validation loss 0.11362516134977341 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.4622],\n",
      "        [0.5908]], device='mps:0')\n",
      "Iteration 5740 Training loss 0.1145077496767044 Validation loss 0.11363382637500763 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.5476],\n",
      "        [0.5794]], device='mps:0')\n",
      "Iteration 5750 Training loss 0.11683154851198196 Validation loss 0.11357206106185913 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.5301],\n",
      "        [0.4613]], device='mps:0')\n",
      "Iteration 5760 Training loss 0.10832849144935608 Validation loss 0.11358164995908737 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.5225],\n",
      "        [0.6094]], device='mps:0')\n",
      "Iteration 5770 Training loss 0.11260724812746048 Validation loss 0.11357328295707703 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5510],\n",
      "        [0.2120]], device='mps:0')\n",
      "Iteration 5780 Training loss 0.11171465367078781 Validation loss 0.11361761391162872 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.5546],\n",
      "        [0.5772]], device='mps:0')\n",
      "Iteration 5790 Training loss 0.11972257494926453 Validation loss 0.11366263031959534 Accuracy 0.659500002861023\n",
      "Output tensor([[0.3699],\n",
      "        [0.5563]], device='mps:0')\n",
      "Iteration 5800 Training loss 0.12349285185337067 Validation loss 0.11353832483291626 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.2746],\n",
      "        [0.4941]], device='mps:0')\n",
      "Iteration 5810 Training loss 0.11089500039815903 Validation loss 0.11349109560251236 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.4338],\n",
      "        [0.3860]], device='mps:0')\n",
      "Iteration 5820 Training loss 0.1128477081656456 Validation loss 0.11349306255578995 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.5826],\n",
      "        [0.4267]], device='mps:0')\n",
      "Iteration 5830 Training loss 0.11883528530597687 Validation loss 0.11347244679927826 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.4635],\n",
      "        [0.5660]], device='mps:0')\n",
      "Iteration 5840 Training loss 0.11859573423862457 Validation loss 0.11346211284399033 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.4662],\n",
      "        [0.4646]], device='mps:0')\n",
      "Iteration 5850 Training loss 0.10956328362226486 Validation loss 0.113461934030056 Accuracy 0.655500054359436\n",
      "Output tensor([[0.4472],\n",
      "        [0.4088]], device='mps:0')\n",
      "Iteration 5860 Training loss 0.10980600863695145 Validation loss 0.11344658583402634 Accuracy 0.6465000510215759\n",
      "Output tensor([[0.5150],\n",
      "        [0.2862]], device='mps:0')\n",
      "Iteration 5870 Training loss 0.10906883329153061 Validation loss 0.11345552653074265 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.5858],\n",
      "        [0.3477]], device='mps:0')\n",
      "Iteration 5880 Training loss 0.11416061222553253 Validation loss 0.11343977600336075 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.4811],\n",
      "        [0.5098]], device='mps:0')\n",
      "Iteration 5890 Training loss 0.12715844810009003 Validation loss 0.11341976374387741 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.4532],\n",
      "        [0.4895]], device='mps:0')\n",
      "Iteration 5900 Training loss 0.1134006455540657 Validation loss 0.11344163864850998 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.4301],\n",
      "        [0.5898]], device='mps:0')\n",
      "Iteration 5910 Training loss 0.11148533225059509 Validation loss 0.11342993378639221 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5839],\n",
      "        [0.3135]], device='mps:0')\n",
      "Iteration 5920 Training loss 0.11084632575511932 Validation loss 0.11340369284152985 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.5096],\n",
      "        [0.5176]], device='mps:0')\n",
      "Iteration 5930 Training loss 0.1156778335571289 Validation loss 0.11340222507715225 Accuracy 0.656000018119812\n",
      "Output tensor([[0.3632],\n",
      "        [0.5573]], device='mps:0')\n",
      "Iteration 5940 Training loss 0.11631861329078674 Validation loss 0.11338380724191666 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5315],\n",
      "        [0.4547]], device='mps:0')\n",
      "Iteration 5950 Training loss 0.10697408765554428 Validation loss 0.113372303545475 Accuracy 0.6510000228881836\n",
      "Output tensor([[0.6675],\n",
      "        [0.5273]], device='mps:0')\n",
      "Iteration 5960 Training loss 0.11808749288320541 Validation loss 0.11336705833673477 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.5990],\n",
      "        [0.5416]], device='mps:0')\n",
      "Iteration 5970 Training loss 0.11553527414798737 Validation loss 0.11335491389036179 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.3625],\n",
      "        [0.5891]], device='mps:0')\n",
      "Iteration 5980 Training loss 0.11039908975362778 Validation loss 0.11335327476263046 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.5891],\n",
      "        [0.3865]], device='mps:0')\n",
      "Iteration 5990 Training loss 0.10999509692192078 Validation loss 0.11334870755672455 Accuracy 0.6485000252723694\n",
      "Output tensor([[0.6392],\n",
      "        [0.5241]], device='mps:0')\n",
      "Iteration 6000 Training loss 0.11237946152687073 Validation loss 0.11333128809928894 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.4015],\n",
      "        [0.4771]], device='mps:0')\n",
      "Iteration 6010 Training loss 0.11638475209474564 Validation loss 0.11332695186138153 Accuracy 0.6470000147819519\n",
      "Output tensor([[0.5687],\n",
      "        [0.4884]], device='mps:0')\n",
      "Iteration 6020 Training loss 0.11856948584318161 Validation loss 0.1133309155702591 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.4049],\n",
      "        [0.6454]], device='mps:0')\n",
      "Iteration 6030 Training loss 0.1081661581993103 Validation loss 0.11331906914710999 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.4673],\n",
      "        [0.5369]], device='mps:0')\n",
      "Iteration 6040 Training loss 0.1161053255200386 Validation loss 0.11331575363874435 Accuracy 0.6480000019073486\n",
      "Output tensor([[0.4979],\n",
      "        [0.5619]], device='mps:0')\n",
      "Iteration 6050 Training loss 0.12148303538560867 Validation loss 0.11329478770494461 Accuracy 0.6495000123977661\n",
      "Output tensor([[0.6031],\n",
      "        [0.5884]], device='mps:0')\n",
      "Iteration 6060 Training loss 0.11734987795352936 Validation loss 0.11329260468482971 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5177],\n",
      "        [0.5375]], device='mps:0')\n",
      "Iteration 6070 Training loss 0.11627235263586044 Validation loss 0.11329583078622818 Accuracy 0.655500054359436\n",
      "Output tensor([[0.2796],\n",
      "        [0.5474]], device='mps:0')\n",
      "Iteration 6080 Training loss 0.10794416069984436 Validation loss 0.11332971602678299 Accuracy 0.656000018119812\n",
      "Output tensor([[0.5161],\n",
      "        [0.5246]], device='mps:0')\n",
      "Iteration 6090 Training loss 0.11314333975315094 Validation loss 0.11331440508365631 Accuracy 0.656000018119812\n",
      "Output tensor([[0.5151],\n",
      "        [0.5850]], device='mps:0')\n",
      "Iteration 6100 Training loss 0.11261256039142609 Validation loss 0.11334788054227829 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.5643],\n",
      "        [0.5560]], device='mps:0')\n",
      "Iteration 6110 Training loss 0.11511766165494919 Validation loss 0.11339101940393448 Accuracy 0.659000039100647\n",
      "Output tensor([[0.4960],\n",
      "        [0.5454]], device='mps:0')\n",
      "Iteration 6120 Training loss 0.1182631105184555 Validation loss 0.11333449184894562 Accuracy 0.659000039100647\n",
      "Output tensor([[0.5597],\n",
      "        [0.6088]], device='mps:0')\n",
      "Iteration 6130 Training loss 0.1096951812505722 Validation loss 0.11334583908319473 Accuracy 0.659000039100647\n",
      "Output tensor([[0.3922],\n",
      "        [0.5722]], device='mps:0')\n",
      "Iteration 6140 Training loss 0.11341352015733719 Validation loss 0.11329624056816101 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.5875],\n",
      "        [0.6350]], device='mps:0')\n",
      "Iteration 6150 Training loss 0.12011171877384186 Validation loss 0.11324962228536606 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.5605],\n",
      "        [0.4678]], device='mps:0')\n",
      "Iteration 6160 Training loss 0.11164042353630066 Validation loss 0.11324950307607651 Accuracy 0.655500054359436\n",
      "Output tensor([[0.3407],\n",
      "        [0.5199]], device='mps:0')\n",
      "Iteration 6170 Training loss 0.11245404928922653 Validation loss 0.11322634667158127 Accuracy 0.6530000567436218\n",
      "Output tensor([[0.4410],\n",
      "        [0.5402]], device='mps:0')\n",
      "Iteration 6180 Training loss 0.11104047298431396 Validation loss 0.11320094764232635 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.6418],\n",
      "        [0.6938]], device='mps:0')\n",
      "Iteration 6190 Training loss 0.12117812037467957 Validation loss 0.11322254687547684 Accuracy 0.656000018119812\n",
      "Output tensor([[0.5461],\n",
      "        [0.6139]], device='mps:0')\n",
      "Iteration 6200 Training loss 0.11590564250946045 Validation loss 0.11320436745882034 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.3420],\n",
      "        [0.5459]], device='mps:0')\n",
      "Iteration 6210 Training loss 0.11159700900316238 Validation loss 0.11322192847728729 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.5866],\n",
      "        [0.5614]], device='mps:0')\n",
      "Iteration 6220 Training loss 0.11186614632606506 Validation loss 0.11318845301866531 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.4024],\n",
      "        [0.5356]], device='mps:0')\n",
      "Iteration 6230 Training loss 0.11540284007787704 Validation loss 0.11319316178560257 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.5757],\n",
      "        [0.4475]], device='mps:0')\n",
      "Iteration 6240 Training loss 0.10555131733417511 Validation loss 0.11315696686506271 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.6030],\n",
      "        [0.5570]], device='mps:0')\n",
      "Iteration 6250 Training loss 0.11092017590999603 Validation loss 0.11316736787557602 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.5708],\n",
      "        [0.4822]], device='mps:0')\n",
      "Iteration 6260 Training loss 0.11136247217655182 Validation loss 0.11318962275981903 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.6110],\n",
      "        [0.4051]], device='mps:0')\n",
      "Iteration 6270 Training loss 0.1187024787068367 Validation loss 0.1131485104560852 Accuracy 0.656000018119812\n",
      "Output tensor([[0.4486],\n",
      "        [0.5719]], device='mps:0')\n",
      "Iteration 6280 Training loss 0.11669548600912094 Validation loss 0.113169826567173 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.5233],\n",
      "        [0.3901]], device='mps:0')\n",
      "Iteration 6290 Training loss 0.10883823037147522 Validation loss 0.11319437623023987 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.5517],\n",
      "        [0.4653]], device='mps:0')\n",
      "Iteration 6300 Training loss 0.11070341616868973 Validation loss 0.11320433765649796 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.5701],\n",
      "        [0.5100]], device='mps:0')\n",
      "Iteration 6310 Training loss 0.11420909315347672 Validation loss 0.11322677880525589 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.4794],\n",
      "        [0.5945]], device='mps:0')\n",
      "Iteration 6320 Training loss 0.1065632700920105 Validation loss 0.11316891759634018 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.5647],\n",
      "        [0.4561]], device='mps:0')\n",
      "Iteration 6330 Training loss 0.13004890084266663 Validation loss 0.11310668289661407 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.3127],\n",
      "        [0.5799]], device='mps:0')\n",
      "Iteration 6340 Training loss 0.11687910556793213 Validation loss 0.11307977885007858 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5266],\n",
      "        [0.3562]], device='mps:0')\n",
      "Iteration 6350 Training loss 0.11685248464345932 Validation loss 0.11304971575737 Accuracy 0.656000018119812\n",
      "Output tensor([[0.4443],\n",
      "        [0.5717]], device='mps:0')\n",
      "Iteration 6360 Training loss 0.11846452206373215 Validation loss 0.11304974555969238 Accuracy 0.655500054359436\n",
      "Output tensor([[0.4605],\n",
      "        [0.5121]], device='mps:0')\n",
      "Iteration 6370 Training loss 0.11441747099161148 Validation loss 0.11304958164691925 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.5781],\n",
      "        [0.5425]], device='mps:0')\n",
      "Iteration 6380 Training loss 0.11422120779752731 Validation loss 0.11304306238889694 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5426],\n",
      "        [0.4502]], device='mps:0')\n",
      "Iteration 6390 Training loss 0.10912633687257767 Validation loss 0.11301217973232269 Accuracy 0.6515000462532043\n",
      "Output tensor([[0.5077],\n",
      "        [0.5039]], device='mps:0')\n",
      "Iteration 6400 Training loss 0.10930784046649933 Validation loss 0.11301150918006897 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.5516],\n",
      "        [0.4813]], device='mps:0')\n",
      "Iteration 6410 Training loss 0.10630571842193604 Validation loss 0.1129930168390274 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.5346],\n",
      "        [0.4225]], device='mps:0')\n",
      "Iteration 6420 Training loss 0.11427522450685501 Validation loss 0.11301226913928986 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.4762],\n",
      "        [0.4026]], device='mps:0')\n",
      "Iteration 6430 Training loss 0.11898719519376755 Validation loss 0.11297780275344849 Accuracy 0.655500054359436\n",
      "Output tensor([[0.4994],\n",
      "        [0.5156]], device='mps:0')\n",
      "Iteration 6440 Training loss 0.11771924793720245 Validation loss 0.11298474669456482 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.6000],\n",
      "        [0.5250]], device='mps:0')\n",
      "Iteration 6450 Training loss 0.11777982115745544 Validation loss 0.11299220472574234 Accuracy 0.655500054359436\n",
      "Output tensor([[0.5400],\n",
      "        [0.5726]], device='mps:0')\n",
      "Iteration 6460 Training loss 0.10863848775625229 Validation loss 0.11304223537445068 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.3686],\n",
      "        [0.4740]], device='mps:0')\n",
      "Iteration 6470 Training loss 0.11409367620944977 Validation loss 0.11299102008342743 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.6172],\n",
      "        [0.4884]], device='mps:0')\n",
      "Iteration 6480 Training loss 0.1123497262597084 Validation loss 0.11294062435626984 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.5427],\n",
      "        [0.5534]], device='mps:0')\n",
      "Iteration 6490 Training loss 0.1221543550491333 Validation loss 0.11297387629747391 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.5519],\n",
      "        [0.4079]], device='mps:0')\n",
      "Iteration 6500 Training loss 0.10342847555875778 Validation loss 0.11295872926712036 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.6263],\n",
      "        [0.4172]], device='mps:0')\n",
      "Iteration 6510 Training loss 0.111392542719841 Validation loss 0.11293043941259384 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.4010],\n",
      "        [0.4988]], device='mps:0')\n",
      "Iteration 6520 Training loss 0.11250787228345871 Validation loss 0.11296635866165161 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.4831],\n",
      "        [0.5872]], device='mps:0')\n",
      "Iteration 6530 Training loss 0.11657359451055527 Validation loss 0.11295600980520248 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.4894],\n",
      "        [0.5615]], device='mps:0')\n",
      "Iteration 6540 Training loss 0.12069442123174667 Validation loss 0.11290325224399567 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.4429],\n",
      "        [0.3802]], device='mps:0')\n",
      "Iteration 6550 Training loss 0.11569590121507645 Validation loss 0.11289827525615692 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.4127],\n",
      "        [0.5907]], device='mps:0')\n",
      "Iteration 6560 Training loss 0.11044860631227493 Validation loss 0.1128726378083229 Accuracy 0.6535000205039978\n",
      "Output tensor([[0.4691],\n",
      "        [0.4902]], device='mps:0')\n",
      "Iteration 6570 Training loss 0.1109120175242424 Validation loss 0.11287114024162292 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.5523],\n",
      "        [0.5707]], device='mps:0')\n",
      "Iteration 6580 Training loss 0.11509702354669571 Validation loss 0.112885981798172 Accuracy 0.656000018119812\n",
      "Output tensor([[0.5169],\n",
      "        [0.5544]], device='mps:0')\n",
      "Iteration 6590 Training loss 0.1136714518070221 Validation loss 0.11289044469594955 Accuracy 0.655500054359436\n",
      "Output tensor([[0.4512],\n",
      "        [0.4762]], device='mps:0')\n",
      "Iteration 6600 Training loss 0.12064553052186966 Validation loss 0.11284828931093216 Accuracy 0.6540000438690186\n",
      "Output tensor([[0.4886],\n",
      "        [0.4819]], device='mps:0')\n",
      "Iteration 6610 Training loss 0.10860574245452881 Validation loss 0.11284741014242172 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.4196],\n",
      "        [0.3195]], device='mps:0')\n",
      "Iteration 6620 Training loss 0.11890201270580292 Validation loss 0.11286867409944534 Accuracy 0.655500054359436\n",
      "Output tensor([[0.5220],\n",
      "        [0.4364]], device='mps:0')\n",
      "Iteration 6630 Training loss 0.11510159820318222 Validation loss 0.112893246114254 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.5253],\n",
      "        [0.4358]], device='mps:0')\n",
      "Iteration 6640 Training loss 0.10434247553348541 Validation loss 0.11287958174943924 Accuracy 0.659000039100647\n",
      "Output tensor([[0.5569],\n",
      "        [0.3601]], device='mps:0')\n",
      "Iteration 6650 Training loss 0.1133292019367218 Validation loss 0.11286365240812302 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.6167],\n",
      "        [0.5079]], device='mps:0')\n",
      "Iteration 6660 Training loss 0.1093934029340744 Validation loss 0.11287323385477066 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.5909],\n",
      "        [0.4789]], device='mps:0')\n",
      "Iteration 6670 Training loss 0.12037095427513123 Validation loss 0.11288245767354965 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.4190],\n",
      "        [0.6005]], device='mps:0')\n",
      "Iteration 6680 Training loss 0.11179123818874359 Validation loss 0.1128753125667572 Accuracy 0.6630000472068787\n",
      "Output tensor([[0.5837],\n",
      "        [0.5249]], device='mps:0')\n",
      "Iteration 6690 Training loss 0.12359821051359177 Validation loss 0.11288690567016602 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.5052],\n",
      "        [0.4389]], device='mps:0')\n",
      "Iteration 6700 Training loss 0.10808675736188889 Validation loss 0.11283298581838608 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.4690],\n",
      "        [0.5706]], device='mps:0')\n",
      "Iteration 6710 Training loss 0.1229650005698204 Validation loss 0.11277896910905838 Accuracy 0.655500054359436\n",
      "Output tensor([[0.3565],\n",
      "        [0.4730]], device='mps:0')\n",
      "Iteration 6720 Training loss 0.1078830435872078 Validation loss 0.11275150626897812 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.5664],\n",
      "        [0.4372]], device='mps:0')\n",
      "Iteration 6730 Training loss 0.11111379414796829 Validation loss 0.11273906379938126 Accuracy 0.655500054359436\n",
      "Output tensor([[0.4566],\n",
      "        [0.2364]], device='mps:0')\n",
      "Iteration 6740 Training loss 0.11104369908571243 Validation loss 0.11275499314069748 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.6845],\n",
      "        [0.3930]], device='mps:0')\n",
      "Iteration 6750 Training loss 0.11558391153812408 Validation loss 0.11273925006389618 Accuracy 0.6545000076293945\n",
      "Output tensor([[0.5465],\n",
      "        [0.4226]], device='mps:0')\n",
      "Iteration 6760 Training loss 0.11336755752563477 Validation loss 0.11272937804460526 Accuracy 0.656000018119812\n",
      "Output tensor([[0.4779],\n",
      "        [0.5838]], device='mps:0')\n",
      "Iteration 6770 Training loss 0.11494266986846924 Validation loss 0.11274772137403488 Accuracy 0.656000018119812\n",
      "Output tensor([[0.6663],\n",
      "        [0.6353]], device='mps:0')\n",
      "Iteration 6780 Training loss 0.11077552288770676 Validation loss 0.11276225745677948 Accuracy 0.659000039100647\n",
      "Output tensor([[0.4571],\n",
      "        [0.4465]], device='mps:0')\n",
      "Iteration 6790 Training loss 0.1060609444975853 Validation loss 0.11275175958871841 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.4447],\n",
      "        [0.5821]], device='mps:0')\n",
      "Iteration 6800 Training loss 0.11316291242837906 Validation loss 0.11274286359548569 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.5865],\n",
      "        [0.4788]], device='mps:0')\n",
      "Iteration 6810 Training loss 0.11530914157629013 Validation loss 0.11271051317453384 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.5386],\n",
      "        [0.5600]], device='mps:0')\n",
      "Iteration 6820 Training loss 0.11917350441217422 Validation loss 0.11270256340503693 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.5762],\n",
      "        [0.4700]], device='mps:0')\n",
      "Iteration 6830 Training loss 0.10866676270961761 Validation loss 0.11271296441555023 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.5396],\n",
      "        [0.5667]], device='mps:0')\n",
      "Iteration 6840 Training loss 0.115071602165699 Validation loss 0.11273691058158875 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.4975],\n",
      "        [0.3474]], device='mps:0')\n",
      "Iteration 6850 Training loss 0.11292091012001038 Validation loss 0.11269832402467728 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.4421],\n",
      "        [0.2511]], device='mps:0')\n",
      "Iteration 6860 Training loss 0.11818546056747437 Validation loss 0.11266077309846878 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.5022],\n",
      "        [0.4176]], device='mps:0')\n",
      "Iteration 6870 Training loss 0.11408629268407822 Validation loss 0.11264154314994812 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.4784],\n",
      "        [0.3125]], device='mps:0')\n",
      "Iteration 6880 Training loss 0.1094026044011116 Validation loss 0.11265747249126434 Accuracy 0.656000018119812\n",
      "Output tensor([[0.4009],\n",
      "        [0.5580]], device='mps:0')\n",
      "Iteration 6890 Training loss 0.11801908910274506 Validation loss 0.11268094182014465 Accuracy 0.659500002861023\n",
      "Output tensor([[0.5845],\n",
      "        [0.4648]], device='mps:0')\n",
      "Iteration 6900 Training loss 0.11084532737731934 Validation loss 0.11264914274215698 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.5229],\n",
      "        [0.3276]], device='mps:0')\n",
      "Iteration 6910 Training loss 0.11525335162878036 Validation loss 0.11274992674589157 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.5329],\n",
      "        [0.5042]], device='mps:0')\n",
      "Iteration 6920 Training loss 0.10968057066202164 Validation loss 0.11266248673200607 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.5641],\n",
      "        [0.6217]], device='mps:0')\n",
      "Iteration 6930 Training loss 0.1077198013663292 Validation loss 0.11259082704782486 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.5049],\n",
      "        [0.6029]], device='mps:0')\n",
      "Iteration 6940 Training loss 0.10893391072750092 Validation loss 0.11258883029222488 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.5515],\n",
      "        [0.4503]], device='mps:0')\n",
      "Iteration 6950 Training loss 0.11561494320631027 Validation loss 0.11259359121322632 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.5724],\n",
      "        [0.4877]], device='mps:0')\n",
      "Iteration 6960 Training loss 0.10905376076698303 Validation loss 0.11261521279811859 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.5114],\n",
      "        [0.5512]], device='mps:0')\n",
      "Iteration 6970 Training loss 0.1141272485256195 Validation loss 0.11261659115552902 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.5218],\n",
      "        [0.5630]], device='mps:0')\n",
      "Iteration 6980 Training loss 0.10379523783922195 Validation loss 0.11256945133209229 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.5331],\n",
      "        [0.5797]], device='mps:0')\n",
      "Iteration 6990 Training loss 0.11817857623100281 Validation loss 0.11255048215389252 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.4595],\n",
      "        [0.6194]], device='mps:0')\n",
      "Iteration 7000 Training loss 0.11263305693864822 Validation loss 0.11257310956716537 Accuracy 0.656000018119812\n",
      "Output tensor([[0.6656],\n",
      "        [0.5948]], device='mps:0')\n",
      "Iteration 7010 Training loss 0.11157453060150146 Validation loss 0.1125425174832344 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.2221],\n",
      "        [0.4794]], device='mps:0')\n",
      "Iteration 7020 Training loss 0.10990831255912781 Validation loss 0.11252539604902267 Accuracy 0.656000018119812\n",
      "Output tensor([[0.4894],\n",
      "        [0.2986]], device='mps:0')\n",
      "Iteration 7030 Training loss 0.11486930400133133 Validation loss 0.11252635717391968 Accuracy 0.6520000100135803\n",
      "Output tensor([[0.6182],\n",
      "        [0.2459]], device='mps:0')\n",
      "Iteration 7040 Training loss 0.11675278097391129 Validation loss 0.11252222955226898 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.4505],\n",
      "        [0.5722]], device='mps:0')\n",
      "Iteration 7050 Training loss 0.12036940455436707 Validation loss 0.11252053081989288 Accuracy 0.6505000591278076\n",
      "Output tensor([[0.5952],\n",
      "        [0.4582]], device='mps:0')\n",
      "Iteration 7060 Training loss 0.11003746837377548 Validation loss 0.11250168085098267 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.5510],\n",
      "        [0.4745]], device='mps:0')\n",
      "Iteration 7070 Training loss 0.11432304233312607 Validation loss 0.11249146610498428 Accuracy 0.656000018119812\n",
      "Output tensor([[0.5496],\n",
      "        [0.5093]], device='mps:0')\n",
      "Iteration 7080 Training loss 0.11782484501600266 Validation loss 0.11248459666967392 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.5555],\n",
      "        [0.4613]], device='mps:0')\n",
      "Iteration 7090 Training loss 0.11374471336603165 Validation loss 0.11247742176055908 Accuracy 0.6550000309944153\n",
      "Output tensor([[0.6190],\n",
      "        [0.5702]], device='mps:0')\n",
      "Iteration 7100 Training loss 0.10663381218910217 Validation loss 0.1124720647931099 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.3430],\n",
      "        [0.4181]], device='mps:0')\n",
      "Iteration 7110 Training loss 0.11126590520143509 Validation loss 0.11247877031564713 Accuracy 0.6525000333786011\n",
      "Output tensor([[0.5630],\n",
      "        [0.6052]], device='mps:0')\n",
      "Iteration 7120 Training loss 0.10788915306329727 Validation loss 0.11245675384998322 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.3716],\n",
      "        [0.4538]], device='mps:0')\n",
      "Iteration 7130 Training loss 0.12420327961444855 Validation loss 0.11244866997003555 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.3287],\n",
      "        [0.6244]], device='mps:0')\n",
      "Iteration 7140 Training loss 0.11399276554584503 Validation loss 0.1124599277973175 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.3909],\n",
      "        [0.5650]], device='mps:0')\n",
      "Iteration 7150 Training loss 0.10269616544246674 Validation loss 0.11247436702251434 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.3049],\n",
      "        [0.5384]], device='mps:0')\n",
      "Iteration 7160 Training loss 0.11355405300855637 Validation loss 0.1124790608882904 Accuracy 0.659000039100647\n",
      "Output tensor([[0.5577],\n",
      "        [0.5851]], device='mps:0')\n",
      "Iteration 7170 Training loss 0.11561097949743271 Validation loss 0.1124548614025116 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.4545],\n",
      "        [0.4411]], device='mps:0')\n",
      "Iteration 7180 Training loss 0.11590908467769623 Validation loss 0.11241713166236877 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.6335],\n",
      "        [0.3706]], device='mps:0')\n",
      "Iteration 7190 Training loss 0.11942898482084274 Validation loss 0.11241384595632553 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.4811],\n",
      "        [0.4355]], device='mps:0')\n",
      "Iteration 7200 Training loss 0.12107254564762115 Validation loss 0.11242615431547165 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.5491],\n",
      "        [0.5346]], device='mps:0')\n",
      "Iteration 7210 Training loss 0.11520083248615265 Validation loss 0.11244252324104309 Accuracy 0.659000039100647\n",
      "Output tensor([[0.5328],\n",
      "        [0.3228]], device='mps:0')\n",
      "Iteration 7220 Training loss 0.11638209968805313 Validation loss 0.11242471635341644 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.4955],\n",
      "        [0.5853]], device='mps:0')\n",
      "Iteration 7230 Training loss 0.1096125990152359 Validation loss 0.11242017894983292 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.4339],\n",
      "        [0.5559]], device='mps:0')\n",
      "Iteration 7240 Training loss 0.11198259145021439 Validation loss 0.11240646243095398 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.5384],\n",
      "        [0.6205]], device='mps:0')\n",
      "Iteration 7250 Training loss 0.11913426220417023 Validation loss 0.11238010972738266 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.3970],\n",
      "        [0.6711]], device='mps:0')\n",
      "Iteration 7260 Training loss 0.12083515524864197 Validation loss 0.1124010682106018 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.2733],\n",
      "        [0.5678]], device='mps:0')\n",
      "Iteration 7270 Training loss 0.11023393273353577 Validation loss 0.11238910257816315 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.6511],\n",
      "        [0.4177]], device='mps:0')\n",
      "Iteration 7280 Training loss 0.11928419023752213 Validation loss 0.11238233000040054 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.4717],\n",
      "        [0.5846]], device='mps:0')\n",
      "Iteration 7290 Training loss 0.1136811226606369 Validation loss 0.11236921697854996 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.4728],\n",
      "        [0.5495]], device='mps:0')\n",
      "Iteration 7300 Training loss 0.11319110542535782 Validation loss 0.1123480498790741 Accuracy 0.659000039100647\n",
      "Output tensor([[0.5356],\n",
      "        [0.6122]], device='mps:0')\n",
      "Iteration 7310 Training loss 0.11302032321691513 Validation loss 0.11234860867261887 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.4109],\n",
      "        [0.5770]], device='mps:0')\n",
      "Iteration 7320 Training loss 0.11308234184980392 Validation loss 0.11234842240810394 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.2619],\n",
      "        [0.4770]], device='mps:0')\n",
      "Iteration 7330 Training loss 0.11195878684520721 Validation loss 0.11234967410564423 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.5189],\n",
      "        [0.6145]], device='mps:0')\n",
      "Iteration 7340 Training loss 0.11136072874069214 Validation loss 0.1123172789812088 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.3556],\n",
      "        [0.4076]], device='mps:0')\n",
      "Iteration 7350 Training loss 0.11646655946969986 Validation loss 0.11236073076725006 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.5199],\n",
      "        [0.4013]], device='mps:0')\n",
      "Iteration 7360 Training loss 0.11561322212219238 Validation loss 0.11234796792268753 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.5994],\n",
      "        [0.5700]], device='mps:0')\n",
      "Iteration 7370 Training loss 0.10919364541769028 Validation loss 0.11231938749551773 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.5432],\n",
      "        [0.5400]], device='mps:0')\n",
      "Iteration 7380 Training loss 0.10902483016252518 Validation loss 0.11232148110866547 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.3121],\n",
      "        [0.5067]], device='mps:0')\n",
      "Iteration 7390 Training loss 0.12185660749673843 Validation loss 0.11230909079313278 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.3723],\n",
      "        [0.5770]], device='mps:0')\n",
      "Iteration 7400 Training loss 0.11068236827850342 Validation loss 0.11226826906204224 Accuracy 0.659000039100647\n",
      "Output tensor([[0.4264],\n",
      "        [0.5039]], device='mps:0')\n",
      "Iteration 7410 Training loss 0.11716271191835403 Validation loss 0.11228504031896591 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.4987],\n",
      "        [0.5334]], device='mps:0')\n",
      "Iteration 7420 Training loss 0.10936427861452103 Validation loss 0.11228080093860626 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.4979],\n",
      "        [0.5752]], device='mps:0')\n",
      "Iteration 7430 Training loss 0.11082057654857635 Validation loss 0.11224048584699631 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.4823],\n",
      "        [0.5203]], device='mps:0')\n",
      "Iteration 7440 Training loss 0.11552148312330246 Validation loss 0.11225174367427826 Accuracy 0.659000039100647\n",
      "Output tensor([[0.4781],\n",
      "        [0.5247]], device='mps:0')\n",
      "Iteration 7450 Training loss 0.11508933454751968 Validation loss 0.11224184930324554 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.4043],\n",
      "        [0.5490]], device='mps:0')\n",
      "Iteration 7460 Training loss 0.11297065764665604 Validation loss 0.11226265132427216 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.5352],\n",
      "        [0.5182]], device='mps:0')\n",
      "Iteration 7470 Training loss 0.122081458568573 Validation loss 0.11225391924381256 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.4593],\n",
      "        [0.3419]], device='mps:0')\n",
      "Iteration 7480 Training loss 0.11026419699192047 Validation loss 0.11226440221071243 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.5889],\n",
      "        [0.4759]], device='mps:0')\n",
      "Iteration 7490 Training loss 0.11131581664085388 Validation loss 0.11225210875272751 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.5829],\n",
      "        [0.5165]], device='mps:0')\n",
      "Iteration 7500 Training loss 0.10756848007440567 Validation loss 0.11221154034137726 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.4586],\n",
      "        [0.4397]], device='mps:0')\n",
      "Iteration 7510 Training loss 0.11410267651081085 Validation loss 0.11218982189893723 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.6031],\n",
      "        [0.5770]], device='mps:0')\n",
      "Iteration 7520 Training loss 0.11092357337474823 Validation loss 0.11220771819353104 Accuracy 0.659000039100647\n",
      "Output tensor([[0.6171],\n",
      "        [0.5646]], device='mps:0')\n",
      "Iteration 7530 Training loss 0.11752261966466904 Validation loss 0.11218193173408508 Accuracy 0.659000039100647\n",
      "Output tensor([[0.4473],\n",
      "        [0.5849]], device='mps:0')\n",
      "Iteration 7540 Training loss 0.11847106367349625 Validation loss 0.11219774186611176 Accuracy 0.659000039100647\n",
      "Output tensor([[0.4923],\n",
      "        [0.6219]], device='mps:0')\n",
      "Iteration 7550 Training loss 0.1172616109251976 Validation loss 0.11218373477458954 Accuracy 0.659500002861023\n",
      "Output tensor([[0.4946],\n",
      "        [0.5389]], device='mps:0')\n",
      "Iteration 7560 Training loss 0.10685959458351135 Validation loss 0.11215823888778687 Accuracy 0.659500002861023\n",
      "Output tensor([[0.5240],\n",
      "        [0.5299]], device='mps:0')\n",
      "Iteration 7570 Training loss 0.12108486145734787 Validation loss 0.11215654760599136 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.3533],\n",
      "        [0.5595]], device='mps:0')\n",
      "Iteration 7580 Training loss 0.11582604795694351 Validation loss 0.11215885728597641 Accuracy 0.659000039100647\n",
      "Output tensor([[0.4105],\n",
      "        [0.4905]], device='mps:0')\n",
      "Iteration 7590 Training loss 0.10901037603616714 Validation loss 0.11216647922992706 Accuracy 0.659500002861023\n",
      "Output tensor([[0.3272],\n",
      "        [0.5690]], device='mps:0')\n",
      "Iteration 7600 Training loss 0.1176375150680542 Validation loss 0.11215997487306595 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.5448],\n",
      "        [0.3034]], device='mps:0')\n",
      "Iteration 7610 Training loss 0.10488443076610565 Validation loss 0.11213888972997665 Accuracy 0.659000039100647\n",
      "Output tensor([[0.4759],\n",
      "        [0.2488]], device='mps:0')\n",
      "Iteration 7620 Training loss 0.10796884447336197 Validation loss 0.11211932450532913 Accuracy 0.659500002861023\n",
      "Output tensor([[0.4984],\n",
      "        [0.4621]], device='mps:0')\n",
      "Iteration 7630 Training loss 0.11487700790166855 Validation loss 0.11209887266159058 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.5896],\n",
      "        [0.4588]], device='mps:0')\n",
      "Iteration 7640 Training loss 0.11450021713972092 Validation loss 0.11209364235401154 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.6503],\n",
      "        [0.6282]], device='mps:0')\n",
      "Iteration 7650 Training loss 0.11893896758556366 Validation loss 0.11209511756896973 Accuracy 0.659500002861023\n",
      "Output tensor([[0.6460],\n",
      "        [0.6271]], device='mps:0')\n",
      "Iteration 7660 Training loss 0.11194491386413574 Validation loss 0.11209464073181152 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.5106],\n",
      "        [0.5512]], device='mps:0')\n",
      "Iteration 7670 Training loss 0.10803624987602234 Validation loss 0.1120733842253685 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.5383],\n",
      "        [0.6161]], device='mps:0')\n",
      "Iteration 7680 Training loss 0.10749682784080505 Validation loss 0.1120818704366684 Accuracy 0.659000039100647\n",
      "Output tensor([[0.3490],\n",
      "        [0.5918]], device='mps:0')\n",
      "Iteration 7690 Training loss 0.10805772244930267 Validation loss 0.11208464950323105 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.5626],\n",
      "        [0.6499]], device='mps:0')\n",
      "Iteration 7700 Training loss 0.10789483040571213 Validation loss 0.11211352795362473 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.5556],\n",
      "        [0.4293]], device='mps:0')\n",
      "Iteration 7710 Training loss 0.11162952333688736 Validation loss 0.1120646744966507 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.6018],\n",
      "        [0.4009]], device='mps:0')\n",
      "Iteration 7720 Training loss 0.11920162290334702 Validation loss 0.11205567419528961 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.3260],\n",
      "        [0.6047]], device='mps:0')\n",
      "Iteration 7730 Training loss 0.11381469666957855 Validation loss 0.11203890293836594 Accuracy 0.659000039100647\n",
      "Output tensor([[0.6128],\n",
      "        [0.5441]], device='mps:0')\n",
      "Iteration 7740 Training loss 0.11139614880084991 Validation loss 0.11205340176820755 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.5432],\n",
      "        [0.4934]], device='mps:0')\n",
      "Iteration 7750 Training loss 0.10998519510030746 Validation loss 0.11203725636005402 Accuracy 0.659000039100647\n",
      "Output tensor([[0.5952],\n",
      "        [0.4980]], device='mps:0')\n",
      "Iteration 7760 Training loss 0.11968722939491272 Validation loss 0.11204034090042114 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.3935],\n",
      "        [0.5362]], device='mps:0')\n",
      "Iteration 7770 Training loss 0.11965705454349518 Validation loss 0.11200923472642899 Accuracy 0.659000039100647\n",
      "Output tensor([[0.6330],\n",
      "        [0.4726]], device='mps:0')\n",
      "Iteration 7780 Training loss 0.10981056839227676 Validation loss 0.11201485246419907 Accuracy 0.659000039100647\n",
      "Output tensor([[0.6269],\n",
      "        [0.5534]], device='mps:0')\n",
      "Iteration 7790 Training loss 0.11094409972429276 Validation loss 0.11199986934661865 Accuracy 0.659000039100647\n",
      "Output tensor([[0.5243],\n",
      "        [0.4293]], device='mps:0')\n",
      "Iteration 7800 Training loss 0.10865829139947891 Validation loss 0.11198858916759491 Accuracy 0.659500002861023\n",
      "Output tensor([[0.2675],\n",
      "        [0.5379]], device='mps:0')\n",
      "Iteration 7810 Training loss 0.12102874368429184 Validation loss 0.11197967827320099 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.3444],\n",
      "        [0.4947]], device='mps:0')\n",
      "Iteration 7820 Training loss 0.11243608593940735 Validation loss 0.11197391152381897 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.4790],\n",
      "        [0.5674]], device='mps:0')\n",
      "Iteration 7830 Training loss 0.11270973086357117 Validation loss 0.11197176575660706 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.6215],\n",
      "        [0.5345]], device='mps:0')\n",
      "Iteration 7840 Training loss 0.11131923645734787 Validation loss 0.11195816099643707 Accuracy 0.659500002861023\n",
      "Output tensor([[0.4902],\n",
      "        [0.5987]], device='mps:0')\n",
      "Iteration 7850 Training loss 0.10181247442960739 Validation loss 0.11197420954704285 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.6444],\n",
      "        [0.3958]], device='mps:0')\n",
      "Iteration 7860 Training loss 0.11010641604661942 Validation loss 0.11195477843284607 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.5358],\n",
      "        [0.5569]], device='mps:0')\n",
      "Iteration 7870 Training loss 0.11837606132030487 Validation loss 0.1119532510638237 Accuracy 0.659000039100647\n",
      "Output tensor([[0.3957],\n",
      "        [0.5923]], device='mps:0')\n",
      "Iteration 7880 Training loss 0.11229884624481201 Validation loss 0.11193051189184189 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.4949],\n",
      "        [0.3531]], device='mps:0')\n",
      "Iteration 7890 Training loss 0.11112848669290543 Validation loss 0.11193007230758667 Accuracy 0.6570000052452087\n",
      "Output tensor([[0.3949],\n",
      "        [0.4653]], device='mps:0')\n",
      "Iteration 7900 Training loss 0.11697876453399658 Validation loss 0.1119307354092598 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.4921],\n",
      "        [0.5986]], device='mps:0')\n",
      "Iteration 7910 Training loss 0.10940861701965332 Validation loss 0.11191917955875397 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.2512],\n",
      "        [0.3788]], device='mps:0')\n",
      "Iteration 7920 Training loss 0.11717725545167923 Validation loss 0.11191371083259583 Accuracy 0.6565000414848328\n",
      "Output tensor([[0.2798],\n",
      "        [0.4165]], device='mps:0')\n",
      "Iteration 7930 Training loss 0.12461219727993011 Validation loss 0.11189574003219604 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.6058],\n",
      "        [0.3206]], device='mps:0')\n",
      "Iteration 7940 Training loss 0.11186392605304718 Validation loss 0.1118951290845871 Accuracy 0.6580000519752502\n",
      "Output tensor([[0.3741],\n",
      "        [0.5264]], device='mps:0')\n",
      "Iteration 7950 Training loss 0.11415808647871017 Validation loss 0.11188840121030807 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.5366],\n",
      "        [0.5254]], device='mps:0')\n",
      "Iteration 7960 Training loss 0.11089633405208588 Validation loss 0.11188521981239319 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.3343],\n",
      "        [0.4810]], device='mps:0')\n",
      "Iteration 7970 Training loss 0.11280067265033722 Validation loss 0.11187350749969482 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.4196],\n",
      "        [0.4829]], device='mps:0')\n",
      "Iteration 7980 Training loss 0.10979999601840973 Validation loss 0.11186057329177856 Accuracy 0.659000039100647\n",
      "Output tensor([[0.2843],\n",
      "        [0.4969]], device='mps:0')\n",
      "Iteration 7990 Training loss 0.10341084003448486 Validation loss 0.11186660826206207 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.2861],\n",
      "        [0.4147]], device='mps:0')\n",
      "Iteration 8000 Training loss 0.10383912175893784 Validation loss 0.11186859756708145 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.4706],\n",
      "        [0.5977]], device='mps:0')\n",
      "Iteration 8010 Training loss 0.11697452515363693 Validation loss 0.11187586188316345 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.4584],\n",
      "        [0.6115]], device='mps:0')\n",
      "Iteration 8020 Training loss 0.11196216195821762 Validation loss 0.11183308064937592 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.5733],\n",
      "        [0.5184]], device='mps:0')\n",
      "Iteration 8030 Training loss 0.10986720770597458 Validation loss 0.11185190081596375 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.5533],\n",
      "        [0.5988]], device='mps:0')\n",
      "Iteration 8040 Training loss 0.10866088420152664 Validation loss 0.11184455454349518 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.6180],\n",
      "        [0.5835]], device='mps:0')\n",
      "Iteration 8050 Training loss 0.1144905835390091 Validation loss 0.1118353083729744 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.6466],\n",
      "        [0.6412]], device='mps:0')\n",
      "Iteration 8060 Training loss 0.10781892389059067 Validation loss 0.11182715743780136 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.3495],\n",
      "        [0.5223]], device='mps:0')\n",
      "Iteration 8070 Training loss 0.11204025894403458 Validation loss 0.11183110624551773 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.6592],\n",
      "        [0.5587]], device='mps:0')\n",
      "Iteration 8080 Training loss 0.11310184001922607 Validation loss 0.11187572032213211 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.3943],\n",
      "        [0.5783]], device='mps:0')\n",
      "Iteration 8090 Training loss 0.11933233588933945 Validation loss 0.11182577162981033 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.5477],\n",
      "        [0.5520]], device='mps:0')\n",
      "Iteration 8100 Training loss 0.11389467120170593 Validation loss 0.11179916560649872 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.6032],\n",
      "        [0.3884]], device='mps:0')\n",
      "Iteration 8110 Training loss 0.11050384491682053 Validation loss 0.1117887943983078 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.2868],\n",
      "        [0.4259]], device='mps:0')\n",
      "Iteration 8120 Training loss 0.11483344435691833 Validation loss 0.11177939176559448 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.4805],\n",
      "        [0.5752]], device='mps:0')\n",
      "Iteration 8130 Training loss 0.11232801526784897 Validation loss 0.11177119612693787 Accuracy 0.659500002861023\n",
      "Output tensor([[0.5736],\n",
      "        [0.5104]], device='mps:0')\n",
      "Iteration 8140 Training loss 0.11065631359815598 Validation loss 0.11180011928081512 Accuracy 0.6630000472068787\n",
      "Output tensor([[0.5460],\n",
      "        [0.6658]], device='mps:0')\n",
      "Iteration 8150 Training loss 0.11386730521917343 Validation loss 0.1117880791425705 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.5936],\n",
      "        [0.6106]], device='mps:0')\n",
      "Iteration 8160 Training loss 0.11381624639034271 Validation loss 0.11179248243570328 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.5376],\n",
      "        [0.5840]], device='mps:0')\n",
      "Iteration 8170 Training loss 0.1096181645989418 Validation loss 0.1117548868060112 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.5644],\n",
      "        [0.3199]], device='mps:0')\n",
      "Iteration 8180 Training loss 0.11511394381523132 Validation loss 0.11176839470863342 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.5371],\n",
      "        [0.4566]], device='mps:0')\n",
      "Iteration 8190 Training loss 0.115861676633358 Validation loss 0.11176908016204834 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.5101],\n",
      "        [0.4529]], device='mps:0')\n",
      "Iteration 8200 Training loss 0.10773090273141861 Validation loss 0.11171907931566238 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.5636],\n",
      "        [0.5724]], device='mps:0')\n",
      "Iteration 8210 Training loss 0.1172865629196167 Validation loss 0.11172761768102646 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.3257],\n",
      "        [0.5272]], device='mps:0')\n",
      "Iteration 8220 Training loss 0.11435556411743164 Validation loss 0.11173000186681747 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.3033],\n",
      "        [0.3981]], device='mps:0')\n",
      "Iteration 8230 Training loss 0.11667034775018692 Validation loss 0.11175262182950974 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.5031],\n",
      "        [0.4294]], device='mps:0')\n",
      "Iteration 8240 Training loss 0.11812309175729752 Validation loss 0.11173898726701736 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.4432],\n",
      "        [0.3275]], device='mps:0')\n",
      "Iteration 8250 Training loss 0.10686925798654556 Validation loss 0.11172075569629669 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.5890],\n",
      "        [0.5612]], device='mps:0')\n",
      "Iteration 8260 Training loss 0.10769744962453842 Validation loss 0.11171689629554749 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.2994],\n",
      "        [0.5631]], device='mps:0')\n",
      "Iteration 8270 Training loss 0.11461014300584793 Validation loss 0.1117415800690651 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.5141],\n",
      "        [0.5733]], device='mps:0')\n",
      "Iteration 8280 Training loss 0.11765255779027939 Validation loss 0.11167789250612259 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.3454],\n",
      "        [0.5132]], device='mps:0')\n",
      "Iteration 8290 Training loss 0.10934896022081375 Validation loss 0.11167003214359283 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.4911],\n",
      "        [0.5702]], device='mps:0')\n",
      "Iteration 8300 Training loss 0.11255206912755966 Validation loss 0.11167484521865845 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.5632],\n",
      "        [0.5423]], device='mps:0')\n",
      "Iteration 8310 Training loss 0.10418559610843658 Validation loss 0.11166229099035263 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.4480],\n",
      "        [0.3510]], device='mps:0')\n",
      "Iteration 8320 Training loss 0.1116197258234024 Validation loss 0.1116526648402214 Accuracy 0.6630000472068787\n",
      "Output tensor([[0.5054],\n",
      "        [0.5069]], device='mps:0')\n",
      "Iteration 8330 Training loss 0.11242493987083435 Validation loss 0.11165950447320938 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.3008],\n",
      "        [0.5251]], device='mps:0')\n",
      "Iteration 8340 Training loss 0.11870704591274261 Validation loss 0.11165003478527069 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.5578],\n",
      "        [0.5480]], device='mps:0')\n",
      "Iteration 8350 Training loss 0.11285483092069626 Validation loss 0.1116601750254631 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.6544],\n",
      "        [0.5646]], device='mps:0')\n",
      "Iteration 8360 Training loss 0.11120077222585678 Validation loss 0.11166074872016907 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.4198],\n",
      "        [0.5367]], device='mps:0')\n",
      "Iteration 8370 Training loss 0.11783015727996826 Validation loss 0.11162474751472473 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.5210],\n",
      "        [0.1981]], device='mps:0')\n",
      "Iteration 8380 Training loss 0.12120764702558517 Validation loss 0.11160564422607422 Accuracy 0.6630000472068787\n",
      "Output tensor([[0.4806],\n",
      "        [0.3853]], device='mps:0')\n",
      "Iteration 8390 Training loss 0.1096019521355629 Validation loss 0.11160040646791458 Accuracy 0.6600000262260437\n",
      "Output tensor([[0.4581],\n",
      "        [0.5286]], device='mps:0')\n",
      "Iteration 8400 Training loss 0.10834009200334549 Validation loss 0.11159752309322357 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.4836],\n",
      "        [0.5156]], device='mps:0')\n",
      "Iteration 8410 Training loss 0.11626213788986206 Validation loss 0.11159057915210724 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.6316],\n",
      "        [0.4408]], device='mps:0')\n",
      "Iteration 8420 Training loss 0.10985399037599564 Validation loss 0.11161348223686218 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.3889],\n",
      "        [0.5213]], device='mps:0')\n",
      "Iteration 8430 Training loss 0.1049778163433075 Validation loss 0.11159644275903702 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.6111],\n",
      "        [0.3678]], device='mps:0')\n",
      "Iteration 8440 Training loss 0.11078174412250519 Validation loss 0.11162198334932327 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.4806],\n",
      "        [0.3853]], device='mps:0')\n",
      "Iteration 8450 Training loss 0.10924512892961502 Validation loss 0.1116093173623085 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.3955],\n",
      "        [0.5804]], device='mps:0')\n",
      "Iteration 8460 Training loss 0.11026643216609955 Validation loss 0.11159466952085495 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.5329],\n",
      "        [0.5827]], device='mps:0')\n",
      "Iteration 8470 Training loss 0.11021247506141663 Validation loss 0.11160347610712051 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.4487],\n",
      "        [0.4738]], device='mps:0')\n",
      "Iteration 8480 Training loss 0.11575152724981308 Validation loss 0.11161483079195023 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.5612],\n",
      "        [0.3909]], device='mps:0')\n",
      "Iteration 8490 Training loss 0.11260724067687988 Validation loss 0.11159416288137436 Accuracy 0.6630000472068787\n",
      "Output tensor([[0.6635],\n",
      "        [0.6838]], device='mps:0')\n",
      "Iteration 8500 Training loss 0.12492556124925613 Validation loss 0.11159168183803558 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.5323],\n",
      "        [0.2965]], device='mps:0')\n",
      "Iteration 8510 Training loss 0.10653053969144821 Validation loss 0.11156905442476273 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.2755],\n",
      "        [0.5439]], device='mps:0')\n",
      "Iteration 8520 Training loss 0.11895493417978287 Validation loss 0.11156382411718369 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.5979],\n",
      "        [0.3804]], device='mps:0')\n",
      "Iteration 8530 Training loss 0.11058860272169113 Validation loss 0.11156958341598511 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.5480],\n",
      "        [0.5551]], device='mps:0')\n",
      "Iteration 8540 Training loss 0.11249221861362457 Validation loss 0.11161374300718307 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.6383],\n",
      "        [0.3015]], device='mps:0')\n",
      "Iteration 8550 Training loss 0.11303439736366272 Validation loss 0.11152791231870651 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.4799],\n",
      "        [0.4336]], device='mps:0')\n",
      "Iteration 8560 Training loss 0.11410284042358398 Validation loss 0.11151490360498428 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5425],\n",
      "        [0.5136]], device='mps:0')\n",
      "Iteration 8570 Training loss 0.10510362684726715 Validation loss 0.11150899529457092 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5568],\n",
      "        [0.6076]], device='mps:0')\n",
      "Iteration 8580 Training loss 0.10875321924686432 Validation loss 0.11149782687425613 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.5221],\n",
      "        [0.5312]], device='mps:0')\n",
      "Iteration 8590 Training loss 0.11341699957847595 Validation loss 0.11150503903627396 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5401],\n",
      "        [0.5420]], device='mps:0')\n",
      "Iteration 8600 Training loss 0.11149328202009201 Validation loss 0.11148285865783691 Accuracy 0.6630000472068787\n",
      "Output tensor([[0.5178],\n",
      "        [0.5991]], device='mps:0')\n",
      "Iteration 8610 Training loss 0.10693176090717316 Validation loss 0.11148396879434586 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.5957],\n",
      "        [0.3736]], device='mps:0')\n",
      "Iteration 8620 Training loss 0.11212915927171707 Validation loss 0.11145749688148499 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.3616],\n",
      "        [0.5060]], device='mps:0')\n",
      "Iteration 8630 Training loss 0.11820357292890549 Validation loss 0.11148379743099213 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5718],\n",
      "        [0.5786]], device='mps:0')\n",
      "Iteration 8640 Training loss 0.11182714253664017 Validation loss 0.11145389080047607 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.3883],\n",
      "        [0.5343]], device='mps:0')\n",
      "Iteration 8650 Training loss 0.10578182339668274 Validation loss 0.11145282536745071 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.3743],\n",
      "        [0.3518]], device='mps:0')\n",
      "Iteration 8660 Training loss 0.10963090509176254 Validation loss 0.11145356297492981 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.5194],\n",
      "        [0.5438]], device='mps:0')\n",
      "Iteration 8670 Training loss 0.11351832747459412 Validation loss 0.11146105080842972 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.2332],\n",
      "        [0.4021]], device='mps:0')\n",
      "Iteration 8680 Training loss 0.1112292930483818 Validation loss 0.11144450306892395 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.6382],\n",
      "        [0.4780]], device='mps:0')\n",
      "Iteration 8690 Training loss 0.1060275062918663 Validation loss 0.11142584681510925 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.4952],\n",
      "        [0.5735]], device='mps:0')\n",
      "Iteration 8700 Training loss 0.10923875868320465 Validation loss 0.11143305897712708 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.6147],\n",
      "        [0.5132]], device='mps:0')\n",
      "Iteration 8710 Training loss 0.11563180387020111 Validation loss 0.11144772171974182 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.5915],\n",
      "        [0.3668]], device='mps:0')\n",
      "Iteration 8720 Training loss 0.11339019238948822 Validation loss 0.11144893616437912 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.6319],\n",
      "        [0.5515]], device='mps:0')\n",
      "Iteration 8730 Training loss 0.11170833557844162 Validation loss 0.11143436282873154 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.5278],\n",
      "        [0.5738]], device='mps:0')\n",
      "Iteration 8740 Training loss 0.10728535056114197 Validation loss 0.11140399426221848 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5832],\n",
      "        [0.6025]], device='mps:0')\n",
      "Iteration 8750 Training loss 0.11114822328090668 Validation loss 0.11137484014034271 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.3830],\n",
      "        [0.6072]], device='mps:0')\n",
      "Iteration 8760 Training loss 0.10871228575706482 Validation loss 0.11136546730995178 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.3585],\n",
      "        [0.3132]], device='mps:0')\n",
      "Iteration 8770 Training loss 0.10779493302106857 Validation loss 0.11135658621788025 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.2270],\n",
      "        [0.4634]], device='mps:0')\n",
      "Iteration 8780 Training loss 0.11367052048444748 Validation loss 0.11135042458772659 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.6204],\n",
      "        [0.4466]], device='mps:0')\n",
      "Iteration 8790 Training loss 0.1102345660328865 Validation loss 0.11135263741016388 Accuracy 0.659000039100647\n",
      "Output tensor([[0.5356],\n",
      "        [0.2273]], device='mps:0')\n",
      "Iteration 8800 Training loss 0.11503373831510544 Validation loss 0.11135086417198181 Accuracy 0.659500002861023\n",
      "Output tensor([[0.5102],\n",
      "        [0.5191]], device='mps:0')\n",
      "Iteration 8810 Training loss 0.11258541792631149 Validation loss 0.11133074015378952 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.4703],\n",
      "        [0.4885]], device='mps:0')\n",
      "Iteration 8820 Training loss 0.11376623064279556 Validation loss 0.11133377254009247 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.5601],\n",
      "        [0.5257]], device='mps:0')\n",
      "Iteration 8830 Training loss 0.1104695126414299 Validation loss 0.11131978034973145 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.2965],\n",
      "        [0.5910]], device='mps:0')\n",
      "Iteration 8840 Training loss 0.11455979198217392 Validation loss 0.11133426427841187 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.2894],\n",
      "        [0.5416]], device='mps:0')\n",
      "Iteration 8850 Training loss 0.11052360385656357 Validation loss 0.11134492605924606 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.5787],\n",
      "        [0.4222]], device='mps:0')\n",
      "Iteration 8860 Training loss 0.11482759565114975 Validation loss 0.11134493350982666 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.3914],\n",
      "        [0.5096]], device='mps:0')\n",
      "Iteration 8870 Training loss 0.10470274835824966 Validation loss 0.1113341748714447 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.6282],\n",
      "        [0.3211]], device='mps:0')\n",
      "Iteration 8880 Training loss 0.10904384404420853 Validation loss 0.11130866408348083 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.4717],\n",
      "        [0.5384]], device='mps:0')\n",
      "Iteration 8890 Training loss 0.11191802471876144 Validation loss 0.11136216670274734 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.5993],\n",
      "        [0.3589]], device='mps:0')\n",
      "Iteration 8900 Training loss 0.10502965748310089 Validation loss 0.11133522540330887 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.5560],\n",
      "        [0.2497]], device='mps:0')\n",
      "Iteration 8910 Training loss 0.11409299820661545 Validation loss 0.11132576316595078 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.4878],\n",
      "        [0.4711]], device='mps:0')\n",
      "Iteration 8920 Training loss 0.11702796816825867 Validation loss 0.11137520521879196 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.3161],\n",
      "        [0.5927]], device='mps:0')\n",
      "Iteration 8930 Training loss 0.10957697778940201 Validation loss 0.11141835898160934 Accuracy 0.6685000061988831\n",
      "Output tensor([[0.6587],\n",
      "        [0.5141]], device='mps:0')\n",
      "Iteration 8940 Training loss 0.11555978655815125 Validation loss 0.11139176040887833 Accuracy 0.6680000424385071\n",
      "Output tensor([[0.6810],\n",
      "        [0.6769]], device='mps:0')\n",
      "Iteration 8950 Training loss 0.1075984463095665 Validation loss 0.11136672645807266 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.5246],\n",
      "        [0.6192]], device='mps:0')\n",
      "Iteration 8960 Training loss 0.10892129689455032 Validation loss 0.11138312518596649 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.3564],\n",
      "        [0.6030]], device='mps:0')\n",
      "Iteration 8970 Training loss 0.11117662489414215 Validation loss 0.11128509789705276 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.5599],\n",
      "        [0.5600]], device='mps:0')\n",
      "Iteration 8980 Training loss 0.10713300108909607 Validation loss 0.11128374934196472 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.2585],\n",
      "        [0.5202]], device='mps:0')\n",
      "Iteration 8990 Training loss 0.11204949766397476 Validation loss 0.11121930927038193 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.5235],\n",
      "        [0.6360]], device='mps:0')\n",
      "Iteration 9000 Training loss 0.11241933703422546 Validation loss 0.1112261414527893 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5223],\n",
      "        [0.5249]], device='mps:0')\n",
      "Iteration 9010 Training loss 0.11264406144618988 Validation loss 0.11122304201126099 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5721],\n",
      "        [0.5810]], device='mps:0')\n",
      "Iteration 9020 Training loss 0.11909563839435577 Validation loss 0.11122138798236847 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.6456],\n",
      "        [0.5469]], device='mps:0')\n",
      "Iteration 9030 Training loss 0.11258061975240707 Validation loss 0.11125390976667404 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.6086],\n",
      "        [0.4480]], device='mps:0')\n",
      "Iteration 9040 Training loss 0.10777028650045395 Validation loss 0.11129573732614517 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.4745],\n",
      "        [0.2872]], device='mps:0')\n",
      "Iteration 9050 Training loss 0.10997670143842697 Validation loss 0.11131127178668976 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5493],\n",
      "        [0.3298]], device='mps:0')\n",
      "Iteration 9060 Training loss 0.10466113686561584 Validation loss 0.11124065518379211 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.5765],\n",
      "        [0.5133]], device='mps:0')\n",
      "Iteration 9070 Training loss 0.11172440648078918 Validation loss 0.11120415478944778 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.4999],\n",
      "        [0.2788]], device='mps:0')\n",
      "Iteration 9080 Training loss 0.11015784740447998 Validation loss 0.11117390543222427 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.5152],\n",
      "        [0.3419]], device='mps:0')\n",
      "Iteration 9090 Training loss 0.11048972606658936 Validation loss 0.11117635667324066 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.5546],\n",
      "        [0.4350]], device='mps:0')\n",
      "Iteration 9100 Training loss 0.11015526205301285 Validation loss 0.11116936057806015 Accuracy 0.659000039100647\n",
      "Output tensor([[0.5937],\n",
      "        [0.4975]], device='mps:0')\n",
      "Iteration 9110 Training loss 0.11511234194040298 Validation loss 0.11115128546953201 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.6229],\n",
      "        [0.6097]], device='mps:0')\n",
      "Iteration 9120 Training loss 0.10513443499803543 Validation loss 0.11115807294845581 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.5143],\n",
      "        [0.4283]], device='mps:0')\n",
      "Iteration 9130 Training loss 0.1147618442773819 Validation loss 0.11116446554660797 Accuracy 0.6585000157356262\n",
      "Output tensor([[0.5992],\n",
      "        [0.5330]], device='mps:0')\n",
      "Iteration 9140 Training loss 0.1126599833369255 Validation loss 0.11113697290420532 Accuracy 0.6630000472068787\n",
      "Output tensor([[0.2115],\n",
      "        [0.2332]], device='mps:0')\n",
      "Iteration 9150 Training loss 0.11161841452121735 Validation loss 0.11112643033266068 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.6315],\n",
      "        [0.3508]], device='mps:0')\n",
      "Iteration 9160 Training loss 0.10837884992361069 Validation loss 0.1111183688044548 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.5599],\n",
      "        [0.5765]], device='mps:0')\n",
      "Iteration 9170 Training loss 0.11288923770189285 Validation loss 0.11111020296812057 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.3488],\n",
      "        [0.5186]], device='mps:0')\n",
      "Iteration 9180 Training loss 0.11280946433544159 Validation loss 0.11110006272792816 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.4051],\n",
      "        [0.3541]], device='mps:0')\n",
      "Iteration 9190 Training loss 0.12295641750097275 Validation loss 0.11109592020511627 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.5173],\n",
      "        [0.3155]], device='mps:0')\n",
      "Iteration 9200 Training loss 0.11063451319932938 Validation loss 0.11108814179897308 Accuracy 0.6615000367164612\n",
      "Output tensor([[0.5192],\n",
      "        [0.6315]], device='mps:0')\n",
      "Iteration 9210 Training loss 0.11162227392196655 Validation loss 0.11109332740306854 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.2543],\n",
      "        [0.3247]], device='mps:0')\n",
      "Iteration 9220 Training loss 0.11501575261354446 Validation loss 0.11110121011734009 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.5432],\n",
      "        [0.5070]], device='mps:0')\n",
      "Iteration 9230 Training loss 0.11520697176456451 Validation loss 0.11109786480665207 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.5730],\n",
      "        [0.4525]], device='mps:0')\n",
      "Iteration 9240 Training loss 0.11433225125074387 Validation loss 0.11107198148965836 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5709],\n",
      "        [0.2323]], device='mps:0')\n",
      "Iteration 9250 Training loss 0.11843229085206985 Validation loss 0.11106514185667038 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5152],\n",
      "        [0.5880]], device='mps:0')\n",
      "Iteration 9260 Training loss 0.1048865094780922 Validation loss 0.11105015128850937 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.5602],\n",
      "        [0.6112]], device='mps:0')\n",
      "Iteration 9270 Training loss 0.11462598294019699 Validation loss 0.11104042828083038 Accuracy 0.6610000133514404\n",
      "Output tensor([[0.4539],\n",
      "        [0.4435]], device='mps:0')\n",
      "Iteration 9280 Training loss 0.1155945211648941 Validation loss 0.1110321581363678 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.5626],\n",
      "        [0.4716]], device='mps:0')\n",
      "Iteration 9290 Training loss 0.10817685723304749 Validation loss 0.11102434247732162 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.3096],\n",
      "        [0.3548]], device='mps:0')\n",
      "Iteration 9300 Training loss 0.10296033322811127 Validation loss 0.11103072762489319 Accuracy 0.6575000286102295\n",
      "Output tensor([[0.4139],\n",
      "        [0.4647]], device='mps:0')\n",
      "Iteration 9310 Training loss 0.1140420138835907 Validation loss 0.11101020127534866 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.4179],\n",
      "        [0.5475]], device='mps:0')\n",
      "Iteration 9320 Training loss 0.10771415382623672 Validation loss 0.11100713908672333 Accuracy 0.6625000238418579\n",
      "Output tensor([[0.6288],\n",
      "        [0.3252]], device='mps:0')\n",
      "Iteration 9330 Training loss 0.11837533861398697 Validation loss 0.11100015044212341 Accuracy 0.6620000600814819\n",
      "Output tensor([[0.5148],\n",
      "        [0.7052]], device='mps:0')\n",
      "Iteration 9340 Training loss 0.10540851205587387 Validation loss 0.11100582778453827 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.4305],\n",
      "        [0.5171]], device='mps:0')\n",
      "Iteration 9350 Training loss 0.11350103467702866 Validation loss 0.11100345849990845 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.2811],\n",
      "        [0.2662]], device='mps:0')\n",
      "Iteration 9360 Training loss 0.10622700303792953 Validation loss 0.11101766675710678 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.4107],\n",
      "        [0.1237]], device='mps:0')\n",
      "Iteration 9370 Training loss 0.11112996935844421 Validation loss 0.11104544997215271 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.6033],\n",
      "        [0.5188]], device='mps:0')\n",
      "Iteration 9380 Training loss 0.11323890089988708 Validation loss 0.11100159585475922 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.5860],\n",
      "        [0.3327]], device='mps:0')\n",
      "Iteration 9390 Training loss 0.10589556396007538 Validation loss 0.1110425740480423 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.4028],\n",
      "        [0.6111]], device='mps:0')\n",
      "Iteration 9400 Training loss 0.10873430222272873 Validation loss 0.11097851395606995 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.3818],\n",
      "        [0.5711]], device='mps:0')\n",
      "Iteration 9410 Training loss 0.10803243517875671 Validation loss 0.11095917224884033 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.4652],\n",
      "        [0.4162]], device='mps:0')\n",
      "Iteration 9420 Training loss 0.11467082798480988 Validation loss 0.11095663905143738 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.4195],\n",
      "        [0.5589]], device='mps:0')\n",
      "Iteration 9430 Training loss 0.12065946310758591 Validation loss 0.11094629019498825 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.3679],\n",
      "        [0.4034]], device='mps:0')\n",
      "Iteration 9440 Training loss 0.10871877521276474 Validation loss 0.11093948036432266 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.4927],\n",
      "        [0.4786]], device='mps:0')\n",
      "Iteration 9450 Training loss 0.11036152392625809 Validation loss 0.11093427240848541 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.5154],\n",
      "        [0.3316]], device='mps:0')\n",
      "Iteration 9460 Training loss 0.11574355512857437 Validation loss 0.11092712730169296 Accuracy 0.6630000472068787\n",
      "Output tensor([[0.6332],\n",
      "        [0.1897]], device='mps:0')\n",
      "Iteration 9470 Training loss 0.10529341548681259 Validation loss 0.11092422902584076 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.2884],\n",
      "        [0.4402]], device='mps:0')\n",
      "Iteration 9480 Training loss 0.1135154664516449 Validation loss 0.11091086268424988 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.6124],\n",
      "        [0.5394]], device='mps:0')\n",
      "Iteration 9490 Training loss 0.1083579957485199 Validation loss 0.1109115406870842 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.4015],\n",
      "        [0.5000]], device='mps:0')\n",
      "Iteration 9500 Training loss 0.11009380221366882 Validation loss 0.11089485883712769 Accuracy 0.6630000472068787\n",
      "Output tensor([[0.3127],\n",
      "        [0.3493]], device='mps:0')\n",
      "Iteration 9510 Training loss 0.11401818692684174 Validation loss 0.11090923100709915 Accuracy 0.6605000495910645\n",
      "Output tensor([[0.5265],\n",
      "        [0.5572]], device='mps:0')\n",
      "Iteration 9520 Training loss 0.1130947470664978 Validation loss 0.11088541150093079 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.5276],\n",
      "        [0.5366]], device='mps:0')\n",
      "Iteration 9530 Training loss 0.10860294103622437 Validation loss 0.11088048666715622 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.6583],\n",
      "        [0.5553]], device='mps:0')\n",
      "Iteration 9540 Training loss 0.10610165446996689 Validation loss 0.110892154276371 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.5592],\n",
      "        [0.6783]], device='mps:0')\n",
      "Iteration 9550 Training loss 0.11321255564689636 Validation loss 0.11087807267904282 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.5440],\n",
      "        [0.4146]], device='mps:0')\n",
      "Iteration 9560 Training loss 0.11585292220115662 Validation loss 0.110885389149189 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.6660],\n",
      "        [0.4700]], device='mps:0')\n",
      "Iteration 9570 Training loss 0.1132277175784111 Validation loss 0.11090315133333206 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.6419],\n",
      "        [0.6198]], device='mps:0')\n",
      "Iteration 9580 Training loss 0.10555541515350342 Validation loss 0.11088405549526215 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.3007],\n",
      "        [0.5571]], device='mps:0')\n",
      "Iteration 9590 Training loss 0.10472336411476135 Validation loss 0.11085636168718338 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.3523],\n",
      "        [0.4454]], device='mps:0')\n",
      "Iteration 9600 Training loss 0.10896724462509155 Validation loss 0.11086434870958328 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.3776],\n",
      "        [0.5776]], device='mps:0')\n",
      "Iteration 9610 Training loss 0.11386079341173172 Validation loss 0.11086714267730713 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.2776],\n",
      "        [0.5379]], device='mps:0')\n",
      "Iteration 9620 Training loss 0.10505131632089615 Validation loss 0.11084561794996262 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.3760],\n",
      "        [0.3398]], device='mps:0')\n",
      "Iteration 9630 Training loss 0.11120843887329102 Validation loss 0.11083927005529404 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5222],\n",
      "        [0.2613]], device='mps:0')\n",
      "Iteration 9640 Training loss 0.11007228493690491 Validation loss 0.11082068830728531 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.3303],\n",
      "        [0.5147]], device='mps:0')\n",
      "Iteration 9650 Training loss 0.11076855659484863 Validation loss 0.11081380397081375 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.6084],\n",
      "        [0.5482]], device='mps:0')\n",
      "Iteration 9660 Training loss 0.1142565980553627 Validation loss 0.11084459722042084 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.6038],\n",
      "        [0.4331]], device='mps:0')\n",
      "Iteration 9670 Training loss 0.11795756220817566 Validation loss 0.11081726849079132 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.6718],\n",
      "        [0.5906]], device='mps:0')\n",
      "Iteration 9680 Training loss 0.10646070539951324 Validation loss 0.11080380529165268 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.6004],\n",
      "        [0.4452]], device='mps:0')\n",
      "Iteration 9690 Training loss 0.10997022688388824 Validation loss 0.1107885017991066 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.4393],\n",
      "        [0.6378]], device='mps:0')\n",
      "Iteration 9700 Training loss 0.11632420867681503 Validation loss 0.1107950210571289 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.4807],\n",
      "        [0.2361]], device='mps:0')\n",
      "Iteration 9710 Training loss 0.11206988245248795 Validation loss 0.11080574989318848 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.5876],\n",
      "        [0.4019]], device='mps:0')\n",
      "Iteration 9720 Training loss 0.1126355528831482 Validation loss 0.11078386753797531 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.6007],\n",
      "        [0.3405]], device='mps:0')\n",
      "Iteration 9730 Training loss 0.09952493757009506 Validation loss 0.11076131463050842 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.4652],\n",
      "        [0.2402]], device='mps:0')\n",
      "Iteration 9740 Training loss 0.10540473461151123 Validation loss 0.11075383424758911 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.4778],\n",
      "        [0.5484]], device='mps:0')\n",
      "Iteration 9750 Training loss 0.1201721578836441 Validation loss 0.11074568331241608 Accuracy 0.6640000343322754\n",
      "Output tensor([[0.4535],\n",
      "        [0.6327]], device='mps:0')\n",
      "Iteration 9760 Training loss 0.10889175534248352 Validation loss 0.11076679080724716 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.4405],\n",
      "        [0.4782]], device='mps:0')\n",
      "Iteration 9770 Training loss 0.11367601901292801 Validation loss 0.11077505350112915 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.5178],\n",
      "        [0.5441]], device='mps:0')\n",
      "Iteration 9780 Training loss 0.10962865501642227 Validation loss 0.11078128218650818 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.4942],\n",
      "        [0.2870]], device='mps:0')\n",
      "Iteration 9790 Training loss 0.10748757421970367 Validation loss 0.11074721068143845 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.4587],\n",
      "        [0.5313]], device='mps:0')\n",
      "Iteration 9800 Training loss 0.1177905797958374 Validation loss 0.11079888790845871 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.5233],\n",
      "        [0.5265]], device='mps:0')\n",
      "Iteration 9810 Training loss 0.11299403756856918 Validation loss 0.110731340944767 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.5150],\n",
      "        [0.4981]], device='mps:0')\n",
      "Iteration 9820 Training loss 0.10546720027923584 Validation loss 0.11075876653194427 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.4129],\n",
      "        [0.3535]], device='mps:0')\n",
      "Iteration 9830 Training loss 0.11501485854387283 Validation loss 0.1107347160577774 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.5811],\n",
      "        [0.2097]], device='mps:0')\n",
      "Iteration 9840 Training loss 0.11002898961305618 Validation loss 0.11070109903812408 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.3881],\n",
      "        [0.4341]], device='mps:0')\n",
      "Iteration 9850 Training loss 0.10992557555437088 Validation loss 0.11070261895656586 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.4969],\n",
      "        [0.6715]], device='mps:0')\n",
      "Iteration 9860 Training loss 0.10842675715684891 Validation loss 0.11070798337459564 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.4829],\n",
      "        [0.5612]], device='mps:0')\n",
      "Iteration 9870 Training loss 0.11089593917131424 Validation loss 0.11069377511739731 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.4833],\n",
      "        [0.2080]], device='mps:0')\n",
      "Iteration 9880 Training loss 0.11393558979034424 Validation loss 0.11069302260875702 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5324],\n",
      "        [0.4947]], device='mps:0')\n",
      "Iteration 9890 Training loss 0.10915805399417877 Validation loss 0.11072557419538498 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.5748],\n",
      "        [0.4637]], device='mps:0')\n",
      "Iteration 9900 Training loss 0.10845132917165756 Validation loss 0.11072172969579697 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.2997],\n",
      "        [0.6081]], device='mps:0')\n",
      "Iteration 9910 Training loss 0.11351478099822998 Validation loss 0.11072967201471329 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.5731],\n",
      "        [0.5766]], device='mps:0')\n",
      "Iteration 9920 Training loss 0.11419545114040375 Validation loss 0.11073023825883865 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.5481],\n",
      "        [0.6067]], device='mps:0')\n",
      "Iteration 9930 Training loss 0.10944673418998718 Validation loss 0.1106620579957962 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.5901],\n",
      "        [0.6085]], device='mps:0')\n",
      "Iteration 9940 Training loss 0.11067061126232147 Validation loss 0.110674187541008 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.4895],\n",
      "        [0.5218]], device='mps:0')\n",
      "Iteration 9950 Training loss 0.11116063594818115 Validation loss 0.11069582402706146 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.5191],\n",
      "        [0.3952]], device='mps:0')\n",
      "Iteration 9960 Training loss 0.10527652502059937 Validation loss 0.11069940775632858 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.3928],\n",
      "        [0.5930]], device='mps:0')\n",
      "Iteration 9970 Training loss 0.10797083377838135 Validation loss 0.11071614921092987 Accuracy 0.6695000529289246\n",
      "Output tensor([[0.3566],\n",
      "        [0.2629]], device='mps:0')\n",
      "Iteration 9980 Training loss 0.11098702996969223 Validation loss 0.11065822839736938 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.5172],\n",
      "        [0.5628]], device='mps:0')\n",
      "Iteration 9990 Training loss 0.10950860381126404 Validation loss 0.1106300875544548 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.5175],\n",
      "        [0.2904]], device='mps:0')\n",
      "Iteration 10000 Training loss 0.10844045132398605 Validation loss 0.11063551902770996 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.4442],\n",
      "        [0.5472]], device='mps:0')\n",
      "Iteration 10010 Training loss 0.12190454453229904 Validation loss 0.11062885820865631 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.4909],\n",
      "        [0.5280]], device='mps:0')\n",
      "Iteration 10020 Training loss 0.11322525143623352 Validation loss 0.11065077781677246 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.3228],\n",
      "        [0.6265]], device='mps:0')\n",
      "Iteration 10030 Training loss 0.12104123830795288 Validation loss 0.11064443737268448 Accuracy 0.6645000576972961\n",
      "Output tensor([[0.5764],\n",
      "        [0.5296]], device='mps:0')\n",
      "Iteration 10040 Training loss 0.11349686235189438 Validation loss 0.11064793914556503 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.3742],\n",
      "        [0.3198]], device='mps:0')\n",
      "Iteration 10050 Training loss 0.1044628694653511 Validation loss 0.11059872061014175 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5009],\n",
      "        [0.5480]], device='mps:0')\n",
      "Iteration 10060 Training loss 0.11069564521312714 Validation loss 0.1105828657746315 Accuracy 0.6680000424385071\n",
      "Output tensor([[0.6367],\n",
      "        [0.3300]], device='mps:0')\n",
      "Iteration 10070 Training loss 0.11498407274484634 Validation loss 0.11059960722923279 Accuracy 0.6680000424385071\n",
      "Output tensor([[0.5687],\n",
      "        [0.4762]], device='mps:0')\n",
      "Iteration 10080 Training loss 0.12105537950992584 Validation loss 0.11061564832925797 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.5751],\n",
      "        [0.5490]], device='mps:0')\n",
      "Iteration 10090 Training loss 0.11857619136571884 Validation loss 0.11061572283506393 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.4705],\n",
      "        [0.5190]], device='mps:0')\n",
      "Iteration 10100 Training loss 0.09953798353672028 Validation loss 0.11058120429515839 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.5320],\n",
      "        [0.5183]], device='mps:0')\n",
      "Iteration 10110 Training loss 0.10539396107196808 Validation loss 0.11054980009794235 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.6451],\n",
      "        [0.6021]], device='mps:0')\n",
      "Iteration 10120 Training loss 0.1110759899020195 Validation loss 0.11055653542280197 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.4565],\n",
      "        [0.4055]], device='mps:0')\n",
      "Iteration 10130 Training loss 0.11352668702602386 Validation loss 0.11054147034883499 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.4417],\n",
      "        [0.5489]], device='mps:0')\n",
      "Iteration 10140 Training loss 0.1142391785979271 Validation loss 0.11052817106246948 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.4075],\n",
      "        [0.4680]], device='mps:0')\n",
      "Iteration 10150 Training loss 0.10770437121391296 Validation loss 0.11051793396472931 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.6354],\n",
      "        [0.5021]], device='mps:0')\n",
      "Iteration 10160 Training loss 0.10857906937599182 Validation loss 0.11051920056343079 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.4212],\n",
      "        [0.5023]], device='mps:0')\n",
      "Iteration 10170 Training loss 0.10347412526607513 Validation loss 0.1105058342218399 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.6321],\n",
      "        [0.5666]], device='mps:0')\n",
      "Iteration 10180 Training loss 0.11263839155435562 Validation loss 0.11050062626600266 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.5341],\n",
      "        [0.4619]], device='mps:0')\n",
      "Iteration 10190 Training loss 0.10804929584264755 Validation loss 0.11049418896436691 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.2711],\n",
      "        [0.4991]], device='mps:0')\n",
      "Iteration 10200 Training loss 0.10901990532875061 Validation loss 0.11050289124250412 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.6017],\n",
      "        [0.4660]], device='mps:0')\n",
      "Iteration 10210 Training loss 0.1071661189198494 Validation loss 0.11049054563045502 Accuracy 0.6680000424385071\n",
      "Output tensor([[0.5681],\n",
      "        [0.4050]], device='mps:0')\n",
      "Iteration 10220 Training loss 0.11091942340135574 Validation loss 0.11049433797597885 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.4833],\n",
      "        [0.7122]], device='mps:0')\n",
      "Iteration 10230 Training loss 0.11071999371051788 Validation loss 0.1104673445224762 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.2859],\n",
      "        [0.4233]], device='mps:0')\n",
      "Iteration 10240 Training loss 0.1115473061800003 Validation loss 0.11047016084194183 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.5810],\n",
      "        [0.1595]], device='mps:0')\n",
      "Iteration 10250 Training loss 0.1092245876789093 Validation loss 0.110457643866539 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.4871],\n",
      "        [0.3021]], device='mps:0')\n",
      "Iteration 10260 Training loss 0.11259052157402039 Validation loss 0.11045277863740921 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.4888],\n",
      "        [0.4420]], device='mps:0')\n",
      "Iteration 10270 Training loss 0.11803729832172394 Validation loss 0.11045732349157333 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.6366],\n",
      "        [0.5557]], device='mps:0')\n",
      "Iteration 10280 Training loss 0.1045399159193039 Validation loss 0.11044632643461227 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.5490],\n",
      "        [0.3978]], device='mps:0')\n",
      "Iteration 10290 Training loss 0.11490649729967117 Validation loss 0.11046045273542404 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.4181],\n",
      "        [0.4244]], device='mps:0')\n",
      "Iteration 10300 Training loss 0.11541827768087387 Validation loss 0.11044836044311523 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.4521],\n",
      "        [0.4214]], device='mps:0')\n",
      "Iteration 10310 Training loss 0.1178707480430603 Validation loss 0.11043507605791092 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.3803],\n",
      "        [0.5715]], device='mps:0')\n",
      "Iteration 10320 Training loss 0.12379888445138931 Validation loss 0.11043532192707062 Accuracy 0.6680000424385071\n",
      "Output tensor([[0.5800],\n",
      "        [0.2671]], device='mps:0')\n",
      "Iteration 10330 Training loss 0.09957994520664215 Validation loss 0.11055605113506317 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.6305],\n",
      "        [0.4909]], device='mps:0')\n",
      "Iteration 10340 Training loss 0.10973554104566574 Validation loss 0.11044322699308395 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.5305],\n",
      "        [0.5714]], device='mps:0')\n",
      "Iteration 10350 Training loss 0.10378621518611908 Validation loss 0.1104450598359108 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.4878],\n",
      "        [0.5355]], device='mps:0')\n",
      "Iteration 10360 Training loss 0.1164211556315422 Validation loss 0.11044472455978394 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.5224],\n",
      "        [0.6609]], device='mps:0')\n",
      "Iteration 10370 Training loss 0.10191233456134796 Validation loss 0.11040668189525604 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.5501],\n",
      "        [0.5025]], device='mps:0')\n",
      "Iteration 10380 Training loss 0.11327172815799713 Validation loss 0.11039471626281738 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.5616],\n",
      "        [0.4892]], device='mps:0')\n",
      "Iteration 10390 Training loss 0.11166780441999435 Validation loss 0.1103767603635788 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.3942],\n",
      "        [0.6502]], device='mps:0')\n",
      "Iteration 10400 Training loss 0.11816457659006119 Validation loss 0.11037131398916245 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.5241],\n",
      "        [0.5793]], device='mps:0')\n",
      "Iteration 10410 Training loss 0.1093151867389679 Validation loss 0.11036887019872665 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.3297],\n",
      "        [0.6377]], device='mps:0')\n",
      "Iteration 10420 Training loss 0.11367710679769516 Validation loss 0.11037137359380722 Accuracy 0.6635000109672546\n",
      "Output tensor([[0.3924],\n",
      "        [0.4637]], device='mps:0')\n",
      "Iteration 10430 Training loss 0.11549663543701172 Validation loss 0.11036891490221024 Accuracy 0.6630000472068787\n",
      "Output tensor([[0.5238],\n",
      "        [0.4969]], device='mps:0')\n",
      "Iteration 10440 Training loss 0.11400201171636581 Validation loss 0.11035186052322388 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.4836],\n",
      "        [0.5512]], device='mps:0')\n",
      "Iteration 10450 Training loss 0.11421371251344681 Validation loss 0.11034312844276428 Accuracy 0.6695000529289246\n",
      "Output tensor([[0.4952],\n",
      "        [0.4550]], device='mps:0')\n",
      "Iteration 10460 Training loss 0.1123742088675499 Validation loss 0.1103384792804718 Accuracy 0.6650000214576721\n",
      "Output tensor([[0.6044],\n",
      "        [0.5162]], device='mps:0')\n",
      "Iteration 10470 Training loss 0.11145474016666412 Validation loss 0.11033537983894348 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.5553],\n",
      "        [0.4921]], device='mps:0')\n",
      "Iteration 10480 Training loss 0.11939261853694916 Validation loss 0.11033137887716293 Accuracy 0.6685000061988831\n",
      "Output tensor([[0.5111],\n",
      "        [0.5468]], device='mps:0')\n",
      "Iteration 10490 Training loss 0.10726078599691391 Validation loss 0.11036345362663269 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.6347],\n",
      "        [0.3950]], device='mps:0')\n",
      "Iteration 10500 Training loss 0.10650481283664703 Validation loss 0.11035628616809845 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.4167],\n",
      "        [0.3589]], device='mps:0')\n",
      "Iteration 10510 Training loss 0.10641651600599289 Validation loss 0.11041652411222458 Accuracy 0.674500048160553\n",
      "Output tensor([[0.4569],\n",
      "        [0.4423]], device='mps:0')\n",
      "Iteration 10520 Training loss 0.11660558730363846 Validation loss 0.11038335412740707 Accuracy 0.671500027179718\n",
      "Output tensor([[0.5967],\n",
      "        [0.4094]], device='mps:0')\n",
      "Iteration 10530 Training loss 0.10594610869884491 Validation loss 0.11041706055402756 Accuracy 0.6740000247955322\n",
      "Output tensor([[0.3885],\n",
      "        [0.5362]], device='mps:0')\n",
      "Iteration 10540 Training loss 0.11969716846942902 Validation loss 0.11039408296346664 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.5238],\n",
      "        [0.4760]], device='mps:0')\n",
      "Iteration 10550 Training loss 0.11088090389966965 Validation loss 0.11033861339092255 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.6131],\n",
      "        [0.4325]], device='mps:0')\n",
      "Iteration 10560 Training loss 0.11540032923221588 Validation loss 0.11032693088054657 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.5989],\n",
      "        [0.6752]], device='mps:0')\n",
      "Iteration 10570 Training loss 0.12544821202754974 Validation loss 0.11039046198129654 Accuracy 0.674500048160553\n",
      "Output tensor([[0.5861],\n",
      "        [0.6005]], device='mps:0')\n",
      "Iteration 10580 Training loss 0.1091935932636261 Validation loss 0.11040318012237549 Accuracy 0.675000011920929\n",
      "Output tensor([[0.4703],\n",
      "        [0.4333]], device='mps:0')\n",
      "Iteration 10590 Training loss 0.11425910890102386 Validation loss 0.11036760360002518 Accuracy 0.6740000247955322\n",
      "Output tensor([[0.5494],\n",
      "        [0.6070]], device='mps:0')\n",
      "Iteration 10600 Training loss 0.11140777170658112 Validation loss 0.11035854369401932 Accuracy 0.6740000247955322\n",
      "Output tensor([[0.5637],\n",
      "        [0.5200]], device='mps:0')\n",
      "Iteration 10610 Training loss 0.10483180731534958 Validation loss 0.11033220589160919 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.5587],\n",
      "        [0.3697]], device='mps:0')\n",
      "Iteration 10620 Training loss 0.10866032540798187 Validation loss 0.11027124524116516 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.6172],\n",
      "        [0.2729]], device='mps:0')\n",
      "Iteration 10630 Training loss 0.1039116233587265 Validation loss 0.11025591939687729 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.5468],\n",
      "        [0.3150]], device='mps:0')\n",
      "Iteration 10640 Training loss 0.1171535775065422 Validation loss 0.11023733764886856 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.5244],\n",
      "        [0.6064]], device='mps:0')\n",
      "Iteration 10650 Training loss 0.11150252819061279 Validation loss 0.11023428291082382 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.6115],\n",
      "        [0.6154]], device='mps:0')\n",
      "Iteration 10660 Training loss 0.10984018445014954 Validation loss 0.1102757528424263 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.4941],\n",
      "        [0.4941]], device='mps:0')\n",
      "Iteration 10670 Training loss 0.11349797993898392 Validation loss 0.11028741300106049 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.2571],\n",
      "        [0.4719]], device='mps:0')\n",
      "Iteration 10680 Training loss 0.11301279813051224 Validation loss 0.11026343703269958 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.5008],\n",
      "        [0.2029]], device='mps:0')\n",
      "Iteration 10690 Training loss 0.11440084129571915 Validation loss 0.1102687418460846 Accuracy 0.6710000038146973\n",
      "Output tensor([[0.3871],\n",
      "        [0.5942]], device='mps:0')\n",
      "Iteration 10700 Training loss 0.11090701818466187 Validation loss 0.11034920066595078 Accuracy 0.674500048160553\n",
      "Output tensor([[0.6879],\n",
      "        [0.2997]], device='mps:0')\n",
      "Iteration 10710 Training loss 0.10593700408935547 Validation loss 0.11027105152606964 Accuracy 0.6730000376701355\n",
      "Output tensor([[0.4773],\n",
      "        [0.5743]], device='mps:0')\n",
      "Iteration 10720 Training loss 0.11487608402967453 Validation loss 0.110251285135746 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.3429],\n",
      "        [0.4812]], device='mps:0')\n",
      "Iteration 10730 Training loss 0.10576678067445755 Validation loss 0.11027026921510696 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.4464],\n",
      "        [0.5718]], device='mps:0')\n",
      "Iteration 10740 Training loss 0.10528218001127243 Validation loss 0.11023323237895966 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.5972],\n",
      "        [0.5750]], device='mps:0')\n",
      "Iteration 10750 Training loss 0.11139457672834396 Validation loss 0.1102338433265686 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.5869],\n",
      "        [0.4371]], device='mps:0')\n",
      "Iteration 10760 Training loss 0.12078817188739777 Validation loss 0.11021249741315842 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.1876],\n",
      "        [0.5213]], device='mps:0')\n",
      "Iteration 10770 Training loss 0.10627981275320053 Validation loss 0.11021510511636734 Accuracy 0.6685000061988831\n",
      "Output tensor([[0.4702],\n",
      "        [0.5091]], device='mps:0')\n",
      "Iteration 10780 Training loss 0.11760792136192322 Validation loss 0.11021498590707779 Accuracy 0.6690000295639038\n",
      "Output tensor([[0.5798],\n",
      "        [0.4703]], device='mps:0')\n",
      "Iteration 10790 Training loss 0.10984177142381668 Validation loss 0.11018314212560654 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.4097],\n",
      "        [0.6879]], device='mps:0')\n",
      "Iteration 10800 Training loss 0.10767748951911926 Validation loss 0.11018425226211548 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.5201],\n",
      "        [0.6518]], device='mps:0')\n",
      "Iteration 10810 Training loss 0.11567576974630356 Validation loss 0.11019305884838104 Accuracy 0.6685000061988831\n",
      "Output tensor([[0.5345],\n",
      "        [0.4712]], device='mps:0')\n",
      "Iteration 10820 Training loss 0.11424757540225983 Validation loss 0.11017552763223648 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.4052],\n",
      "        [0.6145]], device='mps:0')\n",
      "Iteration 10830 Training loss 0.11077319830656052 Validation loss 0.11013737320899963 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.2633],\n",
      "        [0.2440]], device='mps:0')\n",
      "Iteration 10840 Training loss 0.11139234155416489 Validation loss 0.11013773083686829 Accuracy 0.6660000085830688\n",
      "Output tensor([[0.6139],\n",
      "        [0.5598]], device='mps:0')\n",
      "Iteration 10850 Training loss 0.11159078031778336 Validation loss 0.11014638096094131 Accuracy 0.6665000319480896\n",
      "Output tensor([[0.4617],\n",
      "        [0.5739]], device='mps:0')\n",
      "Iteration 10860 Training loss 0.11571700870990753 Validation loss 0.11016850918531418 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.1702],\n",
      "        [0.6224]], device='mps:0')\n",
      "Iteration 10870 Training loss 0.11447864025831223 Validation loss 0.11016951501369476 Accuracy 0.6690000295639038\n",
      "Output tensor([[0.1997],\n",
      "        [0.2728]], device='mps:0')\n",
      "Iteration 10880 Training loss 0.11705926805734634 Validation loss 0.11013584583997726 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.4328],\n",
      "        [0.6408]], device='mps:0')\n",
      "Iteration 10890 Training loss 0.1080637127161026 Validation loss 0.11011319607496262 Accuracy 0.6655000448226929\n",
      "Output tensor([[0.3696],\n",
      "        [0.6547]], device='mps:0')\n",
      "Iteration 10900 Training loss 0.10742729157209396 Validation loss 0.11009221524000168 Accuracy 0.6680000424385071\n",
      "Output tensor([[0.6047],\n",
      "        [0.4494]], device='mps:0')\n",
      "Iteration 10910 Training loss 0.11146938055753708 Validation loss 0.11007685959339142 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.5193],\n",
      "        [0.5129]], device='mps:0')\n",
      "Iteration 10920 Training loss 0.12002147734165192 Validation loss 0.11007262766361237 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.5715],\n",
      "        [0.4743]], device='mps:0')\n",
      "Iteration 10930 Training loss 0.11827436089515686 Validation loss 0.11007727682590485 Accuracy 0.6685000061988831\n",
      "Output tensor([[0.3149],\n",
      "        [0.5771]], device='mps:0')\n",
      "Iteration 10940 Training loss 0.1061490997672081 Validation loss 0.11007601022720337 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.5452],\n",
      "        [0.3392]], device='mps:0')\n",
      "Iteration 10950 Training loss 0.11184468120336533 Validation loss 0.11011786013841629 Accuracy 0.6685000061988831\n",
      "Output tensor([[0.5006],\n",
      "        [0.7153]], device='mps:0')\n",
      "Iteration 10960 Training loss 0.10799433290958405 Validation loss 0.11014018207788467 Accuracy 0.674500048160553\n",
      "Output tensor([[0.4885],\n",
      "        [0.5993]], device='mps:0')\n",
      "Iteration 10970 Training loss 0.11115314066410065 Validation loss 0.11011436581611633 Accuracy 0.671500027179718\n",
      "Output tensor([[0.5559],\n",
      "        [0.4832]], device='mps:0')\n",
      "Iteration 10980 Training loss 0.12330657988786697 Validation loss 0.11012228578329086 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.3478],\n",
      "        [0.5199]], device='mps:0')\n",
      "Iteration 10990 Training loss 0.11700287461280823 Validation loss 0.11013790220022202 Accuracy 0.6740000247955322\n",
      "Output tensor([[0.5181],\n",
      "        [0.4323]], device='mps:0')\n",
      "Iteration 11000 Training loss 0.11172650754451752 Validation loss 0.11008112132549286 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.2367],\n",
      "        [0.5503]], device='mps:0')\n",
      "Iteration 11010 Training loss 0.11285772919654846 Validation loss 0.11007516831159592 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.5441],\n",
      "        [0.6051]], device='mps:0')\n",
      "Iteration 11020 Training loss 0.10721441358327866 Validation loss 0.11002610623836517 Accuracy 0.6690000295639038\n",
      "Output tensor([[0.6194],\n",
      "        [0.5382]], device='mps:0')\n",
      "Iteration 11030 Training loss 0.10826809704303741 Validation loss 0.11005251854658127 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.6222],\n",
      "        [0.6559]], device='mps:0')\n",
      "Iteration 11040 Training loss 0.10694470256567001 Validation loss 0.11008304357528687 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.5836],\n",
      "        [0.2930]], device='mps:0')\n",
      "Iteration 11050 Training loss 0.10281644761562347 Validation loss 0.11005839705467224 Accuracy 0.6680000424385071\n",
      "Output tensor([[0.4918],\n",
      "        [0.5519]], device='mps:0')\n",
      "Iteration 11060 Training loss 0.1106187254190445 Validation loss 0.11003750562667847 Accuracy 0.6680000424385071\n",
      "Output tensor([[0.4477],\n",
      "        [0.6996]], device='mps:0')\n",
      "Iteration 11070 Training loss 0.1090366542339325 Validation loss 0.1100168526172638 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.4481],\n",
      "        [0.6646]], device='mps:0')\n",
      "Iteration 11080 Training loss 0.10782437026500702 Validation loss 0.10999678820371628 Accuracy 0.6685000061988831\n",
      "Output tensor([[0.6913],\n",
      "        [0.5669]], device='mps:0')\n",
      "Iteration 11090 Training loss 0.10303764045238495 Validation loss 0.11001071333885193 Accuracy 0.6670000553131104\n",
      "Output tensor([[0.5140],\n",
      "        [0.3985]], device='mps:0')\n",
      "Iteration 11100 Training loss 0.11103175580501556 Validation loss 0.11002431809902191 Accuracy 0.6675000190734863\n",
      "Output tensor([[0.5624],\n",
      "        [0.4476]], device='mps:0')\n",
      "Iteration 11110 Training loss 0.11222894489765167 Validation loss 0.10999074578285217 Accuracy 0.6680000424385071\n",
      "Output tensor([[0.4176],\n",
      "        [0.4955]], device='mps:0')\n",
      "Iteration 11120 Training loss 0.10533714294433594 Validation loss 0.1100621372461319 Accuracy 0.674500048160553\n",
      "Output tensor([[0.6091],\n",
      "        [0.2371]], device='mps:0')\n",
      "Iteration 11130 Training loss 0.1163400411605835 Validation loss 0.11013956367969513 Accuracy 0.675000011920929\n",
      "Output tensor([[0.5033],\n",
      "        [0.6268]], device='mps:0')\n",
      "Iteration 11140 Training loss 0.11446449160575867 Validation loss 0.11003351211547852 Accuracy 0.6730000376701355\n",
      "Output tensor([[0.4810],\n",
      "        [0.6347]], device='mps:0')\n",
      "Iteration 11150 Training loss 0.1066499650478363 Validation loss 0.11002199351787567 Accuracy 0.6730000376701355\n",
      "Output tensor([[0.4191],\n",
      "        [0.5918]], device='mps:0')\n",
      "Iteration 11160 Training loss 0.10838107764720917 Validation loss 0.10999652743339539 Accuracy 0.6685000061988831\n",
      "Output tensor([[0.6746],\n",
      "        [0.5513]], device='mps:0')\n",
      "Iteration 11170 Training loss 0.10302203893661499 Validation loss 0.11002272367477417 Accuracy 0.675000011920929\n",
      "Output tensor([[0.5425],\n",
      "        [0.3732]], device='mps:0')\n",
      "Iteration 11180 Training loss 0.11338884383440018 Validation loss 0.10999718308448792 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.4430],\n",
      "        [0.1618]], device='mps:0')\n",
      "Iteration 11190 Training loss 0.10555072873830795 Validation loss 0.10998980700969696 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.3643],\n",
      "        [0.6297]], device='mps:0')\n",
      "Iteration 11200 Training loss 0.11391947418451309 Validation loss 0.11009582877159119 Accuracy 0.675000011920929\n",
      "Output tensor([[0.6819],\n",
      "        [0.4245]], device='mps:0')\n",
      "Iteration 11210 Training loss 0.10769796371459961 Validation loss 0.11006788909435272 Accuracy 0.6755000352859497\n",
      "Output tensor([[0.3982],\n",
      "        [0.5932]], device='mps:0')\n",
      "Iteration 11220 Training loss 0.10913697630167007 Validation loss 0.10999584197998047 Accuracy 0.674500048160553\n",
      "Output tensor([[0.6248],\n",
      "        [0.5484]], device='mps:0')\n",
      "Iteration 11230 Training loss 0.11347723007202148 Validation loss 0.11001139134168625 Accuracy 0.6740000247955322\n",
      "Output tensor([[0.4573],\n",
      "        [0.4773]], device='mps:0')\n",
      "Iteration 11240 Training loss 0.10976336896419525 Validation loss 0.1100408211350441 Accuracy 0.6740000247955322\n",
      "Output tensor([[0.6059],\n",
      "        [0.4614]], device='mps:0')\n",
      "Iteration 11250 Training loss 0.10617246478796005 Validation loss 0.10998581349849701 Accuracy 0.675000011920929\n",
      "Output tensor([[0.2560],\n",
      "        [0.2429]], device='mps:0')\n",
      "Iteration 11260 Training loss 0.11121801286935806 Validation loss 0.10996091365814209 Accuracy 0.6740000247955322\n",
      "Output tensor([[0.6463],\n",
      "        [0.6346]], device='mps:0')\n",
      "Iteration 11270 Training loss 0.106098972260952 Validation loss 0.10993032157421112 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.5283],\n",
      "        [0.4407]], device='mps:0')\n",
      "Iteration 11280 Training loss 0.10506284236907959 Validation loss 0.1098840981721878 Accuracy 0.6690000295639038\n",
      "Output tensor([[0.5576],\n",
      "        [0.5097]], device='mps:0')\n",
      "Iteration 11290 Training loss 0.10970748960971832 Validation loss 0.10991498827934265 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.4058],\n",
      "        [0.4188]], device='mps:0')\n",
      "Iteration 11300 Training loss 0.10342077165842056 Validation loss 0.10989385098218918 Accuracy 0.6685000061988831\n",
      "Output tensor([[0.6594],\n",
      "        [0.3780]], device='mps:0')\n",
      "Iteration 11310 Training loss 0.11476586014032364 Validation loss 0.10987607389688492 Accuracy 0.6680000424385071\n",
      "Output tensor([[0.5993],\n",
      "        [0.6679]], device='mps:0')\n",
      "Iteration 11320 Training loss 0.1142224594950676 Validation loss 0.10985088348388672 Accuracy 0.671500027179718\n",
      "Output tensor([[0.3934],\n",
      "        [0.5126]], device='mps:0')\n",
      "Iteration 11330 Training loss 0.11614939570426941 Validation loss 0.10984580218791962 Accuracy 0.671500027179718\n",
      "Output tensor([[0.6343],\n",
      "        [0.2930]], device='mps:0')\n",
      "Iteration 11340 Training loss 0.11905129998922348 Validation loss 0.10984189063310623 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.4457],\n",
      "        [0.4767]], device='mps:0')\n",
      "Iteration 11350 Training loss 0.10399840772151947 Validation loss 0.1098375916481018 Accuracy 0.6695000529289246\n",
      "Output tensor([[0.4161],\n",
      "        [0.2894]], device='mps:0')\n",
      "Iteration 11360 Training loss 0.10997673124074936 Validation loss 0.10984427481889725 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.6541],\n",
      "        [0.3490]], device='mps:0')\n",
      "Iteration 11370 Training loss 0.10821199417114258 Validation loss 0.10983192920684814 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.5930],\n",
      "        [0.5569]], device='mps:0')\n",
      "Iteration 11380 Training loss 0.10758239775896072 Validation loss 0.1098213866353035 Accuracy 0.6690000295639038\n",
      "Output tensor([[0.2947],\n",
      "        [0.6217]], device='mps:0')\n",
      "Iteration 11390 Training loss 0.10835175961256027 Validation loss 0.1098162904381752 Accuracy 0.6690000295639038\n",
      "Output tensor([[0.5360],\n",
      "        [0.2527]], device='mps:0')\n",
      "Iteration 11400 Training loss 0.10748601704835892 Validation loss 0.10982713848352432 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.6391],\n",
      "        [0.6625]], device='mps:0')\n",
      "Iteration 11410 Training loss 0.10398384183645248 Validation loss 0.10988340526819229 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.2114],\n",
      "        [0.4599]], device='mps:0')\n",
      "Iteration 11420 Training loss 0.11154921352863312 Validation loss 0.10988311469554901 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.5368],\n",
      "        [0.6572]], device='mps:0')\n",
      "Iteration 11430 Training loss 0.11118116229772568 Validation loss 0.10980572551488876 Accuracy 0.6700000166893005\n",
      "Output tensor([[0.4541],\n",
      "        [0.6092]], device='mps:0')\n",
      "Iteration 11440 Training loss 0.11625701934099197 Validation loss 0.10981547087430954 Accuracy 0.6695000529289246\n",
      "Output tensor([[0.4002],\n",
      "        [0.6171]], device='mps:0')\n",
      "Iteration 11450 Training loss 0.11782244592905045 Validation loss 0.10979732125997543 Accuracy 0.6710000038146973\n",
      "Output tensor([[0.5789],\n",
      "        [0.4862]], device='mps:0')\n",
      "Iteration 11460 Training loss 0.11649392545223236 Validation loss 0.10978522896766663 Accuracy 0.6700000166893005\n",
      "Output tensor([[0.5441],\n",
      "        [0.5145]], device='mps:0')\n",
      "Iteration 11470 Training loss 0.10757526010274887 Validation loss 0.1097845509648323 Accuracy 0.6680000424385071\n",
      "Output tensor([[0.5565],\n",
      "        [0.6062]], device='mps:0')\n",
      "Iteration 11480 Training loss 0.10863145440816879 Validation loss 0.1097794845700264 Accuracy 0.6690000295639038\n",
      "Output tensor([[0.5561],\n",
      "        [0.2963]], device='mps:0')\n",
      "Iteration 11490 Training loss 0.10601714998483658 Validation loss 0.10977893322706223 Accuracy 0.671500027179718\n",
      "Output tensor([[0.5327],\n",
      "        [0.5156]], device='mps:0')\n",
      "Iteration 11500 Training loss 0.1110529899597168 Validation loss 0.10977829992771149 Accuracy 0.6710000038146973\n",
      "Output tensor([[0.5023],\n",
      "        [0.3740]], device='mps:0')\n",
      "Iteration 11510 Training loss 0.11163701862096786 Validation loss 0.1097927913069725 Accuracy 0.671500027179718\n",
      "Output tensor([[0.4012],\n",
      "        [0.6375]], device='mps:0')\n",
      "Iteration 11520 Training loss 0.11209611594676971 Validation loss 0.10975786298513412 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.3964],\n",
      "        [0.5231]], device='mps:0')\n",
      "Iteration 11530 Training loss 0.1125359982252121 Validation loss 0.10974857211112976 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.7167],\n",
      "        [0.4265]], device='mps:0')\n",
      "Iteration 11540 Training loss 0.11765152961015701 Validation loss 0.10974198579788208 Accuracy 0.671500027179718\n",
      "Output tensor([[0.6874],\n",
      "        [0.5277]], device='mps:0')\n",
      "Iteration 11550 Training loss 0.1075817197561264 Validation loss 0.10974050313234329 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.4516],\n",
      "        [0.4514]], device='mps:0')\n",
      "Iteration 11560 Training loss 0.11329791694879532 Validation loss 0.10972592979669571 Accuracy 0.6710000038146973\n",
      "Output tensor([[0.6396],\n",
      "        [0.5916]], device='mps:0')\n",
      "Iteration 11570 Training loss 0.10340728610754013 Validation loss 0.10973649471998215 Accuracy 0.671500027179718\n",
      "Output tensor([[0.5104],\n",
      "        [0.5562]], device='mps:0')\n",
      "Iteration 11580 Training loss 0.10389131307601929 Validation loss 0.10973087698221207 Accuracy 0.671500027179718\n",
      "Output tensor([[0.6027],\n",
      "        [0.6206]], device='mps:0')\n",
      "Iteration 11590 Training loss 0.11968944221735 Validation loss 0.10974881798028946 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.6215],\n",
      "        [0.3039]], device='mps:0')\n",
      "Iteration 11600 Training loss 0.11598039418458939 Validation loss 0.10974939167499542 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.5305],\n",
      "        [0.5105]], device='mps:0')\n",
      "Iteration 11610 Training loss 0.11874403804540634 Validation loss 0.10980132222175598 Accuracy 0.6730000376701355\n",
      "Output tensor([[0.6416],\n",
      "        [0.6167]], device='mps:0')\n",
      "Iteration 11620 Training loss 0.11623574793338776 Validation loss 0.10972489416599274 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.4043],\n",
      "        [0.5679]], device='mps:0')\n",
      "Iteration 11630 Training loss 0.11743985861539841 Validation loss 0.10977757722139359 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.3871],\n",
      "        [0.6651]], device='mps:0')\n",
      "Iteration 11640 Training loss 0.108181893825531 Validation loss 0.10981816798448563 Accuracy 0.674500048160553\n",
      "Output tensor([[0.5790],\n",
      "        [0.5758]], device='mps:0')\n",
      "Iteration 11650 Training loss 0.10652068257331848 Validation loss 0.10977640002965927 Accuracy 0.6730000376701355\n",
      "Output tensor([[0.4307],\n",
      "        [0.4212]], device='mps:0')\n",
      "Iteration 11660 Training loss 0.10572884976863861 Validation loss 0.10981850326061249 Accuracy 0.6760000586509705\n",
      "Output tensor([[0.4950],\n",
      "        [0.5581]], device='mps:0')\n",
      "Iteration 11670 Training loss 0.11515519767999649 Validation loss 0.10985695570707321 Accuracy 0.675000011920929\n",
      "Output tensor([[0.5290],\n",
      "        [0.5348]], device='mps:0')\n",
      "Iteration 11680 Training loss 0.11457744240760803 Validation loss 0.10980452597141266 Accuracy 0.6760000586509705\n",
      "Output tensor([[0.6490],\n",
      "        [0.6810]], device='mps:0')\n",
      "Iteration 11690 Training loss 0.11049843579530716 Validation loss 0.10972165316343307 Accuracy 0.675000011920929\n",
      "Output tensor([[0.5633],\n",
      "        [0.3671]], device='mps:0')\n",
      "Iteration 11700 Training loss 0.11061324924230576 Validation loss 0.10975787043571472 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.2325],\n",
      "        [0.5515]], device='mps:0')\n",
      "Iteration 11710 Training loss 0.10926660150289536 Validation loss 0.10975034534931183 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.5629],\n",
      "        [0.3093]], device='mps:0')\n",
      "Iteration 11720 Training loss 0.11069676280021667 Validation loss 0.1097070500254631 Accuracy 0.675000011920929\n",
      "Output tensor([[0.4617],\n",
      "        [0.4639]], device='mps:0')\n",
      "Iteration 11730 Training loss 0.11123204976320267 Validation loss 0.10972101241350174 Accuracy 0.6760000586509705\n",
      "Output tensor([[0.5880],\n",
      "        [0.5237]], device='mps:0')\n",
      "Iteration 11740 Training loss 0.11034979671239853 Validation loss 0.10968444496393204 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.5908],\n",
      "        [0.2517]], device='mps:0')\n",
      "Iteration 11750 Training loss 0.10662738233804703 Validation loss 0.10965301096439362 Accuracy 0.6700000166893005\n",
      "Output tensor([[0.5252],\n",
      "        [0.3659]], device='mps:0')\n",
      "Iteration 11760 Training loss 0.11734925955533981 Validation loss 0.10962804406881332 Accuracy 0.671500027179718\n",
      "Output tensor([[0.5769],\n",
      "        [0.4741]], device='mps:0')\n",
      "Iteration 11770 Training loss 0.12224975228309631 Validation loss 0.10961763560771942 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.6487],\n",
      "        [0.4097]], device='mps:0')\n",
      "Iteration 11780 Training loss 0.11348024010658264 Validation loss 0.10961747914552689 Accuracy 0.6710000038146973\n",
      "Output tensor([[0.6204],\n",
      "        [0.4217]], device='mps:0')\n",
      "Iteration 11790 Training loss 0.11399327963590622 Validation loss 0.1096181645989418 Accuracy 0.671500027179718\n",
      "Output tensor([[0.5762],\n",
      "        [0.4679]], device='mps:0')\n",
      "Iteration 11800 Training loss 0.12323718518018723 Validation loss 0.10960990190505981 Accuracy 0.6710000038146973\n",
      "Output tensor([[0.4448],\n",
      "        [0.4300]], device='mps:0')\n",
      "Iteration 11810 Training loss 0.10327144712209702 Validation loss 0.10959847271442413 Accuracy 0.6710000038146973\n",
      "Output tensor([[0.5288],\n",
      "        [0.5200]], device='mps:0')\n",
      "Iteration 11820 Training loss 0.10438133776187897 Validation loss 0.10959339886903763 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.3460],\n",
      "        [0.5027]], device='mps:0')\n",
      "Iteration 11830 Training loss 0.11745535582304001 Validation loss 0.10959042608737946 Accuracy 0.671500027179718\n",
      "Output tensor([[0.1968],\n",
      "        [0.3435]], device='mps:0')\n",
      "Iteration 11840 Training loss 0.10338405519723892 Validation loss 0.10960108041763306 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.4709],\n",
      "        [0.5026]], device='mps:0')\n",
      "Iteration 11850 Training loss 0.11649012565612793 Validation loss 0.1096172109246254 Accuracy 0.6730000376701355\n",
      "Output tensor([[0.4155],\n",
      "        [0.4267]], device='mps:0')\n",
      "Iteration 11860 Training loss 0.11507859826087952 Validation loss 0.10960971564054489 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.4100],\n",
      "        [0.5210]], device='mps:0')\n",
      "Iteration 11870 Training loss 0.11374372243881226 Validation loss 0.10958297550678253 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.6094],\n",
      "        [0.4936]], device='mps:0')\n",
      "Iteration 11880 Training loss 0.11232750862836838 Validation loss 0.10960263758897781 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.4301],\n",
      "        [0.5934]], device='mps:0')\n",
      "Iteration 11890 Training loss 0.11230698227882385 Validation loss 0.10959721356630325 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.6437],\n",
      "        [0.4229]], device='mps:0')\n",
      "Iteration 11900 Training loss 0.1120157390832901 Validation loss 0.10959178954362869 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.4216],\n",
      "        [0.5734]], device='mps:0')\n",
      "Iteration 11910 Training loss 0.1173398569226265 Validation loss 0.10955165326595306 Accuracy 0.6700000166893005\n",
      "Output tensor([[0.5403],\n",
      "        [0.5649]], device='mps:0')\n",
      "Iteration 11920 Training loss 0.1110948696732521 Validation loss 0.10955484956502914 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.5596],\n",
      "        [0.6135]], device='mps:0')\n",
      "Iteration 11930 Training loss 0.10587096959352493 Validation loss 0.10954347997903824 Accuracy 0.671500027179718\n",
      "Output tensor([[0.6363],\n",
      "        [0.3590]], device='mps:0')\n",
      "Iteration 11940 Training loss 0.1041615679860115 Validation loss 0.1095341295003891 Accuracy 0.6710000038146973\n",
      "Output tensor([[0.5865],\n",
      "        [0.4017]], device='mps:0')\n",
      "Iteration 11950 Training loss 0.11150538921356201 Validation loss 0.10953093320131302 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.5406],\n",
      "        [0.6116]], device='mps:0')\n",
      "Iteration 11960 Training loss 0.10422824323177338 Validation loss 0.1095244362950325 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.6200],\n",
      "        [0.5657]], device='mps:0')\n",
      "Iteration 11970 Training loss 0.10736463218927383 Validation loss 0.10951825231313705 Accuracy 0.6710000038146973\n",
      "Output tensor([[0.4813],\n",
      "        [0.5842]], device='mps:0')\n",
      "Iteration 11980 Training loss 0.11839485913515091 Validation loss 0.10951050370931625 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.2372],\n",
      "        [0.2691]], device='mps:0')\n",
      "Iteration 11990 Training loss 0.10764256864786148 Validation loss 0.1095094382762909 Accuracy 0.671500027179718\n",
      "Output tensor([[0.4986],\n",
      "        [0.4218]], device='mps:0')\n",
      "Iteration 12000 Training loss 0.11559387296438217 Validation loss 0.10950842499732971 Accuracy 0.671500027179718\n",
      "Output tensor([[0.3927],\n",
      "        [0.5472]], device='mps:0')\n",
      "Iteration 12010 Training loss 0.10669856518507004 Validation loss 0.10949262976646423 Accuracy 0.6700000166893005\n",
      "Output tensor([[0.5281],\n",
      "        [0.3380]], device='mps:0')\n",
      "Iteration 12020 Training loss 0.10987132042646408 Validation loss 0.10948856174945831 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.4472],\n",
      "        [0.2146]], device='mps:0')\n",
      "Iteration 12030 Training loss 0.10121144354343414 Validation loss 0.10949525982141495 Accuracy 0.671500027179718\n",
      "Output tensor([[0.3153],\n",
      "        [0.5138]], device='mps:0')\n",
      "Iteration 12040 Training loss 0.10958847403526306 Validation loss 0.10947589576244354 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.3568],\n",
      "        [0.5575]], device='mps:0')\n",
      "Iteration 12050 Training loss 0.10942474007606506 Validation loss 0.10946860164403915 Accuracy 0.6730000376701355\n",
      "Output tensor([[0.3597],\n",
      "        [0.4108]], device='mps:0')\n",
      "Iteration 12060 Training loss 0.10172225534915924 Validation loss 0.10947299748659134 Accuracy 0.6705000400543213\n",
      "Output tensor([[0.4068],\n",
      "        [0.7175]], device='mps:0')\n",
      "Iteration 12070 Training loss 0.10625086724758148 Validation loss 0.1094583123922348 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.2985],\n",
      "        [0.5513]], device='mps:0')\n",
      "Iteration 12080 Training loss 0.11555035412311554 Validation loss 0.10948210954666138 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.3146],\n",
      "        [0.5056]], device='mps:0')\n",
      "Iteration 12090 Training loss 0.10139209777116776 Validation loss 0.10946033149957657 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.3857],\n",
      "        [0.6078]], device='mps:0')\n",
      "Iteration 12100 Training loss 0.10631837695837021 Validation loss 0.10945771634578705 Accuracy 0.6720000505447388\n",
      "Output tensor([[0.3690],\n",
      "        [0.5091]], device='mps:0')\n",
      "Iteration 12110 Training loss 0.10891476273536682 Validation loss 0.10948313027620316 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.5621],\n",
      "        [0.5266]], device='mps:0')\n",
      "Iteration 12120 Training loss 0.11517713218927383 Validation loss 0.10946901142597198 Accuracy 0.6740000247955322\n",
      "Output tensor([[0.6259],\n",
      "        [0.4873]], device='mps:0')\n",
      "Iteration 12130 Training loss 0.10466241836547852 Validation loss 0.10945606976747513 Accuracy 0.6740000247955322\n",
      "Output tensor([[0.4527],\n",
      "        [0.6632]], device='mps:0')\n",
      "Iteration 12140 Training loss 0.10686220228672028 Validation loss 0.10946545749902725 Accuracy 0.674500048160553\n",
      "Output tensor([[0.5749],\n",
      "        [0.5669]], device='mps:0')\n",
      "Iteration 12150 Training loss 0.11739858984947205 Validation loss 0.1094423234462738 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.4737],\n",
      "        [0.5776]], device='mps:0')\n",
      "Iteration 12160 Training loss 0.11435756087303162 Validation loss 0.10942316055297852 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.6573],\n",
      "        [0.2477]], device='mps:0')\n",
      "Iteration 12170 Training loss 0.10371661931276321 Validation loss 0.10944370180368423 Accuracy 0.675000011920929\n",
      "Output tensor([[0.5313],\n",
      "        [0.3439]], device='mps:0')\n",
      "Iteration 12180 Training loss 0.10821971297264099 Validation loss 0.10944344103336334 Accuracy 0.675000011920929\n",
      "Output tensor([[0.5638],\n",
      "        [0.5477]], device='mps:0')\n",
      "Iteration 12190 Training loss 0.11523284763097763 Validation loss 0.10943857580423355 Accuracy 0.674500048160553\n",
      "Output tensor([[0.3969],\n",
      "        [0.6869]], device='mps:0')\n",
      "Iteration 12200 Training loss 0.1081608235836029 Validation loss 0.10940829664468765 Accuracy 0.671500027179718\n",
      "Output tensor([[0.5921],\n",
      "        [0.2730]], device='mps:0')\n",
      "Iteration 12210 Training loss 0.11085295677185059 Validation loss 0.10941280424594879 Accuracy 0.6740000247955322\n",
      "Output tensor([[0.4734],\n",
      "        [0.5093]], device='mps:0')\n",
      "Iteration 12220 Training loss 0.10347815603017807 Validation loss 0.10941294580698013 Accuracy 0.675000011920929\n",
      "Output tensor([[0.5924],\n",
      "        [0.4757]], device='mps:0')\n",
      "Iteration 12230 Training loss 0.10494031757116318 Validation loss 0.10942068696022034 Accuracy 0.675000011920929\n",
      "Output tensor([[0.5928],\n",
      "        [0.5394]], device='mps:0')\n",
      "Iteration 12240 Training loss 0.10921592265367508 Validation loss 0.10947932302951813 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.4148],\n",
      "        [0.4316]], device='mps:0')\n",
      "Iteration 12250 Training loss 0.10851434618234634 Validation loss 0.10944651812314987 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.5729],\n",
      "        [0.3959]], device='mps:0')\n",
      "Iteration 12260 Training loss 0.10513231158256531 Validation loss 0.10943585634231567 Accuracy 0.6760000586509705\n",
      "Output tensor([[0.5380],\n",
      "        [0.5386]], device='mps:0')\n",
      "Iteration 12270 Training loss 0.10785901546478271 Validation loss 0.10939382016658783 Accuracy 0.6765000224113464\n",
      "Output tensor([[0.2243],\n",
      "        [0.5378]], device='mps:0')\n",
      "Iteration 12280 Training loss 0.10820212960243225 Validation loss 0.10938198119401932 Accuracy 0.6755000352859497\n",
      "Output tensor([[0.5859],\n",
      "        [0.5515]], device='mps:0')\n",
      "Iteration 12290 Training loss 0.1121688261628151 Validation loss 0.10938867926597595 Accuracy 0.6760000586509705\n",
      "Output tensor([[0.5033],\n",
      "        [0.3103]], device='mps:0')\n",
      "Iteration 12300 Training loss 0.11266443133354187 Validation loss 0.10941030085086823 Accuracy 0.6765000224113464\n",
      "Output tensor([[0.4069],\n",
      "        [0.5850]], device='mps:0')\n",
      "Iteration 12310 Training loss 0.11029128730297089 Validation loss 0.10940874367952347 Accuracy 0.6765000224113464\n",
      "Output tensor([[0.5046],\n",
      "        [0.2781]], device='mps:0')\n",
      "Iteration 12320 Training loss 0.11144513636827469 Validation loss 0.1093917265534401 Accuracy 0.6765000224113464\n",
      "Output tensor([[0.5093],\n",
      "        [0.4076]], device='mps:0')\n",
      "Iteration 12330 Training loss 0.12169677764177322 Validation loss 0.10935720056295395 Accuracy 0.674500048160553\n",
      "Output tensor([[0.5092],\n",
      "        [0.5686]], device='mps:0')\n",
      "Iteration 12340 Training loss 0.1124272346496582 Validation loss 0.10937599837779999 Accuracy 0.6765000224113464\n",
      "Output tensor([[0.6209],\n",
      "        [0.5967]], device='mps:0')\n",
      "Iteration 12350 Training loss 0.10965418815612793 Validation loss 0.10936076939105988 Accuracy 0.6755000352859497\n",
      "Output tensor([[0.4084],\n",
      "        [0.4986]], device='mps:0')\n",
      "Iteration 12360 Training loss 0.10535804182291031 Validation loss 0.1093638464808464 Accuracy 0.6765000224113464\n",
      "Output tensor([[0.3817],\n",
      "        [0.4714]], device='mps:0')\n",
      "Iteration 12370 Training loss 0.10730993002653122 Validation loss 0.10944856703281403 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.4219],\n",
      "        [0.5260]], device='mps:0')\n",
      "Iteration 12380 Training loss 0.1180390939116478 Validation loss 0.10942932963371277 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.4679],\n",
      "        [0.4647]], device='mps:0')\n",
      "Iteration 12390 Training loss 0.1175670325756073 Validation loss 0.10942276567220688 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.3310],\n",
      "        [0.4772]], device='mps:0')\n",
      "Iteration 12400 Training loss 0.10163872689008713 Validation loss 0.10941479355096817 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.5674],\n",
      "        [0.4962]], device='mps:0')\n",
      "Iteration 12410 Training loss 0.11493831127882004 Validation loss 0.10940714180469513 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.3651],\n",
      "        [0.5441]], device='mps:0')\n",
      "Iteration 12420 Training loss 0.11857232451438904 Validation loss 0.1093362495303154 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.4169],\n",
      "        [0.4899]], device='mps:0')\n",
      "Iteration 12430 Training loss 0.11356167495250702 Validation loss 0.10932768881320953 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.5568],\n",
      "        [0.4167]], device='mps:0')\n",
      "Iteration 12440 Training loss 0.1104431226849556 Validation loss 0.10929016768932343 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.4002],\n",
      "        [0.2792]], device='mps:0')\n",
      "Iteration 12450 Training loss 0.1079387292265892 Validation loss 0.10927630960941315 Accuracy 0.6730000376701355\n",
      "Output tensor([[0.6269],\n",
      "        [0.2802]], device='mps:0')\n",
      "Iteration 12460 Training loss 0.10535987466573715 Validation loss 0.10927001386880875 Accuracy 0.6730000376701355\n",
      "Output tensor([[0.5577],\n",
      "        [0.5937]], device='mps:0')\n",
      "Iteration 12470 Training loss 0.11524233222007751 Validation loss 0.10926640778779984 Accuracy 0.6730000376701355\n",
      "Output tensor([[0.5867],\n",
      "        [0.4122]], device='mps:0')\n",
      "Iteration 12480 Training loss 0.10925020277500153 Validation loss 0.10926587134599686 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.4089],\n",
      "        [0.2827]], device='mps:0')\n",
      "Iteration 12490 Training loss 0.11286517977714539 Validation loss 0.10925576090812683 Accuracy 0.671500027179718\n",
      "Output tensor([[0.5361],\n",
      "        [0.5168]], device='mps:0')\n",
      "Iteration 12500 Training loss 0.1047123596072197 Validation loss 0.10925353318452835 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.4857],\n",
      "        [0.3123]], device='mps:0')\n",
      "Iteration 12510 Training loss 0.10802090167999268 Validation loss 0.10926549136638641 Accuracy 0.6760000586509705\n",
      "Output tensor([[0.3054],\n",
      "        [0.4344]], device='mps:0')\n",
      "Iteration 12520 Training loss 0.10652849823236465 Validation loss 0.10929882526397705 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.5186],\n",
      "        [0.4455]], device='mps:0')\n",
      "Iteration 12530 Training loss 0.10107561945915222 Validation loss 0.10924793034791946 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.3718],\n",
      "        [0.4946]], device='mps:0')\n",
      "Iteration 12540 Training loss 0.10212694853544235 Validation loss 0.10923198610544205 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.5307],\n",
      "        [0.5890]], device='mps:0')\n",
      "Iteration 12550 Training loss 0.10906986892223358 Validation loss 0.10923236608505249 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.5245],\n",
      "        [0.5283]], device='mps:0')\n",
      "Iteration 12560 Training loss 0.1183662936091423 Validation loss 0.10922897607088089 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.4269],\n",
      "        [0.6102]], device='mps:0')\n",
      "Iteration 12570 Training loss 0.10339881479740143 Validation loss 0.1092195063829422 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.5902],\n",
      "        [0.5770]], device='mps:0')\n",
      "Iteration 12580 Training loss 0.11823848634958267 Validation loss 0.1092216968536377 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.6007],\n",
      "        [0.4078]], device='mps:0')\n",
      "Iteration 12590 Training loss 0.11178650707006454 Validation loss 0.10920962691307068 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.5221],\n",
      "        [0.4057]], device='mps:0')\n",
      "Iteration 12600 Training loss 0.11460018157958984 Validation loss 0.10923227667808533 Accuracy 0.6760000586509705\n",
      "Output tensor([[0.5122],\n",
      "        [0.5803]], device='mps:0')\n",
      "Iteration 12610 Training loss 0.11566068232059479 Validation loss 0.10922179371118546 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.6217],\n",
      "        [0.4381]], device='mps:0')\n",
      "Iteration 12620 Training loss 0.10712631046772003 Validation loss 0.10919944941997528 Accuracy 0.6735000610351562\n",
      "Output tensor([[0.4222],\n",
      "        [0.4346]], device='mps:0')\n",
      "Iteration 12630 Training loss 0.10700677335262299 Validation loss 0.1092180535197258 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.6548],\n",
      "        [0.5511]], device='mps:0')\n",
      "Iteration 12640 Training loss 0.1004418283700943 Validation loss 0.1092059463262558 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.6373],\n",
      "        [0.4754]], device='mps:0')\n",
      "Iteration 12650 Training loss 0.10908679664134979 Validation loss 0.10919248312711716 Accuracy 0.674500048160553\n",
      "Output tensor([[0.5402],\n",
      "        [0.6235]], device='mps:0')\n",
      "Iteration 12660 Training loss 0.10448285937309265 Validation loss 0.10920831561088562 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.5955],\n",
      "        [0.5888]], device='mps:0')\n",
      "Iteration 12670 Training loss 0.10905832052230835 Validation loss 0.10925982147455215 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.6240],\n",
      "        [0.5614]], device='mps:0')\n",
      "Iteration 12680 Training loss 0.1118512898683548 Validation loss 0.1093122586607933 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.4008],\n",
      "        [0.4752]], device='mps:0')\n",
      "Iteration 12690 Training loss 0.10731643438339233 Validation loss 0.10927162319421768 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.3010],\n",
      "        [0.2009]], device='mps:0')\n",
      "Iteration 12700 Training loss 0.10268436372280121 Validation loss 0.10920292884111404 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.5123],\n",
      "        [0.6088]], device='mps:0')\n",
      "Iteration 12710 Training loss 0.10544811934232712 Validation loss 0.10920703411102295 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.4999],\n",
      "        [0.4438]], device='mps:0')\n",
      "Iteration 12720 Training loss 0.11741892248392105 Validation loss 0.1092069000005722 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.6343],\n",
      "        [0.4518]], device='mps:0')\n",
      "Iteration 12730 Training loss 0.10493266582489014 Validation loss 0.10917244106531143 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.4845],\n",
      "        [0.6955]], device='mps:0')\n",
      "Iteration 12740 Training loss 0.11127351224422455 Validation loss 0.1091642677783966 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.5823],\n",
      "        [0.5963]], device='mps:0')\n",
      "Iteration 12750 Training loss 0.10885453969240189 Validation loss 0.1091500073671341 Accuracy 0.6760000586509705\n",
      "Output tensor([[0.6637],\n",
      "        [0.5377]], device='mps:0')\n",
      "Iteration 12760 Training loss 0.11845023930072784 Validation loss 0.10916627943515778 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.5748],\n",
      "        [0.5993]], device='mps:0')\n",
      "Iteration 12770 Training loss 0.10997480154037476 Validation loss 0.10912461578845978 Accuracy 0.6730000376701355\n",
      "Output tensor([[0.4981],\n",
      "        [0.5275]], device='mps:0')\n",
      "Iteration 12780 Training loss 0.10312537848949432 Validation loss 0.10911890119314194 Accuracy 0.674500048160553\n",
      "Output tensor([[0.4105],\n",
      "        [0.5541]], device='mps:0')\n",
      "Iteration 12790 Training loss 0.10870937258005142 Validation loss 0.10911191254854202 Accuracy 0.675000011920929\n",
      "Output tensor([[0.4176],\n",
      "        [0.3966]], device='mps:0')\n",
      "Iteration 12800 Training loss 0.1074119508266449 Validation loss 0.1091243177652359 Accuracy 0.6760000586509705\n",
      "Output tensor([[0.5919],\n",
      "        [0.6234]], device='mps:0')\n",
      "Iteration 12810 Training loss 0.1099286898970604 Validation loss 0.10909924656152725 Accuracy 0.671500027179718\n",
      "Output tensor([[0.5644],\n",
      "        [0.4846]], device='mps:0')\n",
      "Iteration 12820 Training loss 0.1032184585928917 Validation loss 0.10909505188465118 Accuracy 0.6725000143051147\n",
      "Output tensor([[0.4072],\n",
      "        [0.5547]], device='mps:0')\n",
      "Iteration 12830 Training loss 0.10529457032680511 Validation loss 0.10911836475133896 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.2433],\n",
      "        [0.4613]], device='mps:0')\n",
      "Iteration 12840 Training loss 0.1098284050822258 Validation loss 0.1091117113828659 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.5012],\n",
      "        [0.5976]], device='mps:0')\n",
      "Iteration 12850 Training loss 0.10862597823143005 Validation loss 0.10914194583892822 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.3257],\n",
      "        [0.3344]], device='mps:0')\n",
      "Iteration 12860 Training loss 0.11376269161701202 Validation loss 0.10917836427688599 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.6403],\n",
      "        [0.6774]], device='mps:0')\n",
      "Iteration 12870 Training loss 0.10664771497249603 Validation loss 0.10914087295532227 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.6944],\n",
      "        [0.5221]], device='mps:0')\n",
      "Iteration 12880 Training loss 0.10686749219894409 Validation loss 0.10915373265743256 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.4920],\n",
      "        [0.6112]], device='mps:0')\n",
      "Iteration 12890 Training loss 0.11382806301116943 Validation loss 0.10911288112401962 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.5158],\n",
      "        [0.5161]], device='mps:0')\n",
      "Iteration 12900 Training loss 0.10975427180528641 Validation loss 0.10916096717119217 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.5635],\n",
      "        [0.5168]], device='mps:0')\n",
      "Iteration 12910 Training loss 0.11118682473897934 Validation loss 0.10911073535680771 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.6176],\n",
      "        [0.5999]], device='mps:0')\n",
      "Iteration 12920 Training loss 0.10845457017421722 Validation loss 0.1091596782207489 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.6311],\n",
      "        [0.7601]], device='mps:0')\n",
      "Iteration 12930 Training loss 0.11472342908382416 Validation loss 0.10919927060604095 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.6075],\n",
      "        [0.5916]], device='mps:0')\n",
      "Iteration 12940 Training loss 0.10750734061002731 Validation loss 0.10918337106704712 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.5238],\n",
      "        [0.5597]], device='mps:0')\n",
      "Iteration 12950 Training loss 0.11363861709833145 Validation loss 0.10909700393676758 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.2529],\n",
      "        [0.4550]], device='mps:0')\n",
      "Iteration 12960 Training loss 0.10774926096200943 Validation loss 0.10909722745418549 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.5123],\n",
      "        [0.5915]], device='mps:0')\n",
      "Iteration 12970 Training loss 0.10654625296592712 Validation loss 0.10906792432069778 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.5243],\n",
      "        [0.4036]], device='mps:0')\n",
      "Iteration 12980 Training loss 0.11442670226097107 Validation loss 0.10909236967563629 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.4472],\n",
      "        [0.5987]], device='mps:0')\n",
      "Iteration 12990 Training loss 0.0968094915151596 Validation loss 0.10908029973506927 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.6226],\n",
      "        [0.4354]], device='mps:0')\n",
      "Iteration 13000 Training loss 0.11211372166872025 Validation loss 0.10906093567609787 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.4949],\n",
      "        [0.5015]], device='mps:0')\n",
      "Iteration 13010 Training loss 0.10980530828237534 Validation loss 0.10906622558832169 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.6111],\n",
      "        [0.4429]], device='mps:0')\n",
      "Iteration 13020 Training loss 0.11058175563812256 Validation loss 0.1090657040476799 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.4150],\n",
      "        [0.6128]], device='mps:0')\n",
      "Iteration 13030 Training loss 0.1106949970126152 Validation loss 0.1090531051158905 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.6081],\n",
      "        [0.6294]], device='mps:0')\n",
      "Iteration 13040 Training loss 0.11123599112033844 Validation loss 0.10902135074138641 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.5331],\n",
      "        [0.3799]], device='mps:0')\n",
      "Iteration 13050 Training loss 0.1035897433757782 Validation loss 0.10900261998176575 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.2326],\n",
      "        [0.5565]], device='mps:0')\n",
      "Iteration 13060 Training loss 0.11459386348724365 Validation loss 0.10903201252222061 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.5618],\n",
      "        [0.6234]], device='mps:0')\n",
      "Iteration 13070 Training loss 0.10412683337926865 Validation loss 0.10901512950658798 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.2887],\n",
      "        [0.4525]], device='mps:0')\n",
      "Iteration 13080 Training loss 0.10470765829086304 Validation loss 0.10903452336788177 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.5252],\n",
      "        [0.6547]], device='mps:0')\n",
      "Iteration 13090 Training loss 0.109071746468544 Validation loss 0.10900935530662537 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.4681],\n",
      "        [0.6038]], device='mps:0')\n",
      "Iteration 13100 Training loss 0.12173041701316833 Validation loss 0.10901091247797012 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.6062],\n",
      "        [0.1908]], device='mps:0')\n",
      "Iteration 13110 Training loss 0.10458873212337494 Validation loss 0.10898713022470474 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.6176],\n",
      "        [0.5228]], device='mps:0')\n",
      "Iteration 13120 Training loss 0.1050223633646965 Validation loss 0.10898855328559875 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.6250],\n",
      "        [0.6437]], device='mps:0')\n",
      "Iteration 13130 Training loss 0.1053958460688591 Validation loss 0.1090097725391388 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.4658],\n",
      "        [0.5650]], device='mps:0')\n",
      "Iteration 13140 Training loss 0.10519267618656158 Validation loss 0.10896820574998856 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.2030],\n",
      "        [0.4688]], device='mps:0')\n",
      "Iteration 13150 Training loss 0.1103801503777504 Validation loss 0.10895569622516632 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.5278],\n",
      "        [0.4721]], device='mps:0')\n",
      "Iteration 13160 Training loss 0.11037356406450272 Validation loss 0.10893291980028152 Accuracy 0.674500048160553\n",
      "Output tensor([[0.6019],\n",
      "        [0.5877]], device='mps:0')\n",
      "Iteration 13170 Training loss 0.10794973373413086 Validation loss 0.10893844813108444 Accuracy 0.6765000224113464\n",
      "Output tensor([[0.4640],\n",
      "        [0.5361]], device='mps:0')\n",
      "Iteration 13180 Training loss 0.10932394117116928 Validation loss 0.10892369598150253 Accuracy 0.674500048160553\n",
      "Output tensor([[0.5208],\n",
      "        [0.1876]], device='mps:0')\n",
      "Iteration 13190 Training loss 0.11794408410787582 Validation loss 0.10891856253147125 Accuracy 0.675000011920929\n",
      "Output tensor([[0.5250],\n",
      "        [0.4637]], device='mps:0')\n",
      "Iteration 13200 Training loss 0.10670694708824158 Validation loss 0.10891980677843094 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.5481],\n",
      "        [0.3716]], device='mps:0')\n",
      "Iteration 13210 Training loss 0.1079481691122055 Validation loss 0.10895569622516632 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.4657],\n",
      "        [0.6328]], device='mps:0')\n",
      "Iteration 13220 Training loss 0.11771857738494873 Validation loss 0.1089891865849495 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.6335],\n",
      "        [0.5107]], device='mps:0')\n",
      "Iteration 13230 Training loss 0.11170133203268051 Validation loss 0.10898381471633911 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.2707],\n",
      "        [0.5338]], device='mps:0')\n",
      "Iteration 13240 Training loss 0.10137089341878891 Validation loss 0.10894438624382019 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.5788],\n",
      "        [0.3847]], device='mps:0')\n",
      "Iteration 13250 Training loss 0.11308223754167557 Validation loss 0.10889589786529541 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.6194],\n",
      "        [0.4165]], device='mps:0')\n",
      "Iteration 13260 Training loss 0.10552862286567688 Validation loss 0.10897103697061539 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.6165],\n",
      "        [0.6585]], device='mps:0')\n",
      "Iteration 13270 Training loss 0.11745557188987732 Validation loss 0.10890010744333267 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.4082],\n",
      "        [0.5258]], device='mps:0')\n",
      "Iteration 13280 Training loss 0.11061536520719528 Validation loss 0.10888116806745529 Accuracy 0.6765000224113464\n",
      "Output tensor([[0.3099],\n",
      "        [0.6029]], device='mps:0')\n",
      "Iteration 13290 Training loss 0.10983012616634369 Validation loss 0.10888154804706573 Accuracy 0.6765000224113464\n",
      "Output tensor([[0.3343],\n",
      "        [0.4149]], device='mps:0')\n",
      "Iteration 13300 Training loss 0.10751567780971527 Validation loss 0.10889409482479095 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.4699],\n",
      "        [0.5143]], device='mps:0')\n",
      "Iteration 13310 Training loss 0.11433521658182144 Validation loss 0.10894721746444702 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.4458],\n",
      "        [0.3482]], device='mps:0')\n",
      "Iteration 13320 Training loss 0.1209174394607544 Validation loss 0.1089605912566185 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.5027],\n",
      "        [0.4281]], device='mps:0')\n",
      "Iteration 13330 Training loss 0.11587364226579666 Validation loss 0.1088922917842865 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.5237],\n",
      "        [0.5736]], device='mps:0')\n",
      "Iteration 13340 Training loss 0.11571517586708069 Validation loss 0.10886234790086746 Accuracy 0.6760000586509705\n",
      "Output tensor([[0.5174],\n",
      "        [0.4948]], device='mps:0')\n",
      "Iteration 13350 Training loss 0.1148749589920044 Validation loss 0.1088465228676796 Accuracy 0.674500048160553\n",
      "Output tensor([[0.6334],\n",
      "        [0.5589]], device='mps:0')\n",
      "Iteration 13360 Training loss 0.11594077199697495 Validation loss 0.1088418960571289 Accuracy 0.674500048160553\n",
      "Output tensor([[0.6589],\n",
      "        [0.4487]], device='mps:0')\n",
      "Iteration 13370 Training loss 0.11260855197906494 Validation loss 0.1088552325963974 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.4719],\n",
      "        [0.6716]], device='mps:0')\n",
      "Iteration 13380 Training loss 0.10182544589042664 Validation loss 0.1088494062423706 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.5635],\n",
      "        [0.4306]], device='mps:0')\n",
      "Iteration 13390 Training loss 0.11101201176643372 Validation loss 0.10887308418750763 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.4956],\n",
      "        [0.5303]], device='mps:0')\n",
      "Iteration 13400 Training loss 0.11781619489192963 Validation loss 0.10887180268764496 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.4413],\n",
      "        [0.6684]], device='mps:0')\n",
      "Iteration 13410 Training loss 0.1169959232211113 Validation loss 0.1088961809873581 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.5177],\n",
      "        [0.4478]], device='mps:0')\n",
      "Iteration 13420 Training loss 0.11550409346818924 Validation loss 0.10881456732749939 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.2391],\n",
      "        [0.4665]], device='mps:0')\n",
      "Iteration 13430 Training loss 0.10127774626016617 Validation loss 0.10881396383047104 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.4592],\n",
      "        [0.3604]], device='mps:0')\n",
      "Iteration 13440 Training loss 0.11537715047597885 Validation loss 0.10881050676107407 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.6576],\n",
      "        [0.5120]], device='mps:0')\n",
      "Iteration 13450 Training loss 0.1088317483663559 Validation loss 0.10880514979362488 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.6244],\n",
      "        [0.4924]], device='mps:0')\n",
      "Iteration 13460 Training loss 0.1118999719619751 Validation loss 0.10879546403884888 Accuracy 0.6755000352859497\n",
      "Output tensor([[0.5680],\n",
      "        [0.3833]], device='mps:0')\n",
      "Iteration 13470 Training loss 0.11621448397636414 Validation loss 0.10879411548376083 Accuracy 0.6760000586509705\n",
      "Output tensor([[0.1789],\n",
      "        [0.5907]], device='mps:0')\n",
      "Iteration 13480 Training loss 0.10447873920202255 Validation loss 0.10879842191934586 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.5828],\n",
      "        [0.4938]], device='mps:0')\n",
      "Iteration 13490 Training loss 0.10891877859830856 Validation loss 0.10878653824329376 Accuracy 0.6765000224113464\n",
      "Output tensor([[0.3058],\n",
      "        [0.4262]], device='mps:0')\n",
      "Iteration 13500 Training loss 0.11989042162895203 Validation loss 0.10878381133079529 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.4779],\n",
      "        [0.5310]], device='mps:0')\n",
      "Iteration 13510 Training loss 0.10754323750734329 Validation loss 0.10877653956413269 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.3968],\n",
      "        [0.4406]], device='mps:0')\n",
      "Iteration 13520 Training loss 0.11436840146780014 Validation loss 0.10877478122711182 Accuracy 0.6770000457763672\n",
      "Output tensor([[0.6106],\n",
      "        [0.3850]], device='mps:0')\n",
      "Iteration 13530 Training loss 0.10688531398773193 Validation loss 0.10877100378274918 Accuracy 0.6760000586509705\n",
      "Output tensor([[0.5636],\n",
      "        [0.3871]], device='mps:0')\n",
      "Iteration 13540 Training loss 0.1055544838309288 Validation loss 0.10881198197603226 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.2772],\n",
      "        [0.5799]], device='mps:0')\n",
      "Iteration 13550 Training loss 0.11316787451505661 Validation loss 0.10880044847726822 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.3740],\n",
      "        [0.5244]], device='mps:0')\n",
      "Iteration 13560 Training loss 0.11521010100841522 Validation loss 0.10882189869880676 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.6603],\n",
      "        [0.5226]], device='mps:0')\n",
      "Iteration 13570 Training loss 0.11307472735643387 Validation loss 0.10876639187335968 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.4552],\n",
      "        [0.5435]], device='mps:0')\n",
      "Iteration 13580 Training loss 0.10304464399814606 Validation loss 0.10876486450433731 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.6025],\n",
      "        [0.3941]], device='mps:0')\n",
      "Iteration 13590 Training loss 0.1042807400226593 Validation loss 0.10879477858543396 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.3269],\n",
      "        [0.1110]], device='mps:0')\n",
      "Iteration 13600 Training loss 0.10927844047546387 Validation loss 0.10875838249921799 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.3199],\n",
      "        [0.7326]], device='mps:0')\n",
      "Iteration 13610 Training loss 0.10482688993215561 Validation loss 0.10876379907131195 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.5975],\n",
      "        [0.1750]], device='mps:0')\n",
      "Iteration 13620 Training loss 0.10498136281967163 Validation loss 0.1087433472275734 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.4977],\n",
      "        [0.6773]], device='mps:0')\n",
      "Iteration 13630 Training loss 0.11017186939716339 Validation loss 0.1087372750043869 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.5243],\n",
      "        [0.5615]], device='mps:0')\n",
      "Iteration 13640 Training loss 0.1094842404127121 Validation loss 0.10872448980808258 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.5082],\n",
      "        [0.2875]], device='mps:0')\n",
      "Iteration 13650 Training loss 0.11296864598989487 Validation loss 0.10872828960418701 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.5628],\n",
      "        [0.5537]], device='mps:0')\n",
      "Iteration 13660 Training loss 0.11426558345556259 Validation loss 0.10872917622327805 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.6354],\n",
      "        [0.5631]], device='mps:0')\n",
      "Iteration 13670 Training loss 0.1074988916516304 Validation loss 0.10871626436710358 Accuracy 0.6765000224113464\n",
      "Output tensor([[0.5493],\n",
      "        [0.2761]], device='mps:0')\n",
      "Iteration 13680 Training loss 0.1144837960600853 Validation loss 0.10870625078678131 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.5784],\n",
      "        [0.4728]], device='mps:0')\n",
      "Iteration 13690 Training loss 0.11119386553764343 Validation loss 0.10870352387428284 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.5367],\n",
      "        [0.4565]], device='mps:0')\n",
      "Iteration 13700 Training loss 0.11012683808803558 Validation loss 0.10869459807872772 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.3088],\n",
      "        [0.1965]], device='mps:0')\n",
      "Iteration 13710 Training loss 0.10751449316740036 Validation loss 0.1087091863155365 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.4960],\n",
      "        [0.4889]], device='mps:0')\n",
      "Iteration 13720 Training loss 0.10524932295084 Validation loss 0.10872478038072586 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.3303],\n",
      "        [0.4333]], device='mps:0')\n",
      "Iteration 13730 Training loss 0.10925012081861496 Validation loss 0.10870206356048584 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.5367],\n",
      "        [0.3465]], device='mps:0')\n",
      "Iteration 13740 Training loss 0.10662618279457092 Validation loss 0.10870420187711716 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.6087],\n",
      "        [0.2661]], device='mps:0')\n",
      "Iteration 13750 Training loss 0.1103835180401802 Validation loss 0.10868792980909348 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.4642],\n",
      "        [0.5807]], device='mps:0')\n",
      "Iteration 13760 Training loss 0.10549823194742203 Validation loss 0.10868573933839798 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.5060],\n",
      "        [0.5886]], device='mps:0')\n",
      "Iteration 13770 Training loss 0.10903441905975342 Validation loss 0.10866016149520874 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.4167],\n",
      "        [0.3694]], device='mps:0')\n",
      "Iteration 13780 Training loss 0.10433416068553925 Validation loss 0.10865187644958496 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.4551],\n",
      "        [0.2373]], device='mps:0')\n",
      "Iteration 13790 Training loss 0.11068215221166611 Validation loss 0.10866007208824158 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.4585],\n",
      "        [0.2982]], device='mps:0')\n",
      "Iteration 13800 Training loss 0.10869047790765762 Validation loss 0.10864018648862839 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.5095],\n",
      "        [0.4909]], device='mps:0')\n",
      "Iteration 13810 Training loss 0.11804687231779099 Validation loss 0.10863514989614487 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.4491],\n",
      "        [0.5672]], device='mps:0')\n",
      "Iteration 13820 Training loss 0.11092884093523026 Validation loss 0.10863718390464783 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.4444],\n",
      "        [0.6357]], device='mps:0')\n",
      "Iteration 13830 Training loss 0.11183974146842957 Validation loss 0.10865779966115952 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.4028],\n",
      "        [0.4363]], device='mps:0')\n",
      "Iteration 13840 Training loss 0.10592992603778839 Validation loss 0.10867150872945786 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.5310],\n",
      "        [0.4818]], device='mps:0')\n",
      "Iteration 13850 Training loss 0.1168932095170021 Validation loss 0.10869894921779633 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.2657],\n",
      "        [0.2191]], device='mps:0')\n",
      "Iteration 13860 Training loss 0.10293933749198914 Validation loss 0.10864868760108948 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.6197],\n",
      "        [0.6127]], device='mps:0')\n",
      "Iteration 13870 Training loss 0.1183403953909874 Validation loss 0.10860989987850189 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.5412],\n",
      "        [0.4784]], device='mps:0')\n",
      "Iteration 13880 Training loss 0.1064448431134224 Validation loss 0.10861232876777649 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.6491],\n",
      "        [0.5964]], device='mps:0')\n",
      "Iteration 13890 Training loss 0.112735316157341 Validation loss 0.10860094428062439 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.4429],\n",
      "        [0.4334]], device='mps:0')\n",
      "Iteration 13900 Training loss 0.11093772947788239 Validation loss 0.10859732329845428 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.6030],\n",
      "        [0.6107]], device='mps:0')\n",
      "Iteration 13910 Training loss 0.1103159487247467 Validation loss 0.1086001992225647 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.5115],\n",
      "        [0.6480]], device='mps:0')\n",
      "Iteration 13920 Training loss 0.10634148120880127 Validation loss 0.10862082242965698 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.5917],\n",
      "        [0.4460]], device='mps:0')\n",
      "Iteration 13930 Training loss 0.11062746495008469 Validation loss 0.10867685824632645 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.4205],\n",
      "        [0.6522]], device='mps:0')\n",
      "Iteration 13940 Training loss 0.10327448695898056 Validation loss 0.10870867222547531 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.5579],\n",
      "        [0.2895]], device='mps:0')\n",
      "Iteration 13950 Training loss 0.11490479856729507 Validation loss 0.10873077809810638 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.4430],\n",
      "        [0.6623]], device='mps:0')\n",
      "Iteration 13960 Training loss 0.1103566437959671 Validation loss 0.10866279900074005 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.6324],\n",
      "        [0.5667]], device='mps:0')\n",
      "Iteration 13970 Training loss 0.11078255623579025 Validation loss 0.10869795083999634 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.6811],\n",
      "        [0.5858]], device='mps:0')\n",
      "Iteration 13980 Training loss 0.11080973595380783 Validation loss 0.10862433165311813 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.6760],\n",
      "        [0.5017]], device='mps:0')\n",
      "Iteration 13990 Training loss 0.12038565427064896 Validation loss 0.10858418792486191 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.4422],\n",
      "        [0.5072]], device='mps:0')\n",
      "Iteration 14000 Training loss 0.1146988496184349 Validation loss 0.10858245939016342 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.4834],\n",
      "        [0.4193]], device='mps:0')\n",
      "Iteration 14010 Training loss 0.10886196792125702 Validation loss 0.10858047753572464 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.4773],\n",
      "        [0.6319]], device='mps:0')\n",
      "Iteration 14020 Training loss 0.11255179345607758 Validation loss 0.10855051130056381 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.6323],\n",
      "        [0.5849]], device='mps:0')\n",
      "Iteration 14030 Training loss 0.10967276990413666 Validation loss 0.10854832828044891 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.6672],\n",
      "        [0.3107]], device='mps:0')\n",
      "Iteration 14040 Training loss 0.0998605489730835 Validation loss 0.10856641083955765 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.3596],\n",
      "        [0.6491]], device='mps:0')\n",
      "Iteration 14050 Training loss 0.10692223906517029 Validation loss 0.10854390263557434 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.5245],\n",
      "        [0.6544]], device='mps:0')\n",
      "Iteration 14060 Training loss 0.0993884727358818 Validation loss 0.10853276401758194 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.6288],\n",
      "        [0.6137]], device='mps:0')\n",
      "Iteration 14070 Training loss 0.10065139830112457 Validation loss 0.10852480679750443 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.6112],\n",
      "        [0.6303]], device='mps:0')\n",
      "Iteration 14080 Training loss 0.10715225338935852 Validation loss 0.1085205078125 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.5231],\n",
      "        [0.4987]], device='mps:0')\n",
      "Iteration 14090 Training loss 0.11287447065114975 Validation loss 0.10851145535707474 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.4825],\n",
      "        [0.4865]], device='mps:0')\n",
      "Iteration 14100 Training loss 0.10909940302371979 Validation loss 0.10850640386343002 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.4914],\n",
      "        [0.4945]], device='mps:0')\n",
      "Iteration 14110 Training loss 0.1086670309305191 Validation loss 0.10850406438112259 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.3202],\n",
      "        [0.4779]], device='mps:0')\n",
      "Iteration 14120 Training loss 0.11179746687412262 Validation loss 0.10850745439529419 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.5909],\n",
      "        [0.2800]], device='mps:0')\n",
      "Iteration 14130 Training loss 0.11936046183109283 Validation loss 0.10850300639867783 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.6353],\n",
      "        [0.4926]], device='mps:0')\n",
      "Iteration 14140 Training loss 0.10699474066495895 Validation loss 0.10859666764736176 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.5776],\n",
      "        [0.6084]], device='mps:0')\n",
      "Iteration 14150 Training loss 0.11032204329967499 Validation loss 0.10852445662021637 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.5789],\n",
      "        [0.2125]], device='mps:0')\n",
      "Iteration 14160 Training loss 0.11558446288108826 Validation loss 0.10849170386791229 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.5288],\n",
      "        [0.5858]], device='mps:0')\n",
      "Iteration 14170 Training loss 0.10747869312763214 Validation loss 0.10848331451416016 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.6074],\n",
      "        [0.4186]], device='mps:0')\n",
      "Iteration 14180 Training loss 0.11334364861249924 Validation loss 0.10847795754671097 Accuracy 0.6780000329017639\n",
      "Output tensor([[0.2340],\n",
      "        [0.4948]], device='mps:0')\n",
      "Iteration 14190 Training loss 0.11631325632333755 Validation loss 0.10847526788711548 Accuracy 0.6775000095367432\n",
      "Output tensor([[0.2784],\n",
      "        [0.4336]], device='mps:0')\n",
      "Iteration 14200 Training loss 0.10415339469909668 Validation loss 0.10846228897571564 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.5835],\n",
      "        [0.6222]], device='mps:0')\n",
      "Iteration 14210 Training loss 0.11168442666530609 Validation loss 0.10847444832324982 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.4605],\n",
      "        [0.6336]], device='mps:0')\n",
      "Iteration 14220 Training loss 0.10716716200113297 Validation loss 0.10847754776477814 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.5072],\n",
      "        [0.5577]], device='mps:0')\n",
      "Iteration 14230 Training loss 0.11239553242921829 Validation loss 0.10847166925668716 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.6530],\n",
      "        [0.4374]], device='mps:0')\n",
      "Iteration 14240 Training loss 0.11197502911090851 Validation loss 0.10845667123794556 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.5556],\n",
      "        [0.3435]], device='mps:0')\n",
      "Iteration 14250 Training loss 0.10527142882347107 Validation loss 0.10846149921417236 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.4453],\n",
      "        [0.4741]], device='mps:0')\n",
      "Iteration 14260 Training loss 0.11327928304672241 Validation loss 0.10845036804676056 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.2126],\n",
      "        [0.6050]], device='mps:0')\n",
      "Iteration 14270 Training loss 0.10473436117172241 Validation loss 0.10843338817358017 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.3362],\n",
      "        [0.3863]], device='mps:0')\n",
      "Iteration 14280 Training loss 0.10556892305612564 Validation loss 0.10846178233623505 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.4519],\n",
      "        [0.3462]], device='mps:0')\n",
      "Iteration 14290 Training loss 0.11304501444101334 Validation loss 0.1084267646074295 Accuracy 0.6785000562667847\n",
      "Output tensor([[0.5938],\n",
      "        [0.6134]], device='mps:0')\n",
      "Iteration 14300 Training loss 0.10411475598812103 Validation loss 0.10843515396118164 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.2978],\n",
      "        [0.6381]], device='mps:0')\n",
      "Iteration 14310 Training loss 0.10724269598722458 Validation loss 0.10842352360486984 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.5040],\n",
      "        [0.6542]], device='mps:0')\n",
      "Iteration 14320 Training loss 0.11169598996639252 Validation loss 0.10843680799007416 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.6013],\n",
      "        [0.5452]], device='mps:0')\n",
      "Iteration 14330 Training loss 0.1070372685790062 Validation loss 0.10842213779687881 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.5695],\n",
      "        [0.2524]], device='mps:0')\n",
      "Iteration 14340 Training loss 0.11053107678890228 Validation loss 0.10842497646808624 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.4219],\n",
      "        [0.4364]], device='mps:0')\n",
      "Iteration 14350 Training loss 0.11786199361085892 Validation loss 0.10843420773744583 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.6756],\n",
      "        [0.1580]], device='mps:0')\n",
      "Iteration 14360 Training loss 0.11451385170221329 Validation loss 0.10844331234693527 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.3368],\n",
      "        [0.7123]], device='mps:0')\n",
      "Iteration 14370 Training loss 0.10922394692897797 Validation loss 0.1084352433681488 Accuracy 0.6790000200271606\n",
      "Output tensor([[0.2314],\n",
      "        [0.6246]], device='mps:0')\n",
      "Iteration 14380 Training loss 0.11591722071170807 Validation loss 0.1084396168589592 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.5914],\n",
      "        [0.5700]], device='mps:0')\n",
      "Iteration 14390 Training loss 0.11200353503227234 Validation loss 0.1083921268582344 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.5264],\n",
      "        [0.5063]], device='mps:0')\n",
      "Iteration 14400 Training loss 0.11317066103219986 Validation loss 0.10840277373790741 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.5342],\n",
      "        [0.6352]], device='mps:0')\n",
      "Iteration 14410 Training loss 0.11720842868089676 Validation loss 0.10838501155376434 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.5424],\n",
      "        [0.5519]], device='mps:0')\n",
      "Iteration 14420 Training loss 0.1067381426692009 Validation loss 0.1084057092666626 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.2595],\n",
      "        [0.4968]], device='mps:0')\n",
      "Iteration 14430 Training loss 0.10713540017604828 Validation loss 0.10838150233030319 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.4619],\n",
      "        [0.6104]], device='mps:0')\n",
      "Iteration 14440 Training loss 0.10421112179756165 Validation loss 0.10842281579971313 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.4441],\n",
      "        [0.5947]], device='mps:0')\n",
      "Iteration 14450 Training loss 0.10123103857040405 Validation loss 0.10848594456911087 Accuracy 0.6845000386238098\n",
      "Output tensor([[0.4438],\n",
      "        [0.5971]], device='mps:0')\n",
      "Iteration 14460 Training loss 0.09533116966485977 Validation loss 0.1084650307893753 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.2528],\n",
      "        [0.6192]], device='mps:0')\n",
      "Iteration 14470 Training loss 0.10926245152950287 Validation loss 0.10841213166713715 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.4444],\n",
      "        [0.3923]], device='mps:0')\n",
      "Iteration 14480 Training loss 0.1100081205368042 Validation loss 0.10834358632564545 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.3500],\n",
      "        [0.4976]], device='mps:0')\n",
      "Iteration 14490 Training loss 0.1078522726893425 Validation loss 0.10833217948675156 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.4943],\n",
      "        [0.4097]], device='mps:0')\n",
      "Iteration 14500 Training loss 0.1062675416469574 Validation loss 0.10835492610931396 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.5774],\n",
      "        [0.2735]], device='mps:0')\n",
      "Iteration 14510 Training loss 0.10503259301185608 Validation loss 0.1083403080701828 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.5570],\n",
      "        [0.5542]], device='mps:0')\n",
      "Iteration 14520 Training loss 0.10697811841964722 Validation loss 0.10832369327545166 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.5760],\n",
      "        [0.5530]], device='mps:0')\n",
      "Iteration 14530 Training loss 0.1087159588932991 Validation loss 0.10832136869430542 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.4342],\n",
      "        [0.5299]], device='mps:0')\n",
      "Iteration 14540 Training loss 0.10524461418390274 Validation loss 0.10833771526813507 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.5528],\n",
      "        [0.2609]], device='mps:0')\n",
      "Iteration 14550 Training loss 0.1098504438996315 Validation loss 0.10832808166742325 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.3650],\n",
      "        [0.5447]], device='mps:0')\n",
      "Iteration 14560 Training loss 0.10401307046413422 Validation loss 0.10834123939275742 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.6119],\n",
      "        [0.4497]], device='mps:0')\n",
      "Iteration 14570 Training loss 0.10640010237693787 Validation loss 0.10832735896110535 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.4313],\n",
      "        [0.6402]], device='mps:0')\n",
      "Iteration 14580 Training loss 0.11373978108167648 Validation loss 0.10833270102739334 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.5160],\n",
      "        [0.4068]], device='mps:0')\n",
      "Iteration 14590 Training loss 0.11287128925323486 Validation loss 0.1082892119884491 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.2998],\n",
      "        [0.6828]], device='mps:0')\n",
      "Iteration 14600 Training loss 0.10280885547399521 Validation loss 0.10827548801898956 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.4814],\n",
      "        [0.5318]], device='mps:0')\n",
      "Iteration 14610 Training loss 0.10902002453804016 Validation loss 0.1082700714468956 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.2217],\n",
      "        [0.5610]], device='mps:0')\n",
      "Iteration 14620 Training loss 0.11025924235582352 Validation loss 0.10827359557151794 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.3823],\n",
      "        [0.5760]], device='mps:0')\n",
      "Iteration 14630 Training loss 0.10592986643314362 Validation loss 0.10826930403709412 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.4806],\n",
      "        [0.1884]], device='mps:0')\n",
      "Iteration 14640 Training loss 0.1212586984038353 Validation loss 0.10825510323047638 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.5551],\n",
      "        [0.4017]], device='mps:0')\n",
      "Iteration 14650 Training loss 0.10858652740716934 Validation loss 0.10825631022453308 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.3628],\n",
      "        [0.2778]], device='mps:0')\n",
      "Iteration 14660 Training loss 0.10756480693817139 Validation loss 0.10825704783201218 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.5656],\n",
      "        [0.5967]], device='mps:0')\n",
      "Iteration 14670 Training loss 0.11465227603912354 Validation loss 0.10826490074396133 Accuracy 0.6795000433921814\n",
      "Output tensor([[0.6401],\n",
      "        [0.6545]], device='mps:0')\n",
      "Iteration 14680 Training loss 0.10772232711315155 Validation loss 0.1082763746380806 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.5247],\n",
      "        [0.5821]], device='mps:0')\n",
      "Iteration 14690 Training loss 0.10635829716920853 Validation loss 0.108268603682518 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.5984],\n",
      "        [0.6412]], device='mps:0')\n",
      "Iteration 14700 Training loss 0.11683622002601624 Validation loss 0.10825790464878082 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.4935],\n",
      "        [0.4474]], device='mps:0')\n",
      "Iteration 14710 Training loss 0.11520151048898697 Validation loss 0.10822565853595734 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.5121],\n",
      "        [0.6573]], device='mps:0')\n",
      "Iteration 14720 Training loss 0.11236867308616638 Validation loss 0.10821669548749924 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.4971],\n",
      "        [0.5138]], device='mps:0')\n",
      "Iteration 14730 Training loss 0.101698137819767 Validation loss 0.10822445899248123 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.1947],\n",
      "        [0.6953]], device='mps:0')\n",
      "Iteration 14740 Training loss 0.1059400737285614 Validation loss 0.10820893198251724 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.2084],\n",
      "        [0.5178]], device='mps:0')\n",
      "Iteration 14750 Training loss 0.10969986021518707 Validation loss 0.10820651054382324 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.6555],\n",
      "        [0.5605]], device='mps:0')\n",
      "Iteration 14760 Training loss 0.11147302389144897 Validation loss 0.10822975635528564 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.7166],\n",
      "        [0.4427]], device='mps:0')\n",
      "Iteration 14770 Training loss 0.10798816382884979 Validation loss 0.10821789503097534 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.5370],\n",
      "        [0.2413]], device='mps:0')\n",
      "Iteration 14780 Training loss 0.1086319163441658 Validation loss 0.10821115970611572 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.6154],\n",
      "        [0.4604]], device='mps:0')\n",
      "Iteration 14790 Training loss 0.1108967661857605 Validation loss 0.10821673274040222 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.5956],\n",
      "        [0.3718]], device='mps:0')\n",
      "Iteration 14800 Training loss 0.11138324439525604 Validation loss 0.10820911079645157 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.5572],\n",
      "        [0.6286]], device='mps:0')\n",
      "Iteration 14810 Training loss 0.11480666697025299 Validation loss 0.10825389623641968 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.6140],\n",
      "        [0.4582]], device='mps:0')\n",
      "Iteration 14820 Training loss 0.10882919281721115 Validation loss 0.10820206999778748 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.2698],\n",
      "        [0.3743]], device='mps:0')\n",
      "Iteration 14830 Training loss 0.10982390493154526 Validation loss 0.10818526148796082 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.3964],\n",
      "        [0.5302]], device='mps:0')\n",
      "Iteration 14840 Training loss 0.11237680166959763 Validation loss 0.10816745460033417 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.6156],\n",
      "        [0.5676]], device='mps:0')\n",
      "Iteration 14850 Training loss 0.11240920424461365 Validation loss 0.1081717312335968 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.3123],\n",
      "        [0.5531]], device='mps:0')\n",
      "Iteration 14860 Training loss 0.11244212836027145 Validation loss 0.10818140208721161 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.5875],\n",
      "        [0.3616]], device='mps:0')\n",
      "Iteration 14870 Training loss 0.10464152693748474 Validation loss 0.10822407156229019 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.1796],\n",
      "        [0.4945]], device='mps:0')\n",
      "Iteration 14880 Training loss 0.1074918806552887 Validation loss 0.10818208009004593 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.6076],\n",
      "        [0.5211]], device='mps:0')\n",
      "Iteration 14890 Training loss 0.10517802834510803 Validation loss 0.10816102474927902 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.3946],\n",
      "        [0.4204]], device='mps:0')\n",
      "Iteration 14900 Training loss 0.12279339879751205 Validation loss 0.10820449143648148 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.6294],\n",
      "        [0.4850]], device='mps:0')\n",
      "Iteration 14910 Training loss 0.10760661214590073 Validation loss 0.10815908014774323 Accuracy 0.6800000071525574\n",
      "Output tensor([[0.5310],\n",
      "        [0.4153]], device='mps:0')\n",
      "Iteration 14920 Training loss 0.11826171725988388 Validation loss 0.10818131268024445 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.3100],\n",
      "        [0.5662]], device='mps:0')\n",
      "Iteration 14930 Training loss 0.10316772013902664 Validation loss 0.10821118205785751 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.6041],\n",
      "        [0.5270]], device='mps:0')\n",
      "Iteration 14940 Training loss 0.10636377334594727 Validation loss 0.10820242017507553 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.6020],\n",
      "        [0.5500]], device='mps:0')\n",
      "Iteration 14950 Training loss 0.11365015804767609 Validation loss 0.1081855297088623 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.6380],\n",
      "        [0.5512]], device='mps:0')\n",
      "Iteration 14960 Training loss 0.09256294369697571 Validation loss 0.10824672132730484 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.4211],\n",
      "        [0.4849]], device='mps:0')\n",
      "Iteration 14970 Training loss 0.10545394569635391 Validation loss 0.10819698870182037 Accuracy 0.6845000386238098\n",
      "Output tensor([[0.4154],\n",
      "        [0.4852]], device='mps:0')\n",
      "Iteration 14980 Training loss 0.10161271691322327 Validation loss 0.10812336951494217 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.4346],\n",
      "        [0.4630]], device='mps:0')\n",
      "Iteration 14990 Training loss 0.1096113845705986 Validation loss 0.10810580104589462 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.3155],\n",
      "        [0.5650]], device='mps:0')\n",
      "Iteration 15000 Training loss 0.10795039683580399 Validation loss 0.10810510814189911 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.3699],\n",
      "        [0.5508]], device='mps:0')\n",
      "Iteration 15010 Training loss 0.1106221079826355 Validation loss 0.10810792446136475 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.6064],\n",
      "        [0.3300]], device='mps:0')\n",
      "Iteration 15020 Training loss 0.11376901715993881 Validation loss 0.1081327497959137 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.4754],\n",
      "        [0.5879]], device='mps:0')\n",
      "Iteration 15030 Training loss 0.11278055608272552 Validation loss 0.10814166814088821 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.3444],\n",
      "        [0.5971]], device='mps:0')\n",
      "Iteration 15040 Training loss 0.11526484042406082 Validation loss 0.10808824747800827 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.6432],\n",
      "        [0.3791]], device='mps:0')\n",
      "Iteration 15050 Training loss 0.10534296929836273 Validation loss 0.10807280242443085 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.2942],\n",
      "        [0.5704]], device='mps:0')\n",
      "Iteration 15060 Training loss 0.10948794335126877 Validation loss 0.10807332396507263 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.5520],\n",
      "        [0.6608]], device='mps:0')\n",
      "Iteration 15070 Training loss 0.11006616055965424 Validation loss 0.10807625204324722 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.2501],\n",
      "        [0.4972]], device='mps:0')\n",
      "Iteration 15080 Training loss 0.10987117886543274 Validation loss 0.10809104144573212 Accuracy 0.6805000305175781\n",
      "Output tensor([[0.3646],\n",
      "        [0.2114]], device='mps:0')\n",
      "Iteration 15090 Training loss 0.09918982535600662 Validation loss 0.10808642953634262 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.4304],\n",
      "        [0.6326]], device='mps:0')\n",
      "Iteration 15100 Training loss 0.10429220646619797 Validation loss 0.10809586197137833 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.6434],\n",
      "        [0.4418]], device='mps:0')\n",
      "Iteration 15110 Training loss 0.11437581479549408 Validation loss 0.10814218968153 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.6020],\n",
      "        [0.4305]], device='mps:0')\n",
      "Iteration 15120 Training loss 0.10009101778268814 Validation loss 0.10817374289035797 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.6142],\n",
      "        [0.7067]], device='mps:0')\n",
      "Iteration 15130 Training loss 0.11449364572763443 Validation loss 0.10812384635210037 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.4537],\n",
      "        [0.3414]], device='mps:0')\n",
      "Iteration 15140 Training loss 0.11490050703287125 Validation loss 0.10811946541070938 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.4729],\n",
      "        [0.6165]], device='mps:0')\n",
      "Iteration 15150 Training loss 0.11197979003190994 Validation loss 0.1080654114484787 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.4522],\n",
      "        [0.3818]], device='mps:0')\n",
      "Iteration 15160 Training loss 0.10675065964460373 Validation loss 0.10803081095218658 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.5289],\n",
      "        [0.2511]], device='mps:0')\n",
      "Iteration 15170 Training loss 0.10358307510614395 Validation loss 0.10802523046731949 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.2506],\n",
      "        [0.7075]], device='mps:0')\n",
      "Iteration 15180 Training loss 0.11456885188817978 Validation loss 0.10805496573448181 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.3679],\n",
      "        [0.5698]], device='mps:0')\n",
      "Iteration 15190 Training loss 0.11319862306118011 Validation loss 0.10804066061973572 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.4300],\n",
      "        [0.5333]], device='mps:0')\n",
      "Iteration 15200 Training loss 0.1095672994852066 Validation loss 0.10807979106903076 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.4558],\n",
      "        [0.5361]], device='mps:0')\n",
      "Iteration 15210 Training loss 0.10392457991838455 Validation loss 0.10807035118341446 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.4519],\n",
      "        [0.5570]], device='mps:0')\n",
      "Iteration 15220 Training loss 0.10503139346837997 Validation loss 0.1080661416053772 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.3965],\n",
      "        [0.2546]], device='mps:0')\n",
      "Iteration 15230 Training loss 0.10411273688077927 Validation loss 0.10805539786815643 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.4835],\n",
      "        [0.6269]], device='mps:0')\n",
      "Iteration 15240 Training loss 0.10715121775865555 Validation loss 0.10804244130849838 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.5755],\n",
      "        [0.5804]], device='mps:0')\n",
      "Iteration 15250 Training loss 0.10024432092905045 Validation loss 0.10806131362915039 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.6094],\n",
      "        [0.4536]], device='mps:0')\n",
      "Iteration 15260 Training loss 0.10005154460668564 Validation loss 0.10805345326662064 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.4662],\n",
      "        [0.6721]], device='mps:0')\n",
      "Iteration 15270 Training loss 0.11052878946065903 Validation loss 0.1080220565199852 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.5093],\n",
      "        [0.3808]], device='mps:0')\n",
      "Iteration 15280 Training loss 0.11134059727191925 Validation loss 0.10801339149475098 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.7401],\n",
      "        [0.5409]], device='mps:0')\n",
      "Iteration 15290 Training loss 0.11455059051513672 Validation loss 0.10799983143806458 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.3309],\n",
      "        [0.5104]], device='mps:0')\n",
      "Iteration 15300 Training loss 0.10212722420692444 Validation loss 0.10798679292201996 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.4108],\n",
      "        [0.4530]], device='mps:0')\n",
      "Iteration 15310 Training loss 0.10998769849538803 Validation loss 0.10803370922803879 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.4552],\n",
      "        [0.3558]], device='mps:0')\n",
      "Iteration 15320 Training loss 0.11093278974294662 Validation loss 0.10799171030521393 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.2954],\n",
      "        [0.3545]], device='mps:0')\n",
      "Iteration 15330 Training loss 0.10487648844718933 Validation loss 0.10798928886651993 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.5381],\n",
      "        [0.5604]], device='mps:0')\n",
      "Iteration 15340 Training loss 0.10967370867729187 Validation loss 0.1079864576458931 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.3302],\n",
      "        [0.6655]], device='mps:0')\n",
      "Iteration 15350 Training loss 0.10850409418344498 Validation loss 0.10797199606895447 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.4770],\n",
      "        [0.6310]], device='mps:0')\n",
      "Iteration 15360 Training loss 0.11450959742069244 Validation loss 0.10797616839408875 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.6110],\n",
      "        [0.4748]], device='mps:0')\n",
      "Iteration 15370 Training loss 0.10420646518468857 Validation loss 0.10795656591653824 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.6505],\n",
      "        [0.4981]], device='mps:0')\n",
      "Iteration 15380 Training loss 0.1026536151766777 Validation loss 0.10796122997999191 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.6342],\n",
      "        [0.6370]], device='mps:0')\n",
      "Iteration 15390 Training loss 0.10644376277923584 Validation loss 0.10794781148433685 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.3860],\n",
      "        [0.5906]], device='mps:0')\n",
      "Iteration 15400 Training loss 0.10114947706460953 Validation loss 0.10799665004014969 Accuracy 0.6845000386238098\n",
      "Output tensor([[0.6639],\n",
      "        [0.6640]], device='mps:0')\n",
      "Iteration 15410 Training loss 0.11026681959629059 Validation loss 0.10798463970422745 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.6287],\n",
      "        [0.4950]], device='mps:0')\n",
      "Iteration 15420 Training loss 0.11384788155555725 Validation loss 0.10797800123691559 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.5740],\n",
      "        [0.3654]], device='mps:0')\n",
      "Iteration 15430 Training loss 0.11315707117319107 Validation loss 0.10797878354787827 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.6883],\n",
      "        [0.5759]], device='mps:0')\n",
      "Iteration 15440 Training loss 0.10767751187086105 Validation loss 0.10806764662265778 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5467],\n",
      "        [0.3145]], device='mps:0')\n",
      "Iteration 15450 Training loss 0.11102531105279922 Validation loss 0.10806700587272644 Accuracy 0.687000036239624\n",
      "Output tensor([[0.4026],\n",
      "        [0.2991]], device='mps:0')\n",
      "Iteration 15460 Training loss 0.10466435551643372 Validation loss 0.10807150602340698 Accuracy 0.687000036239624\n",
      "Output tensor([[0.5514],\n",
      "        [0.4594]], device='mps:0')\n",
      "Iteration 15470 Training loss 0.10870223492383957 Validation loss 0.10820227116346359 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.2962],\n",
      "        [0.5544]], device='mps:0')\n",
      "Iteration 15480 Training loss 0.10662734508514404 Validation loss 0.10813035815954208 Accuracy 0.6845000386238098\n",
      "Output tensor([[0.2820],\n",
      "        [0.6155]], device='mps:0')\n",
      "Iteration 15490 Training loss 0.10009930282831192 Validation loss 0.10807371884584427 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.4093],\n",
      "        [0.6620]], device='mps:0')\n",
      "Iteration 15500 Training loss 0.10883365571498871 Validation loss 0.10802042484283447 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.6410],\n",
      "        [0.5591]], device='mps:0')\n",
      "Iteration 15510 Training loss 0.11153163760900497 Validation loss 0.10796835273504257 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.4836],\n",
      "        [0.5816]], device='mps:0')\n",
      "Iteration 15520 Training loss 0.11032366752624512 Validation loss 0.10792923718690872 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.6465],\n",
      "        [0.2357]], device='mps:0')\n",
      "Iteration 15530 Training loss 0.10005362331867218 Validation loss 0.1079074814915657 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.6275],\n",
      "        [0.5989]], device='mps:0')\n",
      "Iteration 15540 Training loss 0.10675939172506332 Validation loss 0.10788474977016449 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.5420],\n",
      "        [0.3802]], device='mps:0')\n",
      "Iteration 15550 Training loss 0.11771436780691147 Validation loss 0.1079043596982956 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.5068],\n",
      "        [0.4990]], device='mps:0')\n",
      "Iteration 15560 Training loss 0.11102074384689331 Validation loss 0.10788418352603912 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.5273],\n",
      "        [0.2298]], device='mps:0')\n",
      "Iteration 15570 Training loss 0.11453497409820557 Validation loss 0.10787316411733627 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.5296],\n",
      "        [0.5771]], device='mps:0')\n",
      "Iteration 15580 Training loss 0.10611352324485779 Validation loss 0.10787957906723022 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.4798],\n",
      "        [0.7124]], device='mps:0')\n",
      "Iteration 15590 Training loss 0.10950466990470886 Validation loss 0.10786502808332443 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.3295],\n",
      "        [0.5471]], device='mps:0')\n",
      "Iteration 15600 Training loss 0.10579129308462143 Validation loss 0.10788921266794205 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.5954],\n",
      "        [0.2994]], device='mps:0')\n",
      "Iteration 15610 Training loss 0.11077360063791275 Validation loss 0.10787009447813034 Accuracy 0.6810000538825989\n",
      "Output tensor([[0.4811],\n",
      "        [0.5872]], device='mps:0')\n",
      "Iteration 15620 Training loss 0.10914517939090729 Validation loss 0.10786084830760956 Accuracy 0.6820000410079956\n",
      "Output tensor([[0.7006],\n",
      "        [0.3331]], device='mps:0')\n",
      "Iteration 15630 Training loss 0.11224118620157242 Validation loss 0.10785254091024399 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.3665],\n",
      "        [0.4481]], device='mps:0')\n",
      "Iteration 15640 Training loss 0.10267552733421326 Validation loss 0.10784944891929626 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.4818],\n",
      "        [0.4994]], device='mps:0')\n",
      "Iteration 15650 Training loss 0.10876510292291641 Validation loss 0.10785512626171112 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.6221],\n",
      "        [0.5613]], device='mps:0')\n",
      "Iteration 15660 Training loss 0.1052529588341713 Validation loss 0.10784439742565155 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.4087],\n",
      "        [0.4464]], device='mps:0')\n",
      "Iteration 15670 Training loss 0.10629218816757202 Validation loss 0.10784709453582764 Accuracy 0.6815000176429749\n",
      "Output tensor([[0.5064],\n",
      "        [0.2744]], device='mps:0')\n",
      "Iteration 15680 Training loss 0.10836967825889587 Validation loss 0.1078297421336174 Accuracy 0.6845000386238098\n",
      "Output tensor([[0.4158],\n",
      "        [0.4685]], device='mps:0')\n",
      "Iteration 15690 Training loss 0.10949581116437912 Validation loss 0.1078343540430069 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.3513],\n",
      "        [0.6540]], device='mps:0')\n",
      "Iteration 15700 Training loss 0.104066863656044 Validation loss 0.10783784091472626 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.5872],\n",
      "        [0.6831]], device='mps:0')\n",
      "Iteration 15710 Training loss 0.11982186883687973 Validation loss 0.10792253166437149 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.5192],\n",
      "        [0.2307]], device='mps:0')\n",
      "Iteration 15720 Training loss 0.11123098433017731 Validation loss 0.10785263776779175 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.6782],\n",
      "        [0.3649]], device='mps:0')\n",
      "Iteration 15730 Training loss 0.111213818192482 Validation loss 0.10791833698749542 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.5723],\n",
      "        [0.6395]], device='mps:0')\n",
      "Iteration 15740 Training loss 0.10773216187953949 Validation loss 0.10790442675352097 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.4123],\n",
      "        [0.6308]], device='mps:0')\n",
      "Iteration 15750 Training loss 0.1143328845500946 Validation loss 0.10786517709493637 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.4140],\n",
      "        [0.6215]], device='mps:0')\n",
      "Iteration 15760 Training loss 0.10687390714883804 Validation loss 0.10787107795476913 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5595],\n",
      "        [0.6035]], device='mps:0')\n",
      "Iteration 15770 Training loss 0.1129249706864357 Validation loss 0.10793552547693253 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.2654],\n",
      "        [0.5689]], device='mps:0')\n",
      "Iteration 15780 Training loss 0.10383684188127518 Validation loss 0.10791155695915222 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.6549],\n",
      "        [0.5267]], device='mps:0')\n",
      "Iteration 15790 Training loss 0.11760526150465012 Validation loss 0.10789047926664352 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.6246],\n",
      "        [0.6592]], device='mps:0')\n",
      "Iteration 15800 Training loss 0.1057761162519455 Validation loss 0.10783454030752182 Accuracy 0.6850000619888306\n",
      "Output tensor([[0.3954],\n",
      "        [0.5558]], device='mps:0')\n",
      "Iteration 15810 Training loss 0.10176118463277817 Validation loss 0.10779821872711182 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.5519],\n",
      "        [0.5296]], device='mps:0')\n",
      "Iteration 15820 Training loss 0.11806081235408783 Validation loss 0.1077931597828865 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.4044],\n",
      "        [0.3240]], device='mps:0')\n",
      "Iteration 15830 Training loss 0.12082330137491226 Validation loss 0.10780345648527145 Accuracy 0.6845000386238098\n",
      "Output tensor([[0.6600],\n",
      "        [0.5026]], device='mps:0')\n",
      "Iteration 15840 Training loss 0.11326103657484055 Validation loss 0.1078418493270874 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.6108],\n",
      "        [0.5140]], device='mps:0')\n",
      "Iteration 15850 Training loss 0.11545747518539429 Validation loss 0.10777602344751358 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.5320],\n",
      "        [0.4064]], device='mps:0')\n",
      "Iteration 15860 Training loss 0.10119378566741943 Validation loss 0.10776495933532715 Accuracy 0.6825000047683716\n",
      "Output tensor([[0.5674],\n",
      "        [0.5877]], device='mps:0')\n",
      "Iteration 15870 Training loss 0.10301968455314636 Validation loss 0.10775575041770935 Accuracy 0.6845000386238098\n",
      "Output tensor([[0.5946],\n",
      "        [0.4233]], device='mps:0')\n",
      "Iteration 15880 Training loss 0.10851183533668518 Validation loss 0.10777704417705536 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.4959],\n",
      "        [0.5295]], device='mps:0')\n",
      "Iteration 15890 Training loss 0.10531766712665558 Validation loss 0.1077757179737091 Accuracy 0.6850000619888306\n",
      "Output tensor([[0.6086],\n",
      "        [0.6098]], device='mps:0')\n",
      "Iteration 15900 Training loss 0.11065024882555008 Validation loss 0.10775376856327057 Accuracy 0.6845000386238098\n",
      "Output tensor([[0.5405],\n",
      "        [0.5563]], device='mps:0')\n",
      "Iteration 15910 Training loss 0.10590437054634094 Validation loss 0.10774015635251999 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.6105],\n",
      "        [0.5896]], device='mps:0')\n",
      "Iteration 15920 Training loss 0.10443691164255142 Validation loss 0.10772888362407684 Accuracy 0.6850000619888306\n",
      "Output tensor([[0.2993],\n",
      "        [0.5797]], device='mps:0')\n",
      "Iteration 15930 Training loss 0.11075013875961304 Validation loss 0.10772659629583359 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.5003],\n",
      "        [0.4096]], device='mps:0')\n",
      "Iteration 15940 Training loss 0.11605174094438553 Validation loss 0.10771983861923218 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.5485],\n",
      "        [0.3607]], device='mps:0')\n",
      "Iteration 15950 Training loss 0.09876464307308197 Validation loss 0.10772953182458878 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.4561],\n",
      "        [0.5896]], device='mps:0')\n",
      "Iteration 15960 Training loss 0.10707635432481766 Validation loss 0.10777334123849869 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.4194],\n",
      "        [0.6342]], device='mps:0')\n",
      "Iteration 15970 Training loss 0.1161675676703453 Validation loss 0.10777589678764343 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.3989],\n",
      "        [0.5473]], device='mps:0')\n",
      "Iteration 15980 Training loss 0.10238015651702881 Validation loss 0.10774930566549301 Accuracy 0.6850000619888306\n",
      "Output tensor([[0.5170],\n",
      "        [0.5854]], device='mps:0')\n",
      "Iteration 15990 Training loss 0.11459429562091827 Validation loss 0.1077469065785408 Accuracy 0.6850000619888306\n",
      "Output tensor([[0.5027],\n",
      "        [0.5622]], device='mps:0')\n",
      "Iteration 16000 Training loss 0.12211088091135025 Validation loss 0.10775226354598999 Accuracy 0.687000036239624\n",
      "Output tensor([[0.5041],\n",
      "        [0.5853]], device='mps:0')\n",
      "Iteration 16010 Training loss 0.10691119730472565 Validation loss 0.10774480551481247 Accuracy 0.6850000619888306\n",
      "Output tensor([[0.4853],\n",
      "        [0.5697]], device='mps:0')\n",
      "Iteration 16020 Training loss 0.10301000624895096 Validation loss 0.10775482654571533 Accuracy 0.687000036239624\n",
      "Output tensor([[0.6101],\n",
      "        [0.5286]], device='mps:0')\n",
      "Iteration 16030 Training loss 0.11155889928340912 Validation loss 0.10776668041944504 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5501],\n",
      "        [0.4155]], device='mps:0')\n",
      "Iteration 16040 Training loss 0.10488884896039963 Validation loss 0.10775294899940491 Accuracy 0.687000036239624\n",
      "Output tensor([[0.4569],\n",
      "        [0.5141]], device='mps:0')\n",
      "Iteration 16050 Training loss 0.11603434383869171 Validation loss 0.10779727250337601 Accuracy 0.687000036239624\n",
      "Output tensor([[0.5214],\n",
      "        [0.6408]], device='mps:0')\n",
      "Iteration 16060 Training loss 0.10969309508800507 Validation loss 0.10783576220273972 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.5772],\n",
      "        [0.6253]], device='mps:0')\n",
      "Iteration 16070 Training loss 0.10501766949892044 Validation loss 0.10785207897424698 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.4862],\n",
      "        [0.4939]], device='mps:0')\n",
      "Iteration 16080 Training loss 0.11774878948926926 Validation loss 0.10772023350000381 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5595],\n",
      "        [0.5198]], device='mps:0')\n",
      "Iteration 16090 Training loss 0.10551336407661438 Validation loss 0.10769324749708176 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.4378],\n",
      "        [0.4189]], device='mps:0')\n",
      "Iteration 16100 Training loss 0.11684184521436691 Validation loss 0.10769114643335342 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.7152],\n",
      "        [0.5439]], device='mps:0')\n",
      "Iteration 16110 Training loss 0.11624445021152496 Validation loss 0.10766518115997314 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.6037],\n",
      "        [0.4804]], device='mps:0')\n",
      "Iteration 16120 Training loss 0.10883387923240662 Validation loss 0.10764797776937485 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.3123],\n",
      "        [0.4729]], device='mps:0')\n",
      "Iteration 16130 Training loss 0.10750791430473328 Validation loss 0.10764458030462265 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.5683],\n",
      "        [0.6537]], device='mps:0')\n",
      "Iteration 16140 Training loss 0.11000505834817886 Validation loss 0.10766259580850601 Accuracy 0.6850000619888306\n",
      "Output tensor([[0.7064],\n",
      "        [0.5644]], device='mps:0')\n",
      "Iteration 16150 Training loss 0.10755857825279236 Validation loss 0.10764528065919876 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.4982],\n",
      "        [0.6274]], device='mps:0')\n",
      "Iteration 16160 Training loss 0.11274583637714386 Validation loss 0.10767851769924164 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.6288],\n",
      "        [0.6771]], device='mps:0')\n",
      "Iteration 16170 Training loss 0.10578461736440659 Validation loss 0.10767746716737747 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.5628],\n",
      "        [0.5862]], device='mps:0')\n",
      "Iteration 16180 Training loss 0.1053873598575592 Validation loss 0.10775385051965714 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.4190],\n",
      "        [0.2096]], device='mps:0')\n",
      "Iteration 16190 Training loss 0.11067591607570648 Validation loss 0.10769045352935791 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.6254],\n",
      "        [0.4627]], device='mps:0')\n",
      "Iteration 16200 Training loss 0.1122807115316391 Validation loss 0.10765568912029266 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.3821],\n",
      "        [0.6587]], device='mps:0')\n",
      "Iteration 16210 Training loss 0.11072909832000732 Validation loss 0.10764020681381226 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.4532],\n",
      "        [0.4972]], device='mps:0')\n",
      "Iteration 16220 Training loss 0.11110208928585052 Validation loss 0.10760753601789474 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.4600],\n",
      "        [0.5978]], device='mps:0')\n",
      "Iteration 16230 Training loss 0.10492728650569916 Validation loss 0.10761056840419769 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.3206],\n",
      "        [0.5858]], device='mps:0')\n",
      "Iteration 16240 Training loss 0.11579606682062149 Validation loss 0.10760265588760376 Accuracy 0.6830000281333923\n",
      "Output tensor([[0.5320],\n",
      "        [0.6657]], device='mps:0')\n",
      "Iteration 16250 Training loss 0.11442205309867859 Validation loss 0.10765530914068222 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.4877],\n",
      "        [0.4577]], device='mps:0')\n",
      "Iteration 16260 Training loss 0.10761568695306778 Validation loss 0.10767379403114319 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.6746],\n",
      "        [0.5598]], device='mps:0')\n",
      "Iteration 16270 Training loss 0.10535428673028946 Validation loss 0.10769698768854141 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.5701],\n",
      "        [0.3584]], device='mps:0')\n",
      "Iteration 16280 Training loss 0.10923744738101959 Validation loss 0.10764562338590622 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.3515],\n",
      "        [0.6077]], device='mps:0')\n",
      "Iteration 16290 Training loss 0.10486239194869995 Validation loss 0.10762033611536026 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.5084],\n",
      "        [0.5839]], device='mps:0')\n",
      "Iteration 16300 Training loss 0.10647767782211304 Validation loss 0.10759478062391281 Accuracy 0.6850000619888306\n",
      "Output tensor([[0.3942],\n",
      "        [0.2600]], device='mps:0')\n",
      "Iteration 16310 Training loss 0.11137353628873825 Validation loss 0.10759752243757248 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.4669],\n",
      "        [0.5197]], device='mps:0')\n",
      "Iteration 16320 Training loss 0.10191646963357925 Validation loss 0.10758733004331589 Accuracy 0.6845000386238098\n",
      "Output tensor([[0.5720],\n",
      "        [0.4281]], device='mps:0')\n",
      "Iteration 16330 Training loss 0.10549205541610718 Validation loss 0.10757586359977722 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.6221],\n",
      "        [0.5280]], device='mps:0')\n",
      "Iteration 16340 Training loss 0.11005665361881256 Validation loss 0.1075812429189682 Accuracy 0.6850000619888306\n",
      "Output tensor([[0.5846],\n",
      "        [0.5470]], device='mps:0')\n",
      "Iteration 16350 Training loss 0.10654079169034958 Validation loss 0.10757599025964737 Accuracy 0.6850000619888306\n",
      "Output tensor([[0.4982],\n",
      "        [0.4270]], device='mps:0')\n",
      "Iteration 16360 Training loss 0.10281795263290405 Validation loss 0.107578806579113 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.6243],\n",
      "        [0.5683]], device='mps:0')\n",
      "Iteration 16370 Training loss 0.10624873638153076 Validation loss 0.10758025199174881 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.5400],\n",
      "        [0.5830]], device='mps:0')\n",
      "Iteration 16380 Training loss 0.11207401752471924 Validation loss 0.10757356882095337 Accuracy 0.687000036239624\n",
      "Output tensor([[0.2279],\n",
      "        [0.2873]], device='mps:0')\n",
      "Iteration 16390 Training loss 0.11467339843511581 Validation loss 0.10754921287298203 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.2753],\n",
      "        [0.3608]], device='mps:0')\n",
      "Iteration 16400 Training loss 0.11343894898891449 Validation loss 0.10754042118787766 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.3568],\n",
      "        [0.5934]], device='mps:0')\n",
      "Iteration 16410 Training loss 0.10511338710784912 Validation loss 0.10757645219564438 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.3395],\n",
      "        [0.3528]], device='mps:0')\n",
      "Iteration 16420 Training loss 0.10531747341156006 Validation loss 0.10757480561733246 Accuracy 0.687000036239624\n",
      "Output tensor([[0.3881],\n",
      "        [0.4107]], device='mps:0')\n",
      "Iteration 16430 Training loss 0.10784855484962463 Validation loss 0.10752859711647034 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.5683],\n",
      "        [0.4721]], device='mps:0')\n",
      "Iteration 16440 Training loss 0.1117125004529953 Validation loss 0.10753169655799866 Accuracy 0.6850000619888306\n",
      "Output tensor([[0.4019],\n",
      "        [0.2940]], device='mps:0')\n",
      "Iteration 16450 Training loss 0.11876130104064941 Validation loss 0.10755541920661926 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.5295],\n",
      "        [0.5884]], device='mps:0')\n",
      "Iteration 16460 Training loss 0.10347830504179001 Validation loss 0.10754120349884033 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.3907],\n",
      "        [0.3241]], device='mps:0')\n",
      "Iteration 16470 Training loss 0.10904191434383392 Validation loss 0.10756482183933258 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.5922],\n",
      "        [0.4931]], device='mps:0')\n",
      "Iteration 16480 Training loss 0.10967818647623062 Validation loss 0.10755081474781036 Accuracy 0.687000036239624\n",
      "Output tensor([[0.4112],\n",
      "        [0.4198]], device='mps:0')\n",
      "Iteration 16490 Training loss 0.11340231448411942 Validation loss 0.10757675766944885 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.5956],\n",
      "        [0.5543]], device='mps:0')\n",
      "Iteration 16500 Training loss 0.11062534153461456 Validation loss 0.10757902264595032 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.6511],\n",
      "        [0.6006]], device='mps:0')\n",
      "Iteration 16510 Training loss 0.10381391644477844 Validation loss 0.10756148397922516 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.5568],\n",
      "        [0.5595]], device='mps:0')\n",
      "Iteration 16520 Training loss 0.1085372269153595 Validation loss 0.10749518126249313 Accuracy 0.6840000152587891\n",
      "Output tensor([[0.5732],\n",
      "        [0.6682]], device='mps:0')\n",
      "Iteration 16530 Training loss 0.1079539880156517 Validation loss 0.10748223960399628 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.2757],\n",
      "        [0.2550]], device='mps:0')\n",
      "Iteration 16540 Training loss 0.10878749936819077 Validation loss 0.10747463256120682 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5408],\n",
      "        [0.4042]], device='mps:0')\n",
      "Iteration 16550 Training loss 0.10265324264764786 Validation loss 0.10748372226953506 Accuracy 0.6835000514984131\n",
      "Output tensor([[0.5294],\n",
      "        [0.3047]], device='mps:0')\n",
      "Iteration 16560 Training loss 0.10147149860858917 Validation loss 0.107496477663517 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.4677],\n",
      "        [0.5120]], device='mps:0')\n",
      "Iteration 16570 Training loss 0.10741707682609558 Validation loss 0.10752538591623306 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.3905],\n",
      "        [0.5301]], device='mps:0')\n",
      "Iteration 16580 Training loss 0.1095452830195427 Validation loss 0.10752249509096146 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.5958],\n",
      "        [0.5226]], device='mps:0')\n",
      "Iteration 16590 Training loss 0.11072402447462082 Validation loss 0.10753192007541656 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.6788],\n",
      "        [0.5445]], device='mps:0')\n",
      "Iteration 16600 Training loss 0.112390898168087 Validation loss 0.1075035110116005 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.4697],\n",
      "        [0.2414]], device='mps:0')\n",
      "Iteration 16610 Training loss 0.10967779904603958 Validation loss 0.10746482014656067 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.5908],\n",
      "        [0.3583]], device='mps:0')\n",
      "Iteration 16620 Training loss 0.11491958051919937 Validation loss 0.1074649840593338 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.4752],\n",
      "        [0.4940]], device='mps:0')\n",
      "Iteration 16630 Training loss 0.11059624701738358 Validation loss 0.10746075958013535 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.3689],\n",
      "        [0.2151]], device='mps:0')\n",
      "Iteration 16640 Training loss 0.10843229293823242 Validation loss 0.10743982344865799 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.6687],\n",
      "        [0.7857]], device='mps:0')\n",
      "Iteration 16650 Training loss 0.100250244140625 Validation loss 0.10743606835603714 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.6329],\n",
      "        [0.2845]], device='mps:0')\n",
      "Iteration 16660 Training loss 0.1099507138133049 Validation loss 0.10743287950754166 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.4856],\n",
      "        [0.5568]], device='mps:0')\n",
      "Iteration 16670 Training loss 0.09905809164047241 Validation loss 0.10743778198957443 Accuracy 0.687000036239624\n",
      "Output tensor([[0.5917],\n",
      "        [0.4767]], device='mps:0')\n",
      "Iteration 16680 Training loss 0.10808537900447845 Validation loss 0.10743072628974915 Accuracy 0.6845000386238098\n",
      "Output tensor([[0.6065],\n",
      "        [0.5843]], device='mps:0')\n",
      "Iteration 16690 Training loss 0.0993141159415245 Validation loss 0.10745906084775925 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.6113],\n",
      "        [0.4544]], device='mps:0')\n",
      "Iteration 16700 Training loss 0.10343604534864426 Validation loss 0.10744934529066086 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.5565],\n",
      "        [0.6294]], device='mps:0')\n",
      "Iteration 16710 Training loss 0.11618861556053162 Validation loss 0.10742775350809097 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.5249],\n",
      "        [0.6262]], device='mps:0')\n",
      "Iteration 16720 Training loss 0.10465013235807419 Validation loss 0.10742378979921341 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.5633],\n",
      "        [0.5566]], device='mps:0')\n",
      "Iteration 16730 Training loss 0.11191613972187042 Validation loss 0.10744170099496841 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.7560],\n",
      "        [0.4497]], device='mps:0')\n",
      "Iteration 16740 Training loss 0.11493381857872009 Validation loss 0.10741560906171799 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.5323],\n",
      "        [0.3404]], device='mps:0')\n",
      "Iteration 16750 Training loss 0.10734779387712479 Validation loss 0.10742764919996262 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.6608],\n",
      "        [0.5649]], device='mps:0')\n",
      "Iteration 16760 Training loss 0.11064229905605316 Validation loss 0.1073993444442749 Accuracy 0.6845000386238098\n",
      "Output tensor([[0.6028],\n",
      "        [0.4855]], device='mps:0')\n",
      "Iteration 16770 Training loss 0.10885713994503021 Validation loss 0.10745987296104431 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.6268],\n",
      "        [0.6017]], device='mps:0')\n",
      "Iteration 16780 Training loss 0.10788962244987488 Validation loss 0.10745209455490112 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.5961],\n",
      "        [0.6702]], device='mps:0')\n",
      "Iteration 16790 Training loss 0.1171705573797226 Validation loss 0.10748787969350815 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.5155],\n",
      "        [0.6506]], device='mps:0')\n",
      "Iteration 16800 Training loss 0.11243769526481628 Validation loss 0.10740091651678085 Accuracy 0.687000036239624\n",
      "Output tensor([[0.6456],\n",
      "        [0.5789]], device='mps:0')\n",
      "Iteration 16810 Training loss 0.11010263860225677 Validation loss 0.10738244652748108 Accuracy 0.6850000619888306\n",
      "Output tensor([[0.5460],\n",
      "        [0.6791]], device='mps:0')\n",
      "Iteration 16820 Training loss 0.10643655806779861 Validation loss 0.10740713030099869 Accuracy 0.687000036239624\n",
      "Output tensor([[0.5790],\n",
      "        [0.3512]], device='mps:0')\n",
      "Iteration 16830 Training loss 0.10894517600536346 Validation loss 0.1073787584900856 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.3470],\n",
      "        [0.5521]], device='mps:0')\n",
      "Iteration 16840 Training loss 0.11026690900325775 Validation loss 0.10736742615699768 Accuracy 0.687000036239624\n",
      "Output tensor([[0.5823],\n",
      "        [0.6341]], device='mps:0')\n",
      "Iteration 16850 Training loss 0.11245398223400116 Validation loss 0.10737118870019913 Accuracy 0.6850000619888306\n",
      "Output tensor([[0.5734],\n",
      "        [0.6089]], device='mps:0')\n",
      "Iteration 16860 Training loss 0.11192481964826584 Validation loss 0.10736657679080963 Accuracy 0.6845000386238098\n",
      "Output tensor([[0.5672],\n",
      "        [0.4451]], device='mps:0')\n",
      "Iteration 16870 Training loss 0.12020864337682724 Validation loss 0.10736745595932007 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.4460],\n",
      "        [0.4804]], device='mps:0')\n",
      "Iteration 16880 Training loss 0.10534165054559708 Validation loss 0.10739065706729889 Accuracy 0.687000036239624\n",
      "Output tensor([[0.5711],\n",
      "        [0.5710]], device='mps:0')\n",
      "Iteration 16890 Training loss 0.10380414873361588 Validation loss 0.10737994313240051 Accuracy 0.687000036239624\n",
      "Output tensor([[0.5115],\n",
      "        [0.6105]], device='mps:0')\n",
      "Iteration 16900 Training loss 0.10988900810480118 Validation loss 0.10743669420480728 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.6034],\n",
      "        [0.3678]], device='mps:0')\n",
      "Iteration 16910 Training loss 0.10186338424682617 Validation loss 0.10743260383605957 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.5651],\n",
      "        [0.2541]], device='mps:0')\n",
      "Iteration 16920 Training loss 0.1119927391409874 Validation loss 0.10737945884466171 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.4112],\n",
      "        [0.2363]], device='mps:0')\n",
      "Iteration 16930 Training loss 0.10411956906318665 Validation loss 0.10733526945114136 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.5722],\n",
      "        [0.5100]], device='mps:0')\n",
      "Iteration 16940 Training loss 0.09991151839494705 Validation loss 0.1073274239897728 Accuracy 0.687000036239624\n",
      "Output tensor([[0.2563],\n",
      "        [0.6498]], device='mps:0')\n",
      "Iteration 16950 Training loss 0.10821105539798737 Validation loss 0.10732292383909225 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.2626],\n",
      "        [0.4878]], device='mps:0')\n",
      "Iteration 16960 Training loss 0.10498540848493576 Validation loss 0.10731972008943558 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.6601],\n",
      "        [0.3294]], device='mps:0')\n",
      "Iteration 16970 Training loss 0.10286145657300949 Validation loss 0.10731750726699829 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.6799],\n",
      "        [0.4392]], device='mps:0')\n",
      "Iteration 16980 Training loss 0.10537387430667877 Validation loss 0.10732981562614441 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.5776],\n",
      "        [0.4709]], device='mps:0')\n",
      "Iteration 16990 Training loss 0.11502879858016968 Validation loss 0.10732243955135345 Accuracy 0.687000036239624\n",
      "Output tensor([[0.4017],\n",
      "        [0.3060]], device='mps:0')\n",
      "Iteration 17000 Training loss 0.11314859241247177 Validation loss 0.10733218491077423 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5760],\n",
      "        [0.2671]], device='mps:0')\n",
      "Iteration 17010 Training loss 0.11015205085277557 Validation loss 0.10742847621440887 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.5205],\n",
      "        [0.6041]], device='mps:0')\n",
      "Iteration 17020 Training loss 0.11628428101539612 Validation loss 0.10739755630493164 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.6335],\n",
      "        [0.5497]], device='mps:0')\n",
      "Iteration 17030 Training loss 0.10572866350412369 Validation loss 0.10732763260602951 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.4068],\n",
      "        [0.5617]], device='mps:0')\n",
      "Iteration 17040 Training loss 0.10669929534196854 Validation loss 0.10733890533447266 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.3937],\n",
      "        [0.4571]], device='mps:0')\n",
      "Iteration 17050 Training loss 0.1042657271027565 Validation loss 0.10733282566070557 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.3065],\n",
      "        [0.6017]], device='mps:0')\n",
      "Iteration 17060 Training loss 0.10883389413356781 Validation loss 0.10733768343925476 Accuracy 0.690000057220459\n",
      "Output tensor([[0.4186],\n",
      "        [0.3938]], device='mps:0')\n",
      "Iteration 17070 Training loss 0.11626493185758591 Validation loss 0.10733340680599213 Accuracy 0.690500020980835\n",
      "Output tensor([[0.5272],\n",
      "        [0.4563]], device='mps:0')\n",
      "Iteration 17080 Training loss 0.10846700519323349 Validation loss 0.10728590935468674 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5465],\n",
      "        [0.3459]], device='mps:0')\n",
      "Iteration 17090 Training loss 0.115231953561306 Validation loss 0.10730061680078506 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.6291],\n",
      "        [0.5107]], device='mps:0')\n",
      "Iteration 17100 Training loss 0.10510275512933731 Validation loss 0.10734961926937103 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.2153],\n",
      "        [0.6340]], device='mps:0')\n",
      "Iteration 17110 Training loss 0.11568519473075867 Validation loss 0.10739067196846008 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.5564],\n",
      "        [0.4568]], device='mps:0')\n",
      "Iteration 17120 Training loss 0.10680615901947021 Validation loss 0.10738860070705414 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.3707],\n",
      "        [0.5681]], device='mps:0')\n",
      "Iteration 17130 Training loss 0.1031162366271019 Validation loss 0.10736387968063354 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.4104],\n",
      "        [0.3976]], device='mps:0')\n",
      "Iteration 17140 Training loss 0.114054374396801 Validation loss 0.10732175409793854 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.4894],\n",
      "        [0.5199]], device='mps:0')\n",
      "Iteration 17150 Training loss 0.11687739938497543 Validation loss 0.1074381172657013 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.4757],\n",
      "        [0.5794]], device='mps:0')\n",
      "Iteration 17160 Training loss 0.1071932390332222 Validation loss 0.10740512609481812 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.5334],\n",
      "        [0.3885]], device='mps:0')\n",
      "Iteration 17170 Training loss 0.1169838085770607 Validation loss 0.1073378250002861 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.5927],\n",
      "        [0.5712]], device='mps:0')\n",
      "Iteration 17180 Training loss 0.1054462119936943 Validation loss 0.10732407867908478 Accuracy 0.690000057220459\n",
      "Output tensor([[0.7383],\n",
      "        [0.5430]], device='mps:0')\n",
      "Iteration 17190 Training loss 0.1053948923945427 Validation loss 0.10735918581485748 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.5611],\n",
      "        [0.5919]], device='mps:0')\n",
      "Iteration 17200 Training loss 0.10650840401649475 Validation loss 0.10742953419685364 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.6181],\n",
      "        [0.3354]], device='mps:0')\n",
      "Iteration 17210 Training loss 0.10947462171316147 Validation loss 0.10736624896526337 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.3576],\n",
      "        [0.5828]], device='mps:0')\n",
      "Iteration 17220 Training loss 0.11486196517944336 Validation loss 0.10734087228775024 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.6415],\n",
      "        [0.5720]], device='mps:0')\n",
      "Iteration 17230 Training loss 0.10851243883371353 Validation loss 0.10731251537799835 Accuracy 0.690000057220459\n",
      "Output tensor([[0.3235],\n",
      "        [0.3754]], device='mps:0')\n",
      "Iteration 17240 Training loss 0.10609161853790283 Validation loss 0.10728541016578674 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.4308],\n",
      "        [0.6142]], device='mps:0')\n",
      "Iteration 17250 Training loss 0.10406463593244553 Validation loss 0.1072545200586319 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.6918],\n",
      "        [0.4644]], device='mps:0')\n",
      "Iteration 17260 Training loss 0.09925281256437302 Validation loss 0.10729838907718658 Accuracy 0.690500020980835\n",
      "Output tensor([[0.6330],\n",
      "        [0.3635]], device='mps:0')\n",
      "Iteration 17270 Training loss 0.10453609377145767 Validation loss 0.10734964907169342 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.3524],\n",
      "        [0.4786]], device='mps:0')\n",
      "Iteration 17280 Training loss 0.10140936821699142 Validation loss 0.10730589926242828 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.3856],\n",
      "        [0.3836]], device='mps:0')\n",
      "Iteration 17290 Training loss 0.10222232341766357 Validation loss 0.10722986608743668 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.5759],\n",
      "        [0.6064]], device='mps:0')\n",
      "Iteration 17300 Training loss 0.10503220558166504 Validation loss 0.10724712163209915 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.3822],\n",
      "        [0.3481]], device='mps:0')\n",
      "Iteration 17310 Training loss 0.11077933013439178 Validation loss 0.10725178569555283 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.2892],\n",
      "        [0.5830]], device='mps:0')\n",
      "Iteration 17320 Training loss 0.10596944391727448 Validation loss 0.10723007470369339 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.4758],\n",
      "        [0.4499]], device='mps:0')\n",
      "Iteration 17330 Training loss 0.1111343577504158 Validation loss 0.10728529840707779 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.4103],\n",
      "        [0.3749]], device='mps:0')\n",
      "Iteration 17340 Training loss 0.11050031334161758 Validation loss 0.10723850131034851 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.4349],\n",
      "        [0.6280]], device='mps:0')\n",
      "Iteration 17350 Training loss 0.1232745498418808 Validation loss 0.10720065981149673 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.6234],\n",
      "        [0.5031]], device='mps:0')\n",
      "Iteration 17360 Training loss 0.11111648380756378 Validation loss 0.10717660933732986 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.4532],\n",
      "        [0.4205]], device='mps:0')\n",
      "Iteration 17370 Training loss 0.12002689391374588 Validation loss 0.10717874020338058 Accuracy 0.687000036239624\n",
      "Output tensor([[0.4584],\n",
      "        [0.5674]], device='mps:0')\n",
      "Iteration 17380 Training loss 0.10146863758563995 Validation loss 0.10717137902975082 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.2601],\n",
      "        [0.4356]], device='mps:0')\n",
      "Iteration 17390 Training loss 0.10196840763092041 Validation loss 0.10715483874082565 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.5555],\n",
      "        [0.6153]], device='mps:0')\n",
      "Iteration 17400 Training loss 0.11453372985124588 Validation loss 0.10715964436531067 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.2722],\n",
      "        [0.4629]], device='mps:0')\n",
      "Iteration 17410 Training loss 0.11321160942316055 Validation loss 0.10714810341596603 Accuracy 0.687000036239624\n",
      "Output tensor([[0.6477],\n",
      "        [0.5567]], device='mps:0')\n",
      "Iteration 17420 Training loss 0.09622813016176224 Validation loss 0.10720868408679962 Accuracy 0.690500020980835\n",
      "Output tensor([[0.3411],\n",
      "        [0.6225]], device='mps:0')\n",
      "Iteration 17430 Training loss 0.1057896763086319 Validation loss 0.10721880197525024 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.4128],\n",
      "        [0.3611]], device='mps:0')\n",
      "Iteration 17440 Training loss 0.10667993128299713 Validation loss 0.10723850131034851 Accuracy 0.690000057220459\n",
      "Output tensor([[0.6869],\n",
      "        [0.5624]], device='mps:0')\n",
      "Iteration 17450 Training loss 0.11870504170656204 Validation loss 0.10720309615135193 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.6405],\n",
      "        [0.6545]], device='mps:0')\n",
      "Iteration 17460 Training loss 0.10546379536390305 Validation loss 0.10721530020236969 Accuracy 0.690500020980835\n",
      "Output tensor([[0.4221],\n",
      "        [0.6237]], device='mps:0')\n",
      "Iteration 17470 Training loss 0.10450491309165955 Validation loss 0.10717615485191345 Accuracy 0.690000057220459\n",
      "Output tensor([[0.6144],\n",
      "        [0.5214]], device='mps:0')\n",
      "Iteration 17480 Training loss 0.10196098685264587 Validation loss 0.10715300589799881 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.5354],\n",
      "        [0.4080]], device='mps:0')\n",
      "Iteration 17490 Training loss 0.10954894870519638 Validation loss 0.10714832693338394 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5879],\n",
      "        [0.6549]], device='mps:0')\n",
      "Iteration 17500 Training loss 0.1185670793056488 Validation loss 0.10712532699108124 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.6169],\n",
      "        [0.3844]], device='mps:0')\n",
      "Iteration 17510 Training loss 0.09979511052370071 Validation loss 0.10711021721363068 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.6111],\n",
      "        [0.6039]], device='mps:0')\n",
      "Iteration 17520 Training loss 0.10652846843004227 Validation loss 0.10710883140563965 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.5813],\n",
      "        [0.2737]], device='mps:0')\n",
      "Iteration 17530 Training loss 0.101663738489151 Validation loss 0.10711540281772614 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5502],\n",
      "        [0.3289]], device='mps:0')\n",
      "Iteration 17540 Training loss 0.1089487075805664 Validation loss 0.10712026059627533 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.4255],\n",
      "        [0.5798]], device='mps:0')\n",
      "Iteration 17550 Training loss 0.10165130347013474 Validation loss 0.10710414499044418 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.5157],\n",
      "        [0.3700]], device='mps:0')\n",
      "Iteration 17560 Training loss 0.11190112680196762 Validation loss 0.10712370276451111 Accuracy 0.687000036239624\n",
      "Output tensor([[0.4733],\n",
      "        [0.3823]], device='mps:0')\n",
      "Iteration 17570 Training loss 0.11260611563920975 Validation loss 0.10711836069822311 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5775],\n",
      "        [0.5740]], device='mps:0')\n",
      "Iteration 17580 Training loss 0.10958608239889145 Validation loss 0.1071011871099472 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.4946],\n",
      "        [0.7557]], device='mps:0')\n",
      "Iteration 17590 Training loss 0.10542328655719757 Validation loss 0.10710908472537994 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.6029],\n",
      "        [0.4241]], device='mps:0')\n",
      "Iteration 17600 Training loss 0.10841712355613708 Validation loss 0.10713808238506317 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.2910],\n",
      "        [0.2628]], device='mps:0')\n",
      "Iteration 17610 Training loss 0.11000490188598633 Validation loss 0.10709542036056519 Accuracy 0.687000036239624\n",
      "Output tensor([[0.4924],\n",
      "        [0.6726]], device='mps:0')\n",
      "Iteration 17620 Training loss 0.10506743937730789 Validation loss 0.1071339026093483 Accuracy 0.690500020980835\n",
      "Output tensor([[0.5880],\n",
      "        [0.5739]], device='mps:0')\n",
      "Iteration 17630 Training loss 0.10592465847730637 Validation loss 0.10712798684835434 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.3881],\n",
      "        [0.6189]], device='mps:0')\n",
      "Iteration 17640 Training loss 0.09705375134944916 Validation loss 0.10713621228933334 Accuracy 0.690500020980835\n",
      "Output tensor([[0.5925],\n",
      "        [0.5387]], device='mps:0')\n",
      "Iteration 17650 Training loss 0.10179906338453293 Validation loss 0.10714150220155716 Accuracy 0.690000057220459\n",
      "Output tensor([[0.6294],\n",
      "        [0.6648]], device='mps:0')\n",
      "Iteration 17660 Training loss 0.11054946482181549 Validation loss 0.10720564424991608 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.6179],\n",
      "        [0.5941]], device='mps:0')\n",
      "Iteration 17670 Training loss 0.1136426106095314 Validation loss 0.10727193206548691 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.6476],\n",
      "        [0.5701]], device='mps:0')\n",
      "Iteration 17680 Training loss 0.11091148108243942 Validation loss 0.10712001472711563 Accuracy 0.690500020980835\n",
      "Output tensor([[0.3031],\n",
      "        [0.5942]], device='mps:0')\n",
      "Iteration 17690 Training loss 0.1027950868010521 Validation loss 0.10708391666412354 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.5033],\n",
      "        [0.6161]], device='mps:0')\n",
      "Iteration 17700 Training loss 0.10381405800580978 Validation loss 0.10707057267427444 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.5176],\n",
      "        [0.6511]], device='mps:0')\n",
      "Iteration 17710 Training loss 0.11372727900743484 Validation loss 0.10710383951663971 Accuracy 0.690000057220459\n",
      "Output tensor([[0.5541],\n",
      "        [0.4402]], device='mps:0')\n",
      "Iteration 17720 Training loss 0.10334527492523193 Validation loss 0.10707136988639832 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.4818],\n",
      "        [0.4787]], device='mps:0')\n",
      "Iteration 17730 Training loss 0.09771296381950378 Validation loss 0.10705357789993286 Accuracy 0.687000036239624\n",
      "Output tensor([[0.4717],\n",
      "        [0.4977]], device='mps:0')\n",
      "Iteration 17740 Training loss 0.10417357832193375 Validation loss 0.10703343152999878 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.4744],\n",
      "        [0.5977]], device='mps:0')\n",
      "Iteration 17750 Training loss 0.10922884941101074 Validation loss 0.10704095661640167 Accuracy 0.690000057220459\n",
      "Output tensor([[0.5164],\n",
      "        [0.6224]], device='mps:0')\n",
      "Iteration 17760 Training loss 0.10552604496479034 Validation loss 0.10701822489500046 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.2116],\n",
      "        [0.4694]], device='mps:0')\n",
      "Iteration 17770 Training loss 0.11737988889217377 Validation loss 0.10701359808444977 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.2434],\n",
      "        [0.6061]], device='mps:0')\n",
      "Iteration 17780 Training loss 0.0967758521437645 Validation loss 0.10701818019151688 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.3733],\n",
      "        [0.6658]], device='mps:0')\n",
      "Iteration 17790 Training loss 0.09935682266950607 Validation loss 0.1070363000035286 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.5826],\n",
      "        [0.6673]], device='mps:0')\n",
      "Iteration 17800 Training loss 0.1079186424612999 Validation loss 0.10710404068231583 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.6881],\n",
      "        [0.5290]], device='mps:0')\n",
      "Iteration 17810 Training loss 0.10627742856740952 Validation loss 0.10709696263074875 Accuracy 0.690500020980835\n",
      "Output tensor([[0.5607],\n",
      "        [0.5215]], device='mps:0')\n",
      "Iteration 17820 Training loss 0.10590588301420212 Validation loss 0.10708966106176376 Accuracy 0.690500020980835\n",
      "Output tensor([[0.1581],\n",
      "        [0.5867]], device='mps:0')\n",
      "Iteration 17830 Training loss 0.1082349643111229 Validation loss 0.10706452280282974 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.1288],\n",
      "        [0.3311]], device='mps:0')\n",
      "Iteration 17840 Training loss 0.10746555030345917 Validation loss 0.10703354328870773 Accuracy 0.690500020980835\n",
      "Output tensor([[0.4479],\n",
      "        [0.2976]], device='mps:0')\n",
      "Iteration 17850 Training loss 0.10834874957799911 Validation loss 0.10702726989984512 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.6079],\n",
      "        [0.1553]], device='mps:0')\n",
      "Iteration 17860 Training loss 0.09915105253458023 Validation loss 0.10698756575584412 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5484],\n",
      "        [0.4463]], device='mps:0')\n",
      "Iteration 17870 Training loss 0.10861904174089432 Validation loss 0.10698076337575912 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5526],\n",
      "        [0.5482]], device='mps:0')\n",
      "Iteration 17880 Training loss 0.09903066605329514 Validation loss 0.10697764903306961 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.4530],\n",
      "        [0.5904]], device='mps:0')\n",
      "Iteration 17890 Training loss 0.11982322484254837 Validation loss 0.1069740504026413 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.4206],\n",
      "        [0.6007]], device='mps:0')\n",
      "Iteration 17900 Training loss 0.10690902173519135 Validation loss 0.1069805771112442 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.5603],\n",
      "        [0.5664]], device='mps:0')\n",
      "Iteration 17910 Training loss 0.10622052103281021 Validation loss 0.10698402673006058 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.3535],\n",
      "        [0.5027]], device='mps:0')\n",
      "Iteration 17920 Training loss 0.09890779107809067 Validation loss 0.10698753595352173 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.5749],\n",
      "        [0.5361]], device='mps:0')\n",
      "Iteration 17930 Training loss 0.10484492033720016 Validation loss 0.106967031955719 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.6163],\n",
      "        [0.6184]], device='mps:0')\n",
      "Iteration 17940 Training loss 0.10882274061441422 Validation loss 0.10695173591375351 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.6015],\n",
      "        [0.5323]], device='mps:0')\n",
      "Iteration 17950 Training loss 0.11033523082733154 Validation loss 0.10695244371891022 Accuracy 0.690000057220459\n",
      "Output tensor([[0.5775],\n",
      "        [0.4244]], device='mps:0')\n",
      "Iteration 17960 Training loss 0.11589885503053665 Validation loss 0.10694479942321777 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.6405],\n",
      "        [0.4227]], device='mps:0')\n",
      "Iteration 17970 Training loss 0.1109849140048027 Validation loss 0.10696642100811005 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.4947],\n",
      "        [0.6315]], device='mps:0')\n",
      "Iteration 17980 Training loss 0.10673169791698456 Validation loss 0.1069754958152771 Accuracy 0.690000057220459\n",
      "Output tensor([[0.3861],\n",
      "        [0.5597]], device='mps:0')\n",
      "Iteration 17990 Training loss 0.10535340756177902 Validation loss 0.10698089748620987 Accuracy 0.690000057220459\n",
      "Output tensor([[0.5628],\n",
      "        [0.5361]], device='mps:0')\n",
      "Iteration 18000 Training loss 0.11384240537881851 Validation loss 0.10694418847560883 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.5362],\n",
      "        [0.6082]], device='mps:0')\n",
      "Iteration 18010 Training loss 0.10391838103532791 Validation loss 0.10692889243364334 Accuracy 0.690000057220459\n",
      "Output tensor([[0.4443],\n",
      "        [0.4691]], device='mps:0')\n",
      "Iteration 18020 Training loss 0.10719279944896698 Validation loss 0.10692135989665985 Accuracy 0.687000036239624\n",
      "Output tensor([[0.5936],\n",
      "        [0.5825]], device='mps:0')\n",
      "Iteration 18030 Training loss 0.11383428424596786 Validation loss 0.10691873729228973 Accuracy 0.690000057220459\n",
      "Output tensor([[0.6831],\n",
      "        [0.6496]], device='mps:0')\n",
      "Iteration 18040 Training loss 0.10408493876457214 Validation loss 0.10692465305328369 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.6743],\n",
      "        [0.3892]], device='mps:0')\n",
      "Iteration 18050 Training loss 0.11530494689941406 Validation loss 0.10690873861312866 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5089],\n",
      "        [0.4603]], device='mps:0')\n",
      "Iteration 18060 Training loss 0.10260290652513504 Validation loss 0.10690490156412125 Accuracy 0.6855000257492065\n",
      "Output tensor([[0.3731],\n",
      "        [0.5673]], device='mps:0')\n",
      "Iteration 18070 Training loss 0.1040971651673317 Validation loss 0.10690081864595413 Accuracy 0.6860000491142273\n",
      "Output tensor([[0.4432],\n",
      "        [0.2484]], device='mps:0')\n",
      "Iteration 18080 Training loss 0.10342295467853546 Validation loss 0.10690012574195862 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.2788],\n",
      "        [0.3315]], device='mps:0')\n",
      "Iteration 18090 Training loss 0.10466233640909195 Validation loss 0.1068958267569542 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.5556],\n",
      "        [0.5781]], device='mps:0')\n",
      "Iteration 18100 Training loss 0.10600175708532333 Validation loss 0.10690940171480179 Accuracy 0.690000057220459\n",
      "Output tensor([[0.5146],\n",
      "        [0.3066]], device='mps:0')\n",
      "Iteration 18110 Training loss 0.1046999916434288 Validation loss 0.1068873256444931 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.2204],\n",
      "        [0.3242]], device='mps:0')\n",
      "Iteration 18120 Training loss 0.1116630882024765 Validation loss 0.10689619928598404 Accuracy 0.690000057220459\n",
      "Output tensor([[0.4296],\n",
      "        [0.7025]], device='mps:0')\n",
      "Iteration 18130 Training loss 0.1020154282450676 Validation loss 0.10688622295856476 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.3509],\n",
      "        [0.4368]], device='mps:0')\n",
      "Iteration 18140 Training loss 0.11652086675167084 Validation loss 0.10689909756183624 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.7167],\n",
      "        [0.5582]], device='mps:0')\n",
      "Iteration 18150 Training loss 0.11024504154920578 Validation loss 0.10688095539808273 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.2466],\n",
      "        [0.2957]], device='mps:0')\n",
      "Iteration 18160 Training loss 0.10645988583564758 Validation loss 0.1068897619843483 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.3741],\n",
      "        [0.4736]], device='mps:0')\n",
      "Iteration 18170 Training loss 0.10095284879207611 Validation loss 0.10690311342477798 Accuracy 0.690000057220459\n",
      "Output tensor([[0.6584],\n",
      "        [0.4286]], device='mps:0')\n",
      "Iteration 18180 Training loss 0.10780870169401169 Validation loss 0.1068825051188469 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.5358],\n",
      "        [0.4477]], device='mps:0')\n",
      "Iteration 18190 Training loss 0.1072448119521141 Validation loss 0.1069260910153389 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.6501],\n",
      "        [0.4830]], device='mps:0')\n",
      "Iteration 18200 Training loss 0.11104481667280197 Validation loss 0.10687963664531708 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.4772],\n",
      "        [0.5042]], device='mps:0')\n",
      "Iteration 18210 Training loss 0.11115419119596481 Validation loss 0.10688907653093338 Accuracy 0.690000057220459\n",
      "Output tensor([[0.6427],\n",
      "        [0.3707]], device='mps:0')\n",
      "Iteration 18220 Training loss 0.09952905774116516 Validation loss 0.10687937587499619 Accuracy 0.690000057220459\n",
      "Output tensor([[0.2615],\n",
      "        [0.5276]], device='mps:0')\n",
      "Iteration 18230 Training loss 0.11035222560167313 Validation loss 0.10686025023460388 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.4849],\n",
      "        [0.2268]], device='mps:0')\n",
      "Iteration 18240 Training loss 0.10946062207221985 Validation loss 0.1068735122680664 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.5399],\n",
      "        [0.3874]], device='mps:0')\n",
      "Iteration 18250 Training loss 0.1042100042104721 Validation loss 0.10691611468791962 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.5877],\n",
      "        [0.6347]], device='mps:0')\n",
      "Iteration 18260 Training loss 0.11474985629320145 Validation loss 0.10692020505666733 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5628],\n",
      "        [0.5061]], device='mps:0')\n",
      "Iteration 18270 Training loss 0.10771773755550385 Validation loss 0.10693994164466858 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.5175],\n",
      "        [0.3278]], device='mps:0')\n",
      "Iteration 18280 Training loss 0.105593241751194 Validation loss 0.10686923563480377 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.4007],\n",
      "        [0.3377]], device='mps:0')\n",
      "Iteration 18290 Training loss 0.10752979665994644 Validation loss 0.10687041282653809 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.5158],\n",
      "        [0.4432]], device='mps:0')\n",
      "Iteration 18300 Training loss 0.10568101704120636 Validation loss 0.1068645715713501 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.5687],\n",
      "        [0.5263]], device='mps:0')\n",
      "Iteration 18310 Training loss 0.11111954599618912 Validation loss 0.10682699084281921 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.6152],\n",
      "        [0.6367]], device='mps:0')\n",
      "Iteration 18320 Training loss 0.10946948081254959 Validation loss 0.10682645440101624 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.5544],\n",
      "        [0.4916]], device='mps:0')\n",
      "Iteration 18330 Training loss 0.11124932765960693 Validation loss 0.10681406408548355 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.4364],\n",
      "        [0.4262]], device='mps:0')\n",
      "Iteration 18340 Training loss 0.1116795688867569 Validation loss 0.10681232064962387 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.2827],\n",
      "        [0.5438]], device='mps:0')\n",
      "Iteration 18350 Training loss 0.1042461022734642 Validation loss 0.10680930316448212 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.3426],\n",
      "        [0.3233]], device='mps:0')\n",
      "Iteration 18360 Training loss 0.10948586463928223 Validation loss 0.10680423676967621 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.6009],\n",
      "        [0.3377]], device='mps:0')\n",
      "Iteration 18370 Training loss 0.11315612494945526 Validation loss 0.10680250078439713 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.5203],\n",
      "        [0.6007]], device='mps:0')\n",
      "Iteration 18380 Training loss 0.11251872777938843 Validation loss 0.10679785162210464 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.2706],\n",
      "        [0.6766]], device='mps:0')\n",
      "Iteration 18390 Training loss 0.11042892187833786 Validation loss 0.10679159313440323 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.5817],\n",
      "        [0.4876]], device='mps:0')\n",
      "Iteration 18400 Training loss 0.11271429061889648 Validation loss 0.10679052025079727 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.4673],\n",
      "        [0.6535]], device='mps:0')\n",
      "Iteration 18410 Training loss 0.11878834664821625 Validation loss 0.1067844107747078 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.4219],\n",
      "        [0.4095]], device='mps:0')\n",
      "Iteration 18420 Training loss 0.10795217007398605 Validation loss 0.1067795380949974 Accuracy 0.6865000128746033\n",
      "Output tensor([[0.4096],\n",
      "        [0.2800]], device='mps:0')\n",
      "Iteration 18430 Training loss 0.11981271207332611 Validation loss 0.10679084062576294 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.4770],\n",
      "        [0.5741]], device='mps:0')\n",
      "Iteration 18440 Training loss 0.11000143736600876 Validation loss 0.10685011744499207 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.5907],\n",
      "        [0.6553]], device='mps:0')\n",
      "Iteration 18450 Training loss 0.11293204873800278 Validation loss 0.10686595737934113 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.3943],\n",
      "        [0.6449]], device='mps:0')\n",
      "Iteration 18460 Training loss 0.1094794049859047 Validation loss 0.10680563747882843 Accuracy 0.690500020980835\n",
      "Output tensor([[0.1683],\n",
      "        [0.3086]], device='mps:0')\n",
      "Iteration 18470 Training loss 0.10933951288461685 Validation loss 0.10676750540733337 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.3736],\n",
      "        [0.5229]], device='mps:0')\n",
      "Iteration 18480 Training loss 0.11257413774728775 Validation loss 0.10677403956651688 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.3784],\n",
      "        [0.5047]], device='mps:0')\n",
      "Iteration 18490 Training loss 0.10129664838314056 Validation loss 0.10679417848587036 Accuracy 0.690500020980835\n",
      "Output tensor([[0.5235],\n",
      "        [0.6041]], device='mps:0')\n",
      "Iteration 18500 Training loss 0.10545384883880615 Validation loss 0.10677281022071838 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.5908],\n",
      "        [0.5429]], device='mps:0')\n",
      "Iteration 18510 Training loss 0.1135028526186943 Validation loss 0.10675030201673508 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.6903],\n",
      "        [0.4611]], device='mps:0')\n",
      "Iteration 18520 Training loss 0.11759050190448761 Validation loss 0.10678314417600632 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.6437],\n",
      "        [0.6294]], device='mps:0')\n",
      "Iteration 18530 Training loss 0.10398443043231964 Validation loss 0.10674799978733063 Accuracy 0.690000057220459\n",
      "Output tensor([[0.4090],\n",
      "        [0.5863]], device='mps:0')\n",
      "Iteration 18540 Training loss 0.10939255356788635 Validation loss 0.10675280541181564 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.6489],\n",
      "        [0.4174]], device='mps:0')\n",
      "Iteration 18550 Training loss 0.11510399729013443 Validation loss 0.1067652776837349 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.5619],\n",
      "        [0.5218]], device='mps:0')\n",
      "Iteration 18560 Training loss 0.10829509794712067 Validation loss 0.1067509651184082 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.4199],\n",
      "        [0.3433]], device='mps:0')\n",
      "Iteration 18570 Training loss 0.10627420246601105 Validation loss 0.10673189908266068 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.5222],\n",
      "        [0.5556]], device='mps:0')\n",
      "Iteration 18580 Training loss 0.10627185553312302 Validation loss 0.10672608017921448 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.6026],\n",
      "        [0.5587]], device='mps:0')\n",
      "Iteration 18590 Training loss 0.11018603295087814 Validation loss 0.10673261433839798 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.5242],\n",
      "        [0.5596]], device='mps:0')\n",
      "Iteration 18600 Training loss 0.10525509715080261 Validation loss 0.10673629492521286 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.3879],\n",
      "        [0.2766]], device='mps:0')\n",
      "Iteration 18610 Training loss 0.10087575018405914 Validation loss 0.10673459619283676 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.3302],\n",
      "        [0.5655]], device='mps:0')\n",
      "Iteration 18620 Training loss 0.11039062589406967 Validation loss 0.10670850425958633 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.5941],\n",
      "        [0.2193]], device='mps:0')\n",
      "Iteration 18630 Training loss 0.10715106874704361 Validation loss 0.10670596361160278 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.6102],\n",
      "        [0.3866]], device='mps:0')\n",
      "Iteration 18640 Training loss 0.10186439007520676 Validation loss 0.10670775175094604 Accuracy 0.690500020980835\n",
      "Output tensor([[0.6321],\n",
      "        [0.3997]], device='mps:0')\n",
      "Iteration 18650 Training loss 0.12011750787496567 Validation loss 0.10670015215873718 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.1922],\n",
      "        [0.2093]], device='mps:0')\n",
      "Iteration 18660 Training loss 0.10542325675487518 Validation loss 0.1066974401473999 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.4919],\n",
      "        [0.5439]], device='mps:0')\n",
      "Iteration 18670 Training loss 0.10457088053226471 Validation loss 0.10669748485088348 Accuracy 0.690000057220459\n",
      "Output tensor([[0.5561],\n",
      "        [0.4457]], device='mps:0')\n",
      "Iteration 18680 Training loss 0.11165497452020645 Validation loss 0.106694795191288 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.4495],\n",
      "        [0.4153]], device='mps:0')\n",
      "Iteration 18690 Training loss 0.10524378716945648 Validation loss 0.10673604160547256 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.5354],\n",
      "        [0.5498]], device='mps:0')\n",
      "Iteration 18700 Training loss 0.09871944040060043 Validation loss 0.10679426789283752 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5262],\n",
      "        [0.6246]], device='mps:0')\n",
      "Iteration 18710 Training loss 0.11015815287828445 Validation loss 0.10683127492666245 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.4422],\n",
      "        [0.6312]], device='mps:0')\n",
      "Iteration 18720 Training loss 0.11487890779972076 Validation loss 0.10679515451192856 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.5105],\n",
      "        [0.4432]], device='mps:0')\n",
      "Iteration 18730 Training loss 0.09845193475484848 Validation loss 0.10677210986614227 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5822],\n",
      "        [0.3039]], device='mps:0')\n",
      "Iteration 18740 Training loss 0.11239045858383179 Validation loss 0.10674457252025604 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5331],\n",
      "        [0.4193]], device='mps:0')\n",
      "Iteration 18750 Training loss 0.11018457263708115 Validation loss 0.10672731697559357 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.6486],\n",
      "        [0.6729]], device='mps:0')\n",
      "Iteration 18760 Training loss 0.10321631282567978 Validation loss 0.10670629143714905 Accuracy 0.690500020980835\n",
      "Output tensor([[0.5462],\n",
      "        [0.1736]], device='mps:0')\n",
      "Iteration 18770 Training loss 0.10540921241044998 Validation loss 0.10668719559907913 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.4063],\n",
      "        [0.3977]], device='mps:0')\n",
      "Iteration 18780 Training loss 0.09608978033065796 Validation loss 0.1066618263721466 Accuracy 0.6880000233650208\n",
      "Output tensor([[0.5179],\n",
      "        [0.4225]], device='mps:0')\n",
      "Iteration 18790 Training loss 0.10912595689296722 Validation loss 0.10665524005889893 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.5289],\n",
      "        [0.2026]], device='mps:0')\n",
      "Iteration 18800 Training loss 0.12434541434049606 Validation loss 0.10665175318717957 Accuracy 0.6875000596046448\n",
      "Output tensor([[0.3958],\n",
      "        [0.2328]], device='mps:0')\n",
      "Iteration 18810 Training loss 0.10849613696336746 Validation loss 0.10664955526590347 Accuracy 0.690000057220459\n",
      "Output tensor([[0.4539],\n",
      "        [0.4255]], device='mps:0')\n",
      "Iteration 18820 Training loss 0.11504430323839188 Validation loss 0.10665477812290192 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.4377],\n",
      "        [0.4204]], device='mps:0')\n",
      "Iteration 18830 Training loss 0.09629566967487335 Validation loss 0.10667119920253754 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.5514],\n",
      "        [0.4832]], device='mps:0')\n",
      "Iteration 18840 Training loss 0.10326818376779556 Validation loss 0.10664316266775131 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.2276],\n",
      "        [0.5186]], device='mps:0')\n",
      "Iteration 18850 Training loss 0.11460979282855988 Validation loss 0.106639564037323 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.4363],\n",
      "        [0.5176]], device='mps:0')\n",
      "Iteration 18860 Training loss 0.1057596504688263 Validation loss 0.10663977265357971 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.5694],\n",
      "        [0.5455]], device='mps:0')\n",
      "Iteration 18870 Training loss 0.11131664365530014 Validation loss 0.10662636160850525 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.3465],\n",
      "        [0.5835]], device='mps:0')\n",
      "Iteration 18880 Training loss 0.1019844338297844 Validation loss 0.10664990544319153 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.4145],\n",
      "        [0.5592]], device='mps:0')\n",
      "Iteration 18890 Training loss 0.10230403393507004 Validation loss 0.10664225369691849 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.5523],\n",
      "        [0.3563]], device='mps:0')\n",
      "Iteration 18900 Training loss 0.10807153582572937 Validation loss 0.10666479170322418 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.4952],\n",
      "        [0.2956]], device='mps:0')\n",
      "Iteration 18910 Training loss 0.1044853925704956 Validation loss 0.10670708119869232 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5447],\n",
      "        [0.5912]], device='mps:0')\n",
      "Iteration 18920 Training loss 0.10239025205373764 Validation loss 0.10670109838247299 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.3666],\n",
      "        [0.1906]], device='mps:0')\n",
      "Iteration 18930 Training loss 0.12474040687084198 Validation loss 0.10673103481531143 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.6786],\n",
      "        [0.5801]], device='mps:0')\n",
      "Iteration 18940 Training loss 0.10790114849805832 Validation loss 0.10681955516338348 Accuracy 0.690500020980835\n",
      "Output tensor([[0.5150],\n",
      "        [0.6659]], device='mps:0')\n",
      "Iteration 18950 Training loss 0.10269612818956375 Validation loss 0.10669723898172379 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.2545],\n",
      "        [0.6141]], device='mps:0')\n",
      "Iteration 18960 Training loss 0.10456494241952896 Validation loss 0.10674213618040085 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.4095],\n",
      "        [0.5542]], device='mps:0')\n",
      "Iteration 18970 Training loss 0.10977136343717575 Validation loss 0.10666342824697495 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.6967],\n",
      "        [0.5436]], device='mps:0')\n",
      "Iteration 18980 Training loss 0.10862578451633453 Validation loss 0.10664964467287064 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.4621],\n",
      "        [0.5755]], device='mps:0')\n",
      "Iteration 18990 Training loss 0.10817234963178635 Validation loss 0.10664141923189163 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.4634],\n",
      "        [0.4850]], device='mps:0')\n",
      "Iteration 19000 Training loss 0.10850401967763901 Validation loss 0.10660208761692047 Accuracy 0.690500020980835\n",
      "Output tensor([[0.4268],\n",
      "        [0.6404]], device='mps:0')\n",
      "Iteration 19010 Training loss 0.10539024323225021 Validation loss 0.10661571472883224 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.4389],\n",
      "        [0.3399]], device='mps:0')\n",
      "Iteration 19020 Training loss 0.11545892804861069 Validation loss 0.10666946321725845 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4919],\n",
      "        [0.5598]], device='mps:0')\n",
      "Iteration 19030 Training loss 0.1020314171910286 Validation loss 0.10671674460172653 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5787],\n",
      "        [0.5125]], device='mps:0')\n",
      "Iteration 19040 Training loss 0.11386065930128098 Validation loss 0.10667940229177475 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6106],\n",
      "        [0.3551]], device='mps:0')\n",
      "Iteration 19050 Training loss 0.10267824679613113 Validation loss 0.10663925111293793 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.6127],\n",
      "        [0.4486]], device='mps:0')\n",
      "Iteration 19060 Training loss 0.1101057380437851 Validation loss 0.10668253153562546 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.2208],\n",
      "        [0.6294]], device='mps:0')\n",
      "Iteration 19070 Training loss 0.10856393724679947 Validation loss 0.10666999965906143 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6287],\n",
      "        [0.5775]], device='mps:0')\n",
      "Iteration 19080 Training loss 0.10517363995313644 Validation loss 0.10661869496107101 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.4770],\n",
      "        [0.5448]], device='mps:0')\n",
      "Iteration 19090 Training loss 0.1102619543671608 Validation loss 0.10663489252328873 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.6024],\n",
      "        [0.4193]], device='mps:0')\n",
      "Iteration 19100 Training loss 0.11201924830675125 Validation loss 0.10667824000120163 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.7140],\n",
      "        [0.6410]], device='mps:0')\n",
      "Iteration 19110 Training loss 0.10910843312740326 Validation loss 0.10661790519952774 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5868],\n",
      "        [0.6781]], device='mps:0')\n",
      "Iteration 19120 Training loss 0.10749652981758118 Validation loss 0.10663166642189026 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.4169],\n",
      "        [0.2417]], device='mps:0')\n",
      "Iteration 19130 Training loss 0.10712631046772003 Validation loss 0.1066211462020874 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5501],\n",
      "        [0.5613]], device='mps:0')\n",
      "Iteration 19140 Training loss 0.09918369352817535 Validation loss 0.1065850704908371 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.5766],\n",
      "        [0.4912]], device='mps:0')\n",
      "Iteration 19150 Training loss 0.11534446477890015 Validation loss 0.10657692700624466 Accuracy 0.690500020980835\n",
      "Output tensor([[0.6810],\n",
      "        [0.4065]], device='mps:0')\n",
      "Iteration 19160 Training loss 0.10891011357307434 Validation loss 0.10656179487705231 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.3690],\n",
      "        [0.5343]], device='mps:0')\n",
      "Iteration 19170 Training loss 0.11189065873622894 Validation loss 0.10656151175498962 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.3238],\n",
      "        [0.6875]], device='mps:0')\n",
      "Iteration 19180 Training loss 0.10908824950456619 Validation loss 0.10656861960887909 Accuracy 0.690000057220459\n",
      "Output tensor([[0.5820],\n",
      "        [0.5753]], device='mps:0')\n",
      "Iteration 19190 Training loss 0.10618557780981064 Validation loss 0.10655322670936584 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.5975],\n",
      "        [0.5649]], device='mps:0')\n",
      "Iteration 19200 Training loss 0.1055666133761406 Validation loss 0.10655548423528671 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.6119],\n",
      "        [0.4457]], device='mps:0')\n",
      "Iteration 19210 Training loss 0.10241912305355072 Validation loss 0.10654594749212265 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.4849],\n",
      "        [0.5397]], device='mps:0')\n",
      "Iteration 19220 Training loss 0.11065538227558136 Validation loss 0.10652419179677963 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.4643],\n",
      "        [0.3307]], device='mps:0')\n",
      "Iteration 19230 Training loss 0.10922355204820633 Validation loss 0.10651570558547974 Accuracy 0.6885000467300415\n",
      "Output tensor([[0.5371],\n",
      "        [0.1609]], device='mps:0')\n",
      "Iteration 19240 Training loss 0.11219699680805206 Validation loss 0.10651467740535736 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.5429],\n",
      "        [0.4251]], device='mps:0')\n",
      "Iteration 19250 Training loss 0.11430594325065613 Validation loss 0.10654306411743164 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.5074],\n",
      "        [0.4671]], device='mps:0')\n",
      "Iteration 19260 Training loss 0.11664681881666183 Validation loss 0.10657841712236404 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.4995],\n",
      "        [0.5687]], device='mps:0')\n",
      "Iteration 19270 Training loss 0.10702028125524521 Validation loss 0.10657688230276108 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.3973],\n",
      "        [0.4791]], device='mps:0')\n",
      "Iteration 19280 Training loss 0.1090310662984848 Validation loss 0.10651881247758865 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5061],\n",
      "        [0.5552]], device='mps:0')\n",
      "Iteration 19290 Training loss 0.09625215083360672 Validation loss 0.1065060943365097 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.4640],\n",
      "        [0.5300]], device='mps:0')\n",
      "Iteration 19300 Training loss 0.10961249470710754 Validation loss 0.10650187730789185 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.4907],\n",
      "        [0.5015]], device='mps:0')\n",
      "Iteration 19310 Training loss 0.10350272059440613 Validation loss 0.1065170168876648 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.7060],\n",
      "        [0.4985]], device='mps:0')\n",
      "Iteration 19320 Training loss 0.1014876663684845 Validation loss 0.10650644451379776 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.4722],\n",
      "        [0.4155]], device='mps:0')\n",
      "Iteration 19330 Training loss 0.10677787661552429 Validation loss 0.10652992874383926 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.5760],\n",
      "        [0.6282]], device='mps:0')\n",
      "Iteration 19340 Training loss 0.10076986253261566 Validation loss 0.10650584846735 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5373],\n",
      "        [0.5159]], device='mps:0')\n",
      "Iteration 19350 Training loss 0.10685571283102036 Validation loss 0.10651524364948273 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.3333],\n",
      "        [0.5066]], device='mps:0')\n",
      "Iteration 19360 Training loss 0.10160715132951736 Validation loss 0.10648591816425323 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.4988],\n",
      "        [0.5521]], device='mps:0')\n",
      "Iteration 19370 Training loss 0.11088794469833374 Validation loss 0.10647285729646683 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.6289],\n",
      "        [0.3633]], device='mps:0')\n",
      "Iteration 19380 Training loss 0.10506394505500793 Validation loss 0.10647092014551163 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5669],\n",
      "        [0.3211]], device='mps:0')\n",
      "Iteration 19390 Training loss 0.10242531448602676 Validation loss 0.10646744817495346 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.2926],\n",
      "        [0.5405]], device='mps:0')\n",
      "Iteration 19400 Training loss 0.11267098784446716 Validation loss 0.10646256059408188 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.6195],\n",
      "        [0.4762]], device='mps:0')\n",
      "Iteration 19410 Training loss 0.10232176631689072 Validation loss 0.106458880007267 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.5710],\n",
      "        [0.6696]], device='mps:0')\n",
      "Iteration 19420 Training loss 0.09394911676645279 Validation loss 0.10649999231100082 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.3860],\n",
      "        [0.3304]], device='mps:0')\n",
      "Iteration 19430 Training loss 0.10667310655117035 Validation loss 0.10650099813938141 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.5014],\n",
      "        [0.3066]], device='mps:0')\n",
      "Iteration 19440 Training loss 0.11384675651788712 Validation loss 0.10650435090065002 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.3477],\n",
      "        [0.6569]], device='mps:0')\n",
      "Iteration 19450 Training loss 0.10655073076486588 Validation loss 0.10646437853574753 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.4005],\n",
      "        [0.4222]], device='mps:0')\n",
      "Iteration 19460 Training loss 0.11435812711715698 Validation loss 0.10644496232271194 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.5334],\n",
      "        [0.7207]], device='mps:0')\n",
      "Iteration 19470 Training loss 0.10940586030483246 Validation loss 0.10644102841615677 Accuracy 0.690500020980835\n",
      "Output tensor([[0.5083],\n",
      "        [0.6075]], device='mps:0')\n",
      "Iteration 19480 Training loss 0.10232466459274292 Validation loss 0.10644163936376572 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.5000],\n",
      "        [0.3720]], device='mps:0')\n",
      "Iteration 19490 Training loss 0.10059694200754166 Validation loss 0.10644728690385818 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.3162],\n",
      "        [0.5443]], device='mps:0')\n",
      "Iteration 19500 Training loss 0.11275315284729004 Validation loss 0.10644923150539398 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.2750],\n",
      "        [0.5845]], device='mps:0')\n",
      "Iteration 19510 Training loss 0.1143321692943573 Validation loss 0.10645709931850433 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.5864],\n",
      "        [0.6148]], device='mps:0')\n",
      "Iteration 19520 Training loss 0.10901469737291336 Validation loss 0.10644832253456116 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.4079],\n",
      "        [0.2222]], device='mps:0')\n",
      "Iteration 19530 Training loss 0.09996442496776581 Validation loss 0.10644792765378952 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.3375],\n",
      "        [0.5216]], device='mps:0')\n",
      "Iteration 19540 Training loss 0.09964967519044876 Validation loss 0.10642902553081512 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.3333],\n",
      "        [0.2701]], device='mps:0')\n",
      "Iteration 19550 Training loss 0.10816878825426102 Validation loss 0.10641948133707047 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.4591],\n",
      "        [0.4029]], device='mps:0')\n",
      "Iteration 19560 Training loss 0.10324244946241379 Validation loss 0.10641981661319733 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5975],\n",
      "        [0.2795]], device='mps:0')\n",
      "Iteration 19570 Training loss 0.11110866069793701 Validation loss 0.10640903562307358 Accuracy 0.690500020980835\n",
      "Output tensor([[0.6341],\n",
      "        [0.5728]], device='mps:0')\n",
      "Iteration 19580 Training loss 0.1095614805817604 Validation loss 0.106432244181633 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.4986],\n",
      "        [0.5271]], device='mps:0')\n",
      "Iteration 19590 Training loss 0.11528801918029785 Validation loss 0.1064242273569107 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.7007],\n",
      "        [0.4708]], device='mps:0')\n",
      "Iteration 19600 Training loss 0.11243871599435806 Validation loss 0.10646256059408188 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.4373],\n",
      "        [0.6293]], device='mps:0')\n",
      "Iteration 19610 Training loss 0.09835819900035858 Validation loss 0.1064344048500061 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.3774],\n",
      "        [0.5351]], device='mps:0')\n",
      "Iteration 19620 Training loss 0.11200477182865143 Validation loss 0.10639402270317078 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5797],\n",
      "        [0.5579]], device='mps:0')\n",
      "Iteration 19630 Training loss 0.10901090502738953 Validation loss 0.1063886433839798 Accuracy 0.690500020980835\n",
      "Output tensor([[0.6765],\n",
      "        [0.5555]], device='mps:0')\n",
      "Iteration 19640 Training loss 0.11515719443559647 Validation loss 0.10638761520385742 Accuracy 0.6890000104904175\n",
      "Output tensor([[0.3985],\n",
      "        [0.5809]], device='mps:0')\n",
      "Iteration 19650 Training loss 0.10694032162427902 Validation loss 0.10639319568872452 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.1784],\n",
      "        [0.5047]], device='mps:0')\n",
      "Iteration 19660 Training loss 0.10951227694749832 Validation loss 0.1063784509897232 Accuracy 0.690000057220459\n",
      "Output tensor([[0.4446],\n",
      "        [0.3564]], device='mps:0')\n",
      "Iteration 19670 Training loss 0.107912577688694 Validation loss 0.10637793689966202 Accuracy 0.6895000338554382\n",
      "Output tensor([[0.5755],\n",
      "        [0.4656]], device='mps:0')\n",
      "Iteration 19680 Training loss 0.11044763028621674 Validation loss 0.10636980831623077 Accuracy 0.690500020980835\n",
      "Output tensor([[0.6466],\n",
      "        [0.4639]], device='mps:0')\n",
      "Iteration 19690 Training loss 0.10865810513496399 Validation loss 0.10637281090021133 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5585],\n",
      "        [0.5526]], device='mps:0')\n",
      "Iteration 19700 Training loss 0.10385164618492126 Validation loss 0.1063682809472084 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.3378],\n",
      "        [0.5869]], device='mps:0')\n",
      "Iteration 19710 Training loss 0.10466083884239197 Validation loss 0.1063631922006607 Accuracy 0.690500020980835\n",
      "Output tensor([[0.5229],\n",
      "        [0.5135]], device='mps:0')\n",
      "Iteration 19720 Training loss 0.10291404277086258 Validation loss 0.10636793822050095 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.5471],\n",
      "        [0.5974]], device='mps:0')\n",
      "Iteration 19730 Training loss 0.10246356576681137 Validation loss 0.10636375844478607 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.5036],\n",
      "        [0.5116]], device='mps:0')\n",
      "Iteration 19740 Training loss 0.09923094511032104 Validation loss 0.10635682940483093 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.7278],\n",
      "        [0.2521]], device='mps:0')\n",
      "Iteration 19750 Training loss 0.10293246805667877 Validation loss 0.10635342448949814 Accuracy 0.690500020980835\n",
      "Output tensor([[0.3862],\n",
      "        [0.4976]], device='mps:0')\n",
      "Iteration 19760 Training loss 0.1073824092745781 Validation loss 0.10635069757699966 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.4863],\n",
      "        [0.6306]], device='mps:0')\n",
      "Iteration 19770 Training loss 0.1112850159406662 Validation loss 0.10634490847587585 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.2157],\n",
      "        [0.4870]], device='mps:0')\n",
      "Iteration 19780 Training loss 0.11901451647281647 Validation loss 0.10634537041187286 Accuracy 0.690500020980835\n",
      "Output tensor([[0.4485],\n",
      "        [0.5561]], device='mps:0')\n",
      "Iteration 19790 Training loss 0.11212614178657532 Validation loss 0.10633958876132965 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.6035],\n",
      "        [0.6307]], device='mps:0')\n",
      "Iteration 19800 Training loss 0.10461270809173584 Validation loss 0.10633853822946548 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.6317],\n",
      "        [0.6651]], device='mps:0')\n",
      "Iteration 19810 Training loss 0.10931643843650818 Validation loss 0.10634088516235352 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.4291],\n",
      "        [0.4927]], device='mps:0')\n",
      "Iteration 19820 Training loss 0.10320722311735153 Validation loss 0.10633146017789841 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.5439],\n",
      "        [0.6495]], device='mps:0')\n",
      "Iteration 19830 Training loss 0.11580920964479446 Validation loss 0.10632724314928055 Accuracy 0.690500020980835\n",
      "Output tensor([[0.6961],\n",
      "        [0.4358]], device='mps:0')\n",
      "Iteration 19840 Training loss 0.1053585410118103 Validation loss 0.10632306337356567 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.3301],\n",
      "        [0.6582]], device='mps:0')\n",
      "Iteration 19850 Training loss 0.11340972781181335 Validation loss 0.1063215434551239 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.5990],\n",
      "        [0.6009]], device='mps:0')\n",
      "Iteration 19860 Training loss 0.10493660718202591 Validation loss 0.10632113367319107 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.4992],\n",
      "        [0.5326]], device='mps:0')\n",
      "Iteration 19870 Training loss 0.10996925085783005 Validation loss 0.10632167011499405 Accuracy 0.690000057220459\n",
      "Output tensor([[0.5107],\n",
      "        [0.5642]], device='mps:0')\n",
      "Iteration 19880 Training loss 0.11551261693239212 Validation loss 0.10631480813026428 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.5764],\n",
      "        [0.5230]], device='mps:0')\n",
      "Iteration 19890 Training loss 0.1098821833729744 Validation loss 0.10630732029676437 Accuracy 0.690000057220459\n",
      "Output tensor([[0.2691],\n",
      "        [0.5645]], device='mps:0')\n",
      "Iteration 19900 Training loss 0.11696135997772217 Validation loss 0.10631070286035538 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.5182],\n",
      "        [0.4624]], device='mps:0')\n",
      "Iteration 19910 Training loss 0.09720633924007416 Validation loss 0.10630089789628983 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.4560],\n",
      "        [0.5967]], device='mps:0')\n",
      "Iteration 19920 Training loss 0.1168326735496521 Validation loss 0.10630043596029282 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.6048],\n",
      "        [0.4365]], device='mps:0')\n",
      "Iteration 19930 Training loss 0.09507804363965988 Validation loss 0.10631508380174637 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.6087],\n",
      "        [0.5574]], device='mps:0')\n",
      "Iteration 19940 Training loss 0.1087142825126648 Validation loss 0.10629304498434067 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.4095],\n",
      "        [0.2849]], device='mps:0')\n",
      "Iteration 19950 Training loss 0.10609256476163864 Validation loss 0.10628976672887802 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.3027],\n",
      "        [0.4587]], device='mps:0')\n",
      "Iteration 19960 Training loss 0.10875879973173141 Validation loss 0.1062876433134079 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.5210],\n",
      "        [0.5602]], device='mps:0')\n",
      "Iteration 19970 Training loss 0.11187499016523361 Validation loss 0.10628941655158997 Accuracy 0.690000057220459\n",
      "Output tensor([[0.5767],\n",
      "        [0.2797]], device='mps:0')\n",
      "Iteration 19980 Training loss 0.09973323345184326 Validation loss 0.10628873854875565 Accuracy 0.690500020980835\n",
      "Output tensor([[0.6993],\n",
      "        [0.1700]], device='mps:0')\n",
      "Iteration 19990 Training loss 0.10869117826223373 Validation loss 0.10629153996706009 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.4869],\n",
      "        [0.5567]], device='mps:0')\n",
      "Iteration 20000 Training loss 0.10585284233093262 Validation loss 0.10631962865591049 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.5825],\n",
      "        [0.2606]], device='mps:0')\n",
      "Iteration 20010 Training loss 0.10497021675109863 Validation loss 0.10629267990589142 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.6581],\n",
      "        [0.5857]], device='mps:0')\n",
      "Iteration 20020 Training loss 0.10030344128608704 Validation loss 0.10631611198186874 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.4330],\n",
      "        [0.6065]], device='mps:0')\n",
      "Iteration 20030 Training loss 0.09462336450815201 Validation loss 0.10634339600801468 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5568],\n",
      "        [0.3927]], device='mps:0')\n",
      "Iteration 20040 Training loss 0.12401264160871506 Validation loss 0.10634595155715942 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4360],\n",
      "        [0.4852]], device='mps:0')\n",
      "Iteration 20050 Training loss 0.11965920776128769 Validation loss 0.10627314448356628 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5141],\n",
      "        [0.3426]], device='mps:0')\n",
      "Iteration 20060 Training loss 0.10963653028011322 Validation loss 0.10628170520067215 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.6413],\n",
      "        [0.7313]], device='mps:0')\n",
      "Iteration 20070 Training loss 0.10880811512470245 Validation loss 0.10629935562610626 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.7186],\n",
      "        [0.6284]], device='mps:0')\n",
      "Iteration 20080 Training loss 0.10328175127506256 Validation loss 0.10627897828817368 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.4819],\n",
      "        [0.6430]], device='mps:0')\n",
      "Iteration 20090 Training loss 0.10866369307041168 Validation loss 0.10630978643894196 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5578],\n",
      "        [0.4381]], device='mps:0')\n",
      "Iteration 20100 Training loss 0.10960002988576889 Validation loss 0.10636435449123383 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.3596],\n",
      "        [0.4479]], device='mps:0')\n",
      "Iteration 20110 Training loss 0.1107824295759201 Validation loss 0.10631895810365677 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4593],\n",
      "        [0.5469]], device='mps:0')\n",
      "Iteration 20120 Training loss 0.10710189491510391 Validation loss 0.10632048547267914 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4363],\n",
      "        [0.6612]], device='mps:0')\n",
      "Iteration 20130 Training loss 0.09966462850570679 Validation loss 0.10637189447879791 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.4806],\n",
      "        [0.3264]], device='mps:0')\n",
      "Iteration 20140 Training loss 0.113789863884449 Validation loss 0.10631100833415985 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.3060],\n",
      "        [0.5365]], device='mps:0')\n",
      "Iteration 20150 Training loss 0.10635218024253845 Validation loss 0.10628707706928253 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.3952],\n",
      "        [0.6151]], device='mps:0')\n",
      "Iteration 20160 Training loss 0.10952012985944748 Validation loss 0.10626405477523804 Accuracy 0.6910000443458557\n",
      "Output tensor([[0.5559],\n",
      "        [0.6988]], device='mps:0')\n",
      "Iteration 20170 Training loss 0.10387949645519257 Validation loss 0.10630742460489273 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5747],\n",
      "        [0.4147]], device='mps:0')\n",
      "Iteration 20180 Training loss 0.11304273456335068 Validation loss 0.10633734613656998 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5235],\n",
      "        [0.5488]], device='mps:0')\n",
      "Iteration 20190 Training loss 0.0992448627948761 Validation loss 0.10628628730773926 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.6304],\n",
      "        [0.2761]], device='mps:0')\n",
      "Iteration 20200 Training loss 0.11193320900201797 Validation loss 0.1062508225440979 Accuracy 0.690500020980835\n",
      "Output tensor([[0.5163],\n",
      "        [0.4253]], device='mps:0')\n",
      "Iteration 20210 Training loss 0.10238756239414215 Validation loss 0.10623609274625778 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.6154],\n",
      "        [0.3716]], device='mps:0')\n",
      "Iteration 20220 Training loss 0.09998948127031326 Validation loss 0.10622017085552216 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.4930],\n",
      "        [0.4529]], device='mps:0')\n",
      "Iteration 20230 Training loss 0.11274641007184982 Validation loss 0.10619764029979706 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.5761],\n",
      "        [0.5769]], device='mps:0')\n",
      "Iteration 20240 Training loss 0.10803472250699997 Validation loss 0.10619568824768066 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.5794],\n",
      "        [0.5755]], device='mps:0')\n",
      "Iteration 20250 Training loss 0.1066310852766037 Validation loss 0.10619282722473145 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.5658],\n",
      "        [0.5618]], device='mps:0')\n",
      "Iteration 20260 Training loss 0.11535610258579254 Validation loss 0.10619234293699265 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.3600],\n",
      "        [0.6230]], device='mps:0')\n",
      "Iteration 20270 Training loss 0.10504806041717529 Validation loss 0.10618593543767929 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.6100],\n",
      "        [0.6856]], device='mps:0')\n",
      "Iteration 20280 Training loss 0.10440690070390701 Validation loss 0.1061832457780838 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.6083],\n",
      "        [0.3801]], device='mps:0')\n",
      "Iteration 20290 Training loss 0.10493756830692291 Validation loss 0.10617835074663162 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.6347],\n",
      "        [0.5886]], device='mps:0')\n",
      "Iteration 20300 Training loss 0.11087821424007416 Validation loss 0.10618290305137634 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.3149],\n",
      "        [0.5302]], device='mps:0')\n",
      "Iteration 20310 Training loss 0.10151897370815277 Validation loss 0.10617455095052719 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.2650],\n",
      "        [0.5040]], device='mps:0')\n",
      "Iteration 20320 Training loss 0.10607656836509705 Validation loss 0.10616867244243622 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.6318],\n",
      "        [0.6053]], device='mps:0')\n",
      "Iteration 20330 Training loss 0.106116384267807 Validation loss 0.10616485029459 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5200],\n",
      "        [0.2482]], device='mps:0')\n",
      "Iteration 20340 Training loss 0.10684297233819962 Validation loss 0.10616781562566757 Accuracy 0.690500020980835\n",
      "Output tensor([[0.5583],\n",
      "        [0.6155]], device='mps:0')\n",
      "Iteration 20350 Training loss 0.10054273158311844 Validation loss 0.10616832226514816 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5159],\n",
      "        [0.4471]], device='mps:0')\n",
      "Iteration 20360 Training loss 0.10372002422809601 Validation loss 0.10616524517536163 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.3423],\n",
      "        [0.5318]], device='mps:0')\n",
      "Iteration 20370 Training loss 0.10994990915060043 Validation loss 0.10615222901105881 Accuracy 0.6915000081062317\n",
      "Output tensor([[0.3763],\n",
      "        [0.4618]], device='mps:0')\n",
      "Iteration 20380 Training loss 0.1230262890458107 Validation loss 0.10615077614784241 Accuracy 0.690000057220459\n",
      "Output tensor([[0.3660],\n",
      "        [0.4662]], device='mps:0')\n",
      "Iteration 20390 Training loss 0.0985245332121849 Validation loss 0.10615076869726181 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.6181],\n",
      "        [0.4119]], device='mps:0')\n",
      "Iteration 20400 Training loss 0.1072232574224472 Validation loss 0.10614349693059921 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.3861],\n",
      "        [0.6699]], device='mps:0')\n",
      "Iteration 20410 Training loss 0.11007560789585114 Validation loss 0.10614465922117233 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.3275],\n",
      "        [0.3497]], device='mps:0')\n",
      "Iteration 20420 Training loss 0.11529640108346939 Validation loss 0.10615282505750656 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5316],\n",
      "        [0.2185]], device='mps:0')\n",
      "Iteration 20430 Training loss 0.10998459905385971 Validation loss 0.10620585829019547 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.3735],\n",
      "        [0.5363]], device='mps:0')\n",
      "Iteration 20440 Training loss 0.10851125419139862 Validation loss 0.10618489235639572 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.6850],\n",
      "        [0.4606]], device='mps:0')\n",
      "Iteration 20450 Training loss 0.10367105156183243 Validation loss 0.10616908967494965 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.4084],\n",
      "        [0.2655]], device='mps:0')\n",
      "Iteration 20460 Training loss 0.11392413079738617 Validation loss 0.10624231398105621 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6650],\n",
      "        [0.5111]], device='mps:0')\n",
      "Iteration 20470 Training loss 0.1038459911942482 Validation loss 0.10617242008447647 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.3305],\n",
      "        [0.4282]], device='mps:0')\n",
      "Iteration 20480 Training loss 0.1084171012043953 Validation loss 0.10616642981767654 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.3759],\n",
      "        [0.3196]], device='mps:0')\n",
      "Iteration 20490 Training loss 0.10195907205343246 Validation loss 0.10616049915552139 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5609],\n",
      "        [0.5014]], device='mps:0')\n",
      "Iteration 20500 Training loss 0.1088455468416214 Validation loss 0.10616510361433029 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.7093],\n",
      "        [0.3765]], device='mps:0')\n",
      "Iteration 20510 Training loss 0.1065964475274086 Validation loss 0.10614604502916336 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.4140],\n",
      "        [0.3632]], device='mps:0')\n",
      "Iteration 20520 Training loss 0.10843263566493988 Validation loss 0.10615745931863785 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5064],\n",
      "        [0.6771]], device='mps:0')\n",
      "Iteration 20530 Training loss 0.10204265266656876 Validation loss 0.10615482181310654 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.4709],\n",
      "        [0.4182]], device='mps:0')\n",
      "Iteration 20540 Training loss 0.10813912749290466 Validation loss 0.10611266642808914 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.3235],\n",
      "        [0.5421]], device='mps:0')\n",
      "Iteration 20550 Training loss 0.10808052122592926 Validation loss 0.10611366480588913 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.6120],\n",
      "        [0.3802]], device='mps:0')\n",
      "Iteration 20560 Training loss 0.10494304448366165 Validation loss 0.10609897971153259 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5210],\n",
      "        [0.6472]], device='mps:0')\n",
      "Iteration 20570 Training loss 0.11372453719377518 Validation loss 0.10610854625701904 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5386],\n",
      "        [0.5586]], device='mps:0')\n",
      "Iteration 20580 Training loss 0.09925208985805511 Validation loss 0.1061500608921051 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.4017],\n",
      "        [0.5567]], device='mps:0')\n",
      "Iteration 20590 Training loss 0.10194169729948044 Validation loss 0.10618039220571518 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.5917],\n",
      "        [0.3836]], device='mps:0')\n",
      "Iteration 20600 Training loss 0.11055774241685867 Validation loss 0.10620217025279999 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.4630],\n",
      "        [0.4295]], device='mps:0')\n",
      "Iteration 20610 Training loss 0.09475214779376984 Validation loss 0.10615706443786621 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.4373],\n",
      "        [0.6268]], device='mps:0')\n",
      "Iteration 20620 Training loss 0.1050058901309967 Validation loss 0.10616936534643173 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6085],\n",
      "        [0.4927]], device='mps:0')\n",
      "Iteration 20630 Training loss 0.10887797921895981 Validation loss 0.10619477182626724 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.2763],\n",
      "        [0.5576]], device='mps:0')\n",
      "Iteration 20640 Training loss 0.11388738453388214 Validation loss 0.10614535957574844 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.6174],\n",
      "        [0.4552]], device='mps:0')\n",
      "Iteration 20650 Training loss 0.1097048670053482 Validation loss 0.10619966685771942 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.3521],\n",
      "        [0.6173]], device='mps:0')\n",
      "Iteration 20660 Training loss 0.10848543047904968 Validation loss 0.10607877373695374 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5950],\n",
      "        [0.7287]], device='mps:0')\n",
      "Iteration 20670 Training loss 0.11296924203634262 Validation loss 0.1060733050107956 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.5402],\n",
      "        [0.5093]], device='mps:0')\n",
      "Iteration 20680 Training loss 0.11800584197044373 Validation loss 0.10607164353132248 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.3500],\n",
      "        [0.5266]], device='mps:0')\n",
      "Iteration 20690 Training loss 0.10721612721681595 Validation loss 0.1060740128159523 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.6292],\n",
      "        [0.5117]], device='mps:0')\n",
      "Iteration 20700 Training loss 0.11708386987447739 Validation loss 0.10613246262073517 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.3932],\n",
      "        [0.6525]], device='mps:0')\n",
      "Iteration 20710 Training loss 0.11124607920646667 Validation loss 0.10611894726753235 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5572],\n",
      "        [0.6847]], device='mps:0')\n",
      "Iteration 20720 Training loss 0.10575935989618301 Validation loss 0.10610809922218323 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.4363],\n",
      "        [0.4680]], device='mps:0')\n",
      "Iteration 20730 Training loss 0.10946931689977646 Validation loss 0.10608282685279846 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.4414],\n",
      "        [0.6204]], device='mps:0')\n",
      "Iteration 20740 Training loss 0.09729012101888657 Validation loss 0.10606564581394196 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5574],\n",
      "        [0.6530]], device='mps:0')\n",
      "Iteration 20750 Training loss 0.10731177031993866 Validation loss 0.1060933992266655 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.3472],\n",
      "        [0.4585]], device='mps:0')\n",
      "Iteration 20760 Training loss 0.11303111165761948 Validation loss 0.10604587197303772 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.5995],\n",
      "        [0.4721]], device='mps:0')\n",
      "Iteration 20770 Training loss 0.10102098435163498 Validation loss 0.10606042295694351 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.4870],\n",
      "        [0.5105]], device='mps:0')\n",
      "Iteration 20780 Training loss 0.10296917706727982 Validation loss 0.10606635361909866 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.2762],\n",
      "        [0.2650]], device='mps:0')\n",
      "Iteration 20790 Training loss 0.1074225977063179 Validation loss 0.10605667531490326 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.7029],\n",
      "        [0.4283]], device='mps:0')\n",
      "Iteration 20800 Training loss 0.1052308976650238 Validation loss 0.10605065524578094 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5222],\n",
      "        [0.6774]], device='mps:0')\n",
      "Iteration 20810 Training loss 0.10608042776584625 Validation loss 0.10602439194917679 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.4879],\n",
      "        [0.6582]], device='mps:0')\n",
      "Iteration 20820 Training loss 0.09808485954999924 Validation loss 0.10602455586194992 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.3647],\n",
      "        [0.6242]], device='mps:0')\n",
      "Iteration 20830 Training loss 0.10768575221300125 Validation loss 0.10602281242609024 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.2422],\n",
      "        [0.6426]], device='mps:0')\n",
      "Iteration 20840 Training loss 0.10897515714168549 Validation loss 0.10602404922246933 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5430],\n",
      "        [0.5833]], device='mps:0')\n",
      "Iteration 20850 Training loss 0.10811574012041092 Validation loss 0.10601522773504257 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5864],\n",
      "        [0.4535]], device='mps:0')\n",
      "Iteration 20860 Training loss 0.11029933393001556 Validation loss 0.1060090959072113 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.3510],\n",
      "        [0.4273]], device='mps:0')\n",
      "Iteration 20870 Training loss 0.10910366475582123 Validation loss 0.1060166209936142 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.4036],\n",
      "        [0.4923]], device='mps:0')\n",
      "Iteration 20880 Training loss 0.11458730697631836 Validation loss 0.10602077096700668 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.6280],\n",
      "        [0.2773]], device='mps:0')\n",
      "Iteration 20890 Training loss 0.10492119193077087 Validation loss 0.106003038585186 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5279],\n",
      "        [0.5707]], device='mps:0')\n",
      "Iteration 20900 Training loss 0.10352911055088043 Validation loss 0.10601427406072617 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.6345],\n",
      "        [0.6032]], device='mps:0')\n",
      "Iteration 20910 Training loss 0.10974868386983871 Validation loss 0.1060040146112442 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.3950],\n",
      "        [0.5957]], device='mps:0')\n",
      "Iteration 20920 Training loss 0.10347262769937515 Validation loss 0.10599811375141144 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.3550],\n",
      "        [0.5079]], device='mps:0')\n",
      "Iteration 20930 Training loss 0.09748151898384094 Validation loss 0.10601603239774704 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5452],\n",
      "        [0.4016]], device='mps:0')\n",
      "Iteration 20940 Training loss 0.11703464388847351 Validation loss 0.1060040071606636 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5554],\n",
      "        [0.4805]], device='mps:0')\n",
      "Iteration 20950 Training loss 0.11537036299705505 Validation loss 0.10600680857896805 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5804],\n",
      "        [0.5810]], device='mps:0')\n",
      "Iteration 20960 Training loss 0.10387767851352692 Validation loss 0.1060132309794426 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5550],\n",
      "        [0.5471]], device='mps:0')\n",
      "Iteration 20970 Training loss 0.10160552710294724 Validation loss 0.10600624233484268 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.4099],\n",
      "        [0.4268]], device='mps:0')\n",
      "Iteration 20980 Training loss 0.10406765341758728 Validation loss 0.1059853583574295 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5417],\n",
      "        [0.4637]], device='mps:0')\n",
      "Iteration 20990 Training loss 0.10385705530643463 Validation loss 0.10599194467067719 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.3668],\n",
      "        [0.4062]], device='mps:0')\n",
      "Iteration 21000 Training loss 0.10480745136737823 Validation loss 0.10601526498794556 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.1554],\n",
      "        [0.4977]], device='mps:0')\n",
      "Iteration 21010 Training loss 0.1034519150853157 Validation loss 0.10599100589752197 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5313],\n",
      "        [0.5169]], device='mps:0')\n",
      "Iteration 21020 Training loss 0.1067403182387352 Validation loss 0.10601794719696045 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5009],\n",
      "        [0.5375]], device='mps:0')\n",
      "Iteration 21030 Training loss 0.10968219488859177 Validation loss 0.10599834471940994 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.3122],\n",
      "        [0.4751]], device='mps:0')\n",
      "Iteration 21040 Training loss 0.11104120314121246 Validation loss 0.10596650838851929 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5431],\n",
      "        [0.5570]], device='mps:0')\n",
      "Iteration 21050 Training loss 0.10962581634521484 Validation loss 0.10595925897359848 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.4759],\n",
      "        [0.5436]], device='mps:0')\n",
      "Iteration 21060 Training loss 0.10802794992923737 Validation loss 0.10598112642765045 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.3899],\n",
      "        [0.5445]], device='mps:0')\n",
      "Iteration 21070 Training loss 0.10256054997444153 Validation loss 0.10596082359552383 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.1189],\n",
      "        [0.4069]], device='mps:0')\n",
      "Iteration 21080 Training loss 0.11153839528560638 Validation loss 0.10594232380390167 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5493],\n",
      "        [0.6157]], device='mps:0')\n",
      "Iteration 21090 Training loss 0.10597635060548782 Validation loss 0.10598406940698624 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5193],\n",
      "        [0.4715]], device='mps:0')\n",
      "Iteration 21100 Training loss 0.11045831441879272 Validation loss 0.10599739104509354 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5356],\n",
      "        [0.5270]], device='mps:0')\n",
      "Iteration 21110 Training loss 0.101790651679039 Validation loss 0.10602504014968872 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.6653],\n",
      "        [0.5479]], device='mps:0')\n",
      "Iteration 21120 Training loss 0.10547883063554764 Validation loss 0.10599817335605621 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6304],\n",
      "        [0.4271]], device='mps:0')\n",
      "Iteration 21130 Training loss 0.11460128426551819 Validation loss 0.10600215941667557 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.2450],\n",
      "        [0.5023]], device='mps:0')\n",
      "Iteration 21140 Training loss 0.10910972207784653 Validation loss 0.10596339404582977 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.4835],\n",
      "        [0.6151]], device='mps:0')\n",
      "Iteration 21150 Training loss 0.12050620466470718 Validation loss 0.10594353079795837 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.3969],\n",
      "        [0.4996]], device='mps:0')\n",
      "Iteration 21160 Training loss 0.10106652230024338 Validation loss 0.10592474788427353 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5729],\n",
      "        [0.4315]], device='mps:0')\n",
      "Iteration 21170 Training loss 0.11229726672172546 Validation loss 0.10591231286525726 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.5507],\n",
      "        [0.3144]], device='mps:0')\n",
      "Iteration 21180 Training loss 0.1081012710928917 Validation loss 0.10590554028749466 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.4506],\n",
      "        [0.4968]], device='mps:0')\n",
      "Iteration 21190 Training loss 0.10283830761909485 Validation loss 0.10590904951095581 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5987],\n",
      "        [0.4644]], device='mps:0')\n",
      "Iteration 21200 Training loss 0.11399278044700623 Validation loss 0.10590129345655441 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.4517],\n",
      "        [0.4603]], device='mps:0')\n",
      "Iteration 21210 Training loss 0.10367343574762344 Validation loss 0.10590062290430069 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.4838],\n",
      "        [0.6380]], device='mps:0')\n",
      "Iteration 21220 Training loss 0.10191642493009567 Validation loss 0.10589917749166489 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.3717],\n",
      "        [0.4754]], device='mps:0')\n",
      "Iteration 21230 Training loss 0.1015467494726181 Validation loss 0.10590370744466782 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.6903],\n",
      "        [0.5188]], device='mps:0')\n",
      "Iteration 21240 Training loss 0.10108727961778641 Validation loss 0.10590088367462158 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5079],\n",
      "        [0.6529]], device='mps:0')\n",
      "Iteration 21250 Training loss 0.11213549971580505 Validation loss 0.1059422641992569 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6352],\n",
      "        [0.3476]], device='mps:0')\n",
      "Iteration 21260 Training loss 0.10882266610860825 Validation loss 0.10590662062168121 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5439],\n",
      "        [0.5851]], device='mps:0')\n",
      "Iteration 21270 Training loss 0.10740584880113602 Validation loss 0.10594428330659866 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.3441],\n",
      "        [0.5346]], device='mps:0')\n",
      "Iteration 21280 Training loss 0.11014250665903091 Validation loss 0.10594115406274796 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6279],\n",
      "        [0.3861]], device='mps:0')\n",
      "Iteration 21290 Training loss 0.1022137999534607 Validation loss 0.10589674860239029 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.6099],\n",
      "        [0.4699]], device='mps:0')\n",
      "Iteration 21300 Training loss 0.10613203048706055 Validation loss 0.10590727627277374 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5226],\n",
      "        [0.1939]], device='mps:0')\n",
      "Iteration 21310 Training loss 0.09381450712680817 Validation loss 0.10591226816177368 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.6748],\n",
      "        [0.5597]], device='mps:0')\n",
      "Iteration 21320 Training loss 0.10514766722917557 Validation loss 0.105914406478405 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.6023],\n",
      "        [0.4632]], device='mps:0')\n",
      "Iteration 21330 Training loss 0.11232022941112518 Validation loss 0.1058693677186966 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.3326],\n",
      "        [0.4526]], device='mps:0')\n",
      "Iteration 21340 Training loss 0.11494405567646027 Validation loss 0.10587149858474731 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.5618],\n",
      "        [0.5696]], device='mps:0')\n",
      "Iteration 21350 Training loss 0.09634212404489517 Validation loss 0.10588651895523071 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.3215],\n",
      "        [0.5790]], device='mps:0')\n",
      "Iteration 21360 Training loss 0.11277274787425995 Validation loss 0.105876125395298 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.3651],\n",
      "        [0.4402]], device='mps:0')\n",
      "Iteration 21370 Training loss 0.1013084128499031 Validation loss 0.10585635155439377 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5504],\n",
      "        [0.5449]], device='mps:0')\n",
      "Iteration 21380 Training loss 0.09608718752861023 Validation loss 0.10586513578891754 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5882],\n",
      "        [0.4378]], device='mps:0')\n",
      "Iteration 21390 Training loss 0.10650022327899933 Validation loss 0.1058841124176979 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.4271],\n",
      "        [0.3445]], device='mps:0')\n",
      "Iteration 21400 Training loss 0.09964599460363388 Validation loss 0.10594765841960907 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5737],\n",
      "        [0.3053]], device='mps:0')\n",
      "Iteration 21410 Training loss 0.10116391628980637 Validation loss 0.10588251054286957 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.4518],\n",
      "        [0.5283]], device='mps:0')\n",
      "Iteration 21420 Training loss 0.10486529767513275 Validation loss 0.10588084161281586 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.5364],\n",
      "        [0.4698]], device='mps:0')\n",
      "Iteration 21430 Training loss 0.1141551211476326 Validation loss 0.10589949041604996 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5922],\n",
      "        [0.3613]], device='mps:0')\n",
      "Iteration 21440 Training loss 0.10032645612955093 Validation loss 0.10594066977500916 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.2326],\n",
      "        [0.4076]], device='mps:0')\n",
      "Iteration 21450 Training loss 0.1064055860042572 Validation loss 0.10592836141586304 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4184],\n",
      "        [0.5538]], device='mps:0')\n",
      "Iteration 21460 Training loss 0.10445889830589294 Validation loss 0.10593056678771973 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4905],\n",
      "        [0.3053]], device='mps:0')\n",
      "Iteration 21470 Training loss 0.10939500480890274 Validation loss 0.10592461377382278 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6923],\n",
      "        [0.6782]], device='mps:0')\n",
      "Iteration 21480 Training loss 0.09417048841714859 Validation loss 0.105929434299469 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5305],\n",
      "        [0.4859]], device='mps:0')\n",
      "Iteration 21490 Training loss 0.09733480215072632 Validation loss 0.10597972571849823 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5743],\n",
      "        [0.3938]], device='mps:0')\n",
      "Iteration 21500 Training loss 0.1136956587433815 Validation loss 0.10594591498374939 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4823],\n",
      "        [0.3664]], device='mps:0')\n",
      "Iteration 21510 Training loss 0.10936857759952545 Validation loss 0.10587761551141739 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.4990],\n",
      "        [0.5200]], device='mps:0')\n",
      "Iteration 21520 Training loss 0.11282264441251755 Validation loss 0.1059022843837738 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.5804],\n",
      "        [0.5444]], device='mps:0')\n",
      "Iteration 21530 Training loss 0.11380865424871445 Validation loss 0.1058356910943985 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5082],\n",
      "        [0.5588]], device='mps:0')\n",
      "Iteration 21540 Training loss 0.10858869552612305 Validation loss 0.10582384467124939 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.2500],\n",
      "        [0.1859]], device='mps:0')\n",
      "Iteration 21550 Training loss 0.11441214382648468 Validation loss 0.10584612190723419 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.5099],\n",
      "        [0.2903]], device='mps:0')\n",
      "Iteration 21560 Training loss 0.1141597256064415 Validation loss 0.10584134608507156 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.4011],\n",
      "        [0.4852]], device='mps:0')\n",
      "Iteration 21570 Training loss 0.10163955390453339 Validation loss 0.10582306236028671 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5245],\n",
      "        [0.1926]], device='mps:0')\n",
      "Iteration 21580 Training loss 0.10031397640705109 Validation loss 0.10582945495843887 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.1535],\n",
      "        [0.5138]], device='mps:0')\n",
      "Iteration 21590 Training loss 0.10802674293518066 Validation loss 0.10582476109266281 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.4167],\n",
      "        [0.5479]], device='mps:0')\n",
      "Iteration 21600 Training loss 0.1055421531200409 Validation loss 0.10581930726766586 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5937],\n",
      "        [0.4472]], device='mps:0')\n",
      "Iteration 21610 Training loss 0.10413655638694763 Validation loss 0.10583047568798065 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.4620],\n",
      "        [0.4844]], device='mps:0')\n",
      "Iteration 21620 Training loss 0.10758914798498154 Validation loss 0.1058068573474884 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.1969],\n",
      "        [0.1257]], device='mps:0')\n",
      "Iteration 21630 Training loss 0.10012464225292206 Validation loss 0.1058129146695137 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.3530],\n",
      "        [0.4754]], device='mps:0')\n",
      "Iteration 21640 Training loss 0.10506968945264816 Validation loss 0.1058604046702385 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.6829],\n",
      "        [0.5572]], device='mps:0')\n",
      "Iteration 21650 Training loss 0.10835521668195724 Validation loss 0.10583040118217468 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5428],\n",
      "        [0.6865]], device='mps:0')\n",
      "Iteration 21660 Training loss 0.11151713877916336 Validation loss 0.10581554472446442 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.6206],\n",
      "        [0.6936]], device='mps:0')\n",
      "Iteration 21670 Training loss 0.11091256141662598 Validation loss 0.10582093149423599 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.2072],\n",
      "        [0.7322]], device='mps:0')\n",
      "Iteration 21680 Training loss 0.11585736274719238 Validation loss 0.10582118481397629 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6681],\n",
      "        [0.5535]], device='mps:0')\n",
      "Iteration 21690 Training loss 0.11056062579154968 Validation loss 0.10577219724655151 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.3484],\n",
      "        [0.4419]], device='mps:0')\n",
      "Iteration 21700 Training loss 0.10618579387664795 Validation loss 0.10576526075601578 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5406],\n",
      "        [0.6024]], device='mps:0')\n",
      "Iteration 21710 Training loss 0.10780072957277298 Validation loss 0.10576033592224121 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.4563],\n",
      "        [0.6557]], device='mps:0')\n",
      "Iteration 21720 Training loss 0.11642567068338394 Validation loss 0.10575518012046814 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.2041],\n",
      "        [0.6035]], device='mps:0')\n",
      "Iteration 21730 Training loss 0.09792281687259674 Validation loss 0.10576939582824707 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5429],\n",
      "        [0.7126]], device='mps:0')\n",
      "Iteration 21740 Training loss 0.10638576000928879 Validation loss 0.10577276349067688 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.6279],\n",
      "        [0.5490]], device='mps:0')\n",
      "Iteration 21750 Training loss 0.1166515052318573 Validation loss 0.10575545579195023 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.3224],\n",
      "        [0.4590]], device='mps:0')\n",
      "Iteration 21760 Training loss 0.0957716852426529 Validation loss 0.1057506799697876 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.4647],\n",
      "        [0.6475]], device='mps:0')\n",
      "Iteration 21770 Training loss 0.10467774420976639 Validation loss 0.10575813055038452 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.3625],\n",
      "        [0.3851]], device='mps:0')\n",
      "Iteration 21780 Training loss 0.10666132718324661 Validation loss 0.10575035959482193 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.2671],\n",
      "        [0.4785]], device='mps:0')\n",
      "Iteration 21790 Training loss 0.09374084323644638 Validation loss 0.10575821250677109 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5303],\n",
      "        [0.5323]], device='mps:0')\n",
      "Iteration 21800 Training loss 0.11378476768732071 Validation loss 0.10575248301029205 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.4677],\n",
      "        [0.3141]], device='mps:0')\n",
      "Iteration 21810 Training loss 0.10844948887825012 Validation loss 0.10576698929071426 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.6203],\n",
      "        [0.3063]], device='mps:0')\n",
      "Iteration 21820 Training loss 0.1113256886601448 Validation loss 0.10575277358293533 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.2800],\n",
      "        [0.7176]], device='mps:0')\n",
      "Iteration 21830 Training loss 0.10011378675699234 Validation loss 0.10574065148830414 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.4742],\n",
      "        [0.5170]], device='mps:0')\n",
      "Iteration 21840 Training loss 0.10138747841119766 Validation loss 0.10572990030050278 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.4712],\n",
      "        [0.2509]], device='mps:0')\n",
      "Iteration 21850 Training loss 0.1105395257472992 Validation loss 0.10574068129062653 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.4109],\n",
      "        [0.4916]], device='mps:0')\n",
      "Iteration 21860 Training loss 0.1086358055472374 Validation loss 0.10571528226137161 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.4993],\n",
      "        [0.6075]], device='mps:0')\n",
      "Iteration 21870 Training loss 0.0995827466249466 Validation loss 0.10573277622461319 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.6749],\n",
      "        [0.5519]], device='mps:0')\n",
      "Iteration 21880 Training loss 0.10545460134744644 Validation loss 0.10573098063468933 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.4143],\n",
      "        [0.5716]], device='mps:0')\n",
      "Iteration 21890 Training loss 0.10910085588693619 Validation loss 0.10572279244661331 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5263],\n",
      "        [0.2322]], device='mps:0')\n",
      "Iteration 21900 Training loss 0.10218087583780289 Validation loss 0.1057003065943718 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6283],\n",
      "        [0.5093]], device='mps:0')\n",
      "Iteration 21910 Training loss 0.10797800123691559 Validation loss 0.10572567582130432 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.1824],\n",
      "        [0.4836]], device='mps:0')\n",
      "Iteration 21920 Training loss 0.10875511169433594 Validation loss 0.10571939498186111 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.7339],\n",
      "        [0.4984]], device='mps:0')\n",
      "Iteration 21930 Training loss 0.10425575077533722 Validation loss 0.10571075975894928 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.3926],\n",
      "        [0.5362]], device='mps:0')\n",
      "Iteration 21940 Training loss 0.11730395257472992 Validation loss 0.10572395473718643 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.2517],\n",
      "        [0.6281]], device='mps:0')\n",
      "Iteration 21950 Training loss 0.0981784239411354 Validation loss 0.10572463274002075 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.2524],\n",
      "        [0.3843]], device='mps:0')\n",
      "Iteration 21960 Training loss 0.09873420745134354 Validation loss 0.10576213896274567 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6303],\n",
      "        [0.5935]], device='mps:0')\n",
      "Iteration 21970 Training loss 0.11231645196676254 Validation loss 0.10576031357049942 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5568],\n",
      "        [0.6321]], device='mps:0')\n",
      "Iteration 21980 Training loss 0.10401291400194168 Validation loss 0.10572615265846252 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5192],\n",
      "        [0.4722]], device='mps:0')\n",
      "Iteration 21990 Training loss 0.10969361662864685 Validation loss 0.10575444251298904 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.3196],\n",
      "        [0.4568]], device='mps:0')\n",
      "Iteration 22000 Training loss 0.09033262729644775 Validation loss 0.10575682669878006 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.3940],\n",
      "        [0.4800]], device='mps:0')\n",
      "Iteration 22010 Training loss 0.09469369053840637 Validation loss 0.10574084520339966 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.3718],\n",
      "        [0.1702]], device='mps:0')\n",
      "Iteration 22020 Training loss 0.11405414342880249 Validation loss 0.10571510344743729 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5390],\n",
      "        [0.6393]], device='mps:0')\n",
      "Iteration 22030 Training loss 0.10860078781843185 Validation loss 0.10572712868452072 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.3874],\n",
      "        [0.3455]], device='mps:0')\n",
      "Iteration 22040 Training loss 0.11529266089200974 Validation loss 0.10571803897619247 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5654],\n",
      "        [0.6095]], device='mps:0')\n",
      "Iteration 22050 Training loss 0.09384637326002121 Validation loss 0.10574624687433243 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4164],\n",
      "        [0.1987]], device='mps:0')\n",
      "Iteration 22060 Training loss 0.1169949397444725 Validation loss 0.1057448759675026 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5313],\n",
      "        [0.3728]], device='mps:0')\n",
      "Iteration 22070 Training loss 0.10683580487966537 Validation loss 0.10583371669054031 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5565],\n",
      "        [0.6188]], device='mps:0')\n",
      "Iteration 22080 Training loss 0.10539645701646805 Validation loss 0.10574095696210861 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5331],\n",
      "        [0.6047]], device='mps:0')\n",
      "Iteration 22090 Training loss 0.1061176061630249 Validation loss 0.10575822740793228 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6613],\n",
      "        [0.6078]], device='mps:0')\n",
      "Iteration 22100 Training loss 0.10709236562252045 Validation loss 0.10576257854700089 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.3813],\n",
      "        [0.2580]], device='mps:0')\n",
      "Iteration 22110 Training loss 0.11005619168281555 Validation loss 0.10569977015256882 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.4467],\n",
      "        [0.5488]], device='mps:0')\n",
      "Iteration 22120 Training loss 0.11477170884609222 Validation loss 0.1056734174489975 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5221],\n",
      "        [0.5839]], device='mps:0')\n",
      "Iteration 22130 Training loss 0.09899555891752243 Validation loss 0.1056554988026619 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.3569],\n",
      "        [0.5069]], device='mps:0')\n",
      "Iteration 22140 Training loss 0.0948391705751419 Validation loss 0.10568094253540039 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.6742],\n",
      "        [0.3697]], device='mps:0')\n",
      "Iteration 22150 Training loss 0.0960983857512474 Validation loss 0.10570356249809265 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4741],\n",
      "        [0.5265]], device='mps:0')\n",
      "Iteration 22160 Training loss 0.1104474812746048 Validation loss 0.1057114377617836 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5193],\n",
      "        [0.4057]], device='mps:0')\n",
      "Iteration 22170 Training loss 0.10699684172868729 Validation loss 0.10571151971817017 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4349],\n",
      "        [0.2792]], device='mps:0')\n",
      "Iteration 22180 Training loss 0.09504802525043488 Validation loss 0.10569481551647186 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.3627],\n",
      "        [0.6961]], device='mps:0')\n",
      "Iteration 22190 Training loss 0.10801122337579727 Validation loss 0.10573306679725647 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5667],\n",
      "        [0.6779]], device='mps:0')\n",
      "Iteration 22200 Training loss 0.09756162762641907 Validation loss 0.10571736842393875 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6425],\n",
      "        [0.6926]], device='mps:0')\n",
      "Iteration 22210 Training loss 0.10068655759096146 Validation loss 0.10572432726621628 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6940],\n",
      "        [0.4752]], device='mps:0')\n",
      "Iteration 22220 Training loss 0.10729129612445831 Validation loss 0.10563915222883224 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5607],\n",
      "        [0.7016]], device='mps:0')\n",
      "Iteration 22230 Training loss 0.10396277904510498 Validation loss 0.10562501847743988 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5522],\n",
      "        [0.2991]], device='mps:0')\n",
      "Iteration 22240 Training loss 0.10516277700662613 Validation loss 0.10566374659538269 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.2033],\n",
      "        [0.5234]], device='mps:0')\n",
      "Iteration 22250 Training loss 0.09600148350000381 Validation loss 0.10570433735847473 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.3630],\n",
      "        [0.4812]], device='mps:0')\n",
      "Iteration 22260 Training loss 0.09678774327039719 Validation loss 0.1057315468788147 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.2799],\n",
      "        [0.6121]], device='mps:0')\n",
      "Iteration 22270 Training loss 0.09998928755521774 Validation loss 0.10568788647651672 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5749],\n",
      "        [0.5040]], device='mps:0')\n",
      "Iteration 22280 Training loss 0.11019648611545563 Validation loss 0.10561487078666687 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5440],\n",
      "        [0.5581]], device='mps:0')\n",
      "Iteration 22290 Training loss 0.10449797660112381 Validation loss 0.10563500225543976 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5807],\n",
      "        [0.4835]], device='mps:0')\n",
      "Iteration 22300 Training loss 0.10813046991825104 Validation loss 0.10565190017223358 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6729],\n",
      "        [0.6240]], device='mps:0')\n",
      "Iteration 22310 Training loss 0.11139453947544098 Validation loss 0.10567768663167953 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5336],\n",
      "        [0.4933]], device='mps:0')\n",
      "Iteration 22320 Training loss 0.1088833436369896 Validation loss 0.10563894361257553 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4357],\n",
      "        [0.6151]], device='mps:0')\n",
      "Iteration 22330 Training loss 0.09914261847734451 Validation loss 0.10561467707157135 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5293],\n",
      "        [0.5393]], device='mps:0')\n",
      "Iteration 22340 Training loss 0.10918295383453369 Validation loss 0.10559231042861938 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5593],\n",
      "        [0.5510]], device='mps:0')\n",
      "Iteration 22350 Training loss 0.10475555062294006 Validation loss 0.10558943450450897 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.4887],\n",
      "        [0.4629]], device='mps:0')\n",
      "Iteration 22360 Training loss 0.10424888134002686 Validation loss 0.10558043420314789 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.4820],\n",
      "        [0.5111]], device='mps:0')\n",
      "Iteration 22370 Training loss 0.11133645474910736 Validation loss 0.1055760383605957 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5205],\n",
      "        [0.5134]], device='mps:0')\n",
      "Iteration 22380 Training loss 0.10072886198759079 Validation loss 0.10556071251630783 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4932],\n",
      "        [0.4099]], device='mps:0')\n",
      "Iteration 22390 Training loss 0.10876435041427612 Validation loss 0.10556261986494064 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5470],\n",
      "        [0.3983]], device='mps:0')\n",
      "Iteration 22400 Training loss 0.10890262573957443 Validation loss 0.10556552559137344 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.4951],\n",
      "        [0.6789]], device='mps:0')\n",
      "Iteration 22410 Training loss 0.10390548408031464 Validation loss 0.10558534413576126 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5328],\n",
      "        [0.4657]], device='mps:0')\n",
      "Iteration 22420 Training loss 0.1199774369597435 Validation loss 0.10556109994649887 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.2804],\n",
      "        [0.6366]], device='mps:0')\n",
      "Iteration 22430 Training loss 0.10411239415407181 Validation loss 0.10559672862291336 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.1376],\n",
      "        [0.1732]], device='mps:0')\n",
      "Iteration 22440 Training loss 0.09261766076087952 Validation loss 0.10558498650789261 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6356],\n",
      "        [0.3068]], device='mps:0')\n",
      "Iteration 22450 Training loss 0.11268366873264313 Validation loss 0.10558042675256729 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6147],\n",
      "        [0.6072]], device='mps:0')\n",
      "Iteration 22460 Training loss 0.10304883867502213 Validation loss 0.1055966392159462 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4829],\n",
      "        [0.5788]], device='mps:0')\n",
      "Iteration 22470 Training loss 0.09923291951417923 Validation loss 0.10558772832155228 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4157],\n",
      "        [0.6023]], device='mps:0')\n",
      "Iteration 22480 Training loss 0.10608687996864319 Validation loss 0.10556726902723312 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5026],\n",
      "        [0.4556]], device='mps:0')\n",
      "Iteration 22490 Training loss 0.11053352057933807 Validation loss 0.10555611550807953 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.4603],\n",
      "        [0.6820]], device='mps:0')\n",
      "Iteration 22500 Training loss 0.11151359230279922 Validation loss 0.10553015768527985 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.3355],\n",
      "        [0.5965]], device='mps:0')\n",
      "Iteration 22510 Training loss 0.1173153817653656 Validation loss 0.10553973913192749 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5661],\n",
      "        [0.5141]], device='mps:0')\n",
      "Iteration 22520 Training loss 0.10347404330968857 Validation loss 0.10555118322372437 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.6696],\n",
      "        [0.5860]], device='mps:0')\n",
      "Iteration 22530 Training loss 0.10832074284553528 Validation loss 0.10556010901927948 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.2827],\n",
      "        [0.6135]], device='mps:0')\n",
      "Iteration 22540 Training loss 0.1039712056517601 Validation loss 0.10558374226093292 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5687],\n",
      "        [0.3412]], device='mps:0')\n",
      "Iteration 22550 Training loss 0.1038212776184082 Validation loss 0.10555383563041687 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4925],\n",
      "        [0.7028]], device='mps:0')\n",
      "Iteration 22560 Training loss 0.10812528431415558 Validation loss 0.10551159828901291 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.4791],\n",
      "        [0.3032]], device='mps:0')\n",
      "Iteration 22570 Training loss 0.11048582196235657 Validation loss 0.10552719980478287 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.2402],\n",
      "        [0.1204]], device='mps:0')\n",
      "Iteration 22580 Training loss 0.1044890359044075 Validation loss 0.1055084615945816 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.2368],\n",
      "        [0.5542]], device='mps:0')\n",
      "Iteration 22590 Training loss 0.09393955022096634 Validation loss 0.1055174320936203 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5791],\n",
      "        [0.3464]], device='mps:0')\n",
      "Iteration 22600 Training loss 0.10312856733798981 Validation loss 0.1055012121796608 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.7037],\n",
      "        [0.2320]], device='mps:0')\n",
      "Iteration 22610 Training loss 0.11091096699237823 Validation loss 0.10548798739910126 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.3664],\n",
      "        [0.1954]], device='mps:0')\n",
      "Iteration 22620 Training loss 0.10773800313472748 Validation loss 0.10548576712608337 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5565],\n",
      "        [0.4177]], device='mps:0')\n",
      "Iteration 22630 Training loss 0.10633190721273422 Validation loss 0.10548286139965057 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5840],\n",
      "        [0.6282]], device='mps:0')\n",
      "Iteration 22640 Training loss 0.10974616557359695 Validation loss 0.10548336803913116 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4306],\n",
      "        [0.3880]], device='mps:0')\n",
      "Iteration 22650 Training loss 0.10295146703720093 Validation loss 0.10548252612352371 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5153],\n",
      "        [0.2567]], device='mps:0')\n",
      "Iteration 22660 Training loss 0.11715781688690186 Validation loss 0.10548809915781021 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.2558],\n",
      "        [0.5158]], device='mps:0')\n",
      "Iteration 22670 Training loss 0.10254188627004623 Validation loss 0.10552146285772324 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4532],\n",
      "        [0.6463]], device='mps:0')\n",
      "Iteration 22680 Training loss 0.10684340447187424 Validation loss 0.10549027472734451 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.6203],\n",
      "        [0.5724]], device='mps:0')\n",
      "Iteration 22690 Training loss 0.09599687159061432 Validation loss 0.1055731549859047 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5515],\n",
      "        [0.6088]], device='mps:0')\n",
      "Iteration 22700 Training loss 0.10077426582574844 Validation loss 0.10555935651063919 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6002],\n",
      "        [0.4270]], device='mps:0')\n",
      "Iteration 22710 Training loss 0.09972736984491348 Validation loss 0.10554572194814682 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.3229],\n",
      "        [0.4904]], device='mps:0')\n",
      "Iteration 22720 Training loss 0.10379868000745773 Validation loss 0.10548131167888641 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.3711],\n",
      "        [0.3079]], device='mps:0')\n",
      "Iteration 22730 Training loss 0.10181016474962234 Validation loss 0.105523981153965 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.2053],\n",
      "        [0.5292]], device='mps:0')\n",
      "Iteration 22740 Training loss 0.10936359316110611 Validation loss 0.10552405565977097 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6843],\n",
      "        [0.6241]], device='mps:0')\n",
      "Iteration 22750 Training loss 0.10856809467077255 Validation loss 0.10552911460399628 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.2677],\n",
      "        [0.4390]], device='mps:0')\n",
      "Iteration 22760 Training loss 0.1076335608959198 Validation loss 0.1055232584476471 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5652],\n",
      "        [0.3528]], device='mps:0')\n",
      "Iteration 22770 Training loss 0.11132100969552994 Validation loss 0.10548337548971176 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.3602],\n",
      "        [0.7077]], device='mps:0')\n",
      "Iteration 22780 Training loss 0.10611020773649216 Validation loss 0.105465367436409 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5015],\n",
      "        [0.5279]], device='mps:0')\n",
      "Iteration 22790 Training loss 0.0971098467707634 Validation loss 0.10544215887784958 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4042],\n",
      "        [0.2623]], device='mps:0')\n",
      "Iteration 22800 Training loss 0.11672273278236389 Validation loss 0.10544465482234955 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.3898],\n",
      "        [0.4497]], device='mps:0')\n",
      "Iteration 22810 Training loss 0.11138732731342316 Validation loss 0.10543470829725266 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5485],\n",
      "        [0.4635]], device='mps:0')\n",
      "Iteration 22820 Training loss 0.10232507437467575 Validation loss 0.10546442121267319 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.6693],\n",
      "        [0.5817]], device='mps:0')\n",
      "Iteration 22830 Training loss 0.10911361128091812 Validation loss 0.10543665289878845 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5397],\n",
      "        [0.5131]], device='mps:0')\n",
      "Iteration 22840 Training loss 0.0975080281496048 Validation loss 0.10546033084392548 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.3270],\n",
      "        [0.5488]], device='mps:0')\n",
      "Iteration 22850 Training loss 0.10324110090732574 Validation loss 0.10553698986768723 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5815],\n",
      "        [0.5188]], device='mps:0')\n",
      "Iteration 22860 Training loss 0.11007115989923477 Validation loss 0.10548023879528046 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6164],\n",
      "        [0.3115]], device='mps:0')\n",
      "Iteration 22870 Training loss 0.10371709614992142 Validation loss 0.10551716387271881 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.3449],\n",
      "        [0.5693]], device='mps:0')\n",
      "Iteration 22880 Training loss 0.11559035629034042 Validation loss 0.10546056181192398 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6413],\n",
      "        [0.2623]], device='mps:0')\n",
      "Iteration 22890 Training loss 0.10789108276367188 Validation loss 0.10542622208595276 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5966],\n",
      "        [0.6600]], device='mps:0')\n",
      "Iteration 22900 Training loss 0.1171131357550621 Validation loss 0.10540907084941864 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4023],\n",
      "        [0.6547]], device='mps:0')\n",
      "Iteration 22910 Training loss 0.10636227577924728 Validation loss 0.10543675720691681 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.2851],\n",
      "        [0.5159]], device='mps:0')\n",
      "Iteration 22920 Training loss 0.10635527968406677 Validation loss 0.1054164469242096 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.2483],\n",
      "        [0.4720]], device='mps:0')\n",
      "Iteration 22930 Training loss 0.09890422970056534 Validation loss 0.10543796420097351 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.4633],\n",
      "        [0.5499]], device='mps:0')\n",
      "Iteration 22940 Training loss 0.10752908140420914 Validation loss 0.1054142490029335 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.5239],\n",
      "        [0.4741]], device='mps:0')\n",
      "Iteration 22950 Training loss 0.1136874333024025 Validation loss 0.10539102554321289 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5523],\n",
      "        [0.6426]], device='mps:0')\n",
      "Iteration 22960 Training loss 0.11364567279815674 Validation loss 0.10540804266929626 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.6870],\n",
      "        [0.4361]], device='mps:0')\n",
      "Iteration 22970 Training loss 0.11002305895090103 Validation loss 0.10539569705724716 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5107],\n",
      "        [0.1873]], device='mps:0')\n",
      "Iteration 22980 Training loss 0.09328527003526688 Validation loss 0.10538019984960556 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.3011],\n",
      "        [0.5696]], device='mps:0')\n",
      "Iteration 22990 Training loss 0.11428667604923248 Validation loss 0.10538385063409805 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.6243],\n",
      "        [0.4795]], device='mps:0')\n",
      "Iteration 23000 Training loss 0.10788042098283768 Validation loss 0.1053776741027832 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6306],\n",
      "        [0.2343]], device='mps:0')\n",
      "Iteration 23010 Training loss 0.117367684841156 Validation loss 0.10537872463464737 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5287],\n",
      "        [0.5176]], device='mps:0')\n",
      "Iteration 23020 Training loss 0.10694083571434021 Validation loss 0.1053774282336235 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5196],\n",
      "        [0.1100]], device='mps:0')\n",
      "Iteration 23030 Training loss 0.11013791710138321 Validation loss 0.10537136346101761 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.4782],\n",
      "        [0.5882]], device='mps:0')\n",
      "Iteration 23040 Training loss 0.11229600757360458 Validation loss 0.10537336021661758 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.6743],\n",
      "        [0.3710]], device='mps:0')\n",
      "Iteration 23050 Training loss 0.1083998754620552 Validation loss 0.10536591708660126 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.4577],\n",
      "        [0.5237]], device='mps:0')\n",
      "Iteration 23060 Training loss 0.10834721475839615 Validation loss 0.10536766052246094 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.6546],\n",
      "        [0.6205]], device='mps:0')\n",
      "Iteration 23070 Training loss 0.11626525968313217 Validation loss 0.10536607354879379 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.3009],\n",
      "        [0.4565]], device='mps:0')\n",
      "Iteration 23080 Training loss 0.09964963793754578 Validation loss 0.10536324232816696 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5256],\n",
      "        [0.3763]], device='mps:0')\n",
      "Iteration 23090 Training loss 0.100954569876194 Validation loss 0.1053503081202507 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5304],\n",
      "        [0.4200]], device='mps:0')\n",
      "Iteration 23100 Training loss 0.10631129890680313 Validation loss 0.10535003244876862 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.4777],\n",
      "        [0.1541]], device='mps:0')\n",
      "Iteration 23110 Training loss 0.09851585328578949 Validation loss 0.10534821450710297 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5919],\n",
      "        [0.4508]], device='mps:0')\n",
      "Iteration 23120 Training loss 0.10066093504428864 Validation loss 0.1053570955991745 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.4402],\n",
      "        [0.6519]], device='mps:0')\n",
      "Iteration 23130 Training loss 0.10276798158884048 Validation loss 0.10535658895969391 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5721],\n",
      "        [0.4579]], device='mps:0')\n",
      "Iteration 23140 Training loss 0.10637269914150238 Validation loss 0.10536253452301025 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.3877],\n",
      "        [0.5652]], device='mps:0')\n",
      "Iteration 23150 Training loss 0.11143746227025986 Validation loss 0.10536076128482819 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5983],\n",
      "        [0.5730]], device='mps:0')\n",
      "Iteration 23160 Training loss 0.09838277846574783 Validation loss 0.10534526407718658 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6407],\n",
      "        [0.4667]], device='mps:0')\n",
      "Iteration 23170 Training loss 0.11671725660562515 Validation loss 0.10533756762742996 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.3713],\n",
      "        [0.6392]], device='mps:0')\n",
      "Iteration 23180 Training loss 0.10605236142873764 Validation loss 0.10534199327230453 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6524],\n",
      "        [0.6163]], device='mps:0')\n",
      "Iteration 23190 Training loss 0.1108652725815773 Validation loss 0.10533615946769714 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.3166],\n",
      "        [0.2461]], device='mps:0')\n",
      "Iteration 23200 Training loss 0.10962177813053131 Validation loss 0.1053275391459465 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.6193],\n",
      "        [0.3614]], device='mps:0')\n",
      "Iteration 23210 Training loss 0.12446935474872589 Validation loss 0.10532311350107193 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.1631],\n",
      "        [0.5110]], device='mps:0')\n",
      "Iteration 23220 Training loss 0.11547257006168365 Validation loss 0.10532129555940628 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.5549],\n",
      "        [0.6888]], device='mps:0')\n",
      "Iteration 23230 Training loss 0.11474021524190903 Validation loss 0.10532112419605255 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5581],\n",
      "        [0.4980]], device='mps:0')\n",
      "Iteration 23240 Training loss 0.10844295471906662 Validation loss 0.10531719774007797 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.5201],\n",
      "        [0.6710]], device='mps:0')\n",
      "Iteration 23250 Training loss 0.09724168479442596 Validation loss 0.10531832277774811 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.4403],\n",
      "        [0.6476]], device='mps:0')\n",
      "Iteration 23260 Training loss 0.10454951226711273 Validation loss 0.105314701795578 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.6875],\n",
      "        [0.2485]], device='mps:0')\n",
      "Iteration 23270 Training loss 0.10611940175294876 Validation loss 0.10531336814165115 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.3774],\n",
      "        [0.6669]], device='mps:0')\n",
      "Iteration 23280 Training loss 0.1083604022860527 Validation loss 0.10530810803174973 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.4826],\n",
      "        [0.5063]], device='mps:0')\n",
      "Iteration 23290 Training loss 0.11382441967725754 Validation loss 0.10531682521104813 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.4821],\n",
      "        [0.6387]], device='mps:0')\n",
      "Iteration 23300 Training loss 0.09954873472452164 Validation loss 0.10530783981084824 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.6015],\n",
      "        [0.4524]], device='mps:0')\n",
      "Iteration 23310 Training loss 0.10509582608938217 Validation loss 0.10531255602836609 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.6371],\n",
      "        [0.4179]], device='mps:0')\n",
      "Iteration 23320 Training loss 0.11236991733312607 Validation loss 0.10530482977628708 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5204],\n",
      "        [0.5674]], device='mps:0')\n",
      "Iteration 23330 Training loss 0.11075540632009506 Validation loss 0.1053021103143692 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5856],\n",
      "        [0.6745]], device='mps:0')\n",
      "Iteration 23340 Training loss 0.10161809623241425 Validation loss 0.10530299693346024 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.3207],\n",
      "        [0.3321]], device='mps:0')\n",
      "Iteration 23350 Training loss 0.11288782209157944 Validation loss 0.10532167553901672 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5239],\n",
      "        [0.3819]], device='mps:0')\n",
      "Iteration 23360 Training loss 0.09985040128231049 Validation loss 0.10530544072389603 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5796],\n",
      "        [0.6232]], device='mps:0')\n",
      "Iteration 23370 Training loss 0.09950067102909088 Validation loss 0.10528868436813354 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5997],\n",
      "        [0.4113]], device='mps:0')\n",
      "Iteration 23380 Training loss 0.11164987087249756 Validation loss 0.1053035780787468 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5026],\n",
      "        [0.3698]], device='mps:0')\n",
      "Iteration 23390 Training loss 0.10600258409976959 Validation loss 0.10535463690757751 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.6582],\n",
      "        [0.5903]], device='mps:0')\n",
      "Iteration 23400 Training loss 0.10887425392866135 Validation loss 0.10534577816724777 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6215],\n",
      "        [0.3069]], device='mps:0')\n",
      "Iteration 23410 Training loss 0.10384826362133026 Validation loss 0.10533016175031662 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.3411],\n",
      "        [0.5369]], device='mps:0')\n",
      "Iteration 23420 Training loss 0.1108013242483139 Validation loss 0.10540182143449783 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.2094],\n",
      "        [0.6291]], device='mps:0')\n",
      "Iteration 23430 Training loss 0.10544608533382416 Validation loss 0.10534629970788956 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.2288],\n",
      "        [0.3207]], device='mps:0')\n",
      "Iteration 23440 Training loss 0.10764326900243759 Validation loss 0.10534505546092987 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.6499],\n",
      "        [0.3065]], device='mps:0')\n",
      "Iteration 23450 Training loss 0.11742481589317322 Validation loss 0.10529337078332901 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5697],\n",
      "        [0.5227]], device='mps:0')\n",
      "Iteration 23460 Training loss 0.10692781209945679 Validation loss 0.10529929399490356 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.6632],\n",
      "        [0.4845]], device='mps:0')\n",
      "Iteration 23470 Training loss 0.1040557250380516 Validation loss 0.10531899333000183 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4740],\n",
      "        [0.4510]], device='mps:0')\n",
      "Iteration 23480 Training loss 0.10838678479194641 Validation loss 0.10531456768512726 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.3572],\n",
      "        [0.6185]], device='mps:0')\n",
      "Iteration 23490 Training loss 0.10771901160478592 Validation loss 0.10529978573322296 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5213],\n",
      "        [0.5059]], device='mps:0')\n",
      "Iteration 23500 Training loss 0.11070895940065384 Validation loss 0.1052732765674591 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5539],\n",
      "        [0.5731]], device='mps:0')\n",
      "Iteration 23510 Training loss 0.10670098662376404 Validation loss 0.10530225932598114 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.5049],\n",
      "        [0.5080]], device='mps:0')\n",
      "Iteration 23520 Training loss 0.10298935323953629 Validation loss 0.10529463738203049 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.6341],\n",
      "        [0.3063]], device='mps:0')\n",
      "Iteration 23530 Training loss 0.10385959595441818 Validation loss 0.10526532679796219 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4353],\n",
      "        [0.4912]], device='mps:0')\n",
      "Iteration 23540 Training loss 0.1171812191605568 Validation loss 0.10531679540872574 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.2127],\n",
      "        [0.5205]], device='mps:0')\n",
      "Iteration 23550 Training loss 0.09864116460084915 Validation loss 0.1053837239742279 Accuracy 0.6930000185966492\n",
      "Output tensor([[0.6814],\n",
      "        [0.5904]], device='mps:0')\n",
      "Iteration 23560 Training loss 0.10936849564313889 Validation loss 0.10532786697149277 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6722],\n",
      "        [0.7299]], device='mps:0')\n",
      "Iteration 23570 Training loss 0.10430208593606949 Validation loss 0.10538959503173828 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.3523],\n",
      "        [0.6453]], device='mps:0')\n",
      "Iteration 23580 Training loss 0.09383893758058548 Validation loss 0.10531646013259888 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.6510],\n",
      "        [0.3674]], device='mps:0')\n",
      "Iteration 23590 Training loss 0.11908882111310959 Validation loss 0.10525541007518768 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.3136],\n",
      "        [0.4973]], device='mps:0')\n",
      "Iteration 23600 Training loss 0.09904619306325912 Validation loss 0.10523686558008194 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.6198],\n",
      "        [0.2691]], device='mps:0')\n",
      "Iteration 23610 Training loss 0.10880794376134872 Validation loss 0.1052519753575325 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5746],\n",
      "        [0.5114]], device='mps:0')\n",
      "Iteration 23620 Training loss 0.11228862404823303 Validation loss 0.10525467246770859 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4007],\n",
      "        [0.1978]], device='mps:0')\n",
      "Iteration 23630 Training loss 0.10312427580356598 Validation loss 0.10528583824634552 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6703],\n",
      "        [0.2270]], device='mps:0')\n",
      "Iteration 23640 Training loss 0.10651735961437225 Validation loss 0.10525906831026077 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.7160],\n",
      "        [0.5700]], device='mps:0')\n",
      "Iteration 23650 Training loss 0.10321394354104996 Validation loss 0.10523603111505508 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6021],\n",
      "        [0.2365]], device='mps:0')\n",
      "Iteration 23660 Training loss 0.10245443135499954 Validation loss 0.10527854412794113 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5679],\n",
      "        [0.5107]], device='mps:0')\n",
      "Iteration 23670 Training loss 0.10502542555332184 Validation loss 0.10526109486818314 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4230],\n",
      "        [0.4999]], device='mps:0')\n",
      "Iteration 23680 Training loss 0.1158219575881958 Validation loss 0.10523421317338943 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4905],\n",
      "        [0.3574]], device='mps:0')\n",
      "Iteration 23690 Training loss 0.09974401444196701 Validation loss 0.10522033274173737 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.3854],\n",
      "        [0.4551]], device='mps:0')\n",
      "Iteration 23700 Training loss 0.10487144440412521 Validation loss 0.1052100732922554 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.3486],\n",
      "        [0.3925]], device='mps:0')\n",
      "Iteration 23710 Training loss 0.10292377322912216 Validation loss 0.10519876331090927 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.5287],\n",
      "        [0.6587]], device='mps:0')\n",
      "Iteration 23720 Training loss 0.1000758707523346 Validation loss 0.10519815236330032 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.6829],\n",
      "        [0.6349]], device='mps:0')\n",
      "Iteration 23730 Training loss 0.10605060309171677 Validation loss 0.10519204288721085 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.3691],\n",
      "        [0.6406]], device='mps:0')\n",
      "Iteration 23740 Training loss 0.10560336709022522 Validation loss 0.10519526153802872 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.4213],\n",
      "        [0.5863]], device='mps:0')\n",
      "Iteration 23750 Training loss 0.10889493674039841 Validation loss 0.10519295185804367 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5042],\n",
      "        [0.5226]], device='mps:0')\n",
      "Iteration 23760 Training loss 0.10614792257547379 Validation loss 0.10520639270544052 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4051],\n",
      "        [0.4387]], device='mps:0')\n",
      "Iteration 23770 Training loss 0.09566253423690796 Validation loss 0.10523539036512375 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.3836],\n",
      "        [0.4187]], device='mps:0')\n",
      "Iteration 23780 Training loss 0.10303463786840439 Validation loss 0.10524118691682816 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4676],\n",
      "        [0.6293]], device='mps:0')\n",
      "Iteration 23790 Training loss 0.1079806387424469 Validation loss 0.1052609458565712 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.3270],\n",
      "        [0.6894]], device='mps:0')\n",
      "Iteration 23800 Training loss 0.11027564108371735 Validation loss 0.1052030473947525 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4438],\n",
      "        [0.3724]], device='mps:0')\n",
      "Iteration 23810 Training loss 0.11409784108400345 Validation loss 0.10519374907016754 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5869],\n",
      "        [0.3491]], device='mps:0')\n",
      "Iteration 23820 Training loss 0.10675748437643051 Validation loss 0.10518035292625427 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.6235],\n",
      "        [0.7296]], device='mps:0')\n",
      "Iteration 23830 Training loss 0.10037282854318619 Validation loss 0.10517477244138718 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.5333],\n",
      "        [0.2584]], device='mps:0')\n",
      "Iteration 23840 Training loss 0.10519769787788391 Validation loss 0.10517943650484085 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5932],\n",
      "        [0.4613]], device='mps:0')\n",
      "Iteration 23850 Training loss 0.0986240953207016 Validation loss 0.10520503669977188 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4113],\n",
      "        [0.4780]], device='mps:0')\n",
      "Iteration 23860 Training loss 0.10683530569076538 Validation loss 0.10522092133760452 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4797],\n",
      "        [0.6569]], device='mps:0')\n",
      "Iteration 23870 Training loss 0.10509052127599716 Validation loss 0.10520973056554794 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6255],\n",
      "        [0.3643]], device='mps:0')\n",
      "Iteration 23880 Training loss 0.10113237053155899 Validation loss 0.10525380074977875 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.6717],\n",
      "        [0.4758]], device='mps:0')\n",
      "Iteration 23890 Training loss 0.1110200509428978 Validation loss 0.10526163130998611 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4703],\n",
      "        [0.4670]], device='mps:0')\n",
      "Iteration 23900 Training loss 0.09840989857912064 Validation loss 0.10522325336933136 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5368],\n",
      "        [0.7577]], device='mps:0')\n",
      "Iteration 23910 Training loss 0.10088014602661133 Validation loss 0.10523578524589539 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.4046],\n",
      "        [0.4346]], device='mps:0')\n",
      "Iteration 23920 Training loss 0.09977302700281143 Validation loss 0.10519888252019882 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5746],\n",
      "        [0.6225]], device='mps:0')\n",
      "Iteration 23930 Training loss 0.11286460608243942 Validation loss 0.10521671175956726 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.4185],\n",
      "        [0.3460]], device='mps:0')\n",
      "Iteration 23940 Training loss 0.10314477980136871 Validation loss 0.10519582033157349 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6570],\n",
      "        [0.4231]], device='mps:0')\n",
      "Iteration 23950 Training loss 0.09977603703737259 Validation loss 0.1052643209695816 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.3440],\n",
      "        [0.4593]], device='mps:0')\n",
      "Iteration 23960 Training loss 0.10882557928562164 Validation loss 0.10528041422367096 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.1136],\n",
      "        [0.4443]], device='mps:0')\n",
      "Iteration 23970 Training loss 0.11015323549509048 Validation loss 0.10534615069627762 Accuracy 0.6925000548362732\n",
      "Output tensor([[0.5503],\n",
      "        [0.2153]], device='mps:0')\n",
      "Iteration 23980 Training loss 0.1093987375497818 Validation loss 0.10533969849348068 Accuracy 0.6920000314712524\n",
      "Output tensor([[0.4673],\n",
      "        [0.6264]], device='mps:0')\n",
      "Iteration 23990 Training loss 0.11661676317453384 Validation loss 0.10525388270616531 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5929],\n",
      "        [0.4470]], device='mps:0')\n",
      "Iteration 24000 Training loss 0.10894487053155899 Validation loss 0.10524959862232208 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.6181],\n",
      "        [0.6188]], device='mps:0')\n",
      "Iteration 24010 Training loss 0.09312375634908676 Validation loss 0.10518010705709457 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6258],\n",
      "        [0.4968]], device='mps:0')\n",
      "Iteration 24020 Training loss 0.11762545257806778 Validation loss 0.10518117994070053 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.6066],\n",
      "        [0.5840]], device='mps:0')\n",
      "Iteration 24030 Training loss 0.1124146357178688 Validation loss 0.10513661801815033 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.3508],\n",
      "        [0.4085]], device='mps:0')\n",
      "Iteration 24040 Training loss 0.11014173179864883 Validation loss 0.10515496134757996 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4230],\n",
      "        [0.3292]], device='mps:0')\n",
      "Iteration 24050 Training loss 0.10590503364801407 Validation loss 0.10514082759618759 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4261],\n",
      "        [0.6812]], device='mps:0')\n",
      "Iteration 24060 Training loss 0.11097053438425064 Validation loss 0.10513587296009064 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.6568],\n",
      "        [0.4560]], device='mps:0')\n",
      "Iteration 24070 Training loss 0.11624377965927124 Validation loss 0.10516365617513657 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6901],\n",
      "        [0.7110]], device='mps:0')\n",
      "Iteration 24080 Training loss 0.09596049785614014 Validation loss 0.10519545525312424 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.2424],\n",
      "        [0.1835]], device='mps:0')\n",
      "Iteration 24090 Training loss 0.10492025315761566 Validation loss 0.10518096387386322 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5345],\n",
      "        [0.4704]], device='mps:0')\n",
      "Iteration 24100 Training loss 0.11410869657993317 Validation loss 0.1051391065120697 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.6255],\n",
      "        [0.5169]], device='mps:0')\n",
      "Iteration 24110 Training loss 0.10682586580514908 Validation loss 0.10512891411781311 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.3424],\n",
      "        [0.4841]], device='mps:0')\n",
      "Iteration 24120 Training loss 0.10785778611898422 Validation loss 0.10513251274824142 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4293],\n",
      "        [0.3158]], device='mps:0')\n",
      "Iteration 24130 Training loss 0.10846957564353943 Validation loss 0.10513466596603394 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5797],\n",
      "        [0.4492]], device='mps:0')\n",
      "Iteration 24140 Training loss 0.10636831074953079 Validation loss 0.10513445734977722 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.2333],\n",
      "        [0.4894]], device='mps:0')\n",
      "Iteration 24150 Training loss 0.11130759865045547 Validation loss 0.10511511564254761 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.2911],\n",
      "        [0.5329]], device='mps:0')\n",
      "Iteration 24160 Training loss 0.11111076176166534 Validation loss 0.10509506613016129 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.4894],\n",
      "        [0.2993]], device='mps:0')\n",
      "Iteration 24170 Training loss 0.10858987271785736 Validation loss 0.10509232431650162 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.4832],\n",
      "        [0.2682]], device='mps:0')\n",
      "Iteration 24180 Training loss 0.10562009364366531 Validation loss 0.10509828478097916 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.3959],\n",
      "        [0.6265]], device='mps:0')\n",
      "Iteration 24190 Training loss 0.10946876555681229 Validation loss 0.10512000322341919 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.2241],\n",
      "        [0.5143]], device='mps:0')\n",
      "Iteration 24200 Training loss 0.10722202062606812 Validation loss 0.1051403284072876 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.4317],\n",
      "        [0.4205]], device='mps:0')\n",
      "Iteration 24210 Training loss 0.102572001516819 Validation loss 0.10510648787021637 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.1847],\n",
      "        [0.1530]], device='mps:0')\n",
      "Iteration 24220 Training loss 0.10392632335424423 Validation loss 0.10510072857141495 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4076],\n",
      "        [0.2980]], device='mps:0')\n",
      "Iteration 24230 Training loss 0.10190834850072861 Validation loss 0.10511884093284607 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4981],\n",
      "        [0.5970]], device='mps:0')\n",
      "Iteration 24240 Training loss 0.10670891404151917 Validation loss 0.10509749501943588 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5355],\n",
      "        [0.5818]], device='mps:0')\n",
      "Iteration 24250 Training loss 0.1066233441233635 Validation loss 0.1051177904009819 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.3762],\n",
      "        [0.4372]], device='mps:0')\n",
      "Iteration 24260 Training loss 0.10642591118812561 Validation loss 0.1051187664270401 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6700],\n",
      "        [0.5527]], device='mps:0')\n",
      "Iteration 24270 Training loss 0.10601551830768585 Validation loss 0.10511220246553421 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5589],\n",
      "        [0.4130]], device='mps:0')\n",
      "Iteration 24280 Training loss 0.10819917172193527 Validation loss 0.10507162660360336 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.4391],\n",
      "        [0.5781]], device='mps:0')\n",
      "Iteration 24290 Training loss 0.10885042697191238 Validation loss 0.10512379556894302 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.5812],\n",
      "        [0.3913]], device='mps:0')\n",
      "Iteration 24300 Training loss 0.11940841376781464 Validation loss 0.10510121285915375 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.3452],\n",
      "        [0.6334]], device='mps:0')\n",
      "Iteration 24310 Training loss 0.09998422116041183 Validation loss 0.10507306456565857 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.2968],\n",
      "        [0.4459]], device='mps:0')\n",
      "Iteration 24320 Training loss 0.10342104732990265 Validation loss 0.10506289452314377 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.3951],\n",
      "        [0.3257]], device='mps:0')\n",
      "Iteration 24330 Training loss 0.10390438139438629 Validation loss 0.10506638139486313 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4571],\n",
      "        [0.4991]], device='mps:0')\n",
      "Iteration 24340 Training loss 0.11955561488866806 Validation loss 0.10505537688732147 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.6078],\n",
      "        [0.4592]], device='mps:0')\n",
      "Iteration 24350 Training loss 0.09455092996358871 Validation loss 0.10507890582084656 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.5230],\n",
      "        [0.5808]], device='mps:0')\n",
      "Iteration 24360 Training loss 0.09612567722797394 Validation loss 0.1050572544336319 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5454],\n",
      "        [0.2372]], device='mps:0')\n",
      "Iteration 24370 Training loss 0.11534249782562256 Validation loss 0.10507616400718689 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.4547],\n",
      "        [0.5486]], device='mps:0')\n",
      "Iteration 24380 Training loss 0.1023467406630516 Validation loss 0.1051073670387268 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.5588],\n",
      "        [0.5186]], device='mps:0')\n",
      "Iteration 24390 Training loss 0.10502929240465164 Validation loss 0.10511384904384613 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4804],\n",
      "        [0.5821]], device='mps:0')\n",
      "Iteration 24400 Training loss 0.11324232071638107 Validation loss 0.10506565123796463 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.6459],\n",
      "        [0.3847]], device='mps:0')\n",
      "Iteration 24410 Training loss 0.10168775171041489 Validation loss 0.10503759980201721 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.4857],\n",
      "        [0.6083]], device='mps:0')\n",
      "Iteration 24420 Training loss 0.10728379338979721 Validation loss 0.1050388514995575 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.7613],\n",
      "        [0.3857]], device='mps:0')\n",
      "Iteration 24430 Training loss 0.10744830965995789 Validation loss 0.10503219813108444 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.3416],\n",
      "        [0.3660]], device='mps:0')\n",
      "Iteration 24440 Training loss 0.10319651663303375 Validation loss 0.1050267219543457 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5782],\n",
      "        [0.4492]], device='mps:0')\n",
      "Iteration 24450 Training loss 0.11826472729444504 Validation loss 0.1050226166844368 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.4577],\n",
      "        [0.4672]], device='mps:0')\n",
      "Iteration 24460 Training loss 0.11699451506137848 Validation loss 0.10502173751592636 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.3728],\n",
      "        [0.4968]], device='mps:0')\n",
      "Iteration 24470 Training loss 0.09637203812599182 Validation loss 0.10502444952726364 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5599],\n",
      "        [0.6205]], device='mps:0')\n",
      "Iteration 24480 Training loss 0.09297408163547516 Validation loss 0.10502412915229797 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6658],\n",
      "        [0.3041]], device='mps:0')\n",
      "Iteration 24490 Training loss 0.10124996304512024 Validation loss 0.1050105094909668 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.6463],\n",
      "        [0.3161]], device='mps:0')\n",
      "Iteration 24500 Training loss 0.1122589111328125 Validation loss 0.10501769185066223 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4487],\n",
      "        [0.3220]], device='mps:0')\n",
      "Iteration 24510 Training loss 0.0979326069355011 Validation loss 0.10503853112459183 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5786],\n",
      "        [0.2720]], device='mps:0')\n",
      "Iteration 24520 Training loss 0.10972776263952255 Validation loss 0.10504438728094101 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4827],\n",
      "        [0.5248]], device='mps:0')\n",
      "Iteration 24530 Training loss 0.11035699397325516 Validation loss 0.10506618767976761 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.1340],\n",
      "        [0.6984]], device='mps:0')\n",
      "Iteration 24540 Training loss 0.09775573760271072 Validation loss 0.10500657558441162 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6376],\n",
      "        [0.6845]], device='mps:0')\n",
      "Iteration 24550 Training loss 0.11515864729881287 Validation loss 0.10503640025854111 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.3863],\n",
      "        [0.5378]], device='mps:0')\n",
      "Iteration 24560 Training loss 0.10865230113267899 Validation loss 0.10501917451620102 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.6977],\n",
      "        [0.5906]], device='mps:0')\n",
      "Iteration 24570 Training loss 0.111116923391819 Validation loss 0.10506641119718552 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.6952],\n",
      "        [0.4835]], device='mps:0')\n",
      "Iteration 24580 Training loss 0.10367322713136673 Validation loss 0.10504323989152908 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.4424],\n",
      "        [0.6951]], device='mps:0')\n",
      "Iteration 24590 Training loss 0.09669721126556396 Validation loss 0.10502608120441437 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5862],\n",
      "        [0.5870]], device='mps:0')\n",
      "Iteration 24600 Training loss 0.10755031555891037 Validation loss 0.10505843907594681 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6257],\n",
      "        [0.3075]], device='mps:0')\n",
      "Iteration 24610 Training loss 0.1120971068739891 Validation loss 0.10504688322544098 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.1329],\n",
      "        [0.5387]], device='mps:0')\n",
      "Iteration 24620 Training loss 0.10203536599874496 Validation loss 0.10502153635025024 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.6171],\n",
      "        [0.6501]], device='mps:0')\n",
      "Iteration 24630 Training loss 0.1017957255244255 Validation loss 0.10498513281345367 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.4097],\n",
      "        [0.5010]], device='mps:0')\n",
      "Iteration 24640 Training loss 0.10941120237112045 Validation loss 0.10501034557819366 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4066],\n",
      "        [0.5417]], device='mps:0')\n",
      "Iteration 24650 Training loss 0.10905694961547852 Validation loss 0.10503135621547699 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6390],\n",
      "        [0.5552]], device='mps:0')\n",
      "Iteration 24660 Training loss 0.10254711657762527 Validation loss 0.10499382764101028 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6345],\n",
      "        [0.2914]], device='mps:0')\n",
      "Iteration 24670 Training loss 0.103614941239357 Validation loss 0.10498451441526413 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.7654],\n",
      "        [0.5135]], device='mps:0')\n",
      "Iteration 24680 Training loss 0.10588065534830093 Validation loss 0.10500519722700119 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.5621],\n",
      "        [0.5078]], device='mps:0')\n",
      "Iteration 24690 Training loss 0.11470771580934525 Validation loss 0.10498731583356857 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.1393],\n",
      "        [0.6257]], device='mps:0')\n",
      "Iteration 24700 Training loss 0.10301824659109116 Validation loss 0.10502266138792038 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.3427],\n",
      "        [0.4692]], device='mps:0')\n",
      "Iteration 24710 Training loss 0.10261271893978119 Validation loss 0.10506267845630646 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5742],\n",
      "        [0.2267]], device='mps:0')\n",
      "Iteration 24720 Training loss 0.10887838900089264 Validation loss 0.10500962287187576 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.4908],\n",
      "        [0.7883]], device='mps:0')\n",
      "Iteration 24730 Training loss 0.09484738856554031 Validation loss 0.10503315925598145 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.6747],\n",
      "        [0.3466]], device='mps:0')\n",
      "Iteration 24740 Training loss 0.10430578142404556 Validation loss 0.10505162179470062 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.4558],\n",
      "        [0.3982]], device='mps:0')\n",
      "Iteration 24750 Training loss 0.10551145672798157 Validation loss 0.10498309880495071 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4722],\n",
      "        [0.4756]], device='mps:0')\n",
      "Iteration 24760 Training loss 0.10851503163576126 Validation loss 0.10496750473976135 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6199],\n",
      "        [0.5498]], device='mps:0')\n",
      "Iteration 24770 Training loss 0.10769161581993103 Validation loss 0.10498727113008499 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.3377],\n",
      "        [0.5060]], device='mps:0')\n",
      "Iteration 24780 Training loss 0.10075082629919052 Validation loss 0.10498877614736557 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6412],\n",
      "        [0.6333]], device='mps:0')\n",
      "Iteration 24790 Training loss 0.10638885945081711 Validation loss 0.10499098896980286 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.6113],\n",
      "        [0.5220]], device='mps:0')\n",
      "Iteration 24800 Training loss 0.10999757796525955 Validation loss 0.10500485450029373 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5705],\n",
      "        [0.5669]], device='mps:0')\n",
      "Iteration 24810 Training loss 0.1013491153717041 Validation loss 0.10496076196432114 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.2128],\n",
      "        [0.6130]], device='mps:0')\n",
      "Iteration 24820 Training loss 0.1036766767501831 Validation loss 0.10495208948850632 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5655],\n",
      "        [0.6232]], device='mps:0')\n",
      "Iteration 24830 Training loss 0.1126796305179596 Validation loss 0.10497276484966278 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6530],\n",
      "        [0.5814]], device='mps:0')\n",
      "Iteration 24840 Training loss 0.10224051028490067 Validation loss 0.10491596162319183 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.5288],\n",
      "        [0.4784]], device='mps:0')\n",
      "Iteration 24850 Training loss 0.1144212856888771 Validation loss 0.10491426289081573 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.4810],\n",
      "        [0.5922]], device='mps:0')\n",
      "Iteration 24860 Training loss 0.10917186737060547 Validation loss 0.10492148995399475 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4102],\n",
      "        [0.4558]], device='mps:0')\n",
      "Iteration 24870 Training loss 0.09920249134302139 Validation loss 0.10493247210979462 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.2100],\n",
      "        [0.6648]], device='mps:0')\n",
      "Iteration 24880 Training loss 0.10788284987211227 Validation loss 0.10492449253797531 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5765],\n",
      "        [0.6198]], device='mps:0')\n",
      "Iteration 24890 Training loss 0.11425101011991501 Validation loss 0.10492907464504242 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.3680],\n",
      "        [0.4716]], device='mps:0')\n",
      "Iteration 24900 Training loss 0.09911142289638519 Validation loss 0.1049451157450676 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.4289],\n",
      "        [0.3546]], device='mps:0')\n",
      "Iteration 24910 Training loss 0.1072506308555603 Validation loss 0.10494495928287506 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5998],\n",
      "        [0.4230]], device='mps:0')\n",
      "Iteration 24920 Training loss 0.1066371500492096 Validation loss 0.10494823008775711 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4389],\n",
      "        [0.5582]], device='mps:0')\n",
      "Iteration 24930 Training loss 0.11107772588729858 Validation loss 0.10491249710321426 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5114],\n",
      "        [0.5049]], device='mps:0')\n",
      "Iteration 24940 Training loss 0.10467061400413513 Validation loss 0.10490808635950089 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.4379],\n",
      "        [0.3850]], device='mps:0')\n",
      "Iteration 24950 Training loss 0.10632532089948654 Validation loss 0.1049005463719368 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6486],\n",
      "        [0.3216]], device='mps:0')\n",
      "Iteration 24960 Training loss 0.11117655783891678 Validation loss 0.10488596558570862 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.7806],\n",
      "        [0.4735]], device='mps:0')\n",
      "Iteration 24970 Training loss 0.10440145432949066 Validation loss 0.10488445311784744 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.6594],\n",
      "        [0.5833]], device='mps:0')\n",
      "Iteration 24980 Training loss 0.10619959235191345 Validation loss 0.10488466173410416 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5645],\n",
      "        [0.3864]], device='mps:0')\n",
      "Iteration 24990 Training loss 0.10391011834144592 Validation loss 0.10489008575677872 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.5323],\n",
      "        [0.4875]], device='mps:0')\n",
      "Iteration 25000 Training loss 0.10930822789669037 Validation loss 0.10487376898527145 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.6346],\n",
      "        [0.4074]], device='mps:0')\n",
      "Iteration 25010 Training loss 0.10834409296512604 Validation loss 0.10490106791257858 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.2407],\n",
      "        [0.3095]], device='mps:0')\n",
      "Iteration 25020 Training loss 0.09487615525722504 Validation loss 0.10492482036352158 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.6792],\n",
      "        [0.5853]], device='mps:0')\n",
      "Iteration 25030 Training loss 0.10357122123241425 Validation loss 0.10491092503070831 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.4534],\n",
      "        [0.3172]], device='mps:0')\n",
      "Iteration 25040 Training loss 0.11114948242902756 Validation loss 0.10494028776884079 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6293],\n",
      "        [0.7306]], device='mps:0')\n",
      "Iteration 25050 Training loss 0.10864997655153275 Validation loss 0.1049351915717125 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4851],\n",
      "        [0.5311]], device='mps:0')\n",
      "Iteration 25060 Training loss 0.1049744039773941 Validation loss 0.10493689775466919 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5207],\n",
      "        [0.6772]], device='mps:0')\n",
      "Iteration 25070 Training loss 0.11008283495903015 Validation loss 0.1049518957734108 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6440],\n",
      "        [0.5260]], device='mps:0')\n",
      "Iteration 25080 Training loss 0.11193367093801498 Validation loss 0.10490953922271729 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.2616],\n",
      "        [0.6441]], device='mps:0')\n",
      "Iteration 25090 Training loss 0.10066618025302887 Validation loss 0.10487586259841919 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.5204],\n",
      "        [0.5714]], device='mps:0')\n",
      "Iteration 25100 Training loss 0.10148358345031738 Validation loss 0.10487870126962662 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5471],\n",
      "        [0.5082]], device='mps:0')\n",
      "Iteration 25110 Training loss 0.11540348082780838 Validation loss 0.10492265224456787 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6246],\n",
      "        [0.3447]], device='mps:0')\n",
      "Iteration 25120 Training loss 0.10575391352176666 Validation loss 0.10490111261606216 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.3272],\n",
      "        [0.6530]], device='mps:0')\n",
      "Iteration 25130 Training loss 0.10629776865243912 Validation loss 0.10487126559019089 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6077],\n",
      "        [0.3689]], device='mps:0')\n",
      "Iteration 25140 Training loss 0.10143226385116577 Validation loss 0.10490495711565018 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5869],\n",
      "        [0.5276]], device='mps:0')\n",
      "Iteration 25150 Training loss 0.10786380618810654 Validation loss 0.10484731942415237 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5367],\n",
      "        [0.5274]], device='mps:0')\n",
      "Iteration 25160 Training loss 0.10526132583618164 Validation loss 0.10484828054904938 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.6295],\n",
      "        [0.6534]], device='mps:0')\n",
      "Iteration 25170 Training loss 0.10804189741611481 Validation loss 0.10483572632074356 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.6666],\n",
      "        [0.5930]], device='mps:0')\n",
      "Iteration 25180 Training loss 0.10156533867120743 Validation loss 0.10482563823461533 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.6811],\n",
      "        [0.4142]], device='mps:0')\n",
      "Iteration 25190 Training loss 0.1077985092997551 Validation loss 0.10483205318450928 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.4732],\n",
      "        [0.6568]], device='mps:0')\n",
      "Iteration 25200 Training loss 0.10835642367601395 Validation loss 0.10483090579509735 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.6217],\n",
      "        [0.4210]], device='mps:0')\n",
      "Iteration 25210 Training loss 0.1098158061504364 Validation loss 0.10483027249574661 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.5811],\n",
      "        [0.4211]], device='mps:0')\n",
      "Iteration 25220 Training loss 0.10419216006994247 Validation loss 0.10483059287071228 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5621],\n",
      "        [0.2750]], device='mps:0')\n",
      "Iteration 25230 Training loss 0.11735660582780838 Validation loss 0.1048438549041748 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5218],\n",
      "        [0.1393]], device='mps:0')\n",
      "Iteration 25240 Training loss 0.09913890808820724 Validation loss 0.10489024966955185 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.2774],\n",
      "        [0.1668]], device='mps:0')\n",
      "Iteration 25250 Training loss 0.11206909269094467 Validation loss 0.10485157370567322 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.3458],\n",
      "        [0.4885]], device='mps:0')\n",
      "Iteration 25260 Training loss 0.10737481713294983 Validation loss 0.10487100481987 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5511],\n",
      "        [0.4136]], device='mps:0')\n",
      "Iteration 25270 Training loss 0.10910474509000778 Validation loss 0.10485540330410004 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.2175],\n",
      "        [0.5555]], device='mps:0')\n",
      "Iteration 25280 Training loss 0.1110181137919426 Validation loss 0.10484390705823898 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.0889],\n",
      "        [0.5283]], device='mps:0')\n",
      "Iteration 25290 Training loss 0.10749275237321854 Validation loss 0.10483869165182114 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.5751],\n",
      "        [0.6249]], device='mps:0')\n",
      "Iteration 25300 Training loss 0.09965308755636215 Validation loss 0.10483567416667938 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.4990],\n",
      "        [0.2202]], device='mps:0')\n",
      "Iteration 25310 Training loss 0.10336314886808395 Validation loss 0.10483647882938385 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.1510],\n",
      "        [0.6265]], device='mps:0')\n",
      "Iteration 25320 Training loss 0.10141447186470032 Validation loss 0.10479617863893509 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.4952],\n",
      "        [0.6163]], device='mps:0')\n",
      "Iteration 25330 Training loss 0.11089493334293365 Validation loss 0.10478740185499191 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.6403],\n",
      "        [0.4286]], device='mps:0')\n",
      "Iteration 25340 Training loss 0.12276946008205414 Validation loss 0.10479173064231873 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5934],\n",
      "        [0.5040]], device='mps:0')\n",
      "Iteration 25350 Training loss 0.10362204909324646 Validation loss 0.10484229028224945 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.5088],\n",
      "        [0.5665]], device='mps:0')\n",
      "Iteration 25360 Training loss 0.10284695029258728 Validation loss 0.1048591285943985 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.2971],\n",
      "        [0.6212]], device='mps:0')\n",
      "Iteration 25370 Training loss 0.10448472946882248 Validation loss 0.10485568642616272 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.7102],\n",
      "        [0.6753]], device='mps:0')\n",
      "Iteration 25380 Training loss 0.10493270307779312 Validation loss 0.10482710599899292 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.6092],\n",
      "        [0.4943]], device='mps:0')\n",
      "Iteration 25390 Training loss 0.10181573033332825 Validation loss 0.10482187569141388 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.3880],\n",
      "        [0.5094]], device='mps:0')\n",
      "Iteration 25400 Training loss 0.11011222004890442 Validation loss 0.10482965409755707 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.3508],\n",
      "        [0.4667]], device='mps:0')\n",
      "Iteration 25410 Training loss 0.11887851357460022 Validation loss 0.10482123494148254 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.4919],\n",
      "        [0.3518]], device='mps:0')\n",
      "Iteration 25420 Training loss 0.10577689856290817 Validation loss 0.1047711968421936 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.3151],\n",
      "        [0.5124]], device='mps:0')\n",
      "Iteration 25430 Training loss 0.10681846737861633 Validation loss 0.10477793961763382 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.6079],\n",
      "        [0.5067]], device='mps:0')\n",
      "Iteration 25440 Training loss 0.11853589862585068 Validation loss 0.1047687903046608 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.5871],\n",
      "        [0.5022]], device='mps:0')\n",
      "Iteration 25450 Training loss 0.1100277379155159 Validation loss 0.10479866713285446 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.5631],\n",
      "        [0.4039]], device='mps:0')\n",
      "Iteration 25460 Training loss 0.105907142162323 Validation loss 0.10475777089595795 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5854],\n",
      "        [0.5236]], device='mps:0')\n",
      "Iteration 25470 Training loss 0.09620940685272217 Validation loss 0.10478497296571732 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.5130],\n",
      "        [0.5800]], device='mps:0')\n",
      "Iteration 25480 Training loss 0.10577455908060074 Validation loss 0.10478135943412781 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.2797],\n",
      "        [0.5206]], device='mps:0')\n",
      "Iteration 25490 Training loss 0.10089270025491714 Validation loss 0.10477113723754883 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.3652],\n",
      "        [0.4024]], device='mps:0')\n",
      "Iteration 25500 Training loss 0.11081159114837646 Validation loss 0.10475602000951767 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5116],\n",
      "        [0.5600]], device='mps:0')\n",
      "Iteration 25510 Training loss 0.11121116578578949 Validation loss 0.1047469973564148 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.5468],\n",
      "        [0.4654]], device='mps:0')\n",
      "Iteration 25520 Training loss 0.10841123014688492 Validation loss 0.10476900637149811 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.2235],\n",
      "        [0.5897]], device='mps:0')\n",
      "Iteration 25530 Training loss 0.10645671933889389 Validation loss 0.10475865751504898 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.4452],\n",
      "        [0.7232]], device='mps:0')\n",
      "Iteration 25540 Training loss 0.09674668312072754 Validation loss 0.10476761311292648 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.4688],\n",
      "        [0.5260]], device='mps:0')\n",
      "Iteration 25550 Training loss 0.1181679517030716 Validation loss 0.10477536171674728 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.4082],\n",
      "        [0.4751]], device='mps:0')\n",
      "Iteration 25560 Training loss 0.09782491624355316 Validation loss 0.10477408021688461 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5847],\n",
      "        [0.5996]], device='mps:0')\n",
      "Iteration 25570 Training loss 0.10876720398664474 Validation loss 0.10486763715744019 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.5133],\n",
      "        [0.6762]], device='mps:0')\n",
      "Iteration 25580 Training loss 0.10594063252210617 Validation loss 0.10487303882837296 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5449],\n",
      "        [0.7409]], device='mps:0')\n",
      "Iteration 25590 Training loss 0.11082402616739273 Validation loss 0.10483986139297485 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.3594],\n",
      "        [0.3684]], device='mps:0')\n",
      "Iteration 25600 Training loss 0.1080695316195488 Validation loss 0.10494093596935272 Accuracy 0.6940000057220459\n",
      "Output tensor([[0.1385],\n",
      "        [0.6050]], device='mps:0')\n",
      "Iteration 25610 Training loss 0.11439123004674911 Validation loss 0.10487428307533264 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6431],\n",
      "        [0.4963]], device='mps:0')\n",
      "Iteration 25620 Training loss 0.10620860755443573 Validation loss 0.10487137734889984 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.5247],\n",
      "        [0.4967]], device='mps:0')\n",
      "Iteration 25630 Training loss 0.11261697858572006 Validation loss 0.10485363006591797 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.6228],\n",
      "        [0.5643]], device='mps:0')\n",
      "Iteration 25640 Training loss 0.10552846640348434 Validation loss 0.10484667122364044 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6126],\n",
      "        [0.6366]], device='mps:0')\n",
      "Iteration 25650 Training loss 0.10525335371494293 Validation loss 0.10475212335586548 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.3374],\n",
      "        [0.5270]], device='mps:0')\n",
      "Iteration 25660 Training loss 0.09984350204467773 Validation loss 0.10477548837661743 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.5100],\n",
      "        [0.5153]], device='mps:0')\n",
      "Iteration 25670 Training loss 0.1106005609035492 Validation loss 0.10476180166006088 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5921],\n",
      "        [0.6485]], device='mps:0')\n",
      "Iteration 25680 Training loss 0.10042255371809006 Validation loss 0.10474972426891327 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5527],\n",
      "        [0.2887]], device='mps:0')\n",
      "Iteration 25690 Training loss 0.10377366840839386 Validation loss 0.10473531484603882 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.3706],\n",
      "        [0.5657]], device='mps:0')\n",
      "Iteration 25700 Training loss 0.1080254316329956 Validation loss 0.10470820963382721 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.3041],\n",
      "        [0.5773]], device='mps:0')\n",
      "Iteration 25710 Training loss 0.11002296954393387 Validation loss 0.10471147298812866 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5608],\n",
      "        [0.3716]], device='mps:0')\n",
      "Iteration 25720 Training loss 0.10353755950927734 Validation loss 0.10473725199699402 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.4133],\n",
      "        [0.5923]], device='mps:0')\n",
      "Iteration 25730 Training loss 0.10455431789159775 Validation loss 0.10480689257383347 Accuracy 0.6945000290870667\n",
      "Output tensor([[0.6385],\n",
      "        [0.4827]], device='mps:0')\n",
      "Iteration 25740 Training loss 0.10443571209907532 Validation loss 0.10475663840770721 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.3775],\n",
      "        [0.3933]], device='mps:0')\n",
      "Iteration 25750 Training loss 0.10949338972568512 Validation loss 0.10480782389640808 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.6506],\n",
      "        [0.5064]], device='mps:0')\n",
      "Iteration 25760 Training loss 0.10196033865213394 Validation loss 0.10476156324148178 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.4040],\n",
      "        [0.4830]], device='mps:0')\n",
      "Iteration 25770 Training loss 0.10461708158254623 Validation loss 0.10473250597715378 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.2660],\n",
      "        [0.6577]], device='mps:0')\n",
      "Iteration 25780 Training loss 0.10109130293130875 Validation loss 0.10472789406776428 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.4355],\n",
      "        [0.5279]], device='mps:0')\n",
      "Iteration 25790 Training loss 0.10487651824951172 Validation loss 0.10474903881549835 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.2904],\n",
      "        [0.6397]], device='mps:0')\n",
      "Iteration 25800 Training loss 0.1033361628651619 Validation loss 0.1047775149345398 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.4786],\n",
      "        [0.3419]], device='mps:0')\n",
      "Iteration 25810 Training loss 0.1055624932050705 Validation loss 0.10473420470952988 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.6521],\n",
      "        [0.1309]], device='mps:0')\n",
      "Iteration 25820 Training loss 0.10499417036771774 Validation loss 0.10471516847610474 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.5800],\n",
      "        [0.5603]], device='mps:0')\n",
      "Iteration 25830 Training loss 0.10170263051986694 Validation loss 0.10468026995658875 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.2502],\n",
      "        [0.2720]], device='mps:0')\n",
      "Iteration 25840 Training loss 0.09953389316797256 Validation loss 0.1046629399061203 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.4428],\n",
      "        [0.5169]], device='mps:0')\n",
      "Iteration 25850 Training loss 0.0995602235198021 Validation loss 0.1046677976846695 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.1963],\n",
      "        [0.5363]], device='mps:0')\n",
      "Iteration 25860 Training loss 0.10756469517946243 Validation loss 0.10468985140323639 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5885],\n",
      "        [0.5953]], device='mps:0')\n",
      "Iteration 25870 Training loss 0.11357039958238602 Validation loss 0.10472025722265244 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.6855],\n",
      "        [0.6020]], device='mps:0')\n",
      "Iteration 25880 Training loss 0.10864347219467163 Validation loss 0.10470075905323029 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5809],\n",
      "        [0.5096]], device='mps:0')\n",
      "Iteration 25890 Training loss 0.1082133948802948 Validation loss 0.10473392903804779 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5513],\n",
      "        [0.4171]], device='mps:0')\n",
      "Iteration 25900 Training loss 0.10046380758285522 Validation loss 0.10470975935459137 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.3942],\n",
      "        [0.3343]], device='mps:0')\n",
      "Iteration 25910 Training loss 0.11060220748186111 Validation loss 0.10468922555446625 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.5313],\n",
      "        [0.5279]], device='mps:0')\n",
      "Iteration 25920 Training loss 0.100002221763134 Validation loss 0.10467749834060669 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.3917],\n",
      "        [0.5521]], device='mps:0')\n",
      "Iteration 25930 Training loss 0.09367694705724716 Validation loss 0.10467293113470078 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.3492],\n",
      "        [0.5953]], device='mps:0')\n",
      "Iteration 25940 Training loss 0.09926007688045502 Validation loss 0.1046985313296318 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.4519],\n",
      "        [0.5619]], device='mps:0')\n",
      "Iteration 25950 Training loss 0.10754846036434174 Validation loss 0.10470441728830338 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.4595],\n",
      "        [0.5380]], device='mps:0')\n",
      "Iteration 25960 Training loss 0.09748687595129013 Validation loss 0.10468194633722305 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5879],\n",
      "        [0.6993]], device='mps:0')\n",
      "Iteration 25970 Training loss 0.12011601775884628 Validation loss 0.10466015338897705 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.6888],\n",
      "        [0.4063]], device='mps:0')\n",
      "Iteration 25980 Training loss 0.11238481104373932 Validation loss 0.10465066879987717 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.5273],\n",
      "        [0.5542]], device='mps:0')\n",
      "Iteration 25990 Training loss 0.10338155180215836 Validation loss 0.10467459261417389 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.4577],\n",
      "        [0.6387]], device='mps:0')\n",
      "Iteration 26000 Training loss 0.10682849586009979 Validation loss 0.10468588024377823 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5313],\n",
      "        [0.4751]], device='mps:0')\n",
      "Iteration 26010 Training loss 0.10413810610771179 Validation loss 0.1046503558754921 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5814],\n",
      "        [0.6430]], device='mps:0')\n",
      "Iteration 26020 Training loss 0.10328536480665207 Validation loss 0.10466483980417252 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.2309],\n",
      "        [0.6294]], device='mps:0')\n",
      "Iteration 26030 Training loss 0.10602056235074997 Validation loss 0.10471916943788528 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5782],\n",
      "        [0.5753]], device='mps:0')\n",
      "Iteration 26040 Training loss 0.10442999005317688 Validation loss 0.10472190380096436 Accuracy 0.6950000524520874\n",
      "Output tensor([[0.5455],\n",
      "        [0.5750]], device='mps:0')\n",
      "Iteration 26050 Training loss 0.10433593392372131 Validation loss 0.10468198359012604 Accuracy 0.6970000267028809\n",
      "Output tensor([[0.4173],\n",
      "        [0.5740]], device='mps:0')\n",
      "Iteration 26060 Training loss 0.10863574594259262 Validation loss 0.10466355830430984 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.7240],\n",
      "        [0.5664]], device='mps:0')\n",
      "Iteration 26070 Training loss 0.105911023914814 Validation loss 0.10469608008861542 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.3515],\n",
      "        [0.2730]], device='mps:0')\n",
      "Iteration 26080 Training loss 0.10927090048789978 Validation loss 0.10463713854551315 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5812],\n",
      "        [0.5892]], device='mps:0')\n",
      "Iteration 26090 Training loss 0.11071982979774475 Validation loss 0.10462897270917892 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.3404],\n",
      "        [0.6038]], device='mps:0')\n",
      "Iteration 26100 Training loss 0.11301500350236893 Validation loss 0.1046612337231636 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.6611],\n",
      "        [0.3002]], device='mps:0')\n",
      "Iteration 26110 Training loss 0.10523912310600281 Validation loss 0.1046147570014 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5077],\n",
      "        [0.3359]], device='mps:0')\n",
      "Iteration 26120 Training loss 0.1151866689324379 Validation loss 0.10460492223501205 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5350],\n",
      "        [0.3791]], device='mps:0')\n",
      "Iteration 26130 Training loss 0.10292593389749527 Validation loss 0.10458244383335114 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.5612],\n",
      "        [0.6828]], device='mps:0')\n",
      "Iteration 26140 Training loss 0.1058453917503357 Validation loss 0.10457834601402283 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.4569],\n",
      "        [0.4662]], device='mps:0')\n",
      "Iteration 26150 Training loss 0.10464859008789062 Validation loss 0.10457532107830048 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.2822],\n",
      "        [0.5355]], device='mps:0')\n",
      "Iteration 26160 Training loss 0.11106286942958832 Validation loss 0.10458164662122726 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.4854],\n",
      "        [0.5871]], device='mps:0')\n",
      "Iteration 26170 Training loss 0.09813006967306137 Validation loss 0.10461258143186569 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.3472],\n",
      "        [0.6496]], device='mps:0')\n",
      "Iteration 26180 Training loss 0.11202535033226013 Validation loss 0.10458622127771378 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.4524],\n",
      "        [0.4641]], device='mps:0')\n",
      "Iteration 26190 Training loss 0.10861772298812866 Validation loss 0.1045801043510437 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.4104],\n",
      "        [0.4059]], device='mps:0')\n",
      "Iteration 26200 Training loss 0.10779493302106857 Validation loss 0.10458505898714066 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.6020],\n",
      "        [0.4886]], device='mps:0')\n",
      "Iteration 26210 Training loss 0.10841081291437149 Validation loss 0.10458187013864517 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.5377],\n",
      "        [0.4907]], device='mps:0')\n",
      "Iteration 26220 Training loss 0.10012071579694748 Validation loss 0.10456708073616028 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.6112],\n",
      "        [0.2469]], device='mps:0')\n",
      "Iteration 26230 Training loss 0.10003276914358139 Validation loss 0.10456081479787827 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5209],\n",
      "        [0.5329]], device='mps:0')\n",
      "Iteration 26240 Training loss 0.11367671936750412 Validation loss 0.1045626550912857 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.5375],\n",
      "        [0.4379]], device='mps:0')\n",
      "Iteration 26250 Training loss 0.12450357526540756 Validation loss 0.10455784946680069 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5679],\n",
      "        [0.4836]], device='mps:0')\n",
      "Iteration 26260 Training loss 0.10790781676769257 Validation loss 0.104573093354702 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.4000],\n",
      "        [0.4885]], device='mps:0')\n",
      "Iteration 26270 Training loss 0.10626634210348129 Validation loss 0.10458233207464218 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.6710],\n",
      "        [0.3581]], device='mps:0')\n",
      "Iteration 26280 Training loss 0.11329902708530426 Validation loss 0.10456502437591553 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.5057],\n",
      "        [0.4800]], device='mps:0')\n",
      "Iteration 26290 Training loss 0.1092204749584198 Validation loss 0.10455986857414246 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.4303],\n",
      "        [0.3368]], device='mps:0')\n",
      "Iteration 26300 Training loss 0.1006040945649147 Validation loss 0.10457277297973633 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5574],\n",
      "        [0.5355]], device='mps:0')\n",
      "Iteration 26310 Training loss 0.10204178839921951 Validation loss 0.10457269847393036 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.6282],\n",
      "        [0.4116]], device='mps:0')\n",
      "Iteration 26320 Training loss 0.11275903135538101 Validation loss 0.10457610338926315 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.5261],\n",
      "        [0.4683]], device='mps:0')\n",
      "Iteration 26330 Training loss 0.10266932100057602 Validation loss 0.10455014556646347 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.5471],\n",
      "        [0.3886]], device='mps:0')\n",
      "Iteration 26340 Training loss 0.11283576488494873 Validation loss 0.10459344834089279 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.4354],\n",
      "        [0.2299]], device='mps:0')\n",
      "Iteration 26350 Training loss 0.11269048601388931 Validation loss 0.10458047688007355 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5577],\n",
      "        [0.5543]], device='mps:0')\n",
      "Iteration 26360 Training loss 0.1110311970114708 Validation loss 0.10458934307098389 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.4662],\n",
      "        [0.6221]], device='mps:0')\n",
      "Iteration 26370 Training loss 0.10435163229703903 Validation loss 0.10462842136621475 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.3304],\n",
      "        [0.4458]], device='mps:0')\n",
      "Iteration 26380 Training loss 0.10644780844449997 Validation loss 0.10474367439746857 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5322],\n",
      "        [0.2625]], device='mps:0')\n",
      "Iteration 26390 Training loss 0.09738948941230774 Validation loss 0.1048440933227539 Accuracy 0.6960000395774841\n",
      "Output tensor([[0.6778],\n",
      "        [0.5625]], device='mps:0')\n",
      "Iteration 26400 Training loss 0.10404392331838608 Validation loss 0.1047501415014267 Accuracy 0.6935000419616699\n",
      "Output tensor([[0.5240],\n",
      "        [0.6880]], device='mps:0')\n",
      "Iteration 26410 Training loss 0.10345816612243652 Validation loss 0.10478995740413666 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.3264],\n",
      "        [0.1557]], device='mps:0')\n",
      "Iteration 26420 Training loss 0.10646624118089676 Validation loss 0.10460879653692245 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.2419],\n",
      "        [0.6748]], device='mps:0')\n",
      "Iteration 26430 Training loss 0.10903339087963104 Validation loss 0.10460599511861801 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.3725],\n",
      "        [0.3223]], device='mps:0')\n",
      "Iteration 26440 Training loss 0.1004362404346466 Validation loss 0.1046229749917984 Accuracy 0.6955000162124634\n",
      "Output tensor([[0.5282],\n",
      "        [0.4881]], device='mps:0')\n",
      "Iteration 26450 Training loss 0.10346128046512604 Validation loss 0.10454927384853363 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5467],\n",
      "        [0.6466]], device='mps:0')\n",
      "Iteration 26460 Training loss 0.10918062180280685 Validation loss 0.10454419255256653 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.3981],\n",
      "        [0.2592]], device='mps:0')\n",
      "Iteration 26470 Training loss 0.10188814252614975 Validation loss 0.10454889386892319 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.6420],\n",
      "        [0.7046]], device='mps:0')\n",
      "Iteration 26480 Training loss 0.10906043648719788 Validation loss 0.10452182590961456 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.5293],\n",
      "        [0.5863]], device='mps:0')\n",
      "Iteration 26490 Training loss 0.10694350302219391 Validation loss 0.10450974106788635 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.5676],\n",
      "        [0.3826]], device='mps:0')\n",
      "Iteration 26500 Training loss 0.1052083671092987 Validation loss 0.10449926555156708 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.5851],\n",
      "        [0.6024]], device='mps:0')\n",
      "Iteration 26510 Training loss 0.10053952783346176 Validation loss 0.10450972616672516 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5950],\n",
      "        [0.4186]], device='mps:0')\n",
      "Iteration 26520 Training loss 0.1076212078332901 Validation loss 0.10451431572437286 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5980],\n",
      "        [0.4878]], device='mps:0')\n",
      "Iteration 26530 Training loss 0.12188483774662018 Validation loss 0.10450976341962814 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.4032],\n",
      "        [0.6639]], device='mps:0')\n",
      "Iteration 26540 Training loss 0.10398153215646744 Validation loss 0.10449863225221634 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.5833],\n",
      "        [0.6582]], device='mps:0')\n",
      "Iteration 26550 Training loss 0.11533122509717941 Validation loss 0.10450610518455505 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.2145],\n",
      "        [0.5782]], device='mps:0')\n",
      "Iteration 26560 Training loss 0.11399147659540176 Validation loss 0.1044938862323761 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.3714],\n",
      "        [0.4852]], device='mps:0')\n",
      "Iteration 26570 Training loss 0.112263984978199 Validation loss 0.10448822379112244 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.4238],\n",
      "        [0.5055]], device='mps:0')\n",
      "Iteration 26580 Training loss 0.10265493392944336 Validation loss 0.10448913276195526 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5464],\n",
      "        [0.6741]], device='mps:0')\n",
      "Iteration 26590 Training loss 0.10342216491699219 Validation loss 0.10448060929775238 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.6290],\n",
      "        [0.5130]], device='mps:0')\n",
      "Iteration 26600 Training loss 0.10462507605552673 Validation loss 0.10447626560926437 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.4384],\n",
      "        [0.6157]], device='mps:0')\n",
      "Iteration 26610 Training loss 0.10965191572904587 Validation loss 0.10448566824197769 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.3602],\n",
      "        [0.6939]], device='mps:0')\n",
      "Iteration 26620 Training loss 0.10562282055616379 Validation loss 0.1044861227273941 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.4388],\n",
      "        [0.5872]], device='mps:0')\n",
      "Iteration 26630 Training loss 0.11371663957834244 Validation loss 0.104474738240242 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.1214],\n",
      "        [0.5566]], device='mps:0')\n",
      "Iteration 26640 Training loss 0.0927344486117363 Validation loss 0.10451389104127884 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.3128],\n",
      "        [0.3220]], device='mps:0')\n",
      "Iteration 26650 Training loss 0.10374240577220917 Validation loss 0.10449562221765518 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5852],\n",
      "        [0.6728]], device='mps:0')\n",
      "Iteration 26660 Training loss 0.08635568618774414 Validation loss 0.10451865196228027 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5660],\n",
      "        [0.3438]], device='mps:0')\n",
      "Iteration 26670 Training loss 0.09640186280012131 Validation loss 0.10452505946159363 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.4038],\n",
      "        [0.4528]], device='mps:0')\n",
      "Iteration 26680 Training loss 0.10579642653465271 Validation loss 0.1045360341668129 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.3185],\n",
      "        [0.6833]], device='mps:0')\n",
      "Iteration 26690 Training loss 0.11017130315303802 Validation loss 0.10449539124965668 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5403],\n",
      "        [0.3602]], device='mps:0')\n",
      "Iteration 26700 Training loss 0.10826835036277771 Validation loss 0.1045076996088028 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.2182],\n",
      "        [0.2406]], device='mps:0')\n",
      "Iteration 26710 Training loss 0.10529167950153351 Validation loss 0.10448861867189407 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.7378],\n",
      "        [0.4720]], device='mps:0')\n",
      "Iteration 26720 Training loss 0.10491172224283218 Validation loss 0.10448197275400162 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.4754],\n",
      "        [0.5987]], device='mps:0')\n",
      "Iteration 26730 Training loss 0.10742151737213135 Validation loss 0.10447373241186142 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.6001],\n",
      "        [0.4072]], device='mps:0')\n",
      "Iteration 26740 Training loss 0.09764132648706436 Validation loss 0.10449590533971786 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.5738],\n",
      "        [0.6198]], device='mps:0')\n",
      "Iteration 26750 Training loss 0.10709401965141296 Validation loss 0.10446323454380035 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5311],\n",
      "        [0.6323]], device='mps:0')\n",
      "Iteration 26760 Training loss 0.09635709226131439 Validation loss 0.10446237772703171 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.4162],\n",
      "        [0.5664]], device='mps:0')\n",
      "Iteration 26770 Training loss 0.11320880055427551 Validation loss 0.10443837195634842 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5653],\n",
      "        [0.5370]], device='mps:0')\n",
      "Iteration 26780 Training loss 0.11160268634557724 Validation loss 0.10444819927215576 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.6067],\n",
      "        [0.2945]], device='mps:0')\n",
      "Iteration 26790 Training loss 0.10978385806083679 Validation loss 0.10446188598871231 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.7298],\n",
      "        [0.6889]], device='mps:0')\n",
      "Iteration 26800 Training loss 0.0987107902765274 Validation loss 0.1044839546084404 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.7002],\n",
      "        [0.5531]], device='mps:0')\n",
      "Iteration 26810 Training loss 0.10306090116500854 Validation loss 0.10452736169099808 Accuracy 0.6965000033378601\n",
      "Output tensor([[0.6782],\n",
      "        [0.6514]], device='mps:0')\n",
      "Iteration 26820 Training loss 0.1038612425327301 Validation loss 0.10446220636367798 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.3930],\n",
      "        [0.5018]], device='mps:0')\n",
      "Iteration 26830 Training loss 0.10576474666595459 Validation loss 0.10444553196430206 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.6253],\n",
      "        [0.3493]], device='mps:0')\n",
      "Iteration 26840 Training loss 0.09696906805038452 Validation loss 0.10446140170097351 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.4685],\n",
      "        [0.4674]], device='mps:0')\n",
      "Iteration 26850 Training loss 0.1066974326968193 Validation loss 0.10445436835289001 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.4196],\n",
      "        [0.6236]], device='mps:0')\n",
      "Iteration 26860 Training loss 0.11488111317157745 Validation loss 0.10443929582834244 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.3926],\n",
      "        [0.4221]], device='mps:0')\n",
      "Iteration 26870 Training loss 0.10694923251867294 Validation loss 0.10442020744085312 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.5705],\n",
      "        [0.5826]], device='mps:0')\n",
      "Iteration 26880 Training loss 0.10360410809516907 Validation loss 0.10443469136953354 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.3473],\n",
      "        [0.5506]], device='mps:0')\n",
      "Iteration 26890 Training loss 0.12056519836187363 Validation loss 0.10441704839468002 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.2979],\n",
      "        [0.4267]], device='mps:0')\n",
      "Iteration 26900 Training loss 0.11251092702150345 Validation loss 0.10441536456346512 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.6645],\n",
      "        [0.5551]], device='mps:0')\n",
      "Iteration 26910 Training loss 0.097338005900383 Validation loss 0.10442119091749191 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5915],\n",
      "        [0.3932]], device='mps:0')\n",
      "Iteration 26920 Training loss 0.102048359811306 Validation loss 0.10440712422132492 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.3802],\n",
      "        [0.6038]], device='mps:0')\n",
      "Iteration 26930 Training loss 0.10541348904371262 Validation loss 0.10440521687269211 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5157],\n",
      "        [0.6419]], device='mps:0')\n",
      "Iteration 26940 Training loss 0.10934526473283768 Validation loss 0.10440396517515182 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5901],\n",
      "        [0.6095]], device='mps:0')\n",
      "Iteration 26950 Training loss 0.10879823565483093 Validation loss 0.10440298914909363 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6594],\n",
      "        [0.4258]], device='mps:0')\n",
      "Iteration 26960 Training loss 0.10497787594795227 Validation loss 0.1043989285826683 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.3412],\n",
      "        [0.5067]], device='mps:0')\n",
      "Iteration 26970 Training loss 0.1134735643863678 Validation loss 0.10439714044332504 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5379],\n",
      "        [0.4906]], device='mps:0')\n",
      "Iteration 26980 Training loss 0.09517017006874084 Validation loss 0.10439115017652512 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.3079],\n",
      "        [0.6617]], device='mps:0')\n",
      "Iteration 26990 Training loss 0.11313582956790924 Validation loss 0.10438991338014603 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5603],\n",
      "        [0.3753]], device='mps:0')\n",
      "Iteration 27000 Training loss 0.10269293189048767 Validation loss 0.10438843816518784 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.4019],\n",
      "        [0.5206]], device='mps:0')\n",
      "Iteration 27010 Training loss 0.1062835156917572 Validation loss 0.10439115017652512 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4661],\n",
      "        [0.6408]], device='mps:0')\n",
      "Iteration 27020 Training loss 0.1026691198348999 Validation loss 0.1043849065899849 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.6601],\n",
      "        [0.5721]], device='mps:0')\n",
      "Iteration 27030 Training loss 0.11084863543510437 Validation loss 0.10439653694629669 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5590],\n",
      "        [0.3047]], device='mps:0')\n",
      "Iteration 27040 Training loss 0.1122102439403534 Validation loss 0.10438254475593567 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.6046],\n",
      "        [0.5091]], device='mps:0')\n",
      "Iteration 27050 Training loss 0.1090877428650856 Validation loss 0.10437807440757751 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5526],\n",
      "        [0.7326]], device='mps:0')\n",
      "Iteration 27060 Training loss 0.10920824110507965 Validation loss 0.10437814891338348 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.6098],\n",
      "        [0.5988]], device='mps:0')\n",
      "Iteration 27070 Training loss 0.11251657456159592 Validation loss 0.10437837243080139 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.4136],\n",
      "        [0.2970]], device='mps:0')\n",
      "Iteration 27080 Training loss 0.10618934035301208 Validation loss 0.10437311232089996 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5635],\n",
      "        [0.6131]], device='mps:0')\n",
      "Iteration 27090 Training loss 0.11392055451869965 Validation loss 0.10442804545164108 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.4388],\n",
      "        [0.2908]], device='mps:0')\n",
      "Iteration 27100 Training loss 0.10111243277788162 Validation loss 0.10445163398981094 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.7587],\n",
      "        [0.4285]], device='mps:0')\n",
      "Iteration 27110 Training loss 0.10774070769548416 Validation loss 0.10447771847248077 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.3661],\n",
      "        [0.5660]], device='mps:0')\n",
      "Iteration 27120 Training loss 0.10828831791877747 Validation loss 0.10447360575199127 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.2945],\n",
      "        [0.5152]], device='mps:0')\n",
      "Iteration 27130 Training loss 0.11545901745557785 Validation loss 0.10440193116664886 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.4984],\n",
      "        [0.4720]], device='mps:0')\n",
      "Iteration 27140 Training loss 0.11905739456415176 Validation loss 0.10441254079341888 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.7049],\n",
      "        [0.6077]], device='mps:0')\n",
      "Iteration 27150 Training loss 0.11932133138179779 Validation loss 0.10440506786108017 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.5675],\n",
      "        [0.7802]], device='mps:0')\n",
      "Iteration 27160 Training loss 0.10562767833471298 Validation loss 0.10440769046545029 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.6117],\n",
      "        [0.3001]], device='mps:0')\n",
      "Iteration 27170 Training loss 0.10790334641933441 Validation loss 0.10437966883182526 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4256],\n",
      "        [0.5399]], device='mps:0')\n",
      "Iteration 27180 Training loss 0.09965873509645462 Validation loss 0.10440543293952942 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.2320],\n",
      "        [0.5573]], device='mps:0')\n",
      "Iteration 27190 Training loss 0.11352506279945374 Validation loss 0.10436739027500153 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5474],\n",
      "        [0.4506]], device='mps:0')\n",
      "Iteration 27200 Training loss 0.09961267560720444 Validation loss 0.1043674573302269 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.6472],\n",
      "        [0.6593]], device='mps:0')\n",
      "Iteration 27210 Training loss 0.11432696878910065 Validation loss 0.10437548905611038 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.3488],\n",
      "        [0.2932]], device='mps:0')\n",
      "Iteration 27220 Training loss 0.09671636670827866 Validation loss 0.10437902808189392 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.3929],\n",
      "        [0.6020]], device='mps:0')\n",
      "Iteration 27230 Training loss 0.10741148889064789 Validation loss 0.10434898734092712 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.7030],\n",
      "        [0.5665]], device='mps:0')\n",
      "Iteration 27240 Training loss 0.10573385655879974 Validation loss 0.10435588657855988 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.4683],\n",
      "        [0.0800]], device='mps:0')\n",
      "Iteration 27250 Training loss 0.09125802665948868 Validation loss 0.10434188693761826 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.4549],\n",
      "        [0.5451]], device='mps:0')\n",
      "Iteration 27260 Training loss 0.10583917051553726 Validation loss 0.10433830320835114 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.4712],\n",
      "        [0.6253]], device='mps:0')\n",
      "Iteration 27270 Training loss 0.1037551686167717 Validation loss 0.10434214025735855 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5766],\n",
      "        [0.6012]], device='mps:0')\n",
      "Iteration 27280 Training loss 0.10202796757221222 Validation loss 0.1043533980846405 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.6011],\n",
      "        [0.5226]], device='mps:0')\n",
      "Iteration 27290 Training loss 0.11052097380161285 Validation loss 0.10433708876371384 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.6252],\n",
      "        [0.5931]], device='mps:0')\n",
      "Iteration 27300 Training loss 0.11225501447916031 Validation loss 0.10432993620634079 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.3285],\n",
      "        [0.5410]], device='mps:0')\n",
      "Iteration 27310 Training loss 0.10656528919935226 Validation loss 0.10432763397693634 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5937],\n",
      "        [0.4925]], device='mps:0')\n",
      "Iteration 27320 Training loss 0.1057954952120781 Validation loss 0.10432685166597366 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5724],\n",
      "        [0.4897]], device='mps:0')\n",
      "Iteration 27330 Training loss 0.10652104765176773 Validation loss 0.10432631522417068 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6074],\n",
      "        [0.4423]], device='mps:0')\n",
      "Iteration 27340 Training loss 0.11111768335103989 Validation loss 0.10432637482881546 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5753],\n",
      "        [0.5357]], device='mps:0')\n",
      "Iteration 27350 Training loss 0.10364604741334915 Validation loss 0.10434658825397491 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.3819],\n",
      "        [0.6595]], device='mps:0')\n",
      "Iteration 27360 Training loss 0.11379219591617584 Validation loss 0.1043323203921318 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.5430],\n",
      "        [0.2822]], device='mps:0')\n",
      "Iteration 27370 Training loss 0.1043490394949913 Validation loss 0.10433698445558548 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.7119],\n",
      "        [0.2311]], device='mps:0')\n",
      "Iteration 27380 Training loss 0.10193586349487305 Validation loss 0.10434035956859589 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.2722],\n",
      "        [0.3805]], device='mps:0')\n",
      "Iteration 27390 Training loss 0.10447745770215988 Validation loss 0.10431650280952454 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.1811],\n",
      "        [0.5056]], device='mps:0')\n",
      "Iteration 27400 Training loss 0.10189700871706009 Validation loss 0.10432234406471252 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.4469],\n",
      "        [0.2020]], device='mps:0')\n",
      "Iteration 27410 Training loss 0.11084949225187302 Validation loss 0.10431170463562012 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.6292],\n",
      "        [0.2451]], device='mps:0')\n",
      "Iteration 27420 Training loss 0.10620617866516113 Validation loss 0.10431085526943207 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5889],\n",
      "        [0.5578]], device='mps:0')\n",
      "Iteration 27430 Training loss 0.10913509130477905 Validation loss 0.1043284684419632 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5697],\n",
      "        [0.6582]], device='mps:0')\n",
      "Iteration 27440 Training loss 0.10386563837528229 Validation loss 0.10431575030088425 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.4586],\n",
      "        [0.5797]], device='mps:0')\n",
      "Iteration 27450 Training loss 0.10401782393455505 Validation loss 0.10431225597858429 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.3649],\n",
      "        [0.4836]], device='mps:0')\n",
      "Iteration 27460 Training loss 0.09772661328315735 Validation loss 0.10431855171918869 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.5599],\n",
      "        [0.6904]], device='mps:0')\n",
      "Iteration 27470 Training loss 0.09626293182373047 Validation loss 0.10430914908647537 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5005],\n",
      "        [0.4433]], device='mps:0')\n",
      "Iteration 27480 Training loss 0.10777144879102707 Validation loss 0.10432042181491852 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.6369],\n",
      "        [0.5355]], device='mps:0')\n",
      "Iteration 27490 Training loss 0.10147418081760406 Validation loss 0.10431894659996033 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.6263],\n",
      "        [0.5221]], device='mps:0')\n",
      "Iteration 27500 Training loss 0.11141093820333481 Validation loss 0.10434015840291977 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.4296],\n",
      "        [0.3523]], device='mps:0')\n",
      "Iteration 27510 Training loss 0.09631162136793137 Validation loss 0.10431229323148727 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.4850],\n",
      "        [0.3064]], device='mps:0')\n",
      "Iteration 27520 Training loss 0.10188951343297958 Validation loss 0.10430300235748291 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.5909],\n",
      "        [0.5602]], device='mps:0')\n",
      "Iteration 27530 Training loss 0.10122267156839371 Validation loss 0.10436571389436722 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.5579],\n",
      "        [0.6712]], device='mps:0')\n",
      "Iteration 27540 Training loss 0.11135164648294449 Validation loss 0.10437161475419998 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.3112],\n",
      "        [0.2123]], device='mps:0')\n",
      "Iteration 27550 Training loss 0.1073172390460968 Validation loss 0.10440336167812347 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.5504],\n",
      "        [0.7230]], device='mps:0')\n",
      "Iteration 27560 Training loss 0.11388727277517319 Validation loss 0.10442119091749191 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.5228],\n",
      "        [0.4494]], device='mps:0')\n",
      "Iteration 27570 Training loss 0.1040576919913292 Validation loss 0.1044064536690712 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.5900],\n",
      "        [0.4809]], device='mps:0')\n",
      "Iteration 27580 Training loss 0.09796488285064697 Validation loss 0.10440818220376968 Accuracy 0.6980000138282776\n",
      "Output tensor([[0.6401],\n",
      "        [0.4764]], device='mps:0')\n",
      "Iteration 27590 Training loss 0.09230604022741318 Validation loss 0.10434605181217194 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.5043],\n",
      "        [0.5169]], device='mps:0')\n",
      "Iteration 27600 Training loss 0.11210056394338608 Validation loss 0.10434629023075104 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.5112],\n",
      "        [0.1459]], device='mps:0')\n",
      "Iteration 27610 Training loss 0.10007932782173157 Validation loss 0.10431227833032608 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5465],\n",
      "        [0.6635]], device='mps:0')\n",
      "Iteration 27620 Training loss 0.11781151592731476 Validation loss 0.1043124869465828 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5089],\n",
      "        [0.5974]], device='mps:0')\n",
      "Iteration 27630 Training loss 0.1141737699508667 Validation loss 0.10431267321109772 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.3976],\n",
      "        [0.5862]], device='mps:0')\n",
      "Iteration 27640 Training loss 0.10515131056308746 Validation loss 0.10433173179626465 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5018],\n",
      "        [0.5773]], device='mps:0')\n",
      "Iteration 27650 Training loss 0.09540665149688721 Validation loss 0.10428296029567719 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.3473],\n",
      "        [0.4774]], device='mps:0')\n",
      "Iteration 27660 Training loss 0.10100865364074707 Validation loss 0.10431551188230515 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.4697],\n",
      "        [0.1979]], device='mps:0')\n",
      "Iteration 27670 Training loss 0.1174093633890152 Validation loss 0.10427727550268173 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5678],\n",
      "        [0.7058]], device='mps:0')\n",
      "Iteration 27680 Training loss 0.10516570508480072 Validation loss 0.10426601022481918 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5316],\n",
      "        [0.6536]], device='mps:0')\n",
      "Iteration 27690 Training loss 0.11125259101390839 Validation loss 0.10425896942615509 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5853],\n",
      "        [0.6194]], device='mps:0')\n",
      "Iteration 27700 Training loss 0.10092043876647949 Validation loss 0.10425161570310593 Accuracy 0.703000009059906\n",
      "Output tensor([[0.3727],\n",
      "        [0.2725]], device='mps:0')\n",
      "Iteration 27710 Training loss 0.09914529323577881 Validation loss 0.10425225645303726 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5892],\n",
      "        [0.4766]], device='mps:0')\n",
      "Iteration 27720 Training loss 0.11049650609493256 Validation loss 0.10426320135593414 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5373],\n",
      "        [0.5066]], device='mps:0')\n",
      "Iteration 27730 Training loss 0.110981784760952 Validation loss 0.10424631088972092 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5981],\n",
      "        [0.4941]], device='mps:0')\n",
      "Iteration 27740 Training loss 0.10892786085605621 Validation loss 0.1042635515332222 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5646],\n",
      "        [0.4734]], device='mps:0')\n",
      "Iteration 27750 Training loss 0.10281675308942795 Validation loss 0.10425357520580292 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.6383],\n",
      "        [0.6535]], device='mps:0')\n",
      "Iteration 27760 Training loss 0.11124778538942337 Validation loss 0.1042713075876236 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5088],\n",
      "        [0.3313]], device='mps:0')\n",
      "Iteration 27770 Training loss 0.09428814798593521 Validation loss 0.10425601899623871 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5598],\n",
      "        [0.4662]], device='mps:0')\n",
      "Iteration 27780 Training loss 0.10543806105852127 Validation loss 0.10423720628023148 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.2792],\n",
      "        [0.5327]], device='mps:0')\n",
      "Iteration 27790 Training loss 0.10416042059659958 Validation loss 0.10423342138528824 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6402],\n",
      "        [0.6052]], device='mps:0')\n",
      "Iteration 27800 Training loss 0.1078779399394989 Validation loss 0.10423040390014648 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5618],\n",
      "        [0.3916]], device='mps:0')\n",
      "Iteration 27810 Training loss 0.10121601074934006 Validation loss 0.10422777384519577 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5322],\n",
      "        [0.1144]], device='mps:0')\n",
      "Iteration 27820 Training loss 0.1168908178806305 Validation loss 0.1042388379573822 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4926],\n",
      "        [0.2755]], device='mps:0')\n",
      "Iteration 27830 Training loss 0.09150218963623047 Validation loss 0.1042308434844017 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5949],\n",
      "        [0.2318]], device='mps:0')\n",
      "Iteration 27840 Training loss 0.11026631295681 Validation loss 0.10422241687774658 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4131],\n",
      "        [0.5861]], device='mps:0')\n",
      "Iteration 27850 Training loss 0.10346158593893051 Validation loss 0.10422930121421814 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.6943],\n",
      "        [0.6977]], device='mps:0')\n",
      "Iteration 27860 Training loss 0.10826810449361801 Validation loss 0.10422920435667038 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.3940],\n",
      "        [0.5367]], device='mps:0')\n",
      "Iteration 27870 Training loss 0.11139383167028427 Validation loss 0.104282446205616 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.4334],\n",
      "        [0.7159]], device='mps:0')\n",
      "Iteration 27880 Training loss 0.10252397507429123 Validation loss 0.1043165922164917 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5596],\n",
      "        [0.5178]], device='mps:0')\n",
      "Iteration 27890 Training loss 0.10400395095348358 Validation loss 0.10428116470575333 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5729],\n",
      "        [0.6544]], device='mps:0')\n",
      "Iteration 27900 Training loss 0.10059098154306412 Validation loss 0.10428124666213989 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.4875],\n",
      "        [0.3242]], device='mps:0')\n",
      "Iteration 27910 Training loss 0.10742143541574478 Validation loss 0.10427681356668472 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.6160],\n",
      "        [0.3415]], device='mps:0')\n",
      "Iteration 27920 Training loss 0.11624785512685776 Validation loss 0.10428087413311005 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.4991],\n",
      "        [0.2700]], device='mps:0')\n",
      "Iteration 27930 Training loss 0.11582136154174805 Validation loss 0.1042170599102974 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.6263],\n",
      "        [0.4248]], device='mps:0')\n",
      "Iteration 27940 Training loss 0.09767008572816849 Validation loss 0.1042303815484047 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5624],\n",
      "        [0.4384]], device='mps:0')\n",
      "Iteration 27950 Training loss 0.0990862026810646 Validation loss 0.1042168140411377 Accuracy 0.70250004529953\n",
      "Output tensor([[0.1559],\n",
      "        [0.5318]], device='mps:0')\n",
      "Iteration 27960 Training loss 0.10912549495697021 Validation loss 0.10423607379198074 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4824],\n",
      "        [0.5797]], device='mps:0')\n",
      "Iteration 27970 Training loss 0.10573694109916687 Validation loss 0.10426337271928787 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5575],\n",
      "        [0.5985]], device='mps:0')\n",
      "Iteration 27980 Training loss 0.10555329918861389 Validation loss 0.10424734652042389 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.3543],\n",
      "        [0.3691]], device='mps:0')\n",
      "Iteration 27990 Training loss 0.11313029378652573 Validation loss 0.10421077162027359 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5792],\n",
      "        [0.3558]], device='mps:0')\n",
      "Iteration 28000 Training loss 0.10111887007951736 Validation loss 0.10422112047672272 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4049],\n",
      "        [0.6693]], device='mps:0')\n",
      "Iteration 28010 Training loss 0.10719896107912064 Validation loss 0.10422763228416443 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4651],\n",
      "        [0.3237]], device='mps:0')\n",
      "Iteration 28020 Training loss 0.09711692482233047 Validation loss 0.10426797717809677 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.6016],\n",
      "        [0.4848]], device='mps:0')\n",
      "Iteration 28030 Training loss 0.10310126841068268 Validation loss 0.10424721986055374 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5374],\n",
      "        [0.6080]], device='mps:0')\n",
      "Iteration 28040 Training loss 0.0919327586889267 Validation loss 0.1042545810341835 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.4596],\n",
      "        [0.5753]], device='mps:0')\n",
      "Iteration 28050 Training loss 0.09673407673835754 Validation loss 0.10420064628124237 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.2491],\n",
      "        [0.5421]], device='mps:0')\n",
      "Iteration 28060 Training loss 0.10918144881725311 Validation loss 0.10421108454465866 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.4379],\n",
      "        [0.3485]], device='mps:0')\n",
      "Iteration 28070 Training loss 0.10823280364274979 Validation loss 0.10419127345085144 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.6137],\n",
      "        [0.6665]], device='mps:0')\n",
      "Iteration 28080 Training loss 0.09411559253931046 Validation loss 0.10419251024723053 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5021],\n",
      "        [0.4324]], device='mps:0')\n",
      "Iteration 28090 Training loss 0.1072138249874115 Validation loss 0.10417775064706802 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5582],\n",
      "        [0.5449]], device='mps:0')\n",
      "Iteration 28100 Training loss 0.09585803747177124 Validation loss 0.10417909920215607 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5806],\n",
      "        [0.4348]], device='mps:0')\n",
      "Iteration 28110 Training loss 0.10680408030748367 Validation loss 0.10416828095912933 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4811],\n",
      "        [0.5784]], device='mps:0')\n",
      "Iteration 28120 Training loss 0.11038266867399216 Validation loss 0.10419316589832306 Accuracy 0.703000009059906\n",
      "Output tensor([[0.0955],\n",
      "        [0.6297]], device='mps:0')\n",
      "Iteration 28130 Training loss 0.10314244776964188 Validation loss 0.1041727289557457 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.3368],\n",
      "        [0.5308]], device='mps:0')\n",
      "Iteration 28140 Training loss 0.11147993057966232 Validation loss 0.10417491942644119 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.5924],\n",
      "        [0.2615]], device='mps:0')\n",
      "Iteration 28150 Training loss 0.10036370158195496 Validation loss 0.10419103503227234 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6902],\n",
      "        [0.4601]], device='mps:0')\n",
      "Iteration 28160 Training loss 0.10759606957435608 Validation loss 0.10424014925956726 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5740],\n",
      "        [0.6009]], device='mps:0')\n",
      "Iteration 28170 Training loss 0.10708267241716385 Validation loss 0.10427631437778473 Accuracy 0.6995000243186951\n",
      "Output tensor([[0.5550],\n",
      "        [0.2116]], device='mps:0')\n",
      "Iteration 28180 Training loss 0.10284516960382462 Validation loss 0.10418470948934555 Accuracy 0.703000009059906\n",
      "Output tensor([[0.3793],\n",
      "        [0.5283]], device='mps:0')\n",
      "Iteration 28190 Training loss 0.10643357783555984 Validation loss 0.10416922718286514 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5150],\n",
      "        [0.5068]], device='mps:0')\n",
      "Iteration 28200 Training loss 0.11146921664476395 Validation loss 0.1041732206940651 Accuracy 0.70250004529953\n",
      "Output tensor([[0.7048],\n",
      "        [0.6835]], device='mps:0')\n",
      "Iteration 28210 Training loss 0.10815612226724625 Validation loss 0.10417160391807556 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4219],\n",
      "        [0.5337]], device='mps:0')\n",
      "Iteration 28220 Training loss 0.10397312045097351 Validation loss 0.10423033684492111 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5064],\n",
      "        [0.4194]], device='mps:0')\n",
      "Iteration 28230 Training loss 0.10874173790216446 Validation loss 0.10420475900173187 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5205],\n",
      "        [0.3134]], device='mps:0')\n",
      "Iteration 28240 Training loss 0.10416846722364426 Validation loss 0.10416960716247559 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6376],\n",
      "        [0.4931]], device='mps:0')\n",
      "Iteration 28250 Training loss 0.10663925856351852 Validation loss 0.10417275875806808 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6359],\n",
      "        [0.5230]], device='mps:0')\n",
      "Iteration 28260 Training loss 0.1046542152762413 Validation loss 0.10417292267084122 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5668],\n",
      "        [0.3909]], device='mps:0')\n",
      "Iteration 28270 Training loss 0.10246288776397705 Validation loss 0.10420802235603333 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5248],\n",
      "        [0.1801]], device='mps:0')\n",
      "Iteration 28280 Training loss 0.11049741506576538 Validation loss 0.10419818758964539 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5991],\n",
      "        [0.4524]], device='mps:0')\n",
      "Iteration 28290 Training loss 0.106767937541008 Validation loss 0.10417602211236954 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6005],\n",
      "        [0.5147]], device='mps:0')\n",
      "Iteration 28300 Training loss 0.1039920225739479 Validation loss 0.10418859124183655 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5132],\n",
      "        [0.6418]], device='mps:0')\n",
      "Iteration 28310 Training loss 0.11263389140367508 Validation loss 0.1041778177022934 Accuracy 0.70250004529953\n",
      "Output tensor([[0.7475],\n",
      "        [0.3468]], device='mps:0')\n",
      "Iteration 28320 Training loss 0.10560911148786545 Validation loss 0.1041831448674202 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5634],\n",
      "        [0.6246]], device='mps:0')\n",
      "Iteration 28330 Training loss 0.10977963358163834 Validation loss 0.10418403893709183 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6578],\n",
      "        [0.6441]], device='mps:0')\n",
      "Iteration 28340 Training loss 0.10706309974193573 Validation loss 0.10417495667934418 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5341],\n",
      "        [0.7166]], device='mps:0')\n",
      "Iteration 28350 Training loss 0.11047632247209549 Validation loss 0.10415094345808029 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5079],\n",
      "        [0.4600]], device='mps:0')\n",
      "Iteration 28360 Training loss 0.11145949363708496 Validation loss 0.10414175689220428 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4886],\n",
      "        [0.3468]], device='mps:0')\n",
      "Iteration 28370 Training loss 0.1090618371963501 Validation loss 0.1041169986128807 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5682],\n",
      "        [0.3606]], device='mps:0')\n",
      "Iteration 28380 Training loss 0.10843934863805771 Validation loss 0.10411962121725082 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.3371],\n",
      "        [0.4864]], device='mps:0')\n",
      "Iteration 28390 Training loss 0.10112617164850235 Validation loss 0.1041162833571434 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6288],\n",
      "        [0.6621]], device='mps:0')\n",
      "Iteration 28400 Training loss 0.10478958487510681 Validation loss 0.10411176085472107 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6385],\n",
      "        [0.4413]], device='mps:0')\n",
      "Iteration 28410 Training loss 0.10002722591161728 Validation loss 0.10410921275615692 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.2323],\n",
      "        [0.6578]], device='mps:0')\n",
      "Iteration 28420 Training loss 0.11399832367897034 Validation loss 0.10410816967487335 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4065],\n",
      "        [0.3842]], device='mps:0')\n",
      "Iteration 28430 Training loss 0.12176554650068283 Validation loss 0.10410981625318527 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3627],\n",
      "        [0.4656]], device='mps:0')\n",
      "Iteration 28440 Training loss 0.1077442318201065 Validation loss 0.10410556942224503 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5216],\n",
      "        [0.0780]], device='mps:0')\n",
      "Iteration 28450 Training loss 0.10148417204618454 Validation loss 0.10411540418863297 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5649],\n",
      "        [0.7758]], device='mps:0')\n",
      "Iteration 28460 Training loss 0.11197157949209213 Validation loss 0.10410710424184799 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.5846],\n",
      "        [0.4563]], device='mps:0')\n",
      "Iteration 28470 Training loss 0.10674842447042465 Validation loss 0.10411901026964188 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.6482],\n",
      "        [0.5721]], device='mps:0')\n",
      "Iteration 28480 Training loss 0.11052238196134567 Validation loss 0.10410676896572113 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.7022],\n",
      "        [0.5991]], device='mps:0')\n",
      "Iteration 28490 Training loss 0.10757024586200714 Validation loss 0.1041039302945137 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.5789],\n",
      "        [0.6281]], device='mps:0')\n",
      "Iteration 28500 Training loss 0.10640081763267517 Validation loss 0.10409171879291534 Accuracy 0.706000030040741\n",
      "Output tensor([[0.4935],\n",
      "        [0.5881]], device='mps:0')\n",
      "Iteration 28510 Training loss 0.10541299730539322 Validation loss 0.10409878194332123 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.6542],\n",
      "        [0.5117]], device='mps:0')\n",
      "Iteration 28520 Training loss 0.0960410013794899 Validation loss 0.10409535467624664 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.6413],\n",
      "        [0.3000]], device='mps:0')\n",
      "Iteration 28530 Training loss 0.1164492517709732 Validation loss 0.104087695479393 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4861],\n",
      "        [0.5276]], device='mps:0')\n",
      "Iteration 28540 Training loss 0.10002988576889038 Validation loss 0.10409155488014221 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.1959],\n",
      "        [0.6044]], device='mps:0')\n",
      "Iteration 28550 Training loss 0.09791675209999084 Validation loss 0.10409227758646011 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5272],\n",
      "        [0.5185]], device='mps:0')\n",
      "Iteration 28560 Training loss 0.10363783687353134 Validation loss 0.10408233851194382 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5882],\n",
      "        [0.5259]], device='mps:0')\n",
      "Iteration 28570 Training loss 0.09651955217123032 Validation loss 0.10408510267734528 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.7038],\n",
      "        [0.2971]], device='mps:0')\n",
      "Iteration 28580 Training loss 0.10482610762119293 Validation loss 0.10408222675323486 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.5308],\n",
      "        [0.2144]], device='mps:0')\n",
      "Iteration 28590 Training loss 0.09732379019260406 Validation loss 0.104086734354496 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5471],\n",
      "        [0.3488]], device='mps:0')\n",
      "Iteration 28600 Training loss 0.09916228801012039 Validation loss 0.10416640341281891 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4409],\n",
      "        [0.5693]], device='mps:0')\n",
      "Iteration 28610 Training loss 0.10072337836027145 Validation loss 0.10416814684867859 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5136],\n",
      "        [0.5409]], device='mps:0')\n",
      "Iteration 28620 Training loss 0.09390553086996078 Validation loss 0.10414563119411469 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5086],\n",
      "        [0.4830]], device='mps:0')\n",
      "Iteration 28630 Training loss 0.10274005681276321 Validation loss 0.10412903130054474 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.6224],\n",
      "        [0.4903]], device='mps:0')\n",
      "Iteration 28640 Training loss 0.10438009351491928 Validation loss 0.1041196659207344 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4594],\n",
      "        [0.7190]], device='mps:0')\n",
      "Iteration 28650 Training loss 0.1140102967619896 Validation loss 0.1041494607925415 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4645],\n",
      "        [0.6720]], device='mps:0')\n",
      "Iteration 28660 Training loss 0.10142750293016434 Validation loss 0.10416730493307114 Accuracy 0.7005000114440918\n",
      "Output tensor([[0.3861],\n",
      "        [0.5319]], device='mps:0')\n",
      "Iteration 28670 Training loss 0.11262990534305573 Validation loss 0.1041681095957756 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.3257],\n",
      "        [0.3750]], device='mps:0')\n",
      "Iteration 28680 Training loss 0.10079841315746307 Validation loss 0.10413976013660431 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5838],\n",
      "        [0.4677]], device='mps:0')\n",
      "Iteration 28690 Training loss 0.11723209917545319 Validation loss 0.10406126081943512 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4888],\n",
      "        [0.3612]], device='mps:0')\n",
      "Iteration 28700 Training loss 0.10508975386619568 Validation loss 0.10406650602817535 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.3828],\n",
      "        [0.5180]], device='mps:0')\n",
      "Iteration 28710 Training loss 0.11262456327676773 Validation loss 0.10407549142837524 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5533],\n",
      "        [0.3094]], device='mps:0')\n",
      "Iteration 28720 Training loss 0.10285890102386475 Validation loss 0.10407041013240814 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.4692],\n",
      "        [0.4938]], device='mps:0')\n",
      "Iteration 28730 Training loss 0.10488387197256088 Validation loss 0.10405640304088593 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5952],\n",
      "        [0.3797]], device='mps:0')\n",
      "Iteration 28740 Training loss 0.10053232312202454 Validation loss 0.10406549274921417 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.4808],\n",
      "        [0.5787]], device='mps:0')\n",
      "Iteration 28750 Training loss 0.11649191379547119 Validation loss 0.10405145585536957 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5086],\n",
      "        [0.2990]], device='mps:0')\n",
      "Iteration 28760 Training loss 0.1133616715669632 Validation loss 0.10405521094799042 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4318],\n",
      "        [0.1478]], device='mps:0')\n",
      "Iteration 28770 Training loss 0.10586174577474594 Validation loss 0.10406318306922913 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.3964],\n",
      "        [0.4366]], device='mps:0')\n",
      "Iteration 28780 Training loss 0.11517573893070221 Validation loss 0.10404569655656815 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.5002],\n",
      "        [0.6351]], device='mps:0')\n",
      "Iteration 28790 Training loss 0.10819583386182785 Validation loss 0.10404820740222931 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.6852],\n",
      "        [0.6393]], device='mps:0')\n",
      "Iteration 28800 Training loss 0.10394418239593506 Validation loss 0.1040351614356041 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2979],\n",
      "        [0.4677]], device='mps:0')\n",
      "Iteration 28810 Training loss 0.10708127915859222 Validation loss 0.10403169691562653 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.2480],\n",
      "        [0.7342]], device='mps:0')\n",
      "Iteration 28820 Training loss 0.0972195416688919 Validation loss 0.10403187572956085 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3977],\n",
      "        [0.1562]], device='mps:0')\n",
      "Iteration 28830 Training loss 0.10492298007011414 Validation loss 0.10402844846248627 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.2910],\n",
      "        [0.4111]], device='mps:0')\n",
      "Iteration 28840 Training loss 0.09852958470582962 Validation loss 0.10402976721525192 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6857],\n",
      "        [0.4584]], device='mps:0')\n",
      "Iteration 28850 Training loss 0.11063338816165924 Validation loss 0.10404465347528458 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.2691],\n",
      "        [0.1903]], device='mps:0')\n",
      "Iteration 28860 Training loss 0.09545296430587769 Validation loss 0.10406801849603653 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6393],\n",
      "        [0.6586]], device='mps:0')\n",
      "Iteration 28870 Training loss 0.10773124545812607 Validation loss 0.10403437167406082 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.3410],\n",
      "        [0.5332]], device='mps:0')\n",
      "Iteration 28880 Training loss 0.10810546576976776 Validation loss 0.1040334701538086 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.3246],\n",
      "        [0.4765]], device='mps:0')\n",
      "Iteration 28890 Training loss 0.10644172132015228 Validation loss 0.10401906073093414 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4241],\n",
      "        [0.5272]], device='mps:0')\n",
      "Iteration 28900 Training loss 0.1102171316742897 Validation loss 0.10401914268732071 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6718],\n",
      "        [0.4142]], device='mps:0')\n",
      "Iteration 28910 Training loss 0.10828088223934174 Validation loss 0.10402896255254745 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5146],\n",
      "        [0.6401]], device='mps:0')\n",
      "Iteration 28920 Training loss 0.1017681285738945 Validation loss 0.10401128232479095 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5859],\n",
      "        [0.5841]], device='mps:0')\n",
      "Iteration 28930 Training loss 0.11235006153583527 Validation loss 0.10400471836328506 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4382],\n",
      "        [0.6020]], device='mps:0')\n",
      "Iteration 28940 Training loss 0.10876645892858505 Validation loss 0.10401412099599838 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.2397],\n",
      "        [0.4452]], device='mps:0')\n",
      "Iteration 28950 Training loss 0.10807233303785324 Validation loss 0.10400602221488953 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5155],\n",
      "        [0.4406]], device='mps:0')\n",
      "Iteration 28960 Training loss 0.09779699146747589 Validation loss 0.1040022149682045 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.7223],\n",
      "        [0.2284]], device='mps:0')\n",
      "Iteration 28970 Training loss 0.11511076986789703 Validation loss 0.10400655120611191 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5394],\n",
      "        [0.5286]], device='mps:0')\n",
      "Iteration 28980 Training loss 0.11678845435380936 Validation loss 0.10399896651506424 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.2281],\n",
      "        [0.3713]], device='mps:0')\n",
      "Iteration 28990 Training loss 0.10899175703525543 Validation loss 0.10400748252868652 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.6245],\n",
      "        [0.5977]], device='mps:0')\n",
      "Iteration 29000 Training loss 0.10137436538934708 Validation loss 0.10400503873825073 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.4104],\n",
      "        [0.6874]], device='mps:0')\n",
      "Iteration 29010 Training loss 0.09353767335414886 Validation loss 0.10400024801492691 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.3048],\n",
      "        [0.4501]], device='mps:0')\n",
      "Iteration 29020 Training loss 0.12193132191896439 Validation loss 0.10400138795375824 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.6277],\n",
      "        [0.7043]], device='mps:0')\n",
      "Iteration 29030 Training loss 0.10037478059530258 Validation loss 0.10398994386196136 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5911],\n",
      "        [0.3016]], device='mps:0')\n",
      "Iteration 29040 Training loss 0.1058637797832489 Validation loss 0.10398851335048676 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4949],\n",
      "        [0.5286]], device='mps:0')\n",
      "Iteration 29050 Training loss 0.1069706529378891 Validation loss 0.10398745536804199 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4755],\n",
      "        [0.6389]], device='mps:0')\n",
      "Iteration 29060 Training loss 0.10513556003570557 Validation loss 0.10399333387613297 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.3284],\n",
      "        [0.5436]], device='mps:0')\n",
      "Iteration 29070 Training loss 0.10145797580480576 Validation loss 0.10400010645389557 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.6153],\n",
      "        [0.6669]], device='mps:0')\n",
      "Iteration 29080 Training loss 0.100941501557827 Validation loss 0.10398837178945541 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4672],\n",
      "        [0.4436]], device='mps:0')\n",
      "Iteration 29090 Training loss 0.10684871673583984 Validation loss 0.10399681329727173 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4258],\n",
      "        [0.1394]], device='mps:0')\n",
      "Iteration 29100 Training loss 0.10271915048360825 Validation loss 0.10398466885089874 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.2863],\n",
      "        [0.5957]], device='mps:0')\n",
      "Iteration 29110 Training loss 0.09934692084789276 Validation loss 0.1040264368057251 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4326],\n",
      "        [0.7381]], device='mps:0')\n",
      "Iteration 29120 Training loss 0.11143224686384201 Validation loss 0.10397661477327347 Accuracy 0.70250004529953\n",
      "Output tensor([[0.7236],\n",
      "        [0.2234]], device='mps:0')\n",
      "Iteration 29130 Training loss 0.1061290055513382 Validation loss 0.10399097204208374 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5917],\n",
      "        [0.2441]], device='mps:0')\n",
      "Iteration 29140 Training loss 0.10469312220811844 Validation loss 0.10398204624652863 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.6550],\n",
      "        [0.4541]], device='mps:0')\n",
      "Iteration 29150 Training loss 0.09764684736728668 Validation loss 0.10397475212812424 Accuracy 0.70250004529953\n",
      "Output tensor([[0.2844],\n",
      "        [0.5371]], device='mps:0')\n",
      "Iteration 29160 Training loss 0.10459253191947937 Validation loss 0.10396187007427216 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4643],\n",
      "        [0.6529]], device='mps:0')\n",
      "Iteration 29170 Training loss 0.10810341686010361 Validation loss 0.10396190732717514 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2664],\n",
      "        [0.4871]], device='mps:0')\n",
      "Iteration 29180 Training loss 0.10134346038103104 Validation loss 0.10396597534418106 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6266],\n",
      "        [0.3832]], device='mps:0')\n",
      "Iteration 29190 Training loss 0.10886043310165405 Validation loss 0.10395575314760208 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4443],\n",
      "        [0.4990]], device='mps:0')\n",
      "Iteration 29200 Training loss 0.10717260092496872 Validation loss 0.10396399348974228 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4715],\n",
      "        [0.6521]], device='mps:0')\n",
      "Iteration 29210 Training loss 0.10688916593790054 Validation loss 0.10394807904958725 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5209],\n",
      "        [0.5657]], device='mps:0')\n",
      "Iteration 29220 Training loss 0.10281240940093994 Validation loss 0.10394487529993057 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4359],\n",
      "        [0.2766]], device='mps:0')\n",
      "Iteration 29230 Training loss 0.10317874699831009 Validation loss 0.10395314544439316 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5737],\n",
      "        [0.3695]], device='mps:0')\n",
      "Iteration 29240 Training loss 0.10140016674995422 Validation loss 0.1039898544549942 Accuracy 0.703000009059906\n",
      "Output tensor([[0.3428],\n",
      "        [0.4947]], device='mps:0')\n",
      "Iteration 29250 Training loss 0.1116996556520462 Validation loss 0.1039808839559555 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5338],\n",
      "        [0.4647]], device='mps:0')\n",
      "Iteration 29260 Training loss 0.10745807737112045 Validation loss 0.10393846780061722 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5570],\n",
      "        [0.5850]], device='mps:0')\n",
      "Iteration 29270 Training loss 0.11173101514577866 Validation loss 0.10394872725009918 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.3845],\n",
      "        [0.6181]], device='mps:0')\n",
      "Iteration 29280 Training loss 0.10797397047281265 Validation loss 0.10393540561199188 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.6246],\n",
      "        [0.4176]], device='mps:0')\n",
      "Iteration 29290 Training loss 0.11564914137125015 Validation loss 0.10392189770936966 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3762],\n",
      "        [0.4530]], device='mps:0')\n",
      "Iteration 29300 Training loss 0.10863317549228668 Validation loss 0.1039198786020279 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4971],\n",
      "        [0.7197]], device='mps:0')\n",
      "Iteration 29310 Training loss 0.09373427927494049 Validation loss 0.10393845289945602 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5303],\n",
      "        [0.5508]], device='mps:0')\n",
      "Iteration 29320 Training loss 0.10898635536432266 Validation loss 0.10395035892724991 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4894],\n",
      "        [0.1764]], device='mps:0')\n",
      "Iteration 29330 Training loss 0.10974529385566711 Validation loss 0.10401701927185059 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3382],\n",
      "        [0.4495]], device='mps:0')\n",
      "Iteration 29340 Training loss 0.1121249571442604 Validation loss 0.10401603579521179 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5496],\n",
      "        [0.5540]], device='mps:0')\n",
      "Iteration 29350 Training loss 0.09896133095026016 Validation loss 0.1039293110370636 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.3083],\n",
      "        [0.5572]], device='mps:0')\n",
      "Iteration 29360 Training loss 0.11480738967657089 Validation loss 0.10393469035625458 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.6105],\n",
      "        [0.4446]], device='mps:0')\n",
      "Iteration 29370 Training loss 0.10830499231815338 Validation loss 0.10396330803632736 Accuracy 0.70250004529953\n",
      "Output tensor([[0.7915],\n",
      "        [0.5338]], device='mps:0')\n",
      "Iteration 29380 Training loss 0.10477489978075027 Validation loss 0.10396146774291992 Accuracy 0.70250004529953\n",
      "Output tensor([[0.3368],\n",
      "        [0.5327]], device='mps:0')\n",
      "Iteration 29390 Training loss 0.11397799104452133 Validation loss 0.1040056049823761 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.0877],\n",
      "        [0.6444]], device='mps:0')\n",
      "Iteration 29400 Training loss 0.10611081123352051 Validation loss 0.10391250997781754 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.6133],\n",
      "        [0.2962]], device='mps:0')\n",
      "Iteration 29410 Training loss 0.10987438261508942 Validation loss 0.10390833020210266 Accuracy 0.70250004529953\n",
      "Output tensor([[0.3432],\n",
      "        [0.1531]], device='mps:0')\n",
      "Iteration 29420 Training loss 0.1054162010550499 Validation loss 0.10391287505626678 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.3864],\n",
      "        [0.2901]], device='mps:0')\n",
      "Iteration 29430 Training loss 0.11032678186893463 Validation loss 0.10390875488519669 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.6542],\n",
      "        [0.5242]], device='mps:0')\n",
      "Iteration 29440 Training loss 0.10585261881351471 Validation loss 0.10389953851699829 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.2646],\n",
      "        [0.5947]], device='mps:0')\n",
      "Iteration 29450 Training loss 0.1154310554265976 Validation loss 0.10393361747264862 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.4452],\n",
      "        [0.7518]], device='mps:0')\n",
      "Iteration 29460 Training loss 0.10434016585350037 Validation loss 0.10399708151817322 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.3745],\n",
      "        [0.7349]], device='mps:0')\n",
      "Iteration 29470 Training loss 0.09056998789310455 Validation loss 0.1039179116487503 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6191],\n",
      "        [0.5849]], device='mps:0')\n",
      "Iteration 29480 Training loss 0.11682692915201187 Validation loss 0.10393151640892029 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4974],\n",
      "        [0.6299]], device='mps:0')\n",
      "Iteration 29490 Training loss 0.09886518120765686 Validation loss 0.10389011353254318 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5338],\n",
      "        [0.1463]], device='mps:0')\n",
      "Iteration 29500 Training loss 0.11044895648956299 Validation loss 0.10387994349002838 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4250],\n",
      "        [0.4414]], device='mps:0')\n",
      "Iteration 29510 Training loss 0.10012246668338776 Validation loss 0.10387596487998962 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5199],\n",
      "        [0.5104]], device='mps:0')\n",
      "Iteration 29520 Training loss 0.10736709088087082 Validation loss 0.1038801372051239 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4633],\n",
      "        [0.5405]], device='mps:0')\n",
      "Iteration 29530 Training loss 0.10505210608243942 Validation loss 0.10387057811021805 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6987],\n",
      "        [0.2678]], device='mps:0')\n",
      "Iteration 29540 Training loss 0.11179624497890472 Validation loss 0.10386474430561066 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6481],\n",
      "        [0.1999]], device='mps:0')\n",
      "Iteration 29550 Training loss 0.10321304202079773 Validation loss 0.10389795154333115 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6128],\n",
      "        [0.3230]], device='mps:0')\n",
      "Iteration 29560 Training loss 0.10893797874450684 Validation loss 0.10389675945043564 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4242],\n",
      "        [0.4110]], device='mps:0')\n",
      "Iteration 29570 Training loss 0.1140887439250946 Validation loss 0.10389277338981628 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.4855],\n",
      "        [0.3394]], device='mps:0')\n",
      "Iteration 29580 Training loss 0.10514426231384277 Validation loss 0.10391451418399811 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6112],\n",
      "        [0.5415]], device='mps:0')\n",
      "Iteration 29590 Training loss 0.10305457562208176 Validation loss 0.10391510277986526 Accuracy 0.70250004529953\n",
      "Output tensor([[0.3851],\n",
      "        [0.4970]], device='mps:0')\n",
      "Iteration 29600 Training loss 0.0924258753657341 Validation loss 0.10392815619707108 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5138],\n",
      "        [0.4398]], device='mps:0')\n",
      "Iteration 29610 Training loss 0.10835635662078857 Validation loss 0.10390006005764008 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4428],\n",
      "        [0.5894]], device='mps:0')\n",
      "Iteration 29620 Training loss 0.1035887822508812 Validation loss 0.10393288731575012 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5161],\n",
      "        [0.6929]], device='mps:0')\n",
      "Iteration 29630 Training loss 0.10472247004508972 Validation loss 0.10389064997434616 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5992],\n",
      "        [0.4308]], device='mps:0')\n",
      "Iteration 29640 Training loss 0.11168082803487778 Validation loss 0.10390985012054443 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4257],\n",
      "        [0.7346]], device='mps:0')\n",
      "Iteration 29650 Training loss 0.09661445766687393 Validation loss 0.10387483984231949 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5693],\n",
      "        [0.5577]], device='mps:0')\n",
      "Iteration 29660 Training loss 0.09616760909557343 Validation loss 0.1038939580321312 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5817],\n",
      "        [0.6391]], device='mps:0')\n",
      "Iteration 29670 Training loss 0.10157173871994019 Validation loss 0.10388583689928055 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4672],\n",
      "        [0.5769]], device='mps:0')\n",
      "Iteration 29680 Training loss 0.10582532733678818 Validation loss 0.10386253893375397 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5443],\n",
      "        [0.5493]], device='mps:0')\n",
      "Iteration 29690 Training loss 0.10227064788341522 Validation loss 0.10386256128549576 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4745],\n",
      "        [0.3168]], device='mps:0')\n",
      "Iteration 29700 Training loss 0.10868389159440994 Validation loss 0.1038656085729599 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5239],\n",
      "        [0.6071]], device='mps:0')\n",
      "Iteration 29710 Training loss 0.1140516996383667 Validation loss 0.10390176624059677 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5723],\n",
      "        [0.5810]], device='mps:0')\n",
      "Iteration 29720 Training loss 0.10957913845777512 Validation loss 0.10386038571596146 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4732],\n",
      "        [0.5568]], device='mps:0')\n",
      "Iteration 29730 Training loss 0.10855463147163391 Validation loss 0.1038907840847969 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4553],\n",
      "        [0.4304]], device='mps:0')\n",
      "Iteration 29740 Training loss 0.10653521120548248 Validation loss 0.10385990887880325 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4177],\n",
      "        [0.6120]], device='mps:0')\n",
      "Iteration 29750 Training loss 0.10766316950321198 Validation loss 0.10387953370809555 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5378],\n",
      "        [0.5372]], device='mps:0')\n",
      "Iteration 29760 Training loss 0.10939616709947586 Validation loss 0.10391202569007874 Accuracy 0.70250004529953\n",
      "Output tensor([[0.3930],\n",
      "        [0.3768]], device='mps:0')\n",
      "Iteration 29770 Training loss 0.11875800788402557 Validation loss 0.10390205681324005 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5295],\n",
      "        [0.5717]], device='mps:0')\n",
      "Iteration 29780 Training loss 0.1042921245098114 Validation loss 0.10386166721582413 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6410],\n",
      "        [0.5917]], device='mps:0')\n",
      "Iteration 29790 Training loss 0.10635451227426529 Validation loss 0.10390350222587585 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5534],\n",
      "        [0.5114]], device='mps:0')\n",
      "Iteration 29800 Training loss 0.10629455000162125 Validation loss 0.1038442924618721 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5646],\n",
      "        [0.4817]], device='mps:0')\n",
      "Iteration 29810 Training loss 0.10965024679899216 Validation loss 0.1038241758942604 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.3234],\n",
      "        [0.4881]], device='mps:0')\n",
      "Iteration 29820 Training loss 0.09745429456233978 Validation loss 0.10385129600763321 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6348],\n",
      "        [0.4846]], device='mps:0')\n",
      "Iteration 29830 Training loss 0.11054348945617676 Validation loss 0.10386555641889572 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5608],\n",
      "        [0.5356]], device='mps:0')\n",
      "Iteration 29840 Training loss 0.1092076450586319 Validation loss 0.10387539863586426 Accuracy 0.703000009059906\n",
      "Output tensor([[0.1524],\n",
      "        [0.4158]], device='mps:0')\n",
      "Iteration 29850 Training loss 0.10347683727741241 Validation loss 0.10398665070533752 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.4024],\n",
      "        [0.3565]], device='mps:0')\n",
      "Iteration 29860 Training loss 0.10359031707048416 Validation loss 0.10404159128665924 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.2179],\n",
      "        [0.5500]], device='mps:0')\n",
      "Iteration 29870 Training loss 0.11141280829906464 Validation loss 0.1040988340973854 Accuracy 0.6975000500679016\n",
      "Output tensor([[0.3889],\n",
      "        [0.6748]], device='mps:0')\n",
      "Iteration 29880 Training loss 0.10891595482826233 Validation loss 0.10398728400468826 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.5068],\n",
      "        [0.5905]], device='mps:0')\n",
      "Iteration 29890 Training loss 0.1053692102432251 Validation loss 0.10405774414539337 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.4807],\n",
      "        [0.4135]], device='mps:0')\n",
      "Iteration 29900 Training loss 0.1219036802649498 Validation loss 0.10406462848186493 Accuracy 0.6985000371932983\n",
      "Output tensor([[0.6228],\n",
      "        [0.3351]], device='mps:0')\n",
      "Iteration 29910 Training loss 0.1093352660536766 Validation loss 0.10401450097560883 Accuracy 0.6990000605583191\n",
      "Output tensor([[0.4296],\n",
      "        [0.6214]], device='mps:0')\n",
      "Iteration 29920 Training loss 0.09678474068641663 Validation loss 0.10394179821014404 Accuracy 0.7010000348091125\n",
      "Output tensor([[0.2519],\n",
      "        [0.4844]], device='mps:0')\n",
      "Iteration 29930 Training loss 0.1001020148396492 Validation loss 0.10399512201547623 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.6370],\n",
      "        [0.3569]], device='mps:0')\n",
      "Iteration 29940 Training loss 0.1039653941988945 Validation loss 0.10393427312374115 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.3441],\n",
      "        [0.5636]], device='mps:0')\n",
      "Iteration 29950 Training loss 0.1073174849152565 Validation loss 0.10398300737142563 Accuracy 0.7000000476837158\n",
      "Output tensor([[0.6105],\n",
      "        [0.4070]], device='mps:0')\n",
      "Iteration 29960 Training loss 0.12636050581932068 Validation loss 0.10388054698705673 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4086],\n",
      "        [0.1087]], device='mps:0')\n",
      "Iteration 29970 Training loss 0.1147802546620369 Validation loss 0.10382707417011261 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.6835],\n",
      "        [0.5683]], device='mps:0')\n",
      "Iteration 29980 Training loss 0.11825187504291534 Validation loss 0.10381926596164703 Accuracy 0.703000009059906\n",
      "Output tensor([[0.1277],\n",
      "        [0.5076]], device='mps:0')\n",
      "Iteration 29990 Training loss 0.10343419760465622 Validation loss 0.10385217517614365 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5316],\n",
      "        [0.5014]], device='mps:0')\n",
      "Iteration 30000 Training loss 0.11007583141326904 Validation loss 0.10383038222789764 Accuracy 0.70250004529953\n",
      "Output tensor([[0.3171],\n",
      "        [0.6850]], device='mps:0')\n",
      "Iteration 30010 Training loss 0.10576493293046951 Validation loss 0.10381205379962921 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5350],\n",
      "        [0.4644]], device='mps:0')\n",
      "Iteration 30020 Training loss 0.11039350926876068 Validation loss 0.10377049446105957 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4909],\n",
      "        [0.7280]], device='mps:0')\n",
      "Iteration 30030 Training loss 0.09862974286079407 Validation loss 0.10377248376607895 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4908],\n",
      "        [0.6055]], device='mps:0')\n",
      "Iteration 30040 Training loss 0.10849355161190033 Validation loss 0.10378246754407883 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5813],\n",
      "        [0.7153]], device='mps:0')\n",
      "Iteration 30050 Training loss 0.10132024437189102 Validation loss 0.10381215065717697 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5532],\n",
      "        [0.4125]], device='mps:0')\n",
      "Iteration 30060 Training loss 0.10656329989433289 Validation loss 0.10382658988237381 Accuracy 0.703000009059906\n",
      "Output tensor([[0.7104],\n",
      "        [0.6837]], device='mps:0')\n",
      "Iteration 30070 Training loss 0.1044725701212883 Validation loss 0.10383152961730957 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6858],\n",
      "        [0.3994]], device='mps:0')\n",
      "Iteration 30080 Training loss 0.1073702946305275 Validation loss 0.10383229702711105 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5683],\n",
      "        [0.2698]], device='mps:0')\n",
      "Iteration 30090 Training loss 0.11526090651750565 Validation loss 0.1038653776049614 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5512],\n",
      "        [0.5934]], device='mps:0')\n",
      "Iteration 30100 Training loss 0.1054171696305275 Validation loss 0.10385192930698395 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.2454],\n",
      "        [0.6440]], device='mps:0')\n",
      "Iteration 30110 Training loss 0.1164170429110527 Validation loss 0.10378380864858627 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6479],\n",
      "        [0.5903]], device='mps:0')\n",
      "Iteration 30120 Training loss 0.11502178013324738 Validation loss 0.10379421710968018 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6698],\n",
      "        [0.3772]], device='mps:0')\n",
      "Iteration 30130 Training loss 0.11111045628786087 Validation loss 0.10376115888357162 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5263],\n",
      "        [0.5175]], device='mps:0')\n",
      "Iteration 30140 Training loss 0.10437551885843277 Validation loss 0.1037435531616211 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.6172],\n",
      "        [0.6072]], device='mps:0')\n",
      "Iteration 30150 Training loss 0.10406487435102463 Validation loss 0.10374138504266739 Accuracy 0.703000009059906\n",
      "Output tensor([[0.3219],\n",
      "        [0.3140]], device='mps:0')\n",
      "Iteration 30160 Training loss 0.11802864074707031 Validation loss 0.10373642295598984 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.6753],\n",
      "        [0.5162]], device='mps:0')\n",
      "Iteration 30170 Training loss 0.09907985478639603 Validation loss 0.10376277565956116 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.6403],\n",
      "        [0.2602]], device='mps:0')\n",
      "Iteration 30180 Training loss 0.1120971068739891 Validation loss 0.1037585437297821 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.6179],\n",
      "        [0.6485]], device='mps:0')\n",
      "Iteration 30190 Training loss 0.09012627601623535 Validation loss 0.10373416543006897 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5516],\n",
      "        [0.4108]], device='mps:0')\n",
      "Iteration 30200 Training loss 0.10889960825443268 Validation loss 0.10372841358184814 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5731],\n",
      "        [0.4927]], device='mps:0')\n",
      "Iteration 30210 Training loss 0.10462421178817749 Validation loss 0.10372638702392578 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3473],\n",
      "        [0.4330]], device='mps:0')\n",
      "Iteration 30220 Training loss 0.09324391931295395 Validation loss 0.1037277951836586 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6140],\n",
      "        [0.6558]], device='mps:0')\n",
      "Iteration 30230 Training loss 0.10257472842931747 Validation loss 0.10372543334960938 Accuracy 0.70250004529953\n",
      "Output tensor([[0.1718],\n",
      "        [0.5385]], device='mps:0')\n",
      "Iteration 30240 Training loss 0.11202540993690491 Validation loss 0.10372481495141983 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.6112],\n",
      "        [0.5938]], device='mps:0')\n",
      "Iteration 30250 Training loss 0.10945845395326614 Validation loss 0.10372819006443024 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4792],\n",
      "        [0.4045]], device='mps:0')\n",
      "Iteration 30260 Training loss 0.10089561343193054 Validation loss 0.1037239208817482 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.3590],\n",
      "        [0.3119]], device='mps:0')\n",
      "Iteration 30270 Training loss 0.09870898723602295 Validation loss 0.10371551662683487 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4592],\n",
      "        [0.4101]], device='mps:0')\n",
      "Iteration 30280 Training loss 0.10490470379590988 Validation loss 0.10370903462171555 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6820],\n",
      "        [0.4047]], device='mps:0')\n",
      "Iteration 30290 Training loss 0.09578340500593185 Validation loss 0.10370949655771255 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2811],\n",
      "        [0.5163]], device='mps:0')\n",
      "Iteration 30300 Training loss 0.10140112787485123 Validation loss 0.10371603071689606 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.5944],\n",
      "        [0.4535]], device='mps:0')\n",
      "Iteration 30310 Training loss 0.11016921699047089 Validation loss 0.10370421409606934 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5063],\n",
      "        [0.5242]], device='mps:0')\n",
      "Iteration 30320 Training loss 0.09357790648937225 Validation loss 0.10369563102722168 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6358],\n",
      "        [0.2997]], device='mps:0')\n",
      "Iteration 30330 Training loss 0.10699823498725891 Validation loss 0.103694386780262 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6293],\n",
      "        [0.2950]], device='mps:0')\n",
      "Iteration 30340 Training loss 0.10976697504520416 Validation loss 0.10369300842285156 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4304],\n",
      "        [0.5787]], device='mps:0')\n",
      "Iteration 30350 Training loss 0.10970937460660934 Validation loss 0.10371436178684235 Accuracy 0.703000009059906\n",
      "Output tensor([[0.3370],\n",
      "        [0.5692]], device='mps:0')\n",
      "Iteration 30360 Training loss 0.09771491587162018 Validation loss 0.1037534549832344 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.3424],\n",
      "        [0.7226]], device='mps:0')\n",
      "Iteration 30370 Training loss 0.11004169285297394 Validation loss 0.10371068865060806 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4671],\n",
      "        [0.6182]], device='mps:0')\n",
      "Iteration 30380 Training loss 0.10775788873434067 Validation loss 0.10371022671461105 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.4063],\n",
      "        [0.4725]], device='mps:0')\n",
      "Iteration 30390 Training loss 0.10435369610786438 Validation loss 0.10370299965143204 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5591],\n",
      "        [0.4180]], device='mps:0')\n",
      "Iteration 30400 Training loss 0.11038514971733093 Validation loss 0.10371339321136475 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5247],\n",
      "        [0.6287]], device='mps:0')\n",
      "Iteration 30410 Training loss 0.10698813945055008 Validation loss 0.10370533913373947 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4509],\n",
      "        [0.5463]], device='mps:0')\n",
      "Iteration 30420 Training loss 0.09895975142717361 Validation loss 0.10367760807275772 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4729],\n",
      "        [0.6156]], device='mps:0')\n",
      "Iteration 30430 Training loss 0.10276918113231659 Validation loss 0.10367272794246674 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3315],\n",
      "        [0.4381]], device='mps:0')\n",
      "Iteration 30440 Training loss 0.10235556960105896 Validation loss 0.1036721020936966 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5603],\n",
      "        [0.3953]], device='mps:0')\n",
      "Iteration 30450 Training loss 0.10308045148849487 Validation loss 0.10366646200418472 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3503],\n",
      "        [0.5230]], device='mps:0')\n",
      "Iteration 30460 Training loss 0.10757995396852493 Validation loss 0.10369212180376053 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.7246],\n",
      "        [0.6676]], device='mps:0')\n",
      "Iteration 30470 Training loss 0.10027512907981873 Validation loss 0.10370947420597076 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4310],\n",
      "        [0.2083]], device='mps:0')\n",
      "Iteration 30480 Training loss 0.09966010600328445 Validation loss 0.10374093800783157 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4524],\n",
      "        [0.4167]], device='mps:0')\n",
      "Iteration 30490 Training loss 0.10684975981712341 Validation loss 0.10368721932172775 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6395],\n",
      "        [0.4894]], device='mps:0')\n",
      "Iteration 30500 Training loss 0.10732300579547882 Validation loss 0.1036887839436531 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5150],\n",
      "        [0.5660]], device='mps:0')\n",
      "Iteration 30510 Training loss 0.10802824050188065 Validation loss 0.103708915412426 Accuracy 0.703000009059906\n",
      "Output tensor([[0.2817],\n",
      "        [0.2109]], device='mps:0')\n",
      "Iteration 30520 Training loss 0.10048635303974152 Validation loss 0.1036970391869545 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.2227],\n",
      "        [0.5868]], device='mps:0')\n",
      "Iteration 30530 Training loss 0.10095110535621643 Validation loss 0.10369685292243958 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5662],\n",
      "        [0.6987]], device='mps:0')\n",
      "Iteration 30540 Training loss 0.09902389347553253 Validation loss 0.10369870811700821 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.2421],\n",
      "        [0.2021]], device='mps:0')\n",
      "Iteration 30550 Training loss 0.10974103957414627 Validation loss 0.10366366803646088 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6689],\n",
      "        [0.5671]], device='mps:0')\n",
      "Iteration 30560 Training loss 0.10370946675539017 Validation loss 0.1036737784743309 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4307],\n",
      "        [0.3815]], device='mps:0')\n",
      "Iteration 30570 Training loss 0.10195507109165192 Validation loss 0.10366520285606384 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6376],\n",
      "        [0.5696]], device='mps:0')\n",
      "Iteration 30580 Training loss 0.11929132789373398 Validation loss 0.10367407649755478 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5635],\n",
      "        [0.5872]], device='mps:0')\n",
      "Iteration 30590 Training loss 0.10414263606071472 Validation loss 0.10368809849023819 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5494],\n",
      "        [0.5732]], device='mps:0')\n",
      "Iteration 30600 Training loss 0.11641759425401688 Validation loss 0.10366258770227432 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5771],\n",
      "        [0.5103]], device='mps:0')\n",
      "Iteration 30610 Training loss 0.11429613083600998 Validation loss 0.10364687442779541 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5910],\n",
      "        [0.1349]], device='mps:0')\n",
      "Iteration 30620 Training loss 0.11134780943393707 Validation loss 0.10363703221082687 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6541],\n",
      "        [0.3652]], device='mps:0')\n",
      "Iteration 30630 Training loss 0.09646457433700562 Validation loss 0.10364176332950592 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6019],\n",
      "        [0.5597]], device='mps:0')\n",
      "Iteration 30640 Training loss 0.10439357161521912 Validation loss 0.1036347821354866 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6049],\n",
      "        [0.2954]], device='mps:0')\n",
      "Iteration 30650 Training loss 0.10319019109010696 Validation loss 0.10365088284015656 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3662],\n",
      "        [0.6922]], device='mps:0')\n",
      "Iteration 30660 Training loss 0.1047004759311676 Validation loss 0.10365546494722366 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5362],\n",
      "        [0.5261]], device='mps:0')\n",
      "Iteration 30670 Training loss 0.11174319684505463 Validation loss 0.10367804765701294 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4520],\n",
      "        [0.5499]], device='mps:0')\n",
      "Iteration 30680 Training loss 0.1065518707036972 Validation loss 0.10365574806928635 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.7462],\n",
      "        [0.5303]], device='mps:0')\n",
      "Iteration 30690 Training loss 0.1051478385925293 Validation loss 0.10363895446062088 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5455],\n",
      "        [0.5903]], device='mps:0')\n",
      "Iteration 30700 Training loss 0.10821717977523804 Validation loss 0.10365153849124908 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3264],\n",
      "        [0.5175]], device='mps:0')\n",
      "Iteration 30710 Training loss 0.10935372859239578 Validation loss 0.10364536195993423 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.7234],\n",
      "        [0.6818]], device='mps:0')\n",
      "Iteration 30720 Training loss 0.09848929941654205 Validation loss 0.10363677144050598 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.2912],\n",
      "        [0.6886]], device='mps:0')\n",
      "Iteration 30730 Training loss 0.10125896334648132 Validation loss 0.1036267951130867 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5469],\n",
      "        [0.6060]], device='mps:0')\n",
      "Iteration 30740 Training loss 0.10208562016487122 Validation loss 0.1036258265376091 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.7226],\n",
      "        [0.5490]], device='mps:0')\n",
      "Iteration 30750 Training loss 0.10450821369886398 Validation loss 0.1036318838596344 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5142],\n",
      "        [0.6426]], device='mps:0')\n",
      "Iteration 30760 Training loss 0.09268102794885635 Validation loss 0.10363467782735825 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5787],\n",
      "        [0.4666]], device='mps:0')\n",
      "Iteration 30770 Training loss 0.10647543519735336 Validation loss 0.10363438725471497 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.2014],\n",
      "        [0.5882]], device='mps:0')\n",
      "Iteration 30780 Training loss 0.10977649688720703 Validation loss 0.10364099591970444 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.7090],\n",
      "        [0.6964]], device='mps:0')\n",
      "Iteration 30790 Training loss 0.10090144723653793 Validation loss 0.10362689942121506 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4758],\n",
      "        [0.4404]], device='mps:0')\n",
      "Iteration 30800 Training loss 0.10999244451522827 Validation loss 0.10361326485872269 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.2979],\n",
      "        [0.3929]], device='mps:0')\n",
      "Iteration 30810 Training loss 0.09848193824291229 Validation loss 0.10359938442707062 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5859],\n",
      "        [0.5485]], device='mps:0')\n",
      "Iteration 30820 Training loss 0.09925319254398346 Validation loss 0.10359807312488556 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5153],\n",
      "        [0.6188]], device='mps:0')\n",
      "Iteration 30830 Training loss 0.11102425307035446 Validation loss 0.10359682887792587 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3685],\n",
      "        [0.6450]], device='mps:0')\n",
      "Iteration 30840 Training loss 0.11095409095287323 Validation loss 0.10359527915716171 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5297],\n",
      "        [0.6104]], device='mps:0')\n",
      "Iteration 30850 Training loss 0.10116590559482574 Validation loss 0.10359739512205124 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4068],\n",
      "        [0.3161]], device='mps:0')\n",
      "Iteration 30860 Training loss 0.11127673834562302 Validation loss 0.10359641164541245 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5657],\n",
      "        [0.4739]], device='mps:0')\n",
      "Iteration 30870 Training loss 0.1126452386379242 Validation loss 0.10358662903308868 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3904],\n",
      "        [0.4955]], device='mps:0')\n",
      "Iteration 30880 Training loss 0.10472911596298218 Validation loss 0.10358349978923798 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6026],\n",
      "        [0.5209]], device='mps:0')\n",
      "Iteration 30890 Training loss 0.10044635832309723 Validation loss 0.10358338057994843 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6910],\n",
      "        [0.5641]], device='mps:0')\n",
      "Iteration 30900 Training loss 0.10497789829969406 Validation loss 0.10358293354511261 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.6932],\n",
      "        [0.5921]], device='mps:0')\n",
      "Iteration 30910 Training loss 0.09891265630722046 Validation loss 0.10358354449272156 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.7142],\n",
      "        [0.5526]], device='mps:0')\n",
      "Iteration 30920 Training loss 0.10769528895616531 Validation loss 0.10357707738876343 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.6377],\n",
      "        [0.0980]], device='mps:0')\n",
      "Iteration 30930 Training loss 0.10926772654056549 Validation loss 0.10358304530382156 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5714],\n",
      "        [0.3736]], device='mps:0')\n",
      "Iteration 30940 Training loss 0.10393815487623215 Validation loss 0.10358434170484543 Accuracy 0.7015000581741333\n",
      "Output tensor([[0.5459],\n",
      "        [0.4633]], device='mps:0')\n",
      "Iteration 30950 Training loss 0.11213184893131256 Validation loss 0.10359475761651993 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.2983],\n",
      "        [0.6605]], device='mps:0')\n",
      "Iteration 30960 Training loss 0.09882799535989761 Validation loss 0.10357426851987839 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5306],\n",
      "        [0.5341]], device='mps:0')\n",
      "Iteration 30970 Training loss 0.11081472039222717 Validation loss 0.1035652682185173 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5158],\n",
      "        [0.1976]], device='mps:0')\n",
      "Iteration 30980 Training loss 0.10107167810201645 Validation loss 0.10356355458498001 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4690],\n",
      "        [0.5211]], device='mps:0')\n",
      "Iteration 30990 Training loss 0.1087331473827362 Validation loss 0.10357261449098587 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4398],\n",
      "        [0.4600]], device='mps:0')\n",
      "Iteration 31000 Training loss 0.10484795272350311 Validation loss 0.1035856306552887 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6328],\n",
      "        [0.4558]], device='mps:0')\n",
      "Iteration 31010 Training loss 0.10858196020126343 Validation loss 0.10359520465135574 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6069],\n",
      "        [0.3565]], device='mps:0')\n",
      "Iteration 31020 Training loss 0.10177205502986908 Validation loss 0.10362791270017624 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.4151],\n",
      "        [0.6033]], device='mps:0')\n",
      "Iteration 31030 Training loss 0.10211454331874847 Validation loss 0.10359612107276917 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.1101],\n",
      "        [0.2699]], device='mps:0')\n",
      "Iteration 31040 Training loss 0.09154834598302841 Validation loss 0.10355877876281738 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.6682],\n",
      "        [0.4240]], device='mps:0')\n",
      "Iteration 31050 Training loss 0.1084679663181305 Validation loss 0.10358922183513641 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.3616],\n",
      "        [0.5218]], device='mps:0')\n",
      "Iteration 31060 Training loss 0.10501125454902649 Validation loss 0.10357468575239182 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5844],\n",
      "        [0.5799]], device='mps:0')\n",
      "Iteration 31070 Training loss 0.10392169654369354 Validation loss 0.10357179492712021 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6179],\n",
      "        [0.6334]], device='mps:0')\n",
      "Iteration 31080 Training loss 0.10413768887519836 Validation loss 0.10361107438802719 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6371],\n",
      "        [0.4927]], device='mps:0')\n",
      "Iteration 31090 Training loss 0.11153420060873032 Validation loss 0.10358475148677826 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.8204],\n",
      "        [0.2302]], device='mps:0')\n",
      "Iteration 31100 Training loss 0.10469593107700348 Validation loss 0.1035698726773262 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5897],\n",
      "        [0.5073]], device='mps:0')\n",
      "Iteration 31110 Training loss 0.10544893145561218 Validation loss 0.10359199345111847 Accuracy 0.703000009059906\n",
      "Output tensor([[0.1640],\n",
      "        [0.5896]], device='mps:0')\n",
      "Iteration 31120 Training loss 0.10397595912218094 Validation loss 0.10359810292720795 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.4988],\n",
      "        [0.1194]], device='mps:0')\n",
      "Iteration 31130 Training loss 0.10061813145875931 Validation loss 0.10361599922180176 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6667],\n",
      "        [0.5773]], device='mps:0')\n",
      "Iteration 31140 Training loss 0.09375251829624176 Validation loss 0.10365492105484009 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.6086],\n",
      "        [0.5054]], device='mps:0')\n",
      "Iteration 31150 Training loss 0.09287193417549133 Validation loss 0.10364288836717606 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6675],\n",
      "        [0.5327]], device='mps:0')\n",
      "Iteration 31160 Training loss 0.11045188456773758 Validation loss 0.10361168533563614 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4534],\n",
      "        [0.2929]], device='mps:0')\n",
      "Iteration 31170 Training loss 0.10052736103534698 Validation loss 0.10364342480897903 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.2874],\n",
      "        [0.3525]], device='mps:0')\n",
      "Iteration 31180 Training loss 0.10379049181938171 Validation loss 0.10358286648988724 Accuracy 0.703000009059906\n",
      "Output tensor([[0.2406],\n",
      "        [0.7541]], device='mps:0')\n",
      "Iteration 31190 Training loss 0.0965350866317749 Validation loss 0.10358411073684692 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.4732],\n",
      "        [0.6903]], device='mps:0')\n",
      "Iteration 31200 Training loss 0.10223014652729034 Validation loss 0.10358894616365433 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.6620],\n",
      "        [0.1172]], device='mps:0')\n",
      "Iteration 31210 Training loss 0.0951726883649826 Validation loss 0.10362563282251358 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6309],\n",
      "        [0.4327]], device='mps:0')\n",
      "Iteration 31220 Training loss 0.1127680093050003 Validation loss 0.10359178483486176 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4586],\n",
      "        [0.6281]], device='mps:0')\n",
      "Iteration 31230 Training loss 0.11362645775079727 Validation loss 0.10362537205219269 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.6119],\n",
      "        [0.5552]], device='mps:0')\n",
      "Iteration 31240 Training loss 0.10363501310348511 Validation loss 0.10371001809835434 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6237],\n",
      "        [0.6964]], device='mps:0')\n",
      "Iteration 31250 Training loss 0.1003030389547348 Validation loss 0.10364040732383728 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5414],\n",
      "        [0.5506]], device='mps:0')\n",
      "Iteration 31260 Training loss 0.10478869825601578 Validation loss 0.10365906357765198 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4853],\n",
      "        [0.4996]], device='mps:0')\n",
      "Iteration 31270 Training loss 0.11019787192344666 Validation loss 0.10362231731414795 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.7187],\n",
      "        [0.5405]], device='mps:0')\n",
      "Iteration 31280 Training loss 0.10441295057535172 Validation loss 0.10373259335756302 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6790],\n",
      "        [0.1472]], device='mps:0')\n",
      "Iteration 31290 Training loss 0.10476499050855637 Validation loss 0.10365607589483261 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4274],\n",
      "        [0.6077]], device='mps:0')\n",
      "Iteration 31300 Training loss 0.10703221708536148 Validation loss 0.10362020134925842 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5877],\n",
      "        [0.4713]], device='mps:0')\n",
      "Iteration 31310 Training loss 0.09695275872945786 Validation loss 0.10357353836297989 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5378],\n",
      "        [0.4272]], device='mps:0')\n",
      "Iteration 31320 Training loss 0.10142074525356293 Validation loss 0.10357802361249924 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5908],\n",
      "        [0.1692]], device='mps:0')\n",
      "Iteration 31330 Training loss 0.11680488288402557 Validation loss 0.10351604223251343 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6218],\n",
      "        [0.4131]], device='mps:0')\n",
      "Iteration 31340 Training loss 0.09681247174739838 Validation loss 0.10349655151367188 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6382],\n",
      "        [0.6348]], device='mps:0')\n",
      "Iteration 31350 Training loss 0.10450330376625061 Validation loss 0.10349426418542862 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5596],\n",
      "        [0.2917]], device='mps:0')\n",
      "Iteration 31360 Training loss 0.10199407488107681 Validation loss 0.10349040478467941 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4037],\n",
      "        [0.5632]], device='mps:0')\n",
      "Iteration 31370 Training loss 0.11351341754198074 Validation loss 0.10348734259605408 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5077],\n",
      "        [0.5366]], device='mps:0')\n",
      "Iteration 31380 Training loss 0.10483060777187347 Validation loss 0.10348523408174515 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.3233],\n",
      "        [0.5194]], device='mps:0')\n",
      "Iteration 31390 Training loss 0.10126214474439621 Validation loss 0.10348572582006454 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.4373],\n",
      "        [0.5479]], device='mps:0')\n",
      "Iteration 31400 Training loss 0.11427433043718338 Validation loss 0.10348154604434967 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5083],\n",
      "        [0.5589]], device='mps:0')\n",
      "Iteration 31410 Training loss 0.10560008138418198 Validation loss 0.10347867757081985 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6435],\n",
      "        [0.2796]], device='mps:0')\n",
      "Iteration 31420 Training loss 0.10266443341970444 Validation loss 0.10347903519868851 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6569],\n",
      "        [0.5736]], device='mps:0')\n",
      "Iteration 31430 Training loss 0.09899794310331345 Validation loss 0.1034766435623169 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6738],\n",
      "        [0.5662]], device='mps:0')\n",
      "Iteration 31440 Training loss 0.10382002592086792 Validation loss 0.1034756526350975 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.4544],\n",
      "        [0.5267]], device='mps:0')\n",
      "Iteration 31450 Training loss 0.11519202589988708 Validation loss 0.10348153859376907 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5429],\n",
      "        [0.6136]], device='mps:0')\n",
      "Iteration 31460 Training loss 0.11163643002510071 Validation loss 0.10349510610103607 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5430],\n",
      "        [0.6106]], device='mps:0')\n",
      "Iteration 31470 Training loss 0.11733175069093704 Validation loss 0.10347320884466171 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6701],\n",
      "        [0.6362]], device='mps:0')\n",
      "Iteration 31480 Training loss 0.09937924146652222 Validation loss 0.1034775823354721 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.6626],\n",
      "        [0.4374]], device='mps:0')\n",
      "Iteration 31490 Training loss 0.10366617888212204 Validation loss 0.10346516221761703 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4858],\n",
      "        [0.5760]], device='mps:0')\n",
      "Iteration 31500 Training loss 0.10032273828983307 Validation loss 0.10346461832523346 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5488],\n",
      "        [0.6395]], device='mps:0')\n",
      "Iteration 31510 Training loss 0.10373804718255997 Validation loss 0.10347262769937515 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5626],\n",
      "        [0.4468]], device='mps:0')\n",
      "Iteration 31520 Training loss 0.11616303771734238 Validation loss 0.10345561802387238 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.7023],\n",
      "        [0.5268]], device='mps:0')\n",
      "Iteration 31530 Training loss 0.09781458973884583 Validation loss 0.10345461964607239 Accuracy 0.706000030040741\n",
      "Output tensor([[0.6906],\n",
      "        [0.5472]], device='mps:0')\n",
      "Iteration 31540 Training loss 0.09854626655578613 Validation loss 0.10345172137022018 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5800],\n",
      "        [0.6797]], device='mps:0')\n",
      "Iteration 31550 Training loss 0.10275082290172577 Validation loss 0.10344944894313812 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5688],\n",
      "        [0.3381]], device='mps:0')\n",
      "Iteration 31560 Training loss 0.10873754322528839 Validation loss 0.10344819724559784 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6857],\n",
      "        [0.1844]], device='mps:0')\n",
      "Iteration 31570 Training loss 0.10862228274345398 Validation loss 0.10344847291707993 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.5329],\n",
      "        [0.5376]], device='mps:0')\n",
      "Iteration 31580 Training loss 0.10401235520839691 Validation loss 0.10345228016376495 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5179],\n",
      "        [0.6550]], device='mps:0')\n",
      "Iteration 31590 Training loss 0.11181703209877014 Validation loss 0.10344275087118149 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.4875],\n",
      "        [0.5702]], device='mps:0')\n",
      "Iteration 31600 Training loss 0.10594335198402405 Validation loss 0.10345862060785294 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3027],\n",
      "        [0.5289]], device='mps:0')\n",
      "Iteration 31610 Training loss 0.10080007463693619 Validation loss 0.10347328335046768 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4011],\n",
      "        [0.4134]], device='mps:0')\n",
      "Iteration 31620 Training loss 0.107692189514637 Validation loss 0.10347404330968857 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5023],\n",
      "        [0.5832]], device='mps:0')\n",
      "Iteration 31630 Training loss 0.09358979761600494 Validation loss 0.10343805700540543 Accuracy 0.703000009059906\n",
      "Output tensor([[0.0982],\n",
      "        [0.6625]], device='mps:0')\n",
      "Iteration 31640 Training loss 0.11211556941270828 Validation loss 0.10343405604362488 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3538],\n",
      "        [0.6757]], device='mps:0')\n",
      "Iteration 31650 Training loss 0.10557838529348373 Validation loss 0.10343816131353378 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6220],\n",
      "        [0.3462]], device='mps:0')\n",
      "Iteration 31660 Training loss 0.11056554317474365 Validation loss 0.10343685746192932 Accuracy 0.70250004529953\n",
      "Output tensor([[0.1884],\n",
      "        [0.3755]], device='mps:0')\n",
      "Iteration 31670 Training loss 0.09779761731624603 Validation loss 0.10343476384878159 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5980],\n",
      "        [0.4520]], device='mps:0')\n",
      "Iteration 31680 Training loss 0.11599238216876984 Validation loss 0.10346981137990952 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4731],\n",
      "        [0.6509]], device='mps:0')\n",
      "Iteration 31690 Training loss 0.10249511152505875 Validation loss 0.1034712865948677 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5353],\n",
      "        [0.5918]], device='mps:0')\n",
      "Iteration 31700 Training loss 0.09701739996671677 Validation loss 0.10345735400915146 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2803],\n",
      "        [0.5316]], device='mps:0')\n",
      "Iteration 31710 Training loss 0.10123277455568314 Validation loss 0.10346553474664688 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5899],\n",
      "        [0.5176]], device='mps:0')\n",
      "Iteration 31720 Training loss 0.11182105541229248 Validation loss 0.10345640778541565 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6965],\n",
      "        [0.5539]], device='mps:0')\n",
      "Iteration 31730 Training loss 0.10742179304361343 Validation loss 0.10347651690244675 Accuracy 0.70250004529953\n",
      "Output tensor([[0.3134],\n",
      "        [0.3817]], device='mps:0')\n",
      "Iteration 31740 Training loss 0.10949820280075073 Validation loss 0.10342928022146225 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5912],\n",
      "        [0.6154]], device='mps:0')\n",
      "Iteration 31750 Training loss 0.10419824719429016 Validation loss 0.10343338549137115 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2123],\n",
      "        [0.5541]], device='mps:0')\n",
      "Iteration 31760 Training loss 0.10124970972537994 Validation loss 0.10342984646558762 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6626],\n",
      "        [0.3849]], device='mps:0')\n",
      "Iteration 31770 Training loss 0.10958976298570633 Validation loss 0.1034315973520279 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.3869],\n",
      "        [0.5996]], device='mps:0')\n",
      "Iteration 31780 Training loss 0.10383334010839462 Validation loss 0.1034204289317131 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6756],\n",
      "        [0.5814]], device='mps:0')\n",
      "Iteration 31790 Training loss 0.11629651486873627 Validation loss 0.10343965142965317 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5844],\n",
      "        [0.3372]], device='mps:0')\n",
      "Iteration 31800 Training loss 0.10332739353179932 Validation loss 0.10344905406236649 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4652],\n",
      "        [0.3925]], device='mps:0')\n",
      "Iteration 31810 Training loss 0.10252365469932556 Validation loss 0.10344778746366501 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3589],\n",
      "        [0.7554]], device='mps:0')\n",
      "Iteration 31820 Training loss 0.10342521220445633 Validation loss 0.10346871614456177 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.3346],\n",
      "        [0.6077]], device='mps:0')\n",
      "Iteration 31830 Training loss 0.09813638776540756 Validation loss 0.10346683114767075 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.5436],\n",
      "        [0.4802]], device='mps:0')\n",
      "Iteration 31840 Training loss 0.0986189991235733 Validation loss 0.10347431153059006 Accuracy 0.703000009059906\n",
      "Output tensor([[0.7144],\n",
      "        [0.5369]], device='mps:0')\n",
      "Iteration 31850 Training loss 0.11847211420536041 Validation loss 0.10345568507909775 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.3572],\n",
      "        [0.3196]], device='mps:0')\n",
      "Iteration 31860 Training loss 0.10820172727108002 Validation loss 0.10347428172826767 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5927],\n",
      "        [0.5407]], device='mps:0')\n",
      "Iteration 31870 Training loss 0.10426015406847 Validation loss 0.10345105826854706 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.4680],\n",
      "        [0.1880]], device='mps:0')\n",
      "Iteration 31880 Training loss 0.11858253180980682 Validation loss 0.1035393550992012 Accuracy 0.703000009059906\n",
      "Output tensor([[0.2958],\n",
      "        [0.3917]], device='mps:0')\n",
      "Iteration 31890 Training loss 0.10236004739999771 Validation loss 0.10348343104124069 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.2372],\n",
      "        [0.5224]], device='mps:0')\n",
      "Iteration 31900 Training loss 0.10047704726457596 Validation loss 0.10351299494504929 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5393],\n",
      "        [0.4379]], device='mps:0')\n",
      "Iteration 31910 Training loss 0.10079070925712585 Validation loss 0.1034628227353096 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5034],\n",
      "        [0.6952]], device='mps:0')\n",
      "Iteration 31920 Training loss 0.10420754551887512 Validation loss 0.10343797504901886 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4989],\n",
      "        [0.4766]], device='mps:0')\n",
      "Iteration 31930 Training loss 0.10444910079240799 Validation loss 0.1034565344452858 Accuracy 0.703000009059906\n",
      "Output tensor([[0.3765],\n",
      "        [0.6338]], device='mps:0')\n",
      "Iteration 31940 Training loss 0.10189203172922134 Validation loss 0.10343272238969803 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6521],\n",
      "        [0.6113]], device='mps:0')\n",
      "Iteration 31950 Training loss 0.10780167579650879 Validation loss 0.1034303680062294 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6530],\n",
      "        [0.5626]], device='mps:0')\n",
      "Iteration 31960 Training loss 0.10111621767282486 Validation loss 0.1034495085477829 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4755],\n",
      "        [0.4712]], device='mps:0')\n",
      "Iteration 31970 Training loss 0.11462999135255814 Validation loss 0.10345075279474258 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5193],\n",
      "        [0.5044]], device='mps:0')\n",
      "Iteration 31980 Training loss 0.10328157246112823 Validation loss 0.10341327637434006 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.1126],\n",
      "        [0.2699]], device='mps:0')\n",
      "Iteration 31990 Training loss 0.10883104801177979 Validation loss 0.10343606770038605 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5087],\n",
      "        [0.4209]], device='mps:0')\n",
      "Iteration 32000 Training loss 0.10325833410024643 Validation loss 0.10339684784412384 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5536],\n",
      "        [0.3769]], device='mps:0')\n",
      "Iteration 32010 Training loss 0.11018432676792145 Validation loss 0.1033819168806076 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.1022],\n",
      "        [0.4620]], device='mps:0')\n",
      "Iteration 32020 Training loss 0.09873616695404053 Validation loss 0.10338275879621506 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5492],\n",
      "        [0.1745]], device='mps:0')\n",
      "Iteration 32030 Training loss 0.10035088658332825 Validation loss 0.10340672731399536 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3921],\n",
      "        [0.5669]], device='mps:0')\n",
      "Iteration 32040 Training loss 0.12145621329545975 Validation loss 0.10338205099105835 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5506],\n",
      "        [0.6015]], device='mps:0')\n",
      "Iteration 32050 Training loss 0.10535237938165665 Validation loss 0.10338551551103592 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.1383],\n",
      "        [0.7333]], device='mps:0')\n",
      "Iteration 32060 Training loss 0.1166529729962349 Validation loss 0.10337339341640472 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5701],\n",
      "        [0.5414]], device='mps:0')\n",
      "Iteration 32070 Training loss 0.11208245903253555 Validation loss 0.10338681936264038 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4850],\n",
      "        [0.1707]], device='mps:0')\n",
      "Iteration 32080 Training loss 0.10569947957992554 Validation loss 0.10338535159826279 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6426],\n",
      "        [0.5153]], device='mps:0')\n",
      "Iteration 32090 Training loss 0.09457503259181976 Validation loss 0.1033882200717926 Accuracy 0.706000030040741\n",
      "Output tensor([[0.7217],\n",
      "        [0.6069]], device='mps:0')\n",
      "Iteration 32100 Training loss 0.10010156035423279 Validation loss 0.10341894626617432 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6478],\n",
      "        [0.5749]], device='mps:0')\n",
      "Iteration 32110 Training loss 0.09853670001029968 Validation loss 0.1034335047006607 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.2945],\n",
      "        [0.2484]], device='mps:0')\n",
      "Iteration 32120 Training loss 0.09591419994831085 Validation loss 0.1033921092748642 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5636],\n",
      "        [0.3758]], device='mps:0')\n",
      "Iteration 32130 Training loss 0.096669040620327 Validation loss 0.1034034937620163 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6137],\n",
      "        [0.6498]], device='mps:0')\n",
      "Iteration 32140 Training loss 0.1088951826095581 Validation loss 0.1033802255988121 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4872],\n",
      "        [0.3902]], device='mps:0')\n",
      "Iteration 32150 Training loss 0.11096778512001038 Validation loss 0.10341809689998627 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5311],\n",
      "        [0.4999]], device='mps:0')\n",
      "Iteration 32160 Training loss 0.10356387495994568 Validation loss 0.10340245813131332 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4531],\n",
      "        [0.4333]], device='mps:0')\n",
      "Iteration 32170 Training loss 0.11069519817829132 Validation loss 0.10339386016130447 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.2928],\n",
      "        [0.4547]], device='mps:0')\n",
      "Iteration 32180 Training loss 0.10277631878852844 Validation loss 0.10341870784759521 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.6087],\n",
      "        [0.6950]], device='mps:0')\n",
      "Iteration 32190 Training loss 0.10912319272756577 Validation loss 0.10341285914182663 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.6137],\n",
      "        [0.5759]], device='mps:0')\n",
      "Iteration 32200 Training loss 0.1023980900645256 Validation loss 0.103421650826931 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5333],\n",
      "        [0.5930]], device='mps:0')\n",
      "Iteration 32210 Training loss 0.10895979404449463 Validation loss 0.10338573902845383 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5498],\n",
      "        [0.4629]], device='mps:0')\n",
      "Iteration 32220 Training loss 0.10549470782279968 Validation loss 0.1033526211977005 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.1457],\n",
      "        [0.5632]], device='mps:0')\n",
      "Iteration 32230 Training loss 0.11412648111581802 Validation loss 0.10333909839391708 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5183],\n",
      "        [0.2271]], device='mps:0')\n",
      "Iteration 32240 Training loss 0.10549625009298325 Validation loss 0.10333887487649918 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5399],\n",
      "        [0.7134]], device='mps:0')\n",
      "Iteration 32250 Training loss 0.10832242667675018 Validation loss 0.10333508998155594 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.2630],\n",
      "        [0.3492]], device='mps:0')\n",
      "Iteration 32260 Training loss 0.09149274975061417 Validation loss 0.10333612561225891 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6500],\n",
      "        [0.5753]], device='mps:0')\n",
      "Iteration 32270 Training loss 0.09951700270175934 Validation loss 0.10333331674337387 Accuracy 0.703000009059906\n",
      "Output tensor([[0.3125],\n",
      "        [0.5778]], device='mps:0')\n",
      "Iteration 32280 Training loss 0.10524014383554459 Validation loss 0.10332896560430527 Accuracy 0.70250004529953\n",
      "Output tensor([[0.1978],\n",
      "        [0.6618]], device='mps:0')\n",
      "Iteration 32290 Training loss 0.10648461431264877 Validation loss 0.10333864390850067 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6695],\n",
      "        [0.6464]], device='mps:0')\n",
      "Iteration 32300 Training loss 0.10255166888237 Validation loss 0.10338105261325836 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4117],\n",
      "        [0.6347]], device='mps:0')\n",
      "Iteration 32310 Training loss 0.09954375773668289 Validation loss 0.10336540639400482 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.7027],\n",
      "        [0.5381]], device='mps:0')\n",
      "Iteration 32320 Training loss 0.11508382856845856 Validation loss 0.10332973301410675 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.6558],\n",
      "        [0.4824]], device='mps:0')\n",
      "Iteration 32330 Training loss 0.10667447745800018 Validation loss 0.10331951081752777 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5948],\n",
      "        [0.5348]], device='mps:0')\n",
      "Iteration 32340 Training loss 0.10853077471256256 Validation loss 0.10331808775663376 Accuracy 0.703000009059906\n",
      "Output tensor([[0.2314],\n",
      "        [0.2654]], device='mps:0')\n",
      "Iteration 32350 Training loss 0.09598109871149063 Validation loss 0.10331534594297409 Accuracy 0.703000009059906\n",
      "Output tensor([[0.2542],\n",
      "        [0.3301]], device='mps:0')\n",
      "Iteration 32360 Training loss 0.09751623123884201 Validation loss 0.10331229865550995 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5512],\n",
      "        [0.5080]], device='mps:0')\n",
      "Iteration 32370 Training loss 0.09702962636947632 Validation loss 0.10330982506275177 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.4940],\n",
      "        [0.6984]], device='mps:0')\n",
      "Iteration 32380 Training loss 0.10627559572458267 Validation loss 0.10330717265605927 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4078],\n",
      "        [0.4389]], device='mps:0')\n",
      "Iteration 32390 Training loss 0.11438503861427307 Validation loss 0.10330473631620407 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5930],\n",
      "        [0.6857]], device='mps:0')\n",
      "Iteration 32400 Training loss 0.10610490292310715 Validation loss 0.10330455005168915 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5895],\n",
      "        [0.3069]], device='mps:0')\n",
      "Iteration 32410 Training loss 0.10062269121408463 Validation loss 0.10330025106668472 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5699],\n",
      "        [0.4939]], device='mps:0')\n",
      "Iteration 32420 Training loss 0.09423757344484329 Validation loss 0.10331205278635025 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.6118],\n",
      "        [0.0796]], device='mps:0')\n",
      "Iteration 32430 Training loss 0.1092902347445488 Validation loss 0.10330503433942795 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4456],\n",
      "        [0.6414]], device='mps:0')\n",
      "Iteration 32440 Training loss 0.10394158959388733 Validation loss 0.10329953581094742 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.4922],\n",
      "        [0.1406]], device='mps:0')\n",
      "Iteration 32450 Training loss 0.10927067697048187 Validation loss 0.10330026596784592 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4425],\n",
      "        [0.4739]], device='mps:0')\n",
      "Iteration 32460 Training loss 0.10041271150112152 Validation loss 0.10329524427652359 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.3867],\n",
      "        [0.3526]], device='mps:0')\n",
      "Iteration 32470 Training loss 0.11068836599588394 Validation loss 0.10328294336795807 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3971],\n",
      "        [0.7417]], device='mps:0')\n",
      "Iteration 32480 Training loss 0.10381893813610077 Validation loss 0.1032806932926178 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6144],\n",
      "        [0.6464]], device='mps:0')\n",
      "Iteration 32490 Training loss 0.09541879594326019 Validation loss 0.10328357666730881 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5241],\n",
      "        [0.2068]], device='mps:0')\n",
      "Iteration 32500 Training loss 0.11572303622961044 Validation loss 0.10328155755996704 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3843],\n",
      "        [0.4158]], device='mps:0')\n",
      "Iteration 32510 Training loss 0.10301559418439865 Validation loss 0.10329117625951767 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6509],\n",
      "        [0.5033]], device='mps:0')\n",
      "Iteration 32520 Training loss 0.10234878212213516 Validation loss 0.10331303626298904 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.4776],\n",
      "        [0.6054]], device='mps:0')\n",
      "Iteration 32530 Training loss 0.1065526008605957 Validation loss 0.1032843068242073 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4282],\n",
      "        [0.5580]], device='mps:0')\n",
      "Iteration 32540 Training loss 0.10642814636230469 Validation loss 0.10329680144786835 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5795],\n",
      "        [0.5763]], device='mps:0')\n",
      "Iteration 32550 Training loss 0.11308790743350983 Validation loss 0.10330723971128464 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.3425],\n",
      "        [0.5262]], device='mps:0')\n",
      "Iteration 32560 Training loss 0.10014695674180984 Validation loss 0.10330408066511154 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.1311],\n",
      "        [0.6869]], device='mps:0')\n",
      "Iteration 32570 Training loss 0.10120753198862076 Validation loss 0.10327441990375519 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3059],\n",
      "        [0.5886]], device='mps:0')\n",
      "Iteration 32580 Training loss 0.10262933373451233 Validation loss 0.10327963531017303 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3394],\n",
      "        [0.4259]], device='mps:0')\n",
      "Iteration 32590 Training loss 0.11214005947113037 Validation loss 0.1032964214682579 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5880],\n",
      "        [0.5545]], device='mps:0')\n",
      "Iteration 32600 Training loss 0.0933583602309227 Validation loss 0.10328646004199982 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5588],\n",
      "        [0.5452]], device='mps:0')\n",
      "Iteration 32610 Training loss 0.10300120711326599 Validation loss 0.10326924175024033 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5382],\n",
      "        [0.3060]], device='mps:0')\n",
      "Iteration 32620 Training loss 0.1003650426864624 Validation loss 0.10326384752988815 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4759],\n",
      "        [0.5861]], device='mps:0')\n",
      "Iteration 32630 Training loss 0.10068576782941818 Validation loss 0.10325704514980316 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4258],\n",
      "        [0.1078]], device='mps:0')\n",
      "Iteration 32640 Training loss 0.09432156383991241 Validation loss 0.10325908660888672 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.3757],\n",
      "        [0.2479]], device='mps:0')\n",
      "Iteration 32650 Training loss 0.10388099402189255 Validation loss 0.10329299420118332 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.2890],\n",
      "        [0.4334]], device='mps:0')\n",
      "Iteration 32660 Training loss 0.10882040113210678 Validation loss 0.10333649069070816 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5079],\n",
      "        [0.0963]], device='mps:0')\n",
      "Iteration 32670 Training loss 0.11581672728061676 Validation loss 0.10331276059150696 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5861],\n",
      "        [0.4818]], device='mps:0')\n",
      "Iteration 32680 Training loss 0.10870815813541412 Validation loss 0.10333029180765152 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3756],\n",
      "        [0.3336]], device='mps:0')\n",
      "Iteration 32690 Training loss 0.10997176170349121 Validation loss 0.10326655954122543 Accuracy 0.706000030040741\n",
      "Output tensor([[0.4455],\n",
      "        [0.4536]], device='mps:0')\n",
      "Iteration 32700 Training loss 0.11545857787132263 Validation loss 0.10326632857322693 Accuracy 0.706000030040741\n",
      "Output tensor([[0.2346],\n",
      "        [0.3856]], device='mps:0')\n",
      "Iteration 32710 Training loss 0.10199061036109924 Validation loss 0.10326750576496124 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5810],\n",
      "        [0.5147]], device='mps:0')\n",
      "Iteration 32720 Training loss 0.098701611161232 Validation loss 0.10325633734464645 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5253],\n",
      "        [0.5660]], device='mps:0')\n",
      "Iteration 32730 Training loss 0.10828360915184021 Validation loss 0.10325579345226288 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2664],\n",
      "        [0.5593]], device='mps:0')\n",
      "Iteration 32740 Training loss 0.09254010766744614 Validation loss 0.10324995964765549 Accuracy 0.706000030040741\n",
      "Output tensor([[0.4042],\n",
      "        [0.5812]], device='mps:0')\n",
      "Iteration 32750 Training loss 0.10768842697143555 Validation loss 0.1032438799738884 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3438],\n",
      "        [0.3175]], device='mps:0')\n",
      "Iteration 32760 Training loss 0.1092878133058548 Validation loss 0.10323027521371841 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.7063],\n",
      "        [0.7380]], device='mps:0')\n",
      "Iteration 32770 Training loss 0.10190185159444809 Validation loss 0.10322955995798111 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6563],\n",
      "        [0.7320]], device='mps:0')\n",
      "Iteration 32780 Training loss 0.10114234685897827 Validation loss 0.10322780907154083 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5163],\n",
      "        [0.5906]], device='mps:0')\n",
      "Iteration 32790 Training loss 0.10230502486228943 Validation loss 0.10323122888803482 Accuracy 0.703000009059906\n",
      "Output tensor([[0.3742],\n",
      "        [0.6769]], device='mps:0')\n",
      "Iteration 32800 Training loss 0.10417425632476807 Validation loss 0.1032257005572319 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6438],\n",
      "        [0.4108]], device='mps:0')\n",
      "Iteration 32810 Training loss 0.0985649973154068 Validation loss 0.10323403775691986 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6952],\n",
      "        [0.5815]], device='mps:0')\n",
      "Iteration 32820 Training loss 0.10652437061071396 Validation loss 0.10324078798294067 Accuracy 0.706000030040741\n",
      "Output tensor([[0.7169],\n",
      "        [0.5249]], device='mps:0')\n",
      "Iteration 32830 Training loss 0.10530473291873932 Validation loss 0.10324738174676895 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5654],\n",
      "        [0.5714]], device='mps:0')\n",
      "Iteration 32840 Training loss 0.11120857298374176 Validation loss 0.10325923562049866 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.7198],\n",
      "        [0.7160]], device='mps:0')\n",
      "Iteration 32850 Training loss 0.09850925207138062 Validation loss 0.10327323526144028 Accuracy 0.70250004529953\n",
      "Output tensor([[0.1808],\n",
      "        [0.6244]], device='mps:0')\n",
      "Iteration 32860 Training loss 0.09954522550106049 Validation loss 0.1032356321811676 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4298],\n",
      "        [0.5257]], device='mps:0')\n",
      "Iteration 32870 Training loss 0.10888958722352982 Validation loss 0.10321931540966034 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.3862],\n",
      "        [0.5068]], device='mps:0')\n",
      "Iteration 32880 Training loss 0.10431303083896637 Validation loss 0.10320688039064407 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.4879],\n",
      "        [0.5996]], device='mps:0')\n",
      "Iteration 32890 Training loss 0.10336879640817642 Validation loss 0.10320644080638885 Accuracy 0.703000009059906\n",
      "Output tensor([[0.1927],\n",
      "        [0.4807]], device='mps:0')\n",
      "Iteration 32900 Training loss 0.10154860466718674 Validation loss 0.10320163518190384 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6025],\n",
      "        [0.1597]], device='mps:0')\n",
      "Iteration 32910 Training loss 0.10175930708646774 Validation loss 0.10321003943681717 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6406],\n",
      "        [0.4324]], device='mps:0')\n",
      "Iteration 32920 Training loss 0.10898982733488083 Validation loss 0.10321810096502304 Accuracy 0.706000030040741\n",
      "Output tensor([[0.3145],\n",
      "        [0.6672]], device='mps:0')\n",
      "Iteration 32930 Training loss 0.10992152243852615 Validation loss 0.10319659113883972 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6141],\n",
      "        [0.3977]], device='mps:0')\n",
      "Iteration 32940 Training loss 0.10782814025878906 Validation loss 0.10319554805755615 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3121],\n",
      "        [0.4895]], device='mps:0')\n",
      "Iteration 32950 Training loss 0.10923486202955246 Validation loss 0.10319658368825912 Accuracy 0.7020000219345093\n",
      "Output tensor([[0.1918],\n",
      "        [0.2933]], device='mps:0')\n",
      "Iteration 32960 Training loss 0.10157667845487595 Validation loss 0.10319536179304123 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.3519],\n",
      "        [0.4864]], device='mps:0')\n",
      "Iteration 32970 Training loss 0.1040651798248291 Validation loss 0.10319831967353821 Accuracy 0.70250004529953\n",
      "Output tensor([[0.2216],\n",
      "        [0.3174]], device='mps:0')\n",
      "Iteration 32980 Training loss 0.10129409283399582 Validation loss 0.10321181267499924 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4541],\n",
      "        [0.5991]], device='mps:0')\n",
      "Iteration 32990 Training loss 0.10752183943986893 Validation loss 0.10325125604867935 Accuracy 0.70250004529953\n",
      "Output tensor([[0.3756],\n",
      "        [0.5460]], device='mps:0')\n",
      "Iteration 33000 Training loss 0.09704490751028061 Validation loss 0.10318953543901443 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5792],\n",
      "        [0.5465]], device='mps:0')\n",
      "Iteration 33010 Training loss 0.1040116474032402 Validation loss 0.10318934917449951 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.1859],\n",
      "        [0.6097]], device='mps:0')\n",
      "Iteration 33020 Training loss 0.10553130507469177 Validation loss 0.10318971425294876 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6334],\n",
      "        [0.3959]], device='mps:0')\n",
      "Iteration 33030 Training loss 0.09920372068881989 Validation loss 0.10323156416416168 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.2509],\n",
      "        [0.5656]], device='mps:0')\n",
      "Iteration 33040 Training loss 0.10913968831300735 Validation loss 0.10324112325906754 Accuracy 0.70250004529953\n",
      "Output tensor([[0.4797],\n",
      "        [0.6554]], device='mps:0')\n",
      "Iteration 33050 Training loss 0.10566259175539017 Validation loss 0.10325095057487488 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.4184],\n",
      "        [0.5359]], device='mps:0')\n",
      "Iteration 33060 Training loss 0.10510186851024628 Validation loss 0.10320041328668594 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6936],\n",
      "        [0.4062]], device='mps:0')\n",
      "Iteration 33070 Training loss 0.11459849774837494 Validation loss 0.10322215408086777 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6511],\n",
      "        [0.1873]], device='mps:0')\n",
      "Iteration 33080 Training loss 0.10023581981658936 Validation loss 0.10318794846534729 Accuracy 0.706000030040741\n",
      "Output tensor([[0.3866],\n",
      "        [0.0714]], device='mps:0')\n",
      "Iteration 33090 Training loss 0.10029131919145584 Validation loss 0.10317632555961609 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4170],\n",
      "        [0.1223]], device='mps:0')\n",
      "Iteration 33100 Training loss 0.10770103335380554 Validation loss 0.1031743735074997 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.5324],\n",
      "        [0.5397]], device='mps:0')\n",
      "Iteration 33110 Training loss 0.10437360405921936 Validation loss 0.10317279398441315 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5952],\n",
      "        [0.5492]], device='mps:0')\n",
      "Iteration 33120 Training loss 0.09598668664693832 Validation loss 0.10317264497280121 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.4939],\n",
      "        [0.6944]], device='mps:0')\n",
      "Iteration 33130 Training loss 0.09172450006008148 Validation loss 0.10317172855138779 Accuracy 0.706000030040741\n",
      "Output tensor([[0.2609],\n",
      "        [0.5192]], device='mps:0')\n",
      "Iteration 33140 Training loss 0.12016866356134415 Validation loss 0.1031641885638237 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6501],\n",
      "        [0.2457]], device='mps:0')\n",
      "Iteration 33150 Training loss 0.09774348884820938 Validation loss 0.1031753420829773 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.2035],\n",
      "        [0.6289]], device='mps:0')\n",
      "Iteration 33160 Training loss 0.10697198659181595 Validation loss 0.1031758040189743 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.3602],\n",
      "        [0.5193]], device='mps:0')\n",
      "Iteration 33170 Training loss 0.09708330035209656 Validation loss 0.10319394618272781 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5766],\n",
      "        [0.0726]], device='mps:0')\n",
      "Iteration 33180 Training loss 0.09796476364135742 Validation loss 0.10317346453666687 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5991],\n",
      "        [0.4475]], device='mps:0')\n",
      "Iteration 33190 Training loss 0.10615351796150208 Validation loss 0.10322283208370209 Accuracy 0.703000009059906\n",
      "Output tensor([[0.2651],\n",
      "        [0.6101]], device='mps:0')\n",
      "Iteration 33200 Training loss 0.10533666610717773 Validation loss 0.10319527238607407 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4691],\n",
      "        [0.6643]], device='mps:0')\n",
      "Iteration 33210 Training loss 0.09882405400276184 Validation loss 0.10325298458337784 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5730],\n",
      "        [0.4482]], device='mps:0')\n",
      "Iteration 33220 Training loss 0.11357208341360092 Validation loss 0.10328561812639236 Accuracy 0.703000009059906\n",
      "Output tensor([[0.3810],\n",
      "        [0.2801]], device='mps:0')\n",
      "Iteration 33230 Training loss 0.09736957401037216 Validation loss 0.10328266024589539 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4192],\n",
      "        [0.6595]], device='mps:0')\n",
      "Iteration 33240 Training loss 0.10545248538255692 Validation loss 0.10329332202672958 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5692],\n",
      "        [0.4774]], device='mps:0')\n",
      "Iteration 33250 Training loss 0.10008618980646133 Validation loss 0.1032724678516388 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4180],\n",
      "        [0.3601]], device='mps:0')\n",
      "Iteration 33260 Training loss 0.10981181263923645 Validation loss 0.10326644778251648 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6693],\n",
      "        [0.4941]], device='mps:0')\n",
      "Iteration 33270 Training loss 0.11274074018001556 Validation loss 0.10320265591144562 Accuracy 0.70250004529953\n",
      "Output tensor([[0.6574],\n",
      "        [0.6519]], device='mps:0')\n",
      "Iteration 33280 Training loss 0.10275150090456009 Validation loss 0.10322955995798111 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5469],\n",
      "        [0.6030]], device='mps:0')\n",
      "Iteration 33290 Training loss 0.10144182294607162 Validation loss 0.10319152474403381 Accuracy 0.703000009059906\n",
      "Output tensor([[0.3533],\n",
      "        [0.6679]], device='mps:0')\n",
      "Iteration 33300 Training loss 0.09895414113998413 Validation loss 0.10320711135864258 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.3802],\n",
      "        [0.6216]], device='mps:0')\n",
      "Iteration 33310 Training loss 0.10554497689008713 Validation loss 0.10323196649551392 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6335],\n",
      "        [0.5008]], device='mps:0')\n",
      "Iteration 33320 Training loss 0.10136529803276062 Validation loss 0.10321933776140213 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.7374],\n",
      "        [0.5725]], device='mps:0')\n",
      "Iteration 33330 Training loss 0.10149947553873062 Validation loss 0.10320481657981873 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.3651],\n",
      "        [0.5687]], device='mps:0')\n",
      "Iteration 33340 Training loss 0.11047183722257614 Validation loss 0.10315876454114914 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4132],\n",
      "        [0.3425]], device='mps:0')\n",
      "Iteration 33350 Training loss 0.09462879598140717 Validation loss 0.10316513478755951 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.3307],\n",
      "        [0.6221]], device='mps:0')\n",
      "Iteration 33360 Training loss 0.10459469258785248 Validation loss 0.10316444933414459 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2883],\n",
      "        [0.5886]], device='mps:0')\n",
      "Iteration 33370 Training loss 0.1023293286561966 Validation loss 0.10323748737573624 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4907],\n",
      "        [0.6264]], device='mps:0')\n",
      "Iteration 33380 Training loss 0.10579654574394226 Validation loss 0.10330351442098618 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6845],\n",
      "        [0.4143]], device='mps:0')\n",
      "Iteration 33390 Training loss 0.11110367625951767 Validation loss 0.1032259464263916 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6273],\n",
      "        [0.3228]], device='mps:0')\n",
      "Iteration 33400 Training loss 0.11865800619125366 Validation loss 0.10317661613225937 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6941],\n",
      "        [0.5988]], device='mps:0')\n",
      "Iteration 33410 Training loss 0.10364428907632828 Validation loss 0.10316619277000427 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4543],\n",
      "        [0.3972]], device='mps:0')\n",
      "Iteration 33420 Training loss 0.10236269980669022 Validation loss 0.1031804233789444 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.2158],\n",
      "        [0.6626]], device='mps:0')\n",
      "Iteration 33430 Training loss 0.09707187116146088 Validation loss 0.10321484506130219 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5857],\n",
      "        [0.5589]], device='mps:0')\n",
      "Iteration 33440 Training loss 0.11166693270206451 Validation loss 0.10316665470600128 Accuracy 0.703000009059906\n",
      "Output tensor([[0.2178],\n",
      "        [0.7360]], device='mps:0')\n",
      "Iteration 33450 Training loss 0.10064329206943512 Validation loss 0.10318755358457565 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.1059],\n",
      "        [0.5249]], device='mps:0')\n",
      "Iteration 33460 Training loss 0.1059052124619484 Validation loss 0.10315738618373871 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.2960],\n",
      "        [0.4204]], device='mps:0')\n",
      "Iteration 33470 Training loss 0.10719607770442963 Validation loss 0.10312286019325256 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.3939],\n",
      "        [0.6916]], device='mps:0')\n",
      "Iteration 33480 Training loss 0.10998977720737457 Validation loss 0.103129081428051 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3514],\n",
      "        [0.7089]], device='mps:0')\n",
      "Iteration 33490 Training loss 0.10622123628854752 Validation loss 0.10313364118337631 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2345],\n",
      "        [0.2406]], device='mps:0')\n",
      "Iteration 33500 Training loss 0.10818492621183395 Validation loss 0.1031564399600029 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6299],\n",
      "        [0.3835]], device='mps:0')\n",
      "Iteration 33510 Training loss 0.1033821776509285 Validation loss 0.1031479761004448 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5988],\n",
      "        [0.4107]], device='mps:0')\n",
      "Iteration 33520 Training loss 0.106297068297863 Validation loss 0.10311422497034073 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5941],\n",
      "        [0.6974]], device='mps:0')\n",
      "Iteration 33530 Training loss 0.10254046320915222 Validation loss 0.1031179279088974 Accuracy 0.706000030040741\n",
      "Output tensor([[0.2855],\n",
      "        [0.5742]], device='mps:0')\n",
      "Iteration 33540 Training loss 0.09991884231567383 Validation loss 0.10310491919517517 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5794],\n",
      "        [0.4405]], device='mps:0')\n",
      "Iteration 33550 Training loss 0.10405046492815018 Validation loss 0.10310596227645874 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4935],\n",
      "        [0.6324]], device='mps:0')\n",
      "Iteration 33560 Training loss 0.10930033773183823 Validation loss 0.10310409218072891 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5503],\n",
      "        [0.3714]], device='mps:0')\n",
      "Iteration 33570 Training loss 0.11272105574607849 Validation loss 0.10310857743024826 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3345],\n",
      "        [0.5251]], device='mps:0')\n",
      "Iteration 33580 Training loss 0.10766530781984329 Validation loss 0.10310444980859756 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.1475],\n",
      "        [0.3698]], device='mps:0')\n",
      "Iteration 33590 Training loss 0.10474361479282379 Validation loss 0.10310877859592438 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.2703],\n",
      "        [0.7281]], device='mps:0')\n",
      "Iteration 33600 Training loss 0.11348448693752289 Validation loss 0.10311387479305267 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.3117],\n",
      "        [0.2110]], device='mps:0')\n",
      "Iteration 33610 Training loss 0.10518761724233627 Validation loss 0.10310836881399155 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5263],\n",
      "        [0.4403]], device='mps:0')\n",
      "Iteration 33620 Training loss 0.10316965728998184 Validation loss 0.1030903086066246 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5276],\n",
      "        [0.3874]], device='mps:0')\n",
      "Iteration 33630 Training loss 0.0950697585940361 Validation loss 0.10310505330562592 Accuracy 0.706000030040741\n",
      "Output tensor([[0.7555],\n",
      "        [0.4058]], device='mps:0')\n",
      "Iteration 33640 Training loss 0.11035910248756409 Validation loss 0.10314100980758667 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5524],\n",
      "        [0.7397]], device='mps:0')\n",
      "Iteration 33650 Training loss 0.10010495781898499 Validation loss 0.10320337116718292 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.6873],\n",
      "        [0.5779]], device='mps:0')\n",
      "Iteration 33660 Training loss 0.1020132452249527 Validation loss 0.10319426655769348 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6476],\n",
      "        [0.6878]], device='mps:0')\n",
      "Iteration 33670 Training loss 0.1095486581325531 Validation loss 0.10318078845739365 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.7194],\n",
      "        [0.6267]], device='mps:0')\n",
      "Iteration 33680 Training loss 0.10713278502225876 Validation loss 0.10316705703735352 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4652],\n",
      "        [0.7170]], device='mps:0')\n",
      "Iteration 33690 Training loss 0.10308007895946503 Validation loss 0.10311062633991241 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4605],\n",
      "        [0.4871]], device='mps:0')\n",
      "Iteration 33700 Training loss 0.10455960780382156 Validation loss 0.1031147837638855 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4813],\n",
      "        [0.4389]], device='mps:0')\n",
      "Iteration 33710 Training loss 0.10918208956718445 Validation loss 0.10310495644807816 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4539],\n",
      "        [0.4782]], device='mps:0')\n",
      "Iteration 33720 Training loss 0.11981940269470215 Validation loss 0.1030731052160263 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.3486],\n",
      "        [0.6089]], device='mps:0')\n",
      "Iteration 33730 Training loss 0.0992642194032669 Validation loss 0.10306857526302338 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5845],\n",
      "        [0.4382]], device='mps:0')\n",
      "Iteration 33740 Training loss 0.10356011241674423 Validation loss 0.10306987166404724 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.2441],\n",
      "        [0.4533]], device='mps:0')\n",
      "Iteration 33750 Training loss 0.10039106011390686 Validation loss 0.10306968539953232 Accuracy 0.706000030040741\n",
      "Output tensor([[0.3197],\n",
      "        [0.3685]], device='mps:0')\n",
      "Iteration 33760 Training loss 0.10778535902500153 Validation loss 0.10306790471076965 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5103],\n",
      "        [0.6357]], device='mps:0')\n",
      "Iteration 33770 Training loss 0.10134654492139816 Validation loss 0.10306276381015778 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6055],\n",
      "        [0.5625]], device='mps:0')\n",
      "Iteration 33780 Training loss 0.1165027767419815 Validation loss 0.10307672619819641 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6671],\n",
      "        [0.5073]], device='mps:0')\n",
      "Iteration 33790 Training loss 0.1214660257101059 Validation loss 0.10306896269321442 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4378],\n",
      "        [0.5067]], device='mps:0')\n",
      "Iteration 33800 Training loss 0.1014108806848526 Validation loss 0.10306896269321442 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.2124],\n",
      "        [0.5126]], device='mps:0')\n",
      "Iteration 33810 Training loss 0.101505808532238 Validation loss 0.10305789113044739 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.2911],\n",
      "        [0.5085]], device='mps:0')\n",
      "Iteration 33820 Training loss 0.10240881890058517 Validation loss 0.10308815538883209 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6189],\n",
      "        [0.5448]], device='mps:0')\n",
      "Iteration 33830 Training loss 0.10518530011177063 Validation loss 0.1031191274523735 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4560],\n",
      "        [0.6492]], device='mps:0')\n",
      "Iteration 33840 Training loss 0.10639625042676926 Validation loss 0.10317060351371765 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5169],\n",
      "        [0.2794]], device='mps:0')\n",
      "Iteration 33850 Training loss 0.09826622903347015 Validation loss 0.10311833024024963 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4028],\n",
      "        [0.6095]], device='mps:0')\n",
      "Iteration 33860 Training loss 0.10062476247549057 Validation loss 0.10308118909597397 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5998],\n",
      "        [0.7732]], device='mps:0')\n",
      "Iteration 33870 Training loss 0.11134963482618332 Validation loss 0.10307666659355164 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6152],\n",
      "        [0.5516]], device='mps:0')\n",
      "Iteration 33880 Training loss 0.10016170144081116 Validation loss 0.1030820980668068 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6605],\n",
      "        [0.6264]], device='mps:0')\n",
      "Iteration 33890 Training loss 0.11747249215841293 Validation loss 0.10306139290332794 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3028],\n",
      "        [0.3395]], device='mps:0')\n",
      "Iteration 33900 Training loss 0.1165771484375 Validation loss 0.10306886583566666 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.2648],\n",
      "        [0.5320]], device='mps:0')\n",
      "Iteration 33910 Training loss 0.10126841068267822 Validation loss 0.10304956883192062 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6575],\n",
      "        [0.2371]], device='mps:0')\n",
      "Iteration 33920 Training loss 0.10277026891708374 Validation loss 0.1030486524105072 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6069],\n",
      "        [0.3280]], device='mps:0')\n",
      "Iteration 33930 Training loss 0.10841071605682373 Validation loss 0.10303957760334015 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5126],\n",
      "        [0.4954]], device='mps:0')\n",
      "Iteration 33940 Training loss 0.10199323296546936 Validation loss 0.10304057598114014 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.0903],\n",
      "        [0.6739]], device='mps:0')\n",
      "Iteration 33950 Training loss 0.10713165253400803 Validation loss 0.10303602367639542 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5412],\n",
      "        [0.6037]], device='mps:0')\n",
      "Iteration 33960 Training loss 0.10543905198574066 Validation loss 0.1030413880944252 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4216],\n",
      "        [0.4027]], device='mps:0')\n",
      "Iteration 33970 Training loss 0.10939762741327286 Validation loss 0.10303009301424026 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4935],\n",
      "        [0.2944]], device='mps:0')\n",
      "Iteration 33980 Training loss 0.09291258454322815 Validation loss 0.10303027927875519 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4446],\n",
      "        [0.2353]], device='mps:0')\n",
      "Iteration 33990 Training loss 0.11198718845844269 Validation loss 0.10302785038948059 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5837],\n",
      "        [0.7068]], device='mps:0')\n",
      "Iteration 34000 Training loss 0.10969479382038116 Validation loss 0.10303489118814468 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5428],\n",
      "        [0.4443]], device='mps:0')\n",
      "Iteration 34010 Training loss 0.10217036306858063 Validation loss 0.10302982479333878 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5537],\n",
      "        [0.4238]], device='mps:0')\n",
      "Iteration 34020 Training loss 0.10401488840579987 Validation loss 0.10303785651922226 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3785],\n",
      "        [0.4416]], device='mps:0')\n",
      "Iteration 34030 Training loss 0.1060355082154274 Validation loss 0.1030244529247284 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6411],\n",
      "        [0.4065]], device='mps:0')\n",
      "Iteration 34040 Training loss 0.10473840683698654 Validation loss 0.10302096605300903 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5136],\n",
      "        [0.4178]], device='mps:0')\n",
      "Iteration 34050 Training loss 0.10021474957466125 Validation loss 0.1030200719833374 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4746],\n",
      "        [0.5457]], device='mps:0')\n",
      "Iteration 34060 Training loss 0.10229524224996567 Validation loss 0.10303360223770142 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6123],\n",
      "        [0.5183]], device='mps:0')\n",
      "Iteration 34070 Training loss 0.10879912972450256 Validation loss 0.10304884612560272 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3592],\n",
      "        [0.5467]], device='mps:0')\n",
      "Iteration 34080 Training loss 0.10714765638113022 Validation loss 0.10304822027683258 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5532],\n",
      "        [0.5247]], device='mps:0')\n",
      "Iteration 34090 Training loss 0.10236060619354248 Validation loss 0.10306106507778168 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6180],\n",
      "        [0.7289]], device='mps:0')\n",
      "Iteration 34100 Training loss 0.10519205778837204 Validation loss 0.1030498743057251 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6978],\n",
      "        [0.2749]], device='mps:0')\n",
      "Iteration 34110 Training loss 0.10669010132551193 Validation loss 0.10303289443254471 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4303],\n",
      "        [0.4404]], device='mps:0')\n",
      "Iteration 34120 Training loss 0.09496989846229553 Validation loss 0.10307742655277252 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6077],\n",
      "        [0.6681]], device='mps:0')\n",
      "Iteration 34130 Training loss 0.10265085846185684 Validation loss 0.10303597152233124 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5766],\n",
      "        [0.6136]], device='mps:0')\n",
      "Iteration 34140 Training loss 0.11207982897758484 Validation loss 0.10303162783384323 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.3340],\n",
      "        [0.3071]], device='mps:0')\n",
      "Iteration 34150 Training loss 0.11289073526859283 Validation loss 0.10303817689418793 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.7033],\n",
      "        [0.6106]], device='mps:0')\n",
      "Iteration 34160 Training loss 0.11164510250091553 Validation loss 0.10303456336259842 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6876],\n",
      "        [0.1943]], device='mps:0')\n",
      "Iteration 34170 Training loss 0.09540916979312897 Validation loss 0.10302393138408661 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6572],\n",
      "        [0.6295]], device='mps:0')\n",
      "Iteration 34180 Training loss 0.11742588877677917 Validation loss 0.10303181409835815 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5011],\n",
      "        [0.3437]], device='mps:0')\n",
      "Iteration 34190 Training loss 0.10394027829170227 Validation loss 0.10299921780824661 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6655],\n",
      "        [0.3286]], device='mps:0')\n",
      "Iteration 34200 Training loss 0.10668527334928513 Validation loss 0.10300439596176147 Accuracy 0.706000030040741\n",
      "Output tensor([[0.6690],\n",
      "        [0.2279]], device='mps:0')\n",
      "Iteration 34210 Training loss 0.10776710510253906 Validation loss 0.10300733149051666 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5877],\n",
      "        [0.5452]], device='mps:0')\n",
      "Iteration 34220 Training loss 0.08669552952051163 Validation loss 0.10300859808921814 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5319],\n",
      "        [0.6058]], device='mps:0')\n",
      "Iteration 34230 Training loss 0.10958240926265717 Validation loss 0.10302010178565979 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.3135],\n",
      "        [0.5768]], device='mps:0')\n",
      "Iteration 34240 Training loss 0.10165739059448242 Validation loss 0.10303827375173569 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.5065],\n",
      "        [0.5334]], device='mps:0')\n",
      "Iteration 34250 Training loss 0.1175139993429184 Validation loss 0.10306665301322937 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3431],\n",
      "        [0.6978]], device='mps:0')\n",
      "Iteration 34260 Training loss 0.10565192997455597 Validation loss 0.10303865373134613 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4592],\n",
      "        [0.5633]], device='mps:0')\n",
      "Iteration 34270 Training loss 0.10478827357292175 Validation loss 0.1030527725815773 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6510],\n",
      "        [0.4705]], device='mps:0')\n",
      "Iteration 34280 Training loss 0.10843568295240402 Validation loss 0.10306335240602493 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5391],\n",
      "        [0.1919]], device='mps:0')\n",
      "Iteration 34290 Training loss 0.11332141607999802 Validation loss 0.1030767560005188 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6448],\n",
      "        [0.5120]], device='mps:0')\n",
      "Iteration 34300 Training loss 0.09660176187753677 Validation loss 0.10305121541023254 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5859],\n",
      "        [0.2853]], device='mps:0')\n",
      "Iteration 34310 Training loss 0.10375038534402847 Validation loss 0.10304080694913864 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.3195],\n",
      "        [0.3369]], device='mps:0')\n",
      "Iteration 34320 Training loss 0.09653663635253906 Validation loss 0.10307057201862335 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4048],\n",
      "        [0.4815]], device='mps:0')\n",
      "Iteration 34330 Training loss 0.11272813379764557 Validation loss 0.10304874926805496 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4762],\n",
      "        [0.4567]], device='mps:0')\n",
      "Iteration 34340 Training loss 0.09947223216295242 Validation loss 0.10299897938966751 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5545],\n",
      "        [0.2965]], device='mps:0')\n",
      "Iteration 34350 Training loss 0.10652434825897217 Validation loss 0.10302327573299408 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.2419],\n",
      "        [0.7013]], device='mps:0')\n",
      "Iteration 34360 Training loss 0.09818881750106812 Validation loss 0.10305560380220413 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4273],\n",
      "        [0.4194]], device='mps:0')\n",
      "Iteration 34370 Training loss 0.10206687450408936 Validation loss 0.10300087183713913 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6759],\n",
      "        [0.5809]], device='mps:0')\n",
      "Iteration 34380 Training loss 0.10492021590471268 Validation loss 0.10298021882772446 Accuracy 0.706000030040741\n",
      "Output tensor([[0.3842],\n",
      "        [0.6080]], device='mps:0')\n",
      "Iteration 34390 Training loss 0.10064201056957245 Validation loss 0.10298708081245422 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6510],\n",
      "        [0.5346]], device='mps:0')\n",
      "Iteration 34400 Training loss 0.11159927397966385 Validation loss 0.10297344624996185 Accuracy 0.706000030040741\n",
      "Output tensor([[0.4884],\n",
      "        [0.3282]], device='mps:0')\n",
      "Iteration 34410 Training loss 0.0983482077717781 Validation loss 0.102970190346241 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5195],\n",
      "        [0.5111]], device='mps:0')\n",
      "Iteration 34420 Training loss 0.10824920237064362 Validation loss 0.1029767319560051 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.3658],\n",
      "        [0.4202]], device='mps:0')\n",
      "Iteration 34430 Training loss 0.10117263346910477 Validation loss 0.10298407822847366 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5827],\n",
      "        [0.4739]], device='mps:0')\n",
      "Iteration 34440 Training loss 0.1105879470705986 Validation loss 0.10298323631286621 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.2594],\n",
      "        [0.5014]], device='mps:0')\n",
      "Iteration 34450 Training loss 0.1108003482222557 Validation loss 0.10298814624547958 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4707],\n",
      "        [0.2225]], device='mps:0')\n",
      "Iteration 34460 Training loss 0.1016668900847435 Validation loss 0.1029680073261261 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.7453],\n",
      "        [0.5137]], device='mps:0')\n",
      "Iteration 34470 Training loss 0.09944973140954971 Validation loss 0.10296604037284851 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3403],\n",
      "        [0.5494]], device='mps:0')\n",
      "Iteration 34480 Training loss 0.10495071858167648 Validation loss 0.10297787934541702 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6513],\n",
      "        [0.2319]], device='mps:0')\n",
      "Iteration 34490 Training loss 0.1099301129579544 Validation loss 0.1029568612575531 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6735],\n",
      "        [0.3998]], device='mps:0')\n",
      "Iteration 34500 Training loss 0.11219707876443863 Validation loss 0.10295030474662781 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5707],\n",
      "        [0.5971]], device='mps:0')\n",
      "Iteration 34510 Training loss 0.10529882460832596 Validation loss 0.1029457151889801 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.4231],\n",
      "        [0.5925]], device='mps:0')\n",
      "Iteration 34520 Training loss 0.1035626083612442 Validation loss 0.10294391214847565 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5917],\n",
      "        [0.6985]], device='mps:0')\n",
      "Iteration 34530 Training loss 0.10994172841310501 Validation loss 0.10294170677661896 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.3738],\n",
      "        [0.6637]], device='mps:0')\n",
      "Iteration 34540 Training loss 0.10031507164239883 Validation loss 0.10294228047132492 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5176],\n",
      "        [0.5196]], device='mps:0')\n",
      "Iteration 34550 Training loss 0.11007870733737946 Validation loss 0.10296103358268738 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.2554],\n",
      "        [0.6464]], device='mps:0')\n",
      "Iteration 34560 Training loss 0.11120389401912689 Validation loss 0.10295448452234268 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6771],\n",
      "        [0.5007]], device='mps:0')\n",
      "Iteration 34570 Training loss 0.09809957444667816 Validation loss 0.10295027494430542 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5478],\n",
      "        [0.1568]], device='mps:0')\n",
      "Iteration 34580 Training loss 0.1198556125164032 Validation loss 0.10294143855571747 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.2319],\n",
      "        [0.3153]], device='mps:0')\n",
      "Iteration 34590 Training loss 0.09846135228872299 Validation loss 0.10296843945980072 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.2281],\n",
      "        [0.1614]], device='mps:0')\n",
      "Iteration 34600 Training loss 0.10168687254190445 Validation loss 0.1029655933380127 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3766],\n",
      "        [0.6377]], device='mps:0')\n",
      "Iteration 34610 Training loss 0.09897316992282867 Validation loss 0.1029946580529213 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5463],\n",
      "        [0.2382]], device='mps:0')\n",
      "Iteration 34620 Training loss 0.09746955335140228 Validation loss 0.1029563695192337 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4353],\n",
      "        [0.3269]], device='mps:0')\n",
      "Iteration 34630 Training loss 0.10164650529623032 Validation loss 0.10294336080551147 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3020],\n",
      "        [0.4764]], device='mps:0')\n",
      "Iteration 34640 Training loss 0.0964515283703804 Validation loss 0.10293255746364594 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6108],\n",
      "        [0.4502]], device='mps:0')\n",
      "Iteration 34650 Training loss 0.10000034421682358 Validation loss 0.10295159369707108 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6001],\n",
      "        [0.4698]], device='mps:0')\n",
      "Iteration 34660 Training loss 0.10384498536586761 Validation loss 0.10298523306846619 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3947],\n",
      "        [0.4444]], device='mps:0')\n",
      "Iteration 34670 Training loss 0.11016976088285446 Validation loss 0.10297943651676178 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.7334],\n",
      "        [0.4657]], device='mps:0')\n",
      "Iteration 34680 Training loss 0.10962535440921783 Validation loss 0.10296458005905151 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.1306],\n",
      "        [0.4009]], device='mps:0')\n",
      "Iteration 34690 Training loss 0.10881830006837845 Validation loss 0.10292448848485947 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5500],\n",
      "        [0.2597]], device='mps:0')\n",
      "Iteration 34700 Training loss 0.1059761568903923 Validation loss 0.10291370004415512 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4912],\n",
      "        [0.5234]], device='mps:0')\n",
      "Iteration 34710 Training loss 0.09950113296508789 Validation loss 0.10290981829166412 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2425],\n",
      "        [0.6143]], device='mps:0')\n",
      "Iteration 34720 Training loss 0.10031228512525558 Validation loss 0.10291197150945663 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.7482],\n",
      "        [0.5012]], device='mps:0')\n",
      "Iteration 34730 Training loss 0.1055641621351242 Validation loss 0.10290860384702682 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5950],\n",
      "        [0.3806]], device='mps:0')\n",
      "Iteration 34740 Training loss 0.09942620247602463 Validation loss 0.10291431844234467 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3147],\n",
      "        [0.3478]], device='mps:0')\n",
      "Iteration 34750 Training loss 0.11062322556972504 Validation loss 0.10292832553386688 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2270],\n",
      "        [0.4214]], device='mps:0')\n",
      "Iteration 34760 Training loss 0.11293334513902664 Validation loss 0.10291522741317749 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5956],\n",
      "        [0.6428]], device='mps:0')\n",
      "Iteration 34770 Training loss 0.10375048220157623 Validation loss 0.10293512791395187 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6470],\n",
      "        [0.4932]], device='mps:0')\n",
      "Iteration 34780 Training loss 0.11382526159286499 Validation loss 0.10295131802558899 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4924],\n",
      "        [0.6673]], device='mps:0')\n",
      "Iteration 34790 Training loss 0.09405474364757538 Validation loss 0.1029529795050621 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5989],\n",
      "        [0.6018]], device='mps:0')\n",
      "Iteration 34800 Training loss 0.11997547000646591 Validation loss 0.10293253511190414 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5681],\n",
      "        [0.5594]], device='mps:0')\n",
      "Iteration 34810 Training loss 0.10273651033639908 Validation loss 0.10291888564825058 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3855],\n",
      "        [0.6108]], device='mps:0')\n",
      "Iteration 34820 Training loss 0.10256806761026382 Validation loss 0.10291320085525513 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6131],\n",
      "        [0.4663]], device='mps:0')\n",
      "Iteration 34830 Training loss 0.11040273308753967 Validation loss 0.10291486233472824 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5320],\n",
      "        [0.5814]], device='mps:0')\n",
      "Iteration 34840 Training loss 0.1157868355512619 Validation loss 0.10292920470237732 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5661],\n",
      "        [0.5484]], device='mps:0')\n",
      "Iteration 34850 Training loss 0.10469076037406921 Validation loss 0.10292012244462967 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.1924],\n",
      "        [0.4694]], device='mps:0')\n",
      "Iteration 34860 Training loss 0.09611528366804123 Validation loss 0.1029723659157753 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4983],\n",
      "        [0.4344]], device='mps:0')\n",
      "Iteration 34870 Training loss 0.09933704137802124 Validation loss 0.10310155898332596 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4590],\n",
      "        [0.7386]], device='mps:0')\n",
      "Iteration 34880 Training loss 0.11809629201889038 Validation loss 0.10302779823541641 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4092],\n",
      "        [0.6371]], device='mps:0')\n",
      "Iteration 34890 Training loss 0.10201240330934525 Validation loss 0.10296063870191574 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5511],\n",
      "        [0.4043]], device='mps:0')\n",
      "Iteration 34900 Training loss 0.0967058539390564 Validation loss 0.10291199386119843 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5504],\n",
      "        [0.5293]], device='mps:0')\n",
      "Iteration 34910 Training loss 0.10726367682218552 Validation loss 0.10289511829614639 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3457],\n",
      "        [0.2873]], device='mps:0')\n",
      "Iteration 34920 Training loss 0.10453898459672928 Validation loss 0.10288338363170624 Accuracy 0.706000030040741\n",
      "Output tensor([[0.3400],\n",
      "        [0.2593]], device='mps:0')\n",
      "Iteration 34930 Training loss 0.11260051280260086 Validation loss 0.10287315398454666 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5389],\n",
      "        [0.6307]], device='mps:0')\n",
      "Iteration 34940 Training loss 0.09731937199831009 Validation loss 0.10288399457931519 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.1153],\n",
      "        [0.6528]], device='mps:0')\n",
      "Iteration 34950 Training loss 0.10083240270614624 Validation loss 0.10287453234195709 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.4801],\n",
      "        [0.5937]], device='mps:0')\n",
      "Iteration 34960 Training loss 0.1122766062617302 Validation loss 0.10286630690097809 Accuracy 0.706000030040741\n",
      "Output tensor([[0.3875],\n",
      "        [0.5233]], device='mps:0')\n",
      "Iteration 34970 Training loss 0.11105719953775406 Validation loss 0.10286851227283478 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.3728],\n",
      "        [0.7110]], device='mps:0')\n",
      "Iteration 34980 Training loss 0.10551391541957855 Validation loss 0.10286705195903778 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.3697],\n",
      "        [0.6102]], device='mps:0')\n",
      "Iteration 34990 Training loss 0.10926350951194763 Validation loss 0.1028721034526825 Accuracy 0.706000030040741\n",
      "Output tensor([[0.3475],\n",
      "        [0.1138]], device='mps:0')\n",
      "Iteration 35000 Training loss 0.10399100184440613 Validation loss 0.10289075970649719 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6692],\n",
      "        [0.3872]], device='mps:0')\n",
      "Iteration 35010 Training loss 0.12246110290288925 Validation loss 0.1028791069984436 Accuracy 0.706000030040741\n",
      "Output tensor([[0.6095],\n",
      "        [0.5866]], device='mps:0')\n",
      "Iteration 35020 Training loss 0.1073419377207756 Validation loss 0.10293079167604446 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5343],\n",
      "        [0.5875]], device='mps:0')\n",
      "Iteration 35030 Training loss 0.10771175473928452 Validation loss 0.10290324687957764 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.6327],\n",
      "        [0.3764]], device='mps:0')\n",
      "Iteration 35040 Training loss 0.09814609587192535 Validation loss 0.10285276174545288 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5048],\n",
      "        [0.3534]], device='mps:0')\n",
      "Iteration 35050 Training loss 0.10255713015794754 Validation loss 0.10285420715808868 Accuracy 0.706000030040741\n",
      "Output tensor([[0.4069],\n",
      "        [0.3520]], device='mps:0')\n",
      "Iteration 35060 Training loss 0.09689111262559891 Validation loss 0.10285082459449768 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.4641],\n",
      "        [0.6641]], device='mps:0')\n",
      "Iteration 35070 Training loss 0.10608834028244019 Validation loss 0.1028483659029007 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.2458],\n",
      "        [0.3829]], device='mps:0')\n",
      "Iteration 35080 Training loss 0.10424396395683289 Validation loss 0.10285231471061707 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3263],\n",
      "        [0.6017]], device='mps:0')\n",
      "Iteration 35090 Training loss 0.09874950349330902 Validation loss 0.10284649580717087 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.3929],\n",
      "        [0.5905]], device='mps:0')\n",
      "Iteration 35100 Training loss 0.1094270646572113 Validation loss 0.1028439998626709 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.5711],\n",
      "        [0.5978]], device='mps:0')\n",
      "Iteration 35110 Training loss 0.09820050746202469 Validation loss 0.10284435003995895 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.6157],\n",
      "        [0.5814]], device='mps:0')\n",
      "Iteration 35120 Training loss 0.09874021261930466 Validation loss 0.1028398647904396 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5196],\n",
      "        [0.1969]], device='mps:0')\n",
      "Iteration 35130 Training loss 0.10822451114654541 Validation loss 0.1028381735086441 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4874],\n",
      "        [0.4099]], device='mps:0')\n",
      "Iteration 35140 Training loss 0.11119990050792694 Validation loss 0.10284414887428284 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.2514],\n",
      "        [0.5479]], device='mps:0')\n",
      "Iteration 35150 Training loss 0.10756871849298477 Validation loss 0.10285165905952454 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5090],\n",
      "        [0.6163]], device='mps:0')\n",
      "Iteration 35160 Training loss 0.11154817044734955 Validation loss 0.10291292518377304 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6235],\n",
      "        [0.5329]], device='mps:0')\n",
      "Iteration 35170 Training loss 0.1079702377319336 Validation loss 0.10292650014162064 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6677],\n",
      "        [0.6659]], device='mps:0')\n",
      "Iteration 35180 Training loss 0.10387006402015686 Validation loss 0.10288932919502258 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.4008],\n",
      "        [0.5916]], device='mps:0')\n",
      "Iteration 35190 Training loss 0.09808029234409332 Validation loss 0.1028389185667038 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.7603],\n",
      "        [0.3564]], device='mps:0')\n",
      "Iteration 35200 Training loss 0.10129153728485107 Validation loss 0.10286632180213928 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.1812],\n",
      "        [0.6336]], device='mps:0')\n",
      "Iteration 35210 Training loss 0.11392883956432343 Validation loss 0.1028457060456276 Accuracy 0.706000030040741\n",
      "Output tensor([[0.2431],\n",
      "        [0.5635]], device='mps:0')\n",
      "Iteration 35220 Training loss 0.09710534662008286 Validation loss 0.10283562541007996 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.1254],\n",
      "        [0.3245]], device='mps:0')\n",
      "Iteration 35230 Training loss 0.10780087858438492 Validation loss 0.10286983102560043 Accuracy 0.703000009059906\n",
      "Output tensor([[0.4336],\n",
      "        [0.5585]], device='mps:0')\n",
      "Iteration 35240 Training loss 0.09972764551639557 Validation loss 0.10289019346237183 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6389],\n",
      "        [0.6304]], device='mps:0')\n",
      "Iteration 35250 Training loss 0.11234842985868454 Validation loss 0.10284320265054703 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.4947],\n",
      "        [0.4777]], device='mps:0')\n",
      "Iteration 35260 Training loss 0.1020587906241417 Validation loss 0.10283824056386948 Accuracy 0.706000030040741\n",
      "Output tensor([[0.6381],\n",
      "        [0.3075]], device='mps:0')\n",
      "Iteration 35270 Training loss 0.10160202533006668 Validation loss 0.1028306782245636 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.3184],\n",
      "        [0.4537]], device='mps:0')\n",
      "Iteration 35280 Training loss 0.1092376559972763 Validation loss 0.10284218192100525 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5832],\n",
      "        [0.5545]], device='mps:0')\n",
      "Iteration 35290 Training loss 0.10613450407981873 Validation loss 0.1028633639216423 Accuracy 0.703000009059906\n",
      "Output tensor([[0.5561],\n",
      "        [0.3290]], device='mps:0')\n",
      "Iteration 35300 Training loss 0.09728167206048965 Validation loss 0.10285539925098419 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4044],\n",
      "        [0.4401]], device='mps:0')\n",
      "Iteration 35310 Training loss 0.11875272542238235 Validation loss 0.1029195562005043 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4683],\n",
      "        [0.6308]], device='mps:0')\n",
      "Iteration 35320 Training loss 0.11599244922399521 Validation loss 0.10291431099176407 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3996],\n",
      "        [0.2488]], device='mps:0')\n",
      "Iteration 35330 Training loss 0.10419904440641403 Validation loss 0.10288102924823761 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2344],\n",
      "        [0.5573]], device='mps:0')\n",
      "Iteration 35340 Training loss 0.10681971907615662 Validation loss 0.10290117561817169 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5338],\n",
      "        [0.1776]], device='mps:0')\n",
      "Iteration 35350 Training loss 0.11135736107826233 Validation loss 0.10284124314785004 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4773],\n",
      "        [0.6547]], device='mps:0')\n",
      "Iteration 35360 Training loss 0.08930185437202454 Validation loss 0.10285399109125137 Accuracy 0.70250004529953\n",
      "Output tensor([[0.2614],\n",
      "        [0.3431]], device='mps:0')\n",
      "Iteration 35370 Training loss 0.08790254592895508 Validation loss 0.1028677448630333 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3649],\n",
      "        [0.5138]], device='mps:0')\n",
      "Iteration 35380 Training loss 0.09801264852285385 Validation loss 0.10287424176931381 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2664],\n",
      "        [0.6758]], device='mps:0')\n",
      "Iteration 35390 Training loss 0.10504202544689178 Validation loss 0.10285744071006775 Accuracy 0.70250004529953\n",
      "Output tensor([[0.5369],\n",
      "        [0.5090]], device='mps:0')\n",
      "Iteration 35400 Training loss 0.1039653941988945 Validation loss 0.10285449028015137 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6421],\n",
      "        [0.5588]], device='mps:0')\n",
      "Iteration 35410 Training loss 0.10253099352121353 Validation loss 0.10287654399871826 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6566],\n",
      "        [0.2971]], device='mps:0')\n",
      "Iteration 35420 Training loss 0.1088678240776062 Validation loss 0.10288925468921661 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6255],\n",
      "        [0.4433]], device='mps:0')\n",
      "Iteration 35430 Training loss 0.1173037514090538 Validation loss 0.10284704715013504 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3998],\n",
      "        [0.5350]], device='mps:0')\n",
      "Iteration 35440 Training loss 0.1007704883813858 Validation loss 0.1028187945485115 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5808],\n",
      "        [0.3248]], device='mps:0')\n",
      "Iteration 35450 Training loss 0.11010222136974335 Validation loss 0.10284731537103653 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.2756],\n",
      "        [0.4140]], device='mps:0')\n",
      "Iteration 35460 Training loss 0.10486903041601181 Validation loss 0.10291880369186401 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4815],\n",
      "        [0.5752]], device='mps:0')\n",
      "Iteration 35470 Training loss 0.10171657055616379 Validation loss 0.10286838561296463 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3502],\n",
      "        [0.5517]], device='mps:0')\n",
      "Iteration 35480 Training loss 0.10977712273597717 Validation loss 0.10283717513084412 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.2919],\n",
      "        [0.4378]], device='mps:0')\n",
      "Iteration 35490 Training loss 0.10528050363063812 Validation loss 0.10281083732843399 Accuracy 0.706000030040741\n",
      "Output tensor([[0.3380],\n",
      "        [0.4998]], device='mps:0')\n",
      "Iteration 35500 Training loss 0.1059209406375885 Validation loss 0.10282733291387558 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3537],\n",
      "        [0.2724]], device='mps:0')\n",
      "Iteration 35510 Training loss 0.09241475909948349 Validation loss 0.10284971445798874 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.3880],\n",
      "        [0.5026]], device='mps:0')\n",
      "Iteration 35520 Training loss 0.10609322041273117 Validation loss 0.10280882567167282 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.6323],\n",
      "        [0.4703]], device='mps:0')\n",
      "Iteration 35530 Training loss 0.10579165071249008 Validation loss 0.10280046612024307 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.4426],\n",
      "        [0.6931]], device='mps:0')\n",
      "Iteration 35540 Training loss 0.10923976451158524 Validation loss 0.10279973596334457 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.6388],\n",
      "        [0.5102]], device='mps:0')\n",
      "Iteration 35550 Training loss 0.10283342003822327 Validation loss 0.10279019176959991 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6711],\n",
      "        [0.6133]], device='mps:0')\n",
      "Iteration 35560 Training loss 0.10944953560829163 Validation loss 0.10278661549091339 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4002],\n",
      "        [0.6435]], device='mps:0')\n",
      "Iteration 35570 Training loss 0.09933877736330032 Validation loss 0.10281952470541 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4762],\n",
      "        [0.6993]], device='mps:0')\n",
      "Iteration 35580 Training loss 0.09918460994958878 Validation loss 0.10278687626123428 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5004],\n",
      "        [0.5561]], device='mps:0')\n",
      "Iteration 35590 Training loss 0.10178342461585999 Validation loss 0.10278177261352539 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.4829],\n",
      "        [0.3153]], device='mps:0')\n",
      "Iteration 35600 Training loss 0.09811616688966751 Validation loss 0.1027923971414566 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.5577],\n",
      "        [0.6151]], device='mps:0')\n",
      "Iteration 35610 Training loss 0.10830232501029968 Validation loss 0.10278123617172241 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.4494],\n",
      "        [0.6790]], device='mps:0')\n",
      "Iteration 35620 Training loss 0.11704797297716141 Validation loss 0.10279212146997452 Accuracy 0.706000030040741\n",
      "Output tensor([[0.4286],\n",
      "        [0.4249]], device='mps:0')\n",
      "Iteration 35630 Training loss 0.1101875826716423 Validation loss 0.10280302911996841 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3943],\n",
      "        [0.4506]], device='mps:0')\n",
      "Iteration 35640 Training loss 0.11219354718923569 Validation loss 0.10280591994524002 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.2281],\n",
      "        [0.6298]], device='mps:0')\n",
      "Iteration 35650 Training loss 0.10388647019863129 Validation loss 0.10285098105669022 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4814],\n",
      "        [0.3083]], device='mps:0')\n",
      "Iteration 35660 Training loss 0.10565615445375443 Validation loss 0.10286685079336166 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4236],\n",
      "        [0.4032]], device='mps:0')\n",
      "Iteration 35670 Training loss 0.1122983917593956 Validation loss 0.10287104547023773 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4142],\n",
      "        [0.3898]], device='mps:0')\n",
      "Iteration 35680 Training loss 0.1083114966750145 Validation loss 0.10282159596681595 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.2065],\n",
      "        [0.5823]], device='mps:0')\n",
      "Iteration 35690 Training loss 0.1068081483244896 Validation loss 0.10277922451496124 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.6363],\n",
      "        [0.6384]], device='mps:0')\n",
      "Iteration 35700 Training loss 0.10777482390403748 Validation loss 0.10283476859331131 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5628],\n",
      "        [0.5286]], device='mps:0')\n",
      "Iteration 35710 Training loss 0.11081881076097488 Validation loss 0.1028127521276474 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5717],\n",
      "        [0.6778]], device='mps:0')\n",
      "Iteration 35720 Training loss 0.10558225214481354 Validation loss 0.10283271968364716 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.3525],\n",
      "        [0.2344]], device='mps:0')\n",
      "Iteration 35730 Training loss 0.09505253285169601 Validation loss 0.10290219634771347 Accuracy 0.706000030040741\n",
      "Output tensor([[0.4225],\n",
      "        [0.6817]], device='mps:0')\n",
      "Iteration 35740 Training loss 0.1006900742650032 Validation loss 0.10292305797338486 Accuracy 0.706000030040741\n",
      "Output tensor([[0.6050],\n",
      "        [0.5996]], device='mps:0')\n",
      "Iteration 35750 Training loss 0.09937116503715515 Validation loss 0.10297156125307083 Accuracy 0.706000030040741\n",
      "Output tensor([[0.4371],\n",
      "        [0.6200]], device='mps:0')\n",
      "Iteration 35760 Training loss 0.10075940936803818 Validation loss 0.10281725227832794 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.7031],\n",
      "        [0.5317]], device='mps:0')\n",
      "Iteration 35770 Training loss 0.10784051567316055 Validation loss 0.10278596729040146 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5638],\n",
      "        [0.4945]], device='mps:0')\n",
      "Iteration 35780 Training loss 0.09762278944253922 Validation loss 0.10284076631069183 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5772],\n",
      "        [0.5477]], device='mps:0')\n",
      "Iteration 35790 Training loss 0.10675525665283203 Validation loss 0.10282036662101746 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6147],\n",
      "        [0.6892]], device='mps:0')\n",
      "Iteration 35800 Training loss 0.11735481023788452 Validation loss 0.10281246155500412 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.1975],\n",
      "        [0.2616]], device='mps:0')\n",
      "Iteration 35810 Training loss 0.09498317539691925 Validation loss 0.10278890281915665 Accuracy 0.7035000324249268\n",
      "Output tensor([[0.7195],\n",
      "        [0.2888]], device='mps:0')\n",
      "Iteration 35820 Training loss 0.1023971363902092 Validation loss 0.10274530202150345 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5186],\n",
      "        [0.6277]], device='mps:0')\n",
      "Iteration 35830 Training loss 0.1026669442653656 Validation loss 0.10274127870798111 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.1687],\n",
      "        [0.6674]], device='mps:0')\n",
      "Iteration 35840 Training loss 0.1105201244354248 Validation loss 0.10274826735258102 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5498],\n",
      "        [0.7174]], device='mps:0')\n",
      "Iteration 35850 Training loss 0.10711733251810074 Validation loss 0.10273874551057816 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5703],\n",
      "        [0.5768]], device='mps:0')\n",
      "Iteration 35860 Training loss 0.09719869494438171 Validation loss 0.10273900628089905 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2448],\n",
      "        [0.7740]], device='mps:0')\n",
      "Iteration 35870 Training loss 0.11169145256280899 Validation loss 0.1027386337518692 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6072],\n",
      "        [0.5870]], device='mps:0')\n",
      "Iteration 35880 Training loss 0.10874538868665695 Validation loss 0.10273713618516922 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4721],\n",
      "        [0.5675]], device='mps:0')\n",
      "Iteration 35890 Training loss 0.09245794266462326 Validation loss 0.10273253172636032 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4468],\n",
      "        [0.6342]], device='mps:0')\n",
      "Iteration 35900 Training loss 0.1061914786696434 Validation loss 0.10273206233978271 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2360],\n",
      "        [0.5514]], device='mps:0')\n",
      "Iteration 35910 Training loss 0.09445331245660782 Validation loss 0.10272985696792603 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3168],\n",
      "        [0.3827]], device='mps:0')\n",
      "Iteration 35920 Training loss 0.10073525458574295 Validation loss 0.1027410626411438 Accuracy 0.706000030040741\n",
      "Output tensor([[0.3015],\n",
      "        [0.6527]], device='mps:0')\n",
      "Iteration 35930 Training loss 0.1041262224316597 Validation loss 0.10276837646961212 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6368],\n",
      "        [0.1924]], device='mps:0')\n",
      "Iteration 35940 Training loss 0.0991797149181366 Validation loss 0.10273260623216629 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.6745],\n",
      "        [0.3267]], device='mps:0')\n",
      "Iteration 35950 Training loss 0.09977690130472183 Validation loss 0.10273274779319763 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.4710],\n",
      "        [0.4254]], device='mps:0')\n",
      "Iteration 35960 Training loss 0.09074258804321289 Validation loss 0.1027505099773407 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5690],\n",
      "        [0.7048]], device='mps:0')\n",
      "Iteration 35970 Training loss 0.10541222989559174 Validation loss 0.10272632539272308 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.5136],\n",
      "        [0.7476]], device='mps:0')\n",
      "Iteration 35980 Training loss 0.11100388318300247 Validation loss 0.10271641612052917 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3157],\n",
      "        [0.5211]], device='mps:0')\n",
      "Iteration 35990 Training loss 0.10451746731996536 Validation loss 0.10271371901035309 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5513],\n",
      "        [0.5289]], device='mps:0')\n",
      "Iteration 36000 Training loss 0.10807834565639496 Validation loss 0.10270737111568451 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4489],\n",
      "        [0.1181]], device='mps:0')\n",
      "Iteration 36010 Training loss 0.1062731146812439 Validation loss 0.10270712524652481 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5505],\n",
      "        [0.5739]], device='mps:0')\n",
      "Iteration 36020 Training loss 0.10128537565469742 Validation loss 0.10270705819129944 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4888],\n",
      "        [0.5117]], device='mps:0')\n",
      "Iteration 36030 Training loss 0.1047411784529686 Validation loss 0.10270821303129196 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6981],\n",
      "        [0.7243]], device='mps:0')\n",
      "Iteration 36040 Training loss 0.08755111694335938 Validation loss 0.10272795706987381 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4288],\n",
      "        [0.2446]], device='mps:0')\n",
      "Iteration 36050 Training loss 0.10092765837907791 Validation loss 0.10273369401693344 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4844],\n",
      "        [0.6738]], device='mps:0')\n",
      "Iteration 36060 Training loss 0.10749811679124832 Validation loss 0.10277948528528214 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6910],\n",
      "        [0.3371]], device='mps:0')\n",
      "Iteration 36070 Training loss 0.10259699821472168 Validation loss 0.10272972285747528 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3425],\n",
      "        [0.2025]], device='mps:0')\n",
      "Iteration 36080 Training loss 0.10166648030281067 Validation loss 0.10271846503019333 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6191],\n",
      "        [0.6357]], device='mps:0')\n",
      "Iteration 36090 Training loss 0.10650356113910675 Validation loss 0.10273072123527527 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6477],\n",
      "        [0.3658]], device='mps:0')\n",
      "Iteration 36100 Training loss 0.11422763764858246 Validation loss 0.10272187739610672 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.1601],\n",
      "        [0.5948]], device='mps:0')\n",
      "Iteration 36110 Training loss 0.09655316919088364 Validation loss 0.10271842032670975 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3086],\n",
      "        [0.4333]], device='mps:0')\n",
      "Iteration 36120 Training loss 0.1130499467253685 Validation loss 0.10270103812217712 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2318],\n",
      "        [0.6548]], device='mps:0')\n",
      "Iteration 36130 Training loss 0.10560888051986694 Validation loss 0.10269398987293243 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6741],\n",
      "        [0.2497]], device='mps:0')\n",
      "Iteration 36140 Training loss 0.10314735025167465 Validation loss 0.10269369930028915 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5406],\n",
      "        [0.4270]], device='mps:0')\n",
      "Iteration 36150 Training loss 0.09334762394428253 Validation loss 0.10269606858491898 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3188],\n",
      "        [0.3981]], device='mps:0')\n",
      "Iteration 36160 Training loss 0.10424880683422089 Validation loss 0.1026926338672638 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3796],\n",
      "        [0.5243]], device='mps:0')\n",
      "Iteration 36170 Training loss 0.09994455426931381 Validation loss 0.102690190076828 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4435],\n",
      "        [0.6871]], device='mps:0')\n",
      "Iteration 36180 Training loss 0.114414282143116 Validation loss 0.10272303968667984 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.8240],\n",
      "        [0.6633]], device='mps:0')\n",
      "Iteration 36190 Training loss 0.1013801097869873 Validation loss 0.10268449783325195 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4639],\n",
      "        [0.6055]], device='mps:0')\n",
      "Iteration 36200 Training loss 0.10664728283882141 Validation loss 0.10269365459680557 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.2676],\n",
      "        [0.7453]], device='mps:0')\n",
      "Iteration 36210 Training loss 0.10150929540395737 Validation loss 0.10273391008377075 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5295],\n",
      "        [0.5974]], device='mps:0')\n",
      "Iteration 36220 Training loss 0.10105098783969879 Validation loss 0.10271964222192764 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4892],\n",
      "        [0.3584]], device='mps:0')\n",
      "Iteration 36230 Training loss 0.10411063581705093 Validation loss 0.10270930081605911 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.5430],\n",
      "        [0.5165]], device='mps:0')\n",
      "Iteration 36240 Training loss 0.11010041832923889 Validation loss 0.10272563993930817 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4792],\n",
      "        [0.6546]], device='mps:0')\n",
      "Iteration 36250 Training loss 0.10202137380838394 Validation loss 0.10274165868759155 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6448],\n",
      "        [0.3636]], device='mps:0')\n",
      "Iteration 36260 Training loss 0.10893583297729492 Validation loss 0.10275396704673767 Accuracy 0.706000030040741\n",
      "Output tensor([[0.2746],\n",
      "        [0.6358]], device='mps:0')\n",
      "Iteration 36270 Training loss 0.10826867818832397 Validation loss 0.10272631794214249 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.5260],\n",
      "        [0.6400]], device='mps:0')\n",
      "Iteration 36280 Training loss 0.12098381668329239 Validation loss 0.10276562720537186 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5225],\n",
      "        [0.6720]], device='mps:0')\n",
      "Iteration 36290 Training loss 0.10384867340326309 Validation loss 0.10275975614786148 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5061],\n",
      "        [0.1259]], device='mps:0')\n",
      "Iteration 36300 Training loss 0.09591718763113022 Validation loss 0.10272420197725296 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.4798],\n",
      "        [0.5012]], device='mps:0')\n",
      "Iteration 36310 Training loss 0.10468198359012604 Validation loss 0.10269027948379517 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6292],\n",
      "        [0.6952]], device='mps:0')\n",
      "Iteration 36320 Training loss 0.10000406205654144 Validation loss 0.10270512104034424 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6594],\n",
      "        [0.4453]], device='mps:0')\n",
      "Iteration 36330 Training loss 0.10037372261285782 Validation loss 0.10269695520401001 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.1285],\n",
      "        [0.1656]], device='mps:0')\n",
      "Iteration 36340 Training loss 0.107472263276577 Validation loss 0.10272311419248581 Accuracy 0.706000030040741\n",
      "Output tensor([[0.6026],\n",
      "        [0.5750]], device='mps:0')\n",
      "Iteration 36350 Training loss 0.10289478302001953 Validation loss 0.10272321850061417 Accuracy 0.706000030040741\n",
      "Output tensor([[0.3281],\n",
      "        [0.5242]], device='mps:0')\n",
      "Iteration 36360 Training loss 0.10288187116384506 Validation loss 0.10268063098192215 Accuracy 0.706000030040741\n",
      "Output tensor([[0.2911],\n",
      "        [0.3226]], device='mps:0')\n",
      "Iteration 36370 Training loss 0.10231781005859375 Validation loss 0.10267676413059235 Accuracy 0.706000030040741\n",
      "Output tensor([[0.3957],\n",
      "        [0.6440]], device='mps:0')\n",
      "Iteration 36380 Training loss 0.10756082087755203 Validation loss 0.10266310721635818 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.7292],\n",
      "        [0.4733]], device='mps:0')\n",
      "Iteration 36390 Training loss 0.1118367537856102 Validation loss 0.1026570200920105 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5941],\n",
      "        [0.6235]], device='mps:0')\n",
      "Iteration 36400 Training loss 0.12136209011077881 Validation loss 0.10265956819057465 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.2242],\n",
      "        [0.1267]], device='mps:0')\n",
      "Iteration 36410 Training loss 0.10516538470983505 Validation loss 0.10266623646020889 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.6662],\n",
      "        [0.2231]], device='mps:0')\n",
      "Iteration 36420 Training loss 0.09656412154436111 Validation loss 0.10268916189670563 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6382],\n",
      "        [0.6189]], device='mps:0')\n",
      "Iteration 36430 Training loss 0.09638231992721558 Validation loss 0.10268013179302216 Accuracy 0.7040000557899475\n",
      "Output tensor([[0.6576],\n",
      "        [0.5824]], device='mps:0')\n",
      "Iteration 36440 Training loss 0.11569460481405258 Validation loss 0.10268361121416092 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5476],\n",
      "        [0.2541]], device='mps:0')\n",
      "Iteration 36450 Training loss 0.11689775437116623 Validation loss 0.10266795754432678 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6275],\n",
      "        [0.3313]], device='mps:0')\n",
      "Iteration 36460 Training loss 0.10305453836917877 Validation loss 0.10264581441879272 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3708],\n",
      "        [0.3961]], device='mps:0')\n",
      "Iteration 36470 Training loss 0.1031990572810173 Validation loss 0.10264404863119125 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5235],\n",
      "        [0.5917]], device='mps:0')\n",
      "Iteration 36480 Training loss 0.10012674331665039 Validation loss 0.1026415079832077 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3253],\n",
      "        [0.5315]], device='mps:0')\n",
      "Iteration 36490 Training loss 0.11132074147462845 Validation loss 0.10263983905315399 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5373],\n",
      "        [0.4629]], device='mps:0')\n",
      "Iteration 36500 Training loss 0.10167859494686127 Validation loss 0.10263752192258835 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5979],\n",
      "        [0.6503]], device='mps:0')\n",
      "Iteration 36510 Training loss 0.1104608103632927 Validation loss 0.10263426601886749 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5404],\n",
      "        [0.4096]], device='mps:0')\n",
      "Iteration 36520 Training loss 0.11628251522779465 Validation loss 0.10263542830944061 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5740],\n",
      "        [0.5292]], device='mps:0')\n",
      "Iteration 36530 Training loss 0.0891033262014389 Validation loss 0.10263887047767639 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.1989],\n",
      "        [0.2534]], device='mps:0')\n",
      "Iteration 36540 Training loss 0.09788133203983307 Validation loss 0.10264649242162704 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5794],\n",
      "        [0.5610]], device='mps:0')\n",
      "Iteration 36550 Training loss 0.10397260636091232 Validation loss 0.10262995958328247 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6091],\n",
      "        [0.6703]], device='mps:0')\n",
      "Iteration 36560 Training loss 0.10624206066131592 Validation loss 0.10262800753116608 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5512],\n",
      "        [0.6216]], device='mps:0')\n",
      "Iteration 36570 Training loss 0.09860407561063766 Validation loss 0.10262489318847656 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5283],\n",
      "        [0.6050]], device='mps:0')\n",
      "Iteration 36580 Training loss 0.10367527604103088 Validation loss 0.10262414813041687 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6256],\n",
      "        [0.6743]], device='mps:0')\n",
      "Iteration 36590 Training loss 0.09002145379781723 Validation loss 0.10264936089515686 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6704],\n",
      "        [0.5902]], device='mps:0')\n",
      "Iteration 36600 Training loss 0.11744333058595657 Validation loss 0.10264033079147339 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2468],\n",
      "        [0.5252]], device='mps:0')\n",
      "Iteration 36610 Training loss 0.09062875807285309 Validation loss 0.10262526571750641 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6100],\n",
      "        [0.1146]], device='mps:0')\n",
      "Iteration 36620 Training loss 0.10479649156332016 Validation loss 0.10261497646570206 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5676],\n",
      "        [0.4892]], device='mps:0')\n",
      "Iteration 36630 Training loss 0.10189751535654068 Validation loss 0.1026177927851677 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4861],\n",
      "        [0.4739]], device='mps:0')\n",
      "Iteration 36640 Training loss 0.11450595408678055 Validation loss 0.10261692106723785 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.7117],\n",
      "        [0.4038]], device='mps:0')\n",
      "Iteration 36650 Training loss 0.09349169582128525 Validation loss 0.1026141345500946 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.7009],\n",
      "        [0.4632]], device='mps:0')\n",
      "Iteration 36660 Training loss 0.1038452684879303 Validation loss 0.10261055827140808 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6082],\n",
      "        [0.6530]], device='mps:0')\n",
      "Iteration 36670 Training loss 0.10894166678190231 Validation loss 0.10260962694883347 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4999],\n",
      "        [0.4411]], device='mps:0')\n",
      "Iteration 36680 Training loss 0.1025383323431015 Validation loss 0.10260824114084244 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5240],\n",
      "        [0.6328]], device='mps:0')\n",
      "Iteration 36690 Training loss 0.10882139205932617 Validation loss 0.10260774940252304 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5457],\n",
      "        [0.3723]], device='mps:0')\n",
      "Iteration 36700 Training loss 0.10252568870782852 Validation loss 0.10260886698961258 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5580],\n",
      "        [0.6442]], device='mps:0')\n",
      "Iteration 36710 Training loss 0.10351233929395676 Validation loss 0.10261281579732895 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6027],\n",
      "        [0.4104]], device='mps:0')\n",
      "Iteration 36720 Training loss 0.10344596952199936 Validation loss 0.10260462015867233 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.1663],\n",
      "        [0.4465]], device='mps:0')\n",
      "Iteration 36730 Training loss 0.10274825990200043 Validation loss 0.10262633860111237 Accuracy 0.706000030040741\n",
      "Output tensor([[0.6426],\n",
      "        [0.2549]], device='mps:0')\n",
      "Iteration 36740 Training loss 0.11387350410223007 Validation loss 0.10261829197406769 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5591],\n",
      "        [0.5351]], device='mps:0')\n",
      "Iteration 36750 Training loss 0.09827158600091934 Validation loss 0.10261497646570206 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5163],\n",
      "        [0.6255]], device='mps:0')\n",
      "Iteration 36760 Training loss 0.11188987642526627 Validation loss 0.10261229425668716 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4236],\n",
      "        [0.5183]], device='mps:0')\n",
      "Iteration 36770 Training loss 0.1018664613366127 Validation loss 0.1026110053062439 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4400],\n",
      "        [0.7274]], device='mps:0')\n",
      "Iteration 36780 Training loss 0.09985696524381638 Validation loss 0.10261049121618271 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3927],\n",
      "        [0.5069]], device='mps:0')\n",
      "Iteration 36790 Training loss 0.10536786913871765 Validation loss 0.10262640565633774 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.1491],\n",
      "        [0.5414]], device='mps:0')\n",
      "Iteration 36800 Training loss 0.09872055798768997 Validation loss 0.1026090607047081 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5744],\n",
      "        [0.0735]], device='mps:0')\n",
      "Iteration 36810 Training loss 0.10832973569631577 Validation loss 0.10264322906732559 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4945],\n",
      "        [0.5966]], device='mps:0')\n",
      "Iteration 36820 Training loss 0.10615795105695724 Validation loss 0.10270015895366669 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.3242],\n",
      "        [0.3815]], device='mps:0')\n",
      "Iteration 36830 Training loss 0.10910654067993164 Validation loss 0.10272440314292908 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.6354],\n",
      "        [0.5892]], device='mps:0')\n",
      "Iteration 36840 Training loss 0.10468034446239471 Validation loss 0.10269951075315475 Accuracy 0.703000009059906\n",
      "Output tensor([[0.6203],\n",
      "        [0.7374]], device='mps:0')\n",
      "Iteration 36850 Training loss 0.10246206820011139 Validation loss 0.1026911586523056 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5759],\n",
      "        [0.3788]], device='mps:0')\n",
      "Iteration 36860 Training loss 0.09756985306739807 Validation loss 0.10268904268741608 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6447],\n",
      "        [0.5469]], device='mps:0')\n",
      "Iteration 36870 Training loss 0.1054103896021843 Validation loss 0.10264905542135239 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.4616],\n",
      "        [0.1845]], device='mps:0')\n",
      "Iteration 36880 Training loss 0.10414985567331314 Validation loss 0.10259324312210083 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5382],\n",
      "        [0.7018]], device='mps:0')\n",
      "Iteration 36890 Training loss 0.10762589424848557 Validation loss 0.10258680582046509 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.2484],\n",
      "        [0.6058]], device='mps:0')\n",
      "Iteration 36900 Training loss 0.1018914207816124 Validation loss 0.10256896167993546 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.1954],\n",
      "        [0.5576]], device='mps:0')\n",
      "Iteration 36910 Training loss 0.11110278964042664 Validation loss 0.1025720089673996 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2304],\n",
      "        [0.3087]], device='mps:0')\n",
      "Iteration 36920 Training loss 0.09886026382446289 Validation loss 0.1025923490524292 Accuracy 0.706000030040741\n",
      "Output tensor([[0.2310],\n",
      "        [0.5167]], device='mps:0')\n",
      "Iteration 36930 Training loss 0.10742989927530289 Validation loss 0.10260887444019318 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5519],\n",
      "        [0.5442]], device='mps:0')\n",
      "Iteration 36940 Training loss 0.10686824470758438 Validation loss 0.1026148721575737 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.7161],\n",
      "        [0.5272]], device='mps:0')\n",
      "Iteration 36950 Training loss 0.10084298998117447 Validation loss 0.1025579571723938 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2809],\n",
      "        [0.6099]], device='mps:0')\n",
      "Iteration 36960 Training loss 0.10000099986791611 Validation loss 0.10255667567253113 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4064],\n",
      "        [0.4058]], device='mps:0')\n",
      "Iteration 36970 Training loss 0.11512338370084763 Validation loss 0.10255495458841324 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6675],\n",
      "        [0.6611]], device='mps:0')\n",
      "Iteration 36980 Training loss 0.09846869111061096 Validation loss 0.10255632549524307 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6316],\n",
      "        [0.0932]], device='mps:0')\n",
      "Iteration 36990 Training loss 0.09532172232866287 Validation loss 0.10256708413362503 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.5516],\n",
      "        [0.4770]], device='mps:0')\n",
      "Iteration 37000 Training loss 0.10493282973766327 Validation loss 0.1025729551911354 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5200],\n",
      "        [0.4818]], device='mps:0')\n",
      "Iteration 37010 Training loss 0.10329630970954895 Validation loss 0.10257243365049362 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.4259],\n",
      "        [0.4641]], device='mps:0')\n",
      "Iteration 37020 Training loss 0.10749740153551102 Validation loss 0.10256429761648178 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5989],\n",
      "        [0.4247]], device='mps:0')\n",
      "Iteration 37030 Training loss 0.10119356960058212 Validation loss 0.1025485247373581 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3822],\n",
      "        [0.3811]], device='mps:0')\n",
      "Iteration 37040 Training loss 0.1114513948559761 Validation loss 0.10254709422588348 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3279],\n",
      "        [0.5426]], device='mps:0')\n",
      "Iteration 37050 Training loss 0.10042427480220795 Validation loss 0.10257973521947861 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3030],\n",
      "        [0.4982]], device='mps:0')\n",
      "Iteration 37060 Training loss 0.09911707043647766 Validation loss 0.10255130380392075 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.1816],\n",
      "        [0.4984]], device='mps:0')\n",
      "Iteration 37070 Training loss 0.10413902252912521 Validation loss 0.10260409116744995 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5788],\n",
      "        [0.6435]], device='mps:0')\n",
      "Iteration 37080 Training loss 0.11359480768442154 Validation loss 0.10262561589479446 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.6200],\n",
      "        [0.3916]], device='mps:0')\n",
      "Iteration 37090 Training loss 0.10391706228256226 Validation loss 0.1025790423154831 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.3972],\n",
      "        [0.5864]], device='mps:0')\n",
      "Iteration 37100 Training loss 0.09998145699501038 Validation loss 0.10260702669620514 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.7188],\n",
      "        [0.3326]], device='mps:0')\n",
      "Iteration 37110 Training loss 0.09752243012189865 Validation loss 0.10263499617576599 Accuracy 0.706000030040741\n",
      "Output tensor([[0.1947],\n",
      "        [0.3833]], device='mps:0')\n",
      "Iteration 37120 Training loss 0.11042840778827667 Validation loss 0.10268868505954742 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2893],\n",
      "        [0.4549]], device='mps:0')\n",
      "Iteration 37130 Training loss 0.10555104911327362 Validation loss 0.10264319181442261 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2181],\n",
      "        [0.5831]], device='mps:0')\n",
      "Iteration 37140 Training loss 0.09905026108026505 Validation loss 0.10266541689634323 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6337],\n",
      "        [0.5982]], device='mps:0')\n",
      "Iteration 37150 Training loss 0.09436014294624329 Validation loss 0.1026124432682991 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.5501],\n",
      "        [0.3568]], device='mps:0')\n",
      "Iteration 37160 Training loss 0.10173145681619644 Validation loss 0.1025909036397934 Accuracy 0.706000030040741\n",
      "Output tensor([[0.4442],\n",
      "        [0.5336]], device='mps:0')\n",
      "Iteration 37170 Training loss 0.11286906898021698 Validation loss 0.10254917293787003 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.2789],\n",
      "        [0.5550]], device='mps:0')\n",
      "Iteration 37180 Training loss 0.0996556431055069 Validation loss 0.10254400223493576 Accuracy 0.706000030040741\n",
      "Output tensor([[0.3324],\n",
      "        [0.7367]], device='mps:0')\n",
      "Iteration 37190 Training loss 0.10103066265583038 Validation loss 0.10253114998340607 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2979],\n",
      "        [0.5317]], device='mps:0')\n",
      "Iteration 37200 Training loss 0.09354221075773239 Validation loss 0.10254235565662384 Accuracy 0.706000030040741\n",
      "Output tensor([[0.2975],\n",
      "        [0.5489]], device='mps:0')\n",
      "Iteration 37210 Training loss 0.09545194357633591 Validation loss 0.10254795849323273 Accuracy 0.706000030040741\n",
      "Output tensor([[0.6194],\n",
      "        [0.1845]], device='mps:0')\n",
      "Iteration 37220 Training loss 0.09512479603290558 Validation loss 0.10258260369300842 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.2277],\n",
      "        [0.6476]], device='mps:0')\n",
      "Iteration 37230 Training loss 0.09960441291332245 Validation loss 0.10258598625659943 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.4263],\n",
      "        [0.5142]], device='mps:0')\n",
      "Iteration 37240 Training loss 0.09770873934030533 Validation loss 0.1025964617729187 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.4373],\n",
      "        [0.5502]], device='mps:0')\n",
      "Iteration 37250 Training loss 0.10228963941335678 Validation loss 0.10256966203451157 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5482],\n",
      "        [0.6144]], device='mps:0')\n",
      "Iteration 37260 Training loss 0.11214380711317062 Validation loss 0.10254397988319397 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5015],\n",
      "        [0.7531]], device='mps:0')\n",
      "Iteration 37270 Training loss 0.09442885965108871 Validation loss 0.10251589119434357 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5762],\n",
      "        [0.5439]], device='mps:0')\n",
      "Iteration 37280 Training loss 0.10455790162086487 Validation loss 0.10254353284835815 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.3873],\n",
      "        [0.1732]], device='mps:0')\n",
      "Iteration 37290 Training loss 0.10089574754238129 Validation loss 0.1025727391242981 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.5679],\n",
      "        [0.6070]], device='mps:0')\n",
      "Iteration 37300 Training loss 0.09077475219964981 Validation loss 0.10262224078178406 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4330],\n",
      "        [0.2437]], device='mps:0')\n",
      "Iteration 37310 Training loss 0.10534562170505524 Validation loss 0.10254257917404175 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.5390],\n",
      "        [0.5771]], device='mps:0')\n",
      "Iteration 37320 Training loss 0.10134667158126831 Validation loss 0.10251051932573318 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5177],\n",
      "        [0.5031]], device='mps:0')\n",
      "Iteration 37330 Training loss 0.09460004419088364 Validation loss 0.10250512510538101 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7290],\n",
      "        [0.5915]], device='mps:0')\n",
      "Iteration 37340 Training loss 0.10172776877880096 Validation loss 0.10250629484653473 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4090],\n",
      "        [0.2701]], device='mps:0')\n",
      "Iteration 37350 Training loss 0.1031041145324707 Validation loss 0.10250802338123322 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6531],\n",
      "        [0.6026]], device='mps:0')\n",
      "Iteration 37360 Training loss 0.10962492972612381 Validation loss 0.10252327471971512 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.5819],\n",
      "        [0.5961]], device='mps:0')\n",
      "Iteration 37370 Training loss 0.10354675352573395 Validation loss 0.10255797207355499 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.6566],\n",
      "        [0.4249]], device='mps:0')\n",
      "Iteration 37380 Training loss 0.11244065314531326 Validation loss 0.10254797339439392 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5773],\n",
      "        [0.6177]], device='mps:0')\n",
      "Iteration 37390 Training loss 0.10257691144943237 Validation loss 0.10261014103889465 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6296],\n",
      "        [0.2345]], device='mps:0')\n",
      "Iteration 37400 Training loss 0.1042836606502533 Validation loss 0.10265686362981796 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4390],\n",
      "        [0.3918]], device='mps:0')\n",
      "Iteration 37410 Training loss 0.09764684736728668 Validation loss 0.10259497165679932 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.6231],\n",
      "        [0.6117]], device='mps:0')\n",
      "Iteration 37420 Training loss 0.10455755144357681 Validation loss 0.10255669057369232 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5213],\n",
      "        [0.5368]], device='mps:0')\n",
      "Iteration 37430 Training loss 0.10366211831569672 Validation loss 0.10256204754114151 Accuracy 0.706000030040741\n",
      "Output tensor([[0.6270],\n",
      "        [0.5636]], device='mps:0')\n",
      "Iteration 37440 Training loss 0.09810006618499756 Validation loss 0.1025439202785492 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.6185],\n",
      "        [0.1393]], device='mps:0')\n",
      "Iteration 37450 Training loss 0.10681160539388657 Validation loss 0.10250021517276764 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5939],\n",
      "        [0.6237]], device='mps:0')\n",
      "Iteration 37460 Training loss 0.09790939837694168 Validation loss 0.10250503569841385 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.1666],\n",
      "        [0.6769]], device='mps:0')\n",
      "Iteration 37470 Training loss 0.10800828039646149 Validation loss 0.10250858217477798 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.5286],\n",
      "        [0.4232]], device='mps:0')\n",
      "Iteration 37480 Training loss 0.10277687013149261 Validation loss 0.10250730067491531 Accuracy 0.706000030040741\n",
      "Output tensor([[0.3893],\n",
      "        [0.2670]], device='mps:0')\n",
      "Iteration 37490 Training loss 0.0975566878914833 Validation loss 0.10253896564245224 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.6051],\n",
      "        [0.3907]], device='mps:0')\n",
      "Iteration 37500 Training loss 0.10026735812425613 Validation loss 0.1025443747639656 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.4376],\n",
      "        [0.3397]], device='mps:0')\n",
      "Iteration 37510 Training loss 0.113449826836586 Validation loss 0.1025315597653389 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.6349],\n",
      "        [0.4070]], device='mps:0')\n",
      "Iteration 37520 Training loss 0.1019955426454544 Validation loss 0.10256022959947586 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.1409],\n",
      "        [0.6861]], device='mps:0')\n",
      "Iteration 37530 Training loss 0.09706848114728928 Validation loss 0.10250099748373032 Accuracy 0.706000030040741\n",
      "Output tensor([[0.4430],\n",
      "        [0.4665]], device='mps:0')\n",
      "Iteration 37540 Training loss 0.09303583204746246 Validation loss 0.10247411578893661 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3831],\n",
      "        [0.3270]], device='mps:0')\n",
      "Iteration 37550 Training loss 0.09457650780677795 Validation loss 0.10247279703617096 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6249],\n",
      "        [0.3503]], device='mps:0')\n",
      "Iteration 37560 Training loss 0.09945721179246902 Validation loss 0.102473683655262 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6325],\n",
      "        [0.3179]], device='mps:0')\n",
      "Iteration 37570 Training loss 0.09899064898490906 Validation loss 0.10248671472072601 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.3397],\n",
      "        [0.6921]], device='mps:0')\n",
      "Iteration 37580 Training loss 0.10876090824604034 Validation loss 0.10251189023256302 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.3355],\n",
      "        [0.4842]], device='mps:0')\n",
      "Iteration 37590 Training loss 0.10755707323551178 Validation loss 0.10249993205070496 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5132],\n",
      "        [0.5836]], device='mps:0')\n",
      "Iteration 37600 Training loss 0.09261064231395721 Validation loss 0.1024889126420021 Accuracy 0.706000030040741\n",
      "Output tensor([[0.4189],\n",
      "        [0.6723]], device='mps:0')\n",
      "Iteration 37610 Training loss 0.1019575297832489 Validation loss 0.10247328877449036 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.1863],\n",
      "        [0.5518]], device='mps:0')\n",
      "Iteration 37620 Training loss 0.10177960991859436 Validation loss 0.10249709337949753 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5376],\n",
      "        [0.3526]], device='mps:0')\n",
      "Iteration 37630 Training loss 0.10619693249464035 Validation loss 0.1024705320596695 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4256],\n",
      "        [0.4529]], device='mps:0')\n",
      "Iteration 37640 Training loss 0.11047112941741943 Validation loss 0.10246720165014267 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6748],\n",
      "        [0.5659]], device='mps:0')\n",
      "Iteration 37650 Training loss 0.10063638538122177 Validation loss 0.10246353596448898 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7361],\n",
      "        [0.3425]], device='mps:0')\n",
      "Iteration 37660 Training loss 0.10254791378974915 Validation loss 0.10245975106954575 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6237],\n",
      "        [0.5875]], device='mps:0')\n",
      "Iteration 37670 Training loss 0.09640194475650787 Validation loss 0.10246317088603973 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5040],\n",
      "        [0.6616]], device='mps:0')\n",
      "Iteration 37680 Training loss 0.10260062664747238 Validation loss 0.10250388085842133 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.5444],\n",
      "        [0.4804]], device='mps:0')\n",
      "Iteration 37690 Training loss 0.10181403905153275 Validation loss 0.10245874524116516 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.3931],\n",
      "        [0.2982]], device='mps:0')\n",
      "Iteration 37700 Training loss 0.10395593196153641 Validation loss 0.10244739055633545 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3923],\n",
      "        [0.2527]], device='mps:0')\n",
      "Iteration 37710 Training loss 0.10806472599506378 Validation loss 0.10245105624198914 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3289],\n",
      "        [0.2012]], device='mps:0')\n",
      "Iteration 37720 Training loss 0.10674336552619934 Validation loss 0.1024494394659996 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.2874],\n",
      "        [0.3587]], device='mps:0')\n",
      "Iteration 37730 Training loss 0.09917398542165756 Validation loss 0.10244344919919968 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6510],\n",
      "        [0.6584]], device='mps:0')\n",
      "Iteration 37740 Training loss 0.09599655121564865 Validation loss 0.10244385153055191 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3983],\n",
      "        [0.7378]], device='mps:0')\n",
      "Iteration 37750 Training loss 0.09495019912719727 Validation loss 0.10244905948638916 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2565],\n",
      "        [0.5713]], device='mps:0')\n",
      "Iteration 37760 Training loss 0.09301572293043137 Validation loss 0.10244148969650269 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5290],\n",
      "        [0.4846]], device='mps:0')\n",
      "Iteration 37770 Training loss 0.1081339418888092 Validation loss 0.10245057195425034 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6191],\n",
      "        [0.1375]], device='mps:0')\n",
      "Iteration 37780 Training loss 0.10329059511423111 Validation loss 0.1024358868598938 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6410],\n",
      "        [0.4726]], device='mps:0')\n",
      "Iteration 37790 Training loss 0.10962881147861481 Validation loss 0.10244246572256088 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5484],\n",
      "        [0.3954]], device='mps:0')\n",
      "Iteration 37800 Training loss 0.09446174651384354 Validation loss 0.10247258841991425 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.6208],\n",
      "        [0.5810]], device='mps:0')\n",
      "Iteration 37810 Training loss 0.11235479265451431 Validation loss 0.1024504154920578 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5465],\n",
      "        [0.4623]], device='mps:0')\n",
      "Iteration 37820 Training loss 0.09883653372526169 Validation loss 0.10243868082761765 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4059],\n",
      "        [0.5902]], device='mps:0')\n",
      "Iteration 37830 Training loss 0.10323034971952438 Validation loss 0.10243397206068039 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5152],\n",
      "        [0.3175]], device='mps:0')\n",
      "Iteration 37840 Training loss 0.10216726362705231 Validation loss 0.10245048254728317 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.2305],\n",
      "        [0.4035]], device='mps:0')\n",
      "Iteration 37850 Training loss 0.12321600317955017 Validation loss 0.10245871543884277 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5196],\n",
      "        [0.6285]], device='mps:0')\n",
      "Iteration 37860 Training loss 0.09963279217481613 Validation loss 0.1024450734257698 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3766],\n",
      "        [0.4005]], device='mps:0')\n",
      "Iteration 37870 Training loss 0.0952666848897934 Validation loss 0.10244958847761154 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.4262],\n",
      "        [0.5414]], device='mps:0')\n",
      "Iteration 37880 Training loss 0.10476590692996979 Validation loss 0.1024937778711319 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.6857],\n",
      "        [0.4301]], device='mps:0')\n",
      "Iteration 37890 Training loss 0.10228710621595383 Validation loss 0.10247476398944855 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.7153],\n",
      "        [0.5950]], device='mps:0')\n",
      "Iteration 37900 Training loss 0.10820992290973663 Validation loss 0.102436862885952 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5243],\n",
      "        [0.4706]], device='mps:0')\n",
      "Iteration 37910 Training loss 0.12287187576293945 Validation loss 0.10243546217679977 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.2608],\n",
      "        [0.3455]], device='mps:0')\n",
      "Iteration 37920 Training loss 0.10865123569965363 Validation loss 0.1024349257349968 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5224],\n",
      "        [0.4475]], device='mps:0')\n",
      "Iteration 37930 Training loss 0.09960855543613434 Validation loss 0.10246437788009644 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.3297],\n",
      "        [0.6768]], device='mps:0')\n",
      "Iteration 37940 Training loss 0.10379927605390549 Validation loss 0.10252643376588821 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.2929],\n",
      "        [0.4747]], device='mps:0')\n",
      "Iteration 37950 Training loss 0.09490872919559479 Validation loss 0.10251349210739136 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.4913],\n",
      "        [0.6115]], device='mps:0')\n",
      "Iteration 37960 Training loss 0.09765134751796722 Validation loss 0.10253044962882996 Accuracy 0.7045000195503235\n",
      "Output tensor([[0.4516],\n",
      "        [0.5338]], device='mps:0')\n",
      "Iteration 37970 Training loss 0.10233869403600693 Validation loss 0.10256623476743698 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5528],\n",
      "        [0.4077]], device='mps:0')\n",
      "Iteration 37980 Training loss 0.10750417411327362 Validation loss 0.10256081819534302 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.3913],\n",
      "        [0.3974]], device='mps:0')\n",
      "Iteration 37990 Training loss 0.11553400754928589 Validation loss 0.10250066220760345 Accuracy 0.706000030040741\n",
      "Output tensor([[0.5371],\n",
      "        [0.5430]], device='mps:0')\n",
      "Iteration 38000 Training loss 0.10795965045690536 Validation loss 0.10248947888612747 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5852],\n",
      "        [0.3481]], device='mps:0')\n",
      "Iteration 38010 Training loss 0.10789892077445984 Validation loss 0.10247065871953964 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.5955],\n",
      "        [0.6137]], device='mps:0')\n",
      "Iteration 38020 Training loss 0.11111879348754883 Validation loss 0.10249380767345428 Accuracy 0.706000030040741\n",
      "Output tensor([[0.1925],\n",
      "        [0.2990]], device='mps:0')\n",
      "Iteration 38030 Training loss 0.10423630475997925 Validation loss 0.10248114913702011 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.2594],\n",
      "        [0.5730]], device='mps:0')\n",
      "Iteration 38040 Training loss 0.10484622418880463 Validation loss 0.10244728624820709 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5536],\n",
      "        [0.5920]], device='mps:0')\n",
      "Iteration 38050 Training loss 0.08939085900783539 Validation loss 0.10242791473865509 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.2057],\n",
      "        [0.5427]], device='mps:0')\n",
      "Iteration 38060 Training loss 0.11339528858661652 Validation loss 0.1024026870727539 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3753],\n",
      "        [0.5471]], device='mps:0')\n",
      "Iteration 38070 Training loss 0.10444436967372894 Validation loss 0.10242441296577454 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.7081],\n",
      "        [0.3634]], device='mps:0')\n",
      "Iteration 38080 Training loss 0.10692913085222244 Validation loss 0.10246694087982178 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.4606],\n",
      "        [0.5188]], device='mps:0')\n",
      "Iteration 38090 Training loss 0.10746533423662186 Validation loss 0.10244698077440262 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.5705],\n",
      "        [0.3294]], device='mps:0')\n",
      "Iteration 38100 Training loss 0.0957091897726059 Validation loss 0.10244107246398926 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.3630],\n",
      "        [0.3568]], device='mps:0')\n",
      "Iteration 38110 Training loss 0.10590408742427826 Validation loss 0.10241270065307617 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.5978],\n",
      "        [0.3782]], device='mps:0')\n",
      "Iteration 38120 Training loss 0.10063233226537704 Validation loss 0.10243166983127594 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.7402],\n",
      "        [0.3612]], device='mps:0')\n",
      "Iteration 38130 Training loss 0.11046556383371353 Validation loss 0.10242097079753876 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5495],\n",
      "        [0.6019]], device='mps:0')\n",
      "Iteration 38140 Training loss 0.09940802305936813 Validation loss 0.10241619497537613 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6299],\n",
      "        [0.4028]], device='mps:0')\n",
      "Iteration 38150 Training loss 0.103299580514431 Validation loss 0.1024482473731041 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.4226],\n",
      "        [0.6263]], device='mps:0')\n",
      "Iteration 38160 Training loss 0.10806131362915039 Validation loss 0.10244472324848175 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.5058],\n",
      "        [0.6635]], device='mps:0')\n",
      "Iteration 38170 Training loss 0.10160604119300842 Validation loss 0.10242782533168793 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5434],\n",
      "        [0.4878]], device='mps:0')\n",
      "Iteration 38180 Training loss 0.10069402307271957 Validation loss 0.10244195908308029 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.6874],\n",
      "        [0.6004]], device='mps:0')\n",
      "Iteration 38190 Training loss 0.11242862790822983 Validation loss 0.10245483368635178 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6687],\n",
      "        [0.5623]], device='mps:0')\n",
      "Iteration 38200 Training loss 0.10617829114198685 Validation loss 0.10243150591850281 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.1513],\n",
      "        [0.6999]], device='mps:0')\n",
      "Iteration 38210 Training loss 0.10332507640123367 Validation loss 0.1024220883846283 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.1467],\n",
      "        [0.6066]], device='mps:0')\n",
      "Iteration 38220 Training loss 0.11554863303899765 Validation loss 0.10243482142686844 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.1931],\n",
      "        [0.3626]], device='mps:0')\n",
      "Iteration 38230 Training loss 0.10720857232809067 Validation loss 0.1024228110909462 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6698],\n",
      "        [0.4901]], device='mps:0')\n",
      "Iteration 38240 Training loss 0.0990980714559555 Validation loss 0.1024114191532135 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.4162],\n",
      "        [0.3746]], device='mps:0')\n",
      "Iteration 38250 Training loss 0.10118220001459122 Validation loss 0.1024162769317627 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6224],\n",
      "        [0.5831]], device='mps:0')\n",
      "Iteration 38260 Training loss 0.10810145735740662 Validation loss 0.10238458961248398 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6251],\n",
      "        [0.3262]], device='mps:0')\n",
      "Iteration 38270 Training loss 0.09279691427946091 Validation loss 0.10239731520414352 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2607],\n",
      "        [0.3572]], device='mps:0')\n",
      "Iteration 38280 Training loss 0.09298042953014374 Validation loss 0.10237845778465271 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3177],\n",
      "        [0.4119]], device='mps:0')\n",
      "Iteration 38290 Training loss 0.09586471319198608 Validation loss 0.10236842930316925 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3338],\n",
      "        [0.3375]], device='mps:0')\n",
      "Iteration 38300 Training loss 0.11339857429265976 Validation loss 0.10236702114343643 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6427],\n",
      "        [0.5509]], device='mps:0')\n",
      "Iteration 38310 Training loss 0.09723062068223953 Validation loss 0.10237147659063339 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5713],\n",
      "        [0.7293]], device='mps:0')\n",
      "Iteration 38320 Training loss 0.10267268121242523 Validation loss 0.10236188769340515 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4828],\n",
      "        [0.6980]], device='mps:0')\n",
      "Iteration 38330 Training loss 0.09900608658790588 Validation loss 0.10236125439405441 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5950],\n",
      "        [0.3033]], device='mps:0')\n",
      "Iteration 38340 Training loss 0.10070083290338516 Validation loss 0.10235977917909622 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5933],\n",
      "        [0.3952]], device='mps:0')\n",
      "Iteration 38350 Training loss 0.11277163028717041 Validation loss 0.10235793143510818 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5279],\n",
      "        [0.4497]], device='mps:0')\n",
      "Iteration 38360 Training loss 0.11087694764137268 Validation loss 0.10235736519098282 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5944],\n",
      "        [0.4706]], device='mps:0')\n",
      "Iteration 38370 Training loss 0.10407464951276779 Validation loss 0.10235458612442017 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4599],\n",
      "        [0.6740]], device='mps:0')\n",
      "Iteration 38380 Training loss 0.09356646239757538 Validation loss 0.1023549810051918 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5029],\n",
      "        [0.2502]], device='mps:0')\n",
      "Iteration 38390 Training loss 0.10853186249732971 Validation loss 0.10236664116382599 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5893],\n",
      "        [0.2992]], device='mps:0')\n",
      "Iteration 38400 Training loss 0.10388978570699692 Validation loss 0.10236218571662903 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5649],\n",
      "        [0.5424]], device='mps:0')\n",
      "Iteration 38410 Training loss 0.1004650890827179 Validation loss 0.10234778374433517 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6888],\n",
      "        [0.3600]], device='mps:0')\n",
      "Iteration 38420 Training loss 0.12005925178527832 Validation loss 0.10234813392162323 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6875],\n",
      "        [0.5106]], device='mps:0')\n",
      "Iteration 38430 Training loss 0.10179902613162994 Validation loss 0.102356918156147 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.1828],\n",
      "        [0.5166]], device='mps:0')\n",
      "Iteration 38440 Training loss 0.10443364828824997 Validation loss 0.10236240178346634 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.2672],\n",
      "        [0.6572]], device='mps:0')\n",
      "Iteration 38450 Training loss 0.10081370174884796 Validation loss 0.10236354917287827 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6423],\n",
      "        [0.3799]], device='mps:0')\n",
      "Iteration 38460 Training loss 0.09680186212062836 Validation loss 0.10235302895307541 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3966],\n",
      "        [0.5562]], device='mps:0')\n",
      "Iteration 38470 Training loss 0.10774041712284088 Validation loss 0.10234181582927704 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6529],\n",
      "        [0.7152]], device='mps:0')\n",
      "Iteration 38480 Training loss 0.10384850949048996 Validation loss 0.10234572738409042 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6206],\n",
      "        [0.2986]], device='mps:0')\n",
      "Iteration 38490 Training loss 0.09944413602352142 Validation loss 0.1023372933268547 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6481],\n",
      "        [0.4850]], device='mps:0')\n",
      "Iteration 38500 Training loss 0.09881124645471573 Validation loss 0.10233590751886368 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6452],\n",
      "        [0.3432]], device='mps:0')\n",
      "Iteration 38510 Training loss 0.09268169105052948 Validation loss 0.10234548151493073 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5578],\n",
      "        [0.4441]], device='mps:0')\n",
      "Iteration 38520 Training loss 0.1009165421128273 Validation loss 0.10234425216913223 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4242],\n",
      "        [0.5604]], device='mps:0')\n",
      "Iteration 38530 Training loss 0.09445346891880035 Validation loss 0.10235338658094406 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.7094],\n",
      "        [0.5459]], device='mps:0')\n",
      "Iteration 38540 Training loss 0.10142888128757477 Validation loss 0.10235414654016495 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5950],\n",
      "        [0.6709]], device='mps:0')\n",
      "Iteration 38550 Training loss 0.1061575785279274 Validation loss 0.10237760096788406 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2206],\n",
      "        [0.5839]], device='mps:0')\n",
      "Iteration 38560 Training loss 0.10180345922708511 Validation loss 0.10236262530088425 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5728],\n",
      "        [0.3360]], device='mps:0')\n",
      "Iteration 38570 Training loss 0.11273173987865448 Validation loss 0.10235089808702469 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5841],\n",
      "        [0.3808]], device='mps:0')\n",
      "Iteration 38580 Training loss 0.1038370132446289 Validation loss 0.10235395282506943 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3696],\n",
      "        [0.5491]], device='mps:0')\n",
      "Iteration 38590 Training loss 0.10730930417776108 Validation loss 0.10240825265645981 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.2685],\n",
      "        [0.4096]], device='mps:0')\n",
      "Iteration 38600 Training loss 0.09687516838312149 Validation loss 0.10238945484161377 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6565],\n",
      "        [0.6348]], device='mps:0')\n",
      "Iteration 38610 Training loss 0.11475236713886261 Validation loss 0.10235182195901871 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.7594],\n",
      "        [0.7264]], device='mps:0')\n",
      "Iteration 38620 Training loss 0.09949939697980881 Validation loss 0.10235631465911865 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.7253],\n",
      "        [0.5747]], device='mps:0')\n",
      "Iteration 38630 Training loss 0.0936058908700943 Validation loss 0.10235290229320526 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5550],\n",
      "        [0.6566]], device='mps:0')\n",
      "Iteration 38640 Training loss 0.1009567603468895 Validation loss 0.10232536494731903 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3783],\n",
      "        [0.7192]], device='mps:0')\n",
      "Iteration 38650 Training loss 0.10328681021928787 Validation loss 0.10234374552965164 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5337],\n",
      "        [0.5522]], device='mps:0')\n",
      "Iteration 38660 Training loss 0.09559915214776993 Validation loss 0.10232745856046677 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5861],\n",
      "        [0.2471]], device='mps:0')\n",
      "Iteration 38670 Training loss 0.09828010201454163 Validation loss 0.10231394320726395 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5367],\n",
      "        [0.4691]], device='mps:0')\n",
      "Iteration 38680 Training loss 0.09212493896484375 Validation loss 0.10231532901525497 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6230],\n",
      "        [0.5084]], device='mps:0')\n",
      "Iteration 38690 Training loss 0.10731633007526398 Validation loss 0.10231348127126694 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.7161],\n",
      "        [0.0878]], device='mps:0')\n",
      "Iteration 38700 Training loss 0.10113649815320969 Validation loss 0.10233751684427261 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5154],\n",
      "        [0.7458]], device='mps:0')\n",
      "Iteration 38710 Training loss 0.10633488744497299 Validation loss 0.10231882333755493 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.3579],\n",
      "        [0.3871]], device='mps:0')\n",
      "Iteration 38720 Training loss 0.11396415531635284 Validation loss 0.10231710225343704 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4296],\n",
      "        [0.5578]], device='mps:0')\n",
      "Iteration 38730 Training loss 0.10647765547037125 Validation loss 0.10230721533298492 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.1901],\n",
      "        [0.6811]], device='mps:0')\n",
      "Iteration 38740 Training loss 0.10222616791725159 Validation loss 0.10231974720954895 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6360],\n",
      "        [0.5541]], device='mps:0')\n",
      "Iteration 38750 Training loss 0.10525671392679214 Validation loss 0.1023162305355072 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2254],\n",
      "        [0.5709]], device='mps:0')\n",
      "Iteration 38760 Training loss 0.11779632419347763 Validation loss 0.10231946408748627 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.3727],\n",
      "        [0.2993]], device='mps:0')\n",
      "Iteration 38770 Training loss 0.1057276725769043 Validation loss 0.10230691730976105 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.1084],\n",
      "        [0.3872]], device='mps:0')\n",
      "Iteration 38780 Training loss 0.11049825698137283 Validation loss 0.1023435890674591 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5294],\n",
      "        [0.5176]], device='mps:0')\n",
      "Iteration 38790 Training loss 0.10835228860378265 Validation loss 0.10232183337211609 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3443],\n",
      "        [0.5511]], device='mps:0')\n",
      "Iteration 38800 Training loss 0.11714418977499008 Validation loss 0.10229391604661942 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.2249],\n",
      "        [0.2794]], device='mps:0')\n",
      "Iteration 38810 Training loss 0.1074332594871521 Validation loss 0.1023101732134819 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.3404],\n",
      "        [0.4375]], device='mps:0')\n",
      "Iteration 38820 Training loss 0.10124816000461578 Validation loss 0.10233935713768005 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5880],\n",
      "        [0.6064]], device='mps:0')\n",
      "Iteration 38830 Training loss 0.10590942203998566 Validation loss 0.10232236236333847 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3944],\n",
      "        [0.5264]], device='mps:0')\n",
      "Iteration 38840 Training loss 0.10654903203248978 Validation loss 0.10231868177652359 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5334],\n",
      "        [0.6606]], device='mps:0')\n",
      "Iteration 38850 Training loss 0.10062950849533081 Validation loss 0.10229388624429703 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5331],\n",
      "        [0.7557]], device='mps:0')\n",
      "Iteration 38860 Training loss 0.11069539189338684 Validation loss 0.10231340676546097 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.0974],\n",
      "        [0.3777]], device='mps:0')\n",
      "Iteration 38870 Training loss 0.09773435443639755 Validation loss 0.1023109182715416 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5077],\n",
      "        [0.2652]], device='mps:0')\n",
      "Iteration 38880 Training loss 0.11264161020517349 Validation loss 0.10236285626888275 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.2152],\n",
      "        [0.6742]], device='mps:0')\n",
      "Iteration 38890 Training loss 0.09937307983636856 Validation loss 0.10234630107879639 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6752],\n",
      "        [0.1699]], device='mps:0')\n",
      "Iteration 38900 Training loss 0.09697451442480087 Validation loss 0.10234515368938446 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5484],\n",
      "        [0.5527]], device='mps:0')\n",
      "Iteration 38910 Training loss 0.10004500299692154 Validation loss 0.10245051234960556 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.2651],\n",
      "        [0.1154]], device='mps:0')\n",
      "Iteration 38920 Training loss 0.09909133613109589 Validation loss 0.10238275676965714 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.6495],\n",
      "        [0.4300]], device='mps:0')\n",
      "Iteration 38930 Training loss 0.10402335971593857 Validation loss 0.10238347947597504 Accuracy 0.7050000429153442\n",
      "Output tensor([[0.5493],\n",
      "        [0.5337]], device='mps:0')\n",
      "Iteration 38940 Training loss 0.10754000395536423 Validation loss 0.1023305281996727 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5806],\n",
      "        [0.5926]], device='mps:0')\n",
      "Iteration 38950 Training loss 0.10903924703598022 Validation loss 0.10233554244041443 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5700],\n",
      "        [0.5274]], device='mps:0')\n",
      "Iteration 38960 Training loss 0.11228429526090622 Validation loss 0.10231958329677582 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4493],\n",
      "        [0.3459]], device='mps:0')\n",
      "Iteration 38970 Training loss 0.09993734210729599 Validation loss 0.10232540220022202 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.7262],\n",
      "        [0.6011]], device='mps:0')\n",
      "Iteration 38980 Training loss 0.10573411732912064 Validation loss 0.10226867347955704 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4390],\n",
      "        [0.6589]], device='mps:0')\n",
      "Iteration 38990 Training loss 0.115210622549057 Validation loss 0.10228230059146881 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4886],\n",
      "        [0.4405]], device='mps:0')\n",
      "Iteration 39000 Training loss 0.09680584073066711 Validation loss 0.10229937732219696 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6619],\n",
      "        [0.2089]], device='mps:0')\n",
      "Iteration 39010 Training loss 0.09090801328420639 Validation loss 0.10228024423122406 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.2423],\n",
      "        [0.7007]], device='mps:0')\n",
      "Iteration 39020 Training loss 0.10570735484361649 Validation loss 0.10228178650140762 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.3174],\n",
      "        [0.4021]], device='mps:0')\n",
      "Iteration 39030 Training loss 0.10495460033416748 Validation loss 0.10229090601205826 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4722],\n",
      "        [0.2593]], device='mps:0')\n",
      "Iteration 39040 Training loss 0.09757833182811737 Validation loss 0.10232187807559967 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2543],\n",
      "        [0.4400]], device='mps:0')\n",
      "Iteration 39050 Training loss 0.09918484836816788 Validation loss 0.10236439853906631 Accuracy 0.7055000066757202\n",
      "Output tensor([[0.5729],\n",
      "        [0.7815]], device='mps:0')\n",
      "Iteration 39060 Training loss 0.10601139068603516 Validation loss 0.10235202312469482 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.6244],\n",
      "        [0.2136]], device='mps:0')\n",
      "Iteration 39070 Training loss 0.1107645183801651 Validation loss 0.10231896489858627 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5384],\n",
      "        [0.7693]], device='mps:0')\n",
      "Iteration 39080 Training loss 0.11440277844667435 Validation loss 0.10228261351585388 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.1204],\n",
      "        [0.5752]], device='mps:0')\n",
      "Iteration 39090 Training loss 0.10638295859098434 Validation loss 0.102276511490345 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6935],\n",
      "        [0.6502]], device='mps:0')\n",
      "Iteration 39100 Training loss 0.10306352376937866 Validation loss 0.1022692322731018 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5803],\n",
      "        [0.5962]], device='mps:0')\n",
      "Iteration 39110 Training loss 0.10309059172868729 Validation loss 0.10227174311876297 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5118],\n",
      "        [0.6438]], device='mps:0')\n",
      "Iteration 39120 Training loss 0.10545626282691956 Validation loss 0.10225040465593338 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6022],\n",
      "        [0.4640]], device='mps:0')\n",
      "Iteration 39130 Training loss 0.09657203406095505 Validation loss 0.10224901139736176 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6681],\n",
      "        [0.2605]], device='mps:0')\n",
      "Iteration 39140 Training loss 0.09911718219518661 Validation loss 0.10224767029285431 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5109],\n",
      "        [0.7189]], device='mps:0')\n",
      "Iteration 39150 Training loss 0.10986261069774628 Validation loss 0.10225488990545273 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.2155],\n",
      "        [0.5684]], device='mps:0')\n",
      "Iteration 39160 Training loss 0.09793361276388168 Validation loss 0.10225547850131989 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.7097],\n",
      "        [0.5231]], device='mps:0')\n",
      "Iteration 39170 Training loss 0.08692867308855057 Validation loss 0.10224734991788864 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5707],\n",
      "        [0.6229]], device='mps:0')\n",
      "Iteration 39180 Training loss 0.10494937747716904 Validation loss 0.10224705934524536 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.2893],\n",
      "        [0.1881]], device='mps:0')\n",
      "Iteration 39190 Training loss 0.1078731119632721 Validation loss 0.10224545747041702 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5929],\n",
      "        [0.6469]], device='mps:0')\n",
      "Iteration 39200 Training loss 0.10196121782064438 Validation loss 0.10224901884794235 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5789],\n",
      "        [0.5216]], device='mps:0')\n",
      "Iteration 39210 Training loss 0.11019133031368256 Validation loss 0.10223964601755142 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.7248],\n",
      "        [0.4931]], device='mps:0')\n",
      "Iteration 39220 Training loss 0.10605083405971527 Validation loss 0.10224317014217377 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6870],\n",
      "        [0.8202]], device='mps:0')\n",
      "Iteration 39230 Training loss 0.10866574198007584 Validation loss 0.10223819315433502 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4355],\n",
      "        [0.5292]], device='mps:0')\n",
      "Iteration 39240 Training loss 0.10109017789363861 Validation loss 0.1022346243262291 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6913],\n",
      "        [0.3366]], device='mps:0')\n",
      "Iteration 39250 Training loss 0.08883592486381531 Validation loss 0.10223549604415894 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5060],\n",
      "        [0.6323]], device='mps:0')\n",
      "Iteration 39260 Training loss 0.11040046811103821 Validation loss 0.10224173218011856 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.2560],\n",
      "        [0.3754]], device='mps:0')\n",
      "Iteration 39270 Training loss 0.1090199425816536 Validation loss 0.10225588083267212 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7058],\n",
      "        [0.5898]], device='mps:0')\n",
      "Iteration 39280 Training loss 0.10944438725709915 Validation loss 0.1022612527012825 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3892],\n",
      "        [0.4564]], device='mps:0')\n",
      "Iteration 39290 Training loss 0.09834165126085281 Validation loss 0.10231048613786697 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5098],\n",
      "        [0.6121]], device='mps:0')\n",
      "Iteration 39300 Training loss 0.10363185405731201 Validation loss 0.1023033931851387 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5534],\n",
      "        [0.6276]], device='mps:0')\n",
      "Iteration 39310 Training loss 0.11702746897935867 Validation loss 0.10233428329229355 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.5742],\n",
      "        [0.7152]], device='mps:0')\n",
      "Iteration 39320 Training loss 0.10795852541923523 Validation loss 0.10227975249290466 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6316],\n",
      "        [0.2526]], device='mps:0')\n",
      "Iteration 39330 Training loss 0.09514625370502472 Validation loss 0.10234183818101883 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5965],\n",
      "        [0.5849]], device='mps:0')\n",
      "Iteration 39340 Training loss 0.10414251685142517 Validation loss 0.10234013944864273 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.4490],\n",
      "        [0.4379]], device='mps:0')\n",
      "Iteration 39350 Training loss 0.10022172331809998 Validation loss 0.1023053526878357 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.7168],\n",
      "        [0.6358]], device='mps:0')\n",
      "Iteration 39360 Training loss 0.10822759568691254 Validation loss 0.10236717015504837 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.6344],\n",
      "        [0.3632]], device='mps:0')\n",
      "Iteration 39370 Training loss 0.10109594464302063 Validation loss 0.1022796705365181 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5063],\n",
      "        [0.6567]], device='mps:0')\n",
      "Iteration 39380 Training loss 0.10439573973417282 Validation loss 0.10228424519300461 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6486],\n",
      "        [0.6026]], device='mps:0')\n",
      "Iteration 39390 Training loss 0.10015321522951126 Validation loss 0.10230962187051773 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.0834],\n",
      "        [0.4770]], device='mps:0')\n",
      "Iteration 39400 Training loss 0.10250274091959 Validation loss 0.1022992879152298 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.4663],\n",
      "        [0.7164]], device='mps:0')\n",
      "Iteration 39410 Training loss 0.10809940099716187 Validation loss 0.10232048481702805 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.6116],\n",
      "        [0.7114]], device='mps:0')\n",
      "Iteration 39420 Training loss 0.11132125556468964 Validation loss 0.10230106860399246 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.3804],\n",
      "        [0.6618]], device='mps:0')\n",
      "Iteration 39430 Training loss 0.11370649188756943 Validation loss 0.10228204727172852 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.1939],\n",
      "        [0.4829]], device='mps:0')\n",
      "Iteration 39440 Training loss 0.0937848761677742 Validation loss 0.1022505834698677 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6631],\n",
      "        [0.6673]], device='mps:0')\n",
      "Iteration 39450 Training loss 0.10141488164663315 Validation loss 0.10221914947032928 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.4641],\n",
      "        [0.5629]], device='mps:0')\n",
      "Iteration 39460 Training loss 0.11207761615514755 Validation loss 0.10223624110221863 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3728],\n",
      "        [0.4339]], device='mps:0')\n",
      "Iteration 39470 Training loss 0.1124623492360115 Validation loss 0.10220473259687424 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3931],\n",
      "        [0.5425]], device='mps:0')\n",
      "Iteration 39480 Training loss 0.10651879012584686 Validation loss 0.10220843553543091 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3134],\n",
      "        [0.7062]], device='mps:0')\n",
      "Iteration 39490 Training loss 0.1028231605887413 Validation loss 0.10220515727996826 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4187],\n",
      "        [0.7437]], device='mps:0')\n",
      "Iteration 39500 Training loss 0.10555292665958405 Validation loss 0.10221225023269653 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6054],\n",
      "        [0.6238]], device='mps:0')\n",
      "Iteration 39510 Training loss 0.10564962774515152 Validation loss 0.10222224146127701 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.2102],\n",
      "        [0.5182]], device='mps:0')\n",
      "Iteration 39520 Training loss 0.10290130227804184 Validation loss 0.10222421586513519 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3143],\n",
      "        [0.6838]], device='mps:0')\n",
      "Iteration 39530 Training loss 0.09075847268104553 Validation loss 0.10220348089933395 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6270],\n",
      "        [0.5215]], device='mps:0')\n",
      "Iteration 39540 Training loss 0.1087060496211052 Validation loss 0.1021948978304863 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5400],\n",
      "        [0.4644]], device='mps:0')\n",
      "Iteration 39550 Training loss 0.09242692589759827 Validation loss 0.1021934375166893 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6289],\n",
      "        [0.5766]], device='mps:0')\n",
      "Iteration 39560 Training loss 0.1024244874715805 Validation loss 0.10219308733940125 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3723],\n",
      "        [0.5839]], device='mps:0')\n",
      "Iteration 39570 Training loss 0.11807429790496826 Validation loss 0.10219326615333557 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3224],\n",
      "        [0.5788]], device='mps:0')\n",
      "Iteration 39580 Training loss 0.10115514695644379 Validation loss 0.1022079735994339 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4332],\n",
      "        [0.6608]], device='mps:0')\n",
      "Iteration 39590 Training loss 0.09883861243724823 Validation loss 0.1022108942270279 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5704],\n",
      "        [0.3929]], device='mps:0')\n",
      "Iteration 39600 Training loss 0.09420005232095718 Validation loss 0.10219117999076843 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4986],\n",
      "        [0.5911]], device='mps:0')\n",
      "Iteration 39610 Training loss 0.1024903655052185 Validation loss 0.10218992829322815 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3734],\n",
      "        [0.4770]], device='mps:0')\n",
      "Iteration 39620 Training loss 0.1105547770857811 Validation loss 0.10219503194093704 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5012],\n",
      "        [0.5579]], device='mps:0')\n",
      "Iteration 39630 Training loss 0.09786214679479599 Validation loss 0.1021832600235939 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6260],\n",
      "        [0.5667]], device='mps:0')\n",
      "Iteration 39640 Training loss 0.09480228275060654 Validation loss 0.10218098759651184 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6244],\n",
      "        [0.4342]], device='mps:0')\n",
      "Iteration 39650 Training loss 0.09786224365234375 Validation loss 0.10218048095703125 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5925],\n",
      "        [0.4441]], device='mps:0')\n",
      "Iteration 39660 Training loss 0.10690833628177643 Validation loss 0.10218103229999542 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.4797],\n",
      "        [0.3870]], device='mps:0')\n",
      "Iteration 39670 Training loss 0.10231225937604904 Validation loss 0.10218442976474762 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.1849],\n",
      "        [0.6413]], device='mps:0')\n",
      "Iteration 39680 Training loss 0.11528366804122925 Validation loss 0.10217567533254623 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.0861],\n",
      "        [0.5341]], device='mps:0')\n",
      "Iteration 39690 Training loss 0.10131116956472397 Validation loss 0.10217319428920746 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6136],\n",
      "        [0.3655]], device='mps:0')\n",
      "Iteration 39700 Training loss 0.10753582417964935 Validation loss 0.10217676311731339 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5787],\n",
      "        [0.6174]], device='mps:0')\n",
      "Iteration 39710 Training loss 0.10306770354509354 Validation loss 0.10217176377773285 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5503],\n",
      "        [0.6863]], device='mps:0')\n",
      "Iteration 39720 Training loss 0.12172559648752213 Validation loss 0.10216684639453888 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5353],\n",
      "        [0.3574]], device='mps:0')\n",
      "Iteration 39730 Training loss 0.09499023109674454 Validation loss 0.1021638959646225 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3216],\n",
      "        [0.3954]], device='mps:0')\n",
      "Iteration 39740 Training loss 0.11031791567802429 Validation loss 0.10218831896781921 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.6632],\n",
      "        [0.4889]], device='mps:0')\n",
      "Iteration 39750 Training loss 0.11154277622699738 Validation loss 0.10217735916376114 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6960],\n",
      "        [0.6229]], device='mps:0')\n",
      "Iteration 39760 Training loss 0.09840232878923416 Validation loss 0.10217401385307312 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5951],\n",
      "        [0.1739]], device='mps:0')\n",
      "Iteration 39770 Training loss 0.09515449404716492 Validation loss 0.10216651856899261 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5600],\n",
      "        [0.4701]], device='mps:0')\n",
      "Iteration 39780 Training loss 0.12436255812644958 Validation loss 0.10215593129396439 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5928],\n",
      "        [0.3621]], device='mps:0')\n",
      "Iteration 39790 Training loss 0.10327751189470291 Validation loss 0.10215763002634048 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4492],\n",
      "        [0.4853]], device='mps:0')\n",
      "Iteration 39800 Training loss 0.10808733105659485 Validation loss 0.10215383023023605 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2936],\n",
      "        [0.3853]], device='mps:0')\n",
      "Iteration 39810 Training loss 0.10081245005130768 Validation loss 0.10219467431306839 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3864],\n",
      "        [0.2695]], device='mps:0')\n",
      "Iteration 39820 Training loss 0.10520190745592117 Validation loss 0.10219039767980576 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5510],\n",
      "        [0.5148]], device='mps:0')\n",
      "Iteration 39830 Training loss 0.09663518518209457 Validation loss 0.10221559554338455 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6879],\n",
      "        [0.6911]], device='mps:0')\n",
      "Iteration 39840 Training loss 0.10358577221632004 Validation loss 0.10217753797769547 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4829],\n",
      "        [0.2194]], device='mps:0')\n",
      "Iteration 39850 Training loss 0.09651704877614975 Validation loss 0.10218692570924759 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5365],\n",
      "        [0.5539]], device='mps:0')\n",
      "Iteration 39860 Training loss 0.11001318693161011 Validation loss 0.10223383456468582 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.3998],\n",
      "        [0.6082]], device='mps:0')\n",
      "Iteration 39870 Training loss 0.10330213606357574 Validation loss 0.1022668182849884 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.3407],\n",
      "        [0.6717]], device='mps:0')\n",
      "Iteration 39880 Training loss 0.10543394088745117 Validation loss 0.10217137634754181 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6592],\n",
      "        [0.7046]], device='mps:0')\n",
      "Iteration 39890 Training loss 0.09947845339775085 Validation loss 0.10216743499040604 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5415],\n",
      "        [0.1833]], device='mps:0')\n",
      "Iteration 39900 Training loss 0.1097002923488617 Validation loss 0.10217861086130142 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7932],\n",
      "        [0.5666]], device='mps:0')\n",
      "Iteration 39910 Training loss 0.1005270928144455 Validation loss 0.10215967148542404 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5602],\n",
      "        [0.6016]], device='mps:0')\n",
      "Iteration 39920 Training loss 0.0996619388461113 Validation loss 0.1021803542971611 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3663],\n",
      "        [0.6506]], device='mps:0')\n",
      "Iteration 39930 Training loss 0.10350251942873001 Validation loss 0.1021965816617012 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3141],\n",
      "        [0.6424]], device='mps:0')\n",
      "Iteration 39940 Training loss 0.10621898621320724 Validation loss 0.10222364962100983 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5093],\n",
      "        [0.2695]], device='mps:0')\n",
      "Iteration 39950 Training loss 0.0965702161192894 Validation loss 0.10222197324037552 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.2051],\n",
      "        [0.5013]], device='mps:0')\n",
      "Iteration 39960 Training loss 0.10115546733140945 Validation loss 0.10214623063802719 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4941],\n",
      "        [0.6408]], device='mps:0')\n",
      "Iteration 39970 Training loss 0.10549363493919373 Validation loss 0.10215918719768524 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2407],\n",
      "        [0.3491]], device='mps:0')\n",
      "Iteration 39980 Training loss 0.1052861288189888 Validation loss 0.10215074568986893 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.7722],\n",
      "        [0.3390]], device='mps:0')\n",
      "Iteration 39990 Training loss 0.10074157267808914 Validation loss 0.10217563062906265 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6041],\n",
      "        [0.6282]], device='mps:0')\n",
      "Iteration 40000 Training loss 0.09926344454288483 Validation loss 0.10216239094734192 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2654],\n",
      "        [0.6786]], device='mps:0')\n",
      "Iteration 40010 Training loss 0.09755513072013855 Validation loss 0.102166548371315 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.1702],\n",
      "        [0.5658]], device='mps:0')\n",
      "Iteration 40020 Training loss 0.11019343882799149 Validation loss 0.10215992480516434 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4118],\n",
      "        [0.2295]], device='mps:0')\n",
      "Iteration 40030 Training loss 0.10004471242427826 Validation loss 0.10218359529972076 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4186],\n",
      "        [0.7114]], device='mps:0')\n",
      "Iteration 40040 Training loss 0.1040651872754097 Validation loss 0.10215027630329132 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2919],\n",
      "        [0.3052]], device='mps:0')\n",
      "Iteration 40050 Training loss 0.0913257971405983 Validation loss 0.102168008685112 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6860],\n",
      "        [0.5454]], device='mps:0')\n",
      "Iteration 40060 Training loss 0.10573472827672958 Validation loss 0.10220729559659958 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4563],\n",
      "        [0.6424]], device='mps:0')\n",
      "Iteration 40070 Training loss 0.09570125490427017 Validation loss 0.10218846797943115 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6442],\n",
      "        [0.6744]], device='mps:0')\n",
      "Iteration 40080 Training loss 0.09499097615480423 Validation loss 0.10219848155975342 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6225],\n",
      "        [0.3725]], device='mps:0')\n",
      "Iteration 40090 Training loss 0.09913749247789383 Validation loss 0.102165587246418 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3192],\n",
      "        [0.7308]], device='mps:0')\n",
      "Iteration 40100 Training loss 0.11288270354270935 Validation loss 0.10215363651514053 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.1811],\n",
      "        [0.6744]], device='mps:0')\n",
      "Iteration 40110 Training loss 0.10494659096002579 Validation loss 0.10215697437524796 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6483],\n",
      "        [0.5182]], device='mps:0')\n",
      "Iteration 40120 Training loss 0.11170768737792969 Validation loss 0.10213976353406906 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7503],\n",
      "        [0.5622]], device='mps:0')\n",
      "Iteration 40130 Training loss 0.10090550780296326 Validation loss 0.10211104899644852 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6360],\n",
      "        [0.5612]], device='mps:0')\n",
      "Iteration 40140 Training loss 0.09288869798183441 Validation loss 0.10211750119924545 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.7314],\n",
      "        [0.5638]], device='mps:0')\n",
      "Iteration 40150 Training loss 0.09926458448171616 Validation loss 0.10211440175771713 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5510],\n",
      "        [0.4592]], device='mps:0')\n",
      "Iteration 40160 Training loss 0.1032266765832901 Validation loss 0.10211949795484543 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4948],\n",
      "        [0.5803]], device='mps:0')\n",
      "Iteration 40170 Training loss 0.1019284576177597 Validation loss 0.10214711725711823 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4675],\n",
      "        [0.2229]], device='mps:0')\n",
      "Iteration 40180 Training loss 0.10819774121046066 Validation loss 0.1021215096116066 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6133],\n",
      "        [0.5828]], device='mps:0')\n",
      "Iteration 40190 Training loss 0.10344931483268738 Validation loss 0.10223567485809326 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.4908],\n",
      "        [0.3777]], device='mps:0')\n",
      "Iteration 40200 Training loss 0.09820087254047394 Validation loss 0.10215172171592712 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.7465],\n",
      "        [0.6675]], device='mps:0')\n",
      "Iteration 40210 Training loss 0.10783189535140991 Validation loss 0.10214634239673615 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3482],\n",
      "        [0.3671]], device='mps:0')\n",
      "Iteration 40220 Training loss 0.10371295362710953 Validation loss 0.10209403187036514 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3475],\n",
      "        [0.6359]], device='mps:0')\n",
      "Iteration 40230 Training loss 0.11232306063175201 Validation loss 0.10209407657384872 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5039],\n",
      "        [0.1922]], device='mps:0')\n",
      "Iteration 40240 Training loss 0.09854535013437271 Validation loss 0.1021011620759964 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.4615],\n",
      "        [0.5614]], device='mps:0')\n",
      "Iteration 40250 Training loss 0.09716277569532394 Validation loss 0.10211285948753357 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.1289],\n",
      "        [0.6551]], device='mps:0')\n",
      "Iteration 40260 Training loss 0.11003532260656357 Validation loss 0.10209561139345169 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.7020],\n",
      "        [0.3485]], device='mps:0')\n",
      "Iteration 40270 Training loss 0.10899175703525543 Validation loss 0.10210073739290237 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5898],\n",
      "        [0.6190]], device='mps:0')\n",
      "Iteration 40280 Training loss 0.10864578187465668 Validation loss 0.10213357955217361 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3092],\n",
      "        [0.5330]], device='mps:0')\n",
      "Iteration 40290 Training loss 0.097299724817276 Validation loss 0.10210245847702026 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7439],\n",
      "        [0.4603]], device='mps:0')\n",
      "Iteration 40300 Training loss 0.09674056619405746 Validation loss 0.10211396962404251 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6553],\n",
      "        [0.5064]], device='mps:0')\n",
      "Iteration 40310 Training loss 0.10153727233409882 Validation loss 0.10209222882986069 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5242],\n",
      "        [0.3027]], device='mps:0')\n",
      "Iteration 40320 Training loss 0.08652827888727188 Validation loss 0.102096788585186 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3604],\n",
      "        [0.4932]], device='mps:0')\n",
      "Iteration 40330 Training loss 0.09929756075143814 Validation loss 0.1020977646112442 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5013],\n",
      "        [0.3175]], device='mps:0')\n",
      "Iteration 40340 Training loss 0.11084665358066559 Validation loss 0.10207918286323547 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3578],\n",
      "        [0.5383]], device='mps:0')\n",
      "Iteration 40350 Training loss 0.1041339635848999 Validation loss 0.10207844525575638 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6620],\n",
      "        [0.5878]], device='mps:0')\n",
      "Iteration 40360 Training loss 0.09954019635915756 Validation loss 0.10210220515727997 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3194],\n",
      "        [0.6290]], device='mps:0')\n",
      "Iteration 40370 Training loss 0.10216418653726578 Validation loss 0.10209166258573532 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5479],\n",
      "        [0.1719]], device='mps:0')\n",
      "Iteration 40380 Training loss 0.10177432745695114 Validation loss 0.10207971930503845 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5525],\n",
      "        [0.4107]], device='mps:0')\n",
      "Iteration 40390 Training loss 0.10156543552875519 Validation loss 0.10207612812519073 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4795],\n",
      "        [0.5098]], device='mps:0')\n",
      "Iteration 40400 Training loss 0.10218887031078339 Validation loss 0.10207443684339523 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.2210],\n",
      "        [0.4876]], device='mps:0')\n",
      "Iteration 40410 Training loss 0.10927974432706833 Validation loss 0.10207640379667282 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3875],\n",
      "        [0.2817]], device='mps:0')\n",
      "Iteration 40420 Training loss 0.11369483917951584 Validation loss 0.10207844525575638 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5226],\n",
      "        [0.6509]], device='mps:0')\n",
      "Iteration 40430 Training loss 0.10503820329904556 Validation loss 0.1021278128027916 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6197],\n",
      "        [0.1541]], device='mps:0')\n",
      "Iteration 40440 Training loss 0.10076504200696945 Validation loss 0.10207460820674896 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4424],\n",
      "        [0.5739]], device='mps:0')\n",
      "Iteration 40450 Training loss 0.10698350518941879 Validation loss 0.10209236294031143 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3302],\n",
      "        [0.2564]], device='mps:0')\n",
      "Iteration 40460 Training loss 0.09873506426811218 Validation loss 0.10206683725118637 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5456],\n",
      "        [0.2578]], device='mps:0')\n",
      "Iteration 40470 Training loss 0.09051337093114853 Validation loss 0.1020767092704773 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4159],\n",
      "        [0.5539]], device='mps:0')\n",
      "Iteration 40480 Training loss 0.10146455466747284 Validation loss 0.10208415240049362 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2469],\n",
      "        [0.4190]], device='mps:0')\n",
      "Iteration 40490 Training loss 0.10935092717409134 Validation loss 0.10207365453243256 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4928],\n",
      "        [0.4384]], device='mps:0')\n",
      "Iteration 40500 Training loss 0.10210653394460678 Validation loss 0.10206380486488342 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3530],\n",
      "        [0.5460]], device='mps:0')\n",
      "Iteration 40510 Training loss 0.10545893758535385 Validation loss 0.10207407176494598 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5906],\n",
      "        [0.2047]], device='mps:0')\n",
      "Iteration 40520 Training loss 0.10177651047706604 Validation loss 0.10209060460329056 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3548],\n",
      "        [0.7000]], device='mps:0')\n",
      "Iteration 40530 Training loss 0.10018142312765121 Validation loss 0.10208222270011902 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2926],\n",
      "        [0.6515]], device='mps:0')\n",
      "Iteration 40540 Training loss 0.10989360511302948 Validation loss 0.10210800170898438 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7282],\n",
      "        [0.1822]], device='mps:0')\n",
      "Iteration 40550 Training loss 0.1050073578953743 Validation loss 0.1020921990275383 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6481],\n",
      "        [0.6921]], device='mps:0')\n",
      "Iteration 40560 Training loss 0.08921167254447937 Validation loss 0.10206753760576248 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3235],\n",
      "        [0.6423]], device='mps:0')\n",
      "Iteration 40570 Training loss 0.09894352406263351 Validation loss 0.10206187516450882 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.2865],\n",
      "        [0.6521]], device='mps:0')\n",
      "Iteration 40580 Training loss 0.0930936336517334 Validation loss 0.10205719619989395 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6811],\n",
      "        [0.5708]], device='mps:0')\n",
      "Iteration 40590 Training loss 0.10225410014390945 Validation loss 0.10206179320812225 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5986],\n",
      "        [0.6819]], device='mps:0')\n",
      "Iteration 40600 Training loss 0.10049030184745789 Validation loss 0.10206151008605957 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5404],\n",
      "        [0.4768]], device='mps:0')\n",
      "Iteration 40610 Training loss 0.11280107498168945 Validation loss 0.1020585373044014 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5481],\n",
      "        [0.4933]], device='mps:0')\n",
      "Iteration 40620 Training loss 0.09542747586965561 Validation loss 0.10209625959396362 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6619],\n",
      "        [0.5448]], device='mps:0')\n",
      "Iteration 40630 Training loss 0.10951124131679535 Validation loss 0.10211456567049026 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5653],\n",
      "        [0.2466]], device='mps:0')\n",
      "Iteration 40640 Training loss 0.09496764093637466 Validation loss 0.10208774358034134 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4957],\n",
      "        [0.2537]], device='mps:0')\n",
      "Iteration 40650 Training loss 0.0995391458272934 Validation loss 0.10207581520080566 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4190],\n",
      "        [0.5528]], device='mps:0')\n",
      "Iteration 40660 Training loss 0.09997186809778214 Validation loss 0.10210228711366653 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6704],\n",
      "        [0.4581]], device='mps:0')\n",
      "Iteration 40670 Training loss 0.1062147319316864 Validation loss 0.10205748677253723 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3900],\n",
      "        [0.4859]], device='mps:0')\n",
      "Iteration 40680 Training loss 0.10624960064888 Validation loss 0.10203569382429123 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6403],\n",
      "        [0.4573]], device='mps:0')\n",
      "Iteration 40690 Training loss 0.09916757047176361 Validation loss 0.10203587263822556 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5258],\n",
      "        [0.6260]], device='mps:0')\n",
      "Iteration 40700 Training loss 0.10263904184103012 Validation loss 0.1020323857665062 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.2889],\n",
      "        [0.2124]], device='mps:0')\n",
      "Iteration 40710 Training loss 0.09566114097833633 Validation loss 0.10202591121196747 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3593],\n",
      "        [0.4168]], device='mps:0')\n",
      "Iteration 40720 Training loss 0.09485149383544922 Validation loss 0.10203589498996735 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6404],\n",
      "        [0.6963]], device='mps:0')\n",
      "Iteration 40730 Training loss 0.10149160772562027 Validation loss 0.1020423173904419 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5167],\n",
      "        [0.4943]], device='mps:0')\n",
      "Iteration 40740 Training loss 0.10331468284130096 Validation loss 0.10206674039363861 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3090],\n",
      "        [0.2482]], device='mps:0')\n",
      "Iteration 40750 Training loss 0.11348340660333633 Validation loss 0.10210811346769333 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.7127],\n",
      "        [0.2052]], device='mps:0')\n",
      "Iteration 40760 Training loss 0.09669170528650284 Validation loss 0.1020575687289238 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6715],\n",
      "        [0.3583]], device='mps:0')\n",
      "Iteration 40770 Training loss 0.10251503437757492 Validation loss 0.10202538222074509 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5559],\n",
      "        [0.4287]], device='mps:0')\n",
      "Iteration 40780 Training loss 0.1105576679110527 Validation loss 0.1020238846540451 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.3996],\n",
      "        [0.4345]], device='mps:0')\n",
      "Iteration 40790 Training loss 0.08900070190429688 Validation loss 0.1020248755812645 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4794],\n",
      "        [0.2698]], device='mps:0')\n",
      "Iteration 40800 Training loss 0.10589853674173355 Validation loss 0.10206466913223267 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3767],\n",
      "        [0.3905]], device='mps:0')\n",
      "Iteration 40810 Training loss 0.09505306929349899 Validation loss 0.10213063657283783 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2496],\n",
      "        [0.7074]], device='mps:0')\n",
      "Iteration 40820 Training loss 0.09558164328336716 Validation loss 0.10208944231271744 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.1393],\n",
      "        [0.4644]], device='mps:0')\n",
      "Iteration 40830 Training loss 0.09542486071586609 Validation loss 0.10208024084568024 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5156],\n",
      "        [0.4891]], device='mps:0')\n",
      "Iteration 40840 Training loss 0.0983496680855751 Validation loss 0.10209013521671295 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.1998],\n",
      "        [0.6608]], device='mps:0')\n",
      "Iteration 40850 Training loss 0.09876351803541183 Validation loss 0.10206465423107147 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6164],\n",
      "        [0.7212]], device='mps:0')\n",
      "Iteration 40860 Training loss 0.10243794322013855 Validation loss 0.10208966583013535 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4330],\n",
      "        [0.6536]], device='mps:0')\n",
      "Iteration 40870 Training loss 0.1034703329205513 Validation loss 0.10206247866153717 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.2556],\n",
      "        [0.6706]], device='mps:0')\n",
      "Iteration 40880 Training loss 0.09911670535802841 Validation loss 0.10207729786634445 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6035],\n",
      "        [0.6027]], device='mps:0')\n",
      "Iteration 40890 Training loss 0.09750966727733612 Validation loss 0.10207556933164597 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4447],\n",
      "        [0.5284]], device='mps:0')\n",
      "Iteration 40900 Training loss 0.10471609234809875 Validation loss 0.10205107927322388 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4446],\n",
      "        [0.6746]], device='mps:0')\n",
      "Iteration 40910 Training loss 0.10468462854623795 Validation loss 0.10209183394908905 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6785],\n",
      "        [0.5513]], device='mps:0')\n",
      "Iteration 40920 Training loss 0.09930448234081268 Validation loss 0.10208890587091446 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4785],\n",
      "        [0.5304]], device='mps:0')\n",
      "Iteration 40930 Training loss 0.10297683626413345 Validation loss 0.10214870423078537 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2837],\n",
      "        [0.6034]], device='mps:0')\n",
      "Iteration 40940 Training loss 0.11399474740028381 Validation loss 0.10206347703933716 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5904],\n",
      "        [0.6112]], device='mps:0')\n",
      "Iteration 40950 Training loss 0.10307584702968597 Validation loss 0.10204475373029709 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4785],\n",
      "        [0.6481]], device='mps:0')\n",
      "Iteration 40960 Training loss 0.09663137048482895 Validation loss 0.10202618688344955 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2786],\n",
      "        [0.3872]], device='mps:0')\n",
      "Iteration 40970 Training loss 0.10221356898546219 Validation loss 0.10200192779302597 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4703],\n",
      "        [0.4508]], device='mps:0')\n",
      "Iteration 40980 Training loss 0.10608058422803879 Validation loss 0.10200335830450058 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6110],\n",
      "        [0.2357]], device='mps:0')\n",
      "Iteration 40990 Training loss 0.09304752200841904 Validation loss 0.10200078040361404 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4558],\n",
      "        [0.4549]], device='mps:0')\n",
      "Iteration 41000 Training loss 0.10074388980865479 Validation loss 0.10200865566730499 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3316],\n",
      "        [0.7388]], device='mps:0')\n",
      "Iteration 41010 Training loss 0.10386385023593903 Validation loss 0.10200053453445435 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4028],\n",
      "        [0.2552]], device='mps:0')\n",
      "Iteration 41020 Training loss 0.1065056174993515 Validation loss 0.10200055688619614 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.1705],\n",
      "        [0.3956]], device='mps:0')\n",
      "Iteration 41030 Training loss 0.10825306922197342 Validation loss 0.10199954360723495 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4597],\n",
      "        [0.6530]], device='mps:0')\n",
      "Iteration 41040 Training loss 0.10760259628295898 Validation loss 0.10201459378004074 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4380],\n",
      "        [0.6661]], device='mps:0')\n",
      "Iteration 41050 Training loss 0.10033059865236282 Validation loss 0.10203015059232712 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3939],\n",
      "        [0.1109]], device='mps:0')\n",
      "Iteration 41060 Training loss 0.11692685633897781 Validation loss 0.10200300067663193 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5610],\n",
      "        [0.4585]], device='mps:0')\n",
      "Iteration 41070 Training loss 0.09528962522745132 Validation loss 0.10206522047519684 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5964],\n",
      "        [0.4860]], device='mps:0')\n",
      "Iteration 41080 Training loss 0.10363944619894028 Validation loss 0.10200487077236176 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4029],\n",
      "        [0.5137]], device='mps:0')\n",
      "Iteration 41090 Training loss 0.10384759306907654 Validation loss 0.1019912138581276 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5881],\n",
      "        [0.1321]], device='mps:0')\n",
      "Iteration 41100 Training loss 0.0957973301410675 Validation loss 0.10199149698019028 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.1052],\n",
      "        [0.6133]], device='mps:0')\n",
      "Iteration 41110 Training loss 0.11667295545339584 Validation loss 0.10201241075992584 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3722],\n",
      "        [0.4975]], device='mps:0')\n",
      "Iteration 41120 Training loss 0.11548332870006561 Validation loss 0.10200103372335434 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5162],\n",
      "        [0.5865]], device='mps:0')\n",
      "Iteration 41130 Training loss 0.10922256112098694 Validation loss 0.10204268246889114 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.2990],\n",
      "        [0.7178]], device='mps:0')\n",
      "Iteration 41140 Training loss 0.11075883358716965 Validation loss 0.10205592960119247 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5551],\n",
      "        [0.6504]], device='mps:0')\n",
      "Iteration 41150 Training loss 0.11382826417684555 Validation loss 0.10208076238632202 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4826],\n",
      "        [0.5842]], device='mps:0')\n",
      "Iteration 41160 Training loss 0.11572049558162689 Validation loss 0.1020747721195221 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5491],\n",
      "        [0.4795]], device='mps:0')\n",
      "Iteration 41170 Training loss 0.11324241757392883 Validation loss 0.10201935470104218 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3063],\n",
      "        [0.5221]], device='mps:0')\n",
      "Iteration 41180 Training loss 0.11319419741630554 Validation loss 0.10202096402645111 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3571],\n",
      "        [0.7589]], device='mps:0')\n",
      "Iteration 41190 Training loss 0.0974939614534378 Validation loss 0.1020151823759079 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6647],\n",
      "        [0.4988]], device='mps:0')\n",
      "Iteration 41200 Training loss 0.10619773715734482 Validation loss 0.10198956727981567 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5701],\n",
      "        [0.5152]], device='mps:0')\n",
      "Iteration 41210 Training loss 0.10780318826436996 Validation loss 0.1019875705242157 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3895],\n",
      "        [0.5922]], device='mps:0')\n",
      "Iteration 41220 Training loss 0.0972898080945015 Validation loss 0.1019805446267128 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4648],\n",
      "        [0.7046]], device='mps:0')\n",
      "Iteration 41230 Training loss 0.10224223136901855 Validation loss 0.10197116434574127 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3774],\n",
      "        [0.5097]], device='mps:0')\n",
      "Iteration 41240 Training loss 0.09988486766815186 Validation loss 0.10197079926729202 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.6631],\n",
      "        [0.1389]], device='mps:0')\n",
      "Iteration 41250 Training loss 0.10747130960226059 Validation loss 0.10197051614522934 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5986],\n",
      "        [0.4997]], device='mps:0')\n",
      "Iteration 41260 Training loss 0.10408931225538254 Validation loss 0.10196849703788757 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.7138],\n",
      "        [0.4186]], device='mps:0')\n",
      "Iteration 41270 Training loss 0.1018553152680397 Validation loss 0.10197148472070694 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5195],\n",
      "        [0.5710]], device='mps:0')\n",
      "Iteration 41280 Training loss 0.09795662760734558 Validation loss 0.10196523368358612 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5122],\n",
      "        [0.1130]], device='mps:0')\n",
      "Iteration 41290 Training loss 0.10111503303050995 Validation loss 0.10197775810956955 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.1243],\n",
      "        [0.6163]], device='mps:0')\n",
      "Iteration 41300 Training loss 0.10956922173500061 Validation loss 0.10196699947118759 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6395],\n",
      "        [0.4263]], device='mps:0')\n",
      "Iteration 41310 Training loss 0.11311708390712738 Validation loss 0.1019681990146637 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5937],\n",
      "        [0.3598]], device='mps:0')\n",
      "Iteration 41320 Training loss 0.1042284145951271 Validation loss 0.10195973515510559 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5861],\n",
      "        [0.4577]], device='mps:0')\n",
      "Iteration 41330 Training loss 0.10267797112464905 Validation loss 0.10196255892515182 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5991],\n",
      "        [0.5765]], device='mps:0')\n",
      "Iteration 41340 Training loss 0.09884324669837952 Validation loss 0.10197132080793381 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5125],\n",
      "        [0.3034]], device='mps:0')\n",
      "Iteration 41350 Training loss 0.11670247465372086 Validation loss 0.10197630524635315 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5677],\n",
      "        [0.6121]], device='mps:0')\n",
      "Iteration 41360 Training loss 0.08969982713460922 Validation loss 0.10196640342473984 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4739],\n",
      "        [0.6435]], device='mps:0')\n",
      "Iteration 41370 Training loss 0.10227043926715851 Validation loss 0.1019725650548935 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6897],\n",
      "        [0.4847]], device='mps:0')\n",
      "Iteration 41380 Training loss 0.10663888603448868 Validation loss 0.10196426510810852 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6524],\n",
      "        [0.4949]], device='mps:0')\n",
      "Iteration 41390 Training loss 0.10479903966188431 Validation loss 0.1019853875041008 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2166],\n",
      "        [0.6662]], device='mps:0')\n",
      "Iteration 41400 Training loss 0.10861090570688248 Validation loss 0.10198388248682022 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7852],\n",
      "        [0.4777]], device='mps:0')\n",
      "Iteration 41410 Training loss 0.11569573730230331 Validation loss 0.10196762531995773 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6385],\n",
      "        [0.5737]], device='mps:0')\n",
      "Iteration 41420 Training loss 0.11043849587440491 Validation loss 0.10199170559644699 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4977],\n",
      "        [0.6448]], device='mps:0')\n",
      "Iteration 41430 Training loss 0.09629654884338379 Validation loss 0.10196403414011002 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3647],\n",
      "        [0.6433]], device='mps:0')\n",
      "Iteration 41440 Training loss 0.1054731085896492 Validation loss 0.10198138654232025 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5125],\n",
      "        [0.2439]], device='mps:0')\n",
      "Iteration 41450 Training loss 0.1041158065199852 Validation loss 0.10197955369949341 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5183],\n",
      "        [0.4839]], device='mps:0')\n",
      "Iteration 41460 Training loss 0.0966002568602562 Validation loss 0.10194955766201019 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5907],\n",
      "        [0.7260]], device='mps:0')\n",
      "Iteration 41470 Training loss 0.09048938751220703 Validation loss 0.10193520039319992 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4405],\n",
      "        [0.7248]], device='mps:0')\n",
      "Iteration 41480 Training loss 0.10723739117383957 Validation loss 0.10194439440965652 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5686],\n",
      "        [0.6443]], device='mps:0')\n",
      "Iteration 41490 Training loss 0.10707855969667435 Validation loss 0.10192941874265671 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.1629],\n",
      "        [0.1614]], device='mps:0')\n",
      "Iteration 41500 Training loss 0.10347616672515869 Validation loss 0.10192741453647614 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.2889],\n",
      "        [0.7197]], device='mps:0')\n",
      "Iteration 41510 Training loss 0.09902805089950562 Validation loss 0.1019275113940239 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5217],\n",
      "        [0.3633]], device='mps:0')\n",
      "Iteration 41520 Training loss 0.09353087842464447 Validation loss 0.10192641615867615 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.7237],\n",
      "        [0.6270]], device='mps:0')\n",
      "Iteration 41530 Training loss 0.11361820250749588 Validation loss 0.10192418098449707 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4109],\n",
      "        [0.5351]], device='mps:0')\n",
      "Iteration 41540 Training loss 0.099783755838871 Validation loss 0.10192298889160156 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3644],\n",
      "        [0.6569]], device='mps:0')\n",
      "Iteration 41550 Training loss 0.09622593969106674 Validation loss 0.10192238539457321 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5105],\n",
      "        [0.4013]], device='mps:0')\n",
      "Iteration 41560 Training loss 0.09740357100963593 Validation loss 0.10192043334245682 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.2302],\n",
      "        [0.3721]], device='mps:0')\n",
      "Iteration 41570 Training loss 0.09886544197797775 Validation loss 0.1019275113940239 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5999],\n",
      "        [0.5915]], device='mps:0')\n",
      "Iteration 41580 Training loss 0.111330047249794 Validation loss 0.10192795097827911 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6545],\n",
      "        [0.5958]], device='mps:0')\n",
      "Iteration 41590 Training loss 0.10628136992454529 Validation loss 0.1019328162074089 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3850],\n",
      "        [0.7304]], device='mps:0')\n",
      "Iteration 41600 Training loss 0.1049729734659195 Validation loss 0.1019677147269249 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2721],\n",
      "        [0.4333]], device='mps:0')\n",
      "Iteration 41610 Training loss 0.1007281020283699 Validation loss 0.10192844271659851 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4014],\n",
      "        [0.1353]], device='mps:0')\n",
      "Iteration 41620 Training loss 0.10391794145107269 Validation loss 0.10193666070699692 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2621],\n",
      "        [0.6558]], device='mps:0')\n",
      "Iteration 41630 Training loss 0.10410992056131363 Validation loss 0.10195063799619675 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6575],\n",
      "        [0.6639]], device='mps:0')\n",
      "Iteration 41640 Training loss 0.1069420799612999 Validation loss 0.10195252299308777 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2639],\n",
      "        [0.5647]], device='mps:0')\n",
      "Iteration 41650 Training loss 0.0997946560382843 Validation loss 0.10198892652988434 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4960],\n",
      "        [0.5074]], device='mps:0')\n",
      "Iteration 41660 Training loss 0.1141320988535881 Validation loss 0.10204881429672241 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6388],\n",
      "        [0.3093]], device='mps:0')\n",
      "Iteration 41670 Training loss 0.10277792066335678 Validation loss 0.1020130068063736 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6088],\n",
      "        [0.5806]], device='mps:0')\n",
      "Iteration 41680 Training loss 0.0922383964061737 Validation loss 0.10195140540599823 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6033],\n",
      "        [0.2493]], device='mps:0')\n",
      "Iteration 41690 Training loss 0.10037887096405029 Validation loss 0.10193619132041931 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3041],\n",
      "        [0.6433]], device='mps:0')\n",
      "Iteration 41700 Training loss 0.09881336241960526 Validation loss 0.10194659233093262 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7325],\n",
      "        [0.5816]], device='mps:0')\n",
      "Iteration 41710 Training loss 0.10428575426340103 Validation loss 0.10192203521728516 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4457],\n",
      "        [0.5874]], device='mps:0')\n",
      "Iteration 41720 Training loss 0.09834903478622437 Validation loss 0.10191522538661957 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4535],\n",
      "        [0.5415]], device='mps:0')\n",
      "Iteration 41730 Training loss 0.10689514130353928 Validation loss 0.10190654546022415 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4943],\n",
      "        [0.5575]], device='mps:0')\n",
      "Iteration 41740 Training loss 0.10572343319654465 Validation loss 0.1019032895565033 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4496],\n",
      "        [0.6886]], device='mps:0')\n",
      "Iteration 41750 Training loss 0.09550496190786362 Validation loss 0.10190487653017044 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4936],\n",
      "        [0.4050]], device='mps:0')\n",
      "Iteration 41760 Training loss 0.09969455748796463 Validation loss 0.10189354419708252 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5225],\n",
      "        [0.5187]], device='mps:0')\n",
      "Iteration 41770 Training loss 0.09809175133705139 Validation loss 0.1018853411078453 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.6913],\n",
      "        [0.5896]], device='mps:0')\n",
      "Iteration 41780 Training loss 0.10532816499471664 Validation loss 0.10188443213701248 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.3066],\n",
      "        [0.3140]], device='mps:0')\n",
      "Iteration 41790 Training loss 0.09109211713075638 Validation loss 0.10188260674476624 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5407],\n",
      "        [0.2990]], device='mps:0')\n",
      "Iteration 41800 Training loss 0.10504356771707535 Validation loss 0.10188443213701248 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5700],\n",
      "        [0.3185]], device='mps:0')\n",
      "Iteration 41810 Training loss 0.10609028488397598 Validation loss 0.10189978778362274 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6286],\n",
      "        [0.5465]], device='mps:0')\n",
      "Iteration 41820 Training loss 0.10838387161493301 Validation loss 0.10190335661172867 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.7487],\n",
      "        [0.6882]], device='mps:0')\n",
      "Iteration 41830 Training loss 0.09593851119279861 Validation loss 0.10187756270170212 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4187],\n",
      "        [0.3820]], device='mps:0')\n",
      "Iteration 41840 Training loss 0.10794191062450409 Validation loss 0.10187456011772156 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.4866],\n",
      "        [0.4395]], device='mps:0')\n",
      "Iteration 41850 Training loss 0.11009427905082703 Validation loss 0.10188107937574387 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.3331],\n",
      "        [0.6387]], device='mps:0')\n",
      "Iteration 41860 Training loss 0.09768357127904892 Validation loss 0.10188480466604233 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5885],\n",
      "        [0.5913]], device='mps:0')\n",
      "Iteration 41870 Training loss 0.09814927726984024 Validation loss 0.10188281536102295 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5515],\n",
      "        [0.6675]], device='mps:0')\n",
      "Iteration 41880 Training loss 0.10750383138656616 Validation loss 0.10187873989343643 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.7000],\n",
      "        [0.5409]], device='mps:0')\n",
      "Iteration 41890 Training loss 0.11568206548690796 Validation loss 0.10186843574047089 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5367],\n",
      "        [0.4596]], device='mps:0')\n",
      "Iteration 41900 Training loss 0.10428508371114731 Validation loss 0.1018691286444664 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2262],\n",
      "        [0.4530]], device='mps:0')\n",
      "Iteration 41910 Training loss 0.09659703820943832 Validation loss 0.10186838358640671 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2375],\n",
      "        [0.6167]], device='mps:0')\n",
      "Iteration 41920 Training loss 0.09449216723442078 Validation loss 0.10187005996704102 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4497],\n",
      "        [0.6476]], device='mps:0')\n",
      "Iteration 41930 Training loss 0.0977223664522171 Validation loss 0.10186362266540527 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5368],\n",
      "        [0.6141]], device='mps:0')\n",
      "Iteration 41940 Training loss 0.1112210750579834 Validation loss 0.10186786204576492 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.2614],\n",
      "        [0.6525]], device='mps:0')\n",
      "Iteration 41950 Training loss 0.10087568312883377 Validation loss 0.10185898840427399 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.7103],\n",
      "        [0.1772]], device='mps:0')\n",
      "Iteration 41960 Training loss 0.12554548680782318 Validation loss 0.10186218470335007 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6284],\n",
      "        [0.7581]], device='mps:0')\n",
      "Iteration 41970 Training loss 0.10685496032238007 Validation loss 0.10186274349689484 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4752],\n",
      "        [0.5996]], device='mps:0')\n",
      "Iteration 41980 Training loss 0.10200897604227066 Validation loss 0.1019296869635582 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7695],\n",
      "        [0.1122]], device='mps:0')\n",
      "Iteration 41990 Training loss 0.10188838094472885 Validation loss 0.10196685045957565 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4761],\n",
      "        [0.6519]], device='mps:0')\n",
      "Iteration 42000 Training loss 0.09887145459651947 Validation loss 0.10190349817276001 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5250],\n",
      "        [0.2733]], device='mps:0')\n",
      "Iteration 42010 Training loss 0.11301912367343903 Validation loss 0.10188374668359756 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.7169],\n",
      "        [0.3263]], device='mps:0')\n",
      "Iteration 42020 Training loss 0.10710997134447098 Validation loss 0.10185745358467102 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3103],\n",
      "        [0.4001]], device='mps:0')\n",
      "Iteration 42030 Training loss 0.10143765807151794 Validation loss 0.10187137126922607 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4136],\n",
      "        [0.7117]], device='mps:0')\n",
      "Iteration 42040 Training loss 0.10569148510694504 Validation loss 0.1018839105963707 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3121],\n",
      "        [0.6453]], device='mps:0')\n",
      "Iteration 42050 Training loss 0.10269883275032043 Validation loss 0.10193130373954773 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4261],\n",
      "        [0.7191]], device='mps:0')\n",
      "Iteration 42060 Training loss 0.10808386653661728 Validation loss 0.10192122310400009 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.7276],\n",
      "        [0.5301]], device='mps:0')\n",
      "Iteration 42070 Training loss 0.10875676572322845 Validation loss 0.10187725722789764 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4943],\n",
      "        [0.6132]], device='mps:0')\n",
      "Iteration 42080 Training loss 0.09897338598966599 Validation loss 0.10186141729354858 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5510],\n",
      "        [0.6728]], device='mps:0')\n",
      "Iteration 42090 Training loss 0.10085834562778473 Validation loss 0.10184772312641144 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4560],\n",
      "        [0.3843]], device='mps:0')\n",
      "Iteration 42100 Training loss 0.10599559545516968 Validation loss 0.10187765210866928 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.7072],\n",
      "        [0.5626]], device='mps:0')\n",
      "Iteration 42110 Training loss 0.10080884397029877 Validation loss 0.10184113681316376 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.7323],\n",
      "        [0.5960]], device='mps:0')\n",
      "Iteration 42120 Training loss 0.11332454532384872 Validation loss 0.10185173153877258 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.1801],\n",
      "        [0.6224]], device='mps:0')\n",
      "Iteration 42130 Training loss 0.09482250362634659 Validation loss 0.10185698419809341 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2935],\n",
      "        [0.3819]], device='mps:0')\n",
      "Iteration 42140 Training loss 0.11031175404787064 Validation loss 0.10185937583446503 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7301],\n",
      "        [0.4622]], device='mps:0')\n",
      "Iteration 42150 Training loss 0.1045076921582222 Validation loss 0.10185214877128601 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6966],\n",
      "        [0.5253]], device='mps:0')\n",
      "Iteration 42160 Training loss 0.09583780169487 Validation loss 0.10186577588319778 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6592],\n",
      "        [0.7047]], device='mps:0')\n",
      "Iteration 42170 Training loss 0.10773226618766785 Validation loss 0.10185171663761139 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5346],\n",
      "        [0.4873]], device='mps:0')\n",
      "Iteration 42180 Training loss 0.1200316995382309 Validation loss 0.10185431689023972 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7206],\n",
      "        [0.4180]], device='mps:0')\n",
      "Iteration 42190 Training loss 0.09200229495763779 Validation loss 0.10185208171606064 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5161],\n",
      "        [0.5392]], device='mps:0')\n",
      "Iteration 42200 Training loss 0.10551157593727112 Validation loss 0.10183992236852646 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6595],\n",
      "        [0.5821]], device='mps:0')\n",
      "Iteration 42210 Training loss 0.11844547092914581 Validation loss 0.10185888409614563 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4570],\n",
      "        [0.2190]], device='mps:0')\n",
      "Iteration 42220 Training loss 0.09718658030033112 Validation loss 0.10185228288173676 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3920],\n",
      "        [0.2928]], device='mps:0')\n",
      "Iteration 42230 Training loss 0.09285593032836914 Validation loss 0.10183198750019073 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.7308],\n",
      "        [0.2876]], device='mps:0')\n",
      "Iteration 42240 Training loss 0.09084386378526688 Validation loss 0.10184599459171295 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4424],\n",
      "        [0.2551]], device='mps:0')\n",
      "Iteration 42250 Training loss 0.10481999814510345 Validation loss 0.10186206549406052 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4106],\n",
      "        [0.6363]], device='mps:0')\n",
      "Iteration 42260 Training loss 0.10466039180755615 Validation loss 0.10186167061328888 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6279],\n",
      "        [0.5840]], device='mps:0')\n",
      "Iteration 42270 Training loss 0.09325587004423141 Validation loss 0.10187575966119766 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6234],\n",
      "        [0.6208]], device='mps:0')\n",
      "Iteration 42280 Training loss 0.11596628278493881 Validation loss 0.10190138220787048 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3558],\n",
      "        [0.6670]], device='mps:0')\n",
      "Iteration 42290 Training loss 0.09748323261737823 Validation loss 0.10187900066375732 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6137],\n",
      "        [0.6534]], device='mps:0')\n",
      "Iteration 42300 Training loss 0.10127200931310654 Validation loss 0.10185642540454865 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6023],\n",
      "        [0.5339]], device='mps:0')\n",
      "Iteration 42310 Training loss 0.0971032977104187 Validation loss 0.101849265396595 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5005],\n",
      "        [0.5195]], device='mps:0')\n",
      "Iteration 42320 Training loss 0.10862031579017639 Validation loss 0.1018357202410698 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3173],\n",
      "        [0.5553]], device='mps:0')\n",
      "Iteration 42330 Training loss 0.11010287702083588 Validation loss 0.10188741236925125 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.1630],\n",
      "        [0.4214]], device='mps:0')\n",
      "Iteration 42340 Training loss 0.10522977262735367 Validation loss 0.10191628336906433 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4678],\n",
      "        [0.1924]], device='mps:0')\n",
      "Iteration 42350 Training loss 0.1125565767288208 Validation loss 0.10195691883563995 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4417],\n",
      "        [0.5294]], device='mps:0')\n",
      "Iteration 42360 Training loss 0.102808378636837 Validation loss 0.10191855579614639 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5608],\n",
      "        [0.6407]], device='mps:0')\n",
      "Iteration 42370 Training loss 0.10944068431854248 Validation loss 0.10188210010528564 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6644],\n",
      "        [0.2224]], device='mps:0')\n",
      "Iteration 42380 Training loss 0.10065843164920807 Validation loss 0.1018824353814125 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5635],\n",
      "        [0.5370]], device='mps:0')\n",
      "Iteration 42390 Training loss 0.09645254164934158 Validation loss 0.10186462849378586 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5329],\n",
      "        [0.4966]], device='mps:0')\n",
      "Iteration 42400 Training loss 0.11343736201524734 Validation loss 0.10188238322734833 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5433],\n",
      "        [0.4359]], device='mps:0')\n",
      "Iteration 42410 Training loss 0.10701015591621399 Validation loss 0.101891428232193 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2551],\n",
      "        [0.5805]], device='mps:0')\n",
      "Iteration 42420 Training loss 0.09803465008735657 Validation loss 0.10188420861959457 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4892],\n",
      "        [0.6659]], device='mps:0')\n",
      "Iteration 42430 Training loss 0.10641691088676453 Validation loss 0.10186245292425156 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6970],\n",
      "        [0.6401]], device='mps:0')\n",
      "Iteration 42440 Training loss 0.10823339968919754 Validation loss 0.10182539373636246 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6249],\n",
      "        [0.6151]], device='mps:0')\n",
      "Iteration 42450 Training loss 0.10805433243513107 Validation loss 0.10182594507932663 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5921],\n",
      "        [0.5484]], device='mps:0')\n",
      "Iteration 42460 Training loss 0.10565921664237976 Validation loss 0.1018378883600235 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2180],\n",
      "        [0.5535]], device='mps:0')\n",
      "Iteration 42470 Training loss 0.09390092641115189 Validation loss 0.101804718375206 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5814],\n",
      "        [0.2958]], device='mps:0')\n",
      "Iteration 42480 Training loss 0.09138057380914688 Validation loss 0.10180287808179855 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5740],\n",
      "        [0.4729]], device='mps:0')\n",
      "Iteration 42490 Training loss 0.106388159096241 Validation loss 0.10179734975099564 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5493],\n",
      "        [0.4404]], device='mps:0')\n",
      "Iteration 42500 Training loss 0.09501778334379196 Validation loss 0.10180432349443436 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4442],\n",
      "        [0.7118]], device='mps:0')\n",
      "Iteration 42510 Training loss 0.10862507671117783 Validation loss 0.1017959788441658 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.2795],\n",
      "        [0.5030]], device='mps:0')\n",
      "Iteration 42520 Training loss 0.09990008920431137 Validation loss 0.1017976626753807 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6355],\n",
      "        [0.3931]], device='mps:0')\n",
      "Iteration 42530 Training loss 0.11000731587409973 Validation loss 0.10181736201047897 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5486],\n",
      "        [0.7552]], device='mps:0')\n",
      "Iteration 42540 Training loss 0.10202547162771225 Validation loss 0.10180440545082092 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6779],\n",
      "        [0.1007]], device='mps:0')\n",
      "Iteration 42550 Training loss 0.10592998564243317 Validation loss 0.10179270058870316 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5509],\n",
      "        [0.4503]], device='mps:0')\n",
      "Iteration 42560 Training loss 0.09711480885744095 Validation loss 0.10179063677787781 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5410],\n",
      "        [0.6254]], device='mps:0')\n",
      "Iteration 42570 Training loss 0.10060659050941467 Validation loss 0.1017967090010643 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.1938],\n",
      "        [0.3502]], device='mps:0')\n",
      "Iteration 42580 Training loss 0.10607744008302689 Validation loss 0.1018008142709732 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5947],\n",
      "        [0.3181]], device='mps:0')\n",
      "Iteration 42590 Training loss 0.10507509112358093 Validation loss 0.10178908705711365 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6398],\n",
      "        [0.6185]], device='mps:0')\n",
      "Iteration 42600 Training loss 0.0964149460196495 Validation loss 0.10179054737091064 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5015],\n",
      "        [0.6962]], device='mps:0')\n",
      "Iteration 42610 Training loss 0.11908549815416336 Validation loss 0.10178613662719727 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4226],\n",
      "        [0.2053]], device='mps:0')\n",
      "Iteration 42620 Training loss 0.10502751916646957 Validation loss 0.10178849846124649 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4897],\n",
      "        [0.6363]], device='mps:0')\n",
      "Iteration 42630 Training loss 0.09546346217393875 Validation loss 0.1017908975481987 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5670],\n",
      "        [0.5096]], device='mps:0')\n",
      "Iteration 42640 Training loss 0.09985906630754471 Validation loss 0.10179278254508972 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3768],\n",
      "        [0.4580]], device='mps:0')\n",
      "Iteration 42650 Training loss 0.09855785220861435 Validation loss 0.10181067138910294 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2630],\n",
      "        [0.5237]], device='mps:0')\n",
      "Iteration 42660 Training loss 0.09388070553541183 Validation loss 0.10178922116756439 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.2822],\n",
      "        [0.6000]], device='mps:0')\n",
      "Iteration 42670 Training loss 0.11740531772375107 Validation loss 0.10177260637283325 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.4015],\n",
      "        [0.6160]], device='mps:0')\n",
      "Iteration 42680 Training loss 0.09859992563724518 Validation loss 0.1017698124051094 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5377],\n",
      "        [0.6965]], device='mps:0')\n",
      "Iteration 42690 Training loss 0.1066029816865921 Validation loss 0.10178368538618088 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6204],\n",
      "        [0.5483]], device='mps:0')\n",
      "Iteration 42700 Training loss 0.12160097807645798 Validation loss 0.10176730155944824 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6264],\n",
      "        [0.3722]], device='mps:0')\n",
      "Iteration 42710 Training loss 0.09786767512559891 Validation loss 0.10176564753055573 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5883],\n",
      "        [0.6017]], device='mps:0')\n",
      "Iteration 42720 Training loss 0.1090599000453949 Validation loss 0.1017700806260109 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6537],\n",
      "        [0.4902]], device='mps:0')\n",
      "Iteration 42730 Training loss 0.09490606933832169 Validation loss 0.101771779358387 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5371],\n",
      "        [0.7174]], device='mps:0')\n",
      "Iteration 42740 Training loss 0.09728287160396576 Validation loss 0.10184504836797714 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4893],\n",
      "        [0.3341]], device='mps:0')\n",
      "Iteration 42750 Training loss 0.1052488461136818 Validation loss 0.1018160730600357 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4460],\n",
      "        [0.4704]], device='mps:0')\n",
      "Iteration 42760 Training loss 0.11280985921621323 Validation loss 0.10178425163030624 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2728],\n",
      "        [0.4020]], device='mps:0')\n",
      "Iteration 42770 Training loss 0.119298055768013 Validation loss 0.10176371037960052 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3450],\n",
      "        [0.4584]], device='mps:0')\n",
      "Iteration 42780 Training loss 0.10906961560249329 Validation loss 0.10175804048776627 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5386],\n",
      "        [0.7339]], device='mps:0')\n",
      "Iteration 42790 Training loss 0.08903714269399643 Validation loss 0.10175395011901855 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.4992],\n",
      "        [0.5805]], device='mps:0')\n",
      "Iteration 42800 Training loss 0.09748237580060959 Validation loss 0.10175347328186035 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3518],\n",
      "        [0.4843]], device='mps:0')\n",
      "Iteration 42810 Training loss 0.09708282351493835 Validation loss 0.10174991190433502 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.5475],\n",
      "        [0.6343]], device='mps:0')\n",
      "Iteration 42820 Training loss 0.11204735189676285 Validation loss 0.10175829380750656 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3990],\n",
      "        [0.7219]], device='mps:0')\n",
      "Iteration 42830 Training loss 0.11406177282333374 Validation loss 0.101749487221241 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.3587],\n",
      "        [0.5079]], device='mps:0')\n",
      "Iteration 42840 Training loss 0.1051407977938652 Validation loss 0.10175243765115738 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4196],\n",
      "        [0.5702]], device='mps:0')\n",
      "Iteration 42850 Training loss 0.09935631603002548 Validation loss 0.10174734145402908 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6937],\n",
      "        [0.7482]], device='mps:0')\n",
      "Iteration 42860 Training loss 0.09704102575778961 Validation loss 0.10174373537302017 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.5464],\n",
      "        [0.3822]], device='mps:0')\n",
      "Iteration 42870 Training loss 0.09291042387485504 Validation loss 0.10174314677715302 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.1963],\n",
      "        [0.5046]], device='mps:0')\n",
      "Iteration 42880 Training loss 0.10063253343105316 Validation loss 0.10173944383859634 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.1748],\n",
      "        [0.7978]], device='mps:0')\n",
      "Iteration 42890 Training loss 0.09326013177633286 Validation loss 0.10174038261175156 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.4019],\n",
      "        [0.4547]], device='mps:0')\n",
      "Iteration 42900 Training loss 0.10353820770978928 Validation loss 0.10174158960580826 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3865],\n",
      "        [0.7179]], device='mps:0')\n",
      "Iteration 42910 Training loss 0.11088287085294724 Validation loss 0.10175091028213501 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6101],\n",
      "        [0.6943]], device='mps:0')\n",
      "Iteration 42920 Training loss 0.11563573777675629 Validation loss 0.10175272077322006 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3405],\n",
      "        [0.6805]], device='mps:0')\n",
      "Iteration 42930 Training loss 0.10069108754396439 Validation loss 0.10173749178647995 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3773],\n",
      "        [0.6600]], device='mps:0')\n",
      "Iteration 42940 Training loss 0.08554296940565109 Validation loss 0.1017315685749054 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.2075],\n",
      "        [0.4849]], device='mps:0')\n",
      "Iteration 42950 Training loss 0.09498685598373413 Validation loss 0.101735420525074 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6523],\n",
      "        [0.5755]], device='mps:0')\n",
      "Iteration 42960 Training loss 0.09897599369287491 Validation loss 0.10173143446445465 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5635],\n",
      "        [0.5393]], device='mps:0')\n",
      "Iteration 42970 Training loss 0.11173823475837708 Validation loss 0.10174472630023956 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5666],\n",
      "        [0.0756]], device='mps:0')\n",
      "Iteration 42980 Training loss 0.09478472173213959 Validation loss 0.10177022963762283 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4998],\n",
      "        [0.4021]], device='mps:0')\n",
      "Iteration 42990 Training loss 0.10190283507108688 Validation loss 0.10173428058624268 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3593],\n",
      "        [0.6394]], device='mps:0')\n",
      "Iteration 43000 Training loss 0.09051038324832916 Validation loss 0.10173086822032928 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5170],\n",
      "        [0.4087]], device='mps:0')\n",
      "Iteration 43010 Training loss 0.10487223416566849 Validation loss 0.10172214359045029 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.2169],\n",
      "        [0.6312]], device='mps:0')\n",
      "Iteration 43020 Training loss 0.10920746624469757 Validation loss 0.10172593593597412 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3517],\n",
      "        [0.5677]], device='mps:0')\n",
      "Iteration 43030 Training loss 0.09767583757638931 Validation loss 0.10174861550331116 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3269],\n",
      "        [0.5316]], device='mps:0')\n",
      "Iteration 43040 Training loss 0.10028163343667984 Validation loss 0.10173366218805313 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3542],\n",
      "        [0.6107]], device='mps:0')\n",
      "Iteration 43050 Training loss 0.11125238239765167 Validation loss 0.1017281711101532 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6636],\n",
      "        [0.6862]], device='mps:0')\n",
      "Iteration 43060 Training loss 0.09416387230157852 Validation loss 0.10172294080257416 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.2012],\n",
      "        [0.5950]], device='mps:0')\n",
      "Iteration 43070 Training loss 0.10688123106956482 Validation loss 0.10172134637832642 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.3019],\n",
      "        [0.0772]], device='mps:0')\n",
      "Iteration 43080 Training loss 0.10051707178354263 Validation loss 0.10172975808382034 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5998],\n",
      "        [0.4178]], device='mps:0')\n",
      "Iteration 43090 Training loss 0.11042530089616776 Validation loss 0.10172051936388016 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.6577],\n",
      "        [0.6205]], device='mps:0')\n",
      "Iteration 43100 Training loss 0.11522673070430756 Validation loss 0.10171713680028915 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.3173],\n",
      "        [0.3110]], device='mps:0')\n",
      "Iteration 43110 Training loss 0.11124233156442642 Validation loss 0.10171684622764587 Accuracy 0.7065000534057617\n",
      "Output tensor([[0.6802],\n",
      "        [0.5638]], device='mps:0')\n",
      "Iteration 43120 Training loss 0.10452083498239517 Validation loss 0.1017240509390831 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.1735],\n",
      "        [0.3932]], device='mps:0')\n",
      "Iteration 43130 Training loss 0.10522457212209702 Validation loss 0.10172245651483536 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3775],\n",
      "        [0.4559]], device='mps:0')\n",
      "Iteration 43140 Training loss 0.10366149991750717 Validation loss 0.10172134637832642 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2850],\n",
      "        [0.6510]], device='mps:0')\n",
      "Iteration 43150 Training loss 0.08866800367832184 Validation loss 0.10175488144159317 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5921],\n",
      "        [0.5890]], device='mps:0')\n",
      "Iteration 43160 Training loss 0.10917219519615173 Validation loss 0.10178117454051971 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5777],\n",
      "        [0.6218]], device='mps:0')\n",
      "Iteration 43170 Training loss 0.09636270999908447 Validation loss 0.1018233671784401 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4142],\n",
      "        [0.5797]], device='mps:0')\n",
      "Iteration 43180 Training loss 0.10186590254306793 Validation loss 0.10178938508033752 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2082],\n",
      "        [0.6367]], device='mps:0')\n",
      "Iteration 43190 Training loss 0.11206796765327454 Validation loss 0.10179968923330307 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5970],\n",
      "        [0.1502]], device='mps:0')\n",
      "Iteration 43200 Training loss 0.10808734595775604 Validation loss 0.10182099044322968 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.1630],\n",
      "        [0.2065]], device='mps:0')\n",
      "Iteration 43210 Training loss 0.11211872100830078 Validation loss 0.10177843272686005 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3960],\n",
      "        [0.4603]], device='mps:0')\n",
      "Iteration 43220 Training loss 0.09996116161346436 Validation loss 0.10175175964832306 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3425],\n",
      "        [0.6762]], device='mps:0')\n",
      "Iteration 43230 Training loss 0.09043357521295547 Validation loss 0.10173393785953522 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3858],\n",
      "        [0.5617]], device='mps:0')\n",
      "Iteration 43240 Training loss 0.09606842696666718 Validation loss 0.10172435641288757 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7529],\n",
      "        [0.7260]], device='mps:0')\n",
      "Iteration 43250 Training loss 0.10653717070817947 Validation loss 0.10172099620103836 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6861],\n",
      "        [0.4760]], device='mps:0')\n",
      "Iteration 43260 Training loss 0.10115242749452591 Validation loss 0.10171234607696533 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.7044],\n",
      "        [0.6315]], device='mps:0')\n",
      "Iteration 43270 Training loss 0.10884314030408859 Validation loss 0.10172668844461441 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6479],\n",
      "        [0.6178]], device='mps:0')\n",
      "Iteration 43280 Training loss 0.11072272062301636 Validation loss 0.10171125829219818 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4414],\n",
      "        [0.5803]], device='mps:0')\n",
      "Iteration 43290 Training loss 0.10368694365024567 Validation loss 0.10171697288751602 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6805],\n",
      "        [0.4426]], device='mps:0')\n",
      "Iteration 43300 Training loss 0.0993722602725029 Validation loss 0.10169681161642075 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6618],\n",
      "        [0.2093]], device='mps:0')\n",
      "Iteration 43310 Training loss 0.10742591321468353 Validation loss 0.10171741992235184 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5962],\n",
      "        [0.2948]], device='mps:0')\n",
      "Iteration 43320 Training loss 0.09268423914909363 Validation loss 0.10171990841627121 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4299],\n",
      "        [0.5516]], device='mps:0')\n",
      "Iteration 43330 Training loss 0.10085621476173401 Validation loss 0.10172650963068008 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3046],\n",
      "        [0.3775]], device='mps:0')\n",
      "Iteration 43340 Training loss 0.10985595732927322 Validation loss 0.10178390145301819 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.1756],\n",
      "        [0.4735]], device='mps:0')\n",
      "Iteration 43350 Training loss 0.10305532068014145 Validation loss 0.10176128149032593 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3620],\n",
      "        [0.6465]], device='mps:0')\n",
      "Iteration 43360 Training loss 0.10414622724056244 Validation loss 0.10172264277935028 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5113],\n",
      "        [0.4353]], device='mps:0')\n",
      "Iteration 43370 Training loss 0.11213525384664536 Validation loss 0.10172786563634872 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3683],\n",
      "        [0.4841]], device='mps:0')\n",
      "Iteration 43380 Training loss 0.10535509139299393 Validation loss 0.10173315554857254 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3159],\n",
      "        [0.1139]], device='mps:0')\n",
      "Iteration 43390 Training loss 0.10121529549360275 Validation loss 0.10172642022371292 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3413],\n",
      "        [0.5619]], device='mps:0')\n",
      "Iteration 43400 Training loss 0.10951419919729233 Validation loss 0.10170958191156387 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5443],\n",
      "        [0.3202]], device='mps:0')\n",
      "Iteration 43410 Training loss 0.12221139669418335 Validation loss 0.10169665515422821 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5352],\n",
      "        [0.6345]], device='mps:0')\n",
      "Iteration 43420 Training loss 0.09580541402101517 Validation loss 0.10168590396642685 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4855],\n",
      "        [0.4852]], device='mps:0')\n",
      "Iteration 43430 Training loss 0.1055954322218895 Validation loss 0.10171627253293991 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3431],\n",
      "        [0.6516]], device='mps:0')\n",
      "Iteration 43440 Training loss 0.10965119302272797 Validation loss 0.10173077881336212 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3601],\n",
      "        [0.4922]], device='mps:0')\n",
      "Iteration 43450 Training loss 0.11384467780590057 Validation loss 0.10179414600133896 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4149],\n",
      "        [0.2531]], device='mps:0')\n",
      "Iteration 43460 Training loss 0.10268863290548325 Validation loss 0.1017870306968689 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3165],\n",
      "        [0.6430]], device='mps:0')\n",
      "Iteration 43470 Training loss 0.09930551052093506 Validation loss 0.1017809510231018 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5528],\n",
      "        [0.5171]], device='mps:0')\n",
      "Iteration 43480 Training loss 0.11060603708028793 Validation loss 0.10169282555580139 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7136],\n",
      "        [0.5595]], device='mps:0')\n",
      "Iteration 43490 Training loss 0.10249671339988708 Validation loss 0.10167177766561508 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.6356],\n",
      "        [0.7834]], device='mps:0')\n",
      "Iteration 43500 Training loss 0.10203372687101364 Validation loss 0.10166901350021362 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.4638],\n",
      "        [0.5336]], device='mps:0')\n",
      "Iteration 43510 Training loss 0.09945983439683914 Validation loss 0.10166868567466736 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.5905],\n",
      "        [0.6630]], device='mps:0')\n",
      "Iteration 43520 Training loss 0.1126961037516594 Validation loss 0.10166614502668381 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5885],\n",
      "        [0.5566]], device='mps:0')\n",
      "Iteration 43530 Training loss 0.11090993881225586 Validation loss 0.10166583955287933 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.3460],\n",
      "        [0.4774]], device='mps:0')\n",
      "Iteration 43540 Training loss 0.10679200291633606 Validation loss 0.10166458040475845 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6384],\n",
      "        [0.6911]], device='mps:0')\n",
      "Iteration 43550 Training loss 0.10620321333408356 Validation loss 0.10167243331670761 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6799],\n",
      "        [0.7320]], device='mps:0')\n",
      "Iteration 43560 Training loss 0.0940157100558281 Validation loss 0.10168906301259995 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5873],\n",
      "        [0.1940]], device='mps:0')\n",
      "Iteration 43570 Training loss 0.09861244261264801 Validation loss 0.10172287374734879 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7214],\n",
      "        [0.6514]], device='mps:0')\n",
      "Iteration 43580 Training loss 0.09977960586547852 Validation loss 0.10175083577632904 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5136],\n",
      "        [0.4365]], device='mps:0')\n",
      "Iteration 43590 Training loss 0.119900643825531 Validation loss 0.10177251696586609 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5215],\n",
      "        [0.6028]], device='mps:0')\n",
      "Iteration 43600 Training loss 0.1058250218629837 Validation loss 0.10174203664064407 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5465],\n",
      "        [0.6045]], device='mps:0')\n",
      "Iteration 43610 Training loss 0.09512282907962799 Validation loss 0.10173558443784714 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3130],\n",
      "        [0.4451]], device='mps:0')\n",
      "Iteration 43620 Training loss 0.09858248382806778 Validation loss 0.10172972083091736 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6149],\n",
      "        [0.3289]], device='mps:0')\n",
      "Iteration 43630 Training loss 0.10052808374166489 Validation loss 0.1017342060804367 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4645],\n",
      "        [0.5511]], device='mps:0')\n",
      "Iteration 43640 Training loss 0.10089150816202164 Validation loss 0.10176201164722443 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3361],\n",
      "        [0.3876]], device='mps:0')\n",
      "Iteration 43650 Training loss 0.09889863431453705 Validation loss 0.10173972696065903 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7387],\n",
      "        [0.3771]], device='mps:0')\n",
      "Iteration 43660 Training loss 0.104628786444664 Validation loss 0.1018093004822731 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6575],\n",
      "        [0.5509]], device='mps:0')\n",
      "Iteration 43670 Training loss 0.10585866868495941 Validation loss 0.1017874926328659 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4852],\n",
      "        [0.3879]], device='mps:0')\n",
      "Iteration 43680 Training loss 0.11956550180912018 Validation loss 0.10176035016775131 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7449],\n",
      "        [0.4213]], device='mps:0')\n",
      "Iteration 43690 Training loss 0.10981397330760956 Validation loss 0.10174877196550369 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2581],\n",
      "        [0.6763]], device='mps:0')\n",
      "Iteration 43700 Training loss 0.11114359647035599 Validation loss 0.10167481005191803 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4280],\n",
      "        [0.5883]], device='mps:0')\n",
      "Iteration 43710 Training loss 0.10528070479631424 Validation loss 0.10168846696615219 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5299],\n",
      "        [0.4579]], device='mps:0')\n",
      "Iteration 43720 Training loss 0.09151864051818848 Validation loss 0.10164088755846024 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.5729],\n",
      "        [0.2647]], device='mps:0')\n",
      "Iteration 43730 Training loss 0.10641889274120331 Validation loss 0.10163825005292892 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6908],\n",
      "        [0.5917]], device='mps:0')\n",
      "Iteration 43740 Training loss 0.10945569723844528 Validation loss 0.1016746535897255 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6516],\n",
      "        [0.6817]], device='mps:0')\n",
      "Iteration 43750 Training loss 0.09982313215732574 Validation loss 0.10168652981519699 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6216],\n",
      "        [0.5263]], device='mps:0')\n",
      "Iteration 43760 Training loss 0.10508830845355988 Validation loss 0.10168842226266861 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3237],\n",
      "        [0.5871]], device='mps:0')\n",
      "Iteration 43770 Training loss 0.09653119742870331 Validation loss 0.1016925647854805 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.1361],\n",
      "        [0.4412]], device='mps:0')\n",
      "Iteration 43780 Training loss 0.10884983092546463 Validation loss 0.101673424243927 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6773],\n",
      "        [0.3957]], device='mps:0')\n",
      "Iteration 43790 Training loss 0.10198414325714111 Validation loss 0.10166763514280319 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2577],\n",
      "        [0.2545]], device='mps:0')\n",
      "Iteration 43800 Training loss 0.10654348134994507 Validation loss 0.10163498669862747 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3478],\n",
      "        [0.6622]], device='mps:0')\n",
      "Iteration 43810 Training loss 0.09803752601146698 Validation loss 0.10163550078868866 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2868],\n",
      "        [0.5088]], device='mps:0')\n",
      "Iteration 43820 Training loss 0.09356222301721573 Validation loss 0.10162460803985596 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.4831],\n",
      "        [0.6507]], device='mps:0')\n",
      "Iteration 43830 Training loss 0.09827429801225662 Validation loss 0.10162543505430222 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6018],\n",
      "        [0.6844]], device='mps:0')\n",
      "Iteration 43840 Training loss 0.09727506339550018 Validation loss 0.10162361711263657 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.7027],\n",
      "        [0.6189]], device='mps:0')\n",
      "Iteration 43850 Training loss 0.0991879478096962 Validation loss 0.10162459313869476 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.4090],\n",
      "        [0.4074]], device='mps:0')\n",
      "Iteration 43860 Training loss 0.099868044257164 Validation loss 0.10165271162986755 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3911],\n",
      "        [0.3754]], device='mps:0')\n",
      "Iteration 43870 Training loss 0.0926365926861763 Validation loss 0.1016276478767395 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4094],\n",
      "        [0.6606]], device='mps:0')\n",
      "Iteration 43880 Training loss 0.10184608399868011 Validation loss 0.10161974281072617 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2087],\n",
      "        [0.5154]], device='mps:0')\n",
      "Iteration 43890 Training loss 0.10515227913856506 Validation loss 0.10161955654621124 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.1904],\n",
      "        [0.4385]], device='mps:0')\n",
      "Iteration 43900 Training loss 0.09255614131689072 Validation loss 0.10161854326725006 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5094],\n",
      "        [0.4175]], device='mps:0')\n",
      "Iteration 43910 Training loss 0.10705570131540298 Validation loss 0.10162179172039032 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5269],\n",
      "        [0.4272]], device='mps:0')\n",
      "Iteration 43920 Training loss 0.11960313469171524 Validation loss 0.10162828862667084 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6515],\n",
      "        [0.6320]], device='mps:0')\n",
      "Iteration 43930 Training loss 0.09845590591430664 Validation loss 0.10161584615707397 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.1958],\n",
      "        [0.6892]], device='mps:0')\n",
      "Iteration 43940 Training loss 0.11164934188127518 Validation loss 0.10162591189146042 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6686],\n",
      "        [0.7112]], device='mps:0')\n",
      "Iteration 43950 Training loss 0.1011776253581047 Validation loss 0.10162311047315598 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5378],\n",
      "        [0.5095]], device='mps:0')\n",
      "Iteration 43960 Training loss 0.11041823774576187 Validation loss 0.10165143013000488 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5357],\n",
      "        [0.2068]], device='mps:0')\n",
      "Iteration 43970 Training loss 0.10630849003791809 Validation loss 0.101634681224823 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.1942],\n",
      "        [0.5021]], device='mps:0')\n",
      "Iteration 43980 Training loss 0.10956496745347977 Validation loss 0.10164964944124222 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5552],\n",
      "        [0.6091]], device='mps:0')\n",
      "Iteration 43990 Training loss 0.09981691092252731 Validation loss 0.10163132101297379 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6589],\n",
      "        [0.5592]], device='mps:0')\n",
      "Iteration 44000 Training loss 0.11112041026353836 Validation loss 0.101610466837883 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6282],\n",
      "        [0.5962]], device='mps:0')\n",
      "Iteration 44010 Training loss 0.09991516172885895 Validation loss 0.10161742568016052 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5011],\n",
      "        [0.2210]], device='mps:0')\n",
      "Iteration 44020 Training loss 0.102115698158741 Validation loss 0.1016077920794487 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4535],\n",
      "        [0.7174]], device='mps:0')\n",
      "Iteration 44030 Training loss 0.10845876485109329 Validation loss 0.10163144767284393 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6901],\n",
      "        [0.3578]], device='mps:0')\n",
      "Iteration 44040 Training loss 0.10360909253358841 Validation loss 0.10160725563764572 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4932],\n",
      "        [0.4564]], device='mps:0')\n",
      "Iteration 44050 Training loss 0.10624538362026215 Validation loss 0.10159726440906525 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5442],\n",
      "        [0.4997]], device='mps:0')\n",
      "Iteration 44060 Training loss 0.10432180762290955 Validation loss 0.10159371048212051 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6174],\n",
      "        [0.1966]], device='mps:0')\n",
      "Iteration 44070 Training loss 0.09693287312984467 Validation loss 0.1015944629907608 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3347],\n",
      "        [0.6518]], device='mps:0')\n",
      "Iteration 44080 Training loss 0.10035105794668198 Validation loss 0.10159482806921005 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.2847],\n",
      "        [0.4497]], device='mps:0')\n",
      "Iteration 44090 Training loss 0.09960321336984634 Validation loss 0.10159918665885925 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.7195],\n",
      "        [0.1737]], device='mps:0')\n",
      "Iteration 44100 Training loss 0.09359390288591385 Validation loss 0.10159046202898026 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5125],\n",
      "        [0.6016]], device='mps:0')\n",
      "Iteration 44110 Training loss 0.08906938135623932 Validation loss 0.10159777104854584 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4596],\n",
      "        [0.2716]], device='mps:0')\n",
      "Iteration 44120 Training loss 0.10834851861000061 Validation loss 0.10161250829696655 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4433],\n",
      "        [0.1270]], device='mps:0')\n",
      "Iteration 44130 Training loss 0.11484086513519287 Validation loss 0.10160937160253525 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6882],\n",
      "        [0.4958]], device='mps:0')\n",
      "Iteration 44140 Training loss 0.09912189096212387 Validation loss 0.10158445686101913 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6172],\n",
      "        [0.2950]], device='mps:0')\n",
      "Iteration 44150 Training loss 0.09687566757202148 Validation loss 0.10157974809408188 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5794],\n",
      "        [0.3849]], device='mps:0')\n",
      "Iteration 44160 Training loss 0.10519804060459137 Validation loss 0.10160499811172485 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3923],\n",
      "        [0.5771]], device='mps:0')\n",
      "Iteration 44170 Training loss 0.0986936166882515 Validation loss 0.10158233344554901 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5831],\n",
      "        [0.6447]], device='mps:0')\n",
      "Iteration 44180 Training loss 0.10386664420366287 Validation loss 0.1015734151005745 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.5264],\n",
      "        [0.5208]], device='mps:0')\n",
      "Iteration 44190 Training loss 0.1057782918214798 Validation loss 0.10157081484794617 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.5347],\n",
      "        [0.4318]], device='mps:0')\n",
      "Iteration 44200 Training loss 0.09800716489553452 Validation loss 0.10156984627246857 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6565],\n",
      "        [0.6263]], device='mps:0')\n",
      "Iteration 44210 Training loss 0.1062488928437233 Validation loss 0.10157673805952072 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5047],\n",
      "        [0.4263]], device='mps:0')\n",
      "Iteration 44220 Training loss 0.1011645570397377 Validation loss 0.1015792191028595 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6978],\n",
      "        [0.6051]], device='mps:0')\n",
      "Iteration 44230 Training loss 0.10669568926095963 Validation loss 0.10158702731132507 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6033],\n",
      "        [0.2903]], device='mps:0')\n",
      "Iteration 44240 Training loss 0.10483182966709137 Validation loss 0.1016114205121994 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3705],\n",
      "        [0.4905]], device='mps:0')\n",
      "Iteration 44250 Training loss 0.10392990708351135 Validation loss 0.10160990059375763 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5990],\n",
      "        [0.4875]], device='mps:0')\n",
      "Iteration 44260 Training loss 0.11249293386936188 Validation loss 0.10163628309965134 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.1799],\n",
      "        [0.6069]], device='mps:0')\n",
      "Iteration 44270 Training loss 0.09549108892679214 Validation loss 0.10179436206817627 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6677],\n",
      "        [0.5628]], device='mps:0')\n",
      "Iteration 44280 Training loss 0.10378249734640121 Validation loss 0.10165216028690338 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3578],\n",
      "        [0.0690]], device='mps:0')\n",
      "Iteration 44290 Training loss 0.10130830854177475 Validation loss 0.10162738710641861 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4981],\n",
      "        [0.5470]], device='mps:0')\n",
      "Iteration 44300 Training loss 0.11690773069858551 Validation loss 0.10162320733070374 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5759],\n",
      "        [0.5219]], device='mps:0')\n",
      "Iteration 44310 Training loss 0.09128211438655853 Validation loss 0.10160387307405472 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4870],\n",
      "        [0.3734]], device='mps:0')\n",
      "Iteration 44320 Training loss 0.10877391695976257 Validation loss 0.10163965076208115 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4662],\n",
      "        [0.7241]], device='mps:0')\n",
      "Iteration 44330 Training loss 0.1197703629732132 Validation loss 0.10162107646465302 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4420],\n",
      "        [0.6750]], device='mps:0')\n",
      "Iteration 44340 Training loss 0.10704512149095535 Validation loss 0.10161080211400986 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4376],\n",
      "        [0.4873]], device='mps:0')\n",
      "Iteration 44350 Training loss 0.10346777737140656 Validation loss 0.10160347819328308 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6606],\n",
      "        [0.3824]], device='mps:0')\n",
      "Iteration 44360 Training loss 0.09512840211391449 Validation loss 0.10160272568464279 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5691],\n",
      "        [0.3172]], device='mps:0')\n",
      "Iteration 44370 Training loss 0.11219211667776108 Validation loss 0.10161186754703522 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5648],\n",
      "        [0.4071]], device='mps:0')\n",
      "Iteration 44380 Training loss 0.12021129578351974 Validation loss 0.10163071006536484 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5974],\n",
      "        [0.3608]], device='mps:0')\n",
      "Iteration 44390 Training loss 0.12257073074579239 Validation loss 0.10157376527786255 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3901],\n",
      "        [0.5269]], device='mps:0')\n",
      "Iteration 44400 Training loss 0.10169649124145508 Validation loss 0.1015675887465477 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4486],\n",
      "        [0.7384]], device='mps:0')\n",
      "Iteration 44410 Training loss 0.10715436935424805 Validation loss 0.10154512524604797 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.3837],\n",
      "        [0.4177]], device='mps:0')\n",
      "Iteration 44420 Training loss 0.10030269622802734 Validation loss 0.10154663026332855 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.2891],\n",
      "        [0.5342]], device='mps:0')\n",
      "Iteration 44430 Training loss 0.09591434895992279 Validation loss 0.10156561434268951 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4245],\n",
      "        [0.5410]], device='mps:0')\n",
      "Iteration 44440 Training loss 0.12291170656681061 Validation loss 0.10156472772359848 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6512],\n",
      "        [0.3264]], device='mps:0')\n",
      "Iteration 44450 Training loss 0.10618829727172852 Validation loss 0.10153687000274658 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.5929],\n",
      "        [0.5059]], device='mps:0')\n",
      "Iteration 44460 Training loss 0.09847618639469147 Validation loss 0.10157275944948196 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.1386],\n",
      "        [0.6577]], device='mps:0')\n",
      "Iteration 44470 Training loss 0.11723321676254272 Validation loss 0.10153373330831528 Accuracy 0.7070000171661377\n",
      "Output tensor([[0.7940],\n",
      "        [0.3657]], device='mps:0')\n",
      "Iteration 44480 Training loss 0.10667194426059723 Validation loss 0.10153084993362427 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5773],\n",
      "        [0.5481]], device='mps:0')\n",
      "Iteration 44490 Training loss 0.11415984481573105 Validation loss 0.10153111815452576 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.3014],\n",
      "        [0.2485]], device='mps:0')\n",
      "Iteration 44500 Training loss 0.1120617538690567 Validation loss 0.1015290692448616 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5976],\n",
      "        [0.4467]], device='mps:0')\n",
      "Iteration 44510 Training loss 0.1052120178937912 Validation loss 0.10152692347764969 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.7206],\n",
      "        [0.4245]], device='mps:0')\n",
      "Iteration 44520 Training loss 0.09238408505916595 Validation loss 0.10152548551559448 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6732],\n",
      "        [0.6849]], device='mps:0')\n",
      "Iteration 44530 Training loss 0.10227684676647186 Validation loss 0.10153798758983612 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4110],\n",
      "        [0.6840]], device='mps:0')\n",
      "Iteration 44540 Training loss 0.10731269419193268 Validation loss 0.10152963548898697 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3222],\n",
      "        [0.3772]], device='mps:0')\n",
      "Iteration 44550 Training loss 0.10352727025747299 Validation loss 0.10152233392000198 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.6199],\n",
      "        [0.6153]], device='mps:0')\n",
      "Iteration 44560 Training loss 0.10174953192472458 Validation loss 0.10155290365219116 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3650],\n",
      "        [0.6915]], device='mps:0')\n",
      "Iteration 44570 Training loss 0.10921099781990051 Validation loss 0.10154905915260315 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2667],\n",
      "        [0.4177]], device='mps:0')\n",
      "Iteration 44580 Training loss 0.11688493192195892 Validation loss 0.10153371095657349 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.2617],\n",
      "        [0.5709]], device='mps:0')\n",
      "Iteration 44590 Training loss 0.11575669050216675 Validation loss 0.1015511080622673 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3904],\n",
      "        [0.3222]], device='mps:0')\n",
      "Iteration 44600 Training loss 0.10127368569374084 Validation loss 0.10152421146631241 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6380],\n",
      "        [0.5328]], device='mps:0')\n",
      "Iteration 44610 Training loss 0.11172069609165192 Validation loss 0.10154012590646744 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5339],\n",
      "        [0.6623]], device='mps:0')\n",
      "Iteration 44620 Training loss 0.11978691071271896 Validation loss 0.10154972970485687 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4411],\n",
      "        [0.4454]], device='mps:0')\n",
      "Iteration 44630 Training loss 0.10468980669975281 Validation loss 0.10152695327997208 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4807],\n",
      "        [0.4814]], device='mps:0')\n",
      "Iteration 44640 Training loss 0.1031235009431839 Validation loss 0.10152115672826767 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5775],\n",
      "        [0.4367]], device='mps:0')\n",
      "Iteration 44650 Training loss 0.1101568192243576 Validation loss 0.10150955617427826 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.7432],\n",
      "        [0.3649]], device='mps:0')\n",
      "Iteration 44660 Training loss 0.1099354699254036 Validation loss 0.10150949656963348 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6896],\n",
      "        [0.2589]], device='mps:0')\n",
      "Iteration 44670 Training loss 0.10248708724975586 Validation loss 0.10150963068008423 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6549],\n",
      "        [0.2312]], device='mps:0')\n",
      "Iteration 44680 Training loss 0.103566013276577 Validation loss 0.10150930285453796 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6172],\n",
      "        [0.4871]], device='mps:0')\n",
      "Iteration 44690 Training loss 0.09658008068799973 Validation loss 0.10151617974042892 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4833],\n",
      "        [0.6100]], device='mps:0')\n",
      "Iteration 44700 Training loss 0.10933937132358551 Validation loss 0.1015549898147583 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6154],\n",
      "        [0.7155]], device='mps:0')\n",
      "Iteration 44710 Training loss 0.09059219807386398 Validation loss 0.10153032094240189 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4503],\n",
      "        [0.2626]], device='mps:0')\n",
      "Iteration 44720 Training loss 0.09172572940587997 Validation loss 0.101537324488163 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7213],\n",
      "        [0.4384]], device='mps:0')\n",
      "Iteration 44730 Training loss 0.11159652471542358 Validation loss 0.10152947902679443 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6790],\n",
      "        [0.5743]], device='mps:0')\n",
      "Iteration 44740 Training loss 0.1051190197467804 Validation loss 0.1015613004565239 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5620],\n",
      "        [0.5364]], device='mps:0')\n",
      "Iteration 44750 Training loss 0.10760027915239334 Validation loss 0.10153590142726898 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5999],\n",
      "        [0.4126]], device='mps:0')\n",
      "Iteration 44760 Training loss 0.10513725131750107 Validation loss 0.10150543600320816 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.2222],\n",
      "        [0.3546]], device='mps:0')\n",
      "Iteration 44770 Training loss 0.10390888154506683 Validation loss 0.10151315480470657 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2502],\n",
      "        [0.4938]], device='mps:0')\n",
      "Iteration 44780 Training loss 0.11387916654348373 Validation loss 0.10151556879281998 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3351],\n",
      "        [0.5825]], device='mps:0')\n",
      "Iteration 44790 Training loss 0.10126468539237976 Validation loss 0.10156434774398804 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7324],\n",
      "        [0.6050]], device='mps:0')\n",
      "Iteration 44800 Training loss 0.09985271096229553 Validation loss 0.10163634270429611 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4241],\n",
      "        [0.1597]], device='mps:0')\n",
      "Iteration 44810 Training loss 0.09884307533502579 Validation loss 0.10151370614767075 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5556],\n",
      "        [0.2551]], device='mps:0')\n",
      "Iteration 44820 Training loss 0.09680742770433426 Validation loss 0.10151291638612747 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3161],\n",
      "        [0.5247]], device='mps:0')\n",
      "Iteration 44830 Training loss 0.1082853451371193 Validation loss 0.10158029943704605 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7247],\n",
      "        [0.4875]], device='mps:0')\n",
      "Iteration 44840 Training loss 0.11071185767650604 Validation loss 0.10157395154237747 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4678],\n",
      "        [0.5863]], device='mps:0')\n",
      "Iteration 44850 Training loss 0.09572161734104156 Validation loss 0.10157965123653412 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4306],\n",
      "        [0.4594]], device='mps:0')\n",
      "Iteration 44860 Training loss 0.09346053749322891 Validation loss 0.10155990719795227 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2408],\n",
      "        [0.5469]], device='mps:0')\n",
      "Iteration 44870 Training loss 0.09981606900691986 Validation loss 0.1015080064535141 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2895],\n",
      "        [0.6733]], device='mps:0')\n",
      "Iteration 44880 Training loss 0.10614120215177536 Validation loss 0.10150911659002304 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.1877],\n",
      "        [0.6781]], device='mps:0')\n",
      "Iteration 44890 Training loss 0.09774529933929443 Validation loss 0.10149495303630829 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2515],\n",
      "        [0.4355]], device='mps:0')\n",
      "Iteration 44900 Training loss 0.09968346357345581 Validation loss 0.10149793326854706 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4574],\n",
      "        [0.4867]], device='mps:0')\n",
      "Iteration 44910 Training loss 0.1051858589053154 Validation loss 0.10150240361690521 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5685],\n",
      "        [0.1914]], device='mps:0')\n",
      "Iteration 44920 Training loss 0.10991886258125305 Validation loss 0.1014954149723053 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4358],\n",
      "        [0.3797]], device='mps:0')\n",
      "Iteration 44930 Training loss 0.1025778204202652 Validation loss 0.1015201285481453 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4424],\n",
      "        [0.3055]], device='mps:0')\n",
      "Iteration 44940 Training loss 0.1049598678946495 Validation loss 0.10147657990455627 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.2503],\n",
      "        [0.6026]], device='mps:0')\n",
      "Iteration 44950 Training loss 0.11727797985076904 Validation loss 0.10147812217473984 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5762],\n",
      "        [0.2335]], device='mps:0')\n",
      "Iteration 44960 Training loss 0.1024586632847786 Validation loss 0.10147508233785629 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5631],\n",
      "        [0.1265]], device='mps:0')\n",
      "Iteration 44970 Training loss 0.10178451240062714 Validation loss 0.10147108137607574 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.5270],\n",
      "        [0.7303]], device='mps:0')\n",
      "Iteration 44980 Training loss 0.11364961415529251 Validation loss 0.10147009789943695 Accuracy 0.7075000405311584\n",
      "Output tensor([[0.5836],\n",
      "        [0.7620]], device='mps:0')\n",
      "Iteration 44990 Training loss 0.10377632081508636 Validation loss 0.10146736353635788 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.3945],\n",
      "        [0.2784]], device='mps:0')\n",
      "Iteration 45000 Training loss 0.10363338142633438 Validation loss 0.10148432850837708 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2537],\n",
      "        [0.4680]], device='mps:0')\n",
      "Iteration 45010 Training loss 0.09935832768678665 Validation loss 0.10147664695978165 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4725],\n",
      "        [0.5881]], device='mps:0')\n",
      "Iteration 45020 Training loss 0.1038244292140007 Validation loss 0.1014760360121727 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5711],\n",
      "        [0.3947]], device='mps:0')\n",
      "Iteration 45030 Training loss 0.10021703690290451 Validation loss 0.101471446454525 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6962],\n",
      "        [0.5262]], device='mps:0')\n",
      "Iteration 45040 Training loss 0.099776491522789 Validation loss 0.10147646069526672 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6142],\n",
      "        [0.2995]], device='mps:0')\n",
      "Iteration 45050 Training loss 0.10973365604877472 Validation loss 0.10149234533309937 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6380],\n",
      "        [0.2843]], device='mps:0')\n",
      "Iteration 45060 Training loss 0.08863820880651474 Validation loss 0.10147173702716827 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6148],\n",
      "        [0.5967]], device='mps:0')\n",
      "Iteration 45070 Training loss 0.10130400210618973 Validation loss 0.10147534310817719 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6562],\n",
      "        [0.4182]], device='mps:0')\n",
      "Iteration 45080 Training loss 0.09463948011398315 Validation loss 0.1014711931347847 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.1375],\n",
      "        [0.5243]], device='mps:0')\n",
      "Iteration 45090 Training loss 0.09640970826148987 Validation loss 0.10146637260913849 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4990],\n",
      "        [0.7117]], device='mps:0')\n",
      "Iteration 45100 Training loss 0.10150167346000671 Validation loss 0.10148883610963821 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5806],\n",
      "        [0.4327]], device='mps:0')\n",
      "Iteration 45110 Training loss 0.10940895974636078 Validation loss 0.10150027275085449 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4723],\n",
      "        [0.1794]], device='mps:0')\n",
      "Iteration 45120 Training loss 0.09778060764074326 Validation loss 0.10148531198501587 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5111],\n",
      "        [0.5203]], device='mps:0')\n",
      "Iteration 45130 Training loss 0.10762318968772888 Validation loss 0.10148206353187561 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4476],\n",
      "        [0.6440]], device='mps:0')\n",
      "Iteration 45140 Training loss 0.11816751211881638 Validation loss 0.10151022672653198 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2910],\n",
      "        [0.5498]], device='mps:0')\n",
      "Iteration 45150 Training loss 0.10313361883163452 Validation loss 0.101551853120327 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2605],\n",
      "        [0.7318]], device='mps:0')\n",
      "Iteration 45160 Training loss 0.0999625101685524 Validation loss 0.10153574496507645 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6909],\n",
      "        [0.3722]], device='mps:0')\n",
      "Iteration 45170 Training loss 0.09742428362369537 Validation loss 0.10156555473804474 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5101],\n",
      "        [0.5922]], device='mps:0')\n",
      "Iteration 45180 Training loss 0.11475757509469986 Validation loss 0.10149591416120529 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2005],\n",
      "        [0.4629]], device='mps:0')\n",
      "Iteration 45190 Training loss 0.10288982093334198 Validation loss 0.10146737843751907 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3810],\n",
      "        [0.5142]], device='mps:0')\n",
      "Iteration 45200 Training loss 0.09450790286064148 Validation loss 0.10147104412317276 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4403],\n",
      "        [0.6393]], device='mps:0')\n",
      "Iteration 45210 Training loss 0.11120837926864624 Validation loss 0.10149449855089188 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3509],\n",
      "        [0.5041]], device='mps:0')\n",
      "Iteration 45220 Training loss 0.10308495908975601 Validation loss 0.10157361626625061 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6477],\n",
      "        [0.3701]], device='mps:0')\n",
      "Iteration 45230 Training loss 0.10796287655830383 Validation loss 0.10148901492357254 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.1902],\n",
      "        [0.5220]], device='mps:0')\n",
      "Iteration 45240 Training loss 0.09594639390707016 Validation loss 0.10146254301071167 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4285],\n",
      "        [0.4979]], device='mps:0')\n",
      "Iteration 45250 Training loss 0.10211391001939774 Validation loss 0.10146530717611313 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7129],\n",
      "        [0.6655]], device='mps:0')\n",
      "Iteration 45260 Training loss 0.10129640996456146 Validation loss 0.10145226866006851 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3519],\n",
      "        [0.5317]], device='mps:0')\n",
      "Iteration 45270 Training loss 0.10946622490882874 Validation loss 0.1014518141746521 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4972],\n",
      "        [0.6731]], device='mps:0')\n",
      "Iteration 45280 Training loss 0.10820852965116501 Validation loss 0.1014396995306015 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5689],\n",
      "        [0.0980]], device='mps:0')\n",
      "Iteration 45290 Training loss 0.1002485603094101 Validation loss 0.10143484175205231 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5316],\n",
      "        [0.2881]], device='mps:0')\n",
      "Iteration 45300 Training loss 0.10332291573286057 Validation loss 0.10144428163766861 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4156],\n",
      "        [0.6350]], device='mps:0')\n",
      "Iteration 45310 Training loss 0.10193189233541489 Validation loss 0.10144270211458206 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3691],\n",
      "        [0.6039]], device='mps:0')\n",
      "Iteration 45320 Training loss 0.10151930898427963 Validation loss 0.10144603252410889 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2798],\n",
      "        [0.5834]], device='mps:0')\n",
      "Iteration 45330 Training loss 0.09290187805891037 Validation loss 0.101451575756073 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.7608],\n",
      "        [0.4352]], device='mps:0')\n",
      "Iteration 45340 Training loss 0.11131462454795837 Validation loss 0.10150037705898285 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4056],\n",
      "        [0.4717]], device='mps:0')\n",
      "Iteration 45350 Training loss 0.0878010094165802 Validation loss 0.101505346596241 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2605],\n",
      "        [0.6020]], device='mps:0')\n",
      "Iteration 45360 Training loss 0.09308721125125885 Validation loss 0.10148805379867554 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5013],\n",
      "        [0.6013]], device='mps:0')\n",
      "Iteration 45370 Training loss 0.0993858128786087 Validation loss 0.1014554351568222 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6560],\n",
      "        [0.1364]], device='mps:0')\n",
      "Iteration 45380 Training loss 0.09799753129482269 Validation loss 0.10146114975214005 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6776],\n",
      "        [0.6999]], device='mps:0')\n",
      "Iteration 45390 Training loss 0.10344430804252625 Validation loss 0.10144400596618652 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6934],\n",
      "        [0.4131]], device='mps:0')\n",
      "Iteration 45400 Training loss 0.10607802867889404 Validation loss 0.10142821073532104 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4056],\n",
      "        [0.5623]], device='mps:0')\n",
      "Iteration 45410 Training loss 0.10211753845214844 Validation loss 0.10141415148973465 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5652],\n",
      "        [0.2787]], device='mps:0')\n",
      "Iteration 45420 Training loss 0.10583540052175522 Validation loss 0.10143062472343445 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2885],\n",
      "        [0.7076]], device='mps:0')\n",
      "Iteration 45430 Training loss 0.10271573066711426 Validation loss 0.10146042704582214 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6831],\n",
      "        [0.5277]], device='mps:0')\n",
      "Iteration 45440 Training loss 0.10033608973026276 Validation loss 0.10148805379867554 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2147],\n",
      "        [0.6487]], device='mps:0')\n",
      "Iteration 45450 Training loss 0.1037881076335907 Validation loss 0.10147079080343246 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5465],\n",
      "        [0.5719]], device='mps:0')\n",
      "Iteration 45460 Training loss 0.1027035042643547 Validation loss 0.10143151879310608 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3268],\n",
      "        [0.4533]], device='mps:0')\n",
      "Iteration 45470 Training loss 0.11196121573448181 Validation loss 0.10141493380069733 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6321],\n",
      "        [0.5773]], device='mps:0')\n",
      "Iteration 45480 Training loss 0.09625489264726639 Validation loss 0.10142244398593903 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4815],\n",
      "        [0.6558]], device='mps:0')\n",
      "Iteration 45490 Training loss 0.0997982770204544 Validation loss 0.10143281519412994 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2522],\n",
      "        [0.6667]], device='mps:0')\n",
      "Iteration 45500 Training loss 0.10966169089078903 Validation loss 0.10144675523042679 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3419],\n",
      "        [0.3655]], device='mps:0')\n",
      "Iteration 45510 Training loss 0.09498566389083862 Validation loss 0.10143470019102097 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2455],\n",
      "        [0.6086]], device='mps:0')\n",
      "Iteration 45520 Training loss 0.10727396607398987 Validation loss 0.10141541063785553 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5103],\n",
      "        [0.7244]], device='mps:0')\n",
      "Iteration 45530 Training loss 0.09857184439897537 Validation loss 0.10142797976732254 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4473],\n",
      "        [0.6071]], device='mps:0')\n",
      "Iteration 45540 Training loss 0.10034669935703278 Validation loss 0.10140837728977203 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4485],\n",
      "        [0.5842]], device='mps:0')\n",
      "Iteration 45550 Training loss 0.0950409322977066 Validation loss 0.10139652341604233 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6670],\n",
      "        [0.7120]], device='mps:0')\n",
      "Iteration 45560 Training loss 0.09847671538591385 Validation loss 0.10141532868146896 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5990],\n",
      "        [0.6296]], device='mps:0')\n",
      "Iteration 45570 Training loss 0.09962181001901627 Validation loss 0.10141468793153763 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3742],\n",
      "        [0.2373]], device='mps:0')\n",
      "Iteration 45580 Training loss 0.10606649518013 Validation loss 0.10140300542116165 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5989],\n",
      "        [0.5493]], device='mps:0')\n",
      "Iteration 45590 Training loss 0.10434406250715256 Validation loss 0.10139816999435425 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5776],\n",
      "        [0.7295]], device='mps:0')\n",
      "Iteration 45600 Training loss 0.10083679109811783 Validation loss 0.10139348357915878 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5140],\n",
      "        [0.4518]], device='mps:0')\n",
      "Iteration 45610 Training loss 0.10160230100154877 Validation loss 0.10140819102525711 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.0714],\n",
      "        [0.6883]], device='mps:0')\n",
      "Iteration 45620 Training loss 0.11602113395929337 Validation loss 0.10139808803796768 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.1638],\n",
      "        [0.6324]], device='mps:0')\n",
      "Iteration 45630 Training loss 0.10979945212602615 Validation loss 0.10143027454614639 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2536],\n",
      "        [0.6457]], device='mps:0')\n",
      "Iteration 45640 Training loss 0.10101257264614105 Validation loss 0.10145175457000732 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6352],\n",
      "        [0.5393]], device='mps:0')\n",
      "Iteration 45650 Training loss 0.09782925993204117 Validation loss 0.10145993530750275 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2281],\n",
      "        [0.5853]], device='mps:0')\n",
      "Iteration 45660 Training loss 0.1126280352473259 Validation loss 0.10149974375963211 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.8409],\n",
      "        [0.2321]], device='mps:0')\n",
      "Iteration 45670 Training loss 0.10174469649791718 Validation loss 0.10149340331554413 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3011],\n",
      "        [0.5252]], device='mps:0')\n",
      "Iteration 45680 Training loss 0.09357298910617828 Validation loss 0.10153324156999588 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7196],\n",
      "        [0.6761]], device='mps:0')\n",
      "Iteration 45690 Training loss 0.10265771299600601 Validation loss 0.1015200987458229 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4269],\n",
      "        [0.4806]], device='mps:0')\n",
      "Iteration 45700 Training loss 0.09439867734909058 Validation loss 0.1015332043170929 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5138],\n",
      "        [0.4066]], device='mps:0')\n",
      "Iteration 45710 Training loss 0.1088004857301712 Validation loss 0.10149754583835602 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2179],\n",
      "        [0.4148]], device='mps:0')\n",
      "Iteration 45720 Training loss 0.09195142239332199 Validation loss 0.10145314782857895 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5018],\n",
      "        [0.4323]], device='mps:0')\n",
      "Iteration 45730 Training loss 0.10846968740224838 Validation loss 0.10144837945699692 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.1726],\n",
      "        [0.4691]], device='mps:0')\n",
      "Iteration 45740 Training loss 0.09570319950580597 Validation loss 0.10143738985061646 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2603],\n",
      "        [0.4249]], device='mps:0')\n",
      "Iteration 45750 Training loss 0.09373714029788971 Validation loss 0.10142306983470917 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5768],\n",
      "        [0.4139]], device='mps:0')\n",
      "Iteration 45760 Training loss 0.1043955609202385 Validation loss 0.10142813622951508 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2131],\n",
      "        [0.4566]], device='mps:0')\n",
      "Iteration 45770 Training loss 0.09391431510448456 Validation loss 0.10143814980983734 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.1909],\n",
      "        [0.6829]], device='mps:0')\n",
      "Iteration 45780 Training loss 0.09675563871860504 Validation loss 0.10140711069107056 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4360],\n",
      "        [0.4215]], device='mps:0')\n",
      "Iteration 45790 Training loss 0.11032405495643616 Validation loss 0.10141228139400482 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6825],\n",
      "        [0.5030]], device='mps:0')\n",
      "Iteration 45800 Training loss 0.09295113384723663 Validation loss 0.10138646513223648 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5067],\n",
      "        [0.5719]], device='mps:0')\n",
      "Iteration 45810 Training loss 0.10669330507516861 Validation loss 0.10140245407819748 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5614],\n",
      "        [0.4517]], device='mps:0')\n",
      "Iteration 45820 Training loss 0.10965946316719055 Validation loss 0.1013827696442604 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3667],\n",
      "        [0.3508]], device='mps:0')\n",
      "Iteration 45830 Training loss 0.10286416858434677 Validation loss 0.1013694703578949 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6833],\n",
      "        [0.6403]], device='mps:0')\n",
      "Iteration 45840 Training loss 0.10474396497011185 Validation loss 0.10137616097927094 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2865],\n",
      "        [0.3866]], device='mps:0')\n",
      "Iteration 45850 Training loss 0.1054389625787735 Validation loss 0.10136626660823822 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5469],\n",
      "        [0.1974]], device='mps:0')\n",
      "Iteration 45860 Training loss 0.11696561425924301 Validation loss 0.10137052088975906 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5652],\n",
      "        [0.6414]], device='mps:0')\n",
      "Iteration 45870 Training loss 0.10431589931249619 Validation loss 0.10136432945728302 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4539],\n",
      "        [0.6269]], device='mps:0')\n",
      "Iteration 45880 Training loss 0.10715761035680771 Validation loss 0.10137001425027847 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5638],\n",
      "        [0.4798]], device='mps:0')\n",
      "Iteration 45890 Training loss 0.10426413267850876 Validation loss 0.10136007517576218 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4882],\n",
      "        [0.5891]], device='mps:0')\n",
      "Iteration 45900 Training loss 0.10362128913402557 Validation loss 0.10136104375123978 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5894],\n",
      "        [0.5250]], device='mps:0')\n",
      "Iteration 45910 Training loss 0.10486249625682831 Validation loss 0.10135957598686218 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.3503],\n",
      "        [0.3283]], device='mps:0')\n",
      "Iteration 45920 Training loss 0.11232519149780273 Validation loss 0.10135436803102493 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.6888],\n",
      "        [0.5807]], device='mps:0')\n",
      "Iteration 45930 Training loss 0.10527854412794113 Validation loss 0.1013568714261055 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6968],\n",
      "        [0.7014]], device='mps:0')\n",
      "Iteration 45940 Training loss 0.11732295900583267 Validation loss 0.10136216133832932 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5127],\n",
      "        [0.6260]], device='mps:0')\n",
      "Iteration 45950 Training loss 0.11369185894727707 Validation loss 0.10135319828987122 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5580],\n",
      "        [0.5682]], device='mps:0')\n",
      "Iteration 45960 Training loss 0.11061511933803558 Validation loss 0.10134874284267426 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4583],\n",
      "        [0.4197]], device='mps:0')\n",
      "Iteration 45970 Training loss 0.1040431410074234 Validation loss 0.10134776681661606 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3817],\n",
      "        [0.5339]], device='mps:0')\n",
      "Iteration 45980 Training loss 0.10602331161499023 Validation loss 0.101348377764225 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.2026],\n",
      "        [0.5509]], device='mps:0')\n",
      "Iteration 45990 Training loss 0.09461251646280289 Validation loss 0.10135606676340103 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3654],\n",
      "        [0.6381]], device='mps:0')\n",
      "Iteration 46000 Training loss 0.09816455841064453 Validation loss 0.1013529896736145 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6705],\n",
      "        [0.6839]], device='mps:0')\n",
      "Iteration 46010 Training loss 0.10458365827798843 Validation loss 0.1013585552573204 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5578],\n",
      "        [0.5820]], device='mps:0')\n",
      "Iteration 46020 Training loss 0.10376330465078354 Validation loss 0.10135447978973389 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3336],\n",
      "        [0.3905]], device='mps:0')\n",
      "Iteration 46030 Training loss 0.09097576886415482 Validation loss 0.10136555135250092 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4956],\n",
      "        [0.4564]], device='mps:0')\n",
      "Iteration 46040 Training loss 0.10571712255477905 Validation loss 0.10139377415180206 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4947],\n",
      "        [0.6763]], device='mps:0')\n",
      "Iteration 46050 Training loss 0.1016957238316536 Validation loss 0.10143611580133438 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.7063],\n",
      "        [0.5860]], device='mps:0')\n",
      "Iteration 46060 Training loss 0.10590406507253647 Validation loss 0.10140460729598999 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5401],\n",
      "        [0.4801]], device='mps:0')\n",
      "Iteration 46070 Training loss 0.11537550389766693 Validation loss 0.10136711597442627 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6070],\n",
      "        [0.7311]], device='mps:0')\n",
      "Iteration 46080 Training loss 0.09624400734901428 Validation loss 0.10136140882968903 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5648],\n",
      "        [0.1933]], device='mps:0')\n",
      "Iteration 46090 Training loss 0.11020652204751968 Validation loss 0.10135035961866379 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2161],\n",
      "        [0.5037]], device='mps:0')\n",
      "Iteration 46100 Training loss 0.1059536337852478 Validation loss 0.10137218981981277 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2576],\n",
      "        [0.6864]], device='mps:0')\n",
      "Iteration 46110 Training loss 0.0995260626077652 Validation loss 0.10137221217155457 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5532],\n",
      "        [0.6722]], device='mps:0')\n",
      "Iteration 46120 Training loss 0.10813264548778534 Validation loss 0.10136523842811584 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3644],\n",
      "        [0.3543]], device='mps:0')\n",
      "Iteration 46130 Training loss 0.1048412099480629 Validation loss 0.10138975828886032 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5414],\n",
      "        [0.6192]], device='mps:0')\n",
      "Iteration 46140 Training loss 0.10021939128637314 Validation loss 0.10134134441614151 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4837],\n",
      "        [0.5253]], device='mps:0')\n",
      "Iteration 46150 Training loss 0.10462329536676407 Validation loss 0.1013655886054039 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6965],\n",
      "        [0.6956]], device='mps:0')\n",
      "Iteration 46160 Training loss 0.09322459995746613 Validation loss 0.10134845972061157 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4678],\n",
      "        [0.4523]], device='mps:0')\n",
      "Iteration 46170 Training loss 0.09907227009534836 Validation loss 0.10140590369701385 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2927],\n",
      "        [0.1914]], device='mps:0')\n",
      "Iteration 46180 Training loss 0.10450393706560135 Validation loss 0.10138262808322906 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5221],\n",
      "        [0.6245]], device='mps:0')\n",
      "Iteration 46190 Training loss 0.09863150119781494 Validation loss 0.10145530849695206 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2368],\n",
      "        [0.5095]], device='mps:0')\n",
      "Iteration 46200 Training loss 0.11554709821939468 Validation loss 0.10152766853570938 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5556],\n",
      "        [0.5971]], device='mps:0')\n",
      "Iteration 46210 Training loss 0.10822108387947083 Validation loss 0.10165652632713318 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.4308],\n",
      "        [0.6263]], device='mps:0')\n",
      "Iteration 46220 Training loss 0.09797950834035873 Validation loss 0.10163040459156036 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.6481],\n",
      "        [0.6184]], device='mps:0')\n",
      "Iteration 46230 Training loss 0.11458799242973328 Validation loss 0.10147593170404434 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.1227],\n",
      "        [0.5511]], device='mps:0')\n",
      "Iteration 46240 Training loss 0.09568248689174652 Validation loss 0.10139793902635574 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6040],\n",
      "        [0.5264]], device='mps:0')\n",
      "Iteration 46250 Training loss 0.11453384160995483 Validation loss 0.1013474389910698 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5576],\n",
      "        [0.2919]], device='mps:0')\n",
      "Iteration 46260 Training loss 0.10988485813140869 Validation loss 0.10135695338249207 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6712],\n",
      "        [0.6452]], device='mps:0')\n",
      "Iteration 46270 Training loss 0.10659052431583405 Validation loss 0.1013379842042923 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5766],\n",
      "        [0.6149]], device='mps:0')\n",
      "Iteration 46280 Training loss 0.1009356677532196 Validation loss 0.1013377457857132 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3212],\n",
      "        [0.6974]], device='mps:0')\n",
      "Iteration 46290 Training loss 0.10834798216819763 Validation loss 0.1013292670249939 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5909],\n",
      "        [0.7333]], device='mps:0')\n",
      "Iteration 46300 Training loss 0.09196710586547852 Validation loss 0.101314976811409 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.6769],\n",
      "        [0.6182]], device='mps:0')\n",
      "Iteration 46310 Training loss 0.09484168887138367 Validation loss 0.10133916139602661 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5193],\n",
      "        [0.4330]], device='mps:0')\n",
      "Iteration 46320 Training loss 0.10145038366317749 Validation loss 0.10135561972856522 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6816],\n",
      "        [0.6143]], device='mps:0')\n",
      "Iteration 46330 Training loss 0.10573159903287888 Validation loss 0.10133328288793564 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.7259],\n",
      "        [0.6055]], device='mps:0')\n",
      "Iteration 46340 Training loss 0.10502281039953232 Validation loss 0.10130887478590012 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4431],\n",
      "        [0.3369]], device='mps:0')\n",
      "Iteration 46350 Training loss 0.10988931357860565 Validation loss 0.10130631923675537 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.1222],\n",
      "        [0.2919]], device='mps:0')\n",
      "Iteration 46360 Training loss 0.09745237231254578 Validation loss 0.10131430625915527 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.1792],\n",
      "        [0.4766]], device='mps:0')\n",
      "Iteration 46370 Training loss 0.09698352962732315 Validation loss 0.10130597651004791 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.2328],\n",
      "        [0.7665]], device='mps:0')\n",
      "Iteration 46380 Training loss 0.10503669828176498 Validation loss 0.10130307823419571 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4692],\n",
      "        [0.3784]], device='mps:0')\n",
      "Iteration 46390 Training loss 0.10842201113700867 Validation loss 0.10130967199802399 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5687],\n",
      "        [0.2282]], device='mps:0')\n",
      "Iteration 46400 Training loss 0.10642734915018082 Validation loss 0.10132559388875961 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3626],\n",
      "        [0.6781]], device='mps:0')\n",
      "Iteration 46410 Training loss 0.10503634810447693 Validation loss 0.10133687406778336 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6627],\n",
      "        [0.5250]], device='mps:0')\n",
      "Iteration 46420 Training loss 0.09797553718090057 Validation loss 0.10133639723062515 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5595],\n",
      "        [0.7442]], device='mps:0')\n",
      "Iteration 46430 Training loss 0.09383956342935562 Validation loss 0.10137017071247101 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4479],\n",
      "        [0.5063]], device='mps:0')\n",
      "Iteration 46440 Training loss 0.09706593304872513 Validation loss 0.10137131065130234 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3848],\n",
      "        [0.4245]], device='mps:0')\n",
      "Iteration 46450 Training loss 0.0941384807229042 Validation loss 0.10135632008314133 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4090],\n",
      "        [0.6809]], device='mps:0')\n",
      "Iteration 46460 Training loss 0.10110887140035629 Validation loss 0.10134817659854889 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.7394],\n",
      "        [0.4281]], device='mps:0')\n",
      "Iteration 46470 Training loss 0.10635026544332504 Validation loss 0.1013234406709671 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6594],\n",
      "        [0.6397]], device='mps:0')\n",
      "Iteration 46480 Training loss 0.09976254403591156 Validation loss 0.1013084426522255 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6888],\n",
      "        [0.5338]], device='mps:0')\n",
      "Iteration 46490 Training loss 0.10745666921138763 Validation loss 0.10129114240407944 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4522],\n",
      "        [0.4601]], device='mps:0')\n",
      "Iteration 46500 Training loss 0.10339459031820297 Validation loss 0.10128326714038849 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4514],\n",
      "        [0.5840]], device='mps:0')\n",
      "Iteration 46510 Training loss 0.10058621317148209 Validation loss 0.10128211975097656 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4421],\n",
      "        [0.4637]], device='mps:0')\n",
      "Iteration 46520 Training loss 0.11682633310556412 Validation loss 0.1012890487909317 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6065],\n",
      "        [0.5229]], device='mps:0')\n",
      "Iteration 46530 Training loss 0.09723974019289017 Validation loss 0.10127954930067062 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5424],\n",
      "        [0.3918]], device='mps:0')\n",
      "Iteration 46540 Training loss 0.09667922556400299 Validation loss 0.10128194093704224 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5639],\n",
      "        [0.2821]], device='mps:0')\n",
      "Iteration 46550 Training loss 0.11389607936143875 Validation loss 0.10127688944339752 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2468],\n",
      "        [0.4983]], device='mps:0')\n",
      "Iteration 46560 Training loss 0.09479979425668716 Validation loss 0.10127967596054077 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3670],\n",
      "        [0.4071]], device='mps:0')\n",
      "Iteration 46570 Training loss 0.09784405678510666 Validation loss 0.10127617418766022 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4873],\n",
      "        [0.2151]], device='mps:0')\n",
      "Iteration 46580 Training loss 0.09632579237222672 Validation loss 0.10127469152212143 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.3212],\n",
      "        [0.4978]], device='mps:0')\n",
      "Iteration 46590 Training loss 0.09520795941352844 Validation loss 0.10126926004886627 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.5319],\n",
      "        [0.3855]], device='mps:0')\n",
      "Iteration 46600 Training loss 0.10718730837106705 Validation loss 0.1012677326798439 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4825],\n",
      "        [0.5275]], device='mps:0')\n",
      "Iteration 46610 Training loss 0.10238448530435562 Validation loss 0.10126786679029465 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.4443],\n",
      "        [0.4374]], device='mps:0')\n",
      "Iteration 46620 Training loss 0.11886865645647049 Validation loss 0.10128438472747803 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2459],\n",
      "        [0.3496]], device='mps:0')\n",
      "Iteration 46630 Training loss 0.10426396131515503 Validation loss 0.10128378868103027 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6194],\n",
      "        [0.5754]], device='mps:0')\n",
      "Iteration 46640 Training loss 0.09347669035196304 Validation loss 0.10128987580537796 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.7234],\n",
      "        [0.3663]], device='mps:0')\n",
      "Iteration 46650 Training loss 0.10176551342010498 Validation loss 0.10127078741788864 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4397],\n",
      "        [0.1320]], device='mps:0')\n",
      "Iteration 46660 Training loss 0.10356864333152771 Validation loss 0.1012643352150917 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6833],\n",
      "        [0.5984]], device='mps:0')\n",
      "Iteration 46670 Training loss 0.10536447167396545 Validation loss 0.10126370191574097 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.5555],\n",
      "        [0.6497]], device='mps:0')\n",
      "Iteration 46680 Training loss 0.0985027477145195 Validation loss 0.10126668214797974 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4786],\n",
      "        [0.4901]], device='mps:0')\n",
      "Iteration 46690 Training loss 0.08942833542823792 Validation loss 0.10126704722642899 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4762],\n",
      "        [0.4931]], device='mps:0')\n",
      "Iteration 46700 Training loss 0.1122368723154068 Validation loss 0.10126039385795593 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.2761],\n",
      "        [0.3630]], device='mps:0')\n",
      "Iteration 46710 Training loss 0.10602472722530365 Validation loss 0.10125957429409027 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.7377],\n",
      "        [0.6631]], device='mps:0')\n",
      "Iteration 46720 Training loss 0.10708370059728622 Validation loss 0.10126222670078278 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5958],\n",
      "        [0.5671]], device='mps:0')\n",
      "Iteration 46730 Training loss 0.10678834468126297 Validation loss 0.10125801712274551 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.3092],\n",
      "        [0.6618]], device='mps:0')\n",
      "Iteration 46740 Training loss 0.11185918748378754 Validation loss 0.1012553945183754 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.6862],\n",
      "        [0.4673]], device='mps:0')\n",
      "Iteration 46750 Training loss 0.1033179834485054 Validation loss 0.1012546718120575 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.2971],\n",
      "        [0.4013]], device='mps:0')\n",
      "Iteration 46760 Training loss 0.11028169095516205 Validation loss 0.10127931833267212 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6128],\n",
      "        [0.5573]], device='mps:0')\n",
      "Iteration 46770 Training loss 0.10503988713026047 Validation loss 0.10128133744001389 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6872],\n",
      "        [0.1056]], device='mps:0')\n",
      "Iteration 46780 Training loss 0.09278419613838196 Validation loss 0.10129771381616592 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6375],\n",
      "        [0.4922]], device='mps:0')\n",
      "Iteration 46790 Training loss 0.11329233646392822 Validation loss 0.10127817094326019 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6385],\n",
      "        [0.1373]], device='mps:0')\n",
      "Iteration 46800 Training loss 0.09712934494018555 Validation loss 0.10126545280218124 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4532],\n",
      "        [0.1640]], device='mps:0')\n",
      "Iteration 46810 Training loss 0.11056302487850189 Validation loss 0.101270891726017 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3970],\n",
      "        [0.3855]], device='mps:0')\n",
      "Iteration 46820 Training loss 0.09652628004550934 Validation loss 0.10128839313983917 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6711],\n",
      "        [0.6911]], device='mps:0')\n",
      "Iteration 46830 Training loss 0.110821932554245 Validation loss 0.10126330703496933 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2631],\n",
      "        [0.6412]], device='mps:0')\n",
      "Iteration 46840 Training loss 0.10419823229312897 Validation loss 0.10127118229866028 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4504],\n",
      "        [0.5500]], device='mps:0')\n",
      "Iteration 46850 Training loss 0.09818936884403229 Validation loss 0.10128255933523178 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2344],\n",
      "        [0.3992]], device='mps:0')\n",
      "Iteration 46860 Training loss 0.08832982927560806 Validation loss 0.1012723445892334 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3877],\n",
      "        [0.5819]], device='mps:0')\n",
      "Iteration 46870 Training loss 0.09584616124629974 Validation loss 0.10124386847019196 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5280],\n",
      "        [0.4915]], device='mps:0')\n",
      "Iteration 46880 Training loss 0.10795174539089203 Validation loss 0.10124418884515762 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5434],\n",
      "        [0.3786]], device='mps:0')\n",
      "Iteration 46890 Training loss 0.11214514821767807 Validation loss 0.10123784095048904 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6224],\n",
      "        [0.6000]], device='mps:0')\n",
      "Iteration 46900 Training loss 0.10132234543561935 Validation loss 0.10123620182275772 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.8298],\n",
      "        [0.5588]], device='mps:0')\n",
      "Iteration 46910 Training loss 0.09971393644809723 Validation loss 0.1012592762708664 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3529],\n",
      "        [0.4786]], device='mps:0')\n",
      "Iteration 46920 Training loss 0.1058262288570404 Validation loss 0.1012353003025055 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3729],\n",
      "        [0.3870]], device='mps:0')\n",
      "Iteration 46930 Training loss 0.10482624173164368 Validation loss 0.10123176872730255 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5446],\n",
      "        [0.5817]], device='mps:0')\n",
      "Iteration 46940 Training loss 0.09343838691711426 Validation loss 0.10122917592525482 Accuracy 0.7085000276565552\n",
      "Output tensor([[0.3761],\n",
      "        [0.5203]], device='mps:0')\n",
      "Iteration 46950 Training loss 0.09923974424600601 Validation loss 0.10125139355659485 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5683],\n",
      "        [0.5087]], device='mps:0')\n",
      "Iteration 46960 Training loss 0.10501567274332047 Validation loss 0.10124093294143677 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5570],\n",
      "        [0.3753]], device='mps:0')\n",
      "Iteration 46970 Training loss 0.09613344818353653 Validation loss 0.10123541951179504 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3124],\n",
      "        [0.3780]], device='mps:0')\n",
      "Iteration 46980 Training loss 0.10461480915546417 Validation loss 0.10128335654735565 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6806],\n",
      "        [0.3783]], device='mps:0')\n",
      "Iteration 46990 Training loss 0.09866682440042496 Validation loss 0.1012558564543724 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4145],\n",
      "        [0.4324]], device='mps:0')\n",
      "Iteration 47000 Training loss 0.09898196905851364 Validation loss 0.10127758979797363 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6617],\n",
      "        [0.7445]], device='mps:0')\n",
      "Iteration 47010 Training loss 0.1009664312005043 Validation loss 0.10129663348197937 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6624],\n",
      "        [0.6165]], device='mps:0')\n",
      "Iteration 47020 Training loss 0.09567805379629135 Validation loss 0.10131232440471649 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5137],\n",
      "        [0.6514]], device='mps:0')\n",
      "Iteration 47030 Training loss 0.11171232163906097 Validation loss 0.10133246332406998 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4768],\n",
      "        [0.4805]], device='mps:0')\n",
      "Iteration 47040 Training loss 0.10795615613460541 Validation loss 0.10131975263357162 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4822],\n",
      "        [0.4164]], device='mps:0')\n",
      "Iteration 47050 Training loss 0.1036488488316536 Validation loss 0.10133340954780579 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.1586],\n",
      "        [0.6875]], device='mps:0')\n",
      "Iteration 47060 Training loss 0.10716637969017029 Validation loss 0.10127557814121246 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6755],\n",
      "        [0.6834]], device='mps:0')\n",
      "Iteration 47070 Training loss 0.09715722501277924 Validation loss 0.10125336050987244 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5614],\n",
      "        [0.5147]], device='mps:0')\n",
      "Iteration 47080 Training loss 0.09545591473579407 Validation loss 0.10125941783189774 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3355],\n",
      "        [0.4989]], device='mps:0')\n",
      "Iteration 47090 Training loss 0.11787605285644531 Validation loss 0.1012628972530365 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.7290],\n",
      "        [0.4012]], device='mps:0')\n",
      "Iteration 47100 Training loss 0.11195731908082962 Validation loss 0.10123509913682938 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6373],\n",
      "        [0.5999]], device='mps:0')\n",
      "Iteration 47110 Training loss 0.1108112707734108 Validation loss 0.10121262818574905 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.5247],\n",
      "        [0.5135]], device='mps:0')\n",
      "Iteration 47120 Training loss 0.09024568647146225 Validation loss 0.101219043135643 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5906],\n",
      "        [0.7068]], device='mps:0')\n",
      "Iteration 47130 Training loss 0.10648001730442047 Validation loss 0.10121291130781174 Accuracy 0.7080000042915344\n",
      "Output tensor([[0.2230],\n",
      "        [0.1649]], device='mps:0')\n",
      "Iteration 47140 Training loss 0.10070163756608963 Validation loss 0.10122881084680557 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6116],\n",
      "        [0.7376]], device='mps:0')\n",
      "Iteration 47150 Training loss 0.09944780170917511 Validation loss 0.10126330703496933 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2365],\n",
      "        [0.2014]], device='mps:0')\n",
      "Iteration 47160 Training loss 0.1050475463271141 Validation loss 0.10123416036367416 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6687],\n",
      "        [0.6186]], device='mps:0')\n",
      "Iteration 47170 Training loss 0.11194062978029251 Validation loss 0.10122805088758469 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3751],\n",
      "        [0.7128]], device='mps:0')\n",
      "Iteration 47180 Training loss 0.10109639167785645 Validation loss 0.101253941655159 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6759],\n",
      "        [0.5759]], device='mps:0')\n",
      "Iteration 47190 Training loss 0.10049112141132355 Validation loss 0.10124378651380539 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5044],\n",
      "        [0.3842]], device='mps:0')\n",
      "Iteration 47200 Training loss 0.09557686001062393 Validation loss 0.10123208165168762 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7012],\n",
      "        [0.5816]], device='mps:0')\n",
      "Iteration 47210 Training loss 0.10102622956037521 Validation loss 0.10121803730726242 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5840],\n",
      "        [0.5727]], device='mps:0')\n",
      "Iteration 47220 Training loss 0.10940924286842346 Validation loss 0.10120461881160736 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.3036],\n",
      "        [0.4741]], device='mps:0')\n",
      "Iteration 47230 Training loss 0.10697196424007416 Validation loss 0.10120327025651932 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.7271],\n",
      "        [0.6558]], device='mps:0')\n",
      "Iteration 47240 Training loss 0.10508719086647034 Validation loss 0.10123811662197113 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5454],\n",
      "        [0.7047]], device='mps:0')\n",
      "Iteration 47250 Training loss 0.09716079384088516 Validation loss 0.10124260187149048 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6387],\n",
      "        [0.1920]], device='mps:0')\n",
      "Iteration 47260 Training loss 0.116830013692379 Validation loss 0.10124307870864868 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4690],\n",
      "        [0.5797]], device='mps:0')\n",
      "Iteration 47270 Training loss 0.0916864201426506 Validation loss 0.10122627764940262 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5797],\n",
      "        [0.5275]], device='mps:0')\n",
      "Iteration 47280 Training loss 0.09981817752122879 Validation loss 0.10122550278902054 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2180],\n",
      "        [0.5256]], device='mps:0')\n",
      "Iteration 47290 Training loss 0.09922505915164948 Validation loss 0.1012500524520874 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5132],\n",
      "        [0.4723]], device='mps:0')\n",
      "Iteration 47300 Training loss 0.10785647481679916 Validation loss 0.10122165083885193 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4412],\n",
      "        [0.6107]], device='mps:0')\n",
      "Iteration 47310 Training loss 0.10772287100553513 Validation loss 0.10120885074138641 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6802],\n",
      "        [0.3254]], device='mps:0')\n",
      "Iteration 47320 Training loss 0.0950004905462265 Validation loss 0.1012207418680191 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5999],\n",
      "        [0.4022]], device='mps:0')\n",
      "Iteration 47330 Training loss 0.10677501559257507 Validation loss 0.10121016204357147 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5921],\n",
      "        [0.4469]], device='mps:0')\n",
      "Iteration 47340 Training loss 0.10945195704698563 Validation loss 0.10121634602546692 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6495],\n",
      "        [0.4571]], device='mps:0')\n",
      "Iteration 47350 Training loss 0.09909923374652863 Validation loss 0.10120349377393723 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4186],\n",
      "        [0.6024]], device='mps:0')\n",
      "Iteration 47360 Training loss 0.11473731696605682 Validation loss 0.1011965423822403 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5502],\n",
      "        [0.6051]], device='mps:0')\n",
      "Iteration 47370 Training loss 0.10573073476552963 Validation loss 0.10121779143810272 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6364],\n",
      "        [0.5342]], device='mps:0')\n",
      "Iteration 47380 Training loss 0.09910522401332855 Validation loss 0.10120244324207306 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5399],\n",
      "        [0.5684]], device='mps:0')\n",
      "Iteration 47390 Training loss 0.10376380383968353 Validation loss 0.10121425241231918 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.0825],\n",
      "        [0.2519]], device='mps:0')\n",
      "Iteration 47400 Training loss 0.09305541962385178 Validation loss 0.10120457410812378 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5543],\n",
      "        [0.6852]], device='mps:0')\n",
      "Iteration 47410 Training loss 0.09924917668104172 Validation loss 0.1012076735496521 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6193],\n",
      "        [0.5147]], device='mps:0')\n",
      "Iteration 47420 Training loss 0.0976044312119484 Validation loss 0.10120068490505219 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.1133],\n",
      "        [0.6537]], device='mps:0')\n",
      "Iteration 47430 Training loss 0.1110643818974495 Validation loss 0.10120154172182083 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5494],\n",
      "        [0.6676]], device='mps:0')\n",
      "Iteration 47440 Training loss 0.11016958951950073 Validation loss 0.10119634121656418 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2172],\n",
      "        [0.5467]], device='mps:0')\n",
      "Iteration 47450 Training loss 0.11100341379642487 Validation loss 0.10118687897920609 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6028],\n",
      "        [0.5949]], device='mps:0')\n",
      "Iteration 47460 Training loss 0.10645896941423416 Validation loss 0.10119429230690002 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5901],\n",
      "        [0.7294]], device='mps:0')\n",
      "Iteration 47470 Training loss 0.10527002066373825 Validation loss 0.10119228810071945 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5069],\n",
      "        [0.3701]], device='mps:0')\n",
      "Iteration 47480 Training loss 0.08901770412921906 Validation loss 0.10120845586061478 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6353],\n",
      "        [0.6924]], device='mps:0')\n",
      "Iteration 47490 Training loss 0.10590288043022156 Validation loss 0.10120854526758194 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6582],\n",
      "        [0.3831]], device='mps:0')\n",
      "Iteration 47500 Training loss 0.10406314581632614 Validation loss 0.10121025890111923 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7005],\n",
      "        [0.1457]], device='mps:0')\n",
      "Iteration 47510 Training loss 0.09388726949691772 Validation loss 0.10120710730552673 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7099],\n",
      "        [0.4893]], device='mps:0')\n",
      "Iteration 47520 Training loss 0.09635620564222336 Validation loss 0.10123499482870102 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5926],\n",
      "        [0.7439]], device='mps:0')\n",
      "Iteration 47530 Training loss 0.10273027420043945 Validation loss 0.10121181607246399 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.1746],\n",
      "        [0.5386]], device='mps:0')\n",
      "Iteration 47540 Training loss 0.10317956656217575 Validation loss 0.10124002397060394 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6102],\n",
      "        [0.2256]], device='mps:0')\n",
      "Iteration 47550 Training loss 0.08900991082191467 Validation loss 0.10122034698724747 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2436],\n",
      "        [0.3142]], device='mps:0')\n",
      "Iteration 47560 Training loss 0.1088751032948494 Validation loss 0.10118070989847183 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6021],\n",
      "        [0.3160]], device='mps:0')\n",
      "Iteration 47570 Training loss 0.10021960735321045 Validation loss 0.10117921978235245 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4394],\n",
      "        [0.2325]], device='mps:0')\n",
      "Iteration 47580 Training loss 0.10947674512863159 Validation loss 0.10118608921766281 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5245],\n",
      "        [0.3763]], device='mps:0')\n",
      "Iteration 47590 Training loss 0.10632003843784332 Validation loss 0.10119827091693878 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4068],\n",
      "        [0.6297]], device='mps:0')\n",
      "Iteration 47600 Training loss 0.10935017466545105 Validation loss 0.1011943593621254 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6646],\n",
      "        [0.5113]], device='mps:0')\n",
      "Iteration 47610 Training loss 0.09910857677459717 Validation loss 0.10118167102336884 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6393],\n",
      "        [0.6402]], device='mps:0')\n",
      "Iteration 47620 Training loss 0.10317059606313705 Validation loss 0.10120931267738342 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.1683],\n",
      "        [0.8235]], device='mps:0')\n",
      "Iteration 47630 Training loss 0.10295646637678146 Validation loss 0.1011880561709404 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2852],\n",
      "        [0.5914]], device='mps:0')\n",
      "Iteration 47640 Training loss 0.11192451417446136 Validation loss 0.10117147862911224 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.4664],\n",
      "        [0.4658]], device='mps:0')\n",
      "Iteration 47650 Training loss 0.09778159856796265 Validation loss 0.10118827223777771 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6174],\n",
      "        [0.5830]], device='mps:0')\n",
      "Iteration 47660 Training loss 0.08654693514108658 Validation loss 0.10122000426054001 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2864],\n",
      "        [0.7340]], device='mps:0')\n",
      "Iteration 47670 Training loss 0.10291574895381927 Validation loss 0.10117004811763763 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3696],\n",
      "        [0.4385]], device='mps:0')\n",
      "Iteration 47680 Training loss 0.11190316826105118 Validation loss 0.10117755085229874 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5832],\n",
      "        [0.3039]], device='mps:0')\n",
      "Iteration 47690 Training loss 0.10516784340143204 Validation loss 0.10120483487844467 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4522],\n",
      "        [0.1992]], device='mps:0')\n",
      "Iteration 47700 Training loss 0.10169870406389236 Validation loss 0.10123462229967117 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5868],\n",
      "        [0.4721]], device='mps:0')\n",
      "Iteration 47710 Training loss 0.10370610654354095 Validation loss 0.10124000906944275 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4535],\n",
      "        [0.3232]], device='mps:0')\n",
      "Iteration 47720 Training loss 0.10831338912248611 Validation loss 0.10134239494800568 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5973],\n",
      "        [0.7489]], device='mps:0')\n",
      "Iteration 47730 Training loss 0.10513364523649216 Validation loss 0.10136101394891739 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4067],\n",
      "        [0.4897]], device='mps:0')\n",
      "Iteration 47740 Training loss 0.10152319073677063 Validation loss 0.10127042233943939 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3689],\n",
      "        [0.5895]], device='mps:0')\n",
      "Iteration 47750 Training loss 0.1009632870554924 Validation loss 0.1012641042470932 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3381],\n",
      "        [0.6543]], device='mps:0')\n",
      "Iteration 47760 Training loss 0.09345000982284546 Validation loss 0.10122519731521606 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4870],\n",
      "        [0.6542]], device='mps:0')\n",
      "Iteration 47770 Training loss 0.10673858225345612 Validation loss 0.10122252255678177 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6792],\n",
      "        [0.3698]], device='mps:0')\n",
      "Iteration 47780 Training loss 0.10898161679506302 Validation loss 0.10119830816984177 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3690],\n",
      "        [0.3708]], device='mps:0')\n",
      "Iteration 47790 Training loss 0.10557069629430771 Validation loss 0.10116554796695709 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5761],\n",
      "        [0.6178]], device='mps:0')\n",
      "Iteration 47800 Training loss 0.10151194036006927 Validation loss 0.10115405917167664 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6868],\n",
      "        [0.7162]], device='mps:0')\n",
      "Iteration 47810 Training loss 0.10263652354478836 Validation loss 0.10116872191429138 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6485],\n",
      "        [0.3763]], device='mps:0')\n",
      "Iteration 47820 Training loss 0.1037697121500969 Validation loss 0.10115260630846024 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7006],\n",
      "        [0.5999]], device='mps:0')\n",
      "Iteration 47830 Training loss 0.10115519911050797 Validation loss 0.10115166753530502 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3584],\n",
      "        [0.5244]], device='mps:0')\n",
      "Iteration 47840 Training loss 0.11853816360235214 Validation loss 0.10115566104650497 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4154],\n",
      "        [0.5727]], device='mps:0')\n",
      "Iteration 47850 Training loss 0.11106986552476883 Validation loss 0.10114166885614395 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5599],\n",
      "        [0.2938]], device='mps:0')\n",
      "Iteration 47860 Training loss 0.10351216793060303 Validation loss 0.10114537179470062 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6493],\n",
      "        [0.4720]], device='mps:0')\n",
      "Iteration 47870 Training loss 0.09148988872766495 Validation loss 0.10114551335573196 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5938],\n",
      "        [0.3462]], device='mps:0')\n",
      "Iteration 47880 Training loss 0.1058601438999176 Validation loss 0.10114630311727524 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5985],\n",
      "        [0.2484]], device='mps:0')\n",
      "Iteration 47890 Training loss 0.10584378987550735 Validation loss 0.10115238279104233 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5559],\n",
      "        [0.4321]], device='mps:0')\n",
      "Iteration 47900 Training loss 0.1019977331161499 Validation loss 0.10113651305437088 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7668],\n",
      "        [0.2471]], device='mps:0')\n",
      "Iteration 47910 Training loss 0.1044052317738533 Validation loss 0.10113196074962616 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.4539],\n",
      "        [0.5121]], device='mps:0')\n",
      "Iteration 47920 Training loss 0.1027996838092804 Validation loss 0.10113271325826645 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4371],\n",
      "        [0.5977]], device='mps:0')\n",
      "Iteration 47930 Training loss 0.1011693924665451 Validation loss 0.10113881528377533 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6593],\n",
      "        [0.3549]], device='mps:0')\n",
      "Iteration 47940 Training loss 0.09702632576227188 Validation loss 0.10113094747066498 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.5082],\n",
      "        [0.5792]], device='mps:0')\n",
      "Iteration 47950 Training loss 0.10516245663166046 Validation loss 0.10114487260580063 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4146],\n",
      "        [0.7072]], device='mps:0')\n",
      "Iteration 47960 Training loss 0.10194152593612671 Validation loss 0.10114229470491409 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6482],\n",
      "        [0.2216]], device='mps:0')\n",
      "Iteration 47970 Training loss 0.10216359794139862 Validation loss 0.10113568603992462 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3060],\n",
      "        [0.2086]], device='mps:0')\n",
      "Iteration 47980 Training loss 0.09331139177083969 Validation loss 0.10116701573133469 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6532],\n",
      "        [0.2099]], device='mps:0')\n",
      "Iteration 47990 Training loss 0.11134990304708481 Validation loss 0.10112317651510239 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5093],\n",
      "        [0.6493]], device='mps:0')\n",
      "Iteration 48000 Training loss 0.09640443325042725 Validation loss 0.1011386513710022 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6743],\n",
      "        [0.4198]], device='mps:0')\n",
      "Iteration 48010 Training loss 0.09804168343544006 Validation loss 0.10112355649471283 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7200],\n",
      "        [0.3210]], device='mps:0')\n",
      "Iteration 48020 Training loss 0.10563843697309494 Validation loss 0.10111896693706512 Accuracy 0.7090000510215759\n",
      "Output tensor([[0.1561],\n",
      "        [0.5963]], device='mps:0')\n",
      "Iteration 48030 Training loss 0.09268172830343246 Validation loss 0.10113635659217834 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2132],\n",
      "        [0.6607]], device='mps:0')\n",
      "Iteration 48040 Training loss 0.11088130623102188 Validation loss 0.1011657863855362 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6467],\n",
      "        [0.2111]], device='mps:0')\n",
      "Iteration 48050 Training loss 0.10399433970451355 Validation loss 0.10115215927362442 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2818],\n",
      "        [0.4928]], device='mps:0')\n",
      "Iteration 48060 Training loss 0.09958437085151672 Validation loss 0.1011517271399498 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4333],\n",
      "        [0.5965]], device='mps:0')\n",
      "Iteration 48070 Training loss 0.10929615050554276 Validation loss 0.101131871342659 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.7237],\n",
      "        [0.7883]], device='mps:0')\n",
      "Iteration 48080 Training loss 0.10183560848236084 Validation loss 0.10116200149059296 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7369],\n",
      "        [0.5739]], device='mps:0')\n",
      "Iteration 48090 Training loss 0.11962581425905228 Validation loss 0.10114265978336334 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5365],\n",
      "        [0.6478]], device='mps:0')\n",
      "Iteration 48100 Training loss 0.09889265149831772 Validation loss 0.10112109035253525 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4657],\n",
      "        [0.4804]], device='mps:0')\n",
      "Iteration 48110 Training loss 0.10222461819648743 Validation loss 0.10112539678812027 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4167],\n",
      "        [0.6518]], device='mps:0')\n",
      "Iteration 48120 Training loss 0.10125056654214859 Validation loss 0.10111475735902786 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6304],\n",
      "        [0.3021]], device='mps:0')\n",
      "Iteration 48130 Training loss 0.10960672795772552 Validation loss 0.1011178195476532 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3845],\n",
      "        [0.6246]], device='mps:0')\n",
      "Iteration 48140 Training loss 0.11301561444997787 Validation loss 0.10113060474395752 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5604],\n",
      "        [0.6227]], device='mps:0')\n",
      "Iteration 48150 Training loss 0.11572480946779251 Validation loss 0.10113546252250671 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3781],\n",
      "        [0.6382]], device='mps:0')\n",
      "Iteration 48160 Training loss 0.09440665692090988 Validation loss 0.10116716474294662 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7292],\n",
      "        [0.3975]], device='mps:0')\n",
      "Iteration 48170 Training loss 0.09809558838605881 Validation loss 0.10113770514726639 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5632],\n",
      "        [0.3311]], device='mps:0')\n",
      "Iteration 48180 Training loss 0.10328978300094604 Validation loss 0.10114673525094986 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5262],\n",
      "        [0.4997]], device='mps:0')\n",
      "Iteration 48190 Training loss 0.1066707968711853 Validation loss 0.10112070292234421 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6954],\n",
      "        [0.3499]], device='mps:0')\n",
      "Iteration 48200 Training loss 0.10102229565382004 Validation loss 0.10111713409423828 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7103],\n",
      "        [0.2292]], device='mps:0')\n",
      "Iteration 48210 Training loss 0.10881857573986053 Validation loss 0.10112115740776062 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3398],\n",
      "        [0.6434]], device='mps:0')\n",
      "Iteration 48220 Training loss 0.10198726505041122 Validation loss 0.10111355036497116 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6298],\n",
      "        [0.4966]], device='mps:0')\n",
      "Iteration 48230 Training loss 0.10583972930908203 Validation loss 0.10110970586538315 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.1535],\n",
      "        [0.4779]], device='mps:0')\n",
      "Iteration 48240 Training loss 0.11461760848760605 Validation loss 0.10111209750175476 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6260],\n",
      "        [0.4541]], device='mps:0')\n",
      "Iteration 48250 Training loss 0.10454711318016052 Validation loss 0.10114718973636627 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3515],\n",
      "        [0.3429]], device='mps:0')\n",
      "Iteration 48260 Training loss 0.110994353890419 Validation loss 0.10111112147569656 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4444],\n",
      "        [0.5892]], device='mps:0')\n",
      "Iteration 48270 Training loss 0.09665481001138687 Validation loss 0.1011108011007309 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6308],\n",
      "        [0.6951]], device='mps:0')\n",
      "Iteration 48280 Training loss 0.10865423083305359 Validation loss 0.10112693160772324 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2170],\n",
      "        [0.1189]], device='mps:0')\n",
      "Iteration 48290 Training loss 0.10840336233377457 Validation loss 0.10112186521291733 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5366],\n",
      "        [0.5522]], device='mps:0')\n",
      "Iteration 48300 Training loss 0.10913324356079102 Validation loss 0.10111261159181595 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.1548],\n",
      "        [0.7278]], device='mps:0')\n",
      "Iteration 48310 Training loss 0.11028985679149628 Validation loss 0.10111014544963837 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6486],\n",
      "        [0.2546]], device='mps:0')\n",
      "Iteration 48320 Training loss 0.11999399960041046 Validation loss 0.10111018270254135 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4068],\n",
      "        [0.3765]], device='mps:0')\n",
      "Iteration 48330 Training loss 0.11887213587760925 Validation loss 0.10114065557718277 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5041],\n",
      "        [0.6792]], device='mps:0')\n",
      "Iteration 48340 Training loss 0.09509148448705673 Validation loss 0.10113859176635742 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2901],\n",
      "        [0.4576]], device='mps:0')\n",
      "Iteration 48350 Training loss 0.09644875675439835 Validation loss 0.10114222764968872 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4703],\n",
      "        [0.5958]], device='mps:0')\n",
      "Iteration 48360 Training loss 0.10724280774593353 Validation loss 0.10111787915229797 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6014],\n",
      "        [0.6570]], device='mps:0')\n",
      "Iteration 48370 Training loss 0.109852634370327 Validation loss 0.1011127382516861 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3315],\n",
      "        [0.6663]], device='mps:0')\n",
      "Iteration 48380 Training loss 0.10849641263484955 Validation loss 0.10111537575721741 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4971],\n",
      "        [0.4272]], device='mps:0')\n",
      "Iteration 48390 Training loss 0.09903419017791748 Validation loss 0.10109812021255493 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6919],\n",
      "        [0.5386]], device='mps:0')\n",
      "Iteration 48400 Training loss 0.1012432873249054 Validation loss 0.101094089448452 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5017],\n",
      "        [0.5507]], device='mps:0')\n",
      "Iteration 48410 Training loss 0.09297943115234375 Validation loss 0.10109490901231766 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6923],\n",
      "        [0.5758]], device='mps:0')\n",
      "Iteration 48420 Training loss 0.11040746420621872 Validation loss 0.10109131038188934 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2392],\n",
      "        [0.1671]], device='mps:0')\n",
      "Iteration 48430 Training loss 0.09584809094667435 Validation loss 0.10110250115394592 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5791],\n",
      "        [0.7159]], device='mps:0')\n",
      "Iteration 48440 Training loss 0.10192893445491791 Validation loss 0.10110844671726227 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6487],\n",
      "        [0.2234]], device='mps:0')\n",
      "Iteration 48450 Training loss 0.12148753553628922 Validation loss 0.10113357752561569 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7229],\n",
      "        [0.5367]], device='mps:0')\n",
      "Iteration 48460 Training loss 0.11245628446340561 Validation loss 0.10112069547176361 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6521],\n",
      "        [0.4132]], device='mps:0')\n",
      "Iteration 48470 Training loss 0.09168153256177902 Validation loss 0.1010911837220192 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2901],\n",
      "        [0.6514]], device='mps:0')\n",
      "Iteration 48480 Training loss 0.10516137629747391 Validation loss 0.10108856856822968 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2363],\n",
      "        [0.5886]], device='mps:0')\n",
      "Iteration 48490 Training loss 0.10522884130477905 Validation loss 0.10109800845384598 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4621],\n",
      "        [0.4719]], device='mps:0')\n",
      "Iteration 48500 Training loss 0.0981016531586647 Validation loss 0.10112565010786057 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1194],\n",
      "        [0.0335]], device='mps:0')\n",
      "Iteration 48510 Training loss 0.10037641227245331 Validation loss 0.10111726820468903 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.1236],\n",
      "        [0.7035]], device='mps:0')\n",
      "Iteration 48520 Training loss 0.10712996870279312 Validation loss 0.10112053155899048 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6625],\n",
      "        [0.5683]], device='mps:0')\n",
      "Iteration 48530 Training loss 0.10395922511816025 Validation loss 0.10110748559236526 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2719],\n",
      "        [0.6246]], device='mps:0')\n",
      "Iteration 48540 Training loss 0.0942852646112442 Validation loss 0.10107722878456116 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3071],\n",
      "        [0.6215]], device='mps:0')\n",
      "Iteration 48550 Training loss 0.11422380059957504 Validation loss 0.10107477009296417 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2990],\n",
      "        [0.4333]], device='mps:0')\n",
      "Iteration 48560 Training loss 0.10244142264127731 Validation loss 0.10107606649398804 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6814],\n",
      "        [0.4071]], device='mps:0')\n",
      "Iteration 48570 Training loss 0.10764911770820618 Validation loss 0.10108455270528793 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6017],\n",
      "        [0.5873]], device='mps:0')\n",
      "Iteration 48580 Training loss 0.11590025573968887 Validation loss 0.101077139377594 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2269],\n",
      "        [0.2468]], device='mps:0')\n",
      "Iteration 48590 Training loss 0.11060715466737747 Validation loss 0.10107730329036713 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.2288],\n",
      "        [0.6172]], device='mps:0')\n",
      "Iteration 48600 Training loss 0.09969425946474075 Validation loss 0.10108133405447006 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4339],\n",
      "        [0.6016]], device='mps:0')\n",
      "Iteration 48610 Training loss 0.10176519304513931 Validation loss 0.10109113156795502 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5407],\n",
      "        [0.5925]], device='mps:0')\n",
      "Iteration 48620 Training loss 0.0967327207326889 Validation loss 0.10109101980924606 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3174],\n",
      "        [0.5952]], device='mps:0')\n",
      "Iteration 48630 Training loss 0.11618261784315109 Validation loss 0.10111469030380249 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5823],\n",
      "        [0.4843]], device='mps:0')\n",
      "Iteration 48640 Training loss 0.10340682417154312 Validation loss 0.10110480338335037 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.1554],\n",
      "        [0.4289]], device='mps:0')\n",
      "Iteration 48650 Training loss 0.1054965928196907 Validation loss 0.10108496993780136 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5043],\n",
      "        [0.6048]], device='mps:0')\n",
      "Iteration 48660 Training loss 0.09756385535001755 Validation loss 0.10111066699028015 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6142],\n",
      "        [0.5286]], device='mps:0')\n",
      "Iteration 48670 Training loss 0.10335872322320938 Validation loss 0.10112089663743973 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4619],\n",
      "        [0.4173]], device='mps:0')\n",
      "Iteration 48680 Training loss 0.0964818000793457 Validation loss 0.10112952440977097 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4187],\n",
      "        [0.1473]], device='mps:0')\n",
      "Iteration 48690 Training loss 0.10652758181095123 Validation loss 0.10109756886959076 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6155],\n",
      "        [0.6892]], device='mps:0')\n",
      "Iteration 48700 Training loss 0.10192220658063889 Validation loss 0.10107488185167313 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4723],\n",
      "        [0.3764]], device='mps:0')\n",
      "Iteration 48710 Training loss 0.10254911333322525 Validation loss 0.10107151418924332 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5309],\n",
      "        [0.4792]], device='mps:0')\n",
      "Iteration 48720 Training loss 0.09335228055715561 Validation loss 0.10108597576618195 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5308],\n",
      "        [0.3105]], device='mps:0')\n",
      "Iteration 48730 Training loss 0.10060696303844452 Validation loss 0.10110962390899658 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6822],\n",
      "        [0.7351]], device='mps:0')\n",
      "Iteration 48740 Training loss 0.11005297303199768 Validation loss 0.10111462324857712 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.7211],\n",
      "        [0.6105]], device='mps:0')\n",
      "Iteration 48750 Training loss 0.10180188715457916 Validation loss 0.101176917552948 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6561],\n",
      "        [0.3090]], device='mps:0')\n",
      "Iteration 48760 Training loss 0.10617818683385849 Validation loss 0.10112683475017548 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4538],\n",
      "        [0.5335]], device='mps:0')\n",
      "Iteration 48770 Training loss 0.09397824108600616 Validation loss 0.10110098123550415 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4263],\n",
      "        [0.3917]], device='mps:0')\n",
      "Iteration 48780 Training loss 0.10266938805580139 Validation loss 0.10107553750276566 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5285],\n",
      "        [0.6705]], device='mps:0')\n",
      "Iteration 48790 Training loss 0.10720871388912201 Validation loss 0.10107865184545517 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6277],\n",
      "        [0.4634]], device='mps:0')\n",
      "Iteration 48800 Training loss 0.1059877946972847 Validation loss 0.10107851028442383 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4276],\n",
      "        [0.1778]], device='mps:0')\n",
      "Iteration 48810 Training loss 0.10154201090335846 Validation loss 0.10109328478574753 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2487],\n",
      "        [0.3130]], device='mps:0')\n",
      "Iteration 48820 Training loss 0.123458132147789 Validation loss 0.10118535161018372 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4460],\n",
      "        [0.7608]], device='mps:0')\n",
      "Iteration 48830 Training loss 0.09692049771547318 Validation loss 0.10116879642009735 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.0756],\n",
      "        [0.6674]], device='mps:0')\n",
      "Iteration 48840 Training loss 0.09878627210855484 Validation loss 0.10117343068122864 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5913],\n",
      "        [0.6665]], device='mps:0')\n",
      "Iteration 48850 Training loss 0.10774967074394226 Validation loss 0.10113877058029175 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5069],\n",
      "        [0.4851]], device='mps:0')\n",
      "Iteration 48860 Training loss 0.10388004034757614 Validation loss 0.10112012922763824 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4967],\n",
      "        [0.6048]], device='mps:0')\n",
      "Iteration 48870 Training loss 0.10838708281517029 Validation loss 0.10106063634157181 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3265],\n",
      "        [0.4712]], device='mps:0')\n",
      "Iteration 48880 Training loss 0.09500232338905334 Validation loss 0.10108348727226257 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5719],\n",
      "        [0.6874]], device='mps:0')\n",
      "Iteration 48890 Training loss 0.09533385187387466 Validation loss 0.10113310068845749 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4620],\n",
      "        [0.4732]], device='mps:0')\n",
      "Iteration 48900 Training loss 0.10223573446273804 Validation loss 0.10111543536186218 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1166],\n",
      "        [0.1455]], device='mps:0')\n",
      "Iteration 48910 Training loss 0.09907329082489014 Validation loss 0.10107507556676865 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4488],\n",
      "        [0.2237]], device='mps:0')\n",
      "Iteration 48920 Training loss 0.10117693990468979 Validation loss 0.10108258575201035 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6598],\n",
      "        [0.3473]], device='mps:0')\n",
      "Iteration 48930 Training loss 0.10034120082855225 Validation loss 0.1010989248752594 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3884],\n",
      "        [0.2537]], device='mps:0')\n",
      "Iteration 48940 Training loss 0.09464078396558762 Validation loss 0.10114739090204239 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4791],\n",
      "        [0.1267]], device='mps:0')\n",
      "Iteration 48950 Training loss 0.10273221880197525 Validation loss 0.10113756358623505 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6566],\n",
      "        [0.6384]], device='mps:0')\n",
      "Iteration 48960 Training loss 0.10816653817892075 Validation loss 0.1011165976524353 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6398],\n",
      "        [0.6244]], device='mps:0')\n",
      "Iteration 48970 Training loss 0.11133954674005508 Validation loss 0.10107136517763138 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2197],\n",
      "        [0.6871]], device='mps:0')\n",
      "Iteration 48980 Training loss 0.10056097060441971 Validation loss 0.10106110572814941 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6507],\n",
      "        [0.3695]], device='mps:0')\n",
      "Iteration 48990 Training loss 0.0988185852766037 Validation loss 0.1010458916425705 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4288],\n",
      "        [0.5626]], device='mps:0')\n",
      "Iteration 49000 Training loss 0.10416267067193985 Validation loss 0.10104198008775711 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3843],\n",
      "        [0.3132]], device='mps:0')\n",
      "Iteration 49010 Training loss 0.10226110368967056 Validation loss 0.10104362666606903 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6709],\n",
      "        [0.5867]], device='mps:0')\n",
      "Iteration 49020 Training loss 0.105686716735363 Validation loss 0.10102283954620361 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2826],\n",
      "        [0.4764]], device='mps:0')\n",
      "Iteration 49030 Training loss 0.10289580374956131 Validation loss 0.10102558881044388 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2139],\n",
      "        [0.4853]], device='mps:0')\n",
      "Iteration 49040 Training loss 0.10465022176504135 Validation loss 0.10102515667676926 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.7146],\n",
      "        [0.2581]], device='mps:0')\n",
      "Iteration 49050 Training loss 0.10867305845022202 Validation loss 0.10102551430463791 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6035],\n",
      "        [0.1783]], device='mps:0')\n",
      "Iteration 49060 Training loss 0.09969809651374817 Validation loss 0.101020947098732 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5989],\n",
      "        [0.3551]], device='mps:0')\n",
      "Iteration 49070 Training loss 0.09656450897455215 Validation loss 0.10104155540466309 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6692],\n",
      "        [0.4669]], device='mps:0')\n",
      "Iteration 49080 Training loss 0.09912553429603577 Validation loss 0.10102196037769318 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3676],\n",
      "        [0.5456]], device='mps:0')\n",
      "Iteration 49090 Training loss 0.10114668309688568 Validation loss 0.10104750841856003 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5662],\n",
      "        [0.5848]], device='mps:0')\n",
      "Iteration 49100 Training loss 0.09726262092590332 Validation loss 0.10105905681848526 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6471],\n",
      "        [0.5540]], device='mps:0')\n",
      "Iteration 49110 Training loss 0.10177907347679138 Validation loss 0.10107127577066422 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5481],\n",
      "        [0.6702]], device='mps:0')\n",
      "Iteration 49120 Training loss 0.10991183668375015 Validation loss 0.10104655474424362 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.2141],\n",
      "        [0.4888]], device='mps:0')\n",
      "Iteration 49130 Training loss 0.09651780873537064 Validation loss 0.10102266818284988 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5451],\n",
      "        [0.4259]], device='mps:0')\n",
      "Iteration 49140 Training loss 0.10716282576322556 Validation loss 0.10100476443767548 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.3698],\n",
      "        [0.5425]], device='mps:0')\n",
      "Iteration 49150 Training loss 0.10278524458408356 Validation loss 0.10101065039634705 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2479],\n",
      "        [0.6102]], device='mps:0')\n",
      "Iteration 49160 Training loss 0.1069800928235054 Validation loss 0.10101243853569031 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5785],\n",
      "        [0.5159]], device='mps:0')\n",
      "Iteration 49170 Training loss 0.10307183861732483 Validation loss 0.10102368891239166 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3860],\n",
      "        [0.4592]], device='mps:0')\n",
      "Iteration 49180 Training loss 0.1061498150229454 Validation loss 0.10099905729293823 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7151],\n",
      "        [0.5838]], device='mps:0')\n",
      "Iteration 49190 Training loss 0.10118869692087173 Validation loss 0.1010049432516098 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6385],\n",
      "        [0.7286]], device='mps:0')\n",
      "Iteration 49200 Training loss 0.1076846644282341 Validation loss 0.1010139212012291 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3429],\n",
      "        [0.6087]], device='mps:0')\n",
      "Iteration 49210 Training loss 0.10488799214363098 Validation loss 0.10100189596414566 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6874],\n",
      "        [0.6970]], device='mps:0')\n",
      "Iteration 49220 Training loss 0.10225201398134232 Validation loss 0.10099649429321289 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3773],\n",
      "        [0.4148]], device='mps:0')\n",
      "Iteration 49230 Training loss 0.09645415097475052 Validation loss 0.10099265724420547 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3471],\n",
      "        [0.4602]], device='mps:0')\n",
      "Iteration 49240 Training loss 0.09927517920732498 Validation loss 0.10099025070667267 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2805],\n",
      "        [0.6380]], device='mps:0')\n",
      "Iteration 49250 Training loss 0.1099943146109581 Validation loss 0.10099558532238007 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6406],\n",
      "        [0.4290]], device='mps:0')\n",
      "Iteration 49260 Training loss 0.11027310788631439 Validation loss 0.10099194198846817 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3230],\n",
      "        [0.7134]], device='mps:0')\n",
      "Iteration 49270 Training loss 0.11940433084964752 Validation loss 0.10100307315587997 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6751],\n",
      "        [0.4053]], device='mps:0')\n",
      "Iteration 49280 Training loss 0.10847283899784088 Validation loss 0.10100430995225906 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6940],\n",
      "        [0.5380]], device='mps:0')\n",
      "Iteration 49290 Training loss 0.09713121503591537 Validation loss 0.10100957751274109 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6393],\n",
      "        [0.4834]], device='mps:0')\n",
      "Iteration 49300 Training loss 0.10750593990087509 Validation loss 0.10099266469478607 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3875],\n",
      "        [0.5065]], device='mps:0')\n",
      "Iteration 49310 Training loss 0.11235582083463669 Validation loss 0.10099527984857559 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7666],\n",
      "        [0.5514]], device='mps:0')\n",
      "Iteration 49320 Training loss 0.1079871654510498 Validation loss 0.10099723935127258 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5321],\n",
      "        [0.2054]], device='mps:0')\n",
      "Iteration 49330 Training loss 0.10227494686841965 Validation loss 0.10099708288908005 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5191],\n",
      "        [0.5310]], device='mps:0')\n",
      "Iteration 49340 Training loss 0.10080372542142868 Validation loss 0.101019486784935 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6460],\n",
      "        [0.7443]], device='mps:0')\n",
      "Iteration 49350 Training loss 0.10481531172990799 Validation loss 0.1010071188211441 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2562],\n",
      "        [0.4450]], device='mps:0')\n",
      "Iteration 49360 Training loss 0.11375594884157181 Validation loss 0.10101642459630966 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4733],\n",
      "        [0.3851]], device='mps:0')\n",
      "Iteration 49370 Training loss 0.10243548452854156 Validation loss 0.10104930400848389 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6293],\n",
      "        [0.4460]], device='mps:0')\n",
      "Iteration 49380 Training loss 0.10594242066144943 Validation loss 0.1010231077671051 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5924],\n",
      "        [0.2514]], device='mps:0')\n",
      "Iteration 49390 Training loss 0.10411225259304047 Validation loss 0.10112018138170242 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3766],\n",
      "        [0.6577]], device='mps:0')\n",
      "Iteration 49400 Training loss 0.09665124863386154 Validation loss 0.10110660642385483 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5292],\n",
      "        [0.6253]], device='mps:0')\n",
      "Iteration 49410 Training loss 0.09341510385274887 Validation loss 0.10102709382772446 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5600],\n",
      "        [0.5851]], device='mps:0')\n",
      "Iteration 49420 Training loss 0.09795993566513062 Validation loss 0.10105962306261063 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4008],\n",
      "        [0.2973]], device='mps:0')\n",
      "Iteration 49430 Training loss 0.09592865407466888 Validation loss 0.10102792829275131 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2326],\n",
      "        [0.4700]], device='mps:0')\n",
      "Iteration 49440 Training loss 0.10875263065099716 Validation loss 0.10098905116319656 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6092],\n",
      "        [0.4622]], device='mps:0')\n",
      "Iteration 49450 Training loss 0.10966193675994873 Validation loss 0.10099219530820847 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6055],\n",
      "        [0.6031]], device='mps:0')\n",
      "Iteration 49460 Training loss 0.10104439407587051 Validation loss 0.10099180787801743 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5069],\n",
      "        [0.6800]], device='mps:0')\n",
      "Iteration 49470 Training loss 0.10617843270301819 Validation loss 0.10099570453166962 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5427],\n",
      "        [0.7010]], device='mps:0')\n",
      "Iteration 49480 Training loss 0.09561015665531158 Validation loss 0.10100802779197693 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6156],\n",
      "        [0.7152]], device='mps:0')\n",
      "Iteration 49490 Training loss 0.10126332193613052 Validation loss 0.10098718851804733 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6161],\n",
      "        [0.6945]], device='mps:0')\n",
      "Iteration 49500 Training loss 0.10619119554758072 Validation loss 0.10098882764577866 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2001],\n",
      "        [0.6844]], device='mps:0')\n",
      "Iteration 49510 Training loss 0.0995062068104744 Validation loss 0.1009819507598877 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7061],\n",
      "        [0.7101]], device='mps:0')\n",
      "Iteration 49520 Training loss 0.10047372430562973 Validation loss 0.10096781700849533 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6323],\n",
      "        [0.2910]], device='mps:0')\n",
      "Iteration 49530 Training loss 0.09480102360248566 Validation loss 0.1009632796049118 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5113],\n",
      "        [0.4550]], device='mps:0')\n",
      "Iteration 49540 Training loss 0.10461634397506714 Validation loss 0.10095977783203125 Accuracy 0.7095000147819519\n",
      "Output tensor([[0.5404],\n",
      "        [0.4807]], device='mps:0')\n",
      "Iteration 49550 Training loss 0.10744672268629074 Validation loss 0.10096558928489685 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3990],\n",
      "        [0.5982]], device='mps:0')\n",
      "Iteration 49560 Training loss 0.10518061369657516 Validation loss 0.10095807164907455 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5212],\n",
      "        [0.6293]], device='mps:0')\n",
      "Iteration 49570 Training loss 0.10039448738098145 Validation loss 0.10096300393342972 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4046],\n",
      "        [0.6065]], device='mps:0')\n",
      "Iteration 49580 Training loss 0.09770192950963974 Validation loss 0.10095757246017456 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5472],\n",
      "        [0.5760]], device='mps:0')\n",
      "Iteration 49590 Training loss 0.10117919743061066 Validation loss 0.10095196962356567 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.1007],\n",
      "        [0.6401]], device='mps:0')\n",
      "Iteration 49600 Training loss 0.09830642491579056 Validation loss 0.10095163434743881 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6631],\n",
      "        [0.5151]], device='mps:0')\n",
      "Iteration 49610 Training loss 0.10635467618703842 Validation loss 0.10095160454511642 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.1135],\n",
      "        [0.5277]], device='mps:0')\n",
      "Iteration 49620 Training loss 0.1054830327630043 Validation loss 0.10095670074224472 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2507],\n",
      "        [0.5676]], device='mps:0')\n",
      "Iteration 49630 Training loss 0.10877681523561478 Validation loss 0.10096326470375061 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4713],\n",
      "        [0.3384]], device='mps:0')\n",
      "Iteration 49640 Training loss 0.1093471348285675 Validation loss 0.10095009952783585 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.3625],\n",
      "        [0.4332]], device='mps:0')\n",
      "Iteration 49650 Training loss 0.11318742483854294 Validation loss 0.10096095502376556 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5187],\n",
      "        [0.5842]], device='mps:0')\n",
      "Iteration 49660 Training loss 0.11037807911634445 Validation loss 0.10095517337322235 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6082],\n",
      "        [0.3132]], device='mps:0')\n",
      "Iteration 49670 Training loss 0.11110585927963257 Validation loss 0.10095712542533875 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7450],\n",
      "        [0.4658]], device='mps:0')\n",
      "Iteration 49680 Training loss 0.09297354519367218 Validation loss 0.10096622258424759 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6139],\n",
      "        [0.6155]], device='mps:0')\n",
      "Iteration 49690 Training loss 0.0965833067893982 Validation loss 0.1009509414434433 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4424],\n",
      "        [0.2127]], device='mps:0')\n",
      "Iteration 49700 Training loss 0.10108622908592224 Validation loss 0.10095598548650742 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6473],\n",
      "        [0.7084]], device='mps:0')\n",
      "Iteration 49710 Training loss 0.1142718493938446 Validation loss 0.10096025466918945 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4551],\n",
      "        [0.6081]], device='mps:0')\n",
      "Iteration 49720 Training loss 0.10632716864347458 Validation loss 0.10097546875476837 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5977],\n",
      "        [0.2895]], device='mps:0')\n",
      "Iteration 49730 Training loss 0.10102597624063492 Validation loss 0.1009911522269249 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4691],\n",
      "        [0.6181]], device='mps:0')\n",
      "Iteration 49740 Training loss 0.11365903913974762 Validation loss 0.10096186399459839 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5784],\n",
      "        [0.5218]], device='mps:0')\n",
      "Iteration 49750 Training loss 0.09867192059755325 Validation loss 0.10097584873437881 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.7441],\n",
      "        [0.5790]], device='mps:0')\n",
      "Iteration 49760 Training loss 0.10120167583227158 Validation loss 0.10096688568592072 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4583],\n",
      "        [0.6087]], device='mps:0')\n",
      "Iteration 49770 Training loss 0.10211397707462311 Validation loss 0.10096681863069534 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5535],\n",
      "        [0.3013]], device='mps:0')\n",
      "Iteration 49780 Training loss 0.1002320647239685 Validation loss 0.10100407898426056 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6724],\n",
      "        [0.6803]], device='mps:0')\n",
      "Iteration 49790 Training loss 0.10600636899471283 Validation loss 0.10101412236690521 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3471],\n",
      "        [0.5940]], device='mps:0')\n",
      "Iteration 49800 Training loss 0.09364309161901474 Validation loss 0.10103355348110199 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3442],\n",
      "        [0.4462]], device='mps:0')\n",
      "Iteration 49810 Training loss 0.09544672071933746 Validation loss 0.10101623833179474 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1352],\n",
      "        [0.3158]], device='mps:0')\n",
      "Iteration 49820 Training loss 0.1005517840385437 Validation loss 0.10105470567941666 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5345],\n",
      "        [0.3074]], device='mps:0')\n",
      "Iteration 49830 Training loss 0.09328969568014145 Validation loss 0.10096066445112228 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2679],\n",
      "        [0.3101]], device='mps:0')\n",
      "Iteration 49840 Training loss 0.09390832483768463 Validation loss 0.10094213485717773 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6200],\n",
      "        [0.6830]], device='mps:0')\n",
      "Iteration 49850 Training loss 0.10732122510671616 Validation loss 0.10097914934158325 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6531],\n",
      "        [0.6540]], device='mps:0')\n",
      "Iteration 49860 Training loss 0.09271083772182465 Validation loss 0.10099799185991287 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4194],\n",
      "        [0.6458]], device='mps:0')\n",
      "Iteration 49870 Training loss 0.09582860767841339 Validation loss 0.10099507868289948 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6257],\n",
      "        [0.7017]], device='mps:0')\n",
      "Iteration 49880 Training loss 0.11270029097795486 Validation loss 0.10094557702541351 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6284],\n",
      "        [0.5376]], device='mps:0')\n",
      "Iteration 49890 Training loss 0.09988027065992355 Validation loss 0.10094640403985977 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.0976],\n",
      "        [0.0951]], device='mps:0')\n",
      "Iteration 49900 Training loss 0.09838708490133286 Validation loss 0.1009410098195076 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4214],\n",
      "        [0.2817]], device='mps:0')\n",
      "Iteration 49910 Training loss 0.10043109953403473 Validation loss 0.10093309730291367 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4291],\n",
      "        [0.2709]], device='mps:0')\n",
      "Iteration 49920 Training loss 0.09321530163288116 Validation loss 0.10093976557254791 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4367],\n",
      "        [0.4090]], device='mps:0')\n",
      "Iteration 49930 Training loss 0.1019548699259758 Validation loss 0.10093706846237183 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4241],\n",
      "        [0.5535]], device='mps:0')\n",
      "Iteration 49940 Training loss 0.10347305238246918 Validation loss 0.10093295574188232 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6601],\n",
      "        [0.4683]], device='mps:0')\n",
      "Iteration 49950 Training loss 0.09974820911884308 Validation loss 0.10094767063856125 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.1691],\n",
      "        [0.3550]], device='mps:0')\n",
      "Iteration 49960 Training loss 0.1040029302239418 Validation loss 0.10094469785690308 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6998],\n",
      "        [0.5578]], device='mps:0')\n",
      "Iteration 49970 Training loss 0.09762999415397644 Validation loss 0.10096099972724915 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6925],\n",
      "        [0.5158]], device='mps:0')\n",
      "Iteration 49980 Training loss 0.10667842626571655 Validation loss 0.10096528381109238 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3978],\n",
      "        [0.4511]], device='mps:0')\n",
      "Iteration 49990 Training loss 0.10042411834001541 Validation loss 0.10093015432357788 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2714],\n",
      "        [0.3758]], device='mps:0')\n",
      "Iteration 50000 Training loss 0.08881471306085587 Validation loss 0.10092512518167496 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4098],\n",
      "        [0.4201]], device='mps:0')\n",
      "Iteration 50010 Training loss 0.10680350661277771 Validation loss 0.10091900825500488 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6199],\n",
      "        [0.1070]], device='mps:0')\n",
      "Iteration 50020 Training loss 0.10746961086988449 Validation loss 0.10094814747571945 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5904],\n",
      "        [0.2631]], device='mps:0')\n",
      "Iteration 50030 Training loss 0.10090770572423935 Validation loss 0.10093248635530472 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6351],\n",
      "        [0.4875]], device='mps:0')\n",
      "Iteration 50040 Training loss 0.10280156135559082 Validation loss 0.10091731697320938 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.1586],\n",
      "        [0.4851]], device='mps:0')\n",
      "Iteration 50050 Training loss 0.1107773408293724 Validation loss 0.10092024505138397 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7376],\n",
      "        [0.6408]], device='mps:0')\n",
      "Iteration 50060 Training loss 0.11220432817935944 Validation loss 0.10092651098966599 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7374],\n",
      "        [0.3126]], device='mps:0')\n",
      "Iteration 50070 Training loss 0.10425780713558197 Validation loss 0.10092569887638092 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4510],\n",
      "        [0.5195]], device='mps:0')\n",
      "Iteration 50080 Training loss 0.11508673429489136 Validation loss 0.10091610252857208 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5910],\n",
      "        [0.7083]], device='mps:0')\n",
      "Iteration 50090 Training loss 0.09848866611719131 Validation loss 0.10091862827539444 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2162],\n",
      "        [0.7002]], device='mps:0')\n",
      "Iteration 50100 Training loss 0.10095208883285522 Validation loss 0.10091539472341537 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5260],\n",
      "        [0.4168]], device='mps:0')\n",
      "Iteration 50110 Training loss 0.09661804139614105 Validation loss 0.10092290490865707 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3997],\n",
      "        [0.6387]], device='mps:0')\n",
      "Iteration 50120 Training loss 0.10083850473165512 Validation loss 0.10092700272798538 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2925],\n",
      "        [0.5214]], device='mps:0')\n",
      "Iteration 50130 Training loss 0.10610534250736237 Validation loss 0.10090980678796768 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7000],\n",
      "        [0.6395]], device='mps:0')\n",
      "Iteration 50140 Training loss 0.11128730326890945 Validation loss 0.10090793669223785 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5437],\n",
      "        [0.3779]], device='mps:0')\n",
      "Iteration 50150 Training loss 0.10754221677780151 Validation loss 0.10093694925308228 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6640],\n",
      "        [0.6248]], device='mps:0')\n",
      "Iteration 50160 Training loss 0.10463506728410721 Validation loss 0.10091737657785416 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5804],\n",
      "        [0.5211]], device='mps:0')\n",
      "Iteration 50170 Training loss 0.09836377948522568 Validation loss 0.10095170885324478 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2751],\n",
      "        [0.5573]], device='mps:0')\n",
      "Iteration 50180 Training loss 0.09708108752965927 Validation loss 0.10098021477460861 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6080],\n",
      "        [0.7636]], device='mps:0')\n",
      "Iteration 50190 Training loss 0.10760768502950668 Validation loss 0.10104109346866608 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5921],\n",
      "        [0.3637]], device='mps:0')\n",
      "Iteration 50200 Training loss 0.10291549563407898 Validation loss 0.10094624012708664 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5557],\n",
      "        [0.5029]], device='mps:0')\n",
      "Iteration 50210 Training loss 0.09501384198665619 Validation loss 0.10094766318798065 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4002],\n",
      "        [0.6186]], device='mps:0')\n",
      "Iteration 50220 Training loss 0.09571763873100281 Validation loss 0.10096718370914459 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5249],\n",
      "        [0.0399]], device='mps:0')\n",
      "Iteration 50230 Training loss 0.10275749862194061 Validation loss 0.10091613233089447 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.1672],\n",
      "        [0.4973]], device='mps:0')\n",
      "Iteration 50240 Training loss 0.11284954100847244 Validation loss 0.10090668499469757 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.5765],\n",
      "        [0.5833]], device='mps:0')\n",
      "Iteration 50250 Training loss 0.10667470097541809 Validation loss 0.10090053081512451 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4569],\n",
      "        [0.0315]], device='mps:0')\n",
      "Iteration 50260 Training loss 0.09913962334394455 Validation loss 0.10089987516403198 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4288],\n",
      "        [0.4388]], device='mps:0')\n",
      "Iteration 50270 Training loss 0.10451659560203552 Validation loss 0.1008988618850708 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6073],\n",
      "        [0.6798]], device='mps:0')\n",
      "Iteration 50280 Training loss 0.10440027713775635 Validation loss 0.10090020298957825 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6302],\n",
      "        [0.5105]], device='mps:0')\n",
      "Iteration 50290 Training loss 0.1075693666934967 Validation loss 0.10094618797302246 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4591],\n",
      "        [0.6193]], device='mps:0')\n",
      "Iteration 50300 Training loss 0.09599321335554123 Validation loss 0.10093958675861359 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6296],\n",
      "        [0.4839]], device='mps:0')\n",
      "Iteration 50310 Training loss 0.10742542892694473 Validation loss 0.10093625634908676 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5363],\n",
      "        [0.6976]], device='mps:0')\n",
      "Iteration 50320 Training loss 0.09264189749956131 Validation loss 0.1009395569562912 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3902],\n",
      "        [0.1911]], device='mps:0')\n",
      "Iteration 50330 Training loss 0.10540327429771423 Validation loss 0.10091710090637207 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.1654],\n",
      "        [0.2294]], device='mps:0')\n",
      "Iteration 50340 Training loss 0.10244476050138474 Validation loss 0.10088835656642914 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6787],\n",
      "        [0.6060]], device='mps:0')\n",
      "Iteration 50350 Training loss 0.08831960707902908 Validation loss 0.10089290887117386 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.0863],\n",
      "        [0.1541]], device='mps:0')\n",
      "Iteration 50360 Training loss 0.10379026085138321 Validation loss 0.10093178600072861 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5447],\n",
      "        [0.2975]], device='mps:0')\n",
      "Iteration 50370 Training loss 0.1116766706109047 Validation loss 0.1009201630949974 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5047],\n",
      "        [0.2182]], device='mps:0')\n",
      "Iteration 50380 Training loss 0.10742776840925217 Validation loss 0.1009739562869072 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.5284],\n",
      "        [0.2942]], device='mps:0')\n",
      "Iteration 50390 Training loss 0.10647676140069962 Validation loss 0.10091996192932129 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4635],\n",
      "        [0.2647]], device='mps:0')\n",
      "Iteration 50400 Training loss 0.10863088071346283 Validation loss 0.1008872240781784 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.2981],\n",
      "        [0.6019]], device='mps:0')\n",
      "Iteration 50410 Training loss 0.0878518596291542 Validation loss 0.10088253021240234 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6021],\n",
      "        [0.1506]], device='mps:0')\n",
      "Iteration 50420 Training loss 0.11216698586940765 Validation loss 0.1008852943778038 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4429],\n",
      "        [0.5745]], device='mps:0')\n",
      "Iteration 50430 Training loss 0.10010192543268204 Validation loss 0.10088732838630676 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5502],\n",
      "        [0.3189]], device='mps:0')\n",
      "Iteration 50440 Training loss 0.1057661771774292 Validation loss 0.10092078894376755 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5520],\n",
      "        [0.4877]], device='mps:0')\n",
      "Iteration 50450 Training loss 0.09781668335199356 Validation loss 0.10092075169086456 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.7032],\n",
      "        [0.3201]], device='mps:0')\n",
      "Iteration 50460 Training loss 0.11516501754522324 Validation loss 0.10088428109884262 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.1126],\n",
      "        [0.5250]], device='mps:0')\n",
      "Iteration 50470 Training loss 0.10038558393716812 Validation loss 0.10090291500091553 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.7913],\n",
      "        [0.6034]], device='mps:0')\n",
      "Iteration 50480 Training loss 0.10541177541017532 Validation loss 0.10091293603181839 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.2873],\n",
      "        [0.2178]], device='mps:0')\n",
      "Iteration 50490 Training loss 0.09345769882202148 Validation loss 0.10091613233089447 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5739],\n",
      "        [0.3202]], device='mps:0')\n",
      "Iteration 50500 Training loss 0.10131336748600006 Validation loss 0.10091330111026764 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.1745],\n",
      "        [0.1112]], device='mps:0')\n",
      "Iteration 50510 Training loss 0.10127469897270203 Validation loss 0.10091815888881683 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.0712],\n",
      "        [0.3939]], device='mps:0')\n",
      "Iteration 50520 Training loss 0.11157193779945374 Validation loss 0.10093622654676437 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6253],\n",
      "        [0.5317]], device='mps:0')\n",
      "Iteration 50530 Training loss 0.09849801659584045 Validation loss 0.10095356404781342 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4877],\n",
      "        [0.6210]], device='mps:0')\n",
      "Iteration 50540 Training loss 0.10451634973287582 Validation loss 0.10097774118185043 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3248],\n",
      "        [0.5507]], device='mps:0')\n",
      "Iteration 50550 Training loss 0.1095631942152977 Validation loss 0.10095828771591187 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6245],\n",
      "        [0.1391]], device='mps:0')\n",
      "Iteration 50560 Training loss 0.11094076931476593 Validation loss 0.10093501955270767 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3061],\n",
      "        [0.2507]], device='mps:0')\n",
      "Iteration 50570 Training loss 0.10639259219169617 Validation loss 0.10089633613824844 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.7260],\n",
      "        [0.6573]], device='mps:0')\n",
      "Iteration 50580 Training loss 0.11327844113111496 Validation loss 0.10092262923717499 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5424],\n",
      "        [0.4668]], device='mps:0')\n",
      "Iteration 50590 Training loss 0.10690884292125702 Validation loss 0.10092192888259888 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2224],\n",
      "        [0.2697]], device='mps:0')\n",
      "Iteration 50600 Training loss 0.10385101288557053 Validation loss 0.10092353075742722 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3794],\n",
      "        [0.5132]], device='mps:0')\n",
      "Iteration 50610 Training loss 0.10455168783664703 Validation loss 0.10088428109884262 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4934],\n",
      "        [0.5598]], device='mps:0')\n",
      "Iteration 50620 Training loss 0.10738325119018555 Validation loss 0.10087140649557114 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4356],\n",
      "        [0.5428]], device='mps:0')\n",
      "Iteration 50630 Training loss 0.09738486260175705 Validation loss 0.10085884481668472 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1148],\n",
      "        [0.6079]], device='mps:0')\n",
      "Iteration 50640 Training loss 0.10802096873521805 Validation loss 0.10085947066545486 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4574],\n",
      "        [0.4594]], device='mps:0')\n",
      "Iteration 50650 Training loss 0.10089504718780518 Validation loss 0.10087672621011734 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4672],\n",
      "        [0.5453]], device='mps:0')\n",
      "Iteration 50660 Training loss 0.10595876723527908 Validation loss 0.10087674111127853 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3275],\n",
      "        [0.6559]], device='mps:0')\n",
      "Iteration 50670 Training loss 0.10443348437547684 Validation loss 0.10085482150316238 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6661],\n",
      "        [0.5376]], device='mps:0')\n",
      "Iteration 50680 Training loss 0.10652093589305878 Validation loss 0.10089094936847687 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.7310],\n",
      "        [0.3207]], device='mps:0')\n",
      "Iteration 50690 Training loss 0.10337421298027039 Validation loss 0.10089865326881409 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6165],\n",
      "        [0.5216]], device='mps:0')\n",
      "Iteration 50700 Training loss 0.10619501024484634 Validation loss 0.10085814446210861 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3768],\n",
      "        [0.3532]], device='mps:0')\n",
      "Iteration 50710 Training loss 0.1090487539768219 Validation loss 0.10086062550544739 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5212],\n",
      "        [0.4575]], device='mps:0')\n",
      "Iteration 50720 Training loss 0.10533344000577927 Validation loss 0.10085690766572952 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6445],\n",
      "        [0.6192]], device='mps:0')\n",
      "Iteration 50730 Training loss 0.10545037686824799 Validation loss 0.10089030116796494 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5412],\n",
      "        [0.5988]], device='mps:0')\n",
      "Iteration 50740 Training loss 0.10689017921686172 Validation loss 0.10089368373155594 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.7333],\n",
      "        [0.6467]], device='mps:0')\n",
      "Iteration 50750 Training loss 0.097403883934021 Validation loss 0.10095345228910446 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2801],\n",
      "        [0.3943]], device='mps:0')\n",
      "Iteration 50760 Training loss 0.08906137198209763 Validation loss 0.10093353688716888 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5443],\n",
      "        [0.7378]], device='mps:0')\n",
      "Iteration 50770 Training loss 0.09747619181871414 Validation loss 0.10089998692274094 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6286],\n",
      "        [0.2580]], device='mps:0')\n",
      "Iteration 50780 Training loss 0.0946345180273056 Validation loss 0.10086310654878616 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7254],\n",
      "        [0.4340]], device='mps:0')\n",
      "Iteration 50790 Training loss 0.09775076061487198 Validation loss 0.10084227472543716 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.1630],\n",
      "        [0.5463]], device='mps:0')\n",
      "Iteration 50800 Training loss 0.11468879133462906 Validation loss 0.10084100812673569 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4187],\n",
      "        [0.2462]], device='mps:0')\n",
      "Iteration 50810 Training loss 0.0991792157292366 Validation loss 0.10084130614995956 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5536],\n",
      "        [0.2394]], device='mps:0')\n",
      "Iteration 50820 Training loss 0.09556028246879578 Validation loss 0.10083749890327454 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5990],\n",
      "        [0.4910]], device='mps:0')\n",
      "Iteration 50830 Training loss 0.10975000262260437 Validation loss 0.10083620995283127 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6344],\n",
      "        [0.4057]], device='mps:0')\n",
      "Iteration 50840 Training loss 0.10712014883756638 Validation loss 0.10085606575012207 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5384],\n",
      "        [0.5718]], device='mps:0')\n",
      "Iteration 50850 Training loss 0.1124599426984787 Validation loss 0.10085204988718033 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5951],\n",
      "        [0.4925]], device='mps:0')\n",
      "Iteration 50860 Training loss 0.0934407040476799 Validation loss 0.10085447877645493 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2204],\n",
      "        [0.4325]], device='mps:0')\n",
      "Iteration 50870 Training loss 0.10005491226911545 Validation loss 0.10083835572004318 Accuracy 0.7100000381469727\n",
      "Output tensor([[0.6660],\n",
      "        [0.3633]], device='mps:0')\n",
      "Iteration 50880 Training loss 0.09218213707208633 Validation loss 0.10085850954055786 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5760],\n",
      "        [0.5311]], device='mps:0')\n",
      "Iteration 50890 Training loss 0.09329888224601746 Validation loss 0.10084578394889832 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4416],\n",
      "        [0.2550]], device='mps:0')\n",
      "Iteration 50900 Training loss 0.1068328469991684 Validation loss 0.10083938390016556 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.4015],\n",
      "        [0.6293]], device='mps:0')\n",
      "Iteration 50910 Training loss 0.1076202541589737 Validation loss 0.10083635151386261 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.7065],\n",
      "        [0.5852]], device='mps:0')\n",
      "Iteration 50920 Training loss 0.09140615165233612 Validation loss 0.10084157437086105 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7352],\n",
      "        [0.5341]], device='mps:0')\n",
      "Iteration 50930 Training loss 0.09365260601043701 Validation loss 0.10084740072488785 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4765],\n",
      "        [0.5779]], device='mps:0')\n",
      "Iteration 50940 Training loss 0.09832476079463959 Validation loss 0.10084221512079239 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5235],\n",
      "        [0.4959]], device='mps:0')\n",
      "Iteration 50950 Training loss 0.09573355317115784 Validation loss 0.10086635500192642 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5315],\n",
      "        [0.6262]], device='mps:0')\n",
      "Iteration 50960 Training loss 0.09482745826244354 Validation loss 0.10089995712041855 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7017],\n",
      "        [0.6273]], device='mps:0')\n",
      "Iteration 50970 Training loss 0.10167025029659271 Validation loss 0.10094652324914932 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3884],\n",
      "        [0.3295]], device='mps:0')\n",
      "Iteration 50980 Training loss 0.0935399979352951 Validation loss 0.1009228527545929 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.2334],\n",
      "        [0.3929]], device='mps:0')\n",
      "Iteration 50990 Training loss 0.11214932054281235 Validation loss 0.10089000314474106 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5918],\n",
      "        [0.3526]], device='mps:0')\n",
      "Iteration 51000 Training loss 0.09969552606344223 Validation loss 0.10093242675065994 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5425],\n",
      "        [0.7462]], device='mps:0')\n",
      "Iteration 51010 Training loss 0.10292254388332367 Validation loss 0.10088203847408295 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2921],\n",
      "        [0.6994]], device='mps:0')\n",
      "Iteration 51020 Training loss 0.09005486220121384 Validation loss 0.10083631426095963 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4858],\n",
      "        [0.3780]], device='mps:0')\n",
      "Iteration 51030 Training loss 0.09992239624261856 Validation loss 0.10085757076740265 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.7481],\n",
      "        [0.5118]], device='mps:0')\n",
      "Iteration 51040 Training loss 0.11982820183038712 Validation loss 0.10087577998638153 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2667],\n",
      "        [0.5101]], device='mps:0')\n",
      "Iteration 51050 Training loss 0.10662771016359329 Validation loss 0.10086029022932053 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.7018],\n",
      "        [0.4545]], device='mps:0')\n",
      "Iteration 51060 Training loss 0.10478021204471588 Validation loss 0.10088971257209778 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3462],\n",
      "        [0.6147]], device='mps:0')\n",
      "Iteration 51070 Training loss 0.1073739230632782 Validation loss 0.10088614374399185 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7058],\n",
      "        [0.6627]], device='mps:0')\n",
      "Iteration 51080 Training loss 0.10701775550842285 Validation loss 0.10097116231918335 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4935],\n",
      "        [0.3256]], device='mps:0')\n",
      "Iteration 51090 Training loss 0.10484954714775085 Validation loss 0.10087823867797852 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1025],\n",
      "        [0.6004]], device='mps:0')\n",
      "Iteration 51100 Training loss 0.10502927750349045 Validation loss 0.10086610913276672 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2133],\n",
      "        [0.3157]], device='mps:0')\n",
      "Iteration 51110 Training loss 0.10079596936702728 Validation loss 0.10083276778459549 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6578],\n",
      "        [0.3047]], device='mps:0')\n",
      "Iteration 51120 Training loss 0.09819915890693665 Validation loss 0.10083630681037903 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4412],\n",
      "        [0.1833]], device='mps:0')\n",
      "Iteration 51130 Training loss 0.09996768832206726 Validation loss 0.10081911087036133 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3837],\n",
      "        [0.4794]], device='mps:0')\n",
      "Iteration 51140 Training loss 0.10784925520420074 Validation loss 0.10086130350828171 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3316],\n",
      "        [0.6743]], device='mps:0')\n",
      "Iteration 51150 Training loss 0.10883159190416336 Validation loss 0.10080862790346146 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.1896],\n",
      "        [0.3358]], device='mps:0')\n",
      "Iteration 51160 Training loss 0.09666000306606293 Validation loss 0.1008077934384346 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7364],\n",
      "        [0.3108]], device='mps:0')\n",
      "Iteration 51170 Training loss 0.1028202623128891 Validation loss 0.10080484300851822 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4022],\n",
      "        [0.5045]], device='mps:0')\n",
      "Iteration 51180 Training loss 0.09822366386651993 Validation loss 0.10082946717739105 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4713],\n",
      "        [0.3607]], device='mps:0')\n",
      "Iteration 51190 Training loss 0.10457094013690948 Validation loss 0.1008295938372612 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6572],\n",
      "        [0.2655]], device='mps:0')\n",
      "Iteration 51200 Training loss 0.09513784199953079 Validation loss 0.10083528608083725 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2273],\n",
      "        [0.6305]], device='mps:0')\n",
      "Iteration 51210 Training loss 0.10595907270908356 Validation loss 0.10081814229488373 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3098],\n",
      "        [0.2668]], device='mps:0')\n",
      "Iteration 51220 Training loss 0.10787612944841385 Validation loss 0.10080162435770035 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6107],\n",
      "        [0.5193]], device='mps:0')\n",
      "Iteration 51230 Training loss 0.1213076189160347 Validation loss 0.10080070793628693 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2117],\n",
      "        [0.4927]], device='mps:0')\n",
      "Iteration 51240 Training loss 0.09869144856929779 Validation loss 0.10080188512802124 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3451],\n",
      "        [0.7530]], device='mps:0')\n",
      "Iteration 51250 Training loss 0.09991201013326645 Validation loss 0.10081055015325546 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3061],\n",
      "        [0.5281]], device='mps:0')\n",
      "Iteration 51260 Training loss 0.1025155633687973 Validation loss 0.10080837458372116 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6881],\n",
      "        [0.1510]], device='mps:0')\n",
      "Iteration 51270 Training loss 0.11430813372135162 Validation loss 0.10083816200494766 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5011],\n",
      "        [0.6343]], device='mps:0')\n",
      "Iteration 51280 Training loss 0.09625184535980225 Validation loss 0.10083141922950745 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3559],\n",
      "        [0.6728]], device='mps:0')\n",
      "Iteration 51290 Training loss 0.12074637413024902 Validation loss 0.10088527202606201 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4527],\n",
      "        [0.6927]], device='mps:0')\n",
      "Iteration 51300 Training loss 0.10472110658884048 Validation loss 0.10081794112920761 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7481],\n",
      "        [0.7171]], device='mps:0')\n",
      "Iteration 51310 Training loss 0.0994444340467453 Validation loss 0.10079969465732574 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6163],\n",
      "        [0.4413]], device='mps:0')\n",
      "Iteration 51320 Training loss 0.10300973802804947 Validation loss 0.10079623758792877 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6690],\n",
      "        [0.5894]], device='mps:0')\n",
      "Iteration 51330 Training loss 0.09096123278141022 Validation loss 0.10079425573348999 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3551],\n",
      "        [0.4883]], device='mps:0')\n",
      "Iteration 51340 Training loss 0.09433315694332123 Validation loss 0.10079433023929596 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3408],\n",
      "        [0.7394]], device='mps:0')\n",
      "Iteration 51350 Training loss 0.09670871496200562 Validation loss 0.10079146176576614 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4425],\n",
      "        [0.2991]], device='mps:0')\n",
      "Iteration 51360 Training loss 0.10027392953634262 Validation loss 0.10079485177993774 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5782],\n",
      "        [0.4901]], device='mps:0')\n",
      "Iteration 51370 Training loss 0.09321913868188858 Validation loss 0.10079669952392578 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4251],\n",
      "        [0.4742]], device='mps:0')\n",
      "Iteration 51380 Training loss 0.10485942661762238 Validation loss 0.10081218183040619 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5126],\n",
      "        [0.3719]], device='mps:0')\n",
      "Iteration 51390 Training loss 0.10082154721021652 Validation loss 0.10078790783882141 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.8314],\n",
      "        [0.6007]], device='mps:0')\n",
      "Iteration 51400 Training loss 0.0958729088306427 Validation loss 0.10079918801784515 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.1390],\n",
      "        [0.6031]], device='mps:0')\n",
      "Iteration 51410 Training loss 0.11633744835853577 Validation loss 0.10081794857978821 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6513],\n",
      "        [0.6476]], device='mps:0')\n",
      "Iteration 51420 Training loss 0.09833893924951553 Validation loss 0.10083229839801788 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.1749],\n",
      "        [0.7095]], device='mps:0')\n",
      "Iteration 51430 Training loss 0.10106465220451355 Validation loss 0.10081730782985687 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3220],\n",
      "        [0.4294]], device='mps:0')\n",
      "Iteration 51440 Training loss 0.09756704419851303 Validation loss 0.10078953951597214 Accuracy 0.7105000615119934\n",
      "Output tensor([[0.6617],\n",
      "        [0.2853]], device='mps:0')\n",
      "Iteration 51450 Training loss 0.09939900785684586 Validation loss 0.10080484300851822 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6362],\n",
      "        [0.5004]], device='mps:0')\n",
      "Iteration 51460 Training loss 0.10099446773529053 Validation loss 0.10080865770578384 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6233],\n",
      "        [0.4966]], device='mps:0')\n",
      "Iteration 51470 Training loss 0.10936076939105988 Validation loss 0.10079723596572876 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3307],\n",
      "        [0.4294]], device='mps:0')\n",
      "Iteration 51480 Training loss 0.103281170129776 Validation loss 0.10077882558107376 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3219],\n",
      "        [0.2997]], device='mps:0')\n",
      "Iteration 51490 Training loss 0.10427909344434738 Validation loss 0.10081663727760315 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6801],\n",
      "        [0.6728]], device='mps:0')\n",
      "Iteration 51500 Training loss 0.10350600630044937 Validation loss 0.1007886454463005 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6749],\n",
      "        [0.1775]], device='mps:0')\n",
      "Iteration 51510 Training loss 0.09651505947113037 Validation loss 0.10080157965421677 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4956],\n",
      "        [0.6034]], device='mps:0')\n",
      "Iteration 51520 Training loss 0.0962604209780693 Validation loss 0.1007847934961319 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6374],\n",
      "        [0.3521]], device='mps:0')\n",
      "Iteration 51530 Training loss 0.10013837367296219 Validation loss 0.1007843092083931 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4769],\n",
      "        [0.3088]], device='mps:0')\n",
      "Iteration 51540 Training loss 0.10908866673707962 Validation loss 0.10080894827842712 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.1170],\n",
      "        [0.4390]], device='mps:0')\n",
      "Iteration 51550 Training loss 0.09636518359184265 Validation loss 0.10080458223819733 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.3018],\n",
      "        [0.4780]], device='mps:0')\n",
      "Iteration 51560 Training loss 0.09964415431022644 Validation loss 0.10081148892641068 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6133],\n",
      "        [0.6307]], device='mps:0')\n",
      "Iteration 51570 Training loss 0.11167469620704651 Validation loss 0.10077371448278427 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7903],\n",
      "        [0.7202]], device='mps:0')\n",
      "Iteration 51580 Training loss 0.11002368479967117 Validation loss 0.10080041736364365 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.6389],\n",
      "        [0.5389]], device='mps:0')\n",
      "Iteration 51590 Training loss 0.10052527487277985 Validation loss 0.10079744458198547 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5088],\n",
      "        [0.6308]], device='mps:0')\n",
      "Iteration 51600 Training loss 0.10215562582015991 Validation loss 0.1008298397064209 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3015],\n",
      "        [0.6407]], device='mps:0')\n",
      "Iteration 51610 Training loss 0.09583297371864319 Validation loss 0.10080800205469131 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6241],\n",
      "        [0.7145]], device='mps:0')\n",
      "Iteration 51620 Training loss 0.10365205258131027 Validation loss 0.10076915472745895 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4941],\n",
      "        [0.6661]], device='mps:0')\n",
      "Iteration 51630 Training loss 0.09948417544364929 Validation loss 0.10081516206264496 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4828],\n",
      "        [0.6095]], device='mps:0')\n",
      "Iteration 51640 Training loss 0.1035645455121994 Validation loss 0.100772924721241 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2740],\n",
      "        [0.4104]], device='mps:0')\n",
      "Iteration 51650 Training loss 0.10330741852521896 Validation loss 0.10076277703046799 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6983],\n",
      "        [0.4838]], device='mps:0')\n",
      "Iteration 51660 Training loss 0.10061151534318924 Validation loss 0.10077470541000366 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5089],\n",
      "        [0.6301]], device='mps:0')\n",
      "Iteration 51670 Training loss 0.09169851988554001 Validation loss 0.10078055411577225 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6016],\n",
      "        [0.2504]], device='mps:0')\n",
      "Iteration 51680 Training loss 0.09611593931913376 Validation loss 0.10079307109117508 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5038],\n",
      "        [0.6008]], device='mps:0')\n",
      "Iteration 51690 Training loss 0.09903115779161453 Validation loss 0.10083840042352676 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4177],\n",
      "        [0.5979]], device='mps:0')\n",
      "Iteration 51700 Training loss 0.10963648557662964 Validation loss 0.10079044103622437 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.4751],\n",
      "        [0.4374]], device='mps:0')\n",
      "Iteration 51710 Training loss 0.10295581817626953 Validation loss 0.1007867231965065 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3587],\n",
      "        [0.5087]], device='mps:0')\n",
      "Iteration 51720 Training loss 0.09763217717409134 Validation loss 0.1007743701338768 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6320],\n",
      "        [0.6572]], device='mps:0')\n",
      "Iteration 51730 Training loss 0.10369935631752014 Validation loss 0.10077960789203644 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5119],\n",
      "        [0.6569]], device='mps:0')\n",
      "Iteration 51740 Training loss 0.10764598846435547 Validation loss 0.10076187551021576 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4020],\n",
      "        [0.2250]], device='mps:0')\n",
      "Iteration 51750 Training loss 0.10907106846570969 Validation loss 0.10075834393501282 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6910],\n",
      "        [0.6130]], device='mps:0')\n",
      "Iteration 51760 Training loss 0.0986352488398552 Validation loss 0.10076697170734406 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5532],\n",
      "        [0.3509]], device='mps:0')\n",
      "Iteration 51770 Training loss 0.10016492754220963 Validation loss 0.10076601058244705 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4245],\n",
      "        [0.4291]], device='mps:0')\n",
      "Iteration 51780 Training loss 0.10946859419345856 Validation loss 0.10076503455638885 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5649],\n",
      "        [0.6955]], device='mps:0')\n",
      "Iteration 51790 Training loss 0.10877034813165665 Validation loss 0.10075856745243073 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3390],\n",
      "        [0.6063]], device='mps:0')\n",
      "Iteration 51800 Training loss 0.10402634739875793 Validation loss 0.10080022364854813 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5619],\n",
      "        [0.4733]], device='mps:0')\n",
      "Iteration 51810 Training loss 0.09173114597797394 Validation loss 0.1007860004901886 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5228],\n",
      "        [0.6287]], device='mps:0')\n",
      "Iteration 51820 Training loss 0.10770475119352341 Validation loss 0.1007731631398201 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.1911],\n",
      "        [0.1507]], device='mps:0')\n",
      "Iteration 51830 Training loss 0.1008189469575882 Validation loss 0.10077664256095886 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7529],\n",
      "        [0.5815]], device='mps:0')\n",
      "Iteration 51840 Training loss 0.09451106935739517 Validation loss 0.10074808448553085 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5220],\n",
      "        [0.6521]], device='mps:0')\n",
      "Iteration 51850 Training loss 0.09927719086408615 Validation loss 0.10074856877326965 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2198],\n",
      "        [0.4338]], device='mps:0')\n",
      "Iteration 51860 Training loss 0.09772304445505142 Validation loss 0.1007460504770279 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.6463],\n",
      "        [0.3179]], device='mps:0')\n",
      "Iteration 51870 Training loss 0.09822516143321991 Validation loss 0.10074867308139801 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6716],\n",
      "        [0.3847]], device='mps:0')\n",
      "Iteration 51880 Training loss 0.08901527523994446 Validation loss 0.10079825669527054 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4734],\n",
      "        [0.7275]], device='mps:0')\n",
      "Iteration 51890 Training loss 0.0928158238530159 Validation loss 0.10079965740442276 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3738],\n",
      "        [0.5652]], device='mps:0')\n",
      "Iteration 51900 Training loss 0.10616130381822586 Validation loss 0.10077104717493057 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6062],\n",
      "        [0.4717]], device='mps:0')\n",
      "Iteration 51910 Training loss 0.11354273557662964 Validation loss 0.10073378682136536 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5158],\n",
      "        [0.6744]], device='mps:0')\n",
      "Iteration 51920 Training loss 0.1031658798456192 Validation loss 0.10073228925466537 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5259],\n",
      "        [0.4878]], device='mps:0')\n",
      "Iteration 51930 Training loss 0.09805694222450256 Validation loss 0.10073620826005936 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.1521],\n",
      "        [0.2779]], device='mps:0')\n",
      "Iteration 51940 Training loss 0.1032312735915184 Validation loss 0.1007382869720459 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3300],\n",
      "        [0.6376]], device='mps:0')\n",
      "Iteration 51950 Training loss 0.10053110122680664 Validation loss 0.10074354708194733 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3328],\n",
      "        [0.6737]], device='mps:0')\n",
      "Iteration 51960 Training loss 0.10178279876708984 Validation loss 0.10073252022266388 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4505],\n",
      "        [0.3380]], device='mps:0')\n",
      "Iteration 51970 Training loss 0.10722064971923828 Validation loss 0.10073018819093704 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7103],\n",
      "        [0.6347]], device='mps:0')\n",
      "Iteration 51980 Training loss 0.11331766843795776 Validation loss 0.10073933750391006 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6306],\n",
      "        [0.3503]], device='mps:0')\n",
      "Iteration 51990 Training loss 0.10202247649431229 Validation loss 0.10075013339519501 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4161],\n",
      "        [0.1119]], device='mps:0')\n",
      "Iteration 52000 Training loss 0.10781510174274445 Validation loss 0.10074983537197113 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6698],\n",
      "        [0.5628]], device='mps:0')\n",
      "Iteration 52010 Training loss 0.09139411896467209 Validation loss 0.10076102614402771 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3073],\n",
      "        [0.5792]], device='mps:0')\n",
      "Iteration 52020 Training loss 0.1040862426161766 Validation loss 0.10080794245004654 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4092],\n",
      "        [0.4325]], device='mps:0')\n",
      "Iteration 52030 Training loss 0.09750860184431076 Validation loss 0.1007688120007515 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6592],\n",
      "        [0.3142]], device='mps:0')\n",
      "Iteration 52040 Training loss 0.1023251861333847 Validation loss 0.10077977180480957 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4646],\n",
      "        [0.3995]], device='mps:0')\n",
      "Iteration 52050 Training loss 0.09389449656009674 Validation loss 0.10073976963758469 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2679],\n",
      "        [0.5448]], device='mps:0')\n",
      "Iteration 52060 Training loss 0.10175016522407532 Validation loss 0.1007443368434906 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6508],\n",
      "        [0.5494]], device='mps:0')\n",
      "Iteration 52070 Training loss 0.11095702648162842 Validation loss 0.10073751956224442 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3602],\n",
      "        [0.6464]], device='mps:0')\n",
      "Iteration 52080 Training loss 0.10516586154699326 Validation loss 0.10071888566017151 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6180],\n",
      "        [0.6723]], device='mps:0')\n",
      "Iteration 52090 Training loss 0.10297133028507233 Validation loss 0.10073407739400864 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6008],\n",
      "        [0.6022]], device='mps:0')\n",
      "Iteration 52100 Training loss 0.11121192574501038 Validation loss 0.10076355189085007 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6953],\n",
      "        [0.3278]], device='mps:0')\n",
      "Iteration 52110 Training loss 0.09631866961717606 Validation loss 0.10077222436666489 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3986],\n",
      "        [0.3890]], device='mps:0')\n",
      "Iteration 52120 Training loss 0.10162626206874847 Validation loss 0.10074629634618759 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5921],\n",
      "        [0.5750]], device='mps:0')\n",
      "Iteration 52130 Training loss 0.09646914154291153 Validation loss 0.10075520724058151 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6798],\n",
      "        [0.6055]], device='mps:0')\n",
      "Iteration 52140 Training loss 0.10909148305654526 Validation loss 0.10073979943990707 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6233],\n",
      "        [0.6108]], device='mps:0')\n",
      "Iteration 52150 Training loss 0.10189100354909897 Validation loss 0.10074472427368164 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6307],\n",
      "        [0.6358]], device='mps:0')\n",
      "Iteration 52160 Training loss 0.11224399507045746 Validation loss 0.10080599784851074 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3692],\n",
      "        [0.7392]], device='mps:0')\n",
      "Iteration 52170 Training loss 0.1052410900592804 Validation loss 0.1008082777261734 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.5388],\n",
      "        [0.5020]], device='mps:0')\n",
      "Iteration 52180 Training loss 0.10464345663785934 Validation loss 0.10078667849302292 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5788],\n",
      "        [0.6102]], device='mps:0')\n",
      "Iteration 52190 Training loss 0.10309495776891708 Validation loss 0.10074705630540848 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3719],\n",
      "        [0.5236]], device='mps:0')\n",
      "Iteration 52200 Training loss 0.11579655110836029 Validation loss 0.10085409134626389 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6324],\n",
      "        [0.6376]], device='mps:0')\n",
      "Iteration 52210 Training loss 0.10809547454118729 Validation loss 0.10086040198802948 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2774],\n",
      "        [0.6437]], device='mps:0')\n",
      "Iteration 52220 Training loss 0.0890754833817482 Validation loss 0.10077214986085892 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.1661],\n",
      "        [0.5218]], device='mps:0')\n",
      "Iteration 52230 Training loss 0.09676191210746765 Validation loss 0.10077791661024094 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5088],\n",
      "        [0.6478]], device='mps:0')\n",
      "Iteration 52240 Training loss 0.10912805050611496 Validation loss 0.10078432410955429 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2989],\n",
      "        [0.7688]], device='mps:0')\n",
      "Iteration 52250 Training loss 0.11608566343784332 Validation loss 0.10075041651725769 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6823],\n",
      "        [0.3054]], device='mps:0')\n",
      "Iteration 52260 Training loss 0.10424352437257767 Validation loss 0.10071434825658798 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5179],\n",
      "        [0.4608]], device='mps:0')\n",
      "Iteration 52270 Training loss 0.0999714657664299 Validation loss 0.10071910917758942 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4098],\n",
      "        [0.7513]], device='mps:0')\n",
      "Iteration 52280 Training loss 0.09775882214307785 Validation loss 0.10070687532424927 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6805],\n",
      "        [0.2289]], device='mps:0')\n",
      "Iteration 52290 Training loss 0.10778830200433731 Validation loss 0.10071168839931488 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5582],\n",
      "        [0.6543]], device='mps:0')\n",
      "Iteration 52300 Training loss 0.10823476314544678 Validation loss 0.10070846974849701 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6569],\n",
      "        [0.7072]], device='mps:0')\n",
      "Iteration 52310 Training loss 0.09173370152711868 Validation loss 0.10070648044347763 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6288],\n",
      "        [0.2019]], device='mps:0')\n",
      "Iteration 52320 Training loss 0.10044524818658829 Validation loss 0.10070203989744186 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.1387],\n",
      "        [0.3365]], device='mps:0')\n",
      "Iteration 52330 Training loss 0.10060161352157593 Validation loss 0.1007097065448761 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4440],\n",
      "        [0.5873]], device='mps:0')\n",
      "Iteration 52340 Training loss 0.10958316922187805 Validation loss 0.10071112960577011 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6217],\n",
      "        [0.5881]], device='mps:0')\n",
      "Iteration 52350 Training loss 0.10388408601284027 Validation loss 0.10072414577007294 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7528],\n",
      "        [0.4786]], device='mps:0')\n",
      "Iteration 52360 Training loss 0.0998522937297821 Validation loss 0.100714311003685 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4520],\n",
      "        [0.4225]], device='mps:0')\n",
      "Iteration 52370 Training loss 0.10157153010368347 Validation loss 0.1007106676697731 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6763],\n",
      "        [0.6465]], device='mps:0')\n",
      "Iteration 52380 Training loss 0.10123033076524734 Validation loss 0.10071215778589249 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5156],\n",
      "        [0.3807]], device='mps:0')\n",
      "Iteration 52390 Training loss 0.09190751612186432 Validation loss 0.10073099285364151 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4756],\n",
      "        [0.5647]], device='mps:0')\n",
      "Iteration 52400 Training loss 0.10722940415143967 Validation loss 0.10078995674848557 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5222],\n",
      "        [0.1798]], device='mps:0')\n",
      "Iteration 52410 Training loss 0.10598398000001907 Validation loss 0.10079576820135117 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.3920],\n",
      "        [0.6475]], device='mps:0')\n",
      "Iteration 52420 Training loss 0.1056537926197052 Validation loss 0.1007951945066452 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.3906],\n",
      "        [0.4124]], device='mps:0')\n",
      "Iteration 52430 Training loss 0.10244365781545639 Validation loss 0.10076634585857391 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6189],\n",
      "        [0.2475]], device='mps:0')\n",
      "Iteration 52440 Training loss 0.10923586785793304 Validation loss 0.10072784125804901 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5327],\n",
      "        [0.5520]], device='mps:0')\n",
      "Iteration 52450 Training loss 0.10708245635032654 Validation loss 0.10073795914649963 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.1653],\n",
      "        [0.2453]], device='mps:0')\n",
      "Iteration 52460 Training loss 0.10972075164318085 Validation loss 0.10069940984249115 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3781],\n",
      "        [0.5035]], device='mps:0')\n",
      "Iteration 52470 Training loss 0.1071866974234581 Validation loss 0.1007101759314537 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6030],\n",
      "        [0.5941]], device='mps:0')\n",
      "Iteration 52480 Training loss 0.09363971650600433 Validation loss 0.10071362555027008 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6360],\n",
      "        [0.5047]], device='mps:0')\n",
      "Iteration 52490 Training loss 0.1056080162525177 Validation loss 0.10073097795248032 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4613],\n",
      "        [0.6605]], device='mps:0')\n",
      "Iteration 52500 Training loss 0.10666831582784653 Validation loss 0.10071699321269989 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4404],\n",
      "        [0.4769]], device='mps:0')\n",
      "Iteration 52510 Training loss 0.11170915514230728 Validation loss 0.10072270780801773 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7797],\n",
      "        [0.4832]], device='mps:0')\n",
      "Iteration 52520 Training loss 0.09716388583183289 Validation loss 0.10070770978927612 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4950],\n",
      "        [0.2834]], device='mps:0')\n",
      "Iteration 52530 Training loss 0.09652558714151382 Validation loss 0.1007377877831459 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4555],\n",
      "        [0.5699]], device='mps:0')\n",
      "Iteration 52540 Training loss 0.09837614744901657 Validation loss 0.10073921829462051 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3736],\n",
      "        [0.4137]], device='mps:0')\n",
      "Iteration 52550 Training loss 0.09948673099279404 Validation loss 0.1007373258471489 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7286],\n",
      "        [0.4498]], device='mps:0')\n",
      "Iteration 52560 Training loss 0.10570993274450302 Validation loss 0.10072129964828491 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.1091],\n",
      "        [0.5408]], device='mps:0')\n",
      "Iteration 52570 Training loss 0.10353130102157593 Validation loss 0.10076536983251572 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1693],\n",
      "        [0.6931]], device='mps:0')\n",
      "Iteration 52580 Training loss 0.08930397033691406 Validation loss 0.10071590542793274 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4307],\n",
      "        [0.5094]], device='mps:0')\n",
      "Iteration 52590 Training loss 0.09920355677604675 Validation loss 0.10069020837545395 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.7610],\n",
      "        [0.6434]], device='mps:0')\n",
      "Iteration 52600 Training loss 0.10303682088851929 Validation loss 0.10068599134683609 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5875],\n",
      "        [0.7358]], device='mps:0')\n",
      "Iteration 52610 Training loss 0.10496556758880615 Validation loss 0.10068289935588837 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4319],\n",
      "        [0.6681]], device='mps:0')\n",
      "Iteration 52620 Training loss 0.11002340167760849 Validation loss 0.10067325830459595 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5780],\n",
      "        [0.5924]], device='mps:0')\n",
      "Iteration 52630 Training loss 0.10456239432096481 Validation loss 0.10070116072893143 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6252],\n",
      "        [0.7475]], device='mps:0')\n",
      "Iteration 52640 Training loss 0.09992370754480362 Validation loss 0.10072911530733109 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3162],\n",
      "        [0.3241]], device='mps:0')\n",
      "Iteration 52650 Training loss 0.10293377935886383 Validation loss 0.10070935636758804 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6274],\n",
      "        [0.6459]], device='mps:0')\n",
      "Iteration 52660 Training loss 0.09700924903154373 Validation loss 0.10073421150445938 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6313],\n",
      "        [0.7485]], device='mps:0')\n",
      "Iteration 52670 Training loss 0.09122113883495331 Validation loss 0.10072459280490875 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.1287],\n",
      "        [0.7643]], device='mps:0')\n",
      "Iteration 52680 Training loss 0.09655679762363434 Validation loss 0.10070652514696121 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3928],\n",
      "        [0.3891]], device='mps:0')\n",
      "Iteration 52690 Training loss 0.11083340644836426 Validation loss 0.10069357603788376 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6069],\n",
      "        [0.5092]], device='mps:0')\n",
      "Iteration 52700 Training loss 0.1094990223646164 Validation loss 0.10067834705114365 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3186],\n",
      "        [0.6930]], device='mps:0')\n",
      "Iteration 52710 Training loss 0.0898871049284935 Validation loss 0.10068120062351227 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6984],\n",
      "        [0.5033]], device='mps:0')\n",
      "Iteration 52720 Training loss 0.09660077095031738 Validation loss 0.10066577047109604 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5646],\n",
      "        [0.4326]], device='mps:0')\n",
      "Iteration 52730 Training loss 0.10642650723457336 Validation loss 0.10066317766904831 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6214],\n",
      "        [0.7231]], device='mps:0')\n",
      "Iteration 52740 Training loss 0.08391053229570389 Validation loss 0.1006762757897377 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.1876],\n",
      "        [0.4234]], device='mps:0')\n",
      "Iteration 52750 Training loss 0.10137735307216644 Validation loss 0.10068690031766891 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4888],\n",
      "        [0.6806]], device='mps:0')\n",
      "Iteration 52760 Training loss 0.10019061714410782 Validation loss 0.10066253691911697 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3691],\n",
      "        [0.4223]], device='mps:0')\n",
      "Iteration 52770 Training loss 0.09871317446231842 Validation loss 0.10067687183618546 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5827],\n",
      "        [0.5271]], device='mps:0')\n",
      "Iteration 52780 Training loss 0.10434909909963608 Validation loss 0.10065790265798569 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5089],\n",
      "        [0.6806]], device='mps:0')\n",
      "Iteration 52790 Training loss 0.10490871220827103 Validation loss 0.10066317766904831 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6441],\n",
      "        [0.6947]], device='mps:0')\n",
      "Iteration 52800 Training loss 0.09772336483001709 Validation loss 0.10066118091344833 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5073],\n",
      "        [0.5081]], device='mps:0')\n",
      "Iteration 52810 Training loss 0.09514125436544418 Validation loss 0.10066350549459457 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6437],\n",
      "        [0.5801]], device='mps:0')\n",
      "Iteration 52820 Training loss 0.10618992894887924 Validation loss 0.1006581112742424 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4730],\n",
      "        [0.3009]], device='mps:0')\n",
      "Iteration 52830 Training loss 0.10057671368122101 Validation loss 0.10068575292825699 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4868],\n",
      "        [0.1020]], device='mps:0')\n",
      "Iteration 52840 Training loss 0.12176267057657242 Validation loss 0.10076539218425751 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6273],\n",
      "        [0.4728]], device='mps:0')\n",
      "Iteration 52850 Training loss 0.10676248371601105 Validation loss 0.1007445752620697 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6009],\n",
      "        [0.2960]], device='mps:0')\n",
      "Iteration 52860 Training loss 0.09822467714548111 Validation loss 0.1007101759314537 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5469],\n",
      "        [0.1247]], device='mps:0')\n",
      "Iteration 52870 Training loss 0.10278407484292984 Validation loss 0.10069679468870163 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3129],\n",
      "        [0.5113]], device='mps:0')\n",
      "Iteration 52880 Training loss 0.11099304258823395 Validation loss 0.10071923583745956 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2285],\n",
      "        [0.1845]], device='mps:0')\n",
      "Iteration 52890 Training loss 0.1142362654209137 Validation loss 0.10069327056407928 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4326],\n",
      "        [0.2910]], device='mps:0')\n",
      "Iteration 52900 Training loss 0.1036129742860794 Validation loss 0.10066159814596176 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6054],\n",
      "        [0.3227]], device='mps:0')\n",
      "Iteration 52910 Training loss 0.10201523452997208 Validation loss 0.1006593406200409 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6976],\n",
      "        [0.5374]], device='mps:0')\n",
      "Iteration 52920 Training loss 0.09655895084142685 Validation loss 0.1006539985537529 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6605],\n",
      "        [0.5691]], device='mps:0')\n",
      "Iteration 52930 Training loss 0.1251131147146225 Validation loss 0.10064350068569183 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.5444],\n",
      "        [0.5870]], device='mps:0')\n",
      "Iteration 52940 Training loss 0.10620249807834625 Validation loss 0.10066252946853638 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5268],\n",
      "        [0.6352]], device='mps:0')\n",
      "Iteration 52950 Training loss 0.10042577236890793 Validation loss 0.10065163671970367 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.1254],\n",
      "        [0.5616]], device='mps:0')\n",
      "Iteration 52960 Training loss 0.09851854294538498 Validation loss 0.10068529099225998 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5714],\n",
      "        [0.6023]], device='mps:0')\n",
      "Iteration 52970 Training loss 0.0936017856001854 Validation loss 0.10066872090101242 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5425],\n",
      "        [0.3439]], device='mps:0')\n",
      "Iteration 52980 Training loss 0.09958887100219727 Validation loss 0.10072001814842224 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6187],\n",
      "        [0.4479]], device='mps:0')\n",
      "Iteration 52990 Training loss 0.10074864327907562 Validation loss 0.10073278844356537 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6044],\n",
      "        [0.2432]], device='mps:0')\n",
      "Iteration 53000 Training loss 0.10099536925554276 Validation loss 0.10066374391317368 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4170],\n",
      "        [0.3698]], device='mps:0')\n",
      "Iteration 53010 Training loss 0.09545545279979706 Validation loss 0.10066962987184525 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4459],\n",
      "        [0.4868]], device='mps:0')\n",
      "Iteration 53020 Training loss 0.1018892452120781 Validation loss 0.10067632049322128 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4548],\n",
      "        [0.5436]], device='mps:0')\n",
      "Iteration 53030 Training loss 0.10534897446632385 Validation loss 0.10071911662817001 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5297],\n",
      "        [0.3190]], device='mps:0')\n",
      "Iteration 53040 Training loss 0.0890081450343132 Validation loss 0.10069064795970917 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3613],\n",
      "        [0.2722]], device='mps:0')\n",
      "Iteration 53050 Training loss 0.1030469462275505 Validation loss 0.10070977360010147 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5695],\n",
      "        [0.6067]], device='mps:0')\n",
      "Iteration 53060 Training loss 0.09756854176521301 Validation loss 0.10071959346532822 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2874],\n",
      "        [0.2202]], device='mps:0')\n",
      "Iteration 53070 Training loss 0.09843696653842926 Validation loss 0.10069447755813599 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.8007],\n",
      "        [0.6664]], device='mps:0')\n",
      "Iteration 53080 Training loss 0.09291449189186096 Validation loss 0.10066307336091995 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6752],\n",
      "        [0.3169]], device='mps:0')\n",
      "Iteration 53090 Training loss 0.10917685925960541 Validation loss 0.10070319473743439 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3626],\n",
      "        [0.4716]], device='mps:0')\n",
      "Iteration 53100 Training loss 0.10584971308708191 Validation loss 0.10067103058099747 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6330],\n",
      "        [0.5640]], device='mps:0')\n",
      "Iteration 53110 Training loss 0.10992661863565445 Validation loss 0.1006729006767273 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2393],\n",
      "        [0.5006]], device='mps:0')\n",
      "Iteration 53120 Training loss 0.10686017572879791 Validation loss 0.10065962374210358 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4554],\n",
      "        [0.6689]], device='mps:0')\n",
      "Iteration 53130 Training loss 0.09857454150915146 Validation loss 0.10064084082841873 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2101],\n",
      "        [0.6156]], device='mps:0')\n",
      "Iteration 53140 Training loss 0.09033824503421783 Validation loss 0.10066475719213486 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4982],\n",
      "        [0.6000]], device='mps:0')\n",
      "Iteration 53150 Training loss 0.10439196974039078 Validation loss 0.10064012557268143 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3576],\n",
      "        [0.2900]], device='mps:0')\n",
      "Iteration 53160 Training loss 0.11155612766742706 Validation loss 0.1006249263882637 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6233],\n",
      "        [0.2494]], device='mps:0')\n",
      "Iteration 53170 Training loss 0.10220000892877579 Validation loss 0.10064934194087982 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.7168],\n",
      "        [0.3797]], device='mps:0')\n",
      "Iteration 53180 Training loss 0.09662351757287979 Validation loss 0.10063347220420837 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.1855],\n",
      "        [0.7103]], device='mps:0')\n",
      "Iteration 53190 Training loss 0.08888212591409683 Validation loss 0.10061895102262497 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2305],\n",
      "        [0.5706]], device='mps:0')\n",
      "Iteration 53200 Training loss 0.1060277596116066 Validation loss 0.1006249263882637 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.7172],\n",
      "        [0.3124]], device='mps:0')\n",
      "Iteration 53210 Training loss 0.10154208540916443 Validation loss 0.10065746307373047 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4997],\n",
      "        [0.6041]], device='mps:0')\n",
      "Iteration 53220 Training loss 0.10131336003541946 Validation loss 0.10065172612667084 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4419],\n",
      "        [0.6426]], device='mps:0')\n",
      "Iteration 53230 Training loss 0.10693863034248352 Validation loss 0.10068215429782867 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3426],\n",
      "        [0.6896]], device='mps:0')\n",
      "Iteration 53240 Training loss 0.1068665161728859 Validation loss 0.10072126984596252 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5116],\n",
      "        [0.5196]], device='mps:0')\n",
      "Iteration 53250 Training loss 0.09230178594589233 Validation loss 0.10069223493337631 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4763],\n",
      "        [0.5663]], device='mps:0')\n",
      "Iteration 53260 Training loss 0.10288290679454803 Validation loss 0.10069799423217773 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3594],\n",
      "        [0.3995]], device='mps:0')\n",
      "Iteration 53270 Training loss 0.09765662252902985 Validation loss 0.10062012076377869 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5489],\n",
      "        [0.2300]], device='mps:0')\n",
      "Iteration 53280 Training loss 0.10300120711326599 Validation loss 0.1006765067577362 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4633],\n",
      "        [0.5839]], device='mps:0')\n",
      "Iteration 53290 Training loss 0.09683702886104584 Validation loss 0.10063298046588898 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6228],\n",
      "        [0.2063]], device='mps:0')\n",
      "Iteration 53300 Training loss 0.11088863760232925 Validation loss 0.10062236338853836 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6243],\n",
      "        [0.0929]], device='mps:0')\n",
      "Iteration 53310 Training loss 0.1014726459980011 Validation loss 0.10061332583427429 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6228],\n",
      "        [0.7096]], device='mps:0')\n",
      "Iteration 53320 Training loss 0.10287519544363022 Validation loss 0.10062187910079956 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4571],\n",
      "        [0.4565]], device='mps:0')\n",
      "Iteration 53330 Training loss 0.10747803747653961 Validation loss 0.10062143206596375 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3421],\n",
      "        [0.6604]], device='mps:0')\n",
      "Iteration 53340 Training loss 0.0936378762125969 Validation loss 0.10061094164848328 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2534],\n",
      "        [0.5410]], device='mps:0')\n",
      "Iteration 53350 Training loss 0.09836810827255249 Validation loss 0.10066340118646622 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3225],\n",
      "        [0.3696]], device='mps:0')\n",
      "Iteration 53360 Training loss 0.09342065453529358 Validation loss 0.10062938183546066 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7993],\n",
      "        [0.5587]], device='mps:0')\n",
      "Iteration 53370 Training loss 0.10384183377027512 Validation loss 0.10062383860349655 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7186],\n",
      "        [0.3062]], device='mps:0')\n",
      "Iteration 53380 Training loss 0.10297710448503494 Validation loss 0.10061115026473999 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4504],\n",
      "        [0.6233]], device='mps:0')\n",
      "Iteration 53390 Training loss 0.1070033609867096 Validation loss 0.10060979425907135 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3403],\n",
      "        [0.1814]], device='mps:0')\n",
      "Iteration 53400 Training loss 0.1028132289648056 Validation loss 0.1006331592798233 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6361],\n",
      "        [0.6252]], device='mps:0')\n",
      "Iteration 53410 Training loss 0.0947219729423523 Validation loss 0.10066724568605423 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4040],\n",
      "        [0.3288]], device='mps:0')\n",
      "Iteration 53420 Training loss 0.09240725636482239 Validation loss 0.10064896196126938 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5613],\n",
      "        [0.4200]], device='mps:0')\n",
      "Iteration 53430 Training loss 0.1010042354464531 Validation loss 0.10060155391693115 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5753],\n",
      "        [0.7139]], device='mps:0')\n",
      "Iteration 53440 Training loss 0.10517529398202896 Validation loss 0.10061176121234894 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6958],\n",
      "        [0.6141]], device='mps:0')\n",
      "Iteration 53450 Training loss 0.10413268953561783 Validation loss 0.10063906759023666 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2502],\n",
      "        [0.5543]], device='mps:0')\n",
      "Iteration 53460 Training loss 0.10948791354894638 Validation loss 0.10061218589544296 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5774],\n",
      "        [0.5440]], device='mps:0')\n",
      "Iteration 53470 Training loss 0.09459278732538223 Validation loss 0.10059350728988647 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5382],\n",
      "        [0.6511]], device='mps:0')\n",
      "Iteration 53480 Training loss 0.08928694576025009 Validation loss 0.10061865299940109 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6361],\n",
      "        [0.3891]], device='mps:0')\n",
      "Iteration 53490 Training loss 0.10788711160421371 Validation loss 0.10064934194087982 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6795],\n",
      "        [0.6217]], device='mps:0')\n",
      "Iteration 53500 Training loss 0.09335248917341232 Validation loss 0.1006511002779007 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2721],\n",
      "        [0.6925]], device='mps:0')\n",
      "Iteration 53510 Training loss 0.09902101010084152 Validation loss 0.10064896196126938 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4602],\n",
      "        [0.7438]], device='mps:0')\n",
      "Iteration 53520 Training loss 0.10599630326032639 Validation loss 0.10064113140106201 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.2788],\n",
      "        [0.4790]], device='mps:0')\n",
      "Iteration 53530 Training loss 0.11232336610555649 Validation loss 0.10061875730752945 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4088],\n",
      "        [0.4037]], device='mps:0')\n",
      "Iteration 53540 Training loss 0.11059892177581787 Validation loss 0.10067567974328995 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.6032],\n",
      "        [0.6390]], device='mps:0')\n",
      "Iteration 53550 Training loss 0.1023147776722908 Validation loss 0.10068673640489578 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.2576],\n",
      "        [0.5958]], device='mps:0')\n",
      "Iteration 53560 Training loss 0.09777698665857315 Validation loss 0.10061684250831604 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7195],\n",
      "        [0.2238]], device='mps:0')\n",
      "Iteration 53570 Training loss 0.10103080421686172 Validation loss 0.10063514113426208 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5446],\n",
      "        [0.2259]], device='mps:0')\n",
      "Iteration 53580 Training loss 0.09280168265104294 Validation loss 0.1006198599934578 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4515],\n",
      "        [0.4806]], device='mps:0')\n",
      "Iteration 53590 Training loss 0.08967462182044983 Validation loss 0.10060960799455643 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4850],\n",
      "        [0.4699]], device='mps:0')\n",
      "Iteration 53600 Training loss 0.08897706866264343 Validation loss 0.10058958828449249 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5280],\n",
      "        [0.4420]], device='mps:0')\n",
      "Iteration 53610 Training loss 0.10792839527130127 Validation loss 0.10058753937482834 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6575],\n",
      "        [0.6787]], device='mps:0')\n",
      "Iteration 53620 Training loss 0.11616334319114685 Validation loss 0.10058200359344482 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.1737],\n",
      "        [0.7318]], device='mps:0')\n",
      "Iteration 53630 Training loss 0.09692853689193726 Validation loss 0.1005828008055687 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4667],\n",
      "        [0.5567]], device='mps:0')\n",
      "Iteration 53640 Training loss 0.09964275360107422 Validation loss 0.10058584809303284 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2108],\n",
      "        [0.3598]], device='mps:0')\n",
      "Iteration 53650 Training loss 0.10535643249750137 Validation loss 0.10058669745922089 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6540],\n",
      "        [0.4988]], device='mps:0')\n",
      "Iteration 53660 Training loss 0.10329348593950272 Validation loss 0.10064377635717392 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3897],\n",
      "        [0.4724]], device='mps:0')\n",
      "Iteration 53670 Training loss 0.11491748690605164 Validation loss 0.10061999410390854 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2354],\n",
      "        [0.2249]], device='mps:0')\n",
      "Iteration 53680 Training loss 0.10035007447004318 Validation loss 0.10060902684926987 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5565],\n",
      "        [0.4570]], device='mps:0')\n",
      "Iteration 53690 Training loss 0.11586956679821014 Validation loss 0.10059427469968796 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6933],\n",
      "        [0.3091]], device='mps:0')\n",
      "Iteration 53700 Training loss 0.10079319030046463 Validation loss 0.10061107575893402 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5987],\n",
      "        [0.7089]], device='mps:0')\n",
      "Iteration 53710 Training loss 0.11228606104850769 Validation loss 0.10062623769044876 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4603],\n",
      "        [0.3848]], device='mps:0')\n",
      "Iteration 53720 Training loss 0.10804668813943863 Validation loss 0.1005975753068924 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5311],\n",
      "        [0.4836]], device='mps:0')\n",
      "Iteration 53730 Training loss 0.10000457614660263 Validation loss 0.10060194134712219 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5666],\n",
      "        [0.4462]], device='mps:0')\n",
      "Iteration 53740 Training loss 0.10351771116256714 Validation loss 0.10064404457807541 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6018],\n",
      "        [0.5484]], device='mps:0')\n",
      "Iteration 53750 Training loss 0.11272934824228287 Validation loss 0.1007051169872284 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.0887],\n",
      "        [0.5188]], device='mps:0')\n",
      "Iteration 53760 Training loss 0.10420333594083786 Validation loss 0.100713349878788 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5471],\n",
      "        [0.5663]], device='mps:0')\n",
      "Iteration 53770 Training loss 0.09386830031871796 Validation loss 0.10064614564180374 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3622],\n",
      "        [0.1327]], device='mps:0')\n",
      "Iteration 53780 Training loss 0.10394512861967087 Validation loss 0.10059891641139984 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2336],\n",
      "        [0.6187]], device='mps:0')\n",
      "Iteration 53790 Training loss 0.10663110762834549 Validation loss 0.10059863328933716 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.1749],\n",
      "        [0.3826]], device='mps:0')\n",
      "Iteration 53800 Training loss 0.09984038025140762 Validation loss 0.10060708969831467 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6742],\n",
      "        [0.3529]], device='mps:0')\n",
      "Iteration 53810 Training loss 0.10596279054880142 Validation loss 0.10060734301805496 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6383],\n",
      "        [0.4752]], device='mps:0')\n",
      "Iteration 53820 Training loss 0.10780869424343109 Validation loss 0.10058274120092392 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5174],\n",
      "        [0.5387]], device='mps:0')\n",
      "Iteration 53830 Training loss 0.10411425679922104 Validation loss 0.1005963459610939 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4257],\n",
      "        [0.6972]], device='mps:0')\n",
      "Iteration 53840 Training loss 0.11108992993831635 Validation loss 0.100630983710289 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4979],\n",
      "        [0.6983]], device='mps:0')\n",
      "Iteration 53850 Training loss 0.0889735221862793 Validation loss 0.10057362169027328 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3826],\n",
      "        [0.3827]], device='mps:0')\n",
      "Iteration 53860 Training loss 0.09368406236171722 Validation loss 0.10058164596557617 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6356],\n",
      "        [0.3124]], device='mps:0')\n",
      "Iteration 53870 Training loss 0.09239888191223145 Validation loss 0.1005813479423523 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3500],\n",
      "        [0.4790]], device='mps:0')\n",
      "Iteration 53880 Training loss 0.10923631489276886 Validation loss 0.10058043152093887 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4662],\n",
      "        [0.1066]], device='mps:0')\n",
      "Iteration 53890 Training loss 0.09521616995334625 Validation loss 0.10057927668094635 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5564],\n",
      "        [0.6497]], device='mps:0')\n",
      "Iteration 53900 Training loss 0.11165933310985565 Validation loss 0.10058934986591339 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5277],\n",
      "        [0.6230]], device='mps:0')\n",
      "Iteration 53910 Training loss 0.10106749087572098 Validation loss 0.10056783258914948 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6107],\n",
      "        [0.5312]], device='mps:0')\n",
      "Iteration 53920 Training loss 0.09073877334594727 Validation loss 0.10057447850704193 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5237],\n",
      "        [0.5169]], device='mps:0')\n",
      "Iteration 53930 Training loss 0.09551423043012619 Validation loss 0.10059919208288193 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3708],\n",
      "        [0.4981]], device='mps:0')\n",
      "Iteration 53940 Training loss 0.10345323383808136 Validation loss 0.10058369487524033 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4695],\n",
      "        [0.5892]], device='mps:0')\n",
      "Iteration 53950 Training loss 0.10492438822984695 Validation loss 0.10057122260332108 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2947],\n",
      "        [0.4601]], device='mps:0')\n",
      "Iteration 53960 Training loss 0.10545694082975388 Validation loss 0.1005597934126854 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5255],\n",
      "        [0.7000]], device='mps:0')\n",
      "Iteration 53970 Training loss 0.09401533007621765 Validation loss 0.1005750298500061 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.1957],\n",
      "        [0.4250]], device='mps:0')\n",
      "Iteration 53980 Training loss 0.09561708569526672 Validation loss 0.10067510604858398 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4444],\n",
      "        [0.2073]], device='mps:0')\n",
      "Iteration 53990 Training loss 0.10817939788103104 Validation loss 0.10061436891555786 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4379],\n",
      "        [0.6370]], device='mps:0')\n",
      "Iteration 54000 Training loss 0.12453576922416687 Validation loss 0.10067258775234222 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5610],\n",
      "        [0.0789]], device='mps:0')\n",
      "Iteration 54010 Training loss 0.10846877843141556 Validation loss 0.10063553601503372 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6719],\n",
      "        [0.2300]], device='mps:0')\n",
      "Iteration 54020 Training loss 0.09286577254533768 Validation loss 0.10061079263687134 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5869],\n",
      "        [0.6646]], device='mps:0')\n",
      "Iteration 54030 Training loss 0.08853418380022049 Validation loss 0.10062303394079208 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2402],\n",
      "        [0.4462]], device='mps:0')\n",
      "Iteration 54040 Training loss 0.11335138231515884 Validation loss 0.10059278458356857 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5369],\n",
      "        [0.2518]], device='mps:0')\n",
      "Iteration 54050 Training loss 0.09538698941469193 Validation loss 0.10060778260231018 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3017],\n",
      "        [0.4728]], device='mps:0')\n",
      "Iteration 54060 Training loss 0.09944265335798264 Validation loss 0.10060515999794006 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6847],\n",
      "        [0.7116]], device='mps:0')\n",
      "Iteration 54070 Training loss 0.10328774899244308 Validation loss 0.10058306902647018 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7259],\n",
      "        [0.5892]], device='mps:0')\n",
      "Iteration 54080 Training loss 0.1054435521364212 Validation loss 0.10058100521564484 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4445],\n",
      "        [0.5508]], device='mps:0')\n",
      "Iteration 54090 Training loss 0.11535552889108658 Validation loss 0.10058051347732544 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7236],\n",
      "        [0.3540]], device='mps:0')\n",
      "Iteration 54100 Training loss 0.1172843873500824 Validation loss 0.10054785758256912 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5128],\n",
      "        [0.2986]], device='mps:0')\n",
      "Iteration 54110 Training loss 0.10211823135614395 Validation loss 0.10054457932710648 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.7305],\n",
      "        [0.6553]], device='mps:0')\n",
      "Iteration 54120 Training loss 0.10626152157783508 Validation loss 0.10054562240839005 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5582],\n",
      "        [0.3072]], device='mps:0')\n",
      "Iteration 54130 Training loss 0.11334897577762604 Validation loss 0.10054676979780197 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7573],\n",
      "        [0.6746]], device='mps:0')\n",
      "Iteration 54140 Training loss 0.09425651282072067 Validation loss 0.1005544662475586 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5362],\n",
      "        [0.3571]], device='mps:0')\n",
      "Iteration 54150 Training loss 0.10715614259243011 Validation loss 0.1005585715174675 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4824],\n",
      "        [0.3498]], device='mps:0')\n",
      "Iteration 54160 Training loss 0.08969667553901672 Validation loss 0.10058695822954178 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6630],\n",
      "        [0.6030]], device='mps:0')\n",
      "Iteration 54170 Training loss 0.10554768145084381 Validation loss 0.1005730852484703 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5054],\n",
      "        [0.6613]], device='mps:0')\n",
      "Iteration 54180 Training loss 0.09682299196720123 Validation loss 0.10058822482824326 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4640],\n",
      "        [0.5335]], device='mps:0')\n",
      "Iteration 54190 Training loss 0.10083459317684174 Validation loss 0.10059427469968796 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4742],\n",
      "        [0.6666]], device='mps:0')\n",
      "Iteration 54200 Training loss 0.1114853098988533 Validation loss 0.10058628767728806 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3838],\n",
      "        [0.5881]], device='mps:0')\n",
      "Iteration 54210 Training loss 0.09977393597364426 Validation loss 0.10056672245264053 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6218],\n",
      "        [0.1696]], device='mps:0')\n",
      "Iteration 54220 Training loss 0.10717371851205826 Validation loss 0.10057411342859268 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4296],\n",
      "        [0.5904]], device='mps:0')\n",
      "Iteration 54230 Training loss 0.10047125816345215 Validation loss 0.10054296255111694 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5542],\n",
      "        [0.3538]], device='mps:0')\n",
      "Iteration 54240 Training loss 0.10404811799526215 Validation loss 0.100547656416893 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4632],\n",
      "        [0.2681]], device='mps:0')\n",
      "Iteration 54250 Training loss 0.09835220128297806 Validation loss 0.1005435436964035 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3902],\n",
      "        [0.2329]], device='mps:0')\n",
      "Iteration 54260 Training loss 0.09791205823421478 Validation loss 0.10053608566522598 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4997],\n",
      "        [0.6050]], device='mps:0')\n",
      "Iteration 54270 Training loss 0.09307645261287689 Validation loss 0.10054055601358414 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2509],\n",
      "        [0.3042]], device='mps:0')\n",
      "Iteration 54280 Training loss 0.10060229897499084 Validation loss 0.10054097324609756 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4616],\n",
      "        [0.4296]], device='mps:0')\n",
      "Iteration 54290 Training loss 0.10765830427408218 Validation loss 0.1005467101931572 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3360],\n",
      "        [0.4953]], device='mps:0')\n",
      "Iteration 54300 Training loss 0.10727855563163757 Validation loss 0.10054031759500504 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3251],\n",
      "        [0.2916]], device='mps:0')\n",
      "Iteration 54310 Training loss 0.10822802037000656 Validation loss 0.10056706517934799 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6184],\n",
      "        [0.6569]], device='mps:0')\n",
      "Iteration 54320 Training loss 0.10295981913805008 Validation loss 0.10054590553045273 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5778],\n",
      "        [0.5775]], device='mps:0')\n",
      "Iteration 54330 Training loss 0.10578262060880661 Validation loss 0.10053140670061111 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3667],\n",
      "        [0.4449]], device='mps:0')\n",
      "Iteration 54340 Training loss 0.09511202573776245 Validation loss 0.10053389519453049 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3869],\n",
      "        [0.4732]], device='mps:0')\n",
      "Iteration 54350 Training loss 0.11202505230903625 Validation loss 0.10053952783346176 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3371],\n",
      "        [0.5205]], device='mps:0')\n",
      "Iteration 54360 Training loss 0.10081712156534195 Validation loss 0.1005374938249588 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5364],\n",
      "        [0.5999]], device='mps:0')\n",
      "Iteration 54370 Training loss 0.10560775548219681 Validation loss 0.10054712742567062 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6037],\n",
      "        [0.6742]], device='mps:0')\n",
      "Iteration 54380 Training loss 0.10225440561771393 Validation loss 0.1005856916308403 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2115],\n",
      "        [0.2559]], device='mps:0')\n",
      "Iteration 54390 Training loss 0.10102575272321701 Validation loss 0.10053201764822006 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5854],\n",
      "        [0.6723]], device='mps:0')\n",
      "Iteration 54400 Training loss 0.09800422191619873 Validation loss 0.10051825642585754 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4857],\n",
      "        [0.6604]], device='mps:0')\n",
      "Iteration 54410 Training loss 0.11130278557538986 Validation loss 0.10052080452442169 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6621],\n",
      "        [0.5001]], device='mps:0')\n",
      "Iteration 54420 Training loss 0.09088059514760971 Validation loss 0.10051601380109787 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3246],\n",
      "        [0.3330]], device='mps:0')\n",
      "Iteration 54430 Training loss 0.10628217458724976 Validation loss 0.10052286833524704 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5945],\n",
      "        [0.6683]], device='mps:0')\n",
      "Iteration 54440 Training loss 0.11187294125556946 Validation loss 0.10056077688932419 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7059],\n",
      "        [0.5040]], device='mps:0')\n",
      "Iteration 54450 Training loss 0.09745347499847412 Validation loss 0.10058671981096268 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4401],\n",
      "        [0.3041]], device='mps:0')\n",
      "Iteration 54460 Training loss 0.11059033870697021 Validation loss 0.10056479275226593 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5809],\n",
      "        [0.2647]], device='mps:0')\n",
      "Iteration 54470 Training loss 0.11351741850376129 Validation loss 0.10055844485759735 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4619],\n",
      "        [0.0346]], device='mps:0')\n",
      "Iteration 54480 Training loss 0.10745225846767426 Validation loss 0.10056544840335846 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3272],\n",
      "        [0.5263]], device='mps:0')\n",
      "Iteration 54490 Training loss 0.10384951531887054 Validation loss 0.1005391925573349 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2903],\n",
      "        [0.3517]], device='mps:0')\n",
      "Iteration 54500 Training loss 0.09415258467197418 Validation loss 0.10051793605089188 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4841],\n",
      "        [0.5282]], device='mps:0')\n",
      "Iteration 54510 Training loss 0.09769996255636215 Validation loss 0.10050754249095917 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3637],\n",
      "        [0.4560]], device='mps:0')\n",
      "Iteration 54520 Training loss 0.10305827856063843 Validation loss 0.10050774365663528 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.1622],\n",
      "        [0.4839]], device='mps:0')\n",
      "Iteration 54530 Training loss 0.09555749595165253 Validation loss 0.10051402449607849 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3601],\n",
      "        [0.6685]], device='mps:0')\n",
      "Iteration 54540 Training loss 0.09832829982042313 Validation loss 0.10050667077302933 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4855],\n",
      "        [0.6973]], device='mps:0')\n",
      "Iteration 54550 Training loss 0.10965421795845032 Validation loss 0.10052142292261124 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5397],\n",
      "        [0.5671]], device='mps:0')\n",
      "Iteration 54560 Training loss 0.10462482273578644 Validation loss 0.10050950199365616 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6329],\n",
      "        [0.3374]], device='mps:0')\n",
      "Iteration 54570 Training loss 0.10100004822015762 Validation loss 0.10050852596759796 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2714],\n",
      "        [0.5371]], device='mps:0')\n",
      "Iteration 54580 Training loss 0.10371191799640656 Validation loss 0.10050338506698608 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2321],\n",
      "        [0.4644]], device='mps:0')\n",
      "Iteration 54590 Training loss 0.10243847966194153 Validation loss 0.10050457715988159 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4304],\n",
      "        [0.2055]], device='mps:0')\n",
      "Iteration 54600 Training loss 0.09174607694149017 Validation loss 0.10050758719444275 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3516],\n",
      "        [0.6959]], device='mps:0')\n",
      "Iteration 54610 Training loss 0.10220582038164139 Validation loss 0.10052654147148132 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6644],\n",
      "        [0.3786]], device='mps:0')\n",
      "Iteration 54620 Training loss 0.0985364317893982 Validation loss 0.10052283108234406 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5520],\n",
      "        [0.5783]], device='mps:0')\n",
      "Iteration 54630 Training loss 0.0915890485048294 Validation loss 0.10052984207868576 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5005],\n",
      "        [0.6592]], device='mps:0')\n",
      "Iteration 54640 Training loss 0.10489553958177567 Validation loss 0.10053834319114685 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2851],\n",
      "        [0.3002]], device='mps:0')\n",
      "Iteration 54650 Training loss 0.10120923817157745 Validation loss 0.1005125418305397 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5733],\n",
      "        [0.4021]], device='mps:0')\n",
      "Iteration 54660 Training loss 0.10436337441205978 Validation loss 0.10050691664218903 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6411],\n",
      "        [0.1966]], device='mps:0')\n",
      "Iteration 54670 Training loss 0.10401399433612823 Validation loss 0.10049634426832199 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5438],\n",
      "        [0.5546]], device='mps:0')\n",
      "Iteration 54680 Training loss 0.10732100158929825 Validation loss 0.10049422085285187 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3216],\n",
      "        [0.2842]], device='mps:0')\n",
      "Iteration 54690 Training loss 0.09517572075128555 Validation loss 0.10049518197774887 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5527],\n",
      "        [0.4497]], device='mps:0')\n",
      "Iteration 54700 Training loss 0.09956243634223938 Validation loss 0.10048864781856537 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2472],\n",
      "        [0.5801]], device='mps:0')\n",
      "Iteration 54710 Training loss 0.11077777296304703 Validation loss 0.1004890501499176 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.7179],\n",
      "        [0.3874]], device='mps:0')\n",
      "Iteration 54720 Training loss 0.09593873471021652 Validation loss 0.10048679262399673 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6872],\n",
      "        [0.7560]], device='mps:0')\n",
      "Iteration 54730 Training loss 0.09242855757474899 Validation loss 0.10049153119325638 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.1983],\n",
      "        [0.4973]], device='mps:0')\n",
      "Iteration 54740 Training loss 0.10399764776229858 Validation loss 0.10048972070217133 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2931],\n",
      "        [0.3254]], device='mps:0')\n",
      "Iteration 54750 Training loss 0.10442980378866196 Validation loss 0.10048510879278183 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6057],\n",
      "        [0.3835]], device='mps:0')\n",
      "Iteration 54760 Training loss 0.10463092476129532 Validation loss 0.10048221051692963 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6702],\n",
      "        [0.6216]], device='mps:0')\n",
      "Iteration 54770 Training loss 0.11433887481689453 Validation loss 0.10049813985824585 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.7146],\n",
      "        [0.4501]], device='mps:0')\n",
      "Iteration 54780 Training loss 0.09563970565795898 Validation loss 0.10048803687095642 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4516],\n",
      "        [0.6805]], device='mps:0')\n",
      "Iteration 54790 Training loss 0.12027720361948013 Validation loss 0.10048015415668488 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4917],\n",
      "        [0.5615]], device='mps:0')\n",
      "Iteration 54800 Training loss 0.09912341833114624 Validation loss 0.10049013048410416 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3893],\n",
      "        [0.4542]], device='mps:0')\n",
      "Iteration 54810 Training loss 0.09736581891775131 Validation loss 0.10047856718301773 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5863],\n",
      "        [0.6603]], device='mps:0')\n",
      "Iteration 54820 Training loss 0.09512915462255478 Validation loss 0.10048992186784744 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4129],\n",
      "        [0.3576]], device='mps:0')\n",
      "Iteration 54830 Training loss 0.09992578625679016 Validation loss 0.10047692060470581 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7623],\n",
      "        [0.5885]], device='mps:0')\n",
      "Iteration 54840 Training loss 0.09404262155294418 Validation loss 0.10047586262226105 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6992],\n",
      "        [0.6601]], device='mps:0')\n",
      "Iteration 54850 Training loss 0.10691163688898087 Validation loss 0.1004738062620163 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6510],\n",
      "        [0.1327]], device='mps:0')\n",
      "Iteration 54860 Training loss 0.09498503804206848 Validation loss 0.1004752442240715 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6215],\n",
      "        [0.2816]], device='mps:0')\n",
      "Iteration 54870 Training loss 0.10201358795166016 Validation loss 0.10047166049480438 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5297],\n",
      "        [0.4484]], device='mps:0')\n",
      "Iteration 54880 Training loss 0.10868627578020096 Validation loss 0.10047633945941925 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5807],\n",
      "        [0.3556]], device='mps:0')\n",
      "Iteration 54890 Training loss 0.10769467800855637 Validation loss 0.10047829896211624 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6939],\n",
      "        [0.4797]], device='mps:0')\n",
      "Iteration 54900 Training loss 0.10113942623138428 Validation loss 0.10047141462564468 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6700],\n",
      "        [0.3327]], device='mps:0')\n",
      "Iteration 54910 Training loss 0.1066676527261734 Validation loss 0.10049565881490707 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2174],\n",
      "        [0.4855]], device='mps:0')\n",
      "Iteration 54920 Training loss 0.10734311491250992 Validation loss 0.10047632455825806 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5118],\n",
      "        [0.3591]], device='mps:0')\n",
      "Iteration 54930 Training loss 0.10077396035194397 Validation loss 0.10052953660488129 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2621],\n",
      "        [0.6924]], device='mps:0')\n",
      "Iteration 54940 Training loss 0.10956942290067673 Validation loss 0.10060959309339523 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5653],\n",
      "        [0.6668]], device='mps:0')\n",
      "Iteration 54950 Training loss 0.09775551408529282 Validation loss 0.10052372515201569 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5900],\n",
      "        [0.3343]], device='mps:0')\n",
      "Iteration 54960 Training loss 0.10586635023355484 Validation loss 0.10051990300416946 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6943],\n",
      "        [0.3937]], device='mps:0')\n",
      "Iteration 54970 Training loss 0.10435016453266144 Validation loss 0.10050250589847565 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6824],\n",
      "        [0.2800]], device='mps:0')\n",
      "Iteration 54980 Training loss 0.09900246560573578 Validation loss 0.1004675030708313 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6262],\n",
      "        [0.5393]], device='mps:0')\n",
      "Iteration 54990 Training loss 0.09982868283987045 Validation loss 0.10046803951263428 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2898],\n",
      "        [0.5600]], device='mps:0')\n",
      "Iteration 55000 Training loss 0.11235513538122177 Validation loss 0.1004594936966896 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5619],\n",
      "        [0.3822]], device='mps:0')\n",
      "Iteration 55010 Training loss 0.1145443394780159 Validation loss 0.10045965015888214 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6527],\n",
      "        [0.5750]], device='mps:0')\n",
      "Iteration 55020 Training loss 0.10632966458797455 Validation loss 0.10045650601387024 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5660],\n",
      "        [0.7363]], device='mps:0')\n",
      "Iteration 55030 Training loss 0.091917484998703 Validation loss 0.10045519471168518 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2015],\n",
      "        [0.6993]], device='mps:0')\n",
      "Iteration 55040 Training loss 0.10702241957187653 Validation loss 0.10046881437301636 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3684],\n",
      "        [0.5448]], device='mps:0')\n",
      "Iteration 55050 Training loss 0.10391140729188919 Validation loss 0.10045456886291504 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5720],\n",
      "        [0.3053]], device='mps:0')\n",
      "Iteration 55060 Training loss 0.11463762819766998 Validation loss 0.1004541739821434 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6468],\n",
      "        [0.1860]], device='mps:0')\n",
      "Iteration 55070 Training loss 0.09811017662286758 Validation loss 0.10045967251062393 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4203],\n",
      "        [0.3827]], device='mps:0')\n",
      "Iteration 55080 Training loss 0.11321929842233658 Validation loss 0.10047733783721924 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3140],\n",
      "        [0.1699]], device='mps:0')\n",
      "Iteration 55090 Training loss 0.10383585095405579 Validation loss 0.10046495497226715 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7033],\n",
      "        [0.4862]], device='mps:0')\n",
      "Iteration 55100 Training loss 0.10387101769447327 Validation loss 0.10048092901706696 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5577],\n",
      "        [0.2749]], device='mps:0')\n",
      "Iteration 55110 Training loss 0.09827455133199692 Validation loss 0.10047656297683716 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5792],\n",
      "        [0.6622]], device='mps:0')\n",
      "Iteration 55120 Training loss 0.0928841382265091 Validation loss 0.1004723310470581 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6365],\n",
      "        [0.2030]], device='mps:0')\n",
      "Iteration 55130 Training loss 0.09611303359270096 Validation loss 0.1005348414182663 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5646],\n",
      "        [0.6499]], device='mps:0')\n",
      "Iteration 55140 Training loss 0.10836537182331085 Validation loss 0.10052008181810379 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2840],\n",
      "        [0.5287]], device='mps:0')\n",
      "Iteration 55150 Training loss 0.10750476270914078 Validation loss 0.10052856057882309 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5870],\n",
      "        [0.3358]], device='mps:0')\n",
      "Iteration 55160 Training loss 0.11448230594396591 Validation loss 0.10051904618740082 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6242],\n",
      "        [0.2078]], device='mps:0')\n",
      "Iteration 55170 Training loss 0.09660884737968445 Validation loss 0.10051905363798141 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2937],\n",
      "        [0.3630]], device='mps:0')\n",
      "Iteration 55180 Training loss 0.09433986246585846 Validation loss 0.10049855709075928 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5728],\n",
      "        [0.5821]], device='mps:0')\n",
      "Iteration 55190 Training loss 0.09728800505399704 Validation loss 0.10047658532857895 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5717],\n",
      "        [0.2499]], device='mps:0')\n",
      "Iteration 55200 Training loss 0.1104598417878151 Validation loss 0.10053680837154388 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3716],\n",
      "        [0.7416]], device='mps:0')\n",
      "Iteration 55210 Training loss 0.11271774023771286 Validation loss 0.10053706169128418 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6569],\n",
      "        [0.4594]], device='mps:0')\n",
      "Iteration 55220 Training loss 0.09930244088172913 Validation loss 0.10049117356538773 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4790],\n",
      "        [0.6804]], device='mps:0')\n",
      "Iteration 55230 Training loss 0.102308489382267 Validation loss 0.10050314664840698 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3414],\n",
      "        [0.4865]], device='mps:0')\n",
      "Iteration 55240 Training loss 0.10834159702062607 Validation loss 0.10046780854463577 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.1864],\n",
      "        [0.4229]], device='mps:0')\n",
      "Iteration 55250 Training loss 0.09177541732788086 Validation loss 0.10047189891338348 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6172],\n",
      "        [0.5570]], device='mps:0')\n",
      "Iteration 55260 Training loss 0.11397267132997513 Validation loss 0.10049661993980408 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2453],\n",
      "        [0.3209]], device='mps:0')\n",
      "Iteration 55270 Training loss 0.08916719257831573 Validation loss 0.100483238697052 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4003],\n",
      "        [0.4617]], device='mps:0')\n",
      "Iteration 55280 Training loss 0.11297713965177536 Validation loss 0.10050620138645172 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6770],\n",
      "        [0.7493]], device='mps:0')\n",
      "Iteration 55290 Training loss 0.10286052525043488 Validation loss 0.10050786286592484 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7049],\n",
      "        [0.4796]], device='mps:0')\n",
      "Iteration 55300 Training loss 0.09621553122997284 Validation loss 0.1005561575293541 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2872],\n",
      "        [0.5274]], device='mps:0')\n",
      "Iteration 55310 Training loss 0.10445558279752731 Validation loss 0.10054216533899307 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5881],\n",
      "        [0.1647]], device='mps:0')\n",
      "Iteration 55320 Training loss 0.09435182064771652 Validation loss 0.10067854821681976 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3458],\n",
      "        [0.4552]], device='mps:0')\n",
      "Iteration 55330 Training loss 0.10697093605995178 Validation loss 0.10061388462781906 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5398],\n",
      "        [0.6614]], device='mps:0')\n",
      "Iteration 55340 Training loss 0.10933379828929901 Validation loss 0.10058053582906723 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7073],\n",
      "        [0.6097]], device='mps:0')\n",
      "Iteration 55350 Training loss 0.09611523151397705 Validation loss 0.10055519640445709 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5094],\n",
      "        [0.5125]], device='mps:0')\n",
      "Iteration 55360 Training loss 0.09676671773195267 Validation loss 0.10058542340993881 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3261],\n",
      "        [0.1518]], device='mps:0')\n",
      "Iteration 55370 Training loss 0.10614580661058426 Validation loss 0.10058018565177917 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6298],\n",
      "        [0.7288]], device='mps:0')\n",
      "Iteration 55380 Training loss 0.0986504852771759 Validation loss 0.10057204216718674 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3765],\n",
      "        [0.5533]], device='mps:0')\n",
      "Iteration 55390 Training loss 0.10792355984449387 Validation loss 0.10057301074266434 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2272],\n",
      "        [0.7237]], device='mps:0')\n",
      "Iteration 55400 Training loss 0.09869946539402008 Validation loss 0.10049518942832947 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5372],\n",
      "        [0.3005]], device='mps:0')\n",
      "Iteration 55410 Training loss 0.09897012263536453 Validation loss 0.10043919831514359 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7063],\n",
      "        [0.3680]], device='mps:0')\n",
      "Iteration 55420 Training loss 0.09577488899230957 Validation loss 0.10045281797647476 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4163],\n",
      "        [0.4410]], device='mps:0')\n",
      "Iteration 55430 Training loss 0.10847415775060654 Validation loss 0.1004205048084259 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7127],\n",
      "        [0.5280]], device='mps:0')\n",
      "Iteration 55440 Training loss 0.10892829298973083 Validation loss 0.10041100531816483 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3534],\n",
      "        [0.5893]], device='mps:0')\n",
      "Iteration 55450 Training loss 0.10707896947860718 Validation loss 0.10041803121566772 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5704],\n",
      "        [0.7343]], device='mps:0')\n",
      "Iteration 55460 Training loss 0.09565030783414841 Validation loss 0.10043705999851227 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4700],\n",
      "        [0.6778]], device='mps:0')\n",
      "Iteration 55470 Training loss 0.10734044760465622 Validation loss 0.10042192786931992 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3224],\n",
      "        [0.4546]], device='mps:0')\n",
      "Iteration 55480 Training loss 0.10788664221763611 Validation loss 0.1004241406917572 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5700],\n",
      "        [0.5708]], device='mps:0')\n",
      "Iteration 55490 Training loss 0.09999755769968033 Validation loss 0.10042909532785416 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5869],\n",
      "        [0.3537]], device='mps:0')\n",
      "Iteration 55500 Training loss 0.09810619056224823 Validation loss 0.10043561458587646 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6094],\n",
      "        [0.5186]], device='mps:0')\n",
      "Iteration 55510 Training loss 0.10549327731132507 Validation loss 0.10043951869010925 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2186],\n",
      "        [0.5939]], device='mps:0')\n",
      "Iteration 55520 Training loss 0.0947335734963417 Validation loss 0.10040196776390076 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4696],\n",
      "        [0.4590]], device='mps:0')\n",
      "Iteration 55530 Training loss 0.1000078022480011 Validation loss 0.10040198266506195 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5068],\n",
      "        [0.7085]], device='mps:0')\n",
      "Iteration 55540 Training loss 0.101965993642807 Validation loss 0.10042697936296463 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5913],\n",
      "        [0.3632]], device='mps:0')\n",
      "Iteration 55550 Training loss 0.10779733210802078 Validation loss 0.10047048330307007 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5912],\n",
      "        [0.6366]], device='mps:0')\n",
      "Iteration 55560 Training loss 0.10285592824220657 Validation loss 0.1004568412899971 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5579],\n",
      "        [0.6636]], device='mps:0')\n",
      "Iteration 55570 Training loss 0.11310625076293945 Validation loss 0.100445955991745 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6685],\n",
      "        [0.5804]], device='mps:0')\n",
      "Iteration 55580 Training loss 0.09280790388584137 Validation loss 0.10046929121017456 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2525],\n",
      "        [0.6714]], device='mps:0')\n",
      "Iteration 55590 Training loss 0.10529858618974686 Validation loss 0.10050192475318909 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4566],\n",
      "        [0.5130]], device='mps:0')\n",
      "Iteration 55600 Training loss 0.11447032541036606 Validation loss 0.10048489272594452 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.7382],\n",
      "        [0.6646]], device='mps:0')\n",
      "Iteration 55610 Training loss 0.09867580235004425 Validation loss 0.1004192903637886 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4090],\n",
      "        [0.5825]], device='mps:0')\n",
      "Iteration 55620 Training loss 0.1000765711069107 Validation loss 0.10041609406471252 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4190],\n",
      "        [0.5637]], device='mps:0')\n",
      "Iteration 55630 Training loss 0.10609637945890427 Validation loss 0.10039828717708588 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4188],\n",
      "        [0.5363]], device='mps:0')\n",
      "Iteration 55640 Training loss 0.10363484919071198 Validation loss 0.10040045529603958 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5891],\n",
      "        [0.5404]], device='mps:0')\n",
      "Iteration 55650 Training loss 0.10413733124732971 Validation loss 0.10040022432804108 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.1277],\n",
      "        [0.3831]], device='mps:0')\n",
      "Iteration 55660 Training loss 0.10707088559865952 Validation loss 0.10040272027254105 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6481],\n",
      "        [0.6220]], device='mps:0')\n",
      "Iteration 55670 Training loss 0.09926147013902664 Validation loss 0.10042348504066467 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1378],\n",
      "        [0.4388]], device='mps:0')\n",
      "Iteration 55680 Training loss 0.09965404123067856 Validation loss 0.10039328038692474 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5265],\n",
      "        [0.4820]], device='mps:0')\n",
      "Iteration 55690 Training loss 0.10773665457963943 Validation loss 0.10038936138153076 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1868],\n",
      "        [0.6832]], device='mps:0')\n",
      "Iteration 55700 Training loss 0.10190366208553314 Validation loss 0.10039122402667999 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6430],\n",
      "        [0.5751]], device='mps:0')\n",
      "Iteration 55710 Training loss 0.10348533093929291 Validation loss 0.10043993592262268 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6034],\n",
      "        [0.4033]], device='mps:0')\n",
      "Iteration 55720 Training loss 0.10985592007637024 Validation loss 0.10047589987516403 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5532],\n",
      "        [0.5102]], device='mps:0')\n",
      "Iteration 55730 Training loss 0.1126498207449913 Validation loss 0.10043281316757202 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.7056],\n",
      "        [0.4394]], device='mps:0')\n",
      "Iteration 55740 Training loss 0.11170899868011475 Validation loss 0.10040227323770523 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6716],\n",
      "        [0.2334]], device='mps:0')\n",
      "Iteration 55750 Training loss 0.09932757169008255 Validation loss 0.10040386021137238 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6229],\n",
      "        [0.5592]], device='mps:0')\n",
      "Iteration 55760 Training loss 0.11682447046041489 Validation loss 0.10039564967155457 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4140],\n",
      "        [0.6318]], device='mps:0')\n",
      "Iteration 55770 Training loss 0.10980011522769928 Validation loss 0.10037998110055923 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.1923],\n",
      "        [0.1949]], device='mps:0')\n",
      "Iteration 55780 Training loss 0.09759219735860825 Validation loss 0.10040606558322906 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6548],\n",
      "        [0.3844]], device='mps:0')\n",
      "Iteration 55790 Training loss 0.11006267368793488 Validation loss 0.10039931535720825 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.0892],\n",
      "        [0.5150]], device='mps:0')\n",
      "Iteration 55800 Training loss 0.10002223402261734 Validation loss 0.1003836840391159 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4072],\n",
      "        [0.1579]], device='mps:0')\n",
      "Iteration 55810 Training loss 0.10647252947092056 Validation loss 0.10037785768508911 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5730],\n",
      "        [0.7596]], device='mps:0')\n",
      "Iteration 55820 Training loss 0.1032504141330719 Validation loss 0.1003945842385292 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.0263],\n",
      "        [0.4940]], device='mps:0')\n",
      "Iteration 55830 Training loss 0.10337086766958237 Validation loss 0.10038013011217117 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6488],\n",
      "        [0.6605]], device='mps:0')\n",
      "Iteration 55840 Training loss 0.10669802129268646 Validation loss 0.10037597268819809 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4270],\n",
      "        [0.6545]], device='mps:0')\n",
      "Iteration 55850 Training loss 0.10180148482322693 Validation loss 0.1003895178437233 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6811],\n",
      "        [0.2253]], device='mps:0')\n",
      "Iteration 55860 Training loss 0.10781066864728928 Validation loss 0.10040660202503204 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6906],\n",
      "        [0.5362]], device='mps:0')\n",
      "Iteration 55870 Training loss 0.09407638013362885 Validation loss 0.1003909483551979 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5445],\n",
      "        [0.7843]], device='mps:0')\n",
      "Iteration 55880 Training loss 0.10483104735612869 Validation loss 0.10040570050477982 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6380],\n",
      "        [0.1706]], device='mps:0')\n",
      "Iteration 55890 Training loss 0.09722047299146652 Validation loss 0.10041510313749313 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7090],\n",
      "        [0.4674]], device='mps:0')\n",
      "Iteration 55900 Training loss 0.10911259055137634 Validation loss 0.10040535032749176 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.1458],\n",
      "        [0.4207]], device='mps:0')\n",
      "Iteration 55910 Training loss 0.10134753584861755 Validation loss 0.10042450577020645 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2053],\n",
      "        [0.1974]], device='mps:0')\n",
      "Iteration 55920 Training loss 0.09127942472696304 Validation loss 0.10039868950843811 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.2767],\n",
      "        [0.4975]], device='mps:0')\n",
      "Iteration 55930 Training loss 0.1030968651175499 Validation loss 0.10039857029914856 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6285],\n",
      "        [0.4255]], device='mps:0')\n",
      "Iteration 55940 Training loss 0.10666967183351517 Validation loss 0.10041067749261856 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5192],\n",
      "        [0.5176]], device='mps:0')\n",
      "Iteration 55950 Training loss 0.1109325960278511 Validation loss 0.10037121921777725 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4900],\n",
      "        [0.2624]], device='mps:0')\n",
      "Iteration 55960 Training loss 0.10021117329597473 Validation loss 0.10037177056074142 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5470],\n",
      "        [0.7237]], device='mps:0')\n",
      "Iteration 55970 Training loss 0.09630713611841202 Validation loss 0.10037730634212494 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6652],\n",
      "        [0.6853]], device='mps:0')\n",
      "Iteration 55980 Training loss 0.10364469140768051 Validation loss 0.10037465393543243 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5434],\n",
      "        [0.1224]], device='mps:0')\n",
      "Iteration 55990 Training loss 0.09320494532585144 Validation loss 0.10040469467639923 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5711],\n",
      "        [0.6226]], device='mps:0')\n",
      "Iteration 56000 Training loss 0.09131918102502823 Validation loss 0.10038075596094131 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3786],\n",
      "        [0.1684]], device='mps:0')\n",
      "Iteration 56010 Training loss 0.10336416214704514 Validation loss 0.10036258399486542 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.0919],\n",
      "        [0.7272]], device='mps:0')\n",
      "Iteration 56020 Training loss 0.10236048698425293 Validation loss 0.10035984218120575 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7358],\n",
      "        [0.4861]], device='mps:0')\n",
      "Iteration 56030 Training loss 0.11848031729459763 Validation loss 0.10036029666662216 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6232],\n",
      "        [0.6954]], device='mps:0')\n",
      "Iteration 56040 Training loss 0.10318223387002945 Validation loss 0.10040038824081421 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7203],\n",
      "        [0.2403]], device='mps:0')\n",
      "Iteration 56050 Training loss 0.11278671026229858 Validation loss 0.10036898404359818 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5003],\n",
      "        [0.6304]], device='mps:0')\n",
      "Iteration 56060 Training loss 0.09559805691242218 Validation loss 0.10040505230426788 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4574],\n",
      "        [0.4889]], device='mps:0')\n",
      "Iteration 56070 Training loss 0.10672296583652496 Validation loss 0.10041399300098419 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5807],\n",
      "        [0.6747]], device='mps:0')\n",
      "Iteration 56080 Training loss 0.0996931716799736 Validation loss 0.10047688335180283 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6927],\n",
      "        [0.5153]], device='mps:0')\n",
      "Iteration 56090 Training loss 0.1042860671877861 Validation loss 0.10046738386154175 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6094],\n",
      "        [0.6730]], device='mps:0')\n",
      "Iteration 56100 Training loss 0.10768727213144302 Validation loss 0.10044272989034653 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5145],\n",
      "        [0.6549]], device='mps:0')\n",
      "Iteration 56110 Training loss 0.12807197868824005 Validation loss 0.10043255984783173 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.0608],\n",
      "        [0.5524]], device='mps:0')\n",
      "Iteration 56120 Training loss 0.1049860343337059 Validation loss 0.10043901205062866 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4064],\n",
      "        [0.4260]], device='mps:0')\n",
      "Iteration 56130 Training loss 0.10895024985074997 Validation loss 0.10040176659822464 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7014],\n",
      "        [0.4255]], device='mps:0')\n",
      "Iteration 56140 Training loss 0.09432550519704819 Validation loss 0.10037060081958771 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7151],\n",
      "        [0.4880]], device='mps:0')\n",
      "Iteration 56150 Training loss 0.10709097981452942 Validation loss 0.10039490461349487 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5057],\n",
      "        [0.5698]], device='mps:0')\n",
      "Iteration 56160 Training loss 0.10033772885799408 Validation loss 0.10040631890296936 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5619],\n",
      "        [0.6376]], device='mps:0')\n",
      "Iteration 56170 Training loss 0.10107507556676865 Validation loss 0.10037030279636383 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6481],\n",
      "        [0.6004]], device='mps:0')\n",
      "Iteration 56180 Training loss 0.10934630781412125 Validation loss 0.10037987679243088 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4095],\n",
      "        [0.6567]], device='mps:0')\n",
      "Iteration 56190 Training loss 0.11005367338657379 Validation loss 0.10039066523313522 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.7209],\n",
      "        [0.4395]], device='mps:0')\n",
      "Iteration 56200 Training loss 0.09702758491039276 Validation loss 0.10041200369596481 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4464],\n",
      "        [0.4478]], device='mps:0')\n",
      "Iteration 56210 Training loss 0.1002703607082367 Validation loss 0.10039426386356354 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5231],\n",
      "        [0.3786]], device='mps:0')\n",
      "Iteration 56220 Training loss 0.10147221386432648 Validation loss 0.10042742639780045 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5597],\n",
      "        [0.6561]], device='mps:0')\n",
      "Iteration 56230 Training loss 0.11058324575424194 Validation loss 0.1003657877445221 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6904],\n",
      "        [0.8020]], device='mps:0')\n",
      "Iteration 56240 Training loss 0.09931857883930206 Validation loss 0.10036614537239075 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3968],\n",
      "        [0.3769]], device='mps:0')\n",
      "Iteration 56250 Training loss 0.09774373471736908 Validation loss 0.10035640746355057 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5034],\n",
      "        [0.6652]], device='mps:0')\n",
      "Iteration 56260 Training loss 0.09575477242469788 Validation loss 0.10037453472614288 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5839],\n",
      "        [0.6339]], device='mps:0')\n",
      "Iteration 56270 Training loss 0.11561407148838043 Validation loss 0.10037273913621902 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5250],\n",
      "        [0.5587]], device='mps:0')\n",
      "Iteration 56280 Training loss 0.0979924276471138 Validation loss 0.10036081820726395 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3443],\n",
      "        [0.2381]], device='mps:0')\n",
      "Iteration 56290 Training loss 0.1055871918797493 Validation loss 0.10038286447525024 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7045],\n",
      "        [0.5948]], device='mps:0')\n",
      "Iteration 56300 Training loss 0.10903220623731613 Validation loss 0.10037469118833542 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6095],\n",
      "        [0.5241]], device='mps:0')\n",
      "Iteration 56310 Training loss 0.10427502542734146 Validation loss 0.1003900095820427 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4090],\n",
      "        [0.6601]], device='mps:0')\n",
      "Iteration 56320 Training loss 0.0938684418797493 Validation loss 0.10037339478731155 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.2746],\n",
      "        [0.4901]], device='mps:0')\n",
      "Iteration 56330 Training loss 0.09870920330286026 Validation loss 0.10039276629686356 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7112],\n",
      "        [0.6027]], device='mps:0')\n",
      "Iteration 56340 Training loss 0.10955783724784851 Validation loss 0.10037099570035934 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.3754],\n",
      "        [0.5175]], device='mps:0')\n",
      "Iteration 56350 Training loss 0.09864981472492218 Validation loss 0.10039617121219635 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7237],\n",
      "        [0.5055]], device='mps:0')\n",
      "Iteration 56360 Training loss 0.10799607634544373 Validation loss 0.1003640666604042 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6686],\n",
      "        [0.6517]], device='mps:0')\n",
      "Iteration 56370 Training loss 0.10099658370018005 Validation loss 0.10041437298059464 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7112],\n",
      "        [0.5563]], device='mps:0')\n",
      "Iteration 56380 Training loss 0.11288690567016602 Validation loss 0.10038825869560242 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5572],\n",
      "        [0.7133]], device='mps:0')\n",
      "Iteration 56390 Training loss 0.10352032631635666 Validation loss 0.1003776267170906 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6208],\n",
      "        [0.5516]], device='mps:0')\n",
      "Iteration 56400 Training loss 0.1086767241358757 Validation loss 0.1003924310207367 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5237],\n",
      "        [0.2378]], device='mps:0')\n",
      "Iteration 56410 Training loss 0.09823262691497803 Validation loss 0.1003904715180397 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3949],\n",
      "        [0.2012]], device='mps:0')\n",
      "Iteration 56420 Training loss 0.10490653663873672 Validation loss 0.10037830471992493 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.3115],\n",
      "        [0.4446]], device='mps:0')\n",
      "Iteration 56430 Training loss 0.09717366099357605 Validation loss 0.10040132701396942 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3672],\n",
      "        [0.4136]], device='mps:0')\n",
      "Iteration 56440 Training loss 0.11021362245082855 Validation loss 0.10040341317653656 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4271],\n",
      "        [0.4039]], device='mps:0')\n",
      "Iteration 56450 Training loss 0.10379360616207123 Validation loss 0.1003933921456337 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5665],\n",
      "        [0.3968]], device='mps:0')\n",
      "Iteration 56460 Training loss 0.09965342283248901 Validation loss 0.1004626676440239 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3501],\n",
      "        [0.4171]], device='mps:0')\n",
      "Iteration 56470 Training loss 0.09956660866737366 Validation loss 0.10041689872741699 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4461],\n",
      "        [0.3630]], device='mps:0')\n",
      "Iteration 56480 Training loss 0.1010793074965477 Validation loss 0.10040199756622314 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2843],\n",
      "        [0.4007]], device='mps:0')\n",
      "Iteration 56490 Training loss 0.10207558423280716 Validation loss 0.10039073973894119 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2988],\n",
      "        [0.3487]], device='mps:0')\n",
      "Iteration 56500 Training loss 0.09927993267774582 Validation loss 0.10033808648586273 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5633],\n",
      "        [0.2606]], device='mps:0')\n",
      "Iteration 56510 Training loss 0.09749482572078705 Validation loss 0.1003279760479927 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5491],\n",
      "        [0.6007]], device='mps:0')\n",
      "Iteration 56520 Training loss 0.087847039103508 Validation loss 0.10033844411373138 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2507],\n",
      "        [0.3954]], device='mps:0')\n",
      "Iteration 56530 Training loss 0.10722846537828445 Validation loss 0.10035582631826401 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5788],\n",
      "        [0.3333]], device='mps:0')\n",
      "Iteration 56540 Training loss 0.1097133457660675 Validation loss 0.10039237141609192 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3299],\n",
      "        [0.6329]], device='mps:0')\n",
      "Iteration 56550 Training loss 0.09290655702352524 Validation loss 0.10043801367282867 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6630],\n",
      "        [0.6256]], device='mps:0')\n",
      "Iteration 56560 Training loss 0.11541375517845154 Validation loss 0.10037720203399658 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2416],\n",
      "        [0.5695]], device='mps:0')\n",
      "Iteration 56570 Training loss 0.11201633512973785 Validation loss 0.10035189986228943 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3440],\n",
      "        [0.6880]], device='mps:0')\n",
      "Iteration 56580 Training loss 0.11197194457054138 Validation loss 0.10031966120004654 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7693],\n",
      "        [0.3599]], device='mps:0')\n",
      "Iteration 56590 Training loss 0.0939190685749054 Validation loss 0.10032445937395096 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7278],\n",
      "        [0.3072]], device='mps:0')\n",
      "Iteration 56600 Training loss 0.10905023664236069 Validation loss 0.10032123327255249 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5866],\n",
      "        [0.2486]], device='mps:0')\n",
      "Iteration 56610 Training loss 0.093491330742836 Validation loss 0.10037460923194885 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4539],\n",
      "        [0.5989]], device='mps:0')\n",
      "Iteration 56620 Training loss 0.10386621952056885 Validation loss 0.10038119554519653 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4466],\n",
      "        [0.6414]], device='mps:0')\n",
      "Iteration 56630 Training loss 0.09000534564256668 Validation loss 0.10033098608255386 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4659],\n",
      "        [0.3940]], device='mps:0')\n",
      "Iteration 56640 Training loss 0.09960359334945679 Validation loss 0.10033069550991058 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4418],\n",
      "        [0.3988]], device='mps:0')\n",
      "Iteration 56650 Training loss 0.10064126551151276 Validation loss 0.10031957924365997 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3546],\n",
      "        [0.5682]], device='mps:0')\n",
      "Iteration 56660 Training loss 0.10611314326524734 Validation loss 0.10031884163618088 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2788],\n",
      "        [0.4949]], device='mps:0')\n",
      "Iteration 56670 Training loss 0.10372783243656158 Validation loss 0.10037889331579208 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5730],\n",
      "        [0.3018]], device='mps:0')\n",
      "Iteration 56680 Training loss 0.10053014755249023 Validation loss 0.10037373751401901 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3205],\n",
      "        [0.4063]], device='mps:0')\n",
      "Iteration 56690 Training loss 0.10498175024986267 Validation loss 0.10033939778804779 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.1673],\n",
      "        [0.3386]], device='mps:0')\n",
      "Iteration 56700 Training loss 0.10098966211080551 Validation loss 0.10033395886421204 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6527],\n",
      "        [0.6527]], device='mps:0')\n",
      "Iteration 56710 Training loss 0.1067269966006279 Validation loss 0.10033058375120163 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5956],\n",
      "        [0.7262]], device='mps:0')\n",
      "Iteration 56720 Training loss 0.10081418603658676 Validation loss 0.10036102682352066 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3691],\n",
      "        [0.4084]], device='mps:0')\n",
      "Iteration 56730 Training loss 0.09903976321220398 Validation loss 0.10030795633792877 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4605],\n",
      "        [0.4430]], device='mps:0')\n",
      "Iteration 56740 Training loss 0.11153893917798996 Validation loss 0.10030381381511688 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5578],\n",
      "        [0.5725]], device='mps:0')\n",
      "Iteration 56750 Training loss 0.0992308184504509 Validation loss 0.10033591091632843 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3545],\n",
      "        [0.4770]], device='mps:0')\n",
      "Iteration 56760 Training loss 0.088556207716465 Validation loss 0.100323885679245 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5133],\n",
      "        [0.4763]], device='mps:0')\n",
      "Iteration 56770 Training loss 0.089063860476017 Validation loss 0.10034331679344177 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5911],\n",
      "        [0.4527]], device='mps:0')\n",
      "Iteration 56780 Training loss 0.10991645604372025 Validation loss 0.10034189373254776 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2462],\n",
      "        [0.5979]], device='mps:0')\n",
      "Iteration 56790 Training loss 0.09576842933893204 Validation loss 0.10034247487783432 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2853],\n",
      "        [0.3547]], device='mps:0')\n",
      "Iteration 56800 Training loss 0.09789825230836868 Validation loss 0.10035792738199234 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.7229],\n",
      "        [0.2688]], device='mps:0')\n",
      "Iteration 56810 Training loss 0.10955992341041565 Validation loss 0.10034698992967606 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1143],\n",
      "        [0.1144]], device='mps:0')\n",
      "Iteration 56820 Training loss 0.09923356026411057 Validation loss 0.1003209576010704 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.7196],\n",
      "        [0.3344]], device='mps:0')\n",
      "Iteration 56830 Training loss 0.10608923435211182 Validation loss 0.10032600909471512 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.2537],\n",
      "        [0.5988]], device='mps:0')\n",
      "Iteration 56840 Training loss 0.09483018517494202 Validation loss 0.10029733180999756 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3029],\n",
      "        [0.2231]], device='mps:0')\n",
      "Iteration 56850 Training loss 0.10757201910018921 Validation loss 0.1002935916185379 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7421],\n",
      "        [0.6496]], device='mps:0')\n",
      "Iteration 56860 Training loss 0.11241558939218521 Validation loss 0.10029472410678864 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6391],\n",
      "        [0.7015]], device='mps:0')\n",
      "Iteration 56870 Training loss 0.09506499767303467 Validation loss 0.10029322654008865 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6939],\n",
      "        [0.6831]], device='mps:0')\n",
      "Iteration 56880 Training loss 0.1099078506231308 Validation loss 0.10030265152454376 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6590],\n",
      "        [0.3981]], device='mps:0')\n",
      "Iteration 56890 Training loss 0.09187322109937668 Validation loss 0.10029520094394684 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6205],\n",
      "        [0.5432]], device='mps:0')\n",
      "Iteration 56900 Training loss 0.10710171610116959 Validation loss 0.10030127316713333 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2889],\n",
      "        [0.5099]], device='mps:0')\n",
      "Iteration 56910 Training loss 0.10185372829437256 Validation loss 0.10029959678649902 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7024],\n",
      "        [0.2168]], device='mps:0')\n",
      "Iteration 56920 Training loss 0.10662799328565598 Validation loss 0.10033256560564041 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.3645],\n",
      "        [0.6369]], device='mps:0')\n",
      "Iteration 56930 Training loss 0.09863092005252838 Validation loss 0.10032835602760315 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6730],\n",
      "        [0.6936]], device='mps:0')\n",
      "Iteration 56940 Training loss 0.09819091856479645 Validation loss 0.10031425207853317 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.5231],\n",
      "        [0.5776]], device='mps:0')\n",
      "Iteration 56950 Training loss 0.10456573218107224 Validation loss 0.10031623393297195 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4685],\n",
      "        [0.5206]], device='mps:0')\n",
      "Iteration 56960 Training loss 0.11168142408132553 Validation loss 0.10031989961862564 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4946],\n",
      "        [0.7603]], device='mps:0')\n",
      "Iteration 56970 Training loss 0.11384651809930801 Validation loss 0.10029996931552887 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6512],\n",
      "        [0.3162]], device='mps:0')\n",
      "Iteration 56980 Training loss 0.1043548732995987 Validation loss 0.10030623525381088 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5165],\n",
      "        [0.6236]], device='mps:0')\n",
      "Iteration 56990 Training loss 0.0952606275677681 Validation loss 0.1003112867474556 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.5437],\n",
      "        [0.6920]], device='mps:0')\n",
      "Iteration 57000 Training loss 0.0992300882935524 Validation loss 0.10033591091632843 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3100],\n",
      "        [0.6532]], device='mps:0')\n",
      "Iteration 57010 Training loss 0.1054188683629036 Validation loss 0.10035644471645355 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6785],\n",
      "        [0.4998]], device='mps:0')\n",
      "Iteration 57020 Training loss 0.11108903586864471 Validation loss 0.10033141821622849 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5873],\n",
      "        [0.5777]], device='mps:0')\n",
      "Iteration 57030 Training loss 0.09537503868341446 Validation loss 0.10031180828809738 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5277],\n",
      "        [0.5041]], device='mps:0')\n",
      "Iteration 57040 Training loss 0.10073535889387131 Validation loss 0.10032510757446289 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.3907],\n",
      "        [0.5494]], device='mps:0')\n",
      "Iteration 57050 Training loss 0.10589519143104553 Validation loss 0.10031963884830475 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4599],\n",
      "        [0.0996]], device='mps:0')\n",
      "Iteration 57060 Training loss 0.09563293308019638 Validation loss 0.10028864443302155 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5723],\n",
      "        [0.5334]], device='mps:0')\n",
      "Iteration 57070 Training loss 0.10956279188394547 Validation loss 0.10029119998216629 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4134],\n",
      "        [0.5658]], device='mps:0')\n",
      "Iteration 57080 Training loss 0.09781849384307861 Validation loss 0.1002981886267662 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.1985],\n",
      "        [0.6266]], device='mps:0')\n",
      "Iteration 57090 Training loss 0.09619934111833572 Validation loss 0.10028375685214996 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7394],\n",
      "        [0.3582]], device='mps:0')\n",
      "Iteration 57100 Training loss 0.1057235524058342 Validation loss 0.10027430951595306 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7575],\n",
      "        [0.1957]], device='mps:0')\n",
      "Iteration 57110 Training loss 0.10433980822563171 Validation loss 0.10027512907981873 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3714],\n",
      "        [0.2757]], device='mps:0')\n",
      "Iteration 57120 Training loss 0.09753541648387909 Validation loss 0.1002737507224083 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4487],\n",
      "        [0.6612]], device='mps:0')\n",
      "Iteration 57130 Training loss 0.09361249953508377 Validation loss 0.10028913617134094 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6000],\n",
      "        [0.4651]], device='mps:0')\n",
      "Iteration 57140 Training loss 0.10874368250370026 Validation loss 0.10028716176748276 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4524],\n",
      "        [0.4065]], device='mps:0')\n",
      "Iteration 57150 Training loss 0.10309064388275146 Validation loss 0.10032439231872559 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6485],\n",
      "        [0.1812]], device='mps:0')\n",
      "Iteration 57160 Training loss 0.10524801164865494 Validation loss 0.10032318532466888 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3501],\n",
      "        [0.4581]], device='mps:0')\n",
      "Iteration 57170 Training loss 0.10365962982177734 Validation loss 0.10028836876153946 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4213],\n",
      "        [0.7367]], device='mps:0')\n",
      "Iteration 57180 Training loss 0.09576747566461563 Validation loss 0.1002979725599289 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4639],\n",
      "        [0.5693]], device='mps:0')\n",
      "Iteration 57190 Training loss 0.10210426151752472 Validation loss 0.10031954944133759 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3885],\n",
      "        [0.5712]], device='mps:0')\n",
      "Iteration 57200 Training loss 0.1022377535700798 Validation loss 0.10032065957784653 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.1959],\n",
      "        [0.4495]], device='mps:0')\n",
      "Iteration 57210 Training loss 0.10216149687767029 Validation loss 0.10030770301818848 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5916],\n",
      "        [0.4123]], device='mps:0')\n",
      "Iteration 57220 Training loss 0.09524398297071457 Validation loss 0.10030169785022736 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6032],\n",
      "        [0.7980]], device='mps:0')\n",
      "Iteration 57230 Training loss 0.09745648503303528 Validation loss 0.10033423453569412 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5716],\n",
      "        [0.4199]], device='mps:0')\n",
      "Iteration 57240 Training loss 0.11577365547418594 Validation loss 0.10034135729074478 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4038],\n",
      "        [0.6397]], device='mps:0')\n",
      "Iteration 57250 Training loss 0.09218417853116989 Validation loss 0.100315161049366 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7329],\n",
      "        [0.4717]], device='mps:0')\n",
      "Iteration 57260 Training loss 0.11528894305229187 Validation loss 0.10030063986778259 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5176],\n",
      "        [0.2639]], device='mps:0')\n",
      "Iteration 57270 Training loss 0.10581149905920029 Validation loss 0.10032232850790024 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4946],\n",
      "        [0.6491]], device='mps:0')\n",
      "Iteration 57280 Training loss 0.09199559688568115 Validation loss 0.1003020703792572 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6188],\n",
      "        [0.1338]], device='mps:0')\n",
      "Iteration 57290 Training loss 0.09229777753353119 Validation loss 0.10028202086687088 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5463],\n",
      "        [0.7029]], device='mps:0')\n",
      "Iteration 57300 Training loss 0.09727337956428528 Validation loss 0.10027356445789337 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6567],\n",
      "        [0.6771]], device='mps:0')\n",
      "Iteration 57310 Training loss 0.10062644630670547 Validation loss 0.10026964545249939 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4939],\n",
      "        [0.4407]], device='mps:0')\n",
      "Iteration 57320 Training loss 0.10997447371482849 Validation loss 0.10027088969945908 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.0841],\n",
      "        [0.2668]], device='mps:0')\n",
      "Iteration 57330 Training loss 0.10956362634897232 Validation loss 0.10026512295007706 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5358],\n",
      "        [0.7423]], device='mps:0')\n",
      "Iteration 57340 Training loss 0.110134057700634 Validation loss 0.10028329491615295 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6877],\n",
      "        [0.5240]], device='mps:0')\n",
      "Iteration 57350 Training loss 0.11190839856863022 Validation loss 0.10027304291725159 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3804],\n",
      "        [0.3142]], device='mps:0')\n",
      "Iteration 57360 Training loss 0.10156106948852539 Validation loss 0.10029412060976028 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5288],\n",
      "        [0.7286]], device='mps:0')\n",
      "Iteration 57370 Training loss 0.10635150969028473 Validation loss 0.10027742385864258 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7277],\n",
      "        [0.5654]], device='mps:0')\n",
      "Iteration 57380 Training loss 0.11829063296318054 Validation loss 0.10031195729970932 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5489],\n",
      "        [0.7615]], device='mps:0')\n",
      "Iteration 57390 Training loss 0.09725320339202881 Validation loss 0.100294329226017 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3292],\n",
      "        [0.6856]], device='mps:0')\n",
      "Iteration 57400 Training loss 0.11073709279298782 Validation loss 0.10028812289237976 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.7888],\n",
      "        [0.4917]], device='mps:0')\n",
      "Iteration 57410 Training loss 0.10928485542535782 Validation loss 0.10030224174261093 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6227],\n",
      "        [0.2468]], device='mps:0')\n",
      "Iteration 57420 Training loss 0.09918094426393509 Validation loss 0.1003115326166153 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5163],\n",
      "        [0.4596]], device='mps:0')\n",
      "Iteration 57430 Training loss 0.1085195541381836 Validation loss 0.10032640397548676 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2730],\n",
      "        [0.7757]], device='mps:0')\n",
      "Iteration 57440 Training loss 0.10617202520370483 Validation loss 0.10032382607460022 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4468],\n",
      "        [0.3074]], device='mps:0')\n",
      "Iteration 57450 Training loss 0.09778688848018646 Validation loss 0.10035631060600281 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7229],\n",
      "        [0.6953]], device='mps:0')\n",
      "Iteration 57460 Training loss 0.08885707706212997 Validation loss 0.10033456981182098 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4888],\n",
      "        [0.6829]], device='mps:0')\n",
      "Iteration 57470 Training loss 0.09932404011487961 Validation loss 0.10037919878959656 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.0755],\n",
      "        [0.7478]], device='mps:0')\n",
      "Iteration 57480 Training loss 0.10293012112379074 Validation loss 0.10035768151283264 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5343],\n",
      "        [0.4168]], device='mps:0')\n",
      "Iteration 57490 Training loss 0.10220708698034286 Validation loss 0.10038148611783981 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5347],\n",
      "        [0.1493]], device='mps:0')\n",
      "Iteration 57500 Training loss 0.09456470608711243 Validation loss 0.10036490857601166 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.0760],\n",
      "        [0.4016]], device='mps:0')\n",
      "Iteration 57510 Training loss 0.1070781797170639 Validation loss 0.10030876100063324 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3901],\n",
      "        [0.3576]], device='mps:0')\n",
      "Iteration 57520 Training loss 0.10290027409791946 Validation loss 0.1002955213189125 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6444],\n",
      "        [0.5821]], device='mps:0')\n",
      "Iteration 57530 Training loss 0.1088050827383995 Validation loss 0.10028859227895737 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1955],\n",
      "        [0.3357]], device='mps:0')\n",
      "Iteration 57540 Training loss 0.0941023975610733 Validation loss 0.1002853512763977 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4905],\n",
      "        [0.4678]], device='mps:0')\n",
      "Iteration 57550 Training loss 0.10934024304151535 Validation loss 0.10027176141738892 Accuracy 0.7160000205039978\n",
      "Output tensor([[0.3342],\n",
      "        [0.7330]], device='mps:0')\n",
      "Iteration 57560 Training loss 0.11276191473007202 Validation loss 0.10026168078184128 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5668],\n",
      "        [0.4132]], device='mps:0')\n",
      "Iteration 57570 Training loss 0.11284705996513367 Validation loss 0.10030597448348999 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5144],\n",
      "        [0.4940]], device='mps:0')\n",
      "Iteration 57580 Training loss 0.10557114332914352 Validation loss 0.10029935836791992 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7125],\n",
      "        [0.5824]], device='mps:0')\n",
      "Iteration 57590 Training loss 0.10217440128326416 Validation loss 0.10025745630264282 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6967],\n",
      "        [0.4865]], device='mps:0')\n",
      "Iteration 57600 Training loss 0.1091349795460701 Validation loss 0.10024837404489517 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6465],\n",
      "        [0.7002]], device='mps:0')\n",
      "Iteration 57610 Training loss 0.08993814140558243 Validation loss 0.10024774819612503 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5998],\n",
      "        [0.2983]], device='mps:0')\n",
      "Iteration 57620 Training loss 0.10362088680267334 Validation loss 0.10024110972881317 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2627],\n",
      "        [0.1981]], device='mps:0')\n",
      "Iteration 57630 Training loss 0.09254571050405502 Validation loss 0.10024077445268631 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6703],\n",
      "        [0.4147]], device='mps:0')\n",
      "Iteration 57640 Training loss 0.09470248967409134 Validation loss 0.10024233162403107 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7321],\n",
      "        [0.4020]], device='mps:0')\n",
      "Iteration 57650 Training loss 0.11153553426265717 Validation loss 0.10023785382509232 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4480],\n",
      "        [0.7069]], device='mps:0')\n",
      "Iteration 57660 Training loss 0.10321459174156189 Validation loss 0.10023865103721619 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6219],\n",
      "        [0.4802]], device='mps:0')\n",
      "Iteration 57670 Training loss 0.10525503009557724 Validation loss 0.10023557394742966 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4073],\n",
      "        [0.6821]], device='mps:0')\n",
      "Iteration 57680 Training loss 0.10967252403497696 Validation loss 0.10024259239435196 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7300],\n",
      "        [0.7275]], device='mps:0')\n",
      "Iteration 57690 Training loss 0.10502536594867706 Validation loss 0.10023633390665054 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3386],\n",
      "        [0.4207]], device='mps:0')\n",
      "Iteration 57700 Training loss 0.10794351249933243 Validation loss 0.10024174302816391 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1781],\n",
      "        [0.6614]], device='mps:0')\n",
      "Iteration 57710 Training loss 0.10153266787528992 Validation loss 0.10026244074106216 Accuracy 0.7160000205039978\n",
      "Output tensor([[0.3839],\n",
      "        [0.6472]], device='mps:0')\n",
      "Iteration 57720 Training loss 0.10092286020517349 Validation loss 0.10026519000530243 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.1491],\n",
      "        [0.3469]], device='mps:0')\n",
      "Iteration 57730 Training loss 0.09729785472154617 Validation loss 0.10023946315050125 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6034],\n",
      "        [0.2733]], device='mps:0')\n",
      "Iteration 57740 Training loss 0.107032909989357 Validation loss 0.1002306267619133 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4201],\n",
      "        [0.2869]], device='mps:0')\n",
      "Iteration 57750 Training loss 0.10938211530447006 Validation loss 0.10023407638072968 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3411],\n",
      "        [0.3253]], device='mps:0')\n",
      "Iteration 57760 Training loss 0.11288851499557495 Validation loss 0.10023118555545807 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7045],\n",
      "        [0.1569]], device='mps:0')\n",
      "Iteration 57770 Training loss 0.10975920408964157 Validation loss 0.10022895038127899 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5343],\n",
      "        [0.4657]], device='mps:0')\n",
      "Iteration 57780 Training loss 0.10017215460538864 Validation loss 0.1002252995967865 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5506],\n",
      "        [0.2399]], device='mps:0')\n",
      "Iteration 57790 Training loss 0.10104074329137802 Validation loss 0.10022450983524323 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6655],\n",
      "        [0.3731]], device='mps:0')\n",
      "Iteration 57800 Training loss 0.08935608714818954 Validation loss 0.10022182017564774 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6500],\n",
      "        [0.3768]], device='mps:0')\n",
      "Iteration 57810 Training loss 0.10756654292345047 Validation loss 0.1002209484577179 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5841],\n",
      "        [0.4928]], device='mps:0')\n",
      "Iteration 57820 Training loss 0.09723390638828278 Validation loss 0.10022669285535812 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2461],\n",
      "        [0.4069]], device='mps:0')\n",
      "Iteration 57830 Training loss 0.10831928253173828 Validation loss 0.10025430470705032 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6637],\n",
      "        [0.6322]], device='mps:0')\n",
      "Iteration 57840 Training loss 0.09197567403316498 Validation loss 0.10026910901069641 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6450],\n",
      "        [0.4389]], device='mps:0')\n",
      "Iteration 57850 Training loss 0.10297688841819763 Validation loss 0.10027313232421875 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4565],\n",
      "        [0.3946]], device='mps:0')\n",
      "Iteration 57860 Training loss 0.08900092542171478 Validation loss 0.10024745762348175 Accuracy 0.7160000205039978\n",
      "Output tensor([[0.2460],\n",
      "        [0.7520]], device='mps:0')\n",
      "Iteration 57870 Training loss 0.11274679005146027 Validation loss 0.10022428631782532 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6315],\n",
      "        [0.4720]], device='mps:0')\n",
      "Iteration 57880 Training loss 0.0989692211151123 Validation loss 0.1002165675163269 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5675],\n",
      "        [0.4194]], device='mps:0')\n",
      "Iteration 57890 Training loss 0.08867185562849045 Validation loss 0.1002156138420105 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4836],\n",
      "        [0.6552]], device='mps:0')\n",
      "Iteration 57900 Training loss 0.09983375668525696 Validation loss 0.10021694004535675 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5845],\n",
      "        [0.6655]], device='mps:0')\n",
      "Iteration 57910 Training loss 0.10035521537065506 Validation loss 0.10023180395364761 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3876],\n",
      "        [0.5974]], device='mps:0')\n",
      "Iteration 57920 Training loss 0.10884229093790054 Validation loss 0.10025223344564438 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4886],\n",
      "        [0.7485]], device='mps:0')\n",
      "Iteration 57930 Training loss 0.09954983741044998 Validation loss 0.10023811459541321 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4469],\n",
      "        [0.5610]], device='mps:0')\n",
      "Iteration 57940 Training loss 0.09403230249881744 Validation loss 0.10024020075798035 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4499],\n",
      "        [0.4674]], device='mps:0')\n",
      "Iteration 57950 Training loss 0.10858900099992752 Validation loss 0.10023383796215057 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5176],\n",
      "        [0.1459]], device='mps:0')\n",
      "Iteration 57960 Training loss 0.11188488453626633 Validation loss 0.10023417323827744 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4898],\n",
      "        [0.4954]], device='mps:0')\n",
      "Iteration 57970 Training loss 0.10066428780555725 Validation loss 0.10024873912334442 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7089],\n",
      "        [0.5560]], device='mps:0')\n",
      "Iteration 57980 Training loss 0.10380445420742035 Validation loss 0.10026022046804428 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3469],\n",
      "        [0.2824]], device='mps:0')\n",
      "Iteration 57990 Training loss 0.09992694854736328 Validation loss 0.1002781093120575 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2642],\n",
      "        [0.2807]], device='mps:0')\n",
      "Iteration 58000 Training loss 0.08940304815769196 Validation loss 0.1002589762210846 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3979],\n",
      "        [0.7998]], device='mps:0')\n",
      "Iteration 58010 Training loss 0.10409862548112869 Validation loss 0.10024665296077728 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7079],\n",
      "        [0.5905]], device='mps:0')\n",
      "Iteration 58020 Training loss 0.11155469715595245 Validation loss 0.10024496912956238 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.1376],\n",
      "        [0.5239]], device='mps:0')\n",
      "Iteration 58030 Training loss 0.10176698118448257 Validation loss 0.1002715528011322 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6410],\n",
      "        [0.7648]], device='mps:0')\n",
      "Iteration 58040 Training loss 0.09458635002374649 Validation loss 0.10026022046804428 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4790],\n",
      "        [0.5252]], device='mps:0')\n",
      "Iteration 58050 Training loss 0.108401820063591 Validation loss 0.10026150941848755 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3238],\n",
      "        [0.6809]], device='mps:0')\n",
      "Iteration 58060 Training loss 0.10678335279226303 Validation loss 0.10036435723304749 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6635],\n",
      "        [0.2119]], device='mps:0')\n",
      "Iteration 58070 Training loss 0.11998092383146286 Validation loss 0.10032565891742706 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6765],\n",
      "        [0.5431]], device='mps:0')\n",
      "Iteration 58080 Training loss 0.10000378638505936 Validation loss 0.10027949512004852 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4456],\n",
      "        [0.6817]], device='mps:0')\n",
      "Iteration 58090 Training loss 0.09755454957485199 Validation loss 0.10026730597019196 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5887],\n",
      "        [0.6503]], device='mps:0')\n",
      "Iteration 58100 Training loss 0.1030527725815773 Validation loss 0.10024836659431458 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2621],\n",
      "        [0.5431]], device='mps:0')\n",
      "Iteration 58110 Training loss 0.09686071425676346 Validation loss 0.10026122629642487 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4627],\n",
      "        [0.5089]], device='mps:0')\n",
      "Iteration 58120 Training loss 0.10138078778982162 Validation loss 0.10022597759962082 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6306],\n",
      "        [0.7535]], device='mps:0')\n",
      "Iteration 58130 Training loss 0.09925872832536697 Validation loss 0.10020303726196289 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4636],\n",
      "        [0.1837]], device='mps:0')\n",
      "Iteration 58140 Training loss 0.09651634842157364 Validation loss 0.10019659996032715 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6348],\n",
      "        [0.5518]], device='mps:0')\n",
      "Iteration 58150 Training loss 0.09511999040842056 Validation loss 0.10020086914300919 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2009],\n",
      "        [0.5540]], device='mps:0')\n",
      "Iteration 58160 Training loss 0.09446027874946594 Validation loss 0.10019150376319885 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3099],\n",
      "        [0.4940]], device='mps:0')\n",
      "Iteration 58170 Training loss 0.09453130513429642 Validation loss 0.10019073635339737 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5224],\n",
      "        [0.5493]], device='mps:0')\n",
      "Iteration 58180 Training loss 0.10112815350294113 Validation loss 0.10018860548734665 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7542],\n",
      "        [0.4086]], device='mps:0')\n",
      "Iteration 58190 Training loss 0.10552641749382019 Validation loss 0.100185826420784 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4653],\n",
      "        [0.5871]], device='mps:0')\n",
      "Iteration 58200 Training loss 0.10130937397480011 Validation loss 0.10018515586853027 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5310],\n",
      "        [0.1282]], device='mps:0')\n",
      "Iteration 58210 Training loss 0.10672631859779358 Validation loss 0.10019659996032715 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6729],\n",
      "        [0.4996]], device='mps:0')\n",
      "Iteration 58220 Training loss 0.10649967938661575 Validation loss 0.10022725909948349 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5083],\n",
      "        [0.5795]], device='mps:0')\n",
      "Iteration 58230 Training loss 0.11282842606306076 Validation loss 0.1002008244395256 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6610],\n",
      "        [0.6221]], device='mps:0')\n",
      "Iteration 58240 Training loss 0.10971580445766449 Validation loss 0.10019031167030334 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5514],\n",
      "        [0.6356]], device='mps:0')\n",
      "Iteration 58250 Training loss 0.1096530631184578 Validation loss 0.10023468732833862 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5164],\n",
      "        [0.5367]], device='mps:0')\n",
      "Iteration 58260 Training loss 0.10816500335931778 Validation loss 0.10024850070476532 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5265],\n",
      "        [0.4979]], device='mps:0')\n",
      "Iteration 58270 Training loss 0.10401112586259842 Validation loss 0.10028211027383804 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.1188],\n",
      "        [0.7252]], device='mps:0')\n",
      "Iteration 58280 Training loss 0.0948396772146225 Validation loss 0.10024265199899673 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4823],\n",
      "        [0.6965]], device='mps:0')\n",
      "Iteration 58290 Training loss 0.10654643923044205 Validation loss 0.10025674849748611 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3915],\n",
      "        [0.3692]], device='mps:0')\n",
      "Iteration 58300 Training loss 0.09005559980869293 Validation loss 0.10029512643814087 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2811],\n",
      "        [0.3835]], device='mps:0')\n",
      "Iteration 58310 Training loss 0.10443630814552307 Validation loss 0.10022292286157608 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2676],\n",
      "        [0.7407]], device='mps:0')\n",
      "Iteration 58320 Training loss 0.10452401638031006 Validation loss 0.100223608314991 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3499],\n",
      "        [0.5685]], device='mps:0')\n",
      "Iteration 58330 Training loss 0.11546196043491364 Validation loss 0.10020363330841064 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3976],\n",
      "        [0.4458]], device='mps:0')\n",
      "Iteration 58340 Training loss 0.10601872950792313 Validation loss 0.10022314637899399 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5454],\n",
      "        [0.5480]], device='mps:0')\n",
      "Iteration 58350 Training loss 0.10551641136407852 Validation loss 0.10020896792411804 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6723],\n",
      "        [0.1922]], device='mps:0')\n",
      "Iteration 58360 Training loss 0.09752833843231201 Validation loss 0.1002526730298996 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7973],\n",
      "        [0.2973]], device='mps:0')\n",
      "Iteration 58370 Training loss 0.10978179425001144 Validation loss 0.100284643471241 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4594],\n",
      "        [0.2358]], device='mps:0')\n",
      "Iteration 58380 Training loss 0.10095985233783722 Validation loss 0.10027392953634262 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3395],\n",
      "        [0.4491]], device='mps:0')\n",
      "Iteration 58390 Training loss 0.10441089421510696 Validation loss 0.10021412372589111 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.0779],\n",
      "        [0.5251]], device='mps:0')\n",
      "Iteration 58400 Training loss 0.10240062326192856 Validation loss 0.10024480521678925 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4443],\n",
      "        [0.5854]], device='mps:0')\n",
      "Iteration 58410 Training loss 0.10597516596317291 Validation loss 0.10024204850196838 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5383],\n",
      "        [0.6561]], device='mps:0')\n",
      "Iteration 58420 Training loss 0.10335037857294083 Validation loss 0.10021883249282837 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4177],\n",
      "        [0.0576]], device='mps:0')\n",
      "Iteration 58430 Training loss 0.10387071222066879 Validation loss 0.10021074116230011 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6636],\n",
      "        [0.5089]], device='mps:0')\n",
      "Iteration 58440 Training loss 0.10816125571727753 Validation loss 0.10020731389522552 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.2553],\n",
      "        [0.5784]], device='mps:0')\n",
      "Iteration 58450 Training loss 0.11698637902736664 Validation loss 0.10017640888690948 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2678],\n",
      "        [0.5254]], device='mps:0')\n",
      "Iteration 58460 Training loss 0.1043362244963646 Validation loss 0.10016697645187378 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5418],\n",
      "        [0.1785]], device='mps:0')\n",
      "Iteration 58470 Training loss 0.10191547125577927 Validation loss 0.10016979277133942 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6046],\n",
      "        [0.6378]], device='mps:0')\n",
      "Iteration 58480 Training loss 0.10252004116773605 Validation loss 0.10019543766975403 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.2548],\n",
      "        [0.3376]], device='mps:0')\n",
      "Iteration 58490 Training loss 0.11523719877004623 Validation loss 0.10020105540752411 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3143],\n",
      "        [0.4178]], device='mps:0')\n",
      "Iteration 58500 Training loss 0.09009610861539841 Validation loss 0.10016249865293503 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3534],\n",
      "        [0.3515]], device='mps:0')\n",
      "Iteration 58510 Training loss 0.09750861674547195 Validation loss 0.10016995668411255 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.0782],\n",
      "        [0.3231]], device='mps:0')\n",
      "Iteration 58520 Training loss 0.09582532942295074 Validation loss 0.10016002506017685 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.1406],\n",
      "        [0.2161]], device='mps:0')\n",
      "Iteration 58530 Training loss 0.09605012834072113 Validation loss 0.10017120838165283 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4212],\n",
      "        [0.3136]], device='mps:0')\n",
      "Iteration 58540 Training loss 0.0992157906293869 Validation loss 0.10015957802534103 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4135],\n",
      "        [0.6250]], device='mps:0')\n",
      "Iteration 58550 Training loss 0.10472407937049866 Validation loss 0.10017424076795578 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4330],\n",
      "        [0.4314]], device='mps:0')\n",
      "Iteration 58560 Training loss 0.101559579372406 Validation loss 0.10017053037881851 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5299],\n",
      "        [0.7748]], device='mps:0')\n",
      "Iteration 58570 Training loss 0.1064460352063179 Validation loss 0.10020096600055695 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6820],\n",
      "        [0.6217]], device='mps:0')\n",
      "Iteration 58580 Training loss 0.10013788193464279 Validation loss 0.1001935824751854 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5638],\n",
      "        [0.6274]], device='mps:0')\n",
      "Iteration 58590 Training loss 0.10260099172592163 Validation loss 0.10018514841794968 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5438],\n",
      "        [0.7174]], device='mps:0')\n",
      "Iteration 58600 Training loss 0.0948585495352745 Validation loss 0.10019000619649887 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6682],\n",
      "        [0.4334]], device='mps:0')\n",
      "Iteration 58610 Training loss 0.09178348630666733 Validation loss 0.10019981861114502 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6382],\n",
      "        [0.6894]], device='mps:0')\n",
      "Iteration 58620 Training loss 0.10657208412885666 Validation loss 0.10020118951797485 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2003],\n",
      "        [0.5636]], device='mps:0')\n",
      "Iteration 58630 Training loss 0.10585111379623413 Validation loss 0.10021039098501205 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4509],\n",
      "        [0.4678]], device='mps:0')\n",
      "Iteration 58640 Training loss 0.09162280708551407 Validation loss 0.10019836574792862 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4568],\n",
      "        [0.5585]], device='mps:0')\n",
      "Iteration 58650 Training loss 0.0927804708480835 Validation loss 0.10020067542791367 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4308],\n",
      "        [0.5589]], device='mps:0')\n",
      "Iteration 58660 Training loss 0.1098305955529213 Validation loss 0.10018344223499298 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2984],\n",
      "        [0.4359]], device='mps:0')\n",
      "Iteration 58670 Training loss 0.106755331158638 Validation loss 0.10021442919969559 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3782],\n",
      "        [0.6736]], device='mps:0')\n",
      "Iteration 58680 Training loss 0.10626517236232758 Validation loss 0.10018999874591827 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.1063],\n",
      "        [0.4631]], device='mps:0')\n",
      "Iteration 58690 Training loss 0.11124852299690247 Validation loss 0.1001550480723381 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2944],\n",
      "        [0.4632]], device='mps:0')\n",
      "Iteration 58700 Training loss 0.10001609474420547 Validation loss 0.10014806687831879 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4268],\n",
      "        [0.1563]], device='mps:0')\n",
      "Iteration 58710 Training loss 0.11003457754850388 Validation loss 0.1001477912068367 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7116],\n",
      "        [0.5920]], device='mps:0')\n",
      "Iteration 58720 Training loss 0.10066132992506027 Validation loss 0.10016319155693054 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5260],\n",
      "        [0.2629]], device='mps:0')\n",
      "Iteration 58730 Training loss 0.09604140371084213 Validation loss 0.10017789155244827 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6055],\n",
      "        [0.5893]], device='mps:0')\n",
      "Iteration 58740 Training loss 0.10872607678174973 Validation loss 0.10017906129360199 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5016],\n",
      "        [0.4985]], device='mps:0')\n",
      "Iteration 58750 Training loss 0.1027248352766037 Validation loss 0.10018420964479446 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6277],\n",
      "        [0.3597]], device='mps:0')\n",
      "Iteration 58760 Training loss 0.1026952788233757 Validation loss 0.1002308651804924 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6215],\n",
      "        [0.5081]], device='mps:0')\n",
      "Iteration 58770 Training loss 0.10566732287406921 Validation loss 0.10021591931581497 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5401],\n",
      "        [0.6286]], device='mps:0')\n",
      "Iteration 58780 Training loss 0.10157062113285065 Validation loss 0.1001809760928154 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6026],\n",
      "        [0.2366]], device='mps:0')\n",
      "Iteration 58790 Training loss 0.10029098391532898 Validation loss 0.10015762597322464 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5726],\n",
      "        [0.5276]], device='mps:0')\n",
      "Iteration 58800 Training loss 0.10990828275680542 Validation loss 0.10019505769014359 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4474],\n",
      "        [0.5175]], device='mps:0')\n",
      "Iteration 58810 Training loss 0.11282908171415329 Validation loss 0.10019166022539139 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4344],\n",
      "        [0.4974]], device='mps:0')\n",
      "Iteration 58820 Training loss 0.10400840640068054 Validation loss 0.10016774386167526 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6372],\n",
      "        [0.2364]], device='mps:0')\n",
      "Iteration 58830 Training loss 0.10040410608053207 Validation loss 0.10014380514621735 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4558],\n",
      "        [0.5391]], device='mps:0')\n",
      "Iteration 58840 Training loss 0.11137857288122177 Validation loss 0.10013984143733978 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3240],\n",
      "        [0.2090]], device='mps:0')\n",
      "Iteration 58850 Training loss 0.09531080722808838 Validation loss 0.10013188421726227 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4313],\n",
      "        [0.3968]], device='mps:0')\n",
      "Iteration 58860 Training loss 0.10132966935634613 Validation loss 0.10014313459396362 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5102],\n",
      "        [0.5256]], device='mps:0')\n",
      "Iteration 58870 Training loss 0.09933173656463623 Validation loss 0.10014167428016663 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3127],\n",
      "        [0.6951]], device='mps:0')\n",
      "Iteration 58880 Training loss 0.10047362744808197 Validation loss 0.10013541579246521 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3221],\n",
      "        [0.1246]], device='mps:0')\n",
      "Iteration 58890 Training loss 0.100000761449337 Validation loss 0.10013000667095184 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6827],\n",
      "        [0.4808]], device='mps:0')\n",
      "Iteration 58900 Training loss 0.11070477962493896 Validation loss 0.1001267358660698 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6104],\n",
      "        [0.0992]], device='mps:0')\n",
      "Iteration 58910 Training loss 0.1090264767408371 Validation loss 0.10013503581285477 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4869],\n",
      "        [0.5487]], device='mps:0')\n",
      "Iteration 58920 Training loss 0.09713613241910934 Validation loss 0.10012291371822357 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.1644],\n",
      "        [0.4525]], device='mps:0')\n",
      "Iteration 58930 Training loss 0.09659399837255478 Validation loss 0.10012135654687881 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4065],\n",
      "        [0.6218]], device='mps:0')\n",
      "Iteration 58940 Training loss 0.09862431138753891 Validation loss 0.10012700408697128 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3060],\n",
      "        [0.6683]], device='mps:0')\n",
      "Iteration 58950 Training loss 0.09295417368412018 Validation loss 0.10014902055263519 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.4870],\n",
      "        [0.5154]], device='mps:0')\n",
      "Iteration 58960 Training loss 0.09913364797830582 Validation loss 0.10013460367918015 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2418],\n",
      "        [0.5246]], device='mps:0')\n",
      "Iteration 58970 Training loss 0.08796367794275284 Validation loss 0.10014626383781433 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6371],\n",
      "        [0.5316]], device='mps:0')\n",
      "Iteration 58980 Training loss 0.09723799675703049 Validation loss 0.10017170011997223 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6516],\n",
      "        [0.4596]], device='mps:0')\n",
      "Iteration 58990 Training loss 0.09646335244178772 Validation loss 0.10021386295557022 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6237],\n",
      "        [0.5504]], device='mps:0')\n",
      "Iteration 59000 Training loss 0.09412780404090881 Validation loss 0.10016695410013199 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7701],\n",
      "        [0.4708]], device='mps:0')\n",
      "Iteration 59010 Training loss 0.10655971616506577 Validation loss 0.10015271604061127 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4718],\n",
      "        [0.6951]], device='mps:0')\n",
      "Iteration 59020 Training loss 0.10447651892900467 Validation loss 0.10020504891872406 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5110],\n",
      "        [0.6387]], device='mps:0')\n",
      "Iteration 59030 Training loss 0.09334174543619156 Validation loss 0.10013731569051743 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4974],\n",
      "        [0.6520]], device='mps:0')\n",
      "Iteration 59040 Training loss 0.11064330488443375 Validation loss 0.1001199260354042 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4474],\n",
      "        [0.6180]], device='mps:0')\n",
      "Iteration 59050 Training loss 0.10698262602090836 Validation loss 0.10011810064315796 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1448],\n",
      "        [0.3695]], device='mps:0')\n",
      "Iteration 59060 Training loss 0.10908815264701843 Validation loss 0.10011395812034607 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7253],\n",
      "        [0.5615]], device='mps:0')\n",
      "Iteration 59070 Training loss 0.10001533478498459 Validation loss 0.10010947287082672 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2602],\n",
      "        [0.2928]], device='mps:0')\n",
      "Iteration 59080 Training loss 0.105865478515625 Validation loss 0.10011589527130127 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2455],\n",
      "        [0.5545]], device='mps:0')\n",
      "Iteration 59090 Training loss 0.09914503246545792 Validation loss 0.10011064261198044 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5935],\n",
      "        [0.1991]], device='mps:0')\n",
      "Iteration 59100 Training loss 0.09111669659614563 Validation loss 0.10011139512062073 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2522],\n",
      "        [0.2452]], device='mps:0')\n",
      "Iteration 59110 Training loss 0.09174064546823502 Validation loss 0.10011040419340134 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5489],\n",
      "        [0.4052]], device='mps:0')\n",
      "Iteration 59120 Training loss 0.09910976141691208 Validation loss 0.1001105085015297 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5435],\n",
      "        [0.3548]], device='mps:0')\n",
      "Iteration 59130 Training loss 0.10916009545326233 Validation loss 0.10012499988079071 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.7361],\n",
      "        [0.6230]], device='mps:0')\n",
      "Iteration 59140 Training loss 0.09853418171405792 Validation loss 0.10013396292924881 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5159],\n",
      "        [0.3523]], device='mps:0')\n",
      "Iteration 59150 Training loss 0.10012597590684891 Validation loss 0.10010732710361481 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3376],\n",
      "        [0.0977]], device='mps:0')\n",
      "Iteration 59160 Training loss 0.10052989423274994 Validation loss 0.10010974854230881 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7456],\n",
      "        [0.3290]], device='mps:0')\n",
      "Iteration 59170 Training loss 0.10680493712425232 Validation loss 0.10011004656553268 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3431],\n",
      "        [0.6002]], device='mps:0')\n",
      "Iteration 59180 Training loss 0.11874532699584961 Validation loss 0.10010886192321777 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6635],\n",
      "        [0.4279]], device='mps:0')\n",
      "Iteration 59190 Training loss 0.0998322144150734 Validation loss 0.1001167818903923 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6405],\n",
      "        [0.2796]], device='mps:0')\n",
      "Iteration 59200 Training loss 0.09955816715955734 Validation loss 0.10011611133813858 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.1167],\n",
      "        [0.4506]], device='mps:0')\n",
      "Iteration 59210 Training loss 0.09233779460191727 Validation loss 0.10014565289020538 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2248],\n",
      "        [0.5757]], device='mps:0')\n",
      "Iteration 59220 Training loss 0.09400047361850739 Validation loss 0.1001380980014801 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.1398],\n",
      "        [0.6448]], device='mps:0')\n",
      "Iteration 59230 Training loss 0.1058858260512352 Validation loss 0.10010982304811478 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6426],\n",
      "        [0.5351]], device='mps:0')\n",
      "Iteration 59240 Training loss 0.10294414311647415 Validation loss 0.10011404007673264 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5197],\n",
      "        [0.3203]], device='mps:0')\n",
      "Iteration 59250 Training loss 0.10599855333566666 Validation loss 0.10011361539363861 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2716],\n",
      "        [0.2696]], device='mps:0')\n",
      "Iteration 59260 Training loss 0.09058311581611633 Validation loss 0.10013249516487122 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4307],\n",
      "        [0.5200]], device='mps:0')\n",
      "Iteration 59270 Training loss 0.09688661247491837 Validation loss 0.1001482605934143 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3877],\n",
      "        [0.6110]], device='mps:0')\n",
      "Iteration 59280 Training loss 0.10359746962785721 Validation loss 0.10016603767871857 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.8107],\n",
      "        [0.4194]], device='mps:0')\n",
      "Iteration 59290 Training loss 0.09398722648620605 Validation loss 0.10016336292028427 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7557],\n",
      "        [0.6021]], device='mps:0')\n",
      "Iteration 59300 Training loss 0.09488502144813538 Validation loss 0.10017669945955276 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6695],\n",
      "        [0.1486]], device='mps:0')\n",
      "Iteration 59310 Training loss 0.09410211443901062 Validation loss 0.10011148452758789 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6609],\n",
      "        [0.3914]], device='mps:0')\n",
      "Iteration 59320 Training loss 0.09647693485021591 Validation loss 0.10011252760887146 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.1605],\n",
      "        [0.2965]], device='mps:0')\n",
      "Iteration 59330 Training loss 0.10137006640434265 Validation loss 0.10013855248689651 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3012],\n",
      "        [0.4924]], device='mps:0')\n",
      "Iteration 59340 Training loss 0.09963048994541168 Validation loss 0.10017793625593185 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2449],\n",
      "        [0.8440]], device='mps:0')\n",
      "Iteration 59350 Training loss 0.10468155890703201 Validation loss 0.10011906921863556 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.2050],\n",
      "        [0.6394]], device='mps:0')\n",
      "Iteration 59360 Training loss 0.1026214212179184 Validation loss 0.10011754184961319 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.5215],\n",
      "        [0.2034]], device='mps:0')\n",
      "Iteration 59370 Training loss 0.11134273558855057 Validation loss 0.10013795644044876 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5666],\n",
      "        [0.2836]], device='mps:0')\n",
      "Iteration 59380 Training loss 0.09137637913227081 Validation loss 0.10014213621616364 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5078],\n",
      "        [0.3133]], device='mps:0')\n",
      "Iteration 59390 Training loss 0.10359280556440353 Validation loss 0.10011273622512817 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.2653],\n",
      "        [0.2665]], device='mps:0')\n",
      "Iteration 59400 Training loss 0.10401365160942078 Validation loss 0.10015527904033661 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5665],\n",
      "        [0.6393]], device='mps:0')\n",
      "Iteration 59410 Training loss 0.11530128121376038 Validation loss 0.10012711584568024 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1545],\n",
      "        [0.4379]], device='mps:0')\n",
      "Iteration 59420 Training loss 0.1056889221072197 Validation loss 0.10011765360832214 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5260],\n",
      "        [0.5224]], device='mps:0')\n",
      "Iteration 59430 Training loss 0.10643766820430756 Validation loss 0.10016734898090363 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6945],\n",
      "        [0.3174]], device='mps:0')\n",
      "Iteration 59440 Training loss 0.09194333851337433 Validation loss 0.10012675821781158 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6940],\n",
      "        [0.3618]], device='mps:0')\n",
      "Iteration 59450 Training loss 0.09460844844579697 Validation loss 0.10015185922384262 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5286],\n",
      "        [0.7300]], device='mps:0')\n",
      "Iteration 59460 Training loss 0.09671268612146378 Validation loss 0.1000920757651329 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3640],\n",
      "        [0.5967]], device='mps:0')\n",
      "Iteration 59470 Training loss 0.09410139173269272 Validation loss 0.10008794069290161 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6744],\n",
      "        [0.6671]], device='mps:0')\n",
      "Iteration 59480 Training loss 0.10239811986684799 Validation loss 0.10008464008569717 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5059],\n",
      "        [0.6049]], device='mps:0')\n",
      "Iteration 59490 Training loss 0.0914703905582428 Validation loss 0.100098617374897 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4632],\n",
      "        [0.2412]], device='mps:0')\n",
      "Iteration 59500 Training loss 0.1077306941151619 Validation loss 0.10008339583873749 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5559],\n",
      "        [0.6768]], device='mps:0')\n",
      "Iteration 59510 Training loss 0.09763530641794205 Validation loss 0.1000818982720375 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.7544],\n",
      "        [0.6771]], device='mps:0')\n",
      "Iteration 59520 Training loss 0.10748045891523361 Validation loss 0.1000821590423584 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3918],\n",
      "        [0.4068]], device='mps:0')\n",
      "Iteration 59530 Training loss 0.11640255898237228 Validation loss 0.10008522868156433 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3982],\n",
      "        [0.4101]], device='mps:0')\n",
      "Iteration 59540 Training loss 0.10726264119148254 Validation loss 0.10007690638303757 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2135],\n",
      "        [0.0996]], device='mps:0')\n",
      "Iteration 59550 Training loss 0.10303117334842682 Validation loss 0.10007401555776596 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5840],\n",
      "        [0.1121]], device='mps:0')\n",
      "Iteration 59560 Training loss 0.09516961872577667 Validation loss 0.10007339715957642 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3305],\n",
      "        [0.2780]], device='mps:0')\n",
      "Iteration 59570 Training loss 0.09678245335817337 Validation loss 0.1000802293419838 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4263],\n",
      "        [0.6312]], device='mps:0')\n",
      "Iteration 59580 Training loss 0.10759393125772476 Validation loss 0.10008034110069275 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6382],\n",
      "        [0.2271]], device='mps:0')\n",
      "Iteration 59590 Training loss 0.11056473851203918 Validation loss 0.10007210820913315 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3028],\n",
      "        [0.4406]], device='mps:0')\n",
      "Iteration 59600 Training loss 0.11213375627994537 Validation loss 0.10007078945636749 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1255],\n",
      "        [0.7484]], device='mps:0')\n",
      "Iteration 59610 Training loss 0.09158231317996979 Validation loss 0.10008884221315384 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6415],\n",
      "        [0.6859]], device='mps:0')\n",
      "Iteration 59620 Training loss 0.09331972151994705 Validation loss 0.10011124610900879 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3563],\n",
      "        [0.5547]], device='mps:0')\n",
      "Iteration 59630 Training loss 0.10232987999916077 Validation loss 0.1000957265496254 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.1853],\n",
      "        [0.6532]], device='mps:0')\n",
      "Iteration 59640 Training loss 0.10116138309240341 Validation loss 0.10007240623235703 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5398],\n",
      "        [0.6821]], device='mps:0')\n",
      "Iteration 59650 Training loss 0.10081304609775543 Validation loss 0.10011471807956696 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6321],\n",
      "        [0.6511]], device='mps:0')\n",
      "Iteration 59660 Training loss 0.10372054576873779 Validation loss 0.10010603815317154 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7344],\n",
      "        [0.6402]], device='mps:0')\n",
      "Iteration 59670 Training loss 0.10018795728683472 Validation loss 0.1001000702381134 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4601],\n",
      "        [0.4549]], device='mps:0')\n",
      "Iteration 59680 Training loss 0.10298798978328705 Validation loss 0.10007085651159286 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5671],\n",
      "        [0.5766]], device='mps:0')\n",
      "Iteration 59690 Training loss 0.10000965744256973 Validation loss 0.10009067505598068 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5216],\n",
      "        [0.2206]], device='mps:0')\n",
      "Iteration 59700 Training loss 0.10611438751220703 Validation loss 0.10008949786424637 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6073],\n",
      "        [0.6034]], device='mps:0')\n",
      "Iteration 59710 Training loss 0.09845805913209915 Validation loss 0.10006362199783325 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7176],\n",
      "        [0.3136]], device='mps:0')\n",
      "Iteration 59720 Training loss 0.11178875714540482 Validation loss 0.1000683605670929 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6436],\n",
      "        [0.2156]], device='mps:0')\n",
      "Iteration 59730 Training loss 0.10037459433078766 Validation loss 0.10005997121334076 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5509],\n",
      "        [0.2298]], device='mps:0')\n",
      "Iteration 59740 Training loss 0.10687962919473648 Validation loss 0.10007631033658981 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2994],\n",
      "        [0.6555]], device='mps:0')\n",
      "Iteration 59750 Training loss 0.1092035248875618 Validation loss 0.10008146613836288 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.2145],\n",
      "        [0.2362]], device='mps:0')\n",
      "Iteration 59760 Training loss 0.11399408429861069 Validation loss 0.10010797530412674 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4537],\n",
      "        [0.5401]], device='mps:0')\n",
      "Iteration 59770 Training loss 0.10648441314697266 Validation loss 0.10013013333082199 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3668],\n",
      "        [0.6353]], device='mps:0')\n",
      "Iteration 59780 Training loss 0.10153418034315109 Validation loss 0.10014679282903671 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4766],\n",
      "        [0.5896]], device='mps:0')\n",
      "Iteration 59790 Training loss 0.10402709990739822 Validation loss 0.10014444589614868 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2767],\n",
      "        [0.6689]], device='mps:0')\n",
      "Iteration 59800 Training loss 0.1046006977558136 Validation loss 0.1001134067773819 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4213],\n",
      "        [0.5823]], device='mps:0')\n",
      "Iteration 59810 Training loss 0.1077452078461647 Validation loss 0.10008440166711807 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4471],\n",
      "        [0.6182]], device='mps:0')\n",
      "Iteration 59820 Training loss 0.10144763439893723 Validation loss 0.10006318986415863 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3683],\n",
      "        [0.4012]], device='mps:0')\n",
      "Iteration 59830 Training loss 0.0990627110004425 Validation loss 0.10005415976047516 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5351],\n",
      "        [0.4171]], device='mps:0')\n",
      "Iteration 59840 Training loss 0.11673015356063843 Validation loss 0.10005347430706024 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6091],\n",
      "        [0.3835]], device='mps:0')\n",
      "Iteration 59850 Training loss 0.08889321982860565 Validation loss 0.10005366802215576 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4525],\n",
      "        [0.1203]], device='mps:0')\n",
      "Iteration 59860 Training loss 0.0890352874994278 Validation loss 0.10005653649568558 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5412],\n",
      "        [0.5538]], device='mps:0')\n",
      "Iteration 59870 Training loss 0.10163165628910065 Validation loss 0.10006631910800934 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4123],\n",
      "        [0.2848]], device='mps:0')\n",
      "Iteration 59880 Training loss 0.09613143652677536 Validation loss 0.10007746517658234 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3927],\n",
      "        [0.3255]], device='mps:0')\n",
      "Iteration 59890 Training loss 0.08905752748250961 Validation loss 0.10008448362350464 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5945],\n",
      "        [0.2934]], device='mps:0')\n",
      "Iteration 59900 Training loss 0.10586857795715332 Validation loss 0.10008756071329117 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3392],\n",
      "        [0.7420]], device='mps:0')\n",
      "Iteration 59910 Training loss 0.10560354590415955 Validation loss 0.10004793107509613 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5859],\n",
      "        [0.2885]], device='mps:0')\n",
      "Iteration 59920 Training loss 0.11036816984415054 Validation loss 0.1000446230173111 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3188],\n",
      "        [0.4900]], device='mps:0')\n",
      "Iteration 59930 Training loss 0.10939785838127136 Validation loss 0.10004635900259018 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5969],\n",
      "        [0.6517]], device='mps:0')\n",
      "Iteration 59940 Training loss 0.10038021206855774 Validation loss 0.1000584289431572 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6222],\n",
      "        [0.5968]], device='mps:0')\n",
      "Iteration 59950 Training loss 0.10824558138847351 Validation loss 0.1000717282295227 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7486],\n",
      "        [0.7935]], device='mps:0')\n",
      "Iteration 59960 Training loss 0.12063290923833847 Validation loss 0.10005960613489151 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4627],\n",
      "        [0.6394]], device='mps:0')\n",
      "Iteration 59970 Training loss 0.10869047790765762 Validation loss 0.10008969902992249 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4100],\n",
      "        [0.5725]], device='mps:0')\n",
      "Iteration 59980 Training loss 0.11127311736345291 Validation loss 0.10009386390447617 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4782],\n",
      "        [0.6476]], device='mps:0')\n",
      "Iteration 59990 Training loss 0.10174985975027084 Validation loss 0.10009370744228363 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2684],\n",
      "        [0.2158]], device='mps:0')\n",
      "Iteration 60000 Training loss 0.10497278720140457 Validation loss 0.10006677359342575 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.7011],\n",
      "        [0.5745]], device='mps:0')\n",
      "Iteration 60010 Training loss 0.10788189619779587 Validation loss 0.10008654743432999 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5728],\n",
      "        [0.6401]], device='mps:0')\n",
      "Iteration 60020 Training loss 0.10288850218057632 Validation loss 0.1000567078590393 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4949],\n",
      "        [0.4258]], device='mps:0')\n",
      "Iteration 60030 Training loss 0.10028409957885742 Validation loss 0.10006949305534363 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.1946],\n",
      "        [0.6403]], device='mps:0')\n",
      "Iteration 60040 Training loss 0.09801574796438217 Validation loss 0.10008306801319122 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5543],\n",
      "        [0.6888]], device='mps:0')\n",
      "Iteration 60050 Training loss 0.10996708273887634 Validation loss 0.10003837198019028 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6311],\n",
      "        [0.2637]], device='mps:0')\n",
      "Iteration 60060 Training loss 0.10403633862733841 Validation loss 0.10004211217164993 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4630],\n",
      "        [0.2680]], device='mps:0')\n",
      "Iteration 60070 Training loss 0.10521882027387619 Validation loss 0.10004906356334686 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.2545],\n",
      "        [0.4718]], device='mps:0')\n",
      "Iteration 60080 Training loss 0.11343947798013687 Validation loss 0.10006218403577805 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.3408],\n",
      "        [0.7800]], device='mps:0')\n",
      "Iteration 60090 Training loss 0.10671033710241318 Validation loss 0.1000455990433693 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4733],\n",
      "        [0.4527]], device='mps:0')\n",
      "Iteration 60100 Training loss 0.10385821759700775 Validation loss 0.10004880279302597 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.7877],\n",
      "        [0.4411]], device='mps:0')\n",
      "Iteration 60110 Training loss 0.11057809740304947 Validation loss 0.10004419088363647 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6634],\n",
      "        [0.6651]], device='mps:0')\n",
      "Iteration 60120 Training loss 0.09141956269741058 Validation loss 0.10003793984651566 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5851],\n",
      "        [0.5700]], device='mps:0')\n",
      "Iteration 60130 Training loss 0.11399436742067337 Validation loss 0.10002952069044113 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5703],\n",
      "        [0.6537]], device='mps:0')\n",
      "Iteration 60140 Training loss 0.09225952625274658 Validation loss 0.10002759099006653 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5458],\n",
      "        [0.2762]], device='mps:0')\n",
      "Iteration 60150 Training loss 0.09998403489589691 Validation loss 0.10003824532032013 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5866],\n",
      "        [0.7068]], device='mps:0')\n",
      "Iteration 60160 Training loss 0.10063986480236053 Validation loss 0.10002940148115158 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7114],\n",
      "        [0.5999]], device='mps:0')\n",
      "Iteration 60170 Training loss 0.11335865408182144 Validation loss 0.10006467252969742 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6652],\n",
      "        [0.5429]], device='mps:0')\n",
      "Iteration 60180 Training loss 0.10044451802968979 Validation loss 0.10002581030130386 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5828],\n",
      "        [0.5981]], device='mps:0')\n",
      "Iteration 60190 Training loss 0.09376140683889389 Validation loss 0.10002371668815613 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2567],\n",
      "        [0.4446]], device='mps:0')\n",
      "Iteration 60200 Training loss 0.10916357487440109 Validation loss 0.10002164542675018 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5307],\n",
      "        [0.4892]], device='mps:0')\n",
      "Iteration 60210 Training loss 0.10741528868675232 Validation loss 0.10002199560403824 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1384],\n",
      "        [0.1980]], device='mps:0')\n",
      "Iteration 60220 Training loss 0.10902099311351776 Validation loss 0.10002560913562775 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6032],\n",
      "        [0.5270]], device='mps:0')\n",
      "Iteration 60230 Training loss 0.11098688095808029 Validation loss 0.10001590102910995 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5864],\n",
      "        [0.3191]], device='mps:0')\n",
      "Iteration 60240 Training loss 0.10031265765428543 Validation loss 0.10003472119569778 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6531],\n",
      "        [0.4434]], device='mps:0')\n",
      "Iteration 60250 Training loss 0.10593321919441223 Validation loss 0.10002582520246506 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4000],\n",
      "        [0.4759]], device='mps:0')\n",
      "Iteration 60260 Training loss 0.10046669095754623 Validation loss 0.10001703351736069 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5837],\n",
      "        [0.4787]], device='mps:0')\n",
      "Iteration 60270 Training loss 0.10792165249586105 Validation loss 0.10001575201749802 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2504],\n",
      "        [0.2361]], device='mps:0')\n",
      "Iteration 60280 Training loss 0.10528972744941711 Validation loss 0.10001838952302933 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5659],\n",
      "        [0.1694]], device='mps:0')\n",
      "Iteration 60290 Training loss 0.10762467980384827 Validation loss 0.1000194251537323 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3606],\n",
      "        [0.6874]], device='mps:0')\n",
      "Iteration 60300 Training loss 0.10216821730136871 Validation loss 0.1000145822763443 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3891],\n",
      "        [0.6834]], device='mps:0')\n",
      "Iteration 60310 Training loss 0.09843722730875015 Validation loss 0.10005401819944382 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2034],\n",
      "        [0.5182]], device='mps:0')\n",
      "Iteration 60320 Training loss 0.1027844101190567 Validation loss 0.10006419569253922 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5970],\n",
      "        [0.2836]], device='mps:0')\n",
      "Iteration 60330 Training loss 0.09997648000717163 Validation loss 0.10002277046442032 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1625],\n",
      "        [0.4973]], device='mps:0')\n",
      "Iteration 60340 Training loss 0.08960963785648346 Validation loss 0.10001153498888016 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5494],\n",
      "        [0.3956]], device='mps:0')\n",
      "Iteration 60350 Training loss 0.11003997921943665 Validation loss 0.10001072287559509 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5300],\n",
      "        [0.7769]], device='mps:0')\n",
      "Iteration 60360 Training loss 0.0932089239358902 Validation loss 0.10002246499061584 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6348],\n",
      "        [0.6286]], device='mps:0')\n",
      "Iteration 60370 Training loss 0.09238050132989883 Validation loss 0.10005610436201096 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.1514],\n",
      "        [0.4762]], device='mps:0')\n",
      "Iteration 60380 Training loss 0.10185616463422775 Validation loss 0.10000939667224884 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7293],\n",
      "        [0.2920]], device='mps:0')\n",
      "Iteration 60390 Training loss 0.09064852446317673 Validation loss 0.10001225769519806 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1607],\n",
      "        [0.5845]], device='mps:0')\n",
      "Iteration 60400 Training loss 0.09025299549102783 Validation loss 0.10001715272665024 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4001],\n",
      "        [0.6463]], device='mps:0')\n",
      "Iteration 60410 Training loss 0.10449879616498947 Validation loss 0.100027896463871 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6430],\n",
      "        [0.4464]], device='mps:0')\n",
      "Iteration 60420 Training loss 0.11134142428636551 Validation loss 0.10001865029335022 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.5098],\n",
      "        [0.5957]], device='mps:0')\n",
      "Iteration 60430 Training loss 0.09963185340166092 Validation loss 0.10003861784934998 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2640],\n",
      "        [0.5989]], device='mps:0')\n",
      "Iteration 60440 Training loss 0.08864806592464447 Validation loss 0.10007614642381668 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4652],\n",
      "        [0.4965]], device='mps:0')\n",
      "Iteration 60450 Training loss 0.10124074667692184 Validation loss 0.100087009370327 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3044],\n",
      "        [0.3293]], device='mps:0')\n",
      "Iteration 60460 Training loss 0.10131101310253143 Validation loss 0.100206159055233 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5831],\n",
      "        [0.0925]], device='mps:0')\n",
      "Iteration 60470 Training loss 0.09794685244560242 Validation loss 0.10012401640415192 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5803],\n",
      "        [0.5490]], device='mps:0')\n",
      "Iteration 60480 Training loss 0.10424556583166122 Validation loss 0.1000414490699768 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5727],\n",
      "        [0.3345]], device='mps:0')\n",
      "Iteration 60490 Training loss 0.09916350990533829 Validation loss 0.100087471306324 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.8372],\n",
      "        [0.7027]], device='mps:0')\n",
      "Iteration 60500 Training loss 0.1155392974615097 Validation loss 0.10007216036319733 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5853],\n",
      "        [0.3731]], device='mps:0')\n",
      "Iteration 60510 Training loss 0.1086958646774292 Validation loss 0.10006462782621384 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6792],\n",
      "        [0.5943]], device='mps:0')\n",
      "Iteration 60520 Training loss 0.10887753218412399 Validation loss 0.1000242829322815 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.7026],\n",
      "        [0.2985]], device='mps:0')\n",
      "Iteration 60530 Training loss 0.10395320504903793 Validation loss 0.10000377893447876 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4148],\n",
      "        [0.8665]], device='mps:0')\n",
      "Iteration 60540 Training loss 0.1049160361289978 Validation loss 0.09999319911003113 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7451],\n",
      "        [0.5724]], device='mps:0')\n",
      "Iteration 60550 Training loss 0.10350868105888367 Validation loss 0.10000026971101761 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5254],\n",
      "        [0.3496]], device='mps:0')\n",
      "Iteration 60560 Training loss 0.11418718844652176 Validation loss 0.10000289231538773 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5738],\n",
      "        [0.5470]], device='mps:0')\n",
      "Iteration 60570 Training loss 0.1066669449210167 Validation loss 0.10000737756490707 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.0870],\n",
      "        [0.3612]], device='mps:0')\n",
      "Iteration 60580 Training loss 0.09227805584669113 Validation loss 0.10000467300415039 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3613],\n",
      "        [0.7009]], device='mps:0')\n",
      "Iteration 60590 Training loss 0.10411502420902252 Validation loss 0.10001789033412933 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5250],\n",
      "        [0.3197]], device='mps:0')\n",
      "Iteration 60600 Training loss 0.10484365373849869 Validation loss 0.0999864712357521 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4326],\n",
      "        [0.3402]], device='mps:0')\n",
      "Iteration 60610 Training loss 0.10513284802436829 Validation loss 0.09998362511396408 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6040],\n",
      "        [0.4325]], device='mps:0')\n",
      "Iteration 60620 Training loss 0.11154982447624207 Validation loss 0.09998569637537003 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6932],\n",
      "        [0.5344]], device='mps:0')\n",
      "Iteration 60630 Training loss 0.10303501039743423 Validation loss 0.09998948872089386 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4939],\n",
      "        [0.5551]], device='mps:0')\n",
      "Iteration 60640 Training loss 0.0935472622513771 Validation loss 0.10000229626893997 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.7985],\n",
      "        [0.6398]], device='mps:0')\n",
      "Iteration 60650 Training loss 0.09543357044458389 Validation loss 0.09999582171440125 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4124],\n",
      "        [0.5315]], device='mps:0')\n",
      "Iteration 60660 Training loss 0.09566076099872589 Validation loss 0.09998149424791336 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4958],\n",
      "        [0.4632]], device='mps:0')\n",
      "Iteration 60670 Training loss 0.10838234424591064 Validation loss 0.09998053312301636 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5625],\n",
      "        [0.5873]], device='mps:0')\n",
      "Iteration 60680 Training loss 0.11397504806518555 Validation loss 0.09997868537902832 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3501],\n",
      "        [0.4123]], device='mps:0')\n",
      "Iteration 60690 Training loss 0.10385940968990326 Validation loss 0.10000230371952057 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.4753],\n",
      "        [0.2027]], device='mps:0')\n",
      "Iteration 60700 Training loss 0.10347486287355423 Validation loss 0.100035160779953 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3491],\n",
      "        [0.4315]], device='mps:0')\n",
      "Iteration 60710 Training loss 0.10232710838317871 Validation loss 0.10007835924625397 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6530],\n",
      "        [0.7258]], device='mps:0')\n",
      "Iteration 60720 Training loss 0.11064884066581726 Validation loss 0.10004552453756332 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6833],\n",
      "        [0.6079]], device='mps:0')\n",
      "Iteration 60730 Training loss 0.09054466336965561 Validation loss 0.10001374036073685 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6041],\n",
      "        [0.4541]], device='mps:0')\n",
      "Iteration 60740 Training loss 0.1037818044424057 Validation loss 0.10005291551351547 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.4294],\n",
      "        [0.5303]], device='mps:0')\n",
      "Iteration 60750 Training loss 0.10507693886756897 Validation loss 0.10003230720758438 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5287],\n",
      "        [0.3151]], device='mps:0')\n",
      "Iteration 60760 Training loss 0.09748052060604095 Validation loss 0.10001212358474731 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5711],\n",
      "        [0.6111]], device='mps:0')\n",
      "Iteration 60770 Training loss 0.09638672322034836 Validation loss 0.10000190138816833 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3366],\n",
      "        [0.3714]], device='mps:0')\n",
      "Iteration 60780 Training loss 0.09200406819581985 Validation loss 0.10000856220722198 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3925],\n",
      "        [0.6569]], device='mps:0')\n",
      "Iteration 60790 Training loss 0.10987770557403564 Validation loss 0.09999916702508926 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2352],\n",
      "        [0.4855]], device='mps:0')\n",
      "Iteration 60800 Training loss 0.10105911642313004 Validation loss 0.10011021047830582 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5054],\n",
      "        [0.5110]], device='mps:0')\n",
      "Iteration 60810 Training loss 0.10429653525352478 Validation loss 0.10009829699993134 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.4448],\n",
      "        [0.3076]], device='mps:0')\n",
      "Iteration 60820 Training loss 0.09454973042011261 Validation loss 0.10005511343479156 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4334],\n",
      "        [0.6026]], device='mps:0')\n",
      "Iteration 60830 Training loss 0.09161214530467987 Validation loss 0.10006396472454071 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4403],\n",
      "        [0.5940]], device='mps:0')\n",
      "Iteration 60840 Training loss 0.09317058324813843 Validation loss 0.1000797376036644 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6468],\n",
      "        [0.7199]], device='mps:0')\n",
      "Iteration 60850 Training loss 0.10970774292945862 Validation loss 0.10009773820638657 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5090],\n",
      "        [0.6057]], device='mps:0')\n",
      "Iteration 60860 Training loss 0.1026478111743927 Validation loss 0.10010625422000885 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5247],\n",
      "        [0.8045]], device='mps:0')\n",
      "Iteration 60870 Training loss 0.11457988619804382 Validation loss 0.10009700804948807 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6780],\n",
      "        [0.8248]], device='mps:0')\n",
      "Iteration 60880 Training loss 0.10008713603019714 Validation loss 0.10005415976047516 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.2148],\n",
      "        [0.6962]], device='mps:0')\n",
      "Iteration 60890 Training loss 0.09831901639699936 Validation loss 0.10000952333211899 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6631],\n",
      "        [0.3764]], device='mps:0')\n",
      "Iteration 60900 Training loss 0.10692238807678223 Validation loss 0.1000138521194458 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6980],\n",
      "        [0.3436]], device='mps:0')\n",
      "Iteration 60910 Training loss 0.10775692015886307 Validation loss 0.09999318420886993 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2815],\n",
      "        [0.5378]], device='mps:0')\n",
      "Iteration 60920 Training loss 0.09686906635761261 Validation loss 0.09999372065067291 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4543],\n",
      "        [0.0557]], device='mps:0')\n",
      "Iteration 60930 Training loss 0.1085350513458252 Validation loss 0.09999646246433258 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.1196],\n",
      "        [0.5428]], device='mps:0')\n",
      "Iteration 60940 Training loss 0.08390314877033234 Validation loss 0.09998133778572083 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5408],\n",
      "        [0.5017]], device='mps:0')\n",
      "Iteration 60950 Training loss 0.10219226777553558 Validation loss 0.09997820109128952 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6168],\n",
      "        [0.5900]], device='mps:0')\n",
      "Iteration 60960 Training loss 0.10121025890111923 Validation loss 0.09998137503862381 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6412],\n",
      "        [0.6052]], device='mps:0')\n",
      "Iteration 60970 Training loss 0.0995701402425766 Validation loss 0.09999019652605057 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7035],\n",
      "        [0.6498]], device='mps:0')\n",
      "Iteration 60980 Training loss 0.10265109688043594 Validation loss 0.09997996687889099 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.1402],\n",
      "        [0.2705]], device='mps:0')\n",
      "Iteration 60990 Training loss 0.10962314158678055 Validation loss 0.0999874398112297 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2788],\n",
      "        [0.4346]], device='mps:0')\n",
      "Iteration 61000 Training loss 0.10202725976705551 Validation loss 0.09998677670955658 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4526],\n",
      "        [0.6498]], device='mps:0')\n",
      "Iteration 61010 Training loss 0.09713535010814667 Validation loss 0.09999286383390427 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3679],\n",
      "        [0.3468]], device='mps:0')\n",
      "Iteration 61020 Training loss 0.11086229234933853 Validation loss 0.09998020529747009 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4080],\n",
      "        [0.5885]], device='mps:0')\n",
      "Iteration 61030 Training loss 0.10567624866962433 Validation loss 0.09997271001338959 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3435],\n",
      "        [0.7071]], device='mps:0')\n",
      "Iteration 61040 Training loss 0.11367753893136978 Validation loss 0.10000436753034592 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6496],\n",
      "        [0.5392]], device='mps:0')\n",
      "Iteration 61050 Training loss 0.09736733138561249 Validation loss 0.10006014257669449 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3535],\n",
      "        [0.6632]], device='mps:0')\n",
      "Iteration 61060 Training loss 0.09917249530553818 Validation loss 0.1000438705086708 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3211],\n",
      "        [0.6776]], device='mps:0')\n",
      "Iteration 61070 Training loss 0.0926242396235466 Validation loss 0.10000064224004745 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.7543],\n",
      "        [0.6333]], device='mps:0')\n",
      "Iteration 61080 Training loss 0.09185191988945007 Validation loss 0.09999554604291916 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5716],\n",
      "        [0.5290]], device='mps:0')\n",
      "Iteration 61090 Training loss 0.1064923107624054 Validation loss 0.09997590631246567 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5375],\n",
      "        [0.4879]], device='mps:0')\n",
      "Iteration 61100 Training loss 0.10767583549022675 Validation loss 0.09997259080410004 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4731],\n",
      "        [0.3554]], device='mps:0')\n",
      "Iteration 61110 Training loss 0.11100804805755615 Validation loss 0.10000171512365341 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4724],\n",
      "        [0.2066]], device='mps:0')\n",
      "Iteration 61120 Training loss 0.09891006350517273 Validation loss 0.09999340027570724 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.5310],\n",
      "        [0.3754]], device='mps:0')\n",
      "Iteration 61130 Training loss 0.08954504132270813 Validation loss 0.0999767854809761 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4194],\n",
      "        [0.4854]], device='mps:0')\n",
      "Iteration 61140 Training loss 0.10753084719181061 Validation loss 0.09996863454580307 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6629],\n",
      "        [0.1646]], device='mps:0')\n",
      "Iteration 61150 Training loss 0.08393089473247528 Validation loss 0.09998073428869247 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3829],\n",
      "        [0.3299]], device='mps:0')\n",
      "Iteration 61160 Training loss 0.10756643861532211 Validation loss 0.10001088678836823 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4925],\n",
      "        [0.6170]], device='mps:0')\n",
      "Iteration 61170 Training loss 0.1043134406208992 Validation loss 0.10002008825540543 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7171],\n",
      "        [0.3724]], device='mps:0')\n",
      "Iteration 61180 Training loss 0.1103576049208641 Validation loss 0.10006355494260788 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4130],\n",
      "        [0.3015]], device='mps:0')\n",
      "Iteration 61190 Training loss 0.10725978761911392 Validation loss 0.10003098100423813 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6680],\n",
      "        [0.4753]], device='mps:0')\n",
      "Iteration 61200 Training loss 0.11170735955238342 Validation loss 0.09999481588602066 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3935],\n",
      "        [0.3364]], device='mps:0')\n",
      "Iteration 61210 Training loss 0.09588927030563354 Validation loss 0.1000065803527832 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3172],\n",
      "        [0.3894]], device='mps:0')\n",
      "Iteration 61220 Training loss 0.10143928974866867 Validation loss 0.10000653564929962 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3692],\n",
      "        [0.5464]], device='mps:0')\n",
      "Iteration 61230 Training loss 0.10149877518415451 Validation loss 0.09997747093439102 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5741],\n",
      "        [0.6671]], device='mps:0')\n",
      "Iteration 61240 Training loss 0.11539170891046524 Validation loss 0.0999717265367508 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3715],\n",
      "        [0.4475]], device='mps:0')\n",
      "Iteration 61250 Training loss 0.10416913032531738 Validation loss 0.09996450692415237 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6659],\n",
      "        [0.5079]], device='mps:0')\n",
      "Iteration 61260 Training loss 0.09846337884664536 Validation loss 0.09996646642684937 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3604],\n",
      "        [0.4543]], device='mps:0')\n",
      "Iteration 61270 Training loss 0.09663794934749603 Validation loss 0.09995655715465546 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5276],\n",
      "        [0.7597]], device='mps:0')\n",
      "Iteration 61280 Training loss 0.09742043912410736 Validation loss 0.0999586209654808 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5815],\n",
      "        [0.5014]], device='mps:0')\n",
      "Iteration 61290 Training loss 0.09949992597103119 Validation loss 0.09995750337839127 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2269],\n",
      "        [0.6938]], device='mps:0')\n",
      "Iteration 61300 Training loss 0.09374547749757767 Validation loss 0.0999521017074585 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6606],\n",
      "        [0.6927]], device='mps:0')\n",
      "Iteration 61310 Training loss 0.10642451792955399 Validation loss 0.0999569445848465 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5655],\n",
      "        [0.4096]], device='mps:0')\n",
      "Iteration 61320 Training loss 0.10886306315660477 Validation loss 0.09996231645345688 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6252],\n",
      "        [0.5618]], device='mps:0')\n",
      "Iteration 61330 Training loss 0.10093418508768082 Validation loss 0.09996575117111206 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3778],\n",
      "        [0.1792]], device='mps:0')\n",
      "Iteration 61340 Training loss 0.09940481930971146 Validation loss 0.1000002846121788 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3560],\n",
      "        [0.0470]], device='mps:0')\n",
      "Iteration 61350 Training loss 0.11128824204206467 Validation loss 0.09999335557222366 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7341],\n",
      "        [0.6300]], device='mps:0')\n",
      "Iteration 61360 Training loss 0.09367605298757553 Validation loss 0.10002228617668152 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6287],\n",
      "        [0.6003]], device='mps:0')\n",
      "Iteration 61370 Training loss 0.10305996984243393 Validation loss 0.09999367594718933 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4060],\n",
      "        [0.3848]], device='mps:0')\n",
      "Iteration 61380 Training loss 0.11205053329467773 Validation loss 0.09995555877685547 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4711],\n",
      "        [0.3413]], device='mps:0')\n",
      "Iteration 61390 Training loss 0.09660609811544418 Validation loss 0.09996234625577927 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4002],\n",
      "        [0.4948]], device='mps:0')\n",
      "Iteration 61400 Training loss 0.10837793350219727 Validation loss 0.09994980692863464 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6061],\n",
      "        [0.6169]], device='mps:0')\n",
      "Iteration 61410 Training loss 0.09771687537431717 Validation loss 0.09994631260633469 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7732],\n",
      "        [0.7018]], device='mps:0')\n",
      "Iteration 61420 Training loss 0.09947612881660461 Validation loss 0.09996265172958374 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3832],\n",
      "        [0.3624]], device='mps:0')\n",
      "Iteration 61430 Training loss 0.10964027047157288 Validation loss 0.09998340904712677 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.4925],\n",
      "        [0.3503]], device='mps:0')\n",
      "Iteration 61440 Training loss 0.10342463850975037 Validation loss 0.09999352693557739 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6687],\n",
      "        [0.6343]], device='mps:0')\n",
      "Iteration 61450 Training loss 0.0980096384882927 Validation loss 0.0999893844127655 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3401],\n",
      "        [0.6953]], device='mps:0')\n",
      "Iteration 61460 Training loss 0.1028059720993042 Validation loss 0.09998643398284912 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.1212],\n",
      "        [0.4772]], device='mps:0')\n",
      "Iteration 61470 Training loss 0.10140419006347656 Validation loss 0.09998100250959396 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6042],\n",
      "        [0.4275]], device='mps:0')\n",
      "Iteration 61480 Training loss 0.09937919676303864 Validation loss 0.09997058659791946 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5797],\n",
      "        [0.5114]], device='mps:0')\n",
      "Iteration 61490 Training loss 0.092267245054245 Validation loss 0.09998752176761627 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2804],\n",
      "        [0.1432]], device='mps:0')\n",
      "Iteration 61500 Training loss 0.10577788203954697 Validation loss 0.09999571740627289 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5843],\n",
      "        [0.2253]], device='mps:0')\n",
      "Iteration 61510 Training loss 0.0996992439031601 Validation loss 0.09995716065168381 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1485],\n",
      "        [0.6072]], device='mps:0')\n",
      "Iteration 61520 Training loss 0.09815087169408798 Validation loss 0.09995697438716888 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3992],\n",
      "        [0.5707]], device='mps:0')\n",
      "Iteration 61530 Training loss 0.09182567149400711 Validation loss 0.0999465063214302 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2115],\n",
      "        [0.7196]], device='mps:0')\n",
      "Iteration 61540 Training loss 0.09724124521017075 Validation loss 0.09994981437921524 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5732],\n",
      "        [0.5955]], device='mps:0')\n",
      "Iteration 61550 Training loss 0.10254369676113129 Validation loss 0.09994412958621979 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4429],\n",
      "        [0.1758]], device='mps:0')\n",
      "Iteration 61560 Training loss 0.10819292068481445 Validation loss 0.0999583750963211 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6703],\n",
      "        [0.3503]], device='mps:0')\n",
      "Iteration 61570 Training loss 0.09374156594276428 Validation loss 0.09994509071111679 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5776],\n",
      "        [0.3073]], device='mps:0')\n",
      "Iteration 61580 Training loss 0.10981439054012299 Validation loss 0.09994251281023026 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3655],\n",
      "        [0.6361]], device='mps:0')\n",
      "Iteration 61590 Training loss 0.09292297810316086 Validation loss 0.09994922578334808 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7387],\n",
      "        [0.5161]], device='mps:0')\n",
      "Iteration 61600 Training loss 0.09969023615121841 Validation loss 0.09996210783720016 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4000],\n",
      "        [0.4460]], device='mps:0')\n",
      "Iteration 61610 Training loss 0.11087959259748459 Validation loss 0.09997165948152542 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7214],\n",
      "        [0.4503]], device='mps:0')\n",
      "Iteration 61620 Training loss 0.10032500326633453 Validation loss 0.09996756166219711 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6416],\n",
      "        [0.6199]], device='mps:0')\n",
      "Iteration 61630 Training loss 0.09791015088558197 Validation loss 0.09994020313024521 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2118],\n",
      "        [0.6736]], device='mps:0')\n",
      "Iteration 61640 Training loss 0.09905004501342773 Validation loss 0.09993337094783783 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6033],\n",
      "        [0.5210]], device='mps:0')\n",
      "Iteration 61650 Training loss 0.09885076433420181 Validation loss 0.09993095695972443 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6639],\n",
      "        [0.1742]], device='mps:0')\n",
      "Iteration 61660 Training loss 0.10459217429161072 Validation loss 0.09993243962526321 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3579],\n",
      "        [0.4829]], device='mps:0')\n",
      "Iteration 61670 Training loss 0.10545403510332108 Validation loss 0.09995294362306595 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6940],\n",
      "        [0.5517]], device='mps:0')\n",
      "Iteration 61680 Training loss 0.10218916833400726 Validation loss 0.09995952248573303 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5409],\n",
      "        [0.6405]], device='mps:0')\n",
      "Iteration 61690 Training loss 0.10307589918375015 Validation loss 0.09994760900735855 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5432],\n",
      "        [0.2499]], device='mps:0')\n",
      "Iteration 61700 Training loss 0.09573369473218918 Validation loss 0.0999365895986557 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5419],\n",
      "        [0.3894]], device='mps:0')\n",
      "Iteration 61710 Training loss 0.11271624267101288 Validation loss 0.09993042796850204 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4588],\n",
      "        [0.5034]], device='mps:0')\n",
      "Iteration 61720 Training loss 0.10923048108816147 Validation loss 0.09994522482156754 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7537],\n",
      "        [0.5765]], device='mps:0')\n",
      "Iteration 61730 Training loss 0.09971395134925842 Validation loss 0.09993458539247513 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5036],\n",
      "        [0.6481]], device='mps:0')\n",
      "Iteration 61740 Training loss 0.10366209596395493 Validation loss 0.09994016587734222 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5388],\n",
      "        [0.4096]], device='mps:0')\n",
      "Iteration 61750 Training loss 0.10465080291032791 Validation loss 0.09994328767061234 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5244],\n",
      "        [0.2925]], device='mps:0')\n",
      "Iteration 61760 Training loss 0.09919313341379166 Validation loss 0.09995292872190475 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.3443],\n",
      "        [0.2222]], device='mps:0')\n",
      "Iteration 61770 Training loss 0.12167324125766754 Validation loss 0.09993741661310196 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4736],\n",
      "        [0.1315]], device='mps:0')\n",
      "Iteration 61780 Training loss 0.09938213229179382 Validation loss 0.09992662817239761 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5515],\n",
      "        [0.4923]], device='mps:0')\n",
      "Iteration 61790 Training loss 0.11682270467281342 Validation loss 0.09992491453886032 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5952],\n",
      "        [0.6963]], device='mps:0')\n",
      "Iteration 61800 Training loss 0.10213761776685715 Validation loss 0.09993113577365875 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4954],\n",
      "        [0.3626]], device='mps:0')\n",
      "Iteration 61810 Training loss 0.1036764532327652 Validation loss 0.09995179623365402 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5260],\n",
      "        [0.1333]], device='mps:0')\n",
      "Iteration 61820 Training loss 0.10026427358388901 Validation loss 0.0999445840716362 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1327],\n",
      "        [0.4895]], device='mps:0')\n",
      "Iteration 61830 Training loss 0.10229559987783432 Validation loss 0.09993159025907516 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2158],\n",
      "        [0.5241]], device='mps:0')\n",
      "Iteration 61840 Training loss 0.09528743475675583 Validation loss 0.09993130713701248 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4125],\n",
      "        [0.2510]], device='mps:0')\n",
      "Iteration 61850 Training loss 0.09560752660036087 Validation loss 0.0999247208237648 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4316],\n",
      "        [0.5278]], device='mps:0')\n",
      "Iteration 61860 Training loss 0.09415393322706223 Validation loss 0.09992940723896027 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.7577],\n",
      "        [0.4004]], device='mps:0')\n",
      "Iteration 61870 Training loss 0.0943509116768837 Validation loss 0.09992900490760803 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3784],\n",
      "        [0.6694]], device='mps:0')\n",
      "Iteration 61880 Training loss 0.10186946392059326 Validation loss 0.09992647916078568 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3571],\n",
      "        [0.5344]], device='mps:0')\n",
      "Iteration 61890 Training loss 0.10948161035776138 Validation loss 0.09992170333862305 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.1353],\n",
      "        [0.6750]], device='mps:0')\n",
      "Iteration 61900 Training loss 0.10663844645023346 Validation loss 0.09992507100105286 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5076],\n",
      "        [0.4216]], device='mps:0')\n",
      "Iteration 61910 Training loss 0.09706485271453857 Validation loss 0.09993027150630951 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3539],\n",
      "        [0.3933]], device='mps:0')\n",
      "Iteration 61920 Training loss 0.10588500648736954 Validation loss 0.10001228749752045 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5506],\n",
      "        [0.6675]], device='mps:0')\n",
      "Iteration 61930 Training loss 0.08074723929166794 Validation loss 0.09997017681598663 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3781],\n",
      "        [0.5093]], device='mps:0')\n",
      "Iteration 61940 Training loss 0.10317322611808777 Validation loss 0.09997633844614029 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3956],\n",
      "        [0.1003]], device='mps:0')\n",
      "Iteration 61950 Training loss 0.09987716376781464 Validation loss 0.09995564073324203 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3166],\n",
      "        [0.4624]], device='mps:0')\n",
      "Iteration 61960 Training loss 0.10909561067819595 Validation loss 0.0999225601553917 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7018],\n",
      "        [0.4271]], device='mps:0')\n",
      "Iteration 61970 Training loss 0.09623008221387863 Validation loss 0.09991227835416794 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5879],\n",
      "        [0.6344]], device='mps:0')\n",
      "Iteration 61980 Training loss 0.0990784615278244 Validation loss 0.09991095215082169 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5633],\n",
      "        [0.1882]], device='mps:0')\n",
      "Iteration 61990 Training loss 0.11595939099788666 Validation loss 0.09991589933633804 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4454],\n",
      "        [0.3703]], device='mps:0')\n",
      "Iteration 62000 Training loss 0.10870472341775894 Validation loss 0.09991712123155594 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3726],\n",
      "        [0.6092]], device='mps:0')\n",
      "Iteration 62010 Training loss 0.10800065845251083 Validation loss 0.09991266578435898 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5999],\n",
      "        [0.3733]], device='mps:0')\n",
      "Iteration 62020 Training loss 0.09629768878221512 Validation loss 0.09991598129272461 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6663],\n",
      "        [0.4169]], device='mps:0')\n",
      "Iteration 62030 Training loss 0.11044728755950928 Validation loss 0.09991182386875153 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4572],\n",
      "        [0.1750]], device='mps:0')\n",
      "Iteration 62040 Training loss 0.09747689962387085 Validation loss 0.09991524368524551 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4629],\n",
      "        [0.4674]], device='mps:0')\n",
      "Iteration 62050 Training loss 0.104827880859375 Validation loss 0.09991644322872162 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5028],\n",
      "        [0.3859]], device='mps:0')\n",
      "Iteration 62060 Training loss 0.09840326011180878 Validation loss 0.09991629421710968 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7251],\n",
      "        [0.4483]], device='mps:0')\n",
      "Iteration 62070 Training loss 0.08555572479963303 Validation loss 0.09991011768579483 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6193],\n",
      "        [0.4970]], device='mps:0')\n",
      "Iteration 62080 Training loss 0.09792079776525497 Validation loss 0.09990818798542023 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6580],\n",
      "        [0.4437]], device='mps:0')\n",
      "Iteration 62090 Training loss 0.1086941808462143 Validation loss 0.09991848468780518 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3783],\n",
      "        [0.3919]], device='mps:0')\n",
      "Iteration 62100 Training loss 0.10348827391862869 Validation loss 0.09991035610437393 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6648],\n",
      "        [0.5192]], device='mps:0')\n",
      "Iteration 62110 Training loss 0.1068042740225792 Validation loss 0.09992045909166336 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6132],\n",
      "        [0.5304]], device='mps:0')\n",
      "Iteration 62120 Training loss 0.10327302664518356 Validation loss 0.09990949183702469 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6695],\n",
      "        [0.3275]], device='mps:0')\n",
      "Iteration 62130 Training loss 0.10614261031150818 Validation loss 0.09990819543600082 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3756],\n",
      "        [0.6634]], device='mps:0')\n",
      "Iteration 62140 Training loss 0.10514712333679199 Validation loss 0.09990143030881882 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5107],\n",
      "        [0.2999]], device='mps:0')\n",
      "Iteration 62150 Training loss 0.1090751439332962 Validation loss 0.09990695863962173 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.5914],\n",
      "        [0.5003]], device='mps:0')\n",
      "Iteration 62160 Training loss 0.08985833823680878 Validation loss 0.09990455955266953 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5462],\n",
      "        [0.6268]], device='mps:0')\n",
      "Iteration 62170 Training loss 0.10284091532230377 Validation loss 0.09990248829126358 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6193],\n",
      "        [0.1300]], device='mps:0')\n",
      "Iteration 62180 Training loss 0.10541422665119171 Validation loss 0.09990012645721436 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3579],\n",
      "        [0.6369]], device='mps:0')\n",
      "Iteration 62190 Training loss 0.0962022915482521 Validation loss 0.09989918768405914 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1362],\n",
      "        [0.1263]], device='mps:0')\n",
      "Iteration 62200 Training loss 0.111304871737957 Validation loss 0.09989777207374573 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3459],\n",
      "        [0.4960]], device='mps:0')\n",
      "Iteration 62210 Training loss 0.10758166760206223 Validation loss 0.09991400688886642 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4715],\n",
      "        [0.5352]], device='mps:0')\n",
      "Iteration 62220 Training loss 0.09110438823699951 Validation loss 0.0999099612236023 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6570],\n",
      "        [0.5664]], device='mps:0')\n",
      "Iteration 62230 Training loss 0.10575500130653381 Validation loss 0.09990604221820831 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.7634],\n",
      "        [0.4411]], device='mps:0')\n",
      "Iteration 62240 Training loss 0.10689565539360046 Validation loss 0.09991398453712463 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2184],\n",
      "        [0.6010]], device='mps:0')\n",
      "Iteration 62250 Training loss 0.10187306255102158 Validation loss 0.09991665184497833 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6023],\n",
      "        [0.3500]], device='mps:0')\n",
      "Iteration 62260 Training loss 0.0946827307343483 Validation loss 0.10000679641962051 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6206],\n",
      "        [0.6509]], device='mps:0')\n",
      "Iteration 62270 Training loss 0.10300794243812561 Validation loss 0.10001059621572495 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3829],\n",
      "        [0.5343]], device='mps:0')\n",
      "Iteration 62280 Training loss 0.09644767642021179 Validation loss 0.09996868669986725 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4886],\n",
      "        [0.7004]], device='mps:0')\n",
      "Iteration 62290 Training loss 0.09380235522985458 Validation loss 0.09992536157369614 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5088],\n",
      "        [0.4329]], device='mps:0')\n",
      "Iteration 62300 Training loss 0.10423793643712997 Validation loss 0.09992427378892899 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7024],\n",
      "        [0.5712]], device='mps:0')\n",
      "Iteration 62310 Training loss 0.1015767827630043 Validation loss 0.09991508722305298 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6667],\n",
      "        [0.3288]], device='mps:0')\n",
      "Iteration 62320 Training loss 0.09773847460746765 Validation loss 0.09993111342191696 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5637],\n",
      "        [0.1608]], device='mps:0')\n",
      "Iteration 62330 Training loss 0.1054171547293663 Validation loss 0.09994226694107056 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4493],\n",
      "        [0.4210]], device='mps:0')\n",
      "Iteration 62340 Training loss 0.11314516514539719 Validation loss 0.0999305471777916 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3692],\n",
      "        [0.5830]], device='mps:0')\n",
      "Iteration 62350 Training loss 0.1090916246175766 Validation loss 0.09991098195314407 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5458],\n",
      "        [0.6580]], device='mps:0')\n",
      "Iteration 62360 Training loss 0.10770544409751892 Validation loss 0.09990889579057693 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5363],\n",
      "        [0.5886]], device='mps:0')\n",
      "Iteration 62370 Training loss 0.10309550911188126 Validation loss 0.09988527745008469 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2257],\n",
      "        [0.4953]], device='mps:0')\n",
      "Iteration 62380 Training loss 0.10080621391534805 Validation loss 0.09990223497152328 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6702],\n",
      "        [0.4626]], device='mps:0')\n",
      "Iteration 62390 Training loss 0.10256530344486237 Validation loss 0.09989380836486816 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.0322],\n",
      "        [0.5190]], device='mps:0')\n",
      "Iteration 62400 Training loss 0.10799939930438995 Validation loss 0.09989407658576965 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5658],\n",
      "        [0.5535]], device='mps:0')\n",
      "Iteration 62410 Training loss 0.11048515141010284 Validation loss 0.09988614916801453 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5600],\n",
      "        [0.6824]], device='mps:0')\n",
      "Iteration 62420 Training loss 0.10108903795480728 Validation loss 0.09990787506103516 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3302],\n",
      "        [0.4476]], device='mps:0')\n",
      "Iteration 62430 Training loss 0.09169875085353851 Validation loss 0.09992246329784393 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5936],\n",
      "        [0.5293]], device='mps:0')\n",
      "Iteration 62440 Training loss 0.09930042177438736 Validation loss 0.09989810734987259 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5416],\n",
      "        [0.5138]], device='mps:0')\n",
      "Iteration 62450 Training loss 0.09710299968719482 Validation loss 0.09987759590148926 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6846],\n",
      "        [0.7045]], device='mps:0')\n",
      "Iteration 62460 Training loss 0.09748262166976929 Validation loss 0.0998905822634697 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3275],\n",
      "        [0.3760]], device='mps:0')\n",
      "Iteration 62470 Training loss 0.0974043533205986 Validation loss 0.09987197816371918 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5036],\n",
      "        [0.6357]], device='mps:0')\n",
      "Iteration 62480 Training loss 0.10164748132228851 Validation loss 0.09987074136734009 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6085],\n",
      "        [0.4330]], device='mps:0')\n",
      "Iteration 62490 Training loss 0.10125760734081268 Validation loss 0.09986923635005951 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6390],\n",
      "        [0.4973]], device='mps:0')\n",
      "Iteration 62500 Training loss 0.10357445478439331 Validation loss 0.09988759458065033 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3267],\n",
      "        [0.2086]], device='mps:0')\n",
      "Iteration 62510 Training loss 0.10571664571762085 Validation loss 0.09988490492105484 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4377],\n",
      "        [0.3683]], device='mps:0')\n",
      "Iteration 62520 Training loss 0.09754236042499542 Validation loss 0.09986493736505508 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3376],\n",
      "        [0.8021]], device='mps:0')\n",
      "Iteration 62530 Training loss 0.10771448910236359 Validation loss 0.09986430406570435 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5607],\n",
      "        [0.3449]], device='mps:0')\n",
      "Iteration 62540 Training loss 0.10284212976694107 Validation loss 0.09986454993486404 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5070],\n",
      "        [0.6346]], device='mps:0')\n",
      "Iteration 62550 Training loss 0.10703156888484955 Validation loss 0.0998712107539177 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.6358],\n",
      "        [0.6433]], device='mps:0')\n",
      "Iteration 62560 Training loss 0.10905829817056656 Validation loss 0.09986399859189987 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7418],\n",
      "        [0.6364]], device='mps:0')\n",
      "Iteration 62570 Training loss 0.12032239884138107 Validation loss 0.09986740350723267 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3692],\n",
      "        [0.2482]], device='mps:0')\n",
      "Iteration 62580 Training loss 0.10061604529619217 Validation loss 0.09985999763011932 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7641],\n",
      "        [0.5516]], device='mps:0')\n",
      "Iteration 62590 Training loss 0.09279501438140869 Validation loss 0.09985700249671936 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4973],\n",
      "        [0.4110]], device='mps:0')\n",
      "Iteration 62600 Training loss 0.10889171063899994 Validation loss 0.09986095130443573 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.1827],\n",
      "        [0.3690]], device='mps:0')\n",
      "Iteration 62610 Training loss 0.10908178240060806 Validation loss 0.09986001998186111 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7402],\n",
      "        [0.0938]], device='mps:0')\n",
      "Iteration 62620 Training loss 0.1086033433675766 Validation loss 0.0998571440577507 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7934],\n",
      "        [0.1303]], device='mps:0')\n",
      "Iteration 62630 Training loss 0.10618104040622711 Validation loss 0.09986089169979095 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6471],\n",
      "        [0.4664]], device='mps:0')\n",
      "Iteration 62640 Training loss 0.1024094671010971 Validation loss 0.09985317289829254 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5738],\n",
      "        [0.7272]], device='mps:0')\n",
      "Iteration 62650 Training loss 0.09748876094818115 Validation loss 0.09985394775867462 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4724],\n",
      "        [0.6363]], device='mps:0')\n",
      "Iteration 62660 Training loss 0.09975332766771317 Validation loss 0.09985017031431198 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7167],\n",
      "        [0.5927]], device='mps:0')\n",
      "Iteration 62670 Training loss 0.11026345193386078 Validation loss 0.09985556453466415 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6494],\n",
      "        [0.3457]], device='mps:0')\n",
      "Iteration 62680 Training loss 0.10454189032316208 Validation loss 0.09985250979661942 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2225],\n",
      "        [0.3869]], device='mps:0')\n",
      "Iteration 62690 Training loss 0.10191261023283005 Validation loss 0.09984815120697021 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5425],\n",
      "        [0.7141]], device='mps:0')\n",
      "Iteration 62700 Training loss 0.10324428975582123 Validation loss 0.09984778612852097 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1334],\n",
      "        [0.4344]], device='mps:0')\n",
      "Iteration 62710 Training loss 0.10736305266618729 Validation loss 0.09985160082578659 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6483],\n",
      "        [0.4954]], device='mps:0')\n",
      "Iteration 62720 Training loss 0.10324157774448395 Validation loss 0.09987078607082367 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4884],\n",
      "        [0.2393]], device='mps:0')\n",
      "Iteration 62730 Training loss 0.1032637506723404 Validation loss 0.09985190629959106 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6319],\n",
      "        [0.4568]], device='mps:0')\n",
      "Iteration 62740 Training loss 0.1090383380651474 Validation loss 0.09984318166971207 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3888],\n",
      "        [0.6959]], device='mps:0')\n",
      "Iteration 62750 Training loss 0.10678442567586899 Validation loss 0.09984613209962845 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4102],\n",
      "        [0.7054]], device='mps:0')\n",
      "Iteration 62760 Training loss 0.11322420090436935 Validation loss 0.0998421311378479 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1931],\n",
      "        [0.4363]], device='mps:0')\n",
      "Iteration 62770 Training loss 0.09531749784946442 Validation loss 0.09983941912651062 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6040],\n",
      "        [0.4260]], device='mps:0')\n",
      "Iteration 62780 Training loss 0.10946343839168549 Validation loss 0.09983687847852707 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7027],\n",
      "        [0.5608]], device='mps:0')\n",
      "Iteration 62790 Training loss 0.0911954864859581 Validation loss 0.09986844658851624 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5613],\n",
      "        [0.3021]], device='mps:0')\n",
      "Iteration 62800 Training loss 0.09034517407417297 Validation loss 0.09987712651491165 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3351],\n",
      "        [0.5856]], device='mps:0')\n",
      "Iteration 62810 Training loss 0.10643980652093887 Validation loss 0.09989413619041443 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2236],\n",
      "        [0.2661]], device='mps:0')\n",
      "Iteration 62820 Training loss 0.11960181593894958 Validation loss 0.09987308084964752 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3676],\n",
      "        [0.3056]], device='mps:0')\n",
      "Iteration 62830 Training loss 0.09406927227973938 Validation loss 0.09986890107393265 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3518],\n",
      "        [0.5309]], device='mps:0')\n",
      "Iteration 62840 Training loss 0.09569307416677475 Validation loss 0.09987173229455948 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4838],\n",
      "        [0.4672]], device='mps:0')\n",
      "Iteration 62850 Training loss 0.10170687735080719 Validation loss 0.09987542033195496 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7306],\n",
      "        [0.3243]], device='mps:0')\n",
      "Iteration 62860 Training loss 0.10347162187099457 Validation loss 0.09991984814405441 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7141],\n",
      "        [0.1239]], device='mps:0')\n",
      "Iteration 62870 Training loss 0.10757824033498764 Validation loss 0.09988722205162048 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7926],\n",
      "        [0.6310]], device='mps:0')\n",
      "Iteration 62880 Training loss 0.08986040949821472 Validation loss 0.09984707832336426 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5373],\n",
      "        [0.6601]], device='mps:0')\n",
      "Iteration 62890 Training loss 0.10473326593637466 Validation loss 0.09983915835618973 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6349],\n",
      "        [0.6720]], device='mps:0')\n",
      "Iteration 62900 Training loss 0.09707118570804596 Validation loss 0.09983518719673157 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4193],\n",
      "        [0.6852]], device='mps:0')\n",
      "Iteration 62910 Training loss 0.10402555018663406 Validation loss 0.09985823184251785 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5490],\n",
      "        [0.6858]], device='mps:0')\n",
      "Iteration 62920 Training loss 0.09395088255405426 Validation loss 0.09989325702190399 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6266],\n",
      "        [0.6538]], device='mps:0')\n",
      "Iteration 62930 Training loss 0.09445343166589737 Validation loss 0.09988043457269669 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3319],\n",
      "        [0.2679]], device='mps:0')\n",
      "Iteration 62940 Training loss 0.09124427288770676 Validation loss 0.09990821033716202 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6345],\n",
      "        [0.1650]], device='mps:0')\n",
      "Iteration 62950 Training loss 0.1026715338230133 Validation loss 0.09991427510976791 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3250],\n",
      "        [0.8242]], device='mps:0')\n",
      "Iteration 62960 Training loss 0.10786756128072739 Validation loss 0.0998760387301445 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6534],\n",
      "        [0.1359]], device='mps:0')\n",
      "Iteration 62970 Training loss 0.11252698302268982 Validation loss 0.09983956068754196 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4202],\n",
      "        [0.2978]], device='mps:0')\n",
      "Iteration 62980 Training loss 0.10109048336744308 Validation loss 0.09982167929410934 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2687],\n",
      "        [0.4825]], device='mps:0')\n",
      "Iteration 62990 Training loss 0.09953156858682632 Validation loss 0.09982083737850189 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3932],\n",
      "        [0.5283]], device='mps:0')\n",
      "Iteration 63000 Training loss 0.10524299740791321 Validation loss 0.09982729703187943 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5083],\n",
      "        [0.5893]], device='mps:0')\n",
      "Iteration 63010 Training loss 0.10433392971754074 Validation loss 0.09981983155012131 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4699],\n",
      "        [0.3037]], device='mps:0')\n",
      "Iteration 63020 Training loss 0.1219741627573967 Validation loss 0.09982473403215408 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5761],\n",
      "        [0.4886]], device='mps:0')\n",
      "Iteration 63030 Training loss 0.11016979068517685 Validation loss 0.09981504082679749 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5159],\n",
      "        [0.3474]], device='mps:0')\n",
      "Iteration 63040 Training loss 0.10825741291046143 Validation loss 0.09981978684663773 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3827],\n",
      "        [0.2891]], device='mps:0')\n",
      "Iteration 63050 Training loss 0.09931707382202148 Validation loss 0.09981532394886017 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3128],\n",
      "        [0.6099]], device='mps:0')\n",
      "Iteration 63060 Training loss 0.08683790266513824 Validation loss 0.09982170909643173 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1965],\n",
      "        [0.4170]], device='mps:0')\n",
      "Iteration 63070 Training loss 0.09552410244941711 Validation loss 0.09981473535299301 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5234],\n",
      "        [0.5323]], device='mps:0')\n",
      "Iteration 63080 Training loss 0.10512836277484894 Validation loss 0.09982472658157349 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4093],\n",
      "        [0.3172]], device='mps:0')\n",
      "Iteration 63090 Training loss 0.10307902842760086 Validation loss 0.09981616586446762 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3831],\n",
      "        [0.6084]], device='mps:0')\n",
      "Iteration 63100 Training loss 0.09668932855129242 Validation loss 0.09981391578912735 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4980],\n",
      "        [0.6567]], device='mps:0')\n",
      "Iteration 63110 Training loss 0.10487517714500427 Validation loss 0.09981289505958557 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4541],\n",
      "        [0.6975]], device='mps:0')\n",
      "Iteration 63120 Training loss 0.11437743157148361 Validation loss 0.09983038902282715 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5107],\n",
      "        [0.6824]], device='mps:0')\n",
      "Iteration 63130 Training loss 0.09602504223585129 Validation loss 0.09982283413410187 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.4769],\n",
      "        [0.8381]], device='mps:0')\n",
      "Iteration 63140 Training loss 0.09895845502614975 Validation loss 0.09982118755578995 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.4486],\n",
      "        [0.3951]], device='mps:0')\n",
      "Iteration 63150 Training loss 0.101778045296669 Validation loss 0.09981248527765274 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5666],\n",
      "        [0.3282]], device='mps:0')\n",
      "Iteration 63160 Training loss 0.10180386900901794 Validation loss 0.09981289505958557 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7322],\n",
      "        [0.5346]], device='mps:0')\n",
      "Iteration 63170 Training loss 0.09951049834489822 Validation loss 0.09981146454811096 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4468],\n",
      "        [0.7339]], device='mps:0')\n",
      "Iteration 63180 Training loss 0.09661713242530823 Validation loss 0.09981169551610947 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2674],\n",
      "        [0.4832]], device='mps:0')\n",
      "Iteration 63190 Training loss 0.09799518436193466 Validation loss 0.09981746226549149 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5909],\n",
      "        [0.6722]], device='mps:0')\n",
      "Iteration 63200 Training loss 0.11016277968883514 Validation loss 0.09981974959373474 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3693],\n",
      "        [0.7393]], device='mps:0')\n",
      "Iteration 63210 Training loss 0.1106104627251625 Validation loss 0.09981633722782135 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4728],\n",
      "        [0.2940]], device='mps:0')\n",
      "Iteration 63220 Training loss 0.09905621409416199 Validation loss 0.09983021765947342 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5139],\n",
      "        [0.6396]], device='mps:0')\n",
      "Iteration 63230 Training loss 0.098563052713871 Validation loss 0.0998629778623581 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5806],\n",
      "        [0.6806]], device='mps:0')\n",
      "Iteration 63240 Training loss 0.09260416030883789 Validation loss 0.09983675926923752 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3778],\n",
      "        [0.4684]], device='mps:0')\n",
      "Iteration 63250 Training loss 0.09523636847734451 Validation loss 0.09984397143125534 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5790],\n",
      "        [0.1088]], device='mps:0')\n",
      "Iteration 63260 Training loss 0.10177729278802872 Validation loss 0.09981629252433777 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4235],\n",
      "        [0.5845]], device='mps:0')\n",
      "Iteration 63270 Training loss 0.0933709517121315 Validation loss 0.0998033881187439 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.0664],\n",
      "        [0.5339]], device='mps:0')\n",
      "Iteration 63280 Training loss 0.104871965944767 Validation loss 0.09980598837137222 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5193],\n",
      "        [0.5617]], device='mps:0')\n",
      "Iteration 63290 Training loss 0.10447384417057037 Validation loss 0.09980405122041702 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3166],\n",
      "        [0.5325]], device='mps:0')\n",
      "Iteration 63300 Training loss 0.09803534299135208 Validation loss 0.09980176389217377 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2017],\n",
      "        [0.5375]], device='mps:0')\n",
      "Iteration 63310 Training loss 0.08842846006155014 Validation loss 0.09980098158121109 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2367],\n",
      "        [0.1513]], device='mps:0')\n",
      "Iteration 63320 Training loss 0.10013631731271744 Validation loss 0.09982112795114517 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4741],\n",
      "        [0.4136]], device='mps:0')\n",
      "Iteration 63330 Training loss 0.10076843202114105 Validation loss 0.09980052709579468 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.2461],\n",
      "        [0.4560]], device='mps:0')\n",
      "Iteration 63340 Training loss 0.10831194370985031 Validation loss 0.0997973158955574 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6659],\n",
      "        [0.6459]], device='mps:0')\n",
      "Iteration 63350 Training loss 0.12038148939609528 Validation loss 0.0998031497001648 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3645],\n",
      "        [0.2178]], device='mps:0')\n",
      "Iteration 63360 Training loss 0.10119269788265228 Validation loss 0.0997987911105156 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5727],\n",
      "        [0.2466]], device='mps:0')\n",
      "Iteration 63370 Training loss 0.08476244658231735 Validation loss 0.09980160742998123 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5598],\n",
      "        [0.5527]], device='mps:0')\n",
      "Iteration 63380 Training loss 0.11428165435791016 Validation loss 0.09980110824108124 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4421],\n",
      "        [0.7110]], device='mps:0')\n",
      "Iteration 63390 Training loss 0.0990171805024147 Validation loss 0.0998002216219902 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3570],\n",
      "        [0.3991]], device='mps:0')\n",
      "Iteration 63400 Training loss 0.0897674709558487 Validation loss 0.09980485588312149 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1903],\n",
      "        [0.3153]], device='mps:0')\n",
      "Iteration 63410 Training loss 0.10240178555250168 Validation loss 0.09980334341526031 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4045],\n",
      "        [0.6810]], device='mps:0')\n",
      "Iteration 63420 Training loss 0.09588343650102615 Validation loss 0.09980075061321259 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6710],\n",
      "        [0.3582]], device='mps:0')\n",
      "Iteration 63430 Training loss 0.09568687528371811 Validation loss 0.09983612596988678 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3219],\n",
      "        [0.3900]], device='mps:0')\n",
      "Iteration 63440 Training loss 0.09281687438488007 Validation loss 0.09981851279735565 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3452],\n",
      "        [0.4859]], device='mps:0')\n",
      "Iteration 63450 Training loss 0.08718296140432358 Validation loss 0.09981738775968552 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3759],\n",
      "        [0.6294]], device='mps:0')\n",
      "Iteration 63460 Training loss 0.10494414716959 Validation loss 0.09985101222991943 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7554],\n",
      "        [0.5201]], device='mps:0')\n",
      "Iteration 63470 Training loss 0.10455228388309479 Validation loss 0.0999186784029007 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5808],\n",
      "        [0.1069]], device='mps:0')\n",
      "Iteration 63480 Training loss 0.1042649894952774 Validation loss 0.09998117387294769 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.7400],\n",
      "        [0.7212]], device='mps:0')\n",
      "Iteration 63490 Training loss 0.10863739997148514 Validation loss 0.09987623244524002 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4978],\n",
      "        [0.5228]], device='mps:0')\n",
      "Iteration 63500 Training loss 0.10486762970685959 Validation loss 0.09987695515155792 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6023],\n",
      "        [0.1169]], device='mps:0')\n",
      "Iteration 63510 Training loss 0.09598348289728165 Validation loss 0.09985237568616867 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4654],\n",
      "        [0.4379]], device='mps:0')\n",
      "Iteration 63520 Training loss 0.10210609436035156 Validation loss 0.09986919164657593 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2225],\n",
      "        [0.5020]], device='mps:0')\n",
      "Iteration 63530 Training loss 0.10229954123497009 Validation loss 0.0999511331319809 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5684],\n",
      "        [0.2352]], device='mps:0')\n",
      "Iteration 63540 Training loss 0.11953666061162949 Validation loss 0.09990361332893372 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5027],\n",
      "        [0.6249]], device='mps:0')\n",
      "Iteration 63550 Training loss 0.11073977500200272 Validation loss 0.09988398849964142 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6314],\n",
      "        [0.3437]], device='mps:0')\n",
      "Iteration 63560 Training loss 0.10562213510274887 Validation loss 0.0999198779463768 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4629],\n",
      "        [0.5813]], device='mps:0')\n",
      "Iteration 63570 Training loss 0.11090599745512009 Validation loss 0.09984627366065979 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4573],\n",
      "        [0.3765]], device='mps:0')\n",
      "Iteration 63580 Training loss 0.10191451758146286 Validation loss 0.09984615445137024 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5392],\n",
      "        [0.1681]], device='mps:0')\n",
      "Iteration 63590 Training loss 0.10163235664367676 Validation loss 0.09981074929237366 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5722],\n",
      "        [0.3322]], device='mps:0')\n",
      "Iteration 63600 Training loss 0.11386872828006744 Validation loss 0.09980790317058563 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3809],\n",
      "        [0.2446]], device='mps:0')\n",
      "Iteration 63610 Training loss 0.11746162176132202 Validation loss 0.09985033422708511 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4813],\n",
      "        [0.4670]], device='mps:0')\n",
      "Iteration 63620 Training loss 0.09620150178670883 Validation loss 0.09987764805555344 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.8148],\n",
      "        [0.5801]], device='mps:0')\n",
      "Iteration 63630 Training loss 0.11258121579885483 Validation loss 0.0999363586306572 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3871],\n",
      "        [0.3229]], device='mps:0')\n",
      "Iteration 63640 Training loss 0.09658731520175934 Validation loss 0.09989550709724426 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6397],\n",
      "        [0.7449]], device='mps:0')\n",
      "Iteration 63650 Training loss 0.10045398771762848 Validation loss 0.0999235063791275 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5996],\n",
      "        [0.3099]], device='mps:0')\n",
      "Iteration 63660 Training loss 0.10720357298851013 Validation loss 0.09988529235124588 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6278],\n",
      "        [0.3359]], device='mps:0')\n",
      "Iteration 63670 Training loss 0.10724613815546036 Validation loss 0.09981869906187057 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.1874],\n",
      "        [0.6204]], device='mps:0')\n",
      "Iteration 63680 Training loss 0.10123860836029053 Validation loss 0.09979002177715302 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4519],\n",
      "        [0.2856]], device='mps:0')\n",
      "Iteration 63690 Training loss 0.10281524062156677 Validation loss 0.09979266673326492 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3780],\n",
      "        [0.2088]], device='mps:0')\n",
      "Iteration 63700 Training loss 0.10542947798967361 Validation loss 0.09977928549051285 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5658],\n",
      "        [0.7027]], device='mps:0')\n",
      "Iteration 63710 Training loss 0.10238269716501236 Validation loss 0.09977862238883972 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3243],\n",
      "        [0.5191]], device='mps:0')\n",
      "Iteration 63720 Training loss 0.10483104735612869 Validation loss 0.09980656951665878 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6274],\n",
      "        [0.5811]], device='mps:0')\n",
      "Iteration 63730 Training loss 0.09746160358190536 Validation loss 0.09978477656841278 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4638],\n",
      "        [0.1629]], device='mps:0')\n",
      "Iteration 63740 Training loss 0.10455630719661713 Validation loss 0.09978938847780228 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6520],\n",
      "        [0.7149]], device='mps:0')\n",
      "Iteration 63750 Training loss 0.11660721153020859 Validation loss 0.09978189319372177 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.0606],\n",
      "        [0.2284]], device='mps:0')\n",
      "Iteration 63760 Training loss 0.10686329007148743 Validation loss 0.09977571666240692 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7315],\n",
      "        [0.5743]], device='mps:0')\n",
      "Iteration 63770 Training loss 0.09509049355983734 Validation loss 0.09978189319372177 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2893],\n",
      "        [0.5574]], device='mps:0')\n",
      "Iteration 63780 Training loss 0.09458829462528229 Validation loss 0.09978461265563965 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7108],\n",
      "        [0.7083]], device='mps:0')\n",
      "Iteration 63790 Training loss 0.09984227269887924 Validation loss 0.09978370368480682 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5100],\n",
      "        [0.5994]], device='mps:0')\n",
      "Iteration 63800 Training loss 0.09577184170484543 Validation loss 0.09977433830499649 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5349],\n",
      "        [0.6432]], device='mps:0')\n",
      "Iteration 63810 Training loss 0.10948161035776138 Validation loss 0.09977269917726517 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3348],\n",
      "        [0.0421]], device='mps:0')\n",
      "Iteration 63820 Training loss 0.0924374908208847 Validation loss 0.09979194402694702 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4454],\n",
      "        [0.3996]], device='mps:0')\n",
      "Iteration 63830 Training loss 0.09746371954679489 Validation loss 0.09978621453046799 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2785],\n",
      "        [0.6959]], device='mps:0')\n",
      "Iteration 63840 Training loss 0.09421979635953903 Validation loss 0.09980957210063934 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4369],\n",
      "        [0.5776]], device='mps:0')\n",
      "Iteration 63850 Training loss 0.09944158047437668 Validation loss 0.09977814555168152 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3692],\n",
      "        [0.3705]], device='mps:0')\n",
      "Iteration 63860 Training loss 0.11145667731761932 Validation loss 0.09977530688047409 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2967],\n",
      "        [0.4941]], device='mps:0')\n",
      "Iteration 63870 Training loss 0.10650914162397385 Validation loss 0.09980368614196777 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3450],\n",
      "        [0.6370]], device='mps:0')\n",
      "Iteration 63880 Training loss 0.09963373094797134 Validation loss 0.09977667778730392 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6542],\n",
      "        [0.2661]], device='mps:0')\n",
      "Iteration 63890 Training loss 0.09697277843952179 Validation loss 0.09977869689464569 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6312],\n",
      "        [0.4559]], device='mps:0')\n",
      "Iteration 63900 Training loss 0.1008959487080574 Validation loss 0.0997685045003891 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2336],\n",
      "        [0.6063]], device='mps:0')\n",
      "Iteration 63910 Training loss 0.0927792564034462 Validation loss 0.09979356825351715 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4541],\n",
      "        [0.2090]], device='mps:0')\n",
      "Iteration 63920 Training loss 0.10774634778499603 Validation loss 0.09978440403938293 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4559],\n",
      "        [0.1977]], device='mps:0')\n",
      "Iteration 63930 Training loss 0.10088659077882767 Validation loss 0.09977149218320847 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.1795],\n",
      "        [0.5126]], device='mps:0')\n",
      "Iteration 63940 Training loss 0.10916021466255188 Validation loss 0.09976578503847122 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6156],\n",
      "        [0.5956]], device='mps:0')\n",
      "Iteration 63950 Training loss 0.11793313920497894 Validation loss 0.09978549182415009 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.1068],\n",
      "        [0.2710]], device='mps:0')\n",
      "Iteration 63960 Training loss 0.0948084145784378 Validation loss 0.09980458766222 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5464],\n",
      "        [0.6300]], device='mps:0')\n",
      "Iteration 63970 Training loss 0.10663063079118729 Validation loss 0.09983593970537186 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5828],\n",
      "        [0.3426]], device='mps:0')\n",
      "Iteration 63980 Training loss 0.10767856240272522 Validation loss 0.09984337538480759 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6615],\n",
      "        [0.7499]], device='mps:0')\n",
      "Iteration 63990 Training loss 0.10210612416267395 Validation loss 0.09988204389810562 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4770],\n",
      "        [0.3743]], device='mps:0')\n",
      "Iteration 64000 Training loss 0.11242793500423431 Validation loss 0.09979984164237976 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.0991],\n",
      "        [0.5331]], device='mps:0')\n",
      "Iteration 64010 Training loss 0.09261846542358398 Validation loss 0.09981607645750046 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5658],\n",
      "        [0.5303]], device='mps:0')\n",
      "Iteration 64020 Training loss 0.0876067727804184 Validation loss 0.09978096187114716 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3412],\n",
      "        [0.0593]], device='mps:0')\n",
      "Iteration 64030 Training loss 0.1046120747923851 Validation loss 0.09976310282945633 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6670],\n",
      "        [0.6085]], device='mps:0')\n",
      "Iteration 64040 Training loss 0.10315510630607605 Validation loss 0.09975741803646088 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3369],\n",
      "        [0.2941]], device='mps:0')\n",
      "Iteration 64050 Training loss 0.0950634554028511 Validation loss 0.09977088123559952 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4417],\n",
      "        [0.6141]], device='mps:0')\n",
      "Iteration 64060 Training loss 0.09724000841379166 Validation loss 0.09975850582122803 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2563],\n",
      "        [0.5280]], device='mps:0')\n",
      "Iteration 64070 Training loss 0.10262398421764374 Validation loss 0.09975869953632355 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.5296],\n",
      "        [0.4168]], device='mps:0')\n",
      "Iteration 64080 Training loss 0.10353297740221024 Validation loss 0.09976039081811905 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3155],\n",
      "        [0.6587]], device='mps:0')\n",
      "Iteration 64090 Training loss 0.09954839199781418 Validation loss 0.09976594895124435 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5996],\n",
      "        [0.2670]], device='mps:0')\n",
      "Iteration 64100 Training loss 0.09564214199781418 Validation loss 0.09978285431861877 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6951],\n",
      "        [0.4815]], device='mps:0')\n",
      "Iteration 64110 Training loss 0.10327142477035522 Validation loss 0.09981188178062439 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.7285],\n",
      "        [0.3765]], device='mps:0')\n",
      "Iteration 64120 Training loss 0.09642630070447922 Validation loss 0.09976809471845627 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4651],\n",
      "        [0.6281]], device='mps:0')\n",
      "Iteration 64130 Training loss 0.09502663463354111 Validation loss 0.09977488964796066 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2361],\n",
      "        [0.5359]], device='mps:0')\n",
      "Iteration 64140 Training loss 0.115895576775074 Validation loss 0.09977502375841141 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3667],\n",
      "        [0.5074]], device='mps:0')\n",
      "Iteration 64150 Training loss 0.09019641578197479 Validation loss 0.09975528717041016 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.3044],\n",
      "        [0.3786]], device='mps:0')\n",
      "Iteration 64160 Training loss 0.10160087049007416 Validation loss 0.099757619202137 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4773],\n",
      "        [0.5139]], device='mps:0')\n",
      "Iteration 64170 Training loss 0.10114902257919312 Validation loss 0.09974672645330429 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5695],\n",
      "        [0.7316]], device='mps:0')\n",
      "Iteration 64180 Training loss 0.10426667332649231 Validation loss 0.0997532457113266 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5642],\n",
      "        [0.5260]], device='mps:0')\n",
      "Iteration 64190 Training loss 0.09947409480810165 Validation loss 0.09974465519189835 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2314],\n",
      "        [0.1479]], device='mps:0')\n",
      "Iteration 64200 Training loss 0.10573972761631012 Validation loss 0.09974553436040878 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2849],\n",
      "        [0.7211]], device='mps:0')\n",
      "Iteration 64210 Training loss 0.10620573908090591 Validation loss 0.09974355250597 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7411],\n",
      "        [0.4661]], device='mps:0')\n",
      "Iteration 64220 Training loss 0.11223622411489487 Validation loss 0.09974125772714615 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4023],\n",
      "        [0.4665]], device='mps:0')\n",
      "Iteration 64230 Training loss 0.10477973520755768 Validation loss 0.09974866360425949 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5416],\n",
      "        [0.1961]], device='mps:0')\n",
      "Iteration 64240 Training loss 0.09833932667970657 Validation loss 0.09979317337274551 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.7339],\n",
      "        [0.4165]], device='mps:0')\n",
      "Iteration 64250 Training loss 0.1081504300236702 Validation loss 0.09980762004852295 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6788],\n",
      "        [0.2952]], device='mps:0')\n",
      "Iteration 64260 Training loss 0.10485643893480301 Validation loss 0.0997874066233635 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6775],\n",
      "        [0.3929]], device='mps:0')\n",
      "Iteration 64270 Training loss 0.08977409452199936 Validation loss 0.09979046881198883 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.7330],\n",
      "        [0.6506]], device='mps:0')\n",
      "Iteration 64280 Training loss 0.10576964169740677 Validation loss 0.09985342621803284 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3447],\n",
      "        [0.5772]], device='mps:0')\n",
      "Iteration 64290 Training loss 0.09913648664951324 Validation loss 0.09987233579158783 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3493],\n",
      "        [0.6085]], device='mps:0')\n",
      "Iteration 64300 Training loss 0.09424357861280441 Validation loss 0.09980876743793488 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6673],\n",
      "        [0.6214]], device='mps:0')\n",
      "Iteration 64310 Training loss 0.10725908726453781 Validation loss 0.09979934990406036 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6794],\n",
      "        [0.4495]], device='mps:0')\n",
      "Iteration 64320 Training loss 0.10224660485982895 Validation loss 0.09978628903627396 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5653],\n",
      "        [0.2118]], device='mps:0')\n",
      "Iteration 64330 Training loss 0.09143386036157608 Validation loss 0.09974966943264008 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5611],\n",
      "        [0.3684]], device='mps:0')\n",
      "Iteration 64340 Training loss 0.10372867435216904 Validation loss 0.09974007308483124 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3281],\n",
      "        [0.2808]], device='mps:0')\n",
      "Iteration 64350 Training loss 0.10821546614170074 Validation loss 0.0997728630900383 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.2493],\n",
      "        [0.3712]], device='mps:0')\n",
      "Iteration 64360 Training loss 0.09594310820102692 Validation loss 0.09978915750980377 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6294],\n",
      "        [0.4232]], device='mps:0')\n",
      "Iteration 64370 Training loss 0.10877889394760132 Validation loss 0.09977441281080246 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5174],\n",
      "        [0.4524]], device='mps:0')\n",
      "Iteration 64380 Training loss 0.11194988340139389 Validation loss 0.09976862370967865 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6333],\n",
      "        [0.6401]], device='mps:0')\n",
      "Iteration 64390 Training loss 0.09815914183855057 Validation loss 0.09979291260242462 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4458],\n",
      "        [0.6550]], device='mps:0')\n",
      "Iteration 64400 Training loss 0.10929333418607712 Validation loss 0.0997939482331276 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.2090],\n",
      "        [0.6315]], device='mps:0')\n",
      "Iteration 64410 Training loss 0.10274823755025864 Validation loss 0.0998215526342392 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4486],\n",
      "        [0.7281]], device='mps:0')\n",
      "Iteration 64420 Training loss 0.0903361514210701 Validation loss 0.0997982919216156 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.3083],\n",
      "        [0.3536]], device='mps:0')\n",
      "Iteration 64430 Training loss 0.0964495986700058 Validation loss 0.09975527226924896 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5531],\n",
      "        [0.2911]], device='mps:0')\n",
      "Iteration 64440 Training loss 0.09278617799282074 Validation loss 0.09978187084197998 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6210],\n",
      "        [0.3500]], device='mps:0')\n",
      "Iteration 64450 Training loss 0.09777423739433289 Validation loss 0.09974560886621475 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5146],\n",
      "        [0.4415]], device='mps:0')\n",
      "Iteration 64460 Training loss 0.10411681234836578 Validation loss 0.09977932274341583 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4912],\n",
      "        [0.6765]], device='mps:0')\n",
      "Iteration 64470 Training loss 0.1106971874833107 Validation loss 0.0997433215379715 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5477],\n",
      "        [0.6063]], device='mps:0')\n",
      "Iteration 64480 Training loss 0.08632777631282806 Validation loss 0.09974651038646698 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5936],\n",
      "        [0.7519]], device='mps:0')\n",
      "Iteration 64490 Training loss 0.08910994976758957 Validation loss 0.09973114728927612 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6445],\n",
      "        [0.7088]], device='mps:0')\n",
      "Iteration 64500 Training loss 0.10402441024780273 Validation loss 0.09972723573446274 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4142],\n",
      "        [0.4807]], device='mps:0')\n",
      "Iteration 64510 Training loss 0.10418159514665604 Validation loss 0.09973116964101791 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4718],\n",
      "        [0.1268]], device='mps:0')\n",
      "Iteration 64520 Training loss 0.1027793288230896 Validation loss 0.0997229591012001 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3160],\n",
      "        [0.4925]], device='mps:0')\n",
      "Iteration 64530 Training loss 0.1145099401473999 Validation loss 0.09972571581602097 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5003],\n",
      "        [0.5948]], device='mps:0')\n",
      "Iteration 64540 Training loss 0.09941812604665756 Validation loss 0.09971854090690613 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4917],\n",
      "        [0.2582]], device='mps:0')\n",
      "Iteration 64550 Training loss 0.10646194219589233 Validation loss 0.09971719980239868 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6683],\n",
      "        [0.5040]], device='mps:0')\n",
      "Iteration 64560 Training loss 0.10667053610086441 Validation loss 0.09971729665994644 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.4465],\n",
      "        [0.1867]], device='mps:0')\n",
      "Iteration 64570 Training loss 0.11134105175733566 Validation loss 0.09972084313631058 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.1042],\n",
      "        [0.3453]], device='mps:0')\n",
      "Iteration 64580 Training loss 0.10289768129587173 Validation loss 0.0997321680188179 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.7129],\n",
      "        [0.1723]], device='mps:0')\n",
      "Iteration 64590 Training loss 0.09166186302900314 Validation loss 0.09974495321512222 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6511],\n",
      "        [0.6681]], device='mps:0')\n",
      "Iteration 64600 Training loss 0.09609174728393555 Validation loss 0.0997231975197792 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4563],\n",
      "        [0.1327]], device='mps:0')\n",
      "Iteration 64610 Training loss 0.09237819164991379 Validation loss 0.09971725940704346 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6085],\n",
      "        [0.5832]], device='mps:0')\n",
      "Iteration 64620 Training loss 0.10609941184520721 Validation loss 0.09972087293863297 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5767],\n",
      "        [0.2617]], device='mps:0')\n",
      "Iteration 64630 Training loss 0.08963008970022202 Validation loss 0.09971801936626434 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6320],\n",
      "        [0.6123]], device='mps:0')\n",
      "Iteration 64640 Training loss 0.10666895657777786 Validation loss 0.09971176832914352 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6035],\n",
      "        [0.6252]], device='mps:0')\n",
      "Iteration 64650 Training loss 0.09099625051021576 Validation loss 0.0997123122215271 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5525],\n",
      "        [0.6639]], device='mps:0')\n",
      "Iteration 64660 Training loss 0.10694974660873413 Validation loss 0.09971345216035843 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6700],\n",
      "        [0.5336]], device='mps:0')\n",
      "Iteration 64670 Training loss 0.10006237775087357 Validation loss 0.0997304916381836 Accuracy 0.718000054359436\n",
      "Output tensor([[0.4969],\n",
      "        [0.4547]], device='mps:0')\n",
      "Iteration 64680 Training loss 0.10769892483949661 Validation loss 0.0997212752699852 Accuracy 0.7160000205039978\n",
      "Output tensor([[0.6048],\n",
      "        [0.4707]], device='mps:0')\n",
      "Iteration 64690 Training loss 0.10466167330741882 Validation loss 0.09971025586128235 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3170],\n",
      "        [0.2827]], device='mps:0')\n",
      "Iteration 64700 Training loss 0.09777072817087173 Validation loss 0.09971173852682114 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3149],\n",
      "        [0.3219]], device='mps:0')\n",
      "Iteration 64710 Training loss 0.10206227004528046 Validation loss 0.09971316158771515 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6600],\n",
      "        [0.5006]], device='mps:0')\n",
      "Iteration 64720 Training loss 0.10319986939430237 Validation loss 0.0997210144996643 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6464],\n",
      "        [0.1977]], device='mps:0')\n",
      "Iteration 64730 Training loss 0.08383233845233917 Validation loss 0.09973367303609848 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5815],\n",
      "        [0.3832]], device='mps:0')\n",
      "Iteration 64740 Training loss 0.09945213049650192 Validation loss 0.09973154217004776 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5408],\n",
      "        [0.4502]], device='mps:0')\n",
      "Iteration 64750 Training loss 0.10006669908761978 Validation loss 0.09973262250423431 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5614],\n",
      "        [0.7004]], device='mps:0')\n",
      "Iteration 64760 Training loss 0.09676419943571091 Validation loss 0.09973272681236267 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5049],\n",
      "        [0.3719]], device='mps:0')\n",
      "Iteration 64770 Training loss 0.09369871020317078 Validation loss 0.0997382253408432 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.4902],\n",
      "        [0.1819]], device='mps:0')\n",
      "Iteration 64780 Training loss 0.10011439770460129 Validation loss 0.09971672296524048 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4992],\n",
      "        [0.6890]], device='mps:0')\n",
      "Iteration 64790 Training loss 0.11449997872114182 Validation loss 0.09973806887865067 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5144],\n",
      "        [0.7538]], device='mps:0')\n",
      "Iteration 64800 Training loss 0.11362864822149277 Validation loss 0.09978079050779343 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5981],\n",
      "        [0.5676]], device='mps:0')\n",
      "Iteration 64810 Training loss 0.10089388489723206 Validation loss 0.09974800795316696 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.3283],\n",
      "        [0.5601]], device='mps:0')\n",
      "Iteration 64820 Training loss 0.10942506790161133 Validation loss 0.09975030273199081 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.5908],\n",
      "        [0.2750]], device='mps:0')\n",
      "Iteration 64830 Training loss 0.10949443280696869 Validation loss 0.09973704814910889 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4013],\n",
      "        [0.2831]], device='mps:0')\n",
      "Iteration 64840 Training loss 0.10619933903217316 Validation loss 0.09976861625909805 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4858],\n",
      "        [0.3848]], device='mps:0')\n",
      "Iteration 64850 Training loss 0.1024448350071907 Validation loss 0.0997455045580864 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5036],\n",
      "        [0.3474]], device='mps:0')\n",
      "Iteration 64860 Training loss 0.10664920508861542 Validation loss 0.0997403934597969 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4540],\n",
      "        [0.2086]], device='mps:0')\n",
      "Iteration 64870 Training loss 0.1058807522058487 Validation loss 0.0997566282749176 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.2279],\n",
      "        [0.4491]], device='mps:0')\n",
      "Iteration 64880 Training loss 0.1100350171327591 Validation loss 0.09981333464384079 Accuracy 0.7160000205039978\n",
      "Output tensor([[0.5623],\n",
      "        [0.1978]], device='mps:0')\n",
      "Iteration 64890 Training loss 0.10254901647567749 Validation loss 0.0998283103108406 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5259],\n",
      "        [0.5834]], device='mps:0')\n",
      "Iteration 64900 Training loss 0.10110153257846832 Validation loss 0.0998225137591362 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7360],\n",
      "        [0.3916]], device='mps:0')\n",
      "Iteration 64910 Training loss 0.11513840407133102 Validation loss 0.0997786596417427 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6014],\n",
      "        [0.1346]], device='mps:0')\n",
      "Iteration 64920 Training loss 0.10421550273895264 Validation loss 0.09977439045906067 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6003],\n",
      "        [0.6590]], device='mps:0')\n",
      "Iteration 64930 Training loss 0.10877742618322372 Validation loss 0.09972678124904633 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5662],\n",
      "        [0.3997]], device='mps:0')\n",
      "Iteration 64940 Training loss 0.0984407588839531 Validation loss 0.09973680227994919 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5420],\n",
      "        [0.5583]], device='mps:0')\n",
      "Iteration 64950 Training loss 0.08996682614088058 Validation loss 0.0997423306107521 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.4597],\n",
      "        [0.5785]], device='mps:0')\n",
      "Iteration 64960 Training loss 0.09364183247089386 Validation loss 0.09972994029521942 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4406],\n",
      "        [0.4565]], device='mps:0')\n",
      "Iteration 64970 Training loss 0.1005355641245842 Validation loss 0.09974534809589386 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6488],\n",
      "        [0.5786]], device='mps:0')\n",
      "Iteration 64980 Training loss 0.11314938962459564 Validation loss 0.09972810000181198 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3542],\n",
      "        [0.4644]], device='mps:0')\n",
      "Iteration 64990 Training loss 0.0899563580751419 Validation loss 0.09971175342798233 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6510],\n",
      "        [0.7604]], device='mps:0')\n",
      "Iteration 65000 Training loss 0.09121567755937576 Validation loss 0.09973253309726715 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4843],\n",
      "        [0.6759]], device='mps:0')\n",
      "Iteration 65010 Training loss 0.09966976195573807 Validation loss 0.09971962124109268 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6590],\n",
      "        [0.3464]], device='mps:0')\n",
      "Iteration 65020 Training loss 0.09680673480033875 Validation loss 0.09971627593040466 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.7099],\n",
      "        [0.6662]], device='mps:0')\n",
      "Iteration 65030 Training loss 0.09799876809120178 Validation loss 0.09969478100538254 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.7185],\n",
      "        [0.4398]], device='mps:0')\n",
      "Iteration 65040 Training loss 0.1044064313173294 Validation loss 0.09969145804643631 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.2425],\n",
      "        [0.4600]], device='mps:0')\n",
      "Iteration 65050 Training loss 0.10495492815971375 Validation loss 0.09969581663608551 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.5570],\n",
      "        [0.4710]], device='mps:0')\n",
      "Iteration 65060 Training loss 0.09816206991672516 Validation loss 0.09971321374177933 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4234],\n",
      "        [0.5906]], device='mps:0')\n",
      "Iteration 65070 Training loss 0.09296353906393051 Validation loss 0.09970557689666748 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6256],\n",
      "        [0.7092]], device='mps:0')\n",
      "Iteration 65080 Training loss 0.10205280780792236 Validation loss 0.09969160705804825 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7535],\n",
      "        [0.3362]], device='mps:0')\n",
      "Iteration 65090 Training loss 0.10537064075469971 Validation loss 0.09969127178192139 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.4908],\n",
      "        [0.5895]], device='mps:0')\n",
      "Iteration 65100 Training loss 0.09350023418664932 Validation loss 0.0996895506978035 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.5903],\n",
      "        [0.5553]], device='mps:0')\n",
      "Iteration 65110 Training loss 0.10597914457321167 Validation loss 0.09973597526550293 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.1159],\n",
      "        [0.2619]], device='mps:0')\n",
      "Iteration 65120 Training loss 0.1071423888206482 Validation loss 0.09974879771471024 Accuracy 0.7115000486373901\n",
      "Output tensor([[0.6287],\n",
      "        [0.4278]], device='mps:0')\n",
      "Iteration 65130 Training loss 0.0965680405497551 Validation loss 0.09970618039369583 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.6284],\n",
      "        [0.7113]], device='mps:0')\n",
      "Iteration 65140 Training loss 0.11471254378557205 Validation loss 0.09970549494028091 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.3514],\n",
      "        [0.3375]], device='mps:0')\n",
      "Iteration 65150 Training loss 0.10076360404491425 Validation loss 0.0997498631477356 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6497],\n",
      "        [0.4214]], device='mps:0')\n",
      "Iteration 65160 Training loss 0.12885501980781555 Validation loss 0.09975958615541458 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6350],\n",
      "        [0.6693]], device='mps:0')\n",
      "Iteration 65170 Training loss 0.10338957607746124 Validation loss 0.09978282451629639 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.3367],\n",
      "        [0.4585]], device='mps:0')\n",
      "Iteration 65180 Training loss 0.10263703763484955 Validation loss 0.09981915354728699 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.8232],\n",
      "        [0.7929]], device='mps:0')\n",
      "Iteration 65190 Training loss 0.11093409359455109 Validation loss 0.09977147728204727 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.4515],\n",
      "        [0.4162]], device='mps:0')\n",
      "Iteration 65200 Training loss 0.10324302315711975 Validation loss 0.0997370108962059 Accuracy 0.7110000252723694\n",
      "Output tensor([[0.3451],\n",
      "        [0.4956]], device='mps:0')\n",
      "Iteration 65210 Training loss 0.09700912237167358 Validation loss 0.09980252385139465 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.6971],\n",
      "        [0.3446]], device='mps:0')\n",
      "Iteration 65220 Training loss 0.10685336589813232 Validation loss 0.09980830550193787 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.4794],\n",
      "        [0.0560]], device='mps:0')\n",
      "Iteration 65230 Training loss 0.09842764586210251 Validation loss 0.09974122792482376 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.5785],\n",
      "        [0.3913]], device='mps:0')\n",
      "Iteration 65240 Training loss 0.10057626664638519 Validation loss 0.0997152328491211 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4293],\n",
      "        [0.5697]], device='mps:0')\n",
      "Iteration 65250 Training loss 0.11065541952848434 Validation loss 0.09970895946025848 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4252],\n",
      "        [0.3609]], device='mps:0')\n",
      "Iteration 65260 Training loss 0.09952579438686371 Validation loss 0.09975162893533707 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.6178],\n",
      "        [0.0516]], device='mps:0')\n",
      "Iteration 65270 Training loss 0.10230047255754471 Validation loss 0.09975884109735489 Accuracy 0.7155000567436218\n",
      "Output tensor([[0.5612],\n",
      "        [0.5179]], device='mps:0')\n",
      "Iteration 65280 Training loss 0.108019158244133 Validation loss 0.09970948100090027 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.6047],\n",
      "        [0.3119]], device='mps:0')\n",
      "Iteration 65290 Training loss 0.10042908787727356 Validation loss 0.09971380978822708 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.4695],\n",
      "        [0.4153]], device='mps:0')\n",
      "Iteration 65300 Training loss 0.0928976908326149 Validation loss 0.09970930218696594 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2970],\n",
      "        [0.6522]], device='mps:0')\n",
      "Iteration 65310 Training loss 0.10074065625667572 Validation loss 0.09969913214445114 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4911],\n",
      "        [0.5084]], device='mps:0')\n",
      "Iteration 65320 Training loss 0.09542357921600342 Validation loss 0.09970603138208389 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.1963],\n",
      "        [0.5407]], device='mps:0')\n",
      "Iteration 65330 Training loss 0.11483176797628403 Validation loss 0.09967781603336334 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.2438],\n",
      "        [0.5669]], device='mps:0')\n",
      "Iteration 65340 Training loss 0.09935551136732101 Validation loss 0.09967809170484543 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5255],\n",
      "        [0.3575]], device='mps:0')\n",
      "Iteration 65350 Training loss 0.10820668935775757 Validation loss 0.09967704117298126 Accuracy 0.7145000100135803\n",
      "Output tensor([[0.6427],\n",
      "        [0.1562]], device='mps:0')\n",
      "Iteration 65360 Training loss 0.09854484349489212 Validation loss 0.09967605024576187 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6598],\n",
      "        [0.2369]], device='mps:0')\n",
      "Iteration 65370 Training loss 0.11293797940015793 Validation loss 0.09968221187591553 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.7640],\n",
      "        [0.5626]], device='mps:0')\n",
      "Iteration 65380 Training loss 0.09371873736381531 Validation loss 0.09968333691358566 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.6166],\n",
      "        [0.4617]], device='mps:0')\n",
      "Iteration 65390 Training loss 0.11736652255058289 Validation loss 0.09967459738254547 Accuracy 0.7150000333786011\n",
      "Output tensor([[0.4438],\n",
      "        [0.3457]], device='mps:0')\n",
      "Iteration 65400 Training loss 0.09299881011247635 Validation loss 0.0996733158826828 Accuracy 0.7140000462532043\n",
      "Output tensor([[0.5185],\n",
      "        [0.5793]], device='mps:0')\n",
      "Iteration 65410 Training loss 0.12042192369699478 Validation loss 0.09969020634889603 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.6319],\n",
      "        [0.1058]], device='mps:0')\n",
      "Iteration 65420 Training loss 0.10301130264997482 Validation loss 0.09969045221805573 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.5568],\n",
      "        [0.2243]], device='mps:0')\n",
      "Iteration 65430 Training loss 0.1058369055390358 Validation loss 0.09968926012516022 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.3373],\n",
      "        [0.5974]], device='mps:0')\n",
      "Iteration 65440 Training loss 0.1081441342830658 Validation loss 0.0996946170926094 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5518],\n",
      "        [0.7134]], device='mps:0')\n",
      "Iteration 65450 Training loss 0.10468956083059311 Validation loss 0.09969031065702438 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.3774],\n",
      "        [0.6974]], device='mps:0')\n",
      "Iteration 65460 Training loss 0.11065442860126495 Validation loss 0.09972915053367615 Accuracy 0.7125000357627869\n",
      "Output tensor([[0.1329],\n",
      "        [0.5140]], device='mps:0')\n",
      "Iteration 65470 Training loss 0.11111082136631012 Validation loss 0.09970181435346603 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.5868],\n",
      "        [0.5636]], device='mps:0')\n",
      "Iteration 65480 Training loss 0.09223282337188721 Validation loss 0.09968400746583939 Accuracy 0.7120000123977661\n",
      "Output tensor([[0.6447],\n",
      "        [0.3174]], device='mps:0')\n",
      "Iteration 65490 Training loss 0.1007687896490097 Validation loss 0.09967697411775589 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.3634],\n",
      "        [0.2723]], device='mps:0')\n",
      "Iteration 65500 Training loss 0.10585975646972656 Validation loss 0.09967177361249924 Accuracy 0.7135000228881836\n",
      "Output tensor([[0.8003],\n",
      "        [0.7301]], device='mps:0')\n",
      "Iteration 65510 Training loss 0.09492100775241852 Validation loss 0.09968005865812302 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.2887],\n",
      "        [0.6489]], device='mps:0')\n",
      "Iteration 65520 Training loss 0.10234280675649643 Validation loss 0.0996902659535408 Accuracy 0.7130000591278076\n",
      "Output tensor([[0.4911],\n",
      "        [0.1912]], device='mps:0')\n",
      "Iteration 65530 Training loss 0.10267969220876694 Validation loss 0.09968958050012589 Accuracy 0.7135000228881836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training done'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 1.6, 1e-9, 0.1, 0.1, 1e-1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08bfb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer = binary_classification_three_layer_NN(1024, 512, 512, eps_init = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c674f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 1.6, the number of datas used for the training is 6553600.000000005 and the number of iterations is 65536.\n",
      "Iteration 0 Training loss 0.12500135600566864 Validation loss 0.12500105798244476 Accuracy 0.453000009059906\n",
      "Iteration 10 Training loss 0.12497668713331223 Validation loss 0.12497969716787338 Accuracy 0.49900001287460327\n",
      "Iteration 20 Training loss 0.1250060647726059 Validation loss 0.12500512599945068 Accuracy 0.5\n",
      "Iteration 30 Training loss 0.12621861696243286 Validation loss 0.12476550787687302 Accuracy 0.5\n",
      "Iteration 40 Training loss 0.12500697374343872 Validation loss 0.12500706315040588 Accuracy 0.5\n",
      "Iteration 50 Training loss 0.12502515316009521 Validation loss 0.12502530217170715 Accuracy 0.5\n",
      "Iteration 60 Training loss 0.12473168969154358 Validation loss 0.12463594973087311 Accuracy 0.5400000214576721\n",
      "Iteration 70 Training loss 0.12545019388198853 Validation loss 0.125022754073143 Accuracy 0.5\n",
      "Iteration 80 Training loss 0.1253579705953598 Validation loss 0.12503279745578766 Accuracy 0.5\n",
      "Iteration 90 Training loss 0.12527069449424744 Validation loss 0.1250286102294922 Accuracy 0.5\n",
      "Iteration 100 Training loss 0.12528513371944427 Validation loss 0.12495490908622742 Accuracy 0.5010000467300415\n",
      "Iteration 110 Training loss 0.12480328977108002 Validation loss 0.12501370906829834 Accuracy 0.5\n",
      "Iteration 120 Training loss 0.12509439885616302 Validation loss 0.1250043511390686 Accuracy 0.5\n",
      "Iteration 130 Training loss 0.12515775859355927 Validation loss 0.1250041276216507 Accuracy 0.5\n",
      "Iteration 140 Training loss 0.12508633732795715 Validation loss 0.12500308454036713 Accuracy 0.5\n",
      "Iteration 150 Training loss 0.1233479455113411 Validation loss 0.12382538616657257 Accuracy 0.5\n",
      "Iteration 160 Training loss 0.12500804662704468 Validation loss 0.125 Accuracy 0.5\n",
      "Iteration 170 Training loss 0.12488202750682831 Validation loss 0.12500014901161194 Accuracy 0.5\n",
      "Iteration 180 Training loss 0.12500976026058197 Validation loss 0.12500034272670746 Accuracy 0.5\n",
      "Iteration 190 Training loss 0.12500512599945068 Validation loss 0.12496461719274521 Accuracy 0.5\n",
      "Iteration 200 Training loss 0.12148254364728928 Validation loss 0.12217980623245239 Accuracy 0.5\n",
      "Iteration 210 Training loss 0.12497987598180771 Validation loss 0.1249999850988388 Accuracy 0.5\n",
      "Iteration 220 Training loss 0.12500016391277313 Validation loss 0.12497760355472565 Accuracy 0.5040000081062317\n",
      "Iteration 230 Training loss 0.12501393258571625 Validation loss 0.12499646842479706 Accuracy 0.5015000104904175\n",
      "Iteration 240 Training loss 0.12499696761369705 Validation loss 0.12499935179948807 Accuracy 0.5005000233650208\n",
      "Iteration 250 Training loss 0.12499803304672241 Validation loss 0.12499456107616425 Accuracy 0.5020000338554382\n",
      "Iteration 260 Training loss 0.12918494641780853 Validation loss 0.12377014756202698 Accuracy 0.4985000193119049\n",
      "Iteration 270 Training loss 0.12522251904010773 Validation loss 0.12501350045204163 Accuracy 0.5\n",
      "Iteration 280 Training loss 0.125052809715271 Validation loss 0.12500938773155212 Accuracy 0.5\n",
      "Iteration 290 Training loss 0.12526021897792816 Validation loss 0.12501782178878784 Accuracy 0.5\n",
      "Iteration 300 Training loss 0.12438849359750748 Validation loss 0.12452980875968933 Accuracy 0.5130000114440918\n",
      "Iteration 310 Training loss 0.12543539702892303 Validation loss 0.12500827014446259 Accuracy 0.5\n",
      "Iteration 320 Training loss 0.1247486099600792 Validation loss 0.124890998005867 Accuracy 0.5024999976158142\n",
      "Iteration 330 Training loss 0.12477435171604156 Validation loss 0.12473154813051224 Accuracy 0.5445000529289246\n",
      "Iteration 340 Training loss 0.11611565947532654 Validation loss 0.1204846203327179 Accuracy 0.5320000052452087\n",
      "Iteration 350 Training loss 0.12495018541812897 Validation loss 0.1250009685754776 Accuracy 0.5\n",
      "Iteration 360 Training loss 0.12455226480960846 Validation loss 0.12488432973623276 Accuracy 0.5\n",
      "Iteration 370 Training loss 0.1250939667224884 Validation loss 0.1249297708272934 Accuracy 0.5\n",
      "Iteration 380 Training loss 0.12351854145526886 Validation loss 0.12497885525226593 Accuracy 0.5\n",
      "Iteration 390 Training loss 0.12457984685897827 Validation loss 0.12477301061153412 Accuracy 0.5205000042915344\n",
      "Iteration 400 Training loss 0.11678087711334229 Validation loss 0.11537616699934006 Accuracy 0.6310000419616699\n",
      "Iteration 410 Training loss 0.10691789537668228 Validation loss 0.11445087194442749 Accuracy 0.6155000329017639\n",
      "Iteration 420 Training loss 0.1167742908000946 Validation loss 0.11672692745923996 Accuracy 0.6190000176429749\n",
      "Iteration 430 Training loss 0.1182117909193039 Validation loss 0.11372267454862595 Accuracy 0.6335000395774841\n",
      "Iteration 440 Training loss 0.12347661703824997 Validation loss 0.12382256239652634 Accuracy 0.5265000462532043\n",
      "Iteration 450 Training loss 0.11149203032255173 Validation loss 0.114950992166996 Accuracy 0.6510000228881836\n",
      "Iteration 460 Training loss 0.11270150542259216 Validation loss 0.11004999279975891 Accuracy 0.6615000367164612\n",
      "Iteration 470 Training loss 0.11980966478586197 Validation loss 0.11939986050128937 Accuracy 0.6605000495910645\n",
      "Iteration 480 Training loss 0.1101309061050415 Validation loss 0.1165897399187088 Accuracy 0.6355000138282776\n",
      "Iteration 490 Training loss 0.11520730704069138 Validation loss 0.10725235193967819 Accuracy 0.6955000162124634\n",
      "Iteration 500 Training loss 0.10923450440168381 Validation loss 0.10789395123720169 Accuracy 0.6520000100135803\n",
      "Iteration 510 Training loss 0.10895989090204239 Validation loss 0.11042783409357071 Accuracy 0.7250000238418579\n",
      "Iteration 520 Training loss 0.1084224283695221 Validation loss 0.10686991363763809 Accuracy 0.7175000309944153\n",
      "Iteration 530 Training loss 0.11531436443328857 Validation loss 0.11130572110414505 Accuracy 0.6945000290870667\n",
      "Iteration 540 Training loss 0.11633992195129395 Validation loss 0.11621218174695969 Accuracy 0.6305000185966492\n",
      "Iteration 550 Training loss 0.10756570845842361 Validation loss 0.10944069176912308 Accuracy 0.6695000529289246\n",
      "Iteration 560 Training loss 0.10470502823591232 Validation loss 0.10490331053733826 Accuracy 0.7040000557899475\n",
      "Iteration 570 Training loss 0.12225019186735153 Validation loss 0.13213582336902618 Accuracy 0.5164999961853027\n",
      "Iteration 580 Training loss 0.12139254063367844 Validation loss 0.11857014894485474 Accuracy 0.5870000123977661\n",
      "Iteration 590 Training loss 0.10344699770212173 Validation loss 0.10149069875478745 Accuracy 0.7145000100135803\n",
      "Iteration 600 Training loss 0.1104372888803482 Validation loss 0.10757802426815033 Accuracy 0.7230000495910645\n",
      "Iteration 610 Training loss 0.10528906434774399 Validation loss 0.1075749546289444 Accuracy 0.6805000305175781\n",
      "Iteration 620 Training loss 0.10633613169193268 Validation loss 0.10261816531419754 Accuracy 0.7155000567436218\n",
      "Iteration 630 Training loss 0.119340680539608 Validation loss 0.10660086572170258 Accuracy 0.6660000085830688\n",
      "Iteration 640 Training loss 0.10594584047794342 Validation loss 0.10365277528762817 Accuracy 0.6825000047683716\n",
      "Iteration 650 Training loss 0.10376199334859848 Validation loss 0.10346478968858719 Accuracy 0.7230000495910645\n",
      "Iteration 660 Training loss 0.11389587819576263 Validation loss 0.11334077268838882 Accuracy 0.6305000185966492\n",
      "Iteration 670 Training loss 0.10527057200670242 Validation loss 0.10439233481884003 Accuracy 0.7220000624656677\n",
      "Iteration 680 Training loss 0.10836326330900192 Validation loss 0.1141972690820694 Accuracy 0.5590000152587891\n",
      "Iteration 690 Training loss 0.11428380757570267 Validation loss 0.10611876100301743 Accuracy 0.6920000314712524\n",
      "Iteration 700 Training loss 0.09257184714078903 Validation loss 0.10294026881456375 Accuracy 0.7065000534057617\n",
      "Iteration 710 Training loss 0.10915699601173401 Validation loss 0.1045912504196167 Accuracy 0.7045000195503235\n",
      "Iteration 720 Training loss 0.1265653371810913 Validation loss 0.11954905837774277 Accuracy 0.546500027179718\n",
      "Iteration 730 Training loss 0.11702527850866318 Validation loss 0.10541771352291107 Accuracy 0.674500048160553\n",
      "Iteration 740 Training loss 0.11558029055595398 Validation loss 0.10581643879413605 Accuracy 0.7005000114440918\n",
      "Iteration 750 Training loss 0.11339619010686874 Validation loss 0.11103533208370209 Accuracy 0.6455000042915344\n",
      "Iteration 760 Training loss 0.11360765993595123 Validation loss 0.11540009081363678 Accuracy 0.6135000586509705\n",
      "Iteration 770 Training loss 0.09231775999069214 Validation loss 0.09983938187360764 Accuracy 0.7255000472068787\n",
      "Iteration 780 Training loss 0.10380289703607559 Validation loss 0.10758668929338455 Accuracy 0.6540000438690186\n",
      "Iteration 790 Training loss 0.108627088367939 Validation loss 0.10287472605705261 Accuracy 0.7175000309944153\n",
      "Iteration 800 Training loss 0.10032981634140015 Validation loss 0.10662242770195007 Accuracy 0.6895000338554382\n",
      "Iteration 810 Training loss 0.10178907215595245 Validation loss 0.10052777081727982 Accuracy 0.7350000143051147\n",
      "Iteration 820 Training loss 0.11098827421665192 Validation loss 0.10722459852695465 Accuracy 0.6795000433921814\n",
      "Iteration 830 Training loss 0.10079967230558395 Validation loss 0.104494608938694 Accuracy 0.6985000371932983\n",
      "Iteration 840 Training loss 0.09285399317741394 Validation loss 0.10090113431215286 Accuracy 0.7255000472068787\n",
      "Iteration 850 Training loss 0.10763312131166458 Validation loss 0.10711174458265305 Accuracy 0.6725000143051147\n",
      "Iteration 860 Training loss 0.09619215130805969 Validation loss 0.10624001175165176 Accuracy 0.6765000224113464\n",
      "Iteration 870 Training loss 0.0937604084610939 Validation loss 0.09892971813678741 Accuracy 0.7210000157356262\n",
      "Iteration 880 Training loss 0.11132899671792984 Validation loss 0.10146673768758774 Accuracy 0.70250004529953\n",
      "Iteration 890 Training loss 0.11174314469099045 Validation loss 0.10860138386487961 Accuracy 0.6895000338554382\n",
      "Iteration 900 Training loss 0.1001967340707779 Validation loss 0.10262089222669601 Accuracy 0.7150000333786011\n",
      "Iteration 910 Training loss 0.09698803722858429 Validation loss 0.101157546043396 Accuracy 0.703000009059906\n",
      "Iteration 920 Training loss 0.10868407785892487 Validation loss 0.1027223989367485 Accuracy 0.7035000324249268\n",
      "Iteration 930 Training loss 0.10149548202753067 Validation loss 0.10552108287811279 Accuracy 0.6695000529289246\n",
      "Iteration 940 Training loss 0.10177426785230637 Validation loss 0.10866288095712662 Accuracy 0.6545000076293945\n",
      "Iteration 950 Training loss 0.10612577199935913 Validation loss 0.12431177496910095 Accuracy 0.5245000123977661\n",
      "Iteration 960 Training loss 0.10082054883241653 Validation loss 0.1059526577591896 Accuracy 0.6630000472068787\n",
      "Iteration 970 Training loss 0.1157926693558693 Validation loss 0.11596807837486267 Accuracy 0.5865000486373901\n",
      "Iteration 980 Training loss 0.1299341768026352 Validation loss 0.11903952807188034 Accuracy 0.5730000138282776\n",
      "Iteration 990 Training loss 0.10026866942644119 Validation loss 0.10656683892011642 Accuracy 0.6855000257492065\n",
      "Iteration 1000 Training loss 0.10451196134090424 Validation loss 0.10564647614955902 Accuracy 0.659500002861023\n",
      "Iteration 1010 Training loss 0.10445209592580795 Validation loss 0.10294638574123383 Accuracy 0.7020000219345093\n",
      "Iteration 1020 Training loss 0.08848290890455246 Validation loss 0.10280519723892212 Accuracy 0.6910000443458557\n",
      "Iteration 1030 Training loss 0.11537549644708633 Validation loss 0.10082040727138519 Accuracy 0.7110000252723694\n",
      "Iteration 1040 Training loss 0.12564776837825775 Validation loss 0.11564244329929352 Accuracy 0.6005000472068787\n",
      "Iteration 1050 Training loss 0.09411948919296265 Validation loss 0.10133910179138184 Accuracy 0.703000009059906\n",
      "Iteration 1060 Training loss 0.10135827958583832 Validation loss 0.11306814849376678 Accuracy 0.6050000190734863\n",
      "Iteration 1070 Training loss 0.10117057710886002 Validation loss 0.1031089797616005 Accuracy 0.706000030040741\n",
      "Iteration 1080 Training loss 0.11894537508487701 Validation loss 0.11791906505823135 Accuracy 0.5725000500679016\n",
      "Iteration 1090 Training loss 0.12226042151451111 Validation loss 0.1181531548500061 Accuracy 0.5915000438690186\n",
      "Iteration 1100 Training loss 0.12974394857883453 Validation loss 0.1242622658610344 Accuracy 0.5385000109672546\n",
      "Iteration 1110 Training loss 0.09556140750646591 Validation loss 0.10753390938043594 Accuracy 0.6710000038146973\n",
      "Iteration 1120 Training loss 0.14123664796352386 Validation loss 0.12952975928783417 Accuracy 0.5265000462532043\n",
      "Iteration 1130 Training loss 0.0857674777507782 Validation loss 0.10012985020875931 Accuracy 0.7165000438690186\n",
      "Iteration 1140 Training loss 0.1210189238190651 Validation loss 0.1208643913269043 Accuracy 0.5675000548362732\n",
      "Iteration 1150 Training loss 0.10373412817716599 Validation loss 0.11142676323652267 Accuracy 0.655500054359436\n",
      "Iteration 1160 Training loss 0.09856867790222168 Validation loss 0.10614264756441116 Accuracy 0.6835000514984131\n",
      "Iteration 1170 Training loss 0.09062401205301285 Validation loss 0.10173264145851135 Accuracy 0.7170000076293945\n",
      "Iteration 1180 Training loss 0.11403512954711914 Validation loss 0.10254865139722824 Accuracy 0.7015000581741333\n",
      "Iteration 1190 Training loss 0.09319383651018143 Validation loss 0.10660411417484283 Accuracy 0.6680000424385071\n",
      "Iteration 1200 Training loss 0.13698048889636993 Validation loss 0.12331394106149673 Accuracy 0.5735000371932983\n",
      "Iteration 1210 Training loss 0.08972109854221344 Validation loss 0.10678543895483017 Accuracy 0.6820000410079956\n",
      "Iteration 1220 Training loss 0.10759130865335464 Validation loss 0.1071847602725029 Accuracy 0.6605000495910645\n",
      "Iteration 1230 Training loss 0.0991181954741478 Validation loss 0.10134649276733398 Accuracy 0.7015000581741333\n",
      "Iteration 1240 Training loss 0.0960860550403595 Validation loss 0.1032228171825409 Accuracy 0.6995000243186951\n",
      "Iteration 1250 Training loss 0.10882348567247391 Validation loss 0.11654066294431686 Accuracy 0.5960000157356262\n",
      "Iteration 1260 Training loss 0.11179076135158539 Validation loss 0.12042870372533798 Accuracy 0.5770000219345093\n",
      "Iteration 1270 Training loss 0.12994831800460815 Validation loss 0.12287777662277222 Accuracy 0.5885000228881836\n",
      "Iteration 1280 Training loss 0.0923784077167511 Validation loss 0.10387268662452698 Accuracy 0.690000057220459\n",
      "Iteration 1290 Training loss 0.11287733167409897 Validation loss 0.10570726543664932 Accuracy 0.6645000576972961\n",
      "Iteration 1300 Training loss 0.16934150457382202 Validation loss 0.1450137346982956 Accuracy 0.5394999980926514\n",
      "Iteration 1310 Training loss 0.10147132724523544 Validation loss 0.11454316973686218 Accuracy 0.6080000400543213\n",
      "Iteration 1320 Training loss 0.12225689738988876 Validation loss 0.12598851323127747 Accuracy 0.596500039100647\n",
      "Iteration 1330 Training loss 0.09375259280204773 Validation loss 0.10033722966909409 Accuracy 0.7085000276565552\n",
      "Iteration 1340 Training loss 0.08294343948364258 Validation loss 0.09727659821510315 Accuracy 0.7320000529289246\n",
      "Iteration 1350 Training loss 0.11362564563751221 Validation loss 0.10855970531702042 Accuracy 0.6460000276565552\n",
      "Iteration 1360 Training loss 0.10443010181188583 Validation loss 0.10108885914087296 Accuracy 0.7120000123977661\n",
      "Iteration 1370 Training loss 0.11435264348983765 Validation loss 0.10264936089515686 Accuracy 0.7165000438690186\n",
      "Iteration 1380 Training loss 0.10293272137641907 Validation loss 0.10645084083080292 Accuracy 0.6670000553131104\n",
      "Iteration 1390 Training loss 0.10445543378591537 Validation loss 0.10469266027212143 Accuracy 0.6865000128746033\n",
      "Iteration 1400 Training loss 0.11152589321136475 Validation loss 0.10876481980085373 Accuracy 0.6575000286102295\n",
      "Iteration 1410 Training loss 0.10998865216970444 Validation loss 0.11758086085319519 Accuracy 0.625\n",
      "Iteration 1420 Training loss 0.10307330638170242 Validation loss 0.10696447640657425 Accuracy 0.6720000505447388\n",
      "Iteration 1430 Training loss 0.1059732437133789 Validation loss 0.11851201951503754 Accuracy 0.5855000019073486\n",
      "Iteration 1440 Training loss 0.10164204984903336 Validation loss 0.09742420166730881 Accuracy 0.718500018119812\n",
      "Iteration 1450 Training loss 0.10896726697683334 Validation loss 0.10642999410629272 Accuracy 0.6795000433921814\n",
      "Iteration 1460 Training loss 0.10173238813877106 Validation loss 0.10443185269832611 Accuracy 0.6780000329017639\n",
      "Iteration 1470 Training loss 0.15326514840126038 Validation loss 0.11844667047262192 Accuracy 0.5665000081062317\n",
      "Iteration 1480 Training loss 0.11662997305393219 Validation loss 0.1181371882557869 Accuracy 0.6465000510215759\n",
      "Iteration 1490 Training loss 0.09953265637159348 Validation loss 0.10415347665548325 Accuracy 0.6845000386238098\n",
      "Iteration 1500 Training loss 0.11415288597345352 Validation loss 0.10640692710876465 Accuracy 0.675000011920929\n",
      "Iteration 1510 Training loss 0.1106734424829483 Validation loss 0.10686987638473511 Accuracy 0.6670000553131104\n",
      "Iteration 1520 Training loss 0.10634408891201019 Validation loss 0.10851307958364487 Accuracy 0.6540000438690186\n",
      "Iteration 1530 Training loss 0.11897186934947968 Validation loss 0.13033273816108704 Accuracy 0.5790000557899475\n",
      "Iteration 1540 Training loss 0.11025004833936691 Validation loss 0.10461732745170593 Accuracy 0.6765000224113464\n",
      "Iteration 1550 Training loss 0.1059640645980835 Validation loss 0.09990687668323517 Accuracy 0.7355000376701355\n",
      "Iteration 1560 Training loss 0.11582209169864655 Validation loss 0.10671651363372803 Accuracy 0.6990000605583191\n",
      "Iteration 1570 Training loss 0.1023559719324112 Validation loss 0.10217009484767914 Accuracy 0.7115000486373901\n",
      "Iteration 1580 Training loss 0.13452932238578796 Validation loss 0.12584972381591797 Accuracy 0.515500009059906\n",
      "Iteration 1590 Training loss 0.09871414303779602 Validation loss 0.10276676714420319 Accuracy 0.6885000467300415\n",
      "Iteration 1600 Training loss 0.10115572810173035 Validation loss 0.10139771550893784 Accuracy 0.7235000133514404\n",
      "Iteration 1610 Training loss 0.09990700334310532 Validation loss 0.10702724009752274 Accuracy 0.6530000567436218\n",
      "Iteration 1620 Training loss 0.1183355525135994 Validation loss 0.11387959867715836 Accuracy 0.612000048160553\n",
      "Iteration 1630 Training loss 0.099967822432518 Validation loss 0.09854645282030106 Accuracy 0.7280000448226929\n",
      "Iteration 1640 Training loss 0.10855535417795181 Validation loss 0.10930876433849335 Accuracy 0.6495000123977661\n",
      "Iteration 1650 Training loss 0.10042432695627213 Validation loss 0.10052130371332169 Accuracy 0.7150000333786011\n",
      "Iteration 1660 Training loss 0.106869637966156 Validation loss 0.10052567720413208 Accuracy 0.7125000357627869\n",
      "Iteration 1670 Training loss 0.127346009016037 Validation loss 0.1250009983778 Accuracy 0.6085000038146973\n",
      "Iteration 1680 Training loss 0.12314069271087646 Validation loss 0.11863961815834045 Accuracy 0.5855000019073486\n",
      "Iteration 1690 Training loss 0.10383640229701996 Validation loss 0.10850522667169571 Accuracy 0.6475000381469727\n",
      "Iteration 1700 Training loss 0.19419288635253906 Validation loss 0.15407103300094604 Accuracy 0.503000020980835\n",
      "Iteration 1710 Training loss 0.10424379259347916 Validation loss 0.10346072912216187 Accuracy 0.6825000047683716\n",
      "Iteration 1720 Training loss 0.09830959141254425 Validation loss 0.10132550448179245 Accuracy 0.7330000400543213\n",
      "Iteration 1730 Training loss 0.11227822303771973 Validation loss 0.1080239936709404 Accuracy 0.6645000576972961\n",
      "Iteration 1740 Training loss 0.09840193390846252 Validation loss 0.09957991540431976 Accuracy 0.6975000500679016\n",
      "Iteration 1750 Training loss 0.1067093089222908 Validation loss 0.11900819092988968 Accuracy 0.5820000171661377\n",
      "Iteration 1760 Training loss 0.1240476742386818 Validation loss 0.1171211302280426 Accuracy 0.5985000133514404\n",
      "Iteration 1770 Training loss 0.13099905848503113 Validation loss 0.13483475148677826 Accuracy 0.47600001096725464\n",
      "Iteration 1780 Training loss 0.09851980209350586 Validation loss 0.11324727535247803 Accuracy 0.6455000042915344\n",
      "Iteration 1790 Training loss 0.09120599180459976 Validation loss 0.100763700902462 Accuracy 0.6940000057220459\n",
      "Iteration 1800 Training loss 0.1106027364730835 Validation loss 0.11137612164020538 Accuracy 0.6535000205039978\n",
      "Iteration 1810 Training loss 0.10519096255302429 Validation loss 0.10428345203399658 Accuracy 0.6675000190734863\n",
      "Iteration 1820 Training loss 0.09593947976827621 Validation loss 0.1283494234085083 Accuracy 0.5785000324249268\n",
      "Iteration 1830 Training loss 0.08549965918064117 Validation loss 0.1026565432548523 Accuracy 0.7100000381469727\n",
      "Iteration 1840 Training loss 0.1066560447216034 Validation loss 0.10490436106920242 Accuracy 0.674500048160553\n",
      "Iteration 1850 Training loss 0.09781825542449951 Validation loss 0.10124890506267548 Accuracy 0.7160000205039978\n",
      "Iteration 1860 Training loss 0.095782071352005 Validation loss 0.10280419141054153 Accuracy 0.6940000057220459\n",
      "Iteration 1870 Training loss 0.10425439476966858 Validation loss 0.1011994332075119 Accuracy 0.6885000467300415\n",
      "Iteration 1880 Training loss 0.08765217661857605 Validation loss 0.10376814007759094 Accuracy 0.6790000200271606\n",
      "Iteration 1890 Training loss 0.10287921875715256 Validation loss 0.09967703372240067 Accuracy 0.7120000123977661\n",
      "Iteration 1900 Training loss 0.08838929980993271 Validation loss 0.11011455208063126 Accuracy 0.6635000109672546\n",
      "Iteration 1910 Training loss 0.10062510520219803 Validation loss 0.10702979564666748 Accuracy 0.6580000519752502\n",
      "Iteration 1920 Training loss 0.11630063503980637 Validation loss 0.11591274291276932 Accuracy 0.6035000085830688\n",
      "Iteration 1930 Training loss 0.10102535039186478 Validation loss 0.10138016939163208 Accuracy 0.7055000066757202\n",
      "Iteration 1940 Training loss 0.10957760363817215 Validation loss 0.11244959384202957 Accuracy 0.6085000038146973\n",
      "Iteration 1950 Training loss 0.11759788542985916 Validation loss 0.12042750418186188 Accuracy 0.6075000166893005\n",
      "Iteration 1960 Training loss 0.10840156674385071 Validation loss 0.11072462052106857 Accuracy 0.640500009059906\n",
      "Iteration 1970 Training loss 0.11134810000658035 Validation loss 0.12613777816295624 Accuracy 0.5715000033378601\n",
      "Iteration 1980 Training loss 0.0935726910829544 Validation loss 0.09851352125406265 Accuracy 0.7115000486373901\n",
      "Iteration 1990 Training loss 0.12727178633213043 Validation loss 0.11314687132835388 Accuracy 0.6305000185966492\n",
      "Iteration 2000 Training loss 0.10683836042881012 Validation loss 0.10288137942552567 Accuracy 0.6875000596046448\n",
      "Iteration 2010 Training loss 0.11374086141586304 Validation loss 0.10851304233074188 Accuracy 0.6505000591278076\n",
      "Iteration 2020 Training loss 0.07613873481750488 Validation loss 0.09850767999887466 Accuracy 0.7305000424385071\n",
      "Iteration 2030 Training loss 0.09913799911737442 Validation loss 0.11332770437002182 Accuracy 0.655500054359436\n",
      "Iteration 2040 Training loss 0.10143514722585678 Validation loss 0.09908589720726013 Accuracy 0.7135000228881836\n",
      "Iteration 2050 Training loss 0.11206300556659698 Validation loss 0.11013272404670715 Accuracy 0.6345000267028809\n",
      "Iteration 2060 Training loss 0.09002437442541122 Validation loss 0.10240107029676437 Accuracy 0.6965000033378601\n",
      "Iteration 2070 Training loss 0.08604071289300919 Validation loss 0.09719187021255493 Accuracy 0.7415000200271606\n",
      "Iteration 2080 Training loss 0.10920930653810501 Validation loss 0.09937955439090729 Accuracy 0.6995000243186951\n",
      "Iteration 2090 Training loss 0.11145681142807007 Validation loss 0.09857744723558426 Accuracy 0.7295000553131104\n",
      "Iteration 2100 Training loss 0.10870209336280823 Validation loss 0.09835734218358994 Accuracy 0.7080000042915344\n",
      "Iteration 2110 Training loss 0.09407217055559158 Validation loss 0.10166575014591217 Accuracy 0.7105000615119934\n",
      "Iteration 2120 Training loss 0.09412694722414017 Validation loss 0.0982598215341568 Accuracy 0.7170000076293945\n",
      "Iteration 2130 Training loss 0.12217624485492706 Validation loss 0.10991031676530838 Accuracy 0.6260000467300415\n",
      "Iteration 2140 Training loss 0.104599729180336 Validation loss 0.09630521386861801 Accuracy 0.7220000624656677\n",
      "Iteration 2150 Training loss 0.11419839411973953 Validation loss 0.10542234033346176 Accuracy 0.674500048160553\n",
      "Iteration 2160 Training loss 0.14207720756530762 Validation loss 0.11860262602567673 Accuracy 0.5910000205039978\n",
      "Iteration 2170 Training loss 0.09149930626153946 Validation loss 0.10285089910030365 Accuracy 0.6840000152587891\n",
      "Iteration 2180 Training loss 0.12413174659013748 Validation loss 0.12544073164463043 Accuracy 0.6020000576972961\n",
      "Iteration 2190 Training loss 0.10139840841293335 Validation loss 0.10113091766834259 Accuracy 0.7190000414848328\n",
      "Iteration 2200 Training loss 0.10592645406723022 Validation loss 0.10050394386053085 Accuracy 0.7070000171661377\n",
      "Iteration 2210 Training loss 0.09018348157405853 Validation loss 0.09694180637598038 Accuracy 0.721500039100647\n",
      "Iteration 2220 Training loss 0.09793683141469955 Validation loss 0.09927388280630112 Accuracy 0.6990000605583191\n",
      "Iteration 2230 Training loss 0.10194943845272064 Validation loss 0.11191840469837189 Accuracy 0.6355000138282776\n",
      "Iteration 2240 Training loss 0.0925791785120964 Validation loss 0.09961695224046707 Accuracy 0.6980000138282776\n",
      "Iteration 2250 Training loss 0.09676039218902588 Validation loss 0.10803190618753433 Accuracy 0.6465000510215759\n",
      "Iteration 2260 Training loss 0.11108016967773438 Validation loss 0.09916635602712631 Accuracy 0.7350000143051147\n",
      "Iteration 2270 Training loss 0.08903727680444717 Validation loss 0.10057924687862396 Accuracy 0.6960000395774841\n",
      "Iteration 2280 Training loss 0.1344841718673706 Validation loss 0.1109975054860115 Accuracy 0.6210000514984131\n",
      "Iteration 2290 Training loss 0.11852064728736877 Validation loss 0.10895813256502151 Accuracy 0.6495000123977661\n",
      "Iteration 2300 Training loss 0.11993972212076187 Validation loss 0.1180439293384552 Accuracy 0.5860000252723694\n",
      "Iteration 2310 Training loss 0.08696035295724869 Validation loss 0.10024414956569672 Accuracy 0.7205000519752502\n",
      "Iteration 2320 Training loss 0.10135658830404282 Validation loss 0.10733283311128616 Accuracy 0.6655000448226929\n",
      "Iteration 2330 Training loss 0.11110207438468933 Validation loss 0.10335344821214676 Accuracy 0.6890000104904175\n",
      "Iteration 2340 Training loss 0.0992647036910057 Validation loss 0.0996575877070427 Accuracy 0.718000054359436\n",
      "Iteration 2350 Training loss 0.09555770456790924 Validation loss 0.09743513911962509 Accuracy 0.7230000495910645\n",
      "Iteration 2360 Training loss 0.10879392176866531 Validation loss 0.1018231213092804 Accuracy 0.6945000290870667\n",
      "Iteration 2370 Training loss 0.11038494855165482 Validation loss 0.09628057479858398 Accuracy 0.7315000295639038\n",
      "Iteration 2380 Training loss 0.09897991269826889 Validation loss 0.10128410160541534 Accuracy 0.6925000548362732\n",
      "Iteration 2390 Training loss 0.08753179013729095 Validation loss 0.09466361254453659 Accuracy 0.7240000367164612\n",
      "Iteration 2400 Training loss 0.0990709736943245 Validation loss 0.09565737098455429 Accuracy 0.733500063419342\n",
      "Iteration 2410 Training loss 0.10596054047346115 Validation loss 0.1071520447731018 Accuracy 0.6545000076293945\n",
      "Iteration 2420 Training loss 0.09890385717153549 Validation loss 0.10506664216518402 Accuracy 0.6910000443458557\n",
      "Iteration 2430 Training loss 0.10924530029296875 Validation loss 0.10413213819265366 Accuracy 0.6755000352859497\n",
      "Iteration 2440 Training loss 0.09572870284318924 Validation loss 0.1002182886004448 Accuracy 0.7175000309944153\n",
      "Iteration 2450 Training loss 0.10560464859008789 Validation loss 0.10543570667505264 Accuracy 0.6645000576972961\n",
      "Iteration 2460 Training loss 0.11544784903526306 Validation loss 0.10612435638904572 Accuracy 0.6860000491142273\n",
      "Iteration 2470 Training loss 0.07867207378149033 Validation loss 0.09951729327440262 Accuracy 0.7020000219345093\n",
      "Iteration 2480 Training loss 0.08918552100658417 Validation loss 0.10083477944135666 Accuracy 0.7230000495910645\n",
      "Iteration 2490 Training loss 0.08979743719100952 Validation loss 0.10839859396219254 Accuracy 0.6510000228881836\n",
      "Iteration 2500 Training loss 0.08903780579566956 Validation loss 0.09912948310375214 Accuracy 0.7200000286102295\n",
      "Iteration 2510 Training loss 0.08438973128795624 Validation loss 0.10074273496866226 Accuracy 0.6875000596046448\n",
      "Iteration 2520 Training loss 0.09096159785985947 Validation loss 0.09446755051612854 Accuracy 0.7385000586509705\n",
      "Iteration 2530 Training loss 0.10635485500097275 Validation loss 0.10254491120576859 Accuracy 0.6930000185966492\n",
      "Iteration 2540 Training loss 0.11768710613250732 Validation loss 0.123371422290802 Accuracy 0.6095000505447388\n",
      "Iteration 2550 Training loss 0.10642427206039429 Validation loss 0.10115883499383926 Accuracy 0.7155000567436218\n",
      "Iteration 2560 Training loss 0.11363639682531357 Validation loss 0.10935631394386292 Accuracy 0.6445000171661377\n",
      "Iteration 2570 Training loss 0.09828580915927887 Validation loss 0.09853462129831314 Accuracy 0.7015000581741333\n",
      "Iteration 2580 Training loss 0.09629648178815842 Validation loss 0.09934686869382858 Accuracy 0.6990000605583191\n",
      "Iteration 2590 Training loss 0.09581880271434784 Validation loss 0.09859179705381393 Accuracy 0.7055000066757202\n",
      "Iteration 2600 Training loss 0.09983008354902267 Validation loss 0.1028004065155983 Accuracy 0.6945000290870667\n",
      "Iteration 2610 Training loss 0.12322231382131577 Validation loss 0.11997666209936142 Accuracy 0.5730000138282776\n",
      "Iteration 2620 Training loss 0.1228739321231842 Validation loss 0.1352354735136032 Accuracy 0.5615000128746033\n",
      "Iteration 2630 Training loss 0.08643642067909241 Validation loss 0.10405051708221436 Accuracy 0.6880000233650208\n",
      "Iteration 2640 Training loss 0.09629710763692856 Validation loss 0.10176288336515427 Accuracy 0.6910000443458557\n",
      "Iteration 2650 Training loss 0.15131956338882446 Validation loss 0.11576814204454422 Accuracy 0.6350000500679016\n",
      "Iteration 2660 Training loss 0.11979247629642487 Validation loss 0.1102588102221489 Accuracy 0.6380000114440918\n",
      "Iteration 2670 Training loss 0.10524362325668335 Validation loss 0.10709788650274277 Accuracy 0.6660000085830688\n",
      "Iteration 2680 Training loss 0.1137290894985199 Validation loss 0.10101548582315445 Accuracy 0.6930000185966492\n",
      "Iteration 2690 Training loss 0.07688532769680023 Validation loss 0.10251400619745255 Accuracy 0.7070000171661377\n",
      "Iteration 2700 Training loss 0.11534711718559265 Validation loss 0.10425645112991333 Accuracy 0.6760000586509705\n",
      "Iteration 2710 Training loss 0.09714805334806442 Validation loss 0.10132227092981339 Accuracy 0.6950000524520874\n",
      "Iteration 2720 Training loss 0.10250205546617508 Validation loss 0.10782362520694733 Accuracy 0.6490000486373901\n",
      "Iteration 2730 Training loss 0.08709774166345596 Validation loss 0.10108612477779388 Accuracy 0.690500020980835\n",
      "Iteration 2740 Training loss 0.10097018629312515 Validation loss 0.10624909400939941 Accuracy 0.6855000257492065\n",
      "Iteration 2750 Training loss 0.10077501088380814 Validation loss 0.09872473776340485 Accuracy 0.703000009059906\n",
      "Iteration 2760 Training loss 0.10336807370185852 Validation loss 0.10105205327272415 Accuracy 0.6860000491142273\n",
      "Iteration 2770 Training loss 0.10703423619270325 Validation loss 0.10214528441429138 Accuracy 0.6880000233650208\n",
      "Iteration 2780 Training loss 0.09833383560180664 Validation loss 0.09880181401968002 Accuracy 0.7145000100135803\n",
      "Iteration 2790 Training loss 0.11028344929218292 Validation loss 0.1058725118637085 Accuracy 0.6710000038146973\n",
      "Iteration 2800 Training loss 0.10387755930423737 Validation loss 0.10804584622383118 Accuracy 0.6585000157356262\n",
      "Iteration 2810 Training loss 0.1147499606013298 Validation loss 0.10398191958665848 Accuracy 0.7080000042915344\n",
      "Iteration 2820 Training loss 0.11388055235147476 Validation loss 0.10641548037528992 Accuracy 0.6650000214576721\n",
      "Iteration 2830 Training loss 0.11047156900167465 Validation loss 0.10858951508998871 Accuracy 0.6665000319480896\n",
      "Iteration 2840 Training loss 0.1094818189740181 Validation loss 0.11817336827516556 Accuracy 0.5830000042915344\n",
      "Iteration 2850 Training loss 0.09669049829244614 Validation loss 0.09863980859518051 Accuracy 0.7125000357627869\n",
      "Iteration 2860 Training loss 0.08982009440660477 Validation loss 0.09511769562959671 Accuracy 0.7330000400543213\n",
      "Iteration 2870 Training loss 0.10901601612567902 Validation loss 0.10012058913707733 Accuracy 0.718000054359436\n",
      "Iteration 2880 Training loss 0.10864698141813278 Validation loss 0.10104522854089737 Accuracy 0.7130000591278076\n",
      "Iteration 2890 Training loss 0.08710172772407532 Validation loss 0.10246258974075317 Accuracy 0.6855000257492065\n",
      "Iteration 2900 Training loss 0.11002662777900696 Validation loss 0.11118724942207336 Accuracy 0.6475000381469727\n",
      "Iteration 2910 Training loss 0.11964195221662521 Validation loss 0.12029687315225601 Accuracy 0.5925000309944153\n",
      "Iteration 2920 Training loss 0.10511644184589386 Validation loss 0.11192111670970917 Accuracy 0.6320000290870667\n",
      "Iteration 2930 Training loss 0.09930559247732162 Validation loss 0.09960916638374329 Accuracy 0.7155000567436218\n",
      "Iteration 2940 Training loss 0.08769848942756653 Validation loss 0.10251129418611526 Accuracy 0.6890000104904175\n",
      "Iteration 2950 Training loss 0.10439532995223999 Validation loss 0.116603784263134 Accuracy 0.5805000066757202\n",
      "Iteration 2960 Training loss 0.12275155633687973 Validation loss 0.11572763323783875 Accuracy 0.612500011920929\n",
      "Iteration 2970 Training loss 0.12252534925937653 Validation loss 0.12835432589054108 Accuracy 0.5815000534057617\n",
      "Iteration 2980 Training loss 0.10471375286579132 Validation loss 0.10281099379062653 Accuracy 0.6735000610351562\n",
      "Iteration 2990 Training loss 0.10820836573839188 Validation loss 0.10225959867238998 Accuracy 0.6910000443458557\n",
      "Iteration 3000 Training loss 0.08650348335504532 Validation loss 0.09435141086578369 Accuracy 0.7420000433921814\n",
      "Iteration 3010 Training loss 0.10498402267694473 Validation loss 0.10002075880765915 Accuracy 0.687000036239624\n",
      "Iteration 3020 Training loss 0.11068908125162125 Validation loss 0.09370790421962738 Accuracy 0.734000027179718\n",
      "Iteration 3030 Training loss 0.10817629843950272 Validation loss 0.10614266246557236 Accuracy 0.6790000200271606\n",
      "Iteration 3040 Training loss 0.09877676516771317 Validation loss 0.10054124891757965 Accuracy 0.7210000157356262\n",
      "Iteration 3050 Training loss 0.10904517769813538 Validation loss 0.10536874830722809 Accuracy 0.7010000348091125\n",
      "Iteration 3060 Training loss 0.11807145923376083 Validation loss 0.11319504678249359 Accuracy 0.6195000410079956\n",
      "Iteration 3070 Training loss 0.10182605683803558 Validation loss 0.10272087901830673 Accuracy 0.6785000562667847\n",
      "Iteration 3080 Training loss 0.09314966946840286 Validation loss 0.1100785955786705 Accuracy 0.6385000348091125\n",
      "Iteration 3090 Training loss 0.09847720712423325 Validation loss 0.10200047492980957 Accuracy 0.7120000123977661\n",
      "Iteration 3100 Training loss 0.09510234743356705 Validation loss 0.10941986739635468 Accuracy 0.6585000157356262\n",
      "Iteration 3110 Training loss 0.10927712917327881 Validation loss 0.10381060093641281 Accuracy 0.6775000095367432\n",
      "Iteration 3120 Training loss 0.12415456771850586 Validation loss 0.10111357271671295 Accuracy 0.6980000138282776\n",
      "Iteration 3130 Training loss 0.0814976766705513 Validation loss 0.09542892873287201 Accuracy 0.7170000076293945\n",
      "Iteration 3140 Training loss 0.09494186192750931 Validation loss 0.10823550820350647 Accuracy 0.6655000448226929\n",
      "Iteration 3150 Training loss 0.08249514549970627 Validation loss 0.09893316775560379 Accuracy 0.7315000295639038\n",
      "Iteration 3160 Training loss 0.1129181906580925 Validation loss 0.11630232632160187 Accuracy 0.6485000252723694\n",
      "Iteration 3170 Training loss 0.10996652394533157 Validation loss 0.11623501777648926 Accuracy 0.6485000252723694\n",
      "Iteration 3180 Training loss 0.10308326780796051 Validation loss 0.10341913253068924 Accuracy 0.6790000200271606\n",
      "Iteration 3190 Training loss 0.0880339965224266 Validation loss 0.10306630283594131 Accuracy 0.7075000405311584\n",
      "Iteration 3200 Training loss 0.09997827559709549 Validation loss 0.09421918541193008 Accuracy 0.737500011920929\n",
      "Iteration 3210 Training loss 0.11449594050645828 Validation loss 0.11018367111682892 Accuracy 0.6720000505447388\n",
      "Iteration 3220 Training loss 0.11757364124059677 Validation loss 0.11284316331148148 Accuracy 0.6075000166893005\n",
      "Iteration 3230 Training loss 0.11130394041538239 Validation loss 0.0964701846241951 Accuracy 0.7125000357627869\n",
      "Iteration 3240 Training loss 0.08387695997953415 Validation loss 0.095732182264328 Accuracy 0.7095000147819519\n",
      "Iteration 3250 Training loss 0.10541122406721115 Validation loss 0.09798658639192581 Accuracy 0.7195000052452087\n",
      "Iteration 3260 Training loss 0.09056971222162247 Validation loss 0.09836795181035995 Accuracy 0.7140000462532043\n",
      "Iteration 3270 Training loss 0.10291345417499542 Validation loss 0.09473950415849686 Accuracy 0.7315000295639038\n",
      "Iteration 3280 Training loss 0.08660393208265305 Validation loss 0.09843903034925461 Accuracy 0.7100000381469727\n",
      "Iteration 3290 Training loss 0.1031210795044899 Validation loss 0.10270806401968002 Accuracy 0.6850000619888306\n",
      "Iteration 3300 Training loss 0.10360170155763626 Validation loss 0.10033232718706131 Accuracy 0.6875000596046448\n",
      "Iteration 3310 Training loss 0.1309213638305664 Validation loss 0.12101368606090546 Accuracy 0.5955000519752502\n",
      "Iteration 3320 Training loss 0.0980677455663681 Validation loss 0.09992539137601852 Accuracy 0.6845000386238098\n",
      "Iteration 3330 Training loss 0.09468812495470047 Validation loss 0.09612084180116653 Accuracy 0.7145000100135803\n",
      "Iteration 3340 Training loss 0.10512452572584152 Validation loss 0.09819549322128296 Accuracy 0.6950000524520874\n",
      "Iteration 3350 Training loss 0.0788891538977623 Validation loss 0.09926976263523102 Accuracy 0.6935000419616699\n",
      "Iteration 3360 Training loss 0.10971414297819138 Validation loss 0.10284750908613205 Accuracy 0.687000036239624\n",
      "Iteration 3370 Training loss 0.0892648696899414 Validation loss 0.09952569752931595 Accuracy 0.6820000410079956\n",
      "Iteration 3380 Training loss 0.1021246537566185 Validation loss 0.11320576816797256 Accuracy 0.6160000562667847\n",
      "Iteration 3390 Training loss 0.09587609767913818 Validation loss 0.0990055575966835 Accuracy 0.6925000548362732\n",
      "Iteration 3400 Training loss 0.09430950880050659 Validation loss 0.10167328268289566 Accuracy 0.6880000233650208\n",
      "Iteration 3410 Training loss 0.10976644605398178 Validation loss 0.09889088571071625 Accuracy 0.7050000429153442\n",
      "Iteration 3420 Training loss 0.09313555806875229 Validation loss 0.09714890271425247 Accuracy 0.7220000624656677\n",
      "Iteration 3430 Training loss 0.10600131750106812 Validation loss 0.10695625841617584 Accuracy 0.6615000367164612\n",
      "Iteration 3440 Training loss 0.12058977782726288 Validation loss 0.11416047811508179 Accuracy 0.6350000500679016\n",
      "Iteration 3450 Training loss 0.11597386747598648 Validation loss 0.11166536808013916 Accuracy 0.6500000357627869\n",
      "Iteration 3460 Training loss 0.10036265850067139 Validation loss 0.10480960458517075 Accuracy 0.6735000610351562\n",
      "Iteration 3470 Training loss 0.11384403705596924 Validation loss 0.11019288003444672 Accuracy 0.6620000600814819\n",
      "Iteration 3480 Training loss 0.11841686815023422 Validation loss 0.106296606361866 Accuracy 0.6805000305175781\n",
      "Iteration 3490 Training loss 0.11639188230037689 Validation loss 0.11204206198453903 Accuracy 0.6300000548362732\n",
      "Iteration 3500 Training loss 0.09054994583129883 Validation loss 0.10279407352209091 Accuracy 0.6810000538825989\n",
      "Iteration 3510 Training loss 0.09767896682024002 Validation loss 0.10252529382705688 Accuracy 0.6980000138282776\n",
      "Iteration 3520 Training loss 0.10006623715162277 Validation loss 0.0971398577094078 Accuracy 0.7130000591278076\n",
      "Iteration 3530 Training loss 0.0897083431482315 Validation loss 0.10526599735021591 Accuracy 0.6695000529289246\n",
      "Iteration 3540 Training loss 0.10008686780929565 Validation loss 0.10117428749799728 Accuracy 0.6910000443458557\n",
      "Iteration 3550 Training loss 0.09840362519025803 Validation loss 0.10006111860275269 Accuracy 0.7075000405311584\n",
      "Iteration 3560 Training loss 0.10580022633075714 Validation loss 0.0982603132724762 Accuracy 0.7145000100135803\n",
      "Iteration 3570 Training loss 0.12091242522001266 Validation loss 0.114891417324543 Accuracy 0.624500036239624\n",
      "Iteration 3580 Training loss 0.10112950205802917 Validation loss 0.10923172533512115 Accuracy 0.6480000019073486\n",
      "Iteration 3590 Training loss 0.11636247485876083 Validation loss 0.10602021217346191 Accuracy 0.6620000600814819\n",
      "Iteration 3600 Training loss 0.10960814356803894 Validation loss 0.09788210690021515 Accuracy 0.7225000262260437\n",
      "Iteration 3610 Training loss 0.10262415558099747 Validation loss 0.10057404637336731 Accuracy 0.7065000534057617\n",
      "Iteration 3620 Training loss 0.09962490200996399 Validation loss 0.10990184545516968 Accuracy 0.6345000267028809\n",
      "Iteration 3630 Training loss 0.09954732656478882 Validation loss 0.09519428014755249 Accuracy 0.7345000505447388\n",
      "Iteration 3640 Training loss 0.0951295718550682 Validation loss 0.0993003398180008 Accuracy 0.7005000114440918\n",
      "Iteration 3650 Training loss 0.09518685191869736 Validation loss 0.09854549914598465 Accuracy 0.7240000367164612\n",
      "Iteration 3660 Training loss 0.11477546393871307 Validation loss 0.10318883508443832 Accuracy 0.6925000548362732\n",
      "Iteration 3670 Training loss 0.09003093838691711 Validation loss 0.0982130765914917 Accuracy 0.7015000581741333\n",
      "Iteration 3680 Training loss 0.10737966001033783 Validation loss 0.1139400452375412 Accuracy 0.6110000014305115\n",
      "Iteration 3690 Training loss 0.10559047758579254 Validation loss 0.10477636009454727 Accuracy 0.6825000047683716\n",
      "Iteration 3700 Training loss 0.10042102634906769 Validation loss 0.09532193094491959 Accuracy 0.718000054359436\n",
      "Iteration 3710 Training loss 0.12303608655929565 Validation loss 0.11659462749958038 Accuracy 0.6565000414848328\n",
      "Iteration 3720 Training loss 0.09772921353578568 Validation loss 0.09647636860609055 Accuracy 0.7205000519752502\n",
      "Iteration 3730 Training loss 0.09630605578422546 Validation loss 0.10389938950538635 Accuracy 0.6780000329017639\n",
      "Iteration 3740 Training loss 0.10579580068588257 Validation loss 0.0989070013165474 Accuracy 0.6935000419616699\n",
      "Iteration 3750 Training loss 0.09404373168945312 Validation loss 0.10978958755731583 Accuracy 0.6330000162124634\n",
      "Iteration 3760 Training loss 0.09996774792671204 Validation loss 0.09539981931447983 Accuracy 0.7345000505447388\n",
      "Iteration 3770 Training loss 0.1008380651473999 Validation loss 0.09733714908361435 Accuracy 0.7135000228881836\n",
      "Iteration 3780 Training loss 0.11441855877637863 Validation loss 0.0977707952260971 Accuracy 0.718500018119812\n",
      "Iteration 3790 Training loss 0.1011647954583168 Validation loss 0.09881451725959778 Accuracy 0.70250004529953\n",
      "Iteration 3800 Training loss 0.16177493333816528 Validation loss 0.13620765507221222 Accuracy 0.5460000038146973\n",
      "Iteration 3810 Training loss 0.10623911768198013 Validation loss 0.09952729940414429 Accuracy 0.7225000262260437\n",
      "Iteration 3820 Training loss 0.09573706239461899 Validation loss 0.10500945150852203 Accuracy 0.671500027179718\n",
      "Iteration 3830 Training loss 0.09384246915578842 Validation loss 0.1028904914855957 Accuracy 0.6875000596046448\n",
      "Iteration 3840 Training loss 0.09796593338251114 Validation loss 0.09984170645475388 Accuracy 0.7145000100135803\n",
      "Iteration 3850 Training loss 0.09540429711341858 Validation loss 0.104984350502491 Accuracy 0.6740000247955322\n",
      "Iteration 3860 Training loss 0.10414645820856094 Validation loss 0.10779160261154175 Accuracy 0.675000011920929\n",
      "Iteration 3870 Training loss 0.09607469290494919 Validation loss 0.09559345245361328 Accuracy 0.7205000519752502\n",
      "Iteration 3880 Training loss 0.09604734182357788 Validation loss 0.10230197757482529 Accuracy 0.6880000233650208\n",
      "Iteration 3890 Training loss 0.11546197533607483 Validation loss 0.11502207070589066 Accuracy 0.6190000176429749\n",
      "Iteration 3900 Training loss 0.11128764599561691 Validation loss 0.10235119611024857 Accuracy 0.6885000467300415\n",
      "Iteration 3910 Training loss 0.09073901921510696 Validation loss 0.10016075521707535 Accuracy 0.733500063419342\n",
      "Iteration 3920 Training loss 0.09846769273281097 Validation loss 0.10691758245229721 Accuracy 0.6930000185966492\n",
      "Iteration 3930 Training loss 0.11416105926036835 Validation loss 0.09925302118062973 Accuracy 0.7090000510215759\n",
      "Iteration 3940 Training loss 0.09881328046321869 Validation loss 0.09912330657243729 Accuracy 0.6920000314712524\n",
      "Iteration 3950 Training loss 0.08548279851675034 Validation loss 0.1031729206442833 Accuracy 0.7015000581741333\n",
      "Iteration 3960 Training loss 0.11037422716617584 Validation loss 0.10422751307487488 Accuracy 0.6770000457763672\n",
      "Iteration 3970 Training loss 0.10648374259471893 Validation loss 0.09438495337963104 Accuracy 0.7475000619888306\n",
      "Iteration 3980 Training loss 0.11586034297943115 Validation loss 0.10385522991418839 Accuracy 0.6795000433921814\n",
      "Iteration 3990 Training loss 0.13966034352779388 Validation loss 0.1288682371377945 Accuracy 0.5540000200271606\n",
      "Iteration 4000 Training loss 0.11134152859449387 Validation loss 0.10496541112661362 Accuracy 0.6820000410079956\n",
      "Iteration 4010 Training loss 0.09216418117284775 Validation loss 0.10180435329675674 Accuracy 0.6875000596046448\n",
      "Iteration 4020 Training loss 0.10042992979288101 Validation loss 0.0980914905667305 Accuracy 0.7270000576972961\n",
      "Iteration 4030 Training loss 0.12470006942749023 Validation loss 0.11164210736751556 Accuracy 0.6285000443458557\n",
      "Iteration 4040 Training loss 0.08839087933301926 Validation loss 0.10218232870101929 Accuracy 0.7270000576972961\n",
      "Iteration 4050 Training loss 0.10870832204818726 Validation loss 0.11555485427379608 Accuracy 0.6315000057220459\n",
      "Iteration 4060 Training loss 0.10346993803977966 Validation loss 0.09982946515083313 Accuracy 0.6895000338554382\n",
      "Iteration 4070 Training loss 0.08872037380933762 Validation loss 0.09766604751348495 Accuracy 0.7305000424385071\n",
      "Iteration 4080 Training loss 0.09973148256540298 Validation loss 0.10382761806249619 Accuracy 0.6820000410079956\n",
      "Iteration 4090 Training loss 0.11030494421720505 Validation loss 0.12279276549816132 Accuracy 0.5950000286102295\n",
      "Iteration 4100 Training loss 0.08675584942102432 Validation loss 0.10173340141773224 Accuracy 0.6840000152587891\n",
      "Iteration 4110 Training loss 0.10565639287233353 Validation loss 0.10356763750314713 Accuracy 0.6680000424385071\n",
      "Iteration 4120 Training loss 0.09883639961481094 Validation loss 0.10145054012537003 Accuracy 0.7075000405311584\n",
      "Iteration 4130 Training loss 0.10035073757171631 Validation loss 0.10207708179950714 Accuracy 0.6850000619888306\n",
      "Iteration 4140 Training loss 0.11876484751701355 Validation loss 0.10405580699443817 Accuracy 0.6835000514984131\n",
      "Iteration 4150 Training loss 0.10716912150382996 Validation loss 0.09600578993558884 Accuracy 0.7275000214576721\n",
      "Iteration 4160 Training loss 0.07960866391658783 Validation loss 0.09485634416341782 Accuracy 0.7255000472068787\n",
      "Iteration 4170 Training loss 0.09550905227661133 Validation loss 0.09763864427804947 Accuracy 0.718500018119812\n",
      "Iteration 4180 Training loss 0.11919610947370529 Validation loss 0.11590535938739777 Accuracy 0.597000002861023\n",
      "Iteration 4190 Training loss 0.08560515940189362 Validation loss 0.09693501144647598 Accuracy 0.7105000615119934\n",
      "Iteration 4200 Training loss 0.108220174908638 Validation loss 0.10840936750173569 Accuracy 0.6515000462532043\n",
      "Iteration 4210 Training loss 0.11029190570116043 Validation loss 0.10576535016298294 Accuracy 0.675000011920929\n",
      "Iteration 4220 Training loss 0.0946287140250206 Validation loss 0.09316302835941315 Accuracy 0.7410000562667847\n",
      "Iteration 4230 Training loss 0.0998598113656044 Validation loss 0.10561459511518478 Accuracy 0.6755000352859497\n",
      "Iteration 4240 Training loss 0.10130409896373749 Validation loss 0.1027694121003151 Accuracy 0.6940000057220459\n",
      "Iteration 4250 Training loss 0.08322487026453018 Validation loss 0.09689828753471375 Accuracy 0.7140000462532043\n",
      "Iteration 4260 Training loss 0.09222133457660675 Validation loss 0.09343424439430237 Accuracy 0.733500063419342\n",
      "Iteration 4270 Training loss 0.09895816445350647 Validation loss 0.1000380590558052 Accuracy 0.7190000414848328\n",
      "Iteration 4280 Training loss 0.09523274004459381 Validation loss 0.09914951026439667 Accuracy 0.6955000162124634\n",
      "Iteration 4290 Training loss 0.1050882637500763 Validation loss 0.0993267074227333 Accuracy 0.7010000348091125\n",
      "Iteration 4300 Training loss 0.11835972219705582 Validation loss 0.10680696368217468 Accuracy 0.6525000333786011\n",
      "Iteration 4310 Training loss 0.10164645314216614 Validation loss 0.09976126253604889 Accuracy 0.690500020980835\n",
      "Iteration 4320 Training loss 0.09772519767284393 Validation loss 0.0976354330778122 Accuracy 0.703000009059906\n",
      "Iteration 4330 Training loss 0.09704675525426865 Validation loss 0.09811066091060638 Accuracy 0.7125000357627869\n",
      "Iteration 4340 Training loss 0.09518079459667206 Validation loss 0.10961207002401352 Accuracy 0.656000018119812\n",
      "Iteration 4350 Training loss 0.1071888878941536 Validation loss 0.10325779020786285 Accuracy 0.6835000514984131\n",
      "Iteration 4360 Training loss 0.10121284425258636 Validation loss 0.10632996261119843 Accuracy 0.6835000514984131\n",
      "Iteration 4370 Training loss 0.11015895754098892 Validation loss 0.10310889035463333 Accuracy 0.6885000467300415\n",
      "Iteration 4380 Training loss 0.10710925608873367 Validation loss 0.0972222164273262 Accuracy 0.7140000462532043\n",
      "Iteration 4390 Training loss 0.09973331540822983 Validation loss 0.09864899516105652 Accuracy 0.7055000066757202\n",
      "Iteration 4400 Training loss 0.13978460431098938 Validation loss 0.13056741654872894 Accuracy 0.6020000576972961\n",
      "Iteration 4410 Training loss 0.09369834512472153 Validation loss 0.10430454462766647 Accuracy 0.6940000057220459\n",
      "Iteration 4420 Training loss 0.09497366100549698 Validation loss 0.09746795147657394 Accuracy 0.7260000109672546\n",
      "Iteration 4430 Training loss 0.08761192113161087 Validation loss 0.09483128786087036 Accuracy 0.7280000448226929\n",
      "Iteration 4440 Training loss 0.10514908283948898 Validation loss 0.10176519304513931 Accuracy 0.7035000324249268\n",
      "Iteration 4450 Training loss 0.0918600931763649 Validation loss 0.09869225323200226 Accuracy 0.7140000462532043\n",
      "Iteration 4460 Training loss 0.1669739931821823 Validation loss 0.14626692235469818 Accuracy 0.5164999961853027\n",
      "Iteration 4470 Training loss 0.09498753398656845 Validation loss 0.09572768211364746 Accuracy 0.733500063419342\n",
      "Iteration 4480 Training loss 0.09572823345661163 Validation loss 0.107272207736969 Accuracy 0.6815000176429749\n",
      "Iteration 4490 Training loss 0.09659329801797867 Validation loss 0.10445715487003326 Accuracy 0.6880000233650208\n",
      "Iteration 4500 Training loss 0.08463120460510254 Validation loss 0.0959458276629448 Accuracy 0.7190000414848328\n",
      "Iteration 4510 Training loss 0.10498803853988647 Validation loss 0.10264288634061813 Accuracy 0.6810000538825989\n",
      "Iteration 4520 Training loss 0.10342265665531158 Validation loss 0.09515408426523209 Accuracy 0.7380000352859497\n",
      "Iteration 4530 Training loss 0.08168579638004303 Validation loss 0.10116151720285416 Accuracy 0.6945000290870667\n",
      "Iteration 4540 Training loss 0.11039499193429947 Validation loss 0.09905335307121277 Accuracy 0.706000030040741\n",
      "Iteration 4550 Training loss 0.0956944152712822 Validation loss 0.10582175850868225 Accuracy 0.6705000400543213\n",
      "Iteration 4560 Training loss 0.10900266468524933 Validation loss 0.10674193501472473 Accuracy 0.659500002861023\n",
      "Iteration 4570 Training loss 0.11590304225683212 Validation loss 0.11194510757923126 Accuracy 0.5985000133514404\n",
      "Iteration 4580 Training loss 0.09348106384277344 Validation loss 0.10430635511875153 Accuracy 0.6765000224113464\n",
      "Iteration 4590 Training loss 0.1263975352048874 Validation loss 0.11416170746088028 Accuracy 0.6185000538825989\n",
      "Iteration 4600 Training loss 0.11963977664709091 Validation loss 0.12328360229730606 Accuracy 0.5635000467300415\n",
      "Iteration 4610 Training loss 0.09439884126186371 Validation loss 0.10050646215677261 Accuracy 0.7055000066757202\n",
      "Iteration 4620 Training loss 0.09600993245840073 Validation loss 0.10071185231208801 Accuracy 0.6995000243186951\n",
      "Iteration 4630 Training loss 0.1054164320230484 Validation loss 0.10159268230199814 Accuracy 0.690000057220459\n",
      "Iteration 4640 Training loss 0.09114638715982437 Validation loss 0.09724976867437363 Accuracy 0.7325000166893005\n",
      "Iteration 4650 Training loss 0.10199670493602753 Validation loss 0.1028297021985054 Accuracy 0.6790000200271606\n",
      "Iteration 4660 Training loss 0.11902986466884613 Validation loss 0.09983646869659424 Accuracy 0.7120000123977661\n",
      "Iteration 4670 Training loss 0.08708620071411133 Validation loss 0.10287322103977203 Accuracy 0.6815000176429749\n",
      "Iteration 4680 Training loss 0.09406717121601105 Validation loss 0.10466568171977997 Accuracy 0.6755000352859497\n",
      "Iteration 4690 Training loss 0.11198247969150543 Validation loss 0.10663480311632156 Accuracy 0.6655000448226929\n",
      "Iteration 4700 Training loss 0.1019960343837738 Validation loss 0.09702861309051514 Accuracy 0.7250000238418579\n",
      "Iteration 4710 Training loss 0.13759705424308777 Validation loss 0.1350957751274109 Accuracy 0.5360000133514404\n",
      "Iteration 4720 Training loss 0.14047691226005554 Validation loss 0.12774446606636047 Accuracy 0.5470000505447388\n",
      "Iteration 4730 Training loss 0.09674528986215591 Validation loss 0.10456879436969757 Accuracy 0.6690000295639038\n",
      "Iteration 4740 Training loss 0.10929851233959198 Validation loss 0.10214444249868393 Accuracy 0.6895000338554382\n",
      "Iteration 4750 Training loss 0.10027135908603668 Validation loss 0.09999892115592957 Accuracy 0.6980000138282776\n",
      "Iteration 4760 Training loss 0.1172967329621315 Validation loss 0.0990188792347908 Accuracy 0.6935000419616699\n",
      "Iteration 4770 Training loss 0.1056634709239006 Validation loss 0.09611563384532928 Accuracy 0.7330000400543213\n",
      "Iteration 4780 Training loss 0.10512155294418335 Validation loss 0.09976479411125183 Accuracy 0.70250004529953\n",
      "Iteration 4790 Training loss 0.10699164122343063 Validation loss 0.10074254870414734 Accuracy 0.6955000162124634\n",
      "Iteration 4800 Training loss 0.09366404265165329 Validation loss 0.09932351857423782 Accuracy 0.6995000243186951\n",
      "Iteration 4810 Training loss 0.11362220346927643 Validation loss 0.11682049185037613 Accuracy 0.6215000152587891\n",
      "Iteration 4820 Training loss 0.1002468466758728 Validation loss 0.09818912297487259 Accuracy 0.7080000042915344\n",
      "Iteration 4830 Training loss 0.10282119363546371 Validation loss 0.09657631069421768 Accuracy 0.7290000319480896\n",
      "Iteration 4840 Training loss 0.09929747134447098 Validation loss 0.10113885998725891 Accuracy 0.7190000414848328\n",
      "Iteration 4850 Training loss 0.09381095319986343 Validation loss 0.09414605051279068 Accuracy 0.7315000295639038\n",
      "Iteration 4860 Training loss 0.0906398594379425 Validation loss 0.09666416794061661 Accuracy 0.734000027179718\n",
      "Iteration 4870 Training loss 0.10674876719713211 Validation loss 0.09793170541524887 Accuracy 0.7245000600814819\n",
      "Iteration 4880 Training loss 0.11931829154491425 Validation loss 0.12375963479280472 Accuracy 0.6065000295639038\n",
      "Iteration 4890 Training loss 0.1026579812169075 Validation loss 0.09607880562543869 Accuracy 0.7205000519752502\n",
      "Iteration 4900 Training loss 0.08764568716287613 Validation loss 0.09638112038373947 Accuracy 0.7170000076293945\n",
      "Iteration 4910 Training loss 0.09383948892354965 Validation loss 0.09392941743135452 Accuracy 0.7395000457763672\n",
      "Iteration 4920 Training loss 0.1014409065246582 Validation loss 0.09778084605932236 Accuracy 0.7165000438690186\n",
      "Iteration 4930 Training loss 0.08475566655397415 Validation loss 0.09621064364910126 Accuracy 0.721500039100647\n",
      "Iteration 4940 Training loss 0.08893786370754242 Validation loss 0.096443310379982 Accuracy 0.7055000066757202\n",
      "Iteration 4950 Training loss 0.0926266461610794 Validation loss 0.09613019227981567 Accuracy 0.7135000228881836\n",
      "Iteration 4960 Training loss 0.08936229348182678 Validation loss 0.0910957083106041 Accuracy 0.7500000596046448\n",
      "Iteration 4970 Training loss 0.09260126203298569 Validation loss 0.0938466340303421 Accuracy 0.7405000329017639\n",
      "Iteration 4980 Training loss 0.10670828819274902 Validation loss 0.09328312426805496 Accuracy 0.7390000224113464\n",
      "Iteration 4990 Training loss 0.13542388379573822 Validation loss 0.15191717445850372 Accuracy 0.5700000524520874\n",
      "Iteration 5000 Training loss 0.1317627727985382 Validation loss 0.11616955697536469 Accuracy 0.6065000295639038\n",
      "Iteration 5010 Training loss 0.11572086811065674 Validation loss 0.11082343012094498 Accuracy 0.6315000057220459\n",
      "Iteration 5020 Training loss 0.0811508372426033 Validation loss 0.09776213765144348 Accuracy 0.7125000357627869\n",
      "Iteration 5030 Training loss 0.12962020933628082 Validation loss 0.11450856924057007 Accuracy 0.6075000166893005\n",
      "Iteration 5040 Training loss 0.11256656050682068 Validation loss 0.10105191171169281 Accuracy 0.7005000114440918\n",
      "Iteration 5050 Training loss 0.09580297023057938 Validation loss 0.11139306426048279 Accuracy 0.6505000591278076\n",
      "Iteration 5060 Training loss 0.08294353634119034 Validation loss 0.1013176366686821 Accuracy 0.7135000228881836\n",
      "Iteration 5070 Training loss 0.10512716323137283 Validation loss 0.11119841784238815 Accuracy 0.6210000514984131\n",
      "Iteration 5080 Training loss 0.10143836587667465 Validation loss 0.09693608433008194 Accuracy 0.7130000591278076\n",
      "Iteration 5090 Training loss 0.11949962377548218 Validation loss 0.11983045190572739 Accuracy 0.5685000419616699\n",
      "Iteration 5100 Training loss 0.13699831068515778 Validation loss 0.14361627399921417 Accuracy 0.5095000267028809\n",
      "Iteration 5110 Training loss 0.0975479930639267 Validation loss 0.10185012221336365 Accuracy 0.675000011920929\n",
      "Iteration 5120 Training loss 0.10976473242044449 Validation loss 0.11889835447072983 Accuracy 0.593500018119812\n",
      "Iteration 5130 Training loss 0.10712814331054688 Validation loss 0.10893655568361282 Accuracy 0.6525000333786011\n",
      "Iteration 5140 Training loss 0.11066710948944092 Validation loss 0.10208964347839355 Accuracy 0.7035000324249268\n",
      "Iteration 5150 Training loss 0.1379462629556656 Validation loss 0.10956353694200516 Accuracy 0.6910000443458557\n",
      "Iteration 5160 Training loss 0.1090417429804802 Validation loss 0.10429771989583969 Accuracy 0.671500027179718\n",
      "Iteration 5170 Training loss 0.10114257782697678 Validation loss 0.09793022274971008 Accuracy 0.7285000085830688\n",
      "Iteration 5180 Training loss 0.0996946319937706 Validation loss 0.10333850234746933 Accuracy 0.687000036239624\n",
      "Iteration 5190 Training loss 0.10673537105321884 Validation loss 0.10193273425102234 Accuracy 0.6910000443458557\n",
      "Iteration 5200 Training loss 0.09233228117227554 Validation loss 0.09661318361759186 Accuracy 0.7305000424385071\n",
      "Iteration 5210 Training loss 0.11922769248485565 Validation loss 0.10473275929689407 Accuracy 0.6765000224113464\n",
      "Iteration 5220 Training loss 0.1145448163151741 Validation loss 0.11503048986196518 Accuracy 0.6380000114440918\n",
      "Iteration 5230 Training loss 0.10506712645292282 Validation loss 0.09318559616804123 Accuracy 0.7435000538825989\n",
      "Iteration 5240 Training loss 0.11037822812795639 Validation loss 0.11604490876197815 Accuracy 0.6060000061988831\n",
      "Iteration 5250 Training loss 0.10660913586616516 Validation loss 0.1054036021232605 Accuracy 0.6655000448226929\n",
      "Iteration 5260 Training loss 0.09939341247081757 Validation loss 0.10564740747213364 Accuracy 0.6680000424385071\n",
      "Iteration 5270 Training loss 0.10048165917396545 Validation loss 0.10077020525932312 Accuracy 0.70250004529953\n",
      "Iteration 5280 Training loss 0.10376019030809402 Validation loss 0.10884854942560196 Accuracy 0.6465000510215759\n",
      "Iteration 5290 Training loss 0.09418191760778427 Validation loss 0.09566953033208847 Accuracy 0.7465000152587891\n",
      "Iteration 5300 Training loss 0.10280024260282516 Validation loss 0.10347428172826767 Accuracy 0.6880000233650208\n",
      "Iteration 5310 Training loss 0.09991279244422913 Validation loss 0.10805919766426086 Accuracy 0.6550000309944153\n",
      "Iteration 5320 Training loss 0.10461351275444031 Validation loss 0.10750832408666611 Accuracy 0.659000039100647\n",
      "Iteration 5330 Training loss 0.07717135548591614 Validation loss 0.0959523394703865 Accuracy 0.7395000457763672\n",
      "Iteration 5340 Training loss 0.13343924283981323 Validation loss 0.13165190815925598 Accuracy 0.5910000205039978\n",
      "Iteration 5350 Training loss 0.10389728099107742 Validation loss 0.10129217803478241 Accuracy 0.6935000419616699\n",
      "Iteration 5360 Training loss 0.11243896186351776 Validation loss 0.0979183167219162 Accuracy 0.703000009059906\n",
      "Iteration 5370 Training loss 0.09852191805839539 Validation loss 0.10271605104207993 Accuracy 0.6840000152587891\n",
      "Iteration 5380 Training loss 0.11955364048480988 Validation loss 0.1127266213297844 Accuracy 0.6380000114440918\n",
      "Iteration 5390 Training loss 0.08829998970031738 Validation loss 0.09823364019393921 Accuracy 0.7325000166893005\n",
      "Iteration 5400 Training loss 0.09259936958551407 Validation loss 0.10060515999794006 Accuracy 0.7210000157356262\n",
      "Iteration 5410 Training loss 0.10093030333518982 Validation loss 0.10224759578704834 Accuracy 0.7050000429153442\n",
      "Iteration 5420 Training loss 0.1072205975651741 Validation loss 0.1062396764755249 Accuracy 0.675000011920929\n",
      "Iteration 5430 Training loss 0.0978737324476242 Validation loss 0.09884443879127502 Accuracy 0.7250000238418579\n",
      "Iteration 5440 Training loss 0.11055023968219757 Validation loss 0.10007725656032562 Accuracy 0.7050000429153442\n",
      "Iteration 5450 Training loss 0.12198986858129501 Validation loss 0.11577922105789185 Accuracy 0.6320000290870667\n",
      "Iteration 5460 Training loss 0.1130031943321228 Validation loss 0.13206526637077332 Accuracy 0.5715000033378601\n",
      "Iteration 5470 Training loss 0.09458226710557938 Validation loss 0.09973616153001785 Accuracy 0.7075000405311584\n",
      "Iteration 5480 Training loss 0.1126742735505104 Validation loss 0.10527147352695465 Accuracy 0.6695000529289246\n",
      "Iteration 5490 Training loss 0.075401671230793 Validation loss 0.09894203394651413 Accuracy 0.6990000605583191\n",
      "Iteration 5500 Training loss 0.09716137498617172 Validation loss 0.09918016195297241 Accuracy 0.6980000138282776\n",
      "Iteration 5510 Training loss 0.11482425779104233 Validation loss 0.10626747459173203 Accuracy 0.6720000505447388\n",
      "Iteration 5520 Training loss 0.10836227238178253 Validation loss 0.11122137308120728 Accuracy 0.6370000243186951\n",
      "Iteration 5530 Training loss 0.11959495395421982 Validation loss 0.11745279282331467 Accuracy 0.6095000505447388\n",
      "Iteration 5540 Training loss 0.12903860211372375 Validation loss 0.128024622797966 Accuracy 0.5910000205039978\n",
      "Iteration 5550 Training loss 0.10994422435760498 Validation loss 0.10466080158948898 Accuracy 0.6850000619888306\n",
      "Iteration 5560 Training loss 0.09668239206075668 Validation loss 0.10340385138988495 Accuracy 0.6805000305175781\n",
      "Iteration 5570 Training loss 0.08716437220573425 Validation loss 0.09511582553386688 Accuracy 0.7255000472068787\n",
      "Iteration 5580 Training loss 0.12577290832996368 Validation loss 0.1209651529788971 Accuracy 0.6140000224113464\n",
      "Iteration 5590 Training loss 0.09635154157876968 Validation loss 0.10029144585132599 Accuracy 0.6960000395774841\n",
      "Iteration 5600 Training loss 0.09454772621393204 Validation loss 0.09768031537532806 Accuracy 0.7175000309944153\n",
      "Iteration 5610 Training loss 0.0917489230632782 Validation loss 0.09808704257011414 Accuracy 0.7175000309944153\n",
      "Iteration 5620 Training loss 0.10911715030670166 Validation loss 0.10365737229585648 Accuracy 0.6725000143051147\n",
      "Iteration 5630 Training loss 0.11186640709638596 Validation loss 0.10220764577388763 Accuracy 0.690500020980835\n",
      "Iteration 5640 Training loss 0.16438636183738708 Validation loss 0.15721319615840912 Accuracy 0.5560000538825989\n",
      "Iteration 5650 Training loss 0.10193780809640884 Validation loss 0.09950222074985504 Accuracy 0.7100000381469727\n",
      "Iteration 5660 Training loss 0.10404279828071594 Validation loss 0.09915869683027267 Accuracy 0.7275000214576721\n",
      "Iteration 5670 Training loss 0.09518472105264664 Validation loss 0.09676086902618408 Accuracy 0.7140000462532043\n",
      "Iteration 5680 Training loss 0.09099607914686203 Validation loss 0.09808874130249023 Accuracy 0.7270000576972961\n",
      "Iteration 5690 Training loss 0.10201280564069748 Validation loss 0.10699290782213211 Accuracy 0.6650000214576721\n",
      "Iteration 5700 Training loss 0.10888707637786865 Validation loss 0.10346393287181854 Accuracy 0.6725000143051147\n",
      "Iteration 5710 Training loss 0.10378510504961014 Validation loss 0.11017405241727829 Accuracy 0.6330000162124634\n",
      "Iteration 5720 Training loss 0.1241234540939331 Validation loss 0.124760702252388 Accuracy 0.6100000143051147\n",
      "Iteration 5730 Training loss 0.09204661846160889 Validation loss 0.10526835918426514 Accuracy 0.6760000586509705\n",
      "Iteration 5740 Training loss 0.13239935040473938 Validation loss 0.1170244812965393 Accuracy 0.5870000123977661\n",
      "Iteration 5750 Training loss 0.0981321707367897 Validation loss 0.09941110014915466 Accuracy 0.7020000219345093\n",
      "Iteration 5760 Training loss 0.11844854801893234 Validation loss 0.11694200336933136 Accuracy 0.5705000162124634\n",
      "Iteration 5770 Training loss 0.09503136575222015 Validation loss 0.10800688713788986 Accuracy 0.6785000562667847\n",
      "Iteration 5780 Training loss 0.09816871583461761 Validation loss 0.09871286153793335 Accuracy 0.7255000472068787\n",
      "Iteration 5790 Training loss 0.09667810052633286 Validation loss 0.09629474580287933 Accuracy 0.7285000085830688\n",
      "Iteration 5800 Training loss 0.09970314055681229 Validation loss 0.09470804035663605 Accuracy 0.7320000529289246\n",
      "Iteration 5810 Training loss 0.1086774542927742 Validation loss 0.0975356474518776 Accuracy 0.7105000615119934\n",
      "Iteration 5820 Training loss 0.07932591438293457 Validation loss 0.09656359255313873 Accuracy 0.734000027179718\n",
      "Iteration 5830 Training loss 0.132425919175148 Validation loss 0.1220247820019722 Accuracy 0.6165000200271606\n",
      "Iteration 5840 Training loss 0.0996088758111 Validation loss 0.10324268043041229 Accuracy 0.7155000567436218\n",
      "Iteration 5850 Training loss 0.10725005716085434 Validation loss 0.09723497927188873 Accuracy 0.718000054359436\n",
      "Iteration 5860 Training loss 0.10774073004722595 Validation loss 0.10633919388055801 Accuracy 0.6690000295639038\n",
      "Iteration 5870 Training loss 0.09075065702199936 Validation loss 0.10245491564273834 Accuracy 0.6885000467300415\n",
      "Iteration 5880 Training loss 0.09971708059310913 Validation loss 0.09825994819402695 Accuracy 0.7055000066757202\n",
      "Iteration 5890 Training loss 0.1035008579492569 Validation loss 0.09521131217479706 Accuracy 0.7265000343322754\n",
      "Iteration 5900 Training loss 0.11152894794940948 Validation loss 0.10668181627988815 Accuracy 0.6630000472068787\n",
      "Iteration 5910 Training loss 0.10232910513877869 Validation loss 0.09873523563146591 Accuracy 0.7125000357627869\n",
      "Iteration 5920 Training loss 0.103871650993824 Validation loss 0.10652132332324982 Accuracy 0.6670000553131104\n",
      "Iteration 5930 Training loss 0.10183760523796082 Validation loss 0.10401193052530289 Accuracy 0.6705000400543213\n",
      "Iteration 5940 Training loss 0.1054830402135849 Validation loss 0.10453861206769943 Accuracy 0.6770000457763672\n",
      "Iteration 5950 Training loss 0.10404890775680542 Validation loss 0.10126074403524399 Accuracy 0.6925000548362732\n",
      "Iteration 5960 Training loss 0.0963238999247551 Validation loss 0.10233798623085022 Accuracy 0.6810000538825989\n",
      "Iteration 5970 Training loss 0.11022429913282394 Validation loss 0.10655321180820465 Accuracy 0.6610000133514404\n",
      "Iteration 5980 Training loss 0.10395337641239166 Validation loss 0.09959159046411514 Accuracy 0.7055000066757202\n",
      "Iteration 5990 Training loss 0.10916595160961151 Validation loss 0.11162640899419785 Accuracy 0.6650000214576721\n",
      "Iteration 6000 Training loss 0.10752595961093903 Validation loss 0.102617546916008 Accuracy 0.7010000348091125\n",
      "Iteration 6010 Training loss 0.09430858492851257 Validation loss 0.10328397899866104 Accuracy 0.6945000290870667\n",
      "Iteration 6020 Training loss 0.10363033413887024 Validation loss 0.0984470546245575 Accuracy 0.7120000123977661\n",
      "Iteration 6030 Training loss 0.09580869227647781 Validation loss 0.09592501819133759 Accuracy 0.7250000238418579\n",
      "Iteration 6040 Training loss 0.09383566677570343 Validation loss 0.09620316326618195 Accuracy 0.7155000567436218\n",
      "Iteration 6050 Training loss 0.10990210622549057 Validation loss 0.10144173353910446 Accuracy 0.6925000548362732\n",
      "Iteration 6060 Training loss 0.09808370471000671 Validation loss 0.09603337198495865 Accuracy 0.7250000238418579\n",
      "Iteration 6070 Training loss 0.0944121703505516 Validation loss 0.10827397555112839 Accuracy 0.659000039100647\n",
      "Iteration 6080 Training loss 0.11132602393627167 Validation loss 0.11273855715990067 Accuracy 0.6265000104904175\n",
      "Iteration 6090 Training loss 0.0938306599855423 Validation loss 0.10127906501293182 Accuracy 0.6920000314712524\n",
      "Iteration 6100 Training loss 0.0903838723897934 Validation loss 0.0988350585103035 Accuracy 0.7275000214576721\n",
      "Iteration 6110 Training loss 0.09493924677371979 Validation loss 0.09526010602712631 Accuracy 0.7270000576972961\n",
      "Iteration 6120 Training loss 0.0988333597779274 Validation loss 0.1076909527182579 Accuracy 0.6820000410079956\n",
      "Iteration 6130 Training loss 0.10004626214504242 Validation loss 0.0984078124165535 Accuracy 0.7355000376701355\n",
      "Iteration 6140 Training loss 0.09478653967380524 Validation loss 0.09798528254032135 Accuracy 0.7095000147819519\n",
      "Iteration 6150 Training loss 0.10904040932655334 Validation loss 0.10104914009571075 Accuracy 0.6960000395774841\n",
      "Iteration 6160 Training loss 0.1244182214140892 Validation loss 0.11127728968858719 Accuracy 0.625\n",
      "Iteration 6170 Training loss 0.0949254035949707 Validation loss 0.1044626235961914 Accuracy 0.6720000505447388\n",
      "Iteration 6180 Training loss 0.08524905890226364 Validation loss 0.09535980969667435 Accuracy 0.7260000109672546\n",
      "Iteration 6190 Training loss 0.07936276495456696 Validation loss 0.0934942215681076 Accuracy 0.7430000305175781\n",
      "Iteration 6200 Training loss 0.08810080587863922 Validation loss 0.09619081765413284 Accuracy 0.7260000109672546\n",
      "Iteration 6210 Training loss 0.10505374521017075 Validation loss 0.10916982591152191 Accuracy 0.6430000066757202\n",
      "Iteration 6220 Training loss 0.11612153798341751 Validation loss 0.10852520912885666 Accuracy 0.6665000319480896\n",
      "Iteration 6230 Training loss 0.10051204264163971 Validation loss 0.1036766916513443 Accuracy 0.6845000386238098\n",
      "Iteration 6240 Training loss 0.10088613629341125 Validation loss 0.09835827350616455 Accuracy 0.7170000076293945\n",
      "Iteration 6250 Training loss 0.09639059007167816 Validation loss 0.09850767999887466 Accuracy 0.6980000138282776\n",
      "Iteration 6260 Training loss 0.09442729502916336 Validation loss 0.09533523768186569 Accuracy 0.7285000085830688\n",
      "Iteration 6270 Training loss 0.11227744817733765 Validation loss 0.11855798959732056 Accuracy 0.6130000352859497\n",
      "Iteration 6280 Training loss 0.09229350835084915 Validation loss 0.10180441290140152 Accuracy 0.6795000433921814\n",
      "Iteration 6290 Training loss 0.10039232671260834 Validation loss 0.09817219525575638 Accuracy 0.7300000190734863\n",
      "Iteration 6300 Training loss 0.11349879950284958 Validation loss 0.12322983145713806 Accuracy 0.5490000247955322\n",
      "Iteration 6310 Training loss 0.1100781112909317 Validation loss 0.11636810749769211 Accuracy 0.6425000429153442\n",
      "Iteration 6320 Training loss 0.10956934094429016 Validation loss 0.11024825274944305 Accuracy 0.6565000414848328\n",
      "Iteration 6330 Training loss 0.08755119889974594 Validation loss 0.09722572565078735 Accuracy 0.7170000076293945\n",
      "Iteration 6340 Training loss 0.13148058950901031 Validation loss 0.1347293108701706 Accuracy 0.534000039100647\n",
      "Iteration 6350 Training loss 0.09055115282535553 Validation loss 0.10383045673370361 Accuracy 0.6690000295639038\n",
      "Iteration 6360 Training loss 0.0823807492852211 Validation loss 0.09725365787744522 Accuracy 0.721500039100647\n",
      "Iteration 6370 Training loss 0.09422770887613297 Validation loss 0.09547571837902069 Accuracy 0.7220000624656677\n",
      "Iteration 6380 Training loss 0.10221841931343079 Validation loss 0.10135374963283539 Accuracy 0.6845000386238098\n",
      "Iteration 6390 Training loss 0.09868953377008438 Validation loss 0.09944096952676773 Accuracy 0.7010000348091125\n",
      "Iteration 6400 Training loss 0.12271124869585037 Validation loss 0.11100742220878601 Accuracy 0.6320000290870667\n",
      "Iteration 6410 Training loss 0.1319049447774887 Validation loss 0.10488255321979523 Accuracy 0.6780000329017639\n",
      "Iteration 6420 Training loss 0.139946848154068 Validation loss 0.13774637877941132 Accuracy 0.5980000495910645\n",
      "Iteration 6430 Training loss 0.09310528635978699 Validation loss 0.09577807039022446 Accuracy 0.7415000200271606\n",
      "Iteration 6440 Training loss 0.08533012866973877 Validation loss 0.09603004902601242 Accuracy 0.7330000400543213\n",
      "Iteration 6450 Training loss 0.08483229577541351 Validation loss 0.09705337882041931 Accuracy 0.7195000052452087\n",
      "Iteration 6460 Training loss 0.09009309858083725 Validation loss 0.10095120966434479 Accuracy 0.6980000138282776\n",
      "Iteration 6470 Training loss 0.09301219880580902 Validation loss 0.10013002157211304 Accuracy 0.6940000057220459\n",
      "Iteration 6480 Training loss 0.09225598722696304 Validation loss 0.10577505081892014 Accuracy 0.6700000166893005\n",
      "Iteration 6490 Training loss 0.0977180078625679 Validation loss 0.09928416460752487 Accuracy 0.7160000205039978\n",
      "Iteration 6500 Training loss 0.09148719906806946 Validation loss 0.09423749148845673 Accuracy 0.737500011920929\n",
      "Iteration 6510 Training loss 0.12721268832683563 Validation loss 0.13258884847164154 Accuracy 0.5450000166893005\n",
      "Iteration 6520 Training loss 0.1100323274731636 Validation loss 0.1075548604130745 Accuracy 0.6955000162124634\n",
      "Iteration 6530 Training loss 0.11772947013378143 Validation loss 0.1185685470700264 Accuracy 0.5795000195503235\n",
      "Iteration 6540 Training loss 0.09839259833097458 Validation loss 0.09515944123268127 Accuracy 0.7355000376701355\n",
      "Iteration 6550 Training loss 0.10071580857038498 Validation loss 0.10258466750383377 Accuracy 0.6805000305175781\n",
      "Iteration 6560 Training loss 0.10214538127183914 Validation loss 0.09948363155126572 Accuracy 0.6895000338554382\n",
      "Iteration 6570 Training loss 0.09850574284791946 Validation loss 0.09530734270811081 Accuracy 0.7230000495910645\n",
      "Iteration 6580 Training loss 0.11287008225917816 Validation loss 0.1069047749042511 Accuracy 0.6635000109672546\n",
      "Iteration 6590 Training loss 0.09605575352907181 Validation loss 0.10178252309560776 Accuracy 0.6805000305175781\n",
      "Iteration 6600 Training loss 0.0957142785191536 Validation loss 0.10238033533096313 Accuracy 0.690000057220459\n",
      "Iteration 6610 Training loss 0.08181021362543106 Validation loss 0.09630778431892395 Accuracy 0.7210000157356262\n",
      "Iteration 6620 Training loss 0.10093971341848373 Validation loss 0.09656713902950287 Accuracy 0.718000054359436\n",
      "Iteration 6630 Training loss 0.10210524499416351 Validation loss 0.10179565101861954 Accuracy 0.6850000619888306\n",
      "Iteration 6640 Training loss 0.07569511234760284 Validation loss 0.0941387340426445 Accuracy 0.7295000553131104\n",
      "Iteration 6650 Training loss 0.09660228341817856 Validation loss 0.09587036818265915 Accuracy 0.7230000495910645\n",
      "Iteration 6660 Training loss 0.1015549823641777 Validation loss 0.10020054131746292 Accuracy 0.6840000152587891\n",
      "Iteration 6670 Training loss 0.09382401406764984 Validation loss 0.10007591545581818 Accuracy 0.6975000500679016\n",
      "Iteration 6680 Training loss 0.09396857768297195 Validation loss 0.09276140481233597 Accuracy 0.7330000400543213\n",
      "Iteration 6690 Training loss 0.1289014220237732 Validation loss 0.12384531646966934 Accuracy 0.5695000290870667\n",
      "Iteration 6700 Training loss 0.10715259611606598 Validation loss 0.1060902327299118 Accuracy 0.6785000562667847\n",
      "Iteration 6710 Training loss 0.11249135434627533 Validation loss 0.09932059049606323 Accuracy 0.6960000395774841\n",
      "Iteration 6720 Training loss 0.11212524026632309 Validation loss 0.10490617901086807 Accuracy 0.6670000553131104\n",
      "Iteration 6730 Training loss 0.0877862349152565 Validation loss 0.0986836701631546 Accuracy 0.7065000534057617\n",
      "Iteration 6740 Training loss 0.09940890222787857 Validation loss 0.10528059303760529 Accuracy 0.6790000200271606\n",
      "Iteration 6750 Training loss 0.11396045237779617 Validation loss 0.10253144055604935 Accuracy 0.6845000386238098\n",
      "Iteration 6760 Training loss 0.09603068977594376 Validation loss 0.09530746936798096 Accuracy 0.737000048160553\n",
      "Iteration 6770 Training loss 0.10714825242757797 Validation loss 0.09392601996660233 Accuracy 0.734000027179718\n",
      "Iteration 6780 Training loss 0.09985242784023285 Validation loss 0.11334679275751114 Accuracy 0.628000020980835\n",
      "Iteration 6790 Training loss 0.08736489713191986 Validation loss 0.09575731307268143 Accuracy 0.7235000133514404\n",
      "Iteration 6800 Training loss 0.1004009023308754 Validation loss 0.10803593695163727 Accuracy 0.6730000376701355\n",
      "Iteration 6810 Training loss 0.11785021424293518 Validation loss 0.10755681246519089 Accuracy 0.6630000472068787\n",
      "Iteration 6820 Training loss 0.09633123129606247 Validation loss 0.10918090492486954 Accuracy 0.6600000262260437\n",
      "Iteration 6830 Training loss 0.10176338255405426 Validation loss 0.09380969405174255 Accuracy 0.7330000400543213\n",
      "Iteration 6840 Training loss 0.0934956818819046 Validation loss 0.09606009721755981 Accuracy 0.7170000076293945\n",
      "Iteration 6850 Training loss 0.10631689429283142 Validation loss 0.10443315654993057 Accuracy 0.6785000562667847\n",
      "Iteration 6860 Training loss 0.10432178527116776 Validation loss 0.10288696736097336 Accuracy 0.6840000152587891\n",
      "Iteration 6870 Training loss 0.11909156292676926 Validation loss 0.10370845347642899 Accuracy 0.6875000596046448\n",
      "Iteration 6880 Training loss 0.0870124027132988 Validation loss 0.09220745414495468 Accuracy 0.7380000352859497\n",
      "Iteration 6890 Training loss 0.10429811477661133 Validation loss 0.11072984337806702 Accuracy 0.6345000267028809\n",
      "Iteration 6900 Training loss 0.09585004299879074 Validation loss 0.09880445897579193 Accuracy 0.6950000524520874\n",
      "Iteration 6910 Training loss 0.11706788837909698 Validation loss 0.10061688721179962 Accuracy 0.6950000524520874\n",
      "Iteration 6920 Training loss 0.09134180098772049 Validation loss 0.0918019637465477 Accuracy 0.7430000305175781\n",
      "Iteration 6930 Training loss 0.08623236417770386 Validation loss 0.09659136831760406 Accuracy 0.7105000615119934\n",
      "Iteration 6940 Training loss 0.09463950991630554 Validation loss 0.09765440225601196 Accuracy 0.7175000309944153\n",
      "Iteration 6950 Training loss 0.10362078249454498 Validation loss 0.11846277117729187 Accuracy 0.6320000290870667\n",
      "Iteration 6960 Training loss 0.08161880075931549 Validation loss 0.09335846453905106 Accuracy 0.7365000247955322\n",
      "Iteration 6970 Training loss 0.09857747703790665 Validation loss 0.10243035107851028 Accuracy 0.6890000104904175\n",
      "Iteration 6980 Training loss 0.11128835380077362 Validation loss 0.10654239356517792 Accuracy 0.6585000157356262\n",
      "Iteration 6990 Training loss 0.09460140764713287 Validation loss 0.09740409255027771 Accuracy 0.7245000600814819\n",
      "Iteration 7000 Training loss 0.09333741664886475 Validation loss 0.09680734574794769 Accuracy 0.7245000600814819\n",
      "Iteration 7010 Training loss 0.11080338060855865 Validation loss 0.10458790510892868 Accuracy 0.6535000205039978\n",
      "Iteration 7020 Training loss 0.09128550440073013 Validation loss 0.0939706563949585 Accuracy 0.7390000224113464\n",
      "Iteration 7030 Training loss 0.09666406363248825 Validation loss 0.09844037145376205 Accuracy 0.6980000138282776\n",
      "Iteration 7040 Training loss 0.08971698582172394 Validation loss 0.09749516099691391 Accuracy 0.718000054359436\n",
      "Iteration 7050 Training loss 0.10544580221176147 Validation loss 0.098769411444664 Accuracy 0.7040000557899475\n",
      "Iteration 7060 Training loss 0.08158572018146515 Validation loss 0.09386202692985535 Accuracy 0.718500018119812\n",
      "Iteration 7070 Training loss 0.11341556906700134 Validation loss 0.1061069518327713 Accuracy 0.6910000443458557\n",
      "Iteration 7080 Training loss 0.10180110484361649 Validation loss 0.09353526681661606 Accuracy 0.7310000061988831\n",
      "Iteration 7090 Training loss 0.09634562581777573 Validation loss 0.095777228474617 Accuracy 0.7070000171661377\n",
      "Iteration 7100 Training loss 0.1092652678489685 Validation loss 0.1023992970585823 Accuracy 0.6910000443458557\n",
      "Iteration 7110 Training loss 0.11402104049921036 Validation loss 0.1053706556558609 Accuracy 0.6630000472068787\n",
      "Iteration 7120 Training loss 0.11085477471351624 Validation loss 0.10426382720470428 Accuracy 0.6820000410079956\n",
      "Iteration 7130 Training loss 0.09098020195960999 Validation loss 0.09622922539710999 Accuracy 0.7115000486373901\n",
      "Iteration 7140 Training loss 0.10270251333713531 Validation loss 0.10915223509073257 Accuracy 0.6410000324249268\n",
      "Iteration 7150 Training loss 0.09735427796840668 Validation loss 0.1037556529045105 Accuracy 0.6675000190734863\n",
      "Iteration 7160 Training loss 0.10250651836395264 Validation loss 0.10847672075033188 Accuracy 0.6495000123977661\n",
      "Iteration 7170 Training loss 0.0948319211602211 Validation loss 0.10223894566297531 Accuracy 0.6850000619888306\n",
      "Iteration 7180 Training loss 0.10835617035627365 Validation loss 0.09661053121089935 Accuracy 0.7265000343322754\n",
      "Iteration 7190 Training loss 0.11764020472764969 Validation loss 0.11734087020158768 Accuracy 0.624500036239624\n",
      "Iteration 7200 Training loss 0.10500609874725342 Validation loss 0.09325248003005981 Accuracy 0.7300000190734863\n",
      "Iteration 7210 Training loss 0.10287825018167496 Validation loss 0.10745605826377869 Accuracy 0.6640000343322754\n",
      "Iteration 7220 Training loss 0.10062923282384872 Validation loss 0.0956006720662117 Accuracy 0.7140000462532043\n",
      "Iteration 7230 Training loss 0.13278429210186005 Validation loss 0.11238464713096619 Accuracy 0.6440000534057617\n",
      "Iteration 7240 Training loss 0.08934307098388672 Validation loss 0.09563533961772919 Accuracy 0.7300000190734863\n",
      "Iteration 7250 Training loss 0.10609092563390732 Validation loss 0.10447623580694199 Accuracy 0.6765000224113464\n",
      "Iteration 7260 Training loss 0.08886127173900604 Validation loss 0.09716397523880005 Accuracy 0.70250004529953\n",
      "Iteration 7270 Training loss 0.08388938009738922 Validation loss 0.10201738774776459 Accuracy 0.687000036239624\n",
      "Iteration 7280 Training loss 0.11997003108263016 Validation loss 0.10262709110975266 Accuracy 0.6815000176429749\n",
      "Iteration 7290 Training loss 0.12011821568012238 Validation loss 0.113179512321949 Accuracy 0.624500036239624\n",
      "Iteration 7300 Training loss 0.10689736157655716 Validation loss 0.10487813502550125 Accuracy 0.6695000529289246\n",
      "Iteration 7310 Training loss 0.09135067462921143 Validation loss 0.10177994519472122 Accuracy 0.6790000200271606\n",
      "Iteration 7320 Training loss 0.10940225422382355 Validation loss 0.10292850434780121 Accuracy 0.6975000500679016\n",
      "Iteration 7330 Training loss 0.10948184132575989 Validation loss 0.10612034797668457 Accuracy 0.6570000052452087\n",
      "Iteration 7340 Training loss 0.08299814164638519 Validation loss 0.0934581309556961 Accuracy 0.7250000238418579\n",
      "Iteration 7350 Training loss 0.08362191915512085 Validation loss 0.09507651627063751 Accuracy 0.7200000286102295\n",
      "Iteration 7360 Training loss 0.08983272314071655 Validation loss 0.09516198933124542 Accuracy 0.7165000438690186\n",
      "Iteration 7370 Training loss 0.09365468472242355 Validation loss 0.09849780052900314 Accuracy 0.7045000195503235\n",
      "Iteration 7380 Training loss 0.13195541501045227 Validation loss 0.13743260502815247 Accuracy 0.5764999985694885\n",
      "Iteration 7390 Training loss 0.10350160300731659 Validation loss 0.1036560907959938 Accuracy 0.6790000200271606\n",
      "Iteration 7400 Training loss 0.10118085891008377 Validation loss 0.09946486353874207 Accuracy 0.6970000267028809\n",
      "Iteration 7410 Training loss 0.09463591128587723 Validation loss 0.09675434231758118 Accuracy 0.7125000357627869\n",
      "Iteration 7420 Training loss 0.13766473531723022 Validation loss 0.14155082404613495 Accuracy 0.5560000538825989\n",
      "Iteration 7430 Training loss 0.10522565245628357 Validation loss 0.11147667467594147 Accuracy 0.6510000228881836\n",
      "Iteration 7440 Training loss 0.08689448982477188 Validation loss 0.09604887664318085 Accuracy 0.7305000424385071\n",
      "Iteration 7450 Training loss 0.1338568478822708 Validation loss 0.1085285097360611 Accuracy 0.6530000567436218\n",
      "Iteration 7460 Training loss 0.0968298390507698 Validation loss 0.10511047393083572 Accuracy 0.675000011920929\n",
      "Iteration 7470 Training loss 0.10369721055030823 Validation loss 0.10606836527585983 Accuracy 0.6700000166893005\n",
      "Iteration 7480 Training loss 0.0885758027434349 Validation loss 0.09618376940488815 Accuracy 0.7230000495910645\n",
      "Iteration 7490 Training loss 0.09495054930448532 Validation loss 0.10090240836143494 Accuracy 0.6985000371932983\n",
      "Iteration 7500 Training loss 0.09290352463722229 Validation loss 0.09522019326686859 Accuracy 0.7120000123977661\n",
      "Iteration 7510 Training loss 0.10120340436697006 Validation loss 0.10091269761323929 Accuracy 0.6965000033378601\n",
      "Iteration 7520 Training loss 0.07968005537986755 Validation loss 0.09408288449048996 Accuracy 0.7430000305175781\n",
      "Iteration 7530 Training loss 0.09557386487722397 Validation loss 0.09576002508401871 Accuracy 0.7345000505447388\n",
      "Iteration 7540 Training loss 0.09963659197092056 Validation loss 0.10180363059043884 Accuracy 0.6885000467300415\n",
      "Iteration 7550 Training loss 0.08072549849748611 Validation loss 0.09431212395429611 Accuracy 0.7385000586509705\n",
      "Iteration 7560 Training loss 0.10618126392364502 Validation loss 0.10731141269207001 Accuracy 0.6810000538825989\n",
      "Iteration 7570 Training loss 0.09440633654594421 Validation loss 0.09303662180900574 Accuracy 0.7365000247955322\n",
      "Iteration 7580 Training loss 0.09388777613639832 Validation loss 0.09274795651435852 Accuracy 0.7430000305175781\n",
      "Iteration 7590 Training loss 0.09566696733236313 Validation loss 0.10259705036878586 Accuracy 0.6930000185966492\n",
      "Iteration 7600 Training loss 0.13224276900291443 Validation loss 0.1221308633685112 Accuracy 0.593000054359436\n",
      "Iteration 7610 Training loss 0.10811366885900497 Validation loss 0.10598773509263992 Accuracy 0.6740000247955322\n",
      "Iteration 7620 Training loss 0.09939728677272797 Validation loss 0.09737569838762283 Accuracy 0.706000030040741\n",
      "Iteration 7630 Training loss 0.10741548985242844 Validation loss 0.09829433262348175 Accuracy 0.7080000042915344\n",
      "Iteration 7640 Training loss 0.11901547759771347 Validation loss 0.11439795792102814 Accuracy 0.6210000514984131\n",
      "Iteration 7650 Training loss 0.09861293435096741 Validation loss 0.09969515353441238 Accuracy 0.6960000395774841\n",
      "Iteration 7660 Training loss 0.12933488190174103 Validation loss 0.12040508538484573 Accuracy 0.6325000524520874\n",
      "Iteration 7670 Training loss 0.09055668860673904 Validation loss 0.10033337771892548 Accuracy 0.6920000314712524\n",
      "Iteration 7680 Training loss 0.16581644117832184 Validation loss 0.14981387555599213 Accuracy 0.515500009059906\n",
      "Iteration 7690 Training loss 0.09839288145303726 Validation loss 0.10080615431070328 Accuracy 0.687000036239624\n",
      "Iteration 7700 Training loss 0.073799267411232 Validation loss 0.0919937789440155 Accuracy 0.7460000514984131\n",
      "Iteration 7710 Training loss 0.11158011853694916 Validation loss 0.12048445641994476 Accuracy 0.6155000329017639\n",
      "Iteration 7720 Training loss 0.12208547443151474 Validation loss 0.10665535181760788 Accuracy 0.6755000352859497\n",
      "Iteration 7730 Training loss 0.09456301480531693 Validation loss 0.09597146511077881 Accuracy 0.7225000262260437\n",
      "Iteration 7740 Training loss 0.10084520280361176 Validation loss 0.10117229074239731 Accuracy 0.7115000486373901\n",
      "Iteration 7750 Training loss 0.08857621997594833 Validation loss 0.1012929305434227 Accuracy 0.7085000276565552\n",
      "Iteration 7760 Training loss 0.09358102828264236 Validation loss 0.09662562608718872 Accuracy 0.7310000061988831\n",
      "Iteration 7770 Training loss 0.16262325644493103 Validation loss 0.13139782845973969 Accuracy 0.5024999976158142\n",
      "Iteration 7780 Training loss 0.10818391293287277 Validation loss 0.10404473543167114 Accuracy 0.6500000357627869\n",
      "Iteration 7790 Training loss 0.11915397644042969 Validation loss 0.1341865062713623 Accuracy 0.5520000457763672\n",
      "Iteration 7800 Training loss 0.1045013964176178 Validation loss 0.10376046597957611 Accuracy 0.7115000486373901\n",
      "Iteration 7810 Training loss 0.09604934602975845 Validation loss 0.10501337051391602 Accuracy 0.6600000262260437\n",
      "Iteration 7820 Training loss 0.09140157699584961 Validation loss 0.09525987505912781 Accuracy 0.7355000376701355\n",
      "Iteration 7830 Training loss 0.09293042868375778 Validation loss 0.09962345659732819 Accuracy 0.6960000395774841\n",
      "Iteration 7840 Training loss 0.09692489355802536 Validation loss 0.09616135060787201 Accuracy 0.718500018119812\n",
      "Iteration 7850 Training loss 0.108782097697258 Validation loss 0.09759567677974701 Accuracy 0.7095000147819519\n",
      "Iteration 7860 Training loss 0.0925321951508522 Validation loss 0.09704641252756119 Accuracy 0.7300000190734863\n",
      "Iteration 7870 Training loss 0.09360478073358536 Validation loss 0.0960664376616478 Accuracy 0.737000048160553\n",
      "Iteration 7880 Training loss 0.09986400604248047 Validation loss 0.1044641062617302 Accuracy 0.6725000143051147\n",
      "Iteration 7890 Training loss 0.09795138984918594 Validation loss 0.09453088790178299 Accuracy 0.7385000586509705\n",
      "Iteration 7900 Training loss 0.0868590697646141 Validation loss 0.103671595454216 Accuracy 0.6765000224113464\n",
      "Iteration 7910 Training loss 0.0929909497499466 Validation loss 0.09602294117212296 Accuracy 0.7160000205039978\n",
      "Iteration 7920 Training loss 0.10737159848213196 Validation loss 0.11008310317993164 Accuracy 0.6515000462532043\n",
      "Iteration 7930 Training loss 0.10669650882482529 Validation loss 0.1030396893620491 Accuracy 0.687000036239624\n",
      "Iteration 7940 Training loss 0.10108087211847305 Validation loss 0.1130911335349083 Accuracy 0.6270000338554382\n",
      "Iteration 7950 Training loss 0.0917210727930069 Validation loss 0.10095160454511642 Accuracy 0.690000057220459\n",
      "Iteration 7960 Training loss 0.10944526642560959 Validation loss 0.10069216787815094 Accuracy 0.690000057220459\n",
      "Iteration 7970 Training loss 0.08722943067550659 Validation loss 0.09473329037427902 Accuracy 0.7305000424385071\n",
      "Iteration 7980 Training loss 0.10601916164159775 Validation loss 0.09933455288410187 Accuracy 0.7000000476837158\n",
      "Iteration 7990 Training loss 0.10450732707977295 Validation loss 0.09227518737316132 Accuracy 0.7490000128746033\n",
      "Iteration 8000 Training loss 0.10106733441352844 Validation loss 0.1056855097413063 Accuracy 0.6510000228881836\n",
      "Iteration 8010 Training loss 0.08154860138893127 Validation loss 0.09671199321746826 Accuracy 0.7360000610351562\n",
      "Iteration 8020 Training loss 0.10748683661222458 Validation loss 0.0978616252541542 Accuracy 0.7010000348091125\n",
      "Iteration 8030 Training loss 0.09748958051204681 Validation loss 0.10291977226734161 Accuracy 0.6815000176429749\n",
      "Iteration 8040 Training loss 0.13420680165290833 Validation loss 0.1326623260974884 Accuracy 0.5580000281333923\n",
      "Iteration 8050 Training loss 0.099466972053051 Validation loss 0.11565326154232025 Accuracy 0.6075000166893005\n",
      "Iteration 8060 Training loss 0.09091750532388687 Validation loss 0.10123400390148163 Accuracy 0.7020000219345093\n",
      "Iteration 8070 Training loss 0.11157121509313583 Validation loss 0.11171005666255951 Accuracy 0.6310000419616699\n",
      "Iteration 8080 Training loss 0.11008772253990173 Validation loss 0.10373107343912125 Accuracy 0.6805000305175781\n",
      "Iteration 8090 Training loss 0.09582079946994781 Validation loss 0.10271701216697693 Accuracy 0.6940000057220459\n",
      "Iteration 8100 Training loss 0.08747800439596176 Validation loss 0.10535845905542374 Accuracy 0.6915000081062317\n",
      "Iteration 8110 Training loss 0.09047847241163254 Validation loss 0.10141951590776443 Accuracy 0.6835000514984131\n",
      "Iteration 8120 Training loss 0.10690852999687195 Validation loss 0.10627315193414688 Accuracy 0.6690000295639038\n",
      "Iteration 8130 Training loss 0.10254104435443878 Validation loss 0.10431252419948578 Accuracy 0.6785000562667847\n",
      "Iteration 8140 Training loss 0.10837491601705551 Validation loss 0.09675510227680206 Accuracy 0.7145000100135803\n",
      "Iteration 8150 Training loss 0.10753045976161957 Validation loss 0.12190970033407211 Accuracy 0.5825000405311584\n",
      "Iteration 8160 Training loss 0.1151869148015976 Validation loss 0.11009638011455536 Accuracy 0.6375000476837158\n",
      "Iteration 8170 Training loss 0.09781862795352936 Validation loss 0.09656673669815063 Accuracy 0.7075000405311584\n",
      "Iteration 8180 Training loss 0.09220731258392334 Validation loss 0.10625799000263214 Accuracy 0.6685000061988831\n",
      "Iteration 8190 Training loss 0.07533983886241913 Validation loss 0.09351303428411484 Accuracy 0.7360000610351562\n",
      "Iteration 8200 Training loss 0.09364210814237595 Validation loss 0.09861449152231216 Accuracy 0.7070000171661377\n",
      "Iteration 8210 Training loss 0.09099695086479187 Validation loss 0.09751863032579422 Accuracy 0.7125000357627869\n",
      "Iteration 8220 Training loss 0.09084701538085938 Validation loss 0.09524573385715485 Accuracy 0.7345000505447388\n",
      "Iteration 8230 Training loss 0.09353788197040558 Validation loss 0.09630417823791504 Accuracy 0.7200000286102295\n",
      "Iteration 8240 Training loss 0.10911928117275238 Validation loss 0.09629057347774506 Accuracy 0.7235000133514404\n",
      "Iteration 8250 Training loss 0.11176595091819763 Validation loss 0.10554248839616776 Accuracy 0.6790000200271606\n",
      "Iteration 8260 Training loss 0.10776150226593018 Validation loss 0.10075166821479797 Accuracy 0.6980000138282776\n",
      "Iteration 8270 Training loss 0.10059086233377457 Validation loss 0.10189457982778549 Accuracy 0.7045000195503235\n",
      "Iteration 8280 Training loss 0.09025037288665771 Validation loss 0.09651943296194077 Accuracy 0.7110000252723694\n",
      "Iteration 8290 Training loss 0.09666184335947037 Validation loss 0.09414079040288925 Accuracy 0.733500063419342\n",
      "Iteration 8300 Training loss 0.10832487791776657 Validation loss 0.10087761282920837 Accuracy 0.7045000195503235\n",
      "Iteration 8310 Training loss 0.10984489321708679 Validation loss 0.1086464449763298 Accuracy 0.6470000147819519\n",
      "Iteration 8320 Training loss 0.08618480712175369 Validation loss 0.10081677883863449 Accuracy 0.6880000233650208\n",
      "Iteration 8330 Training loss 0.11543883383274078 Validation loss 0.11296886950731277 Accuracy 0.6210000514984131\n",
      "Iteration 8340 Training loss 0.09270484745502472 Validation loss 0.09687810391187668 Accuracy 0.7355000376701355\n",
      "Iteration 8350 Training loss 0.13613218069076538 Validation loss 0.12176114320755005 Accuracy 0.578000009059906\n",
      "Iteration 8360 Training loss 0.10388917475938797 Validation loss 0.10321240872144699 Accuracy 0.6810000538825989\n",
      "Iteration 8370 Training loss 0.10956818610429764 Validation loss 0.11171568185091019 Accuracy 0.609000027179718\n",
      "Iteration 8380 Training loss 0.10357148200273514 Validation loss 0.10831328481435776 Accuracy 0.659000039100647\n",
      "Iteration 8390 Training loss 0.12939700484275818 Validation loss 0.131233349442482 Accuracy 0.5205000042915344\n",
      "Iteration 8400 Training loss 0.11095714569091797 Validation loss 0.11134710907936096 Accuracy 0.6450000405311584\n",
      "Iteration 8410 Training loss 0.11582484841346741 Validation loss 0.1206425279378891 Accuracy 0.5770000219345093\n",
      "Iteration 8420 Training loss 0.0980846956372261 Validation loss 0.10217372328042984 Accuracy 0.7000000476837158\n",
      "Iteration 8430 Training loss 0.11602398008108139 Validation loss 0.10771360993385315 Accuracy 0.6665000319480896\n",
      "Iteration 8440 Training loss 0.10468525439500809 Validation loss 0.10924817621707916 Accuracy 0.640500009059906\n",
      "Iteration 8450 Training loss 0.08602937310934067 Validation loss 0.09574266523122787 Accuracy 0.721500039100647\n",
      "Iteration 8460 Training loss 0.10272452980279922 Validation loss 0.10281101614236832 Accuracy 0.6765000224113464\n",
      "Iteration 8470 Training loss 0.11068587005138397 Validation loss 0.1059928759932518 Accuracy 0.6690000295639038\n",
      "Iteration 8480 Training loss 0.10036972165107727 Validation loss 0.10421611368656158 Accuracy 0.6790000200271606\n",
      "Iteration 8490 Training loss 0.0969899371266365 Validation loss 0.10251161456108093 Accuracy 0.6830000281333923\n",
      "Iteration 8500 Training loss 0.13926512002944946 Validation loss 0.11294376850128174 Accuracy 0.6155000329017639\n",
      "Iteration 8510 Training loss 0.09065829962491989 Validation loss 0.09879634529352188 Accuracy 0.7000000476837158\n",
      "Iteration 8520 Training loss 0.10443273931741714 Validation loss 0.09846328943967819 Accuracy 0.7150000333786011\n",
      "Iteration 8530 Training loss 0.08708152920007706 Validation loss 0.09658147394657135 Accuracy 0.7290000319480896\n",
      "Iteration 8540 Training loss 0.12345321476459503 Validation loss 0.11160486191511154 Accuracy 0.6585000157356262\n",
      "Iteration 8550 Training loss 0.08855586498975754 Validation loss 0.09705812484025955 Accuracy 0.7355000376701355\n",
      "Iteration 8560 Training loss 0.08960902690887451 Validation loss 0.0975717082619667 Accuracy 0.7155000567436218\n",
      "Iteration 8570 Training loss 0.10369737446308136 Validation loss 0.102130226790905 Accuracy 0.6930000185966492\n",
      "Iteration 8580 Training loss 0.07413000613451004 Validation loss 0.09746924042701721 Accuracy 0.7285000085830688\n",
      "Iteration 8590 Training loss 0.09530549496412277 Validation loss 0.09704073518514633 Accuracy 0.721500039100647\n",
      "Iteration 8600 Training loss 0.09053070098161697 Validation loss 0.09872052818536758 Accuracy 0.6985000371932983\n",
      "Iteration 8610 Training loss 0.09595236927270889 Validation loss 0.09408525377511978 Accuracy 0.7295000553131104\n",
      "Iteration 8620 Training loss 0.09216409176588058 Validation loss 0.09453921765089035 Accuracy 0.7265000343322754\n",
      "Iteration 8630 Training loss 0.09234903007745743 Validation loss 0.09375819563865662 Accuracy 0.7305000424385071\n",
      "Iteration 8640 Training loss 0.08951026946306229 Validation loss 0.09754741191864014 Accuracy 0.7045000195503235\n",
      "Iteration 8650 Training loss 0.08730804175138474 Validation loss 0.09947500377893448 Accuracy 0.7090000510215759\n",
      "Iteration 8660 Training loss 0.10273734480142593 Validation loss 0.09693852066993713 Accuracy 0.7240000367164612\n",
      "Iteration 8670 Training loss 0.0941048115491867 Validation loss 0.10327938199043274 Accuracy 0.6845000386238098\n",
      "Iteration 8680 Training loss 0.10132861882448196 Validation loss 0.10349126905202866 Accuracy 0.6760000586509705\n",
      "Iteration 8690 Training loss 0.09569863229990005 Validation loss 0.09321251511573792 Accuracy 0.7270000576972961\n",
      "Iteration 8700 Training loss 0.09547044336795807 Validation loss 0.09344857931137085 Accuracy 0.733500063419342\n",
      "Iteration 8710 Training loss 0.10157892107963562 Validation loss 0.10495064407587051 Accuracy 0.6800000071525574\n",
      "Iteration 8720 Training loss 0.11026015877723694 Validation loss 0.10116340219974518 Accuracy 0.6960000395774841\n",
      "Iteration 8730 Training loss 0.07684487849473953 Validation loss 0.09242205321788788 Accuracy 0.7400000095367432\n",
      "Iteration 8740 Training loss 0.09254881739616394 Validation loss 0.09862924367189407 Accuracy 0.6930000185966492\n",
      "Iteration 8750 Training loss 0.1173657774925232 Validation loss 0.09926310181617737 Accuracy 0.7075000405311584\n",
      "Iteration 8760 Training loss 0.09899301081895828 Validation loss 0.0951252207159996 Accuracy 0.7235000133514404\n",
      "Iteration 8770 Training loss 0.09286694973707199 Validation loss 0.09782866388559341 Accuracy 0.7090000510215759\n",
      "Iteration 8780 Training loss 0.07607954740524292 Validation loss 0.09318425506353378 Accuracy 0.7300000190734863\n",
      "Iteration 8790 Training loss 0.10880672186613083 Validation loss 0.11655804514884949 Accuracy 0.6175000071525574\n",
      "Iteration 8800 Training loss 0.0816233903169632 Validation loss 0.0940483883023262 Accuracy 0.734000027179718\n",
      "Iteration 8810 Training loss 0.09404151886701584 Validation loss 0.09454537183046341 Accuracy 0.7195000052452087\n",
      "Iteration 8820 Training loss 0.08785340189933777 Validation loss 0.09960485994815826 Accuracy 0.6945000290870667\n",
      "Iteration 8830 Training loss 0.10550790280103683 Validation loss 0.09988702833652496 Accuracy 0.7095000147819519\n",
      "Iteration 8840 Training loss 0.10882490873336792 Validation loss 0.1092572957277298 Accuracy 0.6490000486373901\n",
      "Iteration 8850 Training loss 0.09435136616230011 Validation loss 0.09233967959880829 Accuracy 0.7320000529289246\n",
      "Iteration 8860 Training loss 0.09638854116201401 Validation loss 0.10147295147180557 Accuracy 0.6875000596046448\n",
      "Iteration 8870 Training loss 0.11064042896032333 Validation loss 0.1011740192770958 Accuracy 0.6805000305175781\n",
      "Iteration 8880 Training loss 0.12202546745538712 Validation loss 0.1154852882027626 Accuracy 0.6340000033378601\n",
      "Iteration 8890 Training loss 0.10228808969259262 Validation loss 0.09366429597139359 Accuracy 0.7355000376701355\n",
      "Iteration 8900 Training loss 0.09057815372943878 Validation loss 0.09639076143503189 Accuracy 0.718500018119812\n",
      "Iteration 8910 Training loss 0.08502211421728134 Validation loss 0.09510693699121475 Accuracy 0.7350000143051147\n",
      "Iteration 8920 Training loss 0.12102288752794266 Validation loss 0.12507013976573944 Accuracy 0.6140000224113464\n",
      "Iteration 8930 Training loss 0.1007872149348259 Validation loss 0.10093649476766586 Accuracy 0.6970000267028809\n",
      "Iteration 8940 Training loss 0.08578073233366013 Validation loss 0.09570732712745667 Accuracy 0.7245000600814819\n",
      "Iteration 8950 Training loss 0.0882984921336174 Validation loss 0.09396158158779144 Accuracy 0.7360000610351562\n",
      "Iteration 8960 Training loss 0.10009050369262695 Validation loss 0.09211980551481247 Accuracy 0.737000048160553\n",
      "Iteration 8970 Training loss 0.09029944986104965 Validation loss 0.10030864179134369 Accuracy 0.6835000514984131\n",
      "Iteration 8980 Training loss 0.09241729229688644 Validation loss 0.10022367537021637 Accuracy 0.6975000500679016\n",
      "Iteration 8990 Training loss 0.09427890926599503 Validation loss 0.09531412273645401 Accuracy 0.7170000076293945\n",
      "Iteration 9000 Training loss 0.09559426456689835 Validation loss 0.09936073422431946 Accuracy 0.7020000219345093\n",
      "Iteration 9010 Training loss 0.11342570930719376 Validation loss 0.09691214561462402 Accuracy 0.7150000333786011\n",
      "Iteration 9020 Training loss 0.1018538847565651 Validation loss 0.0961613655090332 Accuracy 0.7275000214576721\n",
      "Iteration 9030 Training loss 0.10220324993133545 Validation loss 0.103146992623806 Accuracy 0.6735000610351562\n",
      "Iteration 9040 Training loss 0.09854167699813843 Validation loss 0.09298638254404068 Accuracy 0.7200000286102295\n",
      "Iteration 9050 Training loss 0.0834721252322197 Validation loss 0.09539134055376053 Accuracy 0.7095000147819519\n",
      "Iteration 9060 Training loss 0.08875688165426254 Validation loss 0.09479010105133057 Accuracy 0.737500011920929\n",
      "Iteration 9070 Training loss 0.1112867146730423 Validation loss 0.10053792595863342 Accuracy 0.7040000557899475\n",
      "Iteration 9080 Training loss 0.10538732260465622 Validation loss 0.10298760235309601 Accuracy 0.6735000610351562\n",
      "Iteration 9090 Training loss 0.08726806193590164 Validation loss 0.097554512321949 Accuracy 0.7075000405311584\n",
      "Iteration 9100 Training loss 0.09584833681583405 Validation loss 0.10224290937185287 Accuracy 0.6780000329017639\n",
      "Iteration 9110 Training loss 0.08928472548723221 Validation loss 0.09301955252885818 Accuracy 0.7290000319480896\n",
      "Iteration 9120 Training loss 0.1038731187582016 Validation loss 0.09257823973894119 Accuracy 0.7265000343322754\n",
      "Iteration 9130 Training loss 0.09886644035577774 Validation loss 0.10548201203346252 Accuracy 0.6930000185966492\n",
      "Iteration 9140 Training loss 0.08658666163682938 Validation loss 0.0963345393538475 Accuracy 0.7085000276565552\n",
      "Iteration 9150 Training loss 0.09053558111190796 Validation loss 0.09754916280508041 Accuracy 0.6975000500679016\n",
      "Iteration 9160 Training loss 0.08411101251840591 Validation loss 0.0904742032289505 Accuracy 0.7460000514984131\n",
      "Iteration 9170 Training loss 0.08484242856502533 Validation loss 0.09046318382024765 Accuracy 0.7430000305175781\n",
      "Iteration 9180 Training loss 0.09746621549129486 Validation loss 0.0993056520819664 Accuracy 0.718500018119812\n",
      "Iteration 9190 Training loss 0.09942449629306793 Validation loss 0.10158844292163849 Accuracy 0.6890000104904175\n",
      "Iteration 9200 Training loss 0.10428691655397415 Validation loss 0.10550399124622345 Accuracy 0.6735000610351562\n",
      "Iteration 9210 Training loss 0.1189235970377922 Validation loss 0.10005319863557816 Accuracy 0.7005000114440918\n",
      "Iteration 9220 Training loss 0.11916852742433548 Validation loss 0.13157609105110168 Accuracy 0.5995000004768372\n",
      "Iteration 9230 Training loss 0.0805143415927887 Validation loss 0.09510008990764618 Accuracy 0.7100000381469727\n",
      "Iteration 9240 Training loss 0.10191956162452698 Validation loss 0.09851773083209991 Accuracy 0.7115000486373901\n",
      "Iteration 9250 Training loss 0.09976670891046524 Validation loss 0.10457402467727661 Accuracy 0.6620000600814819\n",
      "Iteration 9260 Training loss 0.09889326244592667 Validation loss 0.10835562646389008 Accuracy 0.6520000100135803\n",
      "Iteration 9270 Training loss 0.09196209907531738 Validation loss 0.1036456748843193 Accuracy 0.6825000047683716\n",
      "Iteration 9280 Training loss 0.10823696851730347 Validation loss 0.10325680673122406 Accuracy 0.6875000596046448\n",
      "Iteration 9290 Training loss 0.1107606366276741 Validation loss 0.10629380494356155 Accuracy 0.6670000553131104\n",
      "Iteration 9300 Training loss 0.10367946326732635 Validation loss 0.10338693112134933 Accuracy 0.6755000352859497\n",
      "Iteration 9310 Training loss 0.08321474492549896 Validation loss 0.0940331518650055 Accuracy 0.7300000190734863\n",
      "Iteration 9320 Training loss 0.08779352158308029 Validation loss 0.09825942665338516 Accuracy 0.690500020980835\n",
      "Iteration 9330 Training loss 0.09657134115695953 Validation loss 0.10918404161930084 Accuracy 0.6490000486373901\n",
      "Iteration 9340 Training loss 0.0832546129822731 Validation loss 0.1029079332947731 Accuracy 0.6815000176429749\n",
      "Iteration 9350 Training loss 0.10465245693922043 Validation loss 0.10034043341875076 Accuracy 0.690500020980835\n",
      "Iteration 9360 Training loss 0.10578122735023499 Validation loss 0.11287689954042435 Accuracy 0.6570000052452087\n",
      "Iteration 9370 Training loss 0.08708572387695312 Validation loss 0.09570222347974777 Accuracy 0.7255000472068787\n",
      "Iteration 9380 Training loss 0.11181338876485825 Validation loss 0.11500515788793564 Accuracy 0.6160000562667847\n",
      "Iteration 9390 Training loss 0.14232082664966583 Validation loss 0.1286185383796692 Accuracy 0.5470000505447388\n",
      "Iteration 9400 Training loss 0.09206786751747131 Validation loss 0.10990209132432938 Accuracy 0.6495000123977661\n",
      "Iteration 9410 Training loss 0.08940696716308594 Validation loss 0.09665041416883469 Accuracy 0.7240000367164612\n",
      "Iteration 9420 Training loss 0.1085164025425911 Validation loss 0.10371292382478714 Accuracy 0.6685000061988831\n",
      "Iteration 9430 Training loss 0.0968358963727951 Validation loss 0.10320913791656494 Accuracy 0.6755000352859497\n",
      "Iteration 9440 Training loss 0.09235351532697678 Validation loss 0.10153909772634506 Accuracy 0.6920000314712524\n",
      "Iteration 9450 Training loss 0.11318130046129227 Validation loss 0.10503972321748734 Accuracy 0.6660000085830688\n",
      "Iteration 9460 Training loss 0.11963848024606705 Validation loss 0.10607513040304184 Accuracy 0.6660000085830688\n",
      "Iteration 9470 Training loss 0.11143869161605835 Validation loss 0.11890161782503128 Accuracy 0.5845000147819519\n",
      "Iteration 9480 Training loss 0.12089519202709198 Validation loss 0.10609864443540573 Accuracy 0.6650000214576721\n",
      "Iteration 9490 Training loss 0.09448304772377014 Validation loss 0.10182585567235947 Accuracy 0.687000036239624\n",
      "Iteration 9500 Training loss 0.08254879713058472 Validation loss 0.09491384774446487 Accuracy 0.7285000085830688\n",
      "Iteration 9510 Training loss 0.11175088584423065 Validation loss 0.10082918405532837 Accuracy 0.6990000605583191\n",
      "Iteration 9520 Training loss 0.08747513592243195 Validation loss 0.10026988387107849 Accuracy 0.7140000462532043\n",
      "Iteration 9530 Training loss 0.0969390943646431 Validation loss 0.09704860299825668 Accuracy 0.7145000100135803\n",
      "Iteration 9540 Training loss 0.1039474830031395 Validation loss 0.1012478694319725 Accuracy 0.7055000066757202\n",
      "Iteration 9550 Training loss 0.10412827134132385 Validation loss 0.10151807963848114 Accuracy 0.7000000476837158\n",
      "Iteration 9560 Training loss 0.10013207793235779 Validation loss 0.10284102708101273 Accuracy 0.6860000491142273\n",
      "Iteration 9570 Training loss 0.08972784131765366 Validation loss 0.09511500597000122 Accuracy 0.7195000052452087\n",
      "Iteration 9580 Training loss 0.1089821606874466 Validation loss 0.10790405422449112 Accuracy 0.6755000352859497\n",
      "Iteration 9590 Training loss 0.09400663524866104 Validation loss 0.09913318604230881 Accuracy 0.7110000252723694\n",
      "Iteration 9600 Training loss 0.10614202171564102 Validation loss 0.10191790014505386 Accuracy 0.6765000224113464\n",
      "Iteration 9610 Training loss 0.09027136862277985 Validation loss 0.10037446767091751 Accuracy 0.6890000104904175\n",
      "Iteration 9620 Training loss 0.11758583784103394 Validation loss 0.10187286883592606 Accuracy 0.6995000243186951\n",
      "Iteration 9630 Training loss 0.09199848771095276 Validation loss 0.09679476171731949 Accuracy 0.7170000076293945\n",
      "Iteration 9640 Training loss 0.10231849551200867 Validation loss 0.09592947363853455 Accuracy 0.7385000586509705\n",
      "Iteration 9650 Training loss 0.12239896506071091 Validation loss 0.11857741326093674 Accuracy 0.6195000410079956\n",
      "Iteration 9660 Training loss 0.09392963349819183 Validation loss 0.10064797848463058 Accuracy 0.6845000386238098\n",
      "Iteration 9670 Training loss 0.08617343008518219 Validation loss 0.0952957347035408 Accuracy 0.7275000214576721\n",
      "Iteration 9680 Training loss 0.0891018882393837 Validation loss 0.10171110928058624 Accuracy 0.6740000247955322\n",
      "Iteration 9690 Training loss 0.09853792190551758 Validation loss 0.09538916498422623 Accuracy 0.7165000438690186\n",
      "Iteration 9700 Training loss 0.10848995298147202 Validation loss 0.09847642481327057 Accuracy 0.7070000171661377\n",
      "Iteration 9710 Training loss 0.10409604758024216 Validation loss 0.09644923359155655 Accuracy 0.7225000262260437\n",
      "Iteration 9720 Training loss 0.09618404507637024 Validation loss 0.0944073349237442 Accuracy 0.7275000214576721\n",
      "Iteration 9730 Training loss 0.11478622257709503 Validation loss 0.11167340725660324 Accuracy 0.6270000338554382\n",
      "Iteration 9740 Training loss 0.1038484200835228 Validation loss 0.1135140210390091 Accuracy 0.6325000524520874\n",
      "Iteration 9750 Training loss 0.09396564960479736 Validation loss 0.09824765473604202 Accuracy 0.7145000100135803\n",
      "Iteration 9760 Training loss 0.1218029111623764 Validation loss 0.12981654703617096 Accuracy 0.6115000247955322\n",
      "Iteration 9770 Training loss 0.11377287656068802 Validation loss 0.09860464930534363 Accuracy 0.7145000100135803\n",
      "Iteration 9780 Training loss 0.10668006539344788 Validation loss 0.0966552197933197 Accuracy 0.7055000066757202\n",
      "Iteration 9790 Training loss 0.09777850657701492 Validation loss 0.09843883663415909 Accuracy 0.7080000042915344\n",
      "Iteration 9800 Training loss 0.08935520797967911 Validation loss 0.09447257220745087 Accuracy 0.7245000600814819\n",
      "Iteration 9810 Training loss 0.10068731009960175 Validation loss 0.09905713051557541 Accuracy 0.7015000581741333\n",
      "Iteration 9820 Training loss 0.09471508860588074 Validation loss 0.0938601866364479 Accuracy 0.7315000295639038\n",
      "Iteration 9830 Training loss 0.10157401859760284 Validation loss 0.09660287946462631 Accuracy 0.7290000319480896\n",
      "Iteration 9840 Training loss 0.13255833089351654 Validation loss 0.11864705383777618 Accuracy 0.6395000219345093\n",
      "Iteration 9850 Training loss 0.09572221338748932 Validation loss 0.09610156714916229 Accuracy 0.7165000438690186\n",
      "Iteration 9860 Training loss 0.10077089071273804 Validation loss 0.10012700408697128 Accuracy 0.7000000476837158\n",
      "Iteration 9870 Training loss 0.11282387375831604 Validation loss 0.12142883986234665 Accuracy 0.5955000519752502\n",
      "Iteration 9880 Training loss 0.09208782017230988 Validation loss 0.09706701338291168 Accuracy 0.7090000510215759\n",
      "Iteration 9890 Training loss 0.09090471267700195 Validation loss 0.09879548847675323 Accuracy 0.6980000138282776\n",
      "Iteration 9900 Training loss 0.11145934462547302 Validation loss 0.10344823449850082 Accuracy 0.6740000247955322\n",
      "Iteration 9910 Training loss 0.08889615535736084 Validation loss 0.09389572590589523 Accuracy 0.7345000505447388\n",
      "Iteration 9920 Training loss 0.12828059494495392 Validation loss 0.1125161200761795 Accuracy 0.6220000386238098\n",
      "Iteration 9930 Training loss 0.07588215172290802 Validation loss 0.09444479644298553 Accuracy 0.7270000576972961\n",
      "Iteration 9940 Training loss 0.1259451061487198 Validation loss 0.12608736753463745 Accuracy 0.5885000228881836\n",
      "Iteration 9950 Training loss 0.10947296768426895 Validation loss 0.10455425083637238 Accuracy 0.6820000410079956\n",
      "Iteration 9960 Training loss 0.07944232225418091 Validation loss 0.09691905975341797 Accuracy 0.7105000615119934\n",
      "Iteration 9970 Training loss 0.11442495137453079 Validation loss 0.10697530955076218 Accuracy 0.6780000329017639\n",
      "Iteration 9980 Training loss 0.10526172071695328 Validation loss 0.10216528177261353 Accuracy 0.6875000596046448\n",
      "Iteration 9990 Training loss 0.09127122908830643 Validation loss 0.0972798690199852 Accuracy 0.7195000052452087\n",
      "Iteration 10000 Training loss 0.08661913871765137 Validation loss 0.09659891575574875 Accuracy 0.7150000333786011\n",
      "Iteration 10010 Training loss 0.10752183943986893 Validation loss 0.11054438352584839 Accuracy 0.6330000162124634\n",
      "Iteration 10020 Training loss 0.10161036998033524 Validation loss 0.09547512233257294 Accuracy 0.7110000252723694\n",
      "Iteration 10030 Training loss 0.08791213482618332 Validation loss 0.09704973548650742 Accuracy 0.718500018119812\n",
      "Iteration 10040 Training loss 0.08657630532979965 Validation loss 0.0934114083647728 Accuracy 0.7315000295639038\n",
      "Iteration 10050 Training loss 0.10281386226415634 Validation loss 0.11208806186914444 Accuracy 0.6285000443458557\n",
      "Iteration 10060 Training loss 0.0936967059969902 Validation loss 0.10221221297979355 Accuracy 0.6850000619888306\n",
      "Iteration 10070 Training loss 0.0961630642414093 Validation loss 0.09422173351049423 Accuracy 0.7205000519752502\n",
      "Iteration 10080 Training loss 0.09139074385166168 Validation loss 0.0987059697508812 Accuracy 0.7265000343322754\n",
      "Iteration 10090 Training loss 0.09716273844242096 Validation loss 0.09899275749921799 Accuracy 0.7115000486373901\n",
      "Iteration 10100 Training loss 0.09552863985300064 Validation loss 0.10587611049413681 Accuracy 0.675000011920929\n",
      "Iteration 10110 Training loss 0.09087618440389633 Validation loss 0.09461770951747894 Accuracy 0.7445000410079956\n",
      "Iteration 10120 Training loss 0.1005723774433136 Validation loss 0.10668123513460159 Accuracy 0.6480000019073486\n",
      "Iteration 10130 Training loss 0.09183332324028015 Validation loss 0.09788104891777039 Accuracy 0.7225000262260437\n",
      "Iteration 10140 Training loss 0.09865791350603104 Validation loss 0.10978756099939346 Accuracy 0.6270000338554382\n",
      "Iteration 10150 Training loss 0.07821423560380936 Validation loss 0.09683066606521606 Accuracy 0.7210000157356262\n",
      "Iteration 10160 Training loss 0.09826405346393585 Validation loss 0.10005591064691544 Accuracy 0.7160000205039978\n",
      "Iteration 10170 Training loss 0.08674857765436172 Validation loss 0.09940888732671738 Accuracy 0.6950000524520874\n",
      "Iteration 10180 Training loss 0.101666159927845 Validation loss 0.10152405500411987 Accuracy 0.6965000033378601\n",
      "Iteration 10190 Training loss 0.11517395824193954 Validation loss 0.12242669612169266 Accuracy 0.5915000438690186\n",
      "Iteration 10200 Training loss 0.11323083937168121 Validation loss 0.1123938336968422 Accuracy 0.6335000395774841\n",
      "Iteration 10210 Training loss 0.09453722089529037 Validation loss 0.09964482486248016 Accuracy 0.7140000462532043\n",
      "Iteration 10220 Training loss 0.08289793133735657 Validation loss 0.0941586047410965 Accuracy 0.7250000238418579\n",
      "Iteration 10230 Training loss 0.10828205198049545 Validation loss 0.1020575538277626 Accuracy 0.6760000586509705\n",
      "Iteration 10240 Training loss 0.11507334560155869 Validation loss 0.10249347239732742 Accuracy 0.6800000071525574\n",
      "Iteration 10250 Training loss 0.10053019970655441 Validation loss 0.10343346744775772 Accuracy 0.6990000605583191\n",
      "Iteration 10260 Training loss 0.08720225095748901 Validation loss 0.1040482372045517 Accuracy 0.6720000505447388\n",
      "Iteration 10270 Training loss 0.1010158360004425 Validation loss 0.1078023612499237 Accuracy 0.6470000147819519\n",
      "Iteration 10280 Training loss 0.09625354409217834 Validation loss 0.108157217502594 Accuracy 0.6670000553131104\n",
      "Iteration 10290 Training loss 0.08874794840812683 Validation loss 0.09851424396038055 Accuracy 0.7075000405311584\n",
      "Iteration 10300 Training loss 0.08456956595182419 Validation loss 0.09879855811595917 Accuracy 0.7015000581741333\n",
      "Iteration 10310 Training loss 0.09208691120147705 Validation loss 0.10172177851200104 Accuracy 0.6895000338554382\n",
      "Iteration 10320 Training loss 0.0978723093867302 Validation loss 0.09704436361789703 Accuracy 0.7235000133514404\n",
      "Iteration 10330 Training loss 0.10437536984682083 Validation loss 0.09727894514799118 Accuracy 0.7095000147819519\n",
      "Iteration 10340 Training loss 0.107921302318573 Validation loss 0.09830711781978607 Accuracy 0.7035000324249268\n",
      "Iteration 10350 Training loss 0.09005863219499588 Validation loss 0.09596610814332962 Accuracy 0.7290000319480896\n",
      "Iteration 10360 Training loss 0.09866944700479507 Validation loss 0.09854674339294434 Accuracy 0.7265000343322754\n",
      "Iteration 10370 Training loss 0.09966084361076355 Validation loss 0.09468463063240051 Accuracy 0.7230000495910645\n",
      "Iteration 10380 Training loss 0.10344190150499344 Validation loss 0.09589936584234238 Accuracy 0.7235000133514404\n",
      "Iteration 10390 Training loss 0.09075379371643066 Validation loss 0.10112340748310089 Accuracy 0.7005000114440918\n",
      "Iteration 10400 Training loss 0.10447169840335846 Validation loss 0.1024293601512909 Accuracy 0.6955000162124634\n",
      "Iteration 10410 Training loss 0.11121180653572083 Validation loss 0.10237738490104675 Accuracy 0.6970000267028809\n",
      "Iteration 10420 Training loss 0.11085859686136246 Validation loss 0.10421592742204666 Accuracy 0.6835000514984131\n",
      "Iteration 10430 Training loss 0.1071801483631134 Validation loss 0.10431189090013504 Accuracy 0.6795000433921814\n",
      "Iteration 10440 Training loss 0.10236374288797379 Validation loss 0.10612329840660095 Accuracy 0.6645000576972961\n",
      "Iteration 10450 Training loss 0.08387082815170288 Validation loss 0.09256184101104736 Accuracy 0.7480000257492065\n",
      "Iteration 10460 Training loss 0.1068921610713005 Validation loss 0.10304559022188187 Accuracy 0.6845000386238098\n",
      "Iteration 10470 Training loss 0.1029634103178978 Validation loss 0.0983438640832901 Accuracy 0.706000030040741\n",
      "Iteration 10480 Training loss 0.0976477861404419 Validation loss 0.10002590715885162 Accuracy 0.7045000195503235\n",
      "Iteration 10490 Training loss 0.11367438733577728 Validation loss 0.1179458498954773 Accuracy 0.6175000071525574\n",
      "Iteration 10500 Training loss 0.11677385866641998 Validation loss 0.10553239285945892 Accuracy 0.656000018119812\n",
      "Iteration 10510 Training loss 0.09611048549413681 Validation loss 0.09240132570266724 Accuracy 0.7395000457763672\n",
      "Iteration 10520 Training loss 0.09033047407865524 Validation loss 0.10008183866739273 Accuracy 0.7065000534057617\n",
      "Iteration 10530 Training loss 0.08886045217514038 Validation loss 0.09616739302873611 Accuracy 0.7265000343322754\n",
      "Iteration 10540 Training loss 0.09149578213691711 Validation loss 0.09354547411203384 Accuracy 0.7475000619888306\n",
      "Iteration 10550 Training loss 0.10151465982198715 Validation loss 0.09329930692911148 Accuracy 0.737000048160553\n",
      "Iteration 10560 Training loss 0.11598989367485046 Validation loss 0.09582129120826721 Accuracy 0.7095000147819519\n",
      "Iteration 10570 Training loss 0.09529253095388412 Validation loss 0.09675600379705429 Accuracy 0.7125000357627869\n",
      "Iteration 10580 Training loss 0.10270507633686066 Validation loss 0.10795710235834122 Accuracy 0.6510000228881836\n",
      "Iteration 10590 Training loss 0.09143396466970444 Validation loss 0.09559863805770874 Accuracy 0.7200000286102295\n",
      "Iteration 10600 Training loss 0.10005349665880203 Validation loss 0.10276710987091064 Accuracy 0.6965000033378601\n",
      "Iteration 10610 Training loss 0.09383048862218857 Validation loss 0.09142641723155975 Accuracy 0.7320000529289246\n",
      "Iteration 10620 Training loss 0.09875281900167465 Validation loss 0.10169937461614609 Accuracy 0.6965000033378601\n",
      "Iteration 10630 Training loss 0.08884067088365555 Validation loss 0.09764452278614044 Accuracy 0.7080000042915344\n",
      "Iteration 10640 Training loss 0.0978914424777031 Validation loss 0.10001304745674133 Accuracy 0.7075000405311584\n",
      "Iteration 10650 Training loss 0.0959196388721466 Validation loss 0.09766404330730438 Accuracy 0.7140000462532043\n",
      "Iteration 10660 Training loss 0.11580272763967514 Validation loss 0.10198581218719482 Accuracy 0.6855000257492065\n",
      "Iteration 10670 Training loss 0.10265550017356873 Validation loss 0.09541893005371094 Accuracy 0.7210000157356262\n",
      "Iteration 10680 Training loss 0.1004185676574707 Validation loss 0.09771327674388885 Accuracy 0.7140000462532043\n",
      "Iteration 10690 Training loss 0.08560453355312347 Validation loss 0.09624757617712021 Accuracy 0.7155000567436218\n",
      "Iteration 10700 Training loss 0.09870540350675583 Validation loss 0.10060777515172958 Accuracy 0.6955000162124634\n",
      "Iteration 10710 Training loss 0.09957870095968246 Validation loss 0.09959610551595688 Accuracy 0.6920000314712524\n",
      "Iteration 10720 Training loss 0.12715493142604828 Validation loss 0.1202474907040596 Accuracy 0.6305000185966492\n",
      "Iteration 10730 Training loss 0.0925847589969635 Validation loss 0.09561385959386826 Accuracy 0.7330000400543213\n",
      "Iteration 10740 Training loss 0.12675686180591583 Validation loss 0.11134257167577744 Accuracy 0.640500009059906\n",
      "Iteration 10750 Training loss 0.09884242713451385 Validation loss 0.10170737653970718 Accuracy 0.6920000314712524\n",
      "Iteration 10760 Training loss 0.09304848313331604 Validation loss 0.09133214503526688 Accuracy 0.7380000352859497\n",
      "Iteration 10770 Training loss 0.08263994008302689 Validation loss 0.09730139374732971 Accuracy 0.7175000309944153\n",
      "Iteration 10780 Training loss 0.09621963649988174 Validation loss 0.09759688377380371 Accuracy 0.7090000510215759\n",
      "Iteration 10790 Training loss 0.08018668740987778 Validation loss 0.09950505197048187 Accuracy 0.718500018119812\n",
      "Iteration 10800 Training loss 0.10196353495121002 Validation loss 0.1063256487250328 Accuracy 0.6605000495910645\n",
      "Iteration 10810 Training loss 0.079256571829319 Validation loss 0.09881753474473953 Accuracy 0.6945000290870667\n",
      "Iteration 10820 Training loss 0.09058079868555069 Validation loss 0.0932103767991066 Accuracy 0.7280000448226929\n",
      "Iteration 10830 Training loss 0.09881497919559479 Validation loss 0.0982707068324089 Accuracy 0.7005000114440918\n",
      "Iteration 10840 Training loss 0.08961717039346695 Validation loss 0.09852372854948044 Accuracy 0.6915000081062317\n",
      "Iteration 10850 Training loss 0.09784317761659622 Validation loss 0.09468536823987961 Accuracy 0.7195000052452087\n",
      "Iteration 10860 Training loss 0.09510896354913712 Validation loss 0.09519002586603165 Accuracy 0.7295000553131104\n",
      "Iteration 10870 Training loss 0.08956854790449142 Validation loss 0.09982314705848694 Accuracy 0.6855000257492065\n",
      "Iteration 10880 Training loss 0.09922552108764648 Validation loss 0.10570857673883438 Accuracy 0.6710000038146973\n",
      "Iteration 10890 Training loss 0.08967866748571396 Validation loss 0.09460733085870743 Accuracy 0.721500039100647\n",
      "Iteration 10900 Training loss 0.09097007662057877 Validation loss 0.09948527812957764 Accuracy 0.7050000429153442\n",
      "Iteration 10910 Training loss 0.09659747779369354 Validation loss 0.09283094108104706 Accuracy 0.7300000190734863\n",
      "Iteration 10920 Training loss 0.08470208942890167 Validation loss 0.09576468169689178 Accuracy 0.7155000567436218\n",
      "Iteration 10930 Training loss 0.10719858855009079 Validation loss 0.10644693672657013 Accuracy 0.6665000319480896\n",
      "Iteration 10940 Training loss 0.11141625046730042 Validation loss 0.1076449602842331 Accuracy 0.6565000414848328\n",
      "Iteration 10950 Training loss 0.09517398476600647 Validation loss 0.0984235629439354 Accuracy 0.7145000100135803\n",
      "Iteration 10960 Training loss 0.08748958259820938 Validation loss 0.10725978761911392 Accuracy 0.6700000166893005\n",
      "Iteration 10970 Training loss 0.0764768198132515 Validation loss 0.09665936976671219 Accuracy 0.7140000462532043\n",
      "Iteration 10980 Training loss 0.0903531089425087 Validation loss 0.09513740241527557 Accuracy 0.718000054359436\n",
      "Iteration 10990 Training loss 0.07959167659282684 Validation loss 0.09165216237306595 Accuracy 0.7300000190734863\n",
      "Iteration 11000 Training loss 0.1106649786233902 Validation loss 0.10079693049192429 Accuracy 0.6980000138282776\n",
      "Iteration 11010 Training loss 0.08379118144512177 Validation loss 0.09611999243497849 Accuracy 0.7125000357627869\n",
      "Iteration 11020 Training loss 0.12245435267686844 Validation loss 0.11359655857086182 Accuracy 0.6370000243186951\n",
      "Iteration 11030 Training loss 0.0879017636179924 Validation loss 0.0941082313656807 Accuracy 0.7355000376701355\n",
      "Iteration 11040 Training loss 0.08654581010341644 Validation loss 0.10301875323057175 Accuracy 0.6810000538825989\n",
      "Iteration 11050 Training loss 0.10751956701278687 Validation loss 0.10850118100643158 Accuracy 0.6490000486373901\n",
      "Iteration 11060 Training loss 0.0971168652176857 Validation loss 0.10346260666847229 Accuracy 0.7065000534057617\n",
      "Iteration 11070 Training loss 0.091743603348732 Validation loss 0.09675060957670212 Accuracy 0.7230000495910645\n",
      "Iteration 11080 Training loss 0.10709165036678314 Validation loss 0.10567659139633179 Accuracy 0.6615000367164612\n",
      "Iteration 11090 Training loss 0.10599817335605621 Validation loss 0.10040228813886642 Accuracy 0.687000036239624\n",
      "Iteration 11100 Training loss 0.10036438703536987 Validation loss 0.1014007031917572 Accuracy 0.6910000443458557\n",
      "Iteration 11110 Training loss 0.09408155083656311 Validation loss 0.09505434334278107 Accuracy 0.7105000615119934\n",
      "Iteration 11120 Training loss 0.09267733991146088 Validation loss 0.09635966271162033 Accuracy 0.7145000100135803\n",
      "Iteration 11130 Training loss 0.08959293365478516 Validation loss 0.09450850635766983 Accuracy 0.7245000600814819\n",
      "Iteration 11140 Training loss 0.10193521529436111 Validation loss 0.10287555307149887 Accuracy 0.6790000200271606\n",
      "Iteration 11150 Training loss 0.08675742894411087 Validation loss 0.09323758631944656 Accuracy 0.7265000343322754\n",
      "Iteration 11160 Training loss 0.103666752576828 Validation loss 0.10398723930120468 Accuracy 0.6845000386238098\n",
      "Iteration 11170 Training loss 0.10105930268764496 Validation loss 0.1058371514081955 Accuracy 0.6640000343322754\n",
      "Iteration 11180 Training loss 0.11052251607179642 Validation loss 0.09543240070343018 Accuracy 0.7250000238418579\n",
      "Iteration 11190 Training loss 0.10873547196388245 Validation loss 0.10458949953317642 Accuracy 0.6700000166893005\n",
      "Iteration 11200 Training loss 0.11482539772987366 Validation loss 0.10770031809806824 Accuracy 0.640500009059906\n",
      "Iteration 11210 Training loss 0.0978453978896141 Validation loss 0.10103807598352432 Accuracy 0.6985000371932983\n",
      "Iteration 11220 Training loss 0.10547460615634918 Validation loss 0.10225129127502441 Accuracy 0.7015000581741333\n",
      "Iteration 11230 Training loss 0.09050627797842026 Validation loss 0.09303882718086243 Accuracy 0.7385000586509705\n",
      "Iteration 11240 Training loss 0.07963983714580536 Validation loss 0.09916539490222931 Accuracy 0.7035000324249268\n",
      "Iteration 11250 Training loss 0.07669304311275482 Validation loss 0.09430506825447083 Accuracy 0.7330000400543213\n",
      "Iteration 11260 Training loss 0.09384945780038834 Validation loss 0.09617513418197632 Accuracy 0.718500018119812\n",
      "Iteration 11270 Training loss 0.08889393508434296 Validation loss 0.0966399684548378 Accuracy 0.7130000591278076\n",
      "Iteration 11280 Training loss 0.09243670105934143 Validation loss 0.10691812634468079 Accuracy 0.6740000247955322\n",
      "Iteration 11290 Training loss 0.09464889019727707 Validation loss 0.09539605677127838 Accuracy 0.7150000333786011\n",
      "Iteration 11300 Training loss 0.11487487703561783 Validation loss 0.10459520667791367 Accuracy 0.6695000529289246\n",
      "Iteration 11310 Training loss 0.10615234076976776 Validation loss 0.0980740338563919 Accuracy 0.6985000371932983\n",
      "Iteration 11320 Training loss 0.14696374535560608 Validation loss 0.15807296335697174 Accuracy 0.5015000104904175\n",
      "Iteration 11330 Training loss 0.09950292110443115 Validation loss 0.10415519773960114 Accuracy 0.6790000200271606\n",
      "Iteration 11340 Training loss 0.10236737132072449 Validation loss 0.0926448404788971 Accuracy 0.7220000624656677\n",
      "Iteration 11350 Training loss 0.11237084865570068 Validation loss 0.10582055151462555 Accuracy 0.6665000319480896\n",
      "Iteration 11360 Training loss 0.11142608523368835 Validation loss 0.10388907045125961 Accuracy 0.6770000457763672\n",
      "Iteration 11370 Training loss 0.096074178814888 Validation loss 0.09556915611028671 Accuracy 0.7290000319480896\n",
      "Iteration 11380 Training loss 0.10504934191703796 Validation loss 0.09630922973155975 Accuracy 0.7105000615119934\n",
      "Iteration 11390 Training loss 0.08867022395133972 Validation loss 0.0972147211432457 Accuracy 0.7070000171661377\n",
      "Iteration 11400 Training loss 0.09540674090385437 Validation loss 0.094249427318573 Accuracy 0.7430000305175781\n",
      "Iteration 11410 Training loss 0.09871255606412888 Validation loss 0.0988081693649292 Accuracy 0.6915000081062317\n",
      "Iteration 11420 Training loss 0.10245729982852936 Validation loss 0.09617960453033447 Accuracy 0.7235000133514404\n",
      "Iteration 11430 Training loss 0.13573327660560608 Validation loss 0.12030034512281418 Accuracy 0.5885000228881836\n",
      "Iteration 11440 Training loss 0.09338372200727463 Validation loss 0.09913181513547897 Accuracy 0.6920000314712524\n",
      "Iteration 11450 Training loss 0.11260336637496948 Validation loss 0.10474362969398499 Accuracy 0.6805000305175781\n",
      "Iteration 11460 Training loss 0.09317561984062195 Validation loss 0.10094074159860611 Accuracy 0.6910000443458557\n",
      "Iteration 11470 Training loss 0.09702036529779434 Validation loss 0.09570733457803726 Accuracy 0.718500018119812\n",
      "Iteration 11480 Training loss 0.09434564411640167 Validation loss 0.1027902290225029 Accuracy 0.6760000586509705\n",
      "Iteration 11490 Training loss 0.11074315011501312 Validation loss 0.10556351393461227 Accuracy 0.6450000405311584\n",
      "Iteration 11500 Training loss 0.08788791298866272 Validation loss 0.10989615321159363 Accuracy 0.6330000162124634\n",
      "Iteration 11510 Training loss 0.098483607172966 Validation loss 0.09928721189498901 Accuracy 0.690000057220459\n",
      "Iteration 11520 Training loss 0.107167087495327 Validation loss 0.10343658179044724 Accuracy 0.6785000562667847\n",
      "Iteration 11530 Training loss 0.08485910296440125 Validation loss 0.09795976430177689 Accuracy 0.6930000185966492\n",
      "Iteration 11540 Training loss 0.12120708078145981 Validation loss 0.1096150130033493 Accuracy 0.6480000019073486\n",
      "Iteration 11550 Training loss 0.11115295439958572 Validation loss 0.10717891156673431 Accuracy 0.6605000495910645\n",
      "Iteration 11560 Training loss 0.08809851109981537 Validation loss 0.09527069330215454 Accuracy 0.7230000495910645\n",
      "Iteration 11570 Training loss 0.10059916973114014 Validation loss 0.10384991765022278 Accuracy 0.6815000176429749\n",
      "Iteration 11580 Training loss 0.10395678132772446 Validation loss 0.09942303597927094 Accuracy 0.6970000267028809\n",
      "Iteration 11590 Training loss 0.10427667200565338 Validation loss 0.10658391565084457 Accuracy 0.6455000042915344\n",
      "Iteration 11600 Training loss 0.09677983820438385 Validation loss 0.09971350431442261 Accuracy 0.7160000205039978\n",
      "Iteration 11610 Training loss 0.09656044095754623 Validation loss 0.09652348607778549 Accuracy 0.7085000276565552\n",
      "Iteration 11620 Training loss 0.08650984615087509 Validation loss 0.09184139966964722 Accuracy 0.7435000538825989\n",
      "Iteration 11630 Training loss 0.10916079580783844 Validation loss 0.10160240530967712 Accuracy 0.6790000200271606\n",
      "Iteration 11640 Training loss 0.10485272854566574 Validation loss 0.09776723384857178 Accuracy 0.7050000429153442\n",
      "Iteration 11650 Training loss 0.08633842319250107 Validation loss 0.09627442061901093 Accuracy 0.7190000414848328\n",
      "Iteration 11660 Training loss 0.11507903039455414 Validation loss 0.11378885060548782 Accuracy 0.6075000166893005\n",
      "Iteration 11670 Training loss 0.0902644693851471 Validation loss 0.10030780732631683 Accuracy 0.6935000419616699\n",
      "Iteration 11680 Training loss 0.10617589950561523 Validation loss 0.10088443011045456 Accuracy 0.6965000033378601\n",
      "Iteration 11690 Training loss 0.09837856143712997 Validation loss 0.10288576781749725 Accuracy 0.6885000467300415\n",
      "Iteration 11700 Training loss 0.09620676934719086 Validation loss 0.09758661687374115 Accuracy 0.7150000333786011\n",
      "Iteration 11710 Training loss 0.09776805341243744 Validation loss 0.09898952394723892 Accuracy 0.7085000276565552\n",
      "Iteration 11720 Training loss 0.10534320771694183 Validation loss 0.11592154949903488 Accuracy 0.6235000491142273\n",
      "Iteration 11730 Training loss 0.0943022146821022 Validation loss 0.09999223798513412 Accuracy 0.6975000500679016\n",
      "Iteration 11740 Training loss 0.10003220289945602 Validation loss 0.1123388484120369 Accuracy 0.6535000205039978\n",
      "Iteration 11750 Training loss 0.09299129247665405 Validation loss 0.10132996737957001 Accuracy 0.6925000548362732\n",
      "Iteration 11760 Training loss 0.08994710445404053 Validation loss 0.09586665779352188 Accuracy 0.7285000085830688\n",
      "Iteration 11770 Training loss 0.11257901787757874 Validation loss 0.09852897375822067 Accuracy 0.7090000510215759\n",
      "Iteration 11780 Training loss 0.09606818854808807 Validation loss 0.1017332375049591 Accuracy 0.6880000233650208\n",
      "Iteration 11790 Training loss 0.09075350314378738 Validation loss 0.09207655489444733 Accuracy 0.7295000553131104\n",
      "Iteration 11800 Training loss 0.09294337034225464 Validation loss 0.10025301575660706 Accuracy 0.6945000290870667\n",
      "Iteration 11810 Training loss 0.08698634803295135 Validation loss 0.10075540840625763 Accuracy 0.6965000033378601\n",
      "Iteration 11820 Training loss 0.09336086362600327 Validation loss 0.10661165416240692 Accuracy 0.6695000529289246\n",
      "Iteration 11830 Training loss 0.09775421023368835 Validation loss 0.11498024314641953 Accuracy 0.6130000352859497\n",
      "Iteration 11840 Training loss 0.09733257442712784 Validation loss 0.10381319373846054 Accuracy 0.7000000476837158\n",
      "Iteration 11850 Training loss 0.10712279379367828 Validation loss 0.10322035849094391 Accuracy 0.6800000071525574\n",
      "Iteration 11860 Training loss 0.10757869482040405 Validation loss 0.09627494215965271 Accuracy 0.7290000319480896\n",
      "Iteration 11870 Training loss 0.10096849501132965 Validation loss 0.10349206626415253 Accuracy 0.6935000419616699\n",
      "Iteration 11880 Training loss 0.08480323851108551 Validation loss 0.09605862200260162 Accuracy 0.7290000319480896\n",
      "Iteration 11890 Training loss 0.10710770636796951 Validation loss 0.10726556926965714 Accuracy 0.6830000281333923\n",
      "Iteration 11900 Training loss 0.09903769940137863 Validation loss 0.09559942781925201 Accuracy 0.7280000448226929\n",
      "Iteration 11910 Training loss 0.09160506725311279 Validation loss 0.10483997315168381 Accuracy 0.6655000448226929\n",
      "Iteration 11920 Training loss 0.09910468757152557 Validation loss 0.09514332562685013 Accuracy 0.7310000061988831\n",
      "Iteration 11930 Training loss 0.10193787515163422 Validation loss 0.10148215293884277 Accuracy 0.6920000314712524\n",
      "Iteration 11940 Training loss 0.09462790191173553 Validation loss 0.10444114357233047 Accuracy 0.6720000505447388\n",
      "Iteration 11950 Training loss 0.11338037252426147 Validation loss 0.10738089680671692 Accuracy 0.675000011920929\n",
      "Iteration 11960 Training loss 0.10753696411848068 Validation loss 0.10096114873886108 Accuracy 0.690500020980835\n",
      "Iteration 11970 Training loss 0.10902145504951477 Validation loss 0.11208205670118332 Accuracy 0.6460000276565552\n",
      "Iteration 11980 Training loss 0.09924174845218658 Validation loss 0.09740784764289856 Accuracy 0.7285000085830688\n",
      "Iteration 11990 Training loss 0.0864480659365654 Validation loss 0.09649934619665146 Accuracy 0.7110000252723694\n",
      "Iteration 12000 Training loss 0.11114141345024109 Validation loss 0.11589591205120087 Accuracy 0.5515000224113464\n",
      "Iteration 12010 Training loss 0.08074940741062164 Validation loss 0.10508518666028976 Accuracy 0.7005000114440918\n",
      "Iteration 12020 Training loss 0.09941316395998001 Validation loss 0.09969855099916458 Accuracy 0.7095000147819519\n",
      "Iteration 12030 Training loss 0.09478837996721268 Validation loss 0.10727156698703766 Accuracy 0.659000039100647\n",
      "Iteration 12040 Training loss 0.09934812039136887 Validation loss 0.10115867853164673 Accuracy 0.6815000176429749\n",
      "Iteration 12050 Training loss 0.1039329320192337 Validation loss 0.1056765466928482 Accuracy 0.6610000133514404\n",
      "Iteration 12060 Training loss 0.09752777963876724 Validation loss 0.09413155913352966 Accuracy 0.7235000133514404\n",
      "Iteration 12070 Training loss 0.09565557539463043 Validation loss 0.09793926030397415 Accuracy 0.7100000381469727\n",
      "Iteration 12080 Training loss 0.09635978192090988 Validation loss 0.10638006776571274 Accuracy 0.6575000286102295\n",
      "Iteration 12090 Training loss 0.11315015703439713 Validation loss 0.10035527497529984 Accuracy 0.7105000615119934\n",
      "Iteration 12100 Training loss 0.09295226633548737 Validation loss 0.09668755531311035 Accuracy 0.7075000405311584\n",
      "Iteration 12110 Training loss 0.11900746822357178 Validation loss 0.11934731900691986 Accuracy 0.5835000276565552\n",
      "Iteration 12120 Training loss 0.08096678555011749 Validation loss 0.10304401069879532 Accuracy 0.6855000257492065\n",
      "Iteration 12130 Training loss 0.09574370831251144 Validation loss 0.10294201225042343 Accuracy 0.6895000338554382\n",
      "Iteration 12140 Training loss 0.09315017610788345 Validation loss 0.09877992421388626 Accuracy 0.7100000381469727\n",
      "Iteration 12150 Training loss 0.1133413091301918 Validation loss 0.1058877483010292 Accuracy 0.659500002861023\n",
      "Iteration 12160 Training loss 0.09042815119028091 Validation loss 0.09518726170063019 Accuracy 0.7205000519752502\n",
      "Iteration 12170 Training loss 0.10941724479198456 Validation loss 0.10059286653995514 Accuracy 0.6945000290870667\n",
      "Iteration 12180 Training loss 0.09499778598546982 Validation loss 0.09631321579217911 Accuracy 0.7190000414848328\n",
      "Iteration 12190 Training loss 0.10621652007102966 Validation loss 0.10507307946681976 Accuracy 0.659000039100647\n",
      "Iteration 12200 Training loss 0.09651347249746323 Validation loss 0.10649751126766205 Accuracy 0.6695000529289246\n",
      "Iteration 12210 Training loss 0.09845785051584244 Validation loss 0.09394893050193787 Accuracy 0.7270000576972961\n",
      "Iteration 12220 Training loss 0.08468874543905258 Validation loss 0.10052014887332916 Accuracy 0.7035000324249268\n",
      "Iteration 12230 Training loss 0.11886787414550781 Validation loss 0.11488989740610123 Accuracy 0.6160000562667847\n",
      "Iteration 12240 Training loss 0.09942691773176193 Validation loss 0.09758919477462769 Accuracy 0.7305000424385071\n",
      "Iteration 12250 Training loss 0.10661090910434723 Validation loss 0.0998695120215416 Accuracy 0.6995000243186951\n",
      "Iteration 12260 Training loss 0.09957747161388397 Validation loss 0.1009979099035263 Accuracy 0.7035000324249268\n",
      "Iteration 12270 Training loss 0.0932140126824379 Validation loss 0.10827746242284775 Accuracy 0.6490000486373901\n",
      "Iteration 12280 Training loss 0.11565300077199936 Validation loss 0.09616674482822418 Accuracy 0.7245000600814819\n",
      "Iteration 12290 Training loss 0.08820582181215286 Validation loss 0.09779904782772064 Accuracy 0.7265000343322754\n",
      "Iteration 12300 Training loss 0.10635137557983398 Validation loss 0.10336191207170486 Accuracy 0.6775000095367432\n",
      "Iteration 12310 Training loss 0.08095420897006989 Validation loss 0.09505684673786163 Accuracy 0.7325000166893005\n",
      "Iteration 12320 Training loss 0.09474904835224152 Validation loss 0.09848914295434952 Accuracy 0.7175000309944153\n",
      "Iteration 12330 Training loss 0.09201501309871674 Validation loss 0.09893500059843063 Accuracy 0.7050000429153442\n",
      "Iteration 12340 Training loss 0.08829256892204285 Validation loss 0.09789230674505234 Accuracy 0.703000009059906\n",
      "Iteration 12350 Training loss 0.09227439016103745 Validation loss 0.10186434537172318 Accuracy 0.706000030040741\n",
      "Iteration 12360 Training loss 0.10800985246896744 Validation loss 0.09371398389339447 Accuracy 0.7280000448226929\n",
      "Iteration 12370 Training loss 0.08879678696393967 Validation loss 0.1016196683049202 Accuracy 0.6890000104904175\n",
      "Iteration 12380 Training loss 0.14537492394447327 Validation loss 0.12294559925794601 Accuracy 0.5790000557899475\n",
      "Iteration 12390 Training loss 0.11772990971803665 Validation loss 0.11686690896749496 Accuracy 0.6170000433921814\n",
      "Iteration 12400 Training loss 0.11086324602365494 Validation loss 0.10831429064273834 Accuracy 0.6495000123977661\n",
      "Iteration 12410 Training loss 0.09564729779958725 Validation loss 0.09701036661863327 Accuracy 0.7325000166893005\n",
      "Iteration 12420 Training loss 0.09365672618150711 Validation loss 0.09508489072322845 Accuracy 0.7380000352859497\n",
      "Iteration 12430 Training loss 0.0992458313703537 Validation loss 0.10158398002386093 Accuracy 0.6980000138282776\n",
      "Iteration 12440 Training loss 0.10720954835414886 Validation loss 0.10138452053070068 Accuracy 0.7150000333786011\n",
      "Iteration 12450 Training loss 0.08666234463453293 Validation loss 0.09311163425445557 Accuracy 0.7270000576972961\n",
      "Iteration 12460 Training loss 0.08668592572212219 Validation loss 0.09982837736606598 Accuracy 0.6910000443458557\n",
      "Iteration 12470 Training loss 0.11003322154283524 Validation loss 0.10053635388612747 Accuracy 0.6990000605583191\n",
      "Iteration 12480 Training loss 0.08943818509578705 Validation loss 0.09745440632104874 Accuracy 0.7095000147819519\n",
      "Iteration 12490 Training loss 0.10328402370214462 Validation loss 0.09252260625362396 Accuracy 0.7405000329017639\n",
      "Iteration 12500 Training loss 0.1431826800107956 Validation loss 0.12465716153383255 Accuracy 0.5910000205039978\n",
      "Iteration 12510 Training loss 0.08413272351026535 Validation loss 0.09345630556344986 Accuracy 0.737500011920929\n",
      "Iteration 12520 Training loss 0.09971105307340622 Validation loss 0.10265737771987915 Accuracy 0.687000036239624\n",
      "Iteration 12530 Training loss 0.08661891520023346 Validation loss 0.1002664789557457 Accuracy 0.706000030040741\n",
      "Iteration 12540 Training loss 0.0835893526673317 Validation loss 0.10251947492361069 Accuracy 0.6790000200271606\n",
      "Iteration 12550 Training loss 0.08737104386091232 Validation loss 0.0937960147857666 Accuracy 0.7420000433921814\n",
      "Iteration 12560 Training loss 0.10357614606618881 Validation loss 0.09859457612037659 Accuracy 0.7055000066757202\n",
      "Iteration 12570 Training loss 0.1097177118062973 Validation loss 0.10580148547887802 Accuracy 0.6540000438690186\n",
      "Iteration 12580 Training loss 0.10359380394220352 Validation loss 0.09363501518964767 Accuracy 0.7265000343322754\n",
      "Iteration 12590 Training loss 0.07977030426263809 Validation loss 0.09171972423791885 Accuracy 0.7415000200271606\n",
      "Iteration 12600 Training loss 0.09557656943798065 Validation loss 0.095046266913414 Accuracy 0.7120000123977661\n",
      "Iteration 12610 Training loss 0.0956757515668869 Validation loss 0.09853585064411163 Accuracy 0.7050000429153442\n",
      "Iteration 12620 Training loss 0.09008315205574036 Validation loss 0.09952879697084427 Accuracy 0.70250004529953\n",
      "Iteration 12630 Training loss 0.09863836318254471 Validation loss 0.09825661778450012 Accuracy 0.7055000066757202\n",
      "Iteration 12640 Training loss 0.1317407637834549 Validation loss 0.11010964214801788 Accuracy 0.6580000519752502\n",
      "Iteration 12650 Training loss 0.09643948078155518 Validation loss 0.10001762211322784 Accuracy 0.7045000195503235\n",
      "Iteration 12660 Training loss 0.10910338163375854 Validation loss 0.09743016958236694 Accuracy 0.7315000295639038\n",
      "Iteration 12670 Training loss 0.09474928677082062 Validation loss 0.09635157883167267 Accuracy 0.7265000343322754\n",
      "Iteration 12680 Training loss 0.11428969353437424 Validation loss 0.09446277469396591 Accuracy 0.7290000319480896\n",
      "Iteration 12690 Training loss 0.10706748813390732 Validation loss 0.10984183847904205 Accuracy 0.6665000319480896\n",
      "Iteration 12700 Training loss 0.09981974214315414 Validation loss 0.09967082738876343 Accuracy 0.7000000476837158\n",
      "Iteration 12710 Training loss 0.10168883949518204 Validation loss 0.09592119604349136 Accuracy 0.721500039100647\n",
      "Iteration 12720 Training loss 0.09105141460895538 Validation loss 0.10196826606988907 Accuracy 0.6955000162124634\n",
      "Iteration 12730 Training loss 0.09619595110416412 Validation loss 0.10395403206348419 Accuracy 0.6945000290870667\n",
      "Iteration 12740 Training loss 0.1022343635559082 Validation loss 0.10306679457426071 Accuracy 0.6785000562667847\n",
      "Iteration 12750 Training loss 0.10883760452270508 Validation loss 0.09497528523206711 Accuracy 0.7390000224113464\n",
      "Iteration 12760 Training loss 0.09822151809930801 Validation loss 0.10794398933649063 Accuracy 0.6575000286102295\n",
      "Iteration 12770 Training loss 0.09616386145353317 Validation loss 0.09712851047515869 Accuracy 0.721500039100647\n",
      "Iteration 12780 Training loss 0.0847572535276413 Validation loss 0.09276799112558365 Accuracy 0.7410000562667847\n",
      "Iteration 12790 Training loss 0.10688220709562302 Validation loss 0.1107696145772934 Accuracy 0.6420000195503235\n",
      "Iteration 12800 Training loss 0.09628013521432877 Validation loss 0.09682497382164001 Accuracy 0.7250000238418579\n",
      "Iteration 12810 Training loss 0.08821135014295578 Validation loss 0.0954362154006958 Accuracy 0.718000054359436\n",
      "Iteration 12820 Training loss 0.09690547734498978 Validation loss 0.10460538417100906 Accuracy 0.6790000200271606\n",
      "Iteration 12830 Training loss 0.08080857247114182 Validation loss 0.09557553380727768 Accuracy 0.7240000367164612\n",
      "Iteration 12840 Training loss 0.11035164445638657 Validation loss 0.10371776670217514 Accuracy 0.6850000619888306\n",
      "Iteration 12850 Training loss 0.08931926637887955 Validation loss 0.0970519557595253 Accuracy 0.7170000076293945\n",
      "Iteration 12860 Training loss 0.0994439423084259 Validation loss 0.1052129790186882 Accuracy 0.6690000295639038\n",
      "Iteration 12870 Training loss 0.08906308561563492 Validation loss 0.09977434575557709 Accuracy 0.703000009059906\n",
      "Iteration 12880 Training loss 0.08660375326871872 Validation loss 0.09758118540048599 Accuracy 0.7255000472068787\n",
      "Iteration 12890 Training loss 0.10097943991422653 Validation loss 0.0976630300283432 Accuracy 0.7065000534057617\n",
      "Iteration 12900 Training loss 0.0941086933016777 Validation loss 0.10521051287651062 Accuracy 0.6725000143051147\n",
      "Iteration 12910 Training loss 0.09614099562168121 Validation loss 0.09551307559013367 Accuracy 0.7315000295639038\n",
      "Iteration 12920 Training loss 0.10222205519676208 Validation loss 0.09802091121673584 Accuracy 0.7075000405311584\n",
      "Iteration 12930 Training loss 0.09191639721393585 Validation loss 0.09575186669826508 Accuracy 0.7195000052452087\n",
      "Iteration 12940 Training loss 0.11000507324934006 Validation loss 0.09918293356895447 Accuracy 0.7045000195503235\n",
      "Iteration 12950 Training loss 0.11094169318675995 Validation loss 0.09883461892604828 Accuracy 0.7170000076293945\n",
      "Iteration 12960 Training loss 0.09100542217493057 Validation loss 0.09378882497549057 Accuracy 0.7205000519752502\n",
      "Iteration 12970 Training loss 0.09180879592895508 Validation loss 0.09663337469100952 Accuracy 0.7190000414848328\n",
      "Iteration 12980 Training loss 0.10467459261417389 Validation loss 0.09253465384244919 Accuracy 0.7320000529289246\n",
      "Iteration 12990 Training loss 0.08253610134124756 Validation loss 0.09964408725500107 Accuracy 0.7135000228881836\n",
      "Iteration 13000 Training loss 0.08380573987960815 Validation loss 0.0941353365778923 Accuracy 0.7395000457763672\n",
      "Iteration 13010 Training loss 0.1247873306274414 Validation loss 0.10324887931346893 Accuracy 0.690000057220459\n",
      "Iteration 13020 Training loss 0.08770349621772766 Validation loss 0.09561260044574738 Accuracy 0.7150000333786011\n",
      "Iteration 13030 Training loss 0.08275355398654938 Validation loss 0.09584185481071472 Accuracy 0.7230000495910645\n",
      "Iteration 13040 Training loss 0.08511379361152649 Validation loss 0.09760099649429321 Accuracy 0.7220000624656677\n",
      "Iteration 13050 Training loss 0.10241325199604034 Validation loss 0.09768930822610855 Accuracy 0.7200000286102295\n",
      "Iteration 13060 Training loss 0.0912855714559555 Validation loss 0.09454471617937088 Accuracy 0.7250000238418579\n",
      "Iteration 13070 Training loss 0.16490602493286133 Validation loss 0.1335553675889969 Accuracy 0.5764999985694885\n",
      "Iteration 13080 Training loss 0.10035661607980728 Validation loss 0.09648235887289047 Accuracy 0.737500011920929\n",
      "Iteration 13090 Training loss 0.08673720806837082 Validation loss 0.09577455371618271 Accuracy 0.734000027179718\n",
      "Iteration 13100 Training loss 0.08183330297470093 Validation loss 0.09780967235565186 Accuracy 0.7170000076293945\n",
      "Iteration 13110 Training loss 0.1013944149017334 Validation loss 0.10626333206892014 Accuracy 0.6815000176429749\n",
      "Iteration 13120 Training loss 0.09978959709405899 Validation loss 0.09496090561151505 Accuracy 0.7225000262260437\n",
      "Iteration 13130 Training loss 0.08705732226371765 Validation loss 0.10706236958503723 Accuracy 0.6830000281333923\n",
      "Iteration 13140 Training loss 0.10167024284601212 Validation loss 0.09743378311395645 Accuracy 0.7220000624656677\n",
      "Iteration 13150 Training loss 0.10940848290920258 Validation loss 0.09725583344697952 Accuracy 0.7135000228881836\n",
      "Iteration 13160 Training loss 0.09301366657018661 Validation loss 0.09951824694871902 Accuracy 0.7150000333786011\n",
      "Iteration 13170 Training loss 0.0894581750035286 Validation loss 0.10057369619607925 Accuracy 0.6925000548362732\n",
      "Iteration 13180 Training loss 0.09748010337352753 Validation loss 0.09878560155630112 Accuracy 0.7085000276565552\n",
      "Iteration 13190 Training loss 0.10365091264247894 Validation loss 0.10019876807928085 Accuracy 0.7015000581741333\n",
      "Iteration 13200 Training loss 0.0995231419801712 Validation loss 0.09808465093374252 Accuracy 0.7190000414848328\n",
      "Iteration 13210 Training loss 0.14450378715991974 Validation loss 0.1475066840648651 Accuracy 0.5065000057220459\n",
      "Iteration 13220 Training loss 0.09972745925188065 Validation loss 0.11075454950332642 Accuracy 0.6495000123977661\n",
      "Iteration 13230 Training loss 0.10535918921232224 Validation loss 0.09996393322944641 Accuracy 0.7000000476837158\n",
      "Iteration 13240 Training loss 0.10305431485176086 Validation loss 0.0984954684972763 Accuracy 0.7150000333786011\n",
      "Iteration 13250 Training loss 0.08608710765838623 Validation loss 0.09606026113033295 Accuracy 0.7170000076293945\n",
      "Iteration 13260 Training loss 0.10213592648506165 Validation loss 0.09798405319452286 Accuracy 0.7200000286102295\n",
      "Iteration 13270 Training loss 0.09390893578529358 Validation loss 0.0959821566939354 Accuracy 0.7195000052452087\n",
      "Iteration 13280 Training loss 0.10954955220222473 Validation loss 0.1035405769944191 Accuracy 0.6725000143051147\n",
      "Iteration 13290 Training loss 0.0964568704366684 Validation loss 0.09442625194787979 Accuracy 0.733500063419342\n",
      "Iteration 13300 Training loss 0.10922551900148392 Validation loss 0.10860543698072433 Accuracy 0.6710000038146973\n",
      "Iteration 13310 Training loss 0.09173077344894409 Validation loss 0.09578122198581696 Accuracy 0.7175000309944153\n",
      "Iteration 13320 Training loss 0.09237609803676605 Validation loss 0.09487269073724747 Accuracy 0.7140000462532043\n",
      "Iteration 13330 Training loss 0.09893916547298431 Validation loss 0.0983705222606659 Accuracy 0.7085000276565552\n",
      "Iteration 13340 Training loss 0.09236273914575577 Validation loss 0.09375123679637909 Accuracy 0.7285000085830688\n",
      "Iteration 13350 Training loss 0.1026192232966423 Validation loss 0.09858707338571548 Accuracy 0.7065000534057617\n",
      "Iteration 13360 Training loss 0.10535143315792084 Validation loss 0.10483425110578537 Accuracy 0.6650000214576721\n",
      "Iteration 13370 Training loss 0.10669118911027908 Validation loss 0.09954512864351273 Accuracy 0.7010000348091125\n",
      "Iteration 13380 Training loss 0.12433343380689621 Validation loss 0.12069082260131836 Accuracy 0.5955000519752502\n",
      "Iteration 13390 Training loss 0.10986970365047455 Validation loss 0.10292094945907593 Accuracy 0.6990000605583191\n",
      "Iteration 13400 Training loss 0.08870105445384979 Validation loss 0.09563744068145752 Accuracy 0.734000027179718\n",
      "Iteration 13410 Training loss 0.12086686491966248 Validation loss 0.10999207198619843 Accuracy 0.6425000429153442\n",
      "Iteration 13420 Training loss 0.09997498244047165 Validation loss 0.1024792343378067 Accuracy 0.6860000491142273\n",
      "Iteration 13430 Training loss 0.11070231348276138 Validation loss 0.10395056754350662 Accuracy 0.6720000505447388\n",
      "Iteration 13440 Training loss 0.08721022307872772 Validation loss 0.1064290925860405 Accuracy 0.6725000143051147\n",
      "Iteration 13450 Training loss 0.10271503031253815 Validation loss 0.10167661309242249 Accuracy 0.6860000491142273\n",
      "Iteration 13460 Training loss 0.14609433710575104 Validation loss 0.12385876476764679 Accuracy 0.6105000376701355\n",
      "Iteration 13470 Training loss 0.09643545001745224 Validation loss 0.09434238821268082 Accuracy 0.7365000247955322\n",
      "Iteration 13480 Training loss 0.09014556556940079 Validation loss 0.10925929993391037 Accuracy 0.6605000495910645\n",
      "Iteration 13490 Training loss 0.08367901295423508 Validation loss 0.09625259041786194 Accuracy 0.7240000367164612\n",
      "Iteration 13500 Training loss 0.09819100797176361 Validation loss 0.09811922162771225 Accuracy 0.7270000576972961\n",
      "Iteration 13510 Training loss 0.09923849254846573 Validation loss 0.1024179607629776 Accuracy 0.674500048160553\n",
      "Iteration 13520 Training loss 0.09718377143144608 Validation loss 0.09890086948871613 Accuracy 0.6995000243186951\n",
      "Iteration 13530 Training loss 0.10348071902990341 Validation loss 0.10835330933332443 Accuracy 0.6620000600814819\n",
      "Iteration 13540 Training loss 0.0964631661772728 Validation loss 0.09717573970556259 Accuracy 0.7235000133514404\n",
      "Iteration 13550 Training loss 0.10498889535665512 Validation loss 0.10318364202976227 Accuracy 0.6955000162124634\n",
      "Iteration 13560 Training loss 0.08353652060031891 Validation loss 0.10131187736988068 Accuracy 0.690500020980835\n",
      "Iteration 13570 Training loss 0.08556093275547028 Validation loss 0.09614501893520355 Accuracy 0.7200000286102295\n",
      "Iteration 13580 Training loss 0.10649515688419342 Validation loss 0.10518843680620193 Accuracy 0.6735000610351562\n",
      "Iteration 13590 Training loss 0.08837369084358215 Validation loss 0.09270801395177841 Accuracy 0.737000048160553\n",
      "Iteration 13600 Training loss 0.09630472958087921 Validation loss 0.10110141336917877 Accuracy 0.6935000419616699\n",
      "Iteration 13610 Training loss 0.09216099232435226 Validation loss 0.09507542848587036 Accuracy 0.733500063419342\n",
      "Iteration 13620 Training loss 0.10917188227176666 Validation loss 0.09923288971185684 Accuracy 0.6945000290870667\n",
      "Iteration 13630 Training loss 0.09711137413978577 Validation loss 0.10071210563182831 Accuracy 0.7175000309944153\n",
      "Iteration 13640 Training loss 0.09191480278968811 Validation loss 0.09614528715610504 Accuracy 0.7200000286102295\n",
      "Iteration 13650 Training loss 0.09520719200372696 Validation loss 0.09676755219697952 Accuracy 0.721500039100647\n",
      "Iteration 13660 Training loss 0.09319622069597244 Validation loss 0.09606043994426727 Accuracy 0.7175000309944153\n",
      "Iteration 13670 Training loss 0.1025010272860527 Validation loss 0.0997173935174942 Accuracy 0.6960000395774841\n",
      "Iteration 13680 Training loss 0.1020701602101326 Validation loss 0.10051794350147247 Accuracy 0.6880000233650208\n",
      "Iteration 13690 Training loss 0.08556653559207916 Validation loss 0.0991441085934639 Accuracy 0.7155000567436218\n",
      "Iteration 13700 Training loss 0.08740261197090149 Validation loss 0.10540666431188583 Accuracy 0.6720000505447388\n",
      "Iteration 13710 Training loss 0.09897813946008682 Validation loss 0.10184101015329361 Accuracy 0.6975000500679016\n",
      "Iteration 13720 Training loss 0.12533187866210938 Validation loss 0.12603779137134552 Accuracy 0.5905000567436218\n",
      "Iteration 13730 Training loss 0.10954824835062027 Validation loss 0.1008850634098053 Accuracy 0.6880000233650208\n",
      "Iteration 13740 Training loss 0.10188291221857071 Validation loss 0.10694493353366852 Accuracy 0.6565000414848328\n",
      "Iteration 13750 Training loss 0.111678346991539 Validation loss 0.11427748203277588 Accuracy 0.656000018119812\n",
      "Iteration 13760 Training loss 0.10231376439332962 Validation loss 0.09446679800748825 Accuracy 0.7240000367164612\n",
      "Iteration 13770 Training loss 0.10331833362579346 Validation loss 0.097849041223526 Accuracy 0.7140000462532043\n",
      "Iteration 13780 Training loss 0.0960804745554924 Validation loss 0.1015624925494194 Accuracy 0.6940000057220459\n",
      "Iteration 13790 Training loss 0.09805236011743546 Validation loss 0.09611822664737701 Accuracy 0.7265000343322754\n",
      "Iteration 13800 Training loss 0.0939449667930603 Validation loss 0.10247334092855453 Accuracy 0.690500020980835\n",
      "Iteration 13810 Training loss 0.09054712951183319 Validation loss 0.09896720200777054 Accuracy 0.7100000381469727\n",
      "Iteration 13820 Training loss 0.13536140322685242 Validation loss 0.11625580489635468 Accuracy 0.6570000052452087\n",
      "Iteration 13830 Training loss 0.11057260632514954 Validation loss 0.11663303524255753 Accuracy 0.6350000500679016\n",
      "Iteration 13840 Training loss 0.08972341567277908 Validation loss 0.09850528836250305 Accuracy 0.690500020980835\n",
      "Iteration 13850 Training loss 0.08836211264133453 Validation loss 0.09982331842184067 Accuracy 0.687000036239624\n",
      "Iteration 13860 Training loss 0.09918292611837387 Validation loss 0.0954735204577446 Accuracy 0.7265000343322754\n",
      "Iteration 13870 Training loss 0.0949583426117897 Validation loss 0.09655911475419998 Accuracy 0.7195000052452087\n",
      "Iteration 13880 Training loss 0.09334854036569595 Validation loss 0.09356479346752167 Accuracy 0.7380000352859497\n",
      "Iteration 13890 Training loss 0.09261734783649445 Validation loss 0.09258639812469482 Accuracy 0.7405000329017639\n",
      "Iteration 13900 Training loss 0.08218380063772202 Validation loss 0.0975632518529892 Accuracy 0.6985000371932983\n",
      "Iteration 13910 Training loss 0.09854820370674133 Validation loss 0.10687779635190964 Accuracy 0.656000018119812\n",
      "Iteration 13920 Training loss 0.10010549426078796 Validation loss 0.10001001507043839 Accuracy 0.703000009059906\n",
      "Iteration 13930 Training loss 0.10365840792655945 Validation loss 0.10417148470878601 Accuracy 0.6735000610351562\n",
      "Iteration 13940 Training loss 0.0901808887720108 Validation loss 0.1053735688328743 Accuracy 0.6725000143051147\n",
      "Iteration 13950 Training loss 0.10295871645212173 Validation loss 0.10272473096847534 Accuracy 0.7135000228881836\n",
      "Iteration 13960 Training loss 0.12782834470272064 Validation loss 0.11198031902313232 Accuracy 0.6500000357627869\n",
      "Iteration 13970 Training loss 0.09935484081506729 Validation loss 0.09762628376483917 Accuracy 0.7280000448226929\n",
      "Iteration 13980 Training loss 0.11261996626853943 Validation loss 0.1126183420419693 Accuracy 0.6310000419616699\n",
      "Iteration 13990 Training loss 0.1044532060623169 Validation loss 0.09811033308506012 Accuracy 0.7135000228881836\n",
      "Iteration 14000 Training loss 0.10527560114860535 Validation loss 0.09550609439611435 Accuracy 0.7110000252723694\n",
      "Iteration 14010 Training loss 0.10952850431203842 Validation loss 0.11644629389047623 Accuracy 0.581000030040741\n",
      "Iteration 14020 Training loss 0.09690996259450912 Validation loss 0.0997280478477478 Accuracy 0.7285000085830688\n",
      "Iteration 14030 Training loss 0.10428498685359955 Validation loss 0.10000171512365341 Accuracy 0.6940000057220459\n",
      "Iteration 14040 Training loss 0.07969600707292557 Validation loss 0.09400501847267151 Accuracy 0.7170000076293945\n",
      "Iteration 14050 Training loss 0.09720517694950104 Validation loss 0.09431133419275284 Accuracy 0.7230000495910645\n",
      "Iteration 14060 Training loss 0.09755264222621918 Validation loss 0.09908395260572433 Accuracy 0.6920000314712524\n",
      "Iteration 14070 Training loss 0.09270000457763672 Validation loss 0.1001565009355545 Accuracy 0.7045000195503235\n",
      "Iteration 14080 Training loss 0.09811527281999588 Validation loss 0.1062413677573204 Accuracy 0.6760000586509705\n",
      "Iteration 14090 Training loss 0.09354869276285172 Validation loss 0.10133697092533112 Accuracy 0.6965000033378601\n",
      "Iteration 14100 Training loss 0.08141593635082245 Validation loss 0.09178692102432251 Accuracy 0.7415000200271606\n",
      "Iteration 14110 Training loss 0.10334751009941101 Validation loss 0.10113506764173508 Accuracy 0.6875000596046448\n",
      "Iteration 14120 Training loss 0.11417260020971298 Validation loss 0.09629029780626297 Accuracy 0.7135000228881836\n",
      "Iteration 14130 Training loss 0.0958227887749672 Validation loss 0.09908460080623627 Accuracy 0.7000000476837158\n",
      "Iteration 14140 Training loss 0.0897107794880867 Validation loss 0.09650776535272598 Accuracy 0.7235000133514404\n",
      "Iteration 14150 Training loss 0.07825451344251633 Validation loss 0.09233009815216064 Accuracy 0.7400000095367432\n",
      "Iteration 14160 Training loss 0.10343051701784134 Validation loss 0.104520283639431 Accuracy 0.6670000553131104\n",
      "Iteration 14170 Training loss 0.09712418168783188 Validation loss 0.09840370714664459 Accuracy 0.7145000100135803\n",
      "Iteration 14180 Training loss 0.09459265321493149 Validation loss 0.09311356395483017 Accuracy 0.7420000433921814\n",
      "Iteration 14190 Training loss 0.09571737051010132 Validation loss 0.09524138271808624 Accuracy 0.7125000357627869\n",
      "Iteration 14200 Training loss 0.11228153854608536 Validation loss 0.09933584928512573 Accuracy 0.7045000195503235\n",
      "Iteration 14210 Training loss 0.09769757837057114 Validation loss 0.09669740498065948 Accuracy 0.7330000400543213\n",
      "Iteration 14220 Training loss 0.0894504114985466 Validation loss 0.09373120218515396 Accuracy 0.7245000600814819\n",
      "Iteration 14230 Training loss 0.08677024394273758 Validation loss 0.09612549096345901 Accuracy 0.7135000228881836\n",
      "Iteration 14240 Training loss 0.07993152737617493 Validation loss 0.09513544291257858 Accuracy 0.7165000438690186\n",
      "Iteration 14250 Training loss 0.06925327330827713 Validation loss 0.09151817858219147 Accuracy 0.7315000295639038\n",
      "Iteration 14260 Training loss 0.10450077056884766 Validation loss 0.09916143864393234 Accuracy 0.718500018119812\n",
      "Iteration 14270 Training loss 0.09861503541469574 Validation loss 0.09241348505020142 Accuracy 0.7350000143051147\n",
      "Iteration 14280 Training loss 0.09076527506113052 Validation loss 0.10147575289011002 Accuracy 0.7105000615119934\n",
      "Iteration 14290 Training loss 0.0793422982096672 Validation loss 0.09446800500154495 Accuracy 0.7290000319480896\n",
      "Iteration 14300 Training loss 0.09553229808807373 Validation loss 0.11218677461147308 Accuracy 0.6475000381469727\n",
      "Iteration 14310 Training loss 0.11401784420013428 Validation loss 0.12069983780384064 Accuracy 0.6100000143051147\n",
      "Iteration 14320 Training loss 0.09938730299472809 Validation loss 0.1071958914399147 Accuracy 0.671500027179718\n",
      "Iteration 14330 Training loss 0.10488998889923096 Validation loss 0.10034894198179245 Accuracy 0.6965000033378601\n",
      "Iteration 14340 Training loss 0.094950832426548 Validation loss 0.10243142396211624 Accuracy 0.6930000185966492\n",
      "Iteration 14350 Training loss 0.08863459527492523 Validation loss 0.09694920480251312 Accuracy 0.706000030040741\n",
      "Iteration 14360 Training loss 0.0954698845744133 Validation loss 0.09745731204748154 Accuracy 0.7085000276565552\n",
      "Iteration 14370 Training loss 0.08728358894586563 Validation loss 0.09763236343860626 Accuracy 0.7275000214576721\n",
      "Iteration 14380 Training loss 0.09829679876565933 Validation loss 0.09628917276859283 Accuracy 0.7265000343322754\n",
      "Iteration 14390 Training loss 0.1077699065208435 Validation loss 0.10032565891742706 Accuracy 0.690500020980835\n",
      "Iteration 14400 Training loss 0.09118343889713287 Validation loss 0.09284140914678574 Accuracy 0.7485000491142273\n",
      "Iteration 14410 Training loss 0.11058150231838226 Validation loss 0.1067708283662796 Accuracy 0.6630000472068787\n",
      "Iteration 14420 Training loss 0.10224244743585587 Validation loss 0.10012375563383102 Accuracy 0.6980000138282776\n",
      "Iteration 14430 Training loss 0.08139936625957489 Validation loss 0.09374604374170303 Accuracy 0.7300000190734863\n",
      "Iteration 14440 Training loss 0.09289929270744324 Validation loss 0.0971728190779686 Accuracy 0.7125000357627869\n",
      "Iteration 14450 Training loss 0.08891334384679794 Validation loss 0.1018730029463768 Accuracy 0.6800000071525574\n",
      "Iteration 14460 Training loss 0.1204901859164238 Validation loss 0.12914347648620605 Accuracy 0.5885000228881836\n",
      "Iteration 14470 Training loss 0.11969310790300369 Validation loss 0.10171840339899063 Accuracy 0.690500020980835\n",
      "Iteration 14480 Training loss 0.0849626436829567 Validation loss 0.0926404595375061 Accuracy 0.7345000505447388\n",
      "Iteration 14490 Training loss 0.09129957854747772 Validation loss 0.09324818849563599 Accuracy 0.7290000319480896\n",
      "Iteration 14500 Training loss 0.08389829099178314 Validation loss 0.10357367247343063 Accuracy 0.6840000152587891\n",
      "Iteration 14510 Training loss 0.10222543776035309 Validation loss 0.11109405010938644 Accuracy 0.6580000519752502\n",
      "Iteration 14520 Training loss 0.09000800549983978 Validation loss 0.10091520100831985 Accuracy 0.6915000081062317\n",
      "Iteration 14530 Training loss 0.1029023751616478 Validation loss 0.09520722180604935 Accuracy 0.7125000357627869\n",
      "Iteration 14540 Training loss 0.10271007567644119 Validation loss 0.09454508125782013 Accuracy 0.7295000553131104\n",
      "Iteration 14550 Training loss 0.09964605420827866 Validation loss 0.09610560536384583 Accuracy 0.7155000567436218\n",
      "Iteration 14560 Training loss 0.11707769334316254 Validation loss 0.11474490910768509 Accuracy 0.612500011920929\n",
      "Iteration 14570 Training loss 0.09176693111658096 Validation loss 0.09471885859966278 Accuracy 0.7150000333786011\n",
      "Iteration 14580 Training loss 0.11145581305027008 Validation loss 0.09955228865146637 Accuracy 0.6970000267028809\n",
      "Iteration 14590 Training loss 0.0953487753868103 Validation loss 0.09348335862159729 Accuracy 0.7250000238418579\n",
      "Iteration 14600 Training loss 0.09142818301916122 Validation loss 0.09308858215808868 Accuracy 0.734000027179718\n",
      "Iteration 14610 Training loss 0.10118196159601212 Validation loss 0.09981188178062439 Accuracy 0.7095000147819519\n",
      "Iteration 14620 Training loss 0.10956151783466339 Validation loss 0.10107456147670746 Accuracy 0.6950000524520874\n",
      "Iteration 14630 Training loss 0.10545429587364197 Validation loss 0.09495163708925247 Accuracy 0.7260000109672546\n",
      "Iteration 14640 Training loss 0.0919838696718216 Validation loss 0.10594643652439117 Accuracy 0.6575000286102295\n",
      "Iteration 14650 Training loss 0.10284079611301422 Validation loss 0.0962807685136795 Accuracy 0.7245000600814819\n",
      "Iteration 14660 Training loss 0.09730034321546555 Validation loss 0.10021500289440155 Accuracy 0.6950000524520874\n",
      "Iteration 14670 Training loss 0.07729589939117432 Validation loss 0.09187881648540497 Accuracy 0.7265000343322754\n",
      "Iteration 14680 Training loss 0.09296932071447372 Validation loss 0.10224446654319763 Accuracy 0.6990000605583191\n",
      "Iteration 14690 Training loss 0.11537392437458038 Validation loss 0.11090797930955887 Accuracy 0.6380000114440918\n",
      "Iteration 14700 Training loss 0.08044024556875229 Validation loss 0.09231030941009521 Accuracy 0.718000054359436\n",
      "Iteration 14710 Training loss 0.09669213742017746 Validation loss 0.11215515434741974 Accuracy 0.6340000033378601\n",
      "Iteration 14720 Training loss 0.0952916294336319 Validation loss 0.09628934413194656 Accuracy 0.7225000262260437\n",
      "Iteration 14730 Training loss 0.08421129733324051 Validation loss 0.09549008309841156 Accuracy 0.7360000610351562\n",
      "Iteration 14740 Training loss 0.07400915771722794 Validation loss 0.09091313183307648 Accuracy 0.7485000491142273\n",
      "Iteration 14750 Training loss 0.08921892940998077 Validation loss 0.1016898974776268 Accuracy 0.6825000047683716\n",
      "Iteration 14760 Training loss 0.10853246599435806 Validation loss 0.09965939819812775 Accuracy 0.7070000171661377\n",
      "Iteration 14770 Training loss 0.10520671308040619 Validation loss 0.09840414673089981 Accuracy 0.7225000262260437\n",
      "Iteration 14780 Training loss 0.09819766879081726 Validation loss 0.09711223840713501 Accuracy 0.7020000219345093\n",
      "Iteration 14790 Training loss 0.1138729453086853 Validation loss 0.11264579743146896 Accuracy 0.6585000157356262\n",
      "Iteration 14800 Training loss 0.11409547179937363 Validation loss 0.11482216417789459 Accuracy 0.6495000123977661\n",
      "Iteration 14810 Training loss 0.09558779746294022 Validation loss 0.09387034922838211 Accuracy 0.718000054359436\n",
      "Iteration 14820 Training loss 0.09054828435182571 Validation loss 0.10044010728597641 Accuracy 0.6885000467300415\n",
      "Iteration 14830 Training loss 0.0912121832370758 Validation loss 0.09531412273645401 Accuracy 0.7255000472068787\n",
      "Iteration 14840 Training loss 0.09840509295463562 Validation loss 0.09083551168441772 Accuracy 0.7295000553131104\n",
      "Iteration 14850 Training loss 0.09119558334350586 Validation loss 0.10103641450405121 Accuracy 0.6940000057220459\n",
      "Iteration 14860 Training loss 0.10100055485963821 Validation loss 0.12009324878454208 Accuracy 0.5915000438690186\n",
      "Iteration 14870 Training loss 0.10684459656476974 Validation loss 0.10180428624153137 Accuracy 0.6880000233650208\n",
      "Iteration 14880 Training loss 0.10218634456396103 Validation loss 0.0982285812497139 Accuracy 0.7200000286102295\n",
      "Iteration 14890 Training loss 0.09661343693733215 Validation loss 0.09595800191164017 Accuracy 0.7125000357627869\n",
      "Iteration 14900 Training loss 0.10379809886217117 Validation loss 0.09786355495452881 Accuracy 0.7100000381469727\n",
      "Iteration 14910 Training loss 0.09538594633340836 Validation loss 0.09512197971343994 Accuracy 0.7325000166893005\n",
      "Iteration 14920 Training loss 0.10354610532522202 Validation loss 0.10669175535440445 Accuracy 0.6705000400543213\n",
      "Iteration 14930 Training loss 0.09025949984788895 Validation loss 0.09547892212867737 Accuracy 0.7140000462532043\n",
      "Iteration 14940 Training loss 0.09982386976480484 Validation loss 0.09755730628967285 Accuracy 0.7050000429153442\n",
      "Iteration 14950 Training loss 0.0732402577996254 Validation loss 0.09491350501775742 Accuracy 0.7190000414848328\n",
      "Iteration 14960 Training loss 0.08887424319982529 Validation loss 0.09385734796524048 Accuracy 0.737000048160553\n",
      "Iteration 14970 Training loss 0.08999242633581161 Validation loss 0.11171188950538635 Accuracy 0.6415000557899475\n",
      "Iteration 14980 Training loss 0.07507885992527008 Validation loss 0.09448877722024918 Accuracy 0.7170000076293945\n",
      "Iteration 14990 Training loss 0.1047704666852951 Validation loss 0.09239768236875534 Accuracy 0.7315000295639038\n",
      "Iteration 15000 Training loss 0.12252511829137802 Validation loss 0.12005544453859329 Accuracy 0.6260000467300415\n",
      "Iteration 15010 Training loss 0.11379531770944595 Validation loss 0.09713006764650345 Accuracy 0.7160000205039978\n",
      "Iteration 15020 Training loss 0.09769989550113678 Validation loss 0.1022270992398262 Accuracy 0.6835000514984131\n",
      "Iteration 15030 Training loss 0.09280002117156982 Validation loss 0.0937991812825203 Accuracy 0.7455000281333923\n",
      "Iteration 15040 Training loss 0.09035707265138626 Validation loss 0.09647330641746521 Accuracy 0.7230000495910645\n",
      "Iteration 15050 Training loss 0.11084327101707458 Validation loss 0.09598971903324127 Accuracy 0.7160000205039978\n",
      "Iteration 15060 Training loss 0.10964420437812805 Validation loss 0.10215120762586594 Accuracy 0.6920000314712524\n",
      "Iteration 15070 Training loss 0.09139798581600189 Validation loss 0.0950099378824234 Accuracy 0.718500018119812\n",
      "Iteration 15080 Training loss 0.07480912655591965 Validation loss 0.09801323711872101 Accuracy 0.7010000348091125\n",
      "Iteration 15090 Training loss 0.12982547283172607 Validation loss 0.11201789975166321 Accuracy 0.6465000510215759\n",
      "Iteration 15100 Training loss 0.08944573998451233 Validation loss 0.09437432140111923 Accuracy 0.7230000495910645\n",
      "Iteration 15110 Training loss 0.11073805391788483 Validation loss 0.10828746855258942 Accuracy 0.643500030040741\n",
      "Iteration 15120 Training loss 0.12437856197357178 Validation loss 0.11259296536445618 Accuracy 0.656000018119812\n",
      "Iteration 15130 Training loss 0.12259607762098312 Validation loss 0.10517706722021103 Accuracy 0.6695000529289246\n",
      "Iteration 15140 Training loss 0.13426406681537628 Validation loss 0.11717084050178528 Accuracy 0.6110000014305115\n",
      "Iteration 15150 Training loss 0.10032156109809875 Validation loss 0.10142484307289124 Accuracy 0.6845000386238098\n",
      "Iteration 15160 Training loss 0.12105146050453186 Validation loss 0.1263623684644699 Accuracy 0.5180000066757202\n",
      "Iteration 15170 Training loss 0.09228574484586716 Validation loss 0.09483762830495834 Accuracy 0.7355000376701355\n",
      "Iteration 15180 Training loss 0.09085988998413086 Validation loss 0.09238871932029724 Accuracy 0.733500063419342\n",
      "Iteration 15190 Training loss 0.09757242351770401 Validation loss 0.09161875396966934 Accuracy 0.7360000610351562\n",
      "Iteration 15200 Training loss 0.08965445309877396 Validation loss 0.09792545437812805 Accuracy 0.7065000534057617\n",
      "Iteration 15210 Training loss 0.11618237942457199 Validation loss 0.10930594056844711 Accuracy 0.6485000252723694\n",
      "Iteration 15220 Training loss 0.10406644642353058 Validation loss 0.0978752002120018 Accuracy 0.70250004529953\n",
      "Iteration 15230 Training loss 0.10640369355678558 Validation loss 0.10532990097999573 Accuracy 0.6620000600814819\n",
      "Iteration 15240 Training loss 0.10412765294313431 Validation loss 0.09149713814258575 Accuracy 0.7430000305175781\n",
      "Iteration 15250 Training loss 0.09049250185489655 Validation loss 0.09786554425954819 Accuracy 0.703000009059906\n",
      "Iteration 15260 Training loss 0.08365222066640854 Validation loss 0.09340408444404602 Accuracy 0.7310000061988831\n",
      "Iteration 15270 Training loss 0.09729890525341034 Validation loss 0.09816857427358627 Accuracy 0.7150000333786011\n",
      "Iteration 15280 Training loss 0.08499964326620102 Validation loss 0.0961916372179985 Accuracy 0.7105000615119934\n",
      "Iteration 15290 Training loss 0.08627057820558548 Validation loss 0.09521391987800598 Accuracy 0.7175000309944153\n",
      "Iteration 15300 Training loss 0.090789295732975 Validation loss 0.09220129251480103 Accuracy 0.7255000472068787\n",
      "Iteration 15310 Training loss 0.10260850936174393 Validation loss 0.10047194361686707 Accuracy 0.70250004529953\n",
      "Iteration 15320 Training loss 0.1299150437116623 Validation loss 0.1218876764178276 Accuracy 0.625\n",
      "Iteration 15330 Training loss 0.10254253447055817 Validation loss 0.10608656704425812 Accuracy 0.6630000472068787\n",
      "Iteration 15340 Training loss 0.1014629602432251 Validation loss 0.09944592416286469 Accuracy 0.6920000314712524\n",
      "Iteration 15350 Training loss 0.08483375608921051 Validation loss 0.09169252961874008 Accuracy 0.7430000305175781\n",
      "Iteration 15360 Training loss 0.0809035375714302 Validation loss 0.09966852515935898 Accuracy 0.6935000419616699\n",
      "Iteration 15370 Training loss 0.10535081475973129 Validation loss 0.09744859486818314 Accuracy 0.7105000615119934\n",
      "Iteration 15380 Training loss 0.08596853166818619 Validation loss 0.09293209761381149 Accuracy 0.7355000376701355\n",
      "Iteration 15390 Training loss 0.08954209834337234 Validation loss 0.09077060222625732 Accuracy 0.7355000376701355\n",
      "Iteration 15400 Training loss 0.1069469228386879 Validation loss 0.10557644814252853 Accuracy 0.6575000286102295\n",
      "Iteration 15410 Training loss 0.100518599152565 Validation loss 0.10206960886716843 Accuracy 0.6780000329017639\n",
      "Iteration 15420 Training loss 0.09045913815498352 Validation loss 0.11104470491409302 Accuracy 0.6380000114440918\n",
      "Iteration 15430 Training loss 0.07542502135038376 Validation loss 0.09402593225240707 Accuracy 0.7235000133514404\n",
      "Iteration 15440 Training loss 0.09397084265947342 Validation loss 0.10079648345708847 Accuracy 0.6965000033378601\n",
      "Iteration 15450 Training loss 0.1006210669875145 Validation loss 0.09886379539966583 Accuracy 0.7160000205039978\n",
      "Iteration 15460 Training loss 0.10222303122282028 Validation loss 0.09883926808834076 Accuracy 0.7010000348091125\n",
      "Iteration 15470 Training loss 0.07747593522071838 Validation loss 0.0915520191192627 Accuracy 0.7260000109672546\n",
      "Iteration 15480 Training loss 0.10341110080480576 Validation loss 0.10212267190217972 Accuracy 0.6850000619888306\n",
      "Iteration 15490 Training loss 0.10397933423519135 Validation loss 0.09142470359802246 Accuracy 0.7280000448226929\n",
      "Iteration 15500 Training loss 0.10267703980207443 Validation loss 0.10151486843824387 Accuracy 0.6855000257492065\n",
      "Iteration 15510 Training loss 0.1169361099600792 Validation loss 0.1225699856877327 Accuracy 0.6110000014305115\n",
      "Iteration 15520 Training loss 0.10651149600744247 Validation loss 0.09404857456684113 Accuracy 0.733500063419342\n",
      "Iteration 15530 Training loss 0.10541170090436935 Validation loss 0.09659168124198914 Accuracy 0.7160000205039978\n",
      "Iteration 15540 Training loss 0.09327234327793121 Validation loss 0.0898398905992508 Accuracy 0.7465000152587891\n",
      "Iteration 15550 Training loss 0.089319609105587 Validation loss 0.09238429367542267 Accuracy 0.7355000376701355\n",
      "Iteration 15560 Training loss 0.09089330583810806 Validation loss 0.10541702806949615 Accuracy 0.6820000410079956\n",
      "Iteration 15570 Training loss 0.09067869931459427 Validation loss 0.09611769765615463 Accuracy 0.7075000405311584\n",
      "Iteration 15580 Training loss 0.08975253999233246 Validation loss 0.09448130428791046 Accuracy 0.7240000367164612\n",
      "Iteration 15590 Training loss 0.1029406487941742 Validation loss 0.09726440906524658 Accuracy 0.7130000591278076\n",
      "Iteration 15600 Training loss 0.08783993870019913 Validation loss 0.09915167093276978 Accuracy 0.703000009059906\n",
      "Iteration 15610 Training loss 0.09452325850725174 Validation loss 0.09871184080839157 Accuracy 0.7135000228881836\n",
      "Iteration 15620 Training loss 0.10319723933935165 Validation loss 0.09869426488876343 Accuracy 0.7020000219345093\n",
      "Iteration 15630 Training loss 0.09232760965824127 Validation loss 0.09557203948497772 Accuracy 0.7240000367164612\n",
      "Iteration 15640 Training loss 0.09196043759584427 Validation loss 0.09727906435728073 Accuracy 0.7085000276565552\n",
      "Iteration 15650 Training loss 0.10799501091241837 Validation loss 0.10343959182500839 Accuracy 0.6710000038146973\n",
      "Iteration 15660 Training loss 0.08646760880947113 Validation loss 0.09845361858606339 Accuracy 0.6940000057220459\n",
      "Iteration 15670 Training loss 0.09856210649013519 Validation loss 0.0955238863825798 Accuracy 0.7240000367164612\n",
      "Iteration 15680 Training loss 0.10534747689962387 Validation loss 0.10662905871868134 Accuracy 0.6675000190734863\n",
      "Iteration 15690 Training loss 0.08857817202806473 Validation loss 0.09319526702165604 Accuracy 0.7250000238418579\n",
      "Iteration 15700 Training loss 0.09862497448921204 Validation loss 0.09877728670835495 Accuracy 0.7005000114440918\n",
      "Iteration 15710 Training loss 0.12883955240249634 Validation loss 0.10027771443128586 Accuracy 0.703000009059906\n",
      "Iteration 15720 Training loss 0.08782248198986053 Validation loss 0.09348756819963455 Accuracy 0.7350000143051147\n",
      "Iteration 15730 Training loss 0.0797683447599411 Validation loss 0.09235259145498276 Accuracy 0.7365000247955322\n",
      "Iteration 15740 Training loss 0.08699353784322739 Validation loss 0.091707743704319 Accuracy 0.7325000166893005\n",
      "Iteration 15750 Training loss 0.09446921944618225 Validation loss 0.09568366408348083 Accuracy 0.7230000495910645\n",
      "Iteration 15760 Training loss 0.10286787152290344 Validation loss 0.09656006097793579 Accuracy 0.7175000309944153\n",
      "Iteration 15770 Training loss 0.09738796204328537 Validation loss 0.09506236761808395 Accuracy 0.7240000367164612\n",
      "Iteration 15780 Training loss 0.09544713795185089 Validation loss 0.09869402647018433 Accuracy 0.7010000348091125\n",
      "Iteration 15790 Training loss 0.1034369245171547 Validation loss 0.09798579663038254 Accuracy 0.7015000581741333\n",
      "Iteration 15800 Training loss 0.09390544891357422 Validation loss 0.09815807640552521 Accuracy 0.7095000147819519\n",
      "Iteration 15810 Training loss 0.09481053054332733 Validation loss 0.09423331171274185 Accuracy 0.7235000133514404\n",
      "Iteration 15820 Training loss 0.0914141833782196 Validation loss 0.09729088097810745 Accuracy 0.7095000147819519\n",
      "Iteration 15830 Training loss 0.09148214012384415 Validation loss 0.09317339211702347 Accuracy 0.7230000495910645\n",
      "Iteration 15840 Training loss 0.11862900108098984 Validation loss 0.10982892662286758 Accuracy 0.659000039100647\n",
      "Iteration 15850 Training loss 0.08871092647314072 Validation loss 0.09611514955759048 Accuracy 0.7210000157356262\n",
      "Iteration 15860 Training loss 0.0936385914683342 Validation loss 0.09275984764099121 Accuracy 0.7195000052452087\n",
      "Iteration 15870 Training loss 0.0926549881696701 Validation loss 0.09372889995574951 Accuracy 0.7230000495910645\n",
      "Iteration 15880 Training loss 0.09631074965000153 Validation loss 0.09561878442764282 Accuracy 0.7165000438690186\n",
      "Iteration 15890 Training loss 0.08506812900304794 Validation loss 0.0920199528336525 Accuracy 0.7260000109672546\n",
      "Iteration 15900 Training loss 0.110296830534935 Validation loss 0.10287071019411087 Accuracy 0.6990000605583191\n",
      "Iteration 15910 Training loss 0.09208077937364578 Validation loss 0.09959665685892105 Accuracy 0.690500020980835\n",
      "Iteration 15920 Training loss 0.07225491106510162 Validation loss 0.09141768515110016 Accuracy 0.734000027179718\n",
      "Iteration 15930 Training loss 0.0930345430970192 Validation loss 0.09570687264204025 Accuracy 0.7265000343322754\n",
      "Iteration 15940 Training loss 0.10091789811849594 Validation loss 0.09651393443346024 Accuracy 0.7090000510215759\n",
      "Iteration 15950 Training loss 0.09609077125787735 Validation loss 0.10410802066326141 Accuracy 0.6665000319480896\n",
      "Iteration 15960 Training loss 0.10861280560493469 Validation loss 0.11525916308164597 Accuracy 0.6145000457763672\n",
      "Iteration 15970 Training loss 0.08432099968194962 Validation loss 0.09528356045484543 Accuracy 0.7145000100135803\n",
      "Iteration 15980 Training loss 0.1260790228843689 Validation loss 0.1324489563703537 Accuracy 0.5725000500679016\n",
      "Iteration 15990 Training loss 0.11388963460922241 Validation loss 0.09869042783975601 Accuracy 0.7045000195503235\n",
      "Iteration 16000 Training loss 0.08491676300764084 Validation loss 0.0917607769370079 Accuracy 0.7385000586509705\n",
      "Iteration 16010 Training loss 0.10752388089895248 Validation loss 0.09732094407081604 Accuracy 0.7115000486373901\n",
      "Iteration 16020 Training loss 0.0925571620464325 Validation loss 0.09450295567512512 Accuracy 0.7225000262260437\n",
      "Iteration 16030 Training loss 0.0924304872751236 Validation loss 0.10403218865394592 Accuracy 0.6615000367164612\n",
      "Iteration 16040 Training loss 0.10193569213151932 Validation loss 0.09583932906389236 Accuracy 0.7140000462532043\n",
      "Iteration 16050 Training loss 0.08952080458402634 Validation loss 0.10032720118761063 Accuracy 0.6940000057220459\n",
      "Iteration 16060 Training loss 0.0945887565612793 Validation loss 0.09943299740552902 Accuracy 0.6965000033378601\n",
      "Iteration 16070 Training loss 0.10122695565223694 Validation loss 0.10061206668615341 Accuracy 0.7010000348091125\n",
      "Iteration 16080 Training loss 0.10313637554645538 Validation loss 0.09578912705183029 Accuracy 0.718500018119812\n",
      "Iteration 16090 Training loss 0.09857712686061859 Validation loss 0.09816750138998032 Accuracy 0.6990000605583191\n",
      "Iteration 16100 Training loss 0.09351301193237305 Validation loss 0.10207690298557281 Accuracy 0.6920000314712524\n",
      "Iteration 16110 Training loss 0.09156370908021927 Validation loss 0.09920617192983627 Accuracy 0.6930000185966492\n",
      "Iteration 16120 Training loss 0.09625136852264404 Validation loss 0.10094879567623138 Accuracy 0.6970000267028809\n",
      "Iteration 16130 Training loss 0.094557985663414 Validation loss 0.09487000107765198 Accuracy 0.7210000157356262\n",
      "Iteration 16140 Training loss 0.09909350425004959 Validation loss 0.09383836388587952 Accuracy 0.7365000247955322\n",
      "Iteration 16150 Training loss 0.06831016391515732 Validation loss 0.09292633831501007 Accuracy 0.7255000472068787\n",
      "Iteration 16160 Training loss 0.09566635638475418 Validation loss 0.09353142231702805 Accuracy 0.7385000586509705\n",
      "Iteration 16170 Training loss 0.10660508275032043 Validation loss 0.09045463800430298 Accuracy 0.7325000166893005\n",
      "Iteration 16180 Training loss 0.08103030174970627 Validation loss 0.09715820848941803 Accuracy 0.7110000252723694\n",
      "Iteration 16190 Training loss 0.09773385524749756 Validation loss 0.10496092587709427 Accuracy 0.6830000281333923\n",
      "Iteration 16200 Training loss 0.1115298643708229 Validation loss 0.09398535639047623 Accuracy 0.7265000343322754\n",
      "Iteration 16210 Training loss 0.16726388037204742 Validation loss 0.16885587573051453 Accuracy 0.5010000467300415\n",
      "Iteration 16220 Training loss 0.0880017802119255 Validation loss 0.0975000187754631 Accuracy 0.7175000309944153\n",
      "Iteration 16230 Training loss 0.09760667383670807 Validation loss 0.10539556294679642 Accuracy 0.6625000238418579\n",
      "Iteration 16240 Training loss 0.1100209429860115 Validation loss 0.09566812217235565 Accuracy 0.7345000505447388\n",
      "Iteration 16250 Training loss 0.1082417443394661 Validation loss 0.09930355846881866 Accuracy 0.6990000605583191\n",
      "Iteration 16260 Training loss 0.11517121642827988 Validation loss 0.12669748067855835 Accuracy 0.5860000252723694\n",
      "Iteration 16270 Training loss 0.10065450519323349 Validation loss 0.10705886781215668 Accuracy 0.6700000166893005\n",
      "Iteration 16280 Training loss 0.10325321555137634 Validation loss 0.10042033344507217 Accuracy 0.7015000581741333\n",
      "Iteration 16290 Training loss 0.09766829013824463 Validation loss 0.09699083119630814 Accuracy 0.7080000042915344\n",
      "Iteration 16300 Training loss 0.08691331744194031 Validation loss 0.09654857218265533 Accuracy 0.6985000371932983\n",
      "Iteration 16310 Training loss 0.08328819274902344 Validation loss 0.09555388242006302 Accuracy 0.7300000190734863\n",
      "Iteration 16320 Training loss 0.07324777543544769 Validation loss 0.09380171447992325 Accuracy 0.7245000600814819\n",
      "Iteration 16330 Training loss 0.0770072415471077 Validation loss 0.09023737907409668 Accuracy 0.734000027179718\n",
      "Iteration 16340 Training loss 0.10856138914823532 Validation loss 0.09679683297872543 Accuracy 0.721500039100647\n",
      "Iteration 16350 Training loss 0.09909609705209732 Validation loss 0.09284956008195877 Accuracy 0.7280000448226929\n",
      "Iteration 16360 Training loss 0.09355367720127106 Validation loss 0.09351374953985214 Accuracy 0.7380000352859497\n",
      "Iteration 16370 Training loss 0.09457021206617355 Validation loss 0.0969708189368248 Accuracy 0.7110000252723694\n",
      "Iteration 16380 Training loss 0.0867217630147934 Validation loss 0.09426256269216537 Accuracy 0.7355000376701355\n",
      "Iteration 16390 Training loss 0.11598587036132812 Validation loss 0.10661963373422623 Accuracy 0.6775000095367432\n",
      "Iteration 16400 Training loss 0.09875444322824478 Validation loss 0.09407024085521698 Accuracy 0.7310000061988831\n",
      "Iteration 16410 Training loss 0.10326984524726868 Validation loss 0.10639221966266632 Accuracy 0.6700000166893005\n",
      "Iteration 16420 Training loss 0.09273780882358551 Validation loss 0.10101437568664551 Accuracy 0.70250004529953\n",
      "Iteration 16430 Training loss 0.07397948205471039 Validation loss 0.08984079211950302 Accuracy 0.753000020980835\n",
      "Iteration 16440 Training loss 0.08567605912685394 Validation loss 0.09781482070684433 Accuracy 0.6995000243186951\n",
      "Iteration 16450 Training loss 0.10500170290470123 Validation loss 0.11225314438343048 Accuracy 0.6625000238418579\n",
      "Iteration 16460 Training loss 0.09066656231880188 Validation loss 0.09272154420614243 Accuracy 0.7320000529289246\n",
      "Iteration 16470 Training loss 0.10734241455793381 Validation loss 0.09675095975399017 Accuracy 0.7075000405311584\n",
      "Iteration 16480 Training loss 0.1027156338095665 Validation loss 0.1067306250333786 Accuracy 0.6170000433921814\n",
      "Iteration 16490 Training loss 0.10431335121393204 Validation loss 0.10272874683141708 Accuracy 0.6840000152587891\n",
      "Iteration 16500 Training loss 0.08731179684400558 Validation loss 0.09973175823688507 Accuracy 0.6920000314712524\n",
      "Iteration 16510 Training loss 0.09247001260519028 Validation loss 0.09698959439992905 Accuracy 0.7065000534057617\n",
      "Iteration 16520 Training loss 0.09151890873908997 Validation loss 0.10141713172197342 Accuracy 0.690000057220459\n",
      "Iteration 16530 Training loss 0.08819863945245743 Validation loss 0.09407424181699753 Accuracy 0.7270000576972961\n",
      "Iteration 16540 Training loss 0.09649191051721573 Validation loss 0.10311021655797958 Accuracy 0.6835000514984131\n",
      "Iteration 16550 Training loss 0.09923391044139862 Validation loss 0.09822455793619156 Accuracy 0.7205000519752502\n",
      "Iteration 16560 Training loss 0.10076155513525009 Validation loss 0.09294764697551727 Accuracy 0.7280000448226929\n",
      "Iteration 16570 Training loss 0.09183544665575027 Validation loss 0.10193103551864624 Accuracy 0.7040000557899475\n",
      "Iteration 16580 Training loss 0.0947621539235115 Validation loss 0.0969526395201683 Accuracy 0.7160000205039978\n",
      "Iteration 16590 Training loss 0.09115882962942123 Validation loss 0.08978375792503357 Accuracy 0.7510000467300415\n",
      "Iteration 16600 Training loss 0.09802982211112976 Validation loss 0.09682717174291611 Accuracy 0.7065000534057617\n",
      "Iteration 16610 Training loss 0.09147584438323975 Validation loss 0.09219786524772644 Accuracy 0.734000027179718\n",
      "Iteration 16620 Training loss 0.09501825273036957 Validation loss 0.09960092604160309 Accuracy 0.7080000042915344\n",
      "Iteration 16630 Training loss 0.1081518903374672 Validation loss 0.09979777783155441 Accuracy 0.7090000510215759\n",
      "Iteration 16640 Training loss 0.08586159348487854 Validation loss 0.09554164856672287 Accuracy 0.7170000076293945\n",
      "Iteration 16650 Training loss 0.10596715658903122 Validation loss 0.09957139939069748 Accuracy 0.7045000195503235\n",
      "Iteration 16660 Training loss 0.08688422292470932 Validation loss 0.10688948631286621 Accuracy 0.6630000472068787\n",
      "Iteration 16670 Training loss 0.10455746203660965 Validation loss 0.1169360801577568 Accuracy 0.6300000548362732\n",
      "Iteration 16680 Training loss 0.08332166820764542 Validation loss 0.09312684088945389 Accuracy 0.7325000166893005\n",
      "Iteration 16690 Training loss 0.10015109181404114 Validation loss 0.09956344962120056 Accuracy 0.6925000548362732\n",
      "Iteration 16700 Training loss 0.09707902371883392 Validation loss 0.10897354781627655 Accuracy 0.6495000123977661\n",
      "Iteration 16710 Training loss 0.10306067019701004 Validation loss 0.10254978388547897 Accuracy 0.6795000433921814\n",
      "Iteration 16720 Training loss 0.11510703712701797 Validation loss 0.10351014882326126 Accuracy 0.687000036239624\n",
      "Iteration 16730 Training loss 0.1316475123167038 Validation loss 0.1266927868127823 Accuracy 0.6035000085830688\n",
      "Iteration 16740 Training loss 0.117961086332798 Validation loss 0.12511731684207916 Accuracy 0.5840000510215759\n",
      "Iteration 16750 Training loss 0.12653712928295135 Validation loss 0.10558667778968811 Accuracy 0.6490000486373901\n",
      "Iteration 16760 Training loss 0.08297111093997955 Validation loss 0.09913107752799988 Accuracy 0.7105000615119934\n",
      "Iteration 16770 Training loss 0.08485521376132965 Validation loss 0.09041876345872879 Accuracy 0.7440000176429749\n",
      "Iteration 16780 Training loss 0.09139873087406158 Validation loss 0.0928773432970047 Accuracy 0.7320000529289246\n",
      "Iteration 16790 Training loss 0.08873771131038666 Validation loss 0.10303760319948196 Accuracy 0.6805000305175781\n",
      "Iteration 16800 Training loss 0.10537873953580856 Validation loss 0.10227620601654053 Accuracy 0.6785000562667847\n",
      "Iteration 16810 Training loss 0.14109550416469574 Validation loss 0.13127021491527557 Accuracy 0.5680000185966492\n",
      "Iteration 16820 Training loss 0.0873837098479271 Validation loss 0.09682168811559677 Accuracy 0.7040000557899475\n",
      "Iteration 16830 Training loss 0.09971148520708084 Validation loss 0.08960038423538208 Accuracy 0.7455000281333923\n",
      "Iteration 16840 Training loss 0.12486490607261658 Validation loss 0.11549109220504761 Accuracy 0.6160000562667847\n",
      "Iteration 16850 Training loss 0.09661310166120529 Validation loss 0.09558416903018951 Accuracy 0.7190000414848328\n",
      "Iteration 16860 Training loss 0.08528773486614227 Validation loss 0.09580621868371964 Accuracy 0.7130000591278076\n",
      "Iteration 16870 Training loss 0.09345857053995132 Validation loss 0.09559960663318634 Accuracy 0.7385000586509705\n",
      "Iteration 16880 Training loss 0.10059823840856552 Validation loss 0.09900159388780594 Accuracy 0.6945000290870667\n",
      "Iteration 16890 Training loss 0.09491685777902603 Validation loss 0.09560759365558624 Accuracy 0.7120000123977661\n",
      "Iteration 16900 Training loss 0.09728839993476868 Validation loss 0.09714575111865997 Accuracy 0.7045000195503235\n",
      "Iteration 16910 Training loss 0.09272654354572296 Validation loss 0.09679317474365234 Accuracy 0.7050000429153442\n",
      "Iteration 16920 Training loss 0.09171879291534424 Validation loss 0.09409890323877335 Accuracy 0.7230000495910645\n",
      "Iteration 16930 Training loss 0.09923053532838821 Validation loss 0.0898701548576355 Accuracy 0.7405000329017639\n",
      "Iteration 16940 Training loss 0.07752542197704315 Validation loss 0.0964621752500534 Accuracy 0.7175000309944153\n",
      "Iteration 16950 Training loss 0.09479488432407379 Validation loss 0.09337234497070312 Accuracy 0.7290000319480896\n",
      "Iteration 16960 Training loss 0.08112701028585434 Validation loss 0.09740908443927765 Accuracy 0.7080000042915344\n",
      "Iteration 16970 Training loss 0.08693820983171463 Validation loss 0.09947851300239563 Accuracy 0.6975000500679016\n",
      "Iteration 16980 Training loss 0.09903007000684738 Validation loss 0.09470534324645996 Accuracy 0.7160000205039978\n",
      "Iteration 16990 Training loss 0.08270992338657379 Validation loss 0.09749054163694382 Accuracy 0.7040000557899475\n",
      "Iteration 17000 Training loss 0.09423389285802841 Validation loss 0.09616020321846008 Accuracy 0.7170000076293945\n",
      "Iteration 17010 Training loss 0.09355071932077408 Validation loss 0.09264420717954636 Accuracy 0.7240000367164612\n",
      "Iteration 17020 Training loss 0.08671794086694717 Validation loss 0.10367404669523239 Accuracy 0.6730000376701355\n",
      "Iteration 17030 Training loss 0.09468759596347809 Validation loss 0.09112420678138733 Accuracy 0.749500036239624\n",
      "Iteration 17040 Training loss 0.09852143377065659 Validation loss 0.09784244000911713 Accuracy 0.7130000591278076\n",
      "Iteration 17050 Training loss 0.07917483150959015 Validation loss 0.09916894882917404 Accuracy 0.7090000510215759\n",
      "Iteration 17060 Training loss 0.11280854046344757 Validation loss 0.10097002238035202 Accuracy 0.7135000228881836\n",
      "Iteration 17070 Training loss 0.08771567791700363 Validation loss 0.09760469198226929 Accuracy 0.7105000615119934\n",
      "Iteration 17080 Training loss 0.1151389554142952 Validation loss 0.11428404599428177 Accuracy 0.6370000243186951\n",
      "Iteration 17090 Training loss 0.08543739467859268 Validation loss 0.09981978684663773 Accuracy 0.70250004529953\n",
      "Iteration 17100 Training loss 0.09843145310878754 Validation loss 0.10657915472984314 Accuracy 0.6800000071525574\n",
      "Iteration 17110 Training loss 0.08747690916061401 Validation loss 0.1062980592250824 Accuracy 0.6650000214576721\n",
      "Iteration 17120 Training loss 0.08788429200649261 Validation loss 0.09399113059043884 Accuracy 0.733500063419342\n",
      "Iteration 17130 Training loss 0.10733124613761902 Validation loss 0.09581465274095535 Accuracy 0.718500018119812\n",
      "Iteration 17140 Training loss 0.09476932138204575 Validation loss 0.09236440062522888 Accuracy 0.7395000457763672\n",
      "Iteration 17150 Training loss 0.13255561888217926 Validation loss 0.1291402131319046 Accuracy 0.5940000414848328\n",
      "Iteration 17160 Training loss 0.1038842499256134 Validation loss 0.10468678176403046 Accuracy 0.6675000190734863\n",
      "Iteration 17170 Training loss 0.08721707761287689 Validation loss 0.09410802274942398 Accuracy 0.7300000190734863\n",
      "Iteration 17180 Training loss 0.06914282590150833 Validation loss 0.09076642990112305 Accuracy 0.7475000619888306\n",
      "Iteration 17190 Training loss 0.09251779317855835 Validation loss 0.10882070660591125 Accuracy 0.659500002861023\n",
      "Iteration 17200 Training loss 0.08949718624353409 Validation loss 0.0926741361618042 Accuracy 0.7380000352859497\n",
      "Iteration 17210 Training loss 0.07771441340446472 Validation loss 0.0977853313088417 Accuracy 0.7115000486373901\n",
      "Iteration 17220 Training loss 0.08608794957399368 Validation loss 0.09330211579799652 Accuracy 0.7330000400543213\n",
      "Iteration 17230 Training loss 0.08442895859479904 Validation loss 0.09353059530258179 Accuracy 0.7260000109672546\n",
      "Iteration 17240 Training loss 0.10177306085824966 Validation loss 0.1020594909787178 Accuracy 0.6955000162124634\n",
      "Iteration 17250 Training loss 0.09667674452066422 Validation loss 0.0999162495136261 Accuracy 0.7005000114440918\n",
      "Iteration 17260 Training loss 0.08935394138097763 Validation loss 0.09960441291332245 Accuracy 0.6955000162124634\n",
      "Iteration 17270 Training loss 0.10686976462602615 Validation loss 0.1077110767364502 Accuracy 0.6675000190734863\n",
      "Iteration 17280 Training loss 0.12885141372680664 Validation loss 0.10732864588499069 Accuracy 0.659500002861023\n",
      "Iteration 17290 Training loss 0.11852464079856873 Validation loss 0.12280300259590149 Accuracy 0.6000000238418579\n",
      "Iteration 17300 Training loss 0.09810996800661087 Validation loss 0.09668674319982529 Accuracy 0.7130000591278076\n",
      "Iteration 17310 Training loss 0.08553574234247208 Validation loss 0.09593242406845093 Accuracy 0.7285000085830688\n",
      "Iteration 17320 Training loss 0.11500362306833267 Validation loss 0.10449738800525665 Accuracy 0.6945000290870667\n",
      "Iteration 17330 Training loss 0.10181945562362671 Validation loss 0.0981716513633728 Accuracy 0.7070000171661377\n",
      "Iteration 17340 Training loss 0.09389211237430573 Validation loss 0.09314744174480438 Accuracy 0.7255000472068787\n",
      "Iteration 17350 Training loss 0.09195424616336823 Validation loss 0.09026968479156494 Accuracy 0.737000048160553\n",
      "Iteration 17360 Training loss 0.09095006436109543 Validation loss 0.09341339021921158 Accuracy 0.7160000205039978\n",
      "Iteration 17370 Training loss 0.08039750158786774 Validation loss 0.09377912431955338 Accuracy 0.7230000495910645\n",
      "Iteration 17380 Training loss 0.11054883897304535 Validation loss 0.09822124987840652 Accuracy 0.6925000548362732\n",
      "Iteration 17390 Training loss 0.11822911351919174 Validation loss 0.11989478021860123 Accuracy 0.625\n",
      "Iteration 17400 Training loss 0.0958477035164833 Validation loss 0.09491889923810959 Accuracy 0.7290000319480896\n",
      "Iteration 17410 Training loss 0.07024776190519333 Validation loss 0.08920610696077347 Accuracy 0.7460000514984131\n",
      "Iteration 17420 Training loss 0.0917210727930069 Validation loss 0.09620872884988785 Accuracy 0.7240000367164612\n",
      "Iteration 17430 Training loss 0.08526390045881271 Validation loss 0.09514978528022766 Accuracy 0.7300000190734863\n",
      "Iteration 17440 Training loss 0.13719172775745392 Validation loss 0.11588431894779205 Accuracy 0.612000048160553\n",
      "Iteration 17450 Training loss 0.11388552188873291 Validation loss 0.10929376631975174 Accuracy 0.6505000591278076\n",
      "Iteration 17460 Training loss 0.08569280803203583 Validation loss 0.09530717879533768 Accuracy 0.7140000462532043\n",
      "Iteration 17470 Training loss 0.08596477657556534 Validation loss 0.09482552111148834 Accuracy 0.7230000495910645\n",
      "Iteration 17480 Training loss 0.09999395906925201 Validation loss 0.10240950435400009 Accuracy 0.6860000491142273\n",
      "Iteration 17490 Training loss 0.1082339733839035 Validation loss 0.10222214460372925 Accuracy 0.6800000071525574\n",
      "Iteration 17500 Training loss 0.09420215338468552 Validation loss 0.09787104278802872 Accuracy 0.7175000309944153\n",
      "Iteration 17510 Training loss 0.10018640011548996 Validation loss 0.10002000629901886 Accuracy 0.690500020980835\n",
      "Iteration 17520 Training loss 0.09555714577436447 Validation loss 0.09528885036706924 Accuracy 0.7145000100135803\n",
      "Iteration 17530 Training loss 0.08410570025444031 Validation loss 0.10011176019906998 Accuracy 0.6940000057220459\n",
      "Iteration 17540 Training loss 0.08146412670612335 Validation loss 0.0997266098856926 Accuracy 0.7070000171661377\n",
      "Iteration 17550 Training loss 0.09014342725276947 Validation loss 0.09245733916759491 Accuracy 0.7460000514984131\n",
      "Iteration 17560 Training loss 0.09259072691202164 Validation loss 0.09613199532032013 Accuracy 0.7265000343322754\n",
      "Iteration 17570 Training loss 0.07308768481016159 Validation loss 0.0957120731472969 Accuracy 0.7075000405311584\n",
      "Iteration 17580 Training loss 0.09808322787284851 Validation loss 0.10334669798612595 Accuracy 0.6855000257492065\n",
      "Iteration 17590 Training loss 0.10610262304544449 Validation loss 0.10041823238134384 Accuracy 0.7040000557899475\n",
      "Iteration 17600 Training loss 0.07870486378669739 Validation loss 0.09729111194610596 Accuracy 0.7045000195503235\n",
      "Iteration 17610 Training loss 0.10043755918741226 Validation loss 0.11051005870103836 Accuracy 0.6795000433921814\n",
      "Iteration 17620 Training loss 0.09784767031669617 Validation loss 0.10059549659490585 Accuracy 0.6830000281333923\n",
      "Iteration 17630 Training loss 0.08473972976207733 Validation loss 0.09201425313949585 Accuracy 0.737500011920929\n",
      "Iteration 17640 Training loss 0.0889090895652771 Validation loss 0.10609505325555801 Accuracy 0.6620000600814819\n",
      "Iteration 17650 Training loss 0.09988145530223846 Validation loss 0.09872889518737793 Accuracy 0.7090000510215759\n",
      "Iteration 17660 Training loss 0.08207733929157257 Validation loss 0.08943060785531998 Accuracy 0.7485000491142273\n",
      "Iteration 17670 Training loss 0.07790781557559967 Validation loss 0.09399876743555069 Accuracy 0.7285000085830688\n",
      "Iteration 17680 Training loss 0.0865226536989212 Validation loss 0.09705976396799088 Accuracy 0.7145000100135803\n",
      "Iteration 17690 Training loss 0.09812039881944656 Validation loss 0.09421525150537491 Accuracy 0.7230000495910645\n",
      "Iteration 17700 Training loss 0.08799327909946442 Validation loss 0.09291475266218185 Accuracy 0.7300000190734863\n",
      "Iteration 17710 Training loss 0.09561584144830704 Validation loss 0.09407762438058853 Accuracy 0.7230000495910645\n",
      "Iteration 17720 Training loss 0.08778827637434006 Validation loss 0.09050729870796204 Accuracy 0.7485000491142273\n",
      "Iteration 17730 Training loss 0.08454824239015579 Validation loss 0.09654752910137177 Accuracy 0.7045000195503235\n",
      "Iteration 17740 Training loss 0.10096880048513412 Validation loss 0.09373880922794342 Accuracy 0.7300000190734863\n",
      "Iteration 17750 Training loss 0.07863865047693253 Validation loss 0.09257466346025467 Accuracy 0.7460000514984131\n",
      "Iteration 17760 Training loss 0.09329111129045486 Validation loss 0.10174954682588577 Accuracy 0.6960000395774841\n",
      "Iteration 17770 Training loss 0.09434612095355988 Validation loss 0.10826343297958374 Accuracy 0.6795000433921814\n",
      "Iteration 17780 Training loss 0.08739975839853287 Validation loss 0.09535793215036392 Accuracy 0.7330000400543213\n",
      "Iteration 17790 Training loss 0.08068293333053589 Validation loss 0.09573699533939362 Accuracy 0.7110000252723694\n",
      "Iteration 17800 Training loss 0.101568803191185 Validation loss 0.09619897603988647 Accuracy 0.7170000076293945\n",
      "Iteration 17810 Training loss 0.08327314257621765 Validation loss 0.09169972687959671 Accuracy 0.7465000152587891\n",
      "Iteration 17820 Training loss 0.0923096090555191 Validation loss 0.09847142547369003 Accuracy 0.7005000114440918\n",
      "Iteration 17830 Training loss 0.10478678345680237 Validation loss 0.10313130170106888 Accuracy 0.6740000247955322\n",
      "Iteration 17840 Training loss 0.09266002476215363 Validation loss 0.09131521731615067 Accuracy 0.7385000586509705\n",
      "Iteration 17850 Training loss 0.09672771394252777 Validation loss 0.09819551557302475 Accuracy 0.7130000591278076\n",
      "Iteration 17860 Training loss 0.08042257279157639 Validation loss 0.0939340814948082 Accuracy 0.7140000462532043\n",
      "Iteration 17870 Training loss 0.12185363471508026 Validation loss 0.12121333926916122 Accuracy 0.6315000057220459\n",
      "Iteration 17880 Training loss 0.08582939207553864 Validation loss 0.10105592757463455 Accuracy 0.690500020980835\n",
      "Iteration 17890 Training loss 0.08278621733188629 Validation loss 0.09349004179239273 Accuracy 0.7350000143051147\n",
      "Iteration 17900 Training loss 0.10755666345357895 Validation loss 0.11869826912879944 Accuracy 0.628000020980835\n",
      "Iteration 17910 Training loss 0.10955479741096497 Validation loss 0.09082937985658646 Accuracy 0.7305000424385071\n",
      "Iteration 17920 Training loss 0.1047515869140625 Validation loss 0.0878213495016098 Accuracy 0.7475000619888306\n",
      "Iteration 17930 Training loss 0.08862920850515366 Validation loss 0.09429067373275757 Accuracy 0.7405000329017639\n",
      "Iteration 17940 Training loss 0.10409486293792725 Validation loss 0.09713702648878098 Accuracy 0.7055000066757202\n",
      "Iteration 17950 Training loss 0.10084790736436844 Validation loss 0.09634604305028915 Accuracy 0.7195000052452087\n",
      "Iteration 17960 Training loss 0.10130423307418823 Validation loss 0.09595708549022675 Accuracy 0.7270000576972961\n",
      "Iteration 17970 Training loss 0.10356679558753967 Validation loss 0.1002817302942276 Accuracy 0.6970000267028809\n",
      "Iteration 17980 Training loss 0.11075784265995026 Validation loss 0.1079772561788559 Accuracy 0.6755000352859497\n",
      "Iteration 17990 Training loss 0.10969158262014389 Validation loss 0.10900922119617462 Accuracy 0.6445000171661377\n",
      "Iteration 18000 Training loss 0.07890437543392181 Validation loss 0.09311362355947495 Accuracy 0.7245000600814819\n",
      "Iteration 18010 Training loss 0.09224647283554077 Validation loss 0.08994497358798981 Accuracy 0.7465000152587891\n",
      "Iteration 18020 Training loss 0.09723323583602905 Validation loss 0.10443197935819626 Accuracy 0.6760000586509705\n",
      "Iteration 18030 Training loss 0.08992646634578705 Validation loss 0.09851541370153427 Accuracy 0.6925000548362732\n",
      "Iteration 18040 Training loss 0.08702035993337631 Validation loss 0.09137620776891708 Accuracy 0.7500000596046448\n",
      "Iteration 18050 Training loss 0.10395265370607376 Validation loss 0.09141749888658524 Accuracy 0.7365000247955322\n",
      "Iteration 18060 Training loss 0.09871666878461838 Validation loss 0.09741160273551941 Accuracy 0.7220000624656677\n",
      "Iteration 18070 Training loss 0.0943821594119072 Validation loss 0.09723444283008575 Accuracy 0.7125000357627869\n",
      "Iteration 18080 Training loss 0.09701725095510483 Validation loss 0.09572651982307434 Accuracy 0.7160000205039978\n",
      "Iteration 18090 Training loss 0.08682847768068314 Validation loss 0.09527803212404251 Accuracy 0.7255000472068787\n",
      "Iteration 18100 Training loss 0.1121172308921814 Validation loss 0.11121099442243576 Accuracy 0.6800000071525574\n",
      "Iteration 18110 Training loss 0.11322015523910522 Validation loss 0.09152258932590485 Accuracy 0.7330000400543213\n",
      "Iteration 18120 Training loss 0.08936984837055206 Validation loss 0.09334547072649002 Accuracy 0.7430000305175781\n",
      "Iteration 18130 Training loss 0.06991833448410034 Validation loss 0.09453776478767395 Accuracy 0.7325000166893005\n",
      "Iteration 18140 Training loss 0.0896417573094368 Validation loss 0.09241064637899399 Accuracy 0.7450000643730164\n",
      "Iteration 18150 Training loss 0.07994507253170013 Validation loss 0.09584619104862213 Accuracy 0.7160000205039978\n",
      "Iteration 18160 Training loss 0.09789653867483139 Validation loss 0.10241719335317612 Accuracy 0.6850000619888306\n",
      "Iteration 18170 Training loss 0.0904061496257782 Validation loss 0.10958998650312424 Accuracy 0.6495000123977661\n",
      "Iteration 18180 Training loss 0.11260509490966797 Validation loss 0.11179153621196747 Accuracy 0.6375000476837158\n",
      "Iteration 18190 Training loss 0.11399399489164352 Validation loss 0.11615216732025146 Accuracy 0.6025000214576721\n",
      "Iteration 18200 Training loss 0.08910251408815384 Validation loss 0.10431059449911118 Accuracy 0.6780000329017639\n",
      "Iteration 18210 Training loss 0.10102294385433197 Validation loss 0.10266909003257751 Accuracy 0.6810000538825989\n",
      "Iteration 18220 Training loss 0.10993342846632004 Validation loss 0.10891184210777283 Accuracy 0.6525000333786011\n",
      "Iteration 18230 Training loss 0.09842584282159805 Validation loss 0.09598732739686966 Accuracy 0.7135000228881836\n",
      "Iteration 18240 Training loss 0.0996948629617691 Validation loss 0.10132332891225815 Accuracy 0.6935000419616699\n",
      "Iteration 18250 Training loss 0.09433284401893616 Validation loss 0.09380467236042023 Accuracy 0.7250000238418579\n",
      "Iteration 18260 Training loss 0.10046461969614029 Validation loss 0.10433461517095566 Accuracy 0.6735000610351562\n",
      "Iteration 18270 Training loss 0.10496789962053299 Validation loss 0.09817542135715485 Accuracy 0.7085000276565552\n",
      "Iteration 18280 Training loss 0.09202621877193451 Validation loss 0.09524603188037872 Accuracy 0.7205000519752502\n",
      "Iteration 18290 Training loss 0.10354733467102051 Validation loss 0.1012752503156662 Accuracy 0.6925000548362732\n",
      "Iteration 18300 Training loss 0.0905548632144928 Validation loss 0.09966152161359787 Accuracy 0.6995000243186951\n",
      "Iteration 18310 Training loss 0.08649907261133194 Validation loss 0.09437393397092819 Accuracy 0.7330000400543213\n",
      "Iteration 18320 Training loss 0.09114895761013031 Validation loss 0.0926218330860138 Accuracy 0.7360000610351562\n",
      "Iteration 18330 Training loss 0.08333629369735718 Validation loss 0.09055766463279724 Accuracy 0.7385000586509705\n",
      "Iteration 18340 Training loss 0.09251666814088821 Validation loss 0.094154953956604 Accuracy 0.7250000238418579\n",
      "Iteration 18350 Training loss 0.07770302891731262 Validation loss 0.09070893377065659 Accuracy 0.7475000619888306\n",
      "Iteration 18360 Training loss 0.07969200611114502 Validation loss 0.09525001049041748 Accuracy 0.737000048160553\n",
      "Iteration 18370 Training loss 0.07297702878713608 Validation loss 0.09861807525157928 Accuracy 0.7120000123977661\n",
      "Iteration 18380 Training loss 0.11916372179985046 Validation loss 0.11410766839981079 Accuracy 0.6300000548362732\n",
      "Iteration 18390 Training loss 0.0698232427239418 Validation loss 0.08986906707286835 Accuracy 0.737500011920929\n",
      "Iteration 18400 Training loss 0.08797381818294525 Validation loss 0.09376595169305801 Accuracy 0.7400000095367432\n",
      "Iteration 18410 Training loss 0.09774666279554367 Validation loss 0.09272365272045135 Accuracy 0.7400000095367432\n",
      "Iteration 18420 Training loss 0.08198796957731247 Validation loss 0.09458870440721512 Accuracy 0.7200000286102295\n",
      "Iteration 18430 Training loss 0.0909864753484726 Validation loss 0.09264969080686569 Accuracy 0.721500039100647\n",
      "Iteration 18440 Training loss 0.09301158040761948 Validation loss 0.10119011253118515 Accuracy 0.6940000057220459\n",
      "Iteration 18450 Training loss 0.09410374611616135 Validation loss 0.0975499153137207 Accuracy 0.7075000405311584\n",
      "Iteration 18460 Training loss 0.12237577140331268 Validation loss 0.10202273726463318 Accuracy 0.6875000596046448\n",
      "Iteration 18470 Training loss 0.10665043443441391 Validation loss 0.10337896645069122 Accuracy 0.6810000538825989\n",
      "Iteration 18480 Training loss 0.10759757459163666 Validation loss 0.10618161410093307 Accuracy 0.6635000109672546\n",
      "Iteration 18490 Training loss 0.09178202599287033 Validation loss 0.09660704433917999 Accuracy 0.7075000405311584\n",
      "Iteration 18500 Training loss 0.08347642421722412 Validation loss 0.08959182351827621 Accuracy 0.7450000643730164\n",
      "Iteration 18510 Training loss 0.1072271317243576 Validation loss 0.09989892691373825 Accuracy 0.703000009059906\n",
      "Iteration 18520 Training loss 0.1341511756181717 Validation loss 0.12023798376321793 Accuracy 0.6305000185966492\n",
      "Iteration 18530 Training loss 0.11210115998983383 Validation loss 0.10367146134376526 Accuracy 0.6790000200271606\n",
      "Iteration 18540 Training loss 0.09008466452360153 Validation loss 0.09034591913223267 Accuracy 0.7460000514984131\n",
      "Iteration 18550 Training loss 0.10287895053625107 Validation loss 0.09951415657997131 Accuracy 0.70250004529953\n",
      "Iteration 18560 Training loss 0.10168115049600601 Validation loss 0.09850674867630005 Accuracy 0.6975000500679016\n",
      "Iteration 18570 Training loss 0.07810281217098236 Validation loss 0.09928828477859497 Accuracy 0.7095000147819519\n",
      "Iteration 18580 Training loss 0.07727789878845215 Validation loss 0.09616348892450333 Accuracy 0.7065000534057617\n",
      "Iteration 18590 Training loss 0.1187804564833641 Validation loss 0.10241304337978363 Accuracy 0.6655000448226929\n",
      "Iteration 18600 Training loss 0.10796200484037399 Validation loss 0.11813180893659592 Accuracy 0.6315000057220459\n",
      "Iteration 18610 Training loss 0.09831586480140686 Validation loss 0.10508653521537781 Accuracy 0.6730000376701355\n",
      "Iteration 18620 Training loss 0.10811503976583481 Validation loss 0.10468313097953796 Accuracy 0.6725000143051147\n",
      "Iteration 18630 Training loss 0.0876789391040802 Validation loss 0.0971263200044632 Accuracy 0.7080000042915344\n",
      "Iteration 18640 Training loss 0.10430487990379333 Validation loss 0.09835027903318405 Accuracy 0.7065000534057617\n",
      "Iteration 18650 Training loss 0.09972488135099411 Validation loss 0.09280377626419067 Accuracy 0.7405000329017639\n",
      "Iteration 18660 Training loss 0.10522185266017914 Validation loss 0.10202247649431229 Accuracy 0.6815000176429749\n",
      "Iteration 18670 Training loss 0.08830224722623825 Validation loss 0.08897270262241364 Accuracy 0.7425000071525574\n",
      "Iteration 18680 Training loss 0.10358869284391403 Validation loss 0.09932682663202286 Accuracy 0.6950000524520874\n",
      "Iteration 18690 Training loss 0.10032718628644943 Validation loss 0.09527972340583801 Accuracy 0.7210000157356262\n",
      "Iteration 18700 Training loss 0.10830523073673248 Validation loss 0.1051427498459816 Accuracy 0.6770000457763672\n",
      "Iteration 18710 Training loss 0.0838312953710556 Validation loss 0.10571213811635971 Accuracy 0.671500027179718\n",
      "Iteration 18720 Training loss 0.10868227481842041 Validation loss 0.10199248045682907 Accuracy 0.6825000047683716\n",
      "Iteration 18730 Training loss 0.1240137442946434 Validation loss 0.1063280999660492 Accuracy 0.6585000157356262\n",
      "Iteration 18740 Training loss 0.09687883406877518 Validation loss 0.1113903746008873 Accuracy 0.6505000591278076\n",
      "Iteration 18750 Training loss 0.10024520754814148 Validation loss 0.09660126268863678 Accuracy 0.7110000252723694\n",
      "Iteration 18760 Training loss 0.0817529484629631 Validation loss 0.0965282991528511 Accuracy 0.7130000591278076\n",
      "Iteration 18770 Training loss 0.0937613919377327 Validation loss 0.09765072166919708 Accuracy 0.7150000333786011\n",
      "Iteration 18780 Training loss 0.10448309034109116 Validation loss 0.09148425608873367 Accuracy 0.7350000143051147\n",
      "Iteration 18790 Training loss 0.08419649302959442 Validation loss 0.09196019917726517 Accuracy 0.7405000329017639\n",
      "Iteration 18800 Training loss 0.0921454206109047 Validation loss 0.09567149728536606 Accuracy 0.7310000061988831\n",
      "Iteration 18810 Training loss 0.1164126917719841 Validation loss 0.10115104913711548 Accuracy 0.6955000162124634\n",
      "Iteration 18820 Training loss 0.0960630550980568 Validation loss 0.09333934634923935 Accuracy 0.7285000085830688\n",
      "Iteration 18830 Training loss 0.10933686047792435 Validation loss 0.11335606873035431 Accuracy 0.6260000467300415\n",
      "Iteration 18840 Training loss 0.0957261174917221 Validation loss 0.09580560773611069 Accuracy 0.7110000252723694\n",
      "Iteration 18850 Training loss 0.08697649091482162 Validation loss 0.09341596812009811 Accuracy 0.7380000352859497\n",
      "Iteration 18860 Training loss 0.08754464238882065 Validation loss 0.0924389511346817 Accuracy 0.7360000610351562\n",
      "Iteration 18870 Training loss 0.09505735337734222 Validation loss 0.09580431133508682 Accuracy 0.7115000486373901\n",
      "Iteration 18880 Training loss 0.09779991209506989 Validation loss 0.09719044715166092 Accuracy 0.7190000414848328\n",
      "Iteration 18890 Training loss 0.09395068138837814 Validation loss 0.0987221747636795 Accuracy 0.7010000348091125\n",
      "Iteration 18900 Training loss 0.08221473544836044 Validation loss 0.10533691197633743 Accuracy 0.6845000386238098\n",
      "Iteration 18910 Training loss 0.08922336995601654 Validation loss 0.0969436913728714 Accuracy 0.7140000462532043\n",
      "Iteration 18920 Training loss 0.1055172011256218 Validation loss 0.10530631244182587 Accuracy 0.671500027179718\n",
      "Iteration 18930 Training loss 0.10600322484970093 Validation loss 0.09612226486206055 Accuracy 0.7145000100135803\n",
      "Iteration 18940 Training loss 0.10834571719169617 Validation loss 0.10516879707574844 Accuracy 0.6775000095367432\n",
      "Iteration 18950 Training loss 0.09493907541036606 Validation loss 0.0982801541686058 Accuracy 0.7225000262260437\n",
      "Iteration 18960 Training loss 0.09286554157733917 Validation loss 0.09943880140781403 Accuracy 0.706000030040741\n",
      "Iteration 18970 Training loss 0.09393264353275299 Validation loss 0.09601490199565887 Accuracy 0.7155000567436218\n",
      "Iteration 18980 Training loss 0.09188489615917206 Validation loss 0.09670465439558029 Accuracy 0.7130000591278076\n",
      "Iteration 18990 Training loss 0.09438733756542206 Validation loss 0.09561435878276825 Accuracy 0.7280000448226929\n",
      "Iteration 19000 Training loss 0.10000140964984894 Validation loss 0.0991537868976593 Accuracy 0.6985000371932983\n",
      "Iteration 19010 Training loss 0.08805807679891586 Validation loss 0.09520552307367325 Accuracy 0.7070000171661377\n",
      "Iteration 19020 Training loss 0.09807565808296204 Validation loss 0.0939672589302063 Accuracy 0.7390000224113464\n",
      "Iteration 19030 Training loss 0.07936113327741623 Validation loss 0.09742803126573563 Accuracy 0.7285000085830688\n",
      "Iteration 19040 Training loss 0.0738021656870842 Validation loss 0.1006205603480339 Accuracy 0.6940000057220459\n",
      "Iteration 19050 Training loss 0.0903114452958107 Validation loss 0.09674079716205597 Accuracy 0.7220000624656677\n",
      "Iteration 19060 Training loss 0.09278517216444016 Validation loss 0.0889941081404686 Accuracy 0.7560000419616699\n",
      "Iteration 19070 Training loss 0.14144980907440186 Validation loss 0.12421718239784241 Accuracy 0.612000048160553\n",
      "Iteration 19080 Training loss 0.09639260172843933 Validation loss 0.09291771054267883 Accuracy 0.7295000553131104\n",
      "Iteration 19090 Training loss 0.11387702822685242 Validation loss 0.10651286691427231 Accuracy 0.6690000295639038\n",
      "Iteration 19100 Training loss 0.09557342529296875 Validation loss 0.09574966132640839 Accuracy 0.7205000519752502\n",
      "Iteration 19110 Training loss 0.09232539683580399 Validation loss 0.100706547498703 Accuracy 0.6955000162124634\n",
      "Iteration 19120 Training loss 0.10055772215127945 Validation loss 0.09774137288331985 Accuracy 0.7130000591278076\n",
      "Iteration 19130 Training loss 0.10042426735162735 Validation loss 0.10441264510154724 Accuracy 0.6810000538825989\n",
      "Iteration 19140 Training loss 0.0957336276769638 Validation loss 0.10045419633388519 Accuracy 0.7005000114440918\n",
      "Iteration 19150 Training loss 0.0891837477684021 Validation loss 0.09854021668434143 Accuracy 0.6940000057220459\n",
      "Iteration 19160 Training loss 0.11036978662014008 Validation loss 0.09920943528413773 Accuracy 0.7015000581741333\n",
      "Iteration 19170 Training loss 0.08706715703010559 Validation loss 0.09407597035169601 Accuracy 0.7245000600814819\n",
      "Iteration 19180 Training loss 0.07696089893579483 Validation loss 0.08972128480672836 Accuracy 0.7450000643730164\n",
      "Iteration 19190 Training loss 0.09160590916872025 Validation loss 0.10307168960571289 Accuracy 0.6875000596046448\n",
      "Iteration 19200 Training loss 0.08112741261720657 Validation loss 0.09318869560956955 Accuracy 0.7380000352859497\n",
      "Iteration 19210 Training loss 0.10216976702213287 Validation loss 0.10556784272193909 Accuracy 0.674500048160553\n",
      "Iteration 19220 Training loss 0.10775873064994812 Validation loss 0.12871716916561127 Accuracy 0.6025000214576721\n",
      "Iteration 19230 Training loss 0.08231736719608307 Validation loss 0.0977296307682991 Accuracy 0.7020000219345093\n",
      "Iteration 19240 Training loss 0.09020233899354935 Validation loss 0.09350642561912537 Accuracy 0.7240000367164612\n",
      "Iteration 19250 Training loss 0.08480331301689148 Validation loss 0.10048919916152954 Accuracy 0.706000030040741\n",
      "Iteration 19260 Training loss 0.08314754068851471 Validation loss 0.09133600443601608 Accuracy 0.7435000538825989\n",
      "Iteration 19270 Training loss 0.08041280508041382 Validation loss 0.09406529366970062 Accuracy 0.7230000495910645\n",
      "Iteration 19280 Training loss 0.11254800856113434 Validation loss 0.09867218136787415 Accuracy 0.7055000066757202\n",
      "Iteration 19290 Training loss 0.09320492297410965 Validation loss 0.0900634378194809 Accuracy 0.7445000410079956\n",
      "Iteration 19300 Training loss 0.09179610013961792 Validation loss 0.09468196332454681 Accuracy 0.7280000448226929\n",
      "Iteration 19310 Training loss 0.0821080207824707 Validation loss 0.09533444792032242 Accuracy 0.7100000381469727\n",
      "Iteration 19320 Training loss 0.08529096096754074 Validation loss 0.09286919981241226 Accuracy 0.7390000224113464\n",
      "Iteration 19330 Training loss 0.08461292088031769 Validation loss 0.10473590344190598 Accuracy 0.6780000329017639\n",
      "Iteration 19340 Training loss 0.08860940486192703 Validation loss 0.09503426402807236 Accuracy 0.7280000448226929\n",
      "Iteration 19350 Training loss 0.09563825279474258 Validation loss 0.09897644817829132 Accuracy 0.7140000462532043\n",
      "Iteration 19360 Training loss 0.10025939345359802 Validation loss 0.10155840218067169 Accuracy 0.703000009059906\n",
      "Iteration 19370 Training loss 0.08869283646345139 Validation loss 0.09759718924760818 Accuracy 0.7135000228881836\n",
      "Iteration 19380 Training loss 0.10509906709194183 Validation loss 0.10376662015914917 Accuracy 0.6855000257492065\n",
      "Iteration 19390 Training loss 0.1056634709239006 Validation loss 0.09614656865596771 Accuracy 0.7165000438690186\n",
      "Iteration 19400 Training loss 0.12662236392498016 Validation loss 0.10620623081922531 Accuracy 0.6725000143051147\n",
      "Iteration 19410 Training loss 0.10142125934362411 Validation loss 0.1110180914402008 Accuracy 0.6365000009536743\n",
      "Iteration 19420 Training loss 0.11319669336080551 Validation loss 0.11569389700889587 Accuracy 0.627500057220459\n",
      "Iteration 19430 Training loss 0.0954320877790451 Validation loss 0.10028114914894104 Accuracy 0.7020000219345093\n",
      "Iteration 19440 Training loss 0.08944015949964523 Validation loss 0.09643659740686417 Accuracy 0.7255000472068787\n",
      "Iteration 19450 Training loss 0.1099533662199974 Validation loss 0.10791414231061935 Accuracy 0.6815000176429749\n",
      "Iteration 19460 Training loss 0.18066243827342987 Validation loss 0.1624906212091446 Accuracy 0.5200000405311584\n",
      "Iteration 19470 Training loss 0.08490075170993805 Validation loss 0.09800398349761963 Accuracy 0.6970000267028809\n",
      "Iteration 19480 Training loss 0.1326381415128708 Validation loss 0.11537735164165497 Accuracy 0.6295000314712524\n",
      "Iteration 19490 Training loss 0.09336929023265839 Validation loss 0.1001918762922287 Accuracy 0.6945000290870667\n",
      "Iteration 19500 Training loss 0.08923592418432236 Validation loss 0.10399337857961655 Accuracy 0.6620000600814819\n",
      "Iteration 19510 Training loss 0.0756552517414093 Validation loss 0.09340152889490128 Accuracy 0.7235000133514404\n",
      "Iteration 19520 Training loss 0.09151869267225266 Validation loss 0.09324034303426743 Accuracy 0.7305000424385071\n",
      "Iteration 19530 Training loss 0.0835430696606636 Validation loss 0.09663033485412598 Accuracy 0.7095000147819519\n",
      "Iteration 19540 Training loss 0.09285584837198257 Validation loss 0.09223951399326324 Accuracy 0.733500063419342\n",
      "Iteration 19550 Training loss 0.10782863199710846 Validation loss 0.09622909873723984 Accuracy 0.7125000357627869\n",
      "Iteration 19560 Training loss 0.0975673645734787 Validation loss 0.10094030946493149 Accuracy 0.7020000219345093\n",
      "Iteration 19570 Training loss 0.08408287912607193 Validation loss 0.10096307843923569 Accuracy 0.6985000371932983\n",
      "Iteration 19580 Training loss 0.08331428468227386 Validation loss 0.09428601711988449 Accuracy 0.7355000376701355\n",
      "Iteration 19590 Training loss 0.10501131415367126 Validation loss 0.10014276951551437 Accuracy 0.7085000276565552\n",
      "Iteration 19600 Training loss 0.09806503355503082 Validation loss 0.10044948756694794 Accuracy 0.6960000395774841\n",
      "Iteration 19610 Training loss 0.12484092265367508 Validation loss 0.11693892627954483 Accuracy 0.6310000419616699\n",
      "Iteration 19620 Training loss 0.08379323035478592 Validation loss 0.09445304423570633 Accuracy 0.7210000157356262\n",
      "Iteration 19630 Training loss 0.0921025276184082 Validation loss 0.10385821759700775 Accuracy 0.6935000419616699\n",
      "Iteration 19640 Training loss 0.1013522520661354 Validation loss 0.09570462256669998 Accuracy 0.7085000276565552\n",
      "Iteration 19650 Training loss 0.08118518441915512 Validation loss 0.09464196115732193 Accuracy 0.7230000495910645\n",
      "Iteration 19660 Training loss 0.09333571046590805 Validation loss 0.0948648452758789 Accuracy 0.7225000262260437\n",
      "Iteration 19670 Training loss 0.10171479731798172 Validation loss 0.11348653584718704 Accuracy 0.6680000424385071\n",
      "Iteration 19680 Training loss 0.09668926149606705 Validation loss 0.10823401063680649 Accuracy 0.6545000076293945\n",
      "Iteration 19690 Training loss 0.08902283757925034 Validation loss 0.09572523087263107 Accuracy 0.7150000333786011\n",
      "Iteration 19700 Training loss 0.09162398427724838 Validation loss 0.0995078757405281 Accuracy 0.7085000276565552\n",
      "Iteration 19710 Training loss 0.07879529148340225 Validation loss 0.10287224501371384 Accuracy 0.6725000143051147\n",
      "Iteration 19720 Training loss 0.09776616841554642 Validation loss 0.10289926826953888 Accuracy 0.6950000524520874\n",
      "Iteration 19730 Training loss 0.09860724210739136 Validation loss 0.1079183965921402 Accuracy 0.6415000557899475\n",
      "Iteration 19740 Training loss 0.09934499859809875 Validation loss 0.09939085692167282 Accuracy 0.7125000357627869\n",
      "Iteration 19750 Training loss 0.0956844761967659 Validation loss 0.0956449955701828 Accuracy 0.7345000505447388\n",
      "Iteration 19760 Training loss 0.08973660320043564 Validation loss 0.10059089958667755 Accuracy 0.7070000171661377\n",
      "Iteration 19770 Training loss 0.0758032351732254 Validation loss 0.0924045592546463 Accuracy 0.7390000224113464\n",
      "Iteration 19780 Training loss 0.10556716471910477 Validation loss 0.09095634520053864 Accuracy 0.7360000610351562\n",
      "Iteration 19790 Training loss 0.0883941799402237 Validation loss 0.09657295793294907 Accuracy 0.7265000343322754\n",
      "Iteration 19800 Training loss 0.11692129820585251 Validation loss 0.10611384361982346 Accuracy 0.6675000190734863\n",
      "Iteration 19810 Training loss 0.10070224851369858 Validation loss 0.09717810153961182 Accuracy 0.7055000066757202\n",
      "Iteration 19820 Training loss 0.09838169068098068 Validation loss 0.0990016981959343 Accuracy 0.706000030040741\n",
      "Iteration 19830 Training loss 0.09590073674917221 Validation loss 0.09919659793376923 Accuracy 0.6985000371932983\n",
      "Iteration 19840 Training loss 0.1077079176902771 Validation loss 0.1054234653711319 Accuracy 0.6620000600814819\n",
      "Iteration 19850 Training loss 0.10607238858938217 Validation loss 0.09942836314439774 Accuracy 0.7095000147819519\n",
      "Iteration 19860 Training loss 0.10547952353954315 Validation loss 0.09762179106473923 Accuracy 0.7050000429153442\n",
      "Iteration 19870 Training loss 0.09283553808927536 Validation loss 0.10053784400224686 Accuracy 0.6980000138282776\n",
      "Iteration 19880 Training loss 0.10266988724470139 Validation loss 0.0958685353398323 Accuracy 0.7205000519752502\n",
      "Iteration 19890 Training loss 0.09344390779733658 Validation loss 0.09349732100963593 Accuracy 0.7250000238418579\n",
      "Iteration 19900 Training loss 0.09047631919384003 Validation loss 0.10182933509349823 Accuracy 0.7085000276565552\n",
      "Iteration 19910 Training loss 0.11489152908325195 Validation loss 0.10216602683067322 Accuracy 0.6825000047683716\n",
      "Iteration 19920 Training loss 0.09090731292963028 Validation loss 0.1077803447842598 Accuracy 0.6580000519752502\n",
      "Iteration 19930 Training loss 0.102790467441082 Validation loss 0.09884940832853317 Accuracy 0.6975000500679016\n",
      "Iteration 19940 Training loss 0.09114418923854828 Validation loss 0.09723857790231705 Accuracy 0.7020000219345093\n",
      "Iteration 19950 Training loss 0.1042582094669342 Validation loss 0.09881703555583954 Accuracy 0.7205000519752502\n",
      "Iteration 19960 Training loss 0.09329408407211304 Validation loss 0.09849867969751358 Accuracy 0.6885000467300415\n",
      "Iteration 19970 Training loss 0.10378416627645493 Validation loss 0.10567093640565872 Accuracy 0.6925000548362732\n",
      "Iteration 19980 Training loss 0.09751120954751968 Validation loss 0.09722482413053513 Accuracy 0.7090000510215759\n",
      "Iteration 19990 Training loss 0.09840773046016693 Validation loss 0.09699992835521698 Accuracy 0.7305000424385071\n",
      "Iteration 20000 Training loss 0.0934927761554718 Validation loss 0.10563062131404877 Accuracy 0.6815000176429749\n",
      "Iteration 20010 Training loss 0.09294513612985611 Validation loss 0.09835263341665268 Accuracy 0.7020000219345093\n",
      "Iteration 20020 Training loss 0.1086704209446907 Validation loss 0.0950876772403717 Accuracy 0.7235000133514404\n",
      "Iteration 20030 Training loss 0.09440957754850388 Validation loss 0.0997885912656784 Accuracy 0.7065000534057617\n",
      "Iteration 20040 Training loss 0.08968518674373627 Validation loss 0.09609663486480713 Accuracy 0.7200000286102295\n",
      "Iteration 20050 Training loss 0.10991481691598892 Validation loss 0.09909477829933167 Accuracy 0.7270000576972961\n",
      "Iteration 20060 Training loss 0.08230699598789215 Validation loss 0.09418919682502747 Accuracy 0.7235000133514404\n",
      "Iteration 20070 Training loss 0.10405135154724121 Validation loss 0.1162184476852417 Accuracy 0.6015000343322754\n",
      "Iteration 20080 Training loss 0.08942300826311111 Validation loss 0.09504440426826477 Accuracy 0.7315000295639038\n",
      "Iteration 20090 Training loss 0.10198039561510086 Validation loss 0.10562443733215332 Accuracy 0.6705000400543213\n",
      "Iteration 20100 Training loss 0.10292889177799225 Validation loss 0.10679509490728378 Accuracy 0.6665000319480896\n",
      "Iteration 20110 Training loss 0.08367548882961273 Validation loss 0.09712965786457062 Accuracy 0.7255000472068787\n",
      "Iteration 20120 Training loss 0.1037612333893776 Validation loss 0.09599488973617554 Accuracy 0.7200000286102295\n",
      "Iteration 20130 Training loss 0.11142337322235107 Validation loss 0.09789750725030899 Accuracy 0.7165000438690186\n",
      "Iteration 20140 Training loss 0.09558059275150299 Validation loss 0.09761699289083481 Accuracy 0.7140000462532043\n",
      "Iteration 20150 Training loss 0.10417971014976501 Validation loss 0.10492517799139023 Accuracy 0.6765000224113464\n",
      "Iteration 20160 Training loss 0.07903851568698883 Validation loss 0.09202031046152115 Accuracy 0.737000048160553\n",
      "Iteration 20170 Training loss 0.09695376455783844 Validation loss 0.09826269745826721 Accuracy 0.7115000486373901\n",
      "Iteration 20180 Training loss 0.09876544773578644 Validation loss 0.1005830243229866 Accuracy 0.6975000500679016\n",
      "Iteration 20190 Training loss 0.1032681092619896 Validation loss 0.09893015027046204 Accuracy 0.703000009059906\n",
      "Iteration 20200 Training loss 0.0901312455534935 Validation loss 0.09847911447286606 Accuracy 0.7095000147819519\n",
      "Iteration 20210 Training loss 0.09997306019067764 Validation loss 0.09724850952625275 Accuracy 0.7210000157356262\n",
      "Iteration 20220 Training loss 0.10019868612289429 Validation loss 0.1013510450720787 Accuracy 0.6875000596046448\n",
      "Iteration 20230 Training loss 0.09177206456661224 Validation loss 0.09604845941066742 Accuracy 0.7205000519752502\n",
      "Iteration 20240 Training loss 0.09165041893720627 Validation loss 0.09623468667268753 Accuracy 0.7245000600814819\n",
      "Iteration 20250 Training loss 0.1078210398554802 Validation loss 0.09807591885328293 Accuracy 0.7155000567436218\n",
      "Iteration 20260 Training loss 0.09370791912078857 Validation loss 0.0994780957698822 Accuracy 0.6990000605583191\n",
      "Iteration 20270 Training loss 0.0933094173669815 Validation loss 0.0980706661939621 Accuracy 0.7070000171661377\n",
      "Iteration 20280 Training loss 0.09859241545200348 Validation loss 0.098294697701931 Accuracy 0.7085000276565552\n",
      "Iteration 20290 Training loss 0.09210771322250366 Validation loss 0.09547945111989975 Accuracy 0.7275000214576721\n",
      "Iteration 20300 Training loss 0.10877944529056549 Validation loss 0.09686433523893356 Accuracy 0.7145000100135803\n",
      "Iteration 20310 Training loss 0.10359372943639755 Validation loss 0.09367936849594116 Accuracy 0.7330000400543213\n",
      "Iteration 20320 Training loss 0.10673091560602188 Validation loss 0.09505308419466019 Accuracy 0.7365000247955322\n",
      "Iteration 20330 Training loss 0.12560135126113892 Validation loss 0.10129918158054352 Accuracy 0.6880000233650208\n",
      "Iteration 20340 Training loss 0.1014614924788475 Validation loss 0.09886306524276733 Accuracy 0.7095000147819519\n",
      "Iteration 20350 Training loss 0.1285468488931656 Validation loss 0.12080810964107513 Accuracy 0.628000020980835\n",
      "Iteration 20360 Training loss 0.09060394018888474 Validation loss 0.10521289706230164 Accuracy 0.6725000143051147\n",
      "Iteration 20370 Training loss 0.08997141569852829 Validation loss 0.09724751859903336 Accuracy 0.7135000228881836\n",
      "Iteration 20380 Training loss 0.1032438650727272 Validation loss 0.09948795288801193 Accuracy 0.7050000429153442\n",
      "Iteration 20390 Training loss 0.10514829307794571 Validation loss 0.1101209744811058 Accuracy 0.6495000123977661\n",
      "Iteration 20400 Training loss 0.1111331507563591 Validation loss 0.10445665568113327 Accuracy 0.6790000200271606\n",
      "Iteration 20410 Training loss 0.0978507399559021 Validation loss 0.09982283413410187 Accuracy 0.7040000557899475\n",
      "Iteration 20420 Training loss 0.09198009222745895 Validation loss 0.09906796365976334 Accuracy 0.7050000429153442\n",
      "Iteration 20430 Training loss 0.09424705058336258 Validation loss 0.10207033902406693 Accuracy 0.70250004529953\n",
      "Iteration 20440 Training loss 0.10796291381120682 Validation loss 0.10216066241264343 Accuracy 0.6935000419616699\n",
      "Iteration 20450 Training loss 0.10645410418510437 Validation loss 0.10074244439601898 Accuracy 0.7005000114440918\n",
      "Iteration 20460 Training loss 0.07995211333036423 Validation loss 0.09590110927820206 Accuracy 0.7310000061988831\n",
      "Iteration 20470 Training loss 0.14458110928535461 Validation loss 0.1311158835887909 Accuracy 0.5785000324249268\n",
      "Iteration 20480 Training loss 0.09306301176548004 Validation loss 0.1003383994102478 Accuracy 0.7065000534057617\n",
      "Iteration 20490 Training loss 0.09287065267562866 Validation loss 0.10148884356021881 Accuracy 0.7195000052452087\n",
      "Iteration 20500 Training loss 0.09792273491621017 Validation loss 0.09612694382667542 Accuracy 0.7300000190734863\n",
      "Iteration 20510 Training loss 0.10292809456586838 Validation loss 0.10915254801511765 Accuracy 0.6470000147819519\n",
      "Iteration 20520 Training loss 0.08431989699602127 Validation loss 0.09921561181545258 Accuracy 0.7040000557899475\n",
      "Iteration 20530 Training loss 0.11922359466552734 Validation loss 0.11432509124279022 Accuracy 0.6235000491142273\n",
      "Iteration 20540 Training loss 0.08652173727750778 Validation loss 0.1015259250998497 Accuracy 0.7010000348091125\n",
      "Iteration 20550 Training loss 0.10185787826776505 Validation loss 0.10569732636213303 Accuracy 0.6760000586509705\n",
      "Iteration 20560 Training loss 0.1156308501958847 Validation loss 0.10789473354816437 Accuracy 0.6625000238418579\n",
      "Iteration 20570 Training loss 0.10303738713264465 Validation loss 0.10516039282083511 Accuracy 0.6690000295639038\n",
      "Iteration 20580 Training loss 0.11972778290510178 Validation loss 0.10927599668502808 Accuracy 0.6475000381469727\n",
      "Iteration 20590 Training loss 0.10258761793375015 Validation loss 0.1085369735956192 Accuracy 0.6545000076293945\n",
      "Iteration 20600 Training loss 0.09994310140609741 Validation loss 0.10392752289772034 Accuracy 0.6725000143051147\n",
      "Iteration 20610 Training loss 0.09615381062030792 Validation loss 0.10262218117713928 Accuracy 0.6845000386238098\n",
      "Iteration 20620 Training loss 0.10061021894216537 Validation loss 0.10284079611301422 Accuracy 0.687000036239624\n",
      "Iteration 20630 Training loss 0.08864043653011322 Validation loss 0.0998712033033371 Accuracy 0.7045000195503235\n",
      "Iteration 20640 Training loss 0.09095022082328796 Validation loss 0.09991332143545151 Accuracy 0.7165000438690186\n",
      "Iteration 20650 Training loss 0.08642721176147461 Validation loss 0.09824494272470474 Accuracy 0.7160000205039978\n",
      "Iteration 20660 Training loss 0.10072429478168488 Validation loss 0.10248604416847229 Accuracy 0.6820000410079956\n",
      "Iteration 20670 Training loss 0.10055438429117203 Validation loss 0.10037363320589066 Accuracy 0.7105000615119934\n",
      "Iteration 20680 Training loss 0.09884017705917358 Validation loss 0.1014474630355835 Accuracy 0.6890000104904175\n",
      "Iteration 20690 Training loss 0.11282642185688019 Validation loss 0.1061076819896698 Accuracy 0.6780000329017639\n",
      "Iteration 20700 Training loss 0.10074334591627121 Validation loss 0.09519676864147186 Accuracy 0.7310000061988831\n",
      "Iteration 20710 Training loss 0.10365304350852966 Validation loss 0.10072550922632217 Accuracy 0.7150000333786011\n",
      "Iteration 20720 Training loss 0.10448864102363586 Validation loss 0.09735079854726791 Accuracy 0.7225000262260437\n",
      "Iteration 20730 Training loss 0.09543546289205551 Validation loss 0.10593488067388535 Accuracy 0.6685000061988831\n",
      "Iteration 20740 Training loss 0.10276475548744202 Validation loss 0.0980691909790039 Accuracy 0.7200000286102295\n",
      "Iteration 20750 Training loss 0.10948093235492706 Validation loss 0.10667838156223297 Accuracy 0.6770000457763672\n",
      "Iteration 20760 Training loss 0.10028228163719177 Validation loss 0.09649839997291565 Accuracy 0.7205000519752502\n",
      "Iteration 20770 Training loss 0.10629014670848846 Validation loss 0.09874612838029861 Accuracy 0.721500039100647\n",
      "Iteration 20780 Training loss 0.1304701715707779 Validation loss 0.11658620089292526 Accuracy 0.6410000324249268\n",
      "Iteration 20790 Training loss 0.1007167398929596 Validation loss 0.09887311607599258 Accuracy 0.6990000605583191\n",
      "Iteration 20800 Training loss 0.11405141651630402 Validation loss 0.10176067054271698 Accuracy 0.7100000381469727\n",
      "Iteration 20810 Training loss 0.10180609673261642 Validation loss 0.10363050550222397 Accuracy 0.6820000410079956\n",
      "Iteration 20820 Training loss 0.10295766592025757 Validation loss 0.10527151823043823 Accuracy 0.6740000247955322\n",
      "Iteration 20830 Training loss 0.10822885483503342 Validation loss 0.10040886700153351 Accuracy 0.7020000219345093\n",
      "Iteration 20840 Training loss 0.09639008343219757 Validation loss 0.1030900627374649 Accuracy 0.6840000152587891\n",
      "Iteration 20850 Training loss 0.09232154488563538 Validation loss 0.09529200941324234 Accuracy 0.7360000610351562\n",
      "Iteration 20860 Training loss 0.08420529216527939 Validation loss 0.09684697538614273 Accuracy 0.7220000624656677\n",
      "Iteration 20870 Training loss 0.1042097732424736 Validation loss 0.10410478711128235 Accuracy 0.6780000329017639\n",
      "Iteration 20880 Training loss 0.11305414140224457 Validation loss 0.10667017102241516 Accuracy 0.6575000286102295\n",
      "Iteration 20890 Training loss 0.09295516461133957 Validation loss 0.09792830049991608 Accuracy 0.7000000476837158\n",
      "Iteration 20900 Training loss 0.09980420768260956 Validation loss 0.10158182680606842 Accuracy 0.6850000619888306\n",
      "Iteration 20910 Training loss 0.10325418412685394 Validation loss 0.1000041663646698 Accuracy 0.7000000476837158\n",
      "Iteration 20920 Training loss 0.09485719352960587 Validation loss 0.09829016029834747 Accuracy 0.721500039100647\n",
      "Iteration 20930 Training loss 0.10545676201581955 Validation loss 0.10474900156259537 Accuracy 0.6795000433921814\n",
      "Iteration 20940 Training loss 0.09421543031930923 Validation loss 0.09891605377197266 Accuracy 0.7105000615119934\n",
      "Iteration 20950 Training loss 0.10956808179616928 Validation loss 0.1079556941986084 Accuracy 0.6580000519752502\n",
      "Iteration 20960 Training loss 0.09886319190263748 Validation loss 0.09925076365470886 Accuracy 0.7075000405311584\n",
      "Iteration 20970 Training loss 0.0854429230093956 Validation loss 0.0989191010594368 Accuracy 0.7090000510215759\n",
      "Iteration 20980 Training loss 0.09801879525184631 Validation loss 0.10077289491891861 Accuracy 0.6920000314712524\n",
      "Iteration 20990 Training loss 0.12283063679933548 Validation loss 0.1116008535027504 Accuracy 0.6360000371932983\n",
      "Iteration 21000 Training loss 0.09467893093824387 Validation loss 0.10079013556241989 Accuracy 0.7000000476837158\n",
      "Iteration 21010 Training loss 0.09189937263727188 Validation loss 0.09738413244485855 Accuracy 0.7160000205039978\n",
      "Iteration 21020 Training loss 0.09132129698991776 Validation loss 0.10071422904729843 Accuracy 0.7015000581741333\n",
      "Iteration 21030 Training loss 0.10154487192630768 Validation loss 0.09555449336767197 Accuracy 0.7290000319480896\n",
      "Iteration 21040 Training loss 0.10471893101930618 Validation loss 0.10320087522268295 Accuracy 0.6795000433921814\n",
      "Iteration 21050 Training loss 0.09792575985193253 Validation loss 0.09877532720565796 Accuracy 0.7155000567436218\n",
      "Iteration 21060 Training loss 0.0839608833193779 Validation loss 0.09848319739103317 Accuracy 0.7145000100135803\n",
      "Iteration 21070 Training loss 0.09381242841482162 Validation loss 0.10400602966547012 Accuracy 0.6895000338554382\n",
      "Iteration 21080 Training loss 0.09167802333831787 Validation loss 0.09828749299049377 Accuracy 0.7045000195503235\n",
      "Iteration 21090 Training loss 0.10000968724489212 Validation loss 0.0970740094780922 Accuracy 0.7275000214576721\n",
      "Iteration 21100 Training loss 0.10227663815021515 Validation loss 0.09859912842512131 Accuracy 0.7035000324249268\n",
      "Iteration 21110 Training loss 0.10164427757263184 Validation loss 0.10242433845996857 Accuracy 0.6800000071525574\n",
      "Iteration 21120 Training loss 0.09981408715248108 Validation loss 0.10444196313619614 Accuracy 0.6725000143051147\n",
      "Iteration 21130 Training loss 0.09573343396186829 Validation loss 0.09562116116285324 Accuracy 0.718000054359436\n",
      "Iteration 21140 Training loss 0.09834306687116623 Validation loss 0.09757640212774277 Accuracy 0.7080000042915344\n",
      "Iteration 21150 Training loss 0.10903927683830261 Validation loss 0.11777475476264954 Accuracy 0.6510000228881836\n",
      "Iteration 21160 Training loss 0.07626622915267944 Validation loss 0.09642713516950607 Accuracy 0.7225000262260437\n",
      "Iteration 21170 Training loss 0.09522019326686859 Validation loss 0.10072307288646698 Accuracy 0.7005000114440918\n",
      "Iteration 21180 Training loss 0.09606210142374039 Validation loss 0.10689046233892441 Accuracy 0.6700000166893005\n",
      "Iteration 21190 Training loss 0.09867822378873825 Validation loss 0.09993410110473633 Accuracy 0.7120000123977661\n",
      "Iteration 21200 Training loss 0.0777292400598526 Validation loss 0.09590842574834824 Accuracy 0.7325000166893005\n",
      "Iteration 21210 Training loss 0.10196743905544281 Validation loss 0.10053645819425583 Accuracy 0.6990000605583191\n",
      "Iteration 21220 Training loss 0.09984653443098068 Validation loss 0.09771042317152023 Accuracy 0.7175000309944153\n",
      "Iteration 21230 Training loss 0.09545388072729111 Validation loss 0.0976921021938324 Accuracy 0.7125000357627869\n",
      "Iteration 21240 Training loss 0.08371330797672272 Validation loss 0.09668422490358353 Accuracy 0.7135000228881836\n",
      "Iteration 21250 Training loss 0.11576428264379501 Validation loss 0.10309556871652603 Accuracy 0.6825000047683716\n",
      "Iteration 21260 Training loss 0.09465855360031128 Validation loss 0.10247783362865448 Accuracy 0.6915000081062317\n",
      "Iteration 21270 Training loss 0.10437749326229095 Validation loss 0.10657399147748947 Accuracy 0.6780000329017639\n",
      "Iteration 21280 Training loss 0.09874583780765533 Validation loss 0.10906247049570084 Accuracy 0.6625000238418579\n",
      "Iteration 21290 Training loss 0.08924630284309387 Validation loss 0.0986100509762764 Accuracy 0.7045000195503235\n",
      "Iteration 21300 Training loss 0.11391068994998932 Validation loss 0.12817557156085968 Accuracy 0.562000036239624\n",
      "Iteration 21310 Training loss 0.09612322598695755 Validation loss 0.10286588221788406 Accuracy 0.703000009059906\n",
      "Iteration 21320 Training loss 0.09808339923620224 Validation loss 0.09780356287956238 Accuracy 0.7210000157356262\n",
      "Iteration 21330 Training loss 0.09921281784772873 Validation loss 0.10337097197771072 Accuracy 0.6810000538825989\n",
      "Iteration 21340 Training loss 0.09854083508253098 Validation loss 0.09926383197307587 Accuracy 0.7015000581741333\n",
      "Iteration 21350 Training loss 0.13085852563381195 Validation loss 0.11528491973876953 Accuracy 0.6130000352859497\n",
      "Iteration 21360 Training loss 0.08739740401506424 Validation loss 0.0980314165353775 Accuracy 0.7265000343322754\n",
      "Iteration 21370 Training loss 0.11179004609584808 Validation loss 0.09830127656459808 Accuracy 0.7105000615119934\n",
      "Iteration 21380 Training loss 0.08840236812829971 Validation loss 0.09636058658361435 Accuracy 0.7170000076293945\n",
      "Iteration 21390 Training loss 0.08961967378854752 Validation loss 0.09560173004865646 Accuracy 0.7230000495910645\n",
      "Iteration 21400 Training loss 0.08223974704742432 Validation loss 0.09927539527416229 Accuracy 0.7105000615119934\n",
      "Iteration 21410 Training loss 0.11235793679952621 Validation loss 0.10506559908390045 Accuracy 0.6820000410079956\n",
      "Iteration 21420 Training loss 0.09214206784963608 Validation loss 0.1028817743062973 Accuracy 0.6855000257492065\n",
      "Iteration 21430 Training loss 0.12917758524417877 Validation loss 0.12944889068603516 Accuracy 0.612500011920929\n",
      "Iteration 21440 Training loss 0.08884662389755249 Validation loss 0.0947984978556633 Accuracy 0.734000027179718\n",
      "Iteration 21450 Training loss 0.10630438476800919 Validation loss 0.10051005333662033 Accuracy 0.6895000338554382\n",
      "Iteration 21460 Training loss 0.12096372991800308 Validation loss 0.11110705882310867 Accuracy 0.6665000319480896\n",
      "Iteration 21470 Training loss 0.10146516561508179 Validation loss 0.09513083100318909 Accuracy 0.7260000109672546\n",
      "Iteration 21480 Training loss 0.09515853971242905 Validation loss 0.10090764611959457 Accuracy 0.6955000162124634\n",
      "Iteration 21490 Training loss 0.08000799268484116 Validation loss 0.0946764126420021 Accuracy 0.7305000424385071\n",
      "Iteration 21500 Training loss 0.10221651196479797 Validation loss 0.1073412373661995 Accuracy 0.6645000576972961\n",
      "Iteration 21510 Training loss 0.08681520074605942 Validation loss 0.10603979974985123 Accuracy 0.6600000262260437\n",
      "Iteration 21520 Training loss 0.0982307568192482 Validation loss 0.10663197189569473 Accuracy 0.6395000219345093\n",
      "Iteration 21530 Training loss 0.09432705491781235 Validation loss 0.10244354605674744 Accuracy 0.6890000104904175\n",
      "Iteration 21540 Training loss 0.10489093512296677 Validation loss 0.10524895042181015 Accuracy 0.6665000319480896\n",
      "Iteration 21550 Training loss 0.11210086941719055 Validation loss 0.09797397255897522 Accuracy 0.7170000076293945\n",
      "Iteration 21560 Training loss 0.10827156156301498 Validation loss 0.10915859043598175 Accuracy 0.6500000357627869\n",
      "Iteration 21570 Training loss 0.1228763535618782 Validation loss 0.13729053735733032 Accuracy 0.5590000152587891\n",
      "Iteration 21580 Training loss 0.10041569173336029 Validation loss 0.10211663693189621 Accuracy 0.6995000243186951\n",
      "Iteration 21590 Training loss 0.098618283867836 Validation loss 0.09830652177333832 Accuracy 0.7175000309944153\n",
      "Iteration 21600 Training loss 0.09600475430488586 Validation loss 0.09684114903211594 Accuracy 0.7130000591278076\n",
      "Iteration 21610 Training loss 0.09976434707641602 Validation loss 0.09660407155752182 Accuracy 0.734000027179718\n",
      "Iteration 21620 Training loss 0.09138967096805573 Validation loss 0.09465841948986053 Accuracy 0.7260000109672546\n",
      "Iteration 21630 Training loss 0.10187534242868423 Validation loss 0.1062590703368187 Accuracy 0.6690000295639038\n",
      "Iteration 21640 Training loss 0.09113403409719467 Validation loss 0.10149553418159485 Accuracy 0.6945000290870667\n",
      "Iteration 21650 Training loss 0.09777866303920746 Validation loss 0.09795060753822327 Accuracy 0.7055000066757202\n",
      "Iteration 21660 Training loss 0.09887298196554184 Validation loss 0.09758836030960083 Accuracy 0.7200000286102295\n",
      "Iteration 21670 Training loss 0.11138064414262772 Validation loss 0.12217426300048828 Accuracy 0.6535000205039978\n",
      "Iteration 21680 Training loss 0.09031547605991364 Validation loss 0.09516803175210953 Accuracy 0.7320000529289246\n",
      "Iteration 21690 Training loss 0.110262431204319 Validation loss 0.10804121941328049 Accuracy 0.6840000152587891\n",
      "Iteration 21700 Training loss 0.08516810834407806 Validation loss 0.09646307677030563 Accuracy 0.7105000615119934\n",
      "Iteration 21710 Training loss 0.08341084420681 Validation loss 0.09644322097301483 Accuracy 0.7305000424385071\n",
      "Iteration 21720 Training loss 0.10702413320541382 Validation loss 0.09748602658510208 Accuracy 0.7130000591278076\n",
      "Iteration 21730 Training loss 0.10440957546234131 Validation loss 0.10771693289279938 Accuracy 0.6545000076293945\n",
      "Iteration 21740 Training loss 0.11469457298517227 Validation loss 0.11889247596263885 Accuracy 0.5910000205039978\n",
      "Iteration 21750 Training loss 0.09954708069562912 Validation loss 0.10663799196481705 Accuracy 0.6775000095367432\n",
      "Iteration 21760 Training loss 0.10402907431125641 Validation loss 0.09888606518507004 Accuracy 0.7255000472068787\n",
      "Iteration 21770 Training loss 0.10121503472328186 Validation loss 0.09925980120897293 Accuracy 0.7050000429153442\n",
      "Iteration 21780 Training loss 0.08823326975107193 Validation loss 0.09455566853284836 Accuracy 0.7280000448226929\n",
      "Iteration 21790 Training loss 0.11369545757770538 Validation loss 0.12750467658042908 Accuracy 0.565500020980835\n",
      "Iteration 21800 Training loss 0.10583341121673584 Validation loss 0.10140003263950348 Accuracy 0.7045000195503235\n",
      "Iteration 21810 Training loss 0.09680584073066711 Validation loss 0.0971832200884819 Accuracy 0.7055000066757202\n",
      "Iteration 21820 Training loss 0.10647358000278473 Validation loss 0.09746020287275314 Accuracy 0.7105000615119934\n",
      "Iteration 21830 Training loss 0.10274175554513931 Validation loss 0.11193700134754181 Accuracy 0.6350000500679016\n",
      "Iteration 21840 Training loss 0.09639355540275574 Validation loss 0.10100027173757553 Accuracy 0.6950000524520874\n",
      "Iteration 21850 Training loss 0.10473623126745224 Validation loss 0.10831020772457123 Accuracy 0.6530000567436218\n",
      "Iteration 21860 Training loss 0.09451086074113846 Validation loss 0.0977327972650528 Accuracy 0.7255000472068787\n",
      "Iteration 21870 Training loss 0.1128135621547699 Validation loss 0.09693396091461182 Accuracy 0.7260000109672546\n",
      "Iteration 21880 Training loss 0.11086959391832352 Validation loss 0.09847055375576019 Accuracy 0.7110000252723694\n",
      "Iteration 21890 Training loss 0.0880383849143982 Validation loss 0.09500864893198013 Accuracy 0.7260000109672546\n",
      "Iteration 21900 Training loss 0.09124789386987686 Validation loss 0.0980963259935379 Accuracy 0.7135000228881836\n",
      "Iteration 21910 Training loss 0.08504465967416763 Validation loss 0.10052478313446045 Accuracy 0.6895000338554382\n",
      "Iteration 21920 Training loss 0.09478456526994705 Validation loss 0.10560311377048492 Accuracy 0.6680000424385071\n",
      "Iteration 21930 Training loss 0.09439509361982346 Validation loss 0.10010097920894623 Accuracy 0.7010000348091125\n",
      "Iteration 21940 Training loss 0.09731951355934143 Validation loss 0.09923582524061203 Accuracy 0.7140000462532043\n",
      "Iteration 21950 Training loss 0.10050622373819351 Validation loss 0.09502425044775009 Accuracy 0.7315000295639038\n",
      "Iteration 21960 Training loss 0.08351098746061325 Validation loss 0.09874845296144485 Accuracy 0.7320000529289246\n",
      "Iteration 21970 Training loss 0.10703851282596588 Validation loss 0.09846663475036621 Accuracy 0.7115000486373901\n",
      "Iteration 21980 Training loss 0.09803741425275803 Validation loss 0.09673629701137543 Accuracy 0.7315000295639038\n",
      "Iteration 21990 Training loss 0.09087866544723511 Validation loss 0.09510141611099243 Accuracy 0.7325000166893005\n",
      "Iteration 22000 Training loss 0.09406541287899017 Validation loss 0.0960535779595375 Accuracy 0.7235000133514404\n",
      "Iteration 22010 Training loss 0.07804305106401443 Validation loss 0.09617193043231964 Accuracy 0.7260000109672546\n",
      "Iteration 22020 Training loss 0.10258874297142029 Validation loss 0.10194256901741028 Accuracy 0.6885000467300415\n",
      "Iteration 22030 Training loss 0.08829738199710846 Validation loss 0.09985923022031784 Accuracy 0.7015000581741333\n",
      "Iteration 22040 Training loss 0.08873170614242554 Validation loss 0.09436281025409698 Accuracy 0.7280000448226929\n",
      "Iteration 22050 Training loss 0.09188137203454971 Validation loss 0.09429022669792175 Accuracy 0.7355000376701355\n",
      "Iteration 22060 Training loss 0.11026666313409805 Validation loss 0.09837557375431061 Accuracy 0.7100000381469727\n",
      "Iteration 22070 Training loss 0.0956752747297287 Validation loss 0.0973917692899704 Accuracy 0.7145000100135803\n",
      "Iteration 22080 Training loss 0.09372912347316742 Validation loss 0.10052681714296341 Accuracy 0.7100000381469727\n",
      "Iteration 22090 Training loss 0.09658707678318024 Validation loss 0.09573747962713242 Accuracy 0.7190000414848328\n",
      "Iteration 22100 Training loss 0.08657974749803543 Validation loss 0.09798699617385864 Accuracy 0.7045000195503235\n",
      "Iteration 22110 Training loss 0.10587434470653534 Validation loss 0.09449506551027298 Accuracy 0.734000027179718\n",
      "Iteration 22120 Training loss 0.08127011358737946 Validation loss 0.094188392162323 Accuracy 0.7300000190734863\n",
      "Iteration 22130 Training loss 0.09858863800764084 Validation loss 0.11380204558372498 Accuracy 0.6350000500679016\n",
      "Iteration 22140 Training loss 0.10685934126377106 Validation loss 0.10311245173215866 Accuracy 0.6760000586509705\n",
      "Iteration 22150 Training loss 0.08143971115350723 Validation loss 0.10368803888559341 Accuracy 0.6945000290870667\n",
      "Iteration 22160 Training loss 0.09354101121425629 Validation loss 0.09859651327133179 Accuracy 0.7090000510215759\n",
      "Iteration 22170 Training loss 0.08919351547956467 Validation loss 0.09508062899112701 Accuracy 0.7270000576972961\n",
      "Iteration 22180 Training loss 0.10891778022050858 Validation loss 0.0963190570473671 Accuracy 0.7190000414848328\n",
      "Iteration 22190 Training loss 0.09562472999095917 Validation loss 0.10877734422683716 Accuracy 0.659500002861023\n",
      "Iteration 22200 Training loss 0.09170270711183548 Validation loss 0.11031421273946762 Accuracy 0.643500030040741\n",
      "Iteration 22210 Training loss 0.09439566731452942 Validation loss 0.09981496632099152 Accuracy 0.6995000243186951\n",
      "Iteration 22220 Training loss 0.11313106119632721 Validation loss 0.11022710055112839 Accuracy 0.64000004529953\n",
      "Iteration 22230 Training loss 0.13804571330547333 Validation loss 0.12266544252634048 Accuracy 0.6270000338554382\n",
      "Iteration 22240 Training loss 0.08867282420396805 Validation loss 0.09852559119462967 Accuracy 0.7175000309944153\n",
      "Iteration 22250 Training loss 0.10257827490568161 Validation loss 0.09634196758270264 Accuracy 0.7290000319480896\n",
      "Iteration 22260 Training loss 0.08789140731096268 Validation loss 0.09635988622903824 Accuracy 0.7220000624656677\n",
      "Iteration 22270 Training loss 0.08925853669643402 Validation loss 0.10025671869516373 Accuracy 0.7155000567436218\n",
      "Iteration 22280 Training loss 0.10654685646295547 Validation loss 0.10167444497346878 Accuracy 0.6950000524520874\n",
      "Iteration 22290 Training loss 0.097012959420681 Validation loss 0.1001395732164383 Accuracy 0.7040000557899475\n",
      "Iteration 22300 Training loss 0.10176611691713333 Validation loss 0.10271658003330231 Accuracy 0.6930000185966492\n",
      "Iteration 22310 Training loss 0.09984981268644333 Validation loss 0.10576064139604568 Accuracy 0.6670000553131104\n",
      "Iteration 22320 Training loss 0.10907873511314392 Validation loss 0.11436862498521805 Accuracy 0.6175000071525574\n",
      "Iteration 22330 Training loss 0.1009271964430809 Validation loss 0.09529054909944534 Accuracy 0.7265000343322754\n",
      "Iteration 22340 Training loss 0.09354869276285172 Validation loss 0.09598810970783234 Accuracy 0.7355000376701355\n",
      "Iteration 22350 Training loss 0.12103193998336792 Validation loss 0.09687472134828568 Accuracy 0.7285000085830688\n",
      "Iteration 22360 Training loss 0.0851285308599472 Validation loss 0.09730488806962967 Accuracy 0.721500039100647\n",
      "Iteration 22370 Training loss 0.10327479988336563 Validation loss 0.1074284091591835 Accuracy 0.6515000462532043\n",
      "Iteration 22380 Training loss 0.09070838987827301 Validation loss 0.0959630236029625 Accuracy 0.7130000591278076\n",
      "Iteration 22390 Training loss 0.08161725848913193 Validation loss 0.09718010574579239 Accuracy 0.7275000214576721\n",
      "Iteration 22400 Training loss 0.10093900561332703 Validation loss 0.09514223039150238 Accuracy 0.7290000319480896\n",
      "Iteration 22410 Training loss 0.08997148275375366 Validation loss 0.09861414134502411 Accuracy 0.7255000472068787\n",
      "Iteration 22420 Training loss 0.1081499233841896 Validation loss 0.0980394259095192 Accuracy 0.70250004529953\n",
      "Iteration 22430 Training loss 0.10400844365358353 Validation loss 0.09437272697687149 Accuracy 0.7450000643730164\n",
      "Iteration 22440 Training loss 0.10723274946212769 Validation loss 0.10875613987445831 Accuracy 0.6625000238418579\n",
      "Iteration 22450 Training loss 0.11300452798604965 Validation loss 0.09784766286611557 Accuracy 0.7040000557899475\n",
      "Iteration 22460 Training loss 0.09632182866334915 Validation loss 0.1014121025800705 Accuracy 0.6940000057220459\n",
      "Iteration 22470 Training loss 0.10372837632894516 Validation loss 0.10590076446533203 Accuracy 0.671500027179718\n",
      "Iteration 22480 Training loss 0.09721967577934265 Validation loss 0.09617313742637634 Accuracy 0.7255000472068787\n",
      "Iteration 22490 Training loss 0.11305710673332214 Validation loss 0.11386475712060928 Accuracy 0.6430000066757202\n",
      "Iteration 22500 Training loss 0.09611425548791885 Validation loss 0.09670354425907135 Accuracy 0.7210000157356262\n",
      "Iteration 22510 Training loss 0.10371141135692596 Validation loss 0.09993267059326172 Accuracy 0.70250004529953\n",
      "Iteration 22520 Training loss 0.11515781283378601 Validation loss 0.11627017706632614 Accuracy 0.6290000081062317\n",
      "Iteration 22530 Training loss 0.09444653987884521 Validation loss 0.09653379023075104 Accuracy 0.7285000085830688\n",
      "Iteration 22540 Training loss 0.1055494099855423 Validation loss 0.10424380004405975 Accuracy 0.6655000448226929\n",
      "Iteration 22550 Training loss 0.08669853210449219 Validation loss 0.10038004070520401 Accuracy 0.6940000057220459\n",
      "Iteration 22560 Training loss 0.10480735450983047 Validation loss 0.10486893355846405 Accuracy 0.6740000247955322\n",
      "Iteration 22570 Training loss 0.1063498854637146 Validation loss 0.10597652196884155 Accuracy 0.6640000343322754\n",
      "Iteration 22580 Training loss 0.08802444487810135 Validation loss 0.09812519699335098 Accuracy 0.7070000171661377\n",
      "Iteration 22590 Training loss 0.10290034115314484 Validation loss 0.09784238040447235 Accuracy 0.7085000276565552\n",
      "Iteration 22600 Training loss 0.1093543991446495 Validation loss 0.1073228195309639 Accuracy 0.6525000333786011\n",
      "Iteration 22610 Training loss 0.09768681973218918 Validation loss 0.09424882382154465 Accuracy 0.7235000133514404\n",
      "Iteration 22620 Training loss 0.11015762388706207 Validation loss 0.09881813079118729 Accuracy 0.7150000333786011\n",
      "Iteration 22630 Training loss 0.1269962638616562 Validation loss 0.13188613951206207 Accuracy 0.6095000505447388\n",
      "Iteration 22640 Training loss 0.10486599057912827 Validation loss 0.10929399728775024 Accuracy 0.6455000042915344\n",
      "Iteration 22650 Training loss 0.11747588962316513 Validation loss 0.10994008183479309 Accuracy 0.6620000600814819\n",
      "Iteration 22660 Training loss 0.09493978321552277 Validation loss 0.10215666890144348 Accuracy 0.6935000419616699\n",
      "Iteration 22670 Training loss 0.07884220778942108 Validation loss 0.09694012999534607 Accuracy 0.7190000414848328\n",
      "Iteration 22680 Training loss 0.11721870303153992 Validation loss 0.1148131787776947 Accuracy 0.6215000152587891\n",
      "Iteration 22690 Training loss 0.09153423458337784 Validation loss 0.09496772289276123 Accuracy 0.7360000610351562\n",
      "Iteration 22700 Training loss 0.08430339395999908 Validation loss 0.10575256496667862 Accuracy 0.6690000295639038\n",
      "Iteration 22710 Training loss 0.08445102721452713 Validation loss 0.09544377774000168 Accuracy 0.7235000133514404\n",
      "Iteration 22720 Training loss 0.09648904949426651 Validation loss 0.09715448319911957 Accuracy 0.7160000205039978\n",
      "Iteration 22730 Training loss 0.08944964408874512 Validation loss 0.09702875465154648 Accuracy 0.7240000367164612\n",
      "Iteration 22740 Training loss 0.10266036540269852 Validation loss 0.09271979331970215 Accuracy 0.737000048160553\n",
      "Iteration 22750 Training loss 0.09542588889598846 Validation loss 0.09680002182722092 Accuracy 0.7310000061988831\n",
      "Iteration 22760 Training loss 0.09412680566310883 Validation loss 0.09478802978992462 Accuracy 0.737000048160553\n",
      "Iteration 22770 Training loss 0.12122396379709244 Validation loss 0.10318484157323837 Accuracy 0.671500027179718\n",
      "Iteration 22780 Training loss 0.08582314103841782 Validation loss 0.09722253680229187 Accuracy 0.7135000228881836\n",
      "Iteration 22790 Training loss 0.0885082259774208 Validation loss 0.09534656256437302 Accuracy 0.7210000157356262\n",
      "Iteration 22800 Training loss 0.10663212835788727 Validation loss 0.10627124458551407 Accuracy 0.6675000190734863\n",
      "Iteration 22810 Training loss 0.09022335708141327 Validation loss 0.09730438888072968 Accuracy 0.7165000438690186\n",
      "Iteration 22820 Training loss 0.1000075489282608 Validation loss 0.097115658223629 Accuracy 0.7230000495910645\n",
      "Iteration 22830 Training loss 0.07001782953739166 Validation loss 0.09895271062850952 Accuracy 0.7165000438690186\n",
      "Iteration 22840 Training loss 0.09666264057159424 Validation loss 0.09503002464771271 Accuracy 0.7295000553131104\n",
      "Iteration 22850 Training loss 0.1084451973438263 Validation loss 0.10557633638381958 Accuracy 0.6635000109672546\n",
      "Iteration 22860 Training loss 0.10766411572694778 Validation loss 0.09912630170583725 Accuracy 0.6985000371932983\n",
      "Iteration 22870 Training loss 0.09095169603824615 Validation loss 0.10102809220552444 Accuracy 0.690500020980835\n",
      "Iteration 22880 Training loss 0.10468446463346481 Validation loss 0.09973654896020889 Accuracy 0.6955000162124634\n",
      "Iteration 22890 Training loss 0.08684762567281723 Validation loss 0.09528414160013199 Accuracy 0.7255000472068787\n",
      "Iteration 22900 Training loss 0.11190565675497055 Validation loss 0.10267554223537445 Accuracy 0.706000030040741\n",
      "Iteration 22910 Training loss 0.10124176740646362 Validation loss 0.09860920906066895 Accuracy 0.7020000219345093\n",
      "Iteration 22920 Training loss 0.09740544110536575 Validation loss 0.0934426337480545 Accuracy 0.7420000433921814\n",
      "Iteration 22930 Training loss 0.0880291536450386 Validation loss 0.09418118745088577 Accuracy 0.7295000553131104\n",
      "Iteration 22940 Training loss 0.10603500157594681 Validation loss 0.09518982470035553 Accuracy 0.7135000228881836\n",
      "Iteration 22950 Training loss 0.10417095571756363 Validation loss 0.09591196477413177 Accuracy 0.7275000214576721\n",
      "Iteration 22960 Training loss 0.08813896030187607 Validation loss 0.09347280114889145 Accuracy 0.7365000247955322\n",
      "Iteration 22970 Training loss 0.10308420658111572 Validation loss 0.10950156301259995 Accuracy 0.6440000534057617\n",
      "Iteration 22980 Training loss 0.11322976648807526 Validation loss 0.10626555234193802 Accuracy 0.6640000343322754\n",
      "Iteration 22990 Training loss 0.1034717708826065 Validation loss 0.09995679557323456 Accuracy 0.7130000591278076\n",
      "Iteration 23000 Training loss 0.08789163082838058 Validation loss 0.09941550344228745 Accuracy 0.6930000185966492\n",
      "Iteration 23010 Training loss 0.10174742341041565 Validation loss 0.09260965883731842 Accuracy 0.7330000400543213\n",
      "Iteration 23020 Training loss 0.08723262697458267 Validation loss 0.09336060285568237 Accuracy 0.7295000553131104\n",
      "Iteration 23030 Training loss 0.11861778050661087 Validation loss 0.1130177453160286 Accuracy 0.6530000567436218\n",
      "Iteration 23040 Training loss 0.10011683404445648 Validation loss 0.09396912157535553 Accuracy 0.7310000061988831\n",
      "Iteration 23050 Training loss 0.07775110751390457 Validation loss 0.09777182340621948 Accuracy 0.703000009059906\n",
      "Iteration 23060 Training loss 0.09441253542900085 Validation loss 0.09790291637182236 Accuracy 0.7230000495910645\n",
      "Iteration 23070 Training loss 0.08360220491886139 Validation loss 0.10112976282835007 Accuracy 0.6920000314712524\n",
      "Iteration 23080 Training loss 0.10237354785203934 Validation loss 0.09994882345199585 Accuracy 0.703000009059906\n",
      "Iteration 23090 Training loss 0.1167350709438324 Validation loss 0.09863696992397308 Accuracy 0.7045000195503235\n",
      "Iteration 23100 Training loss 0.09264642745256424 Validation loss 0.0965084582567215 Accuracy 0.7250000238418579\n",
      "Iteration 23110 Training loss 0.07690495997667313 Validation loss 0.09482790529727936 Accuracy 0.733500063419342\n",
      "Iteration 23120 Training loss 0.09510253369808197 Validation loss 0.09650703519582748 Accuracy 0.7160000205039978\n",
      "Iteration 23130 Training loss 0.08621543645858765 Validation loss 0.09509646892547607 Accuracy 0.7275000214576721\n",
      "Iteration 23140 Training loss 0.09336639195680618 Validation loss 0.09535498917102814 Accuracy 0.7170000076293945\n",
      "Iteration 23150 Training loss 0.07869727164506912 Validation loss 0.09422054141759872 Accuracy 0.7260000109672546\n",
      "Iteration 23160 Training loss 0.08816280215978622 Validation loss 0.0924212858080864 Accuracy 0.737500011920929\n",
      "Iteration 23170 Training loss 0.08710647374391556 Validation loss 0.09498964995145798 Accuracy 0.7360000610351562\n",
      "Iteration 23180 Training loss 0.09572040289640427 Validation loss 0.10079549998044968 Accuracy 0.6935000419616699\n",
      "Iteration 23190 Training loss 0.0922086238861084 Validation loss 0.09722919017076492 Accuracy 0.7070000171661377\n",
      "Iteration 23200 Training loss 0.1066778376698494 Validation loss 0.10717516392469406 Accuracy 0.6525000333786011\n",
      "Iteration 23210 Training loss 0.11073373258113861 Validation loss 0.10314607620239258 Accuracy 0.6930000185966492\n",
      "Iteration 23220 Training loss 0.08817578852176666 Validation loss 0.09243246912956238 Accuracy 0.7315000295639038\n",
      "Iteration 23230 Training loss 0.08259784430265427 Validation loss 0.09555281698703766 Accuracy 0.7430000305175781\n",
      "Iteration 23240 Training loss 0.10275156050920486 Validation loss 0.10309474915266037 Accuracy 0.6980000138282776\n",
      "Iteration 23250 Training loss 0.10884862393140793 Validation loss 0.10165654122829437 Accuracy 0.7015000581741333\n",
      "Iteration 23260 Training loss 0.1117137148976326 Validation loss 0.10181427747011185 Accuracy 0.6850000619888306\n",
      "Iteration 23270 Training loss 0.09757385402917862 Validation loss 0.10108252614736557 Accuracy 0.6925000548362732\n",
      "Iteration 23280 Training loss 0.0877445787191391 Validation loss 0.09575580805540085 Accuracy 0.7270000576972961\n",
      "Iteration 23290 Training loss 0.09727876633405685 Validation loss 0.09492576122283936 Accuracy 0.7310000061988831\n",
      "Iteration 23300 Training loss 0.09754239767789841 Validation loss 0.09390946477651596 Accuracy 0.7350000143051147\n",
      "Iteration 23310 Training loss 0.12472525238990784 Validation loss 0.11262867599725723 Accuracy 0.6295000314712524\n",
      "Iteration 23320 Training loss 0.1092875748872757 Validation loss 0.10196803510189056 Accuracy 0.7075000405311584\n",
      "Iteration 23330 Training loss 0.09539850801229477 Validation loss 0.0963464081287384 Accuracy 0.7200000286102295\n",
      "Iteration 23340 Training loss 0.10111791640520096 Validation loss 0.10715451836585999 Accuracy 0.6605000495910645\n",
      "Iteration 23350 Training loss 0.0983848050236702 Validation loss 0.09604758024215698 Accuracy 0.7255000472068787\n",
      "Iteration 23360 Training loss 0.10360605269670486 Validation loss 0.09999369829893112 Accuracy 0.7035000324249268\n",
      "Iteration 23370 Training loss 0.09285758435726166 Validation loss 0.09703612327575684 Accuracy 0.7195000052452087\n",
      "Iteration 23380 Training loss 0.09950334578752518 Validation loss 0.09821907430887222 Accuracy 0.6980000138282776\n",
      "Iteration 23390 Training loss 0.08989481627941132 Validation loss 0.10041283816099167 Accuracy 0.6945000290870667\n",
      "Iteration 23400 Training loss 0.1029052883386612 Validation loss 0.10043007135391235 Accuracy 0.7000000476837158\n",
      "Iteration 23410 Training loss 0.09165363013744354 Validation loss 0.09663669019937515 Accuracy 0.7155000567436218\n",
      "Iteration 23420 Training loss 0.100327268242836 Validation loss 0.09744881093502045 Accuracy 0.7200000286102295\n",
      "Iteration 23430 Training loss 0.0877489447593689 Validation loss 0.09907019883394241 Accuracy 0.7105000615119934\n",
      "Iteration 23440 Training loss 0.09022802114486694 Validation loss 0.09634096175432205 Accuracy 0.718500018119812\n",
      "Iteration 23450 Training loss 0.08605879545211792 Validation loss 0.09314857423305511 Accuracy 0.7315000295639038\n",
      "Iteration 23460 Training loss 0.10550796240568161 Validation loss 0.10381302237510681 Accuracy 0.6895000338554382\n",
      "Iteration 23470 Training loss 0.08257889747619629 Validation loss 0.09546275436878204 Accuracy 0.7435000538825989\n",
      "Iteration 23480 Training loss 0.11130432039499283 Validation loss 0.09519296139478683 Accuracy 0.721500039100647\n",
      "Iteration 23490 Training loss 0.08245551586151123 Validation loss 0.10019553452730179 Accuracy 0.6955000162124634\n",
      "Iteration 23500 Training loss 0.08215131610631943 Validation loss 0.09471885859966278 Accuracy 0.7330000400543213\n",
      "Iteration 23510 Training loss 0.09527643024921417 Validation loss 0.09755605459213257 Accuracy 0.7095000147819519\n",
      "Iteration 23520 Training loss 0.094913549721241 Validation loss 0.11098510771989822 Accuracy 0.6490000486373901\n",
      "Iteration 23530 Training loss 0.09748691320419312 Validation loss 0.09312377125024796 Accuracy 0.7365000247955322\n",
      "Iteration 23540 Training loss 0.12837861478328705 Validation loss 0.1160091981291771 Accuracy 0.6575000286102295\n",
      "Iteration 23550 Training loss 0.11591047048568726 Validation loss 0.09970314055681229 Accuracy 0.7065000534057617\n",
      "Iteration 23560 Training loss 0.0736459344625473 Validation loss 0.09433621913194656 Accuracy 0.7400000095367432\n",
      "Iteration 23570 Training loss 0.09414196759462357 Validation loss 0.0955243781208992 Accuracy 0.7350000143051147\n",
      "Iteration 23580 Training loss 0.1149870827794075 Validation loss 0.1124395877122879 Accuracy 0.6260000467300415\n",
      "Iteration 23590 Training loss 0.09682094305753708 Validation loss 0.09667325019836426 Accuracy 0.7140000462532043\n",
      "Iteration 23600 Training loss 0.08916860073804855 Validation loss 0.09995467215776443 Accuracy 0.7040000557899475\n",
      "Iteration 23610 Training loss 0.09820033609867096 Validation loss 0.09921715408563614 Accuracy 0.7175000309944153\n",
      "Iteration 23620 Training loss 0.06660763174295425 Validation loss 0.09603836387395859 Accuracy 0.7135000228881836\n",
      "Iteration 23630 Training loss 0.08280918002128601 Validation loss 0.0985044315457344 Accuracy 0.706000030040741\n",
      "Iteration 23640 Training loss 0.10453099757432938 Validation loss 0.0966334268450737 Accuracy 0.7175000309944153\n",
      "Iteration 23650 Training loss 0.08725515007972717 Validation loss 0.101737380027771 Accuracy 0.6990000605583191\n",
      "Iteration 23660 Training loss 0.11962994188070297 Validation loss 0.11903539299964905 Accuracy 0.5890000462532043\n",
      "Iteration 23670 Training loss 0.10122750699520111 Validation loss 0.09723032265901566 Accuracy 0.7055000066757202\n",
      "Iteration 23680 Training loss 0.07998764514923096 Validation loss 0.09922649711370468 Accuracy 0.6945000290870667\n",
      "Iteration 23690 Training loss 0.09283668547868729 Validation loss 0.10633984953165054 Accuracy 0.6565000414848328\n",
      "Iteration 23700 Training loss 0.08652708679437637 Validation loss 0.09569986164569855 Accuracy 0.7265000343322754\n",
      "Iteration 23710 Training loss 0.09050877392292023 Validation loss 0.09340368956327438 Accuracy 0.7240000367164612\n",
      "Iteration 23720 Training loss 0.10251820087432861 Validation loss 0.09369904547929764 Accuracy 0.737500011920929\n",
      "Iteration 23730 Training loss 0.09687072783708572 Validation loss 0.09481680393218994 Accuracy 0.7425000071525574\n",
      "Iteration 23740 Training loss 0.09584275633096695 Validation loss 0.09857320040464401 Accuracy 0.7170000076293945\n",
      "Iteration 23750 Training loss 0.0860317051410675 Validation loss 0.10083155333995819 Accuracy 0.6945000290870667\n",
      "Iteration 23760 Training loss 0.10394952446222305 Validation loss 0.10424000024795532 Accuracy 0.671500027179718\n",
      "Iteration 23770 Training loss 0.08369075506925583 Validation loss 0.0937410444021225 Accuracy 0.7445000410079956\n",
      "Iteration 23780 Training loss 0.07560701668262482 Validation loss 0.10489079356193542 Accuracy 0.6775000095367432\n",
      "Iteration 23790 Training loss 0.07889582216739655 Validation loss 0.09376034885644913 Accuracy 0.7275000214576721\n",
      "Iteration 23800 Training loss 0.10921476036310196 Validation loss 0.09713878482580185 Accuracy 0.7255000472068787\n",
      "Iteration 23810 Training loss 0.10236659646034241 Validation loss 0.10757552087306976 Accuracy 0.6600000262260437\n",
      "Iteration 23820 Training loss 0.1008700430393219 Validation loss 0.0941309928894043 Accuracy 0.7250000238418579\n",
      "Iteration 23830 Training loss 0.10347206890583038 Validation loss 0.10746044665575027 Accuracy 0.655500054359436\n",
      "Iteration 23840 Training loss 0.10163968801498413 Validation loss 0.10370274633169174 Accuracy 0.6690000295639038\n",
      "Iteration 23850 Training loss 0.09423784911632538 Validation loss 0.09365720301866531 Accuracy 0.7290000319480896\n",
      "Iteration 23860 Training loss 0.10817503929138184 Validation loss 0.09955684095621109 Accuracy 0.6955000162124634\n",
      "Iteration 23870 Training loss 0.10273875296115875 Validation loss 0.11020051687955856 Accuracy 0.6655000448226929\n",
      "Iteration 23880 Training loss 0.11249395459890366 Validation loss 0.10673379898071289 Accuracy 0.6690000295639038\n",
      "Iteration 23890 Training loss 0.09629396349191666 Validation loss 0.09412436932325363 Accuracy 0.7320000529289246\n",
      "Iteration 23900 Training loss 0.0700850635766983 Validation loss 0.09347934275865555 Accuracy 0.7330000400543213\n",
      "Iteration 23910 Training loss 0.10182367265224457 Validation loss 0.09320595115423203 Accuracy 0.7275000214576721\n",
      "Iteration 23920 Training loss 0.08343873918056488 Validation loss 0.09161334484815598 Accuracy 0.7395000457763672\n",
      "Iteration 23930 Training loss 0.0856071338057518 Validation loss 0.09684477746486664 Accuracy 0.7160000205039978\n",
      "Iteration 23940 Training loss 0.09945724159479141 Validation loss 0.0965430960059166 Accuracy 0.7195000052452087\n",
      "Iteration 23950 Training loss 0.09503262490034103 Validation loss 0.09306424856185913 Accuracy 0.7445000410079956\n",
      "Iteration 23960 Training loss 0.11249101161956787 Validation loss 0.09830126166343689 Accuracy 0.7235000133514404\n",
      "Iteration 23970 Training loss 0.10078617930412292 Validation loss 0.10237502306699753 Accuracy 0.6825000047683716\n",
      "Iteration 23980 Training loss 0.10243932157754898 Validation loss 0.0979837104678154 Accuracy 0.7125000357627869\n",
      "Iteration 23990 Training loss 0.13740301132202148 Validation loss 0.12391319125890732 Accuracy 0.612500011920929\n",
      "Iteration 24000 Training loss 0.09196281433105469 Validation loss 0.09603182226419449 Accuracy 0.7140000462532043\n",
      "Iteration 24010 Training loss 0.09127508103847504 Validation loss 0.09381381422281265 Accuracy 0.7425000071525574\n",
      "Iteration 24020 Training loss 0.10091119259595871 Validation loss 0.09754417836666107 Accuracy 0.7210000157356262\n",
      "Iteration 24030 Training loss 0.10788783431053162 Validation loss 0.09643898159265518 Accuracy 0.7190000414848328\n",
      "Iteration 24040 Training loss 0.11378354579210281 Validation loss 0.10794657468795776 Accuracy 0.6640000343322754\n",
      "Iteration 24050 Training loss 0.09581296890974045 Validation loss 0.093177430331707 Accuracy 0.7435000538825989\n",
      "Iteration 24060 Training loss 0.08569032698869705 Validation loss 0.09637746214866638 Accuracy 0.7140000462532043\n",
      "Iteration 24070 Training loss 0.10258392989635468 Validation loss 0.09755396097898483 Accuracy 0.7230000495910645\n",
      "Iteration 24080 Training loss 0.10314515233039856 Validation loss 0.10329119116067886 Accuracy 0.6850000619888306\n",
      "Iteration 24090 Training loss 0.10600976645946503 Validation loss 0.10084261000156403 Accuracy 0.7150000333786011\n",
      "Iteration 24100 Training loss 0.09566904604434967 Validation loss 0.09652002900838852 Accuracy 0.7280000448226929\n",
      "Iteration 24110 Training loss 0.11098300665616989 Validation loss 0.10746268182992935 Accuracy 0.659500002861023\n",
      "Iteration 24120 Training loss 0.08071447908878326 Validation loss 0.09519851952791214 Accuracy 0.7140000462532043\n",
      "Iteration 24130 Training loss 0.10465815663337708 Validation loss 0.10602463036775589 Accuracy 0.6775000095367432\n",
      "Iteration 24140 Training loss 0.10369811952114105 Validation loss 0.10975395888090134 Accuracy 0.6450000405311584\n",
      "Iteration 24150 Training loss 0.09694665670394897 Validation loss 0.0979236587882042 Accuracy 0.7045000195503235\n",
      "Iteration 24160 Training loss 0.11921447515487671 Validation loss 0.11480990797281265 Accuracy 0.6530000567436218\n",
      "Iteration 24170 Training loss 0.0914207249879837 Validation loss 0.09644031524658203 Accuracy 0.7200000286102295\n",
      "Iteration 24180 Training loss 0.10227657109498978 Validation loss 0.10862705111503601 Accuracy 0.6510000228881836\n",
      "Iteration 24190 Training loss 0.10744782537221909 Validation loss 0.10559970140457153 Accuracy 0.6775000095367432\n",
      "Iteration 24200 Training loss 0.10673625767230988 Validation loss 0.0976499393582344 Accuracy 0.7255000472068787\n",
      "Iteration 24210 Training loss 0.07619982212781906 Validation loss 0.09354834258556366 Accuracy 0.7380000352859497\n",
      "Iteration 24220 Training loss 0.09573163837194443 Validation loss 0.09448662400245667 Accuracy 0.7235000133514404\n",
      "Iteration 24230 Training loss 0.07652337104082108 Validation loss 0.09610755741596222 Accuracy 0.7125000357627869\n",
      "Iteration 24240 Training loss 0.10906316339969635 Validation loss 0.09747327119112015 Accuracy 0.7165000438690186\n",
      "Iteration 24250 Training loss 0.09582992643117905 Validation loss 0.09440214186906815 Accuracy 0.734000027179718\n",
      "Iteration 24260 Training loss 0.11475953459739685 Validation loss 0.10812846571207047 Accuracy 0.6650000214576721\n",
      "Iteration 24270 Training loss 0.08852332830429077 Validation loss 0.09762286394834518 Accuracy 0.7045000195503235\n",
      "Iteration 24280 Training loss 0.09859418869018555 Validation loss 0.1013907790184021 Accuracy 0.7040000557899475\n",
      "Iteration 24290 Training loss 0.09553559869527817 Validation loss 0.10158690065145493 Accuracy 0.7015000581741333\n",
      "Iteration 24300 Training loss 0.110244020819664 Validation loss 0.1024334654211998 Accuracy 0.6845000386238098\n",
      "Iteration 24310 Training loss 0.09710580855607986 Validation loss 0.0933988094329834 Accuracy 0.7400000095367432\n",
      "Iteration 24320 Training loss 0.10068614780902863 Validation loss 0.10075435042381287 Accuracy 0.6975000500679016\n",
      "Iteration 24330 Training loss 0.10828088223934174 Validation loss 0.10186684131622314 Accuracy 0.6815000176429749\n",
      "Iteration 24340 Training loss 0.0852499008178711 Validation loss 0.0949847400188446 Accuracy 0.7220000624656677\n",
      "Iteration 24350 Training loss 0.09857267141342163 Validation loss 0.1064690575003624 Accuracy 0.6690000295639038\n",
      "Iteration 24360 Training loss 0.14229674637317657 Validation loss 0.1255958378314972 Accuracy 0.581000030040741\n",
      "Iteration 24370 Training loss 0.09795667231082916 Validation loss 0.10152214765548706 Accuracy 0.6850000619888306\n",
      "Iteration 24380 Training loss 0.10221479088068008 Validation loss 0.10539131611585617 Accuracy 0.6755000352859497\n",
      "Iteration 24390 Training loss 0.08461524546146393 Validation loss 0.09471020847558975 Accuracy 0.7355000376701355\n",
      "Iteration 24400 Training loss 0.10655410587787628 Validation loss 0.10686945170164108 Accuracy 0.6605000495910645\n",
      "Iteration 24410 Training loss 0.07703324407339096 Validation loss 0.09257245063781738 Accuracy 0.7300000190734863\n",
      "Iteration 24420 Training loss 0.1007932648062706 Validation loss 0.09929533302783966 Accuracy 0.7045000195503235\n",
      "Iteration 24430 Training loss 0.08611849695444107 Validation loss 0.10281378775835037 Accuracy 0.6810000538825989\n",
      "Iteration 24440 Training loss 0.08154454827308655 Validation loss 0.0921485498547554 Accuracy 0.7460000514984131\n",
      "Iteration 24450 Training loss 0.09464459121227264 Validation loss 0.09350230544805527 Accuracy 0.7465000152587891\n",
      "Iteration 24460 Training loss 0.11131202429533005 Validation loss 0.09786321222782135 Accuracy 0.7080000042915344\n",
      "Iteration 24470 Training loss 0.10932742804288864 Validation loss 0.09627022594213486 Accuracy 0.7135000228881836\n",
      "Iteration 24480 Training loss 0.08337084949016571 Validation loss 0.0941246822476387 Accuracy 0.7265000343322754\n",
      "Iteration 24490 Training loss 0.0891142413020134 Validation loss 0.10132615268230438 Accuracy 0.6910000443458557\n",
      "Iteration 24500 Training loss 0.126937136054039 Validation loss 0.09366116672754288 Accuracy 0.7290000319480896\n",
      "Iteration 24510 Training loss 0.09883072972297668 Validation loss 0.09584302455186844 Accuracy 0.7290000319480896\n",
      "Iteration 24520 Training loss 0.08726989477872849 Validation loss 0.09643381088972092 Accuracy 0.7195000052452087\n",
      "Iteration 24530 Training loss 0.1047738716006279 Validation loss 0.09911967813968658 Accuracy 0.7070000171661377\n",
      "Iteration 24540 Training loss 0.09266568720340729 Validation loss 0.09228012710809708 Accuracy 0.7435000538825989\n",
      "Iteration 24550 Training loss 0.08064227551221848 Validation loss 0.09334613382816315 Accuracy 0.7430000305175781\n",
      "Iteration 24560 Training loss 0.11318101733922958 Validation loss 0.1085423156619072 Accuracy 0.6600000262260437\n",
      "Iteration 24570 Training loss 0.10879534482955933 Validation loss 0.10469160228967667 Accuracy 0.6810000538825989\n",
      "Iteration 24580 Training loss 0.09150569885969162 Validation loss 0.09076076000928879 Accuracy 0.7475000619888306\n",
      "Iteration 24590 Training loss 0.1085028275847435 Validation loss 0.09236295521259308 Accuracy 0.7355000376701355\n",
      "Iteration 24600 Training loss 0.10186248272657394 Validation loss 0.10621844977140427 Accuracy 0.6740000247955322\n",
      "Iteration 24610 Training loss 0.09173677116632462 Validation loss 0.09587259590625763 Accuracy 0.7245000600814819\n",
      "Iteration 24620 Training loss 0.09822330623865128 Validation loss 0.09530787914991379 Accuracy 0.7220000624656677\n",
      "Iteration 24630 Training loss 0.10476317256689072 Validation loss 0.09521959722042084 Accuracy 0.7230000495910645\n",
      "Iteration 24640 Training loss 0.10024357587099075 Validation loss 0.09400592744350433 Accuracy 0.733500063419342\n",
      "Iteration 24650 Training loss 0.11283191293478012 Validation loss 0.10299943387508392 Accuracy 0.6765000224113464\n",
      "Iteration 24660 Training loss 0.0916079506278038 Validation loss 0.09845549613237381 Accuracy 0.7010000348091125\n",
      "Iteration 24670 Training loss 0.10009568929672241 Validation loss 0.09950681775808334 Accuracy 0.7010000348091125\n",
      "Iteration 24680 Training loss 0.07927477359771729 Validation loss 0.09332650899887085 Accuracy 0.7440000176429749\n",
      "Iteration 24690 Training loss 0.10247480869293213 Validation loss 0.10192171484231949 Accuracy 0.6820000410079956\n",
      "Iteration 24700 Training loss 0.11981610208749771 Validation loss 0.12030133605003357 Accuracy 0.6095000505447388\n",
      "Iteration 24710 Training loss 0.09746682643890381 Validation loss 0.1050812378525734 Accuracy 0.6660000085830688\n",
      "Iteration 24720 Training loss 0.10202911496162415 Validation loss 0.09934046864509583 Accuracy 0.7050000429153442\n",
      "Iteration 24730 Training loss 0.10982091724872589 Validation loss 0.11288709193468094 Accuracy 0.6545000076293945\n",
      "Iteration 24740 Training loss 0.10202848166227341 Validation loss 0.0988389179110527 Accuracy 0.7040000557899475\n",
      "Iteration 24750 Training loss 0.11713969707489014 Validation loss 0.10363739728927612 Accuracy 0.6755000352859497\n",
      "Iteration 24760 Training loss 0.08752407133579254 Validation loss 0.10123447328805923 Accuracy 0.6815000176429749\n",
      "Iteration 24770 Training loss 0.09629076719284058 Validation loss 0.09760703146457672 Accuracy 0.7250000238418579\n",
      "Iteration 24780 Training loss 0.10022784769535065 Validation loss 0.09573362022638321 Accuracy 0.718000054359436\n",
      "Iteration 24790 Training loss 0.09758111834526062 Validation loss 0.09505284577608109 Accuracy 0.7255000472068787\n",
      "Iteration 24800 Training loss 0.08229658752679825 Validation loss 0.09829143434762955 Accuracy 0.7055000066757202\n",
      "Iteration 24810 Training loss 0.09467267990112305 Validation loss 0.09344832599163055 Accuracy 0.7490000128746033\n",
      "Iteration 24820 Training loss 0.09266683459281921 Validation loss 0.10077497363090515 Accuracy 0.7095000147819519\n",
      "Iteration 24830 Training loss 0.08786320686340332 Validation loss 0.09434565901756287 Accuracy 0.7255000472068787\n",
      "Iteration 24840 Training loss 0.1021885871887207 Validation loss 0.09433525800704956 Accuracy 0.7325000166893005\n",
      "Iteration 24850 Training loss 0.10600069165229797 Validation loss 0.10068217664957047 Accuracy 0.6930000185966492\n",
      "Iteration 24860 Training loss 0.0972859337925911 Validation loss 0.09874805808067322 Accuracy 0.7195000052452087\n",
      "Iteration 24870 Training loss 0.08686687052249908 Validation loss 0.09483477473258972 Accuracy 0.7285000085830688\n",
      "Iteration 24880 Training loss 0.11006234586238861 Validation loss 0.09651192277669907 Accuracy 0.7245000600814819\n",
      "Iteration 24890 Training loss 0.08816420286893845 Validation loss 0.10026207566261292 Accuracy 0.6875000596046448\n",
      "Iteration 24900 Training loss 0.08595815300941467 Validation loss 0.09483098238706589 Accuracy 0.7260000109672546\n",
      "Iteration 24910 Training loss 0.13846254348754883 Validation loss 0.12712566554546356 Accuracy 0.5990000367164612\n",
      "Iteration 24920 Training loss 0.09553522616624832 Validation loss 0.10357926040887833 Accuracy 0.687000036239624\n",
      "Iteration 24930 Training loss 0.09213799983263016 Validation loss 0.10513925552368164 Accuracy 0.6755000352859497\n",
      "Iteration 24940 Training loss 0.0807109847664833 Validation loss 0.10027587413787842 Accuracy 0.6885000467300415\n",
      "Iteration 24950 Training loss 0.0975039079785347 Validation loss 0.10644316673278809 Accuracy 0.6605000495910645\n",
      "Iteration 24960 Training loss 0.07927723973989487 Validation loss 0.09212549775838852 Accuracy 0.7345000505447388\n",
      "Iteration 24970 Training loss 0.08918290585279465 Validation loss 0.09511595964431763 Accuracy 0.7415000200271606\n",
      "Iteration 24980 Training loss 0.09621760994195938 Validation loss 0.10243602842092514 Accuracy 0.6990000605583191\n",
      "Iteration 24990 Training loss 0.09204462170600891 Validation loss 0.09861674904823303 Accuracy 0.7140000462532043\n",
      "Iteration 25000 Training loss 0.0922795906662941 Validation loss 0.09862127900123596 Accuracy 0.721500039100647\n",
      "Iteration 25010 Training loss 0.09737890958786011 Validation loss 0.09884613752365112 Accuracy 0.7065000534057617\n",
      "Iteration 25020 Training loss 0.12195565551519394 Validation loss 0.10966218262910843 Accuracy 0.6505000591278076\n",
      "Iteration 25030 Training loss 0.0930069163441658 Validation loss 0.09476061165332794 Accuracy 0.7270000576972961\n",
      "Iteration 25040 Training loss 0.10629548877477646 Validation loss 0.10451877117156982 Accuracy 0.6850000619888306\n",
      "Iteration 25050 Training loss 0.1108081191778183 Validation loss 0.10391915589570999 Accuracy 0.674500048160553\n",
      "Iteration 25060 Training loss 0.08367979526519775 Validation loss 0.09254173934459686 Accuracy 0.7320000529289246\n",
      "Iteration 25070 Training loss 0.09197116643190384 Validation loss 0.10256507992744446 Accuracy 0.6980000138282776\n",
      "Iteration 25080 Training loss 0.10094871371984482 Validation loss 0.10803781449794769 Accuracy 0.6525000333786011\n",
      "Iteration 25090 Training loss 0.10580451786518097 Validation loss 0.10533716529607773 Accuracy 0.6670000553131104\n",
      "Iteration 25100 Training loss 0.10850334167480469 Validation loss 0.10160825401544571 Accuracy 0.6955000162124634\n",
      "Iteration 25110 Training loss 0.09252292662858963 Validation loss 0.10073161870241165 Accuracy 0.7115000486373901\n",
      "Iteration 25120 Training loss 0.09245305508375168 Validation loss 0.09202717989683151 Accuracy 0.7320000529289246\n",
      "Iteration 25130 Training loss 0.12403082847595215 Validation loss 0.12312018126249313 Accuracy 0.5855000019073486\n",
      "Iteration 25140 Training loss 0.09377013146877289 Validation loss 0.09513786435127258 Accuracy 0.7120000123977661\n",
      "Iteration 25150 Training loss 0.0865703597664833 Validation loss 0.09652811288833618 Accuracy 0.7365000247955322\n",
      "Iteration 25160 Training loss 0.09976097196340561 Validation loss 0.09963266551494598 Accuracy 0.7020000219345093\n",
      "Iteration 25170 Training loss 0.08461908996105194 Validation loss 0.09243173897266388 Accuracy 0.7380000352859497\n",
      "Iteration 25180 Training loss 0.10991258174180984 Validation loss 0.11287933588027954 Accuracy 0.624500036239624\n",
      "Iteration 25190 Training loss 0.10380298644304276 Validation loss 0.10286591947078705 Accuracy 0.7020000219345093\n",
      "Iteration 25200 Training loss 0.09005935490131378 Validation loss 0.09349560737609863 Accuracy 0.7230000495910645\n",
      "Iteration 25210 Training loss 0.0938357338309288 Validation loss 0.09808650612831116 Accuracy 0.7105000615119934\n",
      "Iteration 25220 Training loss 0.10578710585832596 Validation loss 0.09392450749874115 Accuracy 0.7240000367164612\n",
      "Iteration 25230 Training loss 0.09852609783411026 Validation loss 0.10102731734514236 Accuracy 0.6980000138282776\n",
      "Iteration 25240 Training loss 0.10153540968894958 Validation loss 0.10089384764432907 Accuracy 0.6960000395774841\n",
      "Iteration 25250 Training loss 0.09149936586618423 Validation loss 0.0936756506562233 Accuracy 0.7330000400543213\n",
      "Iteration 25260 Training loss 0.08635085821151733 Validation loss 0.09564299881458282 Accuracy 0.7115000486373901\n",
      "Iteration 25270 Training loss 0.10252536833286285 Validation loss 0.0963265597820282 Accuracy 0.7115000486373901\n",
      "Iteration 25280 Training loss 0.0822410061955452 Validation loss 0.09304261952638626 Accuracy 0.7285000085830688\n",
      "Iteration 25290 Training loss 0.11125105619430542 Validation loss 0.09705640375614166 Accuracy 0.7260000109672546\n",
      "Iteration 25300 Training loss 0.10307849943637848 Validation loss 0.0948028638958931 Accuracy 0.7225000262260437\n",
      "Iteration 25310 Training loss 0.11686612665653229 Validation loss 0.10605751723051071 Accuracy 0.6775000095367432\n",
      "Iteration 25320 Training loss 0.09715325385332108 Validation loss 0.10734911262989044 Accuracy 0.6810000538825989\n",
      "Iteration 25330 Training loss 0.10042259842157364 Validation loss 0.10198339819908142 Accuracy 0.6840000152587891\n",
      "Iteration 25340 Training loss 0.10502024739980698 Validation loss 0.09772594273090363 Accuracy 0.7165000438690186\n",
      "Iteration 25350 Training loss 0.09874784201383591 Validation loss 0.1035442054271698 Accuracy 0.6710000038146973\n",
      "Iteration 25360 Training loss 0.09137505292892456 Validation loss 0.1039959043264389 Accuracy 0.6775000095367432\n",
      "Iteration 25370 Training loss 0.10291668772697449 Validation loss 0.10147673636674881 Accuracy 0.7235000133514404\n",
      "Iteration 25380 Training loss 0.07837405055761337 Validation loss 0.09293753653764725 Accuracy 0.7355000376701355\n",
      "Iteration 25390 Training loss 0.09780789166688919 Validation loss 0.093990758061409 Accuracy 0.7305000424385071\n",
      "Iteration 25400 Training loss 0.1126902773976326 Validation loss 0.1113903596997261 Accuracy 0.6620000600814819\n",
      "Iteration 25410 Training loss 0.10515066981315613 Validation loss 0.10342204570770264 Accuracy 0.6810000538825989\n",
      "Iteration 25420 Training loss 0.09420524537563324 Validation loss 0.09877301752567291 Accuracy 0.7010000348091125\n",
      "Iteration 25430 Training loss 0.0996665433049202 Validation loss 0.09454886615276337 Accuracy 0.7395000457763672\n",
      "Iteration 25440 Training loss 0.11615001410245895 Validation loss 0.11463947594165802 Accuracy 0.659000039100647\n",
      "Iteration 25450 Training loss 0.0892808735370636 Validation loss 0.09592262655496597 Accuracy 0.7325000166893005\n",
      "Iteration 25460 Training loss 0.08418091386556625 Validation loss 0.09087264537811279 Accuracy 0.7420000433921814\n",
      "Iteration 25470 Training loss 0.09403003752231598 Validation loss 0.09768674522638321 Accuracy 0.7170000076293945\n",
      "Iteration 25480 Training loss 0.10321643948554993 Validation loss 0.09240923076868057 Accuracy 0.7465000152587891\n",
      "Iteration 25490 Training loss 0.07969699800014496 Validation loss 0.09121249616146088 Accuracy 0.7345000505447388\n",
      "Iteration 25500 Training loss 0.11417607218027115 Validation loss 0.10355289280414581 Accuracy 0.6785000562667847\n",
      "Iteration 25510 Training loss 0.0928042083978653 Validation loss 0.10440271347761154 Accuracy 0.671500027179718\n",
      "Iteration 25520 Training loss 0.10053804516792297 Validation loss 0.10140037536621094 Accuracy 0.6860000491142273\n",
      "Iteration 25530 Training loss 0.07902811467647552 Validation loss 0.09133806079626083 Accuracy 0.737500011920929\n",
      "Iteration 25540 Training loss 0.08729373663663864 Validation loss 0.11158545315265656 Accuracy 0.6450000405311584\n",
      "Iteration 25550 Training loss 0.11142415553331375 Validation loss 0.0994224101305008 Accuracy 0.690500020980835\n",
      "Iteration 25560 Training loss 0.09137184172868729 Validation loss 0.0979999527335167 Accuracy 0.7040000557899475\n",
      "Iteration 25570 Training loss 0.08128514885902405 Validation loss 0.09412967413663864 Accuracy 0.7230000495910645\n",
      "Iteration 25580 Training loss 0.10022514313459396 Validation loss 0.09664984792470932 Accuracy 0.7095000147819519\n",
      "Iteration 25590 Training loss 0.09655717015266418 Validation loss 0.09747116267681122 Accuracy 0.7200000286102295\n",
      "Iteration 25600 Training loss 0.08648744225502014 Validation loss 0.09595059603452682 Accuracy 0.7210000157356262\n",
      "Iteration 25610 Training loss 0.11903587728738785 Validation loss 0.0949418842792511 Accuracy 0.7205000519752502\n",
      "Iteration 25620 Training loss 0.11946050077676773 Validation loss 0.10303162783384323 Accuracy 0.6865000128746033\n",
      "Iteration 25630 Training loss 0.09146922826766968 Validation loss 0.10385487973690033 Accuracy 0.6795000433921814\n",
      "Iteration 25640 Training loss 0.12203482538461685 Validation loss 0.11903519183397293 Accuracy 0.6080000400543213\n",
      "Iteration 25650 Training loss 0.10349006205797195 Validation loss 0.09760267287492752 Accuracy 0.734000027179718\n",
      "Iteration 25660 Training loss 0.08712197840213776 Validation loss 0.09878033399581909 Accuracy 0.6935000419616699\n",
      "Iteration 25670 Training loss 0.08068162947893143 Validation loss 0.09489303827285767 Accuracy 0.7195000052452087\n",
      "Iteration 25680 Training loss 0.08625508844852448 Validation loss 0.09239627420902252 Accuracy 0.7485000491142273\n",
      "Iteration 25690 Training loss 0.12074533849954605 Validation loss 0.11076313257217407 Accuracy 0.6455000042915344\n",
      "Iteration 25700 Training loss 0.0986928716301918 Validation loss 0.09206032007932663 Accuracy 0.7390000224113464\n",
      "Iteration 25710 Training loss 0.11013122648000717 Validation loss 0.10483013093471527 Accuracy 0.6645000576972961\n",
      "Iteration 25720 Training loss 0.09846508502960205 Validation loss 0.09330195933580399 Accuracy 0.7455000281333923\n",
      "Iteration 25730 Training loss 0.09914476424455643 Validation loss 0.0958564430475235 Accuracy 0.7135000228881836\n",
      "Iteration 25740 Training loss 0.10361228883266449 Validation loss 0.10022643953561783 Accuracy 0.6950000524520874\n",
      "Iteration 25750 Training loss 0.09070993214845657 Validation loss 0.09838850796222687 Accuracy 0.7045000195503235\n",
      "Iteration 25760 Training loss 0.09561395645141602 Validation loss 0.09586266428232193 Accuracy 0.7175000309944153\n",
      "Iteration 25770 Training loss 0.09018116444349289 Validation loss 0.09453905373811722 Accuracy 0.7400000095367432\n",
      "Iteration 25780 Training loss 0.10620632022619247 Validation loss 0.10156013816595078 Accuracy 0.6970000267028809\n",
      "Iteration 25790 Training loss 0.10828949511051178 Validation loss 0.10316289216279984 Accuracy 0.6820000410079956\n",
      "Iteration 25800 Training loss 0.09602399915456772 Validation loss 0.0946359932422638 Accuracy 0.7220000624656677\n",
      "Iteration 25810 Training loss 0.09215341508388519 Validation loss 0.09403830766677856 Accuracy 0.7410000562667847\n",
      "Iteration 25820 Training loss 0.09583248943090439 Validation loss 0.10514556616544724 Accuracy 0.6785000562667847\n",
      "Iteration 25830 Training loss 0.10734313726425171 Validation loss 0.09811839461326599 Accuracy 0.7095000147819519\n",
      "Iteration 25840 Training loss 0.07995335757732391 Validation loss 0.0966089516878128 Accuracy 0.7170000076293945\n",
      "Iteration 25850 Training loss 0.08170943707227707 Validation loss 0.09413380175828934 Accuracy 0.721500039100647\n",
      "Iteration 25860 Training loss 0.09947334229946136 Validation loss 0.10745272040367126 Accuracy 0.6605000495910645\n",
      "Iteration 25870 Training loss 0.10373218357563019 Validation loss 0.10087797790765762 Accuracy 0.7010000348091125\n",
      "Iteration 25880 Training loss 0.09376323223114014 Validation loss 0.10043147206306458 Accuracy 0.6895000338554382\n",
      "Iteration 25890 Training loss 0.0906025841832161 Validation loss 0.10237562656402588 Accuracy 0.6805000305175781\n",
      "Iteration 25900 Training loss 0.09268682450056076 Validation loss 0.12172453850507736 Accuracy 0.6380000114440918\n",
      "Iteration 25910 Training loss 0.10302121192216873 Validation loss 0.1004311591386795 Accuracy 0.6945000290870667\n",
      "Iteration 25920 Training loss 0.09365003556013107 Validation loss 0.096711665391922 Accuracy 0.721500039100647\n",
      "Iteration 25930 Training loss 0.08521126210689545 Validation loss 0.09851425886154175 Accuracy 0.7080000042915344\n",
      "Iteration 25940 Training loss 0.09432616829872131 Validation loss 0.0950232446193695 Accuracy 0.7365000247955322\n",
      "Iteration 25950 Training loss 0.1045369952917099 Validation loss 0.0938718244433403 Accuracy 0.7305000424385071\n",
      "Iteration 25960 Training loss 0.09658726304769516 Validation loss 0.09450872242450714 Accuracy 0.7245000600814819\n",
      "Iteration 25970 Training loss 0.09581168740987778 Validation loss 0.09082010388374329 Accuracy 0.7415000200271606\n",
      "Iteration 25980 Training loss 0.09207552671432495 Validation loss 0.0926709845662117 Accuracy 0.7470000386238098\n",
      "Iteration 25990 Training loss 0.077127605676651 Validation loss 0.10339847207069397 Accuracy 0.6890000104904175\n",
      "Iteration 26000 Training loss 0.09805154800415039 Validation loss 0.09573999047279358 Accuracy 0.7325000166893005\n",
      "Iteration 26010 Training loss 0.11157173663377762 Validation loss 0.11111308634281158 Accuracy 0.6365000009536743\n",
      "Iteration 26020 Training loss 0.10875114053487778 Validation loss 0.10845746099948883 Accuracy 0.655500054359436\n",
      "Iteration 26030 Training loss 0.08707275241613388 Validation loss 0.09650026261806488 Accuracy 0.7145000100135803\n",
      "Iteration 26040 Training loss 0.09673462063074112 Validation loss 0.09426745772361755 Accuracy 0.7155000567436218\n",
      "Iteration 26050 Training loss 0.0879359021782875 Validation loss 0.09277252852916718 Accuracy 0.7415000200271606\n",
      "Iteration 26060 Training loss 0.09198125451803207 Validation loss 0.09056857973337173 Accuracy 0.7415000200271606\n",
      "Iteration 26070 Training loss 0.08485008031129837 Validation loss 0.09208511561155319 Accuracy 0.737500011920929\n",
      "Iteration 26080 Training loss 0.1014704704284668 Validation loss 0.10133717954158783 Accuracy 0.6890000104904175\n",
      "Iteration 26090 Training loss 0.10682215541601181 Validation loss 0.09942516684532166 Accuracy 0.7145000100135803\n",
      "Iteration 26100 Training loss 0.10840797424316406 Validation loss 0.0964585617184639 Accuracy 0.7175000309944153\n",
      "Iteration 26110 Training loss 0.09260890632867813 Validation loss 0.09421417117118835 Accuracy 0.718500018119812\n",
      "Iteration 26120 Training loss 0.08956138044595718 Validation loss 0.09276466816663742 Accuracy 0.7280000448226929\n",
      "Iteration 26130 Training loss 0.10328342020511627 Validation loss 0.09132305532693863 Accuracy 0.737500011920929\n",
      "Iteration 26140 Training loss 0.0920759066939354 Validation loss 0.09268927574157715 Accuracy 0.7350000143051147\n",
      "Iteration 26150 Training loss 0.08097076416015625 Validation loss 0.0913035050034523 Accuracy 0.7345000505447388\n",
      "Iteration 26160 Training loss 0.0987921878695488 Validation loss 0.09849852323532104 Accuracy 0.7110000252723694\n",
      "Iteration 26170 Training loss 0.0987577810883522 Validation loss 0.09597396850585938 Accuracy 0.7125000357627869\n",
      "Iteration 26180 Training loss 0.09584707766771317 Validation loss 0.09380678832530975 Accuracy 0.7325000166893005\n",
      "Iteration 26190 Training loss 0.0967460423707962 Validation loss 0.09618911147117615 Accuracy 0.718500018119812\n",
      "Iteration 26200 Training loss 0.09230934083461761 Validation loss 0.10271193832159042 Accuracy 0.6860000491142273\n",
      "Iteration 26210 Training loss 0.09053973853588104 Validation loss 0.09245578199625015 Accuracy 0.7285000085830688\n",
      "Iteration 26220 Training loss 0.08047591894865036 Validation loss 0.09420142322778702 Accuracy 0.7385000586509705\n",
      "Iteration 26230 Training loss 0.10142673552036285 Validation loss 0.09416550397872925 Accuracy 0.7195000052452087\n",
      "Iteration 26240 Training loss 0.09381961822509766 Validation loss 0.09283410012722015 Accuracy 0.7405000329017639\n",
      "Iteration 26250 Training loss 0.09186803549528122 Validation loss 0.09474348276853561 Accuracy 0.7200000286102295\n",
      "Iteration 26260 Training loss 0.08602392673492432 Validation loss 0.1006128191947937 Accuracy 0.6985000371932983\n",
      "Iteration 26270 Training loss 0.09160597622394562 Validation loss 0.09642039984464645 Accuracy 0.7095000147819519\n",
      "Iteration 26280 Training loss 0.11135220527648926 Validation loss 0.10424451529979706 Accuracy 0.6705000400543213\n",
      "Iteration 26290 Training loss 0.09285985678434372 Validation loss 0.09268897026777267 Accuracy 0.7430000305175781\n",
      "Iteration 26300 Training loss 0.1294153481721878 Validation loss 0.11986976861953735 Accuracy 0.6230000257492065\n",
      "Iteration 26310 Training loss 0.0858704000711441 Validation loss 0.09394869953393936 Accuracy 0.7350000143051147\n",
      "Iteration 26320 Training loss 0.11583086848258972 Validation loss 0.11356772482395172 Accuracy 0.6260000467300415\n",
      "Iteration 26330 Training loss 0.10324229300022125 Validation loss 0.09595634788274765 Accuracy 0.7235000133514404\n",
      "Iteration 26340 Training loss 0.0798531100153923 Validation loss 0.09269290417432785 Accuracy 0.737000048160553\n",
      "Iteration 26350 Training loss 0.10252128541469574 Validation loss 0.11473236232995987 Accuracy 0.6325000524520874\n",
      "Iteration 26360 Training loss 0.09430626779794693 Validation loss 0.10355836898088455 Accuracy 0.6920000314712524\n",
      "Iteration 26370 Training loss 0.06262756884098053 Validation loss 0.09148307889699936 Accuracy 0.7445000410079956\n",
      "Iteration 26380 Training loss 0.09116622805595398 Validation loss 0.09362238645553589 Accuracy 0.7280000448226929\n",
      "Iteration 26390 Training loss 0.07475778460502625 Validation loss 0.09175945818424225 Accuracy 0.7425000071525574\n",
      "Iteration 26400 Training loss 0.10410438477993011 Validation loss 0.09909993410110474 Accuracy 0.6950000524520874\n",
      "Iteration 26410 Training loss 0.08079199492931366 Validation loss 0.09806306660175323 Accuracy 0.70250004529953\n",
      "Iteration 26420 Training loss 0.07075084000825882 Validation loss 0.09269127249717712 Accuracy 0.737500011920929\n",
      "Iteration 26430 Training loss 0.11223497241735458 Validation loss 0.10033528506755829 Accuracy 0.7105000615119934\n",
      "Iteration 26440 Training loss 0.10681690275669098 Validation loss 0.09748612344264984 Accuracy 0.706000030040741\n",
      "Iteration 26450 Training loss 0.0885983482003212 Validation loss 0.0993846207857132 Accuracy 0.6945000290870667\n",
      "Iteration 26460 Training loss 0.1054929718375206 Validation loss 0.10549236088991165 Accuracy 0.6610000133514404\n",
      "Iteration 26470 Training loss 0.08723001182079315 Validation loss 0.10108232498168945 Accuracy 0.6875000596046448\n",
      "Iteration 26480 Training loss 0.08952756971120834 Validation loss 0.09494983404874802 Accuracy 0.7195000052452087\n",
      "Iteration 26490 Training loss 0.09476266801357269 Validation loss 0.098875492811203 Accuracy 0.7090000510215759\n",
      "Iteration 26500 Training loss 0.10247354209423065 Validation loss 0.09936434030532837 Accuracy 0.6960000395774841\n",
      "Iteration 26510 Training loss 0.0799478217959404 Validation loss 0.09550710767507553 Accuracy 0.7325000166893005\n",
      "Iteration 26520 Training loss 0.10615629702806473 Validation loss 0.09357156604528427 Accuracy 0.7390000224113464\n",
      "Iteration 26530 Training loss 0.12044324725866318 Validation loss 0.12683026492595673 Accuracy 0.6020000576972961\n",
      "Iteration 26540 Training loss 0.07771170139312744 Validation loss 0.09456344693899155 Accuracy 0.737000048160553\n",
      "Iteration 26550 Training loss 0.11108145117759705 Validation loss 0.09502455592155457 Accuracy 0.7350000143051147\n",
      "Iteration 26560 Training loss 0.09432322531938553 Validation loss 0.10300995409488678 Accuracy 0.6960000395774841\n",
      "Iteration 26570 Training loss 0.0959506556391716 Validation loss 0.09569093585014343 Accuracy 0.7160000205039978\n",
      "Iteration 26580 Training loss 0.10386708378791809 Validation loss 0.09759736806154251 Accuracy 0.7470000386238098\n",
      "Iteration 26590 Training loss 0.09698530286550522 Validation loss 0.10010058432817459 Accuracy 0.687000036239624\n",
      "Iteration 26600 Training loss 0.11752974987030029 Validation loss 0.11858341097831726 Accuracy 0.5985000133514404\n",
      "Iteration 26610 Training loss 0.08869726210832596 Validation loss 0.09240782260894775 Accuracy 0.7225000262260437\n",
      "Iteration 26620 Training loss 0.0856369212269783 Validation loss 0.09799856692552567 Accuracy 0.7245000600814819\n",
      "Iteration 26630 Training loss 0.10047660768032074 Validation loss 0.10179932415485382 Accuracy 0.6955000162124634\n",
      "Iteration 26640 Training loss 0.09237345308065414 Validation loss 0.09209558367729187 Accuracy 0.7400000095367432\n",
      "Iteration 26650 Training loss 0.12047117203474045 Validation loss 0.11670752614736557 Accuracy 0.6695000529289246\n",
      "Iteration 26660 Training loss 0.09170442074537277 Validation loss 0.09582219272851944 Accuracy 0.7405000329017639\n",
      "Iteration 26670 Training loss 0.11522521823644638 Validation loss 0.1105537861585617 Accuracy 0.643500030040741\n",
      "Iteration 26680 Training loss 0.08438493311405182 Validation loss 0.09704448282718658 Accuracy 0.7200000286102295\n",
      "Iteration 26690 Training loss 0.09351852536201477 Validation loss 0.099074587225914 Accuracy 0.7160000205039978\n",
      "Iteration 26700 Training loss 0.09119658917188644 Validation loss 0.10049568861722946 Accuracy 0.6895000338554382\n",
      "Iteration 26710 Training loss 0.0953993946313858 Validation loss 0.0915815606713295 Accuracy 0.7425000071525574\n",
      "Iteration 26720 Training loss 0.1000593900680542 Validation loss 0.10492634773254395 Accuracy 0.6815000176429749\n",
      "Iteration 26730 Training loss 0.08940185606479645 Validation loss 0.09889552742242813 Accuracy 0.718500018119812\n",
      "Iteration 26740 Training loss 0.08376426249742508 Validation loss 0.09318718314170837 Accuracy 0.7250000238418579\n",
      "Iteration 26750 Training loss 0.10752536356449127 Validation loss 0.10120348632335663 Accuracy 0.6965000033378601\n",
      "Iteration 26760 Training loss 0.09893466532230377 Validation loss 0.09985049068927765 Accuracy 0.6915000081062317\n",
      "Iteration 26770 Training loss 0.09285644441843033 Validation loss 0.09610765427350998 Accuracy 0.7265000343322754\n",
      "Iteration 26780 Training loss 0.10198427736759186 Validation loss 0.09928260743618011 Accuracy 0.6970000267028809\n",
      "Iteration 26790 Training loss 0.08681121468544006 Validation loss 0.09805610775947571 Accuracy 0.7120000123977661\n",
      "Iteration 26800 Training loss 0.09577234089374542 Validation loss 0.09383947402238846 Accuracy 0.7280000448226929\n",
      "Iteration 26810 Training loss 0.09894704818725586 Validation loss 0.09622209519147873 Accuracy 0.7110000252723694\n",
      "Iteration 26820 Training loss 0.10313044488430023 Validation loss 0.10194674879312515 Accuracy 0.6775000095367432\n",
      "Iteration 26830 Training loss 0.10430959612131119 Validation loss 0.10056672990322113 Accuracy 0.687000036239624\n",
      "Iteration 26840 Training loss 0.09417159855365753 Validation loss 0.09584905207157135 Accuracy 0.7345000505447388\n",
      "Iteration 26850 Training loss 0.1138564720749855 Validation loss 0.09434197098016739 Accuracy 0.7240000367164612\n",
      "Iteration 26860 Training loss 0.09136003255844116 Validation loss 0.09197281301021576 Accuracy 0.7360000610351562\n",
      "Iteration 26870 Training loss 0.10591889917850494 Validation loss 0.10011669993400574 Accuracy 0.6895000338554382\n",
      "Iteration 26880 Training loss 0.10018842667341232 Validation loss 0.09431300312280655 Accuracy 0.7295000553131104\n",
      "Iteration 26890 Training loss 0.08704463392496109 Validation loss 0.09167931228876114 Accuracy 0.7445000410079956\n",
      "Iteration 26900 Training loss 0.10483065992593765 Validation loss 0.10672065615653992 Accuracy 0.659000039100647\n",
      "Iteration 26910 Training loss 0.09176582098007202 Validation loss 0.09219438582658768 Accuracy 0.7425000071525574\n",
      "Iteration 26920 Training loss 0.10585550218820572 Validation loss 0.09644464403390884 Accuracy 0.7165000438690186\n",
      "Iteration 26930 Training loss 0.10472601652145386 Validation loss 0.09823616594076157 Accuracy 0.7010000348091125\n",
      "Iteration 26940 Training loss 0.1035916656255722 Validation loss 0.10037749260663986 Accuracy 0.7035000324249268\n",
      "Iteration 26950 Training loss 0.10420821607112885 Validation loss 0.11761118471622467 Accuracy 0.6465000510215759\n",
      "Iteration 26960 Training loss 0.0967230573296547 Validation loss 0.10188020765781403 Accuracy 0.6915000081062317\n",
      "Iteration 26970 Training loss 0.08382893353700638 Validation loss 0.09784163534641266 Accuracy 0.7045000195503235\n",
      "Iteration 26980 Training loss 0.0915345549583435 Validation loss 0.09462077915668488 Accuracy 0.7120000123977661\n",
      "Iteration 26990 Training loss 0.09336578100919724 Validation loss 0.09703896939754486 Accuracy 0.7035000324249268\n",
      "Iteration 27000 Training loss 0.13612571358680725 Validation loss 0.15551474690437317 Accuracy 0.5240000486373901\n",
      "Iteration 27010 Training loss 0.11470220237970352 Validation loss 0.09215486794710159 Accuracy 0.737500011920929\n",
      "Iteration 27020 Training loss 0.10866507887840271 Validation loss 0.10209416598081589 Accuracy 0.6785000562667847\n",
      "Iteration 27030 Training loss 0.11368069052696228 Validation loss 0.13313309848308563 Accuracy 0.5800000429153442\n",
      "Iteration 27040 Training loss 0.11754678189754486 Validation loss 0.1000499427318573 Accuracy 0.7150000333786011\n",
      "Iteration 27050 Training loss 0.08443062007427216 Validation loss 0.09530049562454224 Accuracy 0.7130000591278076\n",
      "Iteration 27060 Training loss 0.10269690304994583 Validation loss 0.09392714500427246 Accuracy 0.7430000305175781\n",
      "Iteration 27070 Training loss 0.09168840199708939 Validation loss 0.09461607784032822 Accuracy 0.734000027179718\n",
      "Iteration 27080 Training loss 0.10689527541399002 Validation loss 0.10022838413715363 Accuracy 0.6890000104904175\n",
      "Iteration 27090 Training loss 0.09797964245080948 Validation loss 0.1029176414012909 Accuracy 0.6765000224113464\n",
      "Iteration 27100 Training loss 0.1116546243429184 Validation loss 0.10179536789655685 Accuracy 0.7040000557899475\n",
      "Iteration 27110 Training loss 0.09621025621891022 Validation loss 0.09093824774026871 Accuracy 0.7395000457763672\n",
      "Iteration 27120 Training loss 0.08201766014099121 Validation loss 0.09252461791038513 Accuracy 0.7410000562667847\n",
      "Iteration 27130 Training loss 0.09057629108428955 Validation loss 0.10140594094991684 Accuracy 0.6925000548362732\n",
      "Iteration 27140 Training loss 0.10543599724769592 Validation loss 0.11063052713871002 Accuracy 0.6450000405311584\n",
      "Iteration 27150 Training loss 0.10276040434837341 Validation loss 0.10477001219987869 Accuracy 0.6695000529289246\n",
      "Iteration 27160 Training loss 0.11467567086219788 Validation loss 0.12050382047891617 Accuracy 0.5730000138282776\n",
      "Iteration 27170 Training loss 0.09431163221597672 Validation loss 0.09761025011539459 Accuracy 0.7200000286102295\n",
      "Iteration 27180 Training loss 0.0790102630853653 Validation loss 0.09582263976335526 Accuracy 0.7405000329017639\n",
      "Iteration 27190 Training loss 0.09257069230079651 Validation loss 0.09587688744068146 Accuracy 0.7250000238418579\n",
      "Iteration 27200 Training loss 0.09891167283058167 Validation loss 0.10515210777521133 Accuracy 0.6915000081062317\n",
      "Iteration 27210 Training loss 0.09134222567081451 Validation loss 0.09557681530714035 Accuracy 0.733500063419342\n",
      "Iteration 27220 Training loss 0.10266723483800888 Validation loss 0.10546497255563736 Accuracy 0.6620000600814819\n",
      "Iteration 27230 Training loss 0.09636498987674713 Validation loss 0.09603682160377502 Accuracy 0.733500063419342\n",
      "Iteration 27240 Training loss 0.07183035463094711 Validation loss 0.09692827612161636 Accuracy 0.7270000576972961\n",
      "Iteration 27250 Training loss 0.09830501675605774 Validation loss 0.09606337547302246 Accuracy 0.7205000519752502\n",
      "Iteration 27260 Training loss 0.06975269317626953 Validation loss 0.09426352381706238 Accuracy 0.7320000529289246\n",
      "Iteration 27270 Training loss 0.09871397912502289 Validation loss 0.09280885756015778 Accuracy 0.737000048160553\n",
      "Iteration 27280 Training loss 0.10229092091321945 Validation loss 0.10484285652637482 Accuracy 0.6730000376701355\n",
      "Iteration 27290 Training loss 0.09387946873903275 Validation loss 0.09889145940542221 Accuracy 0.7080000042915344\n",
      "Iteration 27300 Training loss 0.08870624005794525 Validation loss 0.09886380285024643 Accuracy 0.7020000219345093\n",
      "Iteration 27310 Training loss 0.11016751825809479 Validation loss 0.10101328790187836 Accuracy 0.6915000081062317\n",
      "Iteration 27320 Training loss 0.10049071162939072 Validation loss 0.10118641704320908 Accuracy 0.7015000581741333\n",
      "Iteration 27330 Training loss 0.09698613733053207 Validation loss 0.10573924332857132 Accuracy 0.6675000190734863\n",
      "Iteration 27340 Training loss 0.09202664345502853 Validation loss 0.1039220318198204 Accuracy 0.690000057220459\n",
      "Iteration 27350 Training loss 0.08745279163122177 Validation loss 0.09567180275917053 Accuracy 0.7220000624656677\n",
      "Iteration 27360 Training loss 0.08481117337942123 Validation loss 0.0921730175614357 Accuracy 0.7420000433921814\n",
      "Iteration 27370 Training loss 0.08657009899616241 Validation loss 0.0966653972864151 Accuracy 0.7145000100135803\n",
      "Iteration 27380 Training loss 0.11092060059309006 Validation loss 0.10193485021591187 Accuracy 0.6865000128746033\n",
      "Iteration 27390 Training loss 0.0954333022236824 Validation loss 0.09256522357463837 Accuracy 0.7310000061988831\n",
      "Iteration 27400 Training loss 0.09229017049074173 Validation loss 0.0987773910164833 Accuracy 0.7055000066757202\n",
      "Iteration 27410 Training loss 0.08778062462806702 Validation loss 0.0931885614991188 Accuracy 0.7420000433921814\n",
      "Iteration 27420 Training loss 0.1001739650964737 Validation loss 0.09892704337835312 Accuracy 0.7325000166893005\n",
      "Iteration 27430 Training loss 0.1047586128115654 Validation loss 0.10804129391908646 Accuracy 0.6710000038146973\n",
      "Iteration 27440 Training loss 0.08859968930482864 Validation loss 0.09850972145795822 Accuracy 0.7075000405311584\n",
      "Iteration 27450 Training loss 0.08283396810293198 Validation loss 0.09180270880460739 Accuracy 0.7325000166893005\n",
      "Iteration 27460 Training loss 0.09577575325965881 Validation loss 0.09277623891830444 Accuracy 0.7395000457763672\n",
      "Iteration 27470 Training loss 0.10347101837396622 Validation loss 0.10549779236316681 Accuracy 0.6630000472068787\n",
      "Iteration 27480 Training loss 0.09632974117994308 Validation loss 0.09644023329019547 Accuracy 0.7200000286102295\n",
      "Iteration 27490 Training loss 0.10320943593978882 Validation loss 0.096763476729393 Accuracy 0.7080000042915344\n",
      "Iteration 27500 Training loss 0.1012900248169899 Validation loss 0.10209134221076965 Accuracy 0.6935000419616699\n",
      "Iteration 27510 Training loss 0.09104102849960327 Validation loss 0.09755399078130722 Accuracy 0.706000030040741\n",
      "Iteration 27520 Training loss 0.10101670771837234 Validation loss 0.09819983690977097 Accuracy 0.7050000429153442\n",
      "Iteration 27530 Training loss 0.08986131101846695 Validation loss 0.09738205373287201 Accuracy 0.7100000381469727\n",
      "Iteration 27540 Training loss 0.07875913381576538 Validation loss 0.09283068031072617 Accuracy 0.7230000495910645\n",
      "Iteration 27550 Training loss 0.12071119248867035 Validation loss 0.14007194340229034 Accuracy 0.5490000247955322\n",
      "Iteration 27560 Training loss 0.12043464183807373 Validation loss 0.10386312007904053 Accuracy 0.6805000305175781\n",
      "Iteration 27570 Training loss 0.10808098316192627 Validation loss 0.09617471694946289 Accuracy 0.7120000123977661\n",
      "Iteration 27580 Training loss 0.10130681842565536 Validation loss 0.09666992723941803 Accuracy 0.7280000448226929\n",
      "Iteration 27590 Training loss 0.09628375619649887 Validation loss 0.10003793984651566 Accuracy 0.6935000419616699\n",
      "Iteration 27600 Training loss 0.10239091515541077 Validation loss 0.09238302707672119 Accuracy 0.7315000295639038\n",
      "Iteration 27610 Training loss 0.08427537977695465 Validation loss 0.09494500607252121 Accuracy 0.718000054359436\n",
      "Iteration 27620 Training loss 0.10612178593873978 Validation loss 0.1122000440955162 Accuracy 0.6360000371932983\n",
      "Iteration 27630 Training loss 0.09147672355175018 Validation loss 0.10473992675542831 Accuracy 0.6725000143051147\n",
      "Iteration 27640 Training loss 0.08910609781742096 Validation loss 0.09615787118673325 Accuracy 0.7150000333786011\n",
      "Iteration 27650 Training loss 0.10133035480976105 Validation loss 0.09707396477460861 Accuracy 0.718500018119812\n",
      "Iteration 27660 Training loss 0.1034129336476326 Validation loss 0.09833040833473206 Accuracy 0.7160000205039978\n",
      "Iteration 27670 Training loss 0.08908344060182571 Validation loss 0.09649545699357986 Accuracy 0.7235000133514404\n",
      "Iteration 27680 Training loss 0.08256831765174866 Validation loss 0.0960727110505104 Accuracy 0.7225000262260437\n",
      "Iteration 27690 Training loss 0.10176213830709457 Validation loss 0.0995207279920578 Accuracy 0.7115000486373901\n",
      "Iteration 27700 Training loss 0.08436857908964157 Validation loss 0.09249269217252731 Accuracy 0.737500011920929\n",
      "Iteration 27710 Training loss 0.08849375694990158 Validation loss 0.09869853407144547 Accuracy 0.7070000171661377\n",
      "Iteration 27720 Training loss 0.1137048676609993 Validation loss 0.10010095685720444 Accuracy 0.690000057220459\n",
      "Iteration 27730 Training loss 0.09583323448896408 Validation loss 0.09416510164737701 Accuracy 0.7230000495910645\n",
      "Iteration 27740 Training loss 0.10315141081809998 Validation loss 0.09865240752696991 Accuracy 0.7265000343322754\n",
      "Iteration 27750 Training loss 0.09732400625944138 Validation loss 0.09650062024593353 Accuracy 0.7285000085830688\n",
      "Iteration 27760 Training loss 0.07462038844823837 Validation loss 0.09033465385437012 Accuracy 0.7430000305175781\n",
      "Iteration 27770 Training loss 0.08071888983249664 Validation loss 0.09108665585517883 Accuracy 0.7385000586509705\n",
      "Iteration 27780 Training loss 0.09971626102924347 Validation loss 0.09649315476417542 Accuracy 0.721500039100647\n",
      "Iteration 27790 Training loss 0.09320538491010666 Validation loss 0.09087347984313965 Accuracy 0.7355000376701355\n",
      "Iteration 27800 Training loss 0.0912361666560173 Validation loss 0.09264016151428223 Accuracy 0.7315000295639038\n",
      "Iteration 27810 Training loss 0.09095995873212814 Validation loss 0.09621766954660416 Accuracy 0.7130000591278076\n",
      "Iteration 27820 Training loss 0.09348183125257492 Validation loss 0.09502410888671875 Accuracy 0.7310000061988831\n",
      "Iteration 27830 Training loss 0.09862137585878372 Validation loss 0.09738138318061829 Accuracy 0.7160000205039978\n",
      "Iteration 27840 Training loss 0.09899280220270157 Validation loss 0.09563515335321426 Accuracy 0.721500039100647\n",
      "Iteration 27850 Training loss 0.08924756944179535 Validation loss 0.10315725952386856 Accuracy 0.6785000562667847\n",
      "Iteration 27860 Training loss 0.106717050075531 Validation loss 0.10347792506217957 Accuracy 0.6725000143051147\n",
      "Iteration 27870 Training loss 0.10155057907104492 Validation loss 0.10680900514125824 Accuracy 0.6360000371932983\n",
      "Iteration 27880 Training loss 0.09981667995452881 Validation loss 0.10817810148000717 Accuracy 0.6680000424385071\n",
      "Iteration 27890 Training loss 0.08761966228485107 Validation loss 0.09903882443904877 Accuracy 0.6950000524520874\n",
      "Iteration 27900 Training loss 0.09120454639196396 Validation loss 0.10417311638593674 Accuracy 0.6710000038146973\n",
      "Iteration 27910 Training loss 0.10237544775009155 Validation loss 0.0946444645524025 Accuracy 0.7305000424385071\n",
      "Iteration 27920 Training loss 0.09375529736280441 Validation loss 0.09243091940879822 Accuracy 0.7395000457763672\n",
      "Iteration 27930 Training loss 0.08617177605628967 Validation loss 0.09122545272111893 Accuracy 0.7485000491142273\n",
      "Iteration 27940 Training loss 0.0912894532084465 Validation loss 0.09419751167297363 Accuracy 0.7260000109672546\n",
      "Iteration 27950 Training loss 0.11891409009695053 Validation loss 0.09812311828136444 Accuracy 0.7170000076293945\n",
      "Iteration 27960 Training loss 0.07487361878156662 Validation loss 0.09076649695634842 Accuracy 0.7380000352859497\n",
      "Iteration 27970 Training loss 0.117280974984169 Validation loss 0.0984267145395279 Accuracy 0.7150000333786011\n",
      "Iteration 27980 Training loss 0.08408656716346741 Validation loss 0.09430302679538727 Accuracy 0.7265000343322754\n",
      "Iteration 27990 Training loss 0.07947003841400146 Validation loss 0.09504373371601105 Accuracy 0.7195000052452087\n",
      "Iteration 28000 Training loss 0.0917707234621048 Validation loss 0.09994617104530334 Accuracy 0.703000009059906\n",
      "Iteration 28010 Training loss 0.08652965724468231 Validation loss 0.0906427875161171 Accuracy 0.7355000376701355\n",
      "Iteration 28020 Training loss 0.09761811792850494 Validation loss 0.09384085237979889 Accuracy 0.7265000343322754\n",
      "Iteration 28030 Training loss 0.08735470473766327 Validation loss 0.10126974433660507 Accuracy 0.6865000128746033\n",
      "Iteration 28040 Training loss 0.09835343807935715 Validation loss 0.09240703284740448 Accuracy 0.7270000576972961\n",
      "Iteration 28050 Training loss 0.0896315649151802 Validation loss 0.09642940014600754 Accuracy 0.706000030040741\n",
      "Iteration 28060 Training loss 0.10455179214477539 Validation loss 0.09818478673696518 Accuracy 0.7115000486373901\n",
      "Iteration 28070 Training loss 0.09371117502450943 Validation loss 0.09785842150449753 Accuracy 0.7240000367164612\n",
      "Iteration 28080 Training loss 0.07693193852901459 Validation loss 0.08987534791231155 Accuracy 0.7445000410079956\n",
      "Iteration 28090 Training loss 0.09507527947425842 Validation loss 0.10027420520782471 Accuracy 0.6915000081062317\n",
      "Iteration 28100 Training loss 0.10145629942417145 Validation loss 0.09366041421890259 Accuracy 0.7400000095367432\n",
      "Iteration 28110 Training loss 0.07801943272352219 Validation loss 0.08978380262851715 Accuracy 0.7415000200271606\n",
      "Iteration 28120 Training loss 0.07785419374704361 Validation loss 0.09113935381174088 Accuracy 0.7460000514984131\n",
      "Iteration 28130 Training loss 0.11825411766767502 Validation loss 0.09903810918331146 Accuracy 0.7200000286102295\n",
      "Iteration 28140 Training loss 0.08153679966926575 Validation loss 0.09707139432430267 Accuracy 0.7305000424385071\n",
      "Iteration 28150 Training loss 0.09927863627672195 Validation loss 0.09373633563518524 Accuracy 0.7385000586509705\n",
      "Iteration 28160 Training loss 0.09825584292411804 Validation loss 0.10448598861694336 Accuracy 0.6700000166893005\n",
      "Iteration 28170 Training loss 0.07506252080202103 Validation loss 0.09929252415895462 Accuracy 0.7020000219345093\n",
      "Iteration 28180 Training loss 0.08176835626363754 Validation loss 0.09268566966056824 Accuracy 0.7470000386238098\n",
      "Iteration 28190 Training loss 0.08397933840751648 Validation loss 0.09338940680027008 Accuracy 0.7210000157356262\n",
      "Iteration 28200 Training loss 0.09803305566310883 Validation loss 0.10400642454624176 Accuracy 0.6850000619888306\n",
      "Iteration 28210 Training loss 0.09740975499153137 Validation loss 0.09579745680093765 Accuracy 0.7235000133514404\n",
      "Iteration 28220 Training loss 0.09188464283943176 Validation loss 0.09463048726320267 Accuracy 0.7315000295639038\n",
      "Iteration 28230 Training loss 0.0943693071603775 Validation loss 0.09643008559942245 Accuracy 0.718500018119812\n",
      "Iteration 28240 Training loss 0.08261607587337494 Validation loss 0.0959218293428421 Accuracy 0.7280000448226929\n",
      "Iteration 28250 Training loss 0.08355712890625 Validation loss 0.09317786991596222 Accuracy 0.7225000262260437\n",
      "Iteration 28260 Training loss 0.08424623310565948 Validation loss 0.09635533392429352 Accuracy 0.7320000529289246\n",
      "Iteration 28270 Training loss 0.08067253977060318 Validation loss 0.09549499303102493 Accuracy 0.7080000042915344\n",
      "Iteration 28280 Training loss 0.1016976609826088 Validation loss 0.10226243734359741 Accuracy 0.6800000071525574\n",
      "Iteration 28290 Training loss 0.11324334144592285 Validation loss 0.10887471586465836 Accuracy 0.6660000085830688\n",
      "Iteration 28300 Training loss 0.10514012724161148 Validation loss 0.10360247641801834 Accuracy 0.6785000562667847\n",
      "Iteration 28310 Training loss 0.09169574826955795 Validation loss 0.0911674052476883 Accuracy 0.7425000071525574\n",
      "Iteration 28320 Training loss 0.08355388790369034 Validation loss 0.09911065548658371 Accuracy 0.6990000605583191\n",
      "Iteration 28330 Training loss 0.08898571133613586 Validation loss 0.09032098948955536 Accuracy 0.7355000376701355\n",
      "Iteration 28340 Training loss 0.10425060242414474 Validation loss 0.1000203937292099 Accuracy 0.6950000524520874\n",
      "Iteration 28350 Training loss 0.131057009100914 Validation loss 0.12455173581838608 Accuracy 0.6255000233650208\n",
      "Iteration 28360 Training loss 0.08830074220895767 Validation loss 0.11544064432382584 Accuracy 0.6200000047683716\n",
      "Iteration 28370 Training loss 0.0999274030327797 Validation loss 0.10948192328214645 Accuracy 0.612000048160553\n",
      "Iteration 28380 Training loss 0.09961426258087158 Validation loss 0.09999203681945801 Accuracy 0.70250004529953\n",
      "Iteration 28390 Training loss 0.11684346944093704 Validation loss 0.10589631646871567 Accuracy 0.6365000009536743\n",
      "Iteration 28400 Training loss 0.10649839788675308 Validation loss 0.10199248790740967 Accuracy 0.6885000467300415\n",
      "Iteration 28410 Training loss 0.08191337436437607 Validation loss 0.09594244509935379 Accuracy 0.7315000295639038\n",
      "Iteration 28420 Training loss 0.09866425395011902 Validation loss 0.10467632114887238 Accuracy 0.687000036239624\n",
      "Iteration 28430 Training loss 0.0957549586892128 Validation loss 0.09458760172128677 Accuracy 0.7305000424385071\n",
      "Iteration 28440 Training loss 0.09391313046216965 Validation loss 0.09207392483949661 Accuracy 0.7410000562667847\n",
      "Iteration 28450 Training loss 0.10196288675069809 Validation loss 0.10311216861009598 Accuracy 0.6850000619888306\n",
      "Iteration 28460 Training loss 0.08954256027936935 Validation loss 0.09259863942861557 Accuracy 0.7300000190734863\n",
      "Iteration 28470 Training loss 0.09342700988054276 Validation loss 0.09159236401319504 Accuracy 0.7330000400543213\n",
      "Iteration 28480 Training loss 0.10844104737043381 Validation loss 0.09822028875350952 Accuracy 0.7155000567436218\n",
      "Iteration 28490 Training loss 0.08563396334648132 Validation loss 0.09071988612413406 Accuracy 0.7435000538825989\n",
      "Iteration 28500 Training loss 0.09827008843421936 Validation loss 0.09436140954494476 Accuracy 0.7300000190734863\n",
      "Iteration 28510 Training loss 0.08154910057783127 Validation loss 0.10053297877311707 Accuracy 0.6990000605583191\n",
      "Iteration 28520 Training loss 0.11492253839969635 Validation loss 0.11097460240125656 Accuracy 0.6305000185966492\n",
      "Iteration 28530 Training loss 0.09274306148290634 Validation loss 0.09461900591850281 Accuracy 0.7360000610351562\n",
      "Iteration 28540 Training loss 0.08001392334699631 Validation loss 0.09018082171678543 Accuracy 0.7440000176429749\n",
      "Iteration 28550 Training loss 0.08902687579393387 Validation loss 0.09289558976888657 Accuracy 0.7325000166893005\n",
      "Iteration 28560 Training loss 0.10434354841709137 Validation loss 0.09632053971290588 Accuracy 0.7380000352859497\n",
      "Iteration 28570 Training loss 0.07952596992254257 Validation loss 0.10259929299354553 Accuracy 0.6860000491142273\n",
      "Iteration 28580 Training loss 0.12488707900047302 Validation loss 0.11306648701429367 Accuracy 0.64000004529953\n",
      "Iteration 28590 Training loss 0.0976007804274559 Validation loss 0.09358057379722595 Accuracy 0.7250000238418579\n",
      "Iteration 28600 Training loss 0.08005847781896591 Validation loss 0.09367755055427551 Accuracy 0.7300000190734863\n",
      "Iteration 28610 Training loss 0.10962841659784317 Validation loss 0.09934298694133759 Accuracy 0.7005000114440918\n",
      "Iteration 28620 Training loss 0.11145030707120895 Validation loss 0.09534706175327301 Accuracy 0.7210000157356262\n",
      "Iteration 28630 Training loss 0.09448564052581787 Validation loss 0.09304293990135193 Accuracy 0.7320000529289246\n",
      "Iteration 28640 Training loss 0.0958167091012001 Validation loss 0.10239611566066742 Accuracy 0.690000057220459\n",
      "Iteration 28650 Training loss 0.10080168396234512 Validation loss 0.09991592168807983 Accuracy 0.6925000548362732\n",
      "Iteration 28660 Training loss 0.08786052465438843 Validation loss 0.09725940227508545 Accuracy 0.7050000429153442\n",
      "Iteration 28670 Training loss 0.11500746756792068 Validation loss 0.11465020477771759 Accuracy 0.6190000176429749\n",
      "Iteration 28680 Training loss 0.0955444872379303 Validation loss 0.0961698591709137 Accuracy 0.7175000309944153\n",
      "Iteration 28690 Training loss 0.10198955237865448 Validation loss 0.09113942086696625 Accuracy 0.7445000410079956\n",
      "Iteration 28700 Training loss 0.08620774000883102 Validation loss 0.09288034588098526 Accuracy 0.7320000529289246\n",
      "Iteration 28710 Training loss 0.09918583929538727 Validation loss 0.09590796381235123 Accuracy 0.7090000510215759\n",
      "Iteration 28720 Training loss 0.08092048764228821 Validation loss 0.09560538828372955 Accuracy 0.7415000200271606\n",
      "Iteration 28730 Training loss 0.09561979025602341 Validation loss 0.09830813109874725 Accuracy 0.7085000276565552\n",
      "Iteration 28740 Training loss 0.11076784133911133 Validation loss 0.10973390936851501 Accuracy 0.6440000534057617\n",
      "Iteration 28750 Training loss 0.09690435975790024 Validation loss 0.10622427612543106 Accuracy 0.6735000610351562\n",
      "Iteration 28760 Training loss 0.09166228771209717 Validation loss 0.09688013792037964 Accuracy 0.7190000414848328\n",
      "Iteration 28770 Training loss 0.10107333958148956 Validation loss 0.09882327914237976 Accuracy 0.7075000405311584\n",
      "Iteration 28780 Training loss 0.0998825654387474 Validation loss 0.0970996618270874 Accuracy 0.7275000214576721\n",
      "Iteration 28790 Training loss 0.08729248493909836 Validation loss 0.09861712902784348 Accuracy 0.7065000534057617\n",
      "Iteration 28800 Training loss 0.11510933935642242 Validation loss 0.11115102469921112 Accuracy 0.659500002861023\n",
      "Iteration 28810 Training loss 0.09013787657022476 Validation loss 0.09479570388793945 Accuracy 0.7205000519752502\n",
      "Iteration 28820 Training loss 0.1129976212978363 Validation loss 0.1007380336523056 Accuracy 0.7050000429153442\n",
      "Iteration 28830 Training loss 0.10936924070119858 Validation loss 0.108271025121212 Accuracy 0.6660000085830688\n",
      "Iteration 28840 Training loss 0.10811168700456619 Validation loss 0.1011943593621254 Accuracy 0.6975000500679016\n",
      "Iteration 28850 Training loss 0.11132292449474335 Validation loss 0.10475239902734756 Accuracy 0.687000036239624\n",
      "Iteration 28860 Training loss 0.09351081401109695 Validation loss 0.0911499485373497 Accuracy 0.7385000586509705\n",
      "Iteration 28870 Training loss 0.11302398145198822 Validation loss 0.10418082773685455 Accuracy 0.6705000400543213\n",
      "Iteration 28880 Training loss 0.13675568997859955 Validation loss 0.11923637241125107 Accuracy 0.6565000414848328\n",
      "Iteration 28890 Training loss 0.0918462723493576 Validation loss 0.09899773448705673 Accuracy 0.7130000591278076\n",
      "Iteration 28900 Training loss 0.08758655935525894 Validation loss 0.09394461661577225 Accuracy 0.7220000624656677\n",
      "Iteration 28910 Training loss 0.09396537393331528 Validation loss 0.09371607005596161 Accuracy 0.7490000128746033\n",
      "Iteration 28920 Training loss 0.09132590889930725 Validation loss 0.09555689245462418 Accuracy 0.7120000123977661\n",
      "Iteration 28930 Training loss 0.08418145030736923 Validation loss 0.09212036430835724 Accuracy 0.7355000376701355\n",
      "Iteration 28940 Training loss 0.08147123456001282 Validation loss 0.09299799799919128 Accuracy 0.7400000095367432\n",
      "Iteration 28950 Training loss 0.10394039005041122 Validation loss 0.09853394329547882 Accuracy 0.70250004529953\n",
      "Iteration 28960 Training loss 0.10211911052465439 Validation loss 0.0957537591457367 Accuracy 0.7225000262260437\n",
      "Iteration 28970 Training loss 0.08707112073898315 Validation loss 0.09681796282529831 Accuracy 0.718000054359436\n",
      "Iteration 28980 Training loss 0.0979221984744072 Validation loss 0.09357751160860062 Accuracy 0.7225000262260437\n",
      "Iteration 28990 Training loss 0.08797682821750641 Validation loss 0.09485530853271484 Accuracy 0.7255000472068787\n",
      "Iteration 29000 Training loss 0.08514511585235596 Validation loss 0.09143809974193573 Accuracy 0.7330000400543213\n",
      "Iteration 29010 Training loss 0.08312343060970306 Validation loss 0.09570566564798355 Accuracy 0.7135000228881836\n",
      "Iteration 29020 Training loss 0.10643859952688217 Validation loss 0.11081013083457947 Accuracy 0.6450000405311584\n",
      "Iteration 29030 Training loss 0.08450555801391602 Validation loss 0.09684502333402634 Accuracy 0.7020000219345093\n",
      "Iteration 29040 Training loss 0.0798322856426239 Validation loss 0.09808064252138138 Accuracy 0.7200000286102295\n",
      "Iteration 29050 Training loss 0.09552540630102158 Validation loss 0.10152649134397507 Accuracy 0.6990000605583191\n",
      "Iteration 29060 Training loss 0.08455407619476318 Validation loss 0.1138177365064621 Accuracy 0.6515000462532043\n",
      "Iteration 29070 Training loss 0.10281313955783844 Validation loss 0.12117484956979752 Accuracy 0.593500018119812\n",
      "Iteration 29080 Training loss 0.09499411284923553 Validation loss 0.09533385187387466 Accuracy 0.7360000610351562\n",
      "Iteration 29090 Training loss 0.0956418588757515 Validation loss 0.09218475967645645 Accuracy 0.7325000166893005\n",
      "Iteration 29100 Training loss 0.0955052524805069 Validation loss 0.09607074409723282 Accuracy 0.7285000085830688\n",
      "Iteration 29110 Training loss 0.09954677522182465 Validation loss 0.0987420529127121 Accuracy 0.7235000133514404\n",
      "Iteration 29120 Training loss 0.08685767650604248 Validation loss 0.09676823019981384 Accuracy 0.7080000042915344\n",
      "Iteration 29130 Training loss 0.08942921459674835 Validation loss 0.09492295980453491 Accuracy 0.7420000433921814\n",
      "Iteration 29140 Training loss 0.0857098177075386 Validation loss 0.09614431113004684 Accuracy 0.7280000448226929\n",
      "Iteration 29150 Training loss 0.11484118551015854 Validation loss 0.12078443169593811 Accuracy 0.5995000004768372\n",
      "Iteration 29160 Training loss 0.12189143896102905 Validation loss 0.10789431631565094 Accuracy 0.6705000400543213\n",
      "Iteration 29170 Training loss 0.09208114445209503 Validation loss 0.10296415537595749 Accuracy 0.6705000400543213\n",
      "Iteration 29180 Training loss 0.0880761668086052 Validation loss 0.09355736523866653 Accuracy 0.753000020980835\n",
      "Iteration 29190 Training loss 0.10476014763116837 Validation loss 0.11305388063192368 Accuracy 0.6490000486373901\n",
      "Iteration 29200 Training loss 0.09665719419717789 Validation loss 0.09232751280069351 Accuracy 0.7285000085830688\n",
      "Iteration 29210 Training loss 0.10401061922311783 Validation loss 0.09740716964006424 Accuracy 0.7035000324249268\n",
      "Iteration 29220 Training loss 0.09866324067115784 Validation loss 0.09113534539937973 Accuracy 0.7475000619888306\n",
      "Iteration 29230 Training loss 0.09990554302930832 Validation loss 0.09506957232952118 Accuracy 0.7150000333786011\n",
      "Iteration 29240 Training loss 0.09108646214008331 Validation loss 0.10656667500734329 Accuracy 0.6530000567436218\n",
      "Iteration 29250 Training loss 0.09882121533155441 Validation loss 0.09595198929309845 Accuracy 0.7190000414848328\n",
      "Iteration 29260 Training loss 0.11645103245973587 Validation loss 0.11544832587242126 Accuracy 0.6300000548362732\n",
      "Iteration 29270 Training loss 0.09786779433488846 Validation loss 0.10092105716466904 Accuracy 0.70250004529953\n",
      "Iteration 29280 Training loss 0.08482958376407623 Validation loss 0.09859651327133179 Accuracy 0.70250004529953\n",
      "Iteration 29290 Training loss 0.08160100132226944 Validation loss 0.09164009988307953 Accuracy 0.7360000610351562\n",
      "Iteration 29300 Training loss 0.08494596183300018 Validation loss 0.093695268034935 Accuracy 0.7275000214576721\n",
      "Iteration 29310 Training loss 0.10473357141017914 Validation loss 0.09497793763875961 Accuracy 0.7115000486373901\n",
      "Iteration 29320 Training loss 0.0942147746682167 Validation loss 0.09202975779771805 Accuracy 0.7355000376701355\n",
      "Iteration 29330 Training loss 0.12216982245445251 Validation loss 0.10719101130962372 Accuracy 0.6645000576972961\n",
      "Iteration 29340 Training loss 0.08781474828720093 Validation loss 0.09525273740291595 Accuracy 0.7110000252723694\n",
      "Iteration 29350 Training loss 0.105568528175354 Validation loss 0.10245709866285324 Accuracy 0.6985000371932983\n",
      "Iteration 29360 Training loss 0.09793353080749512 Validation loss 0.09985674172639847 Accuracy 0.7000000476837158\n",
      "Iteration 29370 Training loss 0.10393047332763672 Validation loss 0.10460781306028366 Accuracy 0.6700000166893005\n",
      "Iteration 29380 Training loss 0.08896808326244354 Validation loss 0.09479497373104095 Accuracy 0.718000054359436\n",
      "Iteration 29390 Training loss 0.0880192369222641 Validation loss 0.09224853664636612 Accuracy 0.7225000262260437\n",
      "Iteration 29400 Training loss 0.08568764477968216 Validation loss 0.09006252884864807 Accuracy 0.7405000329017639\n",
      "Iteration 29410 Training loss 0.07634110003709793 Validation loss 0.0914839506149292 Accuracy 0.737000048160553\n",
      "Iteration 29420 Training loss 0.09474901854991913 Validation loss 0.10347046703100204 Accuracy 0.6975000500679016\n",
      "Iteration 29430 Training loss 0.10599541664123535 Validation loss 0.10089822858572006 Accuracy 0.7015000581741333\n",
      "Iteration 29440 Training loss 0.09098287671804428 Validation loss 0.09356985986232758 Accuracy 0.7395000457763672\n",
      "Iteration 29450 Training loss 0.07877416163682938 Validation loss 0.09481097757816315 Accuracy 0.7200000286102295\n",
      "Iteration 29460 Training loss 0.10574686527252197 Validation loss 0.10094192624092102 Accuracy 0.7020000219345093\n",
      "Iteration 29470 Training loss 0.09946504235267639 Validation loss 0.10169251263141632 Accuracy 0.6830000281333923\n",
      "Iteration 29480 Training loss 0.11107436567544937 Validation loss 0.0981224924325943 Accuracy 0.7055000066757202\n",
      "Iteration 29490 Training loss 0.18951015174388885 Validation loss 0.15438388288021088 Accuracy 0.5370000004768372\n",
      "Iteration 29500 Training loss 0.09643039852380753 Validation loss 0.10166634619235992 Accuracy 0.6985000371932983\n",
      "Iteration 29510 Training loss 0.07202809303998947 Validation loss 0.09946992993354797 Accuracy 0.6955000162124634\n",
      "Iteration 29520 Training loss 0.12090228497982025 Validation loss 0.10432072728872299 Accuracy 0.6690000295639038\n",
      "Iteration 29530 Training loss 0.08848462998867035 Validation loss 0.09322027862071991 Accuracy 0.7350000143051147\n",
      "Iteration 29540 Training loss 0.10562217235565186 Validation loss 0.09644772857427597 Accuracy 0.7090000510215759\n",
      "Iteration 29550 Training loss 0.1058093011379242 Validation loss 0.10487985610961914 Accuracy 0.6780000329017639\n",
      "Iteration 29560 Training loss 0.09028973430395126 Validation loss 0.10052656382322311 Accuracy 0.70250004529953\n",
      "Iteration 29570 Training loss 0.07943636178970337 Validation loss 0.10262168198823929 Accuracy 0.6880000233650208\n",
      "Iteration 29580 Training loss 0.0821167528629303 Validation loss 0.09462577104568481 Accuracy 0.7350000143051147\n",
      "Iteration 29590 Training loss 0.1393936276435852 Validation loss 0.1175103634595871 Accuracy 0.6305000185966492\n",
      "Iteration 29600 Training loss 0.07929294556379318 Validation loss 0.09403317421674728 Accuracy 0.7285000085830688\n",
      "Iteration 29610 Training loss 0.09119091928005219 Validation loss 0.09787363559007645 Accuracy 0.718000054359436\n",
      "Iteration 29620 Training loss 0.11952138692140579 Validation loss 0.09194764494895935 Accuracy 0.737000048160553\n",
      "Iteration 29630 Training loss 0.07025571912527084 Validation loss 0.09494876861572266 Accuracy 0.7200000286102295\n",
      "Iteration 29640 Training loss 0.07743304967880249 Validation loss 0.0960119217634201 Accuracy 0.7100000381469727\n",
      "Iteration 29650 Training loss 0.0927896499633789 Validation loss 0.09332005679607391 Accuracy 0.7300000190734863\n",
      "Iteration 29660 Training loss 0.09904176741838455 Validation loss 0.09857895225286484 Accuracy 0.7015000581741333\n",
      "Iteration 29670 Training loss 0.1009003072977066 Validation loss 0.10170775651931763 Accuracy 0.70250004529953\n",
      "Iteration 29680 Training loss 0.08749015629291534 Validation loss 0.09469101577997208 Accuracy 0.7255000472068787\n",
      "Iteration 29690 Training loss 0.10436619818210602 Validation loss 0.0955796167254448 Accuracy 0.718500018119812\n",
      "Iteration 29700 Training loss 0.09555879980325699 Validation loss 0.10400170832872391 Accuracy 0.6960000395774841\n",
      "Iteration 29710 Training loss 0.12321151793003082 Validation loss 0.1262403428554535 Accuracy 0.5335000157356262\n",
      "Iteration 29720 Training loss 0.08679907768964767 Validation loss 0.0990728959441185 Accuracy 0.7040000557899475\n",
      "Iteration 29730 Training loss 0.08532875776290894 Validation loss 0.09875708818435669 Accuracy 0.7110000252723694\n",
      "Iteration 29740 Training loss 0.08813144266605377 Validation loss 0.09085115045309067 Accuracy 0.7445000410079956\n",
      "Iteration 29750 Training loss 0.0854576975107193 Validation loss 0.09395509213209152 Accuracy 0.7115000486373901\n",
      "Iteration 29760 Training loss 0.08661998808383942 Validation loss 0.09170319139957428 Accuracy 0.734000027179718\n",
      "Iteration 29770 Training loss 0.08098449558019638 Validation loss 0.09365758299827576 Accuracy 0.7350000143051147\n",
      "Iteration 29780 Training loss 0.07993222773075104 Validation loss 0.09371805191040039 Accuracy 0.7265000343322754\n",
      "Iteration 29790 Training loss 0.08928197622299194 Validation loss 0.09775198996067047 Accuracy 0.7100000381469727\n",
      "Iteration 29800 Training loss 0.0864967629313469 Validation loss 0.09888160228729248 Accuracy 0.6920000314712524\n",
      "Iteration 29810 Training loss 0.09743682295084 Validation loss 0.09941639006137848 Accuracy 0.6975000500679016\n",
      "Iteration 29820 Training loss 0.08674182742834091 Validation loss 0.10033445805311203 Accuracy 0.690000057220459\n",
      "Iteration 29830 Training loss 0.08856689929962158 Validation loss 0.09293656796216965 Accuracy 0.7200000286102295\n",
      "Iteration 29840 Training loss 0.0959976390004158 Validation loss 0.09848163276910782 Accuracy 0.6895000338554382\n",
      "Iteration 29850 Training loss 0.0960761234164238 Validation loss 0.09775728732347488 Accuracy 0.7010000348091125\n",
      "Iteration 29860 Training loss 0.10282561928033829 Validation loss 0.10003850609064102 Accuracy 0.6865000128746033\n",
      "Iteration 29870 Training loss 0.1061798632144928 Validation loss 0.10455412417650223 Accuracy 0.6685000061988831\n",
      "Iteration 29880 Training loss 0.08067396283149719 Validation loss 0.09476258605718613 Accuracy 0.7305000424385071\n",
      "Iteration 29890 Training loss 0.08940206468105316 Validation loss 0.09339668601751328 Accuracy 0.7345000505447388\n",
      "Iteration 29900 Training loss 0.07819845527410507 Validation loss 0.09358751773834229 Accuracy 0.7355000376701355\n",
      "Iteration 29910 Training loss 0.10532613843679428 Validation loss 0.10182832926511765 Accuracy 0.687000036239624\n",
      "Iteration 29920 Training loss 0.10235719382762909 Validation loss 0.09458516538143158 Accuracy 0.7200000286102295\n",
      "Iteration 29930 Training loss 0.09313962608575821 Validation loss 0.09318498522043228 Accuracy 0.7310000061988831\n",
      "Iteration 29940 Training loss 0.09420684725046158 Validation loss 0.09262791275978088 Accuracy 0.721500039100647\n",
      "Iteration 29950 Training loss 0.08832255005836487 Validation loss 0.09217367321252823 Accuracy 0.7275000214576721\n",
      "Iteration 29960 Training loss 0.08933766931295395 Validation loss 0.09263014048337936 Accuracy 0.7355000376701355\n",
      "Iteration 29970 Training loss 0.09543488174676895 Validation loss 0.09571182727813721 Accuracy 0.7050000429153442\n",
      "Iteration 29980 Training loss 0.10546505451202393 Validation loss 0.11348741501569748 Accuracy 0.659500002861023\n",
      "Iteration 29990 Training loss 0.14345069229602814 Validation loss 0.11417608708143234 Accuracy 0.6285000443458557\n",
      "Iteration 30000 Training loss 0.09628530591726303 Validation loss 0.09966574609279633 Accuracy 0.7095000147819519\n",
      "Iteration 30010 Training loss 0.13549235463142395 Validation loss 0.12137588858604431 Accuracy 0.6025000214576721\n",
      "Iteration 30020 Training loss 0.090550996363163 Validation loss 0.09018706530332565 Accuracy 0.7450000643730164\n",
      "Iteration 30030 Training loss 0.10219773650169373 Validation loss 0.10413698107004166 Accuracy 0.6735000610351562\n",
      "Iteration 30040 Training loss 0.06731522083282471 Validation loss 0.09271150827407837 Accuracy 0.7355000376701355\n",
      "Iteration 30050 Training loss 0.08756784349679947 Validation loss 0.09372664242982864 Accuracy 0.7155000567436218\n",
      "Iteration 30060 Training loss 0.09061179310083389 Validation loss 0.09594739228487015 Accuracy 0.7330000400543213\n",
      "Iteration 30070 Training loss 0.08090097457170486 Validation loss 0.09507574141025543 Accuracy 0.7270000576972961\n",
      "Iteration 30080 Training loss 0.07960934937000275 Validation loss 0.09212953597307205 Accuracy 0.7235000133514404\n",
      "Iteration 30090 Training loss 0.10583322495222092 Validation loss 0.0983256995677948 Accuracy 0.7115000486373901\n",
      "Iteration 30100 Training loss 0.10497795045375824 Validation loss 0.09194737672805786 Accuracy 0.7405000329017639\n",
      "Iteration 30110 Training loss 0.11436230689287186 Validation loss 0.10643983632326126 Accuracy 0.6535000205039978\n",
      "Iteration 30120 Training loss 0.11301314830780029 Validation loss 0.0991111695766449 Accuracy 0.6955000162124634\n",
      "Iteration 30130 Training loss 0.09986850619316101 Validation loss 0.09539853036403656 Accuracy 0.7195000052452087\n",
      "Iteration 30140 Training loss 0.07963844388723373 Validation loss 0.09310705959796906 Accuracy 0.7405000329017639\n",
      "Iteration 30150 Training loss 0.0903460830450058 Validation loss 0.09408686310052872 Accuracy 0.7405000329017639\n",
      "Iteration 30160 Training loss 0.08299007266759872 Validation loss 0.09143348783254623 Accuracy 0.7305000424385071\n",
      "Iteration 30170 Training loss 0.09786853939294815 Validation loss 0.09419689327478409 Accuracy 0.721500039100647\n",
      "Iteration 30180 Training loss 0.08609173446893692 Validation loss 0.0985783264040947 Accuracy 0.7110000252723694\n",
      "Iteration 30190 Training loss 0.07850103080272675 Validation loss 0.09355096518993378 Accuracy 0.7265000343322754\n",
      "Iteration 30200 Training loss 0.07954508811235428 Validation loss 0.09235955029726028 Accuracy 0.7400000095367432\n",
      "Iteration 30210 Training loss 0.09686212986707687 Validation loss 0.09664659947156906 Accuracy 0.7255000472068787\n",
      "Iteration 30220 Training loss 0.09776129573583603 Validation loss 0.09031926840543747 Accuracy 0.7440000176429749\n",
      "Iteration 30230 Training loss 0.08947186172008514 Validation loss 0.10414066165685654 Accuracy 0.6855000257492065\n",
      "Iteration 30240 Training loss 0.11195598542690277 Validation loss 0.10850141197443008 Accuracy 0.6635000109672546\n",
      "Iteration 30250 Training loss 0.10021356493234634 Validation loss 0.09462794661521912 Accuracy 0.7205000519752502\n",
      "Iteration 30260 Training loss 0.11238523572683334 Validation loss 0.11339090764522552 Accuracy 0.6420000195503235\n",
      "Iteration 30270 Training loss 0.07945411652326584 Validation loss 0.09089625626802444 Accuracy 0.7325000166893005\n",
      "Iteration 30280 Training loss 0.0997033640742302 Validation loss 0.11027637124061584 Accuracy 0.6480000019073486\n",
      "Iteration 30290 Training loss 0.09789358079433441 Validation loss 0.09497229754924774 Accuracy 0.733500063419342\n",
      "Iteration 30300 Training loss 0.10095126926898956 Validation loss 0.10244046151638031 Accuracy 0.6785000562667847\n",
      "Iteration 30310 Training loss 0.09171529859304428 Validation loss 0.10060343146324158 Accuracy 0.7110000252723694\n",
      "Iteration 30320 Training loss 0.08376533538103104 Validation loss 0.09420918673276901 Accuracy 0.7325000166893005\n",
      "Iteration 30330 Training loss 0.07882953435182571 Validation loss 0.09298025071620941 Accuracy 0.718000054359436\n",
      "Iteration 30340 Training loss 0.09764329344034195 Validation loss 0.0942288264632225 Accuracy 0.7270000576972961\n",
      "Iteration 30350 Training loss 0.1053394079208374 Validation loss 0.10499007254838943 Accuracy 0.6825000047683716\n",
      "Iteration 30360 Training loss 0.10433398187160492 Validation loss 0.10684993118047714 Accuracy 0.6540000438690186\n",
      "Iteration 30370 Training loss 0.08508484065532684 Validation loss 0.09569031000137329 Accuracy 0.7095000147819519\n",
      "Iteration 30380 Training loss 0.07786693423986435 Validation loss 0.09251146018505096 Accuracy 0.7350000143051147\n",
      "Iteration 30390 Training loss 0.07591688632965088 Validation loss 0.09527403861284256 Accuracy 0.7090000510215759\n",
      "Iteration 30400 Training loss 0.0889165848493576 Validation loss 0.09486326575279236 Accuracy 0.734000027179718\n",
      "Iteration 30410 Training loss 0.08977241069078445 Validation loss 0.09630446135997772 Accuracy 0.7090000510215759\n",
      "Iteration 30420 Training loss 0.12120310217142105 Validation loss 0.11892610788345337 Accuracy 0.6450000405311584\n",
      "Iteration 30430 Training loss 0.11372201889753342 Validation loss 0.09172089397907257 Accuracy 0.7330000400543213\n",
      "Iteration 30440 Training loss 0.09891490638256073 Validation loss 0.09822911769151688 Accuracy 0.7205000519752502\n",
      "Iteration 30450 Training loss 0.09449933469295502 Validation loss 0.09582650661468506 Accuracy 0.7210000157356262\n",
      "Iteration 30460 Training loss 0.10068222135305405 Validation loss 0.09992019832134247 Accuracy 0.6925000548362732\n",
      "Iteration 30470 Training loss 0.10642795264720917 Validation loss 0.10496478527784348 Accuracy 0.6725000143051147\n",
      "Iteration 30480 Training loss 0.07884036749601364 Validation loss 0.08942274004220963 Accuracy 0.7470000386238098\n",
      "Iteration 30490 Training loss 0.09922631829977036 Validation loss 0.0920185074210167 Accuracy 0.7275000214576721\n",
      "Iteration 30500 Training loss 0.09384357184171677 Validation loss 0.09331903606653214 Accuracy 0.718000054359436\n",
      "Iteration 30510 Training loss 0.09430640935897827 Validation loss 0.09263136982917786 Accuracy 0.7310000061988831\n",
      "Iteration 30520 Training loss 0.0866270437836647 Validation loss 0.0953153744339943 Accuracy 0.718000054359436\n",
      "Iteration 30530 Training loss 0.09564097970724106 Validation loss 0.09342903643846512 Accuracy 0.7205000519752502\n",
      "Iteration 30540 Training loss 0.08775626868009567 Validation loss 0.09257487207651138 Accuracy 0.7290000319480896\n",
      "Iteration 30550 Training loss 0.10313988476991653 Validation loss 0.10017498582601547 Accuracy 0.6995000243186951\n",
      "Iteration 30560 Training loss 0.11179869621992111 Validation loss 0.1166921928524971 Accuracy 0.6165000200271606\n",
      "Iteration 30570 Training loss 0.08439242839813232 Validation loss 0.09818849712610245 Accuracy 0.7010000348091125\n",
      "Iteration 30580 Training loss 0.09989739954471588 Validation loss 0.0997198224067688 Accuracy 0.7120000123977661\n",
      "Iteration 30590 Training loss 0.08876793831586838 Validation loss 0.09207978844642639 Accuracy 0.7285000085830688\n",
      "Iteration 30600 Training loss 0.108223095536232 Validation loss 0.10118231177330017 Accuracy 0.6910000443458557\n",
      "Iteration 30610 Training loss 0.10301690548658371 Validation loss 0.09198591858148575 Accuracy 0.7430000305175781\n",
      "Iteration 30620 Training loss 0.10463859140872955 Validation loss 0.10100819915533066 Accuracy 0.6895000338554382\n",
      "Iteration 30630 Training loss 0.09353647381067276 Validation loss 0.09227412939071655 Accuracy 0.7310000061988831\n",
      "Iteration 30640 Training loss 0.13061434030532837 Validation loss 0.11913914978504181 Accuracy 0.6345000267028809\n",
      "Iteration 30650 Training loss 0.07693418115377426 Validation loss 0.09301227331161499 Accuracy 0.7280000448226929\n",
      "Iteration 30660 Training loss 0.08644363284111023 Validation loss 0.09031005948781967 Accuracy 0.7445000410079956\n",
      "Iteration 30670 Training loss 0.08311603218317032 Validation loss 0.09793037921190262 Accuracy 0.7255000472068787\n",
      "Iteration 30680 Training loss 0.1040714904665947 Validation loss 0.10546327382326126 Accuracy 0.6675000190734863\n",
      "Iteration 30690 Training loss 0.09900204092264175 Validation loss 0.098433718085289 Accuracy 0.6945000290870667\n",
      "Iteration 30700 Training loss 0.0885566845536232 Validation loss 0.09339459985494614 Accuracy 0.7320000529289246\n",
      "Iteration 30710 Training loss 0.08244207501411438 Validation loss 0.0949544757604599 Accuracy 0.7210000157356262\n",
      "Iteration 30720 Training loss 0.09696313738822937 Validation loss 0.09677761048078537 Accuracy 0.703000009059906\n",
      "Iteration 30730 Training loss 0.09849020093679428 Validation loss 0.09980469197034836 Accuracy 0.6995000243186951\n",
      "Iteration 30740 Training loss 0.08680786192417145 Validation loss 0.0917377844452858 Accuracy 0.7410000562667847\n",
      "Iteration 30750 Training loss 0.08348727226257324 Validation loss 0.09275075048208237 Accuracy 0.721500039100647\n",
      "Iteration 30760 Training loss 0.11679629981517792 Validation loss 0.11175105720758438 Accuracy 0.6185000538825989\n",
      "Iteration 30770 Training loss 0.10288207978010178 Validation loss 0.10251455008983612 Accuracy 0.6850000619888306\n",
      "Iteration 30780 Training loss 0.08985452353954315 Validation loss 0.09550713002681732 Accuracy 0.7305000424385071\n",
      "Iteration 30790 Training loss 0.08623705804347992 Validation loss 0.09331008046865463 Accuracy 0.7170000076293945\n",
      "Iteration 30800 Training loss 0.10431129485368729 Validation loss 0.09697587788105011 Accuracy 0.6945000290870667\n",
      "Iteration 30810 Training loss 0.08319619297981262 Validation loss 0.09227555245161057 Accuracy 0.7290000319480896\n",
      "Iteration 30820 Training loss 0.09171449393033981 Validation loss 0.09238555282354355 Accuracy 0.7295000553131104\n",
      "Iteration 30830 Training loss 0.08307191729545593 Validation loss 0.09937132894992828 Accuracy 0.6980000138282776\n",
      "Iteration 30840 Training loss 0.1146189495921135 Validation loss 0.09876297414302826 Accuracy 0.6970000267028809\n",
      "Iteration 30850 Training loss 0.09237015247344971 Validation loss 0.09394809603691101 Accuracy 0.7155000567436218\n",
      "Iteration 30860 Training loss 0.0818297490477562 Validation loss 0.09172634780406952 Accuracy 0.7440000176429749\n",
      "Iteration 30870 Training loss 0.09511632472276688 Validation loss 0.10124700516462326 Accuracy 0.6855000257492065\n",
      "Iteration 30880 Training loss 0.10734542459249496 Validation loss 0.08995947986841202 Accuracy 0.7425000071525574\n",
      "Iteration 30890 Training loss 0.15480804443359375 Validation loss 0.13875806331634521 Accuracy 0.5445000529289246\n",
      "Iteration 30900 Training loss 0.0780499130487442 Validation loss 0.09076077491044998 Accuracy 0.7395000457763672\n",
      "Iteration 30910 Training loss 0.07947387546300888 Validation loss 0.09150470048189163 Accuracy 0.7450000643730164\n",
      "Iteration 30920 Training loss 0.09744216501712799 Validation loss 0.09250328689813614 Accuracy 0.737000048160553\n",
      "Iteration 30930 Training loss 0.09955579787492752 Validation loss 0.09631229937076569 Accuracy 0.7105000615119934\n",
      "Iteration 30940 Training loss 0.10331897437572479 Validation loss 0.09850054979324341 Accuracy 0.7085000276565552\n",
      "Iteration 30950 Training loss 0.08486046642065048 Validation loss 0.08966145664453506 Accuracy 0.7480000257492065\n",
      "Iteration 30960 Training loss 0.10936111211776733 Validation loss 0.10715730488300323 Accuracy 0.6615000367164612\n",
      "Iteration 30970 Training loss 0.10075216740369797 Validation loss 0.1015419214963913 Accuracy 0.6950000524520874\n",
      "Iteration 30980 Training loss 0.09838810563087463 Validation loss 0.0937589630484581 Accuracy 0.7245000600814819\n",
      "Iteration 30990 Training loss 0.0945674329996109 Validation loss 0.09090045839548111 Accuracy 0.7410000562667847\n",
      "Iteration 31000 Training loss 0.0974314883351326 Validation loss 0.08969444781541824 Accuracy 0.7415000200271606\n",
      "Iteration 31010 Training loss 0.12350772321224213 Validation loss 0.11509047448635101 Accuracy 0.6395000219345093\n",
      "Iteration 31020 Training loss 0.09265349060297012 Validation loss 0.09544569253921509 Accuracy 0.7200000286102295\n",
      "Iteration 31030 Training loss 0.10938645899295807 Validation loss 0.1212829202413559 Accuracy 0.6195000410079956\n",
      "Iteration 31040 Training loss 0.09843818098306656 Validation loss 0.09295986592769623 Accuracy 0.7280000448226929\n",
      "Iteration 31050 Training loss 0.09692015498876572 Validation loss 0.10748329758644104 Accuracy 0.6795000433921814\n",
      "Iteration 31060 Training loss 0.07873685657978058 Validation loss 0.09079421311616898 Accuracy 0.7450000643730164\n",
      "Iteration 31070 Training loss 0.07219722867012024 Validation loss 0.08983679860830307 Accuracy 0.7395000457763672\n",
      "Iteration 31080 Training loss 0.1085314080119133 Validation loss 0.0956202894449234 Accuracy 0.7205000519752502\n",
      "Iteration 31090 Training loss 0.08256613463163376 Validation loss 0.09590793401002884 Accuracy 0.7255000472068787\n",
      "Iteration 31100 Training loss 0.09945935755968094 Validation loss 0.09406758844852448 Accuracy 0.7220000624656677\n",
      "Iteration 31110 Training loss 0.09822596609592438 Validation loss 0.10352061688899994 Accuracy 0.6735000610351562\n",
      "Iteration 31120 Training loss 0.08462455868721008 Validation loss 0.09159920364618301 Accuracy 0.7505000233650208\n",
      "Iteration 31130 Training loss 0.09077849984169006 Validation loss 0.1028381809592247 Accuracy 0.6695000529289246\n",
      "Iteration 31140 Training loss 0.0837874710559845 Validation loss 0.09231112897396088 Accuracy 0.7380000352859497\n",
      "Iteration 31150 Training loss 0.09075869619846344 Validation loss 0.09827016294002533 Accuracy 0.70250004529953\n",
      "Iteration 31160 Training loss 0.09981857240200043 Validation loss 0.0932476669549942 Accuracy 0.7275000214576721\n",
      "Iteration 31170 Training loss 0.09825018793344498 Validation loss 0.10348070412874222 Accuracy 0.6800000071525574\n",
      "Iteration 31180 Training loss 0.10706020146608353 Validation loss 0.10098090022802353 Accuracy 0.690000057220459\n",
      "Iteration 31190 Training loss 0.0804610550403595 Validation loss 0.09604594856500626 Accuracy 0.7160000205039978\n",
      "Iteration 31200 Training loss 0.09591836482286453 Validation loss 0.09189824759960175 Accuracy 0.7450000643730164\n",
      "Iteration 31210 Training loss 0.06733769923448563 Validation loss 0.09064392745494843 Accuracy 0.7420000433921814\n",
      "Iteration 31220 Training loss 0.10616688430309296 Validation loss 0.10096831619739532 Accuracy 0.6875000596046448\n",
      "Iteration 31230 Training loss 0.0964394360780716 Validation loss 0.09958766400814056 Accuracy 0.7080000042915344\n",
      "Iteration 31240 Training loss 0.07233863323926926 Validation loss 0.09046708792448044 Accuracy 0.7430000305175781\n",
      "Iteration 31250 Training loss 0.07225582748651505 Validation loss 0.09147360920906067 Accuracy 0.7190000414848328\n",
      "Iteration 31260 Training loss 0.10462561249732971 Validation loss 0.09931017458438873 Accuracy 0.7065000534057617\n",
      "Iteration 31270 Training loss 0.09192494302988052 Validation loss 0.09383822977542877 Accuracy 0.734000027179718\n",
      "Iteration 31280 Training loss 0.09699654579162598 Validation loss 0.08793767541646957 Accuracy 0.749500036239624\n",
      "Iteration 31290 Training loss 0.0928322970867157 Validation loss 0.09072591364383698 Accuracy 0.7470000386238098\n",
      "Iteration 31300 Training loss 0.10561450570821762 Validation loss 0.0994829460978508 Accuracy 0.7115000486373901\n",
      "Iteration 31310 Training loss 0.09128155559301376 Validation loss 0.09856382012367249 Accuracy 0.721500039100647\n",
      "Iteration 31320 Training loss 0.09915018081665039 Validation loss 0.09096783399581909 Accuracy 0.7440000176429749\n",
      "Iteration 31330 Training loss 0.07054674625396729 Validation loss 0.09215238690376282 Accuracy 0.7315000295639038\n",
      "Iteration 31340 Training loss 0.06848733127117157 Validation loss 0.08791536092758179 Accuracy 0.7510000467300415\n",
      "Iteration 31350 Training loss 0.08976991474628448 Validation loss 0.1027136966586113 Accuracy 0.6815000176429749\n",
      "Iteration 31360 Training loss 0.10434457659721375 Validation loss 0.10933233797550201 Accuracy 0.6520000100135803\n",
      "Iteration 31370 Training loss 0.10182416439056396 Validation loss 0.10477505624294281 Accuracy 0.6650000214576721\n",
      "Iteration 31380 Training loss 0.15335319936275482 Validation loss 0.157157301902771 Accuracy 0.5270000100135803\n",
      "Iteration 31390 Training loss 0.0957789197564125 Validation loss 0.0953679084777832 Accuracy 0.7355000376701355\n",
      "Iteration 31400 Training loss 0.0872299075126648 Validation loss 0.09274224936962128 Accuracy 0.7360000610351562\n",
      "Iteration 31410 Training loss 0.09138724952936172 Validation loss 0.09220331907272339 Accuracy 0.7390000224113464\n",
      "Iteration 31420 Training loss 0.09239087998867035 Validation loss 0.09616947174072266 Accuracy 0.7240000367164612\n",
      "Iteration 31430 Training loss 0.09757214039564133 Validation loss 0.10046195238828659 Accuracy 0.690000057220459\n",
      "Iteration 31440 Training loss 0.07903166860342026 Validation loss 0.09837426990270615 Accuracy 0.6965000033378601\n",
      "Iteration 31450 Training loss 0.10094814002513885 Validation loss 0.09949427098035812 Accuracy 0.7125000357627869\n",
      "Iteration 31460 Training loss 0.08910145610570908 Validation loss 0.09188615530729294 Accuracy 0.7385000586509705\n",
      "Iteration 31470 Training loss 0.09700652956962585 Validation loss 0.10070684552192688 Accuracy 0.6980000138282776\n",
      "Iteration 31480 Training loss 0.10042325407266617 Validation loss 0.10745393484830856 Accuracy 0.6460000276565552\n",
      "Iteration 31490 Training loss 0.10006038099527359 Validation loss 0.1037762239575386 Accuracy 0.6880000233650208\n",
      "Iteration 31500 Training loss 0.0981721505522728 Validation loss 0.10062289237976074 Accuracy 0.6910000443458557\n",
      "Iteration 31510 Training loss 0.07945900410413742 Validation loss 0.09160800278186798 Accuracy 0.7480000257492065\n",
      "Iteration 31520 Training loss 0.1424642950296402 Validation loss 0.14953897893428802 Accuracy 0.5400000214576721\n",
      "Iteration 31530 Training loss 0.08792615681886673 Validation loss 0.0937047004699707 Accuracy 0.7235000133514404\n",
      "Iteration 31540 Training loss 0.09885244071483612 Validation loss 0.10157599300146103 Accuracy 0.6910000443458557\n",
      "Iteration 31550 Training loss 0.0858679860830307 Validation loss 0.09427991509437561 Accuracy 0.718500018119812\n",
      "Iteration 31560 Training loss 0.07765933126211166 Validation loss 0.09011054784059525 Accuracy 0.7345000505447388\n",
      "Iteration 31570 Training loss 0.09052644670009613 Validation loss 0.09720941632986069 Accuracy 0.7090000510215759\n",
      "Iteration 31580 Training loss 0.09917354583740234 Validation loss 0.09269312024116516 Accuracy 0.7265000343322754\n",
      "Iteration 31590 Training loss 0.0888628289103508 Validation loss 0.09538304805755615 Accuracy 0.7230000495910645\n",
      "Iteration 31600 Training loss 0.07992531359195709 Validation loss 0.09637856483459473 Accuracy 0.7095000147819519\n",
      "Iteration 31610 Training loss 0.08344081789255142 Validation loss 0.09147065877914429 Accuracy 0.7360000610351562\n",
      "Iteration 31620 Training loss 0.09879863262176514 Validation loss 0.09255322813987732 Accuracy 0.7355000376701355\n",
      "Iteration 31630 Training loss 0.1092757135629654 Validation loss 0.09924836456775665 Accuracy 0.6985000371932983\n",
      "Iteration 31640 Training loss 0.08667854964733124 Validation loss 0.09285904467105865 Accuracy 0.7305000424385071\n",
      "Iteration 31650 Training loss 0.08014842867851257 Validation loss 0.0914093628525734 Accuracy 0.7385000586509705\n",
      "Iteration 31660 Training loss 0.08618102967739105 Validation loss 0.09378945082426071 Accuracy 0.7310000061988831\n",
      "Iteration 31670 Training loss 0.09548074752092361 Validation loss 0.10015613585710526 Accuracy 0.6925000548362732\n",
      "Iteration 31680 Training loss 0.09639870375394821 Validation loss 0.09608098119497299 Accuracy 0.7130000591278076\n",
      "Iteration 31690 Training loss 0.09425767511129379 Validation loss 0.0946180522441864 Accuracy 0.733500063419342\n",
      "Iteration 31700 Training loss 0.0853317603468895 Validation loss 0.09857194125652313 Accuracy 0.7015000581741333\n",
      "Iteration 31710 Training loss 0.08091747015714645 Validation loss 0.09446558356285095 Accuracy 0.7225000262260437\n",
      "Iteration 31720 Training loss 0.09643031656742096 Validation loss 0.09812772274017334 Accuracy 0.7040000557899475\n",
      "Iteration 31730 Training loss 0.11516793817281723 Validation loss 0.10678675025701523 Accuracy 0.6730000376701355\n",
      "Iteration 31740 Training loss 0.08892793208360672 Validation loss 0.0943322628736496 Accuracy 0.7195000052452087\n",
      "Iteration 31750 Training loss 0.1012050211429596 Validation loss 0.10292720794677734 Accuracy 0.6835000514984131\n",
      "Iteration 31760 Training loss 0.11458102613687515 Validation loss 0.11013372987508774 Accuracy 0.6605000495910645\n",
      "Iteration 31770 Training loss 0.08716464787721634 Validation loss 0.10427998006343842 Accuracy 0.6775000095367432\n",
      "Iteration 31780 Training loss 0.09266889095306396 Validation loss 0.09415993094444275 Accuracy 0.7135000228881836\n",
      "Iteration 31790 Training loss 0.08628737181425095 Validation loss 0.08886931836605072 Accuracy 0.7365000247955322\n",
      "Iteration 31800 Training loss 0.09000078588724136 Validation loss 0.09359677881002426 Accuracy 0.7315000295639038\n",
      "Iteration 31810 Training loss 0.10793052613735199 Validation loss 0.10224572569131851 Accuracy 0.6840000152587891\n",
      "Iteration 31820 Training loss 0.10521745681762695 Validation loss 0.09271194040775299 Accuracy 0.7330000400543213\n",
      "Iteration 31830 Training loss 0.10164199769496918 Validation loss 0.09481567144393921 Accuracy 0.7125000357627869\n",
      "Iteration 31840 Training loss 0.10434383898973465 Validation loss 0.09349164366722107 Accuracy 0.7470000386238098\n",
      "Iteration 31850 Training loss 0.08687218278646469 Validation loss 0.1061306744813919 Accuracy 0.6680000424385071\n",
      "Iteration 31860 Training loss 0.08894815295934677 Validation loss 0.09017091244459152 Accuracy 0.7360000610351562\n",
      "Iteration 31870 Training loss 0.09000769257545471 Validation loss 0.0978289470076561 Accuracy 0.703000009059906\n",
      "Iteration 31880 Training loss 0.09315028041601181 Validation loss 0.09535456448793411 Accuracy 0.7150000333786011\n",
      "Iteration 31890 Training loss 0.08235017955303192 Validation loss 0.09156572073698044 Accuracy 0.7295000553131104\n",
      "Iteration 31900 Training loss 0.12372513860464096 Validation loss 0.11399204283952713 Accuracy 0.6440000534057617\n",
      "Iteration 31910 Training loss 0.09040973335504532 Validation loss 0.08855927735567093 Accuracy 0.7465000152587891\n",
      "Iteration 31920 Training loss 0.10090085864067078 Validation loss 0.09751810878515244 Accuracy 0.718500018119812\n",
      "Iteration 31930 Training loss 0.08938156813383102 Validation loss 0.09543686360120773 Accuracy 0.7150000333786011\n",
      "Iteration 31940 Training loss 0.0903337299823761 Validation loss 0.0926148071885109 Accuracy 0.7435000538825989\n",
      "Iteration 31950 Training loss 0.09445714950561523 Validation loss 0.1024659276008606 Accuracy 0.6850000619888306\n",
      "Iteration 31960 Training loss 0.08005192875862122 Validation loss 0.09216095507144928 Accuracy 0.7280000448226929\n",
      "Iteration 31970 Training loss 0.09059068560600281 Validation loss 0.10271042585372925 Accuracy 0.6895000338554382\n",
      "Iteration 31980 Training loss 0.07538449764251709 Validation loss 0.09334423393011093 Accuracy 0.7300000190734863\n",
      "Iteration 31990 Training loss 0.10667573660612106 Validation loss 0.10090815275907516 Accuracy 0.6980000138282776\n",
      "Iteration 32000 Training loss 0.09780585765838623 Validation loss 0.10938028246164322 Accuracy 0.6695000529289246\n",
      "Iteration 32010 Training loss 0.09273816645145416 Validation loss 0.0898471251130104 Accuracy 0.7440000176429749\n",
      "Iteration 32020 Training loss 0.08186429738998413 Validation loss 0.08895832300186157 Accuracy 0.737500011920929\n",
      "Iteration 32030 Training loss 0.0974893718957901 Validation loss 0.09324298053979874 Accuracy 0.7140000462532043\n",
      "Iteration 32040 Training loss 0.17652738094329834 Validation loss 0.15604983270168304 Accuracy 0.5570000410079956\n",
      "Iteration 32050 Training loss 0.1017114520072937 Validation loss 0.0938468649983406 Accuracy 0.7295000553131104\n",
      "Iteration 32060 Training loss 0.09746287018060684 Validation loss 0.08810052275657654 Accuracy 0.7445000410079956\n",
      "Iteration 32070 Training loss 0.09972476959228516 Validation loss 0.09445325285196304 Accuracy 0.734000027179718\n",
      "Iteration 32080 Training loss 0.09966881573200226 Validation loss 0.09568564593791962 Accuracy 0.7265000343322754\n",
      "Iteration 32090 Training loss 0.13485132157802582 Validation loss 0.15159404277801514 Accuracy 0.5470000505447388\n",
      "Iteration 32100 Training loss 0.09899766743183136 Validation loss 0.09220666438341141 Accuracy 0.7285000085830688\n",
      "Iteration 32110 Training loss 0.09083619713783264 Validation loss 0.08861785382032394 Accuracy 0.753000020980835\n",
      "Iteration 32120 Training loss 0.1022578626871109 Validation loss 0.09851463884115219 Accuracy 0.6985000371932983\n",
      "Iteration 32130 Training loss 0.09577403962612152 Validation loss 0.09895077347755432 Accuracy 0.6930000185966492\n",
      "Iteration 32140 Training loss 0.09285072237253189 Validation loss 0.09443937242031097 Accuracy 0.7270000576972961\n",
      "Iteration 32150 Training loss 0.10223354399204254 Validation loss 0.09334830939769745 Accuracy 0.7170000076293945\n",
      "Iteration 32160 Training loss 0.10184420645236969 Validation loss 0.10708468407392502 Accuracy 0.6785000562667847\n",
      "Iteration 32170 Training loss 0.09429984539747238 Validation loss 0.09680620580911636 Accuracy 0.7140000462532043\n",
      "Iteration 32180 Training loss 0.08790483325719833 Validation loss 0.09851086884737015 Accuracy 0.7050000429153442\n",
      "Iteration 32190 Training loss 0.08081306517124176 Validation loss 0.09125693142414093 Accuracy 0.7305000424385071\n",
      "Iteration 32200 Training loss 0.09247980266809464 Validation loss 0.09080707281827927 Accuracy 0.7390000224113464\n",
      "Iteration 32210 Training loss 0.09808415919542313 Validation loss 0.11540495604276657 Accuracy 0.6240000128746033\n",
      "Iteration 32220 Training loss 0.10445264726877213 Validation loss 0.09353053569793701 Accuracy 0.7310000061988831\n",
      "Iteration 32230 Training loss 0.07655034959316254 Validation loss 0.0905020609498024 Accuracy 0.721500039100647\n",
      "Iteration 32240 Training loss 0.0841442346572876 Validation loss 0.09169723838567734 Accuracy 0.7395000457763672\n",
      "Iteration 32250 Training loss 0.07806334644556046 Validation loss 0.08912790566682816 Accuracy 0.7420000433921814\n",
      "Iteration 32260 Training loss 0.10501541197299957 Validation loss 0.11181159317493439 Accuracy 0.6575000286102295\n",
      "Iteration 32270 Training loss 0.09646961092948914 Validation loss 0.0976816788315773 Accuracy 0.7080000042915344\n",
      "Iteration 32280 Training loss 0.08957217633724213 Validation loss 0.0935014858841896 Accuracy 0.7360000610351562\n",
      "Iteration 32290 Training loss 0.0682162344455719 Validation loss 0.08895834535360336 Accuracy 0.7350000143051147\n",
      "Iteration 32300 Training loss 0.07346230000257492 Validation loss 0.08892287313938141 Accuracy 0.7410000562667847\n",
      "Iteration 32310 Training loss 0.12038581818342209 Validation loss 0.11015234887599945 Accuracy 0.6675000190734863\n",
      "Iteration 32320 Training loss 0.08256570994853973 Validation loss 0.08925186842679977 Accuracy 0.7485000491142273\n",
      "Iteration 32330 Training loss 0.09570682048797607 Validation loss 0.09252295643091202 Accuracy 0.7330000400543213\n",
      "Iteration 32340 Training loss 0.1119660958647728 Validation loss 0.11239629983901978 Accuracy 0.6460000276565552\n",
      "Iteration 32350 Training loss 0.10641181468963623 Validation loss 0.0973936915397644 Accuracy 0.7000000476837158\n",
      "Iteration 32360 Training loss 0.0982024148106575 Validation loss 0.09048596769571304 Accuracy 0.7315000295639038\n",
      "Iteration 32370 Training loss 0.07432550936937332 Validation loss 0.08973339945077896 Accuracy 0.733500063419342\n",
      "Iteration 32380 Training loss 0.09717452526092529 Validation loss 0.09399016201496124 Accuracy 0.7225000262260437\n",
      "Iteration 32390 Training loss 0.09868351370096207 Validation loss 0.09973269701004028 Accuracy 0.7095000147819519\n",
      "Iteration 32400 Training loss 0.08470853418111801 Validation loss 0.0979677066206932 Accuracy 0.7140000462532043\n",
      "Iteration 32410 Training loss 0.08374875038862228 Validation loss 0.09477642178535461 Accuracy 0.733500063419342\n",
      "Iteration 32420 Training loss 0.10582910478115082 Validation loss 0.10157034546136856 Accuracy 0.6940000057220459\n",
      "Iteration 32430 Training loss 0.09472249448299408 Validation loss 0.0977327898144722 Accuracy 0.70250004529953\n",
      "Iteration 32440 Training loss 0.07617683708667755 Validation loss 0.09519469738006592 Accuracy 0.7110000252723694\n",
      "Iteration 32450 Training loss 0.0943112000823021 Validation loss 0.10258448123931885 Accuracy 0.6790000200271606\n",
      "Iteration 32460 Training loss 0.11104033887386322 Validation loss 0.11102667450904846 Accuracy 0.6355000138282776\n",
      "Iteration 32470 Training loss 0.0919237956404686 Validation loss 0.09743915498256683 Accuracy 0.7145000100135803\n",
      "Iteration 32480 Training loss 0.08807514607906342 Validation loss 0.10006054490804672 Accuracy 0.6945000290870667\n",
      "Iteration 32490 Training loss 0.07511384040117264 Validation loss 0.08954060077667236 Accuracy 0.7365000247955322\n",
      "Iteration 32500 Training loss 0.08203199505805969 Validation loss 0.0887095257639885 Accuracy 0.7425000071525574\n",
      "Iteration 32510 Training loss 0.08823484927415848 Validation loss 0.1039041057229042 Accuracy 0.6920000314712524\n",
      "Iteration 32520 Training loss 0.10671411454677582 Validation loss 0.10294631123542786 Accuracy 0.687000036239624\n",
      "Iteration 32530 Training loss 0.10055143386125565 Validation loss 0.09141215682029724 Accuracy 0.7380000352859497\n",
      "Iteration 32540 Training loss 0.10023483633995056 Validation loss 0.09813637286424637 Accuracy 0.6990000605583191\n",
      "Iteration 32550 Training loss 0.10365491360425949 Validation loss 0.10460484772920609 Accuracy 0.6685000061988831\n",
      "Iteration 32560 Training loss 0.07891158014535904 Validation loss 0.09250228106975555 Accuracy 0.7315000295639038\n",
      "Iteration 32570 Training loss 0.08475545793771744 Validation loss 0.09236390143632889 Accuracy 0.7270000576972961\n",
      "Iteration 32580 Training loss 0.09892287850379944 Validation loss 0.0950792208313942 Accuracy 0.7205000519752502\n",
      "Iteration 32590 Training loss 0.08423339575529099 Validation loss 0.10050617903470993 Accuracy 0.6805000305175781\n",
      "Iteration 32600 Training loss 0.10105825215578079 Validation loss 0.10128310322761536 Accuracy 0.7000000476837158\n",
      "Iteration 32610 Training loss 0.09846071153879166 Validation loss 0.09486069530248642 Accuracy 0.7220000624656677\n",
      "Iteration 32620 Training loss 0.09168508648872375 Validation loss 0.09545756876468658 Accuracy 0.7160000205039978\n",
      "Iteration 32630 Training loss 0.09160611778497696 Validation loss 0.09156224131584167 Accuracy 0.7320000529289246\n",
      "Iteration 32640 Training loss 0.08855384588241577 Validation loss 0.09253300726413727 Accuracy 0.7240000367164612\n",
      "Iteration 32650 Training loss 0.08824508637189865 Validation loss 0.0977974683046341 Accuracy 0.7010000348091125\n",
      "Iteration 32660 Training loss 0.08302723616361618 Validation loss 0.09678549319505692 Accuracy 0.7110000252723694\n",
      "Iteration 32670 Training loss 0.08426986634731293 Validation loss 0.09719458967447281 Accuracy 0.6955000162124634\n",
      "Iteration 32680 Training loss 0.11417368799448013 Validation loss 0.08916561305522919 Accuracy 0.7400000095367432\n",
      "Iteration 32690 Training loss 0.08562634885311127 Validation loss 0.09811296314001083 Accuracy 0.7055000066757202\n",
      "Iteration 32700 Training loss 0.09126760065555573 Validation loss 0.10222003608942032 Accuracy 0.6865000128746033\n",
      "Iteration 32710 Training loss 0.09650176763534546 Validation loss 0.09314507246017456 Accuracy 0.7255000472068787\n",
      "Iteration 32720 Training loss 0.08330737799406052 Validation loss 0.09635224938392639 Accuracy 0.7155000567436218\n",
      "Iteration 32730 Training loss 0.09619438648223877 Validation loss 0.09374158084392548 Accuracy 0.7265000343322754\n",
      "Iteration 32740 Training loss 0.12766632437705994 Validation loss 0.10509669780731201 Accuracy 0.6680000424385071\n",
      "Iteration 32750 Training loss 0.08250539004802704 Validation loss 0.0898604616522789 Accuracy 0.7430000305175781\n",
      "Iteration 32760 Training loss 0.08983190357685089 Validation loss 0.0971408486366272 Accuracy 0.7235000133514404\n",
      "Iteration 32770 Training loss 0.07795168459415436 Validation loss 0.09068632870912552 Accuracy 0.7310000061988831\n",
      "Iteration 32780 Training loss 0.10037119686603546 Validation loss 0.09620224684476852 Accuracy 0.70250004529953\n",
      "Iteration 32790 Training loss 0.08359062671661377 Validation loss 0.09074388444423676 Accuracy 0.734000027179718\n",
      "Iteration 32800 Training loss 0.08163844048976898 Validation loss 0.09956872463226318 Accuracy 0.7075000405311584\n",
      "Iteration 32810 Training loss 0.09883569180965424 Validation loss 0.11440695822238922 Accuracy 0.6720000505447388\n",
      "Iteration 32820 Training loss 0.08590396493673325 Validation loss 0.0903589054942131 Accuracy 0.7420000433921814\n",
      "Iteration 32830 Training loss 0.08526727557182312 Validation loss 0.09379339963197708 Accuracy 0.7275000214576721\n",
      "Iteration 32840 Training loss 0.09939704835414886 Validation loss 0.09726253151893616 Accuracy 0.7065000534057617\n",
      "Iteration 32850 Training loss 0.09609498083591461 Validation loss 0.09478317946195602 Accuracy 0.7285000085830688\n",
      "Iteration 32860 Training loss 0.08781175315380096 Validation loss 0.09303615242242813 Accuracy 0.7420000433921814\n",
      "Iteration 32870 Training loss 0.08872132003307343 Validation loss 0.09630829095840454 Accuracy 0.7085000276565552\n",
      "Iteration 32880 Training loss 0.0819626972079277 Validation loss 0.10200700163841248 Accuracy 0.6945000290870667\n",
      "Iteration 32890 Training loss 0.08085322380065918 Validation loss 0.09747510403394699 Accuracy 0.6940000057220459\n",
      "Iteration 32900 Training loss 0.10656385868787766 Validation loss 0.09228049218654633 Accuracy 0.737500011920929\n",
      "Iteration 32910 Training loss 0.08846918493509293 Validation loss 0.10086164623498917 Accuracy 0.690000057220459\n",
      "Iteration 32920 Training loss 0.08693142980337143 Validation loss 0.09283486008644104 Accuracy 0.7220000624656677\n",
      "Iteration 32930 Training loss 0.11205432564020157 Validation loss 0.11576774716377258 Accuracy 0.6030000448226929\n",
      "Iteration 32940 Training loss 0.09430902451276779 Validation loss 0.09778109937906265 Accuracy 0.7050000429153442\n",
      "Iteration 32950 Training loss 0.09994050115346909 Validation loss 0.0966995507478714 Accuracy 0.7075000405311584\n",
      "Iteration 32960 Training loss 0.1179279088973999 Validation loss 0.1154450848698616 Accuracy 0.6360000371932983\n",
      "Iteration 32970 Training loss 0.0822768360376358 Validation loss 0.0928025171160698 Accuracy 0.721500039100647\n",
      "Iteration 32980 Training loss 0.09652823954820633 Validation loss 0.09965050220489502 Accuracy 0.6955000162124634\n",
      "Iteration 32990 Training loss 0.08431121706962585 Validation loss 0.0976581946015358 Accuracy 0.7080000042915344\n",
      "Iteration 33000 Training loss 0.10568609088659286 Validation loss 0.10443999618291855 Accuracy 0.674500048160553\n",
      "Iteration 33010 Training loss 0.1089947298169136 Validation loss 0.08973409235477448 Accuracy 0.7490000128746033\n",
      "Iteration 33020 Training loss 0.08151957392692566 Validation loss 0.09486043453216553 Accuracy 0.7245000600814819\n",
      "Iteration 33030 Training loss 0.08472633361816406 Validation loss 0.10240669548511505 Accuracy 0.6835000514984131\n",
      "Iteration 33040 Training loss 0.09160911291837692 Validation loss 0.09392764419317245 Accuracy 0.7235000133514404\n",
      "Iteration 33050 Training loss 0.09284929186105728 Validation loss 0.09850659221410751 Accuracy 0.7045000195503235\n",
      "Iteration 33060 Training loss 0.1042250245809555 Validation loss 0.08990829437971115 Accuracy 0.7515000104904175\n",
      "Iteration 33070 Training loss 0.07910692691802979 Validation loss 0.08736461400985718 Accuracy 0.7505000233650208\n",
      "Iteration 33080 Training loss 0.1010848879814148 Validation loss 0.1057228296995163 Accuracy 0.6760000586509705\n",
      "Iteration 33090 Training loss 0.09751307219266891 Validation loss 0.09660477191209793 Accuracy 0.7105000615119934\n",
      "Iteration 33100 Training loss 0.11180640012025833 Validation loss 0.10093611478805542 Accuracy 0.6980000138282776\n",
      "Iteration 33110 Training loss 0.07909058034420013 Validation loss 0.09170175343751907 Accuracy 0.7395000457763672\n",
      "Iteration 33120 Training loss 0.09194698184728622 Validation loss 0.09494829177856445 Accuracy 0.734000027179718\n",
      "Iteration 33130 Training loss 0.07932831346988678 Validation loss 0.09017840027809143 Accuracy 0.734000027179718\n",
      "Iteration 33140 Training loss 0.08663804829120636 Validation loss 0.09822654724121094 Accuracy 0.6990000605583191\n",
      "Iteration 33150 Training loss 0.09304773807525635 Validation loss 0.10635211318731308 Accuracy 0.6760000586509705\n",
      "Iteration 33160 Training loss 0.09012876451015472 Validation loss 0.11078372597694397 Accuracy 0.6665000319480896\n",
      "Iteration 33170 Training loss 0.07004880905151367 Validation loss 0.08894099295139313 Accuracy 0.7440000176429749\n",
      "Iteration 33180 Training loss 0.08217243105173111 Validation loss 0.08930251747369766 Accuracy 0.734000027179718\n",
      "Iteration 33190 Training loss 0.10461677610874176 Validation loss 0.10110577195882797 Accuracy 0.7110000252723694\n",
      "Iteration 33200 Training loss 0.10180763155221939 Validation loss 0.09673838317394257 Accuracy 0.7055000066757202\n",
      "Iteration 33210 Training loss 0.09468160569667816 Validation loss 0.09634685516357422 Accuracy 0.7170000076293945\n",
      "Iteration 33220 Training loss 0.08956338465213776 Validation loss 0.09856227040290833 Accuracy 0.7080000042915344\n",
      "Iteration 33230 Training loss 0.10289105772972107 Validation loss 0.0957624688744545 Accuracy 0.7260000109672546\n",
      "Iteration 33240 Training loss 0.10806935280561447 Validation loss 0.10896556079387665 Accuracy 0.6635000109672546\n",
      "Iteration 33250 Training loss 0.10542535781860352 Validation loss 0.09221842139959335 Accuracy 0.7250000238418579\n",
      "Iteration 33260 Training loss 0.07655318081378937 Validation loss 0.08945576846599579 Accuracy 0.7380000352859497\n",
      "Iteration 33270 Training loss 0.0938931256532669 Validation loss 0.08986583352088928 Accuracy 0.7545000314712524\n",
      "Iteration 33280 Training loss 0.07808434218168259 Validation loss 0.09301019459962845 Accuracy 0.7170000076293945\n",
      "Iteration 33290 Training loss 0.10105804353952408 Validation loss 0.08823320269584656 Accuracy 0.7360000610351562\n",
      "Iteration 33300 Training loss 0.10350977629423141 Validation loss 0.08791464567184448 Accuracy 0.7415000200271606\n",
      "Iteration 33310 Training loss 0.08325785398483276 Validation loss 0.08982695639133453 Accuracy 0.7480000257492065\n",
      "Iteration 33320 Training loss 0.0835849866271019 Validation loss 0.09286951273679733 Accuracy 0.7290000319480896\n",
      "Iteration 33330 Training loss 0.09405113756656647 Validation loss 0.10355003923177719 Accuracy 0.6795000433921814\n",
      "Iteration 33340 Training loss 0.10203348100185394 Validation loss 0.10571077466011047 Accuracy 0.6695000529289246\n",
      "Iteration 33350 Training loss 0.09208265691995621 Validation loss 0.09660746157169342 Accuracy 0.7285000085830688\n",
      "Iteration 33360 Training loss 0.07645042985677719 Validation loss 0.09087061136960983 Accuracy 0.7320000529289246\n",
      "Iteration 33370 Training loss 0.09453824162483215 Validation loss 0.10754939913749695 Accuracy 0.6510000228881836\n",
      "Iteration 33380 Training loss 0.09972423315048218 Validation loss 0.09566618502140045 Accuracy 0.7205000519752502\n",
      "Iteration 33390 Training loss 0.08471442759037018 Validation loss 0.08867952227592468 Accuracy 0.7400000095367432\n",
      "Iteration 33400 Training loss 0.12750516831874847 Validation loss 0.11487097293138504 Accuracy 0.6320000290870667\n",
      "Iteration 33410 Training loss 0.0789053738117218 Validation loss 0.09157511591911316 Accuracy 0.7535000443458557\n",
      "Iteration 33420 Training loss 0.08506940305233002 Validation loss 0.08995571732521057 Accuracy 0.752500057220459\n",
      "Iteration 33430 Training loss 0.08804554492235184 Validation loss 0.09106609225273132 Accuracy 0.7355000376701355\n",
      "Iteration 33440 Training loss 0.09244828671216965 Validation loss 0.10008618980646133 Accuracy 0.7045000195503235\n",
      "Iteration 33450 Training loss 0.10105237364768982 Validation loss 0.094367116689682 Accuracy 0.7195000052452087\n",
      "Iteration 33460 Training loss 0.0982862263917923 Validation loss 0.09391515702009201 Accuracy 0.7200000286102295\n",
      "Iteration 33470 Training loss 0.07917473465204239 Validation loss 0.09165352582931519 Accuracy 0.7415000200271606\n",
      "Iteration 33480 Training loss 0.09768987447023392 Validation loss 0.09438745677471161 Accuracy 0.7250000238418579\n",
      "Iteration 33490 Training loss 0.0849706158041954 Validation loss 0.08986833691596985 Accuracy 0.737500011920929\n",
      "Iteration 33500 Training loss 0.08661206811666489 Validation loss 0.09385780990123749 Accuracy 0.7235000133514404\n",
      "Iteration 33510 Training loss 0.08488710224628448 Validation loss 0.08786799013614655 Accuracy 0.7420000433921814\n",
      "Iteration 33520 Training loss 0.07411032915115356 Validation loss 0.09219513088464737 Accuracy 0.7410000562667847\n",
      "Iteration 33530 Training loss 0.09336939454078674 Validation loss 0.09847132116556168 Accuracy 0.7070000171661377\n",
      "Iteration 33540 Training loss 0.08688842505216599 Validation loss 0.0909370630979538 Accuracy 0.734000027179718\n",
      "Iteration 33550 Training loss 0.09952226281166077 Validation loss 0.09846314042806625 Accuracy 0.6995000243186951\n",
      "Iteration 33560 Training loss 0.13153865933418274 Validation loss 0.11994780600070953 Accuracy 0.6380000114440918\n",
      "Iteration 33570 Training loss 0.08899421244859695 Validation loss 0.09080864489078522 Accuracy 0.7430000305175781\n",
      "Iteration 33580 Training loss 0.07814760506153107 Validation loss 0.09092535823583603 Accuracy 0.737000048160553\n",
      "Iteration 33590 Training loss 0.08184735476970673 Validation loss 0.09616608917713165 Accuracy 0.7195000052452087\n",
      "Iteration 33600 Training loss 0.08283143490552902 Validation loss 0.09043269604444504 Accuracy 0.7410000562667847\n",
      "Iteration 33610 Training loss 0.07983443140983582 Validation loss 0.1015329584479332 Accuracy 0.6960000395774841\n",
      "Iteration 33620 Training loss 0.08217957615852356 Validation loss 0.0971258133649826 Accuracy 0.6970000267028809\n",
      "Iteration 33630 Training loss 0.09561235457658768 Validation loss 0.09088322520256042 Accuracy 0.7360000610351562\n",
      "Iteration 33640 Training loss 0.0991063192486763 Validation loss 0.09984634071588516 Accuracy 0.70250004529953\n",
      "Iteration 33650 Training loss 0.08929582685232162 Validation loss 0.09150568395853043 Accuracy 0.7345000505447388\n",
      "Iteration 33660 Training loss 0.09177759289741516 Validation loss 0.09305676817893982 Accuracy 0.7345000505447388\n",
      "Iteration 33670 Training loss 0.09137164801359177 Validation loss 0.09341175109148026 Accuracy 0.7220000624656677\n",
      "Iteration 33680 Training loss 0.07164798676967621 Validation loss 0.09737824648618698 Accuracy 0.7010000348091125\n",
      "Iteration 33690 Training loss 0.09118135273456573 Validation loss 0.09492460638284683 Accuracy 0.7190000414848328\n",
      "Iteration 33700 Training loss 0.06722218543291092 Validation loss 0.09237881749868393 Accuracy 0.7260000109672546\n",
      "Iteration 33710 Training loss 0.10675989091396332 Validation loss 0.09973110258579254 Accuracy 0.6935000419616699\n",
      "Iteration 33720 Training loss 0.09547466784715652 Validation loss 0.09893591701984406 Accuracy 0.7040000557899475\n",
      "Iteration 33730 Training loss 0.0936826765537262 Validation loss 0.10175561904907227 Accuracy 0.6895000338554382\n",
      "Iteration 33740 Training loss 0.10824595391750336 Validation loss 0.11025171726942062 Accuracy 0.659000039100647\n",
      "Iteration 33750 Training loss 0.12618398666381836 Validation loss 0.10007450729608536 Accuracy 0.6920000314712524\n",
      "Iteration 33760 Training loss 0.09444630146026611 Validation loss 0.09197484701871872 Accuracy 0.7275000214576721\n",
      "Iteration 33770 Training loss 0.07659153640270233 Validation loss 0.08944672346115112 Accuracy 0.7415000200271606\n",
      "Iteration 33780 Training loss 0.1058020070195198 Validation loss 0.09998928010463715 Accuracy 0.7040000557899475\n",
      "Iteration 33790 Training loss 0.09739751368761063 Validation loss 0.0927988588809967 Accuracy 0.737000048160553\n",
      "Iteration 33800 Training loss 0.08966562151908875 Validation loss 0.08811666071414948 Accuracy 0.7390000224113464\n",
      "Iteration 33810 Training loss 0.09821096062660217 Validation loss 0.08916828781366348 Accuracy 0.7520000338554382\n",
      "Iteration 33820 Training loss 0.0807228609919548 Validation loss 0.09041690826416016 Accuracy 0.7415000200271606\n",
      "Iteration 33830 Training loss 0.08876362442970276 Validation loss 0.09612812101840973 Accuracy 0.7165000438690186\n",
      "Iteration 33840 Training loss 0.10477893799543381 Validation loss 0.08875251561403275 Accuracy 0.7425000071525574\n",
      "Iteration 33850 Training loss 0.10879707336425781 Validation loss 0.0961766317486763 Accuracy 0.718500018119812\n",
      "Iteration 33860 Training loss 0.08241266757249832 Validation loss 0.09748657792806625 Accuracy 0.7090000510215759\n",
      "Iteration 33870 Training loss 0.09140636026859283 Validation loss 0.09006859362125397 Accuracy 0.7345000505447388\n",
      "Iteration 33880 Training loss 0.09448593109846115 Validation loss 0.1073845848441124 Accuracy 0.6735000610351562\n",
      "Iteration 33890 Training loss 0.09282787144184113 Validation loss 0.09099327027797699 Accuracy 0.7315000295639038\n",
      "Iteration 33900 Training loss 0.13590143620967865 Validation loss 0.12277723103761673 Accuracy 0.6115000247955322\n",
      "Iteration 33910 Training loss 0.10137070715427399 Validation loss 0.09409917145967484 Accuracy 0.7390000224113464\n",
      "Iteration 33920 Training loss 0.10106681287288666 Validation loss 0.09619464725255966 Accuracy 0.7135000228881836\n",
      "Iteration 33930 Training loss 0.09318465739488602 Validation loss 0.09343072772026062 Accuracy 0.7345000505447388\n",
      "Iteration 33940 Training loss 0.09957385808229446 Validation loss 0.1040797308087349 Accuracy 0.6795000433921814\n",
      "Iteration 33950 Training loss 0.08968378603458405 Validation loss 0.09296068549156189 Accuracy 0.7360000610351562\n",
      "Iteration 33960 Training loss 0.09867532551288605 Validation loss 0.09826990216970444 Accuracy 0.7075000405311584\n",
      "Iteration 33970 Training loss 0.10211680084466934 Validation loss 0.10523958504199982 Accuracy 0.6610000133514404\n",
      "Iteration 33980 Training loss 0.07647484540939331 Validation loss 0.1003638505935669 Accuracy 0.7075000405311584\n",
      "Iteration 33990 Training loss 0.10594766587018967 Validation loss 0.09337436407804489 Accuracy 0.7260000109672546\n",
      "Iteration 34000 Training loss 0.08128947764635086 Validation loss 0.09761173278093338 Accuracy 0.7070000171661377\n",
      "Iteration 34010 Training loss 0.08937353640794754 Validation loss 0.08971554040908813 Accuracy 0.7460000514984131\n",
      "Iteration 34020 Training loss 0.10294162482023239 Validation loss 0.09273391216993332 Accuracy 0.737500011920929\n",
      "Iteration 34030 Training loss 0.08647092431783676 Validation loss 0.09390643239021301 Accuracy 0.7225000262260437\n",
      "Iteration 34040 Training loss 0.08236391097307205 Validation loss 0.09380075335502625 Accuracy 0.7205000519752502\n",
      "Iteration 34050 Training loss 0.06987610459327698 Validation loss 0.09329041093587875 Accuracy 0.7310000061988831\n",
      "Iteration 34060 Training loss 0.08771680295467377 Validation loss 0.09644719958305359 Accuracy 0.7230000495910645\n",
      "Iteration 34070 Training loss 0.08025412261486053 Validation loss 0.0894571840763092 Accuracy 0.7415000200271606\n",
      "Iteration 34080 Training loss 0.0841934010386467 Validation loss 0.09610619395971298 Accuracy 0.7125000357627869\n",
      "Iteration 34090 Training loss 0.10186655819416046 Validation loss 0.09673388302326202 Accuracy 0.7120000123977661\n",
      "Iteration 34100 Training loss 0.08311162143945694 Validation loss 0.09346143901348114 Accuracy 0.7225000262260437\n",
      "Iteration 34110 Training loss 0.10837974399328232 Validation loss 0.10588176548480988 Accuracy 0.6610000133514404\n",
      "Iteration 34120 Training loss 0.0879724770784378 Validation loss 0.09456589072942734 Accuracy 0.7065000534057617\n",
      "Iteration 34130 Training loss 0.08739542961120605 Validation loss 0.09445279091596603 Accuracy 0.7085000276565552\n",
      "Iteration 34140 Training loss 0.10229639708995819 Validation loss 0.10305849462747574 Accuracy 0.6765000224113464\n",
      "Iteration 34150 Training loss 0.09393693506717682 Validation loss 0.0900588184595108 Accuracy 0.7345000505447388\n",
      "Iteration 34160 Training loss 0.07679221779108047 Validation loss 0.08888505399227142 Accuracy 0.7360000610351562\n",
      "Iteration 34170 Training loss 0.09168537706136703 Validation loss 0.09217748045921326 Accuracy 0.7430000305175781\n",
      "Iteration 34180 Training loss 0.0907379686832428 Validation loss 0.09216920286417007 Accuracy 0.7255000472068787\n",
      "Iteration 34190 Training loss 0.08293160051107407 Validation loss 0.09483186155557632 Accuracy 0.7170000076293945\n",
      "Iteration 34200 Training loss 0.1134904995560646 Validation loss 0.10411684215068817 Accuracy 0.6645000576972961\n",
      "Iteration 34210 Training loss 0.08137959986925125 Validation loss 0.10295146703720093 Accuracy 0.6770000457763672\n",
      "Iteration 34220 Training loss 0.09659858793020248 Validation loss 0.08873555064201355 Accuracy 0.7460000514984131\n",
      "Iteration 34230 Training loss 0.07565697282552719 Validation loss 0.08951406925916672 Accuracy 0.7515000104904175\n",
      "Iteration 34240 Training loss 0.09401550889015198 Validation loss 0.09227363765239716 Accuracy 0.7350000143051147\n",
      "Iteration 34250 Training loss 0.07761004567146301 Validation loss 0.09187602996826172 Accuracy 0.7420000433921814\n",
      "Iteration 34260 Training loss 0.09989424794912338 Validation loss 0.0962088331580162 Accuracy 0.7195000052452087\n",
      "Iteration 34270 Training loss 0.0991806760430336 Validation loss 0.09167858958244324 Accuracy 0.7360000610351562\n",
      "Iteration 34280 Training loss 0.08467485010623932 Validation loss 0.08892061561346054 Accuracy 0.749500036239624\n",
      "Iteration 34290 Training loss 0.08991993963718414 Validation loss 0.09344705939292908 Accuracy 0.7110000252723694\n",
      "Iteration 34300 Training loss 0.09770924597978592 Validation loss 0.09502103179693222 Accuracy 0.7170000076293945\n",
      "Iteration 34310 Training loss 0.09076668322086334 Validation loss 0.09449627250432968 Accuracy 0.721500039100647\n",
      "Iteration 34320 Training loss 0.07175903767347336 Validation loss 0.09017927199602127 Accuracy 0.734000027179718\n",
      "Iteration 34330 Training loss 0.08446111530065536 Validation loss 0.09417019784450531 Accuracy 0.7310000061988831\n",
      "Iteration 34340 Training loss 0.10121279209852219 Validation loss 0.09738103300333023 Accuracy 0.7090000510215759\n",
      "Iteration 34350 Training loss 0.09572742134332657 Validation loss 0.09416967630386353 Accuracy 0.737500011920929\n",
      "Iteration 34360 Training loss 0.09242028743028641 Validation loss 0.09527952969074249 Accuracy 0.7280000448226929\n",
      "Iteration 34370 Training loss 0.08635971695184708 Validation loss 0.09240613877773285 Accuracy 0.7260000109672546\n",
      "Iteration 34380 Training loss 0.09033337235450745 Validation loss 0.0889286994934082 Accuracy 0.752500057220459\n",
      "Iteration 34390 Training loss 0.09672655910253525 Validation loss 0.09591715037822723 Accuracy 0.7150000333786011\n",
      "Iteration 34400 Training loss 0.09518138319253922 Validation loss 0.09437988698482513 Accuracy 0.7190000414848328\n",
      "Iteration 34410 Training loss 0.09385991096496582 Validation loss 0.09425237029790878 Accuracy 0.718500018119812\n",
      "Iteration 34420 Training loss 0.08767524361610413 Validation loss 0.08933274447917938 Accuracy 0.737500011920929\n",
      "Iteration 34430 Training loss 0.08693545311689377 Validation loss 0.09106219559907913 Accuracy 0.7310000061988831\n",
      "Iteration 34440 Training loss 0.09125252813100815 Validation loss 0.09324023872613907 Accuracy 0.7190000414848328\n",
      "Iteration 34450 Training loss 0.08428291231393814 Validation loss 0.08892340213060379 Accuracy 0.7450000643730164\n",
      "Iteration 34460 Training loss 0.08209621161222458 Validation loss 0.09548714756965637 Accuracy 0.718000054359436\n",
      "Iteration 34470 Training loss 0.0989079624414444 Validation loss 0.10130096226930618 Accuracy 0.7020000219345093\n",
      "Iteration 34480 Training loss 0.11750324815511703 Validation loss 0.11098775267601013 Accuracy 0.671500027179718\n",
      "Iteration 34490 Training loss 0.100199393928051 Validation loss 0.09602918475866318 Accuracy 0.721500039100647\n",
      "Iteration 34500 Training loss 0.11389695107936859 Validation loss 0.10500255227088928 Accuracy 0.6670000553131104\n",
      "Iteration 34510 Training loss 0.09511489421129227 Validation loss 0.09946361184120178 Accuracy 0.6890000104904175\n",
      "Iteration 34520 Training loss 0.08187513053417206 Validation loss 0.09391077607870102 Accuracy 0.7230000495910645\n",
      "Iteration 34530 Training loss 0.07135891914367676 Validation loss 0.08853618800640106 Accuracy 0.7475000619888306\n",
      "Iteration 34540 Training loss 0.0914207249879837 Validation loss 0.09830783307552338 Accuracy 0.6950000524520874\n",
      "Iteration 34550 Training loss 0.08151624351739883 Validation loss 0.0946892723441124 Accuracy 0.7190000414848328\n",
      "Iteration 34560 Training loss 0.09342731535434723 Validation loss 0.09134902060031891 Accuracy 0.7275000214576721\n",
      "Iteration 34570 Training loss 0.08694803714752197 Validation loss 0.09138478338718414 Accuracy 0.7330000400543213\n",
      "Iteration 34580 Training loss 0.11279170960187912 Validation loss 0.10304766148328781 Accuracy 0.687000036239624\n",
      "Iteration 34590 Training loss 0.09385375678539276 Validation loss 0.10126277804374695 Accuracy 0.690000057220459\n",
      "Iteration 34600 Training loss 0.09771052747964859 Validation loss 0.09231475740671158 Accuracy 0.7320000529289246\n",
      "Iteration 34610 Training loss 0.09394264221191406 Validation loss 0.09413028508424759 Accuracy 0.7290000319480896\n",
      "Iteration 34620 Training loss 0.08675879240036011 Validation loss 0.0899139940738678 Accuracy 0.7365000247955322\n",
      "Iteration 34630 Training loss 0.10374186933040619 Validation loss 0.09360243380069733 Accuracy 0.7290000319480896\n",
      "Iteration 34640 Training loss 0.07254388928413391 Validation loss 0.0891791582107544 Accuracy 0.749500036239624\n",
      "Iteration 34650 Training loss 0.11706090718507767 Validation loss 0.12111956626176834 Accuracy 0.5980000495910645\n",
      "Iteration 34660 Training loss 0.1100565493106842 Validation loss 0.10107015073299408 Accuracy 0.6995000243186951\n",
      "Iteration 34670 Training loss 0.10320327430963516 Validation loss 0.09628401696681976 Accuracy 0.7145000100135803\n",
      "Iteration 34680 Training loss 0.09193496406078339 Validation loss 0.09622466564178467 Accuracy 0.7110000252723694\n",
      "Iteration 34690 Training loss 0.08506881445646286 Validation loss 0.09717459976673126 Accuracy 0.70250004529953\n",
      "Iteration 34700 Training loss 0.09891749173402786 Validation loss 0.10153407603502274 Accuracy 0.6925000548362732\n",
      "Iteration 34710 Training loss 0.09653721749782562 Validation loss 0.10174596309661865 Accuracy 0.6925000548362732\n",
      "Iteration 34720 Training loss 0.09264890104532242 Validation loss 0.10797568410634995 Accuracy 0.6610000133514404\n",
      "Iteration 34730 Training loss 0.11215217411518097 Validation loss 0.10748013854026794 Accuracy 0.6700000166893005\n",
      "Iteration 34740 Training loss 0.11016152799129486 Validation loss 0.11378064751625061 Accuracy 0.6230000257492065\n",
      "Iteration 34750 Training loss 0.12039291113615036 Validation loss 0.12185630202293396 Accuracy 0.5820000171661377\n",
      "Iteration 34760 Training loss 0.09934457391500473 Validation loss 0.10697685182094574 Accuracy 0.6600000262260437\n",
      "Iteration 34770 Training loss 0.07945852726697922 Validation loss 0.0937524065375328 Accuracy 0.7205000519752502\n",
      "Iteration 34780 Training loss 0.1265316605567932 Validation loss 0.10785098373889923 Accuracy 0.6690000295639038\n",
      "Iteration 34790 Training loss 0.10797929763793945 Validation loss 0.09567120671272278 Accuracy 0.7080000042915344\n",
      "Iteration 34800 Training loss 0.07512693107128143 Validation loss 0.08833291381597519 Accuracy 0.753000020980835\n",
      "Iteration 34810 Training loss 0.09833993762731552 Validation loss 0.09613727033138275 Accuracy 0.7195000052452087\n",
      "Iteration 34820 Training loss 0.10122854262590408 Validation loss 0.09221776574850082 Accuracy 0.7290000319480896\n",
      "Iteration 34830 Training loss 0.10393701493740082 Validation loss 0.10108327865600586 Accuracy 0.6925000548362732\n",
      "Iteration 34840 Training loss 0.0923096239566803 Validation loss 0.09494375437498093 Accuracy 0.7065000534057617\n",
      "Iteration 34850 Training loss 0.09962073713541031 Validation loss 0.09624021500349045 Accuracy 0.7090000510215759\n",
      "Iteration 34860 Training loss 0.08436502516269684 Validation loss 0.09488434344530106 Accuracy 0.718500018119812\n",
      "Iteration 34870 Training loss 0.09881098568439484 Validation loss 0.10269144177436829 Accuracy 0.6895000338554382\n",
      "Iteration 34880 Training loss 0.11649919301271439 Validation loss 0.10127608478069305 Accuracy 0.6945000290870667\n",
      "Iteration 34890 Training loss 0.10589151084423065 Validation loss 0.10023314505815506 Accuracy 0.6930000185966492\n",
      "Iteration 34900 Training loss 0.09256789088249207 Validation loss 0.10261421650648117 Accuracy 0.6965000033378601\n",
      "Iteration 34910 Training loss 0.11991361528635025 Validation loss 0.12509824335575104 Accuracy 0.5910000205039978\n",
      "Iteration 34920 Training loss 0.08804214000701904 Validation loss 0.09597087651491165 Accuracy 0.7190000414848328\n",
      "Iteration 34930 Training loss 0.07977169007062912 Validation loss 0.09192678332328796 Accuracy 0.7305000424385071\n",
      "Iteration 34940 Training loss 0.09351982176303864 Validation loss 0.0942961797118187 Accuracy 0.7270000576972961\n",
      "Iteration 34950 Training loss 0.07875929772853851 Validation loss 0.0893738716840744 Accuracy 0.7350000143051147\n",
      "Iteration 34960 Training loss 0.0919116958975792 Validation loss 0.08999563753604889 Accuracy 0.7305000424385071\n",
      "Iteration 34970 Training loss 0.14483678340911865 Validation loss 0.12255857139825821 Accuracy 0.6220000386238098\n",
      "Iteration 34980 Training loss 0.10122624784708023 Validation loss 0.09082353860139847 Accuracy 0.7405000329017639\n",
      "Iteration 34990 Training loss 0.1130075454711914 Validation loss 0.09630592912435532 Accuracy 0.7045000195503235\n",
      "Iteration 35000 Training loss 0.08875282108783722 Validation loss 0.08983898907899857 Accuracy 0.7455000281333923\n",
      "Iteration 35010 Training loss 0.08343003690242767 Validation loss 0.09993791580200195 Accuracy 0.6985000371932983\n",
      "Iteration 35020 Training loss 0.07843606919050217 Validation loss 0.09261596947908401 Accuracy 0.7280000448226929\n",
      "Iteration 35030 Training loss 0.09868337213993073 Validation loss 0.10347995162010193 Accuracy 0.6755000352859497\n",
      "Iteration 35040 Training loss 0.10804326087236404 Validation loss 0.0932716652750969 Accuracy 0.7245000600814819\n",
      "Iteration 35050 Training loss 0.10166402906179428 Validation loss 0.09263011813163757 Accuracy 0.7255000472068787\n",
      "Iteration 35060 Training loss 0.08854898065328598 Validation loss 0.09438145905733109 Accuracy 0.721500039100647\n",
      "Iteration 35070 Training loss 0.10224761813879013 Validation loss 0.0972314327955246 Accuracy 0.7120000123977661\n",
      "Iteration 35080 Training loss 0.11548829078674316 Validation loss 0.11435267329216003 Accuracy 0.6390000581741333\n",
      "Iteration 35090 Training loss 0.10869497060775757 Validation loss 0.10030077397823334 Accuracy 0.70250004529953\n",
      "Iteration 35100 Training loss 0.10089027136564255 Validation loss 0.09436748176813126 Accuracy 0.7195000052452087\n",
      "Iteration 35110 Training loss 0.1071520522236824 Validation loss 0.09989282488822937 Accuracy 0.7020000219345093\n",
      "Iteration 35120 Training loss 0.0849408432841301 Validation loss 0.0971403419971466 Accuracy 0.7100000381469727\n",
      "Iteration 35130 Training loss 0.09705141931772232 Validation loss 0.09022926539182663 Accuracy 0.7400000095367432\n",
      "Iteration 35140 Training loss 0.0523751899600029 Validation loss 0.08869767934083939 Accuracy 0.7395000457763672\n",
      "Iteration 35150 Training loss 0.14759264886379242 Validation loss 0.15532773733139038 Accuracy 0.5325000286102295\n",
      "Iteration 35160 Training loss 0.09398631751537323 Validation loss 0.09873145073652267 Accuracy 0.6885000467300415\n",
      "Iteration 35170 Training loss 0.09139738976955414 Validation loss 0.09452171623706818 Accuracy 0.7210000157356262\n",
      "Iteration 35180 Training loss 0.09297183901071548 Validation loss 0.09320519119501114 Accuracy 0.7290000319480896\n",
      "Iteration 35190 Training loss 0.09037068486213684 Validation loss 0.09157633036375046 Accuracy 0.7310000061988831\n",
      "Iteration 35200 Training loss 0.09822221845388412 Validation loss 0.09194321185350418 Accuracy 0.737500011920929\n",
      "Iteration 35210 Training loss 0.11596803367137909 Validation loss 0.10709045827388763 Accuracy 0.6640000343322754\n",
      "Iteration 35220 Training loss 0.09284017235040665 Validation loss 0.09330654889345169 Accuracy 0.7300000190734863\n",
      "Iteration 35230 Training loss 0.0833534449338913 Validation loss 0.10289717465639114 Accuracy 0.6710000038146973\n",
      "Iteration 35240 Training loss 0.09550119191408157 Validation loss 0.095091313123703 Accuracy 0.7045000195503235\n",
      "Iteration 35250 Training loss 0.10527314990758896 Validation loss 0.09530608355998993 Accuracy 0.7275000214576721\n",
      "Iteration 35260 Training loss 0.0965883806347847 Validation loss 0.09268677979707718 Accuracy 0.733500063419342\n",
      "Iteration 35270 Training loss 0.08298046141862869 Validation loss 0.09548986703157425 Accuracy 0.7125000357627869\n",
      "Iteration 35280 Training loss 0.09430453181266785 Validation loss 0.09407130628824234 Accuracy 0.7330000400543213\n",
      "Iteration 35290 Training loss 0.09736286848783493 Validation loss 0.10451901704072952 Accuracy 0.6805000305175781\n",
      "Iteration 35300 Training loss 0.08227880299091339 Validation loss 0.088815838098526 Accuracy 0.7365000247955322\n",
      "Iteration 35310 Training loss 0.08954600244760513 Validation loss 0.09326261281967163 Accuracy 0.7220000624656677\n",
      "Iteration 35320 Training loss 0.10534664988517761 Validation loss 0.0946200042963028 Accuracy 0.7225000262260437\n",
      "Iteration 35330 Training loss 0.08374885469675064 Validation loss 0.09319060295820236 Accuracy 0.7350000143051147\n",
      "Iteration 35340 Training loss 0.09500648081302643 Validation loss 0.09483612328767776 Accuracy 0.7235000133514404\n",
      "Iteration 35350 Training loss 0.08511686325073242 Validation loss 0.09604258835315704 Accuracy 0.7095000147819519\n",
      "Iteration 35360 Training loss 0.0817556381225586 Validation loss 0.09389899671077728 Accuracy 0.7155000567436218\n",
      "Iteration 35370 Training loss 0.08418421447277069 Validation loss 0.0915103480219841 Accuracy 0.7385000586509705\n",
      "Iteration 35380 Training loss 0.08543096482753754 Validation loss 0.0933104157447815 Accuracy 0.7255000472068787\n",
      "Iteration 35390 Training loss 0.10127286612987518 Validation loss 0.10030633211135864 Accuracy 0.6975000500679016\n",
      "Iteration 35400 Training loss 0.09077140688896179 Validation loss 0.09511642903089523 Accuracy 0.7260000109672546\n",
      "Iteration 35410 Training loss 0.09159024059772491 Validation loss 0.10293766111135483 Accuracy 0.6735000610351562\n",
      "Iteration 35420 Training loss 0.08734212070703506 Validation loss 0.09395346790552139 Accuracy 0.7270000576972961\n",
      "Iteration 35430 Training loss 0.10525653511285782 Validation loss 0.09627648442983627 Accuracy 0.7055000066757202\n",
      "Iteration 35440 Training loss 0.09716076403856277 Validation loss 0.09534315764904022 Accuracy 0.703000009059906\n",
      "Iteration 35450 Training loss 0.09194757044315338 Validation loss 0.09978724271059036 Accuracy 0.6950000524520874\n",
      "Iteration 35460 Training loss 0.0791374146938324 Validation loss 0.09103024005889893 Accuracy 0.7380000352859497\n",
      "Iteration 35470 Training loss 0.07645288109779358 Validation loss 0.09296521544456482 Accuracy 0.7255000472068787\n",
      "Iteration 35480 Training loss 0.07618576288223267 Validation loss 0.10192334651947021 Accuracy 0.7010000348091125\n",
      "Iteration 35490 Training loss 0.1205398440361023 Validation loss 0.13168048858642578 Accuracy 0.515500009059906\n",
      "Iteration 35500 Training loss 0.09810808300971985 Validation loss 0.09147891402244568 Accuracy 0.7380000352859497\n",
      "Iteration 35510 Training loss 0.10956288874149323 Validation loss 0.09480644762516022 Accuracy 0.7130000591278076\n",
      "Iteration 35520 Training loss 0.09826324880123138 Validation loss 0.09106661379337311 Accuracy 0.7390000224113464\n",
      "Iteration 35530 Training loss 0.09372388571500778 Validation loss 0.10146800428628922 Accuracy 0.6825000047683716\n",
      "Iteration 35540 Training loss 0.11310755461454391 Validation loss 0.09936623275279999 Accuracy 0.7120000123977661\n",
      "Iteration 35550 Training loss 0.12514179944992065 Validation loss 0.12111890316009521 Accuracy 0.6165000200271606\n",
      "Iteration 35560 Training loss 0.06382818520069122 Validation loss 0.09341615438461304 Accuracy 0.7230000495910645\n",
      "Iteration 35570 Training loss 0.11243810504674911 Validation loss 0.10210619121789932 Accuracy 0.6810000538825989\n",
      "Iteration 35580 Training loss 0.10036876797676086 Validation loss 0.10175013542175293 Accuracy 0.6875000596046448\n",
      "Iteration 35590 Training loss 0.09426776319742203 Validation loss 0.09614845365285873 Accuracy 0.7135000228881836\n",
      "Iteration 35600 Training loss 0.09668855369091034 Validation loss 0.0907779112458229 Accuracy 0.7390000224113464\n",
      "Iteration 35610 Training loss 0.10251220315694809 Validation loss 0.0946773886680603 Accuracy 0.7245000600814819\n",
      "Iteration 35620 Training loss 0.09650707989931107 Validation loss 0.09537271410226822 Accuracy 0.7080000042915344\n",
      "Iteration 35630 Training loss 0.10376699268817902 Validation loss 0.09661896526813507 Accuracy 0.7245000600814819\n",
      "Iteration 35640 Training loss 0.09433674812316895 Validation loss 0.10502948611974716 Accuracy 0.6695000529289246\n",
      "Iteration 35650 Training loss 0.07663718611001968 Validation loss 0.0938790887594223 Accuracy 0.718000054359436\n",
      "Iteration 35660 Training loss 0.09849719703197479 Validation loss 0.0967358723282814 Accuracy 0.7080000042915344\n",
      "Iteration 35670 Training loss 0.10259339213371277 Validation loss 0.09364775568246841 Accuracy 0.7390000224113464\n",
      "Iteration 35680 Training loss 0.09163616597652435 Validation loss 0.09892374277114868 Accuracy 0.7005000114440918\n",
      "Iteration 35690 Training loss 0.08678904175758362 Validation loss 0.09437379986047745 Accuracy 0.7270000576972961\n",
      "Iteration 35700 Training loss 0.10719453543424606 Validation loss 0.08907435089349747 Accuracy 0.7440000176429749\n",
      "Iteration 35710 Training loss 0.10685661435127258 Validation loss 0.09923561662435532 Accuracy 0.7040000557899475\n",
      "Iteration 35720 Training loss 0.10669378936290741 Validation loss 0.09314480423927307 Accuracy 0.7330000400543213\n",
      "Iteration 35730 Training loss 0.11432178318500519 Validation loss 0.09692788124084473 Accuracy 0.7070000171661377\n",
      "Iteration 35740 Training loss 0.10306502133607864 Validation loss 0.09308603405952454 Accuracy 0.7230000495910645\n",
      "Iteration 35750 Training loss 0.09476815909147263 Validation loss 0.09358154982328415 Accuracy 0.7270000576972961\n",
      "Iteration 35760 Training loss 0.07592236250638962 Validation loss 0.0914856493473053 Accuracy 0.7345000505447388\n",
      "Iteration 35770 Training loss 0.09192287921905518 Validation loss 0.09407495707273483 Accuracy 0.7330000400543213\n",
      "Iteration 35780 Training loss 0.07966957241296768 Validation loss 0.09154750406742096 Accuracy 0.7270000576972961\n",
      "Iteration 35790 Training loss 0.08965034037828445 Validation loss 0.09031636267900467 Accuracy 0.7380000352859497\n",
      "Iteration 35800 Training loss 0.12295162677764893 Validation loss 0.11477135121822357 Accuracy 0.6285000443458557\n",
      "Iteration 35810 Training loss 0.08984088897705078 Validation loss 0.09160538017749786 Accuracy 0.7325000166893005\n",
      "Iteration 35820 Training loss 0.07775501906871796 Validation loss 0.08845645189285278 Accuracy 0.7445000410079956\n",
      "Iteration 35830 Training loss 0.06707561761140823 Validation loss 0.090115025639534 Accuracy 0.7295000553131104\n",
      "Iteration 35840 Training loss 0.12601709365844727 Validation loss 0.10922235995531082 Accuracy 0.6540000438690186\n",
      "Iteration 35850 Training loss 0.08941025286912918 Validation loss 0.0926177054643631 Accuracy 0.7310000061988831\n",
      "Iteration 35860 Training loss 0.07147525995969772 Validation loss 0.08859589695930481 Accuracy 0.7410000562667847\n",
      "Iteration 35870 Training loss 0.09181506186723709 Validation loss 0.09368074685335159 Accuracy 0.7350000143051147\n",
      "Iteration 35880 Training loss 0.08635704219341278 Validation loss 0.09719675034284592 Accuracy 0.7155000567436218\n",
      "Iteration 35890 Training loss 0.09579212218523026 Validation loss 0.09404756873846054 Accuracy 0.7115000486373901\n",
      "Iteration 35900 Training loss 0.09876912832260132 Validation loss 0.09617356210947037 Accuracy 0.7220000624656677\n",
      "Iteration 35910 Training loss 0.10253706574440002 Validation loss 0.09562132507562637 Accuracy 0.7150000333786011\n",
      "Iteration 35920 Training loss 0.09976619482040405 Validation loss 0.09894533455371857 Accuracy 0.690500020980835\n",
      "Iteration 35930 Training loss 0.10413837432861328 Validation loss 0.09693478792905807 Accuracy 0.7255000472068787\n",
      "Iteration 35940 Training loss 0.112094946205616 Validation loss 0.10004505515098572 Accuracy 0.7075000405311584\n",
      "Iteration 35950 Training loss 0.09766429662704468 Validation loss 0.09861250966787338 Accuracy 0.7000000476837158\n",
      "Iteration 35960 Training loss 0.10046616941690445 Validation loss 0.1055125817656517 Accuracy 0.6685000061988831\n",
      "Iteration 35970 Training loss 0.0982762798666954 Validation loss 0.10001976788043976 Accuracy 0.7000000476837158\n",
      "Iteration 35980 Training loss 0.09671606123447418 Validation loss 0.10458309203386307 Accuracy 0.6820000410079956\n",
      "Iteration 35990 Training loss 0.09450002759695053 Validation loss 0.10305258631706238 Accuracy 0.6840000152587891\n",
      "Iteration 36000 Training loss 0.08978374302387238 Validation loss 0.10604464262723923 Accuracy 0.6850000619888306\n",
      "Iteration 36010 Training loss 0.11410380154848099 Validation loss 0.09201175719499588 Accuracy 0.7255000472068787\n",
      "Iteration 36020 Training loss 0.10219526290893555 Validation loss 0.10617997497320175 Accuracy 0.6665000319480896\n",
      "Iteration 36030 Training loss 0.09579353779554367 Validation loss 0.10337306559085846 Accuracy 0.6790000200271606\n",
      "Iteration 36040 Training loss 0.08514568954706192 Validation loss 0.09496091306209564 Accuracy 0.7175000309944153\n",
      "Iteration 36050 Training loss 0.08606982976198196 Validation loss 0.09457041323184967 Accuracy 0.7125000357627869\n",
      "Iteration 36060 Training loss 0.0830465778708458 Validation loss 0.09257940948009491 Accuracy 0.7295000553131104\n",
      "Iteration 36070 Training loss 0.0835990235209465 Validation loss 0.09485713392496109 Accuracy 0.7160000205039978\n",
      "Iteration 36080 Training loss 0.08701036125421524 Validation loss 0.09361451864242554 Accuracy 0.7315000295639038\n",
      "Iteration 36090 Training loss 0.1151764765381813 Validation loss 0.09839268028736115 Accuracy 0.7175000309944153\n",
      "Iteration 36100 Training loss 0.09705478698015213 Validation loss 0.10191015154123306 Accuracy 0.6845000386238098\n",
      "Iteration 36110 Training loss 0.08229812234640121 Validation loss 0.09787628799676895 Accuracy 0.7010000348091125\n",
      "Iteration 36120 Training loss 0.09691408276557922 Validation loss 0.09488216042518616 Accuracy 0.7240000367164612\n",
      "Iteration 36130 Training loss 0.09358341246843338 Validation loss 0.09422343224287033 Accuracy 0.7190000414848328\n",
      "Iteration 36140 Training loss 0.09154224395751953 Validation loss 0.10800843685865402 Accuracy 0.6575000286102295\n",
      "Iteration 36150 Training loss 0.09769010543823242 Validation loss 0.09569098055362701 Accuracy 0.7195000052452087\n",
      "Iteration 36160 Training loss 0.08141738921403885 Validation loss 0.09856587648391724 Accuracy 0.6970000267028809\n",
      "Iteration 36170 Training loss 0.0914960652589798 Validation loss 0.0947565957903862 Accuracy 0.7220000624656677\n",
      "Iteration 36180 Training loss 0.0781305655837059 Validation loss 0.09803029149770737 Accuracy 0.7085000276565552\n",
      "Iteration 36190 Training loss 0.13187415897846222 Validation loss 0.1022036001086235 Accuracy 0.6835000514984131\n",
      "Iteration 36200 Training loss 0.09480327367782593 Validation loss 0.10987740010023117 Accuracy 0.6570000052452087\n",
      "Iteration 36210 Training loss 0.08738384395837784 Validation loss 0.09369326382875443 Accuracy 0.7295000553131104\n",
      "Iteration 36220 Training loss 0.08802846819162369 Validation loss 0.0906381905078888 Accuracy 0.7440000176429749\n",
      "Iteration 36230 Training loss 0.11543428152799606 Validation loss 0.10414707660675049 Accuracy 0.6740000247955322\n",
      "Iteration 36240 Training loss 0.10177487879991531 Validation loss 0.10105634480714798 Accuracy 0.6860000491142273\n",
      "Iteration 36250 Training loss 0.10207473486661911 Validation loss 0.09946055710315704 Accuracy 0.7065000534057617\n",
      "Iteration 36260 Training loss 0.09493259340524673 Validation loss 0.093757264316082 Accuracy 0.7270000576972961\n",
      "Iteration 36270 Training loss 0.1115448921918869 Validation loss 0.11192388087511063 Accuracy 0.6500000357627869\n",
      "Iteration 36280 Training loss 0.09219525754451752 Validation loss 0.09476903825998306 Accuracy 0.7280000448226929\n",
      "Iteration 36290 Training loss 0.08847341686487198 Validation loss 0.09669499099254608 Accuracy 0.706000030040741\n",
      "Iteration 36300 Training loss 0.10866132378578186 Validation loss 0.1047094538807869 Accuracy 0.6850000619888306\n",
      "Iteration 36310 Training loss 0.10192069411277771 Validation loss 0.09455936402082443 Accuracy 0.7330000400543213\n",
      "Iteration 36320 Training loss 0.10239087045192719 Validation loss 0.10261858254671097 Accuracy 0.6950000524520874\n",
      "Iteration 36330 Training loss 0.08143800497055054 Validation loss 0.09453055262565613 Accuracy 0.7320000529289246\n",
      "Iteration 36340 Training loss 0.07510349899530411 Validation loss 0.09403780847787857 Accuracy 0.721500039100647\n",
      "Iteration 36350 Training loss 0.07743625342845917 Validation loss 0.09399086236953735 Accuracy 0.737500011920929\n",
      "Iteration 36360 Training loss 0.09985607862472534 Validation loss 0.09956000000238419 Accuracy 0.6930000185966492\n",
      "Iteration 36370 Training loss 0.0970938503742218 Validation loss 0.09826678037643433 Accuracy 0.706000030040741\n",
      "Iteration 36380 Training loss 0.11003902554512024 Validation loss 0.09438921511173248 Accuracy 0.7235000133514404\n",
      "Iteration 36390 Training loss 0.08018286526203156 Validation loss 0.0919085144996643 Accuracy 0.7315000295639038\n",
      "Iteration 36400 Training loss 0.09141217172145844 Validation loss 0.09160912036895752 Accuracy 0.7260000109672546\n",
      "Iteration 36410 Training loss 0.09608835726976395 Validation loss 0.0960007756948471 Accuracy 0.7160000205039978\n",
      "Iteration 36420 Training loss 0.11765825003385544 Validation loss 0.10514003783464432 Accuracy 0.6585000157356262\n",
      "Iteration 36430 Training loss 0.10956905037164688 Validation loss 0.1145731583237648 Accuracy 0.6240000128746033\n",
      "Iteration 36440 Training loss 0.09420014172792435 Validation loss 0.09464894235134125 Accuracy 0.7225000262260437\n",
      "Iteration 36450 Training loss 0.08855076879262924 Validation loss 0.10811563581228256 Accuracy 0.6785000562667847\n",
      "Iteration 36460 Training loss 0.09381072968244553 Validation loss 0.10324075073003769 Accuracy 0.6650000214576721\n",
      "Iteration 36470 Training loss 0.07962293177843094 Validation loss 0.09149762243032455 Accuracy 0.7405000329017639\n",
      "Iteration 36480 Training loss 0.10070079565048218 Validation loss 0.09509433805942535 Accuracy 0.7195000052452087\n",
      "Iteration 36490 Training loss 0.12554089725017548 Validation loss 0.11885830014944077 Accuracy 0.6105000376701355\n",
      "Iteration 36500 Training loss 0.09979639947414398 Validation loss 0.10114145278930664 Accuracy 0.7015000581741333\n",
      "Iteration 36510 Training loss 0.07527133077383041 Validation loss 0.08997151255607605 Accuracy 0.7415000200271606\n",
      "Iteration 36520 Training loss 0.09514791518449783 Validation loss 0.09789421409368515 Accuracy 0.7070000171661377\n",
      "Iteration 36530 Training loss 0.08806344866752625 Validation loss 0.09455706924200058 Accuracy 0.7240000367164612\n",
      "Iteration 36540 Training loss 0.08872556686401367 Validation loss 0.09966503083705902 Accuracy 0.706000030040741\n",
      "Iteration 36550 Training loss 0.135359987616539 Validation loss 0.124676913022995 Accuracy 0.5990000367164612\n",
      "Iteration 36560 Training loss 0.10297694802284241 Validation loss 0.10043880343437195 Accuracy 0.7015000581741333\n",
      "Iteration 36570 Training loss 0.08534573018550873 Validation loss 0.09235221147537231 Accuracy 0.7300000190734863\n",
      "Iteration 36580 Training loss 0.08376608788967133 Validation loss 0.09853802621364594 Accuracy 0.7020000219345093\n",
      "Iteration 36590 Training loss 0.08630439639091492 Validation loss 0.10794589668512344 Accuracy 0.6500000357627869\n",
      "Iteration 36600 Training loss 0.08245989680290222 Validation loss 0.0941745862364769 Accuracy 0.7190000414848328\n",
      "Iteration 36610 Training loss 0.09600847214460373 Validation loss 0.09026237577199936 Accuracy 0.7275000214576721\n",
      "Iteration 36620 Training loss 0.1042671948671341 Validation loss 0.10204616189002991 Accuracy 0.6970000267028809\n",
      "Iteration 36630 Training loss 0.09744727611541748 Validation loss 0.09329275786876678 Accuracy 0.7295000553131104\n",
      "Iteration 36640 Training loss 0.1205691322684288 Validation loss 0.09537898749113083 Accuracy 0.7085000276565552\n",
      "Iteration 36650 Training loss 0.09467268735170364 Validation loss 0.09430953115224838 Accuracy 0.7160000205039978\n",
      "Iteration 36660 Training loss 0.08570075780153275 Validation loss 0.09728147089481354 Accuracy 0.7015000581741333\n",
      "Iteration 36670 Training loss 0.10304557532072067 Validation loss 0.09195047616958618 Accuracy 0.7260000109672546\n",
      "Iteration 36680 Training loss 0.08850812911987305 Validation loss 0.09228777140378952 Accuracy 0.7345000505447388\n",
      "Iteration 36690 Training loss 0.08665893226861954 Validation loss 0.09211397171020508 Accuracy 0.7245000600814819\n",
      "Iteration 36700 Training loss 0.10828620940446854 Validation loss 0.10581892728805542 Accuracy 0.6635000109672546\n",
      "Iteration 36710 Training loss 0.08703266084194183 Validation loss 0.09875074028968811 Accuracy 0.6970000267028809\n",
      "Iteration 36720 Training loss 0.10626579076051712 Validation loss 0.1015428900718689 Accuracy 0.687000036239624\n",
      "Iteration 36730 Training loss 0.09716212749481201 Validation loss 0.09599331766366959 Accuracy 0.7320000529289246\n",
      "Iteration 36740 Training loss 0.11384393274784088 Validation loss 0.09378357231616974 Accuracy 0.7290000319480896\n",
      "Iteration 36750 Training loss 0.09684506058692932 Validation loss 0.10038886964321136 Accuracy 0.7090000510215759\n",
      "Iteration 36760 Training loss 0.09959006309509277 Validation loss 0.09911590069532394 Accuracy 0.7005000114440918\n",
      "Iteration 36770 Training loss 0.09509439021348953 Validation loss 0.10110381245613098 Accuracy 0.7015000581741333\n",
      "Iteration 36780 Training loss 0.0871448665857315 Validation loss 0.09583349525928497 Accuracy 0.7255000472068787\n",
      "Iteration 36790 Training loss 0.09144870191812515 Validation loss 0.09223238378763199 Accuracy 0.7315000295639038\n",
      "Iteration 36800 Training loss 0.10349583625793457 Validation loss 0.10130301862955093 Accuracy 0.690000057220459\n",
      "Iteration 36810 Training loss 0.09841714799404144 Validation loss 0.10749725997447968 Accuracy 0.6495000123977661\n",
      "Iteration 36820 Training loss 0.10129014402627945 Validation loss 0.10828324407339096 Accuracy 0.659500002861023\n",
      "Iteration 36830 Training loss 0.10021168738603592 Validation loss 0.10174480080604553 Accuracy 0.6815000176429749\n",
      "Iteration 36840 Training loss 0.088596872985363 Validation loss 0.09349334985017776 Accuracy 0.7260000109672546\n",
      "Iteration 36850 Training loss 0.07810584455728531 Validation loss 0.09200035035610199 Accuracy 0.7285000085830688\n",
      "Iteration 36860 Training loss 0.09276562184095383 Validation loss 0.10462553799152374 Accuracy 0.6875000596046448\n",
      "Iteration 36870 Training loss 0.10471799969673157 Validation loss 0.11125369369983673 Accuracy 0.6295000314712524\n",
      "Iteration 36880 Training loss 0.07679752260446548 Validation loss 0.09214205294847488 Accuracy 0.7330000400543213\n",
      "Iteration 36890 Training loss 0.10182061046361923 Validation loss 0.0996280238032341 Accuracy 0.6945000290870667\n",
      "Iteration 36900 Training loss 0.07925301045179367 Validation loss 0.09436062723398209 Accuracy 0.737000048160553\n",
      "Iteration 36910 Training loss 0.11300276219844818 Validation loss 0.10565483570098877 Accuracy 0.6790000200271606\n",
      "Iteration 36920 Training loss 0.10023950040340424 Validation loss 0.11686218529939651 Accuracy 0.627500057220459\n",
      "Iteration 36930 Training loss 0.0987711101770401 Validation loss 0.09797318279743195 Accuracy 0.7065000534057617\n",
      "Iteration 36940 Training loss 0.08765383809804916 Validation loss 0.09287874400615692 Accuracy 0.7225000262260437\n",
      "Iteration 36950 Training loss 0.1044110655784607 Validation loss 0.0944819301366806 Accuracy 0.7250000238418579\n",
      "Iteration 36960 Training loss 0.1164407730102539 Validation loss 0.11323660612106323 Accuracy 0.64000004529953\n",
      "Iteration 36970 Training loss 0.10045458376407623 Validation loss 0.09635996073484421 Accuracy 0.7195000052452087\n",
      "Iteration 36980 Training loss 0.0976230800151825 Validation loss 0.10370425879955292 Accuracy 0.6790000200271606\n",
      "Iteration 36990 Training loss 0.11189942806959152 Validation loss 0.11101735383272171 Accuracy 0.6415000557899475\n",
      "Iteration 37000 Training loss 0.1031796932220459 Validation loss 0.10200782865285873 Accuracy 0.6920000314712524\n",
      "Iteration 37010 Training loss 0.09908061474561691 Validation loss 0.09921751916408539 Accuracy 0.6880000233650208\n",
      "Iteration 37020 Training loss 0.10833507031202316 Validation loss 0.09353531152009964 Accuracy 0.737000048160553\n",
      "Iteration 37030 Training loss 0.11444705724716187 Validation loss 0.10369206964969635 Accuracy 0.6865000128746033\n",
      "Iteration 37040 Training loss 0.10328845679759979 Validation loss 0.10137128084897995 Accuracy 0.6995000243186951\n",
      "Iteration 37050 Training loss 0.0894281193614006 Validation loss 0.0982043668627739 Accuracy 0.6985000371932983\n",
      "Iteration 37060 Training loss 0.09898176044225693 Validation loss 0.10169251263141632 Accuracy 0.6920000314712524\n",
      "Iteration 37070 Training loss 0.10656746476888657 Validation loss 0.10295788943767548 Accuracy 0.6925000548362732\n",
      "Iteration 37080 Training loss 0.08186176419258118 Validation loss 0.09650826454162598 Accuracy 0.7260000109672546\n",
      "Iteration 37090 Training loss 0.07662516087293625 Validation loss 0.0914653092622757 Accuracy 0.7440000176429749\n",
      "Iteration 37100 Training loss 0.0943925678730011 Validation loss 0.09467125684022903 Accuracy 0.7255000472068787\n",
      "Iteration 37110 Training loss 0.09850795567035675 Validation loss 0.09194887429475784 Accuracy 0.7470000386238098\n",
      "Iteration 37120 Training loss 0.09764362126588821 Validation loss 0.09084010124206543 Accuracy 0.7465000152587891\n",
      "Iteration 37130 Training loss 0.12145679444074631 Validation loss 0.09786978363990784 Accuracy 0.6975000500679016\n",
      "Iteration 37140 Training loss 0.10407768189907074 Validation loss 0.10066254436969757 Accuracy 0.7035000324249268\n",
      "Iteration 37150 Training loss 0.10030212253332138 Validation loss 0.09998060762882233 Accuracy 0.7070000171661377\n",
      "Iteration 37160 Training loss 0.09269784390926361 Validation loss 0.10698527097702026 Accuracy 0.6505000591278076\n",
      "Iteration 37170 Training loss 0.09411460906267166 Validation loss 0.10052399337291718 Accuracy 0.7020000219345093\n",
      "Iteration 37180 Training loss 0.1036648154258728 Validation loss 0.10035677254199982 Accuracy 0.6990000605583191\n",
      "Iteration 37190 Training loss 0.10885152220726013 Validation loss 0.09380238503217697 Accuracy 0.7275000214576721\n",
      "Iteration 37200 Training loss 0.07975733280181885 Validation loss 0.0964767336845398 Accuracy 0.718500018119812\n",
      "Iteration 37210 Training loss 0.10482696443796158 Validation loss 0.10303701460361481 Accuracy 0.6685000061988831\n",
      "Iteration 37220 Training loss 0.1242106631398201 Validation loss 0.12121117115020752 Accuracy 0.6270000338554382\n",
      "Iteration 37230 Training loss 0.09411836415529251 Validation loss 0.10024338215589523 Accuracy 0.6970000267028809\n",
      "Iteration 37240 Training loss 0.11808311194181442 Validation loss 0.09567197412252426 Accuracy 0.7240000367164612\n",
      "Iteration 37250 Training loss 0.09523626416921616 Validation loss 0.10214397311210632 Accuracy 0.6930000185966492\n",
      "Iteration 37260 Training loss 0.09970288723707199 Validation loss 0.09866566210985184 Accuracy 0.7110000252723694\n",
      "Iteration 37270 Training loss 0.08459746092557907 Validation loss 0.09509189426898956 Accuracy 0.7170000076293945\n",
      "Iteration 37280 Training loss 0.09356366842985153 Validation loss 0.09775841981172562 Accuracy 0.6955000162124634\n",
      "Iteration 37290 Training loss 0.08380606770515442 Validation loss 0.10133000463247299 Accuracy 0.6925000548362732\n",
      "Iteration 37300 Training loss 0.08867047727108002 Validation loss 0.0967719554901123 Accuracy 0.7120000123977661\n",
      "Iteration 37310 Training loss 0.09935226291418076 Validation loss 0.09488417953252792 Accuracy 0.7245000600814819\n",
      "Iteration 37320 Training loss 0.0963938757777214 Validation loss 0.09921113401651382 Accuracy 0.7095000147819519\n",
      "Iteration 37330 Training loss 0.09621188789606094 Validation loss 0.10161537677049637 Accuracy 0.6965000033378601\n",
      "Iteration 37340 Training loss 0.09548898786306381 Validation loss 0.09506860375404358 Accuracy 0.7120000123977661\n",
      "Iteration 37350 Training loss 0.1159314289689064 Validation loss 0.09383281320333481 Accuracy 0.7310000061988831\n",
      "Iteration 37360 Training loss 0.10686290264129639 Validation loss 0.10655808448791504 Accuracy 0.6545000076293945\n",
      "Iteration 37370 Training loss 0.08876198530197144 Validation loss 0.09544133394956589 Accuracy 0.7175000309944153\n",
      "Iteration 37380 Training loss 0.0915551707148552 Validation loss 0.09808497130870819 Accuracy 0.7170000076293945\n",
      "Iteration 37390 Training loss 0.08960889279842377 Validation loss 0.09699408710002899 Accuracy 0.7140000462532043\n",
      "Iteration 37400 Training loss 0.08418955653905869 Validation loss 0.0916643887758255 Accuracy 0.7420000433921814\n",
      "Iteration 37410 Training loss 0.08794292062520981 Validation loss 0.0944933369755745 Accuracy 0.7220000624656677\n",
      "Iteration 37420 Training loss 0.08904395997524261 Validation loss 0.10424483567476273 Accuracy 0.6695000529289246\n",
      "Iteration 37430 Training loss 0.08151630312204361 Validation loss 0.09171438962221146 Accuracy 0.7295000553131104\n",
      "Iteration 37440 Training loss 0.09936286509037018 Validation loss 0.10461121797561646 Accuracy 0.6705000400543213\n",
      "Iteration 37450 Training loss 0.0844346284866333 Validation loss 0.10016746819019318 Accuracy 0.7155000567436218\n",
      "Iteration 37460 Training loss 0.09431301057338715 Validation loss 0.09561244398355484 Accuracy 0.7155000567436218\n",
      "Iteration 37470 Training loss 0.08181004226207733 Validation loss 0.09298481792211533 Accuracy 0.7175000309944153\n",
      "Iteration 37480 Training loss 0.08127155154943466 Validation loss 0.09324663877487183 Accuracy 0.7105000615119934\n",
      "Iteration 37490 Training loss 0.08752451092004776 Validation loss 0.10139049589633942 Accuracy 0.6805000305175781\n",
      "Iteration 37500 Training loss 0.124082051217556 Validation loss 0.10791619122028351 Accuracy 0.6650000214576721\n",
      "Iteration 37510 Training loss 0.095676951110363 Validation loss 0.09891301393508911 Accuracy 0.70250004529953\n",
      "Iteration 37520 Training loss 0.10181077569723129 Validation loss 0.11121504008769989 Accuracy 0.6315000057220459\n",
      "Iteration 37530 Training loss 0.1062856912612915 Validation loss 0.10699695348739624 Accuracy 0.6625000238418579\n",
      "Iteration 37540 Training loss 0.10020433366298676 Validation loss 0.09943657368421555 Accuracy 0.7150000333786011\n",
      "Iteration 37550 Training loss 0.09184027463197708 Validation loss 0.0972280278801918 Accuracy 0.718000054359436\n",
      "Iteration 37560 Training loss 0.11385878920555115 Validation loss 0.09785063564777374 Accuracy 0.7135000228881836\n",
      "Iteration 37570 Training loss 0.09867943823337555 Validation loss 0.09660849720239639 Accuracy 0.734000027179718\n",
      "Iteration 37580 Training loss 0.09175796061754227 Validation loss 0.09190712869167328 Accuracy 0.7365000247955322\n",
      "Iteration 37590 Training loss 0.08926117420196533 Validation loss 0.09879476577043533 Accuracy 0.6970000267028809\n",
      "Iteration 37600 Training loss 0.09669723361730576 Validation loss 0.09493895620107651 Accuracy 0.718000054359436\n",
      "Iteration 37610 Training loss 0.10454560816287994 Validation loss 0.09814009815454483 Accuracy 0.7165000438690186\n",
      "Iteration 37620 Training loss 0.08700521290302277 Validation loss 0.09611152112483978 Accuracy 0.7260000109672546\n",
      "Iteration 37630 Training loss 0.09071578830480576 Validation loss 0.09441156685352325 Accuracy 0.7355000376701355\n",
      "Iteration 37640 Training loss 0.08190863579511642 Validation loss 0.09632989764213562 Accuracy 0.7075000405311584\n",
      "Iteration 37650 Training loss 0.08205309510231018 Validation loss 0.0935528352856636 Accuracy 0.7400000095367432\n",
      "Iteration 37660 Training loss 0.10181788355112076 Validation loss 0.09519093483686447 Accuracy 0.7315000295639038\n",
      "Iteration 37670 Training loss 0.11514007300138474 Validation loss 0.10510125011205673 Accuracy 0.6755000352859497\n",
      "Iteration 37680 Training loss 0.08580508828163147 Validation loss 0.10138490796089172 Accuracy 0.7010000348091125\n",
      "Iteration 37690 Training loss 0.09010319411754608 Validation loss 0.09768155217170715 Accuracy 0.7070000171661377\n",
      "Iteration 37700 Training loss 0.08520825207233429 Validation loss 0.09599688649177551 Accuracy 0.7245000600814819\n",
      "Iteration 37710 Training loss 0.07918499410152435 Validation loss 0.09070153534412384 Accuracy 0.7405000329017639\n",
      "Iteration 37720 Training loss 0.09537407755851746 Validation loss 0.10257379710674286 Accuracy 0.6945000290870667\n",
      "Iteration 37730 Training loss 0.08065040409564972 Validation loss 0.09264441579580307 Accuracy 0.734000027179718\n",
      "Iteration 37740 Training loss 0.09080390632152557 Validation loss 0.10056125372648239 Accuracy 0.6920000314712524\n",
      "Iteration 37750 Training loss 0.09231037646532059 Validation loss 0.10350583493709564 Accuracy 0.6760000586509705\n",
      "Iteration 37760 Training loss 0.09432213753461838 Validation loss 0.09533615410327911 Accuracy 0.7270000576972961\n",
      "Iteration 37770 Training loss 0.10442925989627838 Validation loss 0.10754814743995667 Accuracy 0.6540000438690186\n",
      "Iteration 37780 Training loss 0.10771919786930084 Validation loss 0.09760481864213943 Accuracy 0.7170000076293945\n",
      "Iteration 37790 Training loss 0.09790471196174622 Validation loss 0.0944594219326973 Accuracy 0.7325000166893005\n",
      "Iteration 37800 Training loss 0.08789186924695969 Validation loss 0.09483975172042847 Accuracy 0.7200000286102295\n",
      "Iteration 37810 Training loss 0.10399910062551498 Validation loss 0.10351476818323135 Accuracy 0.6810000538825989\n",
      "Iteration 37820 Training loss 0.0928160771727562 Validation loss 0.09815419465303421 Accuracy 0.7115000486373901\n",
      "Iteration 37830 Training loss 0.08833006769418716 Validation loss 0.09543163329362869 Accuracy 0.7190000414848328\n",
      "Iteration 37840 Training loss 0.10433781147003174 Validation loss 0.09725181013345718 Accuracy 0.7145000100135803\n",
      "Iteration 37850 Training loss 0.09483655542135239 Validation loss 0.09746668487787247 Accuracy 0.7090000510215759\n",
      "Iteration 37860 Training loss 0.09571393579244614 Validation loss 0.09167016297578812 Accuracy 0.7415000200271606\n",
      "Iteration 37870 Training loss 0.10724086314439774 Validation loss 0.10153084248304367 Accuracy 0.7015000581741333\n",
      "Iteration 37880 Training loss 0.10540605336427689 Validation loss 0.10415070503950119 Accuracy 0.6955000162124634\n",
      "Iteration 37890 Training loss 0.11284556239843369 Validation loss 0.10230714827775955 Accuracy 0.6940000057220459\n",
      "Iteration 37900 Training loss 0.10531146824359894 Validation loss 0.10126590728759766 Accuracy 0.6845000386238098\n",
      "Iteration 37910 Training loss 0.07235865294933319 Validation loss 0.09223557263612747 Accuracy 0.7440000176429749\n",
      "Iteration 37920 Training loss 0.08923038095235825 Validation loss 0.09768121689558029 Accuracy 0.7015000581741333\n",
      "Iteration 37930 Training loss 0.09661371260881424 Validation loss 0.09976274520158768 Accuracy 0.7190000414848328\n",
      "Iteration 37940 Training loss 0.07603263109922409 Validation loss 0.09108366817235947 Accuracy 0.7405000329017639\n",
      "Iteration 37950 Training loss 0.11029510200023651 Validation loss 0.10494644939899445 Accuracy 0.6775000095367432\n",
      "Iteration 37960 Training loss 0.10922899097204208 Validation loss 0.10241207480430603 Accuracy 0.6845000386238098\n",
      "Iteration 37970 Training loss 0.08943375200033188 Validation loss 0.09347197413444519 Accuracy 0.7255000472068787\n",
      "Iteration 37980 Training loss 0.08999594300985336 Validation loss 0.09448712319135666 Accuracy 0.7240000367164612\n",
      "Iteration 37990 Training loss 0.10377227514982224 Validation loss 0.09629661589860916 Accuracy 0.7170000076293945\n",
      "Iteration 38000 Training loss 0.10290367156267166 Validation loss 0.10143250972032547 Accuracy 0.6910000443458557\n",
      "Iteration 38010 Training loss 0.10586369037628174 Validation loss 0.10103066265583038 Accuracy 0.7140000462532043\n",
      "Iteration 38020 Training loss 0.09290096908807755 Validation loss 0.09483690559864044 Accuracy 0.7235000133514404\n",
      "Iteration 38030 Training loss 0.09668643027544022 Validation loss 0.09247615188360214 Accuracy 0.7445000410079956\n",
      "Iteration 38040 Training loss 0.09414580464363098 Validation loss 0.09335677325725555 Accuracy 0.7275000214576721\n",
      "Iteration 38050 Training loss 0.09468994289636612 Validation loss 0.09471329301595688 Accuracy 0.734000027179718\n",
      "Iteration 38060 Training loss 0.1071130707859993 Validation loss 0.1013905256986618 Accuracy 0.6910000443458557\n",
      "Iteration 38070 Training loss 0.10147450864315033 Validation loss 0.10285790264606476 Accuracy 0.6850000619888306\n",
      "Iteration 38080 Training loss 0.11729896068572998 Validation loss 0.1141822561621666 Accuracy 0.6540000438690186\n",
      "Iteration 38090 Training loss 0.10001417249441147 Validation loss 0.1018081083893776 Accuracy 0.690000057220459\n",
      "Iteration 38100 Training loss 0.09240631014108658 Validation loss 0.09826860576868057 Accuracy 0.70250004529953\n",
      "Iteration 38110 Training loss 0.08660409599542618 Validation loss 0.09479835629463196 Accuracy 0.7355000376701355\n",
      "Iteration 38120 Training loss 0.10585904866456985 Validation loss 0.10384205728769302 Accuracy 0.6790000200271606\n",
      "Iteration 38130 Training loss 0.08082178235054016 Validation loss 0.09379611909389496 Accuracy 0.7195000052452087\n",
      "Iteration 38140 Training loss 0.0885726734995842 Validation loss 0.10168687254190445 Accuracy 0.70250004529953\n",
      "Iteration 38150 Training loss 0.07720889896154404 Validation loss 0.09369151294231415 Accuracy 0.733500063419342\n",
      "Iteration 38160 Training loss 0.0999976173043251 Validation loss 0.09381801635026932 Accuracy 0.7410000562667847\n",
      "Iteration 38170 Training loss 0.09655138105154037 Validation loss 0.10178613662719727 Accuracy 0.6785000562667847\n",
      "Iteration 38180 Training loss 0.09467790275812149 Validation loss 0.09490567445755005 Accuracy 0.7275000214576721\n",
      "Iteration 38190 Training loss 0.10845286399126053 Validation loss 0.10288780182600021 Accuracy 0.6760000586509705\n",
      "Iteration 38200 Training loss 0.08693358302116394 Validation loss 0.09519397467374802 Accuracy 0.7260000109672546\n",
      "Iteration 38210 Training loss 0.08707384020090103 Validation loss 0.10069810599088669 Accuracy 0.6970000267028809\n",
      "Iteration 38220 Training loss 0.12511490285396576 Validation loss 0.10124558955430984 Accuracy 0.7010000348091125\n",
      "Iteration 38230 Training loss 0.07048528641462326 Validation loss 0.09759500622749329 Accuracy 0.6985000371932983\n",
      "Iteration 38240 Training loss 0.10271935164928436 Validation loss 0.09550995379686356 Accuracy 0.7310000061988831\n",
      "Iteration 38250 Training loss 0.10624942183494568 Validation loss 0.10697301477193832 Accuracy 0.6725000143051147\n",
      "Iteration 38260 Training loss 0.10703946650028229 Validation loss 0.0986609235405922 Accuracy 0.703000009059906\n",
      "Iteration 38270 Training loss 0.09024151414632797 Validation loss 0.09535352140665054 Accuracy 0.7175000309944153\n",
      "Iteration 38280 Training loss 0.09472353756427765 Validation loss 0.09412988275289536 Accuracy 0.7235000133514404\n",
      "Iteration 38290 Training loss 0.1009448990225792 Validation loss 0.09556969255208969 Accuracy 0.7225000262260437\n",
      "Iteration 38300 Training loss 0.11764875799417496 Validation loss 0.13443952798843384 Accuracy 0.5680000185966492\n",
      "Iteration 38310 Training loss 0.10002467781305313 Validation loss 0.09910041093826294 Accuracy 0.6970000267028809\n",
      "Iteration 38320 Training loss 0.0889214426279068 Validation loss 0.0945717915892601 Accuracy 0.7255000472068787\n",
      "Iteration 38330 Training loss 0.08861184865236282 Validation loss 0.10003625601530075 Accuracy 0.6935000419616699\n",
      "Iteration 38340 Training loss 0.09699294716119766 Validation loss 0.1038854569196701 Accuracy 0.6760000586509705\n",
      "Iteration 38350 Training loss 0.10264421254396439 Validation loss 0.09265294671058655 Accuracy 0.7250000238418579\n",
      "Iteration 38360 Training loss 0.10870001465082169 Validation loss 0.10226578265428543 Accuracy 0.6835000514984131\n",
      "Iteration 38370 Training loss 0.09682280570268631 Validation loss 0.09569572657346725 Accuracy 0.7110000252723694\n",
      "Iteration 38380 Training loss 0.08957467973232269 Validation loss 0.09352952986955643 Accuracy 0.7290000319480896\n",
      "Iteration 38390 Training loss 0.09641160815954208 Validation loss 0.09382124990224838 Accuracy 0.7345000505447388\n",
      "Iteration 38400 Training loss 0.10405983775854111 Validation loss 0.09751778095960617 Accuracy 0.7120000123977661\n",
      "Iteration 38410 Training loss 0.12208808958530426 Validation loss 0.1079026535153389 Accuracy 0.6600000262260437\n",
      "Iteration 38420 Training loss 0.09653102606534958 Validation loss 0.0989261344075203 Accuracy 0.7035000324249268\n",
      "Iteration 38430 Training loss 0.09289254993200302 Validation loss 0.09810448437929153 Accuracy 0.706000030040741\n",
      "Iteration 38440 Training loss 0.10831161588430405 Validation loss 0.09684575349092484 Accuracy 0.7070000171661377\n",
      "Iteration 38450 Training loss 0.09241676330566406 Validation loss 0.09736162424087524 Accuracy 0.7255000472068787\n",
      "Iteration 38460 Training loss 0.09042666852474213 Validation loss 0.1049625501036644 Accuracy 0.6735000610351562\n",
      "Iteration 38470 Training loss 0.11834526062011719 Validation loss 0.11568254977464676 Accuracy 0.6385000348091125\n",
      "Iteration 38480 Training loss 0.08933185040950775 Validation loss 0.10073183476924896 Accuracy 0.6935000419616699\n",
      "Iteration 38490 Training loss 0.09870117157697678 Validation loss 0.10038550198078156 Accuracy 0.6975000500679016\n",
      "Iteration 38500 Training loss 0.08766334503889084 Validation loss 0.10401991754770279 Accuracy 0.6810000538825989\n",
      "Iteration 38510 Training loss 0.10640658438205719 Validation loss 0.10121692717075348 Accuracy 0.6950000524520874\n",
      "Iteration 38520 Training loss 0.10497874766588211 Validation loss 0.10489296168088913 Accuracy 0.6780000329017639\n",
      "Iteration 38530 Training loss 0.09251260757446289 Validation loss 0.0957292914390564 Accuracy 0.7105000615119934\n",
      "Iteration 38540 Training loss 0.1023409515619278 Validation loss 0.09445498883724213 Accuracy 0.7315000295639038\n",
      "Iteration 38550 Training loss 0.08920963108539581 Validation loss 0.0965384766459465 Accuracy 0.7210000157356262\n",
      "Iteration 38560 Training loss 0.135696679353714 Validation loss 0.13502876460552216 Accuracy 0.5430000424385071\n",
      "Iteration 38570 Training loss 0.11794985830783844 Validation loss 0.11912453174591064 Accuracy 0.6135000586509705\n",
      "Iteration 38580 Training loss 0.08430556952953339 Validation loss 0.0981661006808281 Accuracy 0.7050000429153442\n",
      "Iteration 38590 Training loss 0.08555348962545395 Validation loss 0.09529713541269302 Accuracy 0.734000027179718\n",
      "Iteration 38600 Training loss 0.08084136992692947 Validation loss 0.09390492737293243 Accuracy 0.7320000529289246\n",
      "Iteration 38610 Training loss 0.0803600400686264 Validation loss 0.09428830444812775 Accuracy 0.7245000600814819\n",
      "Iteration 38620 Training loss 0.10439552366733551 Validation loss 0.10556163638830185 Accuracy 0.675000011920929\n",
      "Iteration 38630 Training loss 0.09150920808315277 Validation loss 0.09766189754009247 Accuracy 0.7100000381469727\n",
      "Iteration 38640 Training loss 0.09099769592285156 Validation loss 0.10006769001483917 Accuracy 0.7000000476837158\n",
      "Iteration 38650 Training loss 0.11098994314670563 Validation loss 0.10526783019304276 Accuracy 0.674500048160553\n",
      "Iteration 38660 Training loss 0.10779153555631638 Validation loss 0.09069747477769852 Accuracy 0.7440000176429749\n",
      "Iteration 38670 Training loss 0.08764123916625977 Validation loss 0.09251303225755692 Accuracy 0.7260000109672546\n",
      "Iteration 38680 Training loss 0.07970593869686127 Validation loss 0.09693628549575806 Accuracy 0.7250000238418579\n",
      "Iteration 38690 Training loss 0.09664332121610641 Validation loss 0.10237256437540054 Accuracy 0.6930000185966492\n",
      "Iteration 38700 Training loss 0.09880097210407257 Validation loss 0.09637118875980377 Accuracy 0.7240000367164612\n",
      "Iteration 38710 Training loss 0.14862462878227234 Validation loss 0.13358959555625916 Accuracy 0.5195000171661377\n",
      "Iteration 38720 Training loss 0.09636491537094116 Validation loss 0.10450195521116257 Accuracy 0.675000011920929\n",
      "Iteration 38730 Training loss 0.07329896837472916 Validation loss 0.0978187620639801 Accuracy 0.7015000581741333\n",
      "Iteration 38740 Training loss 0.10498149693012238 Validation loss 0.09830265492200851 Accuracy 0.7080000042915344\n",
      "Iteration 38750 Training loss 0.11363868415355682 Validation loss 0.10120650380849838 Accuracy 0.6930000185966492\n",
      "Iteration 38760 Training loss 0.09384229779243469 Validation loss 0.09770850092172623 Accuracy 0.7055000066757202\n",
      "Iteration 38770 Training loss 0.10428574681282043 Validation loss 0.09679243713617325 Accuracy 0.7205000519752502\n",
      "Iteration 38780 Training loss 0.11465740203857422 Validation loss 0.09697455167770386 Accuracy 0.7150000333786011\n",
      "Iteration 38790 Training loss 0.0958816334605217 Validation loss 0.09985765069723129 Accuracy 0.703000009059906\n",
      "Iteration 38800 Training loss 0.0920909121632576 Validation loss 0.09383310377597809 Accuracy 0.7360000610351562\n",
      "Iteration 38810 Training loss 0.09772799909114838 Validation loss 0.09859848022460938 Accuracy 0.7205000519752502\n",
      "Iteration 38820 Training loss 0.09272108227014542 Validation loss 0.09853615611791611 Accuracy 0.7095000147819519\n",
      "Iteration 38830 Training loss 0.10475240647792816 Validation loss 0.09669038653373718 Accuracy 0.7350000143051147\n",
      "Iteration 38840 Training loss 0.07928112149238586 Validation loss 0.10147706419229507 Accuracy 0.6990000605583191\n",
      "Iteration 38850 Training loss 0.08419942855834961 Validation loss 0.09318528324365616 Accuracy 0.7295000553131104\n",
      "Iteration 38860 Training loss 0.0946989431977272 Validation loss 0.09384185075759888 Accuracy 0.7265000343322754\n",
      "Iteration 38870 Training loss 0.09017613530158997 Validation loss 0.10432883352041245 Accuracy 0.690000057220459\n",
      "Iteration 38880 Training loss 0.0947093591094017 Validation loss 0.10019481182098389 Accuracy 0.7015000581741333\n",
      "Iteration 38890 Training loss 0.09234905242919922 Validation loss 0.09682051837444305 Accuracy 0.7090000510215759\n",
      "Iteration 38900 Training loss 0.09099464863538742 Validation loss 0.09595731645822525 Accuracy 0.7160000205039978\n",
      "Iteration 38910 Training loss 0.08774905651807785 Validation loss 0.09627319872379303 Accuracy 0.7225000262260437\n",
      "Iteration 38920 Training loss 0.09265139698982239 Validation loss 0.09550092369318008 Accuracy 0.7365000247955322\n",
      "Iteration 38930 Training loss 0.09029630571603775 Validation loss 0.09328358620405197 Accuracy 0.7385000586509705\n",
      "Iteration 38940 Training loss 0.09125407785177231 Validation loss 0.0929999127984047 Accuracy 0.7365000247955322\n",
      "Iteration 38950 Training loss 0.1001485213637352 Validation loss 0.10627500712871552 Accuracy 0.6585000157356262\n",
      "Iteration 38960 Training loss 0.10210287570953369 Validation loss 0.09457631409168243 Accuracy 0.737000048160553\n",
      "Iteration 38970 Training loss 0.09038110822439194 Validation loss 0.0938899889588356 Accuracy 0.7380000352859497\n",
      "Iteration 38980 Training loss 0.11912114173173904 Validation loss 0.10406525433063507 Accuracy 0.6955000162124634\n",
      "Iteration 38990 Training loss 0.08982458710670471 Validation loss 0.09882587939500809 Accuracy 0.706000030040741\n",
      "Iteration 39000 Training loss 0.1077117994427681 Validation loss 0.10106083005666733 Accuracy 0.6960000395774841\n",
      "Iteration 39010 Training loss 0.09458965808153152 Validation loss 0.10358434170484543 Accuracy 0.6700000166893005\n",
      "Iteration 39020 Training loss 0.09137514233589172 Validation loss 0.09494751691818237 Accuracy 0.7345000505447388\n",
      "Iteration 39030 Training loss 0.10615724325180054 Validation loss 0.09422127902507782 Accuracy 0.7305000424385071\n",
      "Iteration 39040 Training loss 0.09362617880105972 Validation loss 0.10585355758666992 Accuracy 0.659500002861023\n",
      "Iteration 39050 Training loss 0.09648259729146957 Validation loss 0.10043111443519592 Accuracy 0.6975000500679016\n",
      "Iteration 39060 Training loss 0.09304466098546982 Validation loss 0.09988119453191757 Accuracy 0.6955000162124634\n",
      "Iteration 39070 Training loss 0.0883994773030281 Validation loss 0.09625522047281265 Accuracy 0.7230000495910645\n",
      "Iteration 39080 Training loss 0.07896260172128677 Validation loss 0.10067982971668243 Accuracy 0.6950000524520874\n",
      "Iteration 39090 Training loss 0.08027717471122742 Validation loss 0.09457305073738098 Accuracy 0.7275000214576721\n",
      "Iteration 39100 Training loss 0.07709598541259766 Validation loss 0.09408600628376007 Accuracy 0.7250000238418579\n",
      "Iteration 39110 Training loss 0.09844367951154709 Validation loss 0.10108741372823715 Accuracy 0.690500020980835\n",
      "Iteration 39120 Training loss 0.09267408400774002 Validation loss 0.0951521024107933 Accuracy 0.7210000157356262\n",
      "Iteration 39130 Training loss 0.09121004492044449 Validation loss 0.09294763952493668 Accuracy 0.7390000224113464\n",
      "Iteration 39140 Training loss 0.14598028361797333 Validation loss 0.1465306133031845 Accuracy 0.6115000247955322\n",
      "Iteration 39150 Training loss 0.10876088589429855 Validation loss 0.09637117385864258 Accuracy 0.7300000190734863\n",
      "Iteration 39160 Training loss 0.1004006564617157 Validation loss 0.09848588705062866 Accuracy 0.7010000348091125\n",
      "Iteration 39170 Training loss 0.08849669992923737 Validation loss 0.09476007521152496 Accuracy 0.7360000610351562\n",
      "Iteration 39180 Training loss 0.09245375543832779 Validation loss 0.09334436804056168 Accuracy 0.7315000295639038\n",
      "Iteration 39190 Training loss 0.09475604444742203 Validation loss 0.10315009206533432 Accuracy 0.6845000386238098\n",
      "Iteration 39200 Training loss 0.11116471886634827 Validation loss 0.09484392404556274 Accuracy 0.7325000166893005\n",
      "Iteration 39210 Training loss 0.08601976186037064 Validation loss 0.09323949366807938 Accuracy 0.7345000505447388\n",
      "Iteration 39220 Training loss 0.10598833113908768 Validation loss 0.0915592834353447 Accuracy 0.737500011920929\n",
      "Iteration 39230 Training loss 0.09899095445871353 Validation loss 0.09703182429075241 Accuracy 0.7145000100135803\n",
      "Iteration 39240 Training loss 0.10429765284061432 Validation loss 0.09274075925350189 Accuracy 0.7290000319480896\n",
      "Iteration 39250 Training loss 0.10457832366228104 Validation loss 0.09491078555583954 Accuracy 0.7205000519752502\n",
      "Iteration 39260 Training loss 0.09640009701251984 Validation loss 0.10526345670223236 Accuracy 0.6690000295639038\n",
      "Iteration 39270 Training loss 0.09179869294166565 Validation loss 0.09708025306463242 Accuracy 0.7140000462532043\n",
      "Iteration 39280 Training loss 0.09809351712465286 Validation loss 0.10910569876432419 Accuracy 0.6390000581741333\n",
      "Iteration 39290 Training loss 0.10388223081827164 Validation loss 0.10816726088523865 Accuracy 0.6630000472068787\n",
      "Iteration 39300 Training loss 0.07771965116262436 Validation loss 0.09810926020145416 Accuracy 0.7080000042915344\n",
      "Iteration 39310 Training loss 0.0908035859465599 Validation loss 0.0945117250084877 Accuracy 0.7270000576972961\n",
      "Iteration 39320 Training loss 0.10431865602731705 Validation loss 0.09552007913589478 Accuracy 0.7210000157356262\n",
      "Iteration 39330 Training loss 0.08850394934415817 Validation loss 0.093550905585289 Accuracy 0.737500011920929\n",
      "Iteration 39340 Training loss 0.1030873954296112 Validation loss 0.09565123170614243 Accuracy 0.7255000472068787\n",
      "Iteration 39350 Training loss 0.0973629578948021 Validation loss 0.09533730149269104 Accuracy 0.7225000262260437\n",
      "Iteration 39360 Training loss 0.09815646708011627 Validation loss 0.10560812801122665 Accuracy 0.6820000410079956\n",
      "Iteration 39370 Training loss 0.08971963822841644 Validation loss 0.09623097628355026 Accuracy 0.7080000042915344\n",
      "Iteration 39380 Training loss 0.0847812220454216 Validation loss 0.09547103941440582 Accuracy 0.7175000309944153\n",
      "Iteration 39390 Training loss 0.0966443344950676 Validation loss 0.09759049117565155 Accuracy 0.7150000333786011\n",
      "Iteration 39400 Training loss 0.10364141315221786 Validation loss 0.1131608635187149 Accuracy 0.6515000462532043\n",
      "Iteration 39410 Training loss 0.10445346683263779 Validation loss 0.0944836437702179 Accuracy 0.7405000329017639\n",
      "Iteration 39420 Training loss 0.09062482416629791 Validation loss 0.09546088427305222 Accuracy 0.7080000042915344\n",
      "Iteration 39430 Training loss 0.10619630664587021 Validation loss 0.09814854711294174 Accuracy 0.70250004529953\n",
      "Iteration 39440 Training loss 0.13186907768249512 Validation loss 0.12202133983373642 Accuracy 0.628000020980835\n",
      "Iteration 39450 Training loss 0.08400554209947586 Validation loss 0.09540125727653503 Accuracy 0.7250000238418579\n",
      "Iteration 39460 Training loss 0.09705478698015213 Validation loss 0.09368660300970078 Accuracy 0.7265000343322754\n",
      "Iteration 39470 Training loss 0.08557983487844467 Validation loss 0.09240972995758057 Accuracy 0.7350000143051147\n",
      "Iteration 39480 Training loss 0.10054996609687805 Validation loss 0.1053779199719429 Accuracy 0.6770000457763672\n",
      "Iteration 39490 Training loss 0.10546470433473587 Validation loss 0.10019589960575104 Accuracy 0.7130000591278076\n",
      "Iteration 39500 Training loss 0.12253434956073761 Validation loss 0.10106566548347473 Accuracy 0.7005000114440918\n",
      "Iteration 39510 Training loss 0.0997157171368599 Validation loss 0.11383574455976486 Accuracy 0.6300000548362732\n",
      "Iteration 39520 Training loss 0.07981999963521957 Validation loss 0.09737709164619446 Accuracy 0.7070000171661377\n",
      "Iteration 39530 Training loss 0.09874791651964188 Validation loss 0.09653116017580032 Accuracy 0.7240000367164612\n",
      "Iteration 39540 Training loss 0.09510549157857895 Validation loss 0.11011481285095215 Accuracy 0.64000004529953\n",
      "Iteration 39550 Training loss 0.08565522730350494 Validation loss 0.09628897905349731 Accuracy 0.7195000052452087\n",
      "Iteration 39560 Training loss 0.10414447635412216 Validation loss 0.09596461057662964 Accuracy 0.7125000357627869\n",
      "Iteration 39570 Training loss 0.10526371002197266 Validation loss 0.09504041820764542 Accuracy 0.7305000424385071\n",
      "Iteration 39580 Training loss 0.11217464506626129 Validation loss 0.11565012484788895 Accuracy 0.6520000100135803\n",
      "Iteration 39590 Training loss 0.10490617156028748 Validation loss 0.10617279261350632 Accuracy 0.675000011920929\n",
      "Iteration 39600 Training loss 0.09405610710382462 Validation loss 0.09575415402650833 Accuracy 0.7235000133514404\n",
      "Iteration 39610 Training loss 0.11652448028326035 Validation loss 0.09815837442874908 Accuracy 0.7110000252723694\n",
      "Iteration 39620 Training loss 0.08753759413957596 Validation loss 0.09595952183008194 Accuracy 0.7270000576972961\n",
      "Iteration 39630 Training loss 0.11734073609113693 Validation loss 0.10400832444429398 Accuracy 0.6755000352859497\n",
      "Iteration 39640 Training loss 0.11373773217201233 Validation loss 0.10606369376182556 Accuracy 0.6765000224113464\n",
      "Iteration 39650 Training loss 0.09114429354667664 Validation loss 0.09688243269920349 Accuracy 0.7155000567436218\n",
      "Iteration 39660 Training loss 0.10799039900302887 Validation loss 0.10133972764015198 Accuracy 0.6965000033378601\n",
      "Iteration 39670 Training loss 0.09193741530179977 Validation loss 0.09958697855472565 Accuracy 0.6990000605583191\n",
      "Iteration 39680 Training loss 0.1008169949054718 Validation loss 0.09971391409635544 Accuracy 0.7130000591278076\n",
      "Iteration 39690 Training loss 0.123773954808712 Validation loss 0.12095827609300613 Accuracy 0.627500057220459\n",
      "Iteration 39700 Training loss 0.0863909125328064 Validation loss 0.09360422939062119 Accuracy 0.7365000247955322\n",
      "Iteration 39710 Training loss 0.09529107064008713 Validation loss 0.09527187049388885 Accuracy 0.7270000576972961\n",
      "Iteration 39720 Training loss 0.08632122725248337 Validation loss 0.09595433622598648 Accuracy 0.7245000600814819\n",
      "Iteration 39730 Training loss 0.09847676008939743 Validation loss 0.0937856063246727 Accuracy 0.7320000529289246\n",
      "Iteration 39740 Training loss 0.09797675907611847 Validation loss 0.0952945277094841 Accuracy 0.7165000438690186\n",
      "Iteration 39750 Training loss 0.09664678573608398 Validation loss 0.09681277722120285 Accuracy 0.7120000123977661\n",
      "Iteration 39760 Training loss 0.10577819496393204 Validation loss 0.10095610469579697 Accuracy 0.7020000219345093\n",
      "Iteration 39770 Training loss 0.09046784788370132 Validation loss 0.09512177854776382 Accuracy 0.7155000567436218\n",
      "Iteration 39780 Training loss 0.09749952703714371 Validation loss 0.09606029093265533 Accuracy 0.7190000414848328\n",
      "Iteration 39790 Training loss 0.10662219673395157 Validation loss 0.09744671732187271 Accuracy 0.7080000042915344\n",
      "Iteration 39800 Training loss 0.09492936730384827 Validation loss 0.09846576303243637 Accuracy 0.7105000615119934\n",
      "Iteration 39810 Training loss 0.11380353569984436 Validation loss 0.10297403484582901 Accuracy 0.6810000538825989\n",
      "Iteration 39820 Training loss 0.09860944747924805 Validation loss 0.09761437028646469 Accuracy 0.7200000286102295\n",
      "Iteration 39830 Training loss 0.0882331058382988 Validation loss 0.09258385002613068 Accuracy 0.7380000352859497\n",
      "Iteration 39840 Training loss 0.09429395198822021 Validation loss 0.09598652273416519 Accuracy 0.7205000519752502\n",
      "Iteration 39850 Training loss 0.08451942354440689 Validation loss 0.09303563833236694 Accuracy 0.7360000610351562\n",
      "Iteration 39860 Training loss 0.11199542135000229 Validation loss 0.11699867248535156 Accuracy 0.659500002861023\n",
      "Iteration 39870 Training loss 0.07958458364009857 Validation loss 0.09546947479248047 Accuracy 0.7140000462532043\n",
      "Iteration 39880 Training loss 0.09143184125423431 Validation loss 0.09856411814689636 Accuracy 0.6985000371932983\n",
      "Iteration 39890 Training loss 0.10519605129957199 Validation loss 0.10014956444501877 Accuracy 0.6860000491142273\n",
      "Iteration 39900 Training loss 0.10640522092580795 Validation loss 0.09825742989778519 Accuracy 0.7110000252723694\n",
      "Iteration 39910 Training loss 0.07871215045452118 Validation loss 0.09828207641839981 Accuracy 0.7100000381469727\n",
      "Iteration 39920 Training loss 0.09981425851583481 Validation loss 0.09669537097215652 Accuracy 0.7100000381469727\n",
      "Iteration 39930 Training loss 0.09683813154697418 Validation loss 0.09608767181634903 Accuracy 0.733500063419342\n",
      "Iteration 39940 Training loss 0.0984976589679718 Validation loss 0.10806214064359665 Accuracy 0.6585000157356262\n",
      "Iteration 39950 Training loss 0.0997508093714714 Validation loss 0.098525270819664 Accuracy 0.703000009059906\n",
      "Iteration 39960 Training loss 0.1089010015130043 Validation loss 0.10079335421323776 Accuracy 0.6955000162124634\n",
      "Iteration 39970 Training loss 0.08883129060268402 Validation loss 0.0981791540980339 Accuracy 0.7075000405311584\n",
      "Iteration 39980 Training loss 0.08189872652292252 Validation loss 0.09395980089902878 Accuracy 0.7165000438690186\n",
      "Iteration 39990 Training loss 0.10185250639915466 Validation loss 0.09328256547451019 Accuracy 0.7220000624656677\n",
      "Iteration 40000 Training loss 0.10490462929010391 Validation loss 0.09347350150346756 Accuracy 0.7310000061988831\n",
      "Iteration 40010 Training loss 0.11262651532888412 Validation loss 0.10963945090770721 Accuracy 0.6365000009536743\n",
      "Iteration 40020 Training loss 0.10249118506908417 Validation loss 0.09919869154691696 Accuracy 0.7095000147819519\n",
      "Iteration 40030 Training loss 0.10232572257518768 Validation loss 0.10891835391521454 Accuracy 0.6350000500679016\n",
      "Iteration 40040 Training loss 0.12269289046525955 Validation loss 0.11633817106485367 Accuracy 0.6345000267028809\n",
      "Iteration 40050 Training loss 0.07955262064933777 Validation loss 0.09386321902275085 Accuracy 0.7150000333786011\n",
      "Iteration 40060 Training loss 0.09472070634365082 Validation loss 0.09804123640060425 Accuracy 0.7145000100135803\n",
      "Iteration 40070 Training loss 0.10012973845005035 Validation loss 0.10071637481451035 Accuracy 0.6915000081062317\n",
      "Iteration 40080 Training loss 0.10054122656583786 Validation loss 0.09570978581905365 Accuracy 0.7080000042915344\n",
      "Iteration 40090 Training loss 0.09006083011627197 Validation loss 0.09527602046728134 Accuracy 0.7245000600814819\n",
      "Iteration 40100 Training loss 0.10166554152965546 Validation loss 0.10168437659740448 Accuracy 0.7035000324249268\n",
      "Iteration 40110 Training loss 0.12338527292013168 Validation loss 0.1236572116613388 Accuracy 0.6135000586509705\n",
      "Iteration 40120 Training loss 0.08882497251033783 Validation loss 0.09321045875549316 Accuracy 0.7460000514984131\n",
      "Iteration 40130 Training loss 0.0952623188495636 Validation loss 0.09674903750419617 Accuracy 0.7245000600814819\n",
      "Iteration 40140 Training loss 0.0955224484205246 Validation loss 0.09649353474378586 Accuracy 0.7155000567436218\n",
      "Iteration 40150 Training loss 0.11024299263954163 Validation loss 0.09789097309112549 Accuracy 0.7145000100135803\n",
      "Iteration 40160 Training loss 0.09255214780569077 Validation loss 0.09747049957513809 Accuracy 0.7055000066757202\n",
      "Iteration 40170 Training loss 0.08871005475521088 Validation loss 0.09433852136135101 Accuracy 0.7295000553131104\n",
      "Iteration 40180 Training loss 0.11482947319746017 Validation loss 0.10627560317516327 Accuracy 0.6620000600814819\n",
      "Iteration 40190 Training loss 0.09591992944478989 Validation loss 0.09948708117008209 Accuracy 0.6920000314712524\n",
      "Iteration 40200 Training loss 0.09036437422037125 Validation loss 0.09715581685304642 Accuracy 0.7205000519752502\n",
      "Iteration 40210 Training loss 0.08737850189208984 Validation loss 0.09568780660629272 Accuracy 0.7295000553131104\n",
      "Iteration 40220 Training loss 0.09209863096475601 Validation loss 0.09357542544603348 Accuracy 0.734000027179718\n",
      "Iteration 40230 Training loss 0.09063935279846191 Validation loss 0.09557683765888214 Accuracy 0.7245000600814819\n",
      "Iteration 40240 Training loss 0.10442563891410828 Validation loss 0.09645198285579681 Accuracy 0.7225000262260437\n",
      "Iteration 40250 Training loss 0.11882129311561584 Validation loss 0.11030704528093338 Accuracy 0.6675000190734863\n",
      "Iteration 40260 Training loss 0.09046092629432678 Validation loss 0.0958530381321907 Accuracy 0.718500018119812\n",
      "Iteration 40270 Training loss 0.09476558119058609 Validation loss 0.09603308886289597 Accuracy 0.721500039100647\n",
      "Iteration 40280 Training loss 0.09389708191156387 Validation loss 0.0947420746088028 Accuracy 0.7205000519752502\n",
      "Iteration 40290 Training loss 0.10250052809715271 Validation loss 0.0991223156452179 Accuracy 0.7045000195503235\n",
      "Iteration 40300 Training loss 0.08980020135641098 Validation loss 0.09622877836227417 Accuracy 0.7160000205039978\n",
      "Iteration 40310 Training loss 0.11248231679201126 Validation loss 0.11265763640403748 Accuracy 0.6615000367164612\n",
      "Iteration 40320 Training loss 0.08382613956928253 Validation loss 0.09765776246786118 Accuracy 0.7085000276565552\n",
      "Iteration 40330 Training loss 0.09707336127758026 Validation loss 0.09385717660188675 Accuracy 0.7325000166893005\n",
      "Iteration 40340 Training loss 0.08329713344573975 Validation loss 0.09545180201530457 Accuracy 0.7135000228881836\n",
      "Iteration 40350 Training loss 0.10494539141654968 Validation loss 0.09230244904756546 Accuracy 0.7250000238418579\n",
      "Iteration 40360 Training loss 0.08341404795646667 Validation loss 0.09395594894886017 Accuracy 0.7280000448226929\n",
      "Iteration 40370 Training loss 0.09388241916894913 Validation loss 0.0997815951704979 Accuracy 0.6980000138282776\n",
      "Iteration 40380 Training loss 0.0949762687087059 Validation loss 0.09766391664743423 Accuracy 0.7110000252723694\n",
      "Iteration 40390 Training loss 0.10199722647666931 Validation loss 0.09635835886001587 Accuracy 0.7170000076293945\n",
      "Iteration 40400 Training loss 0.08828603476285934 Validation loss 0.10001343488693237 Accuracy 0.7075000405311584\n",
      "Iteration 40410 Training loss 0.091623455286026 Validation loss 0.09491622447967529 Accuracy 0.7380000352859497\n",
      "Iteration 40420 Training loss 0.09885063022375107 Validation loss 0.10218248516321182 Accuracy 0.6925000548362732\n",
      "Iteration 40430 Training loss 0.09920740872621536 Validation loss 0.09400223195552826 Accuracy 0.7275000214576721\n",
      "Iteration 40440 Training loss 0.10505170375108719 Validation loss 0.09562426805496216 Accuracy 0.7270000576972961\n",
      "Iteration 40450 Training loss 0.08667029440402985 Validation loss 0.09704390168190002 Accuracy 0.7135000228881836\n",
      "Iteration 40460 Training loss 0.11309191584587097 Validation loss 0.092220738530159 Accuracy 0.7355000376701355\n",
      "Iteration 40470 Training loss 0.09874799847602844 Validation loss 0.09391997009515762 Accuracy 0.7345000505447388\n",
      "Iteration 40480 Training loss 0.10462892055511475 Validation loss 0.10590741783380508 Accuracy 0.6790000200271606\n",
      "Iteration 40490 Training loss 0.11081071943044662 Validation loss 0.10480289161205292 Accuracy 0.6880000233650208\n",
      "Iteration 40500 Training loss 0.09816667437553406 Validation loss 0.09320291131734848 Accuracy 0.7290000319480896\n",
      "Iteration 40510 Training loss 0.08489158749580383 Validation loss 0.0920276790857315 Accuracy 0.7325000166893005\n",
      "Iteration 40520 Training loss 0.09239610284566879 Validation loss 0.09433947503566742 Accuracy 0.7395000457763672\n",
      "Iteration 40530 Training loss 0.10242224484682083 Validation loss 0.11418525129556656 Accuracy 0.6610000133514404\n",
      "Iteration 40540 Training loss 0.10471631586551666 Validation loss 0.09921135753393173 Accuracy 0.706000030040741\n",
      "Iteration 40550 Training loss 0.10128137469291687 Validation loss 0.09538467973470688 Accuracy 0.7245000600814819\n",
      "Iteration 40560 Training loss 0.10651496052742004 Validation loss 0.11130022257566452 Accuracy 0.6440000534057617\n",
      "Iteration 40570 Training loss 0.09673626720905304 Validation loss 0.0979519858956337 Accuracy 0.7160000205039978\n",
      "Iteration 40580 Training loss 0.09296680986881256 Validation loss 0.10048392415046692 Accuracy 0.6985000371932983\n",
      "Iteration 40590 Training loss 0.08384954929351807 Validation loss 0.09277414530515671 Accuracy 0.7360000610351562\n",
      "Iteration 40600 Training loss 0.09990496188402176 Validation loss 0.1021459698677063 Accuracy 0.690000057220459\n",
      "Iteration 40610 Training loss 0.08897984027862549 Validation loss 0.09483877569437027 Accuracy 0.7320000529289246\n",
      "Iteration 40620 Training loss 0.0994856208562851 Validation loss 0.09990954399108887 Accuracy 0.7065000534057617\n",
      "Iteration 40630 Training loss 0.09822836518287659 Validation loss 0.11297965794801712 Accuracy 0.6325000524520874\n",
      "Iteration 40640 Training loss 0.09452708065509796 Validation loss 0.10052374750375748 Accuracy 0.7035000324249268\n",
      "Iteration 40650 Training loss 0.10690503567457199 Validation loss 0.09643302857875824 Accuracy 0.7085000276565552\n",
      "Iteration 40660 Training loss 0.0852801650762558 Validation loss 0.09493599086999893 Accuracy 0.7350000143051147\n",
      "Iteration 40670 Training loss 0.07918886840343475 Validation loss 0.09176676720380783 Accuracy 0.7420000433921814\n",
      "Iteration 40680 Training loss 0.09607850015163422 Validation loss 0.09926670044660568 Accuracy 0.7095000147819519\n",
      "Iteration 40690 Training loss 0.08605069667100906 Validation loss 0.09265363216400146 Accuracy 0.7445000410079956\n",
      "Iteration 40700 Training loss 0.09843704104423523 Validation loss 0.0994650349020958 Accuracy 0.6985000371932983\n",
      "Iteration 40710 Training loss 0.09749368578195572 Validation loss 0.09223613888025284 Accuracy 0.7310000061988831\n",
      "Iteration 40720 Training loss 0.10188229382038116 Validation loss 0.09174525737762451 Accuracy 0.733500063419342\n",
      "Iteration 40730 Training loss 0.10007108747959137 Validation loss 0.0949430838227272 Accuracy 0.718500018119812\n",
      "Iteration 40740 Training loss 0.0949462279677391 Validation loss 0.09696373343467712 Accuracy 0.7065000534057617\n",
      "Iteration 40750 Training loss 0.08267062157392502 Validation loss 0.09502571821212769 Accuracy 0.7230000495910645\n",
      "Iteration 40760 Training loss 0.09508493542671204 Validation loss 0.10850751399993896 Accuracy 0.6585000157356262\n",
      "Iteration 40770 Training loss 0.10786506533622742 Validation loss 0.09547741711139679 Accuracy 0.737000048160553\n",
      "Iteration 40780 Training loss 0.09532847255468369 Validation loss 0.0958128273487091 Accuracy 0.7175000309944153\n",
      "Iteration 40790 Training loss 0.08915773034095764 Validation loss 0.10072436928749084 Accuracy 0.6865000128746033\n",
      "Iteration 40800 Training loss 0.09342113137245178 Validation loss 0.09778095036745071 Accuracy 0.7290000319480896\n",
      "Iteration 40810 Training loss 0.09841848164796829 Validation loss 0.0966089591383934 Accuracy 0.7070000171661377\n",
      "Iteration 40820 Training loss 0.11762643605470657 Validation loss 0.12232257425785065 Accuracy 0.6425000429153442\n",
      "Iteration 40830 Training loss 0.08132138103246689 Validation loss 0.10596994310617447 Accuracy 0.6755000352859497\n",
      "Iteration 40840 Training loss 0.08826646953821182 Validation loss 0.10166626423597336 Accuracy 0.7005000114440918\n",
      "Iteration 40850 Training loss 0.09611976146697998 Validation loss 0.09780188649892807 Accuracy 0.7105000615119934\n",
      "Iteration 40860 Training loss 0.09498061239719391 Validation loss 0.09315254539251328 Accuracy 0.737500011920929\n",
      "Iteration 40870 Training loss 0.1157320886850357 Validation loss 0.10140252858400345 Accuracy 0.6960000395774841\n",
      "Iteration 40880 Training loss 0.11995790153741837 Validation loss 0.11260472983121872 Accuracy 0.6575000286102295\n",
      "Iteration 40890 Training loss 0.0954228863120079 Validation loss 0.09772009402513504 Accuracy 0.7195000052452087\n",
      "Iteration 40900 Training loss 0.10212770104408264 Validation loss 0.09708315134048462 Accuracy 0.7325000166893005\n",
      "Iteration 40910 Training loss 0.1029818207025528 Validation loss 0.09660439938306808 Accuracy 0.7150000333786011\n",
      "Iteration 40920 Training loss 0.10927586257457733 Validation loss 0.10715607553720474 Accuracy 0.6740000247955322\n",
      "Iteration 40930 Training loss 0.10882002860307693 Validation loss 0.09540494531393051 Accuracy 0.7155000567436218\n",
      "Iteration 40940 Training loss 0.114591583609581 Validation loss 0.10265104472637177 Accuracy 0.6890000104904175\n",
      "Iteration 40950 Training loss 0.0987183153629303 Validation loss 0.09776655584573746 Accuracy 0.7100000381469727\n",
      "Iteration 40960 Training loss 0.09750751405954361 Validation loss 0.09255246073007584 Accuracy 0.7345000505447388\n",
      "Iteration 40970 Training loss 0.11243034154176712 Validation loss 0.1033257320523262 Accuracy 0.6740000247955322\n",
      "Iteration 40980 Training loss 0.11790605634450912 Validation loss 0.11042021960020065 Accuracy 0.6330000162124634\n",
      "Iteration 40990 Training loss 0.11374194920063019 Validation loss 0.10593486577272415 Accuracy 0.6660000085830688\n",
      "Iteration 41000 Training loss 0.0971611738204956 Validation loss 0.09611038118600845 Accuracy 0.7255000472068787\n",
      "Iteration 41010 Training loss 0.09242317080497742 Validation loss 0.10649465024471283 Accuracy 0.6630000472068787\n",
      "Iteration 41020 Training loss 0.08644288778305054 Validation loss 0.0925518348813057 Accuracy 0.7355000376701355\n",
      "Iteration 41030 Training loss 0.1318054348230362 Validation loss 0.13733184337615967 Accuracy 0.5190000534057617\n",
      "Iteration 41040 Training loss 0.11029670387506485 Validation loss 0.10069318115711212 Accuracy 0.7020000219345093\n",
      "Iteration 41050 Training loss 0.08790288120508194 Validation loss 0.09505227208137512 Accuracy 0.7145000100135803\n",
      "Iteration 41060 Training loss 0.09686262160539627 Validation loss 0.10208382457494736 Accuracy 0.6835000514984131\n",
      "Iteration 41070 Training loss 0.10169269144535065 Validation loss 0.09849879145622253 Accuracy 0.7145000100135803\n",
      "Iteration 41080 Training loss 0.10989217460155487 Validation loss 0.12045865505933762 Accuracy 0.6065000295639038\n",
      "Iteration 41090 Training loss 0.08590652048587799 Validation loss 0.09423063695430756 Accuracy 0.7195000052452087\n",
      "Iteration 41100 Training loss 0.08197387307882309 Validation loss 0.09713570028543472 Accuracy 0.7220000624656677\n",
      "Iteration 41110 Training loss 0.09787774831056595 Validation loss 0.09971224516630173 Accuracy 0.6940000057220459\n",
      "Iteration 41120 Training loss 0.09991993010044098 Validation loss 0.10073790699243546 Accuracy 0.6965000033378601\n",
      "Iteration 41130 Training loss 0.10134843736886978 Validation loss 0.09601439535617828 Accuracy 0.7150000333786011\n",
      "Iteration 41140 Training loss 0.10434488952159882 Validation loss 0.09735435992479324 Accuracy 0.706000030040741\n",
      "Iteration 41150 Training loss 0.10101068019866943 Validation loss 0.09833325445652008 Accuracy 0.7070000171661377\n",
      "Iteration 41160 Training loss 0.1172134205698967 Validation loss 0.12414339184761047 Accuracy 0.6270000338554382\n",
      "Iteration 41170 Training loss 0.09808674454689026 Validation loss 0.0976913794875145 Accuracy 0.7120000123977661\n",
      "Iteration 41180 Training loss 0.08483870327472687 Validation loss 0.09217052161693573 Accuracy 0.7385000586509705\n",
      "Iteration 41190 Training loss 0.09094779938459396 Validation loss 0.09708194434642792 Accuracy 0.7165000438690186\n",
      "Iteration 41200 Training loss 0.0875159278512001 Validation loss 0.1026206687092781 Accuracy 0.6855000257492065\n",
      "Iteration 41210 Training loss 0.09869805723428726 Validation loss 0.10856438428163528 Accuracy 0.6530000567436218\n",
      "Iteration 41220 Training loss 0.09669492393732071 Validation loss 0.10219324380159378 Accuracy 0.6965000033378601\n",
      "Iteration 41230 Training loss 0.11440475285053253 Validation loss 0.10045241564512253 Accuracy 0.7070000171661377\n",
      "Iteration 41240 Training loss 0.08689874410629272 Validation loss 0.09463365375995636 Accuracy 0.7125000357627869\n",
      "Iteration 41250 Training loss 0.09433740377426147 Validation loss 0.09893166273832321 Accuracy 0.7085000276565552\n",
      "Iteration 41260 Training loss 0.09617161750793457 Validation loss 0.09161537885665894 Accuracy 0.7300000190734863\n",
      "Iteration 41270 Training loss 0.08828256279230118 Validation loss 0.0964149683713913 Accuracy 0.718500018119812\n",
      "Iteration 41280 Training loss 0.09330777078866959 Validation loss 0.09449408948421478 Accuracy 0.7260000109672546\n",
      "Iteration 41290 Training loss 0.07821912318468094 Validation loss 0.09306441247463226 Accuracy 0.7345000505447388\n",
      "Iteration 41300 Training loss 0.12329726666212082 Validation loss 0.10747050493955612 Accuracy 0.6660000085830688\n",
      "Iteration 41310 Training loss 0.10238160938024521 Validation loss 0.10262757539749146 Accuracy 0.6850000619888306\n",
      "Iteration 41320 Training loss 0.11010698229074478 Validation loss 0.1023889034986496 Accuracy 0.6970000267028809\n",
      "Iteration 41330 Training loss 0.10786546766757965 Validation loss 0.09388414770364761 Accuracy 0.7400000095367432\n",
      "Iteration 41340 Training loss 0.08740238100290298 Validation loss 0.0946510061621666 Accuracy 0.7395000457763672\n",
      "Iteration 41350 Training loss 0.10682973265647888 Validation loss 0.10912153869867325 Accuracy 0.6500000357627869\n",
      "Iteration 41360 Training loss 0.11211253702640533 Validation loss 0.10820616781711578 Accuracy 0.6565000414848328\n",
      "Iteration 41370 Training loss 0.08623634278774261 Validation loss 0.10596298426389694 Accuracy 0.6605000495910645\n",
      "Iteration 41380 Training loss 0.07729612290859222 Validation loss 0.09360101819038391 Accuracy 0.7245000600814819\n",
      "Iteration 41390 Training loss 0.08210773020982742 Validation loss 0.09436868876218796 Accuracy 0.7290000319480896\n",
      "Iteration 41400 Training loss 0.0889829769730568 Validation loss 0.09754273295402527 Accuracy 0.7090000510215759\n",
      "Iteration 41410 Training loss 0.09277163445949554 Validation loss 0.09440460801124573 Accuracy 0.7225000262260437\n",
      "Iteration 41420 Training loss 0.1049535945057869 Validation loss 0.10104800760746002 Accuracy 0.6970000267028809\n",
      "Iteration 41430 Training loss 0.09312345087528229 Validation loss 0.09531888365745544 Accuracy 0.733500063419342\n",
      "Iteration 41440 Training loss 0.09421651810407639 Validation loss 0.09782243520021439 Accuracy 0.7175000309944153\n",
      "Iteration 41450 Training loss 0.0903477743268013 Validation loss 0.10264471173286438 Accuracy 0.6885000467300415\n",
      "Iteration 41460 Training loss 0.10889501124620438 Validation loss 0.09722546488046646 Accuracy 0.7090000510215759\n",
      "Iteration 41470 Training loss 0.08298129588365555 Validation loss 0.09924069792032242 Accuracy 0.7100000381469727\n",
      "Iteration 41480 Training loss 0.0935228168964386 Validation loss 0.0988689437508583 Accuracy 0.706000030040741\n",
      "Iteration 41490 Training loss 0.10606709867715836 Validation loss 0.10019282251596451 Accuracy 0.7015000581741333\n",
      "Iteration 41500 Training loss 0.08815204352140427 Validation loss 0.09263245016336441 Accuracy 0.733500063419342\n",
      "Iteration 41510 Training loss 0.10102086514234543 Validation loss 0.10487785190343857 Accuracy 0.6815000176429749\n",
      "Iteration 41520 Training loss 0.10103326290845871 Validation loss 0.09655018895864487 Accuracy 0.718500018119812\n",
      "Iteration 41530 Training loss 0.08864273130893707 Validation loss 0.09863628447055817 Accuracy 0.7155000567436218\n",
      "Iteration 41540 Training loss 0.09371468424797058 Validation loss 0.09419532120227814 Accuracy 0.7380000352859497\n",
      "Iteration 41550 Training loss 0.08695529401302338 Validation loss 0.09383337944746017 Accuracy 0.7270000576972961\n",
      "Iteration 41560 Training loss 0.10314730554819107 Validation loss 0.10468555241823196 Accuracy 0.6720000505447388\n",
      "Iteration 41570 Training loss 0.10026329010725021 Validation loss 0.09972571581602097 Accuracy 0.6950000524520874\n",
      "Iteration 41580 Training loss 0.09863639622926712 Validation loss 0.09502431005239487 Accuracy 0.7170000076293945\n",
      "Iteration 41590 Training loss 0.08190931379795074 Validation loss 0.09383966773748398 Accuracy 0.7190000414848328\n",
      "Iteration 41600 Training loss 0.07703185081481934 Validation loss 0.09425026178359985 Accuracy 0.7280000448226929\n",
      "Iteration 41610 Training loss 0.10368674993515015 Validation loss 0.09440773725509644 Accuracy 0.7350000143051147\n",
      "Iteration 41620 Training loss 0.09710179269313812 Validation loss 0.10779665410518646 Accuracy 0.6645000576972961\n",
      "Iteration 41630 Training loss 0.09336042404174805 Validation loss 0.09784825146198273 Accuracy 0.7160000205039978\n",
      "Iteration 41640 Training loss 0.09865549206733704 Validation loss 0.10399995744228363 Accuracy 0.690500020980835\n",
      "Iteration 41650 Training loss 0.10383981466293335 Validation loss 0.095106340944767 Accuracy 0.7135000228881836\n",
      "Iteration 41660 Training loss 0.09661278873682022 Validation loss 0.10503178834915161 Accuracy 0.6735000610351562\n",
      "Iteration 41670 Training loss 0.08547194302082062 Validation loss 0.09538089483976364 Accuracy 0.7225000262260437\n",
      "Iteration 41680 Training loss 0.08247242122888565 Validation loss 0.09634680300951004 Accuracy 0.7125000357627869\n",
      "Iteration 41690 Training loss 0.124122254550457 Validation loss 0.13120488822460175 Accuracy 0.5760000348091125\n",
      "Iteration 41700 Training loss 0.14171960949897766 Validation loss 0.13826322555541992 Accuracy 0.5534999966621399\n",
      "Iteration 41710 Training loss 0.09201684594154358 Validation loss 0.09392739087343216 Accuracy 0.7310000061988831\n",
      "Iteration 41720 Training loss 0.08344320207834244 Validation loss 0.0952639952301979 Accuracy 0.7280000448226929\n",
      "Iteration 41730 Training loss 0.09279529005289078 Validation loss 0.09479805082082748 Accuracy 0.7300000190734863\n",
      "Iteration 41740 Training loss 0.12142951786518097 Validation loss 0.09654233604669571 Accuracy 0.7120000123977661\n",
      "Iteration 41750 Training loss 0.0917358472943306 Validation loss 0.09708235412836075 Accuracy 0.7220000624656677\n",
      "Iteration 41760 Training loss 0.09183130413293839 Validation loss 0.10109264403581619 Accuracy 0.6960000395774841\n",
      "Iteration 41770 Training loss 0.08739936351776123 Validation loss 0.09446355700492859 Accuracy 0.7365000247955322\n",
      "Iteration 41780 Training loss 0.08591154217720032 Validation loss 0.10154721885919571 Accuracy 0.6915000081062317\n",
      "Iteration 41790 Training loss 0.09939161688089371 Validation loss 0.09904789179563522 Accuracy 0.7125000357627869\n",
      "Iteration 41800 Training loss 0.08675927668809891 Validation loss 0.10029704868793488 Accuracy 0.7035000324249268\n",
      "Iteration 41810 Training loss 0.14042086899280548 Validation loss 0.12108176201581955 Accuracy 0.5860000252723694\n",
      "Iteration 41820 Training loss 0.08844542503356934 Validation loss 0.09407634288072586 Accuracy 0.7285000085830688\n",
      "Iteration 41830 Training loss 0.10878988355398178 Validation loss 0.09640470892190933 Accuracy 0.7230000495910645\n",
      "Iteration 41840 Training loss 0.0946301594376564 Validation loss 0.09442585706710815 Accuracy 0.7175000309944153\n",
      "Iteration 41850 Training loss 0.09617885202169418 Validation loss 0.09509355574846268 Accuracy 0.7275000214576721\n",
      "Iteration 41860 Training loss 0.08768188208341599 Validation loss 0.09264842420816422 Accuracy 0.7345000505447388\n",
      "Iteration 41870 Training loss 0.10163300484418869 Validation loss 0.09639296680688858 Accuracy 0.7155000567436218\n",
      "Iteration 41880 Training loss 0.10956799238920212 Validation loss 0.09461189061403275 Accuracy 0.7200000286102295\n",
      "Iteration 41890 Training loss 0.09420886635780334 Validation loss 0.09658075124025345 Accuracy 0.7285000085830688\n",
      "Iteration 41900 Training loss 0.0916738286614418 Validation loss 0.09721429646015167 Accuracy 0.7095000147819519\n",
      "Iteration 41910 Training loss 0.09077233821153641 Validation loss 0.0950726643204689 Accuracy 0.7275000214576721\n",
      "Iteration 41920 Training loss 0.07527808845043182 Validation loss 0.09612084180116653 Accuracy 0.7190000414848328\n",
      "Iteration 41930 Training loss 0.09267883002758026 Validation loss 0.09162269532680511 Accuracy 0.7290000319480896\n",
      "Iteration 41940 Training loss 0.0955079048871994 Validation loss 0.09421297162771225 Accuracy 0.718000054359436\n",
      "Iteration 41950 Training loss 0.08714097738265991 Validation loss 0.10021262615919113 Accuracy 0.6925000548362732\n",
      "Iteration 41960 Training loss 0.09259925037622452 Validation loss 0.0938614085316658 Accuracy 0.7275000214576721\n",
      "Iteration 41970 Training loss 0.09808418154716492 Validation loss 0.09770218282938004 Accuracy 0.7100000381469727\n",
      "Iteration 41980 Training loss 0.11518876999616623 Validation loss 0.10874049365520477 Accuracy 0.6775000095367432\n",
      "Iteration 41990 Training loss 0.10609645396471024 Validation loss 0.10361389070749283 Accuracy 0.6820000410079956\n",
      "Iteration 42000 Training loss 0.09259660542011261 Validation loss 0.0985979363322258 Accuracy 0.7115000486373901\n",
      "Iteration 42010 Training loss 0.11228305846452713 Validation loss 0.10073035210371017 Accuracy 0.6890000104904175\n",
      "Iteration 42020 Training loss 0.0921568050980568 Validation loss 0.09985443949699402 Accuracy 0.7035000324249268\n",
      "Iteration 42030 Training loss 0.09831960499286652 Validation loss 0.10024207085371017 Accuracy 0.6955000162124634\n",
      "Iteration 42040 Training loss 0.08213489502668381 Validation loss 0.09601813554763794 Accuracy 0.7080000042915344\n",
      "Iteration 42050 Training loss 0.07973675429821014 Validation loss 0.09600480645895004 Accuracy 0.7250000238418579\n",
      "Iteration 42060 Training loss 0.09801961481571198 Validation loss 0.09660990536212921 Accuracy 0.7175000309944153\n",
      "Iteration 42070 Training loss 0.0933941975235939 Validation loss 0.09289894998073578 Accuracy 0.7320000529289246\n",
      "Iteration 42080 Training loss 0.09723436087369919 Validation loss 0.09448821097612381 Accuracy 0.7230000495910645\n",
      "Iteration 42090 Training loss 0.08991235494613647 Validation loss 0.09347653388977051 Accuracy 0.7260000109672546\n",
      "Iteration 42100 Training loss 0.08638284355401993 Validation loss 0.10076532512903214 Accuracy 0.6925000548362732\n",
      "Iteration 42110 Training loss 0.09360960125923157 Validation loss 0.09424889087677002 Accuracy 0.7390000224113464\n",
      "Iteration 42120 Training loss 0.09586995840072632 Validation loss 0.09956871718168259 Accuracy 0.6960000395774841\n",
      "Iteration 42130 Training loss 0.12118899077177048 Validation loss 0.13029703497886658 Accuracy 0.5585000514984131\n",
      "Iteration 42140 Training loss 0.09723324328660965 Validation loss 0.09589548408985138 Accuracy 0.7195000052452087\n",
      "Iteration 42150 Training loss 0.07121594995260239 Validation loss 0.0996345654129982 Accuracy 0.7080000042915344\n",
      "Iteration 42160 Training loss 0.10407315939664841 Validation loss 0.09628671407699585 Accuracy 0.7190000414848328\n",
      "Iteration 42170 Training loss 0.11694589257240295 Validation loss 0.1023535206913948 Accuracy 0.6995000243186951\n",
      "Iteration 42180 Training loss 0.10626552253961563 Validation loss 0.10149694979190826 Accuracy 0.6785000562667847\n",
      "Iteration 42190 Training loss 0.08823453634977341 Validation loss 0.09242880344390869 Accuracy 0.7360000610351562\n",
      "Iteration 42200 Training loss 0.13665273785591125 Validation loss 0.12635083496570587 Accuracy 0.5540000200271606\n",
      "Iteration 42210 Training loss 0.09434448182582855 Validation loss 0.09413214772939682 Accuracy 0.7275000214576721\n",
      "Iteration 42220 Training loss 0.08241469413042068 Validation loss 0.09033869206905365 Accuracy 0.7430000305175781\n",
      "Iteration 42230 Training loss 0.09616265445947647 Validation loss 0.10909631848335266 Accuracy 0.6510000228881836\n",
      "Iteration 42240 Training loss 0.08802865445613861 Validation loss 0.10116845369338989 Accuracy 0.6895000338554382\n",
      "Iteration 42250 Training loss 0.08915044367313385 Validation loss 0.09260199218988419 Accuracy 0.733500063419342\n",
      "Iteration 42260 Training loss 0.10671814531087875 Validation loss 0.09251043945550919 Accuracy 0.7315000295639038\n",
      "Iteration 42270 Training loss 0.08349088579416275 Validation loss 0.09214051067829132 Accuracy 0.7345000505447388\n",
      "Iteration 42280 Training loss 0.1013486385345459 Validation loss 0.09247089177370071 Accuracy 0.7365000247955322\n",
      "Iteration 42290 Training loss 0.09818776696920395 Validation loss 0.09852021932601929 Accuracy 0.7210000157356262\n",
      "Iteration 42300 Training loss 0.10789867490530014 Validation loss 0.10397690534591675 Accuracy 0.6840000152587891\n",
      "Iteration 42310 Training loss 0.09955703467130661 Validation loss 0.10582519322633743 Accuracy 0.6705000400543213\n",
      "Iteration 42320 Training loss 0.08612488210201263 Validation loss 0.09356571733951569 Accuracy 0.7330000400543213\n",
      "Iteration 42330 Training loss 0.08163291960954666 Validation loss 0.09439416974782944 Accuracy 0.7310000061988831\n",
      "Iteration 42340 Training loss 0.10658344626426697 Validation loss 0.09990240633487701 Accuracy 0.7005000114440918\n",
      "Iteration 42350 Training loss 0.07934226095676422 Validation loss 0.09429487586021423 Accuracy 0.7260000109672546\n",
      "Iteration 42360 Training loss 0.08592413365840912 Validation loss 0.09584464132785797 Accuracy 0.7095000147819519\n",
      "Iteration 42370 Training loss 0.09980867058038712 Validation loss 0.10815238207578659 Accuracy 0.6515000462532043\n",
      "Iteration 42380 Training loss 0.13158409297466278 Validation loss 0.1239517405629158 Accuracy 0.593500018119812\n",
      "Iteration 42390 Training loss 0.09691164642572403 Validation loss 0.09520000219345093 Accuracy 0.7140000462532043\n",
      "Iteration 42400 Training loss 0.08446218073368073 Validation loss 0.09333508461713791 Accuracy 0.7190000414848328\n",
      "Iteration 42410 Training loss 0.0849364846944809 Validation loss 0.09981297701597214 Accuracy 0.7105000615119934\n",
      "Iteration 42420 Training loss 0.07985156029462814 Validation loss 0.09327977150678635 Accuracy 0.7285000085830688\n",
      "Iteration 42430 Training loss 0.10489930957555771 Validation loss 0.09870001673698425 Accuracy 0.7130000591278076\n",
      "Iteration 42440 Training loss 0.08047529309988022 Validation loss 0.093989796936512 Accuracy 0.7200000286102295\n",
      "Iteration 42450 Training loss 0.11647399514913559 Validation loss 0.11136699467897415 Accuracy 0.6490000486373901\n",
      "Iteration 42460 Training loss 0.09680107235908508 Validation loss 0.1104002520442009 Accuracy 0.656000018119812\n",
      "Iteration 42470 Training loss 0.10514979809522629 Validation loss 0.11501029878854752 Accuracy 0.6390000581741333\n",
      "Iteration 42480 Training loss 0.0993131622672081 Validation loss 0.09507114440202713 Accuracy 0.7160000205039978\n",
      "Iteration 42490 Training loss 0.10652918368577957 Validation loss 0.118537038564682 Accuracy 0.6410000324249268\n",
      "Iteration 42500 Training loss 0.09797997027635574 Validation loss 0.09607181698083878 Accuracy 0.7100000381469727\n",
      "Iteration 42510 Training loss 0.10670070350170135 Validation loss 0.1053643524646759 Accuracy 0.6640000343322754\n",
      "Iteration 42520 Training loss 0.11603455245494843 Validation loss 0.10614082217216492 Accuracy 0.6685000061988831\n",
      "Iteration 42530 Training loss 0.09521681815385818 Validation loss 0.09398157149553299 Accuracy 0.7280000448226929\n",
      "Iteration 42540 Training loss 0.09141654521226883 Validation loss 0.09470592439174652 Accuracy 0.718500018119812\n",
      "Iteration 42550 Training loss 0.073472760617733 Validation loss 0.09417187422513962 Accuracy 0.7165000438690186\n",
      "Iteration 42560 Training loss 0.10307742655277252 Validation loss 0.10716229677200317 Accuracy 0.6550000309944153\n",
      "Iteration 42570 Training loss 0.1177985891699791 Validation loss 0.11626178026199341 Accuracy 0.6100000143051147\n",
      "Iteration 42580 Training loss 0.0928131639957428 Validation loss 0.09723324328660965 Accuracy 0.7175000309944153\n",
      "Iteration 42590 Training loss 0.09531940519809723 Validation loss 0.09324464946985245 Accuracy 0.7275000214576721\n",
      "Iteration 42600 Training loss 0.08544797450304031 Validation loss 0.1004914939403534 Accuracy 0.7050000429153442\n",
      "Iteration 42610 Training loss 0.09808286279439926 Validation loss 0.09213051199913025 Accuracy 0.7255000472068787\n",
      "Iteration 42620 Training loss 0.1058526560664177 Validation loss 0.09627873450517654 Accuracy 0.7115000486373901\n",
      "Iteration 42630 Training loss 0.10558401793241501 Validation loss 0.09714224934577942 Accuracy 0.7220000624656677\n",
      "Iteration 42640 Training loss 0.09663727879524231 Validation loss 0.09556042402982712 Accuracy 0.7090000510215759\n",
      "Iteration 42650 Training loss 0.08499307185411453 Validation loss 0.09476424008607864 Accuracy 0.7235000133514404\n",
      "Iteration 42660 Training loss 0.096323661506176 Validation loss 0.10301413387060165 Accuracy 0.6760000586509705\n",
      "Iteration 42670 Training loss 0.0933254063129425 Validation loss 0.09300493448972702 Accuracy 0.7320000529289246\n",
      "Iteration 42680 Training loss 0.08663143962621689 Validation loss 0.10187177360057831 Accuracy 0.6975000500679016\n",
      "Iteration 42690 Training loss 0.08572207391262054 Validation loss 0.0940556526184082 Accuracy 0.7255000472068787\n",
      "Iteration 42700 Training loss 0.09842244535684586 Validation loss 0.09420143067836761 Accuracy 0.7310000061988831\n",
      "Iteration 42710 Training loss 0.08378887921571732 Validation loss 0.10463958978652954 Accuracy 0.6790000200271606\n",
      "Iteration 42720 Training loss 0.0818815603852272 Validation loss 0.09251978993415833 Accuracy 0.7225000262260437\n",
      "Iteration 42730 Training loss 0.08760098367929459 Validation loss 0.09215458482503891 Accuracy 0.7420000433921814\n",
      "Iteration 42740 Training loss 0.09339140355587006 Validation loss 0.09763312339782715 Accuracy 0.7145000100135803\n",
      "Iteration 42750 Training loss 0.08657578378915787 Validation loss 0.0942111387848854 Accuracy 0.7245000600814819\n",
      "Iteration 42760 Training loss 0.08703065663576126 Validation loss 0.09414568543434143 Accuracy 0.721500039100647\n",
      "Iteration 42770 Training loss 0.08864232897758484 Validation loss 0.10056786239147186 Accuracy 0.7085000276565552\n",
      "Iteration 42780 Training loss 0.08946152776479721 Validation loss 0.097878098487854 Accuracy 0.7115000486373901\n",
      "Iteration 42790 Training loss 0.10695474594831467 Validation loss 0.11312613636255264 Accuracy 0.6375000476837158\n",
      "Iteration 42800 Training loss 0.10077085345983505 Validation loss 0.09709881246089935 Accuracy 0.7160000205039978\n",
      "Iteration 42810 Training loss 0.10290414839982986 Validation loss 0.09685536473989487 Accuracy 0.7125000357627869\n",
      "Iteration 42820 Training loss 0.08588981628417969 Validation loss 0.09275398403406143 Accuracy 0.7360000610351562\n",
      "Iteration 42830 Training loss 0.10117704421281815 Validation loss 0.10360531508922577 Accuracy 0.6730000376701355\n",
      "Iteration 42840 Training loss 0.09526418149471283 Validation loss 0.09212391823530197 Accuracy 0.7300000190734863\n",
      "Iteration 42850 Training loss 0.08727285265922546 Validation loss 0.09819953143596649 Accuracy 0.7065000534057617\n",
      "Iteration 42860 Training loss 0.07960198819637299 Validation loss 0.09202542901039124 Accuracy 0.7330000400543213\n",
      "Iteration 42870 Training loss 0.09173784404993057 Validation loss 0.09224288910627365 Accuracy 0.733500063419342\n",
      "Iteration 42880 Training loss 0.08673010021448135 Validation loss 0.09404390305280685 Accuracy 0.7310000061988831\n",
      "Iteration 42890 Training loss 0.12037631869316101 Validation loss 0.11267824470996857 Accuracy 0.6430000066757202\n",
      "Iteration 42900 Training loss 0.08433151245117188 Validation loss 0.10021982342004776 Accuracy 0.6960000395774841\n",
      "Iteration 42910 Training loss 0.10227769613265991 Validation loss 0.0956202894449234 Accuracy 0.7305000424385071\n",
      "Iteration 42920 Training loss 0.10744781047105789 Validation loss 0.09178157895803452 Accuracy 0.7360000610351562\n",
      "Iteration 42930 Training loss 0.10041505843400955 Validation loss 0.10390450060367584 Accuracy 0.6875000596046448\n",
      "Iteration 42940 Training loss 0.10030676424503326 Validation loss 0.10695532709360123 Accuracy 0.6410000324249268\n",
      "Iteration 42950 Training loss 0.08944927155971527 Validation loss 0.09395237267017365 Accuracy 0.718000054359436\n",
      "Iteration 42960 Training loss 0.08292010426521301 Validation loss 0.09330891817808151 Accuracy 0.7270000576972961\n",
      "Iteration 42970 Training loss 0.08418309688568115 Validation loss 0.09735555201768875 Accuracy 0.7165000438690186\n",
      "Iteration 42980 Training loss 0.08889682590961456 Validation loss 0.09978560358285904 Accuracy 0.6940000057220459\n",
      "Iteration 42990 Training loss 0.1082567572593689 Validation loss 0.09687750786542892 Accuracy 0.7175000309944153\n",
      "Iteration 43000 Training loss 0.09714008122682571 Validation loss 0.09297247231006622 Accuracy 0.7175000309944153\n",
      "Iteration 43010 Training loss 0.08908088505268097 Validation loss 0.0937257930636406 Accuracy 0.7300000190734863\n",
      "Iteration 43020 Training loss 0.08579397201538086 Validation loss 0.09768613427877426 Accuracy 0.7050000429153442\n",
      "Iteration 43030 Training loss 0.13157442212104797 Validation loss 0.10799993574619293 Accuracy 0.6835000514984131\n",
      "Iteration 43040 Training loss 0.09101756662130356 Validation loss 0.09351784735918045 Accuracy 0.7280000448226929\n",
      "Iteration 43050 Training loss 0.0947720855474472 Validation loss 0.10179170966148376 Accuracy 0.6805000305175781\n",
      "Iteration 43060 Training loss 0.09047651290893555 Validation loss 0.09409429132938385 Accuracy 0.7325000166893005\n",
      "Iteration 43070 Training loss 0.10018914937973022 Validation loss 0.09169614315032959 Accuracy 0.7410000562667847\n",
      "Iteration 43080 Training loss 0.0901266485452652 Validation loss 0.10028538852930069 Accuracy 0.6940000057220459\n",
      "Iteration 43090 Training loss 0.08414370566606522 Validation loss 0.09445340186357498 Accuracy 0.7350000143051147\n",
      "Iteration 43100 Training loss 0.07333538681268692 Validation loss 0.09411182254552841 Accuracy 0.7175000309944153\n",
      "Iteration 43110 Training loss 0.08933980762958527 Validation loss 0.09216742217540741 Accuracy 0.733500063419342\n",
      "Iteration 43120 Training loss 0.1033473089337349 Validation loss 0.10387366265058517 Accuracy 0.6670000553131104\n",
      "Iteration 43130 Training loss 0.09305551648139954 Validation loss 0.09610418230295181 Accuracy 0.721500039100647\n",
      "Iteration 43140 Training loss 0.08154653012752533 Validation loss 0.09261777251958847 Accuracy 0.7350000143051147\n",
      "Iteration 43150 Training loss 0.07439594715833664 Validation loss 0.0912960097193718 Accuracy 0.7360000610351562\n",
      "Iteration 43160 Training loss 0.10700324922800064 Validation loss 0.09736834466457367 Accuracy 0.7135000228881836\n",
      "Iteration 43170 Training loss 0.11367334425449371 Validation loss 0.10259182006120682 Accuracy 0.6980000138282776\n",
      "Iteration 43180 Training loss 0.0897994190454483 Validation loss 0.09297455847263336 Accuracy 0.7270000576972961\n",
      "Iteration 43190 Training loss 0.09240873903036118 Validation loss 0.09948664903640747 Accuracy 0.6910000443458557\n",
      "Iteration 43200 Training loss 0.08886485546827316 Validation loss 0.09405703097581863 Accuracy 0.7260000109672546\n",
      "Iteration 43210 Training loss 0.09019334614276886 Validation loss 0.09781071543693542 Accuracy 0.7240000367164612\n",
      "Iteration 43220 Training loss 0.10488493740558624 Validation loss 0.10248605906963348 Accuracy 0.6915000081062317\n",
      "Iteration 43230 Training loss 0.10773677378892899 Validation loss 0.11059996485710144 Accuracy 0.659500002861023\n",
      "Iteration 43240 Training loss 0.09579136967658997 Validation loss 0.10900667309761047 Accuracy 0.6580000519752502\n",
      "Iteration 43250 Training loss 0.07881590723991394 Validation loss 0.09533612430095673 Accuracy 0.7210000157356262\n",
      "Iteration 43260 Training loss 0.08725643157958984 Validation loss 0.09850072115659714 Accuracy 0.7115000486373901\n",
      "Iteration 43270 Training loss 0.0927322655916214 Validation loss 0.09647980332374573 Accuracy 0.7145000100135803\n",
      "Iteration 43280 Training loss 0.12384194880723953 Validation loss 0.09348860383033752 Accuracy 0.7225000262260437\n",
      "Iteration 43290 Training loss 0.08612926304340363 Validation loss 0.09483088552951813 Accuracy 0.7325000166893005\n",
      "Iteration 43300 Training loss 0.088915154337883 Validation loss 0.09215088933706284 Accuracy 0.7350000143051147\n",
      "Iteration 43310 Training loss 0.08528125286102295 Validation loss 0.0976422056555748 Accuracy 0.7110000252723694\n",
      "Iteration 43320 Training loss 0.0768636092543602 Validation loss 0.09367020428180695 Accuracy 0.7280000448226929\n",
      "Iteration 43330 Training loss 0.10562125593423843 Validation loss 0.09286847710609436 Accuracy 0.7315000295639038\n",
      "Iteration 43340 Training loss 0.091421939432621 Validation loss 0.09384167939424515 Accuracy 0.7280000448226929\n",
      "Iteration 43350 Training loss 0.08969365805387497 Validation loss 0.10286307334899902 Accuracy 0.6855000257492065\n",
      "Iteration 43360 Training loss 0.09397445619106293 Validation loss 0.09940450638532639 Accuracy 0.7010000348091125\n",
      "Iteration 43370 Training loss 0.08263751864433289 Validation loss 0.09713426977396011 Accuracy 0.7125000357627869\n",
      "Iteration 43380 Training loss 0.07707180827856064 Validation loss 0.0962451919913292 Accuracy 0.7190000414848328\n",
      "Iteration 43390 Training loss 0.08673563599586487 Validation loss 0.09370429813861847 Accuracy 0.7325000166893005\n",
      "Iteration 43400 Training loss 0.09356945008039474 Validation loss 0.09865934401750565 Accuracy 0.7020000219345093\n",
      "Iteration 43410 Training loss 0.07660264521837234 Validation loss 0.0941493883728981 Accuracy 0.7265000343322754\n",
      "Iteration 43420 Training loss 0.10117557644844055 Validation loss 0.09884272515773773 Accuracy 0.7205000519752502\n",
      "Iteration 43430 Training loss 0.10016120970249176 Validation loss 0.11383051425218582 Accuracy 0.6365000009536743\n",
      "Iteration 43440 Training loss 0.1363763064146042 Validation loss 0.12610308825969696 Accuracy 0.6450000405311584\n",
      "Iteration 43450 Training loss 0.10608265548944473 Validation loss 0.09719792008399963 Accuracy 0.7090000510215759\n",
      "Iteration 43460 Training loss 0.10652229189872742 Validation loss 0.10467462241649628 Accuracy 0.687000036239624\n",
      "Iteration 43470 Training loss 0.10212470591068268 Validation loss 0.09508664160966873 Accuracy 0.7195000052452087\n",
      "Iteration 43480 Training loss 0.10617312788963318 Validation loss 0.09585496783256531 Accuracy 0.7265000343322754\n",
      "Iteration 43490 Training loss 0.09423082321882248 Validation loss 0.10198365151882172 Accuracy 0.6960000395774841\n",
      "Iteration 43500 Training loss 0.09814498573541641 Validation loss 0.0937567949295044 Accuracy 0.7205000519752502\n",
      "Iteration 43510 Training loss 0.10465310513973236 Validation loss 0.09954898804426193 Accuracy 0.7050000429153442\n",
      "Iteration 43520 Training loss 0.09251509606838226 Validation loss 0.09555815160274506 Accuracy 0.7230000495910645\n",
      "Iteration 43530 Training loss 0.09456291049718857 Validation loss 0.09395158290863037 Accuracy 0.733500063419342\n",
      "Iteration 43540 Training loss 0.10290450602769852 Validation loss 0.09511531889438629 Accuracy 0.7105000615119934\n",
      "Iteration 43550 Training loss 0.07669661194086075 Validation loss 0.09564398974180222 Accuracy 0.7240000367164612\n",
      "Iteration 43560 Training loss 0.08121483027935028 Validation loss 0.09339401125907898 Accuracy 0.7240000367164612\n",
      "Iteration 43570 Training loss 0.09944122284650803 Validation loss 0.09276016056537628 Accuracy 0.733500063419342\n",
      "Iteration 43580 Training loss 0.08302100002765656 Validation loss 0.0950886607170105 Accuracy 0.718000054359436\n",
      "Iteration 43590 Training loss 0.10042644292116165 Validation loss 0.09619690477848053 Accuracy 0.7260000109672546\n",
      "Iteration 43600 Training loss 0.09169531613588333 Validation loss 0.10496655106544495 Accuracy 0.6720000505447388\n",
      "Iteration 43610 Training loss 0.08947059512138367 Validation loss 0.09712747484445572 Accuracy 0.7285000085830688\n",
      "Iteration 43620 Training loss 0.10276761651039124 Validation loss 0.10516904294490814 Accuracy 0.674500048160553\n",
      "Iteration 43630 Training loss 0.12037508934736252 Validation loss 0.09930365532636642 Accuracy 0.7170000076293945\n",
      "Iteration 43640 Training loss 0.10630887746810913 Validation loss 0.10861968249082565 Accuracy 0.6510000228881836\n",
      "Iteration 43650 Training loss 0.09143463522195816 Validation loss 0.09821531921625137 Accuracy 0.7075000405311584\n",
      "Iteration 43660 Training loss 0.08872314542531967 Validation loss 0.09874733537435532 Accuracy 0.718500018119812\n",
      "Iteration 43670 Training loss 0.08853834122419357 Validation loss 0.10703640431165695 Accuracy 0.6670000553131104\n",
      "Iteration 43680 Training loss 0.10797450691461563 Validation loss 0.0998435690999031 Accuracy 0.7115000486373901\n",
      "Iteration 43690 Training loss 0.095723457634449 Validation loss 0.09896280616521835 Accuracy 0.6930000185966492\n",
      "Iteration 43700 Training loss 0.08972135931253433 Validation loss 0.09743472933769226 Accuracy 0.7105000615119934\n",
      "Iteration 43710 Training loss 0.11176478862762451 Validation loss 0.098487488925457 Accuracy 0.6980000138282776\n",
      "Iteration 43720 Training loss 0.08417432755231857 Validation loss 0.10817354172468185 Accuracy 0.6615000367164612\n",
      "Iteration 43730 Training loss 0.08133524656295776 Validation loss 0.09359735250473022 Accuracy 0.7290000319480896\n",
      "Iteration 43740 Training loss 0.09742450714111328 Validation loss 0.09219380468130112 Accuracy 0.7400000095367432\n",
      "Iteration 43750 Training loss 0.11319608241319656 Validation loss 0.10192205011844635 Accuracy 0.6940000057220459\n",
      "Iteration 43760 Training loss 0.08022615313529968 Validation loss 0.09136853367090225 Accuracy 0.7330000400543213\n",
      "Iteration 43770 Training loss 0.08588781207799911 Validation loss 0.09839386492967606 Accuracy 0.6985000371932983\n",
      "Iteration 43780 Training loss 0.11184549331665039 Validation loss 0.09799425303936005 Accuracy 0.7055000066757202\n",
      "Iteration 43790 Training loss 0.11409501731395721 Validation loss 0.11686212569475174 Accuracy 0.6330000162124634\n",
      "Iteration 43800 Training loss 0.08538932353258133 Validation loss 0.09251835197210312 Accuracy 0.7345000505447388\n",
      "Iteration 43810 Training loss 0.10250991582870483 Validation loss 0.09464314579963684 Accuracy 0.7230000495910645\n",
      "Iteration 43820 Training loss 0.09695571660995483 Validation loss 0.09567244350910187 Accuracy 0.7145000100135803\n",
      "Iteration 43830 Training loss 0.09332219511270523 Validation loss 0.09628386795520782 Accuracy 0.7195000052452087\n",
      "Iteration 43840 Training loss 0.09622429311275482 Validation loss 0.09253493696451187 Accuracy 0.7280000448226929\n",
      "Iteration 43850 Training loss 0.09335111081600189 Validation loss 0.0970618724822998 Accuracy 0.7155000567436218\n",
      "Iteration 43860 Training loss 0.1384001076221466 Validation loss 0.12258406728506088 Accuracy 0.5840000510215759\n",
      "Iteration 43870 Training loss 0.09132156521081924 Validation loss 0.10213851183652878 Accuracy 0.6790000200271606\n",
      "Iteration 43880 Training loss 0.10195047408342361 Validation loss 0.10067353397607803 Accuracy 0.7000000476837158\n",
      "Iteration 43890 Training loss 0.08655969798564911 Validation loss 0.0921122282743454 Accuracy 0.7390000224113464\n",
      "Iteration 43900 Training loss 0.09818129986524582 Validation loss 0.09611127525568008 Accuracy 0.7125000357627869\n",
      "Iteration 43910 Training loss 0.09430403262376785 Validation loss 0.09351108223199844 Accuracy 0.7365000247955322\n",
      "Iteration 43920 Training loss 0.08813733607530594 Validation loss 0.09541256725788116 Accuracy 0.7280000448226929\n",
      "Iteration 43930 Training loss 0.08412507176399231 Validation loss 0.10084361582994461 Accuracy 0.6860000491142273\n",
      "Iteration 43940 Training loss 0.11838938295841217 Validation loss 0.12069059163331985 Accuracy 0.6025000214576721\n",
      "Iteration 43950 Training loss 0.08754707127809525 Validation loss 0.09572216123342514 Accuracy 0.7235000133514404\n",
      "Iteration 43960 Training loss 0.13502338528633118 Validation loss 0.12204615771770477 Accuracy 0.627500057220459\n",
      "Iteration 43970 Training loss 0.096086785197258 Validation loss 0.09772355854511261 Accuracy 0.6990000605583191\n",
      "Iteration 43980 Training loss 0.09270097315311432 Validation loss 0.10219554603099823 Accuracy 0.687000036239624\n",
      "Iteration 43990 Training loss 0.10099975019693375 Validation loss 0.09954924136400223 Accuracy 0.7045000195503235\n",
      "Iteration 44000 Training loss 0.09330267459154129 Validation loss 0.10096374154090881 Accuracy 0.687000036239624\n",
      "Iteration 44010 Training loss 0.09382759034633636 Validation loss 0.10012936592102051 Accuracy 0.7015000581741333\n",
      "Iteration 44020 Training loss 0.0760500505566597 Validation loss 0.09441231191158295 Accuracy 0.7295000553131104\n",
      "Iteration 44030 Training loss 0.09561736136674881 Validation loss 0.09404860436916351 Accuracy 0.7330000400543213\n",
      "Iteration 44040 Training loss 0.09772717207670212 Validation loss 0.0994814857840538 Accuracy 0.7100000381469727\n",
      "Iteration 44050 Training loss 0.09401784837245941 Validation loss 0.10071486979722977 Accuracy 0.6910000443458557\n",
      "Iteration 44060 Training loss 0.10170335322618484 Validation loss 0.09240484237670898 Accuracy 0.7240000367164612\n",
      "Iteration 44070 Training loss 0.08913849294185638 Validation loss 0.10021213442087173 Accuracy 0.7020000219345093\n",
      "Iteration 44080 Training loss 0.09781300276517868 Validation loss 0.10075095295906067 Accuracy 0.6960000395774841\n",
      "Iteration 44090 Training loss 0.10803131014108658 Validation loss 0.0985993817448616 Accuracy 0.7085000276565552\n",
      "Iteration 44100 Training loss 0.08738940954208374 Validation loss 0.09698043018579483 Accuracy 0.7095000147819519\n",
      "Iteration 44110 Training loss 0.09588716179132462 Validation loss 0.09911197423934937 Accuracy 0.7075000405311584\n",
      "Iteration 44120 Training loss 0.07737939059734344 Validation loss 0.10202372819185257 Accuracy 0.6840000152587891\n",
      "Iteration 44130 Training loss 0.08657138794660568 Validation loss 0.09718233346939087 Accuracy 0.7245000600814819\n",
      "Iteration 44140 Training loss 0.1164475828409195 Validation loss 0.10331130027770996 Accuracy 0.6890000104904175\n",
      "Iteration 44150 Training loss 0.09585452824831009 Validation loss 0.09972745925188065 Accuracy 0.7010000348091125\n",
      "Iteration 44160 Training loss 0.10492139309644699 Validation loss 0.09857853502035141 Accuracy 0.7105000615119934\n",
      "Iteration 44170 Training loss 0.10258001834154129 Validation loss 0.09888513386249542 Accuracy 0.7115000486373901\n",
      "Iteration 44180 Training loss 0.10752309858798981 Validation loss 0.10074716806411743 Accuracy 0.6880000233650208\n",
      "Iteration 44190 Training loss 0.08978874981403351 Validation loss 0.09758302569389343 Accuracy 0.7145000100135803\n",
      "Iteration 44200 Training loss 0.08512918651103973 Validation loss 0.09772472083568573 Accuracy 0.6990000605583191\n",
      "Iteration 44210 Training loss 0.09748633950948715 Validation loss 0.1001969650387764 Accuracy 0.7075000405311584\n",
      "Iteration 44220 Training loss 0.10230617225170135 Validation loss 0.10008777678012848 Accuracy 0.6970000267028809\n",
      "Iteration 44230 Training loss 0.09165775030851364 Validation loss 0.10485277324914932 Accuracy 0.690000057220459\n",
      "Iteration 44240 Training loss 0.0938107818365097 Validation loss 0.10256745666265488 Accuracy 0.6950000524520874\n",
      "Iteration 44250 Training loss 0.08683948963880539 Validation loss 0.09607294201850891 Accuracy 0.7125000357627869\n",
      "Iteration 44260 Training loss 0.10606806725263596 Validation loss 0.0928434506058693 Accuracy 0.7300000190734863\n",
      "Iteration 44270 Training loss 0.09163744002580643 Validation loss 0.09211602061986923 Accuracy 0.7345000505447388\n",
      "Iteration 44280 Training loss 0.08304880559444427 Validation loss 0.09286311268806458 Accuracy 0.7280000448226929\n",
      "Iteration 44290 Training loss 0.09489447623491287 Validation loss 0.10709300637245178 Accuracy 0.6605000495910645\n",
      "Iteration 44300 Training loss 0.08440797030925751 Validation loss 0.09234485775232315 Accuracy 0.7425000071525574\n",
      "Iteration 44310 Training loss 0.08818267285823822 Validation loss 0.09403173625469208 Accuracy 0.7250000238418579\n",
      "Iteration 44320 Training loss 0.11334134638309479 Validation loss 0.10145500302314758 Accuracy 0.6815000176429749\n",
      "Iteration 44330 Training loss 0.08701387792825699 Validation loss 0.1019253209233284 Accuracy 0.7005000114440918\n",
      "Iteration 44340 Training loss 0.09292491525411606 Validation loss 0.09459627419710159 Accuracy 0.7205000519752502\n",
      "Iteration 44350 Training loss 0.10425826162099838 Validation loss 0.10186522454023361 Accuracy 0.6855000257492065\n",
      "Iteration 44360 Training loss 0.10490308701992035 Validation loss 0.09937846660614014 Accuracy 0.7040000557899475\n",
      "Iteration 44370 Training loss 0.10693854838609695 Validation loss 0.09263594448566437 Accuracy 0.7390000224113464\n",
      "Iteration 44380 Training loss 0.09603864699602127 Validation loss 0.10079976171255112 Accuracy 0.6975000500679016\n",
      "Iteration 44390 Training loss 0.08262849599123001 Validation loss 0.09610351920127869 Accuracy 0.7200000286102295\n",
      "Iteration 44400 Training loss 0.09700742363929749 Validation loss 0.0934874564409256 Accuracy 0.7400000095367432\n",
      "Iteration 44410 Training loss 0.11423077434301376 Validation loss 0.10857368260622025 Accuracy 0.6665000319480896\n",
      "Iteration 44420 Training loss 0.09672199189662933 Validation loss 0.09658224880695343 Accuracy 0.7130000591278076\n",
      "Iteration 44430 Training loss 0.09833087772130966 Validation loss 0.09256844222545624 Accuracy 0.7355000376701355\n",
      "Iteration 44440 Training loss 0.0946207046508789 Validation loss 0.09769686311483383 Accuracy 0.7190000414848328\n",
      "Iteration 44450 Training loss 0.09166496992111206 Validation loss 0.09203256666660309 Accuracy 0.7305000424385071\n",
      "Iteration 44460 Training loss 0.081216961145401 Validation loss 0.09364255517721176 Accuracy 0.7245000600814819\n",
      "Iteration 44470 Training loss 0.09624604880809784 Validation loss 0.09425447136163712 Accuracy 0.7250000238418579\n",
      "Iteration 44480 Training loss 0.08149229735136032 Validation loss 0.09164336323738098 Accuracy 0.7260000109672546\n",
      "Iteration 44490 Training loss 0.10678812116384506 Validation loss 0.10593494027853012 Accuracy 0.6890000104904175\n",
      "Iteration 44500 Training loss 0.06612927466630936 Validation loss 0.0961947813630104 Accuracy 0.7275000214576721\n",
      "Iteration 44510 Training loss 0.10768874734640121 Validation loss 0.10524750500917435 Accuracy 0.6840000152587891\n",
      "Iteration 44520 Training loss 0.0801578089594841 Validation loss 0.09965506196022034 Accuracy 0.706000030040741\n",
      "Iteration 44530 Training loss 0.1036158949136734 Validation loss 0.0956970602273941 Accuracy 0.7245000600814819\n",
      "Iteration 44540 Training loss 0.10484610497951508 Validation loss 0.10971374809741974 Accuracy 0.6610000133514404\n",
      "Iteration 44550 Training loss 0.09358134865760803 Validation loss 0.10111090540885925 Accuracy 0.7080000042915344\n",
      "Iteration 44560 Training loss 0.09346979111433029 Validation loss 0.10007420182228088 Accuracy 0.7050000429153442\n",
      "Iteration 44570 Training loss 0.08775109052658081 Validation loss 0.10069070756435394 Accuracy 0.6955000162124634\n",
      "Iteration 44580 Training loss 0.10755874216556549 Validation loss 0.10539820790290833 Accuracy 0.6940000057220459\n",
      "Iteration 44590 Training loss 0.10047777742147446 Validation loss 0.09826824069023132 Accuracy 0.6955000162124634\n",
      "Iteration 44600 Training loss 0.08427438884973526 Validation loss 0.1004500761628151 Accuracy 0.7015000581741333\n",
      "Iteration 44610 Training loss 0.10451057553291321 Validation loss 0.10630626976490021 Accuracy 0.6765000224113464\n",
      "Iteration 44620 Training loss 0.10175508260726929 Validation loss 0.097210593521595 Accuracy 0.7130000591278076\n",
      "Iteration 44630 Training loss 0.09110859781503677 Validation loss 0.0955682247877121 Accuracy 0.7230000495910645\n",
      "Iteration 44640 Training loss 0.0922551304101944 Validation loss 0.09521801024675369 Accuracy 0.7320000529289246\n",
      "Iteration 44650 Training loss 0.07494781166315079 Validation loss 0.09236060082912445 Accuracy 0.7345000505447388\n",
      "Iteration 44660 Training loss 0.10009685158729553 Validation loss 0.09491844475269318 Accuracy 0.7235000133514404\n",
      "Iteration 44670 Training loss 0.10332217067480087 Validation loss 0.10131186246871948 Accuracy 0.6930000185966492\n",
      "Iteration 44680 Training loss 0.1178891658782959 Validation loss 0.11939294636249542 Accuracy 0.6420000195503235\n",
      "Iteration 44690 Training loss 0.08528769016265869 Validation loss 0.09208708256483078 Accuracy 0.7295000553131104\n",
      "Iteration 44700 Training loss 0.08544544130563736 Validation loss 0.10429371148347855 Accuracy 0.6710000038146973\n",
      "Iteration 44710 Training loss 0.10241212695837021 Validation loss 0.09954649209976196 Accuracy 0.7065000534057617\n",
      "Iteration 44720 Training loss 0.0847388431429863 Validation loss 0.09050633013248444 Accuracy 0.7465000152587891\n",
      "Iteration 44730 Training loss 0.0999605730175972 Validation loss 0.09337044507265091 Accuracy 0.7300000190734863\n",
      "Iteration 44740 Training loss 0.10377111285924911 Validation loss 0.09561228007078171 Accuracy 0.7125000357627869\n",
      "Iteration 44750 Training loss 0.11728513240814209 Validation loss 0.10768848657608032 Accuracy 0.659000039100647\n",
      "Iteration 44760 Training loss 0.08874467015266418 Validation loss 0.09644203633069992 Accuracy 0.718500018119812\n",
      "Iteration 44770 Training loss 0.08843617886304855 Validation loss 0.10907947272062302 Accuracy 0.674500048160553\n",
      "Iteration 44780 Training loss 0.10314682126045227 Validation loss 0.10160690546035767 Accuracy 0.6875000596046448\n",
      "Iteration 44790 Training loss 0.07822021096944809 Validation loss 0.09109930694103241 Accuracy 0.734000027179718\n",
      "Iteration 44800 Training loss 0.08040988445281982 Validation loss 0.09238918870687485 Accuracy 0.7420000433921814\n",
      "Iteration 44810 Training loss 0.09208762645721436 Validation loss 0.09169811010360718 Accuracy 0.7465000152587891\n",
      "Iteration 44820 Training loss 0.09611562639474869 Validation loss 0.10603360831737518 Accuracy 0.6650000214576721\n",
      "Iteration 44830 Training loss 0.12637460231781006 Validation loss 0.09851504862308502 Accuracy 0.7110000252723694\n",
      "Iteration 44840 Training loss 0.09117288142442703 Validation loss 0.0934760794043541 Accuracy 0.7210000157356262\n",
      "Iteration 44850 Training loss 0.09091237932443619 Validation loss 0.09198314696550369 Accuracy 0.7345000505447388\n",
      "Iteration 44860 Training loss 0.09622848033905029 Validation loss 0.11796949058771133 Accuracy 0.6350000500679016\n",
      "Iteration 44870 Training loss 0.09333682805299759 Validation loss 0.09540340304374695 Accuracy 0.718500018119812\n",
      "Iteration 44880 Training loss 0.09959975630044937 Validation loss 0.1033301129937172 Accuracy 0.687000036239624\n",
      "Iteration 44890 Training loss 0.10857623815536499 Validation loss 0.10438276827335358 Accuracy 0.6610000133514404\n",
      "Iteration 44900 Training loss 0.10760242491960526 Validation loss 0.10439781099557877 Accuracy 0.6730000376701355\n",
      "Iteration 44910 Training loss 0.11707520484924316 Validation loss 0.11262430250644684 Accuracy 0.6260000467300415\n",
      "Iteration 44920 Training loss 0.09473792463541031 Validation loss 0.09330638498067856 Accuracy 0.7325000166893005\n",
      "Iteration 44930 Training loss 0.08342927694320679 Validation loss 0.09547164291143417 Accuracy 0.718000054359436\n",
      "Iteration 44940 Training loss 0.09906333684921265 Validation loss 0.10044451057910919 Accuracy 0.7095000147819519\n",
      "Iteration 44950 Training loss 0.10312336683273315 Validation loss 0.10505778342485428 Accuracy 0.6695000529289246\n",
      "Iteration 44960 Training loss 0.0754220113158226 Validation loss 0.09365906566381454 Accuracy 0.7280000448226929\n",
      "Iteration 44970 Training loss 0.09970692545175552 Validation loss 0.1003803163766861 Accuracy 0.6925000548362732\n",
      "Iteration 44980 Training loss 0.0997074767947197 Validation loss 0.10102821886539459 Accuracy 0.7000000476837158\n",
      "Iteration 44990 Training loss 0.10067593306303024 Validation loss 0.10328146070241928 Accuracy 0.6865000128746033\n",
      "Iteration 45000 Training loss 0.10883424431085587 Validation loss 0.09401492029428482 Accuracy 0.7235000133514404\n",
      "Iteration 45010 Training loss 0.10287085920572281 Validation loss 0.1003979817032814 Accuracy 0.7050000429153442\n",
      "Iteration 45020 Training loss 0.07653134316205978 Validation loss 0.09561783075332642 Accuracy 0.7225000262260437\n",
      "Iteration 45030 Training loss 0.07966742664575577 Validation loss 0.09545020014047623 Accuracy 0.7250000238418579\n",
      "Iteration 45040 Training loss 0.07881777733564377 Validation loss 0.09863212704658508 Accuracy 0.7085000276565552\n",
      "Iteration 45050 Training loss 0.09666705131530762 Validation loss 0.09843897074460983 Accuracy 0.7055000066757202\n",
      "Iteration 45060 Training loss 0.08991526067256927 Validation loss 0.09543008357286453 Accuracy 0.7230000495910645\n",
      "Iteration 45070 Training loss 0.10478412359952927 Validation loss 0.09854123741388321 Accuracy 0.7095000147819519\n",
      "Iteration 45080 Training loss 0.08749621361494064 Validation loss 0.09788790345191956 Accuracy 0.703000009059906\n",
      "Iteration 45090 Training loss 0.11389539390802383 Validation loss 0.10239028185606003 Accuracy 0.6845000386238098\n",
      "Iteration 45100 Training loss 0.0907384529709816 Validation loss 0.0991814061999321 Accuracy 0.690500020980835\n",
      "Iteration 45110 Training loss 0.09038268774747849 Validation loss 0.09573973715305328 Accuracy 0.7190000414848328\n",
      "Iteration 45120 Training loss 0.07666236907243729 Validation loss 0.09672977030277252 Accuracy 0.7190000414848328\n",
      "Iteration 45130 Training loss 0.09866654872894287 Validation loss 0.09911265224218369 Accuracy 0.7100000381469727\n",
      "Iteration 45140 Training loss 0.10304326564073563 Validation loss 0.09855996072292328 Accuracy 0.7110000252723694\n",
      "Iteration 45150 Training loss 0.09758833050727844 Validation loss 0.10877243429422379 Accuracy 0.659000039100647\n",
      "Iteration 45160 Training loss 0.09784797579050064 Validation loss 0.11340666562318802 Accuracy 0.6430000066757202\n",
      "Iteration 45170 Training loss 0.10479055345058441 Validation loss 0.09627149254083633 Accuracy 0.7150000333786011\n",
      "Iteration 45180 Training loss 0.10234153270721436 Validation loss 0.09292258322238922 Accuracy 0.7365000247955322\n",
      "Iteration 45190 Training loss 0.08123184740543365 Validation loss 0.09388943761587143 Accuracy 0.7235000133514404\n",
      "Iteration 45200 Training loss 0.09370440244674683 Validation loss 0.09729062020778656 Accuracy 0.7230000495910645\n",
      "Iteration 45210 Training loss 0.11061996221542358 Validation loss 0.09730029851198196 Accuracy 0.7130000591278076\n",
      "Iteration 45220 Training loss 0.11468131840229034 Validation loss 0.09411696344614029 Accuracy 0.7205000519752502\n",
      "Iteration 45230 Training loss 0.10030331462621689 Validation loss 0.09296119958162308 Accuracy 0.7365000247955322\n",
      "Iteration 45240 Training loss 0.09337998926639557 Validation loss 0.09427165985107422 Accuracy 0.7310000061988831\n",
      "Iteration 45250 Training loss 0.09642894566059113 Validation loss 0.0955338329076767 Accuracy 0.7210000157356262\n",
      "Iteration 45260 Training loss 0.09810952097177505 Validation loss 0.09870115667581558 Accuracy 0.7040000557899475\n",
      "Iteration 45270 Training loss 0.12906292080879211 Validation loss 0.13083170354366302 Accuracy 0.6005000472068787\n",
      "Iteration 45280 Training loss 0.10035280883312225 Validation loss 0.09893035143613815 Accuracy 0.6970000267028809\n",
      "Iteration 45290 Training loss 0.10036292672157288 Validation loss 0.09415221214294434 Accuracy 0.7300000190734863\n",
      "Iteration 45300 Training loss 0.08901695162057877 Validation loss 0.10018200427293777 Accuracy 0.690000057220459\n",
      "Iteration 45310 Training loss 0.09763550758361816 Validation loss 0.09362292289733887 Accuracy 0.7200000286102295\n",
      "Iteration 45320 Training loss 0.08950711786746979 Validation loss 0.09109004586935043 Accuracy 0.7380000352859497\n",
      "Iteration 45330 Training loss 0.09105432033538818 Validation loss 0.09319557994604111 Accuracy 0.7310000061988831\n",
      "Iteration 45340 Training loss 0.0811995416879654 Validation loss 0.10174309462308884 Accuracy 0.675000011920929\n",
      "Iteration 45350 Training loss 0.0756903812289238 Validation loss 0.09460567682981491 Accuracy 0.7290000319480896\n",
      "Iteration 45360 Training loss 0.09202245622873306 Validation loss 0.09372809529304504 Accuracy 0.7275000214576721\n",
      "Iteration 45370 Training loss 0.10278333723545074 Validation loss 0.09602153301239014 Accuracy 0.7150000333786011\n",
      "Iteration 45380 Training loss 0.10197876393795013 Validation loss 0.101468026638031 Accuracy 0.6925000548362732\n",
      "Iteration 45390 Training loss 0.11282970011234283 Validation loss 0.09761038422584534 Accuracy 0.7035000324249268\n",
      "Iteration 45400 Training loss 0.10493259131908417 Validation loss 0.0945589691400528 Accuracy 0.7160000205039978\n",
      "Iteration 45410 Training loss 0.10382917523384094 Validation loss 0.0938914567232132 Accuracy 0.7245000600814819\n",
      "Iteration 45420 Training loss 0.10041005909442902 Validation loss 0.09607599675655365 Accuracy 0.7200000286102295\n",
      "Iteration 45430 Training loss 0.09153813123703003 Validation loss 0.09145238250494003 Accuracy 0.7385000586509705\n",
      "Iteration 45440 Training loss 0.1074914038181305 Validation loss 0.10505028069019318 Accuracy 0.6790000200271606\n",
      "Iteration 45450 Training loss 0.08962595462799072 Validation loss 0.09012540429830551 Accuracy 0.7350000143051147\n",
      "Iteration 45460 Training loss 0.1078919842839241 Validation loss 0.10332626849412918 Accuracy 0.6835000514984131\n",
      "Iteration 45470 Training loss 0.09512737393379211 Validation loss 0.09333860874176025 Accuracy 0.7190000414848328\n",
      "Iteration 45480 Training loss 0.08934475481510162 Validation loss 0.0998176783323288 Accuracy 0.703000009059906\n",
      "Iteration 45490 Training loss 0.09446356445550919 Validation loss 0.0964430645108223 Accuracy 0.7100000381469727\n",
      "Iteration 45500 Training loss 0.08968575298786163 Validation loss 0.09342663735151291 Accuracy 0.7295000553131104\n",
      "Iteration 45510 Training loss 0.07992526888847351 Validation loss 0.09501100331544876 Accuracy 0.7220000624656677\n",
      "Iteration 45520 Training loss 0.0935337021946907 Validation loss 0.0969773530960083 Accuracy 0.7005000114440918\n",
      "Iteration 45530 Training loss 0.08662887662649155 Validation loss 0.09805061668157578 Accuracy 0.7065000534057617\n",
      "Iteration 45540 Training loss 0.1046600490808487 Validation loss 0.09365381300449371 Accuracy 0.7170000076293945\n",
      "Iteration 45550 Training loss 0.0675075501203537 Validation loss 0.09474251419305801 Accuracy 0.718500018119812\n",
      "Iteration 45560 Training loss 0.08701320737600327 Validation loss 0.10029811412096024 Accuracy 0.7020000219345093\n",
      "Iteration 45570 Training loss 0.08684997260570526 Validation loss 0.09161318093538284 Accuracy 0.7410000562667847\n",
      "Iteration 45580 Training loss 0.08266779035329819 Validation loss 0.09548482298851013 Accuracy 0.7170000076293945\n",
      "Iteration 45590 Training loss 0.08856508135795593 Validation loss 0.0940103828907013 Accuracy 0.7250000238418579\n",
      "Iteration 45600 Training loss 0.0960448831319809 Validation loss 0.10002634674310684 Accuracy 0.7125000357627869\n",
      "Iteration 45610 Training loss 0.09095335006713867 Validation loss 0.09385751932859421 Accuracy 0.7300000190734863\n",
      "Iteration 45620 Training loss 0.08226106315851212 Validation loss 0.09194344282150269 Accuracy 0.7435000538825989\n",
      "Iteration 45630 Training loss 0.11591523885726929 Validation loss 0.09572761505842209 Accuracy 0.718000054359436\n",
      "Iteration 45640 Training loss 0.10525736212730408 Validation loss 0.10305065661668777 Accuracy 0.6815000176429749\n",
      "Iteration 45650 Training loss 0.08103221654891968 Validation loss 0.09339189529418945 Accuracy 0.7355000376701355\n",
      "Iteration 45660 Training loss 0.10335204750299454 Validation loss 0.09594958275556564 Accuracy 0.7075000405311584\n",
      "Iteration 45670 Training loss 0.10600605607032776 Validation loss 0.11220551282167435 Accuracy 0.6460000276565552\n",
      "Iteration 45680 Training loss 0.10464250296354294 Validation loss 0.09787765145301819 Accuracy 0.7070000171661377\n",
      "Iteration 45690 Training loss 0.1036975085735321 Validation loss 0.09218672662973404 Accuracy 0.7330000400543213\n",
      "Iteration 45700 Training loss 0.09171953052282333 Validation loss 0.0981951653957367 Accuracy 0.7045000195503235\n",
      "Iteration 45710 Training loss 0.08802009373903275 Validation loss 0.09401826560497284 Accuracy 0.7270000576972961\n",
      "Iteration 45720 Training loss 0.08982530236244202 Validation loss 0.09588117152452469 Accuracy 0.7260000109672546\n",
      "Iteration 45730 Training loss 0.09551358968019485 Validation loss 0.0962427407503128 Accuracy 0.7130000591278076\n",
      "Iteration 45740 Training loss 0.09123416990041733 Validation loss 0.09165535122156143 Accuracy 0.7235000133514404\n",
      "Iteration 45750 Training loss 0.1019027903676033 Validation loss 0.09242157638072968 Accuracy 0.7255000472068787\n",
      "Iteration 45760 Training loss 0.09677580744028091 Validation loss 0.10125108063220978 Accuracy 0.6895000338554382\n",
      "Iteration 45770 Training loss 0.09838786721229553 Validation loss 0.10051064938306808 Accuracy 0.6995000243186951\n",
      "Iteration 45780 Training loss 0.0881742388010025 Validation loss 0.09251990914344788 Accuracy 0.7325000166893005\n",
      "Iteration 45790 Training loss 0.11560209095478058 Validation loss 0.12251244485378265 Accuracy 0.6155000329017639\n",
      "Iteration 45800 Training loss 0.09220131486654282 Validation loss 0.092756487429142 Accuracy 0.7275000214576721\n",
      "Iteration 45810 Training loss 0.09346433728933334 Validation loss 0.09781365841627121 Accuracy 0.6975000500679016\n",
      "Iteration 45820 Training loss 0.10967396199703217 Validation loss 0.09358867257833481 Accuracy 0.7210000157356262\n",
      "Iteration 45830 Training loss 0.10390346497297287 Validation loss 0.09900423139333725 Accuracy 0.7040000557899475\n",
      "Iteration 45840 Training loss 0.1014568880200386 Validation loss 0.09713164716959 Accuracy 0.7095000147819519\n",
      "Iteration 45850 Training loss 0.08489317446947098 Validation loss 0.09340043365955353 Accuracy 0.7195000052452087\n",
      "Iteration 45860 Training loss 0.08704955875873566 Validation loss 0.09266611933708191 Accuracy 0.7360000610351562\n",
      "Iteration 45870 Training loss 0.06795089691877365 Validation loss 0.09376230090856552 Accuracy 0.7320000529289246\n",
      "Iteration 45880 Training loss 0.10107755661010742 Validation loss 0.10209161788225174 Accuracy 0.6885000467300415\n",
      "Iteration 45890 Training loss 0.12814931571483612 Validation loss 0.13151316344738007 Accuracy 0.5795000195503235\n",
      "Iteration 45900 Training loss 0.0840197429060936 Validation loss 0.09340531378984451 Accuracy 0.718500018119812\n",
      "Iteration 45910 Training loss 0.10050041973590851 Validation loss 0.092518150806427 Accuracy 0.718500018119812\n",
      "Iteration 45920 Training loss 0.09131072461605072 Validation loss 0.10370125621557236 Accuracy 0.6760000586509705\n",
      "Iteration 45930 Training loss 0.09347355365753174 Validation loss 0.10491156578063965 Accuracy 0.6720000505447388\n",
      "Iteration 45940 Training loss 0.07890014350414276 Validation loss 0.09617277979850769 Accuracy 0.7145000100135803\n",
      "Iteration 45950 Training loss 0.09706316888332367 Validation loss 0.09322618693113327 Accuracy 0.7235000133514404\n",
      "Iteration 45960 Training loss 0.11369069665670395 Validation loss 0.1107158288359642 Accuracy 0.6625000238418579\n",
      "Iteration 45970 Training loss 0.10026784986257553 Validation loss 0.09918208420276642 Accuracy 0.7090000510215759\n",
      "Iteration 45980 Training loss 0.08244870603084564 Validation loss 0.09276492148637772 Accuracy 0.7260000109672546\n",
      "Iteration 45990 Training loss 0.08908500522375107 Validation loss 0.09459534287452698 Accuracy 0.718500018119812\n",
      "Iteration 46000 Training loss 0.08388002216815948 Validation loss 0.0952073410153389 Accuracy 0.7170000076293945\n",
      "Iteration 46010 Training loss 0.09812380373477936 Validation loss 0.09062328189611435 Accuracy 0.737000048160553\n",
      "Iteration 46020 Training loss 0.09866528958082199 Validation loss 0.10082308948040009 Accuracy 0.6930000185966492\n",
      "Iteration 46030 Training loss 0.09310004860162735 Validation loss 0.09452535212039948 Accuracy 0.7265000343322754\n",
      "Iteration 46040 Training loss 0.1253855675458908 Validation loss 0.10859362035989761 Accuracy 0.6665000319480896\n",
      "Iteration 46050 Training loss 0.09058437496423721 Validation loss 0.09141015261411667 Accuracy 0.7420000433921814\n",
      "Iteration 46060 Training loss 0.07771027833223343 Validation loss 0.08987931907176971 Accuracy 0.7395000457763672\n",
      "Iteration 46070 Training loss 0.10056527704000473 Validation loss 0.09649979323148727 Accuracy 0.718500018119812\n",
      "Iteration 46080 Training loss 0.10070907324552536 Validation loss 0.09889890253543854 Accuracy 0.7045000195503235\n",
      "Iteration 46090 Training loss 0.10085691511631012 Validation loss 0.09207826107740402 Accuracy 0.7400000095367432\n",
      "Iteration 46100 Training loss 0.06825685501098633 Validation loss 0.09416211396455765 Accuracy 0.7190000414848328\n",
      "Iteration 46110 Training loss 0.09752055257558823 Validation loss 0.10127770900726318 Accuracy 0.6820000410079956\n",
      "Iteration 46120 Training loss 0.10165733098983765 Validation loss 0.09433753043413162 Accuracy 0.7280000448226929\n",
      "Iteration 46130 Training loss 0.09482742100954056 Validation loss 0.09671295434236526 Accuracy 0.7155000567436218\n",
      "Iteration 46140 Training loss 0.10866859555244446 Validation loss 0.11461470276117325 Accuracy 0.6290000081062317\n",
      "Iteration 46150 Training loss 0.13574743270874023 Validation loss 0.11583231389522552 Accuracy 0.6370000243186951\n",
      "Iteration 46160 Training loss 0.08110421895980835 Validation loss 0.0943387821316719 Accuracy 0.7155000567436218\n",
      "Iteration 46170 Training loss 0.08192270249128342 Validation loss 0.09635774046182632 Accuracy 0.7295000553131104\n",
      "Iteration 46180 Training loss 0.09107406437397003 Validation loss 0.09337913990020752 Accuracy 0.7230000495910645\n",
      "Iteration 46190 Training loss 0.09420950710773468 Validation loss 0.09271509200334549 Accuracy 0.7380000352859497\n",
      "Iteration 46200 Training loss 0.09441959112882614 Validation loss 0.09560564905405045 Accuracy 0.7235000133514404\n",
      "Iteration 46210 Training loss 0.07923465967178345 Validation loss 0.090582937002182 Accuracy 0.7410000562667847\n",
      "Iteration 46220 Training loss 0.07237284630537033 Validation loss 0.09666989743709564 Accuracy 0.7205000519752502\n",
      "Iteration 46230 Training loss 0.06934566050767899 Validation loss 0.09080682694911957 Accuracy 0.7420000433921814\n",
      "Iteration 46240 Training loss 0.09544291347265244 Validation loss 0.09664734452962875 Accuracy 0.7170000076293945\n",
      "Iteration 46250 Training loss 0.11202843487262726 Validation loss 0.12056060880422592 Accuracy 0.6380000114440918\n",
      "Iteration 46260 Training loss 0.09969891607761383 Validation loss 0.09774737805128098 Accuracy 0.7165000438690186\n",
      "Iteration 46270 Training loss 0.0812467560172081 Validation loss 0.09504814445972443 Accuracy 0.7140000462532043\n",
      "Iteration 46280 Training loss 0.08954105526208878 Validation loss 0.09776685386896133 Accuracy 0.7135000228881836\n",
      "Iteration 46290 Training loss 0.09178660064935684 Validation loss 0.09703290462493896 Accuracy 0.7045000195503235\n",
      "Iteration 46300 Training loss 0.12761537730693817 Validation loss 0.14541737735271454 Accuracy 0.5885000228881836\n",
      "Iteration 46310 Training loss 0.10552266985177994 Validation loss 0.09822853654623032 Accuracy 0.7110000252723694\n",
      "Iteration 46320 Training loss 0.09853240847587585 Validation loss 0.09127182513475418 Accuracy 0.7315000295639038\n",
      "Iteration 46330 Training loss 0.08616404980421066 Validation loss 0.09418874979019165 Accuracy 0.7200000286102295\n",
      "Iteration 46340 Training loss 0.10518576204776764 Validation loss 0.09554000943899155 Accuracy 0.7200000286102295\n",
      "Iteration 46350 Training loss 0.09682320058345795 Validation loss 0.10109647363424301 Accuracy 0.7065000534057617\n",
      "Iteration 46360 Training loss 0.08622115850448608 Validation loss 0.09639204293489456 Accuracy 0.7220000624656677\n",
      "Iteration 46370 Training loss 0.10553659498691559 Validation loss 0.0931583121418953 Accuracy 0.7255000472068787\n",
      "Iteration 46380 Training loss 0.08585640043020248 Validation loss 0.0923248827457428 Accuracy 0.7400000095367432\n",
      "Iteration 46390 Training loss 0.08613521605730057 Validation loss 0.09608716517686844 Accuracy 0.7205000519752502\n",
      "Iteration 46400 Training loss 0.08410598337650299 Validation loss 0.10643185675144196 Accuracy 0.6685000061988831\n",
      "Iteration 46410 Training loss 0.0885997787117958 Validation loss 0.0936470553278923 Accuracy 0.7265000343322754\n",
      "Iteration 46420 Training loss 0.08798408508300781 Validation loss 0.09282500296831131 Accuracy 0.7250000238418579\n",
      "Iteration 46430 Training loss 0.10099155455827713 Validation loss 0.10783542692661285 Accuracy 0.6545000076293945\n",
      "Iteration 46440 Training loss 0.10103989392518997 Validation loss 0.09405121207237244 Accuracy 0.7280000448226929\n",
      "Iteration 46450 Training loss 0.10408736020326614 Validation loss 0.09283111244440079 Accuracy 0.7330000400543213\n",
      "Iteration 46460 Training loss 0.08802223950624466 Validation loss 0.09080076962709427 Accuracy 0.734000027179718\n",
      "Iteration 46470 Training loss 0.0927761048078537 Validation loss 0.1274908483028412 Accuracy 0.6415000557899475\n",
      "Iteration 46480 Training loss 0.11094355583190918 Validation loss 0.09728419780731201 Accuracy 0.718000054359436\n",
      "Iteration 46490 Training loss 0.10083916783332825 Validation loss 0.09236817061901093 Accuracy 0.7315000295639038\n",
      "Iteration 46500 Training loss 0.1114099845290184 Validation loss 0.11253100633621216 Accuracy 0.6495000123977661\n",
      "Iteration 46510 Training loss 0.08516132831573486 Validation loss 0.09887958317995071 Accuracy 0.7070000171661377\n",
      "Iteration 46520 Training loss 0.1125345379114151 Validation loss 0.13017979264259338 Accuracy 0.5805000066757202\n",
      "Iteration 46530 Training loss 0.0830044224858284 Validation loss 0.09594443440437317 Accuracy 0.7075000405311584\n",
      "Iteration 46540 Training loss 0.0957188755273819 Validation loss 0.09383798390626907 Accuracy 0.7305000424385071\n",
      "Iteration 46550 Training loss 0.1192660704255104 Validation loss 0.11914056539535522 Accuracy 0.6530000567436218\n",
      "Iteration 46560 Training loss 0.09014201164245605 Validation loss 0.09564109891653061 Accuracy 0.7105000615119934\n",
      "Iteration 46570 Training loss 0.09446487575769424 Validation loss 0.09959466755390167 Accuracy 0.6970000267028809\n",
      "Iteration 46580 Training loss 0.08562835305929184 Validation loss 0.09215404093265533 Accuracy 0.7200000286102295\n",
      "Iteration 46590 Training loss 0.09363675862550735 Validation loss 0.09297089278697968 Accuracy 0.7230000495910645\n",
      "Iteration 46600 Training loss 0.0882113128900528 Validation loss 0.09254492819309235 Accuracy 0.7235000133514404\n",
      "Iteration 46610 Training loss 0.08856064826250076 Validation loss 0.09673281013965607 Accuracy 0.7120000123977661\n",
      "Iteration 46620 Training loss 0.10076401382684708 Validation loss 0.12213306128978729 Accuracy 0.627500057220459\n",
      "Iteration 46630 Training loss 0.10616445541381836 Validation loss 0.09096579998731613 Accuracy 0.7410000562667847\n",
      "Iteration 46640 Training loss 0.08915791660547256 Validation loss 0.09253746271133423 Accuracy 0.7350000143051147\n",
      "Iteration 46650 Training loss 0.09618651866912842 Validation loss 0.10694503784179688 Accuracy 0.6800000071525574\n",
      "Iteration 46660 Training loss 0.08521823585033417 Validation loss 0.09173395484685898 Accuracy 0.733500063419342\n",
      "Iteration 46670 Training loss 0.08293378353118896 Validation loss 0.09356572479009628 Accuracy 0.721500039100647\n",
      "Iteration 46680 Training loss 0.08295382559299469 Validation loss 0.09146793186664581 Accuracy 0.7250000238418579\n",
      "Iteration 46690 Training loss 0.08769407868385315 Validation loss 0.09695561975240707 Accuracy 0.7160000205039978\n",
      "Iteration 46700 Training loss 0.08763296902179718 Validation loss 0.09159330278635025 Accuracy 0.7315000295639038\n",
      "Iteration 46710 Training loss 0.08217526972293854 Validation loss 0.09122038632631302 Accuracy 0.7355000376701355\n",
      "Iteration 46720 Training loss 0.09649747610092163 Validation loss 0.10058518499135971 Accuracy 0.7005000114440918\n",
      "Iteration 46730 Training loss 0.09139148145914078 Validation loss 0.09553183615207672 Accuracy 0.7190000414848328\n",
      "Iteration 46740 Training loss 0.08245847374200821 Validation loss 0.0893251821398735 Accuracy 0.7380000352859497\n",
      "Iteration 46750 Training loss 0.09245101362466812 Validation loss 0.09222215414047241 Accuracy 0.7275000214576721\n",
      "Iteration 46760 Training loss 0.09431363642215729 Validation loss 0.1074720248579979 Accuracy 0.6675000190734863\n",
      "Iteration 46770 Training loss 0.10564443469047546 Validation loss 0.09907963871955872 Accuracy 0.703000009059906\n",
      "Iteration 46780 Training loss 0.09879680722951889 Validation loss 0.09764935821294785 Accuracy 0.718000054359436\n",
      "Iteration 46790 Training loss 0.10132905840873718 Validation loss 0.09569588303565979 Accuracy 0.718000054359436\n",
      "Iteration 46800 Training loss 0.08865755051374435 Validation loss 0.10301809012889862 Accuracy 0.6885000467300415\n",
      "Iteration 46810 Training loss 0.08191530406475067 Validation loss 0.09486027806997299 Accuracy 0.718500018119812\n",
      "Iteration 46820 Training loss 0.08328168094158173 Validation loss 0.0978613793849945 Accuracy 0.7130000591278076\n",
      "Iteration 46830 Training loss 0.09726522117853165 Validation loss 0.0937122106552124 Accuracy 0.7300000190734863\n",
      "Iteration 46840 Training loss 0.08591141551733017 Validation loss 0.0922720655798912 Accuracy 0.7305000424385071\n",
      "Iteration 46850 Training loss 0.08905407786369324 Validation loss 0.09954161942005157 Accuracy 0.6880000233650208\n",
      "Iteration 46860 Training loss 0.1108960285782814 Validation loss 0.10611354559659958 Accuracy 0.6485000252723694\n",
      "Iteration 46870 Training loss 0.09347647428512573 Validation loss 0.09771126508712769 Accuracy 0.7005000114440918\n",
      "Iteration 46880 Training loss 0.0932159423828125 Validation loss 0.09917501360177994 Accuracy 0.7085000276565552\n",
      "Iteration 46890 Training loss 0.08589485287666321 Validation loss 0.09571029990911484 Accuracy 0.7040000557899475\n",
      "Iteration 46900 Training loss 0.10339294373989105 Validation loss 0.10361765325069427 Accuracy 0.6980000138282776\n",
      "Iteration 46910 Training loss 0.07998404651880264 Validation loss 0.09816473722457886 Accuracy 0.7055000066757202\n",
      "Iteration 46920 Training loss 0.0877586230635643 Validation loss 0.09221324324607849 Accuracy 0.7385000586509705\n",
      "Iteration 46930 Training loss 0.08508028090000153 Validation loss 0.09458723664283752 Accuracy 0.7260000109672546\n",
      "Iteration 46940 Training loss 0.09281274676322937 Validation loss 0.09582307934761047 Accuracy 0.7165000438690186\n",
      "Iteration 46950 Training loss 0.10749060660600662 Validation loss 0.1088615134358406 Accuracy 0.6710000038146973\n",
      "Iteration 46960 Training loss 0.09470571577548981 Validation loss 0.09302795678377151 Accuracy 0.7280000448226929\n",
      "Iteration 46970 Training loss 0.07009001821279526 Validation loss 0.092032290995121 Accuracy 0.7380000352859497\n",
      "Iteration 46980 Training loss 0.08957832306623459 Validation loss 0.09840278327465057 Accuracy 0.7110000252723694\n",
      "Iteration 46990 Training loss 0.08065220713615417 Validation loss 0.09234629571437836 Accuracy 0.7310000061988831\n",
      "Iteration 47000 Training loss 0.07592397183179855 Validation loss 0.09495866298675537 Accuracy 0.7115000486373901\n",
      "Iteration 47010 Training loss 0.08986642956733704 Validation loss 0.10054955631494522 Accuracy 0.6935000419616699\n",
      "Iteration 47020 Training loss 0.08857311308383942 Validation loss 0.09587333351373672 Accuracy 0.7105000615119934\n",
      "Iteration 47030 Training loss 0.13376399874687195 Validation loss 0.12484495341777802 Accuracy 0.6335000395774841\n",
      "Iteration 47040 Training loss 0.0927690640091896 Validation loss 0.09502213448286057 Accuracy 0.7155000567436218\n",
      "Iteration 47050 Training loss 0.13276246190071106 Validation loss 0.1394140124320984 Accuracy 0.6005000472068787\n",
      "Iteration 47060 Training loss 0.09446173161268234 Validation loss 0.09426058083772659 Accuracy 0.7145000100135803\n",
      "Iteration 47070 Training loss 0.07966562360525131 Validation loss 0.09508507698774338 Accuracy 0.7315000295639038\n",
      "Iteration 47080 Training loss 0.08344949781894684 Validation loss 0.09393978118896484 Accuracy 0.7250000238418579\n",
      "Iteration 47090 Training loss 0.09977855533361435 Validation loss 0.09866473078727722 Accuracy 0.7035000324249268\n",
      "Iteration 47100 Training loss 0.09805867820978165 Validation loss 0.09844175726175308 Accuracy 0.7195000052452087\n",
      "Iteration 47110 Training loss 0.10354562848806381 Validation loss 0.11166644841432571 Accuracy 0.6390000581741333\n",
      "Iteration 47120 Training loss 0.07330768555402756 Validation loss 0.09322265535593033 Accuracy 0.7350000143051147\n",
      "Iteration 47130 Training loss 0.09420952945947647 Validation loss 0.09513378143310547 Accuracy 0.7230000495910645\n",
      "Iteration 47140 Training loss 0.07990884780883789 Validation loss 0.09291466325521469 Accuracy 0.733500063419342\n",
      "Iteration 47150 Training loss 0.11129084974527359 Validation loss 0.10670916736125946 Accuracy 0.6720000505447388\n",
      "Iteration 47160 Training loss 0.10923433303833008 Validation loss 0.09724609553813934 Accuracy 0.7130000591278076\n",
      "Iteration 47170 Training loss 0.11444733291864395 Validation loss 0.09798169136047363 Accuracy 0.7135000228881836\n",
      "Iteration 47180 Training loss 0.10221723467111588 Validation loss 0.10197300463914871 Accuracy 0.6945000290870667\n",
      "Iteration 47190 Training loss 0.08678839355707169 Validation loss 0.09520084410905838 Accuracy 0.7235000133514404\n",
      "Iteration 47200 Training loss 0.12410865724086761 Validation loss 0.12764419615268707 Accuracy 0.6095000505447388\n",
      "Iteration 47210 Training loss 0.09639188647270203 Validation loss 0.09714747220277786 Accuracy 0.7130000591278076\n",
      "Iteration 47220 Training loss 0.08742572367191315 Validation loss 0.09361358731985092 Accuracy 0.7255000472068787\n",
      "Iteration 47230 Training loss 0.0861770510673523 Validation loss 0.09465871006250381 Accuracy 0.7190000414848328\n",
      "Iteration 47240 Training loss 0.11032422631978989 Validation loss 0.09996334463357925 Accuracy 0.7000000476837158\n",
      "Iteration 47250 Training loss 0.09411200135946274 Validation loss 0.09600436687469482 Accuracy 0.7220000624656677\n",
      "Iteration 47260 Training loss 0.09697507321834564 Validation loss 0.09982150793075562 Accuracy 0.7080000042915344\n",
      "Iteration 47270 Training loss 0.08974535018205643 Validation loss 0.09683314710855484 Accuracy 0.7230000495910645\n",
      "Iteration 47280 Training loss 0.10386189818382263 Validation loss 0.10222867131233215 Accuracy 0.6910000443458557\n",
      "Iteration 47290 Training loss 0.10478191822767258 Validation loss 0.09426116943359375 Accuracy 0.7245000600814819\n",
      "Iteration 47300 Training loss 0.09142093360424042 Validation loss 0.1016387939453125 Accuracy 0.6950000524520874\n",
      "Iteration 47310 Training loss 0.0912914052605629 Validation loss 0.10169072449207306 Accuracy 0.687000036239624\n",
      "Iteration 47320 Training loss 0.09625786542892456 Validation loss 0.09846369177103043 Accuracy 0.7100000381469727\n",
      "Iteration 47330 Training loss 0.09963718056678772 Validation loss 0.10183508694171906 Accuracy 0.6970000267028809\n",
      "Iteration 47340 Training loss 0.08087636530399323 Validation loss 0.10822173207998276 Accuracy 0.6830000281333923\n",
      "Iteration 47350 Training loss 0.11056055128574371 Validation loss 0.10691795498132706 Accuracy 0.6785000562667847\n",
      "Iteration 47360 Training loss 0.08125606179237366 Validation loss 0.09239526093006134 Accuracy 0.7265000343322754\n",
      "Iteration 47370 Training loss 0.09182029217481613 Validation loss 0.09489432722330093 Accuracy 0.7265000343322754\n",
      "Iteration 47380 Training loss 0.10147008299827576 Validation loss 0.09437088668346405 Accuracy 0.7230000495910645\n",
      "Iteration 47390 Training loss 0.0783332884311676 Validation loss 0.09313380718231201 Accuracy 0.7330000400543213\n",
      "Iteration 47400 Training loss 0.11469075828790665 Validation loss 0.09546710550785065 Accuracy 0.718500018119812\n",
      "Iteration 47410 Training loss 0.08423730731010437 Validation loss 0.09216347336769104 Accuracy 0.7295000553131104\n",
      "Iteration 47420 Training loss 0.0919560119509697 Validation loss 0.09018434584140778 Accuracy 0.7350000143051147\n",
      "Iteration 47430 Training loss 0.09563815593719482 Validation loss 0.10581741482019424 Accuracy 0.687000036239624\n",
      "Iteration 47440 Training loss 0.10416077077388763 Validation loss 0.11890984326601028 Accuracy 0.5915000438690186\n",
      "Iteration 47450 Training loss 0.1274234652519226 Validation loss 0.11840395629405975 Accuracy 0.6175000071525574\n",
      "Iteration 47460 Training loss 0.08919308334589005 Validation loss 0.09673314541578293 Accuracy 0.7160000205039978\n",
      "Iteration 47470 Training loss 0.08522599935531616 Validation loss 0.09380685538053513 Accuracy 0.7260000109672546\n",
      "Iteration 47480 Training loss 0.11275729537010193 Validation loss 0.09944190084934235 Accuracy 0.6960000395774841\n",
      "Iteration 47490 Training loss 0.0913325771689415 Validation loss 0.0928056389093399 Accuracy 0.7325000166893005\n",
      "Iteration 47500 Training loss 0.1022975742816925 Validation loss 0.10296694189310074 Accuracy 0.6990000605583191\n",
      "Iteration 47510 Training loss 0.0971231460571289 Validation loss 0.09515692293643951 Accuracy 0.7225000262260437\n",
      "Iteration 47520 Training loss 0.10589433461427689 Validation loss 0.09219560027122498 Accuracy 0.7275000214576721\n",
      "Iteration 47530 Training loss 0.08851107209920883 Validation loss 0.09362331032752991 Accuracy 0.7330000400543213\n",
      "Iteration 47540 Training loss 0.09159278869628906 Validation loss 0.09377595782279968 Accuracy 0.7165000438690186\n",
      "Iteration 47550 Training loss 0.08738120645284653 Validation loss 0.09480589628219604 Accuracy 0.7210000157356262\n",
      "Iteration 47560 Training loss 0.09906667470932007 Validation loss 0.10067014396190643 Accuracy 0.6945000290870667\n",
      "Iteration 47570 Training loss 0.11530370265245438 Validation loss 0.10229082405567169 Accuracy 0.6875000596046448\n",
      "Iteration 47580 Training loss 0.08382127434015274 Validation loss 0.0981639102101326 Accuracy 0.7050000429153442\n",
      "Iteration 47590 Training loss 0.0806371420621872 Validation loss 0.09069539606571198 Accuracy 0.734000027179718\n",
      "Iteration 47600 Training loss 0.08630441129207611 Validation loss 0.09308838844299316 Accuracy 0.718500018119812\n",
      "Iteration 47610 Training loss 0.0885673239827156 Validation loss 0.09418829530477524 Accuracy 0.7210000157356262\n",
      "Iteration 47620 Training loss 0.08793212473392487 Validation loss 0.10093227028846741 Accuracy 0.687000036239624\n",
      "Iteration 47630 Training loss 0.1130170226097107 Validation loss 0.0944768413901329 Accuracy 0.7240000367164612\n",
      "Iteration 47640 Training loss 0.07779397070407867 Validation loss 0.0898757204413414 Accuracy 0.7430000305175781\n",
      "Iteration 47650 Training loss 0.09406913816928864 Validation loss 0.0930766612291336 Accuracy 0.7330000400543213\n",
      "Iteration 47660 Training loss 0.08932383358478546 Validation loss 0.09392755478620529 Accuracy 0.7355000376701355\n",
      "Iteration 47670 Training loss 0.08764076232910156 Validation loss 0.09841513633728027 Accuracy 0.70250004529953\n",
      "Iteration 47680 Training loss 0.09808234125375748 Validation loss 0.09354812651872635 Accuracy 0.7345000505447388\n",
      "Iteration 47690 Training loss 0.08723749220371246 Validation loss 0.10056617856025696 Accuracy 0.6930000185966492\n",
      "Iteration 47700 Training loss 0.1039685383439064 Validation loss 0.10792431980371475 Accuracy 0.690000057220459\n",
      "Iteration 47710 Training loss 0.09851455688476562 Validation loss 0.10841628164052963 Accuracy 0.6530000567436218\n",
      "Iteration 47720 Training loss 0.08525146543979645 Validation loss 0.0944402813911438 Accuracy 0.7235000133514404\n",
      "Iteration 47730 Training loss 0.12027415633201599 Validation loss 0.09847742319107056 Accuracy 0.7000000476837158\n",
      "Iteration 47740 Training loss 0.12119966000318527 Validation loss 0.10599356889724731 Accuracy 0.6800000071525574\n",
      "Iteration 47750 Training loss 0.08574429154396057 Validation loss 0.09532425552606583 Accuracy 0.7145000100135803\n",
      "Iteration 47760 Training loss 0.09800941497087479 Validation loss 0.10280493646860123 Accuracy 0.6710000038146973\n",
      "Iteration 47770 Training loss 0.09467552602291107 Validation loss 0.09014655649662018 Accuracy 0.7355000376701355\n",
      "Iteration 47780 Training loss 0.0876510962843895 Validation loss 0.09407763183116913 Accuracy 0.7265000343322754\n",
      "Iteration 47790 Training loss 0.10022330284118652 Validation loss 0.09981825947761536 Accuracy 0.706000030040741\n",
      "Iteration 47800 Training loss 0.12825627624988556 Validation loss 0.12446539849042892 Accuracy 0.5990000367164612\n",
      "Iteration 47810 Training loss 0.08511935919523239 Validation loss 0.09809140115976334 Accuracy 0.7085000276565552\n",
      "Iteration 47820 Training loss 0.08906307071447372 Validation loss 0.10140370577573776 Accuracy 0.7075000405311584\n",
      "Iteration 47830 Training loss 0.09754806011915207 Validation loss 0.10756032168865204 Accuracy 0.6645000576972961\n",
      "Iteration 47840 Training loss 0.08795704692602158 Validation loss 0.09374304860830307 Accuracy 0.7295000553131104\n",
      "Iteration 47850 Training loss 0.11204377561807632 Validation loss 0.11144863814115524 Accuracy 0.6630000472068787\n",
      "Iteration 47860 Training loss 0.11254887282848358 Validation loss 0.09691279381513596 Accuracy 0.7175000309944153\n",
      "Iteration 47870 Training loss 0.08635171502828598 Validation loss 0.09124895185232162 Accuracy 0.7310000061988831\n",
      "Iteration 47880 Training loss 0.09057853370904922 Validation loss 0.09421896189451218 Accuracy 0.737000048160553\n",
      "Iteration 47890 Training loss 0.10178396105766296 Validation loss 0.09511692821979523 Accuracy 0.7210000157356262\n",
      "Iteration 47900 Training loss 0.0901571661233902 Validation loss 0.09488464891910553 Accuracy 0.7140000462532043\n",
      "Iteration 47910 Training loss 0.09954015165567398 Validation loss 0.10189568996429443 Accuracy 0.690000057220459\n",
      "Iteration 47920 Training loss 0.09653810411691666 Validation loss 0.10205889493227005 Accuracy 0.6835000514984131\n",
      "Iteration 47930 Training loss 0.07873022556304932 Validation loss 0.09710603952407837 Accuracy 0.7090000510215759\n",
      "Iteration 47940 Training loss 0.0711517408490181 Validation loss 0.0979553610086441 Accuracy 0.7065000534057617\n",
      "Iteration 47950 Training loss 0.09086956083774567 Validation loss 0.09511040151119232 Accuracy 0.7315000295639038\n",
      "Iteration 47960 Training loss 0.07617275416851044 Validation loss 0.10558377206325531 Accuracy 0.6830000281333923\n",
      "Iteration 47970 Training loss 0.08099763095378876 Validation loss 0.09683901071548462 Accuracy 0.7040000557899475\n",
      "Iteration 47980 Training loss 0.08664878457784653 Validation loss 0.09972254186868668 Accuracy 0.7085000276565552\n",
      "Iteration 47990 Training loss 0.09488961100578308 Validation loss 0.09882654249668121 Accuracy 0.7115000486373901\n",
      "Iteration 48000 Training loss 0.08184441179037094 Validation loss 0.09447744488716125 Accuracy 0.7125000357627869\n",
      "Iteration 48010 Training loss 0.09567172080278397 Validation loss 0.09706573933362961 Accuracy 0.718000054359436\n",
      "Iteration 48020 Training loss 0.10414963215589523 Validation loss 0.09461846202611923 Accuracy 0.7270000576972961\n",
      "Iteration 48030 Training loss 0.11051551252603531 Validation loss 0.10030793398618698 Accuracy 0.6995000243186951\n",
      "Iteration 48040 Training loss 0.07889089733362198 Validation loss 0.09340199828147888 Accuracy 0.7240000367164612\n",
      "Iteration 48050 Training loss 0.09822242707014084 Validation loss 0.10170336067676544 Accuracy 0.6860000491142273\n",
      "Iteration 48060 Training loss 0.09619798511266708 Validation loss 0.09459579735994339 Accuracy 0.7290000319480896\n",
      "Iteration 48070 Training loss 0.08367819339036942 Validation loss 0.09054441004991531 Accuracy 0.7350000143051147\n",
      "Iteration 48080 Training loss 0.08608012646436691 Validation loss 0.0919208973646164 Accuracy 0.7300000190734863\n",
      "Iteration 48090 Training loss 0.09354442358016968 Validation loss 0.09252988547086716 Accuracy 0.737000048160553\n",
      "Iteration 48100 Training loss 0.09392376989126205 Validation loss 0.10172049701213837 Accuracy 0.6950000524520874\n",
      "Iteration 48110 Training loss 0.08658405393362045 Validation loss 0.09803353250026703 Accuracy 0.7140000462532043\n",
      "Iteration 48120 Training loss 0.0978335365653038 Validation loss 0.09807219356298447 Accuracy 0.7085000276565552\n",
      "Iteration 48130 Training loss 0.08282638341188431 Validation loss 0.09427499771118164 Accuracy 0.7390000224113464\n",
      "Iteration 48140 Training loss 0.0958649218082428 Validation loss 0.09323297441005707 Accuracy 0.7275000214576721\n",
      "Iteration 48150 Training loss 0.09018196910619736 Validation loss 0.09333817660808563 Accuracy 0.7285000085830688\n",
      "Iteration 48160 Training loss 0.09074166417121887 Validation loss 0.09617352485656738 Accuracy 0.7255000472068787\n",
      "Iteration 48170 Training loss 0.09868638962507248 Validation loss 0.08990109711885452 Accuracy 0.7405000329017639\n",
      "Iteration 48180 Training loss 0.10713893920183182 Validation loss 0.097786083817482 Accuracy 0.7020000219345093\n",
      "Iteration 48190 Training loss 0.09273774921894073 Validation loss 0.09863656759262085 Accuracy 0.7170000076293945\n",
      "Iteration 48200 Training loss 0.09659184515476227 Validation loss 0.09185055643320084 Accuracy 0.7360000610351562\n",
      "Iteration 48210 Training loss 0.10259734094142914 Validation loss 0.09237132221460342 Accuracy 0.7300000190734863\n",
      "Iteration 48220 Training loss 0.09099695086479187 Validation loss 0.0987916812300682 Accuracy 0.7035000324249268\n",
      "Iteration 48230 Training loss 0.0806257575750351 Validation loss 0.09107070416212082 Accuracy 0.7365000247955322\n",
      "Iteration 48240 Training loss 0.11111634969711304 Validation loss 0.1003217026591301 Accuracy 0.6995000243186951\n",
      "Iteration 48250 Training loss 0.10181104391813278 Validation loss 0.09482363611459732 Accuracy 0.7250000238418579\n",
      "Iteration 48260 Training loss 0.09957868605852127 Validation loss 0.10307318717241287 Accuracy 0.6865000128746033\n",
      "Iteration 48270 Training loss 0.10195621848106384 Validation loss 0.09239017963409424 Accuracy 0.7295000553131104\n",
      "Iteration 48280 Training loss 0.10231684148311615 Validation loss 0.1122584417462349 Accuracy 0.656000018119812\n",
      "Iteration 48290 Training loss 0.10604218393564224 Validation loss 0.1073916032910347 Accuracy 0.6700000166893005\n",
      "Iteration 48300 Training loss 0.11868906021118164 Validation loss 0.10410686582326889 Accuracy 0.6825000047683716\n",
      "Iteration 48310 Training loss 0.09296301752328873 Validation loss 0.09348972141742706 Accuracy 0.7325000166893005\n",
      "Iteration 48320 Training loss 0.09144307672977448 Validation loss 0.09794417023658752 Accuracy 0.7070000171661377\n",
      "Iteration 48330 Training loss 0.09354367852210999 Validation loss 0.09535011649131775 Accuracy 0.7205000519752502\n",
      "Iteration 48340 Training loss 0.1375223696231842 Validation loss 0.11694884300231934 Accuracy 0.659500002861023\n",
      "Iteration 48350 Training loss 0.1010097861289978 Validation loss 0.09952335804700851 Accuracy 0.7015000581741333\n",
      "Iteration 48360 Training loss 0.08559121191501617 Validation loss 0.09335047751665115 Accuracy 0.718500018119812\n",
      "Iteration 48370 Training loss 0.08517545461654663 Validation loss 0.08883348852396011 Accuracy 0.7465000152587891\n",
      "Iteration 48380 Training loss 0.10598309338092804 Validation loss 0.09817735850811005 Accuracy 0.6965000033378601\n",
      "Iteration 48390 Training loss 0.08546026796102524 Validation loss 0.09771952778100967 Accuracy 0.7040000557899475\n",
      "Iteration 48400 Training loss 0.10935679823160172 Validation loss 0.10047547519207001 Accuracy 0.6890000104904175\n",
      "Iteration 48410 Training loss 0.08403734117746353 Validation loss 0.09103624522686005 Accuracy 0.734000027179718\n",
      "Iteration 48420 Training loss 0.1051827222108841 Validation loss 0.09289294481277466 Accuracy 0.7405000329017639\n",
      "Iteration 48430 Training loss 0.09542392194271088 Validation loss 0.09759969264268875 Accuracy 0.7070000171661377\n",
      "Iteration 48440 Training loss 0.10615848004817963 Validation loss 0.10842618346214294 Accuracy 0.6725000143051147\n",
      "Iteration 48450 Training loss 0.11318320035934448 Validation loss 0.11002025008201599 Accuracy 0.655500054359436\n",
      "Iteration 48460 Training loss 0.11103859543800354 Validation loss 0.10175865888595581 Accuracy 0.7000000476837158\n",
      "Iteration 48470 Training loss 0.08998570591211319 Validation loss 0.08911652117967606 Accuracy 0.7480000257492065\n",
      "Iteration 48480 Training loss 0.09625276178121567 Validation loss 0.09910762310028076 Accuracy 0.7135000228881836\n",
      "Iteration 48490 Training loss 0.09373531490564346 Validation loss 0.10999143123626709 Accuracy 0.6880000233650208\n",
      "Iteration 48500 Training loss 0.08188434690237045 Validation loss 0.09178590029478073 Accuracy 0.7350000143051147\n",
      "Iteration 48510 Training loss 0.09337282180786133 Validation loss 0.09646640717983246 Accuracy 0.7045000195503235\n",
      "Iteration 48520 Training loss 0.08691325038671494 Validation loss 0.10840678960084915 Accuracy 0.6610000133514404\n",
      "Iteration 48530 Training loss 0.08463311940431595 Validation loss 0.09355287253856659 Accuracy 0.7225000262260437\n",
      "Iteration 48540 Training loss 0.08362073451280594 Validation loss 0.09730954468250275 Accuracy 0.7095000147819519\n",
      "Iteration 48550 Training loss 0.09766067564487457 Validation loss 0.10088727623224258 Accuracy 0.6880000233650208\n",
      "Iteration 48560 Training loss 0.1072821244597435 Validation loss 0.09578679502010345 Accuracy 0.7300000190734863\n",
      "Iteration 48570 Training loss 0.08923816680908203 Validation loss 0.09120195358991623 Accuracy 0.737000048160553\n",
      "Iteration 48580 Training loss 0.10259126126766205 Validation loss 0.0947142019867897 Accuracy 0.7365000247955322\n",
      "Iteration 48590 Training loss 0.09789168834686279 Validation loss 0.09427247941493988 Accuracy 0.7245000600814819\n",
      "Iteration 48600 Training loss 0.10064935684204102 Validation loss 0.10314621776342392 Accuracy 0.6855000257492065\n",
      "Iteration 48610 Training loss 0.09288888424634933 Validation loss 0.0976724699139595 Accuracy 0.7170000076293945\n",
      "Iteration 48620 Training loss 0.10327643901109695 Validation loss 0.10869444161653519 Accuracy 0.6790000200271606\n",
      "Iteration 48630 Training loss 0.09554357081651688 Validation loss 0.09389261156320572 Accuracy 0.7330000400543213\n",
      "Iteration 48640 Training loss 0.12998944520950317 Validation loss 0.10334062576293945 Accuracy 0.6945000290870667\n",
      "Iteration 48650 Training loss 0.12916164100170135 Validation loss 0.116639144718647 Accuracy 0.6155000329017639\n",
      "Iteration 48660 Training loss 0.0998258963227272 Validation loss 0.10604475438594818 Accuracy 0.6690000295639038\n",
      "Iteration 48670 Training loss 0.10603442788124084 Validation loss 0.10207989066839218 Accuracy 0.7010000348091125\n",
      "Iteration 48680 Training loss 0.09246700257062912 Validation loss 0.10014450550079346 Accuracy 0.6880000233650208\n",
      "Iteration 48690 Training loss 0.0962919220328331 Validation loss 0.09405553340911865 Accuracy 0.734000027179718\n",
      "Iteration 48700 Training loss 0.08867444097995758 Validation loss 0.09127707779407501 Accuracy 0.7445000410079956\n",
      "Iteration 48710 Training loss 0.09526699036359787 Validation loss 0.09816443175077438 Accuracy 0.7105000615119934\n",
      "Iteration 48720 Training loss 0.08798713237047195 Validation loss 0.09553361684083939 Accuracy 0.7200000286102295\n",
      "Iteration 48730 Training loss 0.10392189770936966 Validation loss 0.09644924104213715 Accuracy 0.7175000309944153\n",
      "Iteration 48740 Training loss 0.09577251225709915 Validation loss 0.10085347294807434 Accuracy 0.7050000429153442\n",
      "Iteration 48750 Training loss 0.08904161304235458 Validation loss 0.09680957347154617 Accuracy 0.7015000581741333\n",
      "Iteration 48760 Training loss 0.10036037117242813 Validation loss 0.10015398263931274 Accuracy 0.6925000548362732\n",
      "Iteration 48770 Training loss 0.08442185074090958 Validation loss 0.09545978158712387 Accuracy 0.7045000195503235\n",
      "Iteration 48780 Training loss 0.0915694385766983 Validation loss 0.0940767377614975 Accuracy 0.7305000424385071\n",
      "Iteration 48790 Training loss 0.08845414966344833 Validation loss 0.0939655750989914 Accuracy 0.7200000286102295\n",
      "Iteration 48800 Training loss 0.10772539675235748 Validation loss 0.11296076327562332 Accuracy 0.6695000529289246\n",
      "Iteration 48810 Training loss 0.08536635339260101 Validation loss 0.09125573188066483 Accuracy 0.7315000295639038\n",
      "Iteration 48820 Training loss 0.07379045337438583 Validation loss 0.09148561209440231 Accuracy 0.7345000505447388\n",
      "Iteration 48830 Training loss 0.08057381212711334 Validation loss 0.09495006501674652 Accuracy 0.7095000147819519\n",
      "Iteration 48840 Training loss 0.08343662321567535 Validation loss 0.10120651870965958 Accuracy 0.6925000548362732\n",
      "Iteration 48850 Training loss 0.09604979306459427 Validation loss 0.0942206159234047 Accuracy 0.7230000495910645\n",
      "Iteration 48860 Training loss 0.13114868104457855 Validation loss 0.13673798739910126 Accuracy 0.5575000047683716\n",
      "Iteration 48870 Training loss 0.09009889513254166 Validation loss 0.09535221010446548 Accuracy 0.718000054359436\n",
      "Iteration 48880 Training loss 0.09168783575296402 Validation loss 0.09992796927690506 Accuracy 0.706000030040741\n",
      "Iteration 48890 Training loss 0.09186914563179016 Validation loss 0.09528390318155289 Accuracy 0.7205000519752502\n",
      "Iteration 48900 Training loss 0.09979400783777237 Validation loss 0.1010105162858963 Accuracy 0.6990000605583191\n",
      "Iteration 48910 Training loss 0.09777864068746567 Validation loss 0.1070210412144661 Accuracy 0.6695000529289246\n",
      "Iteration 48920 Training loss 0.0804322212934494 Validation loss 0.09851689636707306 Accuracy 0.7150000333786011\n",
      "Iteration 48930 Training loss 0.0976140946149826 Validation loss 0.09353102743625641 Accuracy 0.7285000085830688\n",
      "Iteration 48940 Training loss 0.09456611424684525 Validation loss 0.09382610023021698 Accuracy 0.7270000576972961\n",
      "Iteration 48950 Training loss 0.08788495510816574 Validation loss 0.09535451233386993 Accuracy 0.718000054359436\n",
      "Iteration 48960 Training loss 0.10247993469238281 Validation loss 0.10492616146802902 Accuracy 0.671500027179718\n",
      "Iteration 48970 Training loss 0.07912003993988037 Validation loss 0.09943826496601105 Accuracy 0.7050000429153442\n",
      "Iteration 48980 Training loss 0.1204388216137886 Validation loss 0.11437487602233887 Accuracy 0.6550000309944153\n",
      "Iteration 48990 Training loss 0.09859586507081985 Validation loss 0.09005516767501831 Accuracy 0.7470000386238098\n",
      "Iteration 49000 Training loss 0.09481857717037201 Validation loss 0.08989539742469788 Accuracy 0.7390000224113464\n",
      "Iteration 49010 Training loss 0.09876704961061478 Validation loss 0.09610246121883392 Accuracy 0.7220000624656677\n",
      "Iteration 49020 Training loss 0.09444639086723328 Validation loss 0.10850684344768524 Accuracy 0.6620000600814819\n",
      "Iteration 49030 Training loss 0.10775760561227798 Validation loss 0.10091900825500488 Accuracy 0.6975000500679016\n",
      "Iteration 49040 Training loss 0.07741766422986984 Validation loss 0.09338437020778656 Accuracy 0.7415000200271606\n",
      "Iteration 49050 Training loss 0.0887622982263565 Validation loss 0.09202106297016144 Accuracy 0.7305000424385071\n",
      "Iteration 49060 Training loss 0.09446700662374496 Validation loss 0.09019552916288376 Accuracy 0.7465000152587891\n",
      "Iteration 49070 Training loss 0.10012422502040863 Validation loss 0.1310954988002777 Accuracy 0.6135000586509705\n",
      "Iteration 49080 Training loss 0.0765484869480133 Validation loss 0.09288221597671509 Accuracy 0.7265000343322754\n",
      "Iteration 49090 Training loss 0.10511888563632965 Validation loss 0.10074423253536224 Accuracy 0.7020000219345093\n",
      "Iteration 49100 Training loss 0.11322275549173355 Validation loss 0.12900300323963165 Accuracy 0.6055000424385071\n",
      "Iteration 49110 Training loss 0.09950382262468338 Validation loss 0.08936267346143723 Accuracy 0.7435000538825989\n",
      "Iteration 49120 Training loss 0.09772928804159164 Validation loss 0.10120469331741333 Accuracy 0.6925000548362732\n",
      "Iteration 49130 Training loss 0.09292365610599518 Validation loss 0.11014687269926071 Accuracy 0.6755000352859497\n",
      "Iteration 49140 Training loss 0.10588239133358002 Validation loss 0.10973852127790451 Accuracy 0.6830000281333923\n",
      "Iteration 49150 Training loss 0.07930909842252731 Validation loss 0.08956655114889145 Accuracy 0.7450000643730164\n",
      "Iteration 49160 Training loss 0.09009719640016556 Validation loss 0.09468287974596024 Accuracy 0.7135000228881836\n",
      "Iteration 49170 Training loss 0.09815997630357742 Validation loss 0.09155159443616867 Accuracy 0.7260000109672546\n",
      "Iteration 49180 Training loss 0.07791769504547119 Validation loss 0.09530464559793472 Accuracy 0.7230000495910645\n",
      "Iteration 49190 Training loss 0.13997317850589752 Validation loss 0.1337493509054184 Accuracy 0.6270000338554382\n",
      "Iteration 49200 Training loss 0.08736339211463928 Validation loss 0.09472358971834183 Accuracy 0.7295000553131104\n",
      "Iteration 49210 Training loss 0.08617299795150757 Validation loss 0.09013139456510544 Accuracy 0.7285000085830688\n",
      "Iteration 49220 Training loss 0.09834528714418411 Validation loss 0.09527535736560822 Accuracy 0.7210000157356262\n",
      "Iteration 49230 Training loss 0.09856627881526947 Validation loss 0.09634694457054138 Accuracy 0.7195000052452087\n",
      "Iteration 49240 Training loss 0.09401857107877731 Validation loss 0.09518947452306747 Accuracy 0.721500039100647\n",
      "Iteration 49250 Training loss 0.10061392188072205 Validation loss 0.09626366198062897 Accuracy 0.7135000228881836\n",
      "Iteration 49260 Training loss 0.09442054480314255 Validation loss 0.09270862489938736 Accuracy 0.7305000424385071\n",
      "Iteration 49270 Training loss 0.11083611100912094 Validation loss 0.09763040393590927 Accuracy 0.7160000205039978\n",
      "Iteration 49280 Training loss 0.09955714643001556 Validation loss 0.09464526921510696 Accuracy 0.7265000343322754\n",
      "Iteration 49290 Training loss 0.08550529181957245 Validation loss 0.0967944860458374 Accuracy 0.7075000405311584\n",
      "Iteration 49300 Training loss 0.10176873952150345 Validation loss 0.09541569650173187 Accuracy 0.7235000133514404\n",
      "Iteration 49310 Training loss 0.11071550101041794 Validation loss 0.10621313005685806 Accuracy 0.6620000600814819\n",
      "Iteration 49320 Training loss 0.10881330072879791 Validation loss 0.10215668380260468 Accuracy 0.6810000538825989\n",
      "Iteration 49330 Training loss 0.08572807908058167 Validation loss 0.1020902618765831 Accuracy 0.6880000233650208\n",
      "Iteration 49340 Training loss 0.09504502266645432 Validation loss 0.09838555008172989 Accuracy 0.7145000100135803\n",
      "Iteration 49350 Training loss 0.0954529196023941 Validation loss 0.09610556811094284 Accuracy 0.7140000462532043\n",
      "Iteration 49360 Training loss 0.09286010265350342 Validation loss 0.09195493161678314 Accuracy 0.737000048160553\n",
      "Iteration 49370 Training loss 0.09338556230068207 Validation loss 0.09790388494729996 Accuracy 0.7155000567436218\n",
      "Iteration 49380 Training loss 0.09247283637523651 Validation loss 0.09512848407030106 Accuracy 0.7290000319480896\n",
      "Iteration 49390 Training loss 0.09596958756446838 Validation loss 0.09615496546030045 Accuracy 0.7195000052452087\n",
      "Iteration 49400 Training loss 0.10497237741947174 Validation loss 0.09804953634738922 Accuracy 0.7020000219345093\n",
      "Iteration 49410 Training loss 0.08147498965263367 Validation loss 0.09518937766551971 Accuracy 0.7280000448226929\n",
      "Iteration 49420 Training loss 0.06713315099477768 Validation loss 0.09899967163801193 Accuracy 0.6985000371932983\n",
      "Iteration 49430 Training loss 0.10784070938825607 Validation loss 0.09371886402368546 Accuracy 0.7315000295639038\n",
      "Iteration 49440 Training loss 0.09583007544279099 Validation loss 0.09238453954458237 Accuracy 0.7315000295639038\n",
      "Iteration 49450 Training loss 0.07904095202684402 Validation loss 0.09183157980442047 Accuracy 0.7285000085830688\n",
      "Iteration 49460 Training loss 0.09173265099525452 Validation loss 0.09205548465251923 Accuracy 0.734000027179718\n",
      "Iteration 49470 Training loss 0.11639460921287537 Validation loss 0.10365379601716995 Accuracy 0.6775000095367432\n",
      "Iteration 49480 Training loss 0.09948558360338211 Validation loss 0.093144990503788 Accuracy 0.7300000190734863\n",
      "Iteration 49490 Training loss 0.08544077724218369 Validation loss 0.09341412037611008 Accuracy 0.7230000495910645\n",
      "Iteration 49500 Training loss 0.10591697692871094 Validation loss 0.10538990050554276 Accuracy 0.6815000176429749\n",
      "Iteration 49510 Training loss 0.10139207541942596 Validation loss 0.0937817245721817 Accuracy 0.734000027179718\n",
      "Iteration 49520 Training loss 0.11708125472068787 Validation loss 0.12373074889183044 Accuracy 0.5900000333786011\n",
      "Iteration 49530 Training loss 0.11322958767414093 Validation loss 0.09382687509059906 Accuracy 0.7345000505447388\n",
      "Iteration 49540 Training loss 0.09848569333553314 Validation loss 0.0973542332649231 Accuracy 0.7115000486373901\n",
      "Iteration 49550 Training loss 0.08037040382623672 Validation loss 0.09652595221996307 Accuracy 0.7090000510215759\n",
      "Iteration 49560 Training loss 0.09033268690109253 Validation loss 0.09018810093402863 Accuracy 0.7455000281333923\n",
      "Iteration 49570 Training loss 0.10072045773267746 Validation loss 0.09190884977579117 Accuracy 0.7315000295639038\n",
      "Iteration 49580 Training loss 0.09175975620746613 Validation loss 0.09520060569047928 Accuracy 0.7165000438690186\n",
      "Iteration 49590 Training loss 0.09292490035295486 Validation loss 0.0918101817369461 Accuracy 0.7305000424385071\n",
      "Iteration 49600 Training loss 0.09322715550661087 Validation loss 0.09390734136104584 Accuracy 0.721500039100647\n",
      "Iteration 49610 Training loss 0.10006816685199738 Validation loss 0.0964910015463829 Accuracy 0.7150000333786011\n",
      "Iteration 49620 Training loss 0.08444370329380035 Validation loss 0.09403377026319504 Accuracy 0.7275000214576721\n",
      "Iteration 49630 Training loss 0.12594415247440338 Validation loss 0.1053951308131218 Accuracy 0.6805000305175781\n",
      "Iteration 49640 Training loss 0.0667087510228157 Validation loss 0.09362005442380905 Accuracy 0.7220000624656677\n",
      "Iteration 49650 Training loss 0.089228056371212 Validation loss 0.1014101579785347 Accuracy 0.6990000605583191\n",
      "Iteration 49660 Training loss 0.07591908425092697 Validation loss 0.09167146682739258 Accuracy 0.7355000376701355\n",
      "Iteration 49670 Training loss 0.08474302291870117 Validation loss 0.09223854541778564 Accuracy 0.718500018119812\n",
      "Iteration 49680 Training loss 0.09803863614797592 Validation loss 0.09525565057992935 Accuracy 0.718000054359436\n",
      "Iteration 49690 Training loss 0.07817117869853973 Validation loss 0.09213228523731232 Accuracy 0.733500063419342\n",
      "Iteration 49700 Training loss 0.09517930448055267 Validation loss 0.09629063308238983 Accuracy 0.7160000205039978\n",
      "Iteration 49710 Training loss 0.07734537869691849 Validation loss 0.09272240102291107 Accuracy 0.7255000472068787\n",
      "Iteration 49720 Training loss 0.08735537528991699 Validation loss 0.09321139752864838 Accuracy 0.7430000305175781\n",
      "Iteration 49730 Training loss 0.09336928278207779 Validation loss 0.09491235762834549 Accuracy 0.7155000567436218\n",
      "Iteration 49740 Training loss 0.07576892524957657 Validation loss 0.092086061835289 Accuracy 0.7420000433921814\n",
      "Iteration 49750 Training loss 0.1017448753118515 Validation loss 0.09138376265764236 Accuracy 0.7305000424385071\n",
      "Iteration 49760 Training loss 0.08211994171142578 Validation loss 0.0909167006611824 Accuracy 0.7395000457763672\n",
      "Iteration 49770 Training loss 0.08593818545341492 Validation loss 0.09519621729850769 Accuracy 0.7160000205039978\n",
      "Iteration 49780 Training loss 0.11087135225534439 Validation loss 0.09812039881944656 Accuracy 0.6930000185966492\n",
      "Iteration 49790 Training loss 0.10651344060897827 Validation loss 0.1068343073129654 Accuracy 0.6790000200271606\n",
      "Iteration 49800 Training loss 0.07571947574615479 Validation loss 0.09284936636686325 Accuracy 0.7195000052452087\n",
      "Iteration 49810 Training loss 0.08646167814731598 Validation loss 0.0936841145157814 Accuracy 0.7205000519752502\n",
      "Iteration 49820 Training loss 0.10369472950696945 Validation loss 0.10344621539115906 Accuracy 0.6885000467300415\n",
      "Iteration 49830 Training loss 0.10313805192708969 Validation loss 0.1083047017455101 Accuracy 0.6670000553131104\n",
      "Iteration 49840 Training loss 0.12241849303245544 Validation loss 0.11533939093351364 Accuracy 0.6515000462532043\n",
      "Iteration 49850 Training loss 0.08665406703948975 Validation loss 0.09529685229063034 Accuracy 0.7190000414848328\n",
      "Iteration 49860 Training loss 0.08494442701339722 Validation loss 0.09101667255163193 Accuracy 0.7300000190734863\n",
      "Iteration 49870 Training loss 0.0918133556842804 Validation loss 0.10168181359767914 Accuracy 0.6885000467300415\n",
      "Iteration 49880 Training loss 0.09159643948078156 Validation loss 0.10259738564491272 Accuracy 0.6865000128746033\n",
      "Iteration 49890 Training loss 0.1046328917145729 Validation loss 0.1070702075958252 Accuracy 0.6600000262260437\n",
      "Iteration 49900 Training loss 0.08022148162126541 Validation loss 0.09495978057384491 Accuracy 0.7425000071525574\n",
      "Iteration 49910 Training loss 0.13672271370887756 Validation loss 0.11766576021909714 Accuracy 0.6080000400543213\n",
      "Iteration 49920 Training loss 0.08364564925432205 Validation loss 0.0967838391661644 Accuracy 0.721500039100647\n",
      "Iteration 49930 Training loss 0.08743486553430557 Validation loss 0.09597928822040558 Accuracy 0.7160000205039978\n",
      "Iteration 49940 Training loss 0.10985778272151947 Validation loss 0.09845133870840073 Accuracy 0.6990000605583191\n",
      "Iteration 49950 Training loss 0.09647969156503677 Validation loss 0.10050790756940842 Accuracy 0.7035000324249268\n",
      "Iteration 49960 Training loss 0.09662335366010666 Validation loss 0.09214136749505997 Accuracy 0.7280000448226929\n",
      "Iteration 49970 Training loss 0.08895885199308395 Validation loss 0.09139963239431381 Accuracy 0.7400000095367432\n",
      "Iteration 49980 Training loss 0.1041066125035286 Validation loss 0.09277672320604324 Accuracy 0.7325000166893005\n",
      "Iteration 49990 Training loss 0.08973916620016098 Validation loss 0.09583106637001038 Accuracy 0.7325000166893005\n",
      "Iteration 50000 Training loss 0.07772480696439743 Validation loss 0.09182896465063095 Accuracy 0.733500063419342\n",
      "Iteration 50010 Training loss 0.08977503329515457 Validation loss 0.09641128033399582 Accuracy 0.7035000324249268\n",
      "Iteration 50020 Training loss 0.09587833285331726 Validation loss 0.09184379130601883 Accuracy 0.737000048160553\n",
      "Iteration 50030 Training loss 0.09396648406982422 Validation loss 0.09215949475765228 Accuracy 0.7305000424385071\n",
      "Iteration 50040 Training loss 0.12638996541500092 Validation loss 0.11334457993507385 Accuracy 0.6430000066757202\n",
      "Iteration 50050 Training loss 0.10073713958263397 Validation loss 0.09757492691278458 Accuracy 0.6970000267028809\n",
      "Iteration 50060 Training loss 0.10217078775167465 Validation loss 0.1134878620505333 Accuracy 0.6685000061988831\n",
      "Iteration 50070 Training loss 0.1014338955283165 Validation loss 0.09129487723112106 Accuracy 0.7345000505447388\n",
      "Iteration 50080 Training loss 0.10849113017320633 Validation loss 0.09727837145328522 Accuracy 0.7135000228881836\n",
      "Iteration 50090 Training loss 0.08872123807668686 Validation loss 0.09348934888839722 Accuracy 0.7210000157356262\n",
      "Iteration 50100 Training loss 0.10198640823364258 Validation loss 0.09474901109933853 Accuracy 0.7130000591278076\n",
      "Iteration 50110 Training loss 0.09793500602245331 Validation loss 0.09232383966445923 Accuracy 0.7265000343322754\n",
      "Iteration 50120 Training loss 0.0821918249130249 Validation loss 0.09756303578615189 Accuracy 0.7135000228881836\n",
      "Iteration 50130 Training loss 0.09151391685009003 Validation loss 0.097011499106884 Accuracy 0.7135000228881836\n",
      "Iteration 50140 Training loss 0.09537260979413986 Validation loss 0.09187287092208862 Accuracy 0.7395000457763672\n",
      "Iteration 50150 Training loss 0.09811268001794815 Validation loss 0.09412618726491928 Accuracy 0.7385000586509705\n",
      "Iteration 50160 Training loss 0.10371406376361847 Validation loss 0.10544272512197495 Accuracy 0.6725000143051147\n",
      "Iteration 50170 Training loss 0.09051921963691711 Validation loss 0.09128613770008087 Accuracy 0.7400000095367432\n",
      "Iteration 50180 Training loss 0.07426269352436066 Validation loss 0.08960504829883575 Accuracy 0.7400000095367432\n",
      "Iteration 50190 Training loss 0.09646028280258179 Validation loss 0.0917503833770752 Accuracy 0.737000048160553\n",
      "Iteration 50200 Training loss 0.09529503434896469 Validation loss 0.09315979480743408 Accuracy 0.7395000457763672\n",
      "Iteration 50210 Training loss 0.09459782391786575 Validation loss 0.11349570751190186 Accuracy 0.6610000133514404\n",
      "Iteration 50220 Training loss 0.08847744762897491 Validation loss 0.08841614425182343 Accuracy 0.7515000104904175\n",
      "Iteration 50230 Training loss 0.09243003278970718 Validation loss 0.09986144304275513 Accuracy 0.6995000243186951\n",
      "Iteration 50240 Training loss 0.08544693142175674 Validation loss 0.09840134531259537 Accuracy 0.7075000405311584\n",
      "Iteration 50250 Training loss 0.1195862665772438 Validation loss 0.11091693490743637 Accuracy 0.6320000290870667\n",
      "Iteration 50260 Training loss 0.10450233519077301 Validation loss 0.10293298959732056 Accuracy 0.6855000257492065\n",
      "Iteration 50270 Training loss 0.08718685060739517 Validation loss 0.09718408435583115 Accuracy 0.7245000600814819\n",
      "Iteration 50280 Training loss 0.10481545329093933 Validation loss 0.09318917989730835 Accuracy 0.7315000295639038\n",
      "Iteration 50290 Training loss 0.09648777544498444 Validation loss 0.10551866888999939 Accuracy 0.6760000586509705\n",
      "Iteration 50300 Training loss 0.12130557745695114 Validation loss 0.12515494227409363 Accuracy 0.643500030040741\n",
      "Iteration 50310 Training loss 0.06927433609962463 Validation loss 0.09634090214967728 Accuracy 0.7155000567436218\n",
      "Iteration 50320 Training loss 0.08515617251396179 Validation loss 0.09583044797182083 Accuracy 0.7165000438690186\n",
      "Iteration 50330 Training loss 0.10131090134382248 Validation loss 0.09172338992357254 Accuracy 0.7355000376701355\n",
      "Iteration 50340 Training loss 0.07235631346702576 Validation loss 0.09501149505376816 Accuracy 0.718500018119812\n",
      "Iteration 50350 Training loss 0.10129550844430923 Validation loss 0.09079434722661972 Accuracy 0.733500063419342\n",
      "Iteration 50360 Training loss 0.10148933529853821 Validation loss 0.09672969579696655 Accuracy 0.7125000357627869\n",
      "Iteration 50370 Training loss 0.08664676547050476 Validation loss 0.09465945512056351 Accuracy 0.7190000414848328\n",
      "Iteration 50380 Training loss 0.10824039578437805 Validation loss 0.10429032146930695 Accuracy 0.6865000128746033\n",
      "Iteration 50390 Training loss 0.10925713926553726 Validation loss 0.09435997158288956 Accuracy 0.7210000157356262\n",
      "Iteration 50400 Training loss 0.0870753824710846 Validation loss 0.09686741977930069 Accuracy 0.7040000557899475\n",
      "Iteration 50410 Training loss 0.10577075928449631 Validation loss 0.09536746144294739 Accuracy 0.7125000357627869\n",
      "Iteration 50420 Training loss 0.08732445538043976 Validation loss 0.09959766268730164 Accuracy 0.7050000429153442\n",
      "Iteration 50430 Training loss 0.11841470748186111 Validation loss 0.10794078558683395 Accuracy 0.6735000610351562\n",
      "Iteration 50440 Training loss 0.07468569278717041 Validation loss 0.09598875045776367 Accuracy 0.718500018119812\n",
      "Iteration 50450 Training loss 0.09731544554233551 Validation loss 0.09432359784841537 Accuracy 0.7330000400543213\n",
      "Iteration 50460 Training loss 0.08832288533449173 Validation loss 0.09206663072109222 Accuracy 0.7465000152587891\n",
      "Iteration 50470 Training loss 0.09152861684560776 Validation loss 0.09398066997528076 Accuracy 0.737000048160553\n",
      "Iteration 50480 Training loss 0.12402362376451492 Validation loss 0.10503142327070236 Accuracy 0.6660000085830688\n",
      "Iteration 50490 Training loss 0.09383777529001236 Validation loss 0.09627030044794083 Accuracy 0.7280000448226929\n",
      "Iteration 50500 Training loss 0.09741848707199097 Validation loss 0.1003340557217598 Accuracy 0.7095000147819519\n",
      "Iteration 50510 Training loss 0.09468695521354675 Validation loss 0.09320204704999924 Accuracy 0.7325000166893005\n",
      "Iteration 50520 Training loss 0.14426490664482117 Validation loss 0.17212505638599396 Accuracy 0.562000036239624\n",
      "Iteration 50530 Training loss 0.0957503467798233 Validation loss 0.09403474628925323 Accuracy 0.7235000133514404\n",
      "Iteration 50540 Training loss 0.08277955651283264 Validation loss 0.0914660096168518 Accuracy 0.737000048160553\n",
      "Iteration 50550 Training loss 0.08120424300432205 Validation loss 0.09377560019493103 Accuracy 0.7275000214576721\n",
      "Iteration 50560 Training loss 0.08365104347467422 Validation loss 0.09154144674539566 Accuracy 0.7265000343322754\n",
      "Iteration 50570 Training loss 0.09611238539218903 Validation loss 0.0996488556265831 Accuracy 0.7135000228881836\n",
      "Iteration 50580 Training loss 0.08761855959892273 Validation loss 0.09760420769453049 Accuracy 0.7255000472068787\n",
      "Iteration 50590 Training loss 0.09924498200416565 Validation loss 0.09670166671276093 Accuracy 0.7160000205039978\n",
      "Iteration 50600 Training loss 0.08423561602830887 Validation loss 0.09406965970993042 Accuracy 0.7325000166893005\n",
      "Iteration 50610 Training loss 0.0996365025639534 Validation loss 0.10220765322446823 Accuracy 0.6930000185966492\n",
      "Iteration 50620 Training loss 0.11137817800045013 Validation loss 0.1060742661356926 Accuracy 0.674500048160553\n",
      "Iteration 50630 Training loss 0.08268895745277405 Validation loss 0.09248381853103638 Accuracy 0.733500063419342\n",
      "Iteration 50640 Training loss 0.08669331669807434 Validation loss 0.0921243205666542 Accuracy 0.7310000061988831\n",
      "Iteration 50650 Training loss 0.09025386720895767 Validation loss 0.09404250234365463 Accuracy 0.7265000343322754\n",
      "Iteration 50660 Training loss 0.09343355149030685 Validation loss 0.09323205798864365 Accuracy 0.7280000448226929\n",
      "Iteration 50670 Training loss 0.09321772307157516 Validation loss 0.0952746644616127 Accuracy 0.7240000367164612\n",
      "Iteration 50680 Training loss 0.08730112761259079 Validation loss 0.09183158725500107 Accuracy 0.733500063419342\n",
      "Iteration 50690 Training loss 0.11243419349193573 Validation loss 0.0930645763874054 Accuracy 0.734000027179718\n",
      "Iteration 50700 Training loss 0.09023969620466232 Validation loss 0.09466032683849335 Accuracy 0.7265000343322754\n",
      "Iteration 50710 Training loss 0.08327734470367432 Validation loss 0.09818833321332932 Accuracy 0.7130000591278076\n",
      "Iteration 50720 Training loss 0.10731329768896103 Validation loss 0.10564232617616653 Accuracy 0.6625000238418579\n",
      "Iteration 50730 Training loss 0.09752631187438965 Validation loss 0.1023925319314003 Accuracy 0.6950000524520874\n",
      "Iteration 50740 Training loss 0.09452661126852036 Validation loss 0.09674103558063507 Accuracy 0.7150000333786011\n",
      "Iteration 50750 Training loss 0.10530383884906769 Validation loss 0.10604604333639145 Accuracy 0.6760000586509705\n",
      "Iteration 50760 Training loss 0.09209743142127991 Validation loss 0.09213391691446304 Accuracy 0.7280000448226929\n",
      "Iteration 50770 Training loss 0.08539135754108429 Validation loss 0.09350014477968216 Accuracy 0.7430000305175781\n",
      "Iteration 50780 Training loss 0.09191229939460754 Validation loss 0.10557341575622559 Accuracy 0.6655000448226929\n",
      "Iteration 50790 Training loss 0.10116688162088394 Validation loss 0.1023847684264183 Accuracy 0.6975000500679016\n",
      "Iteration 50800 Training loss 0.07839986681938171 Validation loss 0.09694719314575195 Accuracy 0.7080000042915344\n",
      "Iteration 50810 Training loss 0.09121599793434143 Validation loss 0.09614898264408112 Accuracy 0.7290000319480896\n",
      "Iteration 50820 Training loss 0.10383537411689758 Validation loss 0.09439907968044281 Accuracy 0.7245000600814819\n",
      "Iteration 50830 Training loss 0.15010342001914978 Validation loss 0.13060326874256134 Accuracy 0.6000000238418579\n",
      "Iteration 50840 Training loss 0.0910615622997284 Validation loss 0.09394171833992004 Accuracy 0.7240000367164612\n",
      "Iteration 50850 Training loss 0.07463739812374115 Validation loss 0.09325935691595078 Accuracy 0.734000027179718\n",
      "Iteration 50860 Training loss 0.08038157969713211 Validation loss 0.09562729299068451 Accuracy 0.7175000309944153\n",
      "Iteration 50870 Training loss 0.11421535164117813 Validation loss 0.11470834910869598 Accuracy 0.6170000433921814\n",
      "Iteration 50880 Training loss 0.08610927313566208 Validation loss 0.09309117496013641 Accuracy 0.7265000343322754\n",
      "Iteration 50890 Training loss 0.08310382813215256 Validation loss 0.09072406589984894 Accuracy 0.7405000329017639\n",
      "Iteration 50900 Training loss 0.07931671291589737 Validation loss 0.09293880313634872 Accuracy 0.7295000553131104\n",
      "Iteration 50910 Training loss 0.09545671194791794 Validation loss 0.09604207426309586 Accuracy 0.7230000495910645\n",
      "Iteration 50920 Training loss 0.0887332335114479 Validation loss 0.09566280990839005 Accuracy 0.7240000367164612\n",
      "Iteration 50930 Training loss 0.09673406183719635 Validation loss 0.09074705839157104 Accuracy 0.7345000505447388\n",
      "Iteration 50940 Training loss 0.0997663363814354 Validation loss 0.09307877719402313 Accuracy 0.7245000600814819\n",
      "Iteration 50950 Training loss 0.07125275582075119 Validation loss 0.09286589175462723 Accuracy 0.7395000457763672\n",
      "Iteration 50960 Training loss 0.09350217878818512 Validation loss 0.10327810794115067 Accuracy 0.690500020980835\n",
      "Iteration 50970 Training loss 0.10777532309293747 Validation loss 0.11607532948255539 Accuracy 0.6155000329017639\n",
      "Iteration 50980 Training loss 0.09088150411844254 Validation loss 0.09352032840251923 Accuracy 0.7295000553131104\n",
      "Iteration 50990 Training loss 0.10049943625926971 Validation loss 0.09620584547519684 Accuracy 0.7190000414848328\n",
      "Iteration 51000 Training loss 0.10104596614837646 Validation loss 0.09394410997629166 Accuracy 0.7160000205039978\n",
      "Iteration 51010 Training loss 0.09399623423814774 Validation loss 0.097601979970932 Accuracy 0.7140000462532043\n",
      "Iteration 51020 Training loss 0.09663037955760956 Validation loss 0.09835348278284073 Accuracy 0.6995000243186951\n",
      "Iteration 51030 Training loss 0.08826079219579697 Validation loss 0.09841780364513397 Accuracy 0.6935000419616699\n",
      "Iteration 51040 Training loss 0.08343972265720367 Validation loss 0.09175482392311096 Accuracy 0.7290000319480896\n",
      "Iteration 51050 Training loss 0.1091841384768486 Validation loss 0.09543229639530182 Accuracy 0.7235000133514404\n",
      "Iteration 51060 Training loss 0.0918319895863533 Validation loss 0.09263301640748978 Accuracy 0.7260000109672546\n",
      "Iteration 51070 Training loss 0.08348827064037323 Validation loss 0.09280800074338913 Accuracy 0.7275000214576721\n",
      "Iteration 51080 Training loss 0.10168764740228653 Validation loss 0.11039993166923523 Accuracy 0.659500002861023\n",
      "Iteration 51090 Training loss 0.08661429584026337 Validation loss 0.11211932450532913 Accuracy 0.6675000190734863\n",
      "Iteration 51100 Training loss 0.08870313316583633 Validation loss 0.1026735007762909 Accuracy 0.6795000433921814\n",
      "Iteration 51110 Training loss 0.07946951687335968 Validation loss 0.09464959800243378 Accuracy 0.7080000042915344\n",
      "Iteration 51120 Training loss 0.07445038855075836 Validation loss 0.09037131071090698 Accuracy 0.7365000247955322\n",
      "Iteration 51130 Training loss 0.11061876267194748 Validation loss 0.09954312443733215 Accuracy 0.70250004529953\n",
      "Iteration 51140 Training loss 0.08103988319635391 Validation loss 0.09296941757202148 Accuracy 0.737500011920929\n",
      "Iteration 51150 Training loss 0.10406828671693802 Validation loss 0.09657727181911469 Accuracy 0.7120000123977661\n",
      "Iteration 51160 Training loss 0.09690389782190323 Validation loss 0.09998401254415512 Accuracy 0.7040000557899475\n",
      "Iteration 51170 Training loss 0.09507991373538971 Validation loss 0.09613031893968582 Accuracy 0.7115000486373901\n",
      "Iteration 51180 Training loss 0.09831348806619644 Validation loss 0.09915675967931747 Accuracy 0.6960000395774841\n",
      "Iteration 51190 Training loss 0.09852570295333862 Validation loss 0.0955551341176033 Accuracy 0.7295000553131104\n",
      "Iteration 51200 Training loss 0.09288396686315536 Validation loss 0.09694799035787582 Accuracy 0.7020000219345093\n",
      "Iteration 51210 Training loss 0.08003619313240051 Validation loss 0.09869258850812912 Accuracy 0.6935000419616699\n",
      "Iteration 51220 Training loss 0.08922723680734634 Validation loss 0.09092969447374344 Accuracy 0.7350000143051147\n",
      "Iteration 51230 Training loss 0.0937947928905487 Validation loss 0.0949583351612091 Accuracy 0.7260000109672546\n",
      "Iteration 51240 Training loss 0.09767335653305054 Validation loss 0.09262123703956604 Accuracy 0.7225000262260437\n",
      "Iteration 51250 Training loss 0.09733317792415619 Validation loss 0.09472894668579102 Accuracy 0.7270000576972961\n",
      "Iteration 51260 Training loss 0.08574198186397552 Validation loss 0.09620214253664017 Accuracy 0.7170000076293945\n",
      "Iteration 51270 Training loss 0.09170901775360107 Validation loss 0.09530255943536758 Accuracy 0.7190000414848328\n",
      "Iteration 51280 Training loss 0.10532345622777939 Validation loss 0.09445536881685257 Accuracy 0.7240000367164612\n",
      "Iteration 51290 Training loss 0.09273304045200348 Validation loss 0.1025848239660263 Accuracy 0.6950000524520874\n",
      "Iteration 51300 Training loss 0.1095256581902504 Validation loss 0.10176870971918106 Accuracy 0.6840000152587891\n",
      "Iteration 51310 Training loss 0.12491931766271591 Validation loss 0.11856290698051453 Accuracy 0.6550000309944153\n",
      "Iteration 51320 Training loss 0.09831104427576065 Validation loss 0.09578803181648254 Accuracy 0.7105000615119934\n",
      "Iteration 51330 Training loss 0.0877373069524765 Validation loss 0.09572098404169083 Accuracy 0.7175000309944153\n",
      "Iteration 51340 Training loss 0.09096220880746841 Validation loss 0.0991886556148529 Accuracy 0.6955000162124634\n",
      "Iteration 51350 Training loss 0.10373956710100174 Validation loss 0.10228116065263748 Accuracy 0.6920000314712524\n",
      "Iteration 51360 Training loss 0.07027015835046768 Validation loss 0.0937827005982399 Accuracy 0.7290000319480896\n",
      "Iteration 51370 Training loss 0.09701794385910034 Validation loss 0.09426291286945343 Accuracy 0.7355000376701355\n",
      "Iteration 51380 Training loss 0.10213597863912582 Validation loss 0.0964830070734024 Accuracy 0.7205000519752502\n",
      "Iteration 51390 Training loss 0.10613738745450974 Validation loss 0.0955047607421875 Accuracy 0.7355000376701355\n",
      "Iteration 51400 Training loss 0.10739006847143173 Validation loss 0.10344376415014267 Accuracy 0.6860000491142273\n",
      "Iteration 51410 Training loss 0.09651124477386475 Validation loss 0.1007385179400444 Accuracy 0.6925000548362732\n",
      "Iteration 51420 Training loss 0.09433183819055557 Validation loss 0.09342726320028305 Accuracy 0.718000054359436\n",
      "Iteration 51430 Training loss 0.09233894944190979 Validation loss 0.0937192365527153 Accuracy 0.7290000319480896\n",
      "Iteration 51440 Training loss 0.09927872568368912 Validation loss 0.09568106383085251 Accuracy 0.7095000147819519\n",
      "Iteration 51450 Training loss 0.08947069942951202 Validation loss 0.09390418231487274 Accuracy 0.7325000166893005\n",
      "Iteration 51460 Training loss 0.11730392277240753 Validation loss 0.09752847254276276 Accuracy 0.7120000123977661\n",
      "Iteration 51470 Training loss 0.09050998836755753 Validation loss 0.09691984951496124 Accuracy 0.7150000333786011\n",
      "Iteration 51480 Training loss 0.074388787150383 Validation loss 0.09175674617290497 Accuracy 0.7380000352859497\n",
      "Iteration 51490 Training loss 0.14030124247074127 Validation loss 0.13989228010177612 Accuracy 0.5485000014305115\n",
      "Iteration 51500 Training loss 0.09714207798242569 Validation loss 0.09186068177223206 Accuracy 0.733500063419342\n",
      "Iteration 51510 Training loss 0.12738469243049622 Validation loss 0.130039244890213 Accuracy 0.5585000514984131\n",
      "Iteration 51520 Training loss 0.10194212943315506 Validation loss 0.10168536752462387 Accuracy 0.6925000548362732\n",
      "Iteration 51530 Training loss 0.09199754148721695 Validation loss 0.09341419488191605 Accuracy 0.7285000085830688\n",
      "Iteration 51540 Training loss 0.10177846997976303 Validation loss 0.09788307547569275 Accuracy 0.6935000419616699\n",
      "Iteration 51550 Training loss 0.09234865009784698 Validation loss 0.1281362622976303 Accuracy 0.565500020980835\n",
      "Iteration 51560 Training loss 0.07215339690446854 Validation loss 0.09350333362817764 Accuracy 0.7220000624656677\n",
      "Iteration 51570 Training loss 0.08683736622333527 Validation loss 0.09175773710012436 Accuracy 0.733500063419342\n",
      "Iteration 51580 Training loss 0.0837785005569458 Validation loss 0.09108588844537735 Accuracy 0.7350000143051147\n",
      "Iteration 51590 Training loss 0.09594281762838364 Validation loss 0.09575383365154266 Accuracy 0.733500063419342\n",
      "Iteration 51600 Training loss 0.09020499885082245 Validation loss 0.09335896372795105 Accuracy 0.7260000109672546\n",
      "Iteration 51610 Training loss 0.08150880038738251 Validation loss 0.09186212718486786 Accuracy 0.7350000143051147\n",
      "Iteration 51620 Training loss 0.08412929624319077 Validation loss 0.09554294496774673 Accuracy 0.7155000567436218\n",
      "Iteration 51630 Training loss 0.08190873265266418 Validation loss 0.09458279609680176 Accuracy 0.7280000448226929\n",
      "Iteration 51640 Training loss 0.10018652677536011 Validation loss 0.09351874887943268 Accuracy 0.7205000519752502\n",
      "Iteration 51650 Training loss 0.09385231882333755 Validation loss 0.09646371752023697 Accuracy 0.7095000147819519\n",
      "Iteration 51660 Training loss 0.08023801445960999 Validation loss 0.0923699364066124 Accuracy 0.7285000085830688\n",
      "Iteration 51670 Training loss 0.09801050275564194 Validation loss 0.09349203109741211 Accuracy 0.7365000247955322\n",
      "Iteration 51680 Training loss 0.12107601016759872 Validation loss 0.11170350015163422 Accuracy 0.6600000262260437\n",
      "Iteration 51690 Training loss 0.08977680653333664 Validation loss 0.09654810279607773 Accuracy 0.7130000591278076\n",
      "Iteration 51700 Training loss 0.10206322371959686 Validation loss 0.09513993561267853 Accuracy 0.7220000624656677\n",
      "Iteration 51710 Training loss 0.0871150940656662 Validation loss 0.09903351962566376 Accuracy 0.7155000567436218\n",
      "Iteration 51720 Training loss 0.09577808529138565 Validation loss 0.09973742812871933 Accuracy 0.7040000557899475\n",
      "Iteration 51730 Training loss 0.0957331731915474 Validation loss 0.10001042485237122 Accuracy 0.6935000419616699\n",
      "Iteration 51740 Training loss 0.10361745953559875 Validation loss 0.09112150222063065 Accuracy 0.7330000400543213\n",
      "Iteration 51750 Training loss 0.08183859288692474 Validation loss 0.09123572707176208 Accuracy 0.7285000085830688\n",
      "Iteration 51760 Training loss 0.10589148104190826 Validation loss 0.09119685739278793 Accuracy 0.7440000176429749\n",
      "Iteration 51770 Training loss 0.10236415266990662 Validation loss 0.1002945676445961 Accuracy 0.6830000281333923\n",
      "Iteration 51780 Training loss 0.11030041426420212 Validation loss 0.09830033034086227 Accuracy 0.7105000615119934\n",
      "Iteration 51790 Training loss 0.10598732531070709 Validation loss 0.10650220513343811 Accuracy 0.659500002861023\n",
      "Iteration 51800 Training loss 0.09396297484636307 Validation loss 0.09469350427389145 Accuracy 0.7175000309944153\n",
      "Iteration 51810 Training loss 0.08991733938455582 Validation loss 0.09146171063184738 Accuracy 0.7400000095367432\n",
      "Iteration 51820 Training loss 0.09143556654453278 Validation loss 0.10780926793813705 Accuracy 0.6635000109672546\n",
      "Iteration 51830 Training loss 0.09318111091852188 Validation loss 0.09625755995512009 Accuracy 0.7145000100135803\n",
      "Iteration 51840 Training loss 0.1273818165063858 Validation loss 0.13813380897045135 Accuracy 0.5925000309944153\n",
      "Iteration 51850 Training loss 0.09600243717432022 Validation loss 0.09109555184841156 Accuracy 0.737500011920929\n",
      "Iteration 51860 Training loss 0.09447956085205078 Validation loss 0.0965065211057663 Accuracy 0.7155000567436218\n",
      "Iteration 51870 Training loss 0.10861028730869293 Validation loss 0.09750217944383621 Accuracy 0.7115000486373901\n",
      "Iteration 51880 Training loss 0.08594704419374466 Validation loss 0.09480289369821548 Accuracy 0.7290000319480896\n",
      "Iteration 51890 Training loss 0.08999001979827881 Validation loss 0.09268522262573242 Accuracy 0.7210000157356262\n",
      "Iteration 51900 Training loss 0.07465178519487381 Validation loss 0.09304893016815186 Accuracy 0.7260000109672546\n",
      "Iteration 51910 Training loss 0.100534588098526 Validation loss 0.09508493542671204 Accuracy 0.7205000519752502\n",
      "Iteration 51920 Training loss 0.08750637620687485 Validation loss 0.09279203414916992 Accuracy 0.7280000448226929\n",
      "Iteration 51930 Training loss 0.10657411068677902 Validation loss 0.09386055171489716 Accuracy 0.7200000286102295\n",
      "Iteration 51940 Training loss 0.08890125155448914 Validation loss 0.09488466382026672 Accuracy 0.7285000085830688\n",
      "Iteration 51950 Training loss 0.10030307620763779 Validation loss 0.09499518573284149 Accuracy 0.7120000123977661\n",
      "Iteration 51960 Training loss 0.08709950000047684 Validation loss 0.0946425274014473 Accuracy 0.7160000205039978\n",
      "Iteration 51970 Training loss 0.0929170772433281 Validation loss 0.09150096774101257 Accuracy 0.7305000424385071\n",
      "Iteration 51980 Training loss 0.09062240272760391 Validation loss 0.09552841633558273 Accuracy 0.7140000462532043\n",
      "Iteration 51990 Training loss 0.08534528315067291 Validation loss 0.09355178475379944 Accuracy 0.7190000414848328\n",
      "Iteration 52000 Training loss 0.08910690248012543 Validation loss 0.10070887207984924 Accuracy 0.6980000138282776\n",
      "Iteration 52010 Training loss 0.13471490144729614 Validation loss 0.13851694762706757 Accuracy 0.5635000467300415\n",
      "Iteration 52020 Training loss 0.10214909166097641 Validation loss 0.09677430987358093 Accuracy 0.7305000424385071\n",
      "Iteration 52030 Training loss 0.10078079253435135 Validation loss 0.10008925199508667 Accuracy 0.70250004529953\n",
      "Iteration 52040 Training loss 0.09441133588552475 Validation loss 0.0917879268527031 Accuracy 0.7350000143051147\n",
      "Iteration 52050 Training loss 0.08907539397478104 Validation loss 0.09210821986198425 Accuracy 0.7410000562667847\n",
      "Iteration 52060 Training loss 0.09637677669525146 Validation loss 0.10070826858282089 Accuracy 0.7010000348091125\n",
      "Iteration 52070 Training loss 0.07896709442138672 Validation loss 0.09393266588449478 Accuracy 0.7200000286102295\n",
      "Iteration 52080 Training loss 0.08358275890350342 Validation loss 0.10382550954818726 Accuracy 0.6800000071525574\n",
      "Iteration 52090 Training loss 0.10986869782209396 Validation loss 0.09829745441675186 Accuracy 0.7105000615119934\n",
      "Iteration 52100 Training loss 0.07295623421669006 Validation loss 0.0902773067355156 Accuracy 0.7410000562667847\n",
      "Iteration 52110 Training loss 0.11728958040475845 Validation loss 0.10438498109579086 Accuracy 0.6675000190734863\n",
      "Iteration 52120 Training loss 0.09635110944509506 Validation loss 0.10242408514022827 Accuracy 0.6845000386238098\n",
      "Iteration 52130 Training loss 0.10033503919839859 Validation loss 0.09189310669898987 Accuracy 0.7280000448226929\n",
      "Iteration 52140 Training loss 0.07933919876813889 Validation loss 0.09216207265853882 Accuracy 0.7320000529289246\n",
      "Iteration 52150 Training loss 0.11623901128768921 Validation loss 0.10176265239715576 Accuracy 0.6785000562667847\n",
      "Iteration 52160 Training loss 0.10063547641038895 Validation loss 0.09812720119953156 Accuracy 0.7045000195503235\n",
      "Iteration 52170 Training loss 0.08419878035783768 Validation loss 0.0934896320104599 Accuracy 0.7295000553131104\n",
      "Iteration 52180 Training loss 0.08883719891309738 Validation loss 0.09175559133291245 Accuracy 0.737000048160553\n",
      "Iteration 52190 Training loss 0.08212380111217499 Validation loss 0.0910172164440155 Accuracy 0.7395000457763672\n",
      "Iteration 52200 Training loss 0.09582114964723587 Validation loss 0.095713309943676 Accuracy 0.7275000214576721\n",
      "Iteration 52210 Training loss 0.0922861099243164 Validation loss 0.09706318378448486 Accuracy 0.7100000381469727\n",
      "Iteration 52220 Training loss 0.09179840236902237 Validation loss 0.10712593793869019 Accuracy 0.6660000085830688\n",
      "Iteration 52230 Training loss 0.09561304748058319 Validation loss 0.09583133459091187 Accuracy 0.7245000600814819\n",
      "Iteration 52240 Training loss 0.09312963485717773 Validation loss 0.0935630351305008 Accuracy 0.7320000529289246\n",
      "Iteration 52250 Training loss 0.11231011152267456 Validation loss 0.10728129744529724 Accuracy 0.6830000281333923\n",
      "Iteration 52260 Training loss 0.10775090754032135 Validation loss 0.09876755625009537 Accuracy 0.7095000147819519\n",
      "Iteration 52270 Training loss 0.08909247815608978 Validation loss 0.09508540481328964 Accuracy 0.7255000472068787\n",
      "Iteration 52280 Training loss 0.09131871908903122 Validation loss 0.09976369142532349 Accuracy 0.7015000581741333\n",
      "Iteration 52290 Training loss 0.08960265666246414 Validation loss 0.09568565338850021 Accuracy 0.7140000462532043\n",
      "Iteration 52300 Training loss 0.08096212148666382 Validation loss 0.09511148184537888 Accuracy 0.7225000262260437\n",
      "Iteration 52310 Training loss 0.08714044094085693 Validation loss 0.0961994081735611 Accuracy 0.7135000228881836\n",
      "Iteration 52320 Training loss 0.08632434904575348 Validation loss 0.09356802701950073 Accuracy 0.7210000157356262\n",
      "Iteration 52330 Training loss 0.10589984804391861 Validation loss 0.10260982066392899 Accuracy 0.6930000185966492\n",
      "Iteration 52340 Training loss 0.09488999098539352 Validation loss 0.09384315460920334 Accuracy 0.7295000553131104\n",
      "Iteration 52350 Training loss 0.08307899534702301 Validation loss 0.09486807882785797 Accuracy 0.7330000400543213\n",
      "Iteration 52360 Training loss 0.09470472484827042 Validation loss 0.09536433964967728 Accuracy 0.7255000472068787\n",
      "Iteration 52370 Training loss 0.10186673700809479 Validation loss 0.0916837751865387 Accuracy 0.737000048160553\n",
      "Iteration 52380 Training loss 0.0958390012383461 Validation loss 0.0940316691994667 Accuracy 0.7275000214576721\n",
      "Iteration 52390 Training loss 0.08008429408073425 Validation loss 0.09430002421140671 Accuracy 0.7250000238418579\n",
      "Iteration 52400 Training loss 0.0900646224617958 Validation loss 0.10479820519685745 Accuracy 0.6660000085830688\n",
      "Iteration 52410 Training loss 0.10833849757909775 Validation loss 0.10792101174592972 Accuracy 0.674500048160553\n",
      "Iteration 52420 Training loss 0.10507390648126602 Validation loss 0.09159185737371445 Accuracy 0.7305000424385071\n",
      "Iteration 52430 Training loss 0.09534043073654175 Validation loss 0.09328842163085938 Accuracy 0.7255000472068787\n",
      "Iteration 52440 Training loss 0.07545768469572067 Validation loss 0.09359653294086456 Accuracy 0.7285000085830688\n",
      "Iteration 52450 Training loss 0.08236800879240036 Validation loss 0.08963631838560104 Accuracy 0.7360000610351562\n",
      "Iteration 52460 Training loss 0.09462642669677734 Validation loss 0.10687471181154251 Accuracy 0.6615000367164612\n",
      "Iteration 52470 Training loss 0.09999562054872513 Validation loss 0.10138289630413055 Accuracy 0.6840000152587891\n",
      "Iteration 52480 Training loss 0.09119953215122223 Validation loss 0.09461236745119095 Accuracy 0.7190000414848328\n",
      "Iteration 52490 Training loss 0.09505387395620346 Validation loss 0.09927168488502502 Accuracy 0.6930000185966492\n",
      "Iteration 52500 Training loss 0.09728618711233139 Validation loss 0.10207995027303696 Accuracy 0.6760000586509705\n",
      "Iteration 52510 Training loss 0.10011712461709976 Validation loss 0.09329095482826233 Accuracy 0.7280000448226929\n",
      "Iteration 52520 Training loss 0.09884687513113022 Validation loss 0.09584727883338928 Accuracy 0.7000000476837158\n",
      "Iteration 52530 Training loss 0.0901729017496109 Validation loss 0.09759114682674408 Accuracy 0.6910000443458557\n",
      "Iteration 52540 Training loss 0.1290748119354248 Validation loss 0.12045742571353912 Accuracy 0.6005000472068787\n",
      "Iteration 52550 Training loss 0.08305741846561432 Validation loss 0.0938500165939331 Accuracy 0.7110000252723694\n",
      "Iteration 52560 Training loss 0.08202210068702698 Validation loss 0.09876792132854462 Accuracy 0.7115000486373901\n",
      "Iteration 52570 Training loss 0.08682235330343246 Validation loss 0.09138266742229462 Accuracy 0.7435000538825989\n",
      "Iteration 52580 Training loss 0.11366435885429382 Validation loss 0.10593010485172272 Accuracy 0.6780000329017639\n",
      "Iteration 52590 Training loss 0.09046661108732224 Validation loss 0.09514648467302322 Accuracy 0.7225000262260437\n",
      "Iteration 52600 Training loss 0.07914381474256516 Validation loss 0.09290377050638199 Accuracy 0.7325000166893005\n",
      "Iteration 52610 Training loss 0.10178978741168976 Validation loss 0.09601960331201553 Accuracy 0.7035000324249268\n",
      "Iteration 52620 Training loss 0.10007846355438232 Validation loss 0.09099182486534119 Accuracy 0.7330000400543213\n",
      "Iteration 52630 Training loss 0.09703460335731506 Validation loss 0.09983029961585999 Accuracy 0.6980000138282776\n",
      "Iteration 52640 Training loss 0.08434943109750748 Validation loss 0.09649080783128738 Accuracy 0.7230000495910645\n",
      "Iteration 52650 Training loss 0.09888520836830139 Validation loss 0.0943569540977478 Accuracy 0.7270000576972961\n",
      "Iteration 52660 Training loss 0.08324367552995682 Validation loss 0.09973262250423431 Accuracy 0.6975000500679016\n",
      "Iteration 52670 Training loss 0.09342483431100845 Validation loss 0.09791838377714157 Accuracy 0.7220000624656677\n",
      "Iteration 52680 Training loss 0.08943101763725281 Validation loss 0.09619911760091782 Accuracy 0.718000054359436\n",
      "Iteration 52690 Training loss 0.08486621081829071 Validation loss 0.09483450651168823 Accuracy 0.7260000109672546\n",
      "Iteration 52700 Training loss 0.0860767513513565 Validation loss 0.09169895201921463 Accuracy 0.7435000538825989\n",
      "Iteration 52710 Training loss 0.09524264931678772 Validation loss 0.09463564306497574 Accuracy 0.7290000319480896\n",
      "Iteration 52720 Training loss 0.10210085660219193 Validation loss 0.11890717595815659 Accuracy 0.655500054359436\n",
      "Iteration 52730 Training loss 0.12784427404403687 Validation loss 0.12775984406471252 Accuracy 0.578000009059906\n",
      "Iteration 52740 Training loss 0.09550211578607559 Validation loss 0.09287118911743164 Accuracy 0.7225000262260437\n",
      "Iteration 52750 Training loss 0.11148783564567566 Validation loss 0.1002049446105957 Accuracy 0.6845000386238098\n",
      "Iteration 52760 Training loss 0.1278858184814453 Validation loss 0.11619603633880615 Accuracy 0.6550000309944153\n",
      "Iteration 52770 Training loss 0.08252474665641785 Validation loss 0.09263453632593155 Accuracy 0.7240000367164612\n",
      "Iteration 52780 Training loss 0.09091351926326752 Validation loss 0.09841031581163406 Accuracy 0.7145000100135803\n",
      "Iteration 52790 Training loss 0.09181772917509079 Validation loss 0.09137746691703796 Accuracy 0.7360000610351562\n",
      "Iteration 52800 Training loss 0.07740865647792816 Validation loss 0.093559630215168 Accuracy 0.7270000576972961\n",
      "Iteration 52810 Training loss 0.08723452687263489 Validation loss 0.09357179701328278 Accuracy 0.7230000495910645\n",
      "Iteration 52820 Training loss 0.08978099375963211 Validation loss 0.09160399436950684 Accuracy 0.7465000152587891\n",
      "Iteration 52830 Training loss 0.09300027787685394 Validation loss 0.1052202358841896 Accuracy 0.6875000596046448\n",
      "Iteration 52840 Training loss 0.09004075825214386 Validation loss 0.09619705379009247 Accuracy 0.7135000228881836\n",
      "Iteration 52850 Training loss 0.08638366311788559 Validation loss 0.09189021587371826 Accuracy 0.7380000352859497\n",
      "Iteration 52860 Training loss 0.093828484416008 Validation loss 0.09296757727861404 Accuracy 0.7295000553131104\n",
      "Iteration 52870 Training loss 0.08015426248311996 Validation loss 0.09510095417499542 Accuracy 0.7145000100135803\n",
      "Iteration 52880 Training loss 0.08468816429376602 Validation loss 0.09438090026378632 Accuracy 0.7250000238418579\n",
      "Iteration 52890 Training loss 0.09075362980365753 Validation loss 0.09333115816116333 Accuracy 0.7225000262260437\n",
      "Iteration 52900 Training loss 0.09485195577144623 Validation loss 0.09275456517934799 Accuracy 0.7325000166893005\n",
      "Iteration 52910 Training loss 0.10155706852674484 Validation loss 0.1103396788239479 Accuracy 0.6415000557899475\n",
      "Iteration 52920 Training loss 0.11285293102264404 Validation loss 0.09944306313991547 Accuracy 0.6950000524520874\n",
      "Iteration 52930 Training loss 0.07889249920845032 Validation loss 0.09296280890703201 Accuracy 0.7285000085830688\n",
      "Iteration 52940 Training loss 0.0707370713353157 Validation loss 0.09040996432304382 Accuracy 0.7400000095367432\n",
      "Iteration 52950 Training loss 0.07547971606254578 Validation loss 0.0879211351275444 Accuracy 0.7450000643730164\n",
      "Iteration 52960 Training loss 0.07857395708560944 Validation loss 0.09340714663267136 Accuracy 0.7325000166893005\n",
      "Iteration 52970 Training loss 0.08497737348079681 Validation loss 0.09033891558647156 Accuracy 0.7345000505447388\n",
      "Iteration 52980 Training loss 0.09748832136392593 Validation loss 0.09672851860523224 Accuracy 0.7125000357627869\n",
      "Iteration 52990 Training loss 0.06889209896326065 Validation loss 0.0934186726808548 Accuracy 0.7260000109672546\n",
      "Iteration 53000 Training loss 0.09538314491510391 Validation loss 0.08868285268545151 Accuracy 0.7500000596046448\n",
      "Iteration 53010 Training loss 0.0781947672367096 Validation loss 0.0915728509426117 Accuracy 0.7280000448226929\n",
      "Iteration 53020 Training loss 0.11462970823049545 Validation loss 0.11247873306274414 Accuracy 0.6130000352859497\n",
      "Iteration 53030 Training loss 0.09122399985790253 Validation loss 0.09250921756029129 Accuracy 0.7295000553131104\n",
      "Iteration 53040 Training loss 0.0810864046216011 Validation loss 0.10257472842931747 Accuracy 0.7000000476837158\n",
      "Iteration 53050 Training loss 0.1059160903096199 Validation loss 0.10156900435686111 Accuracy 0.6980000138282776\n",
      "Iteration 53060 Training loss 0.1079239696264267 Validation loss 0.09461820125579834 Accuracy 0.7265000343322754\n",
      "Iteration 53070 Training loss 0.0824349895119667 Validation loss 0.09413129091262817 Accuracy 0.7310000061988831\n",
      "Iteration 53080 Training loss 0.08868886530399323 Validation loss 0.09359056502580643 Accuracy 0.7345000505447388\n",
      "Iteration 53090 Training loss 0.07574481517076492 Validation loss 0.09474001079797745 Accuracy 0.7145000100135803\n",
      "Iteration 53100 Training loss 0.1046171560883522 Validation loss 0.09572016447782516 Accuracy 0.7135000228881836\n",
      "Iteration 53110 Training loss 0.08815950155258179 Validation loss 0.10773491114377975 Accuracy 0.6690000295639038\n",
      "Iteration 53120 Training loss 0.07767613977193832 Validation loss 0.09174802154302597 Accuracy 0.734000027179718\n",
      "Iteration 53130 Training loss 0.10188430547714233 Validation loss 0.10275453329086304 Accuracy 0.6800000071525574\n",
      "Iteration 53140 Training loss 0.11665834486484528 Validation loss 0.11932756006717682 Accuracy 0.5920000076293945\n",
      "Iteration 53150 Training loss 0.10190567374229431 Validation loss 0.10344791412353516 Accuracy 0.6830000281333923\n",
      "Iteration 53160 Training loss 0.10960894078016281 Validation loss 0.09636000543832779 Accuracy 0.7120000123977661\n",
      "Iteration 53170 Training loss 0.09381159394979477 Validation loss 0.09184551984071732 Accuracy 0.7425000071525574\n",
      "Iteration 53180 Training loss 0.10357074439525604 Validation loss 0.09332939237356186 Accuracy 0.7285000085830688\n",
      "Iteration 53190 Training loss 0.08836478739976883 Validation loss 0.0949123203754425 Accuracy 0.7225000262260437\n",
      "Iteration 53200 Training loss 0.09119539707899094 Validation loss 0.09377013891935349 Accuracy 0.7330000400543213\n",
      "Iteration 53210 Training loss 0.08131586760282516 Validation loss 0.0941430851817131 Accuracy 0.7240000367164612\n",
      "Iteration 53220 Training loss 0.0776238664984703 Validation loss 0.09029106795787811 Accuracy 0.733500063419342\n",
      "Iteration 53230 Training loss 0.10087590664625168 Validation loss 0.10510578006505966 Accuracy 0.6760000586509705\n",
      "Iteration 53240 Training loss 0.09820131212472916 Validation loss 0.10278646647930145 Accuracy 0.6990000605583191\n",
      "Iteration 53250 Training loss 0.104897640645504 Validation loss 0.09775947779417038 Accuracy 0.7075000405311584\n",
      "Iteration 53260 Training loss 0.08958879113197327 Validation loss 0.0984966903924942 Accuracy 0.7000000476837158\n",
      "Iteration 53270 Training loss 0.10648979246616364 Validation loss 0.09726079553365707 Accuracy 0.7155000567436218\n",
      "Iteration 53280 Training loss 0.08506636321544647 Validation loss 0.09239693731069565 Accuracy 0.733500063419342\n",
      "Iteration 53290 Training loss 0.07941623777151108 Validation loss 0.0959474965929985 Accuracy 0.7120000123977661\n",
      "Iteration 53300 Training loss 0.121604323387146 Validation loss 0.11575828492641449 Accuracy 0.6085000038146973\n",
      "Iteration 53310 Training loss 0.09579461812973022 Validation loss 0.1016358807682991 Accuracy 0.6990000605583191\n",
      "Iteration 53320 Training loss 0.08716412633657455 Validation loss 0.0945083498954773 Accuracy 0.7175000309944153\n",
      "Iteration 53330 Training loss 0.08442355692386627 Validation loss 0.09445401281118393 Accuracy 0.718000054359436\n",
      "Iteration 53340 Training loss 0.08013738691806793 Validation loss 0.09068585932254791 Accuracy 0.7300000190734863\n",
      "Iteration 53350 Training loss 0.09860894829034805 Validation loss 0.09619049727916718 Accuracy 0.7080000042915344\n",
      "Iteration 53360 Training loss 0.09158298373222351 Validation loss 0.10268853604793549 Accuracy 0.6815000176429749\n",
      "Iteration 53370 Training loss 0.09322474896907806 Validation loss 0.09952789545059204 Accuracy 0.703000009059906\n",
      "Iteration 53380 Training loss 0.15232287347316742 Validation loss 0.13442689180374146 Accuracy 0.5560000538825989\n",
      "Iteration 53390 Training loss 0.08358432352542877 Validation loss 0.09464624524116516 Accuracy 0.7280000448226929\n",
      "Iteration 53400 Training loss 0.09539155662059784 Validation loss 0.09572487324476242 Accuracy 0.7105000615119934\n",
      "Iteration 53410 Training loss 0.08639289438724518 Validation loss 0.09526742994785309 Accuracy 0.7135000228881836\n",
      "Iteration 53420 Training loss 0.1096024140715599 Validation loss 0.09504987299442291 Accuracy 0.718000054359436\n",
      "Iteration 53430 Training loss 0.0994906798005104 Validation loss 0.10613387823104858 Accuracy 0.6650000214576721\n",
      "Iteration 53440 Training loss 0.09789744764566422 Validation loss 0.09131217002868652 Accuracy 0.7430000305175781\n",
      "Iteration 53450 Training loss 0.09540250897407532 Validation loss 0.09566394239664078 Accuracy 0.7240000367164612\n",
      "Iteration 53460 Training loss 0.08575956523418427 Validation loss 0.09259533137083054 Accuracy 0.7385000586509705\n",
      "Iteration 53470 Training loss 0.08365621417760849 Validation loss 0.09406726807355881 Accuracy 0.7155000567436218\n",
      "Iteration 53480 Training loss 0.09724804013967514 Validation loss 0.09024005383253098 Accuracy 0.734000027179718\n",
      "Iteration 53490 Training loss 0.09577111154794693 Validation loss 0.09535642713308334 Accuracy 0.7195000052452087\n",
      "Iteration 53500 Training loss 0.09119300544261932 Validation loss 0.09389971941709518 Accuracy 0.7235000133514404\n",
      "Iteration 53510 Training loss 0.0911090075969696 Validation loss 0.09708935767412186 Accuracy 0.7140000462532043\n",
      "Iteration 53520 Training loss 0.0927400216460228 Validation loss 0.09430870413780212 Accuracy 0.7245000600814819\n",
      "Iteration 53530 Training loss 0.10003932565450668 Validation loss 0.09631868451833725 Accuracy 0.7110000252723694\n",
      "Iteration 53540 Training loss 0.09739303588867188 Validation loss 0.09982439130544662 Accuracy 0.6920000314712524\n",
      "Iteration 53550 Training loss 0.09534154832363129 Validation loss 0.0934891402721405 Accuracy 0.7295000553131104\n",
      "Iteration 53560 Training loss 0.09651249647140503 Validation loss 0.09135600924491882 Accuracy 0.7295000553131104\n",
      "Iteration 53570 Training loss 0.11227598786354065 Validation loss 0.11915893852710724 Accuracy 0.6500000357627869\n",
      "Iteration 53580 Training loss 0.09575225412845612 Validation loss 0.10279114544391632 Accuracy 0.6895000338554382\n",
      "Iteration 53590 Training loss 0.0829770416021347 Validation loss 0.09366964548826218 Accuracy 0.7250000238418579\n",
      "Iteration 53600 Training loss 0.09443438053131104 Validation loss 0.10211549699306488 Accuracy 0.6895000338554382\n",
      "Iteration 53610 Training loss 0.09014225006103516 Validation loss 0.09279132634401321 Accuracy 0.7435000538825989\n",
      "Iteration 53620 Training loss 0.09221602976322174 Validation loss 0.09211312234401703 Accuracy 0.7360000610351562\n",
      "Iteration 53630 Training loss 0.09729409962892532 Validation loss 0.09958221763372421 Accuracy 0.7095000147819519\n",
      "Iteration 53640 Training loss 0.07249100506305695 Validation loss 0.09097142517566681 Accuracy 0.7400000095367432\n",
      "Iteration 53650 Training loss 0.08059867471456528 Validation loss 0.09290242940187454 Accuracy 0.7320000529289246\n",
      "Iteration 53660 Training loss 0.08280757814645767 Validation loss 0.09756895899772644 Accuracy 0.7000000476837158\n",
      "Iteration 53670 Training loss 0.0877513661980629 Validation loss 0.092903733253479 Accuracy 0.7345000505447388\n",
      "Iteration 53680 Training loss 0.10250118374824524 Validation loss 0.10038887709379196 Accuracy 0.7035000324249268\n",
      "Iteration 53690 Training loss 0.07830308377742767 Validation loss 0.0962335392832756 Accuracy 0.7085000276565552\n",
      "Iteration 53700 Training loss 0.08896320313215256 Validation loss 0.09799377620220184 Accuracy 0.7020000219345093\n",
      "Iteration 53710 Training loss 0.09514758735895157 Validation loss 0.10043520480394363 Accuracy 0.6955000162124634\n",
      "Iteration 53720 Training loss 0.08239229023456573 Validation loss 0.0916503369808197 Accuracy 0.7310000061988831\n",
      "Iteration 53730 Training loss 0.08709289133548737 Validation loss 0.10172313451766968 Accuracy 0.7040000557899475\n",
      "Iteration 53740 Training loss 0.07705656439065933 Validation loss 0.09261800348758698 Accuracy 0.7315000295639038\n",
      "Iteration 53750 Training loss 0.11693847179412842 Validation loss 0.12767294049263 Accuracy 0.6140000224113464\n",
      "Iteration 53760 Training loss 0.08232321590185165 Validation loss 0.09513017535209656 Accuracy 0.7365000247955322\n",
      "Iteration 53770 Training loss 0.09245893359184265 Validation loss 0.09658645838499069 Accuracy 0.7150000333786011\n",
      "Iteration 53780 Training loss 0.09072007983922958 Validation loss 0.0967312902212143 Accuracy 0.7255000472068787\n",
      "Iteration 53790 Training loss 0.08223370462656021 Validation loss 0.09318923205137253 Accuracy 0.737000048160553\n",
      "Iteration 53800 Training loss 0.0812714472413063 Validation loss 0.09410306066274643 Accuracy 0.7165000438690186\n",
      "Iteration 53810 Training loss 0.09950615465641022 Validation loss 0.09885955601930618 Accuracy 0.7005000114440918\n",
      "Iteration 53820 Training loss 0.09921993315219879 Validation loss 0.09689930826425552 Accuracy 0.718500018119812\n",
      "Iteration 53830 Training loss 0.0988117977976799 Validation loss 0.10262991487979889 Accuracy 0.6800000071525574\n",
      "Iteration 53840 Training loss 0.11363517493009567 Validation loss 0.09645811468362808 Accuracy 0.7210000157356262\n",
      "Iteration 53850 Training loss 0.09066777676343918 Validation loss 0.09529878199100494 Accuracy 0.7285000085830688\n",
      "Iteration 53860 Training loss 0.08323066681623459 Validation loss 0.09310556203126907 Accuracy 0.7330000400543213\n",
      "Iteration 53870 Training loss 0.10182909667491913 Validation loss 0.10391790419816971 Accuracy 0.6800000071525574\n",
      "Iteration 53880 Training loss 0.11387032270431519 Validation loss 0.11121286451816559 Accuracy 0.640500009059906\n",
      "Iteration 53890 Training loss 0.09798300266265869 Validation loss 0.10029104351997375 Accuracy 0.6910000443458557\n",
      "Iteration 53900 Training loss 0.08636864274740219 Validation loss 0.09695030003786087 Accuracy 0.7170000076293945\n",
      "Iteration 53910 Training loss 0.08374636620283127 Validation loss 0.10214383155107498 Accuracy 0.6910000443458557\n",
      "Iteration 53920 Training loss 0.08374358713626862 Validation loss 0.09260380268096924 Accuracy 0.7235000133514404\n",
      "Iteration 53930 Training loss 0.10270725190639496 Validation loss 0.10999190807342529 Accuracy 0.6650000214576721\n",
      "Iteration 53940 Training loss 0.09009039402008057 Validation loss 0.10188385844230652 Accuracy 0.6915000081062317\n",
      "Iteration 53950 Training loss 0.08575332164764404 Validation loss 0.09232217818498611 Accuracy 0.7355000376701355\n",
      "Iteration 53960 Training loss 0.10823138803243637 Validation loss 0.09585200995206833 Accuracy 0.7110000252723694\n",
      "Iteration 53970 Training loss 0.090428426861763 Validation loss 0.09177473187446594 Accuracy 0.7385000586509705\n",
      "Iteration 53980 Training loss 0.0777258574962616 Validation loss 0.09244462847709656 Accuracy 0.7295000553131104\n",
      "Iteration 53990 Training loss 0.10298732668161392 Validation loss 0.0975092276930809 Accuracy 0.7115000486373901\n",
      "Iteration 54000 Training loss 0.10181557387113571 Validation loss 0.09696828573942184 Accuracy 0.7105000615119934\n",
      "Iteration 54010 Training loss 0.09288155287504196 Validation loss 0.09710565209388733 Accuracy 0.6990000605583191\n",
      "Iteration 54020 Training loss 0.08955118060112 Validation loss 0.0995335727930069 Accuracy 0.6970000267028809\n",
      "Iteration 54030 Training loss 0.09730306267738342 Validation loss 0.1009892150759697 Accuracy 0.6890000104904175\n",
      "Iteration 54040 Training loss 0.08556356281042099 Validation loss 0.09182234853506088 Accuracy 0.7310000061988831\n",
      "Iteration 54050 Training loss 0.0971173569560051 Validation loss 0.09793049842119217 Accuracy 0.7010000348091125\n",
      "Iteration 54060 Training loss 0.08369086682796478 Validation loss 0.09542013704776764 Accuracy 0.718500018119812\n",
      "Iteration 54070 Training loss 0.08483247458934784 Validation loss 0.09330051392316818 Accuracy 0.7280000448226929\n",
      "Iteration 54080 Training loss 0.0947549119591713 Validation loss 0.10296021401882172 Accuracy 0.6940000057220459\n",
      "Iteration 54090 Training loss 0.10718003660440445 Validation loss 0.09728579223155975 Accuracy 0.7015000581741333\n",
      "Iteration 54100 Training loss 0.09103867411613464 Validation loss 0.09199429303407669 Accuracy 0.7345000505447388\n",
      "Iteration 54110 Training loss 0.0890905037522316 Validation loss 0.09490346163511276 Accuracy 0.7230000495910645\n",
      "Iteration 54120 Training loss 0.08032650500535965 Validation loss 0.09586287289857864 Accuracy 0.7065000534057617\n",
      "Iteration 54130 Training loss 0.0961071103811264 Validation loss 0.09849326312541962 Accuracy 0.7085000276565552\n",
      "Iteration 54140 Training loss 0.09872911870479584 Validation loss 0.09746674448251724 Accuracy 0.7070000171661377\n",
      "Iteration 54150 Training loss 0.08058181405067444 Validation loss 0.09495458006858826 Accuracy 0.721500039100647\n",
      "Iteration 54160 Training loss 0.0861188992857933 Validation loss 0.09739448875188828 Accuracy 0.7100000381469727\n",
      "Iteration 54170 Training loss 0.11176301538944244 Validation loss 0.10262826830148697 Accuracy 0.6810000538825989\n",
      "Iteration 54180 Training loss 0.0947289764881134 Validation loss 0.09485248476266861 Accuracy 0.7245000600814819\n",
      "Iteration 54190 Training loss 0.10752572864294052 Validation loss 0.09590671956539154 Accuracy 0.7165000438690186\n",
      "Iteration 54200 Training loss 0.1008155420422554 Validation loss 0.09399662166833878 Accuracy 0.7290000319480896\n",
      "Iteration 54210 Training loss 0.09247615933418274 Validation loss 0.09755206853151321 Accuracy 0.7040000557899475\n",
      "Iteration 54220 Training loss 0.08946197479963303 Validation loss 0.09474695473909378 Accuracy 0.7135000228881836\n",
      "Iteration 54230 Training loss 0.0844561755657196 Validation loss 0.09333314746618271 Accuracy 0.734000027179718\n",
      "Iteration 54240 Training loss 0.0869702473282814 Validation loss 0.1029181182384491 Accuracy 0.6815000176429749\n",
      "Iteration 54250 Training loss 0.08940615504980087 Validation loss 0.09388818591833115 Accuracy 0.7255000472068787\n",
      "Iteration 54260 Training loss 0.09202846139669418 Validation loss 0.10873372107744217 Accuracy 0.6505000591278076\n",
      "Iteration 54270 Training loss 0.14207883179187775 Validation loss 0.12145168334245682 Accuracy 0.6525000333786011\n",
      "Iteration 54280 Training loss 0.09315427392721176 Validation loss 0.09681255370378494 Accuracy 0.7175000309944153\n",
      "Iteration 54290 Training loss 0.08153782039880753 Validation loss 0.09823928773403168 Accuracy 0.7090000510215759\n",
      "Iteration 54300 Training loss 0.08570573478937149 Validation loss 0.09374230355024338 Accuracy 0.7425000071525574\n",
      "Iteration 54310 Training loss 0.08290859311819077 Validation loss 0.09401709586381912 Accuracy 0.7265000343322754\n",
      "Iteration 54320 Training loss 0.09924599528312683 Validation loss 0.10945529490709305 Accuracy 0.674500048160553\n",
      "Iteration 54330 Training loss 0.08867577463388443 Validation loss 0.10052116215229034 Accuracy 0.6940000057220459\n",
      "Iteration 54340 Training loss 0.09208734333515167 Validation loss 0.09861201047897339 Accuracy 0.7070000171661377\n",
      "Iteration 54350 Training loss 0.08211380988359451 Validation loss 0.09068631380796432 Accuracy 0.7430000305175781\n",
      "Iteration 54360 Training loss 0.10232583433389664 Validation loss 0.10063249617815018 Accuracy 0.690500020980835\n",
      "Iteration 54370 Training loss 0.1011073961853981 Validation loss 0.10309474915266037 Accuracy 0.6810000538825989\n",
      "Iteration 54380 Training loss 0.09103211015462875 Validation loss 0.09704822301864624 Accuracy 0.703000009059906\n",
      "Iteration 54390 Training loss 0.09494798630475998 Validation loss 0.09725560992956161 Accuracy 0.6990000605583191\n",
      "Iteration 54400 Training loss 0.06885941326618195 Validation loss 0.09368380159139633 Accuracy 0.7210000157356262\n",
      "Iteration 54410 Training loss 0.10473115742206573 Validation loss 0.09307192265987396 Accuracy 0.737000048160553\n",
      "Iteration 54420 Training loss 0.0839584693312645 Validation loss 0.09853865206241608 Accuracy 0.7140000462532043\n",
      "Iteration 54430 Training loss 0.09573917090892792 Validation loss 0.09198212623596191 Accuracy 0.7405000329017639\n",
      "Iteration 54440 Training loss 0.11194133758544922 Validation loss 0.09323059022426605 Accuracy 0.7165000438690186\n",
      "Iteration 54450 Training loss 0.09176509827375412 Validation loss 0.09779489785432816 Accuracy 0.7045000195503235\n",
      "Iteration 54460 Training loss 0.09501798450946808 Validation loss 0.09217390418052673 Accuracy 0.7315000295639038\n",
      "Iteration 54470 Training loss 0.08476097136735916 Validation loss 0.0947122722864151 Accuracy 0.7175000309944153\n",
      "Iteration 54480 Training loss 0.10284219682216644 Validation loss 0.10152089595794678 Accuracy 0.7020000219345093\n",
      "Iteration 54490 Training loss 0.09680047631263733 Validation loss 0.1035303995013237 Accuracy 0.6775000095367432\n",
      "Iteration 54500 Training loss 0.09274139255285263 Validation loss 0.10236243158578873 Accuracy 0.6830000281333923\n",
      "Iteration 54510 Training loss 0.10998973250389099 Validation loss 0.10979465395212173 Accuracy 0.6385000348091125\n",
      "Iteration 54520 Training loss 0.09014914184808731 Validation loss 0.09297124296426773 Accuracy 0.7395000457763672\n",
      "Iteration 54530 Training loss 0.08717135339975357 Validation loss 0.0968777984380722 Accuracy 0.7205000519752502\n",
      "Iteration 54540 Training loss 0.08246651291847229 Validation loss 0.09554623067378998 Accuracy 0.7115000486373901\n",
      "Iteration 54550 Training loss 0.09760913997888565 Validation loss 0.09811008721590042 Accuracy 0.7055000066757202\n",
      "Iteration 54560 Training loss 0.08820822834968567 Validation loss 0.09656503796577454 Accuracy 0.7195000052452087\n",
      "Iteration 54570 Training loss 0.09072965383529663 Validation loss 0.09459971636533737 Accuracy 0.7245000600814819\n",
      "Iteration 54580 Training loss 0.08263883739709854 Validation loss 0.10172269493341446 Accuracy 0.6885000467300415\n",
      "Iteration 54590 Training loss 0.09350653737783432 Validation loss 0.1074613630771637 Accuracy 0.6665000319480896\n",
      "Iteration 54600 Training loss 0.1096457913517952 Validation loss 0.10278117656707764 Accuracy 0.690500020980835\n",
      "Iteration 54610 Training loss 0.07761340588331223 Validation loss 0.09460289776325226 Accuracy 0.7195000052452087\n",
      "Iteration 54620 Training loss 0.09043510258197784 Validation loss 0.09412365406751633 Accuracy 0.7240000367164612\n",
      "Iteration 54630 Training loss 0.10423813760280609 Validation loss 0.10681125521659851 Accuracy 0.6830000281333923\n",
      "Iteration 54640 Training loss 0.11037524789571762 Validation loss 0.11189782619476318 Accuracy 0.6580000519752502\n",
      "Iteration 54650 Training loss 0.08635714650154114 Validation loss 0.09649701416492462 Accuracy 0.7125000357627869\n",
      "Iteration 54660 Training loss 0.09057119488716125 Validation loss 0.09505967795848846 Accuracy 0.7220000624656677\n",
      "Iteration 54670 Training loss 0.07810716331005096 Validation loss 0.09530271589756012 Accuracy 0.7110000252723694\n",
      "Iteration 54680 Training loss 0.0889073833823204 Validation loss 0.09400099515914917 Accuracy 0.7245000600814819\n",
      "Iteration 54690 Training loss 0.0879458412528038 Validation loss 0.09621661901473999 Accuracy 0.7275000214576721\n",
      "Iteration 54700 Training loss 0.09616096317768097 Validation loss 0.10079846531152725 Accuracy 0.6885000467300415\n",
      "Iteration 54710 Training loss 0.08695052564144135 Validation loss 0.0945456475019455 Accuracy 0.721500039100647\n",
      "Iteration 54720 Training loss 0.117998406291008 Validation loss 0.10271114110946655 Accuracy 0.6955000162124634\n",
      "Iteration 54730 Training loss 0.09159461408853531 Validation loss 0.09600099921226501 Accuracy 0.7070000171661377\n",
      "Iteration 54740 Training loss 0.08542905747890472 Validation loss 0.09307755529880524 Accuracy 0.7150000333786011\n",
      "Iteration 54750 Training loss 0.0732676312327385 Validation loss 0.09375651925802231 Accuracy 0.7195000052452087\n",
      "Iteration 54760 Training loss 0.08730033785104752 Validation loss 0.09732803702354431 Accuracy 0.6990000605583191\n",
      "Iteration 54770 Training loss 0.08952588587999344 Validation loss 0.09322553128004074 Accuracy 0.7220000624656677\n",
      "Iteration 54780 Training loss 0.10391398519277573 Validation loss 0.10252183675765991 Accuracy 0.6880000233650208\n",
      "Iteration 54790 Training loss 0.10084094852209091 Validation loss 0.10432333499193192 Accuracy 0.6805000305175781\n",
      "Iteration 54800 Training loss 0.11467497795820236 Validation loss 0.0928928405046463 Accuracy 0.7280000448226929\n",
      "Iteration 54810 Training loss 0.10109689831733704 Validation loss 0.0974157452583313 Accuracy 0.6990000605583191\n",
      "Iteration 54820 Training loss 0.06940823048353195 Validation loss 0.09453614056110382 Accuracy 0.7290000319480896\n",
      "Iteration 54830 Training loss 0.0877007246017456 Validation loss 0.09005124866962433 Accuracy 0.7400000095367432\n",
      "Iteration 54840 Training loss 0.08744534105062485 Validation loss 0.0910540446639061 Accuracy 0.7445000410079956\n",
      "Iteration 54850 Training loss 0.09261421114206314 Validation loss 0.09767478704452515 Accuracy 0.7100000381469727\n",
      "Iteration 54860 Training loss 0.10863521695137024 Validation loss 0.09666575491428375 Accuracy 0.7100000381469727\n",
      "Iteration 54870 Training loss 0.08017506450414658 Validation loss 0.09737753123044968 Accuracy 0.7095000147819519\n",
      "Iteration 54880 Training loss 0.09037758409976959 Validation loss 0.10075442492961884 Accuracy 0.70250004529953\n",
      "Iteration 54890 Training loss 0.10841527581214905 Validation loss 0.10592881590127945 Accuracy 0.6665000319480896\n",
      "Iteration 54900 Training loss 0.08932023495435715 Validation loss 0.09942913800477982 Accuracy 0.690500020980835\n",
      "Iteration 54910 Training loss 0.11168510466814041 Validation loss 0.12754130363464355 Accuracy 0.6170000433921814\n",
      "Iteration 54920 Training loss 0.09800953418016434 Validation loss 0.096854068338871 Accuracy 0.7230000495910645\n",
      "Iteration 54930 Training loss 0.08228051662445068 Validation loss 0.10038790106773376 Accuracy 0.7090000510215759\n",
      "Iteration 54940 Training loss 0.0844029039144516 Validation loss 0.0967179536819458 Accuracy 0.7150000333786011\n",
      "Iteration 54950 Training loss 0.1082596406340599 Validation loss 0.12045574933290482 Accuracy 0.6070000529289246\n",
      "Iteration 54960 Training loss 0.11501368880271912 Validation loss 0.09715047478675842 Accuracy 0.7070000171661377\n",
      "Iteration 54970 Training loss 0.0920012965798378 Validation loss 0.09333304315805435 Accuracy 0.7200000286102295\n",
      "Iteration 54980 Training loss 0.09483133256435394 Validation loss 0.09166023135185242 Accuracy 0.734000027179718\n",
      "Iteration 54990 Training loss 0.10581918805837631 Validation loss 0.10569226741790771 Accuracy 0.659000039100647\n",
      "Iteration 55000 Training loss 0.10437894612550735 Validation loss 0.09470583498477936 Accuracy 0.7200000286102295\n",
      "Iteration 55010 Training loss 0.10279987007379532 Validation loss 0.10421765595674515 Accuracy 0.6775000095367432\n",
      "Iteration 55020 Training loss 0.09237688034772873 Validation loss 0.096073217689991 Accuracy 0.7175000309944153\n",
      "Iteration 55030 Training loss 0.09267109632492065 Validation loss 0.09609680622816086 Accuracy 0.7165000438690186\n",
      "Iteration 55040 Training loss 0.07991907000541687 Validation loss 0.09599962085485458 Accuracy 0.706000030040741\n",
      "Iteration 55050 Training loss 0.10053447633981705 Validation loss 0.09320565313100815 Accuracy 0.7270000576972961\n",
      "Iteration 55060 Training loss 0.09764256328344345 Validation loss 0.09640800952911377 Accuracy 0.7170000076293945\n",
      "Iteration 55070 Training loss 0.09851181507110596 Validation loss 0.09961534291505814 Accuracy 0.6955000162124634\n",
      "Iteration 55080 Training loss 0.10149076581001282 Validation loss 0.09084538370370865 Accuracy 0.7300000190734863\n",
      "Iteration 55090 Training loss 0.08661801367998123 Validation loss 0.0915563553571701 Accuracy 0.7285000085830688\n",
      "Iteration 55100 Training loss 0.10012081265449524 Validation loss 0.09233792126178741 Accuracy 0.7265000343322754\n",
      "Iteration 55110 Training loss 0.10204112529754639 Validation loss 0.09022875130176544 Accuracy 0.7395000457763672\n",
      "Iteration 55120 Training loss 0.09642376005649567 Validation loss 0.09429538995027542 Accuracy 0.7325000166893005\n",
      "Iteration 55130 Training loss 0.10329082608222961 Validation loss 0.09921876341104507 Accuracy 0.690500020980835\n",
      "Iteration 55140 Training loss 0.0863298624753952 Validation loss 0.08996610343456268 Accuracy 0.7325000166893005\n",
      "Iteration 55150 Training loss 0.12094686180353165 Validation loss 0.11583414673805237 Accuracy 0.6240000128746033\n",
      "Iteration 55160 Training loss 0.1031465232372284 Validation loss 0.09394218027591705 Accuracy 0.7270000576972961\n",
      "Iteration 55170 Training loss 0.08024726808071136 Validation loss 0.0921429693698883 Accuracy 0.7325000166893005\n",
      "Iteration 55180 Training loss 0.10168492794036865 Validation loss 0.10241974145174026 Accuracy 0.6850000619888306\n",
      "Iteration 55190 Training loss 0.08507940918207169 Validation loss 0.10063675045967102 Accuracy 0.6990000605583191\n",
      "Iteration 55200 Training loss 0.10371679067611694 Validation loss 0.09813757985830307 Accuracy 0.7005000114440918\n",
      "Iteration 55210 Training loss 0.10308771580457687 Validation loss 0.10130565613508224 Accuracy 0.6860000491142273\n",
      "Iteration 55220 Training loss 0.10018804669380188 Validation loss 0.10410881787538528 Accuracy 0.690000057220459\n",
      "Iteration 55230 Training loss 0.08625337481498718 Validation loss 0.09733821451663971 Accuracy 0.6990000605583191\n",
      "Iteration 55240 Training loss 0.08645575493574142 Validation loss 0.10497598350048065 Accuracy 0.6845000386238098\n",
      "Iteration 55250 Training loss 0.09615286439657211 Validation loss 0.09679786115884781 Accuracy 0.7125000357627869\n",
      "Iteration 55260 Training loss 0.09251387417316437 Validation loss 0.09303321689367294 Accuracy 0.7235000133514404\n",
      "Iteration 55270 Training loss 0.10184527933597565 Validation loss 0.09099797159433365 Accuracy 0.7360000610351562\n",
      "Iteration 55280 Training loss 0.08155026286840439 Validation loss 0.09660452604293823 Accuracy 0.6995000243186951\n",
      "Iteration 55290 Training loss 0.1104666143655777 Validation loss 0.10274795442819595 Accuracy 0.6855000257492065\n",
      "Iteration 55300 Training loss 0.09340830892324448 Validation loss 0.09686047583818436 Accuracy 0.7100000381469727\n",
      "Iteration 55310 Training loss 0.09330916404724121 Validation loss 0.09410113841295242 Accuracy 0.7345000505447388\n",
      "Iteration 55320 Training loss 0.10147235542535782 Validation loss 0.0966871827840805 Accuracy 0.7175000309944153\n",
      "Iteration 55330 Training loss 0.0946224257349968 Validation loss 0.09459144622087479 Accuracy 0.7125000357627869\n",
      "Iteration 55340 Training loss 0.09025678783655167 Validation loss 0.09051401168107986 Accuracy 0.7430000305175781\n",
      "Iteration 55350 Training loss 0.09701265394687653 Validation loss 0.09683743864297867 Accuracy 0.7055000066757202\n",
      "Iteration 55360 Training loss 0.08496202528476715 Validation loss 0.0946257933974266 Accuracy 0.7240000367164612\n",
      "Iteration 55370 Training loss 0.08261886239051819 Validation loss 0.09371884912252426 Accuracy 0.7160000205039978\n",
      "Iteration 55380 Training loss 0.07765990495681763 Validation loss 0.09716662764549255 Accuracy 0.7170000076293945\n",
      "Iteration 55390 Training loss 0.08655034750699997 Validation loss 0.09530310332775116 Accuracy 0.7170000076293945\n",
      "Iteration 55400 Training loss 0.06920243799686432 Validation loss 0.09062807261943817 Accuracy 0.7355000376701355\n",
      "Iteration 55410 Training loss 0.0698552057147026 Validation loss 0.08939845114946365 Accuracy 0.7435000538825989\n",
      "Iteration 55420 Training loss 0.09662799537181854 Validation loss 0.10097306966781616 Accuracy 0.6990000605583191\n",
      "Iteration 55430 Training loss 0.09433504194021225 Validation loss 0.09792119264602661 Accuracy 0.7070000171661377\n",
      "Iteration 55440 Training loss 0.08427607268095016 Validation loss 0.09553269296884537 Accuracy 0.7080000042915344\n",
      "Iteration 55450 Training loss 0.07455398142337799 Validation loss 0.09969314932823181 Accuracy 0.6975000500679016\n",
      "Iteration 55460 Training loss 0.07763488590717316 Validation loss 0.0966169685125351 Accuracy 0.7155000567436218\n",
      "Iteration 55470 Training loss 0.15191960334777832 Validation loss 0.12773092091083527 Accuracy 0.593500018119812\n",
      "Iteration 55480 Training loss 0.10282783210277557 Validation loss 0.10286920517683029 Accuracy 0.6840000152587891\n",
      "Iteration 55490 Training loss 0.08098697662353516 Validation loss 0.0911865159869194 Accuracy 0.7300000190734863\n",
      "Iteration 55500 Training loss 0.11069516837596893 Validation loss 0.09339422732591629 Accuracy 0.7285000085830688\n",
      "Iteration 55510 Training loss 0.09111163765192032 Validation loss 0.09651290625333786 Accuracy 0.7145000100135803\n",
      "Iteration 55520 Training loss 0.08304834365844727 Validation loss 0.09317218512296677 Accuracy 0.7385000586509705\n",
      "Iteration 55530 Training loss 0.1200166642665863 Validation loss 0.10238935798406601 Accuracy 0.6775000095367432\n",
      "Iteration 55540 Training loss 0.08700807392597198 Validation loss 0.09767627716064453 Accuracy 0.70250004529953\n",
      "Iteration 55550 Training loss 0.10025311261415482 Validation loss 0.0895523950457573 Accuracy 0.7435000538825989\n",
      "Iteration 55560 Training loss 0.08987102657556534 Validation loss 0.09595917165279388 Accuracy 0.7155000567436218\n",
      "Iteration 55570 Training loss 0.09591052681207657 Validation loss 0.0936497375369072 Accuracy 0.7250000238418579\n",
      "Iteration 55580 Training loss 0.08363498747348785 Validation loss 0.09121192246675491 Accuracy 0.737000048160553\n",
      "Iteration 55590 Training loss 0.06730319559574127 Validation loss 0.09255971014499664 Accuracy 0.7300000190734863\n",
      "Iteration 55600 Training loss 0.10322602838277817 Validation loss 0.10264414548873901 Accuracy 0.6910000443458557\n",
      "Iteration 55610 Training loss 0.09415572881698608 Validation loss 0.10363201797008514 Accuracy 0.6810000538825989\n",
      "Iteration 55620 Training loss 0.06895797699689865 Validation loss 0.09381154924631119 Accuracy 0.7225000262260437\n",
      "Iteration 55630 Training loss 0.08053401857614517 Validation loss 0.09287892282009125 Accuracy 0.7285000085830688\n",
      "Iteration 55640 Training loss 0.11137785017490387 Validation loss 0.10029011219739914 Accuracy 0.7095000147819519\n",
      "Iteration 55650 Training loss 0.09044679254293442 Validation loss 0.09321104735136032 Accuracy 0.7170000076293945\n",
      "Iteration 55660 Training loss 0.10493503510951996 Validation loss 0.10537579655647278 Accuracy 0.674500048160553\n",
      "Iteration 55670 Training loss 0.09232167899608612 Validation loss 0.09673652797937393 Accuracy 0.7160000205039978\n",
      "Iteration 55680 Training loss 0.10486702620983124 Validation loss 0.10826009511947632 Accuracy 0.6455000042915344\n",
      "Iteration 55690 Training loss 0.09857027977705002 Validation loss 0.09173962473869324 Accuracy 0.7315000295639038\n",
      "Iteration 55700 Training loss 0.07320947200059891 Validation loss 0.09506643563508987 Accuracy 0.7150000333786011\n",
      "Iteration 55710 Training loss 0.10769875347614288 Validation loss 0.10533346980810165 Accuracy 0.674500048160553\n",
      "Iteration 55720 Training loss 0.07518406212329865 Validation loss 0.0907723680138588 Accuracy 0.7290000319480896\n",
      "Iteration 55730 Training loss 0.08272035419940948 Validation loss 0.09370055049657822 Accuracy 0.7275000214576721\n",
      "Iteration 55740 Training loss 0.08724325895309448 Validation loss 0.11060339957475662 Accuracy 0.6585000157356262\n",
      "Iteration 55750 Training loss 0.06889026612043381 Validation loss 0.09013038128614426 Accuracy 0.737500011920929\n",
      "Iteration 55760 Training loss 0.10329131782054901 Validation loss 0.10807568579912186 Accuracy 0.6450000405311584\n",
      "Iteration 55770 Training loss 0.09913709759712219 Validation loss 0.09528714418411255 Accuracy 0.7250000238418579\n",
      "Iteration 55780 Training loss 0.07768850773572922 Validation loss 0.10284207761287689 Accuracy 0.6970000267028809\n",
      "Iteration 55790 Training loss 0.08748634159564972 Validation loss 0.09127533435821533 Accuracy 0.7265000343322754\n",
      "Iteration 55800 Training loss 0.08372034132480621 Validation loss 0.10025853663682938 Accuracy 0.6860000491142273\n",
      "Iteration 55810 Training loss 0.09128362685441971 Validation loss 0.09994789958000183 Accuracy 0.7010000348091125\n",
      "Iteration 55820 Training loss 0.10976064205169678 Validation loss 0.09329736977815628 Accuracy 0.7160000205039978\n",
      "Iteration 55830 Training loss 0.09664897620677948 Validation loss 0.0900345891714096 Accuracy 0.7360000610351562\n",
      "Iteration 55840 Training loss 0.0813337042927742 Validation loss 0.08934537321329117 Accuracy 0.7460000514984131\n",
      "Iteration 55850 Training loss 0.0847621038556099 Validation loss 0.11067011207342148 Accuracy 0.6720000505447388\n",
      "Iteration 55860 Training loss 0.09314461797475815 Validation loss 0.11856812238693237 Accuracy 0.6500000357627869\n",
      "Iteration 55870 Training loss 0.0950557142496109 Validation loss 0.0967622771859169 Accuracy 0.7120000123977661\n",
      "Iteration 55880 Training loss 0.11288116872310638 Validation loss 0.10072505474090576 Accuracy 0.6940000057220459\n",
      "Iteration 55890 Training loss 0.08591344952583313 Validation loss 0.09638454020023346 Accuracy 0.718500018119812\n",
      "Iteration 55900 Training loss 0.08087033778429031 Validation loss 0.09644106030464172 Accuracy 0.7225000262260437\n",
      "Iteration 55910 Training loss 0.10640785098075867 Validation loss 0.09854448586702347 Accuracy 0.7005000114440918\n",
      "Iteration 55920 Training loss 0.09601403772830963 Validation loss 0.10107814520597458 Accuracy 0.6875000596046448\n",
      "Iteration 55930 Training loss 0.11768365651369095 Validation loss 0.11032932251691818 Accuracy 0.6600000262260437\n",
      "Iteration 55940 Training loss 0.10264764726161957 Validation loss 0.10237570106983185 Accuracy 0.6965000033378601\n",
      "Iteration 55950 Training loss 0.08383834362030029 Validation loss 0.0958564355969429 Accuracy 0.7210000157356262\n",
      "Iteration 55960 Training loss 0.08472442626953125 Validation loss 0.09438104182481766 Accuracy 0.7235000133514404\n",
      "Iteration 55970 Training loss 0.09533659368753433 Validation loss 0.0943777933716774 Accuracy 0.7290000319480896\n",
      "Iteration 55980 Training loss 0.07073294371366501 Validation loss 0.09902666509151459 Accuracy 0.7155000567436218\n",
      "Iteration 55990 Training loss 0.09652938693761826 Validation loss 0.09260638803243637 Accuracy 0.737500011920929\n",
      "Iteration 56000 Training loss 0.09324824064970016 Validation loss 0.09649842977523804 Accuracy 0.7170000076293945\n",
      "Iteration 56010 Training loss 0.09089343994855881 Validation loss 0.09183847159147263 Accuracy 0.7350000143051147\n",
      "Iteration 56020 Training loss 0.09501872956752777 Validation loss 0.09980039298534393 Accuracy 0.7085000276565552\n",
      "Iteration 56030 Training loss 0.09602122753858566 Validation loss 0.10262510925531387 Accuracy 0.6860000491142273\n",
      "Iteration 56040 Training loss 0.10423203557729721 Validation loss 0.11079315841197968 Accuracy 0.6710000038146973\n",
      "Iteration 56050 Training loss 0.09195523709058762 Validation loss 0.0978112518787384 Accuracy 0.7145000100135803\n",
      "Iteration 56060 Training loss 0.0682702511548996 Validation loss 0.0966416522860527 Accuracy 0.7085000276565552\n",
      "Iteration 56070 Training loss 0.08458910882472992 Validation loss 0.09623455256223679 Accuracy 0.721500039100647\n",
      "Iteration 56080 Training loss 0.11795911192893982 Validation loss 0.10869739204645157 Accuracy 0.6575000286102295\n",
      "Iteration 56090 Training loss 0.10220187157392502 Validation loss 0.09260434657335281 Accuracy 0.7345000505447388\n",
      "Iteration 56100 Training loss 0.10231909155845642 Validation loss 0.09998147934675217 Accuracy 0.7100000381469727\n",
      "Iteration 56110 Training loss 0.09287313371896744 Validation loss 0.0919656828045845 Accuracy 0.721500039100647\n",
      "Iteration 56120 Training loss 0.08332692086696625 Validation loss 0.097535640001297 Accuracy 0.7065000534057617\n",
      "Iteration 56130 Training loss 0.08500607311725616 Validation loss 0.09323063492774963 Accuracy 0.7290000319480896\n",
      "Iteration 56140 Training loss 0.07206237316131592 Validation loss 0.09362944215536118 Accuracy 0.7380000352859497\n",
      "Iteration 56150 Training loss 0.08492006361484528 Validation loss 0.09264565259218216 Accuracy 0.7330000400543213\n",
      "Iteration 56160 Training loss 0.09526151418685913 Validation loss 0.09362856298685074 Accuracy 0.7295000553131104\n",
      "Iteration 56170 Training loss 0.1036188080906868 Validation loss 0.11201822012662888 Accuracy 0.6305000185966492\n",
      "Iteration 56180 Training loss 0.09943113476037979 Validation loss 0.12128040194511414 Accuracy 0.64000004529953\n",
      "Iteration 56190 Training loss 0.10054636001586914 Validation loss 0.10386387258768082 Accuracy 0.690500020980835\n",
      "Iteration 56200 Training loss 0.08119875937700272 Validation loss 0.09394678473472595 Accuracy 0.7365000247955322\n",
      "Iteration 56210 Training loss 0.11561104655265808 Validation loss 0.12402040511369705 Accuracy 0.6115000247955322\n",
      "Iteration 56220 Training loss 0.10133121907711029 Validation loss 0.09532656520605087 Accuracy 0.7290000319480896\n",
      "Iteration 56230 Training loss 0.12010011076927185 Validation loss 0.10637535899877548 Accuracy 0.6480000019073486\n",
      "Iteration 56240 Training loss 0.09075189381837845 Validation loss 0.09984809160232544 Accuracy 0.6925000548362732\n",
      "Iteration 56250 Training loss 0.12242160737514496 Validation loss 0.0960114374756813 Accuracy 0.7200000286102295\n",
      "Iteration 56260 Training loss 0.10463783144950867 Validation loss 0.09154330939054489 Accuracy 0.7355000376701355\n",
      "Iteration 56270 Training loss 0.0858035758137703 Validation loss 0.09540901333093643 Accuracy 0.7090000510215759\n",
      "Iteration 56280 Training loss 0.1143207997083664 Validation loss 0.12591367959976196 Accuracy 0.6195000410079956\n",
      "Iteration 56290 Training loss 0.09685764461755753 Validation loss 0.0979008674621582 Accuracy 0.7105000615119934\n",
      "Iteration 56300 Training loss 0.09558040648698807 Validation loss 0.09654811769723892 Accuracy 0.7080000042915344\n",
      "Iteration 56310 Training loss 0.07731074839830399 Validation loss 0.09299410879611969 Accuracy 0.7255000472068787\n",
      "Iteration 56320 Training loss 0.09485013037919998 Validation loss 0.09213582426309586 Accuracy 0.7315000295639038\n",
      "Iteration 56330 Training loss 0.0945938378572464 Validation loss 0.09807463735342026 Accuracy 0.7080000042915344\n",
      "Iteration 56340 Training loss 0.09014001488685608 Validation loss 0.09865602105855942 Accuracy 0.7120000123977661\n",
      "Iteration 56350 Training loss 0.09687899053096771 Validation loss 0.09325511753559113 Accuracy 0.737500011920929\n",
      "Iteration 56360 Training loss 0.07895370572805405 Validation loss 0.09661950916051865 Accuracy 0.7145000100135803\n",
      "Iteration 56370 Training loss 0.09481871128082275 Validation loss 0.09870709478855133 Accuracy 0.7100000381469727\n",
      "Iteration 56380 Training loss 0.0985725075006485 Validation loss 0.09750358760356903 Accuracy 0.7175000309944153\n",
      "Iteration 56390 Training loss 0.07536613941192627 Validation loss 0.08978654444217682 Accuracy 0.737500011920929\n",
      "Iteration 56400 Training loss 0.0911814495921135 Validation loss 0.09347742795944214 Accuracy 0.7285000085830688\n",
      "Iteration 56410 Training loss 0.10136254876852036 Validation loss 0.09350575506687164 Accuracy 0.7305000424385071\n",
      "Iteration 56420 Training loss 0.07800374180078506 Validation loss 0.09065030515193939 Accuracy 0.7325000166893005\n",
      "Iteration 56430 Training loss 0.10781009495258331 Validation loss 0.09704326093196869 Accuracy 0.7200000286102295\n",
      "Iteration 56440 Training loss 0.11314870417118073 Validation loss 0.09647934138774872 Accuracy 0.7195000052452087\n",
      "Iteration 56450 Training loss 0.0913805291056633 Validation loss 0.09507057815790176 Accuracy 0.7150000333786011\n",
      "Iteration 56460 Training loss 0.09576577693223953 Validation loss 0.09417098015546799 Accuracy 0.7240000367164612\n",
      "Iteration 56470 Training loss 0.08264937251806259 Validation loss 0.09252125769853592 Accuracy 0.7265000343322754\n",
      "Iteration 56480 Training loss 0.08904502540826797 Validation loss 0.09345558285713196 Accuracy 0.7295000553131104\n",
      "Iteration 56490 Training loss 0.08041112124919891 Validation loss 0.09200429916381836 Accuracy 0.733500063419342\n",
      "Iteration 56500 Training loss 0.09677805751562119 Validation loss 0.09196014702320099 Accuracy 0.7275000214576721\n",
      "Iteration 56510 Training loss 0.09347118437290192 Validation loss 0.09796176850795746 Accuracy 0.7080000042915344\n",
      "Iteration 56520 Training loss 0.09460776299238205 Validation loss 0.09333445131778717 Accuracy 0.7290000319480896\n",
      "Iteration 56530 Training loss 0.09065557271242142 Validation loss 0.09539265930652618 Accuracy 0.7115000486373901\n",
      "Iteration 56540 Training loss 0.09325439482927322 Validation loss 0.0975821390748024 Accuracy 0.706000030040741\n",
      "Iteration 56550 Training loss 0.09400279819965363 Validation loss 0.10356949269771576 Accuracy 0.6915000081062317\n",
      "Iteration 56560 Training loss 0.10029273480176926 Validation loss 0.09547355771064758 Accuracy 0.718500018119812\n",
      "Iteration 56570 Training loss 0.096140056848526 Validation loss 0.10556062310934067 Accuracy 0.6720000505447388\n",
      "Iteration 56580 Training loss 0.10025712847709656 Validation loss 0.09261057525873184 Accuracy 0.7295000553131104\n",
      "Iteration 56590 Training loss 0.08256366848945618 Validation loss 0.09560783952474594 Accuracy 0.7170000076293945\n",
      "Iteration 56600 Training loss 0.09590885788202286 Validation loss 0.09972873330116272 Accuracy 0.70250004529953\n",
      "Iteration 56610 Training loss 0.10189922153949738 Validation loss 0.10421157628297806 Accuracy 0.6775000095367432\n",
      "Iteration 56620 Training loss 0.09964341670274734 Validation loss 0.10438857227563858 Accuracy 0.6710000038146973\n",
      "Iteration 56630 Training loss 0.09317468106746674 Validation loss 0.09660448133945465 Accuracy 0.7315000295639038\n",
      "Iteration 56640 Training loss 0.08590135723352432 Validation loss 0.0969005599617958 Accuracy 0.7210000157356262\n",
      "Iteration 56650 Training loss 0.08801708370447159 Validation loss 0.0978652685880661 Accuracy 0.7095000147819519\n",
      "Iteration 56660 Training loss 0.14001096785068512 Validation loss 0.1217501237988472 Accuracy 0.6140000224113464\n",
      "Iteration 56670 Training loss 0.09698990732431412 Validation loss 0.10068047791719437 Accuracy 0.6890000104904175\n",
      "Iteration 56680 Training loss 0.08806740492582321 Validation loss 0.09310461580753326 Accuracy 0.7295000553131104\n",
      "Iteration 56690 Training loss 0.06009271368384361 Validation loss 0.09333818405866623 Accuracy 0.7325000166893005\n",
      "Iteration 56700 Training loss 0.0989527702331543 Validation loss 0.09978704899549484 Accuracy 0.6990000605583191\n",
      "Iteration 56710 Training loss 0.09290307760238647 Validation loss 0.10819513350725174 Accuracy 0.6525000333786011\n",
      "Iteration 56720 Training loss 0.10053376853466034 Validation loss 0.09643697738647461 Accuracy 0.7140000462532043\n",
      "Iteration 56730 Training loss 0.08317828178405762 Validation loss 0.09434043616056442 Accuracy 0.7285000085830688\n",
      "Iteration 56740 Training loss 0.0744333565235138 Validation loss 0.09246600419282913 Accuracy 0.7300000190734863\n",
      "Iteration 56750 Training loss 0.09783407300710678 Validation loss 0.09204813838005066 Accuracy 0.7420000433921814\n",
      "Iteration 56760 Training loss 0.09305568784475327 Validation loss 0.09351422637701035 Accuracy 0.7285000085830688\n",
      "Iteration 56770 Training loss 0.1121409460902214 Validation loss 0.10745556652545929 Accuracy 0.6570000052452087\n",
      "Iteration 56780 Training loss 0.08675670623779297 Validation loss 0.09650445729494095 Accuracy 0.7130000591278076\n",
      "Iteration 56790 Training loss 0.09594374150037766 Validation loss 0.09492015838623047 Accuracy 0.7140000462532043\n",
      "Iteration 56800 Training loss 0.08561162650585175 Validation loss 0.09433847665786743 Accuracy 0.7365000247955322\n",
      "Iteration 56810 Training loss 0.08426330238580704 Validation loss 0.09251326322555542 Accuracy 0.733500063419342\n",
      "Iteration 56820 Training loss 0.08864611387252808 Validation loss 0.09254942089319229 Accuracy 0.7250000238418579\n",
      "Iteration 56830 Training loss 0.08693097531795502 Validation loss 0.09729158133268356 Accuracy 0.7100000381469727\n",
      "Iteration 56840 Training loss 0.09548381716012955 Validation loss 0.09582553803920746 Accuracy 0.7090000510215759\n",
      "Iteration 56850 Training loss 0.0832795724272728 Validation loss 0.0949537381529808 Accuracy 0.7250000238418579\n",
      "Iteration 56860 Training loss 0.08541406691074371 Validation loss 0.09552694112062454 Accuracy 0.7250000238418579\n",
      "Iteration 56870 Training loss 0.08941027522087097 Validation loss 0.09649926424026489 Accuracy 0.7040000557899475\n",
      "Iteration 56880 Training loss 0.09774304926395416 Validation loss 0.09222971647977829 Accuracy 0.7390000224113464\n",
      "Iteration 56890 Training loss 0.08262480795383453 Validation loss 0.09231185168027878 Accuracy 0.7285000085830688\n",
      "Iteration 56900 Training loss 0.14114613831043243 Validation loss 0.11354818940162659 Accuracy 0.6550000309944153\n",
      "Iteration 56910 Training loss 0.07777571678161621 Validation loss 0.09502436965703964 Accuracy 0.7285000085830688\n",
      "Iteration 56920 Training loss 0.1043364629149437 Validation loss 0.10321062058210373 Accuracy 0.6780000329017639\n",
      "Iteration 56930 Training loss 0.09715213626623154 Validation loss 0.09304255992174149 Accuracy 0.7330000400543213\n",
      "Iteration 56940 Training loss 0.07534220814704895 Validation loss 0.09260398149490356 Accuracy 0.7290000319480896\n",
      "Iteration 56950 Training loss 0.08851931989192963 Validation loss 0.09233498573303223 Accuracy 0.7320000529289246\n",
      "Iteration 56960 Training loss 0.0775105357170105 Validation loss 0.09670089930295944 Accuracy 0.7040000557899475\n",
      "Iteration 56970 Training loss 0.10358480364084244 Validation loss 0.10422655940055847 Accuracy 0.6890000104904175\n",
      "Iteration 56980 Training loss 0.0780046135187149 Validation loss 0.09266127645969391 Accuracy 0.737000048160553\n",
      "Iteration 56990 Training loss 0.07758097350597382 Validation loss 0.09295724332332611 Accuracy 0.7310000061988831\n",
      "Iteration 57000 Training loss 0.07850582897663116 Validation loss 0.0907433032989502 Accuracy 0.7415000200271606\n",
      "Iteration 57010 Training loss 0.08833055943250656 Validation loss 0.09771949052810669 Accuracy 0.7205000519752502\n",
      "Iteration 57020 Training loss 0.10239170491695404 Validation loss 0.099203921854496 Accuracy 0.7140000462532043\n",
      "Iteration 57030 Training loss 0.09471523016691208 Validation loss 0.09625308960676193 Accuracy 0.7135000228881836\n",
      "Iteration 57040 Training loss 0.09472605586051941 Validation loss 0.09701726585626602 Accuracy 0.7020000219345093\n",
      "Iteration 57050 Training loss 0.08493587374687195 Validation loss 0.0976557508111 Accuracy 0.6990000605583191\n",
      "Iteration 57060 Training loss 0.08784904330968857 Validation loss 0.09347444772720337 Accuracy 0.7265000343322754\n",
      "Iteration 57070 Training loss 0.08274781703948975 Validation loss 0.09776076674461365 Accuracy 0.7055000066757202\n",
      "Iteration 57080 Training loss 0.0853179320693016 Validation loss 0.09445686638355255 Accuracy 0.7200000286102295\n",
      "Iteration 57090 Training loss 0.16799800097942352 Validation loss 0.15039487183094025 Accuracy 0.5590000152587891\n",
      "Iteration 57100 Training loss 0.08880189806222916 Validation loss 0.09990239143371582 Accuracy 0.7160000205039978\n",
      "Iteration 57110 Training loss 0.08220987021923065 Validation loss 0.09349022060632706 Accuracy 0.7310000061988831\n",
      "Iteration 57120 Training loss 0.07676035910844803 Validation loss 0.09358969330787659 Accuracy 0.734000027179718\n",
      "Iteration 57130 Training loss 0.0918332114815712 Validation loss 0.09457641839981079 Accuracy 0.7255000472068787\n",
      "Iteration 57140 Training loss 0.08368935436010361 Validation loss 0.0966230034828186 Accuracy 0.7090000510215759\n",
      "Iteration 57150 Training loss 0.09107654541730881 Validation loss 0.09855267405509949 Accuracy 0.7080000042915344\n",
      "Iteration 57160 Training loss 0.09361301362514496 Validation loss 0.09291049838066101 Accuracy 0.7400000095367432\n",
      "Iteration 57170 Training loss 0.08774377405643463 Validation loss 0.09731759876012802 Accuracy 0.7305000424385071\n",
      "Iteration 57180 Training loss 0.088580422103405 Validation loss 0.0964764729142189 Accuracy 0.7120000123977661\n",
      "Iteration 57190 Training loss 0.08294864743947983 Validation loss 0.09163691848516464 Accuracy 0.7345000505447388\n",
      "Iteration 57200 Training loss 0.10130974650382996 Validation loss 0.10518363118171692 Accuracy 0.6700000166893005\n",
      "Iteration 57210 Training loss 0.086687833070755 Validation loss 0.09599661082029343 Accuracy 0.721500039100647\n",
      "Iteration 57220 Training loss 0.10091757774353027 Validation loss 0.10731790959835052 Accuracy 0.6720000505447388\n",
      "Iteration 57230 Training loss 0.0963631421327591 Validation loss 0.10769812762737274 Accuracy 0.6650000214576721\n",
      "Iteration 57240 Training loss 0.09648803621530533 Validation loss 0.10157260298728943 Accuracy 0.6975000500679016\n",
      "Iteration 57250 Training loss 0.08909499645233154 Validation loss 0.09493269771337509 Accuracy 0.7205000519752502\n",
      "Iteration 57260 Training loss 0.09030138701200485 Validation loss 0.0943843349814415 Accuracy 0.7100000381469727\n",
      "Iteration 57270 Training loss 0.10075177252292633 Validation loss 0.10272181034088135 Accuracy 0.6920000314712524\n",
      "Iteration 57280 Training loss 0.10257089138031006 Validation loss 0.09353774040937424 Accuracy 0.7320000529289246\n",
      "Iteration 57290 Training loss 0.08487178385257721 Validation loss 0.0979117751121521 Accuracy 0.7015000581741333\n",
      "Iteration 57300 Training loss 0.16182909905910492 Validation loss 0.14902660250663757 Accuracy 0.5660000443458557\n",
      "Iteration 57310 Training loss 0.08942977339029312 Validation loss 0.09535718709230423 Accuracy 0.7220000624656677\n",
      "Iteration 57320 Training loss 0.1185389906167984 Validation loss 0.1057031899690628 Accuracy 0.6825000047683716\n",
      "Iteration 57330 Training loss 0.08654103428125381 Validation loss 0.09059098362922668 Accuracy 0.733500063419342\n",
      "Iteration 57340 Training loss 0.08419351279735565 Validation loss 0.08961976319551468 Accuracy 0.7415000200271606\n",
      "Iteration 57350 Training loss 0.10114235430955887 Validation loss 0.10894811898469925 Accuracy 0.6415000557899475\n",
      "Iteration 57360 Training loss 0.09630060195922852 Validation loss 0.09391800314188004 Accuracy 0.7280000448226929\n",
      "Iteration 57370 Training loss 0.0853317603468895 Validation loss 0.09230055660009384 Accuracy 0.7275000214576721\n",
      "Iteration 57380 Training loss 0.11019715666770935 Validation loss 0.11796325445175171 Accuracy 0.6530000567436218\n",
      "Iteration 57390 Training loss 0.11044123768806458 Validation loss 0.09241767227649689 Accuracy 0.7265000343322754\n",
      "Iteration 57400 Training loss 0.08460108935832977 Validation loss 0.09080398827791214 Accuracy 0.7245000600814819\n",
      "Iteration 57410 Training loss 0.09032594412565231 Validation loss 0.09039579331874847 Accuracy 0.7315000295639038\n",
      "Iteration 57420 Training loss 0.08506617695093155 Validation loss 0.09400707483291626 Accuracy 0.7245000600814819\n",
      "Iteration 57430 Training loss 0.10982320457696915 Validation loss 0.11827827990055084 Accuracy 0.609000027179718\n",
      "Iteration 57440 Training loss 0.09971398860216141 Validation loss 0.09599722921848297 Accuracy 0.7090000510215759\n",
      "Iteration 57450 Training loss 0.08297127485275269 Validation loss 0.09424814581871033 Accuracy 0.7275000214576721\n",
      "Iteration 57460 Training loss 0.08463449031114578 Validation loss 0.092387855052948 Accuracy 0.7390000224113464\n",
      "Iteration 57470 Training loss 0.0935114249587059 Validation loss 0.09142383188009262 Accuracy 0.7345000505447388\n",
      "Iteration 57480 Training loss 0.09226996451616287 Validation loss 0.09590410441160202 Accuracy 0.7125000357627869\n",
      "Iteration 57490 Training loss 0.08799771219491959 Validation loss 0.0922476127743721 Accuracy 0.7315000295639038\n",
      "Iteration 57500 Training loss 0.08423660695552826 Validation loss 0.09587804228067398 Accuracy 0.7140000462532043\n",
      "Iteration 57510 Training loss 0.08042006194591522 Validation loss 0.09281168133020401 Accuracy 0.7315000295639038\n",
      "Iteration 57520 Training loss 0.08630991727113724 Validation loss 0.09457952529191971 Accuracy 0.7265000343322754\n",
      "Iteration 57530 Training loss 0.09194839000701904 Validation loss 0.0930154025554657 Accuracy 0.7230000495910645\n",
      "Iteration 57540 Training loss 0.09672046452760696 Validation loss 0.0907410979270935 Accuracy 0.7450000643730164\n",
      "Iteration 57550 Training loss 0.08615756779909134 Validation loss 0.09631463140249252 Accuracy 0.718000054359436\n",
      "Iteration 57560 Training loss 0.0954815149307251 Validation loss 0.09130356460809708 Accuracy 0.7440000176429749\n",
      "Iteration 57570 Training loss 0.07261233031749725 Validation loss 0.09713207185268402 Accuracy 0.7070000171661377\n",
      "Iteration 57580 Training loss 0.0999443531036377 Validation loss 0.09025609493255615 Accuracy 0.7325000166893005\n",
      "Iteration 57590 Training loss 0.08590889722108841 Validation loss 0.09422259777784348 Accuracy 0.7095000147819519\n",
      "Iteration 57600 Training loss 0.09993363916873932 Validation loss 0.09624297171831131 Accuracy 0.7205000519752502\n",
      "Iteration 57610 Training loss 0.09510710835456848 Validation loss 0.10438346117734909 Accuracy 0.6815000176429749\n",
      "Iteration 57620 Training loss 0.10307735949754715 Validation loss 0.10133975744247437 Accuracy 0.7000000476837158\n",
      "Iteration 57630 Training loss 0.08585742861032486 Validation loss 0.09760331362485886 Accuracy 0.7210000157356262\n",
      "Iteration 57640 Training loss 0.10108019411563873 Validation loss 0.09346827864646912 Accuracy 0.7205000519752502\n",
      "Iteration 57650 Training loss 0.07953426241874695 Validation loss 0.10102605074644089 Accuracy 0.7000000476837158\n",
      "Iteration 57660 Training loss 0.11802422255277634 Validation loss 0.09293271601200104 Accuracy 0.7295000553131104\n",
      "Iteration 57670 Training loss 0.08900030702352524 Validation loss 0.09485992789268494 Accuracy 0.7325000166893005\n",
      "Iteration 57680 Training loss 0.08775782585144043 Validation loss 0.09335295110940933 Accuracy 0.7150000333786011\n",
      "Iteration 57690 Training loss 0.08668944984674454 Validation loss 0.09020806103944778 Accuracy 0.7430000305175781\n",
      "Iteration 57700 Training loss 0.10957667976617813 Validation loss 0.11278542131185532 Accuracy 0.6720000505447388\n",
      "Iteration 57710 Training loss 0.09562287479639053 Validation loss 0.10158102959394455 Accuracy 0.6980000138282776\n",
      "Iteration 57720 Training loss 0.10512365400791168 Validation loss 0.10649214684963226 Accuracy 0.6650000214576721\n",
      "Iteration 57730 Training loss 0.09213333576917648 Validation loss 0.0999542847275734 Accuracy 0.7070000171661377\n",
      "Iteration 57740 Training loss 0.10012076050043106 Validation loss 0.09425133466720581 Accuracy 0.7295000553131104\n",
      "Iteration 57750 Training loss 0.08881168067455292 Validation loss 0.12295985966920853 Accuracy 0.5915000438690186\n",
      "Iteration 57760 Training loss 0.09868888556957245 Validation loss 0.09506793320178986 Accuracy 0.7245000600814819\n",
      "Iteration 57770 Training loss 0.08002128452062607 Validation loss 0.09659513831138611 Accuracy 0.7100000381469727\n",
      "Iteration 57780 Training loss 0.08903858065605164 Validation loss 0.09590158611536026 Accuracy 0.7210000157356262\n",
      "Iteration 57790 Training loss 0.07643899321556091 Validation loss 0.09181108325719833 Accuracy 0.7310000061988831\n",
      "Iteration 57800 Training loss 0.08482391387224197 Validation loss 0.0911419540643692 Accuracy 0.7385000586509705\n",
      "Iteration 57810 Training loss 0.07121249288320541 Validation loss 0.09288503229618073 Accuracy 0.7295000553131104\n",
      "Iteration 57820 Training loss 0.07769261300563812 Validation loss 0.09187213331460953 Accuracy 0.7310000061988831\n",
      "Iteration 57830 Training loss 0.09282613545656204 Validation loss 0.09390585124492645 Accuracy 0.7170000076293945\n",
      "Iteration 57840 Training loss 0.07930903136730194 Validation loss 0.09286783635616302 Accuracy 0.7325000166893005\n",
      "Iteration 57850 Training loss 0.07117383182048798 Validation loss 0.09249339252710342 Accuracy 0.7325000166893005\n",
      "Iteration 57860 Training loss 0.09607844054698944 Validation loss 0.09455639868974686 Accuracy 0.7145000100135803\n",
      "Iteration 57870 Training loss 0.09329679608345032 Validation loss 0.09791628271341324 Accuracy 0.7080000042915344\n",
      "Iteration 57880 Training loss 0.09216367453336716 Validation loss 0.09753558039665222 Accuracy 0.7150000333786011\n",
      "Iteration 57890 Training loss 0.08092948794364929 Validation loss 0.09535113722085953 Accuracy 0.7195000052452087\n",
      "Iteration 57900 Training loss 0.099576935172081 Validation loss 0.09780000895261765 Accuracy 0.7120000123977661\n",
      "Iteration 57910 Training loss 0.08948047459125519 Validation loss 0.09007039666175842 Accuracy 0.737500011920929\n",
      "Iteration 57920 Training loss 0.08667977899312973 Validation loss 0.09193264693021774 Accuracy 0.7320000529289246\n",
      "Iteration 57930 Training loss 0.09502626210451126 Validation loss 0.09892900288105011 Accuracy 0.7010000348091125\n",
      "Iteration 57940 Training loss 0.09023413807153702 Validation loss 0.09090111404657364 Accuracy 0.7415000200271606\n",
      "Iteration 57950 Training loss 0.09638387709856033 Validation loss 0.09355112165212631 Accuracy 0.7300000190734863\n",
      "Iteration 57960 Training loss 0.07955949008464813 Validation loss 0.09454527497291565 Accuracy 0.7150000333786011\n",
      "Iteration 57970 Training loss 0.08946964889764786 Validation loss 0.09963960945606232 Accuracy 0.7080000042915344\n",
      "Iteration 57980 Training loss 0.10741691291332245 Validation loss 0.09345707297325134 Accuracy 0.7190000414848328\n",
      "Iteration 57990 Training loss 0.1136755645275116 Validation loss 0.1050175353884697 Accuracy 0.6915000081062317\n",
      "Iteration 58000 Training loss 0.09013418108224869 Validation loss 0.10064523667097092 Accuracy 0.6930000185966492\n",
      "Iteration 58010 Training loss 0.0893540233373642 Validation loss 0.09174881130456924 Accuracy 0.7305000424385071\n",
      "Iteration 58020 Training loss 0.08601781725883484 Validation loss 0.0929226502776146 Accuracy 0.7260000109672546\n",
      "Iteration 58030 Training loss 0.10505599528551102 Validation loss 0.10125990957021713 Accuracy 0.6890000104904175\n",
      "Iteration 58040 Training loss 0.08279595524072647 Validation loss 0.09203911572694778 Accuracy 0.733500063419342\n",
      "Iteration 58050 Training loss 0.11389805376529694 Validation loss 0.09543485194444656 Accuracy 0.7195000052452087\n",
      "Iteration 58060 Training loss 0.09806790202856064 Validation loss 0.09323591738939285 Accuracy 0.7270000576972961\n",
      "Iteration 58070 Training loss 0.11159173399209976 Validation loss 0.10916484892368317 Accuracy 0.6680000424385071\n",
      "Iteration 58080 Training loss 0.10202325880527496 Validation loss 0.09961101412773132 Accuracy 0.6945000290870667\n",
      "Iteration 58090 Training loss 0.08923200517892838 Validation loss 0.09283717721700668 Accuracy 0.7390000224113464\n",
      "Iteration 58100 Training loss 0.08561166375875473 Validation loss 0.09280762076377869 Accuracy 0.7190000414848328\n",
      "Iteration 58110 Training loss 0.08066201955080032 Validation loss 0.09513675421476364 Accuracy 0.7265000343322754\n",
      "Iteration 58120 Training loss 0.0950055867433548 Validation loss 0.09036426246166229 Accuracy 0.7390000224113464\n",
      "Iteration 58130 Training loss 0.10141641646623611 Validation loss 0.0927320271730423 Accuracy 0.7320000529289246\n",
      "Iteration 58140 Training loss 0.09084442257881165 Validation loss 0.0951351672410965 Accuracy 0.7155000567436218\n",
      "Iteration 58150 Training loss 0.09856319427490234 Validation loss 0.10051284730434418 Accuracy 0.7045000195503235\n",
      "Iteration 58160 Training loss 0.1111595407128334 Validation loss 0.09702111035585403 Accuracy 0.7110000252723694\n",
      "Iteration 58170 Training loss 0.10183265805244446 Validation loss 0.09320896863937378 Accuracy 0.7355000376701355\n",
      "Iteration 58180 Training loss 0.09356208890676498 Validation loss 0.11383958160877228 Accuracy 0.6785000562667847\n",
      "Iteration 58190 Training loss 0.08486203849315643 Validation loss 0.09231285750865936 Accuracy 0.7295000553131104\n",
      "Iteration 58200 Training loss 0.07442842423915863 Validation loss 0.09588029235601425 Accuracy 0.7165000438690186\n",
      "Iteration 58210 Training loss 0.09043113142251968 Validation loss 0.09937985986471176 Accuracy 0.6985000371932983\n",
      "Iteration 58220 Training loss 0.11831378936767578 Validation loss 0.12488135695457458 Accuracy 0.628000020980835\n",
      "Iteration 58230 Training loss 0.07864020019769669 Validation loss 0.0926479697227478 Accuracy 0.7320000529289246\n",
      "Iteration 58240 Training loss 0.12255694717168808 Validation loss 0.12187769263982773 Accuracy 0.6575000286102295\n",
      "Iteration 58250 Training loss 0.08268310129642487 Validation loss 0.0916634276509285 Accuracy 0.7425000071525574\n",
      "Iteration 58260 Training loss 0.07943511754274368 Validation loss 0.09280909597873688 Accuracy 0.7345000505447388\n",
      "Iteration 58270 Training loss 0.12387609481811523 Validation loss 0.11560814082622528 Accuracy 0.6320000290870667\n",
      "Iteration 58280 Training loss 0.08793000876903534 Validation loss 0.09214796870946884 Accuracy 0.734000027179718\n",
      "Iteration 58290 Training loss 0.08328764140605927 Validation loss 0.09604942798614502 Accuracy 0.7190000414848328\n",
      "Iteration 58300 Training loss 0.08402811735868454 Validation loss 0.09135570377111435 Accuracy 0.7310000061988831\n",
      "Iteration 58310 Training loss 0.09528224915266037 Validation loss 0.09971574693918228 Accuracy 0.6955000162124634\n",
      "Iteration 58320 Training loss 0.07027892768383026 Validation loss 0.09285380691289902 Accuracy 0.7235000133514404\n",
      "Iteration 58330 Training loss 0.09299395233392715 Validation loss 0.09306273609399796 Accuracy 0.7315000295639038\n",
      "Iteration 58340 Training loss 0.09784960746765137 Validation loss 0.09384924918413162 Accuracy 0.7210000157356262\n",
      "Iteration 58350 Training loss 0.09411352872848511 Validation loss 0.09214485436677933 Accuracy 0.7265000343322754\n",
      "Iteration 58360 Training loss 0.09175572544336319 Validation loss 0.0921802744269371 Accuracy 0.7350000143051147\n",
      "Iteration 58370 Training loss 0.09215270727872849 Validation loss 0.09643978625535965 Accuracy 0.7300000190734863\n",
      "Iteration 58380 Training loss 0.09578094631433487 Validation loss 0.09865795075893402 Accuracy 0.7080000042915344\n",
      "Iteration 58390 Training loss 0.10192310065031052 Validation loss 0.09310774505138397 Accuracy 0.733500063419342\n",
      "Iteration 58400 Training loss 0.07452032715082169 Validation loss 0.09132477641105652 Accuracy 0.7385000586509705\n",
      "Iteration 58410 Training loss 0.09621911495923996 Validation loss 0.09730232506990433 Accuracy 0.7050000429153442\n",
      "Iteration 58420 Training loss 0.13374696671962738 Validation loss 0.10155995935201645 Accuracy 0.7010000348091125\n",
      "Iteration 58430 Training loss 0.11021535843610764 Validation loss 0.09870670735836029 Accuracy 0.7145000100135803\n",
      "Iteration 58440 Training loss 0.08461986482143402 Validation loss 0.1006096825003624 Accuracy 0.706000030040741\n",
      "Iteration 58450 Training loss 0.09702824056148529 Validation loss 0.09447957575321198 Accuracy 0.7250000238418579\n",
      "Iteration 58460 Training loss 0.09076469391584396 Validation loss 0.09659776836633682 Accuracy 0.7145000100135803\n",
      "Iteration 58470 Training loss 0.11415045708417892 Validation loss 0.10561461746692657 Accuracy 0.6840000152587891\n",
      "Iteration 58480 Training loss 0.09286873787641525 Validation loss 0.0953235998749733 Accuracy 0.7175000309944153\n",
      "Iteration 58490 Training loss 0.07389801740646362 Validation loss 0.10264767706394196 Accuracy 0.690500020980835\n",
      "Iteration 58500 Training loss 0.08341071754693985 Validation loss 0.09086865931749344 Accuracy 0.7350000143051147\n",
      "Iteration 58510 Training loss 0.11387134343385696 Validation loss 0.10862390697002411 Accuracy 0.6720000505447388\n",
      "Iteration 58520 Training loss 0.12068385630846024 Validation loss 0.1158263236284256 Accuracy 0.6600000262260437\n",
      "Iteration 58530 Training loss 0.0839359238743782 Validation loss 0.09753863513469696 Accuracy 0.7055000066757202\n",
      "Iteration 58540 Training loss 0.09330593049526215 Validation loss 0.09052161127328873 Accuracy 0.734000027179718\n",
      "Iteration 58550 Training loss 0.07809708267450333 Validation loss 0.09112007915973663 Accuracy 0.737500011920929\n",
      "Iteration 58560 Training loss 0.11090858280658722 Validation loss 0.13137641549110413 Accuracy 0.6210000514984131\n",
      "Iteration 58570 Training loss 0.09208133816719055 Validation loss 0.10289650410413742 Accuracy 0.6890000104904175\n",
      "Iteration 58580 Training loss 0.09981012344360352 Validation loss 0.10040183365345001 Accuracy 0.7020000219345093\n",
      "Iteration 58590 Training loss 0.07585041970014572 Validation loss 0.09374070912599564 Accuracy 0.7265000343322754\n",
      "Iteration 58600 Training loss 0.07869799435138702 Validation loss 0.09154314547777176 Accuracy 0.7365000247955322\n",
      "Iteration 58610 Training loss 0.10003284364938736 Validation loss 0.09440680593252182 Accuracy 0.734000027179718\n",
      "Iteration 58620 Training loss 0.10060565918684006 Validation loss 0.09832828491926193 Accuracy 0.7015000581741333\n",
      "Iteration 58630 Training loss 0.0933772623538971 Validation loss 0.0935606062412262 Accuracy 0.7325000166893005\n",
      "Iteration 58640 Training loss 0.09161029756069183 Validation loss 0.09911396354436874 Accuracy 0.6915000081062317\n",
      "Iteration 58650 Training loss 0.09474039077758789 Validation loss 0.09544841200113297 Accuracy 0.7290000319480896\n",
      "Iteration 58660 Training loss 0.10058996081352234 Validation loss 0.09316371381282806 Accuracy 0.7280000448226929\n",
      "Iteration 58670 Training loss 0.10507813841104507 Validation loss 0.09304434806108475 Accuracy 0.7260000109672546\n",
      "Iteration 58680 Training loss 0.09012778848409653 Validation loss 0.09358027577400208 Accuracy 0.7300000190734863\n",
      "Iteration 58690 Training loss 0.0919223502278328 Validation loss 0.10183252394199371 Accuracy 0.671500027179718\n",
      "Iteration 58700 Training loss 0.08247942477464676 Validation loss 0.08944935351610184 Accuracy 0.7440000176429749\n",
      "Iteration 58710 Training loss 0.08832807093858719 Validation loss 0.093510702252388 Accuracy 0.7350000143051147\n",
      "Iteration 58720 Training loss 0.08952298760414124 Validation loss 0.10531435161828995 Accuracy 0.690000057220459\n",
      "Iteration 58730 Training loss 0.12630507349967957 Validation loss 0.1336270272731781 Accuracy 0.5700000524520874\n",
      "Iteration 58740 Training loss 0.07983660697937012 Validation loss 0.0912688598036766 Accuracy 0.7315000295639038\n",
      "Iteration 58750 Training loss 0.07945738732814789 Validation loss 0.08989397436380386 Accuracy 0.7415000200271606\n",
      "Iteration 58760 Training loss 0.08605701476335526 Validation loss 0.09408164024353027 Accuracy 0.7250000238418579\n",
      "Iteration 58770 Training loss 0.10574077069759369 Validation loss 0.09828183799982071 Accuracy 0.7120000123977661\n",
      "Iteration 58780 Training loss 0.0900898352265358 Validation loss 0.10053358972072601 Accuracy 0.6890000104904175\n",
      "Iteration 58790 Training loss 0.10217995196580887 Validation loss 0.11779007315635681 Accuracy 0.6530000567436218\n",
      "Iteration 58800 Training loss 0.09128952026367188 Validation loss 0.10049691051244736 Accuracy 0.6970000267028809\n",
      "Iteration 58810 Training loss 0.07972320169210434 Validation loss 0.0910649448633194 Accuracy 0.7385000586509705\n",
      "Iteration 58820 Training loss 0.07210108637809753 Validation loss 0.09014749526977539 Accuracy 0.7450000643730164\n",
      "Iteration 58830 Training loss 0.09082618355751038 Validation loss 0.09416310489177704 Accuracy 0.7165000438690186\n",
      "Iteration 58840 Training loss 0.0875219777226448 Validation loss 0.0914863720536232 Accuracy 0.737500011920929\n",
      "Iteration 58850 Training loss 0.11355714499950409 Validation loss 0.0936654582619667 Accuracy 0.7135000228881836\n",
      "Iteration 58860 Training loss 0.08160899579524994 Validation loss 0.09222909808158875 Accuracy 0.7290000319480896\n",
      "Iteration 58870 Training loss 0.09454070031642914 Validation loss 0.09064751118421555 Accuracy 0.737000048160553\n",
      "Iteration 58880 Training loss 0.07522205263376236 Validation loss 0.09021106362342834 Accuracy 0.734000027179718\n",
      "Iteration 58890 Training loss 0.08389239013195038 Validation loss 0.09069491922855377 Accuracy 0.7380000352859497\n",
      "Iteration 58900 Training loss 0.08852968364953995 Validation loss 0.09146913886070251 Accuracy 0.7405000329017639\n",
      "Iteration 58910 Training loss 0.09044047445058823 Validation loss 0.0967349261045456 Accuracy 0.7265000343322754\n",
      "Iteration 58920 Training loss 0.08516206592321396 Validation loss 0.09853775799274445 Accuracy 0.7100000381469727\n",
      "Iteration 58930 Training loss 0.07595264166593552 Validation loss 0.09455001354217529 Accuracy 0.7205000519752502\n",
      "Iteration 58940 Training loss 0.08530318737030029 Validation loss 0.09284809976816177 Accuracy 0.7285000085830688\n",
      "Iteration 58950 Training loss 0.09094978123903275 Validation loss 0.09012291580438614 Accuracy 0.7430000305175781\n",
      "Iteration 58960 Training loss 0.08707090467214584 Validation loss 0.09042787551879883 Accuracy 0.7515000104904175\n",
      "Iteration 58970 Training loss 0.09107518941164017 Validation loss 0.09134580940008163 Accuracy 0.7290000319480896\n",
      "Iteration 58980 Training loss 0.07680395990610123 Validation loss 0.09471875429153442 Accuracy 0.7200000286102295\n",
      "Iteration 58990 Training loss 0.10575910657644272 Validation loss 0.10799526423215866 Accuracy 0.6645000576972961\n",
      "Iteration 59000 Training loss 0.09208904206752777 Validation loss 0.09655360877513885 Accuracy 0.7210000157356262\n",
      "Iteration 59010 Training loss 0.08488908410072327 Validation loss 0.09523701667785645 Accuracy 0.7245000600814819\n",
      "Iteration 59020 Training loss 0.09399856626987457 Validation loss 0.13262717425823212 Accuracy 0.6325000524520874\n",
      "Iteration 59030 Training loss 0.08890672773122787 Validation loss 0.09615913778543472 Accuracy 0.7195000052452087\n",
      "Iteration 59040 Training loss 0.0956210270524025 Validation loss 0.09292424470186234 Accuracy 0.7225000262260437\n",
      "Iteration 59050 Training loss 0.12629514932632446 Validation loss 0.12873013317584991 Accuracy 0.6080000400543213\n",
      "Iteration 59060 Training loss 0.10295721888542175 Validation loss 0.09287035465240479 Accuracy 0.7360000610351562\n",
      "Iteration 59070 Training loss 0.07382743805646896 Validation loss 0.09292035549879074 Accuracy 0.7360000610351562\n",
      "Iteration 59080 Training loss 0.0842139720916748 Validation loss 0.09373652935028076 Accuracy 0.7190000414848328\n",
      "Iteration 59090 Training loss 0.09243692457675934 Validation loss 0.09233727306127548 Accuracy 0.733500063419342\n",
      "Iteration 59100 Training loss 0.08785682171583176 Validation loss 0.09468688070774078 Accuracy 0.7210000157356262\n",
      "Iteration 59110 Training loss 0.08383287489414215 Validation loss 0.09482751041650772 Accuracy 0.7110000252723694\n",
      "Iteration 59120 Training loss 0.08973148465156555 Validation loss 0.08905656635761261 Accuracy 0.7405000329017639\n",
      "Iteration 59130 Training loss 0.08910854160785675 Validation loss 0.09263747185468674 Accuracy 0.734000027179718\n",
      "Iteration 59140 Training loss 0.1082390546798706 Validation loss 0.09238643199205399 Accuracy 0.7250000238418579\n",
      "Iteration 59150 Training loss 0.07717028260231018 Validation loss 0.09324674308300018 Accuracy 0.7350000143051147\n",
      "Iteration 59160 Training loss 0.08341087400913239 Validation loss 0.09381134063005447 Accuracy 0.7275000214576721\n",
      "Iteration 59170 Training loss 0.07733134180307388 Validation loss 0.09275481849908829 Accuracy 0.7315000295639038\n",
      "Iteration 59180 Training loss 0.1264042854309082 Validation loss 0.10941809415817261 Accuracy 0.6670000553131104\n",
      "Iteration 59190 Training loss 0.09176475554704666 Validation loss 0.10127286612987518 Accuracy 0.6935000419616699\n",
      "Iteration 59200 Training loss 0.09192679077386856 Validation loss 0.09653452783823013 Accuracy 0.7190000414848328\n",
      "Iteration 59210 Training loss 0.10008924454450607 Validation loss 0.09857117384672165 Accuracy 0.7100000381469727\n",
      "Iteration 59220 Training loss 0.08358414471149445 Validation loss 0.09527406841516495 Accuracy 0.7230000495910645\n",
      "Iteration 59230 Training loss 0.10265058279037476 Validation loss 0.0965571478009224 Accuracy 0.7125000357627869\n",
      "Iteration 59240 Training loss 0.0873018428683281 Validation loss 0.09163955599069595 Accuracy 0.7355000376701355\n",
      "Iteration 59250 Training loss 0.11148297786712646 Validation loss 0.10008484125137329 Accuracy 0.7095000147819519\n",
      "Iteration 59260 Training loss 0.12377835065126419 Validation loss 0.11557690054178238 Accuracy 0.6160000562667847\n",
      "Iteration 59270 Training loss 0.0983792170882225 Validation loss 0.09508106112480164 Accuracy 0.7200000286102295\n",
      "Iteration 59280 Training loss 0.08450475335121155 Validation loss 0.09401268512010574 Accuracy 0.7200000286102295\n",
      "Iteration 59290 Training loss 0.10080934315919876 Validation loss 0.09574167430400848 Accuracy 0.7085000276565552\n",
      "Iteration 59300 Training loss 0.08049583435058594 Validation loss 0.08958953619003296 Accuracy 0.7385000586509705\n",
      "Iteration 59310 Training loss 0.09575831890106201 Validation loss 0.1022711843252182 Accuracy 0.6785000562667847\n",
      "Iteration 59320 Training loss 0.10519155114889145 Validation loss 0.1053847000002861 Accuracy 0.6755000352859497\n",
      "Iteration 59330 Training loss 0.0995904728770256 Validation loss 0.1039392277598381 Accuracy 0.6910000443458557\n",
      "Iteration 59340 Training loss 0.084528349339962 Validation loss 0.09069392830133438 Accuracy 0.7480000257492065\n",
      "Iteration 59350 Training loss 0.08975841104984283 Validation loss 0.09233872592449188 Accuracy 0.7255000472068787\n",
      "Iteration 59360 Training loss 0.09654196351766586 Validation loss 0.08990629762411118 Accuracy 0.7420000433921814\n",
      "Iteration 59370 Training loss 0.0917079746723175 Validation loss 0.09250679612159729 Accuracy 0.7200000286102295\n",
      "Iteration 59380 Training loss 0.11883813887834549 Validation loss 0.0959443673491478 Accuracy 0.7245000600814819\n",
      "Iteration 59390 Training loss 0.13643836975097656 Validation loss 0.14725373685359955 Accuracy 0.5830000042915344\n",
      "Iteration 59400 Training loss 0.08553722500801086 Validation loss 0.0956745371222496 Accuracy 0.7160000205039978\n",
      "Iteration 59410 Training loss 0.08926820755004883 Validation loss 0.09288520365953445 Accuracy 0.7255000472068787\n",
      "Iteration 59420 Training loss 0.10177107900381088 Validation loss 0.10575033724308014 Accuracy 0.6795000433921814\n",
      "Iteration 59430 Training loss 0.08220456540584564 Validation loss 0.09465741366147995 Accuracy 0.7235000133514404\n",
      "Iteration 59440 Training loss 0.09074323624372482 Validation loss 0.09154702723026276 Accuracy 0.737000048160553\n",
      "Iteration 59450 Training loss 0.09439592063426971 Validation loss 0.09341257810592651 Accuracy 0.7255000472068787\n",
      "Iteration 59460 Training loss 0.0936427116394043 Validation loss 0.10217218101024628 Accuracy 0.6860000491142273\n",
      "Iteration 59470 Training loss 0.0854220911860466 Validation loss 0.091190867125988 Accuracy 0.734000027179718\n",
      "Iteration 59480 Training loss 0.08139541745185852 Validation loss 0.09520760178565979 Accuracy 0.7175000309944153\n",
      "Iteration 59490 Training loss 0.09427691996097565 Validation loss 0.09407475590705872 Accuracy 0.7325000166893005\n",
      "Iteration 59500 Training loss 0.0924820676445961 Validation loss 0.09334147721529007 Accuracy 0.7295000553131104\n",
      "Iteration 59510 Training loss 0.10462141782045364 Validation loss 0.09495329856872559 Accuracy 0.721500039100647\n",
      "Iteration 59520 Training loss 0.07794585824012756 Validation loss 0.09002403914928436 Accuracy 0.737000048160553\n",
      "Iteration 59530 Training loss 0.07765468209981918 Validation loss 0.09447924047708511 Accuracy 0.7320000529289246\n",
      "Iteration 59540 Training loss 0.10895804315805435 Validation loss 0.10000154376029968 Accuracy 0.6940000057220459\n",
      "Iteration 59550 Training loss 0.08696593344211578 Validation loss 0.09110584855079651 Accuracy 0.7305000424385071\n",
      "Iteration 59560 Training loss 0.10051373392343521 Validation loss 0.09460849314928055 Accuracy 0.721500039100647\n",
      "Iteration 59570 Training loss 0.07692012935876846 Validation loss 0.09745412319898605 Accuracy 0.7080000042915344\n",
      "Iteration 59580 Training loss 0.08436203002929688 Validation loss 0.09324745833873749 Accuracy 0.737000048160553\n",
      "Iteration 59590 Training loss 0.09746500849723816 Validation loss 0.09223266690969467 Accuracy 0.7345000505447388\n",
      "Iteration 59600 Training loss 0.08890543133020401 Validation loss 0.09161470830440521 Accuracy 0.7380000352859497\n",
      "Iteration 59610 Training loss 0.08218713849782944 Validation loss 0.09163226932287216 Accuracy 0.7250000238418579\n",
      "Iteration 59620 Training loss 0.08200735598802567 Validation loss 0.09200034290552139 Accuracy 0.7385000586509705\n",
      "Iteration 59630 Training loss 0.08230329304933548 Validation loss 0.09399069845676422 Accuracy 0.7225000262260437\n",
      "Iteration 59640 Training loss 0.1031653955578804 Validation loss 0.0918770506978035 Accuracy 0.7245000600814819\n",
      "Iteration 59650 Training loss 0.08243706822395325 Validation loss 0.09147221595048904 Accuracy 0.7295000553131104\n",
      "Iteration 59660 Training loss 0.10043372213840485 Validation loss 0.09549544006586075 Accuracy 0.7200000286102295\n",
      "Iteration 59670 Training loss 0.07941021770238876 Validation loss 0.09519252181053162 Accuracy 0.7170000076293945\n",
      "Iteration 59680 Training loss 0.0957087054848671 Validation loss 0.09314103424549103 Accuracy 0.7360000610351562\n",
      "Iteration 59690 Training loss 0.09464115649461746 Validation loss 0.09155923128128052 Accuracy 0.7275000214576721\n",
      "Iteration 59700 Training loss 0.09328765422105789 Validation loss 0.09359163045883179 Accuracy 0.7225000262260437\n",
      "Iteration 59710 Training loss 0.10277487337589264 Validation loss 0.10214821249246597 Accuracy 0.6980000138282776\n",
      "Iteration 59720 Training loss 0.09649978578090668 Validation loss 0.09605138003826141 Accuracy 0.7210000157356262\n",
      "Iteration 59730 Training loss 0.0908779501914978 Validation loss 0.09250756353139877 Accuracy 0.7265000343322754\n",
      "Iteration 59740 Training loss 0.12347321957349777 Validation loss 0.1090923622250557 Accuracy 0.6460000276565552\n",
      "Iteration 59750 Training loss 0.10667840391397476 Validation loss 0.10238290578126907 Accuracy 0.7010000348091125\n",
      "Iteration 59760 Training loss 0.08132122457027435 Validation loss 0.09958267211914062 Accuracy 0.6895000338554382\n",
      "Iteration 59770 Training loss 0.11254311352968216 Validation loss 0.09619603306055069 Accuracy 0.7125000357627869\n",
      "Iteration 59780 Training loss 0.09362199902534485 Validation loss 0.09497145563364029 Accuracy 0.7170000076293945\n",
      "Iteration 59790 Training loss 0.10463008284568787 Validation loss 0.09571214020252228 Accuracy 0.7110000252723694\n",
      "Iteration 59800 Training loss 0.08595408499240875 Validation loss 0.09093326330184937 Accuracy 0.737500011920929\n",
      "Iteration 59810 Training loss 0.07730984687805176 Validation loss 0.09346291422843933 Accuracy 0.7310000061988831\n",
      "Iteration 59820 Training loss 0.0955960601568222 Validation loss 0.0884656235575676 Accuracy 0.7440000176429749\n",
      "Iteration 59830 Training loss 0.08245578408241272 Validation loss 0.0884692445397377 Accuracy 0.737000048160553\n",
      "Iteration 59840 Training loss 0.08707446604967117 Validation loss 0.09015272557735443 Accuracy 0.7380000352859497\n",
      "Iteration 59850 Training loss 0.09525150060653687 Validation loss 0.09484746307134628 Accuracy 0.7170000076293945\n",
      "Iteration 59860 Training loss 0.08873464167118073 Validation loss 0.09707118570804596 Accuracy 0.7105000615119934\n",
      "Iteration 59870 Training loss 0.10543788969516754 Validation loss 0.09590878337621689 Accuracy 0.7130000591278076\n",
      "Iteration 59880 Training loss 0.08792012929916382 Validation loss 0.09777029603719711 Accuracy 0.6990000605583191\n",
      "Iteration 59890 Training loss 0.08800166845321655 Validation loss 0.09389941394329071 Accuracy 0.7265000343322754\n",
      "Iteration 59900 Training loss 0.09561387449502945 Validation loss 0.09093563258647919 Accuracy 0.733500063419342\n",
      "Iteration 59910 Training loss 0.0988316684961319 Validation loss 0.10323720425367355 Accuracy 0.6825000047683716\n",
      "Iteration 59920 Training loss 0.08597779273986816 Validation loss 0.09270964562892914 Accuracy 0.7380000352859497\n",
      "Iteration 59930 Training loss 0.0901392251253128 Validation loss 0.100530244410038 Accuracy 0.6930000185966492\n",
      "Iteration 59940 Training loss 0.08698659390211105 Validation loss 0.09713195264339447 Accuracy 0.7235000133514404\n",
      "Iteration 59950 Training loss 0.08655717968940735 Validation loss 0.09076322615146637 Accuracy 0.7320000529289246\n",
      "Iteration 59960 Training loss 0.10745652765035629 Validation loss 0.09504476934671402 Accuracy 0.7240000367164612\n",
      "Iteration 59970 Training loss 0.07716601341962814 Validation loss 0.0939268246293068 Accuracy 0.7165000438690186\n",
      "Iteration 59980 Training loss 0.09194065630435944 Validation loss 0.09589622169733047 Accuracy 0.7120000123977661\n",
      "Iteration 59990 Training loss 0.10040416568517685 Validation loss 0.10715118050575256 Accuracy 0.6675000190734863\n",
      "Iteration 60000 Training loss 0.10858228802680969 Validation loss 0.10817821323871613 Accuracy 0.6320000290870667\n",
      "Iteration 60010 Training loss 0.09821119159460068 Validation loss 0.1083998754620552 Accuracy 0.6735000610351562\n",
      "Iteration 60020 Training loss 0.08585956692695618 Validation loss 0.0947074368596077 Accuracy 0.7250000238418579\n",
      "Iteration 60030 Training loss 0.08021459728479385 Validation loss 0.0934138298034668 Accuracy 0.7355000376701355\n",
      "Iteration 60040 Training loss 0.0928146243095398 Validation loss 0.10134012252092361 Accuracy 0.6835000514984131\n",
      "Iteration 60050 Training loss 0.08186370879411697 Validation loss 0.093205526471138 Accuracy 0.7235000133514404\n",
      "Iteration 60060 Training loss 0.08753246068954468 Validation loss 0.08966310322284698 Accuracy 0.7400000095367432\n",
      "Iteration 60070 Training loss 0.08998975902795792 Validation loss 0.09012800455093384 Accuracy 0.7360000610351562\n",
      "Iteration 60080 Training loss 0.104535311460495 Validation loss 0.09431229531764984 Accuracy 0.7305000424385071\n",
      "Iteration 60090 Training loss 0.07611559331417084 Validation loss 0.0910525694489479 Accuracy 0.7420000433921814\n",
      "Iteration 60100 Training loss 0.08026910573244095 Validation loss 0.09787538647651672 Accuracy 0.7075000405311584\n",
      "Iteration 60110 Training loss 0.11231039464473724 Validation loss 0.09410769492387772 Accuracy 0.7225000262260437\n",
      "Iteration 60120 Training loss 0.0901084691286087 Validation loss 0.10588791966438293 Accuracy 0.6740000247955322\n",
      "Iteration 60130 Training loss 0.08250219374895096 Validation loss 0.0940493792295456 Accuracy 0.7175000309944153\n",
      "Iteration 60140 Training loss 0.09585446119308472 Validation loss 0.0961831584572792 Accuracy 0.7050000429153442\n",
      "Iteration 60150 Training loss 0.09341506659984589 Validation loss 0.10653386265039444 Accuracy 0.6600000262260437\n",
      "Iteration 60160 Training loss 0.0752841904759407 Validation loss 0.09576968848705292 Accuracy 0.7230000495910645\n",
      "Iteration 60170 Training loss 0.08242189139127731 Validation loss 0.09353946149349213 Accuracy 0.7275000214576721\n",
      "Iteration 60180 Training loss 0.08232930302619934 Validation loss 0.09823397547006607 Accuracy 0.6995000243186951\n",
      "Iteration 60190 Training loss 0.09203135967254639 Validation loss 0.09907257556915283 Accuracy 0.7040000557899475\n",
      "Iteration 60200 Training loss 0.12247610092163086 Validation loss 0.10446961224079132 Accuracy 0.6880000233650208\n",
      "Iteration 60210 Training loss 0.0997675284743309 Validation loss 0.09559117257595062 Accuracy 0.7280000448226929\n",
      "Iteration 60220 Training loss 0.09132272750139236 Validation loss 0.09602426737546921 Accuracy 0.718500018119812\n",
      "Iteration 60230 Training loss 0.08976226300001144 Validation loss 0.09432623535394669 Accuracy 0.7290000319480896\n",
      "Iteration 60240 Training loss 0.10078077018260956 Validation loss 0.09905733168125153 Accuracy 0.6895000338554382\n",
      "Iteration 60250 Training loss 0.07797286659479141 Validation loss 0.09252966195344925 Accuracy 0.7320000529289246\n",
      "Iteration 60260 Training loss 0.08849624544382095 Validation loss 0.10333246737718582 Accuracy 0.6775000095367432\n",
      "Iteration 60270 Training loss 0.08153289556503296 Validation loss 0.09099675714969635 Accuracy 0.7415000200271606\n",
      "Iteration 60280 Training loss 0.09925726801156998 Validation loss 0.10140194743871689 Accuracy 0.703000009059906\n",
      "Iteration 60290 Training loss 0.09456045180559158 Validation loss 0.10031145066022873 Accuracy 0.6925000548362732\n",
      "Iteration 60300 Training loss 0.0753595307469368 Validation loss 0.09206008166074753 Accuracy 0.737000048160553\n",
      "Iteration 60310 Training loss 0.10069583356380463 Validation loss 0.10717138648033142 Accuracy 0.6585000157356262\n",
      "Iteration 60320 Training loss 0.09713874012231827 Validation loss 0.09593402594327927 Accuracy 0.7175000309944153\n",
      "Iteration 60330 Training loss 0.09470017999410629 Validation loss 0.09024817496538162 Accuracy 0.7425000071525574\n",
      "Iteration 60340 Training loss 0.08796180784702301 Validation loss 0.0922546461224556 Accuracy 0.7320000529289246\n",
      "Iteration 60350 Training loss 0.09636051952838898 Validation loss 0.09739192575216293 Accuracy 0.7085000276565552\n",
      "Iteration 60360 Training loss 0.07223659753799438 Validation loss 0.09344857186079025 Accuracy 0.7320000529289246\n",
      "Iteration 60370 Training loss 0.1011175736784935 Validation loss 0.09373587369918823 Accuracy 0.7235000133514404\n",
      "Iteration 60380 Training loss 0.09528861939907074 Validation loss 0.0911044329404831 Accuracy 0.7285000085830688\n",
      "Iteration 60390 Training loss 0.08132610470056534 Validation loss 0.09392000734806061 Accuracy 0.7310000061988831\n",
      "Iteration 60400 Training loss 0.10246537625789642 Validation loss 0.10270416736602783 Accuracy 0.6965000033378601\n",
      "Iteration 60410 Training loss 0.09352320432662964 Validation loss 0.10555285960435867 Accuracy 0.6730000376701355\n",
      "Iteration 60420 Training loss 0.10099268704652786 Validation loss 0.10171180963516235 Accuracy 0.6985000371932983\n",
      "Iteration 60430 Training loss 0.07824359834194183 Validation loss 0.09640655666589737 Accuracy 0.7260000109672546\n",
      "Iteration 60440 Training loss 0.09855613857507706 Validation loss 0.10298361629247665 Accuracy 0.6795000433921814\n",
      "Iteration 60450 Training loss 0.09542002528905869 Validation loss 0.09594479203224182 Accuracy 0.7210000157356262\n",
      "Iteration 60460 Training loss 0.09617672860622406 Validation loss 0.09575265645980835 Accuracy 0.7115000486373901\n",
      "Iteration 60470 Training loss 0.1135333776473999 Validation loss 0.10321961343288422 Accuracy 0.6930000185966492\n",
      "Iteration 60480 Training loss 0.10245385020971298 Validation loss 0.09828342497348785 Accuracy 0.7130000591278076\n",
      "Iteration 60490 Training loss 0.09274492412805557 Validation loss 0.09449775516986847 Accuracy 0.7165000438690186\n",
      "Iteration 60500 Training loss 0.09284618496894836 Validation loss 0.09936540573835373 Accuracy 0.690500020980835\n",
      "Iteration 60510 Training loss 0.09265705943107605 Validation loss 0.09947197139263153 Accuracy 0.703000009059906\n",
      "Iteration 60520 Training loss 0.09166350960731506 Validation loss 0.09317389130592346 Accuracy 0.7270000576972961\n",
      "Iteration 60530 Training loss 0.07843241840600967 Validation loss 0.09077419340610504 Accuracy 0.7470000386238098\n",
      "Iteration 60540 Training loss 0.10868276655673981 Validation loss 0.09073451906442642 Accuracy 0.7435000538825989\n",
      "Iteration 60550 Training loss 0.07364584505558014 Validation loss 0.08991493284702301 Accuracy 0.7430000305175781\n",
      "Iteration 60560 Training loss 0.10122241824865341 Validation loss 0.10517559945583344 Accuracy 0.6795000433921814\n",
      "Iteration 60570 Training loss 0.08949992805719376 Validation loss 0.09390096366405487 Accuracy 0.721500039100647\n",
      "Iteration 60580 Training loss 0.10082367807626724 Validation loss 0.10000935196876526 Accuracy 0.6995000243186951\n",
      "Iteration 60590 Training loss 0.0773388147354126 Validation loss 0.09368152171373367 Accuracy 0.7285000085830688\n",
      "Iteration 60600 Training loss 0.07754571735858917 Validation loss 0.10060246288776398 Accuracy 0.6865000128746033\n",
      "Iteration 60610 Training loss 0.08732949942350388 Validation loss 0.10300561040639877 Accuracy 0.6755000352859497\n",
      "Iteration 60620 Training loss 0.08013517409563065 Validation loss 0.09707138687372208 Accuracy 0.7105000615119934\n",
      "Iteration 60630 Training loss 0.09706054627895355 Validation loss 0.09979704767465591 Accuracy 0.7080000042915344\n",
      "Iteration 60640 Training loss 0.0701705664396286 Validation loss 0.09098149836063385 Accuracy 0.7470000386238098\n",
      "Iteration 60650 Training loss 0.10001296550035477 Validation loss 0.0987614318728447 Accuracy 0.7090000510215759\n",
      "Iteration 60660 Training loss 0.08953437954187393 Validation loss 0.0942583978176117 Accuracy 0.7270000576972961\n",
      "Iteration 60670 Training loss 0.07810912281274796 Validation loss 0.09062324464321136 Accuracy 0.7400000095367432\n",
      "Iteration 60680 Training loss 0.07877952605485916 Validation loss 0.09550615400075912 Accuracy 0.7225000262260437\n",
      "Iteration 60690 Training loss 0.0645296573638916 Validation loss 0.09465107321739197 Accuracy 0.7295000553131104\n",
      "Iteration 60700 Training loss 0.11091593652963638 Validation loss 0.10089171677827835 Accuracy 0.690000057220459\n",
      "Iteration 60710 Training loss 0.09602274000644684 Validation loss 0.0898146778345108 Accuracy 0.7390000224113464\n",
      "Iteration 60720 Training loss 0.09212739020586014 Validation loss 0.09351155906915665 Accuracy 0.7245000600814819\n",
      "Iteration 60730 Training loss 0.08155902475118637 Validation loss 0.09369692206382751 Accuracy 0.7330000400543213\n",
      "Iteration 60740 Training loss 0.09243427217006683 Validation loss 0.09148819744586945 Accuracy 0.7300000190734863\n",
      "Iteration 60750 Training loss 0.1017090231180191 Validation loss 0.10065121203660965 Accuracy 0.6945000290870667\n",
      "Iteration 60760 Training loss 0.08217775076627731 Validation loss 0.0921909362077713 Accuracy 0.7270000576972961\n",
      "Iteration 60770 Training loss 0.06634137034416199 Validation loss 0.09127708524465561 Accuracy 0.7240000367164612\n",
      "Iteration 60780 Training loss 0.09498194605112076 Validation loss 0.09432506561279297 Accuracy 0.718500018119812\n",
      "Iteration 60790 Training loss 0.08696947246789932 Validation loss 0.09016361832618713 Accuracy 0.7415000200271606\n",
      "Iteration 60800 Training loss 0.08702079206705093 Validation loss 0.09856165945529938 Accuracy 0.7110000252723694\n",
      "Iteration 60810 Training loss 0.08319484442472458 Validation loss 0.09768913686275482 Accuracy 0.6970000267028809\n",
      "Iteration 60820 Training loss 0.11133787035942078 Validation loss 0.09472167491912842 Accuracy 0.7250000238418579\n",
      "Iteration 60830 Training loss 0.08310006558895111 Validation loss 0.0930839404463768 Accuracy 0.7365000247955322\n",
      "Iteration 60840 Training loss 0.08981456607580185 Validation loss 0.09042666852474213 Accuracy 0.7350000143051147\n",
      "Iteration 60850 Training loss 0.0957954004406929 Validation loss 0.100333072245121 Accuracy 0.7020000219345093\n",
      "Iteration 60860 Training loss 0.08411819487810135 Validation loss 0.10264020413160324 Accuracy 0.6875000596046448\n",
      "Iteration 60870 Training loss 0.07302191853523254 Validation loss 0.09065045416355133 Accuracy 0.733500063419342\n",
      "Iteration 60880 Training loss 0.09008388221263885 Validation loss 0.09858554601669312 Accuracy 0.7095000147819519\n",
      "Iteration 60890 Training loss 0.09577050805091858 Validation loss 0.09161453694105148 Accuracy 0.7405000329017639\n",
      "Iteration 60900 Training loss 0.08731193095445633 Validation loss 0.09436163306236267 Accuracy 0.7260000109672546\n",
      "Iteration 60910 Training loss 0.08719873428344727 Validation loss 0.09435117989778519 Accuracy 0.7270000576972961\n",
      "Iteration 60920 Training loss 0.0719907283782959 Validation loss 0.09305676817893982 Accuracy 0.7295000553131104\n",
      "Iteration 60930 Training loss 0.1062375009059906 Validation loss 0.11252737045288086 Accuracy 0.6575000286102295\n",
      "Iteration 60940 Training loss 0.09923132508993149 Validation loss 0.0959228128194809 Accuracy 0.7115000486373901\n",
      "Iteration 60950 Training loss 0.08097503334283829 Validation loss 0.09377246350049973 Accuracy 0.7255000472068787\n",
      "Iteration 60960 Training loss 0.08295644819736481 Validation loss 0.0946345254778862 Accuracy 0.7205000519752502\n",
      "Iteration 60970 Training loss 0.09758871793746948 Validation loss 0.09338569641113281 Accuracy 0.7385000586509705\n",
      "Iteration 60980 Training loss 0.08605342358350754 Validation loss 0.0998862087726593 Accuracy 0.7065000534057617\n",
      "Iteration 60990 Training loss 0.11325196921825409 Validation loss 0.10751010477542877 Accuracy 0.6720000505447388\n",
      "Iteration 61000 Training loss 0.0871623307466507 Validation loss 0.10101217776536942 Accuracy 0.6960000395774841\n",
      "Iteration 61010 Training loss 0.09341640025377274 Validation loss 0.09181933850049973 Accuracy 0.7425000071525574\n",
      "Iteration 61020 Training loss 0.08391784131526947 Validation loss 0.09144478291273117 Accuracy 0.7410000562667847\n",
      "Iteration 61030 Training loss 0.09250492602586746 Validation loss 0.09717993438243866 Accuracy 0.7165000438690186\n",
      "Iteration 61040 Training loss 0.08973634243011475 Validation loss 0.10917206108570099 Accuracy 0.6765000224113464\n",
      "Iteration 61050 Training loss 0.08801242709159851 Validation loss 0.09258163720369339 Accuracy 0.7270000576972961\n",
      "Iteration 61060 Training loss 0.09654152393341064 Validation loss 0.09294939041137695 Accuracy 0.7250000238418579\n",
      "Iteration 61070 Training loss 0.09173247963190079 Validation loss 0.09368333965539932 Accuracy 0.7290000319480896\n",
      "Iteration 61080 Training loss 0.09287761151790619 Validation loss 0.09104999899864197 Accuracy 0.7305000424385071\n",
      "Iteration 61090 Training loss 0.09743225574493408 Validation loss 0.09437675774097443 Accuracy 0.7155000567436218\n",
      "Iteration 61100 Training loss 0.06385113298892975 Validation loss 0.08851122856140137 Accuracy 0.7455000281333923\n",
      "Iteration 61110 Training loss 0.09472543746232986 Validation loss 0.0893872082233429 Accuracy 0.7465000152587891\n",
      "Iteration 61120 Training loss 0.08421134948730469 Validation loss 0.09172983467578888 Accuracy 0.7400000095367432\n",
      "Iteration 61130 Training loss 0.08913516998291016 Validation loss 0.09387243539094925 Accuracy 0.7310000061988831\n",
      "Iteration 61140 Training loss 0.09812814742326736 Validation loss 0.08857019990682602 Accuracy 0.7440000176429749\n",
      "Iteration 61150 Training loss 0.08696097880601883 Validation loss 0.11159162223339081 Accuracy 0.643500030040741\n",
      "Iteration 61160 Training loss 0.09018970280885696 Validation loss 0.09298178553581238 Accuracy 0.7290000319480896\n",
      "Iteration 61170 Training loss 0.09831292927265167 Validation loss 0.09805422276258469 Accuracy 0.7065000534057617\n",
      "Iteration 61180 Training loss 0.09487291425466537 Validation loss 0.11516641080379486 Accuracy 0.6490000486373901\n",
      "Iteration 61190 Training loss 0.09487555921077728 Validation loss 0.09998141974210739 Accuracy 0.7020000219345093\n",
      "Iteration 61200 Training loss 0.09101790934801102 Validation loss 0.09792301058769226 Accuracy 0.7045000195503235\n",
      "Iteration 61210 Training loss 0.09411799907684326 Validation loss 0.09239976108074188 Accuracy 0.7235000133514404\n",
      "Iteration 61220 Training loss 0.07733235508203506 Validation loss 0.0914740040898323 Accuracy 0.7320000529289246\n",
      "Iteration 61230 Training loss 0.10032595694065094 Validation loss 0.09909158200025558 Accuracy 0.7065000534057617\n",
      "Iteration 61240 Training loss 0.09344881772994995 Validation loss 0.09597180038690567 Accuracy 0.7140000462532043\n",
      "Iteration 61250 Training loss 0.10133300721645355 Validation loss 0.1007382869720459 Accuracy 0.706000030040741\n",
      "Iteration 61260 Training loss 0.09812882542610168 Validation loss 0.09447464346885681 Accuracy 0.721500039100647\n",
      "Iteration 61270 Training loss 0.07783038169145584 Validation loss 0.08944973349571228 Accuracy 0.737000048160553\n",
      "Iteration 61280 Training loss 0.09836003929376602 Validation loss 0.09035439789295197 Accuracy 0.7330000400543213\n",
      "Iteration 61290 Training loss 0.08488958328962326 Validation loss 0.0934237539768219 Accuracy 0.7290000319480896\n",
      "Iteration 61300 Training loss 0.09193231165409088 Validation loss 0.09923041611909866 Accuracy 0.6960000395774841\n",
      "Iteration 61310 Training loss 0.08680841326713562 Validation loss 0.09313522279262543 Accuracy 0.7205000519752502\n",
      "Iteration 61320 Training loss 0.09089858829975128 Validation loss 0.09705495089292526 Accuracy 0.7175000309944153\n",
      "Iteration 61330 Training loss 0.07721810042858124 Validation loss 0.08980583399534225 Accuracy 0.7440000176429749\n",
      "Iteration 61340 Training loss 0.08658376336097717 Validation loss 0.0930657833814621 Accuracy 0.7200000286102295\n",
      "Iteration 61350 Training loss 0.1010606586933136 Validation loss 0.08926890790462494 Accuracy 0.7470000386238098\n",
      "Iteration 61360 Training loss 0.09088347107172012 Validation loss 0.09005647897720337 Accuracy 0.7410000562667847\n",
      "Iteration 61370 Training loss 0.08809522539377213 Validation loss 0.09056075662374496 Accuracy 0.733500063419342\n",
      "Iteration 61380 Training loss 0.10023011267185211 Validation loss 0.10844238102436066 Accuracy 0.6640000343322754\n",
      "Iteration 61390 Training loss 0.09616418927907944 Validation loss 0.09461306035518646 Accuracy 0.7295000553131104\n",
      "Iteration 61400 Training loss 0.10997278243303299 Validation loss 0.10256502032279968 Accuracy 0.6865000128746033\n",
      "Iteration 61410 Training loss 0.10593380779027939 Validation loss 0.09611290693283081 Accuracy 0.7175000309944153\n",
      "Iteration 61420 Training loss 0.10684960335493088 Validation loss 0.09910803288221359 Accuracy 0.6975000500679016\n",
      "Iteration 61430 Training loss 0.08212560415267944 Validation loss 0.09132324904203415 Accuracy 0.7330000400543213\n",
      "Iteration 61440 Training loss 0.11327319592237473 Validation loss 0.1168675422668457 Accuracy 0.6470000147819519\n",
      "Iteration 61450 Training loss 0.09484461694955826 Validation loss 0.10071713477373123 Accuracy 0.7005000114440918\n",
      "Iteration 61460 Training loss 0.09258443862199783 Validation loss 0.10162987560033798 Accuracy 0.70250004529953\n",
      "Iteration 61470 Training loss 0.1146710142493248 Validation loss 0.12039364874362946 Accuracy 0.6225000023841858\n",
      "Iteration 61480 Training loss 0.1181977167725563 Validation loss 0.09845304489135742 Accuracy 0.7100000381469727\n",
      "Iteration 61490 Training loss 0.0941740944981575 Validation loss 0.09271867573261261 Accuracy 0.7290000319480896\n",
      "Iteration 61500 Training loss 0.09989562630653381 Validation loss 0.09531789273023605 Accuracy 0.7140000462532043\n",
      "Iteration 61510 Training loss 0.07433055341243744 Validation loss 0.09753035008907318 Accuracy 0.7110000252723694\n",
      "Iteration 61520 Training loss 0.09208128601312637 Validation loss 0.0990545004606247 Accuracy 0.6945000290870667\n",
      "Iteration 61530 Training loss 0.07992879301309586 Validation loss 0.09553651511669159 Accuracy 0.7230000495910645\n",
      "Iteration 61540 Training loss 0.08517489582300186 Validation loss 0.09400559216737747 Accuracy 0.7230000495910645\n",
      "Iteration 61550 Training loss 0.09622274339199066 Validation loss 0.10089165717363358 Accuracy 0.703000009059906\n",
      "Iteration 61560 Training loss 0.0761801153421402 Validation loss 0.09054175019264221 Accuracy 0.7320000529289246\n",
      "Iteration 61570 Training loss 0.06400398910045624 Validation loss 0.0943998172879219 Accuracy 0.7110000252723694\n",
      "Iteration 61580 Training loss 0.10112465918064117 Validation loss 0.10082364827394485 Accuracy 0.6885000467300415\n",
      "Iteration 61590 Training loss 0.09316850453615189 Validation loss 0.10415936261415482 Accuracy 0.671500027179718\n",
      "Iteration 61600 Training loss 0.10514966398477554 Validation loss 0.09052535891532898 Accuracy 0.7380000352859497\n",
      "Iteration 61610 Training loss 0.09003322571516037 Validation loss 0.08981862664222717 Accuracy 0.7405000329017639\n",
      "Iteration 61620 Training loss 0.16109701991081238 Validation loss 0.12744934856891632 Accuracy 0.624500036239624\n",
      "Iteration 61630 Training loss 0.08614577353000641 Validation loss 0.10040483623743057 Accuracy 0.7040000557899475\n",
      "Iteration 61640 Training loss 0.08552837371826172 Validation loss 0.09249315410852432 Accuracy 0.737000048160553\n",
      "Iteration 61650 Training loss 0.10645946860313416 Validation loss 0.1091689020395279 Accuracy 0.6675000190734863\n",
      "Iteration 61660 Training loss 0.07173273712396622 Validation loss 0.09427866339683533 Accuracy 0.737500011920929\n",
      "Iteration 61670 Training loss 0.0899679958820343 Validation loss 0.09390172362327576 Accuracy 0.7190000414848328\n",
      "Iteration 61680 Training loss 0.08614394813776016 Validation loss 0.09415937960147858 Accuracy 0.7220000624656677\n",
      "Iteration 61690 Training loss 0.09857482463121414 Validation loss 0.09815426915884018 Accuracy 0.7205000519752502\n",
      "Iteration 61700 Training loss 0.08119416981935501 Validation loss 0.0951751172542572 Accuracy 0.718500018119812\n",
      "Iteration 61710 Training loss 0.08622096478939056 Validation loss 0.09133418649435043 Accuracy 0.7395000457763672\n",
      "Iteration 61720 Training loss 0.09813366085290909 Validation loss 0.10013633966445923 Accuracy 0.6990000605583191\n",
      "Iteration 61730 Training loss 0.10401096194982529 Validation loss 0.10248744487762451 Accuracy 0.6925000548362732\n",
      "Iteration 61740 Training loss 0.08345270901918411 Validation loss 0.08999110013246536 Accuracy 0.7430000305175781\n",
      "Iteration 61750 Training loss 0.08031368255615234 Validation loss 0.09709692001342773 Accuracy 0.7120000123977661\n",
      "Iteration 61760 Training loss 0.08364778012037277 Validation loss 0.0982305034995079 Accuracy 0.7100000381469727\n",
      "Iteration 61770 Training loss 0.07245350629091263 Validation loss 0.09646192193031311 Accuracy 0.7235000133514404\n",
      "Iteration 61780 Training loss 0.10086528211832047 Validation loss 0.10158934444189072 Accuracy 0.7105000615119934\n",
      "Iteration 61790 Training loss 0.08423111587762833 Validation loss 0.09372914582490921 Accuracy 0.7315000295639038\n",
      "Iteration 61800 Training loss 0.08541471511125565 Validation loss 0.09158860892057419 Accuracy 0.7365000247955322\n",
      "Iteration 61810 Training loss 0.08934615552425385 Validation loss 0.09206055849790573 Accuracy 0.733500063419342\n",
      "Iteration 61820 Training loss 0.09957204014062881 Validation loss 0.09720811247825623 Accuracy 0.7145000100135803\n",
      "Iteration 61830 Training loss 0.08627554029226303 Validation loss 0.09097594767808914 Accuracy 0.7320000529289246\n",
      "Iteration 61840 Training loss 0.09026357531547546 Validation loss 0.09072154760360718 Accuracy 0.7360000610351562\n",
      "Iteration 61850 Training loss 0.10225003212690353 Validation loss 0.09942816942930222 Accuracy 0.6990000605583191\n",
      "Iteration 61860 Training loss 0.08479026705026627 Validation loss 0.0945771336555481 Accuracy 0.7250000238418579\n",
      "Iteration 61870 Training loss 0.07793516665697098 Validation loss 0.10212592780590057 Accuracy 0.6820000410079956\n",
      "Iteration 61880 Training loss 0.10240060836076736 Validation loss 0.09578500688076019 Accuracy 0.7110000252723694\n",
      "Iteration 61890 Training loss 0.08758430182933807 Validation loss 0.09126017987728119 Accuracy 0.749500036239624\n",
      "Iteration 61900 Training loss 0.06885568052530289 Validation loss 0.09116175025701523 Accuracy 0.7260000109672546\n",
      "Iteration 61910 Training loss 0.0943787544965744 Validation loss 0.09437806904315948 Accuracy 0.7140000462532043\n",
      "Iteration 61920 Training loss 0.11865144222974777 Validation loss 0.1196855679154396 Accuracy 0.6540000438690186\n",
      "Iteration 61930 Training loss 0.104502834379673 Validation loss 0.1015075221657753 Accuracy 0.6920000314712524\n",
      "Iteration 61940 Training loss 0.09558439254760742 Validation loss 0.08948265016078949 Accuracy 0.7360000610351562\n",
      "Iteration 61950 Training loss 0.07195490598678589 Validation loss 0.08980889618396759 Accuracy 0.7405000329017639\n",
      "Iteration 61960 Training loss 0.11471302062273026 Validation loss 0.10779176652431488 Accuracy 0.6630000472068787\n",
      "Iteration 61970 Training loss 0.10024963319301605 Validation loss 0.09078145027160645 Accuracy 0.7310000061988831\n",
      "Iteration 61980 Training loss 0.11195946484804153 Validation loss 0.10656966269016266 Accuracy 0.6790000200271606\n",
      "Iteration 61990 Training loss 0.09592714160680771 Validation loss 0.09602321684360504 Accuracy 0.7135000228881836\n",
      "Iteration 62000 Training loss 0.11667599529027939 Validation loss 0.0998663529753685 Accuracy 0.7005000114440918\n",
      "Iteration 62010 Training loss 0.09767220914363861 Validation loss 0.09259665012359619 Accuracy 0.7265000343322754\n",
      "Iteration 62020 Training loss 0.08831776678562164 Validation loss 0.09501868486404419 Accuracy 0.7270000576972961\n",
      "Iteration 62030 Training loss 0.08460243046283722 Validation loss 0.09682978689670563 Accuracy 0.7225000262260437\n",
      "Iteration 62040 Training loss 0.09591419249773026 Validation loss 0.091499462723732 Accuracy 0.7365000247955322\n",
      "Iteration 62050 Training loss 0.08301742374897003 Validation loss 0.09265811741352081 Accuracy 0.7260000109672546\n",
      "Iteration 62060 Training loss 0.07141978293657303 Validation loss 0.09253305196762085 Accuracy 0.7310000061988831\n",
      "Iteration 62070 Training loss 0.08375083655118942 Validation loss 0.09744546562433243 Accuracy 0.7135000228881836\n",
      "Iteration 62080 Training loss 0.08983910083770752 Validation loss 0.09380006790161133 Accuracy 0.7265000343322754\n",
      "Iteration 62090 Training loss 0.08722329884767532 Validation loss 0.08988762646913528 Accuracy 0.7390000224113464\n",
      "Iteration 62100 Training loss 0.08796168863773346 Validation loss 0.09424184262752533 Accuracy 0.7310000061988831\n",
      "Iteration 62110 Training loss 0.09412486106157303 Validation loss 0.09779389202594757 Accuracy 0.7125000357627869\n",
      "Iteration 62120 Training loss 0.08148816972970963 Validation loss 0.09258954972028732 Accuracy 0.7275000214576721\n",
      "Iteration 62130 Training loss 0.07732892036437988 Validation loss 0.09391295164823532 Accuracy 0.7095000147819519\n",
      "Iteration 62140 Training loss 0.08233669400215149 Validation loss 0.09269458800554276 Accuracy 0.7245000600814819\n",
      "Iteration 62150 Training loss 0.10326740145683289 Validation loss 0.10366994887590408 Accuracy 0.6945000290870667\n",
      "Iteration 62160 Training loss 0.09471704065799713 Validation loss 0.09095270186662674 Accuracy 0.7270000576972961\n",
      "Iteration 62170 Training loss 0.08488287031650543 Validation loss 0.09824350476264954 Accuracy 0.7130000591278076\n",
      "Iteration 62180 Training loss 0.0993155837059021 Validation loss 0.09710588306188583 Accuracy 0.7000000476837158\n",
      "Iteration 62190 Training loss 0.1052110493183136 Validation loss 0.10247771441936493 Accuracy 0.6835000514984131\n",
      "Iteration 62200 Training loss 0.06883757561445236 Validation loss 0.0914282277226448 Accuracy 0.737000048160553\n",
      "Iteration 62210 Training loss 0.08981644362211227 Validation loss 0.09353391081094742 Accuracy 0.7345000505447388\n",
      "Iteration 62220 Training loss 0.08947750926017761 Validation loss 0.09258069097995758 Accuracy 0.7300000190734863\n",
      "Iteration 62230 Training loss 0.08826111257076263 Validation loss 0.09291364997625351 Accuracy 0.7310000061988831\n",
      "Iteration 62240 Training loss 0.08298103511333466 Validation loss 0.08972258865833282 Accuracy 0.7390000224113464\n",
      "Iteration 62250 Training loss 0.0820075199007988 Validation loss 0.09465007483959198 Accuracy 0.7200000286102295\n",
      "Iteration 62260 Training loss 0.09381750971078873 Validation loss 0.0900128036737442 Accuracy 0.7360000610351562\n",
      "Iteration 62270 Training loss 0.08062883466482162 Validation loss 0.08847824484109879 Accuracy 0.7415000200271606\n",
      "Iteration 62280 Training loss 0.10298386216163635 Validation loss 0.0941755399107933 Accuracy 0.7280000448226929\n",
      "Iteration 62290 Training loss 0.0815407857298851 Validation loss 0.09174612164497375 Accuracy 0.7445000410079956\n",
      "Iteration 62300 Training loss 0.10549583286046982 Validation loss 0.09841759502887726 Accuracy 0.7005000114440918\n",
      "Iteration 62310 Training loss 0.09893497079610825 Validation loss 0.10231800377368927 Accuracy 0.6990000605583191\n",
      "Iteration 62320 Training loss 0.09919700771570206 Validation loss 0.10045900195837021 Accuracy 0.7005000114440918\n",
      "Iteration 62330 Training loss 0.07912025600671768 Validation loss 0.09307930618524551 Accuracy 0.7345000505447388\n",
      "Iteration 62340 Training loss 0.10011087357997894 Validation loss 0.09170600026845932 Accuracy 0.7315000295639038\n",
      "Iteration 62350 Training loss 0.1086452454328537 Validation loss 0.10440651327371597 Accuracy 0.6960000395774841\n",
      "Iteration 62360 Training loss 0.09717798233032227 Validation loss 0.0998271033167839 Accuracy 0.7045000195503235\n",
      "Iteration 62370 Training loss 0.12385498732328415 Validation loss 0.11750401556491852 Accuracy 0.6300000548362732\n",
      "Iteration 62380 Training loss 0.09224218130111694 Validation loss 0.09851407259702682 Accuracy 0.7035000324249268\n",
      "Iteration 62390 Training loss 0.08368680626153946 Validation loss 0.09400559216737747 Accuracy 0.733500063419342\n",
      "Iteration 62400 Training loss 0.10810384154319763 Validation loss 0.10678065568208694 Accuracy 0.6725000143051147\n",
      "Iteration 62410 Training loss 0.10348989069461823 Validation loss 0.10152297466993332 Accuracy 0.6935000419616699\n",
      "Iteration 62420 Training loss 0.08539073914289474 Validation loss 0.094579316675663 Accuracy 0.718500018119812\n",
      "Iteration 62430 Training loss 0.09242098778486252 Validation loss 0.09340701997280121 Accuracy 0.7245000600814819\n",
      "Iteration 62440 Training loss 0.09936803579330444 Validation loss 0.09264810383319855 Accuracy 0.7345000505447388\n",
      "Iteration 62450 Training loss 0.08481495827436447 Validation loss 0.10102934390306473 Accuracy 0.7100000381469727\n",
      "Iteration 62460 Training loss 0.10270258039236069 Validation loss 0.09259738028049469 Accuracy 0.7300000190734863\n",
      "Iteration 62470 Training loss 0.0981510654091835 Validation loss 0.091361865401268 Accuracy 0.7330000400543213\n",
      "Iteration 62480 Training loss 0.08717942982912064 Validation loss 0.1012083888053894 Accuracy 0.6790000200271606\n",
      "Iteration 62490 Training loss 0.1028914824128151 Validation loss 0.09312169253826141 Accuracy 0.7275000214576721\n",
      "Iteration 62500 Training loss 0.08747150003910065 Validation loss 0.10499786585569382 Accuracy 0.6890000104904175\n",
      "Iteration 62510 Training loss 0.10539787262678146 Validation loss 0.09251634776592255 Accuracy 0.7205000519752502\n",
      "Iteration 62520 Training loss 0.07996372878551483 Validation loss 0.08994266390800476 Accuracy 0.7350000143051147\n",
      "Iteration 62530 Training loss 0.08360704034566879 Validation loss 0.09558377414941788 Accuracy 0.7275000214576721\n",
      "Iteration 62540 Training loss 0.08922556042671204 Validation loss 0.09145752340555191 Accuracy 0.7355000376701355\n",
      "Iteration 62550 Training loss 0.09260907769203186 Validation loss 0.10747179388999939 Accuracy 0.6705000400543213\n",
      "Iteration 62560 Training loss 0.07504057139158249 Validation loss 0.0923091322183609 Accuracy 0.7295000553131104\n",
      "Iteration 62570 Training loss 0.08585081994533539 Validation loss 0.09409505873918533 Accuracy 0.7270000576972961\n",
      "Iteration 62580 Training loss 0.08414210379123688 Validation loss 0.0976114422082901 Accuracy 0.7125000357627869\n",
      "Iteration 62590 Training loss 0.08520543575286865 Validation loss 0.09082522243261337 Accuracy 0.737000048160553\n",
      "Iteration 62600 Training loss 0.10954581201076508 Validation loss 0.11209749430418015 Accuracy 0.6390000581741333\n",
      "Iteration 62610 Training loss 0.08302940428256989 Validation loss 0.09627921134233475 Accuracy 0.7090000510215759\n",
      "Iteration 62620 Training loss 0.09219010174274445 Validation loss 0.09893609583377838 Accuracy 0.6920000314712524\n",
      "Iteration 62630 Training loss 0.09828906506299973 Validation loss 0.09467819333076477 Accuracy 0.7255000472068787\n",
      "Iteration 62640 Training loss 0.08796872943639755 Validation loss 0.09794118255376816 Accuracy 0.7170000076293945\n",
      "Iteration 62650 Training loss 0.09601472318172455 Validation loss 0.11074400693178177 Accuracy 0.6450000405311584\n",
      "Iteration 62660 Training loss 0.077812060713768 Validation loss 0.0958608090877533 Accuracy 0.7250000238418579\n",
      "Iteration 62670 Training loss 0.0925854966044426 Validation loss 0.10566619038581848 Accuracy 0.6885000467300415\n",
      "Iteration 62680 Training loss 0.06949766725301743 Validation loss 0.09661876410245895 Accuracy 0.7235000133514404\n",
      "Iteration 62690 Training loss 0.10399072617292404 Validation loss 0.1152050718665123 Accuracy 0.6710000038146973\n",
      "Iteration 62700 Training loss 0.10297662764787674 Validation loss 0.09932274371385574 Accuracy 0.6875000596046448\n",
      "Iteration 62710 Training loss 0.07609210163354874 Validation loss 0.09753213077783585 Accuracy 0.7070000171661377\n",
      "Iteration 62720 Training loss 0.10523536801338196 Validation loss 0.09609216451644897 Accuracy 0.7100000381469727\n",
      "Iteration 62730 Training loss 0.09251165390014648 Validation loss 0.09243717044591904 Accuracy 0.7255000472068787\n",
      "Iteration 62740 Training loss 0.06327855587005615 Validation loss 0.1004880741238594 Accuracy 0.7095000147819519\n",
      "Iteration 62750 Training loss 0.10568922013044357 Validation loss 0.1194717139005661 Accuracy 0.6210000514984131\n",
      "Iteration 62760 Training loss 0.10404010862112045 Validation loss 0.09750084578990936 Accuracy 0.7105000615119934\n",
      "Iteration 62770 Training loss 0.09438630193471909 Validation loss 0.10879167914390564 Accuracy 0.6520000100135803\n",
      "Iteration 62780 Training loss 0.09843578189611435 Validation loss 0.09681514650583267 Accuracy 0.7020000219345093\n",
      "Iteration 62790 Training loss 0.0858464390039444 Validation loss 0.09084612131118774 Accuracy 0.7390000224113464\n",
      "Iteration 62800 Training loss 0.08843027055263519 Validation loss 0.1060866117477417 Accuracy 0.674500048160553\n",
      "Iteration 62810 Training loss 0.0814308375120163 Validation loss 0.0910748690366745 Accuracy 0.7380000352859497\n",
      "Iteration 62820 Training loss 0.10078475624322891 Validation loss 0.09531672298908234 Accuracy 0.7260000109672546\n",
      "Iteration 62830 Training loss 0.08834011107683182 Validation loss 0.09961237013339996 Accuracy 0.7085000276565552\n",
      "Iteration 62840 Training loss 0.09775439649820328 Validation loss 0.11348787695169449 Accuracy 0.6695000529289246\n",
      "Iteration 62850 Training loss 0.07853187620639801 Validation loss 0.09313695132732391 Accuracy 0.7310000061988831\n",
      "Iteration 62860 Training loss 0.07907260954380035 Validation loss 0.0894082710146904 Accuracy 0.7455000281333923\n",
      "Iteration 62870 Training loss 0.08383826166391373 Validation loss 0.09428852051496506 Accuracy 0.7365000247955322\n",
      "Iteration 62880 Training loss 0.08837646245956421 Validation loss 0.10091420263051987 Accuracy 0.690000057220459\n",
      "Iteration 62890 Training loss 0.10475046932697296 Validation loss 0.09950181841850281 Accuracy 0.6975000500679016\n",
      "Iteration 62900 Training loss 0.12251979857683182 Validation loss 0.10835853964090347 Accuracy 0.6580000519752502\n",
      "Iteration 62910 Training loss 0.08695394545793533 Validation loss 0.09114030748605728 Accuracy 0.737500011920929\n",
      "Iteration 62920 Training loss 0.10084642469882965 Validation loss 0.11400415003299713 Accuracy 0.6365000009536743\n",
      "Iteration 62930 Training loss 0.101094089448452 Validation loss 0.10080983489751816 Accuracy 0.7070000171661377\n",
      "Iteration 62940 Training loss 0.1184355691075325 Validation loss 0.08784395456314087 Accuracy 0.7445000410079956\n",
      "Iteration 62950 Training loss 0.0876973494887352 Validation loss 0.09917473047971725 Accuracy 0.7000000476837158\n",
      "Iteration 62960 Training loss 0.09726317971944809 Validation loss 0.09544844180345535 Accuracy 0.7225000262260437\n",
      "Iteration 62970 Training loss 0.08777520805597305 Validation loss 0.10691928118467331 Accuracy 0.6685000061988831\n",
      "Iteration 62980 Training loss 0.08969565480947495 Validation loss 0.08928967267274857 Accuracy 0.7440000176429749\n",
      "Iteration 62990 Training loss 0.09509605169296265 Validation loss 0.1016359031200409 Accuracy 0.6950000524520874\n",
      "Iteration 63000 Training loss 0.09006204456090927 Validation loss 0.09531220048666 Accuracy 0.7350000143051147\n",
      "Iteration 63010 Training loss 0.10108205676078796 Validation loss 0.10683413594961166 Accuracy 0.6670000553131104\n",
      "Iteration 63020 Training loss 0.0895724669098854 Validation loss 0.09253959357738495 Accuracy 0.7330000400543213\n",
      "Iteration 63030 Training loss 0.0982830822467804 Validation loss 0.09314291179180145 Accuracy 0.7245000600814819\n",
      "Iteration 63040 Training loss 0.08511470258235931 Validation loss 0.10044734925031662 Accuracy 0.6945000290870667\n",
      "Iteration 63050 Training loss 0.08402048051357269 Validation loss 0.09423079341650009 Accuracy 0.7345000505447388\n",
      "Iteration 63060 Training loss 0.08717897534370422 Validation loss 0.09776653349399567 Accuracy 0.6960000395774841\n",
      "Iteration 63070 Training loss 0.11414691805839539 Validation loss 0.10687758773565292 Accuracy 0.6770000457763672\n",
      "Iteration 63080 Training loss 0.09356588125228882 Validation loss 0.0919736996293068 Accuracy 0.7290000319480896\n",
      "Iteration 63090 Training loss 0.10765083134174347 Validation loss 0.09287449717521667 Accuracy 0.7255000472068787\n",
      "Iteration 63100 Training loss 0.09448978304862976 Validation loss 0.09056505560874939 Accuracy 0.7350000143051147\n",
      "Iteration 63110 Training loss 0.10690949857234955 Validation loss 0.11936134845018387 Accuracy 0.6455000042915344\n",
      "Iteration 63120 Training loss 0.08138874173164368 Validation loss 0.09068708121776581 Accuracy 0.7310000061988831\n",
      "Iteration 63130 Training loss 0.08366912603378296 Validation loss 0.0900154560804367 Accuracy 0.7490000128746033\n",
      "Iteration 63140 Training loss 0.08967305719852448 Validation loss 0.0944608747959137 Accuracy 0.7155000567436218\n",
      "Iteration 63150 Training loss 0.08262849599123001 Validation loss 0.09146199375391006 Accuracy 0.7290000319480896\n",
      "Iteration 63160 Training loss 0.09527690708637238 Validation loss 0.09329332411289215 Accuracy 0.718500018119812\n",
      "Iteration 63170 Training loss 0.07359116524457932 Validation loss 0.08964656293392181 Accuracy 0.7410000562667847\n",
      "Iteration 63180 Training loss 0.09444209188222885 Validation loss 0.09138567745685577 Accuracy 0.7345000505447388\n",
      "Iteration 63190 Training loss 0.09598052501678467 Validation loss 0.09744086116552353 Accuracy 0.7150000333786011\n",
      "Iteration 63200 Training loss 0.10685710608959198 Validation loss 0.10659702122211456 Accuracy 0.674500048160553\n",
      "Iteration 63210 Training loss 0.07560686767101288 Validation loss 0.09092050790786743 Accuracy 0.7360000610351562\n",
      "Iteration 63220 Training loss 0.11291036009788513 Validation loss 0.11196839064359665 Accuracy 0.6540000438690186\n",
      "Iteration 63230 Training loss 0.0864005908370018 Validation loss 0.09014375507831573 Accuracy 0.7350000143051147\n",
      "Iteration 63240 Training loss 0.11432897299528122 Validation loss 0.11149924248456955 Accuracy 0.6630000472068787\n",
      "Iteration 63250 Training loss 0.08971992135047913 Validation loss 0.09522481262683868 Accuracy 0.7140000462532043\n",
      "Iteration 63260 Training loss 0.10192342847585678 Validation loss 0.12710751593112946 Accuracy 0.6365000009536743\n",
      "Iteration 63270 Training loss 0.08424874395132065 Validation loss 0.09240096062421799 Accuracy 0.7310000061988831\n",
      "Iteration 63280 Training loss 0.10125524550676346 Validation loss 0.09296675026416779 Accuracy 0.7235000133514404\n",
      "Iteration 63290 Training loss 0.12695497274398804 Validation loss 0.12077902257442474 Accuracy 0.6525000333786011\n",
      "Iteration 63300 Training loss 0.09432490915060043 Validation loss 0.10153621435165405 Accuracy 0.6945000290870667\n",
      "Iteration 63310 Training loss 0.07892999798059464 Validation loss 0.09129142761230469 Accuracy 0.7350000143051147\n",
      "Iteration 63320 Training loss 0.09729413688182831 Validation loss 0.10503040254116058 Accuracy 0.6710000038146973\n",
      "Iteration 63330 Training loss 0.09267446398735046 Validation loss 0.09345787763595581 Accuracy 0.7295000553131104\n",
      "Iteration 63340 Training loss 0.07508902996778488 Validation loss 0.08877928555011749 Accuracy 0.7385000586509705\n",
      "Iteration 63350 Training loss 0.08256404846906662 Validation loss 0.08973316103219986 Accuracy 0.7465000152587891\n",
      "Iteration 63360 Training loss 0.09269142895936966 Validation loss 0.09518152475357056 Accuracy 0.7190000414848328\n",
      "Iteration 63370 Training loss 0.09248901158571243 Validation loss 0.09872420132160187 Accuracy 0.6945000290870667\n",
      "Iteration 63380 Training loss 0.08828771114349365 Validation loss 0.09695503860712051 Accuracy 0.7190000414848328\n",
      "Iteration 63390 Training loss 0.07238638401031494 Validation loss 0.09428483247756958 Accuracy 0.7330000400543213\n",
      "Iteration 63400 Training loss 0.12019019573926926 Validation loss 0.09740109741687775 Accuracy 0.7090000510215759\n",
      "Iteration 63410 Training loss 0.09592902660369873 Validation loss 0.10230489075183868 Accuracy 0.6975000500679016\n",
      "Iteration 63420 Training loss 0.10438548773527145 Validation loss 0.10995101928710938 Accuracy 0.6630000472068787\n",
      "Iteration 63430 Training loss 0.0781087726354599 Validation loss 0.09678160399198532 Accuracy 0.718000054359436\n",
      "Iteration 63440 Training loss 0.08655095100402832 Validation loss 0.09498494863510132 Accuracy 0.718500018119812\n",
      "Iteration 63450 Training loss 0.11333130300045013 Validation loss 0.09844823181629181 Accuracy 0.7085000276565552\n",
      "Iteration 63460 Training loss 0.10092780739068985 Validation loss 0.10233112424612045 Accuracy 0.6935000419616699\n",
      "Iteration 63470 Training loss 0.086360402405262 Validation loss 0.09238827228546143 Accuracy 0.7285000085830688\n",
      "Iteration 63480 Training loss 0.08183497190475464 Validation loss 0.0931834876537323 Accuracy 0.7200000286102295\n",
      "Iteration 63490 Training loss 0.07815084606409073 Validation loss 0.10420145094394684 Accuracy 0.6985000371932983\n",
      "Iteration 63500 Training loss 0.08354030549526215 Validation loss 0.09143906086683273 Accuracy 0.7355000376701355\n",
      "Iteration 63510 Training loss 0.08237013965845108 Validation loss 0.09003891050815582 Accuracy 0.7325000166893005\n",
      "Iteration 63520 Training loss 0.0814126506447792 Validation loss 0.0906042754650116 Accuracy 0.7415000200271606\n",
      "Iteration 63530 Training loss 0.09424468874931335 Validation loss 0.09267047792673111 Accuracy 0.7345000505447388\n",
      "Iteration 63540 Training loss 0.08217573165893555 Validation loss 0.09049282968044281 Accuracy 0.7385000586509705\n",
      "Iteration 63550 Training loss 0.08874768018722534 Validation loss 0.09085173159837723 Accuracy 0.733500063419342\n",
      "Iteration 63560 Training loss 0.08486538380384445 Validation loss 0.09074951708316803 Accuracy 0.7365000247955322\n",
      "Iteration 63570 Training loss 0.09261216968297958 Validation loss 0.09059326350688934 Accuracy 0.7395000457763672\n",
      "Iteration 63580 Training loss 0.09260038286447525 Validation loss 0.08977655321359634 Accuracy 0.7425000071525574\n",
      "Iteration 63590 Training loss 0.123079314827919 Validation loss 0.12402129173278809 Accuracy 0.6265000104904175\n",
      "Iteration 63600 Training loss 0.09640203416347504 Validation loss 0.1051587238907814 Accuracy 0.6880000233650208\n",
      "Iteration 63610 Training loss 0.09262311458587646 Validation loss 0.09872562438249588 Accuracy 0.703000009059906\n",
      "Iteration 63620 Training loss 0.08442379534244537 Validation loss 0.08824293315410614 Accuracy 0.7505000233650208\n",
      "Iteration 63630 Training loss 0.1146882027387619 Validation loss 0.10558802634477615 Accuracy 0.6855000257492065\n",
      "Iteration 63640 Training loss 0.08237478882074356 Validation loss 0.09074407815933228 Accuracy 0.7385000586509705\n",
      "Iteration 63650 Training loss 0.08003188669681549 Validation loss 0.09041504561901093 Accuracy 0.7425000071525574\n",
      "Iteration 63660 Training loss 0.11032191663980484 Validation loss 0.1214391365647316 Accuracy 0.6265000104904175\n",
      "Iteration 63670 Training loss 0.08018684387207031 Validation loss 0.08862309902906418 Accuracy 0.7460000514984131\n",
      "Iteration 63680 Training loss 0.09024221450090408 Validation loss 0.08857770264148712 Accuracy 0.7455000281333923\n",
      "Iteration 63690 Training loss 0.0847550556063652 Validation loss 0.09742075204849243 Accuracy 0.7000000476837158\n",
      "Iteration 63700 Training loss 0.08805673569440842 Validation loss 0.09293404966592789 Accuracy 0.7250000238418579\n",
      "Iteration 63710 Training loss 0.09678586572408676 Validation loss 0.0930529236793518 Accuracy 0.7325000166893005\n",
      "Iteration 63720 Training loss 0.09341314435005188 Validation loss 0.09663449972867966 Accuracy 0.7050000429153442\n",
      "Iteration 63730 Training loss 0.08240604400634766 Validation loss 0.08879552781581879 Accuracy 0.7385000586509705\n",
      "Iteration 63740 Training loss 0.08788110315799713 Validation loss 0.09384660422801971 Accuracy 0.7155000567436218\n",
      "Iteration 63750 Training loss 0.0848967656493187 Validation loss 0.09730241447687149 Accuracy 0.7130000591278076\n",
      "Iteration 63760 Training loss 0.10044316202402115 Validation loss 0.10873828828334808 Accuracy 0.6710000038146973\n",
      "Iteration 63770 Training loss 0.07918792963027954 Validation loss 0.0971347838640213 Accuracy 0.7195000052452087\n",
      "Iteration 63780 Training loss 0.10440712422132492 Validation loss 0.09716905653476715 Accuracy 0.7140000462532043\n",
      "Iteration 63790 Training loss 0.09226004034280777 Validation loss 0.10673299431800842 Accuracy 0.6790000200271606\n",
      "Iteration 63800 Training loss 0.10465734452009201 Validation loss 0.10348484665155411 Accuracy 0.6965000033378601\n",
      "Iteration 63810 Training loss 0.07204744964838028 Validation loss 0.09218890219926834 Accuracy 0.718500018119812\n",
      "Iteration 63820 Training loss 0.08513098210096359 Validation loss 0.10366382449865341 Accuracy 0.6860000491142273\n",
      "Iteration 63830 Training loss 0.10042357444763184 Validation loss 0.09967271983623505 Accuracy 0.6990000605583191\n",
      "Iteration 63840 Training loss 0.07440143823623657 Validation loss 0.09000508487224579 Accuracy 0.7400000095367432\n",
      "Iteration 63850 Training loss 0.08028145879507065 Validation loss 0.1021236777305603 Accuracy 0.6830000281333923\n",
      "Iteration 63860 Training loss 0.07921584695577621 Validation loss 0.09047593921422958 Accuracy 0.733500063419342\n",
      "Iteration 63870 Training loss 0.0861884281039238 Validation loss 0.09489180892705917 Accuracy 0.7195000052452087\n",
      "Iteration 63880 Training loss 0.11352678388357162 Validation loss 0.1070241779088974 Accuracy 0.6795000433921814\n",
      "Iteration 63890 Training loss 0.09543211758136749 Validation loss 0.09116250276565552 Accuracy 0.7275000214576721\n",
      "Iteration 63900 Training loss 0.0823192372918129 Validation loss 0.09107411652803421 Accuracy 0.7325000166893005\n",
      "Iteration 63910 Training loss 0.0985272005200386 Validation loss 0.10751097649335861 Accuracy 0.671500027179718\n",
      "Iteration 63920 Training loss 0.09456730633974075 Validation loss 0.09813662618398666 Accuracy 0.7065000534057617\n",
      "Iteration 63930 Training loss 0.0784091055393219 Validation loss 0.09290514886379242 Accuracy 0.7275000214576721\n",
      "Iteration 63940 Training loss 0.08514942228794098 Validation loss 0.0916953906416893 Accuracy 0.7250000238418579\n",
      "Iteration 63950 Training loss 0.09812031686306 Validation loss 0.11395779997110367 Accuracy 0.6440000534057617\n",
      "Iteration 63960 Training loss 0.12142781168222427 Validation loss 0.11709665507078171 Accuracy 0.6415000557899475\n",
      "Iteration 63970 Training loss 0.10397861152887344 Validation loss 0.10202940553426743 Accuracy 0.690500020980835\n",
      "Iteration 63980 Training loss 0.08516429364681244 Validation loss 0.09515666216611862 Accuracy 0.7085000276565552\n",
      "Iteration 63990 Training loss 0.08932632952928543 Validation loss 0.090953029692173 Accuracy 0.733500063419342\n",
      "Iteration 64000 Training loss 0.07921062409877777 Validation loss 0.08934375643730164 Accuracy 0.7425000071525574\n",
      "Iteration 64010 Training loss 0.09366682171821594 Validation loss 0.0917716845870018 Accuracy 0.7300000190734863\n",
      "Iteration 64020 Training loss 0.10123290866613388 Validation loss 0.09024139493703842 Accuracy 0.7425000071525574\n",
      "Iteration 64030 Training loss 0.08613870292901993 Validation loss 0.09125164896249771 Accuracy 0.7260000109672546\n",
      "Iteration 64040 Training loss 0.08504101634025574 Validation loss 0.09364186972379684 Accuracy 0.7275000214576721\n",
      "Iteration 64050 Training loss 0.08877410739660263 Validation loss 0.09368932992219925 Accuracy 0.7250000238418579\n",
      "Iteration 64060 Training loss 0.0986863523721695 Validation loss 0.09857504814863205 Accuracy 0.7110000252723694\n",
      "Iteration 64070 Training loss 0.08700626343488693 Validation loss 0.0908796414732933 Accuracy 0.737500011920929\n",
      "Iteration 64080 Training loss 0.10263523459434509 Validation loss 0.10097279399633408 Accuracy 0.7005000114440918\n",
      "Iteration 64090 Training loss 0.11599171161651611 Validation loss 0.09948325157165527 Accuracy 0.706000030040741\n",
      "Iteration 64100 Training loss 0.0852622240781784 Validation loss 0.09015227109193802 Accuracy 0.7395000457763672\n",
      "Iteration 64110 Training loss 0.08561236411333084 Validation loss 0.10330922156572342 Accuracy 0.674500048160553\n",
      "Iteration 64120 Training loss 0.08923619985580444 Validation loss 0.10073170810937881 Accuracy 0.6940000057220459\n",
      "Iteration 64130 Training loss 0.106727235019207 Validation loss 0.11692573875188828 Accuracy 0.6485000252723694\n",
      "Iteration 64140 Training loss 0.09520508348941803 Validation loss 0.09544644504785538 Accuracy 0.7160000205039978\n",
      "Iteration 64150 Training loss 0.0965774729847908 Validation loss 0.09551496058702469 Accuracy 0.7200000286102295\n",
      "Iteration 64160 Training loss 0.0851449966430664 Validation loss 0.09414038807153702 Accuracy 0.7280000448226929\n",
      "Iteration 64170 Training loss 0.10465383529663086 Validation loss 0.09057087451219559 Accuracy 0.7410000562667847\n",
      "Iteration 64180 Training loss 0.0968804731965065 Validation loss 0.09417349845170975 Accuracy 0.7280000448226929\n",
      "Iteration 64190 Training loss 0.09777307510375977 Validation loss 0.09595615416765213 Accuracy 0.7130000591278076\n",
      "Iteration 64200 Training loss 0.07672028243541718 Validation loss 0.09589632600545883 Accuracy 0.7165000438690186\n",
      "Iteration 64210 Training loss 0.09131140261888504 Validation loss 0.09564422816038132 Accuracy 0.7045000195503235\n",
      "Iteration 64220 Training loss 0.08818095177412033 Validation loss 0.09151966124773026 Accuracy 0.7285000085830688\n",
      "Iteration 64230 Training loss 0.09788673371076584 Validation loss 0.09731489419937134 Accuracy 0.7160000205039978\n",
      "Iteration 64240 Training loss 0.10082823783159256 Validation loss 0.0929587259888649 Accuracy 0.734000027179718\n",
      "Iteration 64250 Training loss 0.07020407915115356 Validation loss 0.0917113795876503 Accuracy 0.7305000424385071\n",
      "Iteration 64260 Training loss 0.08053479343652725 Validation loss 0.09008271992206573 Accuracy 0.7355000376701355\n",
      "Iteration 64270 Training loss 0.09502330422401428 Validation loss 0.09408539533615112 Accuracy 0.7300000190734863\n",
      "Iteration 64280 Training loss 0.06543096154928207 Validation loss 0.09197406470775604 Accuracy 0.7250000238418579\n",
      "Iteration 64290 Training loss 0.07663064450025558 Validation loss 0.0969095528125763 Accuracy 0.718000054359436\n",
      "Iteration 64300 Training loss 0.09700656682252884 Validation loss 0.09197096526622772 Accuracy 0.737000048160553\n",
      "Iteration 64310 Training loss 0.08213214576244354 Validation loss 0.090177021920681 Accuracy 0.734000027179718\n",
      "Iteration 64320 Training loss 0.09053774923086166 Validation loss 0.09377360343933105 Accuracy 0.7190000414848328\n",
      "Iteration 64330 Training loss 0.08666422963142395 Validation loss 0.09361863881349564 Accuracy 0.7240000367164612\n",
      "Iteration 64340 Training loss 0.07135442644357681 Validation loss 0.08959978818893433 Accuracy 0.7465000152587891\n",
      "Iteration 64350 Training loss 0.07847326993942261 Validation loss 0.09185117483139038 Accuracy 0.7240000367164612\n",
      "Iteration 64360 Training loss 0.08250104635953903 Validation loss 0.10253231972455978 Accuracy 0.690000057220459\n",
      "Iteration 64370 Training loss 0.08057193458080292 Validation loss 0.08957074582576752 Accuracy 0.7400000095367432\n",
      "Iteration 64380 Training loss 0.12978991866111755 Validation loss 0.11975298076868057 Accuracy 0.6295000314712524\n",
      "Iteration 64390 Training loss 0.0875777155160904 Validation loss 0.0890468955039978 Accuracy 0.7430000305175781\n",
      "Iteration 64400 Training loss 0.07561811804771423 Validation loss 0.09114639461040497 Accuracy 0.7345000505447388\n",
      "Iteration 64410 Training loss 0.08204422891139984 Validation loss 0.09201861917972565 Accuracy 0.7250000238418579\n",
      "Iteration 64420 Training loss 0.09236707538366318 Validation loss 0.09888530522584915 Accuracy 0.6965000033378601\n",
      "Iteration 64430 Training loss 0.0850001871585846 Validation loss 0.09310921281576157 Accuracy 0.7250000238418579\n",
      "Iteration 64440 Training loss 0.07112816721200943 Validation loss 0.09082141518592834 Accuracy 0.7190000414848328\n",
      "Iteration 64450 Training loss 0.07475413382053375 Validation loss 0.0959877148270607 Accuracy 0.7150000333786011\n",
      "Iteration 64460 Training loss 0.07198020815849304 Validation loss 0.09113835543394089 Accuracy 0.737500011920929\n",
      "Iteration 64470 Training loss 0.08257654309272766 Validation loss 0.09586968272924423 Accuracy 0.7195000052452087\n",
      "Iteration 64480 Training loss 0.07804296165704727 Validation loss 0.09164492785930634 Accuracy 0.7380000352859497\n",
      "Iteration 64490 Training loss 0.09434932470321655 Validation loss 0.09685135632753372 Accuracy 0.7170000076293945\n",
      "Iteration 64500 Training loss 0.0898871198296547 Validation loss 0.09036720544099808 Accuracy 0.7385000586509705\n",
      "Iteration 64510 Training loss 0.08255908638238907 Validation loss 0.08766131848096848 Accuracy 0.7510000467300415\n",
      "Iteration 64520 Training loss 0.08460109680891037 Validation loss 0.1020105704665184 Accuracy 0.7020000219345093\n",
      "Iteration 64530 Training loss 0.07525191456079483 Validation loss 0.09070700407028198 Accuracy 0.737000048160553\n",
      "Iteration 64540 Training loss 0.08984974771738052 Validation loss 0.09177084267139435 Accuracy 0.7385000586509705\n",
      "Iteration 64550 Training loss 0.09097643941640854 Validation loss 0.09069933742284775 Accuracy 0.733500063419342\n",
      "Iteration 64560 Training loss 0.0827682837843895 Validation loss 0.09151053428649902 Accuracy 0.7385000586509705\n",
      "Iteration 64570 Training loss 0.07413306087255478 Validation loss 0.09275785088539124 Accuracy 0.7285000085830688\n",
      "Iteration 64580 Training loss 0.06884932518005371 Validation loss 0.08971265703439713 Accuracy 0.7395000457763672\n",
      "Iteration 64590 Training loss 0.12512454390525818 Validation loss 0.12267640233039856 Accuracy 0.6385000348091125\n",
      "Iteration 64600 Training loss 0.09583620727062225 Validation loss 0.0953226163983345 Accuracy 0.7160000205039978\n",
      "Iteration 64610 Training loss 0.09615997225046158 Validation loss 0.09413043409585953 Accuracy 0.7165000438690186\n",
      "Iteration 64620 Training loss 0.08350934833288193 Validation loss 0.09842702001333237 Accuracy 0.7000000476837158\n",
      "Iteration 64630 Training loss 0.11308135092258453 Validation loss 0.10719849914312363 Accuracy 0.6630000472068787\n",
      "Iteration 64640 Training loss 0.08612164109945297 Validation loss 0.09560712426900864 Accuracy 0.7220000624656677\n",
      "Iteration 64650 Training loss 0.07333937287330627 Validation loss 0.08814001828432083 Accuracy 0.7450000643730164\n",
      "Iteration 64660 Training loss 0.09554007649421692 Validation loss 0.0949256494641304 Accuracy 0.7105000615119934\n",
      "Iteration 64670 Training loss 0.08037469536066055 Validation loss 0.09198874980211258 Accuracy 0.7310000061988831\n",
      "Iteration 64680 Training loss 0.11237812042236328 Validation loss 0.09648963809013367 Accuracy 0.7205000519752502\n",
      "Iteration 64690 Training loss 0.09079370647668839 Validation loss 0.10258592665195465 Accuracy 0.6895000338554382\n",
      "Iteration 64700 Training loss 0.11108240485191345 Validation loss 0.11197037249803543 Accuracy 0.6615000367164612\n",
      "Iteration 64710 Training loss 0.07428033649921417 Validation loss 0.09849800169467926 Accuracy 0.7080000042915344\n",
      "Iteration 64720 Training loss 0.08894247561693192 Validation loss 0.09363850951194763 Accuracy 0.7275000214576721\n",
      "Iteration 64730 Training loss 0.11807869374752045 Validation loss 0.098872609436512 Accuracy 0.7075000405311584\n",
      "Iteration 64740 Training loss 0.08078388124704361 Validation loss 0.09167607873678207 Accuracy 0.7360000610351562\n",
      "Iteration 64750 Training loss 0.09767389297485352 Validation loss 0.09828206151723862 Accuracy 0.7055000066757202\n",
      "Iteration 64760 Training loss 0.08346996456384659 Validation loss 0.1032402515411377 Accuracy 0.6980000138282776\n",
      "Iteration 64770 Training loss 0.08991828560829163 Validation loss 0.09746053814888 Accuracy 0.7000000476837158\n",
      "Iteration 64780 Training loss 0.09427833557128906 Validation loss 0.0923411175608635 Accuracy 0.7285000085830688\n",
      "Iteration 64790 Training loss 0.09430377930402756 Validation loss 0.09697207808494568 Accuracy 0.721500039100647\n",
      "Iteration 64800 Training loss 0.1006857380270958 Validation loss 0.09692556411027908 Accuracy 0.7095000147819519\n",
      "Iteration 64810 Training loss 0.08021709322929382 Validation loss 0.09550830721855164 Accuracy 0.7120000123977661\n",
      "Iteration 64820 Training loss 0.10137668997049332 Validation loss 0.09355707466602325 Accuracy 0.7225000262260437\n",
      "Iteration 64830 Training loss 0.07134275138378143 Validation loss 0.09401924908161163 Accuracy 0.7265000343322754\n",
      "Iteration 64840 Training loss 0.07901018112897873 Validation loss 0.091319240629673 Accuracy 0.733500063419342\n",
      "Iteration 64850 Training loss 0.08557331562042236 Validation loss 0.09493213891983032 Accuracy 0.7320000529289246\n",
      "Iteration 64860 Training loss 0.08965346217155457 Validation loss 0.09815213829278946 Accuracy 0.7155000567436218\n",
      "Iteration 64870 Training loss 0.09309188276529312 Validation loss 0.09184092283248901 Accuracy 0.7305000424385071\n",
      "Iteration 64880 Training loss 0.09343059360980988 Validation loss 0.08965529501438141 Accuracy 0.737500011920929\n",
      "Iteration 64890 Training loss 0.0873933956027031 Validation loss 0.10364891588687897 Accuracy 0.6860000491142273\n",
      "Iteration 64900 Training loss 0.09792628884315491 Validation loss 0.09070254862308502 Accuracy 0.7360000610351562\n",
      "Iteration 64910 Training loss 0.09274940192699432 Validation loss 0.08945892751216888 Accuracy 0.7380000352859497\n",
      "Iteration 64920 Training loss 0.09449537843465805 Validation loss 0.0925518050789833 Accuracy 0.733500063419342\n",
      "Iteration 64930 Training loss 0.07669542729854584 Validation loss 0.08977445214986801 Accuracy 0.7310000061988831\n",
      "Iteration 64940 Training loss 0.07105258107185364 Validation loss 0.08976297825574875 Accuracy 0.7360000610351562\n",
      "Iteration 64950 Training loss 0.108612060546875 Validation loss 0.10100583732128143 Accuracy 0.6995000243186951\n",
      "Iteration 64960 Training loss 0.10993348807096481 Validation loss 0.11998460441827774 Accuracy 0.5980000495910645\n",
      "Iteration 64970 Training loss 0.1037040650844574 Validation loss 0.14625097811222076 Accuracy 0.6220000386238098\n",
      "Iteration 64980 Training loss 0.10581216216087341 Validation loss 0.09737303853034973 Accuracy 0.7145000100135803\n",
      "Iteration 64990 Training loss 0.11061662435531616 Validation loss 0.10435134917497635 Accuracy 0.6850000619888306\n",
      "Iteration 65000 Training loss 0.09030257165431976 Validation loss 0.09688052535057068 Accuracy 0.7160000205039978\n",
      "Iteration 65010 Training loss 0.09556590765714645 Validation loss 0.10507205873727798 Accuracy 0.6815000176429749\n",
      "Iteration 65020 Training loss 0.09003540128469467 Validation loss 0.09572382271289825 Accuracy 0.7120000123977661\n",
      "Iteration 65030 Training loss 0.07575169205665588 Validation loss 0.09474316984415054 Accuracy 0.7250000238418579\n",
      "Iteration 65040 Training loss 0.08432354032993317 Validation loss 0.09193065017461777 Accuracy 0.7395000457763672\n",
      "Iteration 65050 Training loss 0.10649479925632477 Validation loss 0.1023833379149437 Accuracy 0.6970000267028809\n",
      "Iteration 65060 Training loss 0.09785063564777374 Validation loss 0.09420952200889587 Accuracy 0.7245000600814819\n",
      "Iteration 65070 Training loss 0.1034175455570221 Validation loss 0.09959854930639267 Accuracy 0.6980000138282776\n",
      "Iteration 65080 Training loss 0.10055951029062271 Validation loss 0.09327145665884018 Accuracy 0.7365000247955322\n",
      "Iteration 65090 Training loss 0.07009223848581314 Validation loss 0.10460751503705978 Accuracy 0.6730000376701355\n",
      "Iteration 65100 Training loss 0.0880126804113388 Validation loss 0.09148074686527252 Accuracy 0.7400000095367432\n",
      "Iteration 65110 Training loss 0.05784688889980316 Validation loss 0.08770894259214401 Accuracy 0.7445000410079956\n",
      "Iteration 65120 Training loss 0.082550048828125 Validation loss 0.09063855558633804 Accuracy 0.7440000176429749\n",
      "Iteration 65130 Training loss 0.09333501756191254 Validation loss 0.09349673241376877 Accuracy 0.737500011920929\n",
      "Iteration 65140 Training loss 0.09777259826660156 Validation loss 0.09567222744226456 Accuracy 0.7160000205039978\n",
      "Iteration 65150 Training loss 0.07922150194644928 Validation loss 0.09054411947727203 Accuracy 0.7365000247955322\n",
      "Iteration 65160 Training loss 0.10225947946310043 Validation loss 0.1084088459610939 Accuracy 0.6610000133514404\n",
      "Iteration 65170 Training loss 0.07121554762125015 Validation loss 0.08917412906885147 Accuracy 0.7490000128746033\n",
      "Iteration 65180 Training loss 0.07936502993106842 Validation loss 0.08978145569562912 Accuracy 0.7305000424385071\n",
      "Iteration 65190 Training loss 0.07935348153114319 Validation loss 0.0889827087521553 Accuracy 0.7455000281333923\n",
      "Iteration 65200 Training loss 0.09740125387907028 Validation loss 0.09424775838851929 Accuracy 0.7210000157356262\n",
      "Iteration 65210 Training loss 0.08385848999023438 Validation loss 0.09280862659215927 Accuracy 0.7260000109672546\n",
      "Iteration 65220 Training loss 0.09009692817926407 Validation loss 0.0922337993979454 Accuracy 0.7380000352859497\n",
      "Iteration 65230 Training loss 0.10092515498399734 Validation loss 0.09660036861896515 Accuracy 0.7145000100135803\n",
      "Iteration 65240 Training loss 0.09176688641309738 Validation loss 0.10008420050144196 Accuracy 0.6935000419616699\n",
      "Iteration 65250 Training loss 0.0730675458908081 Validation loss 0.08919302374124527 Accuracy 0.7485000491142273\n",
      "Iteration 65260 Training loss 0.0717746689915657 Validation loss 0.09277965128421783 Accuracy 0.734000027179718\n",
      "Iteration 65270 Training loss 0.09864091873168945 Validation loss 0.09239864349365234 Accuracy 0.7385000586509705\n",
      "Iteration 65280 Training loss 0.11733099818229675 Validation loss 0.10003510117530823 Accuracy 0.6855000257492065\n",
      "Iteration 65290 Training loss 0.09827975928783417 Validation loss 0.09485958516597748 Accuracy 0.7130000591278076\n",
      "Iteration 65300 Training loss 0.07095535844564438 Validation loss 0.09087900072336197 Accuracy 0.7220000624656677\n",
      "Iteration 65310 Training loss 0.07943139225244522 Validation loss 0.08924701064825058 Accuracy 0.7395000457763672\n",
      "Iteration 65320 Training loss 0.09085346758365631 Validation loss 0.09217328578233719 Accuracy 0.7345000505447388\n",
      "Iteration 65330 Training loss 0.0787377804517746 Validation loss 0.09318388998508453 Accuracy 0.7425000071525574\n",
      "Iteration 65340 Training loss 0.07933352887630463 Validation loss 0.0917428731918335 Accuracy 0.7210000157356262\n",
      "Iteration 65350 Training loss 0.09776109457015991 Validation loss 0.09582124650478363 Accuracy 0.7190000414848328\n",
      "Iteration 65360 Training loss 0.07697675377130508 Validation loss 0.091158926486969 Accuracy 0.7240000367164612\n",
      "Iteration 65370 Training loss 0.0958503857254982 Validation loss 0.0936460867524147 Accuracy 0.7310000061988831\n",
      "Iteration 65380 Training loss 0.0901486724615097 Validation loss 0.09074762463569641 Accuracy 0.7440000176429749\n",
      "Iteration 65390 Training loss 0.07719892263412476 Validation loss 0.09129834920167923 Accuracy 0.7380000352859497\n",
      "Iteration 65400 Training loss 0.08810392022132874 Validation loss 0.09274698048830032 Accuracy 0.7195000052452087\n",
      "Iteration 65410 Training loss 0.07691846787929535 Validation loss 0.08730065822601318 Accuracy 0.7500000596046448\n",
      "Iteration 65420 Training loss 0.085802361369133 Validation loss 0.09248042106628418 Accuracy 0.7310000061988831\n",
      "Iteration 65430 Training loss 0.08434892445802689 Validation loss 0.09101424366235733 Accuracy 0.7270000576972961\n",
      "Iteration 65440 Training loss 0.08246204257011414 Validation loss 0.09134399890899658 Accuracy 0.7295000553131104\n",
      "Iteration 65450 Training loss 0.08629197627305984 Validation loss 0.09236513078212738 Accuracy 0.7220000624656677\n",
      "Iteration 65460 Training loss 0.11530845612287521 Validation loss 0.1039966568350792 Accuracy 0.6885000467300415\n",
      "Iteration 65470 Training loss 0.08753835409879684 Validation loss 0.0969998836517334 Accuracy 0.7010000348091125\n",
      "Iteration 65480 Training loss 0.09580381959676743 Validation loss 0.08999016135931015 Accuracy 0.733500063419342\n",
      "Iteration 65490 Training loss 0.08712805807590485 Validation loss 0.08851001411676407 Accuracy 0.7470000386238098\n",
      "Iteration 65500 Training loss 0.09198679029941559 Validation loss 0.099749855697155 Accuracy 0.6940000057220459\n",
      "Iteration 65510 Training loss 0.07700604200363159 Validation loss 0.08889003843069077 Accuracy 0.7435000538825989\n",
      "Iteration 65520 Training loss 0.09072709083557129 Validation loss 0.09985879808664322 Accuracy 0.7055000066757202\n",
      "Iteration 65530 Training loss 0.08333922922611237 Validation loss 0.08935670554637909 Accuracy 0.7390000224113464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training done'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model_3_layer.train_layers(x_train, y_train, x_valid, y_valid, 1.6, 1e-2, 5e3, 0.1, 0.1, 0.1, 1, 0.01, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "347eb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3_layer_1_untrained = binary_classification_three_layer_NN(3072, 2048, 2048, eps_init = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3149bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa = 1.5, the number of datas used for the training is 17026752.25872509 and the number of iterations is 42566.\n",
      "Iteration 0 Training loss 0.12500081956386566 Validation loss 0.1250009983778 Accuracy 0.5\n",
      "Iteration 10 Training loss 0.12426601350307465 Validation loss 0.12450466305017471 Accuracy 0.5\n",
      "Iteration 20 Training loss 0.12398973107337952 Validation loss 0.12418626993894577 Accuracy 0.5\n",
      "Iteration 30 Training loss 0.1239936351776123 Validation loss 0.12389950454235077 Accuracy 0.5003750324249268\n",
      "Iteration 40 Training loss 0.12299794703722 Validation loss 0.12358492612838745 Accuracy 0.5003750324249268\n",
      "Iteration 50 Training loss 0.1238676980137825 Validation loss 0.12329433858394623 Accuracy 0.5015000104904175\n",
      "Iteration 60 Training loss 0.12382584065198898 Validation loss 0.12299849838018417 Accuracy 0.5056250095367432\n",
      "Iteration 70 Training loss 0.12160582095384598 Validation loss 0.12270954251289368 Accuracy 0.5121250152587891\n",
      "Iteration 80 Training loss 0.12226653844118118 Validation loss 0.12241272628307343 Accuracy 0.5383750200271606\n",
      "Iteration 90 Training loss 0.12233316153287888 Validation loss 0.12208987772464752 Accuracy 0.530750036239624\n",
      "Iteration 100 Training loss 0.12150275707244873 Validation loss 0.12177664786577225 Accuracy 0.5545000433921814\n",
      "Iteration 110 Training loss 0.12068451941013336 Validation loss 0.12143169343471527 Accuracy 0.5478750467300415\n",
      "Iteration 120 Training loss 0.12114730477333069 Validation loss 0.12109870463609695 Accuracy 0.5532500147819519\n",
      "Iteration 130 Training loss 0.12187673151493073 Validation loss 0.12075270712375641 Accuracy 0.5758750438690186\n",
      "Iteration 140 Training loss 0.12150336802005768 Validation loss 0.1203947365283966 Accuracy 0.5901250243186951\n",
      "Iteration 150 Training loss 0.11914005130529404 Validation loss 0.12002328038215637 Accuracy 0.6076250076293945\n",
      "Iteration 160 Training loss 0.11922638863325119 Validation loss 0.11964116990566254 Accuracy 0.6163750290870667\n",
      "Iteration 170 Training loss 0.11833209544420242 Validation loss 0.11923659592866898 Accuracy 0.643125057220459\n",
      "Iteration 180 Training loss 0.11932384222745895 Validation loss 0.11882791668176651 Accuracy 0.6541250348091125\n",
      "Iteration 190 Training loss 0.11824792623519897 Validation loss 0.1184067577123642 Accuracy 0.6630000472068787\n",
      "Iteration 200 Training loss 0.1171611025929451 Validation loss 0.11796920001506805 Accuracy 0.6808750033378601\n",
      "Iteration 210 Training loss 0.11763301491737366 Validation loss 0.11749573051929474 Accuracy 0.6797500252723694\n",
      "Iteration 220 Training loss 0.1177871897816658 Validation loss 0.11701357364654541 Accuracy 0.6805000305175781\n",
      "Iteration 230 Training loss 0.1171034574508667 Validation loss 0.11656138300895691 Accuracy 0.6876250505447388\n",
      "Iteration 240 Training loss 0.11567439883947372 Validation loss 0.11603683978319168 Accuracy 0.6953750252723694\n",
      "Iteration 250 Training loss 0.11566507071256638 Validation loss 0.11552048474550247 Accuracy 0.6852500438690186\n",
      "Iteration 260 Training loss 0.11386704444885254 Validation loss 0.11498410999774933 Accuracy 0.6816250085830688\n",
      "Iteration 270 Training loss 0.11195962876081467 Validation loss 0.11439892649650574 Accuracy 0.6955000162124634\n",
      "Iteration 280 Training loss 0.11295853555202484 Validation loss 0.11378650367259979 Accuracy 0.7035000324249268\n",
      "Iteration 290 Training loss 0.11095702648162842 Validation loss 0.11316952109336853 Accuracy 0.7051250338554382\n",
      "Iteration 300 Training loss 0.11122428625822067 Validation loss 0.11253130435943604 Accuracy 0.7175000309944153\n",
      "Iteration 310 Training loss 0.11130460351705551 Validation loss 0.1119128167629242 Accuracy 0.7251250147819519\n",
      "Iteration 320 Training loss 0.10985273122787476 Validation loss 0.11126255989074707 Accuracy 0.721875011920929\n",
      "Iteration 330 Training loss 0.11197946965694427 Validation loss 0.11063212901353836 Accuracy 0.7240000367164612\n",
      "Iteration 340 Training loss 0.11040907353162766 Validation loss 0.10995689779520035 Accuracy 0.7357500195503235\n",
      "Iteration 350 Training loss 0.10559430718421936 Validation loss 0.109294094145298 Accuracy 0.7305000424385071\n",
      "Iteration 360 Training loss 0.10954295843839645 Validation loss 0.10865043848752975 Accuracy 0.7255000472068787\n",
      "Iteration 370 Training loss 0.1082962229847908 Validation loss 0.1079115942120552 Accuracy 0.7361250519752502\n",
      "Iteration 380 Training loss 0.10929186642169952 Validation loss 0.10722336918115616 Accuracy 0.7378750443458557\n",
      "Iteration 390 Training loss 0.10715131461620331 Validation loss 0.10651329904794693 Accuracy 0.7392500638961792\n",
      "Iteration 400 Training loss 0.10558973252773285 Validation loss 0.10580719262361526 Accuracy 0.7402500510215759\n",
      "Iteration 410 Training loss 0.10467255115509033 Validation loss 0.10512010008096695 Accuracy 0.7395000457763672\n",
      "Iteration 420 Training loss 0.10309846699237823 Validation loss 0.10440416634082794 Accuracy 0.7416250109672546\n",
      "Iteration 430 Training loss 0.10294951498508453 Validation loss 0.10370064526796341 Accuracy 0.7435000538825989\n",
      "Iteration 440 Training loss 0.10257615149021149 Validation loss 0.103031225502491 Accuracy 0.7450000643730164\n",
      "Iteration 450 Training loss 0.1024819165468216 Validation loss 0.10238690674304962 Accuracy 0.7455000281333923\n",
      "Iteration 460 Training loss 0.10018076002597809 Validation loss 0.10169459879398346 Accuracy 0.7457500100135803\n",
      "Iteration 470 Training loss 0.09848402440547943 Validation loss 0.10101952403783798 Accuracy 0.7457500100135803\n",
      "Iteration 480 Training loss 0.10178877413272858 Validation loss 0.10037302225828171 Accuracy 0.7453750371932983\n",
      "Iteration 490 Training loss 0.09751712530851364 Validation loss 0.09975741803646088 Accuracy 0.7455000281333923\n",
      "Iteration 500 Training loss 0.10108835995197296 Validation loss 0.09909859299659729 Accuracy 0.7473750114440918\n",
      "Iteration 510 Training loss 0.09647805988788605 Validation loss 0.09849175810813904 Accuracy 0.7472500205039978\n",
      "Iteration 520 Training loss 0.09702835232019424 Validation loss 0.09790407866239548 Accuracy 0.7490000128746033\n",
      "Iteration 530 Training loss 0.09752210229635239 Validation loss 0.097310870885849 Accuracy 0.7486250400543213\n",
      "Iteration 540 Training loss 0.09715737402439117 Validation loss 0.0967499241232872 Accuracy 0.7487500309944153\n",
      "Iteration 550 Training loss 0.09880515187978745 Validation loss 0.09619378298521042 Accuracy 0.7485000491142273\n",
      "Iteration 560 Training loss 0.09609565883874893 Validation loss 0.09567249566316605 Accuracy 0.749125063419342\n",
      "Iteration 570 Training loss 0.09655209630727768 Validation loss 0.09515262395143509 Accuracy 0.749750018119812\n",
      "Iteration 580 Training loss 0.0950784683227539 Validation loss 0.09468814730644226 Accuracy 0.7508750557899475\n",
      "Iteration 590 Training loss 0.09321288019418716 Validation loss 0.0942227765917778 Accuracy 0.7503750324249268\n",
      "Iteration 600 Training loss 0.09369157254695892 Validation loss 0.09377056360244751 Accuracy 0.7535000443458557\n",
      "Iteration 610 Training loss 0.10002002120018005 Validation loss 0.09334088861942291 Accuracy 0.7532500624656677\n",
      "Iteration 620 Training loss 0.09706538915634155 Validation loss 0.0928640365600586 Accuracy 0.752875030040741\n",
      "Iteration 630 Training loss 0.0896449089050293 Validation loss 0.09247201681137085 Accuracy 0.7538750171661377\n",
      "Iteration 640 Training loss 0.08701477199792862 Validation loss 0.09204709529876709 Accuracy 0.7533750534057617\n",
      "Iteration 650 Training loss 0.0869930163025856 Validation loss 0.0916605293750763 Accuracy 0.7541250586509705\n",
      "Iteration 660 Training loss 0.09268945455551147 Validation loss 0.09128878265619278 Accuracy 0.7548750638961792\n",
      "Iteration 670 Training loss 0.09121432900428772 Validation loss 0.09091450273990631 Accuracy 0.7562500238418579\n",
      "Iteration 680 Training loss 0.09003812819719315 Validation loss 0.09053632616996765 Accuracy 0.7557500600814819\n",
      "Iteration 690 Training loss 0.0978630781173706 Validation loss 0.09019993990659714 Accuracy 0.7561250329017639\n",
      "Iteration 700 Training loss 0.08488410711288452 Validation loss 0.08983557671308517 Accuracy 0.7571250200271606\n",
      "Iteration 710 Training loss 0.08829464018344879 Validation loss 0.0894998162984848 Accuracy 0.7576250433921814\n",
      "Iteration 720 Training loss 0.0869097113609314 Validation loss 0.089225172996521 Accuracy 0.7572500109672546\n",
      "Iteration 730 Training loss 0.09241233021020889 Validation loss 0.08887903392314911 Accuracy 0.7592500448226929\n",
      "Iteration 740 Training loss 0.08989213407039642 Validation loss 0.08862049132585526 Accuracy 0.7592500448226929\n",
      "Iteration 750 Training loss 0.08999384939670563 Validation loss 0.08833251148462296 Accuracy 0.7597500085830688\n",
      "Iteration 760 Training loss 0.09232745319604874 Validation loss 0.08807039260864258 Accuracy 0.7608750462532043\n",
      "Iteration 770 Training loss 0.08918840438127518 Validation loss 0.08774840086698532 Accuracy 0.7612500190734863\n",
      "Iteration 780 Training loss 0.08537967503070831 Validation loss 0.0875166729092598 Accuracy 0.7601250410079956\n",
      "Iteration 790 Training loss 0.083948515355587 Validation loss 0.0871991440653801 Accuracy 0.7617500424385071\n",
      "Iteration 800 Training loss 0.08654917776584625 Validation loss 0.08696941286325455 Accuracy 0.7617500424385071\n",
      "Iteration 810 Training loss 0.08588296920061111 Validation loss 0.08670075982809067 Accuracy 0.7632500529289246\n",
      "Iteration 820 Training loss 0.08182067424058914 Validation loss 0.08645156025886536 Accuracy 0.7635000348091125\n",
      "Iteration 830 Training loss 0.0886494591832161 Validation loss 0.08622951060533524 Accuracy 0.7633750438690186\n",
      "Iteration 840 Training loss 0.0856594443321228 Validation loss 0.08598906546831131 Accuracy 0.7645000219345093\n",
      "Iteration 850 Training loss 0.08595116436481476 Validation loss 0.08575630187988281 Accuracy 0.764875054359436\n",
      "Iteration 860 Training loss 0.08998819440603256 Validation loss 0.0856635794043541 Accuracy 0.7636250257492065\n",
      "Iteration 870 Training loss 0.08833852410316467 Validation loss 0.08532962203025818 Accuracy 0.7657500505447388\n",
      "Iteration 880 Training loss 0.08810758590698242 Validation loss 0.08513540029525757 Accuracy 0.7667500376701355\n",
      "Iteration 890 Training loss 0.09075355529785156 Validation loss 0.08493229746818542 Accuracy 0.7662500143051147\n",
      "Iteration 900 Training loss 0.08255898952484131 Validation loss 0.08474907279014587 Accuracy 0.7671250104904175\n",
      "Iteration 910 Training loss 0.08788169920444489 Validation loss 0.08456875383853912 Accuracy 0.7678750157356262\n",
      "Iteration 920 Training loss 0.07703004032373428 Validation loss 0.08435263484716415 Accuracy 0.768375039100647\n",
      "Iteration 930 Training loss 0.0860258936882019 Validation loss 0.08413396030664444 Accuracy 0.768375039100647\n",
      "Iteration 940 Training loss 0.07857412099838257 Validation loss 0.08395860344171524 Accuracy 0.7688750624656677\n",
      "Iteration 950 Training loss 0.08394858986139297 Validation loss 0.08378169685602188 Accuracy 0.768375039100647\n",
      "Iteration 960 Training loss 0.0870414525270462 Validation loss 0.0835854709148407 Accuracy 0.7690000534057617\n",
      "Iteration 970 Training loss 0.08496209979057312 Validation loss 0.08340293914079666 Accuracy 0.7688750624656677\n",
      "Iteration 980 Training loss 0.08516985923051834 Validation loss 0.08323801308870316 Accuracy 0.7688750624656677\n",
      "Iteration 990 Training loss 0.08830976486206055 Validation loss 0.08306573331356049 Accuracy 0.7702500224113464\n",
      "Iteration 1000 Training loss 0.0902993381023407 Validation loss 0.08292009681463242 Accuracy 0.7712500095367432\n",
      "Iteration 1010 Training loss 0.08626513928174973 Validation loss 0.08276107907295227 Accuracy 0.7701250314712524\n",
      "Iteration 1020 Training loss 0.08271272480487823 Validation loss 0.08262576907873154 Accuracy 0.7712500095367432\n",
      "Iteration 1030 Training loss 0.07569755613803864 Validation loss 0.08250627666711807 Accuracy 0.7711250185966492\n",
      "Iteration 1040 Training loss 0.0806470438838005 Validation loss 0.08233862370252609 Accuracy 0.7716250419616699\n",
      "Iteration 1050 Training loss 0.08433479070663452 Validation loss 0.08217282593250275 Accuracy 0.7726250290870667\n",
      "Iteration 1060 Training loss 0.0827551856637001 Validation loss 0.08209371566772461 Accuracy 0.7731250524520874\n",
      "Iteration 1070 Training loss 0.08452484011650085 Validation loss 0.08189354836940765 Accuracy 0.7723750472068787\n",
      "Iteration 1080 Training loss 0.08294137567281723 Validation loss 0.0817965641617775 Accuracy 0.7726250290870667\n",
      "Iteration 1090 Training loss 0.0835752859711647 Validation loss 0.0816134512424469 Accuracy 0.7733750343322754\n",
      "Iteration 1100 Training loss 0.0804288238286972 Validation loss 0.08149406313896179 Accuracy 0.7738750576972961\n",
      "Iteration 1110 Training loss 0.08161729574203491 Validation loss 0.08136319369077682 Accuracy 0.7740000486373901\n",
      "Iteration 1120 Training loss 0.07789608836174011 Validation loss 0.08124019205570221 Accuracy 0.7740000486373901\n",
      "Iteration 1130 Training loss 0.08513131737709045 Validation loss 0.0810912474989891 Accuracy 0.7751250267028809\n",
      "Iteration 1140 Training loss 0.07651800662279129 Validation loss 0.08101620525121689 Accuracy 0.7750000357627869\n",
      "Iteration 1150 Training loss 0.08178745210170746 Validation loss 0.08085707575082779 Accuracy 0.7753750085830688\n",
      "Iteration 1160 Training loss 0.0875249058008194 Validation loss 0.08072426170110703 Accuracy 0.7766250371932983\n",
      "Iteration 1170 Training loss 0.08908329159021378 Validation loss 0.08061575889587402 Accuracy 0.7768750190734863\n",
      "Iteration 1180 Training loss 0.0788378119468689 Validation loss 0.0805370956659317 Accuracy 0.7760000228881836\n",
      "Iteration 1190 Training loss 0.0844629630446434 Validation loss 0.08041328936815262 Accuracy 0.7763750553131104\n",
      "Iteration 1200 Training loss 0.07579416781663895 Validation loss 0.08026114851236343 Accuracy 0.7773750424385071\n",
      "Iteration 1210 Training loss 0.0815722718834877 Validation loss 0.08021536469459534 Accuracy 0.7767500281333923\n",
      "Iteration 1220 Training loss 0.08250720053911209 Validation loss 0.08006010949611664 Accuracy 0.7777500152587891\n",
      "Iteration 1230 Training loss 0.07611890882253647 Validation loss 0.07994731515645981 Accuracy 0.7776250243186951\n",
      "Iteration 1240 Training loss 0.08220675587654114 Validation loss 0.07987350225448608 Accuracy 0.7788750529289246\n",
      "Iteration 1250 Training loss 0.07933767884969711 Validation loss 0.07974812388420105 Accuracy 0.7790000438690186\n",
      "Iteration 1260 Training loss 0.08456609398126602 Validation loss 0.07965018600225449 Accuracy 0.7791250348091125\n",
      "Iteration 1270 Training loss 0.07414901256561279 Validation loss 0.07953554391860962 Accuracy 0.7796250581741333\n",
      "Iteration 1280 Training loss 0.07269716262817383 Validation loss 0.07946469634771347 Accuracy 0.7797500491142273\n",
      "Iteration 1290 Training loss 0.07197772711515427 Validation loss 0.07934992760419846 Accuracy 0.780500054359436\n",
      "Iteration 1300 Training loss 0.0796135812997818 Validation loss 0.07923373579978943 Accuracy 0.78062504529953\n",
      "Iteration 1310 Training loss 0.08395934849977493 Validation loss 0.0791463553905487 Accuracy 0.7816250324249268\n",
      "Iteration 1320 Training loss 0.08301802724599838 Validation loss 0.07906699180603027 Accuracy 0.7816250324249268\n",
      "Iteration 1330 Training loss 0.07675515115261078 Validation loss 0.07921020686626434 Accuracy 0.7795000076293945\n",
      "Iteration 1340 Training loss 0.07745920121669769 Validation loss 0.07888954877853394 Accuracy 0.7821250557899475\n",
      "Iteration 1350 Training loss 0.08142798393964767 Validation loss 0.07879985123872757 Accuracy 0.7826250195503235\n",
      "Iteration 1360 Training loss 0.07890051603317261 Validation loss 0.07872089743614197 Accuracy 0.7827500104904175\n",
      "Iteration 1370 Training loss 0.07304943352937698 Validation loss 0.07866278290748596 Accuracy 0.7830000519752502\n",
      "Iteration 1380 Training loss 0.07631705701351166 Validation loss 0.07855980098247528 Accuracy 0.7832500338554382\n",
      "Iteration 1390 Training loss 0.06942962855100632 Validation loss 0.07847470790147781 Accuracy 0.784250020980835\n",
      "Iteration 1400 Training loss 0.08236724883317947 Validation loss 0.0784090980887413 Accuracy 0.7832500338554382\n",
      "Iteration 1410 Training loss 0.07567863911390305 Validation loss 0.07834114134311676 Accuracy 0.783625066280365\n",
      "Iteration 1420 Training loss 0.07866965979337692 Validation loss 0.07826743274927139 Accuracy 0.783625066280365\n",
      "Iteration 1430 Training loss 0.07591080665588379 Validation loss 0.0781686007976532 Accuracy 0.784250020980835\n",
      "Iteration 1440 Training loss 0.07879028469324112 Validation loss 0.07806336134672165 Accuracy 0.7860000133514404\n",
      "Iteration 1450 Training loss 0.08202651143074036 Validation loss 0.07808768004179001 Accuracy 0.784000039100647\n",
      "Iteration 1460 Training loss 0.08068237453699112 Validation loss 0.07797626405954361 Accuracy 0.7853750586509705\n",
      "Iteration 1470 Training loss 0.080562524497509 Validation loss 0.07784796506166458 Accuracy 0.7863750457763672\n",
      "Iteration 1480 Training loss 0.07605735957622528 Validation loss 0.0777961015701294 Accuracy 0.7853750586509705\n",
      "Iteration 1490 Training loss 0.07448433339595795 Validation loss 0.07785169780254364 Accuracy 0.784375011920929\n",
      "Iteration 1500 Training loss 0.07990968227386475 Validation loss 0.07765179127454758 Accuracy 0.7855000495910645\n",
      "Iteration 1510 Training loss 0.0838869959115982 Validation loss 0.07757455855607986 Accuracy 0.7866250276565552\n",
      "Iteration 1520 Training loss 0.07133245468139648 Validation loss 0.07752121984958649 Accuracy 0.7860000133514404\n",
      "Iteration 1530 Training loss 0.07873836159706116 Validation loss 0.07742434740066528 Accuracy 0.7871250510215759\n",
      "Iteration 1540 Training loss 0.08019859343767166 Validation loss 0.07738204300403595 Accuracy 0.7861250638961792\n",
      "Iteration 1550 Training loss 0.07601220160722733 Validation loss 0.07729317992925644 Accuracy 0.7870000600814819\n",
      "Iteration 1560 Training loss 0.08470995724201202 Validation loss 0.07731877267360687 Accuracy 0.7871250510215759\n",
      "Iteration 1570 Training loss 0.08084079623222351 Validation loss 0.07716485112905502 Accuracy 0.7878750562667847\n",
      "Iteration 1580 Training loss 0.06936134397983551 Validation loss 0.07710935920476913 Accuracy 0.7873750329017639\n",
      "Iteration 1590 Training loss 0.07760916650295258 Validation loss 0.07705733925104141 Accuracy 0.7875000238418579\n",
      "Iteration 1600 Training loss 0.08148814737796783 Validation loss 0.07698780298233032 Accuracy 0.7875000238418579\n",
      "Iteration 1610 Training loss 0.07906291633844376 Validation loss 0.07693081349134445 Accuracy 0.7880000472068787\n",
      "Iteration 1620 Training loss 0.07367800176143646 Validation loss 0.07686685770750046 Accuracy 0.7876250147819519\n",
      "Iteration 1630 Training loss 0.07657156139612198 Validation loss 0.07684598118066788 Accuracy 0.7883750200271606\n",
      "Iteration 1640 Training loss 0.0811157375574112 Validation loss 0.07676874846220016 Accuracy 0.7881250381469727\n",
      "Iteration 1650 Training loss 0.07620014995336533 Validation loss 0.07669717073440552 Accuracy 0.7880000472068787\n",
      "Iteration 1660 Training loss 0.07851108908653259 Validation loss 0.07666098326444626 Accuracy 0.7891250252723694\n",
      "Iteration 1670 Training loss 0.07520639151334763 Validation loss 0.0765959694981575 Accuracy 0.7901250123977661\n",
      "Iteration 1680 Training loss 0.07305534183979034 Validation loss 0.07652217894792557 Accuracy 0.7891250252723694\n",
      "Iteration 1690 Training loss 0.07734179496765137 Validation loss 0.07653726637363434 Accuracy 0.7887500524520874\n",
      "Iteration 1700 Training loss 0.07776688039302826 Validation loss 0.07647203654050827 Accuracy 0.7892500162124634\n",
      "Iteration 1710 Training loss 0.07652387768030167 Validation loss 0.0764005184173584 Accuracy 0.7897500395774841\n",
      "Iteration 1720 Training loss 0.08166800439357758 Validation loss 0.07633833587169647 Accuracy 0.7891250252723694\n",
      "Iteration 1730 Training loss 0.06782712042331696 Validation loss 0.07631752640008926 Accuracy 0.7900000214576721\n",
      "Iteration 1740 Training loss 0.07304937392473221 Validation loss 0.0762556940317154 Accuracy 0.7902500629425049\n",
      "Iteration 1750 Training loss 0.07613351941108704 Validation loss 0.07616215199232101 Accuracy 0.7906250357627869\n",
      "Iteration 1760 Training loss 0.07892165333032608 Validation loss 0.0761197954416275 Accuracy 0.7905000448226929\n",
      "Iteration 1770 Training loss 0.07501106709241867 Validation loss 0.07606765627861023 Accuracy 0.7912500500679016\n",
      "Iteration 1780 Training loss 0.06460000574588776 Validation loss 0.07601669430732727 Accuracy 0.7911250591278076\n",
      "Iteration 1790 Training loss 0.07084934413433075 Validation loss 0.07605762034654617 Accuracy 0.7915000319480896\n",
      "Iteration 1800 Training loss 0.07434269040822983 Validation loss 0.0759955644607544 Accuracy 0.7915000319480896\n",
      "Iteration 1810 Training loss 0.07568871974945068 Validation loss 0.07592558860778809 Accuracy 0.7916250228881836\n",
      "Iteration 1820 Training loss 0.07313165813684464 Validation loss 0.07583063095808029 Accuracy 0.7912500500679016\n",
      "Iteration 1830 Training loss 0.07097706943750381 Validation loss 0.07577585428953171 Accuracy 0.7908750176429749\n",
      "Iteration 1840 Training loss 0.07496620714664459 Validation loss 0.07577577978372574 Accuracy 0.7916250228881836\n",
      "Iteration 1850 Training loss 0.07472603023052216 Validation loss 0.07571398466825485 Accuracy 0.7913750410079956\n",
      "Iteration 1860 Training loss 0.08507903665304184 Validation loss 0.07570895552635193 Accuracy 0.7927500605583191\n",
      "Iteration 1870 Training loss 0.0747726783156395 Validation loss 0.07560334354639053 Accuracy 0.7920000553131104\n",
      "Iteration 1880 Training loss 0.06905543804168701 Validation loss 0.0755612850189209 Accuracy 0.7917500138282776\n",
      "Iteration 1890 Training loss 0.07380829751491547 Validation loss 0.07550891488790512 Accuracy 0.7928750514984131\n",
      "Iteration 1900 Training loss 0.07556455582380295 Validation loss 0.07547669112682343 Accuracy 0.7925000190734863\n",
      "Iteration 1910 Training loss 0.07837358862161636 Validation loss 0.07547091692686081 Accuracy 0.7918750643730164\n",
      "Iteration 1920 Training loss 0.06604398787021637 Validation loss 0.07540234923362732 Accuracy 0.7930000424385071\n",
      "Iteration 1930 Training loss 0.0748293474316597 Validation loss 0.0753852128982544 Accuracy 0.7917500138282776\n",
      "Iteration 1940 Training loss 0.08128052204847336 Validation loss 0.07533309608697891 Accuracy 0.7927500605583191\n",
      "Iteration 1950 Training loss 0.07277613133192062 Validation loss 0.07529819011688232 Accuracy 0.7923750281333923\n",
      "Iteration 1960 Training loss 0.0684836208820343 Validation loss 0.07537209987640381 Accuracy 0.7928750514984131\n",
      "Iteration 1970 Training loss 0.07772310078144073 Validation loss 0.07524769753217697 Accuracy 0.7936250567436218\n",
      "Iteration 1980 Training loss 0.07132722437381744 Validation loss 0.07517683506011963 Accuracy 0.7938750386238098\n",
      "Iteration 1990 Training loss 0.07373211532831192 Validation loss 0.07515423744916916 Accuracy 0.7937500476837158\n",
      "Iteration 2000 Training loss 0.06784453988075256 Validation loss 0.07510066777467728 Accuracy 0.7930000424385071\n",
      "Iteration 2010 Training loss 0.08176133036613464 Validation loss 0.07506067305803299 Accuracy 0.7933750152587891\n",
      "Iteration 2020 Training loss 0.07753005623817444 Validation loss 0.07508715987205505 Accuracy 0.7932500243186951\n",
      "Iteration 2030 Training loss 0.07543663680553436 Validation loss 0.07499469071626663 Accuracy 0.7932500243186951\n",
      "Iteration 2040 Training loss 0.0752001702785492 Validation loss 0.07495597004890442 Accuracy 0.7938750386238098\n",
      "Iteration 2050 Training loss 0.0747697651386261 Validation loss 0.07492338120937347 Accuracy 0.7941250205039978\n",
      "Iteration 2060 Training loss 0.0718640387058258 Validation loss 0.07493016868829727 Accuracy 0.7940000295639038\n",
      "Iteration 2070 Training loss 0.076322041451931 Validation loss 0.0748581662774086 Accuracy 0.7937500476837158\n",
      "Iteration 2080 Training loss 0.07322431355714798 Validation loss 0.07482489943504333 Accuracy 0.7952500581741333\n",
      "Iteration 2090 Training loss 0.07038703560829163 Validation loss 0.07478520274162292 Accuracy 0.7945000529289246\n",
      "Iteration 2100 Training loss 0.07230309396982193 Validation loss 0.07480881363153458 Accuracy 0.7947500348091125\n",
      "Iteration 2110 Training loss 0.07040201872587204 Validation loss 0.07473766058683395 Accuracy 0.7946250438690186\n",
      "Iteration 2120 Training loss 0.07287418097257614 Validation loss 0.07471123337745667 Accuracy 0.7947500348091125\n",
      "Iteration 2130 Training loss 0.07190269231796265 Validation loss 0.0746530145406723 Accuracy 0.7951250672340393\n",
      "Iteration 2140 Training loss 0.06641519069671631 Validation loss 0.0747576430439949 Accuracy 0.7948750257492065\n",
      "Iteration 2150 Training loss 0.07893022149801254 Validation loss 0.07469239830970764 Accuracy 0.7948750257492065\n",
      "Iteration 2160 Training loss 0.0824098214507103 Validation loss 0.07456795126199722 Accuracy 0.7946250438690186\n",
      "Iteration 2170 Training loss 0.07512049376964569 Validation loss 0.0745367631316185 Accuracy 0.7951250672340393\n",
      "Iteration 2180 Training loss 0.07608155161142349 Validation loss 0.07454229891300201 Accuracy 0.7941250205039978\n",
      "Iteration 2190 Training loss 0.07665325701236725 Validation loss 0.07448773086071014 Accuracy 0.7945000529289246\n",
      "Iteration 2200 Training loss 0.08410421013832092 Validation loss 0.07443573325872421 Accuracy 0.7956250309944153\n",
      "Iteration 2210 Training loss 0.07432086765766144 Validation loss 0.07439981400966644 Accuracy 0.7958750128746033\n",
      "Iteration 2220 Training loss 0.07341577112674713 Validation loss 0.07440076768398285 Accuracy 0.7951250672340393\n",
      "Iteration 2230 Training loss 0.07147843390703201 Validation loss 0.07439509779214859 Accuracy 0.7955000400543213\n",
      "Iteration 2240 Training loss 0.07534076273441315 Validation loss 0.07439205795526505 Accuracy 0.7950000166893005\n",
      "Iteration 2250 Training loss 0.07949277758598328 Validation loss 0.07429081946611404 Accuracy 0.796000063419342\n",
      "Iteration 2260 Training loss 0.07523565739393234 Validation loss 0.07424736022949219 Accuracy 0.7956250309944153\n",
      "Iteration 2270 Training loss 0.07175001502037048 Validation loss 0.07422690093517303 Accuracy 0.7958750128746033\n",
      "Iteration 2280 Training loss 0.07212706655263901 Validation loss 0.07421012967824936 Accuracy 0.7957500219345093\n",
      "Iteration 2290 Training loss 0.06996498256921768 Validation loss 0.07420067489147186 Accuracy 0.7946250438690186\n",
      "Iteration 2300 Training loss 0.07471217960119247 Validation loss 0.07413876801729202 Accuracy 0.796375036239624\n",
      "Iteration 2310 Training loss 0.06969838589429855 Validation loss 0.0740988478064537 Accuracy 0.79625004529953\n",
      "Iteration 2320 Training loss 0.08004437386989594 Validation loss 0.07411834597587585 Accuracy 0.7957500219345093\n",
      "Iteration 2330 Training loss 0.08049610257148743 Validation loss 0.07405346632003784 Accuracy 0.796000063419342\n",
      "Iteration 2340 Training loss 0.07445331662893295 Validation loss 0.0741269588470459 Accuracy 0.7951250672340393\n",
      "Iteration 2350 Training loss 0.08056192100048065 Validation loss 0.07400095462799072 Accuracy 0.796500027179718\n",
      "Iteration 2360 Training loss 0.07664041221141815 Validation loss 0.07396408170461655 Accuracy 0.796375036239624\n",
      "Iteration 2370 Training loss 0.06798074394464493 Validation loss 0.0739496573805809 Accuracy 0.796625018119812\n",
      "Iteration 2380 Training loss 0.08278919756412506 Validation loss 0.07390126585960388 Accuracy 0.7970000505447388\n",
      "Iteration 2390 Training loss 0.07392948865890503 Validation loss 0.07387613505125046 Accuracy 0.7970000505447388\n",
      "Iteration 2400 Training loss 0.0665614902973175 Validation loss 0.07388699799776077 Accuracy 0.79625004529953\n",
      "Iteration 2410 Training loss 0.0744505226612091 Validation loss 0.07393117249011993 Accuracy 0.7951250672340393\n",
      "Iteration 2420 Training loss 0.07603314518928528 Validation loss 0.07379617542028427 Accuracy 0.7972500324249268\n",
      "Iteration 2430 Training loss 0.08418529480695724 Validation loss 0.07376988232135773 Accuracy 0.7971250414848328\n",
      "Iteration 2440 Training loss 0.06890098750591278 Validation loss 0.07374103367328644 Accuracy 0.7968750596046448\n",
      "Iteration 2450 Training loss 0.07786723226308823 Validation loss 0.07370993494987488 Accuracy 0.796625018119812\n",
      "Iteration 2460 Training loss 0.07351399213075638 Validation loss 0.0738418772816658 Accuracy 0.79625004529953\n",
      "Iteration 2470 Training loss 0.0671362355351448 Validation loss 0.07367382943630219 Accuracy 0.796750009059906\n",
      "Iteration 2480 Training loss 0.07665778696537018 Validation loss 0.07368507236242294 Accuracy 0.796500027179718\n",
      "Iteration 2490 Training loss 0.06935188174247742 Validation loss 0.07361213862895966 Accuracy 0.7972500324249268\n",
      "Iteration 2500 Training loss 0.07708201557397842 Validation loss 0.07358492165803909 Accuracy 0.7976250648498535\n",
      "Iteration 2510 Training loss 0.07594063878059387 Validation loss 0.07357074320316315 Accuracy 0.796500027179718\n",
      "Iteration 2520 Training loss 0.07247944921255112 Validation loss 0.07353894412517548 Accuracy 0.7983750104904175\n",
      "Iteration 2530 Training loss 0.07226338982582092 Validation loss 0.07351043820381165 Accuracy 0.7976250648498535\n",
      "Iteration 2540 Training loss 0.07405360043048859 Validation loss 0.07350537180900574 Accuracy 0.796750009059906\n",
      "Iteration 2550 Training loss 0.06517679244279861 Validation loss 0.07346460223197937 Accuracy 0.7977500557899475\n",
      "Iteration 2560 Training loss 0.07659529894590378 Validation loss 0.07344119250774384 Accuracy 0.7977500557899475\n",
      "Iteration 2570 Training loss 0.07652585953474045 Validation loss 0.07344365119934082 Accuracy 0.7980000376701355\n",
      "Iteration 2580 Training loss 0.07323537021875381 Validation loss 0.07340479642152786 Accuracy 0.7975000143051147\n",
      "Iteration 2590 Training loss 0.07621943950653076 Validation loss 0.07342185080051422 Accuracy 0.7968750596046448\n",
      "Iteration 2600 Training loss 0.06671798974275589 Validation loss 0.07333491742610931 Accuracy 0.7968750596046448\n",
      "Iteration 2610 Training loss 0.08007659018039703 Validation loss 0.07335580885410309 Accuracy 0.7976250648498535\n",
      "Iteration 2620 Training loss 0.0667579248547554 Validation loss 0.07329860329627991 Accuracy 0.7977500557899475\n",
      "Iteration 2630 Training loss 0.07311851531267166 Validation loss 0.07326474785804749 Accuracy 0.7982500195503235\n",
      "Iteration 2640 Training loss 0.073794424533844 Validation loss 0.0732751339673996 Accuracy 0.7973750233650208\n",
      "Iteration 2650 Training loss 0.08104858547449112 Validation loss 0.07328470051288605 Accuracy 0.7976250648498535\n",
      "Iteration 2660 Training loss 0.06798277050256729 Validation loss 0.07321249693632126 Accuracy 0.7980000376701355\n",
      "Iteration 2670 Training loss 0.07401538640260696 Validation loss 0.07318928837776184 Accuracy 0.7978750467300415\n",
      "Iteration 2680 Training loss 0.06321489065885544 Validation loss 0.07317650318145752 Accuracy 0.7978750467300415\n",
      "Iteration 2690 Training loss 0.0681290552020073 Validation loss 0.07315851002931595 Accuracy 0.7986250519752502\n",
      "Iteration 2700 Training loss 0.07777782529592514 Validation loss 0.07314177602529526 Accuracy 0.7987500429153442\n",
      "Iteration 2710 Training loss 0.0777454823255539 Validation loss 0.07310072332620621 Accuracy 0.799250066280365\n",
      "Iteration 2720 Training loss 0.07757590711116791 Validation loss 0.07307213544845581 Accuracy 0.799375057220459\n",
      "Iteration 2730 Training loss 0.06805232167243958 Validation loss 0.07302254438400269 Accuracy 0.7981250286102295\n",
      "Iteration 2740 Training loss 0.06987344473600388 Validation loss 0.07314115017652512 Accuracy 0.7977500557899475\n",
      "Iteration 2750 Training loss 0.07721387594938278 Validation loss 0.07318083196878433 Accuracy 0.796750009059906\n",
      "Iteration 2760 Training loss 0.0830935463309288 Validation loss 0.07295485585927963 Accuracy 0.7981250286102295\n",
      "Iteration 2770 Training loss 0.07425507158041 Validation loss 0.07293600589036942 Accuracy 0.7986250519752502\n",
      "Iteration 2780 Training loss 0.07418768107891083 Validation loss 0.0729382336139679 Accuracy 0.7987500429153442\n",
      "Iteration 2790 Training loss 0.08010537922382355 Validation loss 0.07295316457748413 Accuracy 0.7991250157356262\n",
      "Iteration 2800 Training loss 0.06773335486650467 Validation loss 0.07287026196718216 Accuracy 0.7980000376701355\n",
      "Iteration 2810 Training loss 0.07761643081903458 Validation loss 0.07284707576036453 Accuracy 0.7985000610351562\n",
      "Iteration 2820 Training loss 0.0810185968875885 Validation loss 0.0728253573179245 Accuracy 0.7982500195503235\n",
      "Iteration 2830 Training loss 0.07200945913791656 Validation loss 0.07294786721467972 Accuracy 0.7983750104904175\n",
      "Iteration 2840 Training loss 0.07330203056335449 Validation loss 0.07284219563007355 Accuracy 0.7987500429153442\n",
      "Iteration 2850 Training loss 0.07288617640733719 Validation loss 0.07290605455636978 Accuracy 0.7983750104904175\n",
      "Iteration 2860 Training loss 0.07183211296796799 Validation loss 0.07273608446121216 Accuracy 0.7988750338554382\n",
      "Iteration 2870 Training loss 0.07036435604095459 Validation loss 0.07275611907243729 Accuracy 0.7983750104904175\n",
      "Iteration 2880 Training loss 0.0693356916308403 Validation loss 0.07275790721178055 Accuracy 0.799375057220459\n",
      "Iteration 2890 Training loss 0.06160421296954155 Validation loss 0.07277558743953705 Accuracy 0.7982500195503235\n",
      "Iteration 2900 Training loss 0.07903008908033371 Validation loss 0.07265381515026093 Accuracy 0.7988750338554382\n",
      "Iteration 2910 Training loss 0.07936158776283264 Validation loss 0.07266044616699219 Accuracy 0.7990000247955322\n",
      "Iteration 2920 Training loss 0.07377505302429199 Validation loss 0.07260335236787796 Accuracy 0.799250066280365\n",
      "Iteration 2930 Training loss 0.07184424251317978 Validation loss 0.0725816935300827 Accuracy 0.799750030040741\n",
      "Iteration 2940 Training loss 0.06748507171869278 Validation loss 0.07257507741451263 Accuracy 0.799625039100647\n",
      "Iteration 2950 Training loss 0.07010187208652496 Validation loss 0.07254014909267426 Accuracy 0.799750030040741\n",
      "Iteration 2960 Training loss 0.0748240277171135 Validation loss 0.07260973751544952 Accuracy 0.7988750338554382\n",
      "Iteration 2970 Training loss 0.08080741763114929 Validation loss 0.07251520454883575 Accuracy 0.799500048160553\n",
      "Iteration 2980 Training loss 0.06905092298984528 Validation loss 0.07249002158641815 Accuracy 0.800000011920929\n",
      "Iteration 2990 Training loss 0.06816404312849045 Validation loss 0.07245989143848419 Accuracy 0.8008750677108765\n",
      "Iteration 3000 Training loss 0.06646289676427841 Validation loss 0.07271667569875717 Accuracy 0.7976250648498535\n",
      "Iteration 3010 Training loss 0.0635305643081665 Validation loss 0.07241691648960114 Accuracy 0.8003750443458557\n",
      "Iteration 3020 Training loss 0.07410811632871628 Validation loss 0.07239628583192825 Accuracy 0.8003750443458557\n",
      "Iteration 3030 Training loss 0.07020808011293411 Validation loss 0.07245334982872009 Accuracy 0.799625039100647\n",
      "Iteration 3040 Training loss 0.06367260217666626 Validation loss 0.07235564291477203 Accuracy 0.8010000586509705\n",
      "Iteration 3050 Training loss 0.06599414348602295 Validation loss 0.07234388589859009 Accuracy 0.799875020980835\n",
      "Iteration 3060 Training loss 0.07245224714279175 Validation loss 0.07241240888834 Accuracy 0.8002500534057617\n",
      "Iteration 3070 Training loss 0.06819896399974823 Validation loss 0.0723203495144844 Accuracy 0.799750030040741\n",
      "Iteration 3080 Training loss 0.07162871211767197 Validation loss 0.07227475196123123 Accuracy 0.8011250495910645\n",
      "Iteration 3090 Training loss 0.06790843605995178 Validation loss 0.07231926172971725 Accuracy 0.799875020980835\n",
      "Iteration 3100 Training loss 0.07166637480258942 Validation loss 0.07223203033208847 Accuracy 0.8010000586509705\n",
      "Iteration 3110 Training loss 0.08292218297719955 Validation loss 0.0724823847413063 Accuracy 0.7977500557899475\n",
      "Iteration 3120 Training loss 0.06740809231996536 Validation loss 0.07224304974079132 Accuracy 0.8002500534057617\n",
      "Iteration 3130 Training loss 0.07655785232782364 Validation loss 0.07217229157686234 Accuracy 0.8012500405311584\n",
      "Iteration 3140 Training loss 0.06691630184650421 Validation loss 0.07216601818799973 Accuracy 0.8003750443458557\n",
      "Iteration 3150 Training loss 0.06835442036390305 Validation loss 0.07213626801967621 Accuracy 0.8015000224113464\n",
      "Iteration 3160 Training loss 0.06792858242988586 Validation loss 0.0721987932920456 Accuracy 0.8007500171661377\n",
      "Iteration 3170 Training loss 0.06714006513357162 Validation loss 0.07218316942453384 Accuracy 0.8010000586509705\n",
      "Iteration 3180 Training loss 0.07053600251674652 Validation loss 0.07214803248643875 Accuracy 0.8012500405311584\n",
      "Iteration 3190 Training loss 0.06826357543468475 Validation loss 0.0722007304430008 Accuracy 0.8005000352859497\n",
      "Iteration 3200 Training loss 0.06902074068784714 Validation loss 0.07203204929828644 Accuracy 0.8008750677108765\n",
      "Iteration 3210 Training loss 0.0675649493932724 Validation loss 0.07201743870973587 Accuracy 0.8007500171661377\n",
      "Iteration 3220 Training loss 0.06949525326490402 Validation loss 0.07212666422128677 Accuracy 0.8006250262260437\n",
      "Iteration 3230 Training loss 0.07956857979297638 Validation loss 0.0719914585351944 Accuracy 0.8001250624656677\n",
      "Iteration 3240 Training loss 0.06908312439918518 Validation loss 0.07219959795475006 Accuracy 0.7988750338554382\n",
      "Iteration 3250 Training loss 0.06450214982032776 Validation loss 0.07201758027076721 Accuracy 0.8006250262260437\n",
      "Iteration 3260 Training loss 0.07913318276405334 Validation loss 0.07198917865753174 Accuracy 0.8008750677108765\n",
      "Iteration 3270 Training loss 0.07065603882074356 Validation loss 0.07190076261758804 Accuracy 0.8015000224113464\n",
      "Iteration 3280 Training loss 0.07214716821908951 Validation loss 0.0719069316983223 Accuracy 0.8010000586509705\n",
      "Iteration 3290 Training loss 0.07585066556930542 Validation loss 0.07187128067016602 Accuracy 0.8006250262260437\n",
      "Iteration 3300 Training loss 0.07491893321275711 Validation loss 0.07187323272228241 Accuracy 0.8010000586509705\n",
      "Iteration 3310 Training loss 0.07561793923377991 Validation loss 0.07182042300701141 Accuracy 0.8016250133514404\n",
      "Iteration 3320 Training loss 0.06969986110925674 Validation loss 0.07182435691356659 Accuracy 0.8007500171661377\n",
      "Iteration 3330 Training loss 0.06826253980398178 Validation loss 0.07178961485624313 Accuracy 0.8011250495910645\n",
      "Iteration 3340 Training loss 0.06758937984704971 Validation loss 0.0717688500881195 Accuracy 0.8016250133514404\n",
      "Iteration 3350 Training loss 0.07804305106401443 Validation loss 0.07175830751657486 Accuracy 0.8013750314712524\n",
      "Iteration 3360 Training loss 0.07600688934326172 Validation loss 0.07172349840402603 Accuracy 0.8023750185966492\n",
      "Iteration 3370 Training loss 0.07542187720537186 Validation loss 0.07201462239027023 Accuracy 0.799875020980835\n",
      "Iteration 3380 Training loss 0.06393507122993469 Validation loss 0.07175282388925552 Accuracy 0.8013750314712524\n",
      "Iteration 3390 Training loss 0.06652601808309555 Validation loss 0.07177285850048065 Accuracy 0.8010000586509705\n",
      "Iteration 3400 Training loss 0.06993720680475235 Validation loss 0.07177326083183289 Accuracy 0.8012500405311584\n",
      "Iteration 3410 Training loss 0.0810505673289299 Validation loss 0.0716598704457283 Accuracy 0.8022500276565552\n",
      "Iteration 3420 Training loss 0.06376972794532776 Validation loss 0.07163985073566437 Accuracy 0.8025000095367432\n",
      "Iteration 3430 Training loss 0.0766749233007431 Validation loss 0.07162924110889435 Accuracy 0.8017500638961792\n",
      "Iteration 3440 Training loss 0.0735900029540062 Validation loss 0.07157022505998611 Accuracy 0.8020000457763672\n",
      "Iteration 3450 Training loss 0.07012838870286942 Validation loss 0.07155757397413254 Accuracy 0.8015000224113464\n",
      "Iteration 3460 Training loss 0.07474269717931747 Validation loss 0.0715525671839714 Accuracy 0.8021250367164612\n",
      "Iteration 3470 Training loss 0.06787716597318649 Validation loss 0.07151184976100922 Accuracy 0.8026250600814819\n",
      "Iteration 3480 Training loss 0.07139566540718079 Validation loss 0.07154211401939392 Accuracy 0.8016250133514404\n",
      "Iteration 3490 Training loss 0.07222888618707657 Validation loss 0.07154755294322968 Accuracy 0.8017500638961792\n",
      "Iteration 3500 Training loss 0.06294973939657211 Validation loss 0.07144627720117569 Accuracy 0.8025000095367432\n",
      "Iteration 3510 Training loss 0.0728740319609642 Validation loss 0.07142693549394608 Accuracy 0.8025000095367432\n",
      "Iteration 3520 Training loss 0.07374553382396698 Validation loss 0.07141807675361633 Accuracy 0.8030000329017639\n",
      "Iteration 3530 Training loss 0.05924046412110329 Validation loss 0.07140010595321655 Accuracy 0.8022500276565552\n",
      "Iteration 3540 Training loss 0.07696978747844696 Validation loss 0.07137741893529892 Accuracy 0.8030000329017639\n",
      "Iteration 3550 Training loss 0.0710626170039177 Validation loss 0.07136940211057663 Accuracy 0.8022500276565552\n",
      "Iteration 3560 Training loss 0.08694321662187576 Validation loss 0.07133767753839493 Accuracy 0.8031250238418579\n",
      "Iteration 3570 Training loss 0.06885123252868652 Validation loss 0.07133247703313828 Accuracy 0.8022500276565552\n",
      "Iteration 3580 Training loss 0.06793428957462311 Validation loss 0.07130545377731323 Accuracy 0.8028750419616699\n",
      "Iteration 3590 Training loss 0.06855963915586472 Validation loss 0.07135222852230072 Accuracy 0.8018750548362732\n",
      "Iteration 3600 Training loss 0.07102790474891663 Validation loss 0.07130034267902374 Accuracy 0.8028750419616699\n",
      "Iteration 3610 Training loss 0.07044090330600739 Validation loss 0.0712505504488945 Accuracy 0.8035000562667847\n",
      "Iteration 3620 Training loss 0.07808980345726013 Validation loss 0.07124673575162888 Accuracy 0.8038750290870667\n",
      "Iteration 3630 Training loss 0.07485304772853851 Validation loss 0.0712398812174797 Accuracy 0.8036250472068787\n",
      "Iteration 3640 Training loss 0.07483182102441788 Validation loss 0.07119931280612946 Accuracy 0.8036250472068787\n",
      "Iteration 3650 Training loss 0.06283745914697647 Validation loss 0.07119546085596085 Accuracy 0.8040000200271606\n",
      "Iteration 3660 Training loss 0.08067134767770767 Validation loss 0.07117345184087753 Accuracy 0.8037500381469727\n",
      "Iteration 3670 Training loss 0.07517638802528381 Validation loss 0.0711456760764122 Accuracy 0.8031250238418579\n",
      "Iteration 3680 Training loss 0.07413721829652786 Validation loss 0.07128606736660004 Accuracy 0.8022500276565552\n",
      "Iteration 3690 Training loss 0.06837095320224762 Validation loss 0.07110269367694855 Accuracy 0.8033750653266907\n",
      "Iteration 3700 Training loss 0.07507756352424622 Validation loss 0.07110258936882019 Accuracy 0.8026250600814819\n",
      "Iteration 3710 Training loss 0.07259897887706757 Validation loss 0.07118728756904602 Accuracy 0.8042500615119934\n",
      "Iteration 3720 Training loss 0.06690250337123871 Validation loss 0.07105691730976105 Accuracy 0.8035000562667847\n",
      "Iteration 3730 Training loss 0.06680051982402802 Validation loss 0.07114484161138535 Accuracy 0.8023750185966492\n",
      "Iteration 3740 Training loss 0.06831053644418716 Validation loss 0.07101564854383469 Accuracy 0.8035000562667847\n",
      "Iteration 3750 Training loss 0.0671226903796196 Validation loss 0.07102087885141373 Accuracy 0.8035000562667847\n",
      "Iteration 3760 Training loss 0.07300592213869095 Validation loss 0.07099299132823944 Accuracy 0.8040000200271606\n",
      "Iteration 3770 Training loss 0.07020771503448486 Validation loss 0.07105197012424469 Accuracy 0.8027500510215759\n",
      "Iteration 3780 Training loss 0.07413887232542038 Validation loss 0.070942722260952 Accuracy 0.8043750524520874\n",
      "Iteration 3790 Training loss 0.07013528048992157 Validation loss 0.07098214328289032 Accuracy 0.8032500147819519\n",
      "Iteration 3800 Training loss 0.07206670939922333 Validation loss 0.07091034203767776 Accuracy 0.8050000667572021\n",
      "Iteration 3810 Training loss 0.0753210112452507 Validation loss 0.07091527432203293 Accuracy 0.8037500381469727\n",
      "Iteration 3820 Training loss 0.07017415016889572 Validation loss 0.07091449201107025 Accuracy 0.8035000562667847\n",
      "Iteration 3830 Training loss 0.07375708967447281 Validation loss 0.07112288475036621 Accuracy 0.8020000457763672\n",
      "Iteration 3840 Training loss 0.06951861083507538 Validation loss 0.07082030177116394 Accuracy 0.8047500252723694\n",
      "Iteration 3850 Training loss 0.07362071424722672 Validation loss 0.07081124186515808 Accuracy 0.8042500615119934\n",
      "Iteration 3860 Training loss 0.07044078409671783 Validation loss 0.07080009579658508 Accuracy 0.8036250472068787\n",
      "Iteration 3870 Training loss 0.0711682140827179 Validation loss 0.07076672464609146 Accuracy 0.8043750524520874\n",
      "Iteration 3880 Training loss 0.07046423852443695 Validation loss 0.07074493914842606 Accuracy 0.8043750524520874\n",
      "Iteration 3890 Training loss 0.07432675361633301 Validation loss 0.07072463631629944 Accuracy 0.8045000433921814\n",
      "Iteration 3900 Training loss 0.07795611023902893 Validation loss 0.07070963829755783 Accuracy 0.8050000667572021\n",
      "Iteration 3910 Training loss 0.06380810588598251 Validation loss 0.07080995291471481 Accuracy 0.8032500147819519\n",
      "Iteration 3920 Training loss 0.06873860210180283 Validation loss 0.07070338726043701 Accuracy 0.8035000562667847\n",
      "Iteration 3930 Training loss 0.07232432812452316 Validation loss 0.07068213075399399 Accuracy 0.8040000200271606\n",
      "Iteration 3940 Training loss 0.06635141372680664 Validation loss 0.07063914090394974 Accuracy 0.8050000667572021\n",
      "Iteration 3950 Training loss 0.07281038165092468 Validation loss 0.07071256637573242 Accuracy 0.8037500381469727\n",
      "Iteration 3960 Training loss 0.06840934604406357 Validation loss 0.07077590376138687 Accuracy 0.8040000200271606\n",
      "Iteration 3970 Training loss 0.07174690067768097 Validation loss 0.07063945382833481 Accuracy 0.8041250109672546\n",
      "Iteration 3980 Training loss 0.06857209652662277 Validation loss 0.07058035582304001 Accuracy 0.8048750162124634\n",
      "Iteration 3990 Training loss 0.07111220806837082 Validation loss 0.07054620236158371 Accuracy 0.8055000305175781\n",
      "Iteration 4000 Training loss 0.0728607252240181 Validation loss 0.07055456191301346 Accuracy 0.8048750162124634\n",
      "Iteration 4010 Training loss 0.07305591553449631 Validation loss 0.07050725817680359 Accuracy 0.8048750162124634\n",
      "Iteration 4020 Training loss 0.07473781704902649 Validation loss 0.07052989304065704 Accuracy 0.8045000433921814\n",
      "Iteration 4030 Training loss 0.07077868282794952 Validation loss 0.07055310159921646 Accuracy 0.8036250472068787\n",
      "Iteration 4040 Training loss 0.06346543878316879 Validation loss 0.0704461857676506 Accuracy 0.8050000667572021\n",
      "Iteration 4050 Training loss 0.07824940979480743 Validation loss 0.07042615115642548 Accuracy 0.8052500486373901\n",
      "Iteration 4060 Training loss 0.07174387574195862 Validation loss 0.07045600563287735 Accuracy 0.8048750162124634\n",
      "Iteration 4070 Training loss 0.06589169055223465 Validation loss 0.07041531056165695 Accuracy 0.8052500486373901\n",
      "Iteration 4080 Training loss 0.06842599809169769 Validation loss 0.07041043788194656 Accuracy 0.8050000667572021\n",
      "Iteration 4090 Training loss 0.07280638813972473 Validation loss 0.0704447403550148 Accuracy 0.8040000200271606\n",
      "Iteration 4100 Training loss 0.06912905722856522 Validation loss 0.07042339444160461 Accuracy 0.8037500381469727\n",
      "Iteration 4110 Training loss 0.0735563263297081 Validation loss 0.07039929181337357 Accuracy 0.8041250109672546\n",
      "Iteration 4120 Training loss 0.06082690879702568 Validation loss 0.07035132497549057 Accuracy 0.8045000433921814\n",
      "Iteration 4130 Training loss 0.07423931360244751 Validation loss 0.07034362852573395 Accuracy 0.8042500615119934\n",
      "Iteration 4140 Training loss 0.06079094856977463 Validation loss 0.07030320912599564 Accuracy 0.8050000667572021\n",
      "Iteration 4150 Training loss 0.06263775378465652 Validation loss 0.07023900747299194 Accuracy 0.8058750629425049\n",
      "Iteration 4160 Training loss 0.0676441639661789 Validation loss 0.07021704316139221 Accuracy 0.8058750629425049\n",
      "Iteration 4170 Training loss 0.0668819472193718 Validation loss 0.07021971046924591 Accuracy 0.8050000667572021\n",
      "Iteration 4180 Training loss 0.07239525765180588 Validation loss 0.07022395730018616 Accuracy 0.8050000667572021\n",
      "Iteration 4190 Training loss 0.07188630849123001 Validation loss 0.07016925513744354 Accuracy 0.8052500486373901\n",
      "Iteration 4200 Training loss 0.06977832317352295 Validation loss 0.07015345990657806 Accuracy 0.8055000305175781\n",
      "Iteration 4210 Training loss 0.06527801603078842 Validation loss 0.07018397748470306 Accuracy 0.8051250576972961\n",
      "Iteration 4220 Training loss 0.0666932687163353 Validation loss 0.07013580203056335 Accuracy 0.8051250576972961\n",
      "Iteration 4230 Training loss 0.0750841349363327 Validation loss 0.07009028643369675 Accuracy 0.8058750629425049\n",
      "Iteration 4240 Training loss 0.0703980103135109 Validation loss 0.07011764496564865 Accuracy 0.8062500357627869\n",
      "Iteration 4250 Training loss 0.0717308446764946 Validation loss 0.07007016241550446 Accuracy 0.8067500591278076\n",
      "Iteration 4260 Training loss 0.06941141188144684 Validation loss 0.07003741711378098 Accuracy 0.8057500123977661\n",
      "Iteration 4270 Training loss 0.06596838682889938 Validation loss 0.07001042366027832 Accuracy 0.8063750267028809\n",
      "Iteration 4280 Training loss 0.07276669144630432 Validation loss 0.07002885639667511 Accuracy 0.8065000176429749\n",
      "Iteration 4290 Training loss 0.06939446926116943 Validation loss 0.07006236165761948 Accuracy 0.8047500252723694\n",
      "Iteration 4300 Training loss 0.0704224556684494 Validation loss 0.07006068527698517 Accuracy 0.8060000538825989\n",
      "Iteration 4310 Training loss 0.06813626736402512 Validation loss 0.06995537132024765 Accuracy 0.8057500123977661\n",
      "Iteration 4320 Training loss 0.07472784072160721 Validation loss 0.06992397457361221 Accuracy 0.8066250085830688\n",
      "Iteration 4330 Training loss 0.07230310142040253 Validation loss 0.06995682418346405 Accuracy 0.8067500591278076\n",
      "Iteration 4340 Training loss 0.0761503204703331 Validation loss 0.06993617862462997 Accuracy 0.8056250214576721\n",
      "Iteration 4350 Training loss 0.06747172772884369 Validation loss 0.06987523287534714 Accuracy 0.8066250085830688\n",
      "Iteration 4360 Training loss 0.0638599544763565 Validation loss 0.06993753463029861 Accuracy 0.8055000305175781\n",
      "Iteration 4370 Training loss 0.0675746500492096 Validation loss 0.06984753161668777 Accuracy 0.8075000643730164\n",
      "Iteration 4380 Training loss 0.07025221735239029 Validation loss 0.06984969228506088 Accuracy 0.8063750267028809\n",
      "Iteration 4390 Training loss 0.06253715604543686 Validation loss 0.06982719898223877 Accuracy 0.8066250085830688\n",
      "Iteration 4400 Training loss 0.06914077699184418 Validation loss 0.06994496285915375 Accuracy 0.8068750500679016\n",
      "Iteration 4410 Training loss 0.07156899571418762 Validation loss 0.0697813406586647 Accuracy 0.8062500357627869\n",
      "Iteration 4420 Training loss 0.0704060047864914 Validation loss 0.06978371739387512 Accuracy 0.8063750267028809\n",
      "Iteration 4430 Training loss 0.0720221996307373 Validation loss 0.06973081827163696 Accuracy 0.8067500591278076\n",
      "Iteration 4440 Training loss 0.06248336285352707 Validation loss 0.06977079063653946 Accuracy 0.8070000410079956\n",
      "Iteration 4450 Training loss 0.07056073844432831 Validation loss 0.06972838193178177 Accuracy 0.8068750500679016\n",
      "Iteration 4460 Training loss 0.06569996476173401 Validation loss 0.06967476010322571 Accuracy 0.8075000643730164\n",
      "Iteration 4470 Training loss 0.07035113126039505 Validation loss 0.06969550251960754 Accuracy 0.8067500591278076\n",
      "Iteration 4480 Training loss 0.0620996467769146 Validation loss 0.06963786482810974 Accuracy 0.8068750500679016\n",
      "Iteration 4490 Training loss 0.06380527466535568 Validation loss 0.06963192671537399 Accuracy 0.8058750629425049\n",
      "Iteration 4500 Training loss 0.06722178310155869 Validation loss 0.06960739195346832 Accuracy 0.8068750500679016\n",
      "Iteration 4510 Training loss 0.06817004829645157 Validation loss 0.06959381699562073 Accuracy 0.8065000176429749\n",
      "Iteration 4520 Training loss 0.06363127380609512 Validation loss 0.06958264857530594 Accuracy 0.8068750500679016\n",
      "Iteration 4530 Training loss 0.06276129931211472 Validation loss 0.06965992599725723 Accuracy 0.8063750267028809\n",
      "Iteration 4540 Training loss 0.06318415701389313 Validation loss 0.06960809975862503 Accuracy 0.8065000176429749\n",
      "Iteration 4550 Training loss 0.07300708442926407 Validation loss 0.06952162832021713 Accuracy 0.8085000514984131\n",
      "Iteration 4560 Training loss 0.07337844371795654 Validation loss 0.06955328583717346 Accuracy 0.8070000410079956\n",
      "Iteration 4570 Training loss 0.06913930922746658 Validation loss 0.0695687010884285 Accuracy 0.8068750500679016\n",
      "Iteration 4580 Training loss 0.06757329404354095 Validation loss 0.0696350634098053 Accuracy 0.8062500357627869\n",
      "Iteration 4590 Training loss 0.07166654616594315 Validation loss 0.06956791877746582 Accuracy 0.8066250085830688\n",
      "Iteration 4600 Training loss 0.0696079358458519 Validation loss 0.0695377066731453 Accuracy 0.8070000410079956\n",
      "Iteration 4610 Training loss 0.06423448771238327 Validation loss 0.0694504976272583 Accuracy 0.8075000643730164\n",
      "Iteration 4620 Training loss 0.06534391641616821 Validation loss 0.06952480971813202 Accuracy 0.8071250319480896\n",
      "Iteration 4630 Training loss 0.06607230007648468 Validation loss 0.06939452141523361 Accuracy 0.8091250658035278\n",
      "Iteration 4640 Training loss 0.0713273286819458 Validation loss 0.06946435570716858 Accuracy 0.8072500228881836\n",
      "Iteration 4650 Training loss 0.06579814106225967 Validation loss 0.06935615092515945 Accuracy 0.8095000386238098\n",
      "Iteration 4660 Training loss 0.07248109579086304 Validation loss 0.0693328008055687 Accuracy 0.8088750243186951\n",
      "Iteration 4670 Training loss 0.06966856867074966 Validation loss 0.06938852369785309 Accuracy 0.8080000281333923\n",
      "Iteration 4680 Training loss 0.06465445458889008 Validation loss 0.0693042203783989 Accuracy 0.8080000281333923\n",
      "Iteration 4690 Training loss 0.07390394061803818 Validation loss 0.06929074227809906 Accuracy 0.8078750371932983\n",
      "Iteration 4700 Training loss 0.06467465311288834 Validation loss 0.06930246204137802 Accuracy 0.8086250424385071\n",
      "Iteration 4710 Training loss 0.06227733567357063 Validation loss 0.06926851719617844 Accuracy 0.8080000281333923\n",
      "Iteration 4720 Training loss 0.07141076773405075 Validation loss 0.06925106793642044 Accuracy 0.8080000281333923\n",
      "Iteration 4730 Training loss 0.06927764415740967 Validation loss 0.06920473277568817 Accuracy 0.8095000386238098\n",
      "Iteration 4740 Training loss 0.06735017895698547 Validation loss 0.06926828622817993 Accuracy 0.8082500100135803\n",
      "Iteration 4750 Training loss 0.07463537901639938 Validation loss 0.06925973296165466 Accuracy 0.8082500100135803\n",
      "Iteration 4760 Training loss 0.07601236552000046 Validation loss 0.06924581527709961 Accuracy 0.8081250190734863\n",
      "Iteration 4770 Training loss 0.07097075134515762 Validation loss 0.06912713497877121 Accuracy 0.8090000152587891\n",
      "Iteration 4780 Training loss 0.0671858936548233 Validation loss 0.06911734491586685 Accuracy 0.8095000386238098\n",
      "Iteration 4790 Training loss 0.06821039319038391 Validation loss 0.06911627948284149 Accuracy 0.8091250658035278\n",
      "Iteration 4800 Training loss 0.07075590640306473 Validation loss 0.06914499402046204 Accuracy 0.8086250424385071\n",
      "Iteration 4810 Training loss 0.07128152996301651 Validation loss 0.06906753033399582 Accuracy 0.8090000152587891\n",
      "Iteration 4820 Training loss 0.06410770118236542 Validation loss 0.06905001401901245 Accuracy 0.8092500567436218\n",
      "Iteration 4830 Training loss 0.07062831521034241 Validation loss 0.06905572861433029 Accuracy 0.8090000152587891\n",
      "Iteration 4840 Training loss 0.06536655128002167 Validation loss 0.06912684440612793 Accuracy 0.8088750243186951\n",
      "Iteration 4850 Training loss 0.062318313866853714 Validation loss 0.06904605776071548 Accuracy 0.8093750476837158\n",
      "Iteration 4860 Training loss 0.07205020636320114 Validation loss 0.06897902488708496 Accuracy 0.8097500205039978\n",
      "Iteration 4870 Training loss 0.06202457845211029 Validation loss 0.06895472854375839 Accuracy 0.8101250529289246\n",
      "Iteration 4880 Training loss 0.07891467213630676 Validation loss 0.06892988830804825 Accuracy 0.8093750476837158\n",
      "Iteration 4890 Training loss 0.0771636813879013 Validation loss 0.06908141821622849 Accuracy 0.8090000152587891\n",
      "Iteration 4900 Training loss 0.07399594038724899 Validation loss 0.06890730559825897 Accuracy 0.8090000152587891\n",
      "Iteration 4910 Training loss 0.06922633945941925 Validation loss 0.06889785826206207 Accuracy 0.8103750348091125\n",
      "Iteration 4920 Training loss 0.07402683794498444 Validation loss 0.06893371790647507 Accuracy 0.8095000386238098\n",
      "Iteration 4930 Training loss 0.07560610771179199 Validation loss 0.06886932998895645 Accuracy 0.8098750114440918\n",
      "Iteration 4940 Training loss 0.07733318209648132 Validation loss 0.06882762908935547 Accuracy 0.8095000386238098\n",
      "Iteration 4950 Training loss 0.07058291137218475 Validation loss 0.06881909817457199 Accuracy 0.8103750348091125\n",
      "Iteration 4960 Training loss 0.0634264424443245 Validation loss 0.06879822164773941 Accuracy 0.8097500205039978\n",
      "Iteration 4970 Training loss 0.07671688497066498 Validation loss 0.06877990067005157 Accuracy 0.8102500438690186\n",
      "Iteration 4980 Training loss 0.06384564936161041 Validation loss 0.06886526197195053 Accuracy 0.8103750348091125\n",
      "Iteration 4990 Training loss 0.06347727030515671 Validation loss 0.06886731088161469 Accuracy 0.8096250295639038\n",
      "Iteration 5000 Training loss 0.06813990324735641 Validation loss 0.068762868642807 Accuracy 0.8100000619888306\n",
      "Iteration 5010 Training loss 0.0676223635673523 Validation loss 0.06872769445180893 Accuracy 0.8103750348091125\n",
      "Iteration 5020 Training loss 0.06656154245138168 Validation loss 0.06872157752513885 Accuracy 0.8105000257492065\n",
      "Iteration 5030 Training loss 0.06161394342780113 Validation loss 0.06868526339530945 Accuracy 0.8107500672340393\n",
      "Iteration 5040 Training loss 0.06993299722671509 Validation loss 0.06868334114551544 Accuracy 0.8095000386238098\n",
      "Iteration 5050 Training loss 0.06212587654590607 Validation loss 0.06865768134593964 Accuracy 0.8088750243186951\n",
      "Iteration 5060 Training loss 0.07530692219734192 Validation loss 0.06863418966531754 Accuracy 0.8096250295639038\n",
      "Iteration 5070 Training loss 0.06481023877859116 Validation loss 0.06871747225522995 Accuracy 0.8100000619888306\n",
      "Iteration 5080 Training loss 0.07173888385295868 Validation loss 0.06863075494766235 Accuracy 0.8105000257492065\n",
      "Iteration 5090 Training loss 0.06902382522821426 Validation loss 0.06858514994382858 Accuracy 0.8107500672340393\n",
      "Iteration 5100 Training loss 0.07141461968421936 Validation loss 0.06858143955469131 Accuracy 0.811625063419342\n",
      "Iteration 5110 Training loss 0.06766408681869507 Validation loss 0.0685431957244873 Accuracy 0.8102500438690186\n",
      "Iteration 5120 Training loss 0.07488507777452469 Validation loss 0.06853432953357697 Accuracy 0.8110000491142273\n",
      "Iteration 5130 Training loss 0.0709400400519371 Validation loss 0.06903249025344849 Accuracy 0.8071250319480896\n",
      "Iteration 5140 Training loss 0.06664574146270752 Validation loss 0.06852646172046661 Accuracy 0.8112500309944153\n",
      "Iteration 5150 Training loss 0.06909778714179993 Validation loss 0.06849633902311325 Accuracy 0.811625063419342\n",
      "Iteration 5160 Training loss 0.06328463554382324 Validation loss 0.0684666857123375 Accuracy 0.8098750114440918\n",
      "Iteration 5170 Training loss 0.06741790473461151 Validation loss 0.06845347583293915 Accuracy 0.8097500205039978\n",
      "Iteration 5180 Training loss 0.06865940988063812 Validation loss 0.06846454739570618 Accuracy 0.81187504529953\n",
      "Iteration 5190 Training loss 0.060323309153318405 Validation loss 0.06844642758369446 Accuracy 0.812000036239624\n",
      "Iteration 5200 Training loss 0.07353369891643524 Validation loss 0.06846814602613449 Accuracy 0.8115000128746033\n",
      "Iteration 5210 Training loss 0.07299325615167618 Validation loss 0.06846136599779129 Accuracy 0.8111250400543213\n",
      "Iteration 5220 Training loss 0.06084202602505684 Validation loss 0.06852369755506516 Accuracy 0.8096250295639038\n",
      "Iteration 5230 Training loss 0.05677320808172226 Validation loss 0.06836771219968796 Accuracy 0.8111250400543213\n",
      "Iteration 5240 Training loss 0.06538720428943634 Validation loss 0.06845082342624664 Accuracy 0.8105000257492065\n",
      "Iteration 5250 Training loss 0.0715891569852829 Validation loss 0.06835642457008362 Accuracy 0.8125000596046448\n",
      "Iteration 5260 Training loss 0.06391393393278122 Validation loss 0.06843792647123337 Accuracy 0.8101250529289246\n",
      "Iteration 5270 Training loss 0.06934122741222382 Validation loss 0.06835326552391052 Accuracy 0.8113750219345093\n",
      "Iteration 5280 Training loss 0.06957779824733734 Validation loss 0.0682913213968277 Accuracy 0.81187504529953\n",
      "Iteration 5290 Training loss 0.06957679986953735 Validation loss 0.06829030066728592 Accuracy 0.811750054359436\n",
      "Iteration 5300 Training loss 0.08151497691869736 Validation loss 0.06828390806913376 Accuracy 0.812250018119812\n",
      "Iteration 5310 Training loss 0.0741339847445488 Validation loss 0.06822627782821655 Accuracy 0.8103750348091125\n",
      "Iteration 5320 Training loss 0.07742121815681458 Validation loss 0.06820158660411835 Accuracy 0.8107500672340393\n",
      "Iteration 5330 Training loss 0.06332281231880188 Validation loss 0.06828337907791138 Accuracy 0.8108750581741333\n",
      "Iteration 5340 Training loss 0.0747729167342186 Validation loss 0.06817898899316788 Accuracy 0.8112500309944153\n",
      "Iteration 5350 Training loss 0.06549639254808426 Validation loss 0.0683104395866394 Accuracy 0.8101250529289246\n",
      "Iteration 5360 Training loss 0.07491707801818848 Validation loss 0.06815484911203384 Accuracy 0.8108750581741333\n",
      "Iteration 5370 Training loss 0.06354418396949768 Validation loss 0.06815026700496674 Accuracy 0.812125027179718\n",
      "Iteration 5380 Training loss 0.06997144222259521 Validation loss 0.06821473687887192 Accuracy 0.8107500672340393\n",
      "Iteration 5390 Training loss 0.0738506093621254 Validation loss 0.06809442490339279 Accuracy 0.8107500672340393\n",
      "Iteration 5400 Training loss 0.06715433299541473 Validation loss 0.06809848546981812 Accuracy 0.812125027179718\n",
      "Iteration 5410 Training loss 0.06910700350999832 Validation loss 0.0682380422949791 Accuracy 0.8102500438690186\n",
      "Iteration 5420 Training loss 0.0657014325261116 Validation loss 0.06803032755851746 Accuracy 0.8111250400543213\n",
      "Iteration 5430 Training loss 0.07112831622362137 Validation loss 0.06827376782894135 Accuracy 0.8101250529289246\n",
      "Iteration 5440 Training loss 0.06428921222686768 Validation loss 0.06803019344806671 Accuracy 0.812250018119812\n",
      "Iteration 5450 Training loss 0.0745135024189949 Validation loss 0.06805814057588577 Accuracy 0.812125027179718\n",
      "Iteration 5460 Training loss 0.06127610057592392 Validation loss 0.06812437623739243 Accuracy 0.8110000491142273\n",
      "Iteration 5470 Training loss 0.0655306726694107 Validation loss 0.06802229583263397 Accuracy 0.812125027179718\n",
      "Iteration 5480 Training loss 0.07158353924751282 Validation loss 0.06793513894081116 Accuracy 0.8110000491142273\n",
      "Iteration 5490 Training loss 0.06500132381916046 Validation loss 0.06804244965314865 Accuracy 0.8113750219345093\n",
      "Iteration 5500 Training loss 0.07164785265922546 Validation loss 0.06801297515630722 Accuracy 0.812125027179718\n",
      "Iteration 5510 Training loss 0.07379264384508133 Validation loss 0.06821222603321075 Accuracy 0.8106250166893005\n",
      "Iteration 5520 Training loss 0.062062423676252365 Validation loss 0.06792964786291122 Accuracy 0.812250018119812\n",
      "Iteration 5530 Training loss 0.058985065668821335 Validation loss 0.0678604319691658 Accuracy 0.8113750219345093\n",
      "Iteration 5540 Training loss 0.06891385465860367 Validation loss 0.06782405823469162 Accuracy 0.812000036239624\n",
      "Iteration 5550 Training loss 0.07414619624614716 Validation loss 0.0678061693906784 Accuracy 0.812000036239624\n",
      "Iteration 5560 Training loss 0.06799854338169098 Validation loss 0.0678112804889679 Accuracy 0.8115000128746033\n",
      "Iteration 5570 Training loss 0.07395044714212418 Validation loss 0.06778611242771149 Accuracy 0.811750054359436\n",
      "Iteration 5580 Training loss 0.05888330191373825 Validation loss 0.06778064370155334 Accuracy 0.811625063419342\n",
      "Iteration 5590 Training loss 0.06269029527902603 Validation loss 0.06776396930217743 Accuracy 0.8115000128746033\n",
      "Iteration 5600 Training loss 0.067670077085495 Validation loss 0.06782568246126175 Accuracy 0.8125000596046448\n",
      "Iteration 5610 Training loss 0.07756190747022629 Validation loss 0.06771358102560043 Accuracy 0.812250018119812\n",
      "Iteration 5620 Training loss 0.0718170553445816 Validation loss 0.06768383830785751 Accuracy 0.812000036239624\n",
      "Iteration 5630 Training loss 0.060091469436883926 Validation loss 0.06769368052482605 Accuracy 0.812375009059906\n",
      "Iteration 5640 Training loss 0.06318109482526779 Validation loss 0.06770037859678268 Accuracy 0.81187504529953\n",
      "Iteration 5650 Training loss 0.06420603394508362 Validation loss 0.06787123531103134 Accuracy 0.8112500309944153\n",
      "Iteration 5660 Training loss 0.07081302255392075 Validation loss 0.06762554496526718 Accuracy 0.812125027179718\n",
      "Iteration 5670 Training loss 0.07489240169525146 Validation loss 0.06760311871767044 Accuracy 0.8127500414848328\n",
      "Iteration 5680 Training loss 0.06801357120275497 Validation loss 0.06773500144481659 Accuracy 0.812250018119812\n",
      "Iteration 5690 Training loss 0.06921539455652237 Validation loss 0.06759871542453766 Accuracy 0.81187504529953\n",
      "Iteration 5700 Training loss 0.07083968073129654 Validation loss 0.06755435466766357 Accuracy 0.8130000233650208\n",
      "Iteration 5710 Training loss 0.061791323125362396 Validation loss 0.06763888895511627 Accuracy 0.8125000596046448\n",
      "Iteration 5720 Training loss 0.07171528041362762 Validation loss 0.06753165274858475 Accuracy 0.812000036239624\n",
      "Iteration 5730 Training loss 0.06295082718133926 Validation loss 0.06753633916378021 Accuracy 0.8125000596046448\n",
      "Iteration 5740 Training loss 0.06696009635925293 Validation loss 0.06747746467590332 Accuracy 0.8126250505447388\n",
      "Iteration 5750 Training loss 0.06800226867198944 Validation loss 0.06748190522193909 Accuracy 0.8128750324249268\n",
      "Iteration 5760 Training loss 0.07028143852949142 Validation loss 0.06749800592660904 Accuracy 0.8125000596046448\n",
      "Iteration 5770 Training loss 0.07751080393791199 Validation loss 0.06741175800561905 Accuracy 0.8130000233650208\n",
      "Iteration 5780 Training loss 0.060945603996515274 Validation loss 0.06749621778726578 Accuracy 0.8133750557899475\n",
      "Iteration 5790 Training loss 0.06140337511897087 Validation loss 0.06744915246963501 Accuracy 0.8132500648498535\n",
      "Iteration 5800 Training loss 0.053180232644081116 Validation loss 0.06746207922697067 Accuracy 0.8135000467300415\n",
      "Iteration 5810 Training loss 0.0611632876098156 Validation loss 0.06745556741952896 Accuracy 0.8133750557899475\n",
      "Iteration 5820 Training loss 0.06909959018230438 Validation loss 0.06737016141414642 Accuracy 0.8133750557899475\n",
      "Iteration 5830 Training loss 0.06353221833705902 Validation loss 0.06731695681810379 Accuracy 0.8130000233650208\n",
      "Iteration 5840 Training loss 0.06993842869997025 Validation loss 0.06751477718353271 Accuracy 0.81187504529953\n",
      "Iteration 5850 Training loss 0.06844541430473328 Validation loss 0.06732930988073349 Accuracy 0.8137500286102295\n",
      "Iteration 5860 Training loss 0.0693671852350235 Validation loss 0.06732885539531708 Accuracy 0.8136250376701355\n",
      "Iteration 5870 Training loss 0.07209201902151108 Validation loss 0.06732385605573654 Accuracy 0.8137500286102295\n",
      "Iteration 5880 Training loss 0.06787533313035965 Validation loss 0.06729882210493088 Accuracy 0.8142500519752502\n",
      "Iteration 5890 Training loss 0.06990060210227966 Validation loss 0.06723140925168991 Accuracy 0.8132500648498535\n",
      "Iteration 5900 Training loss 0.06515707820653915 Validation loss 0.06728368252515793 Accuracy 0.8136250376701355\n",
      "Iteration 5910 Training loss 0.0707579180598259 Validation loss 0.0671984925866127 Accuracy 0.8131250143051147\n",
      "Iteration 5920 Training loss 0.07884246110916138 Validation loss 0.06720919907093048 Accuracy 0.8133750557899475\n",
      "Iteration 5930 Training loss 0.06905848532915115 Validation loss 0.0671568289399147 Accuracy 0.8135000467300415\n",
      "Iteration 5940 Training loss 0.059458520263433456 Validation loss 0.06728573888540268 Accuracy 0.8138750195503235\n",
      "Iteration 5950 Training loss 0.06127655506134033 Validation loss 0.06714073568582535 Accuracy 0.8141250610351562\n",
      "Iteration 5960 Training loss 0.06962031871080399 Validation loss 0.06751534342765808 Accuracy 0.8131250143051147\n",
      "Iteration 5970 Training loss 0.062109917402267456 Validation loss 0.06714053452014923 Accuracy 0.8136250376701355\n",
      "Iteration 5980 Training loss 0.06201697140932083 Validation loss 0.06706871837377548 Accuracy 0.8140000104904175\n",
      "Iteration 5990 Training loss 0.07180998474359512 Validation loss 0.06710544973611832 Accuracy 0.8136250376701355\n",
      "Iteration 6000 Training loss 0.0696384459733963 Validation loss 0.0670362263917923 Accuracy 0.8140000104904175\n",
      "Iteration 6010 Training loss 0.06365544348955154 Validation loss 0.06714435666799545 Accuracy 0.8133750557899475\n",
      "Iteration 6020 Training loss 0.06122339144349098 Validation loss 0.06699086725711823 Accuracy 0.8141250610351562\n",
      "Iteration 6030 Training loss 0.07146988809108734 Validation loss 0.06697886437177658 Accuracy 0.8143750429153442\n",
      "Iteration 6040 Training loss 0.06444161385297775 Validation loss 0.06696029007434845 Accuracy 0.8140000104904175\n",
      "Iteration 6050 Training loss 0.06955195963382721 Validation loss 0.06746967136859894 Accuracy 0.8125000596046448\n",
      "Iteration 6060 Training loss 0.06885996460914612 Validation loss 0.0669315904378891 Accuracy 0.8143750429153442\n",
      "Iteration 6070 Training loss 0.06951221823692322 Validation loss 0.06698374450206757 Accuracy 0.8138750195503235\n",
      "Iteration 6080 Training loss 0.06464351713657379 Validation loss 0.06714721769094467 Accuracy 0.815250039100647\n",
      "Iteration 6090 Training loss 0.06484196335077286 Validation loss 0.06690876185894012 Accuracy 0.8138750195503235\n",
      "Iteration 6100 Training loss 0.06134750694036484 Validation loss 0.06688965857028961 Accuracy 0.8141250610351562\n",
      "Iteration 6110 Training loss 0.06604740768671036 Validation loss 0.06688820570707321 Accuracy 0.8147500157356262\n",
      "Iteration 6120 Training loss 0.07030268758535385 Validation loss 0.06684661656618118 Accuracy 0.8141250610351562\n",
      "Iteration 6130 Training loss 0.0694299191236496 Validation loss 0.06681974232196808 Accuracy 0.8147500157356262\n",
      "Iteration 6140 Training loss 0.05934039130806923 Validation loss 0.06684854626655579 Accuracy 0.8143750429153442\n",
      "Iteration 6150 Training loss 0.06355643272399902 Validation loss 0.06680586189031601 Accuracy 0.8138750195503235\n",
      "Iteration 6160 Training loss 0.062158968299627304 Validation loss 0.06701521575450897 Accuracy 0.814875066280365\n",
      "Iteration 6170 Training loss 0.06529438495635986 Validation loss 0.06683012843132019 Accuracy 0.814875066280365\n",
      "Iteration 6180 Training loss 0.06011557579040527 Validation loss 0.06683474034070969 Accuracy 0.8145000338554382\n",
      "Iteration 6190 Training loss 0.06511890143156052 Validation loss 0.06688551604747772 Accuracy 0.815125048160553\n",
      "Iteration 6200 Training loss 0.05802155286073685 Validation loss 0.06676550209522247 Accuracy 0.8145000338554382\n",
      "Iteration 6210 Training loss 0.05804198607802391 Validation loss 0.06668265163898468 Accuracy 0.8147500157356262\n",
      "Iteration 6220 Training loss 0.06416597217321396 Validation loss 0.06717368960380554 Accuracy 0.8136250376701355\n",
      "Iteration 6230 Training loss 0.06478268653154373 Validation loss 0.06695225834846497 Accuracy 0.8145000338554382\n",
      "Iteration 6240 Training loss 0.06392757594585419 Validation loss 0.06664230674505234 Accuracy 0.8147500157356262\n",
      "Iteration 6250 Training loss 0.06463950872421265 Validation loss 0.06665302067995071 Accuracy 0.815125048160553\n",
      "Iteration 6260 Training loss 0.07338415086269379 Validation loss 0.06668159365653992 Accuracy 0.815250039100647\n",
      "Iteration 6270 Training loss 0.07666267454624176 Validation loss 0.0665920227766037 Accuracy 0.815125048160553\n",
      "Iteration 6280 Training loss 0.057215455919504166 Validation loss 0.06662444025278091 Accuracy 0.815250039100647\n",
      "Iteration 6290 Training loss 0.06484883278608322 Validation loss 0.06656254082918167 Accuracy 0.815250039100647\n",
      "Iteration 6300 Training loss 0.06753669679164886 Validation loss 0.06654231250286102 Accuracy 0.815250039100647\n",
      "Iteration 6310 Training loss 0.06204644963145256 Validation loss 0.06655927747488022 Accuracy 0.815625011920929\n",
      "Iteration 6320 Training loss 0.06569284200668335 Validation loss 0.06678388267755508 Accuracy 0.8146250247955322\n",
      "Iteration 6330 Training loss 0.06575359404087067 Validation loss 0.06649381667375565 Accuracy 0.815500020980835\n",
      "Iteration 6340 Training loss 0.06954080611467361 Validation loss 0.06648688018321991 Accuracy 0.815375030040741\n",
      "Iteration 6350 Training loss 0.06352047622203827 Validation loss 0.066510871052742 Accuracy 0.8161250352859497\n",
      "Iteration 6360 Training loss 0.06885291635990143 Validation loss 0.06647554039955139 Accuracy 0.815500020980835\n",
      "Iteration 6370 Training loss 0.06693705171346664 Validation loss 0.06648856401443481 Accuracy 0.815125048160553\n",
      "Iteration 6380 Training loss 0.0676521360874176 Validation loss 0.06650693714618683 Accuracy 0.8161250352859497\n",
      "Iteration 6390 Training loss 0.06498828530311584 Validation loss 0.06646312028169632 Accuracy 0.815625011920929\n",
      "Iteration 6400 Training loss 0.07558368146419525 Validation loss 0.06637006253004074 Accuracy 0.8157500624656677\n",
      "Iteration 6410 Training loss 0.06242396682500839 Validation loss 0.06636635214090347 Accuracy 0.815375030040741\n",
      "Iteration 6420 Training loss 0.06085114926099777 Validation loss 0.06640893220901489 Accuracy 0.8158750534057617\n",
      "Iteration 6430 Training loss 0.05742424353957176 Validation loss 0.06634160131216049 Accuracy 0.815500020980835\n",
      "Iteration 6440 Training loss 0.07654811441898346 Validation loss 0.06637532263994217 Accuracy 0.8160000443458557\n",
      "Iteration 6450 Training loss 0.07264188677072525 Validation loss 0.06646301597356796 Accuracy 0.8166250586509705\n",
      "Iteration 6460 Training loss 0.06627220660448074 Validation loss 0.06630758941173553 Accuracy 0.8160000443458557\n",
      "Iteration 6470 Training loss 0.07352431863546371 Validation loss 0.06627347320318222 Accuracy 0.8160000443458557\n",
      "Iteration 6480 Training loss 0.06849682331085205 Validation loss 0.0663248598575592 Accuracy 0.8168750405311584\n",
      "Iteration 6490 Training loss 0.06786877661943436 Validation loss 0.06623814254999161 Accuracy 0.815375030040741\n",
      "Iteration 6500 Training loss 0.07306573539972305 Validation loss 0.06623640656471252 Accuracy 0.8168750405311584\n",
      "Iteration 6510 Training loss 0.0679226666688919 Validation loss 0.06618831306695938 Accuracy 0.8160000443458557\n",
      "Iteration 6520 Training loss 0.07867877930402756 Validation loss 0.06619240343570709 Accuracy 0.8170000314712524\n",
      "Iteration 6530 Training loss 0.06963752955198288 Validation loss 0.0661531388759613 Accuracy 0.8162500262260437\n",
      "Iteration 6540 Training loss 0.06919605284929276 Validation loss 0.06617972254753113 Accuracy 0.8163750171661377\n",
      "Iteration 6550 Training loss 0.06868761032819748 Validation loss 0.06612777709960938 Accuracy 0.8160000443458557\n",
      "Iteration 6560 Training loss 0.06656426191329956 Validation loss 0.06610679626464844 Accuracy 0.8166250586509705\n",
      "Iteration 6570 Training loss 0.06588927656412125 Validation loss 0.06611263006925583 Accuracy 0.8175000548362732\n",
      "Iteration 6580 Training loss 0.06273451447486877 Validation loss 0.06612387299537659 Accuracy 0.8167500495910645\n",
      "Iteration 6590 Training loss 0.07128636538982391 Validation loss 0.06607582420110703 Accuracy 0.8172500133514404\n",
      "Iteration 6600 Training loss 0.06122797355055809 Validation loss 0.06617046147584915 Accuracy 0.8175000548362732\n",
      "Iteration 6610 Training loss 0.06364850699901581 Validation loss 0.06618800759315491 Accuracy 0.8172500133514404\n",
      "Iteration 6620 Training loss 0.061800550669431686 Validation loss 0.06606514751911163 Accuracy 0.8170000314712524\n",
      "Iteration 6630 Training loss 0.05932433903217316 Validation loss 0.06601110100746155 Accuracy 0.8166250586509705\n",
      "Iteration 6640 Training loss 0.0663241297006607 Validation loss 0.06603292375802994 Accuracy 0.8176250457763672\n",
      "Iteration 6650 Training loss 0.06500868499279022 Validation loss 0.06598573923110962 Accuracy 0.8165000677108765\n",
      "Iteration 6660 Training loss 0.0659283846616745 Validation loss 0.06595534086227417 Accuracy 0.8171250224113464\n",
      "Iteration 6670 Training loss 0.06111398711800575 Validation loss 0.06593203544616699 Accuracy 0.8170000314712524\n",
      "Iteration 6680 Training loss 0.06576399505138397 Validation loss 0.06593814492225647 Accuracy 0.8166250586509705\n",
      "Iteration 6690 Training loss 0.06674408912658691 Validation loss 0.06589721143245697 Accuracy 0.8173750638961792\n",
      "Iteration 6700 Training loss 0.05981559306383133 Validation loss 0.06587304919958115 Accuracy 0.8171250224113464\n",
      "Iteration 6710 Training loss 0.06411616504192352 Validation loss 0.06594458222389221 Accuracy 0.8185000419616699\n",
      "Iteration 6720 Training loss 0.07587116956710815 Validation loss 0.06595205515623093 Accuracy 0.8183750510215759\n",
      "Iteration 6730 Training loss 0.0621701218187809 Validation loss 0.06584685295820236 Accuracy 0.8180000185966492\n",
      "Iteration 6740 Training loss 0.06147259473800659 Validation loss 0.06593780964612961 Accuracy 0.8181250095367432\n",
      "Iteration 6750 Training loss 0.05532769858837128 Validation loss 0.06580080837011337 Accuracy 0.8180000185966492\n",
      "Iteration 6760 Training loss 0.061986491084098816 Validation loss 0.06584802269935608 Accuracy 0.8185000419616699\n",
      "Iteration 6770 Training loss 0.06526541709899902 Validation loss 0.06578461825847626 Accuracy 0.8181250095367432\n",
      "Iteration 6780 Training loss 0.06275837868452072 Validation loss 0.06592737883329391 Accuracy 0.8186250329017639\n",
      "Iteration 6790 Training loss 0.0628361701965332 Validation loss 0.06585020571947098 Accuracy 0.8178750276565552\n",
      "Iteration 6800 Training loss 0.05680691450834274 Validation loss 0.06579364091157913 Accuracy 0.8185000419616699\n",
      "Iteration 6810 Training loss 0.06343197077512741 Validation loss 0.06598860770463943 Accuracy 0.8191250562667847\n",
      "Iteration 6820 Training loss 0.07937970757484436 Validation loss 0.06572052091360092 Accuracy 0.8183750510215759\n",
      "Iteration 6830 Training loss 0.05972816422581673 Validation loss 0.06571338325738907 Accuracy 0.8183750510215759\n",
      "Iteration 6840 Training loss 0.06153246387839317 Validation loss 0.06567111611366272 Accuracy 0.8176250457763672\n",
      "Iteration 6850 Training loss 0.07239577174186707 Validation loss 0.06567619740962982 Accuracy 0.8180000185966492\n",
      "Iteration 6860 Training loss 0.06378145515918732 Validation loss 0.06563730537891388 Accuracy 0.8175000548362732\n",
      "Iteration 6870 Training loss 0.0645333006978035 Validation loss 0.06560754776000977 Accuracy 0.8178750276565552\n",
      "Iteration 6880 Training loss 0.06605759263038635 Validation loss 0.06559233367443085 Accuracy 0.8178750276565552\n",
      "Iteration 6890 Training loss 0.06375133991241455 Validation loss 0.0655781552195549 Accuracy 0.8183750510215759\n",
      "Iteration 6900 Training loss 0.056093666702508926 Validation loss 0.06555455178022385 Accuracy 0.8177500367164612\n",
      "Iteration 6910 Training loss 0.06245870515704155 Validation loss 0.06554847955703735 Accuracy 0.8177500367164612\n",
      "Iteration 6920 Training loss 0.06143127381801605 Validation loss 0.06561457365751266 Accuracy 0.8188750147819519\n",
      "Iteration 6930 Training loss 0.06488392502069473 Validation loss 0.06553332507610321 Accuracy 0.8182500600814819\n",
      "Iteration 6940 Training loss 0.07253771275281906 Validation loss 0.06552177667617798 Accuracy 0.8183750510215759\n",
      "Iteration 6950 Training loss 0.058245182037353516 Validation loss 0.06549205631017685 Accuracy 0.8190000653266907\n",
      "Iteration 6960 Training loss 0.07019993662834167 Validation loss 0.06547053158283234 Accuracy 0.8185000419616699\n",
      "Iteration 6970 Training loss 0.06535323709249496 Validation loss 0.06553738564252853 Accuracy 0.8182500600814819\n",
      "Iteration 6980 Training loss 0.06164005026221275 Validation loss 0.06542995572090149 Accuracy 0.8188750147819519\n",
      "Iteration 6990 Training loss 0.0704704150557518 Validation loss 0.06541652232408524 Accuracy 0.8190000653266907\n",
      "Iteration 7000 Training loss 0.059174373745918274 Validation loss 0.06538428366184235 Accuracy 0.8188750147819519\n",
      "Iteration 7010 Training loss 0.060783594846725464 Validation loss 0.06550928205251694 Accuracy 0.8201250433921814\n",
      "Iteration 7020 Training loss 0.06283921748399734 Validation loss 0.06540346145629883 Accuracy 0.8196250200271606\n",
      "Iteration 7030 Training loss 0.06568188965320587 Validation loss 0.06535187363624573 Accuracy 0.8197500109672546\n",
      "Iteration 7040 Training loss 0.07636140286922455 Validation loss 0.06532035768032074 Accuracy 0.8196250200271606\n",
      "Iteration 7050 Training loss 0.06487905234098434 Validation loss 0.06531520932912827 Accuracy 0.8196250200271606\n",
      "Iteration 7060 Training loss 0.05892190337181091 Validation loss 0.06536171585321426 Accuracy 0.8198750615119934\n",
      "Iteration 7070 Training loss 0.06221131980419159 Validation loss 0.06535772234201431 Accuracy 0.8181250095367432\n",
      "Iteration 7080 Training loss 0.06644958257675171 Validation loss 0.06542645394802094 Accuracy 0.8197500109672546\n",
      "Iteration 7090 Training loss 0.0686681941151619 Validation loss 0.06526752561330795 Accuracy 0.8198750615119934\n",
      "Iteration 7100 Training loss 0.06249213218688965 Validation loss 0.06530024856328964 Accuracy 0.8188750147819519\n",
      "Iteration 7110 Training loss 0.06513381004333496 Validation loss 0.06521221995353699 Accuracy 0.8191250562667847\n",
      "Iteration 7120 Training loss 0.06606797873973846 Validation loss 0.06522060185670853 Accuracy 0.8197500109672546\n",
      "Iteration 7130 Training loss 0.05679646506905556 Validation loss 0.06518199294805527 Accuracy 0.8196250200271606\n",
      "Iteration 7140 Training loss 0.068801648914814 Validation loss 0.06527859717607498 Accuracy 0.8197500109672546\n",
      "Iteration 7150 Training loss 0.06098879128694534 Validation loss 0.06515397876501083 Accuracy 0.8193750381469727\n",
      "Iteration 7160 Training loss 0.065358467400074 Validation loss 0.0651698186993599 Accuracy 0.8202500343322754\n",
      "Iteration 7170 Training loss 0.0592239685356617 Validation loss 0.06512140482664108 Accuracy 0.8200000524520874\n",
      "Iteration 7180 Training loss 0.06453320384025574 Validation loss 0.06510630995035172 Accuracy 0.8200000524520874\n",
      "Iteration 7190 Training loss 0.06213752552866936 Validation loss 0.06509480625391006 Accuracy 0.8203750252723694\n",
      "Iteration 7200 Training loss 0.06841178983449936 Validation loss 0.06506866216659546 Accuracy 0.8200000524520874\n",
      "Iteration 7210 Training loss 0.05824866145849228 Validation loss 0.0650637075304985 Accuracy 0.8207500576972961\n",
      "Iteration 7220 Training loss 0.06995488703250885 Validation loss 0.06508273631334305 Accuracy 0.8210000395774841\n",
      "Iteration 7230 Training loss 0.06352919340133667 Validation loss 0.06506050378084183 Accuracy 0.8212500214576721\n",
      "Iteration 7240 Training loss 0.06504819542169571 Validation loss 0.06501471251249313 Accuracy 0.8203750252723694\n",
      "Iteration 7250 Training loss 0.06101822853088379 Validation loss 0.06499026715755463 Accuracy 0.8202500343322754\n",
      "Iteration 7260 Training loss 0.0573812760412693 Validation loss 0.06501699984073639 Accuracy 0.8211250305175781\n",
      "Iteration 7270 Training loss 0.07067101448774338 Validation loss 0.06495317816734314 Accuracy 0.8205000162124634\n",
      "Iteration 7280 Training loss 0.06803537905216217 Validation loss 0.06495924293994904 Accuracy 0.8205000162124634\n",
      "Iteration 7290 Training loss 0.0682692676782608 Validation loss 0.06492208689451218 Accuracy 0.8212500214576721\n",
      "Iteration 7300 Training loss 0.06732482463121414 Validation loss 0.06489667296409607 Accuracy 0.8205000162124634\n",
      "Iteration 7310 Training loss 0.06475352495908737 Validation loss 0.06488486379384995 Accuracy 0.8215000629425049\n",
      "Iteration 7320 Training loss 0.06423351168632507 Validation loss 0.06487151235342026 Accuracy 0.8205000162124634\n",
      "Iteration 7330 Training loss 0.05664129927754402 Validation loss 0.06485437601804733 Accuracy 0.8208750486373901\n",
      "Iteration 7340 Training loss 0.0659378245472908 Validation loss 0.064947709441185 Accuracy 0.8211250305175781\n",
      "Iteration 7350 Training loss 0.05988754704594612 Validation loss 0.06495656073093414 Accuracy 0.8208750486373901\n",
      "Iteration 7360 Training loss 0.07373979687690735 Validation loss 0.06491722911596298 Accuracy 0.8208750486373901\n",
      "Iteration 7370 Training loss 0.06114506721496582 Validation loss 0.06486544758081436 Accuracy 0.8216250538825989\n",
      "Iteration 7380 Training loss 0.05167557671666145 Validation loss 0.06482452154159546 Accuracy 0.8213750123977661\n",
      "Iteration 7390 Training loss 0.06665558367967606 Validation loss 0.06476671248674393 Accuracy 0.8211250305175781\n",
      "Iteration 7400 Training loss 0.06468239426612854 Validation loss 0.06487250328063965 Accuracy 0.8213750123977661\n",
      "Iteration 7410 Training loss 0.06984629482030869 Validation loss 0.0647544264793396 Accuracy 0.8213750123977661\n",
      "Iteration 7420 Training loss 0.06132214888930321 Validation loss 0.06471320241689682 Accuracy 0.8211250305175781\n",
      "Iteration 7430 Training loss 0.06295667588710785 Validation loss 0.0649951696395874 Accuracy 0.8220000267028809\n",
      "Iteration 7440 Training loss 0.05657610669732094 Validation loss 0.06468679010868073 Accuracy 0.8213750123977661\n",
      "Iteration 7450 Training loss 0.05879834666848183 Validation loss 0.06475677341222763 Accuracy 0.8210000395774841\n",
      "Iteration 7460 Training loss 0.06640230864286423 Validation loss 0.06472204625606537 Accuracy 0.8225000500679016\n",
      "Iteration 7470 Training loss 0.06576027721166611 Validation loss 0.06463424116373062 Accuracy 0.8215000629425049\n",
      "Iteration 7480 Training loss 0.0679720938205719 Validation loss 0.0646134614944458 Accuracy 0.8213750123977661\n",
      "Iteration 7490 Training loss 0.06747990101575851 Validation loss 0.06460929661989212 Accuracy 0.8222500681877136\n",
      "Iteration 7500 Training loss 0.06675033271312714 Validation loss 0.06459496915340424 Accuracy 0.8221250176429749\n",
      "Iteration 7510 Training loss 0.07028558850288391 Validation loss 0.06464210897684097 Accuracy 0.8222500681877136\n",
      "Iteration 7520 Training loss 0.06208143010735512 Validation loss 0.06454013288021088 Accuracy 0.8222500681877136\n",
      "Iteration 7530 Training loss 0.06676601618528366 Validation loss 0.06464597582817078 Accuracy 0.8227500319480896\n",
      "Iteration 7540 Training loss 0.06740011274814606 Validation loss 0.06460293382406235 Accuracy 0.8225000500679016\n",
      "Iteration 7550 Training loss 0.06251496076583862 Validation loss 0.06450843811035156 Accuracy 0.8231250643730164\n",
      "Iteration 7560 Training loss 0.05575350672006607 Validation loss 0.06448228657245636 Accuracy 0.8213750123977661\n",
      "Iteration 7570 Training loss 0.06300414353609085 Validation loss 0.06446444988250732 Accuracy 0.8212500214576721\n",
      "Iteration 7580 Training loss 0.057811055332422256 Validation loss 0.06445436179637909 Accuracy 0.8236250281333923\n",
      "Iteration 7590 Training loss 0.058705948293209076 Validation loss 0.06445199251174927 Accuracy 0.8233750462532043\n",
      "Iteration 7600 Training loss 0.06362738460302353 Validation loss 0.06463358551263809 Accuracy 0.8228750228881836\n",
      "Iteration 7610 Training loss 0.07020117342472076 Validation loss 0.0645533949136734 Accuracy 0.8226250410079956\n",
      "Iteration 7620 Training loss 0.06250862777233124 Validation loss 0.06442099064588547 Accuracy 0.8235000371932983\n",
      "Iteration 7630 Training loss 0.06060314178466797 Validation loss 0.06439575552940369 Accuracy 0.8217500448226929\n",
      "Iteration 7640 Training loss 0.058280814439058304 Validation loss 0.06442642211914062 Accuracy 0.8223750591278076\n",
      "Iteration 7650 Training loss 0.057554781436920166 Validation loss 0.06434386223554611 Accuracy 0.8227500319480896\n",
      "Iteration 7660 Training loss 0.06690476089715958 Validation loss 0.06451370567083359 Accuracy 0.8236250281333923\n",
      "Iteration 7670 Training loss 0.06534121185541153 Validation loss 0.06430882960557938 Accuracy 0.8231250643730164\n",
      "Iteration 7680 Training loss 0.06893236935138702 Validation loss 0.06434354186058044 Accuracy 0.8230000138282776\n",
      "Iteration 7690 Training loss 0.05858687311410904 Validation loss 0.06429849565029144 Accuracy 0.8230000138282776\n",
      "Iteration 7700 Training loss 0.06713304668664932 Validation loss 0.06426837295293808 Accuracy 0.8226250410079956\n",
      "Iteration 7710 Training loss 0.07071702927350998 Validation loss 0.06449083983898163 Accuracy 0.8245000243186951\n",
      "Iteration 7720 Training loss 0.06458428502082825 Validation loss 0.06422484666109085 Accuracy 0.8227500319480896\n",
      "Iteration 7730 Training loss 0.06383545696735382 Validation loss 0.06422499567270279 Accuracy 0.8235000371932983\n",
      "Iteration 7740 Training loss 0.0666830837726593 Validation loss 0.06419548392295837 Accuracy 0.8233750462532043\n",
      "Iteration 7750 Training loss 0.06117631122469902 Validation loss 0.06433077901601791 Accuracy 0.8235000371932983\n",
      "Iteration 7760 Training loss 0.05795083940029144 Validation loss 0.0641668364405632 Accuracy 0.8236250281333923\n",
      "Iteration 7770 Training loss 0.06332764029502869 Validation loss 0.0642726942896843 Accuracy 0.8236250281333923\n",
      "Iteration 7780 Training loss 0.0647461861371994 Validation loss 0.06439542025327682 Accuracy 0.8248750567436218\n",
      "Iteration 7790 Training loss 0.05940411612391472 Validation loss 0.0642106831073761 Accuracy 0.8241250514984131\n",
      "Iteration 7800 Training loss 0.05950479209423065 Validation loss 0.06425195932388306 Accuracy 0.8232500553131104\n",
      "Iteration 7810 Training loss 0.059675201773643494 Validation loss 0.06409170478582382 Accuracy 0.8235000371932983\n",
      "Iteration 7820 Training loss 0.06121901795268059 Validation loss 0.06415107101202011 Accuracy 0.8236250281333923\n",
      "Iteration 7830 Training loss 0.06377773731946945 Validation loss 0.06409305334091187 Accuracy 0.8241250514984131\n",
      "Iteration 7840 Training loss 0.0704035684466362 Validation loss 0.06405504047870636 Accuracy 0.8238750100135803\n",
      "Iteration 7850 Training loss 0.0673663541674614 Validation loss 0.0640232190489769 Accuracy 0.8235000371932983\n",
      "Iteration 7860 Training loss 0.06666581332683563 Validation loss 0.0640380010008812 Accuracy 0.8247500658035278\n",
      "Iteration 7870 Training loss 0.0593809150159359 Validation loss 0.06401482224464417 Accuracy 0.8238750100135803\n",
      "Iteration 7880 Training loss 0.06596700102090836 Validation loss 0.06406188011169434 Accuracy 0.8243750333786011\n",
      "Iteration 7890 Training loss 0.06085066497325897 Validation loss 0.0640425831079483 Accuracy 0.8238750100135803\n",
      "Iteration 7900 Training loss 0.060429323464632034 Validation loss 0.06395772099494934 Accuracy 0.8237500190734863\n",
      "Iteration 7910 Training loss 0.059117019176483154 Validation loss 0.06448879092931747 Accuracy 0.8243750333786011\n",
      "Iteration 7920 Training loss 0.06078754737973213 Validation loss 0.06390845775604248 Accuracy 0.8250000476837158\n",
      "Iteration 7930 Training loss 0.06546617299318314 Validation loss 0.06389353424310684 Accuracy 0.8247500658035278\n",
      "Iteration 7940 Training loss 0.0631253570318222 Validation loss 0.06388434022665024 Accuracy 0.8247500658035278\n",
      "Iteration 7950 Training loss 0.06551475077867508 Validation loss 0.06398577988147736 Accuracy 0.8238750100135803\n",
      "Iteration 7960 Training loss 0.06627746671438217 Validation loss 0.06385333091020584 Accuracy 0.8238750100135803\n",
      "Iteration 7970 Training loss 0.06158829480409622 Validation loss 0.06413561850786209 Accuracy 0.8252500295639038\n",
      "Iteration 7980 Training loss 0.07449058443307877 Validation loss 0.06383226066827774 Accuracy 0.8245000243186951\n",
      "Iteration 7990 Training loss 0.06676703691482544 Validation loss 0.06384895741939545 Accuracy 0.8246250152587891\n",
      "Iteration 8000 Training loss 0.06579456478357315 Validation loss 0.06380448490381241 Accuracy 0.8247500658035278\n",
      "Iteration 8010 Training loss 0.06273773312568665 Validation loss 0.06378350406885147 Accuracy 0.8248750567436218\n",
      "Iteration 8020 Training loss 0.06536094099283218 Validation loss 0.063778817653656 Accuracy 0.8245000243186951\n",
      "Iteration 8030 Training loss 0.05883318930864334 Validation loss 0.06375737488269806 Accuracy 0.8255000114440918\n",
      "Iteration 8040 Training loss 0.06286898255348206 Validation loss 0.06397468596696854 Accuracy 0.8246250152587891\n",
      "Iteration 8050 Training loss 0.05775174871087074 Validation loss 0.06404710561037064 Accuracy 0.8255000114440918\n",
      "Iteration 8060 Training loss 0.07355529814958572 Validation loss 0.0637185350060463 Accuracy 0.8242500424385071\n",
      "Iteration 8070 Training loss 0.07035409659147263 Validation loss 0.0637255311012268 Accuracy 0.8242500424385071\n",
      "Iteration 8080 Training loss 0.05761409550905228 Validation loss 0.06378041952848434 Accuracy 0.8245000243186951\n",
      "Iteration 8090 Training loss 0.06804726272821426 Validation loss 0.06370091438293457 Accuracy 0.8245000243186951\n",
      "Iteration 8100 Training loss 0.06062045320868492 Validation loss 0.06364040821790695 Accuracy 0.8252500295639038\n",
      "Iteration 8110 Training loss 0.05697614699602127 Validation loss 0.06389676779508591 Accuracy 0.8246250152587891\n",
      "Iteration 8120 Training loss 0.05813146382570267 Validation loss 0.06383495777845383 Accuracy 0.8243750333786011\n",
      "Iteration 8130 Training loss 0.06266575306653976 Validation loss 0.06359969824552536 Accuracy 0.8251250386238098\n",
      "Iteration 8140 Training loss 0.06772144883871078 Validation loss 0.06371352076530457 Accuracy 0.8262500166893005\n",
      "Iteration 8150 Training loss 0.06479518115520477 Validation loss 0.06356547027826309 Accuracy 0.8255000114440918\n",
      "Iteration 8160 Training loss 0.0630655363202095 Validation loss 0.06362144649028778 Accuracy 0.8258750438690186\n",
      "Iteration 8170 Training loss 0.058712128549814224 Validation loss 0.06355379521846771 Accuracy 0.8253750205039978\n",
      "Iteration 8180 Training loss 0.06086951121687889 Validation loss 0.0636327713727951 Accuracy 0.8263750672340393\n",
      "Iteration 8190 Training loss 0.061709463596343994 Validation loss 0.06364304572343826 Accuracy 0.8256250619888306\n",
      "Iteration 8200 Training loss 0.06007910892367363 Validation loss 0.06352023035287857 Accuracy 0.8267500400543213\n",
      "Iteration 8210 Training loss 0.055049389600753784 Validation loss 0.0634731724858284 Accuracy 0.8260000348091125\n",
      "Iteration 8220 Training loss 0.06462343782186508 Validation loss 0.06361540406942368 Accuracy 0.8255000114440918\n",
      "Iteration 8230 Training loss 0.06589678674936295 Validation loss 0.06380491703748703 Accuracy 0.8271250128746033\n",
      "Iteration 8240 Training loss 0.06446638703346252 Validation loss 0.06347116827964783 Accuracy 0.8263750672340393\n",
      "Iteration 8250 Training loss 0.05815356969833374 Validation loss 0.06341861188411713 Accuracy 0.8266250491142273\n",
      "Iteration 8260 Training loss 0.057723596692085266 Validation loss 0.06340299546718597 Accuracy 0.8268750309944153\n",
      "Iteration 8270 Training loss 0.06554900109767914 Validation loss 0.0634191632270813 Accuracy 0.8262500166893005\n",
      "Iteration 8280 Training loss 0.06899344176054001 Validation loss 0.06339739263057709 Accuracy 0.8270000219345093\n",
      "Iteration 8290 Training loss 0.05962804704904556 Validation loss 0.0633796751499176 Accuracy 0.8266250491142273\n",
      "Iteration 8300 Training loss 0.06243416294455528 Validation loss 0.0634177029132843 Accuracy 0.8270000219345093\n",
      "Iteration 8310 Training loss 0.06766363978385925 Validation loss 0.06333498656749725 Accuracy 0.827875018119812\n",
      "Iteration 8320 Training loss 0.06187113747000694 Validation loss 0.06345817446708679 Accuracy 0.827250063419342\n",
      "Iteration 8330 Training loss 0.06309868395328522 Validation loss 0.06361176818609238 Accuracy 0.8270000219345093\n",
      "Iteration 8340 Training loss 0.06934244185686111 Validation loss 0.06336937099695206 Accuracy 0.827625036239624\n",
      "Iteration 8350 Training loss 0.06250031292438507 Validation loss 0.06328681111335754 Accuracy 0.827750027179718\n",
      "Iteration 8360 Training loss 0.05867442488670349 Validation loss 0.06326641887426376 Accuracy 0.827750027179718\n",
      "Iteration 8370 Training loss 0.05699126049876213 Validation loss 0.06338091939687729 Accuracy 0.8268750309944153\n",
      "Iteration 8380 Training loss 0.06391111016273499 Validation loss 0.06325899809598923 Accuracy 0.827250063419342\n",
      "Iteration 8390 Training loss 0.05970574542880058 Validation loss 0.06331667304039001 Accuracy 0.827875018119812\n",
      "Iteration 8400 Training loss 0.05195623263716698 Validation loss 0.06322073936462402 Accuracy 0.8280000686645508\n",
      "Iteration 8410 Training loss 0.06607142090797424 Validation loss 0.06322398036718369 Accuracy 0.827875018119812\n",
      "Iteration 8420 Training loss 0.061275601387023926 Validation loss 0.06328698247671127 Accuracy 0.8281250596046448\n",
      "Iteration 8430 Training loss 0.07757452130317688 Validation loss 0.06321229040622711 Accuracy 0.8286250233650208\n",
      "Iteration 8440 Training loss 0.06550489366054535 Validation loss 0.06319081038236618 Accuracy 0.82750004529953\n",
      "Iteration 8450 Training loss 0.06027007848024368 Validation loss 0.06317513436079025 Accuracy 0.827375054359436\n",
      "Iteration 8460 Training loss 0.06219421699643135 Validation loss 0.06314347684383392 Accuracy 0.8287500143051147\n",
      "Iteration 8470 Training loss 0.05830104649066925 Validation loss 0.06318280100822449 Accuracy 0.8280000686645508\n",
      "Iteration 8480 Training loss 0.06643068790435791 Validation loss 0.06332395225763321 Accuracy 0.827750027179718\n",
      "Iteration 8490 Training loss 0.055936772376298904 Validation loss 0.06318441033363342 Accuracy 0.827375054359436\n",
      "Iteration 8500 Training loss 0.060718778520822525 Validation loss 0.06323815882205963 Accuracy 0.8270000219345093\n",
      "Iteration 8510 Training loss 0.05892210826277733 Validation loss 0.06306347250938416 Accuracy 0.8286250233650208\n",
      "Iteration 8520 Training loss 0.06316913664340973 Validation loss 0.06304727494716644 Accuracy 0.8286250233650208\n",
      "Iteration 8530 Training loss 0.06742783635854721 Validation loss 0.06305672973394394 Accuracy 0.8285000324249268\n",
      "Iteration 8540 Training loss 0.055141057819128036 Validation loss 0.06310997903347015 Accuracy 0.8268750309944153\n",
      "Iteration 8550 Training loss 0.06478102505207062 Validation loss 0.06309791654348373 Accuracy 0.8268750309944153\n",
      "Iteration 8560 Training loss 0.06336572021245956 Validation loss 0.0629761815071106 Accuracy 0.8280000686645508\n",
      "Iteration 8570 Training loss 0.06242198869585991 Validation loss 0.06295983493328094 Accuracy 0.827750027179718\n",
      "Iteration 8580 Training loss 0.06100780516862869 Validation loss 0.06294353306293488 Accuracy 0.8281250596046448\n",
      "Iteration 8590 Training loss 0.06455179303884506 Validation loss 0.0629744604229927 Accuracy 0.8281250596046448\n",
      "Iteration 8600 Training loss 0.06169450655579567 Validation loss 0.06311909854412079 Accuracy 0.827750027179718\n",
      "Iteration 8610 Training loss 0.0569692887365818 Validation loss 0.0629458948969841 Accuracy 0.8283750414848328\n",
      "Iteration 8620 Training loss 0.06120028346776962 Validation loss 0.0629521980881691 Accuracy 0.8283750414848328\n",
      "Iteration 8630 Training loss 0.057939350605010986 Validation loss 0.0629049688577652 Accuracy 0.827750027179718\n",
      "Iteration 8640 Training loss 0.06366004049777985 Validation loss 0.0628744587302208 Accuracy 0.8286250233650208\n",
      "Iteration 8650 Training loss 0.05588121712207794 Validation loss 0.0629473477602005 Accuracy 0.8270000219345093\n",
      "Iteration 8660 Training loss 0.06396863609552383 Validation loss 0.06284452974796295 Accuracy 0.8282500505447388\n",
      "Iteration 8670 Training loss 0.06972324848175049 Validation loss 0.06285765022039413 Accuracy 0.8283750414848328\n",
      "Iteration 8680 Training loss 0.06543035060167313 Validation loss 0.06281612068414688 Accuracy 0.8283750414848328\n",
      "Iteration 8690 Training loss 0.06104384735226631 Validation loss 0.06279504299163818 Accuracy 0.8283750414848328\n",
      "Iteration 8700 Training loss 0.05459421128034592 Validation loss 0.06278718262910843 Accuracy 0.827875018119812\n",
      "Iteration 8710 Training loss 0.061413366347551346 Validation loss 0.06280215084552765 Accuracy 0.8283750414848328\n",
      "Iteration 8720 Training loss 0.06371971219778061 Validation loss 0.06275486946105957 Accuracy 0.8287500143051147\n",
      "Iteration 8730 Training loss 0.06068936362862587 Validation loss 0.06275871396064758 Accuracy 0.8288750648498535\n",
      "Iteration 8740 Training loss 0.0623430535197258 Validation loss 0.06282125413417816 Accuracy 0.82750004529953\n",
      "Iteration 8750 Training loss 0.055310625582933426 Validation loss 0.0627286434173584 Accuracy 0.8286250233650208\n",
      "Iteration 8760 Training loss 0.07338967174291611 Validation loss 0.0628606528043747 Accuracy 0.8283750414848328\n",
      "Iteration 8770 Training loss 0.06634344160556793 Validation loss 0.06301600486040115 Accuracy 0.827875018119812\n",
      "Iteration 8780 Training loss 0.05508169159293175 Validation loss 0.06270846724510193 Accuracy 0.8286250233650208\n",
      "Iteration 8790 Training loss 0.06142221763730049 Validation loss 0.06275387853384018 Accuracy 0.8282500505447388\n",
      "Iteration 8800 Training loss 0.06059122830629349 Validation loss 0.06267163157463074 Accuracy 0.8288750648498535\n",
      "Iteration 8810 Training loss 0.059424757957458496 Validation loss 0.06289251148700714 Accuracy 0.827750027179718\n",
      "Iteration 8820 Training loss 0.060651443898677826 Validation loss 0.06264347583055496 Accuracy 0.8283750414848328\n",
      "Iteration 8830 Training loss 0.06353291869163513 Validation loss 0.06286728382110596 Accuracy 0.8281250596046448\n",
      "Iteration 8840 Training loss 0.06247570365667343 Validation loss 0.06263428926467896 Accuracy 0.8293750286102295\n",
      "Iteration 8850 Training loss 0.06022094935178757 Validation loss 0.06275512278079987 Accuracy 0.8288750648498535\n",
      "Iteration 8860 Training loss 0.074077308177948 Validation loss 0.06257563829421997 Accuracy 0.8285000324249268\n",
      "Iteration 8870 Training loss 0.06029944866895676 Validation loss 0.06258858740329742 Accuracy 0.8295000195503235\n",
      "Iteration 8880 Training loss 0.05660772696137428 Validation loss 0.06263484805822372 Accuracy 0.8297500610351562\n",
      "Iteration 8890 Training loss 0.05531863868236542 Validation loss 0.06252165883779526 Accuracy 0.8297500610351562\n",
      "Iteration 8900 Training loss 0.0636223554611206 Validation loss 0.0625220537185669 Accuracy 0.8295000195503235\n",
      "Iteration 8910 Training loss 0.05915731191635132 Validation loss 0.06251490116119385 Accuracy 0.8286250233650208\n",
      "Iteration 8920 Training loss 0.0700436532497406 Validation loss 0.06249425187706947 Accuracy 0.830625057220459\n",
      "Iteration 8930 Training loss 0.062014978379011154 Validation loss 0.06247835233807564 Accuracy 0.8301250338554382\n",
      "Iteration 8940 Training loss 0.06304941326379776 Validation loss 0.062459927052259445 Accuracy 0.8301250338554382\n",
      "Iteration 8950 Training loss 0.061479661613702774 Validation loss 0.06243712082505226 Accuracy 0.8300000429153442\n",
      "Iteration 8960 Training loss 0.04882488027215004 Validation loss 0.062436703592538834 Accuracy 0.8292500376701355\n",
      "Iteration 8970 Training loss 0.05826449394226074 Validation loss 0.06243481487035751 Accuracy 0.830625057220459\n",
      "Iteration 8980 Training loss 0.05678556486964226 Validation loss 0.062477219849824905 Accuracy 0.8295000195503235\n",
      "Iteration 8990 Training loss 0.06313569098711014 Validation loss 0.06256032735109329 Accuracy 0.8302500247955322\n",
      "Iteration 9000 Training loss 0.07253398001194 Validation loss 0.06250855326652527 Accuracy 0.8283750414848328\n",
      "Iteration 9010 Training loss 0.06410873681306839 Validation loss 0.06238720938563347 Accuracy 0.830875039100647\n",
      "Iteration 9020 Training loss 0.060641635209321976 Validation loss 0.06234614923596382 Accuracy 0.830500066280365\n",
      "Iteration 9030 Training loss 0.060803402215242386 Validation loss 0.06248365342617035 Accuracy 0.830500066280365\n",
      "Iteration 9040 Training loss 0.06263427436351776 Validation loss 0.062426015734672546 Accuracy 0.8298750519752502\n",
      "Iteration 9050 Training loss 0.06058548018336296 Validation loss 0.062305301427841187 Accuracy 0.830625057220459\n",
      "Iteration 9060 Training loss 0.059764061123132706 Validation loss 0.0622808113694191 Accuracy 0.8300000429153442\n",
      "Iteration 9070 Training loss 0.06757082045078278 Validation loss 0.06232551857829094 Accuracy 0.830500066280365\n",
      "Iteration 9080 Training loss 0.05957592651247978 Validation loss 0.062546007335186 Accuracy 0.8302500247955322\n",
      "Iteration 9090 Training loss 0.06231047585606575 Validation loss 0.062260277569293976 Accuracy 0.8293750286102295\n",
      "Iteration 9100 Training loss 0.0597890168428421 Validation loss 0.062275055795907974 Accuracy 0.8302500247955322\n",
      "Iteration 9110 Training loss 0.05433391407132149 Validation loss 0.06222475692629814 Accuracy 0.830875039100647\n",
      "Iteration 9120 Training loss 0.06483665108680725 Validation loss 0.06222722679376602 Accuracy 0.831000030040741\n",
      "Iteration 9130 Training loss 0.06041062995791435 Validation loss 0.06233394145965576 Accuracy 0.830875039100647\n",
      "Iteration 9140 Training loss 0.06913720816373825 Validation loss 0.0621831975877285 Accuracy 0.8303750157356262\n",
      "Iteration 9150 Training loss 0.06390277296304703 Validation loss 0.06226220354437828 Accuracy 0.8302500247955322\n",
      "Iteration 9160 Training loss 0.05781654641032219 Validation loss 0.06219852715730667 Accuracy 0.8301250338554382\n",
      "Iteration 9170 Training loss 0.059800803661346436 Validation loss 0.06217190995812416 Accuracy 0.8298750519752502\n",
      "Iteration 9180 Training loss 0.05999474227428436 Validation loss 0.06212804839015007 Accuracy 0.8303750157356262\n",
      "Iteration 9190 Training loss 0.0600135512650013 Validation loss 0.06211366876959801 Accuracy 0.8301250338554382\n",
      "Iteration 9200 Training loss 0.06132279708981514 Validation loss 0.06210160255432129 Accuracy 0.8302500247955322\n",
      "Iteration 9210 Training loss 0.061099063605070114 Validation loss 0.06218720227479935 Accuracy 0.8303750157356262\n",
      "Iteration 9220 Training loss 0.06217128783464432 Validation loss 0.062375590205192566 Accuracy 0.8301250338554382\n",
      "Iteration 9230 Training loss 0.0601874478161335 Validation loss 0.06219496950507164 Accuracy 0.831000030040741\n",
      "Iteration 9240 Training loss 0.06172236427664757 Validation loss 0.06210112199187279 Accuracy 0.8303750157356262\n",
      "Iteration 9250 Training loss 0.05777747556567192 Validation loss 0.06216387450695038 Accuracy 0.830625057220459\n",
      "Iteration 9260 Training loss 0.07030071318149567 Validation loss 0.06202363967895508 Accuracy 0.831000030040741\n",
      "Iteration 9270 Training loss 0.06262604147195816 Validation loss 0.06200169771909714 Accuracy 0.831000030040741\n",
      "Iteration 9280 Training loss 0.057538993656635284 Validation loss 0.062049586325883865 Accuracy 0.830625057220459\n",
      "Iteration 9290 Training loss 0.05816974490880966 Validation loss 0.06197896972298622 Accuracy 0.831125020980835\n",
      "Iteration 9300 Training loss 0.07465141266584396 Validation loss 0.061989642679691315 Accuracy 0.831250011920929\n",
      "Iteration 9310 Training loss 0.05701766535639763 Validation loss 0.06201820820569992 Accuracy 0.830750048160553\n",
      "Iteration 9320 Training loss 0.05747956037521362 Validation loss 0.06195361912250519 Accuracy 0.8313750624656677\n",
      "Iteration 9330 Training loss 0.057163432240486145 Validation loss 0.06214643269777298 Accuracy 0.8316250443458557\n",
      "Iteration 9340 Training loss 0.05546308308839798 Validation loss 0.06201031059026718 Accuracy 0.8316250443458557\n",
      "Iteration 9350 Training loss 0.0608053021132946 Validation loss 0.06191359832882881 Accuracy 0.8313750624656677\n",
      "Iteration 9360 Training loss 0.06652727723121643 Validation loss 0.061919569969177246 Accuracy 0.8313750624656677\n",
      "Iteration 9370 Training loss 0.058009564876556396 Validation loss 0.06191880255937576 Accuracy 0.8316250443458557\n",
      "Iteration 9380 Training loss 0.060131337493658066 Validation loss 0.06189605966210365 Accuracy 0.831000030040741\n",
      "Iteration 9390 Training loss 0.05797988176345825 Validation loss 0.061887226998806 Accuracy 0.831250011920929\n",
      "Iteration 9400 Training loss 0.055684298276901245 Validation loss 0.06185819208621979 Accuracy 0.831000030040741\n",
      "Iteration 9410 Training loss 0.05995018407702446 Validation loss 0.06185014545917511 Accuracy 0.8320000171661377\n",
      "Iteration 9420 Training loss 0.06564249098300934 Validation loss 0.06184917688369751 Accuracy 0.8316250443458557\n",
      "Iteration 9430 Training loss 0.0671771764755249 Validation loss 0.06185649335384369 Accuracy 0.831125020980835\n",
      "Iteration 9440 Training loss 0.0687437504529953 Validation loss 0.06181354448199272 Accuracy 0.8320000171661377\n",
      "Iteration 9450 Training loss 0.057040341198444366 Validation loss 0.06190013885498047 Accuracy 0.830875039100647\n",
      "Iteration 9460 Training loss 0.05315463989973068 Validation loss 0.061782438308000565 Accuracy 0.8320000171661377\n",
      "Iteration 9470 Training loss 0.06005672365427017 Validation loss 0.061760660260915756 Accuracy 0.8320000171661377\n",
      "Iteration 9480 Training loss 0.0637679249048233 Validation loss 0.06198808550834656 Accuracy 0.8317500352859497\n",
      "Iteration 9490 Training loss 0.06515093892812729 Validation loss 0.061747871339321136 Accuracy 0.8321250677108765\n",
      "Iteration 9500 Training loss 0.05848204344511032 Validation loss 0.06175821274518967 Accuracy 0.8320000171661377\n",
      "Iteration 9510 Training loss 0.06445484608411789 Validation loss 0.0617254413664341 Accuracy 0.8317500352859497\n",
      "Iteration 9520 Training loss 0.06492580473423004 Validation loss 0.061708372086286545 Accuracy 0.8317500352859497\n",
      "Iteration 9530 Training loss 0.05499007925391197 Validation loss 0.061696916818618774 Accuracy 0.8323750495910645\n",
      "Iteration 9540 Training loss 0.06681739538908005 Validation loss 0.061688750982284546 Accuracy 0.8317500352859497\n",
      "Iteration 9550 Training loss 0.05900375172495842 Validation loss 0.061901479959487915 Accuracy 0.8315000534057617\n",
      "Iteration 9560 Training loss 0.05866876617074013 Validation loss 0.06204397231340408 Accuracy 0.8313750624656677\n",
      "Iteration 9570 Training loss 0.06342672556638718 Validation loss 0.061638232320547104 Accuracy 0.8320000171661377\n",
      "Iteration 9580 Training loss 0.07109182327985764 Validation loss 0.06161094829440117 Accuracy 0.8327500224113464\n",
      "Iteration 9590 Training loss 0.06709613651037216 Validation loss 0.0616043396294117 Accuracy 0.8328750133514404\n",
      "Iteration 9600 Training loss 0.05865989252924919 Validation loss 0.061580635607242584 Accuracy 0.8327500224113464\n",
      "Iteration 9610 Training loss 0.05933103337883949 Validation loss 0.06162498891353607 Accuracy 0.8326250314712524\n",
      "Iteration 9620 Training loss 0.04635726660490036 Validation loss 0.061571210622787476 Accuracy 0.8322500586509705\n",
      "Iteration 9630 Training loss 0.06075919419527054 Validation loss 0.06160390377044678 Accuracy 0.8321250677108765\n",
      "Iteration 9640 Training loss 0.059752922505140305 Validation loss 0.061589889228343964 Accuracy 0.8318750262260437\n",
      "Iteration 9650 Training loss 0.07100678980350494 Validation loss 0.0615789070725441 Accuracy 0.8330000638961792\n",
      "Iteration 9660 Training loss 0.05616699159145355 Validation loss 0.061545755714178085 Accuracy 0.8326250314712524\n",
      "Iteration 9670 Training loss 0.05681871250271797 Validation loss 0.06152000278234482 Accuracy 0.8321250677108765\n",
      "Iteration 9680 Training loss 0.05907059088349342 Validation loss 0.06151900440454483 Accuracy 0.8322500586509705\n",
      "Iteration 9690 Training loss 0.05971716716885567 Validation loss 0.06168586015701294 Accuracy 0.8327500224113464\n",
      "Iteration 9700 Training loss 0.05804628133773804 Validation loss 0.06165319308638573 Accuracy 0.8325000405311584\n",
      "Iteration 9710 Training loss 0.054661162197589874 Validation loss 0.06145570054650307 Accuracy 0.8325000405311584\n",
      "Iteration 9720 Training loss 0.06323660910129547 Validation loss 0.06161251664161682 Accuracy 0.8323750495910645\n",
      "Iteration 9730 Training loss 0.06582736223936081 Validation loss 0.061432577669620514 Accuracy 0.8318750262260437\n",
      "Iteration 9740 Training loss 0.051516760140657425 Validation loss 0.06144721806049347 Accuracy 0.8325000405311584\n",
      "Iteration 9750 Training loss 0.0702643096446991 Validation loss 0.061439089477062225 Accuracy 0.8326250314712524\n",
      "Iteration 9760 Training loss 0.0590965636074543 Validation loss 0.06149857118725777 Accuracy 0.8313750624656677\n",
      "Iteration 9770 Training loss 0.06441880762577057 Validation loss 0.06143394857645035 Accuracy 0.8325000405311584\n",
      "Iteration 9780 Training loss 0.05804551765322685 Validation loss 0.06143629923462868 Accuracy 0.8328750133514404\n",
      "Iteration 9790 Training loss 0.057424016296863556 Validation loss 0.061430059373378754 Accuracy 0.8315000534057617\n",
      "Iteration 9800 Training loss 0.05983257666230202 Validation loss 0.06141671538352966 Accuracy 0.8322500586509705\n",
      "Iteration 9810 Training loss 0.061636026948690414 Validation loss 0.06147322803735733 Accuracy 0.8336250185966492\n",
      "Iteration 9820 Training loss 0.06487969309091568 Validation loss 0.06143219769001007 Accuracy 0.8325000405311584\n",
      "Iteration 9830 Training loss 0.06521597504615784 Validation loss 0.06143178418278694 Accuracy 0.8320000171661377\n",
      "Iteration 9840 Training loss 0.06052454933524132 Validation loss 0.06140538677573204 Accuracy 0.8337500691413879\n",
      "Iteration 9850 Training loss 0.06086629629135132 Validation loss 0.06139146536588669 Accuracy 0.8326250314712524\n",
      "Iteration 9860 Training loss 0.06165337562561035 Validation loss 0.06134482100605965 Accuracy 0.8337500691413879\n",
      "Iteration 9870 Training loss 0.06427982449531555 Validation loss 0.062084559351205826 Accuracy 0.830625057220459\n",
      "Iteration 9880 Training loss 0.05523554980754852 Validation loss 0.06128415837883949 Accuracy 0.8333750367164612\n",
      "Iteration 9890 Training loss 0.06337251514196396 Validation loss 0.06125548109412193 Accuracy 0.8335000276565552\n",
      "Iteration 9900 Training loss 0.06293679028749466 Validation loss 0.061254385858774185 Accuracy 0.8326250314712524\n",
      "Iteration 9910 Training loss 0.05446687713265419 Validation loss 0.06124520301818848 Accuracy 0.8343750238418579\n",
      "Iteration 9920 Training loss 0.05852711945772171 Validation loss 0.06124630942940712 Accuracy 0.8336250185966492\n",
      "Iteration 9930 Training loss 0.05779675021767616 Validation loss 0.06119069457054138 Accuracy 0.8343750238418579\n",
      "Iteration 9940 Training loss 0.06815511733293533 Validation loss 0.06119289994239807 Accuracy 0.8342500329017639\n",
      "Iteration 9950 Training loss 0.061049871146678925 Validation loss 0.0611807145178318 Accuracy 0.8341250419616699\n",
      "Iteration 9960 Training loss 0.06361058354377747 Validation loss 0.06131015345454216 Accuracy 0.8323750495910645\n",
      "Iteration 9970 Training loss 0.06943662464618683 Validation loss 0.06115225329995155 Accuracy 0.8338750600814819\n",
      "Iteration 9980 Training loss 0.06458202004432678 Validation loss 0.061169691383838654 Accuracy 0.8335000276565552\n",
      "Iteration 9990 Training loss 0.062369346618652344 Validation loss 0.061143215745687485 Accuracy 0.8335000276565552\n",
      "Iteration 10000 Training loss 0.061643704771995544 Validation loss 0.06122385337948799 Accuracy 0.8330000638961792\n",
      "Iteration 10010 Training loss 0.058652620762586594 Validation loss 0.0612952895462513 Accuracy 0.8323750495910645\n",
      "Iteration 10020 Training loss 0.056075166910886765 Validation loss 0.06113538518548012 Accuracy 0.8340000510215759\n",
      "Iteration 10030 Training loss 0.06136776879429817 Validation loss 0.06121569871902466 Accuracy 0.8330000638961792\n",
      "Iteration 10040 Training loss 0.05965045839548111 Validation loss 0.06110880523920059 Accuracy 0.8332500457763672\n",
      "Iteration 10050 Training loss 0.06535176932811737 Validation loss 0.06133938208222389 Accuracy 0.8328750133514404\n",
      "Iteration 10060 Training loss 0.056208934634923935 Validation loss 0.061106692999601364 Accuracy 0.8356250524520874\n",
      "Iteration 10070 Training loss 0.06431347876787186 Validation loss 0.061044562608003616 Accuracy 0.8346250653266907\n",
      "Iteration 10080 Training loss 0.06349523365497589 Validation loss 0.06105339899659157 Accuracy 0.8348750472068787\n",
      "Iteration 10090 Training loss 0.060988254845142365 Validation loss 0.06129058822989464 Accuracy 0.8331250548362732\n",
      "Iteration 10100 Training loss 0.05600178986787796 Validation loss 0.06118191406130791 Accuracy 0.8341250419616699\n",
      "Iteration 10110 Training loss 0.05896935239434242 Validation loss 0.06099987402558327 Accuracy 0.8342500329017639\n",
      "Iteration 10120 Training loss 0.06019657850265503 Validation loss 0.06100098416209221 Accuracy 0.8348750472068787\n",
      "Iteration 10130 Training loss 0.05652981624007225 Validation loss 0.06103162840008736 Accuracy 0.8343750238418579\n",
      "Iteration 10140 Training loss 0.05822256952524185 Validation loss 0.06098911538720131 Accuracy 0.8348750472068787\n",
      "Iteration 10150 Training loss 0.05656985193490982 Validation loss 0.060967978090047836 Accuracy 0.8346250653266907\n",
      "Iteration 10160 Training loss 0.06556518375873566 Validation loss 0.061100978404283524 Accuracy 0.8337500691413879\n",
      "Iteration 10170 Training loss 0.05884891375899315 Validation loss 0.06096462160348892 Accuracy 0.8342500329017639\n",
      "Iteration 10180 Training loss 0.05850359424948692 Validation loss 0.06114189326763153 Accuracy 0.8333750367164612\n",
      "Iteration 10190 Training loss 0.05812946334481239 Validation loss 0.06092504411935806 Accuracy 0.8340000510215759\n",
      "Iteration 10200 Training loss 0.05818253383040428 Validation loss 0.060913898050785065 Accuracy 0.8352500200271606\n",
      "Iteration 10210 Training loss 0.06108352541923523 Validation loss 0.060947444289922714 Accuracy 0.8347500562667847\n",
      "Iteration 10220 Training loss 0.0662495344877243 Validation loss 0.060878511518239975 Accuracy 0.8347500562667847\n",
      "Iteration 10230 Training loss 0.05902595445513725 Validation loss 0.06089315935969353 Accuracy 0.8348750472068787\n",
      "Iteration 10240 Training loss 0.062352489680051804 Validation loss 0.06087141111493111 Accuracy 0.8352500200271606\n",
      "Iteration 10250 Training loss 0.05497507005929947 Validation loss 0.06097578629851341 Accuracy 0.8342500329017639\n",
      "Iteration 10260 Training loss 0.05873136222362518 Validation loss 0.060843102633953094 Accuracy 0.8353750109672546\n",
      "Iteration 10270 Training loss 0.06453724950551987 Validation loss 0.06105458363890648 Accuracy 0.8330000638961792\n",
      "Iteration 10280 Training loss 0.06124653294682503 Validation loss 0.060817912220954895 Accuracy 0.8348750472068787\n",
      "Iteration 10290 Training loss 0.05601595342159271 Validation loss 0.06081249192357063 Accuracy 0.8352500200271606\n",
      "Iteration 10300 Training loss 0.05570853129029274 Validation loss 0.06080411747097969 Accuracy 0.8348750472068787\n",
      "Iteration 10310 Training loss 0.0604938343167305 Validation loss 0.0609799288213253 Accuracy 0.8338750600814819\n",
      "Iteration 10320 Training loss 0.058933552354574203 Validation loss 0.060748107731342316 Accuracy 0.8355000615119934\n",
      "Iteration 10330 Training loss 0.06566210836172104 Validation loss 0.06084209308028221 Accuracy 0.8341250419616699\n",
      "Iteration 10340 Training loss 0.05880890414118767 Validation loss 0.06073339283466339 Accuracy 0.8352500200271606\n",
      "Iteration 10350 Training loss 0.052128035575151443 Validation loss 0.06090154871344566 Accuracy 0.8341250419616699\n",
      "Iteration 10360 Training loss 0.057407986372709274 Validation loss 0.060707759112119675 Accuracy 0.8347500562667847\n",
      "Iteration 10370 Training loss 0.053926169872283936 Validation loss 0.06076979637145996 Accuracy 0.8343750238418579\n",
      "Iteration 10380 Training loss 0.061534710228443146 Validation loss 0.06070423871278763 Accuracy 0.8351250290870667\n",
      "Iteration 10390 Training loss 0.05219841003417969 Validation loss 0.06080794706940651 Accuracy 0.8346250653266907\n",
      "Iteration 10400 Training loss 0.05815291404724121 Validation loss 0.060695718973875046 Accuracy 0.8345000147819519\n",
      "Iteration 10410 Training loss 0.059124905616045 Validation loss 0.0606951005756855 Accuracy 0.8352500200271606\n",
      "Iteration 10420 Training loss 0.06105823069810867 Validation loss 0.060695260763168335 Accuracy 0.8342500329017639\n",
      "Iteration 10430 Training loss 0.05952335521578789 Validation loss 0.06070869415998459 Accuracy 0.8332500457763672\n",
      "Iteration 10440 Training loss 0.06851956248283386 Validation loss 0.06065927818417549 Accuracy 0.8343750238418579\n",
      "Iteration 10450 Training loss 0.05334091931581497 Validation loss 0.06066097691655159 Accuracy 0.8356250524520874\n",
      "Iteration 10460 Training loss 0.05567879602313042 Validation loss 0.060600221157073975 Accuracy 0.8351250290870667\n",
      "Iteration 10470 Training loss 0.06534167379140854 Validation loss 0.06059951335191727 Accuracy 0.8346250653266907\n",
      "Iteration 10480 Training loss 0.059125788509845734 Validation loss 0.06059729680418968 Accuracy 0.8351250290870667\n",
      "Iteration 10490 Training loss 0.06316080689430237 Validation loss 0.06061631813645363 Accuracy 0.8365000486373901\n",
      "Iteration 10500 Training loss 0.060882844030857086 Validation loss 0.060591623187065125 Accuracy 0.8366250395774841\n",
      "Iteration 10510 Training loss 0.05601062998175621 Validation loss 0.06056267023086548 Accuracy 0.8365000486373901\n",
      "Iteration 10520 Training loss 0.06282287836074829 Validation loss 0.060541871935129166 Accuracy 0.8350000381469727\n",
      "Iteration 10530 Training loss 0.0663829818367958 Validation loss 0.06057615205645561 Accuracy 0.8352500200271606\n",
      "Iteration 10540 Training loss 0.05995020642876625 Validation loss 0.06056603416800499 Accuracy 0.8348750472068787\n",
      "Iteration 10550 Training loss 0.058601949363946915 Validation loss 0.06072394177317619 Accuracy 0.8345000147819519\n",
      "Iteration 10560 Training loss 0.04603057727217674 Validation loss 0.06058184430003166 Accuracy 0.8337500691413879\n",
      "Iteration 10570 Training loss 0.055008336901664734 Validation loss 0.06052376702427864 Accuracy 0.8345000147819519\n",
      "Iteration 10580 Training loss 0.0570051483809948 Validation loss 0.060519758611917496 Accuracy 0.8350000381469727\n",
      "Iteration 10590 Training loss 0.05827292427420616 Validation loss 0.06046878919005394 Accuracy 0.8353750109672546\n",
      "Iteration 10600 Training loss 0.06094168499112129 Validation loss 0.06046684458851814 Accuracy 0.8360000252723694\n",
      "Iteration 10610 Training loss 0.05440815910696983 Validation loss 0.060453422367572784 Accuracy 0.8355000615119934\n",
      "Iteration 10620 Training loss 0.05755364149808884 Validation loss 0.060500793159008026 Accuracy 0.8357500433921814\n",
      "Iteration 10630 Training loss 0.06722918152809143 Validation loss 0.06083778664469719 Accuracy 0.8345000147819519\n",
      "Iteration 10640 Training loss 0.06641658395528793 Validation loss 0.060506902635097504 Accuracy 0.8362500667572021\n",
      "Iteration 10650 Training loss 0.06473975628614426 Validation loss 0.06042545288801193 Accuracy 0.8355000615119934\n",
      "Iteration 10660 Training loss 0.05072722211480141 Validation loss 0.06049840897321701 Accuracy 0.8363750576972961\n",
      "Iteration 10670 Training loss 0.06288579106330872 Validation loss 0.06045141816139221 Accuracy 0.8368750214576721\n",
      "Iteration 10680 Training loss 0.04885728284716606 Validation loss 0.0603933222591877 Accuracy 0.8367500305175781\n",
      "Iteration 10690 Training loss 0.0545099712908268 Validation loss 0.060415126383304596 Accuracy 0.8357500433921814\n",
      "Iteration 10700 Training loss 0.05546797811985016 Validation loss 0.06042959913611412 Accuracy 0.8350000381469727\n",
      "Iteration 10710 Training loss 0.06478247046470642 Validation loss 0.06051965802907944 Accuracy 0.8351250290870667\n",
      "Iteration 10720 Training loss 0.0549435093998909 Validation loss 0.06037333980202675 Accuracy 0.8366250395774841\n",
      "Iteration 10730 Training loss 0.0634787306189537 Validation loss 0.06045187637209892 Accuracy 0.8347500562667847\n",
      "Iteration 10740 Training loss 0.059151165187358856 Validation loss 0.06033630296587944 Accuracy 0.8356250524520874\n",
      "Iteration 10750 Training loss 0.06841747462749481 Validation loss 0.0603281706571579 Accuracy 0.8356250524520874\n",
      "Iteration 10760 Training loss 0.07167518138885498 Validation loss 0.06032685190439224 Accuracy 0.8353750109672546\n",
      "Iteration 10770 Training loss 0.05453085899353027 Validation loss 0.06033497303724289 Accuracy 0.8377500176429749\n",
      "Iteration 10780 Training loss 0.061920490115880966 Validation loss 0.06029285863041878 Accuracy 0.8356250524520874\n",
      "Iteration 10790 Training loss 0.04721067473292351 Validation loss 0.06035307049751282 Accuracy 0.8357500433921814\n",
      "Iteration 10800 Training loss 0.06375111639499664 Validation loss 0.060267023742198944 Accuracy 0.8363750576972961\n",
      "Iteration 10810 Training loss 0.05681310594081879 Validation loss 0.06029706820845604 Accuracy 0.8367500305175781\n",
      "Iteration 10820 Training loss 0.060860030353069305 Validation loss 0.06022702530026436 Accuracy 0.8353750109672546\n",
      "Iteration 10830 Training loss 0.061237651854753494 Validation loss 0.06021760776638985 Accuracy 0.8358750343322754\n",
      "Iteration 10840 Training loss 0.05757461115717888 Validation loss 0.060207974165678024 Accuracy 0.8351250290870667\n",
      "Iteration 10850 Training loss 0.05353357270359993 Validation loss 0.060312096029520035 Accuracy 0.8360000252723694\n",
      "Iteration 10860 Training loss 0.056788451969623566 Validation loss 0.060190487653017044 Accuracy 0.8375000357627869\n",
      "Iteration 10870 Training loss 0.06087913736701012 Validation loss 0.06024814769625664 Accuracy 0.8362500667572021\n",
      "Iteration 10880 Training loss 0.05948806554079056 Validation loss 0.06017450615763664 Accuracy 0.8358750343322754\n",
      "Iteration 10890 Training loss 0.05185327306389809 Validation loss 0.06015222147107124 Accuracy 0.8371250629425049\n",
      "Iteration 10900 Training loss 0.061734285205602646 Validation loss 0.06017325073480606 Accuracy 0.8377500176429749\n",
      "Iteration 10910 Training loss 0.056460823863744736 Validation loss 0.060376740992069244 Accuracy 0.8363750576972961\n",
      "Iteration 10920 Training loss 0.05267434939742088 Validation loss 0.060112178325653076 Accuracy 0.8382500410079956\n",
      "Iteration 10930 Training loss 0.04974210634827614 Validation loss 0.06009872630238533 Accuracy 0.8381250500679016\n",
      "Iteration 10940 Training loss 0.057614028453826904 Validation loss 0.060124147683382034 Accuracy 0.8382500410079956\n",
      "Iteration 10950 Training loss 0.06185255944728851 Validation loss 0.060076866298913956 Accuracy 0.8373750448226929\n",
      "Iteration 10960 Training loss 0.05325544625520706 Validation loss 0.06006874516606331 Accuracy 0.8375000357627869\n",
      "Iteration 10970 Training loss 0.06480344384908676 Validation loss 0.06026564911007881 Accuracy 0.8368750214576721\n",
      "Iteration 10980 Training loss 0.05287845432758331 Validation loss 0.06007431447505951 Accuracy 0.8363750576972961\n",
      "Iteration 10990 Training loss 0.05375208705663681 Validation loss 0.06018882617354393 Accuracy 0.8391250371932983\n",
      "Iteration 11000 Training loss 0.058369528502225876 Validation loss 0.060145314782857895 Accuracy 0.8368750214576721\n",
      "Iteration 11010 Training loss 0.056760337203741074 Validation loss 0.06009475514292717 Accuracy 0.8370000123977661\n",
      "Iteration 11020 Training loss 0.05929994583129883 Validation loss 0.06035660579800606 Accuracy 0.8363750576972961\n",
      "Iteration 11030 Training loss 0.05856533721089363 Validation loss 0.06003083288669586 Accuracy 0.8372500538825989\n",
      "Iteration 11040 Training loss 0.06963049620389938 Validation loss 0.06002048775553703 Accuracy 0.8373750448226929\n",
      "Iteration 11050 Training loss 0.06278582662343979 Validation loss 0.06000648811459541 Accuracy 0.8383750319480896\n",
      "Iteration 11060 Training loss 0.05433505401015282 Validation loss 0.05998857319355011 Accuracy 0.8376250267028809\n",
      "Iteration 11070 Training loss 0.06159108877182007 Validation loss 0.059990327805280685 Accuracy 0.8388750553131104\n",
      "Iteration 11080 Training loss 0.06852876394987106 Validation loss 0.060026880353689194 Accuracy 0.8383750319480896\n",
      "Iteration 11090 Training loss 0.06363645941019058 Validation loss 0.060005344450473785 Accuracy 0.8365000486373901\n",
      "Iteration 11100 Training loss 0.0641007348895073 Validation loss 0.0599675215780735 Accuracy 0.8365000486373901\n",
      "Iteration 11110 Training loss 0.051067184656858444 Validation loss 0.06014971807599068 Accuracy 0.8361250162124634\n",
      "Iteration 11120 Training loss 0.05433327704668045 Validation loss 0.05991186946630478 Accuracy 0.8385000228881836\n",
      "Iteration 11130 Training loss 0.061045531183481216 Validation loss 0.05996455252170563 Accuracy 0.8385000228881836\n",
      "Iteration 11140 Training loss 0.06023307517170906 Validation loss 0.059911374002695084 Accuracy 0.8373750448226929\n",
      "Iteration 11150 Training loss 0.06265823543071747 Validation loss 0.059893377125263214 Accuracy 0.8368750214576721\n",
      "Iteration 11160 Training loss 0.0567815862596035 Validation loss 0.06002827361226082 Accuracy 0.8362500667572021\n",
      "Iteration 11170 Training loss 0.05600695312023163 Validation loss 0.05994652211666107 Accuracy 0.8391250371932983\n",
      "Iteration 11180 Training loss 0.05386223644018173 Validation loss 0.060014285147190094 Accuracy 0.8358750343322754\n",
      "Iteration 11190 Training loss 0.058414820581674576 Validation loss 0.059981394559144974 Accuracy 0.8362500667572021\n",
      "Iteration 11200 Training loss 0.06071190536022186 Validation loss 0.059853266924619675 Accuracy 0.8386250138282776\n",
      "Iteration 11210 Training loss 0.05902830883860588 Validation loss 0.05981598049402237 Accuracy 0.8385000228881836\n",
      "Iteration 11220 Training loss 0.05764656886458397 Validation loss 0.05983014777302742 Accuracy 0.8397500514984131\n",
      "Iteration 11230 Training loss 0.04555526748299599 Validation loss 0.059815824031829834 Accuracy 0.8378750681877136\n",
      "Iteration 11240 Training loss 0.06388945132493973 Validation loss 0.05980739742517471 Accuracy 0.8382500410079956\n",
      "Iteration 11250 Training loss 0.061285827308893204 Validation loss 0.0598333403468132 Accuracy 0.8383750319480896\n",
      "Iteration 11260 Training loss 0.06343626230955124 Validation loss 0.059785082936286926 Accuracy 0.8381250500679016\n",
      "Iteration 11270 Training loss 0.05837155133485794 Validation loss 0.0597890205681324 Accuracy 0.8380000591278076\n",
      "Iteration 11280 Training loss 0.06387314200401306 Validation loss 0.059858813881874084 Accuracy 0.8370000123977661\n",
      "Iteration 11290 Training loss 0.059055015444755554 Validation loss 0.059823065996170044 Accuracy 0.8401250243186951\n",
      "Iteration 11300 Training loss 0.06987511366605759 Validation loss 0.05994885414838791 Accuracy 0.8377500176429749\n",
      "Iteration 11310 Training loss 0.058882780373096466 Validation loss 0.05973165109753609 Accuracy 0.8387500643730164\n",
      "Iteration 11320 Training loss 0.062393318861722946 Validation loss 0.05973416194319725 Accuracy 0.8385000228881836\n",
      "Iteration 11330 Training loss 0.06000332161784172 Validation loss 0.059961531311273575 Accuracy 0.8366250395774841\n",
      "Iteration 11340 Training loss 0.06211785227060318 Validation loss 0.059718403965234756 Accuracy 0.8386250138282776\n",
      "Iteration 11350 Training loss 0.05110052600502968 Validation loss 0.05969887971878052 Accuracy 0.8390000462532043\n",
      "Iteration 11360 Training loss 0.058828458189964294 Validation loss 0.05970189347863197 Accuracy 0.8397500514984131\n",
      "Iteration 11370 Training loss 0.05564024671912193 Validation loss 0.05978076532483101 Accuracy 0.8380000591278076\n",
      "Iteration 11380 Training loss 0.06311263144016266 Validation loss 0.05968224257230759 Accuracy 0.8390000462532043\n",
      "Iteration 11390 Training loss 0.07036788016557693 Validation loss 0.05973906069993973 Accuracy 0.8382500410079956\n",
      "Iteration 11400 Training loss 0.05842607468366623 Validation loss 0.05979928746819496 Accuracy 0.8381250500679016\n",
      "Iteration 11410 Training loss 0.06158897280693054 Validation loss 0.05963031202554703 Accuracy 0.8395000696182251\n",
      "Iteration 11420 Training loss 0.05912217125296593 Validation loss 0.059601668268442154 Accuracy 0.8400000333786011\n",
      "Iteration 11430 Training loss 0.05485108122229576 Validation loss 0.05963800475001335 Accuracy 0.8385000228881836\n",
      "Iteration 11440 Training loss 0.05709671974182129 Validation loss 0.05985979735851288 Accuracy 0.8381250500679016\n",
      "Iteration 11450 Training loss 0.05438348650932312 Validation loss 0.05965278297662735 Accuracy 0.8382500410079956\n",
      "Iteration 11460 Training loss 0.05236617848277092 Validation loss 0.05960124731063843 Accuracy 0.8390000462532043\n",
      "Iteration 11470 Training loss 0.053878918290138245 Validation loss 0.059703368693590164 Accuracy 0.8383750319480896\n",
      "Iteration 11480 Training loss 0.06745821982622147 Validation loss 0.0596066415309906 Accuracy 0.8386250138282776\n",
      "Iteration 11490 Training loss 0.06428265571594238 Validation loss 0.059607889503240585 Accuracy 0.8381250500679016\n",
      "Iteration 11500 Training loss 0.058691829442977905 Validation loss 0.0595984049141407 Accuracy 0.8375000357627869\n",
      "Iteration 11510 Training loss 0.05300190672278404 Validation loss 0.05961740389466286 Accuracy 0.8381250500679016\n",
      "Iteration 11520 Training loss 0.058357495814561844 Validation loss 0.05954832211136818 Accuracy 0.8378750681877136\n",
      "Iteration 11530 Training loss 0.060204897075891495 Validation loss 0.05951443687081337 Accuracy 0.8402500152587891\n",
      "Iteration 11540 Training loss 0.0645410344004631 Validation loss 0.05967294052243233 Accuracy 0.8376250267028809\n",
      "Iteration 11550 Training loss 0.05662795528769493 Validation loss 0.05969288945198059 Accuracy 0.8387500643730164\n",
      "Iteration 11560 Training loss 0.05784473195672035 Validation loss 0.05967729538679123 Accuracy 0.8383750319480896\n",
      "Iteration 11570 Training loss 0.05931558459997177 Validation loss 0.05966153368353844 Accuracy 0.8386250138282776\n",
      "Iteration 11580 Training loss 0.05617864429950714 Validation loss 0.05949879065155983 Accuracy 0.8390000462532043\n",
      "Iteration 11590 Training loss 0.06785035878419876 Validation loss 0.059457872062921524 Accuracy 0.8401250243186951\n",
      "Iteration 11600 Training loss 0.059974391013383865 Validation loss 0.059551090002059937 Accuracy 0.8385000228881836\n",
      "Iteration 11610 Training loss 0.0593731589615345 Validation loss 0.059429366141557693 Accuracy 0.8397500514984131\n",
      "Iteration 11620 Training loss 0.055637769401073456 Validation loss 0.05941310524940491 Accuracy 0.8408750295639038\n",
      "Iteration 11630 Training loss 0.058284785598516464 Validation loss 0.059411562979221344 Accuracy 0.8402500152587891\n",
      "Iteration 11640 Training loss 0.054420214146375656 Validation loss 0.05942271649837494 Accuracy 0.8397500514984131\n",
      "Iteration 11650 Training loss 0.0506298802793026 Validation loss 0.05943923443555832 Accuracy 0.8402500152587891\n",
      "Iteration 11660 Training loss 0.05993793532252312 Validation loss 0.05940695479512215 Accuracy 0.8395000696182251\n",
      "Iteration 11670 Training loss 0.05801244080066681 Validation loss 0.05939009413123131 Accuracy 0.8400000333786011\n",
      "Iteration 11680 Training loss 0.06071499362587929 Validation loss 0.0593838207423687 Accuracy 0.8402500152587891\n",
      "Iteration 11690 Training loss 0.054928481578826904 Validation loss 0.059401508420705795 Accuracy 0.8417500257492065\n",
      "Iteration 11700 Training loss 0.05783338472247124 Validation loss 0.05934590846300125 Accuracy 0.8402500152587891\n",
      "Iteration 11710 Training loss 0.06293056905269623 Validation loss 0.05942322686314583 Accuracy 0.8392500281333923\n",
      "Iteration 11720 Training loss 0.0548536479473114 Validation loss 0.05943769961595535 Accuracy 0.8393750190734863\n",
      "Iteration 11730 Training loss 0.070744089782238 Validation loss 0.05934032052755356 Accuracy 0.8418750166893005\n",
      "Iteration 11740 Training loss 0.05605945363640785 Validation loss 0.059692200273275375 Accuracy 0.8387500643730164\n",
      "Iteration 11750 Training loss 0.06179344654083252 Validation loss 0.05930172652006149 Accuracy 0.8403750658035278\n",
      "Iteration 11760 Training loss 0.0629647970199585 Validation loss 0.05935866758227348 Accuracy 0.8406250476837158\n",
      "Iteration 11770 Training loss 0.06253200024366379 Validation loss 0.05929557979106903 Accuracy 0.8406250476837158\n",
      "Iteration 11780 Training loss 0.06233308091759682 Validation loss 0.05933170020580292 Accuracy 0.8403750658035278\n",
      "Iteration 11790 Training loss 0.06556510180234909 Validation loss 0.059321023523807526 Accuracy 0.8406250476837158\n",
      "Iteration 11800 Training loss 0.059859514236450195 Validation loss 0.05926302447915077 Accuracy 0.8412500619888306\n",
      "Iteration 11810 Training loss 0.05641559511423111 Validation loss 0.059324439615011215 Accuracy 0.8408750295639038\n",
      "Iteration 11820 Training loss 0.06314655393362045 Validation loss 0.05926729738712311 Accuracy 0.8401250243186951\n",
      "Iteration 11830 Training loss 0.06223849207162857 Validation loss 0.059262972325086594 Accuracy 0.8403750658035278\n",
      "Iteration 11840 Training loss 0.056808970868587494 Validation loss 0.05925417318940163 Accuracy 0.8402500152587891\n",
      "Iteration 11850 Training loss 0.06101999059319496 Validation loss 0.05930561572313309 Accuracy 0.8391250371932983\n",
      "Iteration 11860 Training loss 0.060802459716796875 Validation loss 0.05923129618167877 Accuracy 0.8397500514984131\n",
      "Iteration 11870 Training loss 0.05158855393528938 Validation loss 0.05961492285132408 Accuracy 0.8396250605583191\n",
      "Iteration 11880 Training loss 0.051695480942726135 Validation loss 0.05957664176821709 Accuracy 0.8398750424385071\n",
      "Iteration 11890 Training loss 0.051883332431316376 Validation loss 0.059304285794496536 Accuracy 0.8397500514984131\n",
      "Iteration 11900 Training loss 0.054043129086494446 Validation loss 0.059248026460409164 Accuracy 0.8415000438690186\n",
      "Iteration 11910 Training loss 0.0577378086745739 Validation loss 0.05917147174477577 Accuracy 0.8418750166893005\n",
      "Iteration 11920 Training loss 0.060419850051403046 Validation loss 0.05918336659669876 Accuracy 0.8408750295639038\n",
      "Iteration 11930 Training loss 0.05796882510185242 Validation loss 0.0592324323952198 Accuracy 0.8401250243186951\n",
      "Iteration 11940 Training loss 0.05265704169869423 Validation loss 0.05913757532835007 Accuracy 0.8416250348091125\n",
      "Iteration 11950 Training loss 0.06272220611572266 Validation loss 0.05932608246803284 Accuracy 0.8402500152587891\n",
      "Iteration 11960 Training loss 0.05664999037981033 Validation loss 0.059134770184755325 Accuracy 0.8417500257492065\n",
      "Iteration 11970 Training loss 0.061749376356601715 Validation loss 0.05925510451197624 Accuracy 0.8410000205039978\n",
      "Iteration 11980 Training loss 0.056647151708602905 Validation loss 0.059193942695856094 Accuracy 0.8408750295639038\n",
      "Iteration 11990 Training loss 0.0576435923576355 Validation loss 0.0592113733291626 Accuracy 0.8410000205039978\n",
      "Iteration 12000 Training loss 0.054660335183143616 Validation loss 0.05930382385849953 Accuracy 0.8406250476837158\n",
      "Iteration 12010 Training loss 0.06293127685785294 Validation loss 0.05910452455282211 Accuracy 0.8410000205039978\n",
      "Iteration 12020 Training loss 0.05145423859357834 Validation loss 0.0590890534222126 Accuracy 0.8411250114440918\n",
      "Iteration 12030 Training loss 0.057985857129096985 Validation loss 0.059071846306324005 Accuracy 0.8417500257492065\n",
      "Iteration 12040 Training loss 0.05320802703499794 Validation loss 0.05981588363647461 Accuracy 0.8387500643730164\n",
      "Iteration 12050 Training loss 0.06279351562261581 Validation loss 0.059067387133836746 Accuracy 0.8422500491142273\n",
      "Iteration 12060 Training loss 0.05833040550351143 Validation loss 0.05912427976727486 Accuracy 0.8413750529289246\n",
      "Iteration 12070 Training loss 0.05900602042675018 Validation loss 0.059092357754707336 Accuracy 0.8415000438690186\n",
      "Iteration 12080 Training loss 0.0630417987704277 Validation loss 0.05910155922174454 Accuracy 0.8408750295639038\n",
      "Iteration 12090 Training loss 0.05773531273007393 Validation loss 0.05910491198301315 Accuracy 0.8413750529289246\n",
      "Iteration 12100 Training loss 0.051226403564214706 Validation loss 0.05906647816300392 Accuracy 0.8403750658035278\n",
      "Iteration 12110 Training loss 0.06076812744140625 Validation loss 0.05913807824254036 Accuracy 0.8415000438690186\n",
      "Iteration 12120 Training loss 0.05887981131672859 Validation loss 0.05907182767987251 Accuracy 0.8412500619888306\n",
      "Iteration 12130 Training loss 0.06197870150208473 Validation loss 0.05897746607661247 Accuracy 0.8423750400543213\n",
      "Iteration 12140 Training loss 0.05659956857562065 Validation loss 0.05908830091357231 Accuracy 0.8412500619888306\n",
      "Iteration 12150 Training loss 0.05826393514871597 Validation loss 0.059070881456136703 Accuracy 0.8417500257492065\n",
      "Iteration 12160 Training loss 0.06361042708158493 Validation loss 0.059011392295360565 Accuracy 0.8403750658035278\n",
      "Iteration 12170 Training loss 0.05611870810389519 Validation loss 0.05897551402449608 Accuracy 0.8418750166893005\n",
      "Iteration 12180 Training loss 0.0573270209133625 Validation loss 0.05906854569911957 Accuracy 0.8415000438690186\n",
      "Iteration 12190 Training loss 0.056516073644161224 Validation loss 0.058914944529533386 Accuracy 0.843250036239624\n",
      "Iteration 12200 Training loss 0.06810074299573898 Validation loss 0.058894503861665726 Accuracy 0.84312504529953\n",
      "Iteration 12210 Training loss 0.05387384444475174 Validation loss 0.058916639536619186 Accuracy 0.8417500257492065\n",
      "Iteration 12220 Training loss 0.05452428385615349 Validation loss 0.058939266949892044 Accuracy 0.8423750400543213\n",
      "Iteration 12230 Training loss 0.06493140012025833 Validation loss 0.05900095775723457 Accuracy 0.8416250348091125\n",
      "Iteration 12240 Training loss 0.05524349957704544 Validation loss 0.05884460732340813 Accuracy 0.8426250219345093\n",
      "Iteration 12250 Training loss 0.05683834105730057 Validation loss 0.05898968130350113 Accuracy 0.8413750529289246\n",
      "Iteration 12260 Training loss 0.05416315048933029 Validation loss 0.05891871079802513 Accuracy 0.8408750295639038\n",
      "Iteration 12270 Training loss 0.06632063537836075 Validation loss 0.058836184442043304 Accuracy 0.8427500128746033\n",
      "Iteration 12280 Training loss 0.06191160902380943 Validation loss 0.05885941535234451 Accuracy 0.8423750400543213\n",
      "Iteration 12290 Training loss 0.05867614597082138 Validation loss 0.05883662402629852 Accuracy 0.8423750400543213\n",
      "Iteration 12300 Training loss 0.06423872709274292 Validation loss 0.05884828791022301 Accuracy 0.8426250219345093\n",
      "Iteration 12310 Training loss 0.05277642980217934 Validation loss 0.05880998820066452 Accuracy 0.84312504529953\n",
      "Iteration 12320 Training loss 0.061959464102983475 Validation loss 0.05947897210717201 Accuracy 0.8391250371932983\n",
      "Iteration 12330 Training loss 0.05264019966125488 Validation loss 0.05881347879767418 Accuracy 0.8421250581741333\n",
      "Iteration 12340 Training loss 0.059841081500053406 Validation loss 0.058813340961933136 Accuracy 0.84312504529953\n",
      "Iteration 12350 Training loss 0.06190372258424759 Validation loss 0.05881837010383606 Accuracy 0.8421250581741333\n",
      "Iteration 12360 Training loss 0.06534549593925476 Validation loss 0.05884727090597153 Accuracy 0.8417500257492065\n",
      "Iteration 12370 Training loss 0.05983920022845268 Validation loss 0.05878531187772751 Accuracy 0.8418750166893005\n",
      "Iteration 12380 Training loss 0.057099368423223495 Validation loss 0.058805957436561584 Accuracy 0.8425000309944153\n",
      "Iteration 12390 Training loss 0.05400539189577103 Validation loss 0.059319622814655304 Accuracy 0.8403750658035278\n",
      "Iteration 12400 Training loss 0.05452634394168854 Validation loss 0.058870408684015274 Accuracy 0.8425000309944153\n",
      "Iteration 12410 Training loss 0.054678935557603836 Validation loss 0.05881287530064583 Accuracy 0.8425000309944153\n",
      "Iteration 12420 Training loss 0.05372314155101776 Validation loss 0.05878390744328499 Accuracy 0.8411250114440918\n",
      "Iteration 12430 Training loss 0.06112349033355713 Validation loss 0.05875670909881592 Accuracy 0.8408750295639038\n",
      "Iteration 12440 Training loss 0.05174078047275543 Validation loss 0.05874289199709892 Accuracy 0.8411250114440918\n",
      "Iteration 12450 Training loss 0.06354443728923798 Validation loss 0.05872802436351776 Accuracy 0.8423750400543213\n",
      "Iteration 12460 Training loss 0.05714985728263855 Validation loss 0.05873144790530205 Accuracy 0.8418750166893005\n",
      "Iteration 12470 Training loss 0.05971904471516609 Validation loss 0.05870119109749794 Accuracy 0.8437500596046448\n",
      "Iteration 12480 Training loss 0.06120891124010086 Validation loss 0.05871954560279846 Accuracy 0.8426250219345093\n",
      "Iteration 12490 Training loss 0.05285384878516197 Validation loss 0.05871303007006645 Accuracy 0.8426250219345093\n",
      "Iteration 12500 Training loss 0.06501125544309616 Validation loss 0.05869986116886139 Accuracy 0.843375027179718\n",
      "Iteration 12510 Training loss 0.04981962963938713 Validation loss 0.05871450901031494 Accuracy 0.8421250581741333\n",
      "Iteration 12520 Training loss 0.0639273077249527 Validation loss 0.05867641791701317 Accuracy 0.843375027179718\n",
      "Iteration 12530 Training loss 0.057947855442762375 Validation loss 0.05868217721581459 Accuracy 0.8437500596046448\n",
      "Iteration 12540 Training loss 0.060375671833753586 Validation loss 0.05868929624557495 Accuracy 0.843250036239624\n",
      "Iteration 12550 Training loss 0.05297744646668434 Validation loss 0.05874057114124298 Accuracy 0.842875063419342\n",
      "Iteration 12560 Training loss 0.05233514681458473 Validation loss 0.0586777999997139 Accuracy 0.843250036239624\n",
      "Iteration 12570 Training loss 0.060817621648311615 Validation loss 0.05868563801050186 Accuracy 0.8415000438690186\n",
      "Iteration 12580 Training loss 0.06293249130249023 Validation loss 0.058655306696891785 Accuracy 0.8421250581741333\n",
      "Iteration 12590 Training loss 0.056792326271533966 Validation loss 0.058726731687784195 Accuracy 0.8420000672340393\n",
      "Iteration 12600 Training loss 0.05822159722447395 Validation loss 0.0586218424141407 Accuracy 0.8420000672340393\n",
      "Iteration 12610 Training loss 0.054389819502830505 Validation loss 0.05862896144390106 Accuracy 0.8423750400543213\n",
      "Iteration 12620 Training loss 0.055246833711862564 Validation loss 0.05860046669840813 Accuracy 0.8441250324249268\n",
      "Iteration 12630 Training loss 0.054292354732751846 Validation loss 0.058952972292900085 Accuracy 0.8417500257492065\n",
      "Iteration 12640 Training loss 0.05075245350599289 Validation loss 0.05863356217741966 Accuracy 0.8422500491142273\n",
      "Iteration 12650 Training loss 0.06165735051035881 Validation loss 0.05858331173658371 Accuracy 0.84312504529953\n",
      "Iteration 12660 Training loss 0.06583625823259354 Validation loss 0.058553729206323624 Accuracy 0.8422500491142273\n",
      "Iteration 12670 Training loss 0.052788011729717255 Validation loss 0.05855758115649223 Accuracy 0.8426250219345093\n",
      "Iteration 12680 Training loss 0.057397861033678055 Validation loss 0.05857254937291145 Accuracy 0.8437500596046448\n",
      "Iteration 12690 Training loss 0.050326645374298096 Validation loss 0.058510832488536835 Accuracy 0.84312504529953\n",
      "Iteration 12700 Training loss 0.05380234122276306 Validation loss 0.05851226672530174 Accuracy 0.843500018119812\n",
      "Iteration 12710 Training loss 0.06400751322507858 Validation loss 0.058691974729299545 Accuracy 0.8425000309944153\n",
      "Iteration 12720 Training loss 0.0516887903213501 Validation loss 0.058476291596889496 Accuracy 0.84312504529953\n",
      "Iteration 12730 Training loss 0.06174628064036369 Validation loss 0.05844337120652199 Accuracy 0.8438750505447388\n",
      "Iteration 12740 Training loss 0.05140332132577896 Validation loss 0.05846097320318222 Accuracy 0.8436250686645508\n",
      "Iteration 12750 Training loss 0.05404318869113922 Validation loss 0.05843222141265869 Accuracy 0.8437500596046448\n",
      "Iteration 12760 Training loss 0.057802267372608185 Validation loss 0.058588866144418716 Accuracy 0.8427500128746033\n",
      "Iteration 12770 Training loss 0.06176012009382248 Validation loss 0.05842247232794762 Accuracy 0.8448750376701355\n",
      "Iteration 12780 Training loss 0.05482171103358269 Validation loss 0.058433715254068375 Accuracy 0.8446250557899475\n",
      "Iteration 12790 Training loss 0.05075351521372795 Validation loss 0.058420274406671524 Accuracy 0.8438750505447388\n",
      "Iteration 12800 Training loss 0.05870573967695236 Validation loss 0.058462101966142654 Accuracy 0.8437500596046448\n",
      "Iteration 12810 Training loss 0.05434601753950119 Validation loss 0.058397937566041946 Accuracy 0.8445000648498535\n",
      "Iteration 12820 Training loss 0.05911651626229286 Validation loss 0.05841876566410065 Accuracy 0.8443750143051147\n",
      "Iteration 12830 Training loss 0.05463787913322449 Validation loss 0.05845414847135544 Accuracy 0.842875063419342\n",
      "Iteration 12840 Training loss 0.04982564225792885 Validation loss 0.05844280868768692 Accuracy 0.8436250686645508\n",
      "Iteration 12850 Training loss 0.05691905319690704 Validation loss 0.058490149676799774 Accuracy 0.8436250686645508\n",
      "Iteration 12860 Training loss 0.06308627128601074 Validation loss 0.05868377164006233 Accuracy 0.84312504529953\n",
      "Iteration 12870 Training loss 0.06213495135307312 Validation loss 0.05886618420481682 Accuracy 0.8420000672340393\n",
      "Iteration 12880 Training loss 0.05737645551562309 Validation loss 0.058370865881443024 Accuracy 0.8436250686645508\n",
      "Iteration 12890 Training loss 0.06023643910884857 Validation loss 0.058672647923231125 Accuracy 0.842875063419342\n",
      "Iteration 12900 Training loss 0.0546838752925396 Validation loss 0.05854913219809532 Accuracy 0.8423750400543213\n",
      "Iteration 12910 Training loss 0.06183210760354996 Validation loss 0.05834213271737099 Accuracy 0.8438750505447388\n",
      "Iteration 12920 Training loss 0.05525066331028938 Validation loss 0.05834977328777313 Accuracy 0.8442500233650208\n",
      "Iteration 12930 Training loss 0.05807885527610779 Validation loss 0.05838114395737648 Accuracy 0.843500018119812\n",
      "Iteration 12940 Training loss 0.05826446786522865 Validation loss 0.058402273803949356 Accuracy 0.8442500233650208\n",
      "Iteration 12950 Training loss 0.0673767626285553 Validation loss 0.05827809125185013 Accuracy 0.8452500104904175\n",
      "Iteration 12960 Training loss 0.053074758499860764 Validation loss 0.058287136256694794 Accuracy 0.8440000414848328\n",
      "Iteration 12970 Training loss 0.05424336716532707 Validation loss 0.05828757584095001 Accuracy 0.8442500233650208\n",
      "Iteration 12980 Training loss 0.06347381323575974 Validation loss 0.058398425579071045 Accuracy 0.8440000414848328\n",
      "Iteration 12990 Training loss 0.06243934482336044 Validation loss 0.05828333646059036 Accuracy 0.8448750376701355\n",
      "Iteration 13000 Training loss 0.05686004459857941 Validation loss 0.05840613320469856 Accuracy 0.842875063419342\n",
      "Iteration 13010 Training loss 0.0587199330329895 Validation loss 0.05829516798257828 Accuracy 0.8448750376701355\n",
      "Iteration 13020 Training loss 0.057624444365501404 Validation loss 0.05829555541276932 Accuracy 0.8447500467300415\n",
      "Iteration 13030 Training loss 0.053140923380851746 Validation loss 0.05835741013288498 Accuracy 0.843500018119812\n",
      "Iteration 13040 Training loss 0.05092909187078476 Validation loss 0.058246638625860214 Accuracy 0.8448750376701355\n",
      "Iteration 13050 Training loss 0.06033682823181152 Validation loss 0.058299869298934937 Accuracy 0.8443750143051147\n",
      "Iteration 13060 Training loss 0.06879988312721252 Validation loss 0.058209337294101715 Accuracy 0.8440000414848328\n",
      "Iteration 13070 Training loss 0.05913669615983963 Validation loss 0.05819357931613922 Accuracy 0.8442500233650208\n",
      "Iteration 13080 Training loss 0.053538549691438675 Validation loss 0.05818483233451843 Accuracy 0.843250036239624\n",
      "Iteration 13090 Training loss 0.0544683113694191 Validation loss 0.058173585683107376 Accuracy 0.8442500233650208\n",
      "Iteration 13100 Training loss 0.06274347007274628 Validation loss 0.05817464366555214 Accuracy 0.8441250324249268\n",
      "Iteration 13110 Training loss 0.05648795887827873 Validation loss 0.05818569287657738 Accuracy 0.8442500233650208\n",
      "Iteration 13120 Training loss 0.056250713765621185 Validation loss 0.05830719321966171 Accuracy 0.843000054359436\n",
      "Iteration 13130 Training loss 0.06123415753245354 Validation loss 0.05819287523627281 Accuracy 0.8442500233650208\n",
      "Iteration 13140 Training loss 0.06021309643983841 Validation loss 0.05825043469667435 Accuracy 0.843000054359436\n",
      "Iteration 13150 Training loss 0.048743836581707 Validation loss 0.058262646198272705 Accuracy 0.8445000648498535\n",
      "Iteration 13160 Training loss 0.05852777510881424 Validation loss 0.058126144111156464 Accuracy 0.8442500233650208\n",
      "Iteration 13170 Training loss 0.05882059782743454 Validation loss 0.058129481971263885 Accuracy 0.843500018119812\n",
      "Iteration 13180 Training loss 0.06103420630097389 Validation loss 0.058111026883125305 Accuracy 0.8436250686645508\n",
      "Iteration 13190 Training loss 0.05898793414235115 Validation loss 0.05815569683909416 Accuracy 0.8438750505447388\n",
      "Iteration 13200 Training loss 0.0615302175283432 Validation loss 0.05817048251628876 Accuracy 0.8443750143051147\n",
      "Iteration 13210 Training loss 0.05550527572631836 Validation loss 0.05860484391450882 Accuracy 0.8423750400543213\n",
      "Iteration 13220 Training loss 0.056223392486572266 Validation loss 0.058161988854408264 Accuracy 0.8450000286102295\n",
      "Iteration 13230 Training loss 0.06698858737945557 Validation loss 0.05805889144539833 Accuracy 0.8456250429153442\n",
      "Iteration 13240 Training loss 0.0587894581258297 Validation loss 0.05806193873286247 Accuracy 0.8451250195503235\n",
      "Iteration 13250 Training loss 0.056541964411735535 Validation loss 0.05805679038167 Accuracy 0.8455000519752502\n",
      "Iteration 13260 Training loss 0.05113939940929413 Validation loss 0.05806327238678932 Accuracy 0.8450000286102295\n",
      "Iteration 13270 Training loss 0.061316099017858505 Validation loss 0.05802975967526436 Accuracy 0.8457500338554382\n",
      "Iteration 13280 Training loss 0.062058404088020325 Validation loss 0.058019112795591354 Accuracy 0.8448750376701355\n",
      "Iteration 13290 Training loss 0.06448782980442047 Validation loss 0.05815090611577034 Accuracy 0.8451250195503235\n",
      "Iteration 13300 Training loss 0.056448668241500854 Validation loss 0.05815541371703148 Accuracy 0.8445000648498535\n",
      "Iteration 13310 Training loss 0.07011017948389053 Validation loss 0.057994257658720016 Accuracy 0.8458750247955322\n",
      "Iteration 13320 Training loss 0.06023078411817551 Validation loss 0.05798742547631264 Accuracy 0.8452500104904175\n",
      "Iteration 13330 Training loss 0.0532526895403862 Validation loss 0.05801256746053696 Accuracy 0.846375048160553\n",
      "Iteration 13340 Training loss 0.05922871083021164 Validation loss 0.0582856684923172 Accuracy 0.8437500596046448\n",
      "Iteration 13350 Training loss 0.057277850806713104 Validation loss 0.057991378009319305 Accuracy 0.846375048160553\n",
      "Iteration 13360 Training loss 0.0700841024518013 Validation loss 0.057967912405729294 Accuracy 0.8451250195503235\n",
      "Iteration 13370 Training loss 0.05264822021126747 Validation loss 0.05800959840416908 Accuracy 0.8470000624656677\n",
      "Iteration 13380 Training loss 0.0521199069917202 Validation loss 0.057999175041913986 Accuracy 0.8476250171661377\n",
      "Iteration 13390 Training loss 0.0614243745803833 Validation loss 0.05804121494293213 Accuracy 0.846500039100647\n",
      "Iteration 13400 Training loss 0.059696536511182785 Validation loss 0.05790097266435623 Accuracy 0.8448750376701355\n",
      "Iteration 13410 Training loss 0.05688988417387009 Validation loss 0.058135997503995895 Accuracy 0.8445000648498535\n",
      "Iteration 13420 Training loss 0.06302334368228912 Validation loss 0.05788552388548851 Accuracy 0.8456250429153442\n",
      "Iteration 13430 Training loss 0.053768184036016464 Validation loss 0.05789712443947792 Accuracy 0.8452500104904175\n",
      "Iteration 13440 Training loss 0.05979907512664795 Validation loss 0.05809516832232475 Accuracy 0.8450000286102295\n",
      "Iteration 13450 Training loss 0.057068441063165665 Validation loss 0.05784318968653679 Accuracy 0.8448750376701355\n",
      "Iteration 13460 Training loss 0.05975469574332237 Validation loss 0.05786348506808281 Accuracy 0.8471250534057617\n",
      "Iteration 13470 Training loss 0.055989041924476624 Validation loss 0.05802244320511818 Accuracy 0.846125066280365\n",
      "Iteration 13480 Training loss 0.06399247795343399 Validation loss 0.05783698707818985 Accuracy 0.8450000286102295\n",
      "Iteration 13490 Training loss 0.06549648195505142 Validation loss 0.05784925818443298 Accuracy 0.8453750610351562\n",
      "Iteration 13500 Training loss 0.05868053436279297 Validation loss 0.0579078234732151 Accuracy 0.846125066280365\n",
      "Iteration 13510 Training loss 0.06374157965183258 Validation loss 0.05787232145667076 Accuracy 0.846125066280365\n",
      "Iteration 13520 Training loss 0.059746403247117996 Validation loss 0.057930175215005875 Accuracy 0.8470000624656677\n",
      "Iteration 13530 Training loss 0.06876780837774277 Validation loss 0.057901062071323395 Accuracy 0.8448750376701355\n",
      "Iteration 13540 Training loss 0.05276670679450035 Validation loss 0.05785476043820381 Accuracy 0.846125066280365\n",
      "Iteration 13550 Training loss 0.06262433528900146 Validation loss 0.057812776416540146 Accuracy 0.8460000157356262\n",
      "Iteration 13560 Training loss 0.0506092831492424 Validation loss 0.05780620127916336 Accuracy 0.8456250429153442\n",
      "Iteration 13570 Training loss 0.055864255875349045 Validation loss 0.05781731754541397 Accuracy 0.8457500338554382\n",
      "Iteration 13580 Training loss 0.0502786710858345 Validation loss 0.05780170112848282 Accuracy 0.8457500338554382\n",
      "Iteration 13590 Training loss 0.05802503973245621 Validation loss 0.05785596743226051 Accuracy 0.8456250429153442\n",
      "Iteration 13600 Training loss 0.06311824917793274 Validation loss 0.05850227549672127 Accuracy 0.8437500596046448\n",
      "Iteration 13610 Training loss 0.05086454376578331 Validation loss 0.05783892050385475 Accuracy 0.8460000157356262\n",
      "Iteration 13620 Training loss 0.053161878138780594 Validation loss 0.057768840342760086 Accuracy 0.846125066280365\n",
      "Iteration 13630 Training loss 0.055009875446558 Validation loss 0.057758670300245285 Accuracy 0.846125066280365\n",
      "Iteration 13640 Training loss 0.06127851456403732 Validation loss 0.05784233286976814 Accuracy 0.8460000157356262\n",
      "Iteration 13650 Training loss 0.0558026023209095 Validation loss 0.05798321217298508 Accuracy 0.8453750610351562\n",
      "Iteration 13660 Training loss 0.05008362606167793 Validation loss 0.05805572122335434 Accuracy 0.8456250429153442\n",
      "Iteration 13670 Training loss 0.06610589474439621 Validation loss 0.05772506445646286 Accuracy 0.8471250534057617\n",
      "Iteration 13680 Training loss 0.05137541517615318 Validation loss 0.05771743878722191 Accuracy 0.8481250405311584\n",
      "Iteration 13690 Training loss 0.05534294620156288 Validation loss 0.057710979133844376 Accuracy 0.8472500443458557\n",
      "Iteration 13700 Training loss 0.06366821378469467 Validation loss 0.05767546221613884 Accuracy 0.8457500338554382\n",
      "Iteration 13710 Training loss 0.0483214370906353 Validation loss 0.0576859787106514 Accuracy 0.846375048160553\n",
      "Iteration 13720 Training loss 0.0560297854244709 Validation loss 0.05770540609955788 Accuracy 0.846750020980835\n",
      "Iteration 13730 Training loss 0.049154751002788544 Validation loss 0.05799395218491554 Accuracy 0.8455000519752502\n",
      "Iteration 13740 Training loss 0.05831734463572502 Validation loss 0.0576942078769207 Accuracy 0.8470000624656677\n",
      "Iteration 13750 Training loss 0.05505950003862381 Validation loss 0.05767657980322838 Accuracy 0.8470000624656677\n",
      "Iteration 13760 Training loss 0.055355824530124664 Validation loss 0.05766401067376137 Accuracy 0.846250057220459\n",
      "Iteration 13770 Training loss 0.053236979991197586 Validation loss 0.057842448353767395 Accuracy 0.8458750247955322\n",
      "Iteration 13780 Training loss 0.05143672972917557 Validation loss 0.05767720192670822 Accuracy 0.8471250534057617\n",
      "Iteration 13790 Training loss 0.06303542852401733 Validation loss 0.057647354900836945 Accuracy 0.846250057220459\n",
      "Iteration 13800 Training loss 0.0639912411570549 Validation loss 0.057681821286678314 Accuracy 0.8472500443458557\n",
      "Iteration 13810 Training loss 0.056310221552848816 Validation loss 0.05771177262067795 Accuracy 0.8450000286102295\n",
      "Iteration 13820 Training loss 0.04965488240122795 Validation loss 0.057650964707136154 Accuracy 0.8477500677108765\n",
      "Iteration 13830 Training loss 0.0554688423871994 Validation loss 0.057772960513830185 Accuracy 0.846375048160553\n",
      "Iteration 13840 Training loss 0.06412137299776077 Validation loss 0.057693205773830414 Accuracy 0.8476250171661377\n",
      "Iteration 13850 Training loss 0.05145459994673729 Validation loss 0.05785255879163742 Accuracy 0.846375048160553\n",
      "Iteration 13860 Training loss 0.05502127483487129 Validation loss 0.05759444087743759 Accuracy 0.8455000519752502\n",
      "Iteration 13870 Training loss 0.05816440284252167 Validation loss 0.05762776359915733 Accuracy 0.8456250429153442\n",
      "Iteration 13880 Training loss 0.05950517579913139 Validation loss 0.05755623057484627 Accuracy 0.846125066280365\n",
      "Iteration 13890 Training loss 0.05637533217668533 Validation loss 0.057678475975990295 Accuracy 0.846875011920929\n",
      "Iteration 13900 Training loss 0.05881844460964203 Validation loss 0.05755995213985443 Accuracy 0.8456250429153442\n",
      "Iteration 13910 Training loss 0.05190311372280121 Validation loss 0.05756603181362152 Accuracy 0.8460000157356262\n",
      "Iteration 13920 Training loss 0.05226646363735199 Validation loss 0.05781600624322891 Accuracy 0.846500039100647\n",
      "Iteration 13930 Training loss 0.05126720294356346 Validation loss 0.057590335607528687 Accuracy 0.8470000624656677\n",
      "Iteration 13940 Training loss 0.053040388971567154 Validation loss 0.05755436047911644 Accuracy 0.8473750352859497\n",
      "Iteration 13950 Training loss 0.054317593574523926 Validation loss 0.057521045207977295 Accuracy 0.8457500338554382\n",
      "Iteration 13960 Training loss 0.04888003319501877 Validation loss 0.057603754103183746 Accuracy 0.8477500677108765\n",
      "Iteration 13970 Training loss 0.05606365576386452 Validation loss 0.057525698095560074 Accuracy 0.8472500443458557\n",
      "Iteration 13980 Training loss 0.061003487557172775 Validation loss 0.057498835027217865 Accuracy 0.8480000495910645\n",
      "Iteration 13990 Training loss 0.05835377424955368 Validation loss 0.05765678733587265 Accuracy 0.846500039100647\n",
      "Iteration 14000 Training loss 0.06077735871076584 Validation loss 0.05763007700443268 Accuracy 0.846500039100647\n",
      "Iteration 14010 Training loss 0.061044786125421524 Validation loss 0.057468995451927185 Accuracy 0.8471250534057617\n",
      "Iteration 14020 Training loss 0.058159392327070236 Validation loss 0.057469531893730164 Accuracy 0.8470000624656677\n",
      "Iteration 14030 Training loss 0.05228089168667793 Validation loss 0.057466812431812286 Accuracy 0.8470000624656677\n",
      "Iteration 14040 Training loss 0.059133775532245636 Validation loss 0.057457469403743744 Accuracy 0.8472500443458557\n",
      "Iteration 14050 Training loss 0.04864847660064697 Validation loss 0.05754098668694496 Accuracy 0.8471250534057617\n",
      "Iteration 14060 Training loss 0.053932350128889084 Validation loss 0.05741517245769501 Accuracy 0.846500039100647\n",
      "Iteration 14070 Training loss 0.05771635100245476 Validation loss 0.057415109127759933 Accuracy 0.846125066280365\n",
      "Iteration 14080 Training loss 0.06222159415483475 Validation loss 0.057637378573417664 Accuracy 0.846125066280365\n",
      "Iteration 14090 Training loss 0.05355292558670044 Validation loss 0.05745069310069084 Accuracy 0.8476250171661377\n",
      "Iteration 14100 Training loss 0.05954771861433983 Validation loss 0.05740859732031822 Accuracy 0.8470000624656677\n",
      "Iteration 14110 Training loss 0.05548526719212532 Validation loss 0.05740763992071152 Accuracy 0.8481250405311584\n",
      "Iteration 14120 Training loss 0.05840618908405304 Validation loss 0.057416364550590515 Accuracy 0.8471250534057617\n",
      "Iteration 14130 Training loss 0.06424427777528763 Validation loss 0.057406261563301086 Accuracy 0.8475000262260437\n",
      "Iteration 14140 Training loss 0.05621837452054024 Validation loss 0.05739757418632507 Accuracy 0.8472500443458557\n",
      "Iteration 14150 Training loss 0.06307999789714813 Validation loss 0.05738942325115204 Accuracy 0.8483750224113464\n",
      "Iteration 14160 Training loss 0.06194823980331421 Validation loss 0.057489026337862015 Accuracy 0.8475000262260437\n",
      "Iteration 14170 Training loss 0.06119373068213463 Validation loss 0.05740071088075638 Accuracy 0.8478750586509705\n",
      "Iteration 14180 Training loss 0.05641733482480049 Validation loss 0.0573992021381855 Accuracy 0.846625030040741\n",
      "Iteration 14190 Training loss 0.052630890160799026 Validation loss 0.05742907151579857 Accuracy 0.8477500677108765\n",
      "Iteration 14200 Training loss 0.06485334783792496 Validation loss 0.0574239082634449 Accuracy 0.8478750586509705\n",
      "Iteration 14210 Training loss 0.0499577522277832 Validation loss 0.057323817163705826 Accuracy 0.8475000262260437\n",
      "Iteration 14220 Training loss 0.054477594792842865 Validation loss 0.057482436299324036 Accuracy 0.846875011920929\n",
      "Iteration 14230 Training loss 0.0602039135992527 Validation loss 0.057695962488651276 Accuracy 0.846375048160553\n",
      "Iteration 14240 Training loss 0.0567605160176754 Validation loss 0.05737972632050514 Accuracy 0.8476250171661377\n",
      "Iteration 14250 Training loss 0.05905338004231453 Validation loss 0.05733712762594223 Accuracy 0.8471250534057617\n",
      "Iteration 14260 Training loss 0.06588367372751236 Validation loss 0.057283058762550354 Accuracy 0.8476250171661377\n",
      "Iteration 14270 Training loss 0.05252368003129959 Validation loss 0.057255037128925323 Accuracy 0.8481250405311584\n",
      "Iteration 14280 Training loss 0.060832131654024124 Validation loss 0.0573749765753746 Accuracy 0.8477500677108765\n",
      "Iteration 14290 Training loss 0.05622829496860504 Validation loss 0.057319026440382004 Accuracy 0.8482500314712524\n",
      "Iteration 14300 Training loss 0.04943879321217537 Validation loss 0.05768775939941406 Accuracy 0.846375048160553\n",
      "Iteration 14310 Training loss 0.05361798033118248 Validation loss 0.057606350630521774 Accuracy 0.846125066280365\n",
      "Iteration 14320 Training loss 0.058507613837718964 Validation loss 0.05726010724902153 Accuracy 0.8481250405311584\n",
      "Iteration 14330 Training loss 0.0573585070669651 Validation loss 0.05724390596151352 Accuracy 0.8477500677108765\n",
      "Iteration 14340 Training loss 0.05924595147371292 Validation loss 0.05735957622528076 Accuracy 0.8472500443458557\n",
      "Iteration 14350 Training loss 0.062302712351083755 Validation loss 0.05722098425030708 Accuracy 0.8483750224113464\n",
      "Iteration 14360 Training loss 0.0585189089179039 Validation loss 0.05728987976908684 Accuracy 0.8481250405311584\n",
      "Iteration 14370 Training loss 0.05127481371164322 Validation loss 0.057260408997535706 Accuracy 0.8478750586509705\n",
      "Iteration 14380 Training loss 0.05602960288524628 Validation loss 0.05723007768392563 Accuracy 0.8480000495910645\n",
      "Iteration 14390 Training loss 0.058769892901182175 Validation loss 0.0572098046541214 Accuracy 0.8488750457763672\n",
      "Iteration 14400 Training loss 0.06183589622378349 Validation loss 0.05729062855243683 Accuracy 0.8480000495910645\n",
      "Iteration 14410 Training loss 0.05988899990916252 Validation loss 0.05725519731640816 Accuracy 0.8490000367164612\n",
      "Iteration 14420 Training loss 0.05552653595805168 Validation loss 0.05718143656849861 Accuracy 0.8488750457763672\n",
      "Iteration 14430 Training loss 0.05419856309890747 Validation loss 0.057253215461969376 Accuracy 0.8487500548362732\n",
      "Iteration 14440 Training loss 0.05328434705734253 Validation loss 0.05717219039797783 Accuracy 0.8483750224113464\n",
      "Iteration 14450 Training loss 0.05995851755142212 Validation loss 0.057222988456487656 Accuracy 0.8483750224113464\n",
      "Iteration 14460 Training loss 0.054388854652643204 Validation loss 0.05720486491918564 Accuracy 0.8486250638961792\n",
      "Iteration 14470 Training loss 0.053926363587379456 Validation loss 0.057205941528081894 Accuracy 0.8488750457763672\n",
      "Iteration 14480 Training loss 0.056084975600242615 Validation loss 0.057603150606155396 Accuracy 0.8471250534057617\n",
      "Iteration 14490 Training loss 0.06110748276114464 Validation loss 0.05712103471159935 Accuracy 0.8492500185966492\n",
      "Iteration 14500 Training loss 0.061098143458366394 Validation loss 0.05730608478188515 Accuracy 0.8476250171661377\n",
      "Iteration 14510 Training loss 0.05579468980431557 Validation loss 0.057311952114105225 Accuracy 0.8475000262260437\n",
      "Iteration 14520 Training loss 0.06054425984621048 Validation loss 0.0572122223675251 Accuracy 0.8471250534057617\n",
      "Iteration 14530 Training loss 0.05747617036104202 Validation loss 0.05716119706630707 Accuracy 0.8485000133514404\n",
      "Iteration 14540 Training loss 0.04813329502940178 Validation loss 0.057103481143713 Accuracy 0.8491250276565552\n",
      "Iteration 14550 Training loss 0.05864565819501877 Validation loss 0.05709254741668701 Accuracy 0.8496250510215759\n",
      "Iteration 14560 Training loss 0.05731956660747528 Validation loss 0.05709515139460564 Accuracy 0.8487500548362732\n",
      "Iteration 14570 Training loss 0.0529182106256485 Validation loss 0.057155970484018326 Accuracy 0.8487500548362732\n",
      "Iteration 14580 Training loss 0.07215237617492676 Validation loss 0.0574851855635643 Accuracy 0.846125066280365\n",
      "Iteration 14590 Training loss 0.051724448800086975 Validation loss 0.057079415768384933 Accuracy 0.8485000133514404\n",
      "Iteration 14600 Training loss 0.06268635392189026 Validation loss 0.05708475038409233 Accuracy 0.8490000367164612\n",
      "Iteration 14610 Training loss 0.05277689918875694 Validation loss 0.05711711570620537 Accuracy 0.8485000133514404\n",
      "Iteration 14620 Training loss 0.06419043242931366 Validation loss 0.05711021646857262 Accuracy 0.8488750457763672\n",
      "Iteration 14630 Training loss 0.057626981288194656 Validation loss 0.05708855018019676 Accuracy 0.8486250638961792\n",
      "Iteration 14640 Training loss 0.05750510096549988 Validation loss 0.05715049430727959 Accuracy 0.8486250638961792\n",
      "Iteration 14650 Training loss 0.05513615533709526 Validation loss 0.05707141011953354 Accuracy 0.8478750586509705\n",
      "Iteration 14660 Training loss 0.05371912941336632 Validation loss 0.057093482464551926 Accuracy 0.8487500548362732\n",
      "Iteration 14670 Training loss 0.05802494287490845 Validation loss 0.05704960227012634 Accuracy 0.8490000367164612\n",
      "Iteration 14680 Training loss 0.060278329998254776 Validation loss 0.05702170729637146 Accuracy 0.8493750691413879\n",
      "Iteration 14690 Training loss 0.05675909295678139 Validation loss 0.0570223331451416 Accuracy 0.8495000600814819\n",
      "Iteration 14700 Training loss 0.05289310961961746 Validation loss 0.05703219771385193 Accuracy 0.8493750691413879\n",
      "Iteration 14710 Training loss 0.06832891702651978 Validation loss 0.05732933431863785 Accuracy 0.8470000624656677\n",
      "Iteration 14720 Training loss 0.05792330577969551 Validation loss 0.0570165291428566 Accuracy 0.8492500185966492\n",
      "Iteration 14730 Training loss 0.055289898067712784 Validation loss 0.0569966584444046 Accuracy 0.8492500185966492\n",
      "Iteration 14740 Training loss 0.06733982264995575 Validation loss 0.057147614657878876 Accuracy 0.8473750352859497\n",
      "Iteration 14750 Training loss 0.057320959866046906 Validation loss 0.05700288340449333 Accuracy 0.8492500185966492\n",
      "Iteration 14760 Training loss 0.052733954042196274 Validation loss 0.05698586627840996 Accuracy 0.8497500419616699\n",
      "Iteration 14770 Training loss 0.06167672947049141 Validation loss 0.0570426769554615 Accuracy 0.8486250638961792\n",
      "Iteration 14780 Training loss 0.05182238295674324 Validation loss 0.05698193982243538 Accuracy 0.8500000238418579\n",
      "Iteration 14790 Training loss 0.04967811331152916 Validation loss 0.056963931769132614 Accuracy 0.8506250381469727\n",
      "Iteration 14800 Training loss 0.063311867415905 Validation loss 0.05697900429368019 Accuracy 0.8496250510215759\n",
      "Iteration 14810 Training loss 0.06335554271936417 Validation loss 0.05711217597126961 Accuracy 0.8483750224113464\n",
      "Iteration 14820 Training loss 0.06211361661553383 Validation loss 0.05693250149488449 Accuracy 0.8498750329017639\n",
      "Iteration 14830 Training loss 0.050543367862701416 Validation loss 0.05696115270256996 Accuracy 0.8490000367164612\n",
      "Iteration 14840 Training loss 0.05933045223355293 Validation loss 0.05744984373450279 Accuracy 0.846750020980835\n",
      "Iteration 14850 Training loss 0.056300610303878784 Validation loss 0.056929104030132294 Accuracy 0.8506250381469727\n",
      "Iteration 14860 Training loss 0.06128522753715515 Validation loss 0.05717269703745842 Accuracy 0.8473750352859497\n",
      "Iteration 14870 Training loss 0.049153245985507965 Validation loss 0.05693308636546135 Accuracy 0.8496250510215759\n",
      "Iteration 14880 Training loss 0.053041234612464905 Validation loss 0.05691125616431236 Accuracy 0.8495000600814819\n",
      "Iteration 14890 Training loss 0.055537812411785126 Validation loss 0.05700300261378288 Accuracy 0.8492500185966492\n",
      "Iteration 14900 Training loss 0.05838390067219734 Validation loss 0.05690276622772217 Accuracy 0.8497500419616699\n",
      "Iteration 14910 Training loss 0.05723021551966667 Validation loss 0.05728738009929657 Accuracy 0.846750020980835\n",
      "Iteration 14920 Training loss 0.06153169646859169 Validation loss 0.056886255741119385 Accuracy 0.8497500419616699\n",
      "Iteration 14930 Training loss 0.05599317327141762 Validation loss 0.05691349506378174 Accuracy 0.8492500185966492\n",
      "Iteration 14940 Training loss 0.05348212271928787 Validation loss 0.05694034695625305 Accuracy 0.8495000600814819\n",
      "Iteration 14950 Training loss 0.0503363311290741 Validation loss 0.05690451338887215 Accuracy 0.8501250147819519\n",
      "Iteration 14960 Training loss 0.056245267391204834 Validation loss 0.056878700852394104 Accuracy 0.8508750200271606\n",
      "Iteration 14970 Training loss 0.05565037950873375 Validation loss 0.056934088468551636 Accuracy 0.8491250276565552\n",
      "Iteration 14980 Training loss 0.05755399540066719 Validation loss 0.05727877467870712 Accuracy 0.846750020980835\n",
      "Iteration 14990 Training loss 0.06202219799160957 Validation loss 0.05710689723491669 Accuracy 0.8481250405311584\n",
      "Iteration 15000 Training loss 0.05810694023966789 Validation loss 0.056851595640182495 Accuracy 0.8508750200271606\n",
      "Iteration 15010 Training loss 0.06031918525695801 Validation loss 0.056830745190382004 Accuracy 0.8508750200271606\n",
      "Iteration 15020 Training loss 0.05086357891559601 Validation loss 0.05688222870230675 Accuracy 0.8497500419616699\n",
      "Iteration 15030 Training loss 0.05835637450218201 Validation loss 0.05682393163442612 Accuracy 0.8505000472068787\n",
      "Iteration 15040 Training loss 0.052328143268823624 Validation loss 0.05697821453213692 Accuracy 0.8480000495910645\n",
      "Iteration 15050 Training loss 0.06061772629618645 Validation loss 0.056833431124687195 Accuracy 0.8507500290870667\n",
      "Iteration 15060 Training loss 0.05317375063896179 Validation loss 0.056907761842012405 Accuracy 0.8480000495910645\n",
      "Iteration 15070 Training loss 0.0590413473546505 Validation loss 0.05678891763091087 Accuracy 0.8515000343322754\n",
      "Iteration 15080 Training loss 0.05919872224330902 Validation loss 0.056760042905807495 Accuracy 0.8506250381469727\n",
      "Iteration 15090 Training loss 0.05612370744347572 Validation loss 0.056767258793115616 Accuracy 0.8502500653266907\n",
      "Iteration 15100 Training loss 0.0526127964258194 Validation loss 0.05673038586974144 Accuracy 0.8521250486373901\n",
      "Iteration 15110 Training loss 0.053870439529418945 Validation loss 0.05678068473935127 Accuracy 0.8488750457763672\n",
      "Iteration 15120 Training loss 0.05432213842868805 Validation loss 0.056859422475099564 Accuracy 0.8481250405311584\n",
      "Iteration 15130 Training loss 0.04693056643009186 Validation loss 0.0567716620862484 Accuracy 0.8500000238418579\n",
      "Iteration 15140 Training loss 0.05235231667757034 Validation loss 0.05671520531177521 Accuracy 0.8507500290870667\n",
      "Iteration 15150 Training loss 0.05187758430838585 Validation loss 0.056696366518735886 Accuracy 0.8506250381469727\n",
      "Iteration 15160 Training loss 0.041409868746995926 Validation loss 0.05668853223323822 Accuracy 0.8515000343322754\n",
      "Iteration 15170 Training loss 0.05714564025402069 Validation loss 0.056829821318387985 Accuracy 0.8490000367164612\n",
      "Iteration 15180 Training loss 0.060369424521923065 Validation loss 0.05666612833738327 Accuracy 0.8511250615119934\n",
      "Iteration 15190 Training loss 0.06349515169858932 Validation loss 0.05665060877799988 Accuracy 0.8516250252723694\n",
      "Iteration 15200 Training loss 0.05913815274834633 Validation loss 0.05667455494403839 Accuracy 0.8510000109672546\n",
      "Iteration 15210 Training loss 0.0601937472820282 Validation loss 0.056724268943071365 Accuracy 0.8497500419616699\n",
      "Iteration 15220 Training loss 0.06162368506193161 Validation loss 0.05677545815706253 Accuracy 0.8488750457763672\n",
      "Iteration 15230 Training loss 0.053943932056427 Validation loss 0.05740689858794212 Accuracy 0.8453750610351562\n",
      "Iteration 15240 Training loss 0.05483634024858475 Validation loss 0.056777290999889374 Accuracy 0.8498750329017639\n",
      "Iteration 15250 Training loss 0.04669015482068062 Validation loss 0.05684804916381836 Accuracy 0.8498750329017639\n",
      "Iteration 15260 Training loss 0.051240697503089905 Validation loss 0.05665970966219902 Accuracy 0.8517500162124634\n",
      "Iteration 15270 Training loss 0.05833104997873306 Validation loss 0.056926179677248 Accuracy 0.8491250276565552\n",
      "Iteration 15280 Training loss 0.053576644510030746 Validation loss 0.0567016564309597 Accuracy 0.8513750433921814\n",
      "Iteration 15290 Training loss 0.057853784412145615 Validation loss 0.056662265211343765 Accuracy 0.8512500524520874\n",
      "Iteration 15300 Training loss 0.05413365364074707 Validation loss 0.056589219719171524 Accuracy 0.8527500629425049\n",
      "Iteration 15310 Training loss 0.05607239529490471 Validation loss 0.05666666850447655 Accuracy 0.8501250147819519\n",
      "Iteration 15320 Training loss 0.053084611892700195 Validation loss 0.05656467005610466 Accuracy 0.8531250357627869\n",
      "Iteration 15330 Training loss 0.05978722497820854 Validation loss 0.056559037417173386 Accuracy 0.8526250123977661\n",
      "Iteration 15340 Training loss 0.05342685431241989 Validation loss 0.05655167996883392 Accuracy 0.8516250252723694\n",
      "Iteration 15350 Training loss 0.04895061254501343 Validation loss 0.05658228322863579 Accuracy 0.8502500653266907\n",
      "Iteration 15360 Training loss 0.062310151755809784 Validation loss 0.056542035192251205 Accuracy 0.8525000214576721\n",
      "Iteration 15370 Training loss 0.05418505519628525 Validation loss 0.056546494364738464 Accuracy 0.8513750433921814\n",
      "Iteration 15380 Training loss 0.05269230902194977 Validation loss 0.05668451264500618 Accuracy 0.8498750329017639\n",
      "Iteration 15390 Training loss 0.05502992868423462 Validation loss 0.05655103549361229 Accuracy 0.8520000576972961\n",
      "Iteration 15400 Training loss 0.05178726464509964 Validation loss 0.05658290162682533 Accuracy 0.8505000472068787\n",
      "Iteration 15410 Training loss 0.06025455892086029 Validation loss 0.05669450759887695 Accuracy 0.8498750329017639\n",
      "Iteration 15420 Training loss 0.04412991553544998 Validation loss 0.05656185373663902 Accuracy 0.8507500290870667\n",
      "Iteration 15430 Training loss 0.051082003861665726 Validation loss 0.056551042944192886 Accuracy 0.8517500162124634\n",
      "Iteration 15440 Training loss 0.046261295676231384 Validation loss 0.05661172419786453 Accuracy 0.8502500653266907\n",
      "Iteration 15450 Training loss 0.05451742187142372 Validation loss 0.05650106444954872 Accuracy 0.8525000214576721\n",
      "Iteration 15460 Training loss 0.05111785605549812 Validation loss 0.0564817450940609 Accuracy 0.8525000214576721\n",
      "Iteration 15470 Training loss 0.049245938658714294 Validation loss 0.056491222232580185 Accuracy 0.8515000343322754\n",
      "Iteration 15480 Training loss 0.05674992501735687 Validation loss 0.056474871933460236 Accuracy 0.8518750667572021\n",
      "Iteration 15490 Training loss 0.04917558655142784 Validation loss 0.05648002400994301 Accuracy 0.8521250486373901\n",
      "Iteration 15500 Training loss 0.05702344700694084 Validation loss 0.0564572811126709 Accuracy 0.8517500162124634\n",
      "Iteration 15510 Training loss 0.05333733558654785 Validation loss 0.056511636823415756 Accuracy 0.8511250615119934\n",
      "Iteration 15520 Training loss 0.054824844002723694 Validation loss 0.056454189121723175 Accuracy 0.8511250615119934\n",
      "Iteration 15530 Training loss 0.06538869440555573 Validation loss 0.056690096855163574 Accuracy 0.8506250381469727\n",
      "Iteration 15540 Training loss 0.05747606232762337 Validation loss 0.05647418648004532 Accuracy 0.8510000109672546\n",
      "Iteration 15550 Training loss 0.05816446244716644 Validation loss 0.05650961771607399 Accuracy 0.8507500290870667\n",
      "Iteration 15560 Training loss 0.04932991415262222 Validation loss 0.056461550295352936 Accuracy 0.8511250615119934\n",
      "Iteration 15570 Training loss 0.04996080324053764 Validation loss 0.0565343014895916 Accuracy 0.8495000600814819\n",
      "Iteration 15580 Training loss 0.06303846091032028 Validation loss 0.05647847428917885 Accuracy 0.8508750200271606\n",
      "Iteration 15590 Training loss 0.055342793464660645 Validation loss 0.056725986301898956 Accuracy 0.8492500185966492\n",
      "Iteration 15600 Training loss 0.049353837966918945 Validation loss 0.05665719881653786 Accuracy 0.8498750329017639\n",
      "Iteration 15610 Training loss 0.052260883152484894 Validation loss 0.056844860315322876 Accuracy 0.8492500185966492\n",
      "Iteration 15620 Training loss 0.06465581059455872 Validation loss 0.0565510131418705 Accuracy 0.8507500290870667\n",
      "Iteration 15630 Training loss 0.05341588705778122 Validation loss 0.05641510337591171 Accuracy 0.8511250615119934\n",
      "Iteration 15640 Training loss 0.05663025751709938 Validation loss 0.05653312802314758 Accuracy 0.8496250510215759\n",
      "Iteration 15650 Training loss 0.05113779753446579 Validation loss 0.05638786405324936 Accuracy 0.8517500162124634\n",
      "Iteration 15660 Training loss 0.055856846272945404 Validation loss 0.05640324577689171 Accuracy 0.8516250252723694\n",
      "Iteration 15670 Training loss 0.05734649673104286 Validation loss 0.05638619139790535 Accuracy 0.8517500162124634\n",
      "Iteration 15680 Training loss 0.05181765556335449 Validation loss 0.05667915567755699 Accuracy 0.8487500548362732\n",
      "Iteration 15690 Training loss 0.05782795697450638 Validation loss 0.05636502429842949 Accuracy 0.8530000448226929\n",
      "Iteration 15700 Training loss 0.05300593376159668 Validation loss 0.056459102779626846 Accuracy 0.8501250147819519\n",
      "Iteration 15710 Training loss 0.05336688831448555 Validation loss 0.05637332424521446 Accuracy 0.8523750305175781\n",
      "Iteration 15720 Training loss 0.05802193656563759 Validation loss 0.05635682865977287 Accuracy 0.8526250123977661\n",
      "Iteration 15730 Training loss 0.05440584197640419 Validation loss 0.056373462080955505 Accuracy 0.8520000576972961\n",
      "Iteration 15740 Training loss 0.06202490255236626 Validation loss 0.056386128067970276 Accuracy 0.8516250252723694\n",
      "Iteration 15750 Training loss 0.0500907264649868 Validation loss 0.05681460723280907 Accuracy 0.8485000133514404\n",
      "Iteration 15760 Training loss 0.05679130554199219 Validation loss 0.0563628114759922 Accuracy 0.8531250357627869\n",
      "Iteration 15770 Training loss 0.04904981330037117 Validation loss 0.056362736970186234 Accuracy 0.8506250381469727\n",
      "Iteration 15780 Training loss 0.06291746348142624 Validation loss 0.0565650649368763 Accuracy 0.8491250276565552\n",
      "Iteration 15790 Training loss 0.05370044708251953 Validation loss 0.056369632482528687 Accuracy 0.8517500162124634\n",
      "Iteration 15800 Training loss 0.05276806280016899 Validation loss 0.056469082832336426 Accuracy 0.8497500419616699\n",
      "Iteration 15810 Training loss 0.05056662857532501 Validation loss 0.05632631853222847 Accuracy 0.8516250252723694\n",
      "Iteration 15820 Training loss 0.054551463574171066 Validation loss 0.056345537304878235 Accuracy 0.8525000214576721\n",
      "Iteration 15830 Training loss 0.0538809560239315 Validation loss 0.056318849325180054 Accuracy 0.8528750538825989\n",
      "Iteration 15840 Training loss 0.06359194964170456 Validation loss 0.056362733244895935 Accuracy 0.8508750200271606\n",
      "Iteration 15850 Training loss 0.05973859503865242 Validation loss 0.05630286782979965 Accuracy 0.8521250486373901\n",
      "Iteration 15860 Training loss 0.05441899970173836 Validation loss 0.05636262893676758 Accuracy 0.8502500653266907\n",
      "Iteration 15870 Training loss 0.057233672589063644 Validation loss 0.056668445467948914 Accuracy 0.8486250638961792\n",
      "Iteration 15880 Training loss 0.0542779341340065 Validation loss 0.056222591549158096 Accuracy 0.8537500500679016\n",
      "Iteration 15890 Training loss 0.05233708396553993 Validation loss 0.05626029521226883 Accuracy 0.8520000576972961\n",
      "Iteration 15900 Training loss 0.055114924907684326 Validation loss 0.056225892156362534 Accuracy 0.8533750176429749\n",
      "Iteration 15910 Training loss 0.051819004118442535 Validation loss 0.05627445504069328 Accuracy 0.8511250615119934\n",
      "Iteration 15920 Training loss 0.058569762855768204 Validation loss 0.056258391588926315 Accuracy 0.8516250252723694\n",
      "Iteration 15930 Training loss 0.046569664031267166 Validation loss 0.056211650371551514 Accuracy 0.8525000214576721\n",
      "Iteration 15940 Training loss 0.0461277961730957 Validation loss 0.05618049576878548 Accuracy 0.8541250228881836\n",
      "Iteration 15950 Training loss 0.05118929594755173 Validation loss 0.05622788146138191 Accuracy 0.8525000214576721\n",
      "Iteration 15960 Training loss 0.05602974817156792 Validation loss 0.056187935173511505 Accuracy 0.8537500500679016\n",
      "Iteration 15970 Training loss 0.05870857089757919 Validation loss 0.05618753656744957 Accuracy 0.8536250591278076\n",
      "Iteration 15980 Training loss 0.05375251546502113 Validation loss 0.05676316097378731 Accuracy 0.8485000133514404\n",
      "Iteration 15990 Training loss 0.04835887625813484 Validation loss 0.056250255554914474 Accuracy 0.8513750433921814\n",
      "Iteration 16000 Training loss 0.05214225500822067 Validation loss 0.056248780339956284 Accuracy 0.8511250615119934\n",
      "Iteration 16010 Training loss 0.04967173561453819 Validation loss 0.05614418536424637 Accuracy 0.8536250591278076\n",
      "Iteration 16020 Training loss 0.04946610704064369 Validation loss 0.056229785084724426 Accuracy 0.8518750667572021\n",
      "Iteration 16030 Training loss 0.060709305107593536 Validation loss 0.05645168945193291 Accuracy 0.8498750329017639\n",
      "Iteration 16040 Training loss 0.06762981414794922 Validation loss 0.056279972195625305 Accuracy 0.8515000343322754\n",
      "Iteration 16050 Training loss 0.052556008100509644 Validation loss 0.05615769326686859 Accuracy 0.8535000681877136\n",
      "Iteration 16060 Training loss 0.04889510199427605 Validation loss 0.0562509261071682 Accuracy 0.8515000343322754\n",
      "Iteration 16070 Training loss 0.052267007529735565 Validation loss 0.05618860945105553 Accuracy 0.8516250252723694\n",
      "Iteration 16080 Training loss 0.05045692250132561 Validation loss 0.05615353211760521 Accuracy 0.8531250357627869\n",
      "Iteration 16090 Training loss 0.054498203098773956 Validation loss 0.05620463192462921 Accuracy 0.8515000343322754\n",
      "Iteration 16100 Training loss 0.05116927996277809 Validation loss 0.05625661835074425 Accuracy 0.8510000109672546\n",
      "Iteration 16110 Training loss 0.053924258798360825 Validation loss 0.056208569556474686 Accuracy 0.8516250252723694\n",
      "Iteration 16120 Training loss 0.05811182036995888 Validation loss 0.05612267553806305 Accuracy 0.8517500162124634\n",
      "Iteration 16130 Training loss 0.05235745757818222 Validation loss 0.05614330992102623 Accuracy 0.8512500524520874\n",
      "Iteration 16140 Training loss 0.05300459638237953 Validation loss 0.05612841993570328 Accuracy 0.8517500162124634\n",
      "Iteration 16150 Training loss 0.05311107635498047 Validation loss 0.05676387995481491 Accuracy 0.8480000495910645\n",
      "Iteration 16160 Training loss 0.0539029985666275 Validation loss 0.05617543309926987 Accuracy 0.8511250615119934\n",
      "Iteration 16170 Training loss 0.05697911977767944 Validation loss 0.05606972053647041 Accuracy 0.8518750667572021\n",
      "Iteration 16180 Training loss 0.0546734519302845 Validation loss 0.056084971874952316 Accuracy 0.8531250357627869\n",
      "Iteration 16190 Training loss 0.06023469939827919 Validation loss 0.05604809895157814 Accuracy 0.8532500267028809\n",
      "Iteration 16200 Training loss 0.048308875411748886 Validation loss 0.05606187880039215 Accuracy 0.8528750538825989\n",
      "Iteration 16210 Training loss 0.05921591445803642 Validation loss 0.056109603494405746 Accuracy 0.8526250123977661\n",
      "Iteration 16220 Training loss 0.05407604202628136 Validation loss 0.05604218319058418 Accuracy 0.8536250591278076\n",
      "Iteration 16230 Training loss 0.04904822260141373 Validation loss 0.056098852306604385 Accuracy 0.8522500395774841\n",
      "Iteration 16240 Training loss 0.059308987110853195 Validation loss 0.05627761781215668 Accuracy 0.8511250615119934\n",
      "Iteration 16250 Training loss 0.05541706830263138 Validation loss 0.05605296418070793 Accuracy 0.8533750176429749\n",
      "Iteration 16260 Training loss 0.05298725515604019 Validation loss 0.05607366934418678 Accuracy 0.8530000448226929\n",
      "Iteration 16270 Training loss 0.06454894691705704 Validation loss 0.056245967745780945 Accuracy 0.8496250510215759\n",
      "Iteration 16280 Training loss 0.05294905602931976 Validation loss 0.05601000413298607 Accuracy 0.8536250591278076\n",
      "Iteration 16290 Training loss 0.048510607331991196 Validation loss 0.05599364638328552 Accuracy 0.8532500267028809\n",
      "Iteration 16300 Training loss 0.05219988524913788 Validation loss 0.056105442345142365 Accuracy 0.8513750433921814\n",
      "Iteration 16310 Training loss 0.050322797149419785 Validation loss 0.055991534143686295 Accuracy 0.8533750176429749\n",
      "Iteration 16320 Training loss 0.05146535113453865 Validation loss 0.05608534440398216 Accuracy 0.8517500162124634\n",
      "Iteration 16330 Training loss 0.05724409967660904 Validation loss 0.055976495146751404 Accuracy 0.8538750410079956\n",
      "Iteration 16340 Training loss 0.06004810333251953 Validation loss 0.05599427968263626 Accuracy 0.8537500500679016\n",
      "Iteration 16350 Training loss 0.04861989989876747 Validation loss 0.056033551692962646 Accuracy 0.8526250123977661\n",
      "Iteration 16360 Training loss 0.04927133396267891 Validation loss 0.055975157767534256 Accuracy 0.8527500629425049\n",
      "Iteration 16370 Training loss 0.058516938239336014 Validation loss 0.056041110306978226 Accuracy 0.8520000576972961\n",
      "Iteration 16380 Training loss 0.05143364891409874 Validation loss 0.05593985319137573 Accuracy 0.8530000448226929\n",
      "Iteration 16390 Training loss 0.054960183799266815 Validation loss 0.05604451149702072 Accuracy 0.8518750667572021\n",
      "Iteration 16400 Training loss 0.05965317413210869 Validation loss 0.056002285331487656 Accuracy 0.8525000214576721\n",
      "Iteration 16410 Training loss 0.04765785112977028 Validation loss 0.05593664199113846 Accuracy 0.8528750538825989\n",
      "Iteration 16420 Training loss 0.05257507041096687 Validation loss 0.05611632019281387 Accuracy 0.8510000109672546\n",
      "Iteration 16430 Training loss 0.050087690353393555 Validation loss 0.05610604211688042 Accuracy 0.8512500524520874\n",
      "Iteration 16440 Training loss 0.052957210689783096 Validation loss 0.05592940375208855 Accuracy 0.8535000681877136\n",
      "Iteration 16450 Training loss 0.055414337664842606 Validation loss 0.05595950409770012 Accuracy 0.8518750667572021\n",
      "Iteration 16460 Training loss 0.044933538883924484 Validation loss 0.0559152327477932 Accuracy 0.8530000448226929\n",
      "Iteration 16470 Training loss 0.05609128996729851 Validation loss 0.056078217923641205 Accuracy 0.8503750562667847\n",
      "Iteration 16480 Training loss 0.05765479430556297 Validation loss 0.0559120737016201 Accuracy 0.8545000553131104\n",
      "Iteration 16490 Training loss 0.052871912717819214 Validation loss 0.05603303015232086 Accuracy 0.8523750305175781\n",
      "Iteration 16500 Training loss 0.05248846486210823 Validation loss 0.05626391991972923 Accuracy 0.8518750667572021\n",
      "Iteration 16510 Training loss 0.05656302347779274 Validation loss 0.05644436925649643 Accuracy 0.8498750329017639\n",
      "Iteration 16520 Training loss 0.05727056413888931 Validation loss 0.05586889013648033 Accuracy 0.8538750410079956\n",
      "Iteration 16530 Training loss 0.04455440491437912 Validation loss 0.05637520179152489 Accuracy 0.8496250510215759\n",
      "Iteration 16540 Training loss 0.05882192403078079 Validation loss 0.05585088953375816 Accuracy 0.8537500500679016\n",
      "Iteration 16550 Training loss 0.05996266379952431 Validation loss 0.05586248263716698 Accuracy 0.8540000319480896\n",
      "Iteration 16560 Training loss 0.05277664214372635 Validation loss 0.055901192128658295 Accuracy 0.8527500629425049\n",
      "Iteration 16570 Training loss 0.0633918046951294 Validation loss 0.05580637603998184 Accuracy 0.8537500500679016\n",
      "Iteration 16580 Training loss 0.06572610139846802 Validation loss 0.05619525909423828 Accuracy 0.8508750200271606\n",
      "Iteration 16590 Training loss 0.059724483639001846 Validation loss 0.05579296499490738 Accuracy 0.8542500138282776\n",
      "Iteration 16600 Training loss 0.05332329124212265 Validation loss 0.05582153797149658 Accuracy 0.8538750410079956\n",
      "Iteration 16610 Training loss 0.05765558406710625 Validation loss 0.05581643059849739 Accuracy 0.8543750643730164\n",
      "Iteration 16620 Training loss 0.05110381916165352 Validation loss 0.05577964335680008 Accuracy 0.8545000553131104\n",
      "Iteration 16630 Training loss 0.05882362276315689 Validation loss 0.055766962468624115 Accuracy 0.8541250228881836\n",
      "Iteration 16640 Training loss 0.06977315992116928 Validation loss 0.05577797815203667 Accuracy 0.8541250228881836\n",
      "Iteration 16650 Training loss 0.05732119455933571 Validation loss 0.055768635123968124 Accuracy 0.8545000553131104\n",
      "Iteration 16660 Training loss 0.04978556931018829 Validation loss 0.05613341182470322 Accuracy 0.8500000238418579\n",
      "Iteration 16670 Training loss 0.04838040843605995 Validation loss 0.05583500117063522 Accuracy 0.8535000681877136\n",
      "Iteration 16680 Training loss 0.053500037640333176 Validation loss 0.05577469244599342 Accuracy 0.8541250228881836\n",
      "Iteration 16690 Training loss 0.05466608703136444 Validation loss 0.055762879550457 Accuracy 0.8541250228881836\n",
      "Iteration 16700 Training loss 0.049244172871112823 Validation loss 0.05610235035419464 Accuracy 0.8493750691413879\n",
      "Iteration 16710 Training loss 0.05547718703746796 Validation loss 0.05577385798096657 Accuracy 0.8543750643730164\n",
      "Iteration 16720 Training loss 0.05852997303009033 Validation loss 0.05571579560637474 Accuracy 0.8543750643730164\n",
      "Iteration 16730 Training loss 0.05342923104763031 Validation loss 0.05591011419892311 Accuracy 0.8521250486373901\n",
      "Iteration 16740 Training loss 0.048819731920957565 Validation loss 0.05611962825059891 Accuracy 0.8511250615119934\n",
      "Iteration 16750 Training loss 0.05548249930143356 Validation loss 0.055697694420814514 Accuracy 0.8545000553131104\n",
      "Iteration 16760 Training loss 0.06116986274719238 Validation loss 0.055740609765052795 Accuracy 0.8538750410079956\n",
      "Iteration 16770 Training loss 0.04961534962058067 Validation loss 0.05580548942089081 Accuracy 0.8532500267028809\n",
      "Iteration 16780 Training loss 0.054253432899713516 Validation loss 0.05571252852678299 Accuracy 0.8547500371932983\n",
      "Iteration 16790 Training loss 0.05661344900727272 Validation loss 0.05594749376177788 Accuracy 0.8517500162124634\n",
      "Iteration 16800 Training loss 0.048558760434389114 Validation loss 0.0557672455906868 Accuracy 0.8536250591278076\n",
      "Iteration 16810 Training loss 0.05342831835150719 Validation loss 0.055678024888038635 Accuracy 0.8541250228881836\n",
      "Iteration 16820 Training loss 0.057984787970781326 Validation loss 0.055801909416913986 Accuracy 0.8521250486373901\n",
      "Iteration 16830 Training loss 0.06380724161863327 Validation loss 0.0556814968585968 Accuracy 0.8533750176429749\n",
      "Iteration 16840 Training loss 0.057683832943439484 Validation loss 0.05565302446484566 Accuracy 0.8540000319480896\n",
      "Iteration 16850 Training loss 0.05353529006242752 Validation loss 0.05563036724925041 Accuracy 0.8542500138282776\n",
      "Iteration 16860 Training loss 0.05320069193840027 Validation loss 0.05570080131292343 Accuracy 0.8540000319480896\n",
      "Iteration 16870 Training loss 0.04632050544023514 Validation loss 0.05561821162700653 Accuracy 0.8545000553131104\n",
      "Iteration 16880 Training loss 0.054050855338573456 Validation loss 0.05568506941199303 Accuracy 0.8540000319480896\n",
      "Iteration 16890 Training loss 0.0459870770573616 Validation loss 0.05565890297293663 Accuracy 0.8538750410079956\n",
      "Iteration 16900 Training loss 0.05161772668361664 Validation loss 0.05567988008260727 Accuracy 0.8538750410079956\n",
      "Iteration 16910 Training loss 0.04992513731122017 Validation loss 0.05558416247367859 Accuracy 0.8543750643730164\n",
      "Iteration 16920 Training loss 0.05681465566158295 Validation loss 0.05558108538389206 Accuracy 0.8551250696182251\n",
      "Iteration 16930 Training loss 0.04334833845496178 Validation loss 0.055574771016836166 Accuracy 0.8550000190734863\n",
      "Iteration 16940 Training loss 0.05299248546361923 Validation loss 0.055632904171943665 Accuracy 0.8527500629425049\n",
      "Iteration 16950 Training loss 0.05733388662338257 Validation loss 0.055634837597608566 Accuracy 0.8536250591278076\n",
      "Iteration 16960 Training loss 0.053123630583286285 Validation loss 0.05560992285609245 Accuracy 0.8541250228881836\n",
      "Iteration 16970 Training loss 0.0604349784553051 Validation loss 0.05554790794849396 Accuracy 0.8541250228881836\n",
      "Iteration 16980 Training loss 0.052649516612291336 Validation loss 0.05553973838686943 Accuracy 0.8543750643730164\n",
      "Iteration 16990 Training loss 0.05876313894987106 Validation loss 0.05556720867753029 Accuracy 0.8543750643730164\n",
      "Iteration 17000 Training loss 0.050004616379737854 Validation loss 0.055714964866638184 Accuracy 0.8521250486373901\n",
      "Iteration 17010 Training loss 0.05426415428519249 Validation loss 0.05556991696357727 Accuracy 0.8550000190734863\n",
      "Iteration 17020 Training loss 0.04870321601629257 Validation loss 0.05561244860291481 Accuracy 0.8541250228881836\n",
      "Iteration 17030 Training loss 0.04743476212024689 Validation loss 0.05563966929912567 Accuracy 0.8533750176429749\n",
      "Iteration 17040 Training loss 0.055708497762680054 Validation loss 0.05556269735097885 Accuracy 0.8547500371932983\n",
      "Iteration 17050 Training loss 0.05423632636666298 Validation loss 0.055538762360811234 Accuracy 0.8543750643730164\n",
      "Iteration 17060 Training loss 0.0586700513958931 Validation loss 0.055832233279943466 Accuracy 0.8508750200271606\n",
      "Iteration 17070 Training loss 0.05988175794482231 Validation loss 0.05551595613360405 Accuracy 0.8545000553131104\n",
      "Iteration 17080 Training loss 0.0546836256980896 Validation loss 0.055538471788167953 Accuracy 0.8548750281333923\n",
      "Iteration 17090 Training loss 0.06315702944993973 Validation loss 0.05552350729703903 Accuracy 0.8552500605583191\n",
      "Iteration 17100 Training loss 0.053154680877923965 Validation loss 0.05549550801515579 Accuracy 0.8546250462532043\n",
      "Iteration 17110 Training loss 0.04984678328037262 Validation loss 0.055530160665512085 Accuracy 0.8543750643730164\n",
      "Iteration 17120 Training loss 0.05589773878455162 Validation loss 0.055948853492736816 Accuracy 0.8522500395774841\n",
      "Iteration 17130 Training loss 0.04946041852235794 Validation loss 0.055507052689790726 Accuracy 0.8538750410079956\n",
      "Iteration 17140 Training loss 0.0610772967338562 Validation loss 0.055482737720012665 Accuracy 0.8537500500679016\n",
      "Iteration 17150 Training loss 0.05515842139720917 Validation loss 0.05552499741315842 Accuracy 0.8538750410079956\n",
      "Iteration 17160 Training loss 0.05909609794616699 Validation loss 0.05552660673856735 Accuracy 0.8541250228881836\n",
      "Iteration 17170 Training loss 0.05933982878923416 Validation loss 0.055619075894355774 Accuracy 0.8525000214576721\n",
      "Iteration 17180 Training loss 0.05885617434978485 Validation loss 0.05548561364412308 Accuracy 0.8542500138282776\n",
      "Iteration 17190 Training loss 0.05297017842531204 Validation loss 0.05562926456332207 Accuracy 0.8527500629425049\n",
      "Iteration 17200 Training loss 0.04719769209623337 Validation loss 0.055574946105480194 Accuracy 0.8533750176429749\n",
      "Iteration 17210 Training loss 0.057447269558906555 Validation loss 0.05543695390224457 Accuracy 0.8543750643730164\n",
      "Iteration 17220 Training loss 0.05091268569231033 Validation loss 0.05544394254684448 Accuracy 0.8542500138282776\n",
      "Iteration 17230 Training loss 0.058737415820360184 Validation loss 0.05541650205850601 Accuracy 0.8548750281333923\n",
      "Iteration 17240 Training loss 0.05046224594116211 Validation loss 0.05541887879371643 Accuracy 0.8541250228881836\n",
      "Iteration 17250 Training loss 0.051602672785520554 Validation loss 0.055428918451070786 Accuracy 0.8542500138282776\n",
      "Iteration 17260 Training loss 0.05661085620522499 Validation loss 0.05541227385401726 Accuracy 0.8548750281333923\n",
      "Iteration 17270 Training loss 0.05553159490227699 Validation loss 0.05542288348078728 Accuracy 0.8545000553131104\n",
      "Iteration 17280 Training loss 0.05536237731575966 Validation loss 0.055403932929039 Accuracy 0.8550000190734863\n",
      "Iteration 17290 Training loss 0.054655950516462326 Validation loss 0.05538869649171829 Accuracy 0.8547500371932983\n",
      "Iteration 17300 Training loss 0.05640431120991707 Validation loss 0.05540198087692261 Accuracy 0.8547500371932983\n",
      "Iteration 17310 Training loss 0.06194661185145378 Validation loss 0.05550337955355644 Accuracy 0.8528750538825989\n",
      "Iteration 17320 Training loss 0.04821933060884476 Validation loss 0.05544283241033554 Accuracy 0.8527500629425049\n",
      "Iteration 17330 Training loss 0.05885454639792442 Validation loss 0.05544167757034302 Accuracy 0.8540000319480896\n",
      "Iteration 17340 Training loss 0.043565452098846436 Validation loss 0.055348701775074005 Accuracy 0.8555000424385071\n",
      "Iteration 17350 Training loss 0.0493382029235363 Validation loss 0.05538806691765785 Accuracy 0.8546250462532043\n",
      "Iteration 17360 Training loss 0.05683385580778122 Validation loss 0.055567704141139984 Accuracy 0.8531250357627869\n",
      "Iteration 17370 Training loss 0.04911818355321884 Validation loss 0.05542684346437454 Accuracy 0.8537500500679016\n",
      "Iteration 17380 Training loss 0.05330396071076393 Validation loss 0.05563434585928917 Accuracy 0.8516250252723694\n",
      "Iteration 17390 Training loss 0.05315537378191948 Validation loss 0.05536169558763504 Accuracy 0.8545000553131104\n",
      "Iteration 17400 Training loss 0.0488680936396122 Validation loss 0.05535626411437988 Accuracy 0.8540000319480896\n",
      "Iteration 17410 Training loss 0.05905551835894585 Validation loss 0.055528368800878525 Accuracy 0.8523750305175781\n",
      "Iteration 17420 Training loss 0.05809426307678223 Validation loss 0.0554562471807003 Accuracy 0.8527500629425049\n",
      "Iteration 17430 Training loss 0.05673269182443619 Validation loss 0.05569871887564659 Accuracy 0.8516250252723694\n",
      "Iteration 17440 Training loss 0.05592725798487663 Validation loss 0.05529032275080681 Accuracy 0.8547500371932983\n",
      "Iteration 17450 Training loss 0.06310810148715973 Validation loss 0.055286165326833725 Accuracy 0.8553750514984131\n",
      "Iteration 17460 Training loss 0.05884392559528351 Validation loss 0.055415909737348557 Accuracy 0.8540000319480896\n",
      "Iteration 17470 Training loss 0.0546085387468338 Validation loss 0.055292438715696335 Accuracy 0.8561250567436218\n",
      "Iteration 17480 Training loss 0.06155714392662048 Validation loss 0.05533652380108833 Accuracy 0.8551250696182251\n",
      "Iteration 17490 Training loss 0.05900534614920616 Validation loss 0.05562527850270271 Accuracy 0.8512500524520874\n",
      "Iteration 17500 Training loss 0.05169422924518585 Validation loss 0.05528324097394943 Accuracy 0.8557500243186951\n",
      "Iteration 17510 Training loss 0.04853259027004242 Validation loss 0.055322062224149704 Accuracy 0.8553750514984131\n",
      "Iteration 17520 Training loss 0.05472668632864952 Validation loss 0.055270250886678696 Accuracy 0.8550000190734863\n",
      "Iteration 17530 Training loss 0.05705254524946213 Validation loss 0.05526584014296532 Accuracy 0.8556250333786011\n",
      "Iteration 17540 Training loss 0.056716859340667725 Validation loss 0.05527632683515549 Accuracy 0.8562500476837158\n",
      "Iteration 17550 Training loss 0.04834609106183052 Validation loss 0.05532281845808029 Accuracy 0.8542500138282776\n",
      "Iteration 17560 Training loss 0.04474997892975807 Validation loss 0.05527693033218384 Accuracy 0.8547500371932983\n",
      "Iteration 17570 Training loss 0.053882844746112823 Validation loss 0.05537218973040581 Accuracy 0.8532500267028809\n",
      "Iteration 17580 Training loss 0.05229899659752846 Validation loss 0.05525275692343712 Accuracy 0.8545000553131104\n",
      "Iteration 17590 Training loss 0.05907301604747772 Validation loss 0.05526388809084892 Accuracy 0.8548750281333923\n",
      "Iteration 17600 Training loss 0.049014363437891006 Validation loss 0.05527959764003754 Accuracy 0.8537500500679016\n",
      "Iteration 17610 Training loss 0.05027416720986366 Validation loss 0.05542130768299103 Accuracy 0.8528750538825989\n",
      "Iteration 17620 Training loss 0.05257626250386238 Validation loss 0.055285822600126266 Accuracy 0.8536250591278076\n",
      "Iteration 17630 Training loss 0.056361522525548935 Validation loss 0.055329445749521255 Accuracy 0.8537500500679016\n",
      "Iteration 17640 Training loss 0.05968104302883148 Validation loss 0.05544590204954147 Accuracy 0.8523750305175781\n",
      "Iteration 17650 Training loss 0.052561063319444656 Validation loss 0.055199358612298965 Accuracy 0.8546250462532043\n",
      "Iteration 17660 Training loss 0.05366681516170502 Validation loss 0.05565924197435379 Accuracy 0.8517500162124634\n",
      "Iteration 17670 Training loss 0.05859231948852539 Validation loss 0.05529889091849327 Accuracy 0.8532500267028809\n",
      "Iteration 17680 Training loss 0.044172219932079315 Validation loss 0.05524969846010208 Accuracy 0.8550000190734863\n",
      "Iteration 17690 Training loss 0.04344560578465462 Validation loss 0.055220142006874084 Accuracy 0.8538750410079956\n",
      "Iteration 17700 Training loss 0.055657919496297836 Validation loss 0.05538361147046089 Accuracy 0.8532500267028809\n",
      "Iteration 17710 Training loss 0.05749332904815674 Validation loss 0.05518519878387451 Accuracy 0.8546250462532043\n",
      "Iteration 17720 Training loss 0.05822042375802994 Validation loss 0.05518811196088791 Accuracy 0.8542500138282776\n",
      "Iteration 17730 Training loss 0.05212012305855751 Validation loss 0.05523372069001198 Accuracy 0.8532500267028809\n",
      "Iteration 17740 Training loss 0.04928514361381531 Validation loss 0.055168747901916504 Accuracy 0.8546250462532043\n",
      "Iteration 17750 Training loss 0.04333190992474556 Validation loss 0.05518689379096031 Accuracy 0.8538750410079956\n",
      "Iteration 17760 Training loss 0.05559141933917999 Validation loss 0.05517854169011116 Accuracy 0.8546250462532043\n",
      "Iteration 17770 Training loss 0.05117868259549141 Validation loss 0.055239517241716385 Accuracy 0.8533750176429749\n",
      "Iteration 17780 Training loss 0.051032159477472305 Validation loss 0.05518997460603714 Accuracy 0.8540000319480896\n",
      "Iteration 17790 Training loss 0.05427783355116844 Validation loss 0.055134519934654236 Accuracy 0.8551250696182251\n",
      "Iteration 17800 Training loss 0.04345834255218506 Validation loss 0.0555880144238472 Accuracy 0.8516250252723694\n",
      "Iteration 17810 Training loss 0.05608726292848587 Validation loss 0.05513886734843254 Accuracy 0.8545000553131104\n",
      "Iteration 17820 Training loss 0.05259397253394127 Validation loss 0.0554409883916378 Accuracy 0.8520000576972961\n",
      "Iteration 17830 Training loss 0.0544821061193943 Validation loss 0.05514201521873474 Accuracy 0.8553750514984131\n",
      "Iteration 17840 Training loss 0.0556262731552124 Validation loss 0.055117372423410416 Accuracy 0.8560000658035278\n",
      "Iteration 17850 Training loss 0.05192266404628754 Validation loss 0.05546318739652634 Accuracy 0.8530000448226929\n",
      "Iteration 17860 Training loss 0.05139211565256119 Validation loss 0.05536308512091637 Accuracy 0.8531250357627869\n",
      "Iteration 17870 Training loss 0.05442716181278229 Validation loss 0.055195439606904984 Accuracy 0.8536250591278076\n",
      "Iteration 17880 Training loss 0.056876812130212784 Validation loss 0.05507863312959671 Accuracy 0.8552500605583191\n",
      "Iteration 17890 Training loss 0.054629307240247726 Validation loss 0.055094435811042786 Accuracy 0.8561250567436218\n",
      "Iteration 17900 Training loss 0.054916009306907654 Validation loss 0.05507584661245346 Accuracy 0.8560000658035278\n",
      "Iteration 17910 Training loss 0.05320396646857262 Validation loss 0.05531534552574158 Accuracy 0.8535000681877136\n",
      "Iteration 17920 Training loss 0.05619223043322563 Validation loss 0.0551149807870388 Accuracy 0.8551250696182251\n",
      "Iteration 17930 Training loss 0.04667448252439499 Validation loss 0.0550728477537632 Accuracy 0.8566250205039978\n",
      "Iteration 17940 Training loss 0.04916653409600258 Validation loss 0.05509514361619949 Accuracy 0.8555000424385071\n",
      "Iteration 17950 Training loss 0.05403786525130272 Validation loss 0.055074021220207214 Accuracy 0.8550000190734863\n",
      "Iteration 17960 Training loss 0.05276230722665787 Validation loss 0.055423084646463394 Accuracy 0.8535000681877136\n",
      "Iteration 17970 Training loss 0.053640563040971756 Validation loss 0.05510224401950836 Accuracy 0.8542500138282776\n",
      "Iteration 17980 Training loss 0.059545304626226425 Validation loss 0.05515160411596298 Accuracy 0.8530000448226929\n",
      "Iteration 17990 Training loss 0.04384644329547882 Validation loss 0.05506076291203499 Accuracy 0.8548750281333923\n",
      "Iteration 18000 Training loss 0.050521593540906906 Validation loss 0.05505188927054405 Accuracy 0.8553750514984131\n",
      "Iteration 18010 Training loss 0.055049359798431396 Validation loss 0.05501273646950722 Accuracy 0.8548750281333923\n",
      "Iteration 18020 Training loss 0.05064908042550087 Validation loss 0.055304765701293945 Accuracy 0.8517500162124634\n",
      "Iteration 18030 Training loss 0.0544544979929924 Validation loss 0.0550566092133522 Accuracy 0.8565000295639038\n",
      "Iteration 18040 Training loss 0.05126186087727547 Validation loss 0.05501072108745575 Accuracy 0.8562500476837158\n",
      "Iteration 18050 Training loss 0.04822699353098869 Validation loss 0.05499701201915741 Accuracy 0.8553750514984131\n",
      "Iteration 18060 Training loss 0.059048864990472794 Validation loss 0.05500102415680885 Accuracy 0.8563750386238098\n",
      "Iteration 18070 Training loss 0.05607728660106659 Validation loss 0.05500207468867302 Accuracy 0.8562500476837158\n",
      "Iteration 18080 Training loss 0.05529998615384102 Validation loss 0.055045682936906815 Accuracy 0.8552500605583191\n",
      "Iteration 18090 Training loss 0.05892748758196831 Validation loss 0.055220723152160645 Accuracy 0.8528750538825989\n",
      "Iteration 18100 Training loss 0.06300762295722961 Validation loss 0.054993655532598495 Accuracy 0.8548750281333923\n",
      "Iteration 18110 Training loss 0.05370130389928818 Validation loss 0.055187828838825226 Accuracy 0.8527500629425049\n",
      "Iteration 18120 Training loss 0.049563661217689514 Validation loss 0.05493674799799919 Accuracy 0.8565000295639038\n",
      "Iteration 18130 Training loss 0.061448171734809875 Validation loss 0.05496245250105858 Accuracy 0.8553750514984131\n",
      "Iteration 18140 Training loss 0.05786534771323204 Validation loss 0.055006857961416245 Accuracy 0.8553750514984131\n",
      "Iteration 18150 Training loss 0.05593622103333473 Validation loss 0.05502846837043762 Accuracy 0.8543750643730164\n",
      "Iteration 18160 Training loss 0.05207715928554535 Validation loss 0.05505741387605667 Accuracy 0.8538750410079956\n",
      "Iteration 18170 Training loss 0.05627094954252243 Validation loss 0.05504447966814041 Accuracy 0.8547500371932983\n",
      "Iteration 18180 Training loss 0.05262397602200508 Validation loss 0.05524508282542229 Accuracy 0.8527500629425049\n",
      "Iteration 18190 Training loss 0.04874080419540405 Validation loss 0.055119216442108154 Accuracy 0.8527500629425049\n",
      "Iteration 18200 Training loss 0.04816234111785889 Validation loss 0.054953716695308685 Accuracy 0.8556250333786011\n",
      "Iteration 18210 Training loss 0.05527739226818085 Validation loss 0.05489934980869293 Accuracy 0.8561250567436218\n",
      "Iteration 18220 Training loss 0.04994581267237663 Validation loss 0.05499432235956192 Accuracy 0.8552500605583191\n",
      "Iteration 18230 Training loss 0.04898778721690178 Validation loss 0.0549527071416378 Accuracy 0.8556250333786011\n",
      "Iteration 18240 Training loss 0.053558871150016785 Validation loss 0.05503333359956741 Accuracy 0.8552500605583191\n",
      "Iteration 18250 Training loss 0.046290624886751175 Validation loss 0.05498133599758148 Accuracy 0.8551250696182251\n",
      "Iteration 18260 Training loss 0.050139836966991425 Validation loss 0.05492567643523216 Accuracy 0.8566250205039978\n",
      "Iteration 18270 Training loss 0.048213012516498566 Validation loss 0.05488638952374458 Accuracy 0.8562500476837158\n",
      "Iteration 18280 Training loss 0.05493983253836632 Validation loss 0.05505108833312988 Accuracy 0.8546250462532043\n",
      "Iteration 18290 Training loss 0.059656210243701935 Validation loss 0.05491382256150246 Accuracy 0.8561250567436218\n",
      "Iteration 18300 Training loss 0.049990177154541016 Validation loss 0.05490851029753685 Accuracy 0.8557500243186951\n",
      "Iteration 18310 Training loss 0.05821283534169197 Validation loss 0.05487661063671112 Accuracy 0.8552500605583191\n",
      "Iteration 18320 Training loss 0.05862832069396973 Validation loss 0.05488194525241852 Accuracy 0.8558750152587891\n",
      "Iteration 18330 Training loss 0.0584695041179657 Validation loss 0.05505330488085747 Accuracy 0.8555000424385071\n",
      "Iteration 18340 Training loss 0.04941609129309654 Validation loss 0.05487874150276184 Accuracy 0.8560000658035278\n",
      "Iteration 18350 Training loss 0.05007654055953026 Validation loss 0.054867006838321686 Accuracy 0.8563750386238098\n",
      "Iteration 18360 Training loss 0.0522853285074234 Validation loss 0.05510291829705238 Accuracy 0.8532500267028809\n",
      "Iteration 18370 Training loss 0.05834057554602623 Validation loss 0.05485772341489792 Accuracy 0.8557500243186951\n",
      "Iteration 18380 Training loss 0.056689534336328506 Validation loss 0.05494685471057892 Accuracy 0.8545000553131104\n",
      "Iteration 18390 Training loss 0.04824502021074295 Validation loss 0.054863475263118744 Accuracy 0.8551250696182251\n",
      "Iteration 18400 Training loss 0.04812314361333847 Validation loss 0.054807670414447784 Accuracy 0.8553750514984131\n",
      "Iteration 18410 Training loss 0.05205968767404556 Validation loss 0.054861605167388916 Accuracy 0.8550000190734863\n",
      "Iteration 18420 Training loss 0.04975137487053871 Validation loss 0.05479168891906738 Accuracy 0.8560000658035278\n",
      "Iteration 18430 Training loss 0.057046279311180115 Validation loss 0.054805878549814224 Accuracy 0.8556250333786011\n",
      "Iteration 18440 Training loss 0.04958946257829666 Validation loss 0.054809603840112686 Accuracy 0.8556250333786011\n",
      "Iteration 18450 Training loss 0.049733731895685196 Validation loss 0.054947905242443085 Accuracy 0.8543750643730164\n",
      "Iteration 18460 Training loss 0.05920984223484993 Validation loss 0.05484114959836006 Accuracy 0.8552500605583191\n",
      "Iteration 18470 Training loss 0.05183232203125954 Validation loss 0.05487750470638275 Accuracy 0.8545000553131104\n",
      "Iteration 18480 Training loss 0.04890424758195877 Validation loss 0.05475720390677452 Accuracy 0.8557500243186951\n",
      "Iteration 18490 Training loss 0.045667391270399094 Validation loss 0.054824039340019226 Accuracy 0.8557500243186951\n",
      "Iteration 18500 Training loss 0.051700275391340256 Validation loss 0.05476411059498787 Accuracy 0.8568750619888306\n",
      "Iteration 18510 Training loss 0.04551494121551514 Validation loss 0.05480343848466873 Accuracy 0.8553750514984131\n",
      "Iteration 18520 Training loss 0.05559851974248886 Validation loss 0.05496366322040558 Accuracy 0.8537500500679016\n",
      "Iteration 18530 Training loss 0.054968204349279404 Validation loss 0.05490263178944588 Accuracy 0.8540000319480896\n",
      "Iteration 18540 Training loss 0.06090758368372917 Validation loss 0.05475245416164398 Accuracy 0.8561250567436218\n",
      "Iteration 18550 Training loss 0.044504277408123016 Validation loss 0.05473149195313454 Accuracy 0.8568750619888306\n",
      "Iteration 18560 Training loss 0.04925492778420448 Validation loss 0.05484161525964737 Accuracy 0.8543750643730164\n",
      "Iteration 18570 Training loss 0.04943355545401573 Validation loss 0.05505792424082756 Accuracy 0.8531250357627869\n",
      "Iteration 18580 Training loss 0.053860366344451904 Validation loss 0.05481322109699249 Accuracy 0.8557500243186951\n",
      "Iteration 18590 Training loss 0.05826018378138542 Validation loss 0.05491786077618599 Accuracy 0.8550000190734863\n",
      "Iteration 18600 Training loss 0.051808807998895645 Validation loss 0.05507450923323631 Accuracy 0.8525000214576721\n",
      "Iteration 18610 Training loss 0.05416105315089226 Validation loss 0.05476754158735275 Accuracy 0.8555000424385071\n",
      "Iteration 18620 Training loss 0.05490938946604729 Validation loss 0.05470113083720207 Accuracy 0.8567500114440918\n",
      "Iteration 18630 Training loss 0.05322163552045822 Validation loss 0.054891061037778854 Accuracy 0.8548750281333923\n",
      "Iteration 18640 Training loss 0.053941503167152405 Validation loss 0.054769936949014664 Accuracy 0.8560000658035278\n",
      "Iteration 18650 Training loss 0.0517287440598011 Validation loss 0.05486728623509407 Accuracy 0.8552500605583191\n",
      "Iteration 18660 Training loss 0.05529789626598358 Validation loss 0.05473433807492256 Accuracy 0.8558750152587891\n",
      "Iteration 18670 Training loss 0.0597846582531929 Validation loss 0.054939426481723785 Accuracy 0.8541250228881836\n",
      "Iteration 18680 Training loss 0.061526086181402206 Validation loss 0.05473502725362778 Accuracy 0.8557500243186951\n",
      "Iteration 18690 Training loss 0.05786263942718506 Validation loss 0.05473601445555687 Accuracy 0.8560000658035278\n",
      "Iteration 18700 Training loss 0.04896588996052742 Validation loss 0.054842036217451096 Accuracy 0.8541250228881836\n",
      "Iteration 18710 Training loss 0.0517784021794796 Validation loss 0.05483699217438698 Accuracy 0.8547500371932983\n",
      "Iteration 18720 Training loss 0.05478915944695473 Validation loss 0.054701387882232666 Accuracy 0.8557500243186951\n",
      "Iteration 18730 Training loss 0.05787905678153038 Validation loss 0.05476414039731026 Accuracy 0.8552500605583191\n",
      "Iteration 18740 Training loss 0.05902685597538948 Validation loss 0.054673925042152405 Accuracy 0.8561250567436218\n",
      "Iteration 18750 Training loss 0.05208395794034004 Validation loss 0.05479533597826958 Accuracy 0.8540000319480896\n",
      "Iteration 18760 Training loss 0.05452654883265495 Validation loss 0.05462133139371872 Accuracy 0.8565000295639038\n",
      "Iteration 18770 Training loss 0.04707113653421402 Validation loss 0.05488544702529907 Accuracy 0.8550000190734863\n",
      "Iteration 18780 Training loss 0.060587406158447266 Validation loss 0.05484180152416229 Accuracy 0.8535000681877136\n",
      "Iteration 18790 Training loss 0.053681179881095886 Validation loss 0.05503229796886444 Accuracy 0.8536250591278076\n",
      "Iteration 18800 Training loss 0.05304818972945213 Validation loss 0.05482775717973709 Accuracy 0.8542500138282776\n",
      "Iteration 18810 Training loss 0.06042138487100601 Validation loss 0.054662469774484634 Accuracy 0.8548750281333923\n",
      "Iteration 18820 Training loss 0.05816160514950752 Validation loss 0.054679617285728455 Accuracy 0.8567500114440918\n",
      "Iteration 18830 Training loss 0.05756736546754837 Validation loss 0.054716698825359344 Accuracy 0.8555000424385071\n",
      "Iteration 18840 Training loss 0.04921702295541763 Validation loss 0.054609060287475586 Accuracy 0.8568750619888306\n",
      "Iteration 18850 Training loss 0.057270314544439316 Validation loss 0.054632242769002914 Accuracy 0.8557500243186951\n",
      "Iteration 18860 Training loss 0.05169958993792534 Validation loss 0.054570186883211136 Accuracy 0.8565000295639038\n",
      "Iteration 18870 Training loss 0.059892747551202774 Validation loss 0.05477435886859894 Accuracy 0.8545000553131104\n",
      "Iteration 18880 Training loss 0.05768144130706787 Validation loss 0.05457903444766998 Accuracy 0.8568750619888306\n",
      "Iteration 18890 Training loss 0.053981442004442215 Validation loss 0.0546734444797039 Accuracy 0.8561250567436218\n",
      "Iteration 18900 Training loss 0.052595410495996475 Validation loss 0.0546366386115551 Accuracy 0.8562500476837158\n",
      "Iteration 18910 Training loss 0.054713085293769836 Validation loss 0.054682303220033646 Accuracy 0.8556250333786011\n",
      "Iteration 18920 Training loss 0.05828644707798958 Validation loss 0.0546812079846859 Accuracy 0.8550000190734863\n",
      "Iteration 18930 Training loss 0.058212537318468094 Validation loss 0.05453268811106682 Accuracy 0.8567500114440918\n",
      "Iteration 18940 Training loss 0.05823889747262001 Validation loss 0.05457136034965515 Accuracy 0.8568750619888306\n",
      "Iteration 18950 Training loss 0.055859245359897614 Validation loss 0.05456965044140816 Accuracy 0.8578750491142273\n",
      "Iteration 18960 Training loss 0.04729669541120529 Validation loss 0.0546746701002121 Accuracy 0.8552500605583191\n",
      "Iteration 18970 Training loss 0.04184355586767197 Validation loss 0.0544770210981369 Accuracy 0.8567500114440918\n",
      "Iteration 18980 Training loss 0.0483151413500309 Validation loss 0.054672449827194214 Accuracy 0.8550000190734863\n",
      "Iteration 18990 Training loss 0.05084070935845375 Validation loss 0.054485950618982315 Accuracy 0.8557500243186951\n",
      "Iteration 19000 Training loss 0.0563732385635376 Validation loss 0.054505605250597 Accuracy 0.8562500476837158\n",
      "Iteration 19010 Training loss 0.047773294150829315 Validation loss 0.054625254124403 Accuracy 0.8545000553131104\n",
      "Iteration 19020 Training loss 0.05157534405589104 Validation loss 0.05463738739490509 Accuracy 0.8552500605583191\n",
      "Iteration 19030 Training loss 0.05661192908883095 Validation loss 0.054769985377788544 Accuracy 0.8541250228881836\n",
      "Iteration 19040 Training loss 0.056086309254169464 Validation loss 0.054496634751558304 Accuracy 0.8561250567436218\n",
      "Iteration 19050 Training loss 0.056236766278743744 Validation loss 0.05449041351675987 Accuracy 0.8562500476837158\n",
      "Iteration 19060 Training loss 0.053160276263952255 Validation loss 0.05464041605591774 Accuracy 0.8546250462532043\n",
      "Iteration 19070 Training loss 0.049692943692207336 Validation loss 0.0549168586730957 Accuracy 0.8541250228881836\n",
      "Iteration 19080 Training loss 0.051702830940485 Validation loss 0.05449112877249718 Accuracy 0.8566250205039978\n",
      "Iteration 19090 Training loss 0.05706257373094559 Validation loss 0.054471757262945175 Accuracy 0.8560000658035278\n",
      "Iteration 19100 Training loss 0.05286167562007904 Validation loss 0.05453099310398102 Accuracy 0.8562500476837158\n",
      "Iteration 19110 Training loss 0.05914707109332085 Validation loss 0.054526813328266144 Accuracy 0.8560000658035278\n",
      "Iteration 19120 Training loss 0.051931727677583694 Validation loss 0.054735612124204636 Accuracy 0.8551250696182251\n",
      "Iteration 19130 Training loss 0.05221056938171387 Validation loss 0.0544237457215786 Accuracy 0.8567500114440918\n",
      "Iteration 19140 Training loss 0.05009615421295166 Validation loss 0.05445289611816406 Accuracy 0.8560000658035278\n",
      "Iteration 19150 Training loss 0.049857307225465775 Validation loss 0.05455996096134186 Accuracy 0.8547500371932983\n",
      "Iteration 19160 Training loss 0.046197984367609024 Validation loss 0.054809484630823135 Accuracy 0.8532500267028809\n",
      "Iteration 19170 Training loss 0.05731409043073654 Validation loss 0.05478730425238609 Accuracy 0.8530000448226929\n",
      "Iteration 19180 Training loss 0.057270441204309464 Validation loss 0.054426100105047226 Accuracy 0.8556250333786011\n",
      "Iteration 19190 Training loss 0.05099670588970184 Validation loss 0.054649654775857925 Accuracy 0.8546250462532043\n",
      "Iteration 19200 Training loss 0.06743662804365158 Validation loss 0.0545133501291275 Accuracy 0.8553750514984131\n",
      "Iteration 19210 Training loss 0.04774361476302147 Validation loss 0.05448780208826065 Accuracy 0.8567500114440918\n",
      "Iteration 19220 Training loss 0.053985510021448135 Validation loss 0.054466333240270615 Accuracy 0.8568750619888306\n",
      "Iteration 19230 Training loss 0.05022154375910759 Validation loss 0.05443186312913895 Accuracy 0.8570000529289246\n",
      "Iteration 19240 Training loss 0.052974846214056015 Validation loss 0.05437532439827919 Accuracy 0.8572500348091125\n",
      "Iteration 19250 Training loss 0.04942991957068443 Validation loss 0.05438198894262314 Accuracy 0.8573750257492065\n",
      "Iteration 19260 Training loss 0.06007295474410057 Validation loss 0.05445929989218712 Accuracy 0.8566250205039978\n",
      "Iteration 19270 Training loss 0.05227508395910263 Validation loss 0.05435977876186371 Accuracy 0.8570000529289246\n",
      "Iteration 19280 Training loss 0.058922383934259415 Validation loss 0.05446314811706543 Accuracy 0.8567500114440918\n",
      "Iteration 19290 Training loss 0.056075457483530045 Validation loss 0.054345957934856415 Accuracy 0.8572500348091125\n",
      "Iteration 19300 Training loss 0.056116268038749695 Validation loss 0.05437067151069641 Accuracy 0.8573750257492065\n",
      "Iteration 19310 Training loss 0.052400216460227966 Validation loss 0.05442899093031883 Accuracy 0.8567500114440918\n",
      "Iteration 19320 Training loss 0.049418170005083084 Validation loss 0.054331324994564056 Accuracy 0.8575000166893005\n",
      "Iteration 19330 Training loss 0.044299058616161346 Validation loss 0.05439671501517296 Accuracy 0.8563750386238098\n",
      "Iteration 19340 Training loss 0.04330980032682419 Validation loss 0.054490476846694946 Accuracy 0.8556250333786011\n",
      "Iteration 19350 Training loss 0.058342669159173965 Validation loss 0.05459293723106384 Accuracy 0.8547500371932983\n",
      "Iteration 19360 Training loss 0.046898867934942245 Validation loss 0.05465436726808548 Accuracy 0.8545000553131104\n",
      "Iteration 19370 Training loss 0.03841526806354523 Validation loss 0.05434991791844368 Accuracy 0.8567500114440918\n",
      "Iteration 19380 Training loss 0.0436277873814106 Validation loss 0.054340943694114685 Accuracy 0.8570000529289246\n",
      "Iteration 19390 Training loss 0.055647529661655426 Validation loss 0.05436577647924423 Accuracy 0.8566250205039978\n",
      "Iteration 19400 Training loss 0.05379649996757507 Validation loss 0.054773952811956406 Accuracy 0.8546250462532043\n",
      "Iteration 19410 Training loss 0.05254342034459114 Validation loss 0.05436041206121445 Accuracy 0.8563750386238098\n",
      "Iteration 19420 Training loss 0.05339035391807556 Validation loss 0.0543401725590229 Accuracy 0.8567500114440918\n",
      "Iteration 19430 Training loss 0.056697528809309006 Validation loss 0.054369356483221054 Accuracy 0.8561250567436218\n",
      "Iteration 19440 Training loss 0.05226842686533928 Validation loss 0.054438550025224686 Accuracy 0.8557500243186951\n",
      "Iteration 19450 Training loss 0.04486515745520592 Validation loss 0.0542890727519989 Accuracy 0.8575000166893005\n",
      "Iteration 19460 Training loss 0.047869663685560226 Validation loss 0.05427732318639755 Accuracy 0.8572500348091125\n",
      "Iteration 19470 Training loss 0.048322685062885284 Validation loss 0.05444673076272011 Accuracy 0.8553750514984131\n",
      "Iteration 19480 Training loss 0.057420339435338974 Validation loss 0.054302629083395004 Accuracy 0.8561250567436218\n",
      "Iteration 19490 Training loss 0.0529097355902195 Validation loss 0.054317768663167953 Accuracy 0.8556250333786011\n",
      "Iteration 19500 Training loss 0.06139400228857994 Validation loss 0.054290495812892914 Accuracy 0.8577500581741333\n",
      "Iteration 19510 Training loss 0.054375533014535904 Validation loss 0.05445919930934906 Accuracy 0.8550000190734863\n",
      "Iteration 19520 Training loss 0.04905920475721359 Validation loss 0.0542825423181057 Accuracy 0.8572500348091125\n",
      "Iteration 19530 Training loss 0.0483882911503315 Validation loss 0.05431396886706352 Accuracy 0.8572500348091125\n",
      "Iteration 19540 Training loss 0.047950249165296555 Validation loss 0.054298579692840576 Accuracy 0.8575000166893005\n",
      "Iteration 19550 Training loss 0.04745396599173546 Validation loss 0.05425693094730377 Accuracy 0.8566250205039978\n",
      "Iteration 19560 Training loss 0.06359567493200302 Validation loss 0.054491449147462845 Accuracy 0.8553750514984131\n",
      "Iteration 19570 Training loss 0.04754982516169548 Validation loss 0.054285887628793716 Accuracy 0.8575000166893005\n",
      "Iteration 19580 Training loss 0.05361134931445122 Validation loss 0.05424584448337555 Accuracy 0.8573750257492065\n",
      "Iteration 19590 Training loss 0.04999544098973274 Validation loss 0.05426627770066261 Accuracy 0.8568750619888306\n",
      "Iteration 19600 Training loss 0.05317180976271629 Validation loss 0.054203473031520844 Accuracy 0.8573750257492065\n",
      "Iteration 19610 Training loss 0.05217524990439415 Validation loss 0.05419398844242096 Accuracy 0.8581250309944153\n",
      "Iteration 19620 Training loss 0.04933202639222145 Validation loss 0.05423697829246521 Accuracy 0.8573750257492065\n",
      "Iteration 19630 Training loss 0.047302570194005966 Validation loss 0.054179735481739044 Accuracy 0.8577500581741333\n",
      "Iteration 19640 Training loss 0.04748404026031494 Validation loss 0.05416273698210716 Accuracy 0.8580000400543213\n",
      "Iteration 19650 Training loss 0.051360998302698135 Validation loss 0.05417228862643242 Accuracy 0.8577500581741333\n",
      "Iteration 19660 Training loss 0.05064013972878456 Validation loss 0.054495371878147125 Accuracy 0.8558750152587891\n",
      "Iteration 19670 Training loss 0.04934873431921005 Validation loss 0.05415603145956993 Accuracy 0.8578750491142273\n",
      "Iteration 19680 Training loss 0.06130647659301758 Validation loss 0.054368432611227036 Accuracy 0.8573750257492065\n",
      "Iteration 19690 Training loss 0.051703937351703644 Validation loss 0.05426590144634247 Accuracy 0.8576250672340393\n",
      "Iteration 19700 Training loss 0.045259226113557816 Validation loss 0.05424075573682785 Accuracy 0.8571250438690186\n",
      "Iteration 19710 Training loss 0.053101442754268646 Validation loss 0.05413595959544182 Accuracy 0.8578750491142273\n",
      "Iteration 19720 Training loss 0.05209442600607872 Validation loss 0.05415085703134537 Accuracy 0.85875004529953\n",
      "Iteration 19730 Training loss 0.04626357927918434 Validation loss 0.05414096266031265 Accuracy 0.8581250309944153\n",
      "Iteration 19740 Training loss 0.04978908598423004 Validation loss 0.0544426366686821 Accuracy 0.8556250333786011\n",
      "Iteration 19750 Training loss 0.05085405334830284 Validation loss 0.05413505807518959 Accuracy 0.8581250309944153\n",
      "Iteration 19760 Training loss 0.04888853803277016 Validation loss 0.05415746569633484 Accuracy 0.8577500581741333\n",
      "Iteration 19770 Training loss 0.05283641070127487 Validation loss 0.05415375903248787 Accuracy 0.8573750257492065\n",
      "Iteration 19780 Training loss 0.051332369446754456 Validation loss 0.05406970530748367 Accuracy 0.8581250309944153\n",
      "Iteration 19790 Training loss 0.054656729102134705 Validation loss 0.05407901853322983 Accuracy 0.8573750257492065\n",
      "Iteration 19800 Training loss 0.0532221682369709 Validation loss 0.05408426746726036 Accuracy 0.8567500114440918\n",
      "Iteration 19810 Training loss 0.04901132360100746 Validation loss 0.05407130718231201 Accuracy 0.8575000166893005\n",
      "Iteration 19820 Training loss 0.04957609996199608 Validation loss 0.054109301418066025 Accuracy 0.8581250309944153\n",
      "Iteration 19830 Training loss 0.05600878596305847 Validation loss 0.05409032851457596 Accuracy 0.8573750257492065\n",
      "Iteration 19840 Training loss 0.0583939291536808 Validation loss 0.05447882041335106 Accuracy 0.8536250591278076\n",
      "Iteration 19850 Training loss 0.050648316740989685 Validation loss 0.05409420654177666 Accuracy 0.8580000400543213\n",
      "Iteration 19860 Training loss 0.0573929026722908 Validation loss 0.05411948636174202 Accuracy 0.8568750619888306\n",
      "Iteration 19870 Training loss 0.054061003029346466 Validation loss 0.054255999624729156 Accuracy 0.8570000529289246\n",
      "Iteration 19880 Training loss 0.057475823909044266 Validation loss 0.05409570410847664 Accuracy 0.8568750619888306\n",
      "Iteration 19890 Training loss 0.05550113692879677 Validation loss 0.05407819524407387 Accuracy 0.8581250309944153\n",
      "Iteration 19900 Training loss 0.04950545355677605 Validation loss 0.05448456108570099 Accuracy 0.8546250462532043\n",
      "Iteration 19910 Training loss 0.05030517280101776 Validation loss 0.05404911935329437 Accuracy 0.8576250672340393\n",
      "Iteration 19920 Training loss 0.057717278599739075 Validation loss 0.05402781069278717 Accuracy 0.8580000400543213\n",
      "Iteration 19930 Training loss 0.045665282756090164 Validation loss 0.054262202233076096 Accuracy 0.8563750386238098\n",
      "Iteration 19940 Training loss 0.05552133917808533 Validation loss 0.05406477674841881 Accuracy 0.8581250309944153\n",
      "Iteration 19950 Training loss 0.04345669597387314 Validation loss 0.054018307477235794 Accuracy 0.858500063419342\n",
      "Iteration 19960 Training loss 0.045964181423187256 Validation loss 0.05399806797504425 Accuracy 0.8582500219345093\n",
      "Iteration 19970 Training loss 0.04828186333179474 Validation loss 0.05408253148198128 Accuracy 0.8565000295639038\n",
      "Iteration 19980 Training loss 0.05015743151307106 Validation loss 0.054259639233350754 Accuracy 0.8552500605583191\n",
      "Iteration 19990 Training loss 0.05693656951189041 Validation loss 0.05407308042049408 Accuracy 0.8576250672340393\n",
      "Iteration 20000 Training loss 0.05164346098899841 Validation loss 0.05397561937570572 Accuracy 0.8582500219345093\n",
      "Iteration 20010 Training loss 0.05185523256659508 Validation loss 0.05417748540639877 Accuracy 0.8568750619888306\n",
      "Iteration 20020 Training loss 0.053598612546920776 Validation loss 0.053987544029951096 Accuracy 0.8578750491142273\n",
      "Iteration 20030 Training loss 0.04961394518613815 Validation loss 0.05397039279341698 Accuracy 0.8582500219345093\n",
      "Iteration 20040 Training loss 0.054861828684806824 Validation loss 0.05399404466152191 Accuracy 0.8581250309944153\n",
      "Iteration 20050 Training loss 0.048021964728832245 Validation loss 0.05398216098546982 Accuracy 0.858500063419342\n",
      "Iteration 20060 Training loss 0.04685652628540993 Validation loss 0.05402374640107155 Accuracy 0.8572500348091125\n",
      "Iteration 20070 Training loss 0.0525972917675972 Validation loss 0.053948841989040375 Accuracy 0.858500063419342\n",
      "Iteration 20080 Training loss 0.06323235481977463 Validation loss 0.053981270641088486 Accuracy 0.8580000400543213\n",
      "Iteration 20090 Training loss 0.053911320865154266 Validation loss 0.05402752012014389 Accuracy 0.8572500348091125\n",
      "Iteration 20100 Training loss 0.053672030568122864 Validation loss 0.05395039543509483 Accuracy 0.858500063419342\n",
      "Iteration 20110 Training loss 0.04859739542007446 Validation loss 0.0539398230612278 Accuracy 0.8572500348091125\n",
      "Iteration 20120 Training loss 0.0465359166264534 Validation loss 0.053995102643966675 Accuracy 0.8581250309944153\n",
      "Iteration 20130 Training loss 0.05102990195155144 Validation loss 0.05431443825364113 Accuracy 0.8548750281333923\n",
      "Iteration 20140 Training loss 0.04599588364362717 Validation loss 0.05395227670669556 Accuracy 0.8581250309944153\n",
      "Iteration 20150 Training loss 0.04741948843002319 Validation loss 0.05404900014400482 Accuracy 0.8563750386238098\n",
      "Iteration 20160 Training loss 0.05711040273308754 Validation loss 0.05402654781937599 Accuracy 0.8561250567436218\n",
      "Iteration 20170 Training loss 0.04272209107875824 Validation loss 0.05391153320670128 Accuracy 0.858625054359436\n",
      "Iteration 20180 Training loss 0.049279697239398956 Validation loss 0.053904514759778976 Accuracy 0.8570000529289246\n",
      "Iteration 20190 Training loss 0.048966262489557266 Validation loss 0.053915392607450485 Accuracy 0.8577500581741333\n",
      "Iteration 20200 Training loss 0.04486888274550438 Validation loss 0.0539047047495842 Accuracy 0.858625054359436\n",
      "Iteration 20210 Training loss 0.053894560784101486 Validation loss 0.05390576273202896 Accuracy 0.8566250205039978\n",
      "Iteration 20220 Training loss 0.0563676543533802 Validation loss 0.05434153974056244 Accuracy 0.8550000190734863\n",
      "Iteration 20230 Training loss 0.056925367563962936 Validation loss 0.053858134895563126 Accuracy 0.8581250309944153\n",
      "Iteration 20240 Training loss 0.06367369741201401 Validation loss 0.05393484607338905 Accuracy 0.8573750257492065\n",
      "Iteration 20250 Training loss 0.05802473425865173 Validation loss 0.05394510179758072 Accuracy 0.8567500114440918\n",
      "Iteration 20260 Training loss 0.05742422863841057 Validation loss 0.05388180539011955 Accuracy 0.8578750491142273\n",
      "Iteration 20270 Training loss 0.04813528060913086 Validation loss 0.05384819954633713 Accuracy 0.858875036239624\n",
      "Iteration 20280 Training loss 0.048189472407102585 Validation loss 0.05392962321639061 Accuracy 0.8568750619888306\n",
      "Iteration 20290 Training loss 0.061909016221761703 Validation loss 0.05386214703321457 Accuracy 0.858625054359436\n",
      "Iteration 20300 Training loss 0.054939381778240204 Validation loss 0.05387125536799431 Accuracy 0.858500063419342\n",
      "Iteration 20310 Training loss 0.055327873677015305 Validation loss 0.053861912339925766 Accuracy 0.8592500686645508\n",
      "Iteration 20320 Training loss 0.05335156247019768 Validation loss 0.05389193817973137 Accuracy 0.858875036239624\n",
      "Iteration 20330 Training loss 0.0469239316880703 Validation loss 0.05382908135652542 Accuracy 0.8573750257492065\n",
      "Iteration 20340 Training loss 0.05189572274684906 Validation loss 0.053952399641275406 Accuracy 0.8571250438690186\n",
      "Iteration 20350 Training loss 0.05800895392894745 Validation loss 0.05389268696308136 Accuracy 0.8592500686645508\n",
      "Iteration 20360 Training loss 0.05696900188922882 Validation loss 0.05381399765610695 Accuracy 0.8583750128746033\n",
      "Iteration 20370 Training loss 0.06063395366072655 Validation loss 0.05465216934680939 Accuracy 0.8536250591278076\n",
      "Iteration 20380 Training loss 0.05715445429086685 Validation loss 0.05396804213523865 Accuracy 0.8571250438690186\n",
      "Iteration 20390 Training loss 0.049607548862695694 Validation loss 0.05389031395316124 Accuracy 0.8571250438690186\n",
      "Iteration 20400 Training loss 0.05308283865451813 Validation loss 0.05452630668878555 Accuracy 0.8538750410079956\n",
      "Iteration 20410 Training loss 0.05141519755125046 Validation loss 0.05378314480185509 Accuracy 0.858625054359436\n",
      "Iteration 20420 Training loss 0.04913834482431412 Validation loss 0.053794343024492264 Accuracy 0.8582500219345093\n",
      "Iteration 20430 Training loss 0.04272777587175369 Validation loss 0.053913991898298264 Accuracy 0.8570000529289246\n",
      "Iteration 20440 Training loss 0.06159557029604912 Validation loss 0.053804680705070496 Accuracy 0.8578750491142273\n",
      "Iteration 20450 Training loss 0.04742254316806793 Validation loss 0.05406095087528229 Accuracy 0.8571250438690186\n",
      "Iteration 20460 Training loss 0.04887806624174118 Validation loss 0.054005034267902374 Accuracy 0.8562500476837158\n",
      "Iteration 20470 Training loss 0.05640678107738495 Validation loss 0.05375105142593384 Accuracy 0.858625054359436\n",
      "Iteration 20480 Training loss 0.05479482188820839 Validation loss 0.0538129061460495 Accuracy 0.859000027179718\n",
      "Iteration 20490 Training loss 0.04310058429837227 Validation loss 0.053880371153354645 Accuracy 0.8576250672340393\n",
      "Iteration 20500 Training loss 0.057194989174604416 Validation loss 0.05378960818052292 Accuracy 0.859125018119812\n",
      "Iteration 20510 Training loss 0.05129304900765419 Validation loss 0.053761519491672516 Accuracy 0.858625054359436\n",
      "Iteration 20520 Training loss 0.05337200686335564 Validation loss 0.05379069596529007 Accuracy 0.85875004529953\n",
      "Iteration 20530 Training loss 0.050942562520504 Validation loss 0.05377158522605896 Accuracy 0.858500063419342\n",
      "Iteration 20540 Training loss 0.05243108794093132 Validation loss 0.05376821756362915 Accuracy 0.8583750128746033\n",
      "Iteration 20550 Training loss 0.04987340420484543 Validation loss 0.05380591005086899 Accuracy 0.8583750128746033\n",
      "Iteration 20560 Training loss 0.0444827638566494 Validation loss 0.05409083515405655 Accuracy 0.8570000529289246\n",
      "Iteration 20570 Training loss 0.038934968411922455 Validation loss 0.05381312966346741 Accuracy 0.8583750128746033\n",
      "Iteration 20580 Training loss 0.05619592219591141 Validation loss 0.05380594730377197 Accuracy 0.8578750491142273\n",
      "Iteration 20590 Training loss 0.048189934343099594 Validation loss 0.05386151745915413 Accuracy 0.8573750257492065\n",
      "Iteration 20600 Training loss 0.05218419432640076 Validation loss 0.05400773882865906 Accuracy 0.8567500114440918\n",
      "Iteration 20610 Training loss 0.05133690685033798 Validation loss 0.05375547334551811 Accuracy 0.859000027179718\n",
      "Iteration 20620 Training loss 0.05514305830001831 Validation loss 0.05378160998225212 Accuracy 0.8575000166893005\n",
      "Iteration 20630 Training loss 0.0582914724946022 Validation loss 0.05374503880739212 Accuracy 0.8576250672340393\n",
      "Iteration 20640 Training loss 0.05684052035212517 Validation loss 0.0544218048453331 Accuracy 0.8552500605583191\n",
      "Iteration 20650 Training loss 0.057617537677288055 Validation loss 0.05374032258987427 Accuracy 0.8580000400543213\n",
      "Iteration 20660 Training loss 0.054508913308382034 Validation loss 0.05369485914707184 Accuracy 0.859125018119812\n",
      "Iteration 20670 Training loss 0.05543901026248932 Validation loss 0.05473397299647331 Accuracy 0.8531250357627869\n",
      "Iteration 20680 Training loss 0.05376853048801422 Validation loss 0.05369612202048302 Accuracy 0.8582500219345093\n",
      "Iteration 20690 Training loss 0.052021510899066925 Validation loss 0.05368395894765854 Accuracy 0.858875036239624\n",
      "Iteration 20700 Training loss 0.04690876230597496 Validation loss 0.05368823558092117 Accuracy 0.8582500219345093\n",
      "Iteration 20710 Training loss 0.05370094254612923 Validation loss 0.05381013825535774 Accuracy 0.8573750257492065\n",
      "Iteration 20720 Training loss 0.044662922620773315 Validation loss 0.05368475615978241 Accuracy 0.859000027179718\n",
      "Iteration 20730 Training loss 0.047355689108371735 Validation loss 0.053722865879535675 Accuracy 0.858875036239624\n",
      "Iteration 20740 Training loss 0.05146351829171181 Validation loss 0.05372887849807739 Accuracy 0.858625054359436\n",
      "Iteration 20750 Training loss 0.047857679426670074 Validation loss 0.05365080386400223 Accuracy 0.8581250309944153\n",
      "Iteration 20760 Training loss 0.06214861571788788 Validation loss 0.05382130295038223 Accuracy 0.8573750257492065\n",
      "Iteration 20770 Training loss 0.044715337455272675 Validation loss 0.05370645225048065 Accuracy 0.859000027179718\n",
      "Iteration 20780 Training loss 0.05902905389666557 Validation loss 0.053958795964717865 Accuracy 0.8571250438690186\n",
      "Iteration 20790 Training loss 0.055815935134887695 Validation loss 0.05362075939774513 Accuracy 0.858875036239624\n",
      "Iteration 20800 Training loss 0.05465809628367424 Validation loss 0.05367779731750488 Accuracy 0.8597500324249268\n",
      "Iteration 20810 Training loss 0.057610392570495605 Validation loss 0.053625378757715225 Accuracy 0.859125018119812\n",
      "Iteration 20820 Training loss 0.04858306795358658 Validation loss 0.05381776764988899 Accuracy 0.8566250205039978\n",
      "Iteration 20830 Training loss 0.050527047365903854 Validation loss 0.053711749613285065 Accuracy 0.8593750596046448\n",
      "Iteration 20840 Training loss 0.06259440630674362 Validation loss 0.05359845981001854 Accuracy 0.8582500219345093\n",
      "Iteration 20850 Training loss 0.048280972987413406 Validation loss 0.0537281408905983 Accuracy 0.859125018119812\n",
      "Iteration 20860 Training loss 0.05579298734664917 Validation loss 0.0537673719227314 Accuracy 0.8575000166893005\n",
      "Iteration 20870 Training loss 0.0557851605117321 Validation loss 0.05358516797423363 Accuracy 0.858625054359436\n",
      "Iteration 20880 Training loss 0.056907568126916885 Validation loss 0.05366438627243042 Accuracy 0.8578750491142273\n",
      "Iteration 20890 Training loss 0.057723693549633026 Validation loss 0.053549814969301224 Accuracy 0.8580000400543213\n",
      "Iteration 20900 Training loss 0.05230192840099335 Validation loss 0.053557030856609344 Accuracy 0.8583750128746033\n",
      "Iteration 20910 Training loss 0.05867207422852516 Validation loss 0.0536997988820076 Accuracy 0.8575000166893005\n",
      "Iteration 20920 Training loss 0.049653418362140656 Validation loss 0.05361229181289673 Accuracy 0.8582500219345093\n",
      "Iteration 20930 Training loss 0.05503334477543831 Validation loss 0.05364016443490982 Accuracy 0.85875004529953\n",
      "Iteration 20940 Training loss 0.04922928661108017 Validation loss 0.05360826477408409 Accuracy 0.8593750596046448\n",
      "Iteration 20950 Training loss 0.0503140352666378 Validation loss 0.053604550659656525 Accuracy 0.8595000505447388\n",
      "Iteration 20960 Training loss 0.04781248793005943 Validation loss 0.053584735840559006 Accuracy 0.859000027179718\n",
      "Iteration 20970 Training loss 0.04752287641167641 Validation loss 0.053612083196640015 Accuracy 0.8593750596046448\n",
      "Iteration 20980 Training loss 0.04722914472222328 Validation loss 0.05368812382221222 Accuracy 0.8570000529289246\n",
      "Iteration 20990 Training loss 0.059252459555864334 Validation loss 0.053586672991514206 Accuracy 0.8575000166893005\n",
      "Iteration 21000 Training loss 0.054420918226242065 Validation loss 0.05382074415683746 Accuracy 0.8568750619888306\n",
      "Iteration 21010 Training loss 0.056444015353918076 Validation loss 0.05355796962976456 Accuracy 0.859125018119812\n",
      "Iteration 21020 Training loss 0.05105224624276161 Validation loss 0.05351852625608444 Accuracy 0.858500063419342\n",
      "Iteration 21030 Training loss 0.05104542151093483 Validation loss 0.05356930196285248 Accuracy 0.8580000400543213\n",
      "Iteration 21040 Training loss 0.048859670758247375 Validation loss 0.053755469620227814 Accuracy 0.8577500581741333\n",
      "Iteration 21050 Training loss 0.046366527676582336 Validation loss 0.053512390702962875 Accuracy 0.858875036239624\n",
      "Iteration 21060 Training loss 0.04545803740620613 Validation loss 0.05348354950547218 Accuracy 0.8597500324249268\n",
      "Iteration 21070 Training loss 0.056556835770606995 Validation loss 0.053551677614450455 Accuracy 0.85875004529953\n",
      "Iteration 21080 Training loss 0.055655911564826965 Validation loss 0.053494058549404144 Accuracy 0.8592500686645508\n",
      "Iteration 21090 Training loss 0.05033795163035393 Validation loss 0.05350124463438988 Accuracy 0.85875004529953\n",
      "Iteration 21100 Training loss 0.04951351881027222 Validation loss 0.05346473678946495 Accuracy 0.85875004529953\n",
      "Iteration 21110 Training loss 0.057467687875032425 Validation loss 0.05406339839100838 Accuracy 0.8566250205039978\n",
      "Iteration 21120 Training loss 0.05588438734412193 Validation loss 0.053465183824300766 Accuracy 0.8592500686645508\n",
      "Iteration 21130 Training loss 0.04846625775098801 Validation loss 0.053463712334632874 Accuracy 0.859125018119812\n",
      "Iteration 21140 Training loss 0.056375425308942795 Validation loss 0.053486548364162445 Accuracy 0.858500063419342\n",
      "Iteration 21150 Training loss 0.05158571898937225 Validation loss 0.053452666848897934 Accuracy 0.859000027179718\n",
      "Iteration 21160 Training loss 0.05974173545837402 Validation loss 0.05352174863219261 Accuracy 0.8577500581741333\n",
      "Iteration 21170 Training loss 0.04685003682971001 Validation loss 0.05359938368201256 Accuracy 0.8582500219345093\n",
      "Iteration 21180 Training loss 0.056147802621126175 Validation loss 0.05345718562602997 Accuracy 0.859125018119812\n",
      "Iteration 21190 Training loss 0.04970858618617058 Validation loss 0.05356426537036896 Accuracy 0.859000027179718\n",
      "Iteration 21200 Training loss 0.04732143133878708 Validation loss 0.05346682295203209 Accuracy 0.858500063419342\n",
      "Iteration 21210 Training loss 0.05548717826604843 Validation loss 0.053487833589315414 Accuracy 0.858500063419342\n",
      "Iteration 21220 Training loss 0.04206039384007454 Validation loss 0.05346555635333061 Accuracy 0.8582500219345093\n",
      "Iteration 21230 Training loss 0.05916152894496918 Validation loss 0.05343521758913994 Accuracy 0.8582500219345093\n",
      "Iteration 21240 Training loss 0.05352579429745674 Validation loss 0.05342301353812218 Accuracy 0.85875004529953\n",
      "Iteration 21250 Training loss 0.04840188845992088 Validation loss 0.05352574586868286 Accuracy 0.8580000400543213\n",
      "Iteration 21260 Training loss 0.04706597328186035 Validation loss 0.05340193957090378 Accuracy 0.8593750596046448\n",
      "Iteration 21270 Training loss 0.04691663011908531 Validation loss 0.053384799510240555 Accuracy 0.8595000505447388\n",
      "Iteration 21280 Training loss 0.05088723078370094 Validation loss 0.05372919887304306 Accuracy 0.8576250672340393\n",
      "Iteration 21290 Training loss 0.04741918295621872 Validation loss 0.05364632233977318 Accuracy 0.8570000529289246\n",
      "Iteration 21300 Training loss 0.051831815391778946 Validation loss 0.05388595908880234 Accuracy 0.8563750386238098\n",
      "Iteration 21310 Training loss 0.05633044242858887 Validation loss 0.053697094321250916 Accuracy 0.8557500243186951\n",
      "Iteration 21320 Training loss 0.047796983271837234 Validation loss 0.053433775901794434 Accuracy 0.859000027179718\n",
      "Iteration 21330 Training loss 0.05349399521946907 Validation loss 0.05337819084525108 Accuracy 0.8598750233650208\n",
      "Iteration 21340 Training loss 0.04508739337325096 Validation loss 0.053458116948604584 Accuracy 0.8595000505447388\n",
      "Iteration 21350 Training loss 0.05042115971446037 Validation loss 0.05337351933121681 Accuracy 0.8593750596046448\n",
      "Iteration 21360 Training loss 0.0507059171795845 Validation loss 0.053368281573057175 Accuracy 0.8593750596046448\n",
      "Iteration 21370 Training loss 0.04325602948665619 Validation loss 0.05336013436317444 Accuracy 0.8592500686645508\n",
      "Iteration 21380 Training loss 0.05781973525881767 Validation loss 0.05334587022662163 Accuracy 0.8595000505447388\n",
      "Iteration 21390 Training loss 0.056900832802057266 Validation loss 0.053682129830121994 Accuracy 0.8566250205039978\n",
      "Iteration 21400 Training loss 0.05755668878555298 Validation loss 0.053511034697294235 Accuracy 0.8578750491142273\n",
      "Iteration 21410 Training loss 0.045849330723285675 Validation loss 0.05332987755537033 Accuracy 0.859000027179718\n",
      "Iteration 21420 Training loss 0.050998058170080185 Validation loss 0.053312018513679504 Accuracy 0.8593750596046448\n",
      "Iteration 21430 Training loss 0.04886867478489876 Validation loss 0.05329067260026932 Accuracy 0.8595000505447388\n",
      "Iteration 21440 Training loss 0.05985800549387932 Validation loss 0.053549036383628845 Accuracy 0.858625054359436\n",
      "Iteration 21450 Training loss 0.04782227426767349 Validation loss 0.05333966761827469 Accuracy 0.8598750233650208\n",
      "Iteration 21460 Training loss 0.048660438507795334 Validation loss 0.0532960407435894 Accuracy 0.8592500686645508\n",
      "Iteration 21470 Training loss 0.05663979426026344 Validation loss 0.054173074662685394 Accuracy 0.8550000190734863\n",
      "Iteration 21480 Training loss 0.05243920162320137 Validation loss 0.05423488840460777 Accuracy 0.8551250696182251\n",
      "Iteration 21490 Training loss 0.04935064539313316 Validation loss 0.053306058049201965 Accuracy 0.85875004529953\n",
      "Iteration 21500 Training loss 0.05105867236852646 Validation loss 0.05326222628355026 Accuracy 0.8601250648498535\n",
      "Iteration 21510 Training loss 0.052149638533592224 Validation loss 0.05329854041337967 Accuracy 0.8583750128746033\n",
      "Iteration 21520 Training loss 0.050668392330408096 Validation loss 0.053322773426771164 Accuracy 0.85875004529953\n",
      "Iteration 21530 Training loss 0.04770524799823761 Validation loss 0.05361150577664375 Accuracy 0.8580000400543213\n",
      "Iteration 21540 Training loss 0.04475288838148117 Validation loss 0.053312044590711594 Accuracy 0.85875004529953\n",
      "Iteration 21550 Training loss 0.046080321073532104 Validation loss 0.05330482870340347 Accuracy 0.8603750467300415\n",
      "Iteration 21560 Training loss 0.045812368392944336 Validation loss 0.053262658417224884 Accuracy 0.8601250648498535\n",
      "Iteration 21570 Training loss 0.05386470630764961 Validation loss 0.05342046916484833 Accuracy 0.858500063419342\n",
      "Iteration 21580 Training loss 0.05301174893975258 Validation loss 0.05325159803032875 Accuracy 0.8596250414848328\n",
      "Iteration 21590 Training loss 0.05463719367980957 Validation loss 0.05338563397526741 Accuracy 0.8600000143051147\n",
      "Iteration 21600 Training loss 0.06134415417909622 Validation loss 0.05336443707346916 Accuracy 0.858625054359436\n",
      "Iteration 21610 Training loss 0.057146959006786346 Validation loss 0.05353296920657158 Accuracy 0.85875004529953\n",
      "Iteration 21620 Training loss 0.061402447521686554 Validation loss 0.053252387791872025 Accuracy 0.8593750596046448\n",
      "Iteration 21630 Training loss 0.04836975410580635 Validation loss 0.05326392501592636 Accuracy 0.8600000143051147\n",
      "Iteration 21640 Training loss 0.05368147790431976 Validation loss 0.05330204218626022 Accuracy 0.8603750467300415\n",
      "Iteration 21650 Training loss 0.053460147231817245 Validation loss 0.05366964265704155 Accuracy 0.8583750128746033\n",
      "Iteration 21660 Training loss 0.05089174583554268 Validation loss 0.05322747677564621 Accuracy 0.8601250648498535\n",
      "Iteration 21670 Training loss 0.05471653863787651 Validation loss 0.05321536958217621 Accuracy 0.8600000143051147\n",
      "Iteration 21680 Training loss 0.05737147107720375 Validation loss 0.05325063318014145 Accuracy 0.8598750233650208\n",
      "Iteration 21690 Training loss 0.05358068272471428 Validation loss 0.05375731736421585 Accuracy 0.8563750386238098\n",
      "Iteration 21700 Training loss 0.05461834371089935 Validation loss 0.05316399037837982 Accuracy 0.8601250648498535\n",
      "Iteration 21710 Training loss 0.05016982927918434 Validation loss 0.05327465385198593 Accuracy 0.858875036239624\n",
      "Iteration 21720 Training loss 0.05226578563451767 Validation loss 0.05316707119345665 Accuracy 0.8598750233650208\n",
      "Iteration 21730 Training loss 0.04759930446743965 Validation loss 0.05352533608675003 Accuracy 0.8566250205039978\n",
      "Iteration 21740 Training loss 0.05890313908457756 Validation loss 0.05350661650300026 Accuracy 0.8572500348091125\n",
      "Iteration 21750 Training loss 0.055015504360198975 Validation loss 0.053258027881383896 Accuracy 0.8593750596046448\n",
      "Iteration 21760 Training loss 0.05324818938970566 Validation loss 0.05326303467154503 Accuracy 0.858875036239624\n",
      "Iteration 21770 Training loss 0.04949101433157921 Validation loss 0.053148139268159866 Accuracy 0.8600000143051147\n",
      "Iteration 21780 Training loss 0.056144509464502335 Validation loss 0.05313696712255478 Accuracy 0.8606250286102295\n",
      "Iteration 21790 Training loss 0.06038070470094681 Validation loss 0.053171563893556595 Accuracy 0.8606250286102295\n",
      "Iteration 21800 Training loss 0.06040486320853233 Validation loss 0.053252264857292175 Accuracy 0.8593750596046448\n",
      "Iteration 21810 Training loss 0.0558706559240818 Validation loss 0.05318109691143036 Accuracy 0.8592500686645508\n",
      "Iteration 21820 Training loss 0.052336301654577255 Validation loss 0.05311973765492439 Accuracy 0.8600000143051147\n",
      "Iteration 21830 Training loss 0.04589937999844551 Validation loss 0.05313177779316902 Accuracy 0.8592500686645508\n",
      "Iteration 21840 Training loss 0.039751384407281876 Validation loss 0.05351141467690468 Accuracy 0.8573750257492065\n",
      "Iteration 21850 Training loss 0.04610937461256981 Validation loss 0.053174939006567 Accuracy 0.8596250414848328\n",
      "Iteration 21860 Training loss 0.05021487921476364 Validation loss 0.05314399302005768 Accuracy 0.8603750467300415\n",
      "Iteration 21870 Training loss 0.049846142530441284 Validation loss 0.053504932671785355 Accuracy 0.8578750491142273\n",
      "Iteration 21880 Training loss 0.05573035031557083 Validation loss 0.05313658341765404 Accuracy 0.8596250414848328\n",
      "Iteration 21890 Training loss 0.04690557345747948 Validation loss 0.053195346146821976 Accuracy 0.8593750596046448\n",
      "Iteration 21900 Training loss 0.0475921630859375 Validation loss 0.053080156445503235 Accuracy 0.8603750467300415\n",
      "Iteration 21910 Training loss 0.0532422810792923 Validation loss 0.053096704185009 Accuracy 0.8600000143051147\n",
      "Iteration 21920 Training loss 0.05590376630425453 Validation loss 0.05315500125288963 Accuracy 0.8592500686645508\n",
      "Iteration 21930 Training loss 0.05397367477416992 Validation loss 0.05311516672372818 Accuracy 0.8605000376701355\n",
      "Iteration 21940 Training loss 0.052074041217565536 Validation loss 0.05310547351837158 Accuracy 0.859125018119812\n",
      "Iteration 21950 Training loss 0.05594315752387047 Validation loss 0.05314192548394203 Accuracy 0.8593750596046448\n",
      "Iteration 21960 Training loss 0.050743844360113144 Validation loss 0.053075242787599564 Accuracy 0.8602500557899475\n",
      "Iteration 21970 Training loss 0.04825008660554886 Validation loss 0.05305136367678642 Accuracy 0.8598750233650208\n",
      "Iteration 21980 Training loss 0.05750851333141327 Validation loss 0.05303794518113136 Accuracy 0.8593750596046448\n",
      "Iteration 21990 Training loss 0.05585165694355965 Validation loss 0.05304410681128502 Accuracy 0.8598750233650208\n",
      "Iteration 22000 Training loss 0.04214686155319214 Validation loss 0.05337578058242798 Accuracy 0.8577500581741333\n",
      "Iteration 22010 Training loss 0.047440607100725174 Validation loss 0.053366221487522125 Accuracy 0.8571250438690186\n",
      "Iteration 22020 Training loss 0.05562898516654968 Validation loss 0.05307154729962349 Accuracy 0.859000027179718\n",
      "Iteration 22030 Training loss 0.047315988689661026 Validation loss 0.05310169234871864 Accuracy 0.859125018119812\n",
      "Iteration 22040 Training loss 0.055014800280332565 Validation loss 0.053046900779008865 Accuracy 0.8597500324249268\n",
      "Iteration 22050 Training loss 0.052522141486406326 Validation loss 0.05306105688214302 Accuracy 0.85875004529953\n",
      "Iteration 22060 Training loss 0.05107958987355232 Validation loss 0.053158555179834366 Accuracy 0.858875036239624\n",
      "Iteration 22070 Training loss 0.04971519485116005 Validation loss 0.05303436517715454 Accuracy 0.8597500324249268\n",
      "Iteration 22080 Training loss 0.044549550861120224 Validation loss 0.05305151268839836 Accuracy 0.8601250648498535\n",
      "Iteration 22090 Training loss 0.053639911115169525 Validation loss 0.053251590579748154 Accuracy 0.859125018119812\n",
      "Iteration 22100 Training loss 0.04963255673646927 Validation loss 0.053395338356494904 Accuracy 0.859000027179718\n",
      "Iteration 22110 Training loss 0.04906203970313072 Validation loss 0.05304689705371857 Accuracy 0.8595000505447388\n",
      "Iteration 22120 Training loss 0.050704434514045715 Validation loss 0.05315038189291954 Accuracy 0.858875036239624\n",
      "Iteration 22130 Training loss 0.058268554508686066 Validation loss 0.053264107555150986 Accuracy 0.858625054359436\n",
      "Iteration 22140 Training loss 0.046914514154195786 Validation loss 0.05303115025162697 Accuracy 0.8598750233650208\n",
      "Iteration 22150 Training loss 0.0502249151468277 Validation loss 0.05303201824426651 Accuracy 0.8596250414848328\n",
      "Iteration 22160 Training loss 0.05154746025800705 Validation loss 0.05305415391921997 Accuracy 0.861750066280365\n",
      "Iteration 22170 Training loss 0.0534890815615654 Validation loss 0.05309341102838516 Accuracy 0.8595000505447388\n",
      "Iteration 22180 Training loss 0.06122330576181412 Validation loss 0.05300590768456459 Accuracy 0.8598750233650208\n",
      "Iteration 22190 Training loss 0.05736459419131279 Validation loss 0.053036052733659744 Accuracy 0.8600000143051147\n",
      "Iteration 22200 Training loss 0.045783694833517075 Validation loss 0.05302208289504051 Accuracy 0.8598750233650208\n",
      "Iteration 22210 Training loss 0.04455243796110153 Validation loss 0.05300401151180267 Accuracy 0.8598750233650208\n",
      "Iteration 22220 Training loss 0.04999314621090889 Validation loss 0.0530763640999794 Accuracy 0.8612500429153442\n",
      "Iteration 22230 Training loss 0.04814288020133972 Validation loss 0.05300891026854515 Accuracy 0.8603750467300415\n",
      "Iteration 22240 Training loss 0.04729384183883667 Validation loss 0.05353790521621704 Accuracy 0.8572500348091125\n",
      "Iteration 22250 Training loss 0.05369609221816063 Validation loss 0.05318053811788559 Accuracy 0.85875004529953\n",
      "Iteration 22260 Training loss 0.0543660931289196 Validation loss 0.053136326372623444 Accuracy 0.858625054359436\n",
      "Iteration 22270 Training loss 0.0553806908428669 Validation loss 0.053017374128103256 Accuracy 0.8593750596046448\n",
      "Iteration 22280 Training loss 0.05897200480103493 Validation loss 0.05317404493689537 Accuracy 0.8582500219345093\n",
      "Iteration 22290 Training loss 0.05067696422338486 Validation loss 0.05320582166314125 Accuracy 0.8580000400543213\n",
      "Iteration 22300 Training loss 0.05755942314863205 Validation loss 0.053032286465168 Accuracy 0.8605000376701355\n",
      "Iteration 22310 Training loss 0.04949541762471199 Validation loss 0.053020838648080826 Accuracy 0.8582500219345093\n",
      "Iteration 22320 Training loss 0.05711809918284416 Validation loss 0.0529770702123642 Accuracy 0.8612500429153442\n",
      "Iteration 22330 Training loss 0.05503835529088974 Validation loss 0.05293348804116249 Accuracy 0.8602500557899475\n",
      "Iteration 22340 Training loss 0.0513598807156086 Validation loss 0.05296522006392479 Accuracy 0.8592500686645508\n",
      "Iteration 22350 Training loss 0.0539994053542614 Validation loss 0.05297540873289108 Accuracy 0.8598750233650208\n",
      "Iteration 22360 Training loss 0.05308626964688301 Validation loss 0.05292985960841179 Accuracy 0.8603750467300415\n",
      "Iteration 22370 Training loss 0.045488934963941574 Validation loss 0.05297643318772316 Accuracy 0.8592500686645508\n",
      "Iteration 22380 Training loss 0.04739517718553543 Validation loss 0.052931081503629684 Accuracy 0.8598750233650208\n",
      "Iteration 22390 Training loss 0.05606709420681 Validation loss 0.05319027230143547 Accuracy 0.8578750491142273\n",
      "Iteration 22400 Training loss 0.04966964200139046 Validation loss 0.052947480231523514 Accuracy 0.8607500195503235\n",
      "Iteration 22410 Training loss 0.05420372262597084 Validation loss 0.05291792377829552 Accuracy 0.8595000505447388\n",
      "Iteration 22420 Training loss 0.054873302578926086 Validation loss 0.05301615595817566 Accuracy 0.858500063419342\n",
      "Iteration 22430 Training loss 0.052659738808870316 Validation loss 0.052933815866708755 Accuracy 0.8608750700950623\n",
      "Iteration 22440 Training loss 0.048777129501104355 Validation loss 0.05321604013442993 Accuracy 0.8582500219345093\n",
      "Iteration 22450 Training loss 0.0512831024825573 Validation loss 0.05305492877960205 Accuracy 0.8608750700950623\n",
      "Iteration 22460 Training loss 0.050598811358213425 Validation loss 0.05300119146704674 Accuracy 0.8583750128746033\n",
      "Iteration 22470 Training loss 0.04699493944644928 Validation loss 0.05322634056210518 Accuracy 0.858625054359436\n",
      "Iteration 22480 Training loss 0.04779265820980072 Validation loss 0.05344977229833603 Accuracy 0.8581250309944153\n",
      "Iteration 22490 Training loss 0.05504826083779335 Validation loss 0.05397099629044533 Accuracy 0.8565000295639038\n",
      "Iteration 22500 Training loss 0.04824160411953926 Validation loss 0.05301378294825554 Accuracy 0.8580000400543213\n",
      "Iteration 22510 Training loss 0.05744398385286331 Validation loss 0.053244635462760925 Accuracy 0.859125018119812\n",
      "Iteration 22520 Training loss 0.05097511038184166 Validation loss 0.0528901070356369 Accuracy 0.8597500324249268\n",
      "Iteration 22530 Training loss 0.04857126623392105 Validation loss 0.05297957733273506 Accuracy 0.8602500557899475\n",
      "Iteration 22540 Training loss 0.0452326275408268 Validation loss 0.0532374382019043 Accuracy 0.858625054359436\n",
      "Iteration 22550 Training loss 0.04978189244866371 Validation loss 0.05294782668352127 Accuracy 0.8592500686645508\n",
      "Iteration 22560 Training loss 0.05578373000025749 Validation loss 0.05328942462801933 Accuracy 0.8577500581741333\n",
      "Iteration 22570 Training loss 0.055233754217624664 Validation loss 0.05282670259475708 Accuracy 0.8606250286102295\n",
      "Iteration 22580 Training loss 0.05422001704573631 Validation loss 0.05294262245297432 Accuracy 0.8592500686645508\n",
      "Iteration 22590 Training loss 0.04957493022084236 Validation loss 0.052820101380348206 Accuracy 0.8610000610351562\n",
      "Iteration 22600 Training loss 0.05038914084434509 Validation loss 0.05298784002661705 Accuracy 0.858625054359436\n",
      "Iteration 22610 Training loss 0.05251162871718407 Validation loss 0.05279083177447319 Accuracy 0.8603750467300415\n",
      "Iteration 22620 Training loss 0.04401664063334465 Validation loss 0.05281383916735649 Accuracy 0.8602500557899475\n",
      "Iteration 22630 Training loss 0.05067694932222366 Validation loss 0.05292695015668869 Accuracy 0.8593750596046448\n",
      "Iteration 22640 Training loss 0.05699579045176506 Validation loss 0.0529371052980423 Accuracy 0.8593750596046448\n",
      "Iteration 22650 Training loss 0.04614618793129921 Validation loss 0.05280618742108345 Accuracy 0.8607500195503235\n",
      "Iteration 22660 Training loss 0.04582207649946213 Validation loss 0.053091861307621 Accuracy 0.858875036239624\n",
      "Iteration 22670 Training loss 0.05080382525920868 Validation loss 0.05289221182465553 Accuracy 0.858875036239624\n",
      "Iteration 22680 Training loss 0.052753522992134094 Validation loss 0.05282573029398918 Accuracy 0.8602500557899475\n",
      "Iteration 22690 Training loss 0.05103149265050888 Validation loss 0.05293514207005501 Accuracy 0.8592500686645508\n",
      "Iteration 22700 Training loss 0.046628646552562714 Validation loss 0.052817631512880325 Accuracy 0.8592500686645508\n",
      "Iteration 22710 Training loss 0.04843434318900108 Validation loss 0.0527673065662384 Accuracy 0.8600000143051147\n",
      "Iteration 22720 Training loss 0.05831003561615944 Validation loss 0.052809007465839386 Accuracy 0.8613750338554382\n",
      "Iteration 22730 Training loss 0.051159895956516266 Validation loss 0.052770864218473434 Accuracy 0.8600000143051147\n",
      "Iteration 22740 Training loss 0.04385417327284813 Validation loss 0.052747342735528946 Accuracy 0.8602500557899475\n",
      "Iteration 22750 Training loss 0.05604398250579834 Validation loss 0.05285672843456268 Accuracy 0.8602500557899475\n",
      "Iteration 22760 Training loss 0.05133616924285889 Validation loss 0.05282067507505417 Accuracy 0.8593750596046448\n",
      "Iteration 22770 Training loss 0.061290841549634933 Validation loss 0.05309082940220833 Accuracy 0.858500063419342\n",
      "Iteration 22780 Training loss 0.055882856249809265 Validation loss 0.052816640585660934 Accuracy 0.8606250286102295\n",
      "Iteration 22790 Training loss 0.04509802535176277 Validation loss 0.052980199456214905 Accuracy 0.8596250414848328\n",
      "Iteration 22800 Training loss 0.049623358994722366 Validation loss 0.05308028310537338 Accuracy 0.859125018119812\n",
      "Iteration 22810 Training loss 0.046988144516944885 Validation loss 0.0527108795940876 Accuracy 0.8606250286102295\n",
      "Iteration 22820 Training loss 0.053351230919361115 Validation loss 0.05272787809371948 Accuracy 0.8605000376701355\n",
      "Iteration 22830 Training loss 0.05935315787792206 Validation loss 0.052831728011369705 Accuracy 0.8612500429153442\n",
      "Iteration 22840 Training loss 0.04861746355891228 Validation loss 0.052701257169246674 Accuracy 0.8600000143051147\n",
      "Iteration 22850 Training loss 0.049702245742082596 Validation loss 0.052706606686115265 Accuracy 0.8615000247955322\n",
      "Iteration 22860 Training loss 0.053020376712083817 Validation loss 0.052726976573467255 Accuracy 0.862250030040741\n",
      "Iteration 22870 Training loss 0.052635423839092255 Validation loss 0.052657805383205414 Accuracy 0.8606250286102295\n",
      "Iteration 22880 Training loss 0.05712959170341492 Validation loss 0.05282192677259445 Accuracy 0.8596250414848328\n",
      "Iteration 22890 Training loss 0.04796494543552399 Validation loss 0.05281253531575203 Accuracy 0.8608750700950623\n",
      "Iteration 22900 Training loss 0.05375375598669052 Validation loss 0.052609965205192566 Accuracy 0.8616250157356262\n",
      "Iteration 22910 Training loss 0.048077523708343506 Validation loss 0.052865467965602875 Accuracy 0.8595000505447388\n",
      "Iteration 22920 Training loss 0.05260370299220085 Validation loss 0.052687402814626694 Accuracy 0.8600000143051147\n",
      "Iteration 22930 Training loss 0.04730067029595375 Validation loss 0.052631087601184845 Accuracy 0.8615000247955322\n",
      "Iteration 22940 Training loss 0.0624031238257885 Validation loss 0.05266817286610603 Accuracy 0.8611250519752502\n",
      "Iteration 22950 Training loss 0.050961095839738846 Validation loss 0.052668772637844086 Accuracy 0.8612500429153442\n",
      "Iteration 22960 Training loss 0.05320687219500542 Validation loss 0.0526731051504612 Accuracy 0.8612500429153442\n",
      "Iteration 22970 Training loss 0.05211641266942024 Validation loss 0.052628111094236374 Accuracy 0.8611250519752502\n",
      "Iteration 22980 Training loss 0.04239194840192795 Validation loss 0.052640609443187714 Accuracy 0.8606250286102295\n",
      "Iteration 22990 Training loss 0.05519821122288704 Validation loss 0.05266088247299194 Accuracy 0.8611250519752502\n",
      "Iteration 23000 Training loss 0.04376494139432907 Validation loss 0.05294566601514816 Accuracy 0.858875036239624\n",
      "Iteration 23010 Training loss 0.052124567329883575 Validation loss 0.05264627933502197 Accuracy 0.8610000610351562\n",
      "Iteration 23020 Training loss 0.04449564963579178 Validation loss 0.052596643567085266 Accuracy 0.862000048160553\n",
      "Iteration 23030 Training loss 0.05285176634788513 Validation loss 0.05274302884936333 Accuracy 0.859125018119812\n",
      "Iteration 23040 Training loss 0.04647218436002731 Validation loss 0.052629563957452774 Accuracy 0.8616250157356262\n",
      "Iteration 23050 Training loss 0.05373712256550789 Validation loss 0.05262311175465584 Accuracy 0.8607500195503235\n",
      "Iteration 23060 Training loss 0.04902331158518791 Validation loss 0.052599627524614334 Accuracy 0.861750066280365\n",
      "Iteration 23070 Training loss 0.04596288502216339 Validation loss 0.052614886313676834 Accuracy 0.8608750700950623\n",
      "Iteration 23080 Training loss 0.05461631342768669 Validation loss 0.052791059017181396 Accuracy 0.8605000376701355\n",
      "Iteration 23090 Training loss 0.05186441168189049 Validation loss 0.052842944860458374 Accuracy 0.8598750233650208\n",
      "Iteration 23100 Training loss 0.050204962491989136 Validation loss 0.05260080099105835 Accuracy 0.8603750467300415\n",
      "Iteration 23110 Training loss 0.048418886959552765 Validation loss 0.05286233127117157 Accuracy 0.8597500324249268\n",
      "Iteration 23120 Training loss 0.04737634211778641 Validation loss 0.05272836610674858 Accuracy 0.8607500195503235\n",
      "Iteration 23130 Training loss 0.05935192480683327 Validation loss 0.052626390010118484 Accuracy 0.8603750467300415\n",
      "Iteration 23140 Training loss 0.05297383293509483 Validation loss 0.052658166736364365 Accuracy 0.8600000143051147\n",
      "Iteration 23150 Training loss 0.04887143895030022 Validation loss 0.052607759833335876 Accuracy 0.8598750233650208\n",
      "Iteration 23160 Training loss 0.05577215179800987 Validation loss 0.05261721834540367 Accuracy 0.8612500429153442\n",
      "Iteration 23170 Training loss 0.04932709410786629 Validation loss 0.052555833011865616 Accuracy 0.8600000143051147\n",
      "Iteration 23180 Training loss 0.04576978459954262 Validation loss 0.05257940664887428 Accuracy 0.8612500429153442\n",
      "Iteration 23190 Training loss 0.05335104465484619 Validation loss 0.052511170506477356 Accuracy 0.8602500557899475\n",
      "Iteration 23200 Training loss 0.05138318985700607 Validation loss 0.05259628966450691 Accuracy 0.8615000247955322\n",
      "Iteration 23210 Training loss 0.05123679339885712 Validation loss 0.05250123143196106 Accuracy 0.8606250286102295\n",
      "Iteration 23220 Training loss 0.051582079380750656 Validation loss 0.052583396434783936 Accuracy 0.8597500324249268\n",
      "Iteration 23230 Training loss 0.04659871384501457 Validation loss 0.05269499495625496 Accuracy 0.8595000505447388\n",
      "Iteration 23240 Training loss 0.04625346139073372 Validation loss 0.052499208599328995 Accuracy 0.8603750467300415\n",
      "Iteration 23250 Training loss 0.047408681362867355 Validation loss 0.052550703287124634 Accuracy 0.8597500324249268\n",
      "Iteration 23260 Training loss 0.047898709774017334 Validation loss 0.0525590218603611 Accuracy 0.8601250648498535\n",
      "Iteration 23270 Training loss 0.050857409834861755 Validation loss 0.05254150554537773 Accuracy 0.8596250414848328\n",
      "Iteration 23280 Training loss 0.04373377561569214 Validation loss 0.05249727517366409 Accuracy 0.8595000505447388\n",
      "Iteration 23290 Training loss 0.043182339519262314 Validation loss 0.05246018245816231 Accuracy 0.8615000247955322\n",
      "Iteration 23300 Training loss 0.05374075844883919 Validation loss 0.052492495626211166 Accuracy 0.8612500429153442\n",
      "Iteration 23310 Training loss 0.051058877259492874 Validation loss 0.05262856185436249 Accuracy 0.8613750338554382\n",
      "Iteration 23320 Training loss 0.04454483464360237 Validation loss 0.0526193343102932 Accuracy 0.8610000610351562\n",
      "Iteration 23330 Training loss 0.05122250318527222 Validation loss 0.05250032991170883 Accuracy 0.8605000376701355\n",
      "Iteration 23340 Training loss 0.05562416836619377 Validation loss 0.05265388637781143 Accuracy 0.8600000143051147\n",
      "Iteration 23350 Training loss 0.06023591011762619 Validation loss 0.052547745406627655 Accuracy 0.8611250519752502\n",
      "Iteration 23360 Training loss 0.0497514009475708 Validation loss 0.05243847146630287 Accuracy 0.8613750338554382\n",
      "Iteration 23370 Training loss 0.050774574279785156 Validation loss 0.05271249637007713 Accuracy 0.8597500324249268\n",
      "Iteration 23380 Training loss 0.05286189913749695 Validation loss 0.05256626009941101 Accuracy 0.8601250648498535\n",
      "Iteration 23390 Training loss 0.04899827018380165 Validation loss 0.052529893815517426 Accuracy 0.8612500429153442\n",
      "Iteration 23400 Training loss 0.05453891679644585 Validation loss 0.05247221514582634 Accuracy 0.8607500195503235\n",
      "Iteration 23410 Training loss 0.05004826933145523 Validation loss 0.05244998633861542 Accuracy 0.8607500195503235\n",
      "Iteration 23420 Training loss 0.04621557518839836 Validation loss 0.052422575652599335 Accuracy 0.8613750338554382\n",
      "Iteration 23430 Training loss 0.050088070333004 Validation loss 0.052419502288103104 Accuracy 0.8615000247955322\n",
      "Iteration 23440 Training loss 0.05174138769507408 Validation loss 0.05238521099090576 Accuracy 0.8612500429153442\n",
      "Iteration 23450 Training loss 0.04649689048528671 Validation loss 0.052402157336473465 Accuracy 0.8611250519752502\n",
      "Iteration 23460 Training loss 0.04211312532424927 Validation loss 0.052521366626024246 Accuracy 0.8595000505447388\n",
      "Iteration 23470 Training loss 0.054585978388786316 Validation loss 0.05280059203505516 Accuracy 0.8597500324249268\n",
      "Iteration 23480 Training loss 0.05729000270366669 Validation loss 0.05243009328842163 Accuracy 0.8605000376701355\n",
      "Iteration 23490 Training loss 0.040492042899131775 Validation loss 0.0525222010910511 Accuracy 0.8613750338554382\n",
      "Iteration 23500 Training loss 0.03682830557227135 Validation loss 0.05279621109366417 Accuracy 0.8595000505447388\n",
      "Iteration 23510 Training loss 0.04591437056660652 Validation loss 0.05243752524256706 Accuracy 0.8598750233650208\n",
      "Iteration 23520 Training loss 0.050318386405706406 Validation loss 0.05244000628590584 Accuracy 0.8603750467300415\n",
      "Iteration 23530 Training loss 0.0410119965672493 Validation loss 0.05242370069026947 Accuracy 0.8612500429153442\n",
      "Iteration 23540 Training loss 0.04893732815980911 Validation loss 0.05259961262345314 Accuracy 0.8598750233650208\n",
      "Iteration 23550 Training loss 0.05151870474219322 Validation loss 0.05242792144417763 Accuracy 0.8612500429153442\n",
      "Iteration 23560 Training loss 0.050463028252124786 Validation loss 0.052655357867479324 Accuracy 0.8596250414848328\n",
      "Iteration 23570 Training loss 0.05056329444050789 Validation loss 0.05236727371811867 Accuracy 0.8613750338554382\n",
      "Iteration 23580 Training loss 0.058752257376909256 Validation loss 0.05237041413784027 Accuracy 0.8615000247955322\n",
      "Iteration 23590 Training loss 0.0552484393119812 Validation loss 0.05313733220100403 Accuracy 0.8592500686645508\n",
      "Iteration 23600 Training loss 0.043935880064964294 Validation loss 0.05243357643485069 Accuracy 0.8610000610351562\n",
      "Iteration 23610 Training loss 0.059658922255039215 Validation loss 0.05236139893531799 Accuracy 0.8607500195503235\n",
      "Iteration 23620 Training loss 0.04676620289683342 Validation loss 0.052316684275865555 Accuracy 0.861750066280365\n",
      "Iteration 23630 Training loss 0.04643094539642334 Validation loss 0.05236915498971939 Accuracy 0.8611250519752502\n",
      "Iteration 23640 Training loss 0.04200267791748047 Validation loss 0.052295587956905365 Accuracy 0.862000048160553\n",
      "Iteration 23650 Training loss 0.052272114902734756 Validation loss 0.05264822021126747 Accuracy 0.8606250286102295\n",
      "Iteration 23660 Training loss 0.055684249848127365 Validation loss 0.052604567259550095 Accuracy 0.8610000610351562\n",
      "Iteration 23670 Training loss 0.04568958654999733 Validation loss 0.05238042026758194 Accuracy 0.8615000247955322\n",
      "Iteration 23680 Training loss 0.04853347688913345 Validation loss 0.05242454260587692 Accuracy 0.8606250286102295\n",
      "Iteration 23690 Training loss 0.0441334992647171 Validation loss 0.05235333368182182 Accuracy 0.8612500429153442\n",
      "Iteration 23700 Training loss 0.06175926700234413 Validation loss 0.05250868573784828 Accuracy 0.8597500324249268\n",
      "Iteration 23710 Training loss 0.04497213661670685 Validation loss 0.052349191159009933 Accuracy 0.8610000610351562\n",
      "Iteration 23720 Training loss 0.05785280093550682 Validation loss 0.052750878036022186 Accuracy 0.8597500324249268\n",
      "Iteration 23730 Training loss 0.047027792781591415 Validation loss 0.05282211676239967 Accuracy 0.8597500324249268\n",
      "Iteration 23740 Training loss 0.04252723604440689 Validation loss 0.052420858293771744 Accuracy 0.8602500557899475\n",
      "Iteration 23750 Training loss 0.04661306366324425 Validation loss 0.05257406830787659 Accuracy 0.8596250414848328\n",
      "Iteration 23760 Training loss 0.04670866206288338 Validation loss 0.05249427631497383 Accuracy 0.8600000143051147\n",
      "Iteration 23770 Training loss 0.05662935972213745 Validation loss 0.052348434925079346 Accuracy 0.8608750700950623\n",
      "Iteration 23780 Training loss 0.046379994601011276 Validation loss 0.052372902631759644 Accuracy 0.8607500195503235\n",
      "Iteration 23790 Training loss 0.054854631423950195 Validation loss 0.0523497499525547 Accuracy 0.861875057220459\n",
      "Iteration 23800 Training loss 0.04880750924348831 Validation loss 0.05230396240949631 Accuracy 0.862125039100647\n",
      "Iteration 23810 Training loss 0.05055107921361923 Validation loss 0.0522882342338562 Accuracy 0.861875057220459\n",
      "Iteration 23820 Training loss 0.045578908175230026 Validation loss 0.05228744074702263 Accuracy 0.8627500534057617\n",
      "Iteration 23830 Training loss 0.048574332147836685 Validation loss 0.05230507627129555 Accuracy 0.862500011920929\n",
      "Iteration 23840 Training loss 0.057206492871046066 Validation loss 0.05249441787600517 Accuracy 0.8603750467300415\n",
      "Iteration 23850 Training loss 0.04642771929502487 Validation loss 0.05235471576452255 Accuracy 0.8615000247955322\n",
      "Iteration 23860 Training loss 0.05092328414320946 Validation loss 0.05229444429278374 Accuracy 0.862125039100647\n",
      "Iteration 23870 Training loss 0.04754564166069031 Validation loss 0.05231352150440216 Accuracy 0.862500011920929\n",
      "Iteration 23880 Training loss 0.05523200333118439 Validation loss 0.052266791462898254 Accuracy 0.862000048160553\n",
      "Iteration 23890 Training loss 0.05082398280501366 Validation loss 0.052272532135248184 Accuracy 0.862500011920929\n",
      "Iteration 23900 Training loss 0.057137381285429 Validation loss 0.05243051052093506 Accuracy 0.8608750700950623\n",
      "Iteration 23910 Training loss 0.05505027994513512 Validation loss 0.052305109798908234 Accuracy 0.861750066280365\n",
      "Iteration 23920 Training loss 0.044469527900218964 Validation loss 0.052314117550849915 Accuracy 0.862125039100647\n",
      "Iteration 23930 Training loss 0.05594389885663986 Validation loss 0.05235607549548149 Accuracy 0.8608750700950623\n",
      "Iteration 23940 Training loss 0.04622916132211685 Validation loss 0.05227915197610855 Accuracy 0.8615000247955322\n",
      "Iteration 23950 Training loss 0.05537163093686104 Validation loss 0.05232942849397659 Accuracy 0.8610000610351562\n",
      "Iteration 23960 Training loss 0.048895079642534256 Validation loss 0.05223246291279793 Accuracy 0.8616250157356262\n",
      "Iteration 23970 Training loss 0.05262542515993118 Validation loss 0.05236465111374855 Accuracy 0.8608750700950623\n",
      "Iteration 23980 Training loss 0.05075132101774216 Validation loss 0.052191972732543945 Accuracy 0.862125039100647\n",
      "Iteration 23990 Training loss 0.05379718542098999 Validation loss 0.05218721181154251 Accuracy 0.862500011920929\n",
      "Iteration 24000 Training loss 0.05349091812968254 Validation loss 0.05217508226633072 Accuracy 0.8628750443458557\n",
      "Iteration 24010 Training loss 0.04207061976194382 Validation loss 0.052145425230264664 Accuracy 0.862000048160553\n",
      "Iteration 24020 Training loss 0.043209295719861984 Validation loss 0.0521387904882431 Accuracy 0.8626250624656677\n",
      "Iteration 24030 Training loss 0.054464854300022125 Validation loss 0.052156079560518265 Accuracy 0.8616250157356262\n",
      "Iteration 24040 Training loss 0.05057825893163681 Validation loss 0.05217086896300316 Accuracy 0.8607500195503235\n",
      "Iteration 24050 Training loss 0.049300555139780045 Validation loss 0.05224334076046944 Accuracy 0.8610000610351562\n",
      "Iteration 24060 Training loss 0.05287037789821625 Validation loss 0.052132852375507355 Accuracy 0.862000048160553\n",
      "Iteration 24070 Training loss 0.05100155621767044 Validation loss 0.05214519798755646 Accuracy 0.862500011920929\n",
      "Iteration 24080 Training loss 0.05585001781582832 Validation loss 0.052525829523801804 Accuracy 0.8601250648498535\n",
      "Iteration 24090 Training loss 0.04379772022366524 Validation loss 0.05210217460989952 Accuracy 0.862500011920929\n",
      "Iteration 24100 Training loss 0.05315553396940231 Validation loss 0.05211392790079117 Accuracy 0.8630000352859497\n",
      "Iteration 24110 Training loss 0.048659056425094604 Validation loss 0.052141353487968445 Accuracy 0.862250030040741\n",
      "Iteration 24120 Training loss 0.04929029196500778 Validation loss 0.05222323164343834 Accuracy 0.8610000610351562\n",
      "Iteration 24130 Training loss 0.04854053258895874 Validation loss 0.052144855260849 Accuracy 0.862250030040741\n",
      "Iteration 24140 Training loss 0.05177152156829834 Validation loss 0.05214247480034828 Accuracy 0.8616250157356262\n",
      "Iteration 24150 Training loss 0.04389726370573044 Validation loss 0.05211569741368294 Accuracy 0.8631250262260437\n",
      "Iteration 24160 Training loss 0.04816292226314545 Validation loss 0.052146486937999725 Accuracy 0.8615000247955322\n",
      "Iteration 24170 Training loss 0.05641881003975868 Validation loss 0.05213623121380806 Accuracy 0.862125039100647\n",
      "Iteration 24180 Training loss 0.0501587837934494 Validation loss 0.052276596426963806 Accuracy 0.8610000610351562\n",
      "Iteration 24190 Training loss 0.045350152999162674 Validation loss 0.052167970687150955 Accuracy 0.8608750700950623\n",
      "Iteration 24200 Training loss 0.04992508888244629 Validation loss 0.05208829045295715 Accuracy 0.861875057220459\n",
      "Iteration 24210 Training loss 0.052330732345581055 Validation loss 0.05206657573580742 Accuracy 0.862125039100647\n",
      "Iteration 24220 Training loss 0.05356524884700775 Validation loss 0.05273247882723808 Accuracy 0.858875036239624\n",
      "Iteration 24230 Training loss 0.052582625299692154 Validation loss 0.05207853019237518 Accuracy 0.8612500429153442\n",
      "Iteration 24240 Training loss 0.049865543842315674 Validation loss 0.052141934633255005 Accuracy 0.862250030040741\n",
      "Iteration 24250 Training loss 0.0376121886074543 Validation loss 0.05210155248641968 Accuracy 0.8608750700950623\n",
      "Iteration 24260 Training loss 0.049935441464185715 Validation loss 0.05211448669433594 Accuracy 0.8606250286102295\n",
      "Iteration 24270 Training loss 0.046911418437957764 Validation loss 0.052069757133722305 Accuracy 0.8606250286102295\n",
      "Iteration 24280 Training loss 0.04678267240524292 Validation loss 0.05207282677292824 Accuracy 0.8612500429153442\n",
      "Iteration 24290 Training loss 0.047414615750312805 Validation loss 0.052090417593717575 Accuracy 0.862375020980835\n",
      "Iteration 24300 Training loss 0.05599330738186836 Validation loss 0.05220013111829758 Accuracy 0.8613750338554382\n",
      "Iteration 24310 Training loss 0.055883120745420456 Validation loss 0.05208173021674156 Accuracy 0.861750066280365\n",
      "Iteration 24320 Training loss 0.0555843822658062 Validation loss 0.05216628313064575 Accuracy 0.8610000610351562\n",
      "Iteration 24330 Training loss 0.04704222455620766 Validation loss 0.052740197628736496 Accuracy 0.8610000610351562\n",
      "Iteration 24340 Training loss 0.046506576240062714 Validation loss 0.052035827189683914 Accuracy 0.862250030040741\n",
      "Iteration 24350 Training loss 0.05093011632561684 Validation loss 0.05199860781431198 Accuracy 0.8627500534057617\n",
      "Iteration 24360 Training loss 0.05759384110569954 Validation loss 0.05200311169028282 Accuracy 0.8631250262260437\n",
      "Iteration 24370 Training loss 0.05459900572896004 Validation loss 0.05202812701463699 Accuracy 0.8630000352859497\n",
      "Iteration 24380 Training loss 0.05657466873526573 Validation loss 0.0519905723631382 Accuracy 0.862250030040741\n",
      "Iteration 24390 Training loss 0.04890363663434982 Validation loss 0.05202076584100723 Accuracy 0.861750066280365\n",
      "Iteration 24400 Training loss 0.0387054942548275 Validation loss 0.052019741386175156 Accuracy 0.862000048160553\n",
      "Iteration 24410 Training loss 0.04675712808966637 Validation loss 0.05200609192252159 Accuracy 0.8633750677108765\n",
      "Iteration 24420 Training loss 0.053258076310157776 Validation loss 0.05251681059598923 Accuracy 0.8605000376701355\n",
      "Iteration 24430 Training loss 0.04686153307557106 Validation loss 0.05204144865274429 Accuracy 0.862125039100647\n",
      "Iteration 24440 Training loss 0.045037344098091125 Validation loss 0.05201086774468422 Accuracy 0.861875057220459\n",
      "Iteration 24450 Training loss 0.04866013303399086 Validation loss 0.05199841037392616 Accuracy 0.862375020980835\n",
      "Iteration 24460 Training loss 0.05414220690727234 Validation loss 0.052250076085329056 Accuracy 0.8611250519752502\n",
      "Iteration 24470 Training loss 0.0459422767162323 Validation loss 0.05253397673368454 Accuracy 0.8602500557899475\n",
      "Iteration 24480 Training loss 0.048113156110048294 Validation loss 0.05219557136297226 Accuracy 0.8608750700950623\n",
      "Iteration 24490 Training loss 0.04968541860580444 Validation loss 0.052055757492780685 Accuracy 0.8627500534057617\n",
      "Iteration 24500 Training loss 0.05013667047023773 Validation loss 0.052215807139873505 Accuracy 0.8607500195503235\n",
      "Iteration 24510 Training loss 0.046813711524009705 Validation loss 0.05203421413898468 Accuracy 0.862250030040741\n",
      "Iteration 24520 Training loss 0.04992111027240753 Validation loss 0.05200326070189476 Accuracy 0.8633750677108765\n",
      "Iteration 24530 Training loss 0.05842253565788269 Validation loss 0.05219158157706261 Accuracy 0.8605000376701355\n",
      "Iteration 24540 Training loss 0.04463433101773262 Validation loss 0.05208215489983559 Accuracy 0.861750066280365\n",
      "Iteration 24550 Training loss 0.050570353865623474 Validation loss 0.05252508446574211 Accuracy 0.8593750596046448\n",
      "Iteration 24560 Training loss 0.052547868341207504 Validation loss 0.05200217664241791 Accuracy 0.862250030040741\n",
      "Iteration 24570 Training loss 0.05179747939109802 Validation loss 0.05194877088069916 Accuracy 0.8637500405311584\n",
      "Iteration 24580 Training loss 0.04106699302792549 Validation loss 0.05200734734535217 Accuracy 0.8616250157356262\n",
      "Iteration 24590 Training loss 0.04728111997246742 Validation loss 0.051926422864198685 Accuracy 0.862000048160553\n",
      "Iteration 24600 Training loss 0.05430231988430023 Validation loss 0.05191446840763092 Accuracy 0.8630000352859497\n",
      "Iteration 24610 Training loss 0.047070443630218506 Validation loss 0.052034251391887665 Accuracy 0.862250030040741\n",
      "Iteration 24620 Training loss 0.05376593768596649 Validation loss 0.05191487818956375 Accuracy 0.8626250624656677\n",
      "Iteration 24630 Training loss 0.05232809856534004 Validation loss 0.051939066499471664 Accuracy 0.8628750443458557\n",
      "Iteration 24640 Training loss 0.048597678542137146 Validation loss 0.05186847969889641 Accuracy 0.862375020980835\n",
      "Iteration 24650 Training loss 0.04569659009575844 Validation loss 0.05201065167784691 Accuracy 0.862375020980835\n",
      "Iteration 24660 Training loss 0.0500461719930172 Validation loss 0.05187160149216652 Accuracy 0.861750066280365\n",
      "Iteration 24670 Training loss 0.04881201684474945 Validation loss 0.051920175552368164 Accuracy 0.862375020980835\n",
      "Iteration 24680 Training loss 0.05216808244585991 Validation loss 0.051894526928663254 Accuracy 0.862500011920929\n",
      "Iteration 24690 Training loss 0.03409130126237869 Validation loss 0.051883161067962646 Accuracy 0.862375020980835\n",
      "Iteration 24700 Training loss 0.04309086874127388 Validation loss 0.051872678101062775 Accuracy 0.862250030040741\n",
      "Iteration 24710 Training loss 0.05620031803846359 Validation loss 0.05195636302232742 Accuracy 0.8616250157356262\n",
      "Iteration 24720 Training loss 0.05886329337954521 Validation loss 0.051968272775411606 Accuracy 0.8615000247955322\n",
      "Iteration 24730 Training loss 0.04737585410475731 Validation loss 0.0519152507185936 Accuracy 0.8635000586509705\n",
      "Iteration 24740 Training loss 0.04323050007224083 Validation loss 0.051895394921302795 Accuracy 0.8628750443458557\n",
      "Iteration 24750 Training loss 0.0489453487098217 Validation loss 0.05190618708729744 Accuracy 0.8628750443458557\n",
      "Iteration 24760 Training loss 0.041968945413827896 Validation loss 0.05204791575670242 Accuracy 0.8612500429153442\n",
      "Iteration 24770 Training loss 0.05148673802614212 Validation loss 0.05184795334935188 Accuracy 0.862125039100647\n",
      "Iteration 24780 Training loss 0.044995177537202835 Validation loss 0.051882579922676086 Accuracy 0.862250030040741\n",
      "Iteration 24790 Training loss 0.057816751301288605 Validation loss 0.051861170679330826 Accuracy 0.8627500534057617\n",
      "Iteration 24800 Training loss 0.048012588173151016 Validation loss 0.051840610802173615 Accuracy 0.8626250624656677\n",
      "Iteration 24810 Training loss 0.04207421839237213 Validation loss 0.05183637514710426 Accuracy 0.8626250624656677\n",
      "Iteration 24820 Training loss 0.04483281075954437 Validation loss 0.0519099086523056 Accuracy 0.8626250624656677\n",
      "Iteration 24830 Training loss 0.0443403534591198 Validation loss 0.05185903236269951 Accuracy 0.8627500534057617\n",
      "Iteration 24840 Training loss 0.049420394003391266 Validation loss 0.052119165658950806 Accuracy 0.8610000610351562\n",
      "Iteration 24850 Training loss 0.05237575247883797 Validation loss 0.05209683999419212 Accuracy 0.8613750338554382\n",
      "Iteration 24860 Training loss 0.0553598627448082 Validation loss 0.05213603749871254 Accuracy 0.8612500429153442\n",
      "Iteration 24870 Training loss 0.049995459616184235 Validation loss 0.05185447260737419 Accuracy 0.8630000352859497\n",
      "Iteration 24880 Training loss 0.05613311752676964 Validation loss 0.051878198981285095 Accuracy 0.8632500171661377\n",
      "Iteration 24890 Training loss 0.0434318482875824 Validation loss 0.0519549623131752 Accuracy 0.8627500534057617\n",
      "Iteration 24900 Training loss 0.05366257578134537 Validation loss 0.05185861885547638 Accuracy 0.862500011920929\n",
      "Iteration 24910 Training loss 0.05328739061951637 Validation loss 0.051892999559640884 Accuracy 0.862500011920929\n",
      "Iteration 24920 Training loss 0.04929820075631142 Validation loss 0.052174508571624756 Accuracy 0.862375020980835\n",
      "Iteration 24930 Training loss 0.060726601630449295 Validation loss 0.05182330682873726 Accuracy 0.862250030040741\n",
      "Iteration 24940 Training loss 0.04934006556868553 Validation loss 0.05187729001045227 Accuracy 0.861875057220459\n",
      "Iteration 24950 Training loss 0.04961128532886505 Validation loss 0.05199456959962845 Accuracy 0.862500011920929\n",
      "Iteration 24960 Training loss 0.04409581050276756 Validation loss 0.051822975277900696 Accuracy 0.8631250262260437\n",
      "Iteration 24970 Training loss 0.056741952896118164 Validation loss 0.051807016134262085 Accuracy 0.8640000224113464\n",
      "Iteration 24980 Training loss 0.0496084950864315 Validation loss 0.0519227534532547 Accuracy 0.8626250624656677\n",
      "Iteration 24990 Training loss 0.04998660832643509 Validation loss 0.051826778799295425 Accuracy 0.8635000586509705\n",
      "Iteration 25000 Training loss 0.051230695098638535 Validation loss 0.052046578377485275 Accuracy 0.861875057220459\n",
      "Iteration 25010 Training loss 0.04149812459945679 Validation loss 0.051814302802085876 Accuracy 0.862500011920929\n",
      "Iteration 25020 Training loss 0.04850177839398384 Validation loss 0.05177536979317665 Accuracy 0.8632500171661377\n",
      "Iteration 25030 Training loss 0.04929402098059654 Validation loss 0.051918551325798035 Accuracy 0.862250030040741\n",
      "Iteration 25040 Training loss 0.05549544095993042 Validation loss 0.05193132907152176 Accuracy 0.862000048160553\n",
      "Iteration 25050 Training loss 0.05585285648703575 Validation loss 0.05183761194348335 Accuracy 0.8628750443458557\n",
      "Iteration 25060 Training loss 0.04851745441555977 Validation loss 0.05181334912776947 Accuracy 0.8637500405311584\n",
      "Iteration 25070 Training loss 0.04311665892601013 Validation loss 0.051772698760032654 Accuracy 0.8633750677108765\n",
      "Iteration 25080 Training loss 0.051321204751729965 Validation loss 0.05232271924614906 Accuracy 0.8615000247955322\n",
      "Iteration 25090 Training loss 0.05034820735454559 Validation loss 0.05177430808544159 Accuracy 0.8635000586509705\n",
      "Iteration 25100 Training loss 0.05038389191031456 Validation loss 0.05182769522070885 Accuracy 0.8632500171661377\n",
      "Iteration 25110 Training loss 0.043076351284980774 Validation loss 0.051791299134492874 Accuracy 0.8638750314712524\n",
      "Iteration 25120 Training loss 0.04904583469033241 Validation loss 0.05179634317755699 Accuracy 0.8635000586509705\n",
      "Iteration 25130 Training loss 0.05096571147441864 Validation loss 0.05178247392177582 Accuracy 0.8636250495910645\n",
      "Iteration 25140 Training loss 0.05525492504239082 Validation loss 0.05212899670004845 Accuracy 0.8610000610351562\n",
      "Iteration 25150 Training loss 0.042785391211509705 Validation loss 0.05188428610563278 Accuracy 0.862500011920929\n",
      "Iteration 25160 Training loss 0.049048393964767456 Validation loss 0.05180928483605385 Accuracy 0.8635000586509705\n",
      "Iteration 25170 Training loss 0.04990389198064804 Validation loss 0.05174039304256439 Accuracy 0.862375020980835\n",
      "Iteration 25180 Training loss 0.05610664561390877 Validation loss 0.051713116466999054 Accuracy 0.8626250624656677\n",
      "Iteration 25190 Training loss 0.04091845452785492 Validation loss 0.0517193041741848 Accuracy 0.8631250262260437\n",
      "Iteration 25200 Training loss 0.04035717993974686 Validation loss 0.05183248966932297 Accuracy 0.8633750677108765\n",
      "Iteration 25210 Training loss 0.05053965374827385 Validation loss 0.0516827292740345 Accuracy 0.8635000586509705\n",
      "Iteration 25220 Training loss 0.04632196202874184 Validation loss 0.05174203962087631 Accuracy 0.8641250133514404\n",
      "Iteration 25230 Training loss 0.04224403202533722 Validation loss 0.05168452113866806 Accuracy 0.8627500534057617\n",
      "Iteration 25240 Training loss 0.046751100569963455 Validation loss 0.05177650600671768 Accuracy 0.8632500171661377\n",
      "Iteration 25250 Training loss 0.048765409737825394 Validation loss 0.0517544150352478 Accuracy 0.8632500171661377\n",
      "Iteration 25260 Training loss 0.05180354043841362 Validation loss 0.05177881196141243 Accuracy 0.8633750677108765\n",
      "Iteration 25270 Training loss 0.05199269577860832 Validation loss 0.051686063408851624 Accuracy 0.8633750677108765\n",
      "Iteration 25280 Training loss 0.046974554657936096 Validation loss 0.05188027024269104 Accuracy 0.862125039100647\n",
      "Iteration 25290 Training loss 0.03727535530924797 Validation loss 0.0517345666885376 Accuracy 0.8643750548362732\n",
      "Iteration 25300 Training loss 0.044433627277612686 Validation loss 0.05167991295456886 Accuracy 0.8626250624656677\n",
      "Iteration 25310 Training loss 0.05450555309653282 Validation loss 0.05167306587100029 Accuracy 0.8641250133514404\n",
      "Iteration 25320 Training loss 0.057407770305871964 Validation loss 0.05171993747353554 Accuracy 0.8643750548362732\n",
      "Iteration 25330 Training loss 0.0503314808011055 Validation loss 0.051825735718011856 Accuracy 0.8635000586509705\n",
      "Iteration 25340 Training loss 0.061699628829956055 Validation loss 0.051728472113609314 Accuracy 0.8632500171661377\n",
      "Iteration 25350 Training loss 0.05607186257839203 Validation loss 0.051664918661117554 Accuracy 0.8628750443458557\n",
      "Iteration 25360 Training loss 0.04399718716740608 Validation loss 0.051659274846315384 Accuracy 0.8630000352859497\n",
      "Iteration 25370 Training loss 0.05998166278004646 Validation loss 0.05178265646100044 Accuracy 0.8632500171661377\n",
      "Iteration 25380 Training loss 0.04753375053405762 Validation loss 0.05178473889827728 Accuracy 0.8636250495910645\n",
      "Iteration 25390 Training loss 0.05871514230966568 Validation loss 0.0516466461122036 Accuracy 0.862500011920929\n",
      "Iteration 25400 Training loss 0.05348780378699303 Validation loss 0.05165177956223488 Accuracy 0.8630000352859497\n",
      "Iteration 25410 Training loss 0.04342634230852127 Validation loss 0.05169939622282982 Accuracy 0.8633750677108765\n",
      "Iteration 25420 Training loss 0.046261586248874664 Validation loss 0.051670048385858536 Accuracy 0.8636250495910645\n",
      "Iteration 25430 Training loss 0.04967649281024933 Validation loss 0.05162468180060387 Accuracy 0.8638750314712524\n",
      "Iteration 25440 Training loss 0.051051631569862366 Validation loss 0.05163130536675453 Accuracy 0.8633750677108765\n",
      "Iteration 25450 Training loss 0.03512585908174515 Validation loss 0.05176004022359848 Accuracy 0.862500011920929\n",
      "Iteration 25460 Training loss 0.055575210601091385 Validation loss 0.05177348479628563 Accuracy 0.8633750677108765\n",
      "Iteration 25470 Training loss 0.05851438269019127 Validation loss 0.05177254602313042 Accuracy 0.8631250262260437\n",
      "Iteration 25480 Training loss 0.047006674110889435 Validation loss 0.05186384171247482 Accuracy 0.862500011920929\n",
      "Iteration 25490 Training loss 0.049583643674850464 Validation loss 0.051670655608177185 Accuracy 0.8628750443458557\n",
      "Iteration 25500 Training loss 0.03888934105634689 Validation loss 0.05163697898387909 Accuracy 0.8638750314712524\n",
      "Iteration 25510 Training loss 0.05183662474155426 Validation loss 0.05159858986735344 Accuracy 0.8637500405311584\n",
      "Iteration 25520 Training loss 0.06198790296912193 Validation loss 0.05159062147140503 Accuracy 0.8628750443458557\n",
      "Iteration 25530 Training loss 0.05278979241847992 Validation loss 0.05177726224064827 Accuracy 0.8632500171661377\n",
      "Iteration 25540 Training loss 0.05122818052768707 Validation loss 0.05159810557961464 Accuracy 0.8637500405311584\n",
      "Iteration 25550 Training loss 0.040838997811079025 Validation loss 0.051633045077323914 Accuracy 0.8640000224113464\n",
      "Iteration 25560 Training loss 0.047118425369262695 Validation loss 0.051626577973365784 Accuracy 0.8627500534057617\n",
      "Iteration 25570 Training loss 0.048650551587343216 Validation loss 0.05160069465637207 Accuracy 0.8638750314712524\n",
      "Iteration 25580 Training loss 0.054425809532403946 Validation loss 0.051683541387319565 Accuracy 0.8628750443458557\n",
      "Iteration 25590 Training loss 0.04300137609243393 Validation loss 0.05165323242545128 Accuracy 0.8633750677108765\n",
      "Iteration 25600 Training loss 0.05185411497950554 Validation loss 0.05253882333636284 Accuracy 0.8611250519752502\n",
      "Iteration 25610 Training loss 0.054100941866636276 Validation loss 0.051578883081674576 Accuracy 0.8631250262260437\n",
      "Iteration 25620 Training loss 0.04600496590137482 Validation loss 0.051949914544820786 Accuracy 0.8607500195503235\n",
      "Iteration 25630 Training loss 0.05482421815395355 Validation loss 0.05163601413369179 Accuracy 0.8632500171661377\n",
      "Iteration 25640 Training loss 0.04813617467880249 Validation loss 0.051588986068964005 Accuracy 0.8631250262260437\n",
      "Iteration 25650 Training loss 0.05381790176033974 Validation loss 0.051565904170274734 Accuracy 0.8633750677108765\n",
      "Iteration 25660 Training loss 0.047107961028814316 Validation loss 0.05150596797466278 Accuracy 0.8630000352859497\n",
      "Iteration 25670 Training loss 0.05043407902121544 Validation loss 0.05149739980697632 Accuracy 0.8628750443458557\n",
      "Iteration 25680 Training loss 0.051613181829452515 Validation loss 0.05167161673307419 Accuracy 0.8635000586509705\n",
      "Iteration 25690 Training loss 0.04660015180706978 Validation loss 0.0516246072947979 Accuracy 0.862375020980835\n",
      "Iteration 25700 Training loss 0.041605137288570404 Validation loss 0.051594726741313934 Accuracy 0.8626250624656677\n",
      "Iteration 25710 Training loss 0.05216364562511444 Validation loss 0.05182502046227455 Accuracy 0.862500011920929\n",
      "Iteration 25720 Training loss 0.05186837911605835 Validation loss 0.052144601941108704 Accuracy 0.862125039100647\n",
      "Iteration 25730 Training loss 0.04611041769385338 Validation loss 0.05150974541902542 Accuracy 0.8633750677108765\n",
      "Iteration 25740 Training loss 0.05042770132422447 Validation loss 0.051590751856565475 Accuracy 0.8643750548362732\n",
      "Iteration 25750 Training loss 0.04836173728108406 Validation loss 0.05188753083348274 Accuracy 0.8633750677108765\n",
      "Iteration 25760 Training loss 0.04297070577740669 Validation loss 0.05185144394636154 Accuracy 0.8633750677108765\n",
      "Iteration 25770 Training loss 0.0522732138633728 Validation loss 0.051533713936805725 Accuracy 0.8638750314712524\n",
      "Iteration 25780 Training loss 0.05077485740184784 Validation loss 0.051514819264411926 Accuracy 0.8637500405311584\n",
      "Iteration 25790 Training loss 0.054609593003988266 Validation loss 0.05149116739630699 Accuracy 0.8633750677108765\n",
      "Iteration 25800 Training loss 0.04960053041577339 Validation loss 0.05151698365807533 Accuracy 0.8633750677108765\n",
      "Iteration 25810 Training loss 0.04827279970049858 Validation loss 0.05169132351875305 Accuracy 0.862375020980835\n",
      "Iteration 25820 Training loss 0.05239621922373772 Validation loss 0.051523324102163315 Accuracy 0.8640000224113464\n",
      "Iteration 25830 Training loss 0.04997977241873741 Validation loss 0.05147989094257355 Accuracy 0.8636250495910645\n",
      "Iteration 25840 Training loss 0.05261041224002838 Validation loss 0.051453761756420135 Accuracy 0.8641250133514404\n",
      "Iteration 25850 Training loss 0.050967223942279816 Validation loss 0.051545821130275726 Accuracy 0.8636250495910645\n",
      "Iteration 25860 Training loss 0.05116714909672737 Validation loss 0.051569536328315735 Accuracy 0.8636250495910645\n",
      "Iteration 25870 Training loss 0.055643558502197266 Validation loss 0.05150386691093445 Accuracy 0.8633750677108765\n",
      "Iteration 25880 Training loss 0.04314977675676346 Validation loss 0.05145128443837166 Accuracy 0.8641250133514404\n",
      "Iteration 25890 Training loss 0.057230617851018906 Validation loss 0.05145663022994995 Accuracy 0.8632500171661377\n",
      "Iteration 25900 Training loss 0.047186899930238724 Validation loss 0.05166332796216011 Accuracy 0.862500011920929\n",
      "Iteration 25910 Training loss 0.052962664514780045 Validation loss 0.05149015411734581 Accuracy 0.8640000224113464\n",
      "Iteration 25920 Training loss 0.04455361142754555 Validation loss 0.0513874776661396 Accuracy 0.8641250133514404\n",
      "Iteration 25930 Training loss 0.04824718460440636 Validation loss 0.05152727663516998 Accuracy 0.8630000352859497\n",
      "Iteration 25940 Training loss 0.05415807664394379 Validation loss 0.05138924717903137 Accuracy 0.8646250367164612\n",
      "Iteration 25950 Training loss 0.043574023991823196 Validation loss 0.05144783854484558 Accuracy 0.8636250495910645\n",
      "Iteration 25960 Training loss 0.05176771059632301 Validation loss 0.05159398168325424 Accuracy 0.8638750314712524\n",
      "Iteration 25970 Training loss 0.048873331397771835 Validation loss 0.051364459097385406 Accuracy 0.8647500276565552\n",
      "Iteration 25980 Training loss 0.04518960416316986 Validation loss 0.05140748247504234 Accuracy 0.8646250367164612\n",
      "Iteration 25990 Training loss 0.04610147327184677 Validation loss 0.051373548805713654 Accuracy 0.8653750419616699\n",
      "Iteration 26000 Training loss 0.045343805104494095 Validation loss 0.05137254670262337 Accuracy 0.8648750185966492\n",
      "Iteration 26010 Training loss 0.05416406691074371 Validation loss 0.05137782543897629 Accuracy 0.8650000691413879\n",
      "Iteration 26020 Training loss 0.049117643386125565 Validation loss 0.051480960100889206 Accuracy 0.8648750185966492\n",
      "Iteration 26030 Training loss 0.051487404853105545 Validation loss 0.05138842388987541 Accuracy 0.8636250495910645\n",
      "Iteration 26040 Training loss 0.05507602170109749 Validation loss 0.05138318985700607 Accuracy 0.8646250367164612\n",
      "Iteration 26050 Training loss 0.052697353065013885 Validation loss 0.051526278257369995 Accuracy 0.8633750677108765\n",
      "Iteration 26060 Training loss 0.04527679830789566 Validation loss 0.05178079381585121 Accuracy 0.8633750677108765\n",
      "Iteration 26070 Training loss 0.04970358684659004 Validation loss 0.05135329067707062 Accuracy 0.8643750548362732\n",
      "Iteration 26080 Training loss 0.047006066888570786 Validation loss 0.051476601511240005 Accuracy 0.8636250495910645\n",
      "Iteration 26090 Training loss 0.05770609900355339 Validation loss 0.051363978534936905 Accuracy 0.8643750548362732\n",
      "Iteration 26100 Training loss 0.052244871854782104 Validation loss 0.051443278789520264 Accuracy 0.8631250262260437\n",
      "Iteration 26110 Training loss 0.0463499017059803 Validation loss 0.05139783024787903 Accuracy 0.8647500276565552\n",
      "Iteration 26120 Training loss 0.05060224235057831 Validation loss 0.05136191099882126 Accuracy 0.8642500638961792\n",
      "Iteration 26130 Training loss 0.05212463438510895 Validation loss 0.05163273215293884 Accuracy 0.862125039100647\n",
      "Iteration 26140 Training loss 0.04596978425979614 Validation loss 0.0513305738568306 Accuracy 0.8642500638961792\n",
      "Iteration 26150 Training loss 0.049417443573474884 Validation loss 0.05166082829236984 Accuracy 0.8631250262260437\n",
      "Iteration 26160 Training loss 0.05401177331805229 Validation loss 0.05139520391821861 Accuracy 0.8645000457763672\n",
      "Iteration 26170 Training loss 0.04579212889075279 Validation loss 0.05138900876045227 Accuracy 0.8638750314712524\n",
      "Iteration 26180 Training loss 0.04723236337304115 Validation loss 0.05133098363876343 Accuracy 0.8643750548362732\n",
      "Iteration 26190 Training loss 0.04773911461234093 Validation loss 0.051361117511987686 Accuracy 0.8643750548362732\n",
      "Iteration 26200 Training loss 0.04945850372314453 Validation loss 0.05141036584973335 Accuracy 0.8636250495910645\n",
      "Iteration 26210 Training loss 0.04817147180438042 Validation loss 0.05139525979757309 Accuracy 0.8643750548362732\n",
      "Iteration 26220 Training loss 0.04715095832943916 Validation loss 0.05135186016559601 Accuracy 0.8645000457763672\n",
      "Iteration 26230 Training loss 0.0430181510746479 Validation loss 0.051374442875385284 Accuracy 0.8635000586509705\n",
      "Iteration 26240 Training loss 0.04806745424866676 Validation loss 0.05139267072081566 Accuracy 0.8638750314712524\n",
      "Iteration 26250 Training loss 0.04469090327620506 Validation loss 0.05184777081012726 Accuracy 0.8635000586509705\n",
      "Iteration 26260 Training loss 0.05452185496687889 Validation loss 0.051291316747665405 Accuracy 0.8645000457763672\n",
      "Iteration 26270 Training loss 0.04561327025294304 Validation loss 0.05128052085638046 Accuracy 0.8643750548362732\n",
      "Iteration 26280 Training loss 0.043900325894355774 Validation loss 0.05132025480270386 Accuracy 0.8646250367164612\n",
      "Iteration 26290 Training loss 0.04635104909539223 Validation loss 0.05150336027145386 Accuracy 0.8632500171661377\n",
      "Iteration 26300 Training loss 0.04781913757324219 Validation loss 0.0512956865131855 Accuracy 0.8643750548362732\n",
      "Iteration 26310 Training loss 0.04978618025779724 Validation loss 0.05142694339156151 Accuracy 0.8641250133514404\n",
      "Iteration 26320 Training loss 0.051841139793395996 Validation loss 0.051314182579517365 Accuracy 0.8642500638961792\n",
      "Iteration 26330 Training loss 0.05059577524662018 Validation loss 0.05139901116490364 Accuracy 0.8643750548362732\n",
      "Iteration 26340 Training loss 0.05868426337838173 Validation loss 0.051292650401592255 Accuracy 0.8651250600814819\n",
      "Iteration 26350 Training loss 0.04783111438155174 Validation loss 0.05178016051650047 Accuracy 0.8633750677108765\n",
      "Iteration 26360 Training loss 0.0467468686401844 Validation loss 0.051342010498046875 Accuracy 0.8636250495910645\n",
      "Iteration 26370 Training loss 0.048419076949357986 Validation loss 0.051402315497398376 Accuracy 0.8632500171661377\n",
      "Iteration 26380 Training loss 0.04620923846960068 Validation loss 0.05154198035597801 Accuracy 0.8628750443458557\n",
      "Iteration 26390 Training loss 0.04600696638226509 Validation loss 0.051491379737854004 Accuracy 0.8636250495910645\n",
      "Iteration 26400 Training loss 0.04784165695309639 Validation loss 0.051263730973005295 Accuracy 0.8636250495910645\n",
      "Iteration 26410 Training loss 0.05195900797843933 Validation loss 0.05136507749557495 Accuracy 0.8636250495910645\n",
      "Iteration 26420 Training loss 0.04120378941297531 Validation loss 0.051252543926239014 Accuracy 0.8655000329017639\n",
      "Iteration 26430 Training loss 0.04503778740763664 Validation loss 0.05131242051720619 Accuracy 0.8636250495910645\n",
      "Iteration 26440 Training loss 0.05394134297966957 Validation loss 0.05178016424179077 Accuracy 0.862125039100647\n",
      "Iteration 26450 Training loss 0.051851775497198105 Validation loss 0.05168250575661659 Accuracy 0.8628750443458557\n",
      "Iteration 26460 Training loss 0.04920167848467827 Validation loss 0.05120787397027016 Accuracy 0.8647500276565552\n",
      "Iteration 26470 Training loss 0.05176883563399315 Validation loss 0.051305703818798065 Accuracy 0.8638750314712524\n",
      "Iteration 26480 Training loss 0.046840064227581024 Validation loss 0.05145048350095749 Accuracy 0.8632500171661377\n",
      "Iteration 26490 Training loss 0.04786459729075432 Validation loss 0.05117672309279442 Accuracy 0.8650000691413879\n",
      "Iteration 26500 Training loss 0.04940690100193024 Validation loss 0.05122605338692665 Accuracy 0.8638750314712524\n",
      "Iteration 26510 Training loss 0.050127509981393814 Validation loss 0.05129776895046234 Accuracy 0.8637500405311584\n",
      "Iteration 26520 Training loss 0.04445495456457138 Validation loss 0.0512160062789917 Accuracy 0.8647500276565552\n",
      "Iteration 26530 Training loss 0.04988409951329231 Validation loss 0.05127207562327385 Accuracy 0.8636250495910645\n",
      "Iteration 26540 Training loss 0.04959266632795334 Validation loss 0.05139991268515587 Accuracy 0.8637500405311584\n",
      "Iteration 26550 Training loss 0.05991274118423462 Validation loss 0.051277559250593185 Accuracy 0.8630000352859497\n",
      "Iteration 26560 Training loss 0.049564164131879807 Validation loss 0.051467131823301315 Accuracy 0.862500011920929\n",
      "Iteration 26570 Training loss 0.04987950995564461 Validation loss 0.05115819722414017 Accuracy 0.8648750185966492\n",
      "Iteration 26580 Training loss 0.04927223548293114 Validation loss 0.05119284987449646 Accuracy 0.8657500147819519\n",
      "Iteration 26590 Training loss 0.03967426344752312 Validation loss 0.0512176975607872 Accuracy 0.8641250133514404\n",
      "Iteration 26600 Training loss 0.04801124334335327 Validation loss 0.05112380534410477 Accuracy 0.8655000329017639\n",
      "Iteration 26610 Training loss 0.03723037987947464 Validation loss 0.051120005548000336 Accuracy 0.8633750677108765\n",
      "Iteration 26620 Training loss 0.04135308042168617 Validation loss 0.051082223653793335 Accuracy 0.8653750419616699\n",
      "Iteration 26630 Training loss 0.052867479622364044 Validation loss 0.05133510008454323 Accuracy 0.8642500638961792\n",
      "Iteration 26640 Training loss 0.04758589714765549 Validation loss 0.051107294857501984 Accuracy 0.8643750548362732\n",
      "Iteration 26650 Training loss 0.05097854137420654 Validation loss 0.051106344908475876 Accuracy 0.8647500276565552\n",
      "Iteration 26660 Training loss 0.05197596922516823 Validation loss 0.05109965056180954 Accuracy 0.8650000691413879\n",
      "Iteration 26670 Training loss 0.053795523941516876 Validation loss 0.05109759047627449 Accuracy 0.8657500147819519\n",
      "Iteration 26680 Training loss 0.04300906881690025 Validation loss 0.05134090781211853 Accuracy 0.8637500405311584\n",
      "Iteration 26690 Training loss 0.04806462302803993 Validation loss 0.05122694373130798 Accuracy 0.8653750419616699\n",
      "Iteration 26700 Training loss 0.04768700525164604 Validation loss 0.05127275362610817 Accuracy 0.8633750677108765\n",
      "Iteration 26710 Training loss 0.050190214067697525 Validation loss 0.05113444849848747 Accuracy 0.8636250495910645\n",
      "Iteration 26720 Training loss 0.05183641240000725 Validation loss 0.05122129246592522 Accuracy 0.8650000691413879\n",
      "Iteration 26730 Training loss 0.04927925020456314 Validation loss 0.051505107432603836 Accuracy 0.8636250495910645\n",
      "Iteration 26740 Training loss 0.051209867000579834 Validation loss 0.051203928887844086 Accuracy 0.8650000691413879\n",
      "Iteration 26750 Training loss 0.043434351682662964 Validation loss 0.05150756984949112 Accuracy 0.8628750443458557\n",
      "Iteration 26760 Training loss 0.04290899261832237 Validation loss 0.05104943364858627 Accuracy 0.8655000329017639\n",
      "Iteration 26770 Training loss 0.039986565709114075 Validation loss 0.051067616790533066 Accuracy 0.8651250600814819\n",
      "Iteration 26780 Training loss 0.04792490974068642 Validation loss 0.05115356296300888 Accuracy 0.8642500638961792\n",
      "Iteration 26790 Training loss 0.049498338252305984 Validation loss 0.05107911676168442 Accuracy 0.8655000329017639\n",
      "Iteration 26800 Training loss 0.04400761425495148 Validation loss 0.05105903372168541 Accuracy 0.8652500510215759\n",
      "Iteration 26810 Training loss 0.05514506623148918 Validation loss 0.05132920295000076 Accuracy 0.8635000586509705\n",
      "Iteration 26820 Training loss 0.04891326278448105 Validation loss 0.05110689252614975 Accuracy 0.8650000691413879\n",
      "Iteration 26830 Training loss 0.05281955748796463 Validation loss 0.05126626417040825 Accuracy 0.8636250495910645\n",
      "Iteration 26840 Training loss 0.045717451721429825 Validation loss 0.051117055118083954 Accuracy 0.8645000457763672\n",
      "Iteration 26850 Training loss 0.04346897825598717 Validation loss 0.05112744867801666 Accuracy 0.8657500147819519\n",
      "Iteration 26860 Training loss 0.05156494677066803 Validation loss 0.05152653902769089 Accuracy 0.862000048160553\n",
      "Iteration 26870 Training loss 0.04836464673280716 Validation loss 0.05108892172574997 Accuracy 0.8647500276565552\n",
      "Iteration 26880 Training loss 0.04204428568482399 Validation loss 0.05121023580431938 Accuracy 0.8648750185966492\n",
      "Iteration 26890 Training loss 0.05132102966308594 Validation loss 0.05132623389363289 Accuracy 0.8637500405311584\n",
      "Iteration 26900 Training loss 0.0494198352098465 Validation loss 0.05112415552139282 Accuracy 0.8650000691413879\n",
      "Iteration 26910 Training loss 0.05653444305062294 Validation loss 0.05110423266887665 Accuracy 0.8636250495910645\n",
      "Iteration 26920 Training loss 0.058215294033288956 Validation loss 0.05102655664086342 Accuracy 0.8646250367164612\n",
      "Iteration 26930 Training loss 0.04371168091893196 Validation loss 0.05107344686985016 Accuracy 0.8652500510215759\n",
      "Iteration 26940 Training loss 0.04373668506741524 Validation loss 0.05116057023406029 Accuracy 0.8645000457763672\n",
      "Iteration 26950 Training loss 0.048675019294023514 Validation loss 0.05223472788929939 Accuracy 0.8611250519752502\n",
      "Iteration 26960 Training loss 0.05456492304801941 Validation loss 0.05101429671049118 Accuracy 0.8651250600814819\n",
      "Iteration 26970 Training loss 0.05897710099816322 Validation loss 0.051001887768507004 Accuracy 0.8657500147819519\n",
      "Iteration 26980 Training loss 0.0496007464826107 Validation loss 0.05133463069796562 Accuracy 0.8626250624656677\n",
      "Iteration 26990 Training loss 0.040539950132369995 Validation loss 0.051023218780756 Accuracy 0.8638750314712524\n",
      "Iteration 27000 Training loss 0.04867887496948242 Validation loss 0.050966035574674606 Accuracy 0.8645000457763672\n",
      "Iteration 27010 Training loss 0.039785269647836685 Validation loss 0.05105945095419884 Accuracy 0.8657500147819519\n",
      "Iteration 27020 Training loss 0.05219954997301102 Validation loss 0.05138954520225525 Accuracy 0.8637500405311584\n",
      "Iteration 27030 Training loss 0.05480263754725456 Validation loss 0.050986986607313156 Accuracy 0.8652500510215759\n",
      "Iteration 27040 Training loss 0.04493863880634308 Validation loss 0.05098474398255348 Accuracy 0.8651250600814819\n",
      "Iteration 27050 Training loss 0.040306758135557175 Validation loss 0.05123814567923546 Accuracy 0.8643750548362732\n",
      "Iteration 27060 Training loss 0.04626268148422241 Validation loss 0.051012035459280014 Accuracy 0.8651250600814819\n",
      "Iteration 27070 Training loss 0.04909973964095116 Validation loss 0.05094185471534729 Accuracy 0.8656250238418579\n",
      "Iteration 27080 Training loss 0.048506371676921844 Validation loss 0.050944436341524124 Accuracy 0.8648750185966492\n",
      "Iteration 27090 Training loss 0.04666849970817566 Validation loss 0.05093935877084732 Accuracy 0.8653750419616699\n",
      "Iteration 27100 Training loss 0.043816667050123215 Validation loss 0.050973981618881226 Accuracy 0.8657500147819519\n",
      "Iteration 27110 Training loss 0.039309825748205185 Validation loss 0.050988491624593735 Accuracy 0.8651250600814819\n",
      "Iteration 27120 Training loss 0.047078292816877365 Validation loss 0.05110006779432297 Accuracy 0.8646250367164612\n",
      "Iteration 27130 Training loss 0.04997127875685692 Validation loss 0.05098381265997887 Accuracy 0.8651250600814819\n",
      "Iteration 27140 Training loss 0.042390499264001846 Validation loss 0.050972238183021545 Accuracy 0.8648750185966492\n",
      "Iteration 27150 Training loss 0.04460156336426735 Validation loss 0.051048021763563156 Accuracy 0.8647500276565552\n",
      "Iteration 27160 Training loss 0.038988154381513596 Validation loss 0.050955288112163544 Accuracy 0.8662500381469727\n",
      "Iteration 27170 Training loss 0.046518225222826004 Validation loss 0.05093434453010559 Accuracy 0.8651250600814819\n",
      "Iteration 27180 Training loss 0.04755103960633278 Validation loss 0.05113675072789192 Accuracy 0.8638750314712524\n",
      "Iteration 27190 Training loss 0.0464203916490078 Validation loss 0.05127827078104019 Accuracy 0.8650000691413879\n",
      "Iteration 27200 Training loss 0.0450056828558445 Validation loss 0.05088364705443382 Accuracy 0.8653750419616699\n",
      "Iteration 27210 Training loss 0.046224191784858704 Validation loss 0.050894610583782196 Accuracy 0.8652500510215759\n",
      "Iteration 27220 Training loss 0.049836166203022 Validation loss 0.05107957869768143 Accuracy 0.8640000224113464\n",
      "Iteration 27230 Training loss 0.05024653300642967 Validation loss 0.0509321466088295 Accuracy 0.8650000691413879\n",
      "Iteration 27240 Training loss 0.0451851487159729 Validation loss 0.05119236186146736 Accuracy 0.8638750314712524\n",
      "Iteration 27250 Training loss 0.05065349489450455 Validation loss 0.050970714539289474 Accuracy 0.8652500510215759\n",
      "Iteration 27260 Training loss 0.052313435822725296 Validation loss 0.05114298313856125 Accuracy 0.8638750314712524\n",
      "Iteration 27270 Training loss 0.04020330682396889 Validation loss 0.0511639304459095 Accuracy 0.8638750314712524\n",
      "Iteration 27280 Training loss 0.05575643852353096 Validation loss 0.05098024383187294 Accuracy 0.8660000562667847\n",
      "Iteration 27290 Training loss 0.05662931501865387 Validation loss 0.05098489299416542 Accuracy 0.8653750419616699\n",
      "Iteration 27300 Training loss 0.05616849660873413 Validation loss 0.05106198042631149 Accuracy 0.8646250367164612\n",
      "Iteration 27310 Training loss 0.03662968426942825 Validation loss 0.05129576474428177 Accuracy 0.8636250495910645\n",
      "Iteration 27320 Training loss 0.05168784037232399 Validation loss 0.05102251097559929 Accuracy 0.8651250600814819\n",
      "Iteration 27330 Training loss 0.05742586776614189 Validation loss 0.05132192373275757 Accuracy 0.8637500405311584\n",
      "Iteration 27340 Training loss 0.057527653872966766 Validation loss 0.051402054727077484 Accuracy 0.862000048160553\n",
      "Iteration 27350 Training loss 0.04527639225125313 Validation loss 0.05119717866182327 Accuracy 0.8636250495910645\n",
      "Iteration 27360 Training loss 0.04958796873688698 Validation loss 0.050889261066913605 Accuracy 0.8656250238418579\n",
      "Iteration 27370 Training loss 0.04660545289516449 Validation loss 0.05088600516319275 Accuracy 0.8658750653266907\n",
      "Iteration 27380 Training loss 0.04944726452231407 Validation loss 0.051377132534980774 Accuracy 0.861750066280365\n",
      "Iteration 27390 Training loss 0.04802697151899338 Validation loss 0.051272373646497726 Accuracy 0.8632500171661377\n",
      "Iteration 27400 Training loss 0.049758508801460266 Validation loss 0.05098939687013626 Accuracy 0.8652500510215759\n",
      "Iteration 27410 Training loss 0.05246683955192566 Validation loss 0.05107143148779869 Accuracy 0.8640000224113464\n",
      "Iteration 27420 Training loss 0.04845808818936348 Validation loss 0.05092926323413849 Accuracy 0.8648750185966492\n",
      "Iteration 27430 Training loss 0.043589625507593155 Validation loss 0.05121244117617607 Accuracy 0.8631250262260437\n",
      "Iteration 27440 Training loss 0.05452960729598999 Validation loss 0.050838273018598557 Accuracy 0.8658750653266907\n",
      "Iteration 27450 Training loss 0.050430964678525925 Validation loss 0.05115462839603424 Accuracy 0.8638750314712524\n",
      "Iteration 27460 Training loss 0.04431818053126335 Validation loss 0.05101752653717995 Accuracy 0.8655000329017639\n",
      "Iteration 27470 Training loss 0.043878406286239624 Validation loss 0.05108683928847313 Accuracy 0.8642500638961792\n",
      "Iteration 27480 Training loss 0.04162224754691124 Validation loss 0.05084966495633125 Accuracy 0.8655000329017639\n",
      "Iteration 27490 Training loss 0.05258768051862717 Validation loss 0.051238518208265305 Accuracy 0.8635000586509705\n",
      "Iteration 27500 Training loss 0.047507550567388535 Validation loss 0.050853244960308075 Accuracy 0.8650000691413879\n",
      "Iteration 27510 Training loss 0.04509410634636879 Validation loss 0.05095742642879486 Accuracy 0.8646250367164612\n",
      "Iteration 27520 Training loss 0.04216720536351204 Validation loss 0.05083784833550453 Accuracy 0.8657500147819519\n",
      "Iteration 27530 Training loss 0.041345272213220596 Validation loss 0.050927773118019104 Accuracy 0.8651250600814819\n",
      "Iteration 27540 Training loss 0.04179198294878006 Validation loss 0.05096405744552612 Accuracy 0.8652500510215759\n",
      "Iteration 27550 Training loss 0.04283018037676811 Validation loss 0.0509687140583992 Accuracy 0.8643750548362732\n",
      "Iteration 27560 Training loss 0.04367241635918617 Validation loss 0.05092143639922142 Accuracy 0.8640000224113464\n",
      "Iteration 27570 Training loss 0.04696713015437126 Validation loss 0.05086560919880867 Accuracy 0.8645000457763672\n",
      "Iteration 27580 Training loss 0.044880617409944534 Validation loss 0.05090566724538803 Accuracy 0.8661250472068787\n",
      "Iteration 27590 Training loss 0.04882313683629036 Validation loss 0.05089876800775528 Accuracy 0.8656250238418579\n",
      "Iteration 27600 Training loss 0.04583078250288963 Validation loss 0.0509532131254673 Accuracy 0.8640000224113464\n",
      "Iteration 27610 Training loss 0.06350652873516083 Validation loss 0.050823431462049484 Accuracy 0.8660000562667847\n",
      "Iteration 27620 Training loss 0.0463610403239727 Validation loss 0.050857819616794586 Accuracy 0.8660000562667847\n",
      "Iteration 27630 Training loss 0.04484172537922859 Validation loss 0.050924986600875854 Accuracy 0.8662500381469727\n",
      "Iteration 27640 Training loss 0.04486404359340668 Validation loss 0.05094463750720024 Accuracy 0.8655000329017639\n",
      "Iteration 27650 Training loss 0.045839957892894745 Validation loss 0.05089754983782768 Accuracy 0.8640000224113464\n",
      "Iteration 27660 Training loss 0.046512868255376816 Validation loss 0.05132215470075607 Accuracy 0.8640000224113464\n",
      "Iteration 27670 Training loss 0.05701381713151932 Validation loss 0.05083771049976349 Accuracy 0.8661250472068787\n",
      "Iteration 27680 Training loss 0.050317343324422836 Validation loss 0.051007404923439026 Accuracy 0.8645000457763672\n",
      "Iteration 27690 Training loss 0.05366509407758713 Validation loss 0.05081494525074959 Accuracy 0.8657500147819519\n",
      "Iteration 27700 Training loss 0.04614397883415222 Validation loss 0.05161410942673683 Accuracy 0.8637500405311584\n",
      "Iteration 27710 Training loss 0.0604875274002552 Validation loss 0.050753988325595856 Accuracy 0.8660000562667847\n",
      "Iteration 27720 Training loss 0.049099188297986984 Validation loss 0.05076710134744644 Accuracy 0.8660000562667847\n",
      "Iteration 27730 Training loss 0.045753199607133865 Validation loss 0.051772940903902054 Accuracy 0.8627500534057617\n",
      "Iteration 27740 Training loss 0.047103289514780045 Validation loss 0.05076080933213234 Accuracy 0.8651250600814819\n",
      "Iteration 27750 Training loss 0.05118582770228386 Validation loss 0.05078161507844925 Accuracy 0.8663750290870667\n",
      "Iteration 27760 Training loss 0.041784659028053284 Validation loss 0.05092090740799904 Accuracy 0.8641250133514404\n",
      "Iteration 27770 Training loss 0.05371715873479843 Validation loss 0.05085103213787079 Accuracy 0.8648750185966492\n",
      "Iteration 27780 Training loss 0.04202971234917641 Validation loss 0.05082391947507858 Accuracy 0.8648750185966492\n",
      "Iteration 27790 Training loss 0.04510252922773361 Validation loss 0.05078783631324768 Accuracy 0.8658750653266907\n",
      "Iteration 27800 Training loss 0.06290259957313538 Validation loss 0.050846587866544724 Accuracy 0.8653750419616699\n",
      "Iteration 27810 Training loss 0.04728478193283081 Validation loss 0.0513073094189167 Accuracy 0.8646250367164612\n",
      "Iteration 27820 Training loss 0.04278144612908363 Validation loss 0.05082283169031143 Accuracy 0.8650000691413879\n",
      "Iteration 27830 Training loss 0.044604651629924774 Validation loss 0.050859756767749786 Accuracy 0.8660000562667847\n",
      "Iteration 27840 Training loss 0.04881076142191887 Validation loss 0.050704989582300186 Accuracy 0.8653750419616699\n",
      "Iteration 27850 Training loss 0.055501971393823624 Validation loss 0.05152016878128052 Accuracy 0.8642500638961792\n",
      "Iteration 27860 Training loss 0.05546801909804344 Validation loss 0.05073757842183113 Accuracy 0.8656250238418579\n",
      "Iteration 27870 Training loss 0.051760587841272354 Validation loss 0.05067775771021843 Accuracy 0.8663750290870667\n",
      "Iteration 27880 Training loss 0.05131153762340546 Validation loss 0.05072567239403725 Accuracy 0.8660000562667847\n",
      "Iteration 27890 Training loss 0.043533310294151306 Validation loss 0.05069343000650406 Accuracy 0.8665000200271606\n",
      "Iteration 27900 Training loss 0.04953176900744438 Validation loss 0.05070456117391586 Accuracy 0.8658750653266907\n",
      "Iteration 27910 Training loss 0.041776444762945175 Validation loss 0.050859563052654266 Accuracy 0.8651250600814819\n",
      "Iteration 27920 Training loss 0.04853232204914093 Validation loss 0.05065513774752617 Accuracy 0.8662500381469727\n",
      "Iteration 27930 Training loss 0.05373593047261238 Validation loss 0.050748832523822784 Accuracy 0.8646250367164612\n",
      "Iteration 27940 Training loss 0.04078100621700287 Validation loss 0.05066759139299393 Accuracy 0.8658750653266907\n",
      "Iteration 27950 Training loss 0.03978320583701134 Validation loss 0.050660088658332825 Accuracy 0.8665000200271606\n",
      "Iteration 27960 Training loss 0.047804106026887894 Validation loss 0.050815608352422714 Accuracy 0.8661250472068787\n",
      "Iteration 27970 Training loss 0.05075082555413246 Validation loss 0.05069070681929588 Accuracy 0.8666250705718994\n",
      "Iteration 27980 Training loss 0.05352235585451126 Validation loss 0.050636228173971176 Accuracy 0.8661250472068787\n",
      "Iteration 27990 Training loss 0.04229726269841194 Validation loss 0.050924722105264664 Accuracy 0.8652500510215759\n",
      "Iteration 28000 Training loss 0.04706476256251335 Validation loss 0.05064288526773453 Accuracy 0.8667500615119934\n",
      "Iteration 28010 Training loss 0.05050479620695114 Validation loss 0.05067458376288414 Accuracy 0.8671250343322754\n",
      "Iteration 28020 Training loss 0.05107670649886131 Validation loss 0.05112878978252411 Accuracy 0.8637500405311584\n",
      "Iteration 28030 Training loss 0.04602811858057976 Validation loss 0.05085653066635132 Accuracy 0.8662500381469727\n",
      "Iteration 28040 Training loss 0.03738142549991608 Validation loss 0.050851717591285706 Accuracy 0.8657500147819519\n",
      "Iteration 28050 Training loss 0.04121715575456619 Validation loss 0.05069303885102272 Accuracy 0.8652500510215759\n",
      "Iteration 28060 Training loss 0.046905163675546646 Validation loss 0.050708286464214325 Accuracy 0.8666250705718994\n",
      "Iteration 28070 Training loss 0.048839833587408066 Validation loss 0.05079608038067818 Accuracy 0.8661250472068787\n",
      "Iteration 28080 Training loss 0.05247541144490242 Validation loss 0.050696924328804016 Accuracy 0.8658750653266907\n",
      "Iteration 28090 Training loss 0.04650930315256119 Validation loss 0.05061287805438042 Accuracy 0.8672500252723694\n",
      "Iteration 28100 Training loss 0.05084481090307236 Validation loss 0.050604045391082764 Accuracy 0.8665000200271606\n",
      "Iteration 28110 Training loss 0.04061258211731911 Validation loss 0.0505998395383358 Accuracy 0.8661250472068787\n",
      "Iteration 28120 Training loss 0.05113692954182625 Validation loss 0.05075482279062271 Accuracy 0.8647500276565552\n",
      "Iteration 28130 Training loss 0.052333611994981766 Validation loss 0.05059904232621193 Accuracy 0.8672500252723694\n",
      "Iteration 28140 Training loss 0.03989898040890694 Validation loss 0.05066458880901337 Accuracy 0.8675000667572021\n",
      "Iteration 28150 Training loss 0.04637187719345093 Validation loss 0.0506972074508667 Accuracy 0.8670000433921814\n",
      "Iteration 28160 Training loss 0.04120596498250961 Validation loss 0.05100373178720474 Accuracy 0.8661250472068787\n",
      "Iteration 28170 Training loss 0.04546409845352173 Validation loss 0.050693903118371964 Accuracy 0.8668750524520874\n",
      "Iteration 28180 Training loss 0.051928263157606125 Validation loss 0.05074309557676315 Accuracy 0.8665000200271606\n",
      "Iteration 28190 Training loss 0.049626629799604416 Validation loss 0.05064884573221207 Accuracy 0.8653750419616699\n",
      "Iteration 28200 Training loss 0.054156165570020676 Validation loss 0.05081569030880928 Accuracy 0.8661250472068787\n",
      "Iteration 28210 Training loss 0.04047868773341179 Validation loss 0.05060160905122757 Accuracy 0.8656250238418579\n",
      "Iteration 28220 Training loss 0.047226179391145706 Validation loss 0.050694726407527924 Accuracy 0.8652500510215759\n",
      "Iteration 28230 Training loss 0.04180426523089409 Validation loss 0.05065307393670082 Accuracy 0.8672500252723694\n",
      "Iteration 28240 Training loss 0.05123616009950638 Validation loss 0.05099012330174446 Accuracy 0.8628750443458557\n",
      "Iteration 28250 Training loss 0.04233879595994949 Validation loss 0.050572264939546585 Accuracy 0.8660000562667847\n",
      "Iteration 28260 Training loss 0.04023444652557373 Validation loss 0.05056067928671837 Accuracy 0.8667500615119934\n",
      "Iteration 28270 Training loss 0.047279518097639084 Validation loss 0.0506129115819931 Accuracy 0.8665000200271606\n",
      "Iteration 28280 Training loss 0.04873758554458618 Validation loss 0.050653908401727676 Accuracy 0.8658750653266907\n",
      "Iteration 28290 Training loss 0.053399305790662766 Validation loss 0.05063130706548691 Accuracy 0.8673750162124634\n",
      "Iteration 28300 Training loss 0.058294426649808884 Validation loss 0.05081108957529068 Accuracy 0.8655000329017639\n",
      "Iteration 28310 Training loss 0.04622148349881172 Validation loss 0.050572190433740616 Accuracy 0.8673750162124634\n",
      "Iteration 28320 Training loss 0.04650646075606346 Validation loss 0.050520624965429306 Accuracy 0.8665000200271606\n",
      "Iteration 28330 Training loss 0.04771241173148155 Validation loss 0.0507720448076725 Accuracy 0.8663750290870667\n",
      "Iteration 28340 Training loss 0.049731653183698654 Validation loss 0.0506526380777359 Accuracy 0.8657500147819519\n",
      "Iteration 28350 Training loss 0.05552470684051514 Validation loss 0.05088163912296295 Accuracy 0.8666250705718994\n",
      "Iteration 28360 Training loss 0.045768335461616516 Validation loss 0.05051460862159729 Accuracy 0.8687500357627869\n",
      "Iteration 28370 Training loss 0.04253308102488518 Validation loss 0.05094693973660469 Accuracy 0.8656250238418579\n",
      "Iteration 28380 Training loss 0.04388531669974327 Validation loss 0.05047913268208504 Accuracy 0.8680000305175781\n",
      "Iteration 28390 Training loss 0.05742167681455612 Validation loss 0.05053222179412842 Accuracy 0.8671250343322754\n",
      "Iteration 28400 Training loss 0.04488882049918175 Validation loss 0.05056034028530121 Accuracy 0.8672500252723694\n",
      "Iteration 28410 Training loss 0.05442512780427933 Validation loss 0.05046199634671211 Accuracy 0.8673750162124634\n",
      "Iteration 28420 Training loss 0.03963566944003105 Validation loss 0.05051148310303688 Accuracy 0.8671250343322754\n",
      "Iteration 28430 Training loss 0.04366043955087662 Validation loss 0.050492435693740845 Accuracy 0.8672500252723694\n",
      "Iteration 28440 Training loss 0.04370943456888199 Validation loss 0.05059892684221268 Accuracy 0.8670000433921814\n",
      "Iteration 28450 Training loss 0.03588264808058739 Validation loss 0.050465814769268036 Accuracy 0.8668750524520874\n",
      "Iteration 28460 Training loss 0.04547892510890961 Validation loss 0.050523050129413605 Accuracy 0.8668750524520874\n",
      "Iteration 28470 Training loss 0.04416915774345398 Validation loss 0.0505690723657608 Accuracy 0.8672500252723694\n",
      "Iteration 28480 Training loss 0.056571029126644135 Validation loss 0.050512537360191345 Accuracy 0.8675000667572021\n",
      "Iteration 28490 Training loss 0.04403848573565483 Validation loss 0.05056074261665344 Accuracy 0.8658750653266907\n",
      "Iteration 28500 Training loss 0.040203265845775604 Validation loss 0.050461556762456894 Accuracy 0.8670000433921814\n",
      "Iteration 28510 Training loss 0.05392194166779518 Validation loss 0.05070074647665024 Accuracy 0.8638750314712524\n",
      "Iteration 28520 Training loss 0.0518643856048584 Validation loss 0.050617121160030365 Accuracy 0.8648750185966492\n",
      "Iteration 28530 Training loss 0.05076289921998978 Validation loss 0.050422873347997665 Accuracy 0.8670000433921814\n",
      "Iteration 28540 Training loss 0.047768499702215195 Validation loss 0.050511304289102554 Accuracy 0.8660000562667847\n",
      "Iteration 28550 Training loss 0.0565938875079155 Validation loss 0.050390321761369705 Accuracy 0.8678750395774841\n",
      "Iteration 28560 Training loss 0.050323743373155594 Validation loss 0.05041130259633064 Accuracy 0.8677500486373901\n",
      "Iteration 28570 Training loss 0.0523005872964859 Validation loss 0.0505281463265419 Accuracy 0.8668750524520874\n",
      "Iteration 28580 Training loss 0.05069582164287567 Validation loss 0.050431422889232635 Accuracy 0.8665000200271606\n",
      "Iteration 28590 Training loss 0.048455409705638885 Validation loss 0.050546687096357346 Accuracy 0.8655000329017639\n",
      "Iteration 28600 Training loss 0.04721830412745476 Validation loss 0.050578538328409195 Accuracy 0.8658750653266907\n",
      "Iteration 28610 Training loss 0.0448756106197834 Validation loss 0.05056087672710419 Accuracy 0.8668750524520874\n",
      "Iteration 28620 Training loss 0.0445474311709404 Validation loss 0.05038050562143326 Accuracy 0.8668750524520874\n",
      "Iteration 28630 Training loss 0.043070316314697266 Validation loss 0.05040310323238373 Accuracy 0.8670000433921814\n",
      "Iteration 28640 Training loss 0.043781936168670654 Validation loss 0.05050364136695862 Accuracy 0.8673750162124634\n",
      "Iteration 28650 Training loss 0.0465841181576252 Validation loss 0.05041857063770294 Accuracy 0.8675000667572021\n",
      "Iteration 28660 Training loss 0.04860846325755119 Validation loss 0.050402186810970306 Accuracy 0.8681250214576721\n",
      "Iteration 28670 Training loss 0.04868033155798912 Validation loss 0.050394438207149506 Accuracy 0.8667500615119934\n",
      "Iteration 28680 Training loss 0.048679009079933167 Validation loss 0.050571221858263016 Accuracy 0.8658750653266907\n",
      "Iteration 28690 Training loss 0.050978031009435654 Validation loss 0.05041133239865303 Accuracy 0.8678750395774841\n",
      "Iteration 28700 Training loss 0.05076197534799576 Validation loss 0.050448060035705566 Accuracy 0.8665000200271606\n",
      "Iteration 28710 Training loss 0.048347968608140945 Validation loss 0.05037938058376312 Accuracy 0.8678750395774841\n",
      "Iteration 28720 Training loss 0.04448819160461426 Validation loss 0.050471000373363495 Accuracy 0.8682500123977661\n",
      "Iteration 28730 Training loss 0.05311640724539757 Validation loss 0.05044789984822273 Accuracy 0.8676250576972961\n",
      "Iteration 28740 Training loss 0.046673670411109924 Validation loss 0.050473157316446304 Accuracy 0.8656250238418579\n",
      "Iteration 28750 Training loss 0.045663654804229736 Validation loss 0.050376035273075104 Accuracy 0.8677500486373901\n",
      "Iteration 28760 Training loss 0.047014884650707245 Validation loss 0.0509641058743 Accuracy 0.8647500276565552\n",
      "Iteration 28770 Training loss 0.05107437074184418 Validation loss 0.050573088228702545 Accuracy 0.8666250705718994\n",
      "Iteration 28780 Training loss 0.05308568850159645 Validation loss 0.050326310098171234 Accuracy 0.8671250343322754\n",
      "Iteration 28790 Training loss 0.04151124507188797 Validation loss 0.05035887286067009 Accuracy 0.8673750162124634\n",
      "Iteration 28800 Training loss 0.0569915771484375 Validation loss 0.05035590007901192 Accuracy 0.8680000305175781\n",
      "Iteration 28810 Training loss 0.05392534285783768 Validation loss 0.050344325602054596 Accuracy 0.8678750395774841\n",
      "Iteration 28820 Training loss 0.03860343247652054 Validation loss 0.05050346627831459 Accuracy 0.8675000667572021\n",
      "Iteration 28830 Training loss 0.04533139243721962 Validation loss 0.05127142742276192 Accuracy 0.8647500276565552\n",
      "Iteration 28840 Training loss 0.044208768755197525 Validation loss 0.05035989359021187 Accuracy 0.8682500123977661\n",
      "Iteration 28850 Training loss 0.04003698751330376 Validation loss 0.05045349895954132 Accuracy 0.8671250343322754\n",
      "Iteration 28860 Training loss 0.04254809394478798 Validation loss 0.05028171092271805 Accuracy 0.8680000305175781\n",
      "Iteration 28870 Training loss 0.040714800357818604 Validation loss 0.05027400329709053 Accuracy 0.8685000538825989\n",
      "Iteration 28880 Training loss 0.04900599271059036 Validation loss 0.05024202913045883 Accuracy 0.8690000176429749\n",
      "Iteration 28890 Training loss 0.04189961776137352 Validation loss 0.05030366778373718 Accuracy 0.8678750395774841\n",
      "Iteration 28900 Training loss 0.0474916435778141 Validation loss 0.05107078328728676 Accuracy 0.8658750653266907\n",
      "Iteration 28910 Training loss 0.052137039601802826 Validation loss 0.0502978079020977 Accuracy 0.8678750395774841\n",
      "Iteration 28920 Training loss 0.045664120465517044 Validation loss 0.050301164388656616 Accuracy 0.8675000667572021\n",
      "Iteration 28930 Training loss 0.05273885279893875 Validation loss 0.05030816048383713 Accuracy 0.8676250576972961\n",
      "Iteration 28940 Training loss 0.04448306933045387 Validation loss 0.05039402097463608 Accuracy 0.8671250343322754\n",
      "Iteration 28950 Training loss 0.05951869860291481 Validation loss 0.05034814029932022 Accuracy 0.8667500615119934\n",
      "Iteration 28960 Training loss 0.04879390075802803 Validation loss 0.05031084641814232 Accuracy 0.8662500381469727\n",
      "Iteration 28970 Training loss 0.04538591206073761 Validation loss 0.05035433545708656 Accuracy 0.8671250343322754\n",
      "Iteration 28980 Training loss 0.043192308396101 Validation loss 0.0503205768764019 Accuracy 0.8666250705718994\n",
      "Iteration 28990 Training loss 0.04743044823408127 Validation loss 0.05030052363872528 Accuracy 0.8676250576972961\n",
      "Iteration 29000 Training loss 0.0478285476565361 Validation loss 0.05028562620282173 Accuracy 0.8680000305175781\n",
      "Iteration 29010 Training loss 0.059460777789354324 Validation loss 0.05031198263168335 Accuracy 0.8671250343322754\n",
      "Iteration 29020 Training loss 0.05562613904476166 Validation loss 0.050456706434488297 Accuracy 0.8672500252723694\n",
      "Iteration 29030 Training loss 0.04334506019949913 Validation loss 0.05028262734413147 Accuracy 0.8680000305175781\n",
      "Iteration 29040 Training loss 0.046756647527217865 Validation loss 0.0506444051861763 Accuracy 0.8662500381469727\n",
      "Iteration 29050 Training loss 0.05423319339752197 Validation loss 0.05036517605185509 Accuracy 0.8675000667572021\n",
      "Iteration 29060 Training loss 0.047956470400094986 Validation loss 0.050384730100631714 Accuracy 0.8672500252723694\n",
      "Iteration 29070 Training loss 0.05598278343677521 Validation loss 0.05057704821228981 Accuracy 0.8666250705718994\n",
      "Iteration 29080 Training loss 0.041417840868234634 Validation loss 0.05034251883625984 Accuracy 0.8673750162124634\n",
      "Iteration 29090 Training loss 0.03935901075601578 Validation loss 0.05033735930919647 Accuracy 0.8668750524520874\n",
      "Iteration 29100 Training loss 0.04138088971376419 Validation loss 0.05031171068549156 Accuracy 0.8671250343322754\n",
      "Iteration 29110 Training loss 0.04955591633915901 Validation loss 0.050465237349271774 Accuracy 0.8676250576972961\n",
      "Iteration 29120 Training loss 0.048585906624794006 Validation loss 0.05045527219772339 Accuracy 0.8660000562667847\n",
      "Iteration 29130 Training loss 0.04671677574515343 Validation loss 0.05027342587709427 Accuracy 0.8680000305175781\n",
      "Iteration 29140 Training loss 0.0423285998404026 Validation loss 0.050235699862241745 Accuracy 0.8682500123977661\n",
      "Iteration 29150 Training loss 0.04360764101147652 Validation loss 0.05042245611548424 Accuracy 0.8668750524520874\n",
      "Iteration 29160 Training loss 0.0499972440302372 Validation loss 0.05060858279466629 Accuracy 0.8660000562667847\n",
      "Iteration 29170 Training loss 0.05477086827158928 Validation loss 0.05047304928302765 Accuracy 0.8668750524520874\n",
      "Iteration 29180 Training loss 0.03740890324115753 Validation loss 0.050499483942985535 Accuracy 0.8670000433921814\n",
      "Iteration 29190 Training loss 0.04399007558822632 Validation loss 0.05025028437376022 Accuracy 0.8686250448226929\n",
      "Iteration 29200 Training loss 0.044172100722789764 Validation loss 0.05022953823208809 Accuracy 0.8687500357627869\n",
      "Iteration 29210 Training loss 0.04107336327433586 Validation loss 0.05028267949819565 Accuracy 0.8665000200271606\n",
      "Iteration 29220 Training loss 0.04440625384449959 Validation loss 0.050276871770620346 Accuracy 0.8671250343322754\n",
      "Iteration 29230 Training loss 0.04044385999441147 Validation loss 0.0502653494477272 Accuracy 0.8678750395774841\n",
      "Iteration 29240 Training loss 0.04656078293919563 Validation loss 0.050324007868766785 Accuracy 0.8660000562667847\n",
      "Iteration 29250 Training loss 0.05142466351389885 Validation loss 0.050298430025577545 Accuracy 0.8668750524520874\n",
      "Iteration 29260 Training loss 0.047426749020814896 Validation loss 0.05025414749979973 Accuracy 0.8678750395774841\n",
      "Iteration 29270 Training loss 0.04891632869839668 Validation loss 0.05026562139391899 Accuracy 0.8665000200271606\n",
      "Iteration 29280 Training loss 0.04643673822283745 Validation loss 0.05020378902554512 Accuracy 0.8686250448226929\n",
      "Iteration 29290 Training loss 0.04682965949177742 Validation loss 0.05046658590435982 Accuracy 0.8661250472068787\n",
      "Iteration 29300 Training loss 0.05236268788576126 Validation loss 0.050212617963552475 Accuracy 0.8668750524520874\n",
      "Iteration 29310 Training loss 0.048515308648347855 Validation loss 0.05019429326057434 Accuracy 0.8673750162124634\n",
      "Iteration 29320 Training loss 0.05518653616309166 Validation loss 0.050226885825395584 Accuracy 0.8672500252723694\n",
      "Iteration 29330 Training loss 0.04891530051827431 Validation loss 0.05017685890197754 Accuracy 0.8685000538825989\n",
      "Iteration 29340 Training loss 0.04574356973171234 Validation loss 0.050179820507764816 Accuracy 0.8692500591278076\n",
      "Iteration 29350 Training loss 0.04693220183253288 Validation loss 0.050163302570581436 Accuracy 0.8686250448226929\n",
      "Iteration 29360 Training loss 0.051037974655628204 Validation loss 0.05034423992037773 Accuracy 0.8663750290870667\n",
      "Iteration 29370 Training loss 0.045782286673784256 Validation loss 0.050163935869932175 Accuracy 0.8680000305175781\n",
      "Iteration 29380 Training loss 0.05193376541137695 Validation loss 0.050366420298814774 Accuracy 0.8682500123977661\n",
      "Iteration 29390 Training loss 0.03817848488688469 Validation loss 0.050130587071180344 Accuracy 0.8685000538825989\n",
      "Iteration 29400 Training loss 0.04269370064139366 Validation loss 0.0501512810587883 Accuracy 0.8682500123977661\n",
      "Iteration 29410 Training loss 0.04879068583250046 Validation loss 0.05016764625906944 Accuracy 0.8673750162124634\n",
      "Iteration 29420 Training loss 0.045578110963106155 Validation loss 0.05010191723704338 Accuracy 0.8675000667572021\n",
      "Iteration 29430 Training loss 0.04530256241559982 Validation loss 0.050287097692489624 Accuracy 0.8662500381469727\n",
      "Iteration 29440 Training loss 0.05210479721426964 Validation loss 0.05007254332304001 Accuracy 0.8685000538825989\n",
      "Iteration 29450 Training loss 0.0403897799551487 Validation loss 0.05008343234658241 Accuracy 0.8682500123977661\n",
      "Iteration 29460 Training loss 0.050699956715106964 Validation loss 0.05021427571773529 Accuracy 0.8673750162124634\n",
      "Iteration 29470 Training loss 0.04927199333906174 Validation loss 0.05021991208195686 Accuracy 0.8670000433921814\n",
      "Iteration 29480 Training loss 0.04128658026456833 Validation loss 0.050114694982767105 Accuracy 0.8685000538825989\n",
      "Iteration 29490 Training loss 0.05338194593787193 Validation loss 0.05051812157034874 Accuracy 0.8658750653266907\n",
      "Iteration 29500 Training loss 0.05888766050338745 Validation loss 0.050615016371011734 Accuracy 0.8632500171661377\n",
      "Iteration 29510 Training loss 0.04876463860273361 Validation loss 0.05009178817272186 Accuracy 0.8672500252723694\n",
      "Iteration 29520 Training loss 0.05032970383763313 Validation loss 0.050080880522727966 Accuracy 0.8687500357627869\n",
      "Iteration 29530 Training loss 0.042808130383491516 Validation loss 0.05064735934138298 Accuracy 0.8668750524520874\n",
      "Iteration 29540 Training loss 0.04474479705095291 Validation loss 0.05033290386199951 Accuracy 0.8675000667572021\n",
      "Iteration 29550 Training loss 0.042153749614953995 Validation loss 0.0501258485019207 Accuracy 0.8670000433921814\n",
      "Iteration 29560 Training loss 0.04948972165584564 Validation loss 0.050272975116968155 Accuracy 0.8662500381469727\n",
      "Iteration 29570 Training loss 0.04507274180650711 Validation loss 0.050257351249456406 Accuracy 0.8673750162124634\n",
      "Iteration 29580 Training loss 0.04020193964242935 Validation loss 0.05058746412396431 Accuracy 0.8662500381469727\n",
      "Iteration 29590 Training loss 0.04471760615706444 Validation loss 0.05005183070898056 Accuracy 0.8682500123977661\n",
      "Iteration 29600 Training loss 0.0458843894302845 Validation loss 0.050059858709573746 Accuracy 0.8687500357627869\n",
      "Iteration 29610 Training loss 0.045628275722265244 Validation loss 0.05010102316737175 Accuracy 0.8677500486373901\n",
      "Iteration 29620 Training loss 0.036040689796209335 Validation loss 0.05021478608250618 Accuracy 0.8658750653266907\n",
      "Iteration 29630 Training loss 0.05131494998931885 Validation loss 0.05009216442704201 Accuracy 0.8680000305175781\n",
      "Iteration 29640 Training loss 0.04870140925049782 Validation loss 0.05012356862425804 Accuracy 0.8675000667572021\n",
      "Iteration 29650 Training loss 0.044053979218006134 Validation loss 0.05009147897362709 Accuracy 0.8682500123977661\n",
      "Iteration 29660 Training loss 0.05095761641860008 Validation loss 0.050038307905197144 Accuracy 0.8695000410079956\n",
      "Iteration 29670 Training loss 0.04435458034276962 Validation loss 0.05006934329867363 Accuracy 0.8687500357627869\n",
      "Iteration 29680 Training loss 0.04171547666192055 Validation loss 0.05025535821914673 Accuracy 0.8678750395774841\n",
      "Iteration 29690 Training loss 0.04595368355512619 Validation loss 0.05008435249328613 Accuracy 0.8682500123977661\n",
      "Iteration 29700 Training loss 0.055454835295677185 Validation loss 0.04999595135450363 Accuracy 0.8697500228881836\n",
      "Iteration 29710 Training loss 0.046813104301691055 Validation loss 0.0499904491007328 Accuracy 0.8688750267028809\n",
      "Iteration 29720 Training loss 0.046766314655542374 Validation loss 0.05004534497857094 Accuracy 0.8691250681877136\n",
      "Iteration 29730 Training loss 0.0488450787961483 Validation loss 0.05026772990822792 Accuracy 0.8663750290870667\n",
      "Iteration 29740 Training loss 0.05746918171644211 Validation loss 0.04996079206466675 Accuracy 0.8695000410079956\n",
      "Iteration 29750 Training loss 0.049354199320077896 Validation loss 0.050128206610679626 Accuracy 0.8681250214576721\n",
      "Iteration 29760 Training loss 0.051235608756542206 Validation loss 0.05026108771562576 Accuracy 0.8660000562667847\n",
      "Iteration 29770 Training loss 0.052719488739967346 Validation loss 0.04996013268828392 Accuracy 0.8696250319480896\n",
      "Iteration 29780 Training loss 0.038337551057338715 Validation loss 0.05034342408180237 Accuracy 0.8673750162124634\n",
      "Iteration 29790 Training loss 0.04529157653450966 Validation loss 0.04994082823395729 Accuracy 0.8681250214576721\n",
      "Iteration 29800 Training loss 0.05429317057132721 Validation loss 0.05025564879179001 Accuracy 0.8682500123977661\n",
      "Iteration 29810 Training loss 0.04712922126054764 Validation loss 0.04995999112725258 Accuracy 0.8697500228881836\n",
      "Iteration 29820 Training loss 0.04155413433909416 Validation loss 0.049979519098997116 Accuracy 0.8692500591278076\n",
      "Iteration 29830 Training loss 0.05748153105378151 Validation loss 0.05009876564145088 Accuracy 0.8683750629425049\n",
      "Iteration 29840 Training loss 0.045766446739435196 Validation loss 0.04997529461979866 Accuracy 0.8691250681877136\n",
      "Iteration 29850 Training loss 0.05191687494516373 Validation loss 0.04996873810887337 Accuracy 0.8681250214576721\n",
      "Iteration 29860 Training loss 0.05594668164849281 Validation loss 0.04992283135652542 Accuracy 0.8693750500679016\n",
      "Iteration 29870 Training loss 0.0428566075861454 Validation loss 0.049912240356206894 Accuracy 0.8692500591278076\n",
      "Iteration 29880 Training loss 0.055435530841350555 Validation loss 0.049920082092285156 Accuracy 0.8692500591278076\n",
      "Iteration 29890 Training loss 0.040729276835918427 Validation loss 0.05001053214073181 Accuracy 0.8680000305175781\n",
      "Iteration 29900 Training loss 0.04392743110656738 Validation loss 0.05010027810931206 Accuracy 0.8670000433921814\n",
      "Iteration 29910 Training loss 0.04507480561733246 Validation loss 0.04994874820113182 Accuracy 0.8690000176429749\n",
      "Iteration 29920 Training loss 0.050802797079086304 Validation loss 0.04997361823916435 Accuracy 0.8686250448226929\n",
      "Iteration 29930 Training loss 0.04517132788896561 Validation loss 0.04995564743876457 Accuracy 0.8685000538825989\n",
      "Iteration 29940 Training loss 0.04714201018214226 Validation loss 0.050116755068302155 Accuracy 0.8673750162124634\n",
      "Iteration 29950 Training loss 0.04164566844701767 Validation loss 0.05002974346280098 Accuracy 0.8672500252723694\n",
      "Iteration 29960 Training loss 0.05292782559990883 Validation loss 0.049941424280405045 Accuracy 0.8688750267028809\n",
      "Iteration 29970 Training loss 0.046926967799663544 Validation loss 0.04991902410984039 Accuracy 0.8696250319480896\n",
      "Iteration 29980 Training loss 0.0507417768239975 Validation loss 0.0510089211165905 Accuracy 0.8646250367164612\n",
      "Iteration 29990 Training loss 0.04757130891084671 Validation loss 0.049943070858716965 Accuracy 0.8688750267028809\n",
      "Iteration 30000 Training loss 0.0443638414144516 Validation loss 0.05000186711549759 Accuracy 0.8680000305175781\n",
      "Iteration 30010 Training loss 0.04520739987492561 Validation loss 0.050026100128889084 Accuracy 0.8678750395774841\n",
      "Iteration 30020 Training loss 0.05194399878382683 Validation loss 0.050095949321985245 Accuracy 0.8682500123977661\n",
      "Iteration 30030 Training loss 0.0502854660153389 Validation loss 0.049960389733314514 Accuracy 0.8687500357627869\n",
      "Iteration 30040 Training loss 0.04712678864598274 Validation loss 0.05027179792523384 Accuracy 0.8682500123977661\n",
      "Iteration 30050 Training loss 0.04327134042978287 Validation loss 0.049953754991292953 Accuracy 0.8672500252723694\n",
      "Iteration 30060 Training loss 0.051584843546152115 Validation loss 0.04988939315080643 Accuracy 0.8698750138282776\n",
      "Iteration 30070 Training loss 0.04594840854406357 Validation loss 0.05038653686642647 Accuracy 0.8663750290870667\n",
      "Iteration 30080 Training loss 0.04515208303928375 Validation loss 0.05002908408641815 Accuracy 0.8673750162124634\n",
      "Iteration 30090 Training loss 0.04286831617355347 Validation loss 0.049885209649801254 Accuracy 0.8690000176429749\n",
      "Iteration 30100 Training loss 0.04791492223739624 Validation loss 0.04992825537919998 Accuracy 0.8693750500679016\n",
      "Iteration 30110 Training loss 0.041013214737176895 Validation loss 0.049860090017318726 Accuracy 0.8690000176429749\n",
      "Iteration 30120 Training loss 0.04674225673079491 Validation loss 0.04989178106188774 Accuracy 0.8683750629425049\n",
      "Iteration 30130 Training loss 0.0383375883102417 Validation loss 0.05019370838999748 Accuracy 0.8678750395774841\n",
      "Iteration 30140 Training loss 0.05126523971557617 Validation loss 0.04986311495304108 Accuracy 0.8687500357627869\n",
      "Iteration 30150 Training loss 0.04892222210764885 Validation loss 0.04987654462456703 Accuracy 0.8683750629425049\n",
      "Iteration 30160 Training loss 0.05491182208061218 Validation loss 0.0498475506901741 Accuracy 0.8687500357627869\n",
      "Iteration 30170 Training loss 0.04587018862366676 Validation loss 0.04983644559979439 Accuracy 0.8696250319480896\n",
      "Iteration 30180 Training loss 0.04289579391479492 Validation loss 0.049903180450201035 Accuracy 0.8682500123977661\n",
      "Iteration 30190 Training loss 0.05006112903356552 Validation loss 0.04996350407600403 Accuracy 0.8675000667572021\n",
      "Iteration 30200 Training loss 0.052186835557222366 Validation loss 0.04988778382539749 Accuracy 0.8687500357627869\n",
      "Iteration 30210 Training loss 0.0444486029446125 Validation loss 0.04991587996482849 Accuracy 0.8673750162124634\n",
      "Iteration 30220 Training loss 0.048225633800029755 Validation loss 0.049825262278318405 Accuracy 0.8688750267028809\n",
      "Iteration 30230 Training loss 0.045867402106523514 Validation loss 0.0498243048787117 Accuracy 0.8695000410079956\n",
      "Iteration 30240 Training loss 0.05220832675695419 Validation loss 0.04984322562813759 Accuracy 0.8683750629425049\n",
      "Iteration 30250 Training loss 0.050890058279037476 Validation loss 0.04983418807387352 Accuracy 0.8681250214576721\n",
      "Iteration 30260 Training loss 0.049984175711870193 Validation loss 0.04983391985297203 Accuracy 0.8693750500679016\n",
      "Iteration 30270 Training loss 0.04845232889056206 Validation loss 0.0498703233897686 Accuracy 0.8683750629425049\n",
      "Iteration 30280 Training loss 0.04176460579037666 Validation loss 0.04982319846749306 Accuracy 0.8691250681877136\n",
      "Iteration 30290 Training loss 0.043913524597883224 Validation loss 0.05041949823498726 Accuracy 0.8637500405311584\n",
      "Iteration 30300 Training loss 0.04760563746094704 Validation loss 0.0498533733189106 Accuracy 0.8690000176429749\n",
      "Iteration 30310 Training loss 0.04648219048976898 Validation loss 0.04977898299694061 Accuracy 0.8700000643730164\n",
      "Iteration 30320 Training loss 0.0514749139547348 Validation loss 0.049837496131658554 Accuracy 0.8687500357627869\n",
      "Iteration 30330 Training loss 0.04084373265504837 Validation loss 0.0497405044734478 Accuracy 0.8708750605583191\n",
      "Iteration 30340 Training loss 0.04662076011300087 Validation loss 0.04976517707109451 Accuracy 0.8698750138282776\n",
      "Iteration 30350 Training loss 0.04725608602166176 Validation loss 0.0497860424220562 Accuracy 0.8697500228881836\n",
      "Iteration 30360 Training loss 0.05364900827407837 Validation loss 0.050058793276548386 Accuracy 0.8671250343322754\n",
      "Iteration 30370 Training loss 0.03626814857125282 Validation loss 0.049761056900024414 Accuracy 0.8693750500679016\n",
      "Iteration 30380 Training loss 0.04535285010933876 Validation loss 0.049775224179029465 Accuracy 0.8697500228881836\n",
      "Iteration 30390 Training loss 0.044530630111694336 Validation loss 0.050091877579689026 Accuracy 0.8672500252723694\n",
      "Iteration 30400 Training loss 0.04195624217391014 Validation loss 0.05028460547327995 Accuracy 0.8670000433921814\n",
      "Iteration 30410 Training loss 0.047889575362205505 Validation loss 0.04978073388338089 Accuracy 0.8692500591278076\n",
      "Iteration 30420 Training loss 0.04047404229640961 Validation loss 0.049840979278087616 Accuracy 0.8696250319480896\n",
      "Iteration 30430 Training loss 0.05154816433787346 Validation loss 0.050116166472435 Accuracy 0.8677500486373901\n",
      "Iteration 30440 Training loss 0.04686445742845535 Validation loss 0.0510173961520195 Accuracy 0.8651250600814819\n",
      "Iteration 30450 Training loss 0.0471080020070076 Validation loss 0.049779925495386124 Accuracy 0.8700000643730164\n",
      "Iteration 30460 Training loss 0.04763970896601677 Validation loss 0.04980430752038956 Accuracy 0.8695000410079956\n",
      "Iteration 30470 Training loss 0.04476993903517723 Validation loss 0.04976610094308853 Accuracy 0.8691250681877136\n",
      "Iteration 30480 Training loss 0.044003553688526154 Validation loss 0.049882326275110245 Accuracy 0.8683750629425049\n",
      "Iteration 30490 Training loss 0.05825695022940636 Validation loss 0.04975458234548569 Accuracy 0.8695000410079956\n",
      "Iteration 30500 Training loss 0.04014232009649277 Validation loss 0.049830660223960876 Accuracy 0.8682500123977661\n",
      "Iteration 30510 Training loss 0.04791660234332085 Validation loss 0.04988561570644379 Accuracy 0.8677500486373901\n",
      "Iteration 30520 Training loss 0.05106644332408905 Validation loss 0.04970052093267441 Accuracy 0.8683750629425049\n",
      "Iteration 30530 Training loss 0.040643032640218735 Validation loss 0.04971856623888016 Accuracy 0.8691250681877136\n",
      "Iteration 30540 Training loss 0.044021595269441605 Validation loss 0.04969443753361702 Accuracy 0.8690000176429749\n",
      "Iteration 30550 Training loss 0.04550129175186157 Validation loss 0.04968808591365814 Accuracy 0.8695000410079956\n",
      "Iteration 30560 Training loss 0.043783169239759445 Validation loss 0.049697209149599075 Accuracy 0.8692500591278076\n",
      "Iteration 30570 Training loss 0.049566201865673065 Validation loss 0.049817804247140884 Accuracy 0.8687500357627869\n",
      "Iteration 30580 Training loss 0.046894997358322144 Validation loss 0.04969625174999237 Accuracy 0.8688750267028809\n",
      "Iteration 30590 Training loss 0.04841145873069763 Validation loss 0.04994689300656319 Accuracy 0.8681250214576721\n",
      "Iteration 30600 Training loss 0.04236570745706558 Validation loss 0.049692410975694656 Accuracy 0.8697500228881836\n",
      "Iteration 30610 Training loss 0.05146478861570358 Validation loss 0.050861749798059464 Accuracy 0.8656250238418579\n",
      "Iteration 30620 Training loss 0.04222644865512848 Validation loss 0.04970209300518036 Accuracy 0.8695000410079956\n",
      "Iteration 30630 Training loss 0.050085656344890594 Validation loss 0.04970535635948181 Accuracy 0.8687500357627869\n",
      "Iteration 30640 Training loss 0.04780076816678047 Validation loss 0.049804456532001495 Accuracy 0.8675000667572021\n",
      "Iteration 30650 Training loss 0.04217773303389549 Validation loss 0.04965902864933014 Accuracy 0.8698750138282776\n",
      "Iteration 30660 Training loss 0.04883529618382454 Validation loss 0.049644920974969864 Accuracy 0.8695000410079956\n",
      "Iteration 30670 Training loss 0.0466967336833477 Validation loss 0.04988919198513031 Accuracy 0.8695000410079956\n",
      "Iteration 30680 Training loss 0.043575841933488846 Validation loss 0.04984476789832115 Accuracy 0.8697500228881836\n",
      "Iteration 30690 Training loss 0.05399496108293533 Validation loss 0.04987962543964386 Accuracy 0.8666250705718994\n",
      "Iteration 30700 Training loss 0.04781213775277138 Validation loss 0.04978165030479431 Accuracy 0.8681250214576721\n",
      "Iteration 30710 Training loss 0.04809480533003807 Validation loss 0.04962779954075813 Accuracy 0.8698750138282776\n",
      "Iteration 30720 Training loss 0.038346707820892334 Validation loss 0.049666497856378555 Accuracy 0.8698750138282776\n",
      "Iteration 30730 Training loss 0.050604939460754395 Validation loss 0.04991549253463745 Accuracy 0.8676250576972961\n",
      "Iteration 30740 Training loss 0.04571264982223511 Validation loss 0.04972356557846069 Accuracy 0.8691250681877136\n",
      "Iteration 30750 Training loss 0.03911516070365906 Validation loss 0.049897920340299606 Accuracy 0.8680000305175781\n",
      "Iteration 30760 Training loss 0.051302745938301086 Validation loss 0.05061180517077446 Accuracy 0.8671250343322754\n",
      "Iteration 30770 Training loss 0.05244771018624306 Validation loss 0.04968178644776344 Accuracy 0.8690000176429749\n",
      "Iteration 30780 Training loss 0.045688655227422714 Validation loss 0.049748294055461884 Accuracy 0.8690000176429749\n",
      "Iteration 30790 Training loss 0.0430402047932148 Validation loss 0.04976867884397507 Accuracy 0.8683750629425049\n",
      "Iteration 30800 Training loss 0.0424656867980957 Validation loss 0.04974709078669548 Accuracy 0.8686250448226929\n",
      "Iteration 30810 Training loss 0.05030612647533417 Validation loss 0.05057016387581825 Accuracy 0.8676250576972961\n",
      "Iteration 30820 Training loss 0.05268361419439316 Validation loss 0.05064794421195984 Accuracy 0.8671250343322754\n",
      "Iteration 30830 Training loss 0.04870746657252312 Validation loss 0.05030272528529167 Accuracy 0.8645000457763672\n",
      "Iteration 30840 Training loss 0.0478210486471653 Validation loss 0.04979509860277176 Accuracy 0.8680000305175781\n",
      "Iteration 30850 Training loss 0.03942018747329712 Validation loss 0.049760423600673676 Accuracy 0.8675000667572021\n",
      "Iteration 30860 Training loss 0.04407653212547302 Validation loss 0.049962326884269714 Accuracy 0.8686250448226929\n",
      "Iteration 30870 Training loss 0.04682275652885437 Validation loss 0.04979486018419266 Accuracy 0.8665000200271606\n",
      "Iteration 30880 Training loss 0.04762348160147667 Validation loss 0.04961834475398064 Accuracy 0.8683750629425049\n",
      "Iteration 30890 Training loss 0.04746195673942566 Validation loss 0.04976683109998703 Accuracy 0.8687500357627869\n",
      "Iteration 30900 Training loss 0.046951692551374435 Validation loss 0.049605999141931534 Accuracy 0.8696250319480896\n",
      "Iteration 30910 Training loss 0.049842946231365204 Validation loss 0.049720555543899536 Accuracy 0.8695000410079956\n",
      "Iteration 30920 Training loss 0.051956452429294586 Validation loss 0.049631524831056595 Accuracy 0.8688750267028809\n",
      "Iteration 30930 Training loss 0.03787469118833542 Validation loss 0.04986310005187988 Accuracy 0.8678750395774841\n",
      "Iteration 30940 Training loss 0.04042496904730797 Validation loss 0.04964316263794899 Accuracy 0.8690000176429749\n",
      "Iteration 30950 Training loss 0.0485830195248127 Validation loss 0.04961412772536278 Accuracy 0.8686250448226929\n",
      "Iteration 30960 Training loss 0.04587429389357567 Validation loss 0.049601249396800995 Accuracy 0.8698750138282776\n",
      "Iteration 30970 Training loss 0.05100206658244133 Validation loss 0.04967794194817543 Accuracy 0.8678750395774841\n",
      "Iteration 30980 Training loss 0.042549170553684235 Validation loss 0.05030696466565132 Accuracy 0.8635000586509705\n",
      "Iteration 30990 Training loss 0.04523032531142235 Validation loss 0.04990572854876518 Accuracy 0.8697500228881836\n",
      "Iteration 31000 Training loss 0.053299058228731155 Validation loss 0.0498160719871521 Accuracy 0.8661250472068787\n",
      "Iteration 31010 Training loss 0.05014592036604881 Validation loss 0.049646131694316864 Accuracy 0.8690000176429749\n",
      "Iteration 31020 Training loss 0.04130150377750397 Validation loss 0.04962312430143356 Accuracy 0.8698750138282776\n",
      "Iteration 31030 Training loss 0.0448082871735096 Validation loss 0.04972398281097412 Accuracy 0.8683750629425049\n",
      "Iteration 31040 Training loss 0.03825486823916435 Validation loss 0.04979073628783226 Accuracy 0.8690000176429749\n",
      "Iteration 31050 Training loss 0.04189542680978775 Validation loss 0.0496172197163105 Accuracy 0.8692500591278076\n",
      "Iteration 31060 Training loss 0.04690984636545181 Validation loss 0.0496259480714798 Accuracy 0.8690000176429749\n",
      "Iteration 31070 Training loss 0.054995451122522354 Validation loss 0.04964837804436684 Accuracy 0.8696250319480896\n",
      "Iteration 31080 Training loss 0.04141532629728317 Validation loss 0.05027453228831291 Accuracy 0.8680000305175781\n",
      "Iteration 31090 Training loss 0.04330039769411087 Validation loss 0.05002265423536301 Accuracy 0.8682500123977661\n",
      "Iteration 31100 Training loss 0.03992095962166786 Validation loss 0.0502639040350914 Accuracy 0.8673750162124634\n",
      "Iteration 31110 Training loss 0.04622700437903404 Validation loss 0.049915410578250885 Accuracy 0.8693750500679016\n",
      "Iteration 31120 Training loss 0.043799202889204025 Validation loss 0.04969320073723793 Accuracy 0.8691250681877136\n",
      "Iteration 31130 Training loss 0.044437993317842484 Validation loss 0.04969024285674095 Accuracy 0.8700000643730164\n",
      "Iteration 31140 Training loss 0.04252638667821884 Validation loss 0.049955353140830994 Accuracy 0.8693750500679016\n",
      "Iteration 31150 Training loss 0.0468897745013237 Validation loss 0.04953301325440407 Accuracy 0.8692500591278076\n",
      "Iteration 31160 Training loss 0.04702000692486763 Validation loss 0.04954008013010025 Accuracy 0.8701250553131104\n",
      "Iteration 31170 Training loss 0.04302624240517616 Validation loss 0.04999049752950668 Accuracy 0.8660000562667847\n",
      "Iteration 31180 Training loss 0.04493814706802368 Validation loss 0.049703359603881836 Accuracy 0.8678750395774841\n",
      "Iteration 31190 Training loss 0.04275600239634514 Validation loss 0.04953474923968315 Accuracy 0.8700000643730164\n",
      "Iteration 31200 Training loss 0.04161151871085167 Validation loss 0.04968754202127457 Accuracy 0.8672500252723694\n",
      "Iteration 31210 Training loss 0.051792919635772705 Validation loss 0.04951161891222 Accuracy 0.8688750267028809\n",
      "Iteration 31220 Training loss 0.050275132060050964 Validation loss 0.0494876392185688 Accuracy 0.8700000643730164\n",
      "Iteration 31230 Training loss 0.04477132856845856 Validation loss 0.049577876925468445 Accuracy 0.8692500591278076\n",
      "Iteration 31240 Training loss 0.0383487306535244 Validation loss 0.04950094223022461 Accuracy 0.8702500462532043\n",
      "Iteration 31250 Training loss 0.03652083873748779 Validation loss 0.04958455264568329 Accuracy 0.8690000176429749\n",
      "Iteration 31260 Training loss 0.05331110954284668 Validation loss 0.04972383379936218 Accuracy 0.8670000433921814\n",
      "Iteration 31270 Training loss 0.05428424850106239 Validation loss 0.04947866126894951 Accuracy 0.8707500696182251\n",
      "Iteration 31280 Training loss 0.048365868628025055 Validation loss 0.049454912543296814 Accuracy 0.8703750371932983\n",
      "Iteration 31290 Training loss 0.04574103280901909 Validation loss 0.049463823437690735 Accuracy 0.8706250190734863\n",
      "Iteration 31300 Training loss 0.037287387996912 Validation loss 0.04954046383500099 Accuracy 0.8692500591278076\n",
      "Iteration 31310 Training loss 0.04812576249241829 Validation loss 0.04943850636482239 Accuracy 0.8701250553131104\n",
      "Iteration 31320 Training loss 0.04664578288793564 Validation loss 0.04947778210043907 Accuracy 0.8700000643730164\n",
      "Iteration 31330 Training loss 0.055436328053474426 Validation loss 0.04942474514245987 Accuracy 0.8696250319480896\n",
      "Iteration 31340 Training loss 0.04686063155531883 Validation loss 0.049485884606838226 Accuracy 0.8698750138282776\n",
      "Iteration 31350 Training loss 0.05168847739696503 Validation loss 0.04970892518758774 Accuracy 0.8681250214576721\n",
      "Iteration 31360 Training loss 0.04369441047310829 Validation loss 0.04950815066695213 Accuracy 0.8696250319480896\n",
      "Iteration 31370 Training loss 0.043986670672893524 Validation loss 0.04950053244829178 Accuracy 0.8695000410079956\n",
      "Iteration 31380 Training loss 0.04473455622792244 Validation loss 0.04949158802628517 Accuracy 0.8693750500679016\n",
      "Iteration 31390 Training loss 0.045408643782138824 Validation loss 0.0496547631919384 Accuracy 0.8688750267028809\n",
      "Iteration 31400 Training loss 0.04003497213125229 Validation loss 0.04954368993639946 Accuracy 0.8686250448226929\n",
      "Iteration 31410 Training loss 0.04600710794329643 Validation loss 0.04947312921285629 Accuracy 0.8702500462532043\n",
      "Iteration 31420 Training loss 0.046662092208862305 Validation loss 0.049502428621053696 Accuracy 0.8706250190734863\n",
      "Iteration 31430 Training loss 0.04055047780275345 Validation loss 0.04944920539855957 Accuracy 0.8712500333786011\n",
      "Iteration 31440 Training loss 0.04642626643180847 Validation loss 0.04960658401250839 Accuracy 0.8693750500679016\n",
      "Iteration 31450 Training loss 0.04192645847797394 Validation loss 0.04947346821427345 Accuracy 0.8695000410079956\n",
      "Iteration 31460 Training loss 0.04642830416560173 Validation loss 0.04942961037158966 Accuracy 0.8700000643730164\n",
      "Iteration 31470 Training loss 0.050100214779376984 Validation loss 0.049453120678663254 Accuracy 0.8701250553131104\n",
      "Iteration 31480 Training loss 0.04260769113898277 Validation loss 0.049649689346551895 Accuracy 0.8682500123977661\n",
      "Iteration 31490 Training loss 0.04805576801300049 Validation loss 0.04966093599796295 Accuracy 0.8673750162124634\n",
      "Iteration 31500 Training loss 0.049822527915239334 Validation loss 0.049621399492025375 Accuracy 0.8678750395774841\n",
      "Iteration 31510 Training loss 0.04857302084565163 Validation loss 0.04945986345410347 Accuracy 0.8693750500679016\n",
      "Iteration 31520 Training loss 0.050573185086250305 Validation loss 0.04940063878893852 Accuracy 0.8695000410079956\n",
      "Iteration 31530 Training loss 0.04739425703883171 Validation loss 0.049491528421640396 Accuracy 0.8687500357627869\n",
      "Iteration 31540 Training loss 0.03662396967411041 Validation loss 0.04955922067165375 Accuracy 0.8676250576972961\n",
      "Iteration 31550 Training loss 0.05215878412127495 Validation loss 0.04964849725365639 Accuracy 0.8667500615119934\n",
      "Iteration 31560 Training loss 0.037969160825014114 Validation loss 0.04950911924242973 Accuracy 0.8676250576972961\n",
      "Iteration 31570 Training loss 0.039878711104393005 Validation loss 0.049402181059122086 Accuracy 0.8698750138282776\n",
      "Iteration 31580 Training loss 0.04013899713754654 Validation loss 0.049396153539419174 Accuracy 0.8703750371932983\n",
      "Iteration 31590 Training loss 0.043691229075193405 Validation loss 0.04946941137313843 Accuracy 0.8707500696182251\n",
      "Iteration 31600 Training loss 0.04526380077004433 Validation loss 0.04945871978998184 Accuracy 0.8710000514984131\n",
      "Iteration 31610 Training loss 0.04322507232427597 Validation loss 0.049469783902168274 Accuracy 0.8701250553131104\n",
      "Iteration 31620 Training loss 0.04829464480280876 Validation loss 0.04939817264676094 Accuracy 0.8695000410079956\n",
      "Iteration 31630 Training loss 0.0483323372900486 Validation loss 0.04948623105883598 Accuracy 0.8675000667572021\n",
      "Iteration 31640 Training loss 0.0498264878988266 Validation loss 0.049435924738645554 Accuracy 0.8702500462532043\n",
      "Iteration 31650 Training loss 0.041957635432481766 Validation loss 0.049409326165914536 Accuracy 0.8696250319480896\n",
      "Iteration 31660 Training loss 0.04248903691768646 Validation loss 0.049559444189071655 Accuracy 0.8692500591278076\n",
      "Iteration 31670 Training loss 0.047698725014925 Validation loss 0.049479890614748 Accuracy 0.8691250681877136\n",
      "Iteration 31680 Training loss 0.039557721465826035 Validation loss 0.049331631511449814 Accuracy 0.8701250553131104\n",
      "Iteration 31690 Training loss 0.05113472789525986 Validation loss 0.04936540499329567 Accuracy 0.8702500462532043\n",
      "Iteration 31700 Training loss 0.05073001608252525 Validation loss 0.049342937767505646 Accuracy 0.8706250190734863\n",
      "Iteration 31710 Training loss 0.04746472090482712 Validation loss 0.0498865470290184 Accuracy 0.8698750138282776\n",
      "Iteration 31720 Training loss 0.04221009090542793 Validation loss 0.04946988821029663 Accuracy 0.8695000410079956\n",
      "Iteration 31730 Training loss 0.04541133716702461 Validation loss 0.04933498799800873 Accuracy 0.8697500228881836\n",
      "Iteration 31740 Training loss 0.04811001569032669 Validation loss 0.04970037192106247 Accuracy 0.8673750162124634\n",
      "Iteration 31750 Training loss 0.04769256338477135 Validation loss 0.04934097081422806 Accuracy 0.8703750371932983\n",
      "Iteration 31760 Training loss 0.04872683435678482 Validation loss 0.04931369423866272 Accuracy 0.8700000643730164\n",
      "Iteration 31770 Training loss 0.041824452579021454 Validation loss 0.049370720982551575 Accuracy 0.8691250681877136\n",
      "Iteration 31780 Training loss 0.04971687123179436 Validation loss 0.04932216554880142 Accuracy 0.8696250319480896\n",
      "Iteration 31790 Training loss 0.04280632734298706 Validation loss 0.049330610781908035 Accuracy 0.8712500333786011\n",
      "Iteration 31800 Training loss 0.04361676424741745 Validation loss 0.04958249256014824 Accuracy 0.8701250553131104\n",
      "Iteration 31810 Training loss 0.0400245264172554 Validation loss 0.049453433603048325 Accuracy 0.8701250553131104\n",
      "Iteration 31820 Training loss 0.04706908017396927 Validation loss 0.049573492258787155 Accuracy 0.8696250319480896\n",
      "Iteration 31830 Training loss 0.043115586042404175 Validation loss 0.04933631420135498 Accuracy 0.8697500228881836\n",
      "Iteration 31840 Training loss 0.04457099735736847 Validation loss 0.049379702657461166 Accuracy 0.8685000538825989\n",
      "Iteration 31850 Training loss 0.03999362885951996 Validation loss 0.04939519613981247 Accuracy 0.8702500462532043\n",
      "Iteration 31860 Training loss 0.03972681984305382 Validation loss 0.04926681146025658 Accuracy 0.8696250319480896\n",
      "Iteration 31870 Training loss 0.044513098895549774 Validation loss 0.049483537673950195 Accuracy 0.8688750267028809\n",
      "Iteration 31880 Training loss 0.05103231221437454 Validation loss 0.04941904544830322 Accuracy 0.8698750138282776\n",
      "Iteration 31890 Training loss 0.04680384695529938 Validation loss 0.04940569028258324 Accuracy 0.8700000643730164\n",
      "Iteration 31900 Training loss 0.043203286826610565 Validation loss 0.049345195293426514 Accuracy 0.8693750500679016\n",
      "Iteration 31910 Training loss 0.04857160523533821 Validation loss 0.04936375841498375 Accuracy 0.8696250319480896\n",
      "Iteration 31920 Training loss 0.04968925192952156 Validation loss 0.049944184720516205 Accuracy 0.8685000538825989\n",
      "Iteration 31930 Training loss 0.05109822750091553 Validation loss 0.04942695051431656 Accuracy 0.8675000667572021\n",
      "Iteration 31940 Training loss 0.04539280757308006 Validation loss 0.049715351313352585 Accuracy 0.8665000200271606\n",
      "Iteration 31950 Training loss 0.05002216622233391 Validation loss 0.04943525791168213 Accuracy 0.8686250448226929\n",
      "Iteration 31960 Training loss 0.05085909739136696 Validation loss 0.04937680438160896 Accuracy 0.8688750267028809\n",
      "Iteration 31970 Training loss 0.05478818342089653 Validation loss 0.04940958321094513 Accuracy 0.8702500462532043\n",
      "Iteration 31980 Training loss 0.05790195241570473 Validation loss 0.04945804178714752 Accuracy 0.8687500357627869\n",
      "Iteration 31990 Training loss 0.044065237045288086 Validation loss 0.04928434267640114 Accuracy 0.8705000281333923\n",
      "Iteration 32000 Training loss 0.04259559512138367 Validation loss 0.04919654130935669 Accuracy 0.8715000152587891\n",
      "Iteration 32010 Training loss 0.05026652291417122 Validation loss 0.04923143982887268 Accuracy 0.8706250190734863\n",
      "Iteration 32020 Training loss 0.04727062210440636 Validation loss 0.04949387162923813 Accuracy 0.8698750138282776\n",
      "Iteration 32030 Training loss 0.04261814057826996 Validation loss 0.0494469553232193 Accuracy 0.8680000305175781\n",
      "Iteration 32040 Training loss 0.04292536526918411 Validation loss 0.049696844071149826 Accuracy 0.8668750524520874\n",
      "Iteration 32050 Training loss 0.040524229407310486 Validation loss 0.04917917028069496 Accuracy 0.8712500333786011\n",
      "Iteration 32060 Training loss 0.04509764164686203 Validation loss 0.04922407120466232 Accuracy 0.8707500696182251\n",
      "Iteration 32070 Training loss 0.04443443566560745 Validation loss 0.049226149916648865 Accuracy 0.8712500333786011\n",
      "Iteration 32080 Training loss 0.04710669070482254 Validation loss 0.04927690327167511 Accuracy 0.8706250190734863\n",
      "Iteration 32090 Training loss 0.04637965187430382 Validation loss 0.04941707104444504 Accuracy 0.8683750629425049\n",
      "Iteration 32100 Training loss 0.048136577010154724 Validation loss 0.04919067397713661 Accuracy 0.8702500462532043\n",
      "Iteration 32110 Training loss 0.042792461812496185 Validation loss 0.049172546714544296 Accuracy 0.8710000514984131\n",
      "Iteration 32120 Training loss 0.043125562369823456 Validation loss 0.0492367185652256 Accuracy 0.8706250190734863\n",
      "Iteration 32130 Training loss 0.045544013381004333 Validation loss 0.0492505244910717 Accuracy 0.8700000643730164\n",
      "Iteration 32140 Training loss 0.037224724888801575 Validation loss 0.04924697428941727 Accuracy 0.8702500462532043\n",
      "Iteration 32150 Training loss 0.04595726355910301 Validation loss 0.049281440675258636 Accuracy 0.8702500462532043\n",
      "Iteration 32160 Training loss 0.049175094813108444 Validation loss 0.049230802804231644 Accuracy 0.8711250424385071\n",
      "Iteration 32170 Training loss 0.04363495111465454 Validation loss 0.04957021772861481 Accuracy 0.8672500252723694\n",
      "Iteration 32180 Training loss 0.049818843603134155 Validation loss 0.04922033101320267 Accuracy 0.8705000281333923\n",
      "Iteration 32190 Training loss 0.045151472091674805 Validation loss 0.04938558489084244 Accuracy 0.8690000176429749\n",
      "Iteration 32200 Training loss 0.04583654925227165 Validation loss 0.04920166730880737 Accuracy 0.8700000643730164\n",
      "Iteration 32210 Training loss 0.04118838161230087 Validation loss 0.04916909709572792 Accuracy 0.8705000281333923\n",
      "Iteration 32220 Training loss 0.04067637398838997 Validation loss 0.049186863005161285 Accuracy 0.8703750371932983\n",
      "Iteration 32230 Training loss 0.049257174134254456 Validation loss 0.04977545514702797 Accuracy 0.8682500123977661\n",
      "Iteration 32240 Training loss 0.04443199932575226 Validation loss 0.04909785836935043 Accuracy 0.8703750371932983\n",
      "Iteration 32250 Training loss 0.0421573743224144 Validation loss 0.04910161346197128 Accuracy 0.8710000514984131\n",
      "Iteration 32260 Training loss 0.037553559988737106 Validation loss 0.049167849123477936 Accuracy 0.8698750138282776\n",
      "Iteration 32270 Training loss 0.04440778121352196 Validation loss 0.04917966201901436 Accuracy 0.8703750371932983\n",
      "Iteration 32280 Training loss 0.047550834715366364 Validation loss 0.04929307848215103 Accuracy 0.8701250553131104\n",
      "Iteration 32290 Training loss 0.053160410374403 Validation loss 0.04915826767683029 Accuracy 0.8711250424385071\n",
      "Iteration 32300 Training loss 0.04703177511692047 Validation loss 0.04914749786257744 Accuracy 0.8715000152587891\n",
      "Iteration 32310 Training loss 0.04173508659005165 Validation loss 0.04912082105875015 Accuracy 0.8701250553131104\n",
      "Iteration 32320 Training loss 0.04437969624996185 Validation loss 0.04964180290699005 Accuracy 0.8697500228881836\n",
      "Iteration 32330 Training loss 0.045076511800289154 Validation loss 0.04914895072579384 Accuracy 0.8712500333786011\n",
      "Iteration 32340 Training loss 0.05004628375172615 Validation loss 0.049139536917209625 Accuracy 0.8706250190734863\n",
      "Iteration 32350 Training loss 0.04668360576033592 Validation loss 0.04927707836031914 Accuracy 0.8690000176429749\n",
      "Iteration 32360 Training loss 0.0449172779917717 Validation loss 0.049159035086631775 Accuracy 0.8715000152587891\n",
      "Iteration 32370 Training loss 0.04413488879799843 Validation loss 0.04945606365799904 Accuracy 0.8670000433921814\n",
      "Iteration 32380 Training loss 0.039547257125377655 Validation loss 0.04913359507918358 Accuracy 0.8698750138282776\n",
      "Iteration 32390 Training loss 0.045382097363471985 Validation loss 0.049263596534729004 Accuracy 0.8686250448226929\n",
      "Iteration 32400 Training loss 0.04176332429051399 Validation loss 0.049178194254636765 Accuracy 0.8688750267028809\n",
      "Iteration 32410 Training loss 0.033423617482185364 Validation loss 0.049191005527973175 Accuracy 0.8702500462532043\n",
      "Iteration 32420 Training loss 0.04785468429327011 Validation loss 0.04917578771710396 Accuracy 0.8706250190734863\n",
      "Iteration 32430 Training loss 0.05407532677054405 Validation loss 0.04958001896739006 Accuracy 0.8701250553131104\n",
      "Iteration 32440 Training loss 0.0484066940844059 Validation loss 0.04908649995923042 Accuracy 0.8707500696182251\n",
      "Iteration 32450 Training loss 0.04289165511727333 Validation loss 0.04961341246962547 Accuracy 0.8708750605583191\n",
      "Iteration 32460 Training loss 0.04881800711154938 Validation loss 0.04918928071856499 Accuracy 0.8710000514984131\n",
      "Iteration 32470 Training loss 0.043630607426166534 Validation loss 0.04910924285650253 Accuracy 0.8710000514984131\n",
      "Iteration 32480 Training loss 0.045045290142297745 Validation loss 0.049069181084632874 Accuracy 0.8701250553131104\n",
      "Iteration 32490 Training loss 0.05455901846289635 Validation loss 0.04905252531170845 Accuracy 0.8713750243186951\n",
      "Iteration 32500 Training loss 0.04769362509250641 Validation loss 0.04909645393490791 Accuracy 0.8703750371932983\n",
      "Iteration 32510 Training loss 0.04024917632341385 Validation loss 0.04904238507151604 Accuracy 0.8711250424385071\n",
      "Iteration 32520 Training loss 0.05040182173252106 Validation loss 0.0490436889231205 Accuracy 0.8701250553131104\n",
      "Iteration 32530 Training loss 0.04531949758529663 Validation loss 0.04904945567250252 Accuracy 0.8720000386238098\n",
      "Iteration 32540 Training loss 0.04788563773036003 Validation loss 0.04931027442216873 Accuracy 0.8686250448226929\n",
      "Iteration 32550 Training loss 0.04776381701231003 Validation loss 0.049118731170892715 Accuracy 0.8708750605583191\n",
      "Iteration 32560 Training loss 0.0501658134162426 Validation loss 0.049050942063331604 Accuracy 0.8715000152587891\n",
      "Iteration 32570 Training loss 0.048898305743932724 Validation loss 0.04902211204171181 Accuracy 0.8710000514984131\n",
      "Iteration 32580 Training loss 0.04373099282383919 Validation loss 0.04915944114327431 Accuracy 0.8710000514984131\n",
      "Iteration 32590 Training loss 0.03902733698487282 Validation loss 0.04909123107790947 Accuracy 0.8708750605583191\n",
      "Iteration 32600 Training loss 0.04750822111964226 Validation loss 0.049218371510505676 Accuracy 0.8703750371932983\n",
      "Iteration 32610 Training loss 0.04667384549975395 Validation loss 0.04910169541835785 Accuracy 0.8712500333786011\n",
      "Iteration 32620 Training loss 0.041859474033117294 Validation loss 0.049089450389146805 Accuracy 0.8708750605583191\n",
      "Iteration 32630 Training loss 0.03989503160119057 Validation loss 0.04932329058647156 Accuracy 0.8705000281333923\n",
      "Iteration 32640 Training loss 0.040745023638010025 Validation loss 0.04906383901834488 Accuracy 0.8701250553131104\n",
      "Iteration 32650 Training loss 0.042126283049583435 Validation loss 0.04903849959373474 Accuracy 0.8717500567436218\n",
      "Iteration 32660 Training loss 0.0435919463634491 Validation loss 0.0491066500544548 Accuracy 0.8703750371932983\n",
      "Iteration 32670 Training loss 0.05047076940536499 Validation loss 0.049082208424806595 Accuracy 0.8712500333786011\n",
      "Iteration 32680 Training loss 0.042403072118759155 Validation loss 0.04914884269237518 Accuracy 0.8696250319480896\n",
      "Iteration 32690 Training loss 0.04796759411692619 Validation loss 0.04932539910078049 Accuracy 0.8706250190734863\n",
      "Iteration 32700 Training loss 0.04153810068964958 Validation loss 0.04981747269630432 Accuracy 0.8652500510215759\n",
      "Iteration 32710 Training loss 0.03829857334494591 Validation loss 0.04905546084046364 Accuracy 0.8715000152587891\n",
      "Iteration 32720 Training loss 0.052482202649116516 Validation loss 0.049208298325538635 Accuracy 0.8712500333786011\n",
      "Iteration 32730 Training loss 0.04474920034408569 Validation loss 0.04894925653934479 Accuracy 0.8711250424385071\n",
      "Iteration 32740 Training loss 0.045460257679224014 Validation loss 0.049805909395217896 Accuracy 0.8687500357627869\n",
      "Iteration 32750 Training loss 0.04994329437613487 Validation loss 0.04918334633111954 Accuracy 0.8691250681877136\n",
      "Iteration 32760 Training loss 0.03973877429962158 Validation loss 0.04906124994158745 Accuracy 0.8711250424385071\n",
      "Iteration 32770 Training loss 0.05040867626667023 Validation loss 0.04899272695183754 Accuracy 0.8710000514984131\n",
      "Iteration 32780 Training loss 0.05092252790927887 Validation loss 0.049036070704460144 Accuracy 0.8705000281333923\n",
      "Iteration 32790 Training loss 0.045215606689453125 Validation loss 0.049158334732055664 Accuracy 0.8712500333786011\n",
      "Iteration 32800 Training loss 0.04695666581392288 Validation loss 0.04909820854663849 Accuracy 0.8701250553131104\n",
      "Iteration 32810 Training loss 0.049129389226436615 Validation loss 0.048995569348335266 Accuracy 0.8710000514984131\n",
      "Iteration 32820 Training loss 0.05686641484498978 Validation loss 0.04894133284687996 Accuracy 0.8717500567436218\n",
      "Iteration 32830 Training loss 0.03893977403640747 Validation loss 0.048968493938446045 Accuracy 0.8715000152587891\n",
      "Iteration 32840 Training loss 0.04881502687931061 Validation loss 0.04893568903207779 Accuracy 0.8711250424385071\n",
      "Iteration 32850 Training loss 0.04615537449717522 Validation loss 0.04911272972822189 Accuracy 0.8698750138282776\n",
      "Iteration 32860 Training loss 0.044698379933834076 Validation loss 0.04930077865719795 Accuracy 0.8692500591278076\n",
      "Iteration 32870 Training loss 0.045148514211177826 Validation loss 0.0490877702832222 Accuracy 0.8715000152587891\n",
      "Iteration 32880 Training loss 0.050161972641944885 Validation loss 0.048951730132102966 Accuracy 0.8715000152587891\n",
      "Iteration 32890 Training loss 0.04707883298397064 Validation loss 0.048902906477451324 Accuracy 0.8717500567436218\n",
      "Iteration 32900 Training loss 0.04589464142918587 Validation loss 0.04915817454457283 Accuracy 0.8713750243186951\n",
      "Iteration 32910 Training loss 0.0356692336499691 Validation loss 0.048926182091236115 Accuracy 0.8720000386238098\n",
      "Iteration 32920 Training loss 0.041740067303180695 Validation loss 0.04906882718205452 Accuracy 0.8707500696182251\n",
      "Iteration 32930 Training loss 0.04656614363193512 Validation loss 0.04888666421175003 Accuracy 0.8722500205039978\n",
      "Iteration 32940 Training loss 0.052704665809869766 Validation loss 0.048904795199632645 Accuracy 0.8712500333786011\n",
      "Iteration 32950 Training loss 0.04066799208521843 Validation loss 0.048916809260845184 Accuracy 0.8708750605583191\n",
      "Iteration 32960 Training loss 0.052230510860681534 Validation loss 0.04974942281842232 Accuracy 0.8685000538825989\n",
      "Iteration 32970 Training loss 0.0480438657104969 Validation loss 0.05028148740530014 Accuracy 0.8663750290870667\n",
      "Iteration 32980 Training loss 0.04639584571123123 Validation loss 0.049363140016794205 Accuracy 0.8702500462532043\n",
      "Iteration 32990 Training loss 0.05168521776795387 Validation loss 0.04931849241256714 Accuracy 0.8710000514984131\n",
      "Iteration 33000 Training loss 0.04200316220521927 Validation loss 0.048906825482845306 Accuracy 0.8701250553131104\n",
      "Iteration 33010 Training loss 0.05041983351111412 Validation loss 0.04890289530158043 Accuracy 0.8702500462532043\n",
      "Iteration 33020 Training loss 0.04896407946944237 Validation loss 0.04899607226252556 Accuracy 0.8706250190734863\n",
      "Iteration 33030 Training loss 0.04344099760055542 Validation loss 0.04939195513725281 Accuracy 0.8706250190734863\n",
      "Iteration 33040 Training loss 0.05039570853114128 Validation loss 0.04923529922962189 Accuracy 0.8686250448226929\n",
      "Iteration 33050 Training loss 0.043207380920648575 Validation loss 0.0488879419863224 Accuracy 0.8713750243186951\n",
      "Iteration 33060 Training loss 0.040571101009845734 Validation loss 0.048892028629779816 Accuracy 0.8720000386238098\n",
      "Iteration 33070 Training loss 0.043404631316661835 Validation loss 0.04948744550347328 Accuracy 0.8718750476837158\n",
      "Iteration 33080 Training loss 0.04442324489355087 Validation loss 0.04911387711763382 Accuracy 0.8703750371932983\n",
      "Iteration 33090 Training loss 0.04412126913666725 Validation loss 0.04891284555196762 Accuracy 0.8717500567436218\n",
      "Iteration 33100 Training loss 0.05162570998072624 Validation loss 0.048913005739450455 Accuracy 0.8716250658035278\n",
      "Iteration 33110 Training loss 0.04672779142856598 Validation loss 0.048944659531116486 Accuracy 0.8701250553131104\n",
      "Iteration 33120 Training loss 0.04031888023018837 Validation loss 0.04889901354908943 Accuracy 0.8710000514984131\n",
      "Iteration 33130 Training loss 0.04530857130885124 Validation loss 0.048952218145132065 Accuracy 0.8718750476837158\n",
      "Iteration 33140 Training loss 0.04590413346886635 Validation loss 0.04890965297818184 Accuracy 0.8700000643730164\n",
      "Iteration 33150 Training loss 0.04593544453382492 Validation loss 0.04890747740864754 Accuracy 0.8701250553131104\n",
      "Iteration 33160 Training loss 0.04318510368466377 Validation loss 0.04892774298787117 Accuracy 0.8712500333786011\n",
      "Iteration 33170 Training loss 0.036129359155893326 Validation loss 0.04903656989336014 Accuracy 0.8681250214576721\n",
      "Iteration 33180 Training loss 0.04747088626027107 Validation loss 0.04884021729230881 Accuracy 0.8717500567436218\n",
      "Iteration 33190 Training loss 0.04756684973835945 Validation loss 0.048980455845594406 Accuracy 0.8700000643730164\n",
      "Iteration 33200 Training loss 0.05020812526345253 Validation loss 0.049124088138341904 Accuracy 0.8718750476837158\n",
      "Iteration 33210 Training loss 0.0489315539598465 Validation loss 0.04929261654615402 Accuracy 0.8717500567436218\n",
      "Iteration 33220 Training loss 0.04486100748181343 Validation loss 0.049038439989089966 Accuracy 0.8716250658035278\n",
      "Iteration 33230 Training loss 0.0472651869058609 Validation loss 0.048809122294187546 Accuracy 0.8721250295639038\n",
      "Iteration 33240 Training loss 0.05439719930291176 Validation loss 0.04907281696796417 Accuracy 0.8711250424385071\n",
      "Iteration 33250 Training loss 0.04367758706212044 Validation loss 0.04936223477125168 Accuracy 0.8711250424385071\n",
      "Iteration 33260 Training loss 0.043022263795137405 Validation loss 0.04884892329573631 Accuracy 0.8710000514984131\n",
      "Iteration 33270 Training loss 0.0463985875248909 Validation loss 0.048962634056806564 Accuracy 0.8713750243186951\n",
      "Iteration 33280 Training loss 0.04537727311253548 Validation loss 0.0500430203974247 Accuracy 0.8676250576972961\n",
      "Iteration 33290 Training loss 0.049321968108415604 Validation loss 0.04895123094320297 Accuracy 0.8710000514984131\n",
      "Iteration 33300 Training loss 0.04411550983786583 Validation loss 0.04888175055384636 Accuracy 0.8705000281333923\n",
      "Iteration 33310 Training loss 0.05267318710684776 Validation loss 0.04889430105686188 Accuracy 0.8698750138282776\n",
      "Iteration 33320 Training loss 0.04442736506462097 Validation loss 0.04893847927451134 Accuracy 0.8720000386238098\n",
      "Iteration 33330 Training loss 0.051407866179943085 Validation loss 0.04914766922593117 Accuracy 0.8720000386238098\n",
      "Iteration 33340 Training loss 0.04879537969827652 Validation loss 0.0492975190281868 Accuracy 0.8715000152587891\n",
      "Iteration 33350 Training loss 0.04439827799797058 Validation loss 0.048834558576345444 Accuracy 0.8720000386238098\n",
      "Iteration 33360 Training loss 0.039460863918066025 Validation loss 0.04876480624079704 Accuracy 0.8715000152587891\n",
      "Iteration 33370 Training loss 0.04251755028963089 Validation loss 0.04889470338821411 Accuracy 0.8697500228881836\n",
      "Iteration 33380 Training loss 0.05283114314079285 Validation loss 0.04893431439995766 Accuracy 0.8718750476837158\n",
      "Iteration 33390 Training loss 0.045732803642749786 Validation loss 0.048753708600997925 Accuracy 0.8730000257492065\n",
      "Iteration 33400 Training loss 0.051689162850379944 Validation loss 0.04892893508076668 Accuracy 0.8701250553131104\n",
      "Iteration 33410 Training loss 0.04969193786382675 Validation loss 0.049326688051223755 Accuracy 0.8672500252723694\n",
      "Iteration 33420 Training loss 0.05406804010272026 Validation loss 0.04879559949040413 Accuracy 0.8721250295639038\n",
      "Iteration 33430 Training loss 0.0510094054043293 Validation loss 0.04948262870311737 Accuracy 0.8713750243186951\n",
      "Iteration 33440 Training loss 0.04110186547040939 Validation loss 0.04878045246005058 Accuracy 0.8721250295639038\n",
      "Iteration 33450 Training loss 0.039672065526247025 Validation loss 0.04976717755198479 Accuracy 0.8663750290870667\n",
      "Iteration 33460 Training loss 0.04534490779042244 Validation loss 0.04878830537199974 Accuracy 0.8726250529289246\n",
      "Iteration 33470 Training loss 0.04322631657123566 Validation loss 0.04878642410039902 Accuracy 0.8713750243186951\n",
      "Iteration 33480 Training loss 0.04881640151143074 Validation loss 0.04877045750617981 Accuracy 0.8718750476837158\n",
      "Iteration 33490 Training loss 0.050746120512485504 Validation loss 0.04875612631440163 Accuracy 0.8725000619888306\n",
      "Iteration 33500 Training loss 0.05227968096733093 Validation loss 0.04882150515913963 Accuracy 0.8706250190734863\n",
      "Iteration 33510 Training loss 0.04849814251065254 Validation loss 0.048714231699705124 Accuracy 0.8723750710487366\n",
      "Iteration 33520 Training loss 0.04138942062854767 Validation loss 0.04873514175415039 Accuracy 0.8728750348091125\n",
      "Iteration 33530 Training loss 0.03745577484369278 Validation loss 0.04875338822603226 Accuracy 0.8708750605583191\n",
      "Iteration 33540 Training loss 0.051061466336250305 Validation loss 0.049395617097616196 Accuracy 0.8696250319480896\n",
      "Iteration 33550 Training loss 0.03811971843242645 Validation loss 0.049348361790180206 Accuracy 0.8700000643730164\n",
      "Iteration 33560 Training loss 0.04667382314801216 Validation loss 0.04876984655857086 Accuracy 0.8723750710487366\n",
      "Iteration 33570 Training loss 0.04878357797861099 Validation loss 0.04901451617479324 Accuracy 0.8706250190734863\n",
      "Iteration 33580 Training loss 0.04453303664922714 Validation loss 0.04874609783291817 Accuracy 0.8723750710487366\n",
      "Iteration 33590 Training loss 0.04819324240088463 Validation loss 0.04881894215941429 Accuracy 0.8721250295639038\n",
      "Iteration 33600 Training loss 0.052648548036813736 Validation loss 0.04877243936061859 Accuracy 0.8722500205039978\n",
      "Iteration 33610 Training loss 0.037671659141778946 Validation loss 0.048756130039691925 Accuracy 0.8715000152587891\n",
      "Iteration 33620 Training loss 0.04625605046749115 Validation loss 0.04923178255558014 Accuracy 0.8708750605583191\n",
      "Iteration 33630 Training loss 0.04406481608748436 Validation loss 0.04870268329977989 Accuracy 0.8727500438690186\n",
      "Iteration 33640 Training loss 0.044048357754945755 Validation loss 0.04884755238890648 Accuracy 0.8706250190734863\n",
      "Iteration 33650 Training loss 0.04108479246497154 Validation loss 0.04894118756055832 Accuracy 0.8723750710487366\n",
      "Iteration 33660 Training loss 0.04472685232758522 Validation loss 0.04879426211118698 Accuracy 0.8692500591278076\n",
      "Iteration 33670 Training loss 0.04546770080924034 Validation loss 0.04956910014152527 Accuracy 0.8701250553131104\n",
      "Iteration 33680 Training loss 0.04191751405596733 Validation loss 0.04868645966053009 Accuracy 0.8726250529289246\n",
      "Iteration 33690 Training loss 0.04141481965780258 Validation loss 0.04960353299975395 Accuracy 0.8701250553131104\n",
      "Iteration 33700 Training loss 0.04858298972249031 Validation loss 0.048719700425863266 Accuracy 0.8728750348091125\n",
      "Iteration 33710 Training loss 0.055819071829319 Validation loss 0.04933660477399826 Accuracy 0.8710000514984131\n",
      "Iteration 33720 Training loss 0.043378766626119614 Validation loss 0.048982713371515274 Accuracy 0.8723750710487366\n",
      "Iteration 33730 Training loss 0.04604965075850487 Validation loss 0.04869328439235687 Accuracy 0.8720000386238098\n",
      "Iteration 33740 Training loss 0.04751555249094963 Validation loss 0.04950416460633278 Accuracy 0.8691250681877136\n",
      "Iteration 33750 Training loss 0.04846055060625076 Validation loss 0.049015406519174576 Accuracy 0.8723750710487366\n",
      "Iteration 33760 Training loss 0.045033980160951614 Validation loss 0.048690877854824066 Accuracy 0.8725000619888306\n",
      "Iteration 33770 Training loss 0.04152379930019379 Validation loss 0.04878227412700653 Accuracy 0.8730000257492065\n",
      "Iteration 33780 Training loss 0.049712855368852615 Validation loss 0.04873711243271828 Accuracy 0.8726250529289246\n",
      "Iteration 33790 Training loss 0.042836859822273254 Validation loss 0.04920095205307007 Accuracy 0.8721250295639038\n",
      "Iteration 33800 Training loss 0.04257657378911972 Validation loss 0.048853132873773575 Accuracy 0.8728750348091125\n",
      "Iteration 33810 Training loss 0.04148345813155174 Validation loss 0.04872617498040199 Accuracy 0.8732500672340393\n",
      "Iteration 33820 Training loss 0.03679841384291649 Validation loss 0.048662446439266205 Accuracy 0.8721250295639038\n",
      "Iteration 33830 Training loss 0.04372360184788704 Validation loss 0.048784948885440826 Accuracy 0.8705000281333923\n",
      "Iteration 33840 Training loss 0.04170632362365723 Validation loss 0.048667605966329575 Accuracy 0.8722500205039978\n",
      "Iteration 33850 Training loss 0.04184798523783684 Validation loss 0.04865478724241257 Accuracy 0.8727500438690186\n",
      "Iteration 33860 Training loss 0.048985280096530914 Validation loss 0.04865667596459389 Accuracy 0.8718750476837158\n",
      "Iteration 33870 Training loss 0.05091526731848717 Validation loss 0.05010475218296051 Accuracy 0.8681250214576721\n",
      "Iteration 33880 Training loss 0.03653515875339508 Validation loss 0.048928312957286835 Accuracy 0.8716250658035278\n",
      "Iteration 33890 Training loss 0.05210365355014801 Validation loss 0.049197085201740265 Accuracy 0.8680000305175781\n",
      "Iteration 33900 Training loss 0.04345858842134476 Validation loss 0.04863642528653145 Accuracy 0.8720000386238098\n",
      "Iteration 33910 Training loss 0.051052626222372055 Validation loss 0.048771344125270844 Accuracy 0.8731250166893005\n",
      "Iteration 33920 Training loss 0.053251102566719055 Validation loss 0.0486554279923439 Accuracy 0.8716250658035278\n",
      "Iteration 33930 Training loss 0.04018731042742729 Validation loss 0.04924910143017769 Accuracy 0.8706250190734863\n",
      "Iteration 33940 Training loss 0.04507288709282875 Validation loss 0.04881511256098747 Accuracy 0.8723750710487366\n",
      "Iteration 33950 Training loss 0.046080078929662704 Validation loss 0.049256425350904465 Accuracy 0.8702500462532043\n",
      "Iteration 33960 Training loss 0.04261961951851845 Validation loss 0.04874607175588608 Accuracy 0.8711250424385071\n",
      "Iteration 33970 Training loss 0.05076243355870247 Validation loss 0.04868470877408981 Accuracy 0.8716250658035278\n",
      "Iteration 33980 Training loss 0.04367862269282341 Validation loss 0.048701196908950806 Accuracy 0.8717500567436218\n",
      "Iteration 33990 Training loss 0.04888061806559563 Validation loss 0.04871644079685211 Accuracy 0.8716250658035278\n",
      "Iteration 34000 Training loss 0.04719424992799759 Validation loss 0.04871759191155434 Accuracy 0.8715000152587891\n",
      "Iteration 34010 Training loss 0.043347716331481934 Validation loss 0.04884854704141617 Accuracy 0.8691250681877136\n",
      "Iteration 34020 Training loss 0.04529215767979622 Validation loss 0.04920302703976631 Accuracy 0.8712500333786011\n",
      "Iteration 34030 Training loss 0.046649713069200516 Validation loss 0.049336694180965424 Accuracy 0.8718750476837158\n",
      "Iteration 34040 Training loss 0.05309046804904938 Validation loss 0.04864763095974922 Accuracy 0.8715000152587891\n",
      "Iteration 34050 Training loss 0.047231778502464294 Validation loss 0.04883913695812225 Accuracy 0.8723750710487366\n",
      "Iteration 34060 Training loss 0.0506853312253952 Validation loss 0.04901178926229477 Accuracy 0.8686250448226929\n",
      "Iteration 34070 Training loss 0.04299900308251381 Validation loss 0.04862276464700699 Accuracy 0.8702500462532043\n",
      "Iteration 34080 Training loss 0.044628772884607315 Validation loss 0.04869098216295242 Accuracy 0.8711250424385071\n",
      "Iteration 34090 Training loss 0.04557601734995842 Validation loss 0.04868590086698532 Accuracy 0.8713750243186951\n",
      "Iteration 34100 Training loss 0.035404570400714874 Validation loss 0.048753298819065094 Accuracy 0.8732500672340393\n",
      "Iteration 34110 Training loss 0.04658612981438637 Validation loss 0.04887253791093826 Accuracy 0.8691250681877136\n",
      "Iteration 34120 Training loss 0.042558372020721436 Validation loss 0.04884391650557518 Accuracy 0.8718750476837158\n",
      "Iteration 34130 Training loss 0.04395231604576111 Validation loss 0.04859369993209839 Accuracy 0.8712500333786011\n",
      "Iteration 34140 Training loss 0.04900356009602547 Validation loss 0.04860265552997589 Accuracy 0.8715000152587891\n",
      "Iteration 34150 Training loss 0.050689201802015305 Validation loss 0.04860911890864372 Accuracy 0.8722500205039978\n",
      "Iteration 34160 Training loss 0.038859523832798004 Validation loss 0.04862538352608681 Accuracy 0.8716250658035278\n",
      "Iteration 34170 Training loss 0.044400881975889206 Validation loss 0.04884718731045723 Accuracy 0.8726250529289246\n",
      "Iteration 34180 Training loss 0.03352133557200432 Validation loss 0.04869496449828148 Accuracy 0.8708750605583191\n",
      "Iteration 34190 Training loss 0.050962552428245544 Validation loss 0.048558786511421204 Accuracy 0.8730000257492065\n",
      "Iteration 34200 Training loss 0.05036255717277527 Validation loss 0.04876153543591499 Accuracy 0.8718750476837158\n",
      "Iteration 34210 Training loss 0.03971722349524498 Validation loss 0.04856906458735466 Accuracy 0.8718750476837158\n",
      "Iteration 34220 Training loss 0.043287504464387894 Validation loss 0.04860835149884224 Accuracy 0.8713750243186951\n",
      "Iteration 34230 Training loss 0.044370636343955994 Validation loss 0.04875731095671654 Accuracy 0.8696250319480896\n",
      "Iteration 34240 Training loss 0.03982006385922432 Validation loss 0.04865699261426926 Accuracy 0.8707500696182251\n",
      "Iteration 34250 Training loss 0.04226244240999222 Validation loss 0.048644281923770905 Accuracy 0.8736250400543213\n",
      "Iteration 34260 Training loss 0.041702527552843094 Validation loss 0.0487433485686779 Accuracy 0.8726250529289246\n",
      "Iteration 34270 Training loss 0.04763789102435112 Validation loss 0.048824869096279144 Accuracy 0.8728750348091125\n",
      "Iteration 34280 Training loss 0.0419604629278183 Validation loss 0.04871218651533127 Accuracy 0.8725000619888306\n",
      "Iteration 34290 Training loss 0.04404473677277565 Validation loss 0.04861980676651001 Accuracy 0.8716250658035278\n",
      "Iteration 34300 Training loss 0.045239705592393875 Validation loss 0.048630405217409134 Accuracy 0.8717500567436218\n",
      "Iteration 34310 Training loss 0.04689165949821472 Validation loss 0.04861835017800331 Accuracy 0.8715000152587891\n",
      "Iteration 34320 Training loss 0.04563407972455025 Validation loss 0.04908749461174011 Accuracy 0.8675000667572021\n",
      "Iteration 34330 Training loss 0.044654734432697296 Validation loss 0.04864373803138733 Accuracy 0.8721250295639038\n",
      "Iteration 34340 Training loss 0.04728727415204048 Validation loss 0.04859491437673569 Accuracy 0.8718750476837158\n",
      "Iteration 34350 Training loss 0.04188638553023338 Validation loss 0.04883386939764023 Accuracy 0.8695000410079956\n",
      "Iteration 34360 Training loss 0.045966871082782745 Validation loss 0.048662129789590836 Accuracy 0.8715000152587891\n",
      "Iteration 34370 Training loss 0.04212773218750954 Validation loss 0.04851877689361572 Accuracy 0.8728750348091125\n",
      "Iteration 34380 Training loss 0.047785766422748566 Validation loss 0.048659708350896835 Accuracy 0.8712500333786011\n",
      "Iteration 34390 Training loss 0.04256588965654373 Validation loss 0.05060041323304176 Accuracy 0.8627500534057617\n",
      "Iteration 34400 Training loss 0.04498966038227081 Validation loss 0.04853155463933945 Accuracy 0.8716250658035278\n",
      "Iteration 34410 Training loss 0.05429946631193161 Validation loss 0.048485126346349716 Accuracy 0.8712500333786011\n",
      "Iteration 34420 Training loss 0.046395719051361084 Validation loss 0.04851019009947777 Accuracy 0.8718750476837158\n",
      "Iteration 34430 Training loss 0.04350120574235916 Validation loss 0.04871268570423126 Accuracy 0.8718750476837158\n",
      "Iteration 34440 Training loss 0.046450477093458176 Validation loss 0.048479046672582626 Accuracy 0.8717500567436218\n",
      "Iteration 34450 Training loss 0.052976761013269424 Validation loss 0.04894234240055084 Accuracy 0.8682500123977661\n",
      "Iteration 34460 Training loss 0.034171316772699356 Validation loss 0.04850928112864494 Accuracy 0.8721250295639038\n",
      "Iteration 34470 Training loss 0.04133063182234764 Validation loss 0.048568956553936005 Accuracy 0.8732500672340393\n",
      "Iteration 34480 Training loss 0.0415569469332695 Validation loss 0.04855223745107651 Accuracy 0.8720000386238098\n",
      "Iteration 34490 Training loss 0.045321960002183914 Validation loss 0.04861827194690704 Accuracy 0.8723750710487366\n",
      "Iteration 34500 Training loss 0.051033057272434235 Validation loss 0.048511914908885956 Accuracy 0.8716250658035278\n",
      "Iteration 34510 Training loss 0.05001489818096161 Validation loss 0.04848726838827133 Accuracy 0.8722500205039978\n",
      "Iteration 34520 Training loss 0.046596042811870575 Validation loss 0.04884970188140869 Accuracy 0.8717500567436218\n",
      "Iteration 34530 Training loss 0.04492779076099396 Validation loss 0.04864075407385826 Accuracy 0.8728750348091125\n",
      "Iteration 34540 Training loss 0.037458695471286774 Validation loss 0.04880482330918312 Accuracy 0.8728750348091125\n",
      "Iteration 34550 Training loss 0.04808177053928375 Validation loss 0.048530999571084976 Accuracy 0.8732500672340393\n",
      "Iteration 34560 Training loss 0.040457285940647125 Validation loss 0.04850217327475548 Accuracy 0.8720000386238098\n",
      "Iteration 34570 Training loss 0.04533444344997406 Validation loss 0.048479821532964706 Accuracy 0.8730000257492065\n",
      "Iteration 34580 Training loss 0.03688017651438713 Validation loss 0.048747073858976364 Accuracy 0.8728750348091125\n",
      "Iteration 34590 Training loss 0.04293128103017807 Validation loss 0.04847768694162369 Accuracy 0.8725000619888306\n",
      "Iteration 34600 Training loss 0.03908933699131012 Validation loss 0.048616185784339905 Accuracy 0.8712500333786011\n",
      "Iteration 34610 Training loss 0.043503254652023315 Validation loss 0.04850982502102852 Accuracy 0.8728750348091125\n",
      "Iteration 34620 Training loss 0.043491773307323456 Validation loss 0.048851508647203445 Accuracy 0.8688750267028809\n",
      "Iteration 34630 Training loss 0.047574520111083984 Validation loss 0.048471033573150635 Accuracy 0.8725000619888306\n",
      "Iteration 34640 Training loss 0.04642808809876442 Validation loss 0.048479169607162476 Accuracy 0.8727500438690186\n",
      "Iteration 34650 Training loss 0.0439237542450428 Validation loss 0.04848159849643707 Accuracy 0.8721250295639038\n",
      "Iteration 34660 Training loss 0.043851662427186966 Validation loss 0.048480987548828125 Accuracy 0.874250054359436\n",
      "Iteration 34670 Training loss 0.04202040657401085 Validation loss 0.048633843660354614 Accuracy 0.8728750348091125\n",
      "Iteration 34680 Training loss 0.05146924778819084 Validation loss 0.04896249249577522 Accuracy 0.8731250166893005\n",
      "Iteration 34690 Training loss 0.04903515800833702 Validation loss 0.04852453991770744 Accuracy 0.8731250166893005\n",
      "Iteration 34700 Training loss 0.042672932147979736 Validation loss 0.048490218818187714 Accuracy 0.8720000386238098\n",
      "Iteration 34710 Training loss 0.04285191372036934 Validation loss 0.04852686822414398 Accuracy 0.8737500309944153\n",
      "Iteration 34720 Training loss 0.04265850782394409 Validation loss 0.04844294860959053 Accuracy 0.8728750348091125\n",
      "Iteration 34730 Training loss 0.043652743101119995 Validation loss 0.048455122858285904 Accuracy 0.8731250166893005\n",
      "Iteration 34740 Training loss 0.04189109802246094 Validation loss 0.048753321170806885 Accuracy 0.8727500438690186\n",
      "Iteration 34750 Training loss 0.04134088754653931 Validation loss 0.048489343374967575 Accuracy 0.8720000386238098\n",
      "Iteration 34760 Training loss 0.04776914417743683 Validation loss 0.049220941960811615 Accuracy 0.8723750710487366\n",
      "Iteration 34770 Training loss 0.04360797628760338 Validation loss 0.04854602739214897 Accuracy 0.8711250424385071\n",
      "Iteration 34780 Training loss 0.04713486507534981 Validation loss 0.04974496364593506 Accuracy 0.8696250319480896\n",
      "Iteration 34790 Training loss 0.05352930352091789 Validation loss 0.048458244651556015 Accuracy 0.8722500205039978\n",
      "Iteration 34800 Training loss 0.04594655707478523 Validation loss 0.04843809828162193 Accuracy 0.8722500205039978\n",
      "Iteration 34810 Training loss 0.05309172347187996 Validation loss 0.04972830042243004 Accuracy 0.8701250553131104\n",
      "Iteration 34820 Training loss 0.04918039217591286 Validation loss 0.04835541546344757 Accuracy 0.8730000257492065\n",
      "Iteration 34830 Training loss 0.05450303480029106 Validation loss 0.048668283969163895 Accuracy 0.8737500309944153\n",
      "Iteration 34840 Training loss 0.04464014992117882 Validation loss 0.04836030304431915 Accuracy 0.8732500672340393\n",
      "Iteration 34850 Training loss 0.04228091984987259 Validation loss 0.04848857969045639 Accuracy 0.8738750219345093\n",
      "Iteration 34860 Training loss 0.04083189368247986 Validation loss 0.04835400730371475 Accuracy 0.8740000128746033\n",
      "Iteration 34870 Training loss 0.04710569232702255 Validation loss 0.04851846396923065 Accuracy 0.8731250166893005\n",
      "Iteration 34880 Training loss 0.042694032192230225 Validation loss 0.04838643595576286 Accuracy 0.8735000491142273\n",
      "Iteration 34890 Training loss 0.03697468340396881 Validation loss 0.04843728989362717 Accuracy 0.8732500672340393\n",
      "Iteration 34900 Training loss 0.04278995469212532 Validation loss 0.04839315637946129 Accuracy 0.874250054359436\n",
      "Iteration 34910 Training loss 0.04535401239991188 Validation loss 0.04851919040083885 Accuracy 0.8710000514984131\n",
      "Iteration 34920 Training loss 0.04192621260881424 Validation loss 0.04832955449819565 Accuracy 0.8730000257492065\n",
      "Iteration 34930 Training loss 0.04945982247591019 Validation loss 0.04875697195529938 Accuracy 0.8733750581741333\n",
      "Iteration 34940 Training loss 0.04842056334018707 Validation loss 0.04844946041703224 Accuracy 0.8738750219345093\n",
      "Iteration 34950 Training loss 0.04743234068155289 Validation loss 0.04833868891000748 Accuracy 0.8730000257492065\n",
      "Iteration 34960 Training loss 0.051940687000751495 Validation loss 0.04832989722490311 Accuracy 0.8728750348091125\n",
      "Iteration 34970 Training loss 0.05058100074529648 Validation loss 0.0483771450817585 Accuracy 0.8726250529289246\n",
      "Iteration 34980 Training loss 0.04609532654285431 Validation loss 0.048423875123262405 Accuracy 0.8715000152587891\n",
      "Iteration 34990 Training loss 0.04710612818598747 Validation loss 0.04837287962436676 Accuracy 0.8728750348091125\n",
      "Iteration 35000 Training loss 0.035876836627721786 Validation loss 0.04920302703976631 Accuracy 0.8723750710487366\n",
      "Iteration 35010 Training loss 0.04276444390416145 Validation loss 0.048943471163511276 Accuracy 0.8737500309944153\n",
      "Iteration 35020 Training loss 0.04810502380132675 Validation loss 0.048505496233701706 Accuracy 0.8711250424385071\n",
      "Iteration 35030 Training loss 0.03884308412671089 Validation loss 0.04840809106826782 Accuracy 0.8717500567436218\n",
      "Iteration 35040 Training loss 0.04733530804514885 Validation loss 0.04830553010106087 Accuracy 0.8731250166893005\n",
      "Iteration 35050 Training loss 0.04251492768526077 Validation loss 0.048432838171720505 Accuracy 0.8713750243186951\n",
      "Iteration 35060 Training loss 0.04625007137656212 Validation loss 0.04832521453499794 Accuracy 0.8728750348091125\n",
      "Iteration 35070 Training loss 0.04061213880777359 Validation loss 0.0483207032084465 Accuracy 0.8721250295639038\n",
      "Iteration 35080 Training loss 0.05400065332651138 Validation loss 0.049424998462200165 Accuracy 0.8715000152587891\n",
      "Iteration 35090 Training loss 0.04259761422872543 Validation loss 0.04837796092033386 Accuracy 0.8726250529289246\n",
      "Iteration 35100 Training loss 0.053131844848394394 Validation loss 0.048805829137563705 Accuracy 0.8737500309944153\n",
      "Iteration 35110 Training loss 0.04641616716980934 Validation loss 0.04856988415122032 Accuracy 0.8737500309944153\n",
      "Iteration 35120 Training loss 0.04374169185757637 Validation loss 0.04921144247055054 Accuracy 0.8730000257492065\n",
      "Iteration 35130 Training loss 0.051725950092077255 Validation loss 0.048558518290519714 Accuracy 0.874125063419342\n",
      "Iteration 35140 Training loss 0.04329530522227287 Validation loss 0.048279888927936554 Accuracy 0.8733750581741333\n",
      "Iteration 35150 Training loss 0.03614044561982155 Validation loss 0.04877530783414841 Accuracy 0.8687500357627869\n",
      "Iteration 35160 Training loss 0.0424480065703392 Validation loss 0.04841471463441849 Accuracy 0.8737500309944153\n",
      "Iteration 35170 Training loss 0.040717270225286484 Validation loss 0.04925493523478508 Accuracy 0.8723750710487366\n",
      "Iteration 35180 Training loss 0.03903651982545853 Validation loss 0.04829110950231552 Accuracy 0.8738750219345093\n",
      "Iteration 35190 Training loss 0.04347463324666023 Validation loss 0.04832448810338974 Accuracy 0.8740000128746033\n",
      "Iteration 35200 Training loss 0.041379936039447784 Validation loss 0.04825698956847191 Accuracy 0.8732500672340393\n",
      "Iteration 35210 Training loss 0.042843759059906006 Validation loss 0.04841116443276405 Accuracy 0.8715000152587891\n",
      "Iteration 35220 Training loss 0.04311937093734741 Validation loss 0.04836716875433922 Accuracy 0.8737500309944153\n",
      "Iteration 35230 Training loss 0.04840777441859245 Validation loss 0.04843924939632416 Accuracy 0.8728750348091125\n",
      "Iteration 35240 Training loss 0.046997539699077606 Validation loss 0.04838438704609871 Accuracy 0.8720000386238098\n",
      "Iteration 35250 Training loss 0.03932361677289009 Validation loss 0.04822716489434242 Accuracy 0.874125063419342\n",
      "Iteration 35260 Training loss 0.037463512271642685 Validation loss 0.048250921070575714 Accuracy 0.8738750219345093\n",
      "Iteration 35270 Training loss 0.043622493743896484 Validation loss 0.04828757047653198 Accuracy 0.8721250295639038\n",
      "Iteration 35280 Training loss 0.043728332966566086 Validation loss 0.04845649003982544 Accuracy 0.8725000619888306\n",
      "Iteration 35290 Training loss 0.04182305186986923 Validation loss 0.04829200729727745 Accuracy 0.8735000491142273\n",
      "Iteration 35300 Training loss 0.032692741602659225 Validation loss 0.04871708154678345 Accuracy 0.8701250553131104\n",
      "Iteration 35310 Training loss 0.0453779511153698 Validation loss 0.04833752661943436 Accuracy 0.8721250295639038\n",
      "Iteration 35320 Training loss 0.04681488871574402 Validation loss 0.04824979975819588 Accuracy 0.8736250400543213\n",
      "Iteration 35330 Training loss 0.046863969415426254 Validation loss 0.04864051192998886 Accuracy 0.8692500591278076\n",
      "Iteration 35340 Training loss 0.037463799118995667 Validation loss 0.04837585985660553 Accuracy 0.8733750581741333\n",
      "Iteration 35350 Training loss 0.05689852684736252 Validation loss 0.048262789845466614 Accuracy 0.8717500567436218\n",
      "Iteration 35360 Training loss 0.047736603766679764 Validation loss 0.04818335920572281 Accuracy 0.8732500672340393\n",
      "Iteration 35370 Training loss 0.04307398572564125 Validation loss 0.048233479261398315 Accuracy 0.8723750710487366\n",
      "Iteration 35380 Training loss 0.0388752743601799 Validation loss 0.0482928492128849 Accuracy 0.8736250400543213\n",
      "Iteration 35390 Training loss 0.04117245599627495 Validation loss 0.048199594020843506 Accuracy 0.8722500205039978\n",
      "Iteration 35400 Training loss 0.04825971648097038 Validation loss 0.04861224442720413 Accuracy 0.8730000257492065\n",
      "Iteration 35410 Training loss 0.044087089598178864 Validation loss 0.04819956421852112 Accuracy 0.8727500438690186\n",
      "Iteration 35420 Training loss 0.04567062109708786 Validation loss 0.04843854904174805 Accuracy 0.8732500672340393\n",
      "Iteration 35430 Training loss 0.047890499234199524 Validation loss 0.04816827550530434 Accuracy 0.8731250166893005\n",
      "Iteration 35440 Training loss 0.04440813139081001 Validation loss 0.04836270213127136 Accuracy 0.8731250166893005\n",
      "Iteration 35450 Training loss 0.04946878179907799 Validation loss 0.04826214164495468 Accuracy 0.8740000128746033\n",
      "Iteration 35460 Training loss 0.05012302100658417 Validation loss 0.04859634116292 Accuracy 0.8732500672340393\n",
      "Iteration 35470 Training loss 0.04294518381357193 Validation loss 0.04840471222996712 Accuracy 0.8717500567436218\n",
      "Iteration 35480 Training loss 0.05063575506210327 Validation loss 0.04838944599032402 Accuracy 0.8730000257492065\n",
      "Iteration 35490 Training loss 0.0459345243871212 Validation loss 0.04817604646086693 Accuracy 0.8740000128746033\n",
      "Iteration 35500 Training loss 0.04436517506837845 Validation loss 0.048227302730083466 Accuracy 0.874250054359436\n",
      "Iteration 35510 Training loss 0.043507665395736694 Validation loss 0.04820416495203972 Accuracy 0.8735000491142273\n",
      "Iteration 35520 Training loss 0.0381958894431591 Validation loss 0.048231080174446106 Accuracy 0.8740000128746033\n",
      "Iteration 35530 Training loss 0.05589132755994797 Validation loss 0.048565879464149475 Accuracy 0.874250054359436\n",
      "Iteration 35540 Training loss 0.05025840550661087 Validation loss 0.04824608191847801 Accuracy 0.8731250166893005\n",
      "Iteration 35550 Training loss 0.04661703109741211 Validation loss 0.048602160066366196 Accuracy 0.8740000128746033\n",
      "Iteration 35560 Training loss 0.040597692131996155 Validation loss 0.048257749527692795 Accuracy 0.8725000619888306\n",
      "Iteration 35570 Training loss 0.04799085110425949 Validation loss 0.0483759306371212 Accuracy 0.8735000491142273\n",
      "Iteration 35580 Training loss 0.032020583748817444 Validation loss 0.04840175062417984 Accuracy 0.8736250400543213\n",
      "Iteration 35590 Training loss 0.04074554517865181 Validation loss 0.04821149632334709 Accuracy 0.8730000257492065\n",
      "Iteration 35600 Training loss 0.050075359642505646 Validation loss 0.048652805387973785 Accuracy 0.8686250448226929\n",
      "Iteration 35610 Training loss 0.0400780625641346 Validation loss 0.048174869269132614 Accuracy 0.8731250166893005\n",
      "Iteration 35620 Training loss 0.05233879014849663 Validation loss 0.04822228476405144 Accuracy 0.8725000619888306\n",
      "Iteration 35630 Training loss 0.036872748285532 Validation loss 0.04813951626420021 Accuracy 0.8740000128746033\n",
      "Iteration 35640 Training loss 0.04738827049732208 Validation loss 0.04857344925403595 Accuracy 0.8708750605583191\n",
      "Iteration 35650 Training loss 0.04695451632142067 Validation loss 0.0481996163725853 Accuracy 0.874625027179718\n",
      "Iteration 35660 Training loss 0.046023719012737274 Validation loss 0.04850737005472183 Accuracy 0.8712500333786011\n",
      "Iteration 35670 Training loss 0.05086031183600426 Validation loss 0.048243213444948196 Accuracy 0.8726250529289246\n",
      "Iteration 35680 Training loss 0.040440578013658524 Validation loss 0.04814369976520538 Accuracy 0.8738750219345093\n",
      "Iteration 35690 Training loss 0.045382242649793625 Validation loss 0.04827330261468887 Accuracy 0.8732500672340393\n",
      "Iteration 35700 Training loss 0.04574938118457794 Validation loss 0.048474010080099106 Accuracy 0.8700000643730164\n",
      "Iteration 35710 Training loss 0.04013654589653015 Validation loss 0.048122555017471313 Accuracy 0.8732500672340393\n",
      "Iteration 35720 Training loss 0.05155225470662117 Validation loss 0.04815342277288437 Accuracy 0.874125063419342\n",
      "Iteration 35730 Training loss 0.05050908029079437 Validation loss 0.04863811284303665 Accuracy 0.8692500591278076\n",
      "Iteration 35740 Training loss 0.044751692563295364 Validation loss 0.0480462983250618 Accuracy 0.874125063419342\n",
      "Iteration 35750 Training loss 0.04194003343582153 Validation loss 0.04810108616948128 Accuracy 0.874125063419342\n",
      "Iteration 35760 Training loss 0.034551337361335754 Validation loss 0.048061590641736984 Accuracy 0.874750018119812\n",
      "Iteration 35770 Training loss 0.04313346743583679 Validation loss 0.0481022372841835 Accuracy 0.8727500438690186\n",
      "Iteration 35780 Training loss 0.04740336909890175 Validation loss 0.048241883516311646 Accuracy 0.874250054359436\n",
      "Iteration 35790 Training loss 0.04896637797355652 Validation loss 0.04868273437023163 Accuracy 0.8758750557899475\n",
      "Iteration 35800 Training loss 0.044235486537218094 Validation loss 0.04861655831336975 Accuracy 0.874750018119812\n",
      "Iteration 35810 Training loss 0.0422406904399395 Validation loss 0.04815289378166199 Accuracy 0.8740000128746033\n",
      "Iteration 35820 Training loss 0.04634946584701538 Validation loss 0.04846354201436043 Accuracy 0.8696250319480896\n",
      "Iteration 35830 Training loss 0.04687714949250221 Validation loss 0.048132821917533875 Accuracy 0.87437504529953\n",
      "Iteration 35840 Training loss 0.04525305703282356 Validation loss 0.0482737235724926 Accuracy 0.8706250190734863\n",
      "Iteration 35850 Training loss 0.03953256830573082 Validation loss 0.048297811299562454 Accuracy 0.8703750371932983\n",
      "Iteration 35860 Training loss 0.04261576384305954 Validation loss 0.048216018825769424 Accuracy 0.874250054359436\n",
      "Iteration 35870 Training loss 0.04094887524843216 Validation loss 0.048163384199142456 Accuracy 0.8736250400543213\n",
      "Iteration 35880 Training loss 0.043808963149785995 Validation loss 0.048129819333553314 Accuracy 0.8732500672340393\n",
      "Iteration 35890 Training loss 0.04493892565369606 Validation loss 0.048133641481399536 Accuracy 0.8736250400543213\n",
      "Iteration 35900 Training loss 0.037597425282001495 Validation loss 0.04823144152760506 Accuracy 0.8756250143051147\n",
      "Iteration 35910 Training loss 0.043902553617954254 Validation loss 0.0483408086001873 Accuracy 0.8716250658035278\n",
      "Iteration 35920 Training loss 0.054286591708660126 Validation loss 0.04840845614671707 Accuracy 0.874625027179718\n",
      "Iteration 35930 Training loss 0.0469573549926281 Validation loss 0.048607610166072845 Accuracy 0.874750018119812\n",
      "Iteration 35940 Training loss 0.0546896830201149 Validation loss 0.04804420843720436 Accuracy 0.87437504529953\n",
      "Iteration 35950 Training loss 0.04283000901341438 Validation loss 0.04810322821140289 Accuracy 0.8736250400543213\n",
      "Iteration 35960 Training loss 0.04179263859987259 Validation loss 0.04941849038004875 Accuracy 0.8716250658035278\n",
      "Iteration 35970 Training loss 0.03791940212249756 Validation loss 0.04815281182527542 Accuracy 0.8726250529289246\n",
      "Iteration 35980 Training loss 0.03953007981181145 Validation loss 0.048068538308143616 Accuracy 0.874125063419342\n",
      "Iteration 35990 Training loss 0.045561302453279495 Validation loss 0.04804759472608566 Accuracy 0.874250054359436\n",
      "Iteration 36000 Training loss 0.04885035380721092 Validation loss 0.04864615201950073 Accuracy 0.8737500309944153\n",
      "Iteration 36010 Training loss 0.03788397088646889 Validation loss 0.04813527315855026 Accuracy 0.8727500438690186\n",
      "Iteration 36020 Training loss 0.048572372645139694 Validation loss 0.04810795933008194 Accuracy 0.8718750476837158\n",
      "Iteration 36030 Training loss 0.03797416761517525 Validation loss 0.048073943704366684 Accuracy 0.8733750581741333\n",
      "Iteration 36040 Training loss 0.04717544466257095 Validation loss 0.04811432957649231 Accuracy 0.8733750581741333\n",
      "Iteration 36050 Training loss 0.03610476478934288 Validation loss 0.04829884693026543 Accuracy 0.8710000514984131\n",
      "Iteration 36060 Training loss 0.04211254045367241 Validation loss 0.04862223193049431 Accuracy 0.874125063419342\n",
      "Iteration 36070 Training loss 0.046477556228637695 Validation loss 0.04824460670351982 Accuracy 0.8751250505447388\n",
      "Iteration 36080 Training loss 0.05210115388035774 Validation loss 0.048157285898923874 Accuracy 0.8757500648498535\n",
      "Iteration 36090 Training loss 0.0497773177921772 Validation loss 0.04804525524377823 Accuracy 0.8725000619888306\n",
      "Iteration 36100 Training loss 0.043918807059526443 Validation loss 0.04819450527429581 Accuracy 0.8740000128746033\n",
      "Iteration 36110 Training loss 0.045216891914606094 Validation loss 0.04816116392612457 Accuracy 0.8726250529289246\n",
      "Iteration 36120 Training loss 0.04474280774593353 Validation loss 0.04882451891899109 Accuracy 0.8730000257492065\n",
      "Iteration 36130 Training loss 0.04666777327656746 Validation loss 0.048057373613119125 Accuracy 0.874250054359436\n",
      "Iteration 36140 Training loss 0.04020005092024803 Validation loss 0.04809951409697533 Accuracy 0.8738750219345093\n",
      "Iteration 36150 Training loss 0.04028327390551567 Validation loss 0.04803873598575592 Accuracy 0.8755000233650208\n",
      "Iteration 36160 Training loss 0.0476202629506588 Validation loss 0.04804325848817825 Accuracy 0.874625027179718\n",
      "Iteration 36170 Training loss 0.04439561814069748 Validation loss 0.04812920093536377 Accuracy 0.874500036239624\n",
      "Iteration 36180 Training loss 0.03646238148212433 Validation loss 0.04819754138588905 Accuracy 0.8723750710487366\n",
      "Iteration 36190 Training loss 0.03921915963292122 Validation loss 0.04828713461756706 Accuracy 0.8718750476837158\n",
      "Iteration 36200 Training loss 0.041844237595796585 Validation loss 0.0480615608394146 Accuracy 0.8733750581741333\n",
      "Iteration 36210 Training loss 0.051619499921798706 Validation loss 0.048219479620456696 Accuracy 0.8718750476837158\n",
      "Iteration 36220 Training loss 0.05151677504181862 Validation loss 0.048379942774772644 Accuracy 0.874625027179718\n",
      "Iteration 36230 Training loss 0.05003225803375244 Validation loss 0.0484333299100399 Accuracy 0.8701250553131104\n",
      "Iteration 36240 Training loss 0.047411464154720306 Validation loss 0.049470994621515274 Accuracy 0.8651250600814819\n",
      "Iteration 36250 Training loss 0.040337055921554565 Validation loss 0.04795905202627182 Accuracy 0.874625027179718\n",
      "Iteration 36260 Training loss 0.038573674857616425 Validation loss 0.048169028013944626 Accuracy 0.8750000596046448\n",
      "Iteration 36270 Training loss 0.04704848304390907 Validation loss 0.04849569872021675 Accuracy 0.8697500228881836\n",
      "Iteration 36280 Training loss 0.039513859897851944 Validation loss 0.04796183481812477 Accuracy 0.8736250400543213\n",
      "Iteration 36290 Training loss 0.044970978051424026 Validation loss 0.048521559685468674 Accuracy 0.8686250448226929\n",
      "Iteration 36300 Training loss 0.04321473091840744 Validation loss 0.04811881110072136 Accuracy 0.8720000386238098\n",
      "Iteration 36310 Training loss 0.05131510645151138 Validation loss 0.04807395115494728 Accuracy 0.8728750348091125\n",
      "Iteration 36320 Training loss 0.038160186260938644 Validation loss 0.04870471730828285 Accuracy 0.87437504529953\n",
      "Iteration 36330 Training loss 0.04188952222466469 Validation loss 0.04795095697045326 Accuracy 0.8751250505447388\n",
      "Iteration 36340 Training loss 0.04089302569627762 Validation loss 0.048000916838645935 Accuracy 0.8751250505447388\n",
      "Iteration 36350 Training loss 0.04801357164978981 Validation loss 0.04821775481104851 Accuracy 0.8751250505447388\n",
      "Iteration 36360 Training loss 0.042941369116306305 Validation loss 0.04801298305392265 Accuracy 0.8727500438690186\n",
      "Iteration 36370 Training loss 0.04078907519578934 Validation loss 0.04795489087700844 Accuracy 0.8750000596046448\n",
      "Iteration 36380 Training loss 0.046044740825891495 Validation loss 0.048249825835227966 Accuracy 0.8710000514984131\n",
      "Iteration 36390 Training loss 0.048701465129852295 Validation loss 0.047954075038433075 Accuracy 0.8733750581741333\n",
      "Iteration 36400 Training loss 0.04275359958410263 Validation loss 0.0481126643717289 Accuracy 0.8750000596046448\n",
      "Iteration 36410 Training loss 0.04636773094534874 Validation loss 0.048100199550390244 Accuracy 0.8725000619888306\n",
      "Iteration 36420 Training loss 0.0398377887904644 Validation loss 0.04799484461545944 Accuracy 0.8726250529289246\n",
      "Iteration 36430 Training loss 0.04800482466816902 Validation loss 0.04828023165464401 Accuracy 0.8711250424385071\n",
      "Iteration 36440 Training loss 0.03990209102630615 Validation loss 0.04797022417187691 Accuracy 0.8732500672340393\n",
      "Iteration 36450 Training loss 0.053954146802425385 Validation loss 0.047975990921258926 Accuracy 0.8732500672340393\n",
      "Iteration 36460 Training loss 0.03476230427622795 Validation loss 0.048683930188417435 Accuracy 0.8692500591278076\n",
      "Iteration 36470 Training loss 0.04577895998954773 Validation loss 0.04841925576329231 Accuracy 0.8702500462532043\n",
      "Iteration 36480 Training loss 0.048498377203941345 Validation loss 0.04795289784669876 Accuracy 0.8732500672340393\n",
      "Iteration 36490 Training loss 0.04482660070061684 Validation loss 0.048929911106824875 Accuracy 0.8738750219345093\n",
      "Iteration 36500 Training loss 0.042653921991586685 Validation loss 0.0481736920773983 Accuracy 0.874500036239624\n",
      "Iteration 36510 Training loss 0.05002773180603981 Validation loss 0.04802972823381424 Accuracy 0.8748750686645508\n",
      "Iteration 36520 Training loss 0.04176412522792816 Validation loss 0.0480104424059391 Accuracy 0.8728750348091125\n",
      "Iteration 36530 Training loss 0.0435965433716774 Validation loss 0.04792100936174393 Accuracy 0.8735000491142273\n",
      "Iteration 36540 Training loss 0.0442081019282341 Validation loss 0.04804059863090515 Accuracy 0.8722500205039978\n",
      "Iteration 36550 Training loss 0.04644004628062248 Validation loss 0.04788675904273987 Accuracy 0.874250054359436\n",
      "Iteration 36560 Training loss 0.04841810092329979 Validation loss 0.047977082431316376 Accuracy 0.8727500438690186\n",
      "Iteration 36570 Training loss 0.04130024462938309 Validation loss 0.04791263863444328 Accuracy 0.8736250400543213\n",
      "Iteration 36580 Training loss 0.04587097838521004 Validation loss 0.04788484424352646 Accuracy 0.874250054359436\n",
      "Iteration 36590 Training loss 0.03580455854535103 Validation loss 0.04800458997488022 Accuracy 0.8752500414848328\n",
      "Iteration 36600 Training loss 0.05069040134549141 Validation loss 0.048061054199934006 Accuracy 0.8725000619888306\n",
      "Iteration 36610 Training loss 0.04550553113222122 Validation loss 0.04899794980883598 Accuracy 0.8681250214576721\n",
      "Iteration 36620 Training loss 0.04060100018978119 Validation loss 0.04817988723516464 Accuracy 0.8722500205039978\n",
      "Iteration 36630 Training loss 0.03889157623052597 Validation loss 0.047986820340156555 Accuracy 0.8727500438690186\n",
      "Iteration 36640 Training loss 0.04306598752737045 Validation loss 0.04813261330127716 Accuracy 0.8728750348091125\n",
      "Iteration 36650 Training loss 0.04054315388202667 Validation loss 0.04790177196264267 Accuracy 0.8732500672340393\n",
      "Iteration 36660 Training loss 0.04681622236967087 Validation loss 0.0484081506729126 Accuracy 0.8756250143051147\n",
      "Iteration 36670 Training loss 0.03833156079053879 Validation loss 0.04792480543255806 Accuracy 0.874500036239624\n",
      "Iteration 36680 Training loss 0.044933442026376724 Validation loss 0.04803432896733284 Accuracy 0.8755000233650208\n",
      "Iteration 36690 Training loss 0.0407090000808239 Validation loss 0.04830685630440712 Accuracy 0.8755000233650208\n",
      "Iteration 36700 Training loss 0.044710416346788406 Validation loss 0.048059601336717606 Accuracy 0.8717500567436218\n",
      "Iteration 36710 Training loss 0.04636510834097862 Validation loss 0.048150040209293365 Accuracy 0.8727500438690186\n",
      "Iteration 36720 Training loss 0.04074995964765549 Validation loss 0.048019733279943466 Accuracy 0.8738750219345093\n",
      "Iteration 36730 Training loss 0.04951843246817589 Validation loss 0.04791073128581047 Accuracy 0.874125063419342\n",
      "Iteration 36740 Training loss 0.04278356581926346 Validation loss 0.047965601086616516 Accuracy 0.8732500672340393\n",
      "Iteration 36750 Training loss 0.043116550892591476 Validation loss 0.04790081828832626 Accuracy 0.8756250143051147\n",
      "Iteration 36760 Training loss 0.03974108770489693 Validation loss 0.04864704981446266 Accuracy 0.8686250448226929\n",
      "Iteration 36770 Training loss 0.03784204646945 Validation loss 0.04788100719451904 Accuracy 0.8753750324249268\n",
      "Iteration 36780 Training loss 0.04759841039776802 Validation loss 0.04786323755979538 Accuracy 0.8755000233650208\n",
      "Iteration 36790 Training loss 0.04202146455645561 Validation loss 0.04816356301307678 Accuracy 0.8711250424385071\n",
      "Iteration 36800 Training loss 0.04730570316314697 Validation loss 0.04824744910001755 Accuracy 0.87437504529953\n",
      "Iteration 36810 Training loss 0.03649463877081871 Validation loss 0.04790686443448067 Accuracy 0.8740000128746033\n",
      "Iteration 36820 Training loss 0.04464351385831833 Validation loss 0.04790967330336571 Accuracy 0.8738750219345093\n",
      "Iteration 36830 Training loss 0.041298069059848785 Validation loss 0.04794731363654137 Accuracy 0.8723750710487366\n",
      "Iteration 36840 Training loss 0.053433410823345184 Validation loss 0.04827747493982315 Accuracy 0.8750000596046448\n",
      "Iteration 36850 Training loss 0.03938277065753937 Validation loss 0.04783935472369194 Accuracy 0.874125063419342\n",
      "Iteration 36860 Training loss 0.03610670194029808 Validation loss 0.048046499490737915 Accuracy 0.8755000233650208\n",
      "Iteration 36870 Training loss 0.04425252974033356 Validation loss 0.04794381558895111 Accuracy 0.8752500414848328\n",
      "Iteration 36880 Training loss 0.05045582726597786 Validation loss 0.048324666917324066 Accuracy 0.8707500696182251\n",
      "Iteration 36890 Training loss 0.04458397626876831 Validation loss 0.048105355352163315 Accuracy 0.8760000467300415\n",
      "Iteration 36900 Training loss 0.04751387611031532 Validation loss 0.04788830876350403 Accuracy 0.8736250400543213\n",
      "Iteration 36910 Training loss 0.041894759982824326 Validation loss 0.047869741916656494 Accuracy 0.8735000491142273\n",
      "Iteration 36920 Training loss 0.04298629239201546 Validation loss 0.04791657626628876 Accuracy 0.8732500672340393\n",
      "Iteration 36930 Training loss 0.04682280868291855 Validation loss 0.047832027077674866 Accuracy 0.87437504529953\n",
      "Iteration 36940 Training loss 0.044359106570482254 Validation loss 0.04808855801820755 Accuracy 0.8721250295639038\n",
      "Iteration 36950 Training loss 0.04138496145606041 Validation loss 0.04799198359251022 Accuracy 0.8756250143051147\n",
      "Iteration 36960 Training loss 0.04307284206151962 Validation loss 0.0482320562005043 Accuracy 0.8748750686645508\n",
      "Iteration 36970 Training loss 0.043411023914813995 Validation loss 0.0483739972114563 Accuracy 0.8701250553131104\n",
      "Iteration 36980 Training loss 0.042009901255369186 Validation loss 0.04781191051006317 Accuracy 0.8758750557899475\n",
      "Iteration 36990 Training loss 0.04163427650928497 Validation loss 0.04802490398287773 Accuracy 0.8713750243186951\n",
      "Iteration 37000 Training loss 0.033975329250097275 Validation loss 0.04781954362988472 Accuracy 0.8748750686645508\n",
      "Iteration 37010 Training loss 0.05761552229523659 Validation loss 0.04800422117114067 Accuracy 0.8763750195503235\n",
      "Iteration 37020 Training loss 0.04379703477025032 Validation loss 0.04790131747722626 Accuracy 0.8725000619888306\n",
      "Iteration 37030 Training loss 0.04610283672809601 Validation loss 0.04812459275126457 Accuracy 0.8750000596046448\n",
      "Iteration 37040 Training loss 0.0441744364798069 Validation loss 0.04799098148941994 Accuracy 0.8721250295639038\n",
      "Iteration 37050 Training loss 0.048424091190099716 Validation loss 0.04789004847407341 Accuracy 0.8721250295639038\n",
      "Iteration 37060 Training loss 0.03891780227422714 Validation loss 0.04777878150343895 Accuracy 0.87437504529953\n",
      "Iteration 37070 Training loss 0.038105811923742294 Validation loss 0.04791347682476044 Accuracy 0.8765000700950623\n",
      "Iteration 37080 Training loss 0.0446174219250679 Validation loss 0.04778926447033882 Accuracy 0.8756250143051147\n",
      "Iteration 37090 Training loss 0.047467298805713654 Validation loss 0.047825828194618225 Accuracy 0.8755000233650208\n",
      "Iteration 37100 Training loss 0.0456848181784153 Validation loss 0.04783846437931061 Accuracy 0.8765000700950623\n",
      "Iteration 37110 Training loss 0.042195308953523636 Validation loss 0.04781582951545715 Accuracy 0.8730000257492065\n",
      "Iteration 37120 Training loss 0.03977065905928612 Validation loss 0.04879399761557579 Accuracy 0.8688750267028809\n",
      "Iteration 37130 Training loss 0.04131140559911728 Validation loss 0.04777604341506958 Accuracy 0.8728750348091125\n",
      "Iteration 37140 Training loss 0.049850624054670334 Validation loss 0.04792667180299759 Accuracy 0.8757500648498535\n",
      "Iteration 37150 Training loss 0.04051627218723297 Validation loss 0.04805247485637665 Accuracy 0.8723750710487366\n",
      "Iteration 37160 Training loss 0.04708530753850937 Validation loss 0.04771897941827774 Accuracy 0.874250054359436\n",
      "Iteration 37170 Training loss 0.05192567780613899 Validation loss 0.048793110996484756 Accuracy 0.8678750395774841\n",
      "Iteration 37180 Training loss 0.042293183505535126 Validation loss 0.04802507162094116 Accuracy 0.8722500205039978\n",
      "Iteration 37190 Training loss 0.047013059258461 Validation loss 0.047884501516819 Accuracy 0.8763750195503235\n",
      "Iteration 37200 Training loss 0.040834978222846985 Validation loss 0.04848852753639221 Accuracy 0.8695000410079956\n",
      "Iteration 37210 Training loss 0.045582160353660583 Validation loss 0.04785225912928581 Accuracy 0.8762500286102295\n",
      "Iteration 37220 Training loss 0.035536739975214005 Validation loss 0.04794991388916969 Accuracy 0.8753750324249268\n",
      "Iteration 37230 Training loss 0.04217500612139702 Validation loss 0.04800430312752724 Accuracy 0.8716250658035278\n",
      "Iteration 37240 Training loss 0.048699770122766495 Validation loss 0.047745898365974426 Accuracy 0.8751250505447388\n",
      "Iteration 37250 Training loss 0.03975164145231247 Validation loss 0.04772893711924553 Accuracy 0.8748750686645508\n",
      "Iteration 37260 Training loss 0.042559996247291565 Validation loss 0.04849519208073616 Accuracy 0.8691250681877136\n",
      "Iteration 37270 Training loss 0.04227262735366821 Validation loss 0.047759365290403366 Accuracy 0.874750018119812\n",
      "Iteration 37280 Training loss 0.04522157460451126 Validation loss 0.047771576792001724 Accuracy 0.874125063419342\n",
      "Iteration 37290 Training loss 0.04708665609359741 Validation loss 0.047987308353185654 Accuracy 0.8762500286102295\n",
      "Iteration 37300 Training loss 0.04458681121468544 Validation loss 0.04768206179141998 Accuracy 0.874625027179718\n",
      "Iteration 37310 Training loss 0.03524841368198395 Validation loss 0.0477370023727417 Accuracy 0.8736250400543213\n",
      "Iteration 37320 Training loss 0.04718076437711716 Validation loss 0.047775719314813614 Accuracy 0.8757500648498535\n",
      "Iteration 37330 Training loss 0.042109787464141846 Validation loss 0.0476691871881485 Accuracy 0.874125063419342\n",
      "Iteration 37340 Training loss 0.04669184610247612 Validation loss 0.04768580570816994 Accuracy 0.8740000128746033\n",
      "Iteration 37350 Training loss 0.04223174601793289 Validation loss 0.04779938608407974 Accuracy 0.8737500309944153\n",
      "Iteration 37360 Training loss 0.045613136142492294 Validation loss 0.04769672080874443 Accuracy 0.8752500414848328\n",
      "Iteration 37370 Training loss 0.036532897502183914 Validation loss 0.047749750316143036 Accuracy 0.8766250610351562\n",
      "Iteration 37380 Training loss 0.04705062136054039 Validation loss 0.047781433910131454 Accuracy 0.8731250166893005\n",
      "Iteration 37390 Training loss 0.046175260096788406 Validation loss 0.04767458513379097 Accuracy 0.8770000338554382\n",
      "Iteration 37400 Training loss 0.03704606741666794 Validation loss 0.047812510281801224 Accuracy 0.8717500567436218\n",
      "Iteration 37410 Training loss 0.04398474469780922 Validation loss 0.04765511676669121 Accuracy 0.874250054359436\n",
      "Iteration 37420 Training loss 0.03818060830235481 Validation loss 0.04773704707622528 Accuracy 0.8757500648498535\n",
      "Iteration 37430 Training loss 0.04634629935026169 Validation loss 0.04778563976287842 Accuracy 0.8765000700950623\n",
      "Iteration 37440 Training loss 0.039690032601356506 Validation loss 0.04785001277923584 Accuracy 0.8767500519752502\n",
      "Iteration 37450 Training loss 0.05056963860988617 Validation loss 0.04766179621219635 Accuracy 0.8761250376701355\n",
      "Iteration 37460 Training loss 0.04005681350827217 Validation loss 0.04766836762428284 Accuracy 0.8753750324249268\n",
      "Iteration 37470 Training loss 0.04365156963467598 Validation loss 0.047685861587524414 Accuracy 0.8750000596046448\n",
      "Iteration 37480 Training loss 0.03970777615904808 Validation loss 0.0477815605700016 Accuracy 0.8758750557899475\n",
      "Iteration 37490 Training loss 0.04844525083899498 Validation loss 0.04773341491818428 Accuracy 0.8758750557899475\n",
      "Iteration 37500 Training loss 0.039176538586616516 Validation loss 0.04819268360733986 Accuracy 0.8763750195503235\n",
      "Iteration 37510 Training loss 0.03499271348118782 Validation loss 0.0477299690246582 Accuracy 0.8748750686645508\n",
      "Iteration 37520 Training loss 0.048569392412900925 Validation loss 0.047744762152433395 Accuracy 0.874750018119812\n",
      "Iteration 37530 Training loss 0.04067515209317207 Validation loss 0.047754984349012375 Accuracy 0.8730000257492065\n",
      "Iteration 37540 Training loss 0.04821246117353439 Validation loss 0.048061493784189224 Accuracy 0.8758750557899475\n",
      "Iteration 37550 Training loss 0.03697853907942772 Validation loss 0.047727108001708984 Accuracy 0.8755000233650208\n",
      "Iteration 37560 Training loss 0.047781363129615784 Validation loss 0.047952134162187576 Accuracy 0.8748750686645508\n",
      "Iteration 37570 Training loss 0.04293859750032425 Validation loss 0.04797149822115898 Accuracy 0.8748750686645508\n",
      "Iteration 37580 Training loss 0.05080051347613335 Validation loss 0.047921184450387955 Accuracy 0.8752500414848328\n",
      "Iteration 37590 Training loss 0.04595223814249039 Validation loss 0.04772436246275902 Accuracy 0.874250054359436\n",
      "Iteration 37600 Training loss 0.043428801000118256 Validation loss 0.04768945276737213 Accuracy 0.8756250143051147\n",
      "Iteration 37610 Training loss 0.041782692074775696 Validation loss 0.0477164164185524 Accuracy 0.8738750219345093\n",
      "Iteration 37620 Training loss 0.05196765437722206 Validation loss 0.04778607562184334 Accuracy 0.8755000233650208\n",
      "Iteration 37630 Training loss 0.037963591516017914 Validation loss 0.048065897077322006 Accuracy 0.8707500696182251\n",
      "Iteration 37640 Training loss 0.04996814578771591 Validation loss 0.04794909805059433 Accuracy 0.8712500333786011\n",
      "Iteration 37650 Training loss 0.04685395956039429 Validation loss 0.04810614138841629 Accuracy 0.8758750557899475\n",
      "Iteration 37660 Training loss 0.05296855792403221 Validation loss 0.04769448563456535 Accuracy 0.8748750686645508\n",
      "Iteration 37670 Training loss 0.04522523656487465 Validation loss 0.04766349121928215 Accuracy 0.8751250505447388\n",
      "Iteration 37680 Training loss 0.05058074742555618 Validation loss 0.047837067395448685 Accuracy 0.8730000257492065\n",
      "Iteration 37690 Training loss 0.04697096347808838 Validation loss 0.04771106317639351 Accuracy 0.8727500438690186\n",
      "Iteration 37700 Training loss 0.05275589972734451 Validation loss 0.047720301896333694 Accuracy 0.8730000257492065\n",
      "Iteration 37710 Training loss 0.04240458086133003 Validation loss 0.04766390472650528 Accuracy 0.874125063419342\n",
      "Iteration 37720 Training loss 0.04320507124066353 Validation loss 0.04764951393008232 Accuracy 0.874250054359436\n",
      "Iteration 37730 Training loss 0.03996796905994415 Validation loss 0.047656938433647156 Accuracy 0.8752500414848328\n",
      "Iteration 37740 Training loss 0.04888853430747986 Validation loss 0.04772185906767845 Accuracy 0.8725000619888306\n",
      "Iteration 37750 Training loss 0.044677067548036575 Validation loss 0.04767274856567383 Accuracy 0.874500036239624\n",
      "Iteration 37760 Training loss 0.04344712197780609 Validation loss 0.047676753252744675 Accuracy 0.8730000257492065\n",
      "Iteration 37770 Training loss 0.0461677722632885 Validation loss 0.047730639576911926 Accuracy 0.8738750219345093\n",
      "Iteration 37780 Training loss 0.039825115352869034 Validation loss 0.047722116112709045 Accuracy 0.8731250166893005\n",
      "Iteration 37790 Training loss 0.04583091661334038 Validation loss 0.04769868403673172 Accuracy 0.8738750219345093\n",
      "Iteration 37800 Training loss 0.04555448517203331 Validation loss 0.04771692305803299 Accuracy 0.874125063419342\n",
      "Iteration 37810 Training loss 0.04280926659703255 Validation loss 0.04781029745936394 Accuracy 0.8763750195503235\n",
      "Iteration 37820 Training loss 0.04091736674308777 Validation loss 0.047655001282691956 Accuracy 0.8758750557899475\n",
      "Iteration 37830 Training loss 0.03653364256024361 Validation loss 0.04804941639304161 Accuracy 0.8762500286102295\n",
      "Iteration 37840 Training loss 0.03955874219536781 Validation loss 0.04767674580216408 Accuracy 0.8762500286102295\n",
      "Iteration 37850 Training loss 0.04113054648041725 Validation loss 0.047593455761671066 Accuracy 0.8755000233650208\n",
      "Iteration 37860 Training loss 0.04113953188061714 Validation loss 0.04785744845867157 Accuracy 0.8767500519752502\n",
      "Iteration 37870 Training loss 0.04556116461753845 Validation loss 0.0478774793446064 Accuracy 0.8771250247955322\n",
      "Iteration 37880 Training loss 0.04590579867362976 Validation loss 0.047861017286777496 Accuracy 0.8721250295639038\n",
      "Iteration 37890 Training loss 0.042285773903131485 Validation loss 0.04779369756579399 Accuracy 0.8723750710487366\n",
      "Iteration 37900 Training loss 0.04534919187426567 Validation loss 0.04784252494573593 Accuracy 0.8757500648498535\n",
      "Iteration 37910 Training loss 0.052813902497291565 Validation loss 0.04755319654941559 Accuracy 0.8753750324249268\n",
      "Iteration 37920 Training loss 0.048757534474134445 Validation loss 0.047509729862213135 Accuracy 0.8750000596046448\n",
      "Iteration 37930 Training loss 0.03684240207076073 Validation loss 0.047581929713487625 Accuracy 0.874125063419342\n",
      "Iteration 37940 Training loss 0.04206821322441101 Validation loss 0.04751673713326454 Accuracy 0.8755000233650208\n",
      "Iteration 37950 Training loss 0.03770381957292557 Validation loss 0.04795512557029724 Accuracy 0.8757500648498535\n",
      "Iteration 37960 Training loss 0.04690876975655556 Validation loss 0.04832247644662857 Accuracy 0.874625027179718\n",
      "Iteration 37970 Training loss 0.04534419998526573 Validation loss 0.04787641763687134 Accuracy 0.8765000700950623\n",
      "Iteration 37980 Training loss 0.038516681641340256 Validation loss 0.04753546416759491 Accuracy 0.8757500648498535\n",
      "Iteration 37990 Training loss 0.04178696498274803 Validation loss 0.04753110185265541 Accuracy 0.8757500648498535\n",
      "Iteration 38000 Training loss 0.04251653701066971 Validation loss 0.047797758132219315 Accuracy 0.8735000491142273\n",
      "Iteration 38010 Training loss 0.046586453914642334 Validation loss 0.04754631966352463 Accuracy 0.8750000596046448\n",
      "Iteration 38020 Training loss 0.04463978484272957 Validation loss 0.04753970727324486 Accuracy 0.8757500648498535\n",
      "Iteration 38030 Training loss 0.0447208397090435 Validation loss 0.047583866864442825 Accuracy 0.8760000467300415\n",
      "Iteration 38040 Training loss 0.04316491633653641 Validation loss 0.04751713201403618 Accuracy 0.8765000700950623\n",
      "Iteration 38050 Training loss 0.04435097053647041 Validation loss 0.04778456687927246 Accuracy 0.8763750195503235\n",
      "Iteration 38060 Training loss 0.04008400812745094 Validation loss 0.04756901040673256 Accuracy 0.8738750219345093\n",
      "Iteration 38070 Training loss 0.0390881709754467 Validation loss 0.04749784618616104 Accuracy 0.874500036239624\n",
      "Iteration 38080 Training loss 0.045378707349300385 Validation loss 0.04746640846133232 Accuracy 0.8757500648498535\n",
      "Iteration 38090 Training loss 0.0525643490254879 Validation loss 0.04764704406261444 Accuracy 0.8771250247955322\n",
      "Iteration 38100 Training loss 0.03945256769657135 Validation loss 0.04747268185019493 Accuracy 0.8753750324249268\n",
      "Iteration 38110 Training loss 0.044412530958652496 Validation loss 0.047681454569101334 Accuracy 0.8732500672340393\n",
      "Iteration 38120 Training loss 0.03711109235882759 Validation loss 0.047464121133089066 Accuracy 0.8753750324249268\n",
      "Iteration 38130 Training loss 0.043022945523262024 Validation loss 0.04746543988585472 Accuracy 0.8755000233650208\n",
      "Iteration 38140 Training loss 0.04493051394820213 Validation loss 0.04746830835938454 Accuracy 0.874625027179718\n",
      "Iteration 38150 Training loss 0.04009707272052765 Validation loss 0.047635145485401154 Accuracy 0.8732500672340393\n",
      "Iteration 38160 Training loss 0.04978847876191139 Validation loss 0.04755949601531029 Accuracy 0.8748750686645508\n",
      "Iteration 38170 Training loss 0.04540649428963661 Validation loss 0.047536451369524 Accuracy 0.8757500648498535\n",
      "Iteration 38180 Training loss 0.04849576950073242 Validation loss 0.04821465164422989 Accuracy 0.8711250424385071\n",
      "Iteration 38190 Training loss 0.03710184618830681 Validation loss 0.04752142354846001 Accuracy 0.874250054359436\n",
      "Iteration 38200 Training loss 0.04673415422439575 Validation loss 0.04745187610387802 Accuracy 0.8740000128746033\n",
      "Iteration 38210 Training loss 0.041970670223236084 Validation loss 0.04750669375061989 Accuracy 0.8760000467300415\n",
      "Iteration 38220 Training loss 0.0474361926317215 Validation loss 0.04744129627943039 Accuracy 0.8760000467300415\n",
      "Iteration 38230 Training loss 0.04146556183695793 Validation loss 0.04738321900367737 Accuracy 0.8755000233650208\n",
      "Iteration 38240 Training loss 0.04688353091478348 Validation loss 0.0473523810505867 Accuracy 0.8756250143051147\n",
      "Iteration 38250 Training loss 0.04612268507480621 Validation loss 0.04747272655367851 Accuracy 0.8740000128746033\n",
      "Iteration 38260 Training loss 0.0498378612101078 Validation loss 0.04740174859762192 Accuracy 0.8765000700950623\n",
      "Iteration 38270 Training loss 0.04227558895945549 Validation loss 0.04743306338787079 Accuracy 0.8755000233650208\n",
      "Iteration 38280 Training loss 0.04308508709073067 Validation loss 0.04749229922890663 Accuracy 0.878125011920929\n",
      "Iteration 38290 Training loss 0.05078844726085663 Validation loss 0.04807785525918007 Accuracy 0.8698750138282776\n",
      "Iteration 38300 Training loss 0.04152197390794754 Validation loss 0.04741751030087471 Accuracy 0.8758750557899475\n",
      "Iteration 38310 Training loss 0.04457346349954605 Validation loss 0.04751159995794296 Accuracy 0.8768750429153442\n",
      "Iteration 38320 Training loss 0.040833283215761185 Validation loss 0.047418542206287384 Accuracy 0.8751250505447388\n",
      "Iteration 38330 Training loss 0.04140990972518921 Validation loss 0.04740674048662186 Accuracy 0.8757500648498535\n",
      "Iteration 38340 Training loss 0.0450013242661953 Validation loss 0.04758705198764801 Accuracy 0.874125063419342\n",
      "Iteration 38350 Training loss 0.042154617607593536 Validation loss 0.04755330830812454 Accuracy 0.8750000596046448\n",
      "Iteration 38360 Training loss 0.03865881264209747 Validation loss 0.04746489226818085 Accuracy 0.8751250505447388\n",
      "Iteration 38370 Training loss 0.049301885068416595 Validation loss 0.047416023910045624 Accuracy 0.8763750195503235\n",
      "Iteration 38380 Training loss 0.03770184889435768 Validation loss 0.04745103046298027 Accuracy 0.8760000467300415\n",
      "Iteration 38390 Training loss 0.04350399971008301 Validation loss 0.047796960920095444 Accuracy 0.8717500567436218\n",
      "Iteration 38400 Training loss 0.03935014456510544 Validation loss 0.04756128415465355 Accuracy 0.8735000491142273\n",
      "Iteration 38410 Training loss 0.04944156855344772 Validation loss 0.04741806164383888 Accuracy 0.8748750686645508\n",
      "Iteration 38420 Training loss 0.03638177737593651 Validation loss 0.04745710268616676 Accuracy 0.8765000700950623\n",
      "Iteration 38430 Training loss 0.0462564080953598 Validation loss 0.04738537594676018 Accuracy 0.8748750686645508\n",
      "Iteration 38440 Training loss 0.046490032225847244 Validation loss 0.048154812306165695 Accuracy 0.874250054359436\n",
      "Iteration 38450 Training loss 0.04253266379237175 Validation loss 0.04736483842134476 Accuracy 0.8761250376701355\n",
      "Iteration 38460 Training loss 0.04343916475772858 Validation loss 0.04750702530145645 Accuracy 0.8735000491142273\n",
      "Iteration 38470 Training loss 0.04265991970896721 Validation loss 0.047346919775009155 Accuracy 0.877875030040741\n",
      "Iteration 38480 Training loss 0.04317444562911987 Validation loss 0.04732125997543335 Accuracy 0.8767500519752502\n",
      "Iteration 38490 Training loss 0.04294019937515259 Validation loss 0.04734395816922188 Accuracy 0.8748750686645508\n",
      "Iteration 38500 Training loss 0.0395687110722065 Validation loss 0.04753411188721657 Accuracy 0.8735000491142273\n",
      "Iteration 38510 Training loss 0.041133277118206024 Validation loss 0.04776870086789131 Accuracy 0.8753750324249268\n",
      "Iteration 38520 Training loss 0.027296820655465126 Validation loss 0.04739793390035629 Accuracy 0.874625027179718\n",
      "Iteration 38530 Training loss 0.04532033950090408 Validation loss 0.047393932938575745 Accuracy 0.8758750557899475\n",
      "Iteration 38540 Training loss 0.040953416377305984 Validation loss 0.04744809865951538 Accuracy 0.8756250143051147\n",
      "Iteration 38550 Training loss 0.042688921093940735 Validation loss 0.047831278294324875 Accuracy 0.8713750243186951\n",
      "Iteration 38560 Training loss 0.0479663647711277 Validation loss 0.047518666833639145 Accuracy 0.874250054359436\n",
      "Iteration 38570 Training loss 0.04209528863430023 Validation loss 0.04747111722826958 Accuracy 0.874500036239624\n",
      "Iteration 38580 Training loss 0.04200439527630806 Validation loss 0.04749320447444916 Accuracy 0.8737500309944153\n",
      "Iteration 38590 Training loss 0.04674135893583298 Validation loss 0.04740729182958603 Accuracy 0.8762500286102295\n",
      "Iteration 38600 Training loss 0.03693216294050217 Validation loss 0.047481801360845566 Accuracy 0.8757500648498535\n",
      "Iteration 38610 Training loss 0.046522367745637894 Validation loss 0.048554033041000366 Accuracy 0.8737500309944153\n",
      "Iteration 38620 Training loss 0.04717704653739929 Validation loss 0.0481865368783474 Accuracy 0.8753750324249268\n",
      "Iteration 38630 Training loss 0.04018193483352661 Validation loss 0.04742985963821411 Accuracy 0.8762500286102295\n",
      "Iteration 38640 Training loss 0.041670091450214386 Validation loss 0.047391586005687714 Accuracy 0.8753750324249268\n",
      "Iteration 38650 Training loss 0.03625495731830597 Validation loss 0.04743635281920433 Accuracy 0.874625027179718\n",
      "Iteration 38660 Training loss 0.037341874092817307 Validation loss 0.04738472029566765 Accuracy 0.8760000467300415\n",
      "Iteration 38670 Training loss 0.048746515065431595 Validation loss 0.04737016558647156 Accuracy 0.8760000467300415\n",
      "Iteration 38680 Training loss 0.04916567727923393 Validation loss 0.04769011214375496 Accuracy 0.8761250376701355\n",
      "Iteration 38690 Training loss 0.046953897923231125 Validation loss 0.04762818291783333 Accuracy 0.8765000700950623\n",
      "Iteration 38700 Training loss 0.03813505172729492 Validation loss 0.04733999818563461 Accuracy 0.8757500648498535\n",
      "Iteration 38710 Training loss 0.03737189993262291 Validation loss 0.04759644344449043 Accuracy 0.8768750429153442\n",
      "Iteration 38720 Training loss 0.03979610279202461 Validation loss 0.04767946898937225 Accuracy 0.8772500157356262\n",
      "Iteration 38730 Training loss 0.042378973215818405 Validation loss 0.04790642112493515 Accuracy 0.8756250143051147\n",
      "Iteration 38740 Training loss 0.04563334956765175 Validation loss 0.047382332384586334 Accuracy 0.8752500414848328\n",
      "Iteration 38750 Training loss 0.03835509717464447 Validation loss 0.04758547618985176 Accuracy 0.874250054359436\n",
      "Iteration 38760 Training loss 0.038684677332639694 Validation loss 0.04735179990530014 Accuracy 0.8757500648498535\n",
      "Iteration 38770 Training loss 0.04592609778046608 Validation loss 0.047395579516887665 Accuracy 0.8752500414848328\n",
      "Iteration 38780 Training loss 0.04532342404127121 Validation loss 0.04737840220332146 Accuracy 0.8753750324249268\n",
      "Iteration 38790 Training loss 0.04697732999920845 Validation loss 0.04776841029524803 Accuracy 0.8756250143051147\n",
      "Iteration 38800 Training loss 0.03846634551882744 Validation loss 0.04742736369371414 Accuracy 0.874125063419342\n",
      "Iteration 38810 Training loss 0.04888516291975975 Validation loss 0.047310229390859604 Accuracy 0.8765000700950623\n",
      "Iteration 38820 Training loss 0.03801896423101425 Validation loss 0.04747011512517929 Accuracy 0.8770000338554382\n",
      "Iteration 38830 Training loss 0.04549328610301018 Validation loss 0.047335948795080185 Accuracy 0.8760000467300415\n",
      "Iteration 38840 Training loss 0.04704727977514267 Validation loss 0.04751037806272507 Accuracy 0.8738750219345093\n",
      "Iteration 38850 Training loss 0.04610048234462738 Validation loss 0.04745648428797722 Accuracy 0.8772500157356262\n",
      "Iteration 38860 Training loss 0.04382618889212608 Validation loss 0.047375619411468506 Accuracy 0.874500036239624\n",
      "Iteration 38870 Training loss 0.03868973255157471 Validation loss 0.04746615141630173 Accuracy 0.8733750581741333\n",
      "Iteration 38880 Training loss 0.05248153954744339 Validation loss 0.047460611909627914 Accuracy 0.8758750557899475\n",
      "Iteration 38890 Training loss 0.044900137931108475 Validation loss 0.047323424369096756 Accuracy 0.8755000233650208\n",
      "Iteration 38900 Training loss 0.04510298743844032 Validation loss 0.047368135303258896 Accuracy 0.8753750324249268\n",
      "Iteration 38910 Training loss 0.04002321511507034 Validation loss 0.047374892979860306 Accuracy 0.8753750324249268\n",
      "Iteration 38920 Training loss 0.042957477271556854 Validation loss 0.04773001745343208 Accuracy 0.8765000700950623\n",
      "Iteration 38930 Training loss 0.041494958102703094 Validation loss 0.04730115085840225 Accuracy 0.8748750686645508\n",
      "Iteration 38940 Training loss 0.03761223331093788 Validation loss 0.047391556203365326 Accuracy 0.8757500648498535\n",
      "Iteration 38950 Training loss 0.041957926005125046 Validation loss 0.047336146235466 Accuracy 0.8756250143051147\n",
      "Iteration 38960 Training loss 0.04314853996038437 Validation loss 0.047391701489686966 Accuracy 0.8753750324249268\n",
      "Iteration 38970 Training loss 0.04445264860987663 Validation loss 0.047333572059869766 Accuracy 0.8740000128746033\n",
      "Iteration 38980 Training loss 0.0444338321685791 Validation loss 0.04729372635483742 Accuracy 0.8761250376701355\n",
      "Iteration 38990 Training loss 0.05107685923576355 Validation loss 0.047354958951473236 Accuracy 0.8756250143051147\n",
      "Iteration 39000 Training loss 0.044507306069135666 Validation loss 0.04730620235204697 Accuracy 0.8751250505447388\n",
      "Iteration 39010 Training loss 0.0442221574485302 Validation loss 0.047815170139074326 Accuracy 0.8711250424385071\n",
      "Iteration 39020 Training loss 0.041691310703754425 Validation loss 0.04742686450481415 Accuracy 0.87437504529953\n",
      "Iteration 39030 Training loss 0.045927371829748154 Validation loss 0.04749332740902901 Accuracy 0.8768750429153442\n",
      "Iteration 39040 Training loss 0.041852276772260666 Validation loss 0.04733327776193619 Accuracy 0.8757500648498535\n",
      "Iteration 39050 Training loss 0.05176340416073799 Validation loss 0.047265443950891495 Accuracy 0.8751250505447388\n",
      "Iteration 39060 Training loss 0.04177853465080261 Validation loss 0.047330088913440704 Accuracy 0.8761250376701355\n",
      "Iteration 39070 Training loss 0.043493397533893585 Validation loss 0.047278665006160736 Accuracy 0.8756250143051147\n",
      "Iteration 39080 Training loss 0.04254193231463432 Validation loss 0.0473184660077095 Accuracy 0.8748750686645508\n",
      "Iteration 39090 Training loss 0.044274814426898956 Validation loss 0.04729282483458519 Accuracy 0.8760000467300415\n",
      "Iteration 39100 Training loss 0.041752465069293976 Validation loss 0.04733729735016823 Accuracy 0.8752500414848328\n",
      "Iteration 39110 Training loss 0.03677409142255783 Validation loss 0.04752035811543465 Accuracy 0.8763750195503235\n",
      "Iteration 39120 Training loss 0.04933174327015877 Validation loss 0.04725213348865509 Accuracy 0.8752500414848328\n",
      "Iteration 39130 Training loss 0.04446813091635704 Validation loss 0.0473676398396492 Accuracy 0.8751250505447388\n",
      "Iteration 39140 Training loss 0.042966216802597046 Validation loss 0.0479300431907177 Accuracy 0.8748750686645508\n",
      "Iteration 39150 Training loss 0.04357270523905754 Validation loss 0.04768846556544304 Accuracy 0.8723750710487366\n",
      "Iteration 39160 Training loss 0.041037850081920624 Validation loss 0.04724503681063652 Accuracy 0.8760000467300415\n",
      "Iteration 39170 Training loss 0.04597261920571327 Validation loss 0.04738626256585121 Accuracy 0.8770000338554382\n",
      "Iteration 39180 Training loss 0.03689657896757126 Validation loss 0.047291070222854614 Accuracy 0.877625048160553\n",
      "Iteration 39190 Training loss 0.03815080597996712 Validation loss 0.047557588666677475 Accuracy 0.8760000467300415\n",
      "Iteration 39200 Training loss 0.0427689254283905 Validation loss 0.047213755548000336 Accuracy 0.874250054359436\n",
      "Iteration 39210 Training loss 0.041265662759542465 Validation loss 0.047672729939222336 Accuracy 0.8763750195503235\n",
      "Iteration 39220 Training loss 0.04174747318029404 Validation loss 0.04746421426534653 Accuracy 0.8730000257492065\n",
      "Iteration 39230 Training loss 0.04108728468418121 Validation loss 0.04725221171975136 Accuracy 0.8738750219345093\n",
      "Iteration 39240 Training loss 0.03869267553091049 Validation loss 0.04732483997941017 Accuracy 0.8768750429153442\n",
      "Iteration 39250 Training loss 0.04024519771337509 Validation loss 0.04715694859623909 Accuracy 0.8771250247955322\n",
      "Iteration 39260 Training loss 0.03756439685821533 Validation loss 0.047318655997514725 Accuracy 0.87437504529953\n",
      "Iteration 39270 Training loss 0.04952072352170944 Validation loss 0.04750913009047508 Accuracy 0.8740000128746033\n",
      "Iteration 39280 Training loss 0.037002500146627426 Validation loss 0.04717564582824707 Accuracy 0.8752500414848328\n",
      "Iteration 39290 Training loss 0.03963695839047432 Validation loss 0.04766707867383957 Accuracy 0.8725000619888306\n",
      "Iteration 39300 Training loss 0.04260651394724846 Validation loss 0.04806945100426674 Accuracy 0.8707500696182251\n",
      "Iteration 39310 Training loss 0.04411191865801811 Validation loss 0.04749758541584015 Accuracy 0.8758750557899475\n",
      "Iteration 39320 Training loss 0.04184465482831001 Validation loss 0.04727064073085785 Accuracy 0.8765000700950623\n",
      "Iteration 39330 Training loss 0.05726027861237526 Validation loss 0.047229379415512085 Accuracy 0.8758750557899475\n",
      "Iteration 39340 Training loss 0.04492154344916344 Validation loss 0.04753679037094116 Accuracy 0.8761250376701355\n",
      "Iteration 39350 Training loss 0.03689524903893471 Validation loss 0.04726305603981018 Accuracy 0.8758750557899475\n",
      "Iteration 39360 Training loss 0.043159570544958115 Validation loss 0.04727058857679367 Accuracy 0.8763750195503235\n",
      "Iteration 39370 Training loss 0.04693486541509628 Validation loss 0.047345101833343506 Accuracy 0.8758750557899475\n",
      "Iteration 39380 Training loss 0.04302534461021423 Validation loss 0.04755420237779617 Accuracy 0.8755000233650208\n",
      "Iteration 39390 Training loss 0.046591050922870636 Validation loss 0.047193657606840134 Accuracy 0.8762500286102295\n",
      "Iteration 39400 Training loss 0.034531887620687485 Validation loss 0.04756768047809601 Accuracy 0.8765000700950623\n",
      "Iteration 39410 Training loss 0.04561540484428406 Validation loss 0.0472569465637207 Accuracy 0.8760000467300415\n",
      "Iteration 39420 Training loss 0.04314766824245453 Validation loss 0.047555599361658096 Accuracy 0.8770000338554382\n",
      "Iteration 39430 Training loss 0.040544040501117706 Validation loss 0.04725397005677223 Accuracy 0.874625027179718\n",
      "Iteration 39440 Training loss 0.04241640865802765 Validation loss 0.04737083986401558 Accuracy 0.8738750219345093\n",
      "Iteration 39450 Training loss 0.042436279356479645 Validation loss 0.04759618639945984 Accuracy 0.8756250143051147\n",
      "Iteration 39460 Training loss 0.03962172567844391 Validation loss 0.04735476151108742 Accuracy 0.874500036239624\n",
      "Iteration 39470 Training loss 0.03636068105697632 Validation loss 0.04763098806142807 Accuracy 0.8722500205039978\n",
      "Iteration 39480 Training loss 0.04743752256035805 Validation loss 0.047376323491334915 Accuracy 0.8762500286102295\n",
      "Iteration 39490 Training loss 0.04693274572491646 Validation loss 0.047255244106054306 Accuracy 0.8748750686645508\n",
      "Iteration 39500 Training loss 0.041870005428791046 Validation loss 0.04726159945130348 Accuracy 0.878125011920929\n",
      "Iteration 39510 Training loss 0.040433380752801895 Validation loss 0.04728111997246742 Accuracy 0.874500036239624\n",
      "Iteration 39520 Training loss 0.03653347119688988 Validation loss 0.047270677983760834 Accuracy 0.877750039100647\n",
      "Iteration 39530 Training loss 0.045810602605342865 Validation loss 0.04805813357234001 Accuracy 0.874250054359436\n",
      "Iteration 39540 Training loss 0.051669079810380936 Validation loss 0.04759141802787781 Accuracy 0.8735000491142273\n",
      "Iteration 39550 Training loss 0.03974241763353348 Validation loss 0.04728736728429794 Accuracy 0.877375066280365\n",
      "Iteration 39560 Training loss 0.039662089198827744 Validation loss 0.04729054495692253 Accuracy 0.877625048160553\n",
      "Iteration 39570 Training loss 0.04132131487131119 Validation loss 0.04750499129295349 Accuracy 0.877750039100647\n",
      "Iteration 39580 Training loss 0.04415152966976166 Validation loss 0.04801472648978233 Accuracy 0.8707500696182251\n",
      "Iteration 39590 Training loss 0.045516613870859146 Validation loss 0.047860536724328995 Accuracy 0.8757500648498535\n",
      "Iteration 39600 Training loss 0.04342033341526985 Validation loss 0.047187838703393936 Accuracy 0.8762500286102295\n",
      "Iteration 39610 Training loss 0.04190867021679878 Validation loss 0.0471983402967453 Accuracy 0.8770000338554382\n",
      "Iteration 39620 Training loss 0.04642757400870323 Validation loss 0.047203026711940765 Accuracy 0.8763750195503235\n",
      "Iteration 39630 Training loss 0.039928682148456573 Validation loss 0.04728611931204796 Accuracy 0.8765000700950623\n",
      "Iteration 39640 Training loss 0.04102756083011627 Validation loss 0.047099411487579346 Accuracy 0.877500057220459\n",
      "Iteration 39650 Training loss 0.03753454238176346 Validation loss 0.04770423844456673 Accuracy 0.8757500648498535\n",
      "Iteration 39660 Training loss 0.040924519300460815 Validation loss 0.04716802015900612 Accuracy 0.8765000700950623\n",
      "Iteration 39670 Training loss 0.03902069106698036 Validation loss 0.04711555317044258 Accuracy 0.8761250376701355\n",
      "Iteration 39680 Training loss 0.045570969581604004 Validation loss 0.047166720032691956 Accuracy 0.8768750429153442\n",
      "Iteration 39690 Training loss 0.04787883162498474 Validation loss 0.04710659384727478 Accuracy 0.8772500157356262\n",
      "Iteration 39700 Training loss 0.04157336801290512 Validation loss 0.047908201813697815 Accuracy 0.8712500333786011\n",
      "Iteration 39710 Training loss 0.04254596680402756 Validation loss 0.04707745835185051 Accuracy 0.8766250610351562\n",
      "Iteration 39720 Training loss 0.043997038155794144 Validation loss 0.0471770353615284 Accuracy 0.8762500286102295\n",
      "Iteration 39730 Training loss 0.04393540322780609 Validation loss 0.04732630401849747 Accuracy 0.8740000128746033\n",
      "Iteration 39740 Training loss 0.041796669363975525 Validation loss 0.04705347493290901 Accuracy 0.8756250143051147\n",
      "Iteration 39750 Training loss 0.04780927672982216 Validation loss 0.04717577248811722 Accuracy 0.8762500286102295\n",
      "Iteration 39760 Training loss 0.041694268584251404 Validation loss 0.047047801315784454 Accuracy 0.8768750429153442\n",
      "Iteration 39770 Training loss 0.0421457476913929 Validation loss 0.04702780395746231 Accuracy 0.8762500286102295\n",
      "Iteration 39780 Training loss 0.03841651231050491 Validation loss 0.047046251595020294 Accuracy 0.8760000467300415\n",
      "Iteration 39790 Training loss 0.04025131091475487 Validation loss 0.04718811437487602 Accuracy 0.8765000700950623\n",
      "Iteration 39800 Training loss 0.03782213479280472 Validation loss 0.04701252654194832 Accuracy 0.8772500157356262\n",
      "Iteration 39810 Training loss 0.052154283970594406 Validation loss 0.047362037003040314 Accuracy 0.8770000338554382\n",
      "Iteration 39820 Training loss 0.04544495791196823 Validation loss 0.04705327749252319 Accuracy 0.8762500286102295\n",
      "Iteration 39830 Training loss 0.039009712636470795 Validation loss 0.04725617542862892 Accuracy 0.8765000700950623\n",
      "Iteration 39840 Training loss 0.041341960430145264 Validation loss 0.04706161096692085 Accuracy 0.8771250247955322\n",
      "Iteration 39850 Training loss 0.038764167577028275 Validation loss 0.04803475737571716 Accuracy 0.87437504529953\n",
      "Iteration 39860 Training loss 0.039728231728076935 Validation loss 0.04704941064119339 Accuracy 0.8755000233650208\n",
      "Iteration 39870 Training loss 0.045592937618494034 Validation loss 0.047007966786623 Accuracy 0.8756250143051147\n",
      "Iteration 39880 Training loss 0.04114792123436928 Validation loss 0.04704718664288521 Accuracy 0.874125063419342\n",
      "Iteration 39890 Training loss 0.03830597177147865 Validation loss 0.047201789915561676 Accuracy 0.8762500286102295\n",
      "Iteration 39900 Training loss 0.049951568245887756 Validation loss 0.048353683203458786 Accuracy 0.8688750267028809\n",
      "Iteration 39910 Training loss 0.045290060341358185 Validation loss 0.04727144539356232 Accuracy 0.8736250400543213\n",
      "Iteration 39920 Training loss 0.0420033223927021 Validation loss 0.047184642404317856 Accuracy 0.8737500309944153\n",
      "Iteration 39930 Training loss 0.04039433225989342 Validation loss 0.04700639098882675 Accuracy 0.8756250143051147\n",
      "Iteration 39940 Training loss 0.0411858931183815 Validation loss 0.04802361875772476 Accuracy 0.874125063419342\n",
      "Iteration 39950 Training loss 0.04316221922636032 Validation loss 0.047248583287000656 Accuracy 0.87437504529953\n",
      "Iteration 39960 Training loss 0.05332232266664505 Validation loss 0.04698667675256729 Accuracy 0.8760000467300415\n",
      "Iteration 39970 Training loss 0.0463155061006546 Validation loss 0.04698958992958069 Accuracy 0.877875030040741\n",
      "Iteration 39980 Training loss 0.03822150453925133 Validation loss 0.04697521775960922 Accuracy 0.877750039100647\n",
      "Iteration 39990 Training loss 0.038025952875614166 Validation loss 0.04695839807391167 Accuracy 0.8772500157356262\n",
      "Iteration 40000 Training loss 0.04539550095796585 Validation loss 0.0470607690513134 Accuracy 0.878125011920929\n",
      "Iteration 40010 Training loss 0.04766395315527916 Validation loss 0.046935904771089554 Accuracy 0.8772500157356262\n",
      "Iteration 40020 Training loss 0.04468638449907303 Validation loss 0.047079045325517654 Accuracy 0.8762500286102295\n",
      "Iteration 40030 Training loss 0.04518403112888336 Validation loss 0.04706651344895363 Accuracy 0.8762500286102295\n",
      "Iteration 40040 Training loss 0.04188302904367447 Validation loss 0.047345805913209915 Accuracy 0.8728750348091125\n",
      "Iteration 40050 Training loss 0.03624729812145233 Validation loss 0.04701899364590645 Accuracy 0.8751250505447388\n",
      "Iteration 40060 Training loss 0.04823892191052437 Validation loss 0.04774124547839165 Accuracy 0.8760000467300415\n",
      "Iteration 40070 Training loss 0.037770770490169525 Validation loss 0.046940822154283524 Accuracy 0.8767500519752502\n",
      "Iteration 40080 Training loss 0.04417344555258751 Validation loss 0.046938683837652206 Accuracy 0.8756250143051147\n",
      "Iteration 40090 Training loss 0.052151069045066833 Validation loss 0.047088541090488434 Accuracy 0.8757500648498535\n",
      "Iteration 40100 Training loss 0.046106968075037 Validation loss 0.04697731137275696 Accuracy 0.8760000467300415\n",
      "Iteration 40110 Training loss 0.04239000380039215 Validation loss 0.046984970569610596 Accuracy 0.877750039100647\n",
      "Iteration 40120 Training loss 0.04717318341135979 Validation loss 0.047013841569423676 Accuracy 0.8766250610351562\n",
      "Iteration 40130 Training loss 0.04088042303919792 Validation loss 0.04832702502608299 Accuracy 0.8685000538825989\n",
      "Iteration 40140 Training loss 0.05096138268709183 Validation loss 0.046994853764772415 Accuracy 0.8763750195503235\n",
      "Iteration 40150 Training loss 0.038565635681152344 Validation loss 0.04703393951058388 Accuracy 0.8768750429153442\n",
      "Iteration 40160 Training loss 0.04376503452658653 Validation loss 0.04740118980407715 Accuracy 0.8772500157356262\n",
      "Iteration 40170 Training loss 0.043636396527290344 Validation loss 0.04708375036716461 Accuracy 0.877500057220459\n",
      "Iteration 40180 Training loss 0.04224953427910805 Validation loss 0.04699259251356125 Accuracy 0.8765000700950623\n",
      "Iteration 40190 Training loss 0.043050508946180344 Validation loss 0.04713861271739006 Accuracy 0.8750000596046448\n",
      "Iteration 40200 Training loss 0.04259902983903885 Validation loss 0.046986937522888184 Accuracy 0.8768750429153442\n",
      "Iteration 40210 Training loss 0.038906827569007874 Validation loss 0.046939052641391754 Accuracy 0.8765000700950623\n",
      "Iteration 40220 Training loss 0.04029424116015434 Validation loss 0.04697871580719948 Accuracy 0.8760000467300415\n",
      "Iteration 40230 Training loss 0.03743651509284973 Validation loss 0.04692431166768074 Accuracy 0.8762500286102295\n",
      "Iteration 40240 Training loss 0.03652257099747658 Validation loss 0.046957939863204956 Accuracy 0.8761250376701355\n",
      "Iteration 40250 Training loss 0.04691895470023155 Validation loss 0.04699673131108284 Accuracy 0.8758750557899475\n",
      "Iteration 40260 Training loss 0.03645511716604233 Validation loss 0.04694391041994095 Accuracy 0.8756250143051147\n",
      "Iteration 40270 Training loss 0.03660096973180771 Validation loss 0.04744182154536247 Accuracy 0.8765000700950623\n",
      "Iteration 40280 Training loss 0.04045726731419563 Validation loss 0.04713309556245804 Accuracy 0.8751250505447388\n",
      "Iteration 40290 Training loss 0.04759825021028519 Validation loss 0.0481710322201252 Accuracy 0.8731250166893005\n",
      "Iteration 40300 Training loss 0.04637691751122475 Validation loss 0.04739190265536308 Accuracy 0.8736250400543213\n",
      "Iteration 40310 Training loss 0.04649882763624191 Validation loss 0.04710019379854202 Accuracy 0.877375066280365\n",
      "Iteration 40320 Training loss 0.04531047120690346 Validation loss 0.04698437079787254 Accuracy 0.8771250247955322\n",
      "Iteration 40330 Training loss 0.04930600896477699 Validation loss 0.047826554626226425 Accuracy 0.8711250424385071\n",
      "Iteration 40340 Training loss 0.04219019412994385 Validation loss 0.046969544142484665 Accuracy 0.8768750429153442\n",
      "Iteration 40350 Training loss 0.040815237909555435 Validation loss 0.04697326198220253 Accuracy 0.8758750557899475\n",
      "Iteration 40360 Training loss 0.04270397871732712 Validation loss 0.04697741940617561 Accuracy 0.8760000467300415\n",
      "Iteration 40370 Training loss 0.051076795905828476 Validation loss 0.0469825305044651 Accuracy 0.8761250376701355\n",
      "Iteration 40380 Training loss 0.04634576663374901 Validation loss 0.0469227209687233 Accuracy 0.8760000467300415\n",
      "Iteration 40390 Training loss 0.0435919463634491 Validation loss 0.047173768281936646 Accuracy 0.877500057220459\n",
      "Iteration 40400 Training loss 0.0384407564997673 Validation loss 0.0470007061958313 Accuracy 0.877375066280365\n",
      "Iteration 40410 Training loss 0.04299160838127136 Validation loss 0.04697958752512932 Accuracy 0.8765000700950623\n",
      "Iteration 40420 Training loss 0.039966095238924026 Validation loss 0.04691798612475395 Accuracy 0.8758750557899475\n",
      "Iteration 40430 Training loss 0.051546573638916016 Validation loss 0.04741066321730614 Accuracy 0.8767500519752502\n",
      "Iteration 40440 Training loss 0.039844922721385956 Validation loss 0.046934425830841064 Accuracy 0.8760000467300415\n",
      "Iteration 40450 Training loss 0.04462622106075287 Validation loss 0.04694626107811928 Accuracy 0.8765000700950623\n",
      "Iteration 40460 Training loss 0.04169687256217003 Validation loss 0.04696207493543625 Accuracy 0.8765000700950623\n",
      "Iteration 40470 Training loss 0.05043565481901169 Validation loss 0.04693274199962616 Accuracy 0.8762500286102295\n",
      "Iteration 40480 Training loss 0.04693808779120445 Validation loss 0.046917401254177094 Accuracy 0.8762500286102295\n",
      "Iteration 40490 Training loss 0.03368678316473961 Validation loss 0.04746273159980774 Accuracy 0.8763750195503235\n",
      "Iteration 40500 Training loss 0.04581528156995773 Validation loss 0.047240063548088074 Accuracy 0.8732500672340393\n",
      "Iteration 40510 Training loss 0.05681554228067398 Validation loss 0.047112978994846344 Accuracy 0.8758750557899475\n",
      "Iteration 40520 Training loss 0.04364996775984764 Validation loss 0.046924982219934464 Accuracy 0.8766250610351562\n",
      "Iteration 40530 Training loss 0.04349781945347786 Validation loss 0.04689701274037361 Accuracy 0.8771250247955322\n",
      "Iteration 40540 Training loss 0.04649600014090538 Validation loss 0.04705851525068283 Accuracy 0.877500057220459\n",
      "Iteration 40550 Training loss 0.041491370648145676 Validation loss 0.04714887589216232 Accuracy 0.874625027179718\n",
      "Iteration 40560 Training loss 0.03543645888566971 Validation loss 0.046975672245025635 Accuracy 0.877500057220459\n",
      "Iteration 40570 Training loss 0.04299319162964821 Validation loss 0.04825938120484352 Accuracy 0.8728750348091125\n",
      "Iteration 40580 Training loss 0.04590805992484093 Validation loss 0.046926744282245636 Accuracy 0.877875030040741\n",
      "Iteration 40590 Training loss 0.041273653507232666 Validation loss 0.04691322520375252 Accuracy 0.8767500519752502\n",
      "Iteration 40600 Training loss 0.04272341728210449 Validation loss 0.04716448485851288 Accuracy 0.877875030040741\n",
      "Iteration 40610 Training loss 0.039856962859630585 Validation loss 0.04695454612374306 Accuracy 0.8755000233650208\n",
      "Iteration 40620 Training loss 0.03971691429615021 Validation loss 0.04690568521618843 Accuracy 0.8762500286102295\n",
      "Iteration 40630 Training loss 0.04276387020945549 Validation loss 0.04688045009970665 Accuracy 0.8772500157356262\n",
      "Iteration 40640 Training loss 0.04179265722632408 Validation loss 0.04687400907278061 Accuracy 0.8772500157356262\n",
      "Iteration 40650 Training loss 0.04193601384758949 Validation loss 0.046876732259988785 Accuracy 0.878125011920929\n",
      "Iteration 40660 Training loss 0.04102964326739311 Validation loss 0.04699542373418808 Accuracy 0.8763750195503235\n",
      "Iteration 40670 Training loss 0.03396012634038925 Validation loss 0.04690602049231529 Accuracy 0.877625048160553\n",
      "Iteration 40680 Training loss 0.04159180447459221 Validation loss 0.047054536640644073 Accuracy 0.8782500624656677\n",
      "Iteration 40690 Training loss 0.04467124864459038 Validation loss 0.04694412276148796 Accuracy 0.8788750171661377\n",
      "Iteration 40700 Training loss 0.05057873949408531 Validation loss 0.046940527856349945 Accuracy 0.8768750429153442\n",
      "Iteration 40710 Training loss 0.042595818638801575 Validation loss 0.04748833552002907 Accuracy 0.8733750581741333\n",
      "Iteration 40720 Training loss 0.03472956269979477 Validation loss 0.04710086062550545 Accuracy 0.8786250352859497\n",
      "Iteration 40730 Training loss 0.03655650466680527 Validation loss 0.046927232295274734 Accuracy 0.8767500519752502\n",
      "Iteration 40740 Training loss 0.04535144567489624 Validation loss 0.04692689701914787 Accuracy 0.877625048160553\n",
      "Iteration 40750 Training loss 0.041773781180381775 Validation loss 0.046897053718566895 Accuracy 0.8766250610351562\n",
      "Iteration 40760 Training loss 0.04157339781522751 Validation loss 0.04706763103604317 Accuracy 0.877375066280365\n",
      "Iteration 40770 Training loss 0.038517165929079056 Validation loss 0.04693066328763962 Accuracy 0.8762500286102295\n",
      "Iteration 40780 Training loss 0.04652153328061104 Validation loss 0.046877432614564896 Accuracy 0.8768750429153442\n",
      "Iteration 40790 Training loss 0.04427637159824371 Validation loss 0.047008875757455826 Accuracy 0.8787500262260437\n",
      "Iteration 40800 Training loss 0.04986787214875221 Validation loss 0.0468982569873333 Accuracy 0.8771250247955322\n",
      "Iteration 40810 Training loss 0.04421404376626015 Validation loss 0.04725975915789604 Accuracy 0.8768750429153442\n",
      "Iteration 40820 Training loss 0.04428533464670181 Validation loss 0.04770646244287491 Accuracy 0.8752500414848328\n",
      "Iteration 40830 Training loss 0.04193919897079468 Validation loss 0.04850464314222336 Accuracy 0.8703750371932983\n",
      "Iteration 40840 Training loss 0.04311827942728996 Validation loss 0.04691930115222931 Accuracy 0.878125011920929\n",
      "Iteration 40850 Training loss 0.03735140711069107 Validation loss 0.04682708904147148 Accuracy 0.8783750534057617\n",
      "Iteration 40860 Training loss 0.046402521431446075 Validation loss 0.04682019352912903 Accuracy 0.8782500624656677\n",
      "Iteration 40870 Training loss 0.04098769277334213 Validation loss 0.046914976090192795 Accuracy 0.8767500519752502\n",
      "Iteration 40880 Training loss 0.042940668761730194 Validation loss 0.04720963537693024 Accuracy 0.874250054359436\n",
      "Iteration 40890 Training loss 0.043803948909044266 Validation loss 0.046953216195106506 Accuracy 0.877750039100647\n",
      "Iteration 40900 Training loss 0.04318591207265854 Validation loss 0.0477716438472271 Accuracy 0.8758750557899475\n",
      "Iteration 40910 Training loss 0.045894868671894073 Validation loss 0.0467904694378376 Accuracy 0.877500057220459\n",
      "Iteration 40920 Training loss 0.03843450918793678 Validation loss 0.04693117365241051 Accuracy 0.8785000443458557\n",
      "Iteration 40930 Training loss 0.04455462470650673 Validation loss 0.04675428196787834 Accuracy 0.878000020980835\n",
      "Iteration 40940 Training loss 0.03901052102446556 Validation loss 0.046740952879190445 Accuracy 0.8772500157356262\n",
      "Iteration 40950 Training loss 0.047060687094926834 Validation loss 0.048619769513607025 Accuracy 0.8706250190734863\n",
      "Iteration 40960 Training loss 0.04116593301296234 Validation loss 0.04681849479675293 Accuracy 0.877875030040741\n",
      "Iteration 40970 Training loss 0.04331362247467041 Validation loss 0.04681381210684776 Accuracy 0.877625048160553\n",
      "Iteration 40980 Training loss 0.037700504064559937 Validation loss 0.047346506267786026 Accuracy 0.874250054359436\n",
      "Iteration 40990 Training loss 0.04634712263941765 Validation loss 0.046820979565382004 Accuracy 0.878000020980835\n",
      "Iteration 41000 Training loss 0.0375664159655571 Validation loss 0.04688810184597969 Accuracy 0.8786250352859497\n",
      "Iteration 41010 Training loss 0.054459162056446075 Validation loss 0.04678720235824585 Accuracy 0.8790000677108765\n",
      "Iteration 41020 Training loss 0.04458102583885193 Validation loss 0.04692428559064865 Accuracy 0.8755000233650208\n",
      "Iteration 41030 Training loss 0.04376881569623947 Validation loss 0.04694009944796562 Accuracy 0.8783750534057617\n",
      "Iteration 41040 Training loss 0.041712190955877304 Validation loss 0.046768367290496826 Accuracy 0.877500057220459\n",
      "Iteration 41050 Training loss 0.04159869998693466 Validation loss 0.046848539263010025 Accuracy 0.877500057220459\n",
      "Iteration 41060 Training loss 0.04960217326879501 Validation loss 0.04678205028176308 Accuracy 0.8770000338554382\n",
      "Iteration 41070 Training loss 0.04265567660331726 Validation loss 0.04684208706021309 Accuracy 0.877875030040741\n",
      "Iteration 41080 Training loss 0.04278038442134857 Validation loss 0.046759456396102905 Accuracy 0.878125011920929\n",
      "Iteration 41090 Training loss 0.0366983599960804 Validation loss 0.04685255140066147 Accuracy 0.8787500262260437\n",
      "Iteration 41100 Training loss 0.04164549708366394 Validation loss 0.0469360388815403 Accuracy 0.8771250247955322\n",
      "Iteration 41110 Training loss 0.04113046079874039 Validation loss 0.04754406958818436 Accuracy 0.8726250529289246\n",
      "Iteration 41120 Training loss 0.03226489573717117 Validation loss 0.04729144647717476 Accuracy 0.874625027179718\n",
      "Iteration 41130 Training loss 0.04983179643750191 Validation loss 0.04684378206729889 Accuracy 0.8767500519752502\n",
      "Iteration 41140 Training loss 0.037818919867277145 Validation loss 0.04674097150564194 Accuracy 0.8787500262260437\n",
      "Iteration 41150 Training loss 0.042802099138498306 Validation loss 0.04742977395653725 Accuracy 0.8735000491142273\n",
      "Iteration 41160 Training loss 0.0452071838080883 Validation loss 0.04705677926540375 Accuracy 0.874500036239624\n",
      "Iteration 41170 Training loss 0.03949807956814766 Validation loss 0.04677872732281685 Accuracy 0.8768750429153442\n",
      "Iteration 41180 Training loss 0.042486608028411865 Validation loss 0.046858806163072586 Accuracy 0.8770000338554382\n",
      "Iteration 41190 Training loss 0.04033937305212021 Validation loss 0.046719394624233246 Accuracy 0.8782500624656677\n",
      "Iteration 41200 Training loss 0.039670638740062714 Validation loss 0.047044090926647186 Accuracy 0.8768750429153442\n",
      "Iteration 41210 Training loss 0.04494413733482361 Validation loss 0.046724844723939896 Accuracy 0.877750039100647\n",
      "Iteration 41220 Training loss 0.03289675712585449 Validation loss 0.04673673212528229 Accuracy 0.8771250247955322\n",
      "Iteration 41230 Training loss 0.038603831082582474 Validation loss 0.04676812142133713 Accuracy 0.877625048160553\n",
      "Iteration 41240 Training loss 0.04327801614999771 Validation loss 0.046876490116119385 Accuracy 0.8782500624656677\n",
      "Iteration 41250 Training loss 0.04569718614220619 Validation loss 0.046832673251628876 Accuracy 0.8771250247955322\n",
      "Iteration 41260 Training loss 0.03898429125547409 Validation loss 0.04692970588803291 Accuracy 0.8765000700950623\n",
      "Iteration 41270 Training loss 0.042184896767139435 Validation loss 0.04672667011618614 Accuracy 0.878125011920929\n",
      "Iteration 41280 Training loss 0.036594703793525696 Validation loss 0.0467383973300457 Accuracy 0.878125011920929\n",
      "Iteration 41290 Training loss 0.04451339691877365 Validation loss 0.046855758875608444 Accuracy 0.877750039100647\n",
      "Iteration 41300 Training loss 0.039971914142370224 Validation loss 0.04688210412859917 Accuracy 0.8757500648498535\n",
      "Iteration 41310 Training loss 0.04166889190673828 Validation loss 0.047442734241485596 Accuracy 0.8723750710487366\n",
      "Iteration 41320 Training loss 0.04223398491740227 Validation loss 0.04705765098333359 Accuracy 0.8771250247955322\n",
      "Iteration 41330 Training loss 0.041905730962753296 Validation loss 0.046804167330265045 Accuracy 0.8768750429153442\n",
      "Iteration 41340 Training loss 0.04735102877020836 Validation loss 0.04696744307875633 Accuracy 0.8763750195503235\n",
      "Iteration 41350 Training loss 0.0392770878970623 Validation loss 0.04725146293640137 Accuracy 0.8767500519752502\n",
      "Iteration 41360 Training loss 0.04548131674528122 Validation loss 0.04711133614182472 Accuracy 0.8770000338554382\n",
      "Iteration 41370 Training loss 0.04577012360095978 Validation loss 0.047100234776735306 Accuracy 0.8750000596046448\n",
      "Iteration 41380 Training loss 0.041142839938402176 Validation loss 0.04664216190576553 Accuracy 0.8783750534057617\n",
      "Iteration 41390 Training loss 0.04499812796711922 Validation loss 0.046679604798555374 Accuracy 0.8786250352859497\n",
      "Iteration 41400 Training loss 0.05316166579723358 Validation loss 0.046693965792655945 Accuracy 0.8788750171661377\n",
      "Iteration 41410 Training loss 0.05024697631597519 Validation loss 0.04667118191719055 Accuracy 0.878000020980835\n",
      "Iteration 41420 Training loss 0.04263676702976227 Validation loss 0.046697281301021576 Accuracy 0.8771250247955322\n",
      "Iteration 41430 Training loss 0.04388487711548805 Validation loss 0.0471169538795948 Accuracy 0.8767500519752502\n",
      "Iteration 41440 Training loss 0.04420770704746246 Validation loss 0.046730026602745056 Accuracy 0.8782500624656677\n",
      "Iteration 41450 Training loss 0.04291342571377754 Validation loss 0.047202449291944504 Accuracy 0.877500057220459\n",
      "Iteration 41460 Training loss 0.03937229514122009 Validation loss 0.04671298339962959 Accuracy 0.878125011920929\n",
      "Iteration 41470 Training loss 0.04194449260830879 Validation loss 0.04665958508849144 Accuracy 0.878125011920929\n",
      "Iteration 41480 Training loss 0.04151111841201782 Validation loss 0.047456782311201096 Accuracy 0.8750000596046448\n",
      "Iteration 41490 Training loss 0.03746086359024048 Validation loss 0.046739306300878525 Accuracy 0.877625048160553\n",
      "Iteration 41500 Training loss 0.035189468413591385 Validation loss 0.046755243092775345 Accuracy 0.8768750429153442\n",
      "Iteration 41510 Training loss 0.038995396345853806 Validation loss 0.04673057049512863 Accuracy 0.8767500519752502\n",
      "Iteration 41520 Training loss 0.04203370586037636 Validation loss 0.047483012080192566 Accuracy 0.8728750348091125\n",
      "Iteration 41530 Training loss 0.03834587335586548 Validation loss 0.04666108265519142 Accuracy 0.8786250352859497\n",
      "Iteration 41540 Training loss 0.0449204258620739 Validation loss 0.04680660739541054 Accuracy 0.877500057220459\n",
      "Iteration 41550 Training loss 0.048134759068489075 Validation loss 0.0470455102622509 Accuracy 0.877500057220459\n",
      "Iteration 41560 Training loss 0.0401545986533165 Validation loss 0.046711213886737823 Accuracy 0.878000020980835\n",
      "Iteration 41570 Training loss 0.04460589960217476 Validation loss 0.04671993479132652 Accuracy 0.877625048160553\n",
      "Iteration 41580 Training loss 0.04367892071604729 Validation loss 0.04699935391545296 Accuracy 0.8788750171661377\n",
      "Iteration 41590 Training loss 0.04753856733441353 Validation loss 0.04684150964021683 Accuracy 0.8792500495910645\n",
      "Iteration 41600 Training loss 0.04454965889453888 Validation loss 0.04673153534531593 Accuracy 0.8786250352859497\n",
      "Iteration 41610 Training loss 0.037919677793979645 Validation loss 0.04670697823166847 Accuracy 0.8782500624656677\n",
      "Iteration 41620 Training loss 0.042725466191768646 Validation loss 0.046827174723148346 Accuracy 0.8786250352859497\n",
      "Iteration 41630 Training loss 0.04873524233698845 Validation loss 0.04744832217693329 Accuracy 0.874625027179718\n",
      "Iteration 41640 Training loss 0.0345979668200016 Validation loss 0.04661885276436806 Accuracy 0.878000020980835\n",
      "Iteration 41650 Training loss 0.04042435437440872 Validation loss 0.046715203672647476 Accuracy 0.8783750534057617\n",
      "Iteration 41660 Training loss 0.042830657213926315 Validation loss 0.04670850560069084 Accuracy 0.8786250352859497\n",
      "Iteration 41670 Training loss 0.04226303845643997 Validation loss 0.04668941721320152 Accuracy 0.8783750534057617\n",
      "Iteration 41680 Training loss 0.03951798751950264 Validation loss 0.04759080708026886 Accuracy 0.8710000514984131\n",
      "Iteration 41690 Training loss 0.03423086926341057 Validation loss 0.04676574096083641 Accuracy 0.8763750195503235\n",
      "Iteration 41700 Training loss 0.047875355929136276 Validation loss 0.04708171263337135 Accuracy 0.8750000596046448\n",
      "Iteration 41710 Training loss 0.03660273551940918 Validation loss 0.04726152867078781 Accuracy 0.8771250247955322\n",
      "Iteration 41720 Training loss 0.03637994825839996 Validation loss 0.04674476012587547 Accuracy 0.877875030040741\n",
      "Iteration 41730 Training loss 0.04430929198861122 Validation loss 0.04672696813941002 Accuracy 0.878000020980835\n",
      "Iteration 41740 Training loss 0.041698601096868515 Validation loss 0.04681392014026642 Accuracy 0.8753750324249268\n",
      "Iteration 41750 Training loss 0.03850916400551796 Validation loss 0.046654071658849716 Accuracy 0.877375066280365\n",
      "Iteration 41760 Training loss 0.04739544913172722 Validation loss 0.0466546006500721 Accuracy 0.878000020980835\n",
      "Iteration 41770 Training loss 0.048226673156023026 Validation loss 0.04689362645149231 Accuracy 0.8782500624656677\n",
      "Iteration 41780 Training loss 0.03880270570516586 Validation loss 0.046708617359399796 Accuracy 0.8767500519752502\n",
      "Iteration 41790 Training loss 0.045520417392253876 Validation loss 0.04656291380524635 Accuracy 0.8785000443458557\n",
      "Iteration 41800 Training loss 0.03685217350721359 Validation loss 0.046621453016996384 Accuracy 0.8783750534057617\n",
      "Iteration 41810 Training loss 0.043265704065561295 Validation loss 0.04686698317527771 Accuracy 0.8748750686645508\n",
      "Iteration 41820 Training loss 0.042327649891376495 Validation loss 0.04672388732433319 Accuracy 0.8783750534057617\n",
      "Iteration 41830 Training loss 0.03873366862535477 Validation loss 0.0473349392414093 Accuracy 0.8750000596046448\n",
      "Iteration 41840 Training loss 0.04307999089360237 Validation loss 0.04726245999336243 Accuracy 0.8756250143051147\n",
      "Iteration 41850 Training loss 0.04867270216345787 Validation loss 0.047908611595630646 Accuracy 0.874125063419342\n",
      "Iteration 41860 Training loss 0.04186958819627762 Validation loss 0.047891926020383835 Accuracy 0.8740000128746033\n",
      "Iteration 41870 Training loss 0.04251553863286972 Validation loss 0.04662424698472023 Accuracy 0.877625048160553\n",
      "Iteration 41880 Training loss 0.048114076256752014 Validation loss 0.04664885252714157 Accuracy 0.878000020980835\n",
      "Iteration 41890 Training loss 0.04305819794535637 Validation loss 0.046729471534490585 Accuracy 0.8786250352859497\n",
      "Iteration 41900 Training loss 0.04657566919922829 Validation loss 0.04711920768022537 Accuracy 0.877375066280365\n",
      "Iteration 41910 Training loss 0.042502760887145996 Validation loss 0.047352295368909836 Accuracy 0.8737500309944153\n",
      "Iteration 41920 Training loss 0.040624819695949554 Validation loss 0.04660218954086304 Accuracy 0.8783750534057617\n",
      "Iteration 41930 Training loss 0.04119977727532387 Validation loss 0.047083932906389236 Accuracy 0.8765000700950623\n",
      "Iteration 41940 Training loss 0.0368184968829155 Validation loss 0.04665244370698929 Accuracy 0.878000020980835\n",
      "Iteration 41950 Training loss 0.0433964729309082 Validation loss 0.04701191931962967 Accuracy 0.8765000700950623\n",
      "Iteration 41960 Training loss 0.04381348937749863 Validation loss 0.046646736562252045 Accuracy 0.8770000338554382\n",
      "Iteration 41970 Training loss 0.03913380205631256 Validation loss 0.046574387699365616 Accuracy 0.878000020980835\n",
      "Iteration 41980 Training loss 0.04620080068707466 Validation loss 0.04680737107992172 Accuracy 0.8770000338554382\n",
      "Iteration 41990 Training loss 0.041320111602544785 Validation loss 0.04657459259033203 Accuracy 0.877500057220459\n",
      "Iteration 42000 Training loss 0.03729825094342232 Validation loss 0.048108138144016266 Accuracy 0.8727500438690186\n",
      "Iteration 42010 Training loss 0.04173006862401962 Validation loss 0.046916358172893524 Accuracy 0.8765000700950623\n",
      "Iteration 42020 Training loss 0.039565376937389374 Validation loss 0.04660630226135254 Accuracy 0.877625048160553\n",
      "Iteration 42030 Training loss 0.04114986211061478 Validation loss 0.046761516481637955 Accuracy 0.8765000700950623\n",
      "Iteration 42040 Training loss 0.04487433284521103 Validation loss 0.04663146287202835 Accuracy 0.877375066280365\n",
      "Iteration 42050 Training loss 0.03578557074069977 Validation loss 0.04675210639834404 Accuracy 0.877625048160553\n",
      "Iteration 42060 Training loss 0.05282365158200264 Validation loss 0.04657093435525894 Accuracy 0.878125011920929\n",
      "Iteration 42070 Training loss 0.03633498400449753 Validation loss 0.046701692044734955 Accuracy 0.8765000700950623\n",
      "Iteration 42080 Training loss 0.03843941539525986 Validation loss 0.047373779118061066 Accuracy 0.8728750348091125\n",
      "Iteration 42090 Training loss 0.039720457047224045 Validation loss 0.046588946133852005 Accuracy 0.8785000443458557\n",
      "Iteration 42100 Training loss 0.04790776968002319 Validation loss 0.04686933383345604 Accuracy 0.877500057220459\n",
      "Iteration 42110 Training loss 0.04359113425016403 Validation loss 0.04654966667294502 Accuracy 0.877500057220459\n",
      "Iteration 42120 Training loss 0.04188917949795723 Validation loss 0.04688788205385208 Accuracy 0.878125011920929\n",
      "Iteration 42130 Training loss 0.04912981763482094 Validation loss 0.04657289385795593 Accuracy 0.878000020980835\n",
      "Iteration 42140 Training loss 0.0324777252972126 Validation loss 0.04658733680844307 Accuracy 0.8787500262260437\n",
      "Iteration 42150 Training loss 0.036740440875291824 Validation loss 0.046625103801488876 Accuracy 0.8782500624656677\n",
      "Iteration 42160 Training loss 0.03860168531537056 Validation loss 0.046719301491975784 Accuracy 0.8792500495910645\n",
      "Iteration 42170 Training loss 0.04670887440443039 Validation loss 0.046569596976041794 Accuracy 0.877875030040741\n",
      "Iteration 42180 Training loss 0.04876387491822243 Validation loss 0.04676401987671852 Accuracy 0.8785000443458557\n",
      "Iteration 42190 Training loss 0.03896903991699219 Validation loss 0.04662670940160751 Accuracy 0.8791250586509705\n",
      "Iteration 42200 Training loss 0.04301758110523224 Validation loss 0.046518586575984955 Accuracy 0.8772500157356262\n",
      "Iteration 42210 Training loss 0.04256075248122215 Validation loss 0.046828873455524445 Accuracy 0.8765000700950623\n",
      "Iteration 42220 Training loss 0.03523290902376175 Validation loss 0.046719636768102646 Accuracy 0.877750039100647\n",
      "Iteration 42230 Training loss 0.04161932319402695 Validation loss 0.04695073515176773 Accuracy 0.877875030040741\n",
      "Iteration 42240 Training loss 0.037073198705911636 Validation loss 0.04649991914629936 Accuracy 0.8770000338554382\n",
      "Iteration 42250 Training loss 0.03863589093089104 Validation loss 0.04648267477750778 Accuracy 0.8765000700950623\n",
      "Iteration 42260 Training loss 0.04641987755894661 Validation loss 0.046536583453416824 Accuracy 0.8786250352859497\n",
      "Iteration 42270 Training loss 0.04526730254292488 Validation loss 0.047020476311445236 Accuracy 0.877375066280365\n",
      "Iteration 42280 Training loss 0.036507751792669296 Validation loss 0.04702452942728996 Accuracy 0.87437504529953\n",
      "Iteration 42290 Training loss 0.03509807959198952 Validation loss 0.04663991183042526 Accuracy 0.8795000314712524\n",
      "Iteration 42300 Training loss 0.03984331712126732 Validation loss 0.047022294253110886 Accuracy 0.8783750534057617\n",
      "Iteration 42310 Training loss 0.03885302320122719 Validation loss 0.04685036465525627 Accuracy 0.8762500286102295\n",
      "Iteration 42320 Training loss 0.03917285427451134 Validation loss 0.047564439475536346 Accuracy 0.8716250658035278\n",
      "Iteration 42330 Training loss 0.0382281169295311 Validation loss 0.047226432710886 Accuracy 0.8771250247955322\n",
      "Iteration 42340 Training loss 0.04229661822319031 Validation loss 0.046503931283950806 Accuracy 0.8783750534057617\n",
      "Iteration 42350 Training loss 0.03837358206510544 Validation loss 0.04646565392613411 Accuracy 0.878125011920929\n",
      "Iteration 42360 Training loss 0.045251186937093735 Validation loss 0.0464923121035099 Accuracy 0.8786250352859497\n",
      "Iteration 42370 Training loss 0.037725914269685745 Validation loss 0.04669896140694618 Accuracy 0.877375066280365\n",
      "Iteration 42380 Training loss 0.03704821318387985 Validation loss 0.04691701754927635 Accuracy 0.8751250505447388\n",
      "Iteration 42390 Training loss 0.04352520778775215 Validation loss 0.04786036163568497 Accuracy 0.8733750581741333\n",
      "Iteration 42400 Training loss 0.04434698447585106 Validation loss 0.0466325506567955 Accuracy 0.8771250247955322\n",
      "Iteration 42410 Training loss 0.04557659104466438 Validation loss 0.04763910174369812 Accuracy 0.8705000281333923\n",
      "Iteration 42420 Training loss 0.04233168065547943 Validation loss 0.04653850197792053 Accuracy 0.8782500624656677\n",
      "Iteration 42430 Training loss 0.04227332025766373 Validation loss 0.04647206515073776 Accuracy 0.877750039100647\n",
      "Iteration 42440 Training loss 0.04257091507315636 Validation loss 0.04647946357727051 Accuracy 0.8790000677108765\n",
      "Iteration 42450 Training loss 0.04003534093499184 Validation loss 0.046839211136102676 Accuracy 0.8756250143051147\n",
      "Iteration 42460 Training loss 0.03621673211455345 Validation loss 0.047043222934007645 Accuracy 0.8772500157356262\n",
      "Iteration 42470 Training loss 0.0445575937628746 Validation loss 0.04686363413929939 Accuracy 0.8783750534057617\n",
      "Iteration 42480 Training loss 0.04771290719509125 Validation loss 0.04670363664627075 Accuracy 0.877375066280365\n",
      "Iteration 42490 Training loss 0.04499567300081253 Validation loss 0.04652577266097069 Accuracy 0.8787500262260437\n",
      "Iteration 42500 Training loss 0.037032317370176315 Validation loss 0.046588290482759476 Accuracy 0.8798750638961792\n",
      "Iteration 42510 Training loss 0.046189308166503906 Validation loss 0.04670002683997154 Accuracy 0.8791250586509705\n",
      "Iteration 42520 Training loss 0.04064662754535675 Validation loss 0.04646201804280281 Accuracy 0.8788750171661377\n",
      "Iteration 42530 Training loss 0.03935462236404419 Validation loss 0.04676548019051552 Accuracy 0.8785000443458557\n",
      "Iteration 42540 Training loss 0.046368587762117386 Validation loss 0.04789271950721741 Accuracy 0.8727500438690186\n",
      "Iteration 42550 Training loss 0.044055551290512085 Validation loss 0.046862222254276276 Accuracy 0.878000020980835\n",
      "Iteration 42560 Training loss 0.04215564206242561 Validation loss 0.04738985747098923 Accuracy 0.8728750348091125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training done'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 1.5, 1e-3, 0, 0, 0, 1, 0.01, 10, True, False, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "16479094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer = two_layer_NN(3072,2048,10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_layer.train_layers(x_train,y_train, x_valid, y_valid, 2, 1e-5, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer = three_layer_NN(784, 512, 512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_trained_layer.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44572e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained = three_layer_NN(784, 2048, 2048, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_layer_1_untrained.train_layers(x_train, y_train, x_valid, y_valid, 2, 1e-3, 0, 0, 0, 1, 0.01, 10, True, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a65f0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save = True\n",
    "date = \"06_05_25_\"\n",
    "if save :\n",
    "    # Save the model\n",
    "    model = binary_model_3_layer\n",
    "    model_name = \"CIFAR10_model_(1024+512+512+1)_save_3\"\n",
    "    save_path = \"Classifiers/\" + date + model_name + \"/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model , save_path + model_name + \".pt\")\n",
    "\n",
    "\n",
    "    # Save Architecture\n",
    "    with open(save_path + \"architecture.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model.architecture + str(model.training_time))\n",
    "\n",
    "    # Save performances of the model\n",
    "    os.makedirs(save_path + \"figures/\", exist_ok=True) \n",
    "    # Plot accuracy = f(n)\n",
    "    plt.plot(np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, len(model.accuracy_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0, len(model.accuracy_trajectory)*model.observation_rate, len(model.accuracy_trajectory)), model.accuracy_trajectory)) \n",
    "    np.savetxt(save_path +\"figures/accuracy_of_\" + model_name + \".txt\", data, delimiter =\",\", header=\"n,accuracy\")\n",
    "    plt.show() \n",
    "    \n",
    "    # Plot training and validation loss = f(n)\n",
    "    plt.plot(np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(np.linspace(0,len(model.validation_loss_trajectory)*model.observation_rate, len(model.validation_loss_trajectory)), model.validation_loss_trajectory, label=\"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, len(model.training_loss_trajectory)*model.observation_rate)\n",
    "    plt.ylim(0, np.max([model.training_loss_trajectory, model.validation_loss_trajectory])+0.01)\n",
    "    plt.xlabel(\"Number of iterations\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((np.linspace(0,len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory)), model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/loss_training_\" + model_name + \".txt\", data, delimiter=\",\", header=\"n, training_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot accuracy = f(kappa)\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.accuracy_trajectory)*model.observation_rate+1, len(model.accuracy_trajectory))]\n",
    "    plt.plot(kappa, model.accuracy_trajectory, label = \"Best accuracy \" + str(np.round(np.max(model.accuracy_trajectory), 2)))\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.linspace(0,1,11))\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Accuracy of the \" + model_name + \" on the validation set\", pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_accuracy_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data = np.column_stack((kappa, model.accuracy_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_accuracy_\" + model_name + \".txt\", data, delimiter=\",\", header=\"kappa, accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    kappa = [np.log(n)/np.log(model.input_dimension) for n in np.linspace(1, len(model.training_loss_trajectory)*model.observation_rate, len(model.training_loss_trajectory))]\n",
    "    plt.plot(kappa, model.training_loss_trajectory, label = \"Training loss\", color = \"orange\")\n",
    "    plt.plot(kappa, model.validation_loss_trajectory, label = \"Validation loss\", color = \"green\")\n",
    "    plt.xlim(0, np.max(kappa))\n",
    "    plt.ylim(0, np.max([model.training_loss_trajectory, model.validation_loss_trajectory])+0.01)\n",
    "    plt.xlabel(r\"$\\kappa  = \\frac{ln(n)}{ln(d)}$\")  \n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Loss of the \" + model_name, pad = 20)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".png\", bbox_inches='tight')\n",
    "    plt.savefig(save_path + \"figures/kappa_loss_of_\" + model_name + \".svg\", bbox_inches='tight')\n",
    "    data_training = np.column_stack((kappa, model.training_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_training_\" + model_name + \".txt\", data_training, delimiter=\",\", header=\"kappa, training_loss\")\n",
    "    data_validation = np.column_stack((kappa, model.validation_loss_trajectory))\n",
    "    np.savetxt(save_path + \"figures/kappa_loss_validation_\" + model_name + \".txt\", data_validation, delimiter=\",\", header=\"kappa, validation_loss\")\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"datas\\models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "tensor(0.8692)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import model and plot performances\n",
    "\n",
    "model_name = \"model_2_layer_trained_save_1\"\n",
    "assessed_model = torch.load(\"datas/models/model_2_layer_save_1.pt\", weights_only=False)\n",
    "\n",
    "# Details of the model\n",
    "print(assessed_model.architecture)\n",
    "\n",
    "# Plots of performances\n",
    "accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(accuracy)\n",
    "kappa_accuracy = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_accuracy)\n",
    "loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(loss)\n",
    "kappa_loss = mpimg.imread(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".png\")\n",
    "plt.imshow(kappa_loss)\n",
    "plt.show()\n",
    "\n",
    "# Import datas\n",
    "accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/accuracy_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_accuracy_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_accuracy_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n",
    "kappa_loss_data = np.loadtxt(\"Classifiers/\" + model_name + \"/figures/kappa_loss_of_\" + model_name + \".txt\", delimiter=\",\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8346c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "def ReLU(x):\n",
    "    return torch.max(torch.tensor(0),x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0).to(dtype)\n",
    "\n",
    "def softmax_derivative(s):\n",
    "    \"\"\"\n",
    "    s : Tensor de shape (n_batch, num_classes), dj softmax\n",
    "    Renvoie : Tensor de shape (n_batch, num_classes, num_classes) contenant la jacobienne de softmax pour chaque chantillon\n",
    "    \"\"\"\n",
    "    n, C = s.shape\n",
    "    jacobians = torch.zeros(n, C, C, dtype=s.dtype) # Initialisation du tenseur Jacobien (n_batch, num_classes, num_classes) => (i,j,k) = drive de softmax au logit j du ime batch par rapport au logit k du mme batch\n",
    "    for i in range(n):  # Pour chaque chantillon du batch, on calcule la jacobienne de softmax\n",
    "        si = s[i].unsqueeze(1)  # shape (C, 1) # vecteur softmax(logits) pour la i-me donne du batch\n",
    "        jacobians[i] = torch.diagflat(si) - torch.mm(si,si.t()) # calcul de la jacobienne (matrice des drives croises) de softmax au point z_i = vect(logits_i) # shape (C, C) -> Indice du Jacobien : lignes, Indice des logits : colonnes\n",
    "    return jacobians\n",
    "\n",
    "\n",
    "\n",
    "class three_layer_NN(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_1_size,number_of_classes,lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, esp_init = 1, fraction_batch=0.01):\n",
    "        \"\"\"\n",
    "        Constructor of the two-layer neural network class.\n",
    "        \"\"\"\n",
    "        super(two_layer_NN,self).__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.lr = lr\n",
    "        self.eps_init = esp_init\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.fraction_batch = fraction_batch\n",
    "        self.W1 = torch.randn(hidden_1_size, input_dimension, dtype=dtype) / np.sqrt(input_dimension) # will lead to a sum over \"input_dimension\" coefficients, thus to normalise the norm, we divide by \"input_dimension\"\n",
    "        self.W2 = self.eps_init*torch.randn(number_of_classes, hidden_1_size, dtype=dtype ) / np.sqrt(hidden_1_size)\n",
    "        self.b1 = (2*torch.rand(hidden_1_size,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-1,1)\n",
    "        self.b2 = esp_init*(2*torch.rand(number_of_classes,1,dtype=dtype)-1) # les biais sont initialiss alatoirement selon une loi uniforme U(-eps,eps) # on mutiplie par eps pour que les biais soient petits et ainsi viter l'explosion ou le vanishing des gradients\n",
    "        self.softmax = nn.Softmax(dim=1) # on applique la fonction softmax sur la dimension 1 (c'est  dire sur les classes) # dim=0 correspond  la dimension des batchs\n",
    "        self.validation_loss_trajectory = []\n",
    "        self.training_loss_trajectory = []\n",
    "        self.accuracy_trajectory = []\n",
    "        # Activation = ReLU\n",
    "        # Loss = 0.5*MSE\n",
    "        # Optimizer = GD\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = (torch.mm(self.W1, x.t()) + self.b1).t() # shape (n_data, hidden_1_size) # logits layer 1\n",
    "        h1 = ReLU(z1)  # hidden neurons layer 1\n",
    "        z2 = (torch.mm(self.W2, h1.t()) + self.b2).t() # shape (n_data, number_of_classes ) # logits layer 2\n",
    "        h2 = ReLU(z2) # # hidden neurons layer 2\n",
    "        z3 = torch.mm(self.W3, h2.t() + self.b3)\n",
    "        output = self.softmax(z2) # output layer # shape (n_data, number_of_classes)\n",
    "        return output, z2, h1, z1\n",
    "    \n",
    "    # coef_iter est a ajust en fonction du seuil kappa_eff d'apprentissage des donnes.\n",
    "    def train_layers(self, x_train, y_train, x_valid, y_valid, coef_iter = 1, lr=1e-3, reg1 = 0, reg2 = 0, reg3 = 0, eps_init=0.2, fraction_batch=1e-2, train_layer1=True, train_layer2=True, train_layer3=True):\n",
    "        self.lr = lr\n",
    "        self.reg1 = reg1\n",
    "        self.reg2 = reg2\n",
    "        self.reg3 = reg3\n",
    "        self.eps_init = eps_init\n",
    "        self.fraction_batch = fraction_batch\n",
    "        kappa_max = 1 + coef_iter\n",
    "        max_iter = self.input_dimension**(kappa_max)\n",
    "        print(\"max_iter\", max_iter)\n",
    "        minibatch_size = int(x_train.shape[0]*self.fraction_batch)\n",
    "        N_minibatches = int(max_iter / self.fraction_batch) # Nombre de minibatches utiliss pour l'apprentissage de la premire couche\n",
    "        for i in range(N_minibatches):\n",
    "            indices_minibatch = torch.randperm(x_train.shape[0])[:minibatch_size]\n",
    "            x_minibatch, y_minibatch = x_train[indices_minibatch], y_train[indices_minibatch] # slection un lot de donnes alatoires parmis les donnes d'entrainement \n",
    "            # Calcul de la prdiction\n",
    "            output, z2, h1, z1 = self.forward(x_minibatch)\n",
    "            # Suivi de l'apprentissage\n",
    "            if i % 100 == 0:\n",
    "                training_loss = torch.mean(0.5*(output - y_minibatch)**2, dim=0) # shape (number_of_classes, 1) # on divise par le nombre d'chantillons du minibatch pour obtenir la moyenne empirique de la loss\n",
    "                overall_training_loss = torch.mean(training_loss,dim=0)\n",
    "                validation_loss = torch.mean(0.5*(self.forward(x_valid)[0] - y_valid)**2, dim=0)\n",
    "                overall_validation_loss = torch.mean(validation_loss,dim=0)\n",
    "                self.training_loss_trajectory.append(overall_training_loss.item())\n",
    "                self.validation_loss_trajectory.append(overall_validation_loss.item())\n",
    "                accuracy = torch.mean((torch.argmax(self.forward(x_valid)[0], dim=1) == torch.argmax(y_valid, dim=1)).to(dtype))\n",
    "                self.accuracy_trajectory.append(accuracy.item())\n",
    "                print(\"Iteration\", i, \"Training loss\", overall_training_loss.item(), \"Validation loss\", overall_validation_loss.item(), \"Accuracy\", accuracy.item())\n",
    "            # Loss = 0.5*(output - y_batch)**2 + reg1*||W1||**2 + reg1*||b1||**2 + reg2*||W2||**2 + reg2*||b2||**2 # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1) # on ne pnalise pas les biais car ils sont dj petits (initialiss alatoirement entre -1 et 1)\n",
    "            # Calcul des gradients\n",
    "            grad_output = output - y_minibatch\n",
    "            grad_z2 = torch.einsum('noz, no->nz', softmax_derivative(output), grad_output) # shape(n_data, num_classes) # produit du gradient de la loss par rapport aux outputs shape(n_data, num_classes) et du tenseur des Jacobiennes des outputs (n_data, num_classes, num_classes) # On multiplie la dimension des outputs (=dL/dy_i) du gradient avec la dimensions des outputs de la jacobienne (=dy_i/dz_j) pour obtenir le grad_z2 (dL(y_i)/dy_i)*(dy_i/dz_j)\n",
    "            grad_h1 = torch.mm(grad_z2, self.W2)  # shape (n_data, hidden_1_size)\n",
    "            grad_z1 = grad_h1*ReLU_derivative(z1) # shape (n_data, hidden_1_size)\n",
    "            \n",
    "            # Calcul de la moyenne empirique de dLoss/dW1 par backpropagation\n",
    "            grad_W1 = torch.mm(grad_z1.t(), x_minibatch)/x_minibatch.shape[0] # shape (hidden_1_size, input_dimension)\n",
    "            # Calcul de la moyenne empirique de dLoss/db1 par backpropagation\n",
    "            grad_b1 = torch.mean(grad_z1, dim=0).unsqueeze(1) \n",
    "            # Calcul de la moyenne empirique de dLoss/dW2 par backpropagation\n",
    "            grad_W2 = torch.mm(grad_z2.t(), h1)/x_minibatch.shape[0] # shape (number_of_classes, hidden_1_size)\n",
    "            # Calcul de la moyenne empirique de dLoss/db2 par backpropagation\n",
    "            grad_b2 = torch.mean(grad_z2, dim=0).unsqueeze(1)\n",
    "            \n",
    "            # Mise  jours des paramtres de la premire couche\n",
    "            self.W1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_W1/self.eps_init**2 + self.reg1*self.W1) # on multiplie par (hidden_1_size)**2 pour compenser la dilution du gradient avec l'augmentation de la taille de la couche de neurone (correction de la variance) # on divise par eps^2 pour compenser la faible amplitude des couches suivantes (Rajustement d'chelle) # on pnalise l'augmentation de la norme des poids de W1\n",
    "            self.b1 -= self.lr*(torch.sqrt(torch.tensor(self.hidden_1_size))*grad_b1/self.eps_init**2 + self.reg1*self.b1)\n",
    "            self.W2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_W2/self.eps_init**2 +self.reg2*self.W2)\n",
    "            self.b2 -= self.lr*(torch.sqrt(torch.tensor(self.number_of_classes))*grad_b2/self.eps_init**2 + self.reg2*self.b2)\n",
    "        return \"Training done\"\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
